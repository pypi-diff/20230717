# Comparing `tmp/text2vec-1.2.1.tar.gz` & `tmp/text2vec-1.2.2.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "text2vec-1.2.1.tar", last modified: Wed Jun 21 03:03:37 2023, max compression
+gzip compressed data, was "text2vec-1.2.2.tar", last modified: Mon Jul 17 08:29:15 2023, max compression
```

## Comparing `text2vec-1.2.1.tar` & `text2vec-1.2.2.tar`

### file list

```diff
@@ -1,35 +1,35 @@
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-06-21 03:03:37.425662 text2vec-1.2.1/
--rw-r--r--   0 xuming     (501) staff       (20)    49036 2023-06-21 03:03:37.425133 text2vec-1.2.1/PKG-INFO
--rw-r--r--   0 xuming     (501) staff       (20)    42020 2023-06-21 02:19:56.000000 text2vec-1.2.1/README.md
--rw-r--r--   0 xuming     (501) staff       (20)       38 2023-06-21 03:03:37.425895 text2vec-1.2.1/setup.cfg
--rw-r--r--   0 xuming     (501) staff       (20)     1606 2023-06-14 08:11:02.000000 text2vec-1.2.1/setup.py
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-06-21 03:03:37.386479 text2vec-1.2.1/text2vec/
--rw-r--r--   0 xuming     (501) staff       (20)     1221 2023-06-14 14:22:28.000000 text2vec-1.2.1/text2vec/__init__.py
--rw-r--r--   0 xuming     (501) staff       (20)     3715 2023-06-14 14:12:57.000000 text2vec-1.2.1/text2vec/bertmatching_dataset.py
--rw-r--r--   0 xuming     (501) staff       (20)    18322 2023-06-15 02:51:06.000000 text2vec-1.2.1/text2vec/bertmatching_model.py
--rw-r--r--   0 xuming     (501) staff       (20)     1803 2023-06-14 08:11:02.000000 text2vec-1.2.1/text2vec/bm25.py
--rw-r--r--   0 xuming     (501) staff       (20)     3377 2023-06-18 04:55:14.000000 text2vec-1.2.1/text2vec/cosent_dataset.py
--rw-r--r--   0 xuming     (501) staff       (20)    13840 2023-06-15 02:51:06.000000 text2vec-1.2.1/text2vec/cosent_model.py
--rw-r--r--   0 xuming     (501) staff       (20)     3252 2023-06-14 12:45:18.000000 text2vec-1.2.1/text2vec/ngram.py
--rw-r--r--   0 xuming     (501) staff       (20)    12378 2023-06-20 03:07:55.000000 text2vec-1.2.1/text2vec/sentence_model.py
--rw-r--r--   0 xuming     (501) staff       (20)    14566 2023-06-15 02:51:06.000000 text2vec-1.2.1/text2vec/sentencebert_model.py
--rw-r--r--   0 xuming     (501) staff       (20)     9813 2023-06-20 03:12:38.000000 text2vec-1.2.1/text2vec/similarity.py
--rw-r--r--   0 xuming     (501) staff       (20)     9136 2021-09-11 10:12:24.000000 text2vec-1.2.1/text2vec/stopwords.txt
--rw-r--r--   0 xuming     (501) staff       (20)     6358 2023-06-18 04:53:18.000000 text2vec-1.2.1/text2vec/text_matching_dataset.py
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-06-21 03:03:37.421648 text2vec-1.2.1/text2vec/utils/
--rw-r--r--   0 xuming     (501) staff       (20)        0 2021-09-11 10:12:24.000000 text2vec-1.2.1/text2vec/utils/__init__.py
--rw-r--r--   0 xuming     (501) staff       (20)     6413 2022-01-21 13:50:54.000000 text2vec-1.2.1/text2vec/utils/distance.py
--rw-r--r--   0 xuming     (501) staff       (20)    15066 2022-03-11 08:18:40.000000 text2vec-1.2.1/text2vec/utils/get_file.py
--rw-r--r--   0 xuming     (501) staff       (20)     1595 2023-06-14 13:27:05.000000 text2vec-1.2.1/text2vec/utils/io_util.py
--rw-r--r--   0 xuming     (501) staff       (20)     5630 2022-02-13 15:54:49.000000 text2vec-1.2.1/text2vec/utils/rank_bm25.py
--rw-r--r--   0 xuming     (501) staff       (20)      859 2022-02-25 17:04:46.000000 text2vec-1.2.1/text2vec/utils/stats_util.py
--rw-r--r--   0 xuming     (501) staff       (20)     1916 2022-01-23 04:13:14.000000 text2vec-1.2.1/text2vec/utils/tokenizer.py
--rw-r--r--   0 xuming     (501) staff       (20)       84 2023-06-21 03:03:19.000000 text2vec-1.2.1/text2vec/version.py
--rw-r--r--   0 xuming     (501) staff       (20)     6180 2023-06-14 12:45:19.000000 text2vec-1.2.1/text2vec/word2vec.py
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-06-21 03:03:37.413588 text2vec-1.2.1/text2vec.egg-info/
--rw-r--r--   0 xuming     (501) staff       (20)    49036 2023-06-21 03:03:37.000000 text2vec-1.2.1/text2vec.egg-info/PKG-INFO
--rw-r--r--   0 xuming     (501) staff       (20)      751 2023-06-21 03:03:37.000000 text2vec-1.2.1/text2vec.egg-info/SOURCES.txt
--rw-r--r--   0 xuming     (501) staff       (20)        1 2023-06-21 03:03:37.000000 text2vec-1.2.1/text2vec.egg-info/dependency_links.txt
--rw-r--r--   0 xuming     (501) staff       (20)        1 2022-05-17 02:41:49.000000 text2vec-1.2.1/text2vec.egg-info/not-zip-safe
--rw-r--r--   0 xuming     (501) staff       (20)       74 2023-06-21 03:03:37.000000 text2vec-1.2.1/text2vec.egg-info/requires.txt
--rw-r--r--   0 xuming     (501) staff       (20)        9 2023-06-21 03:03:37.000000 text2vec-1.2.1/text2vec.egg-info/top_level.txt
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-07-17 08:29:15.290018 text2vec-1.2.2/
+-rw-r--r--   0 xuming     (501) staff       (20)    51655 2023-07-17 08:29:15.289376 text2vec-1.2.2/PKG-INFO
+-rw-r--r--   0 xuming     (501) staff       (20)    44575 2023-07-16 07:50:04.000000 text2vec-1.2.2/README.md
+-rw-r--r--   0 xuming     (501) staff       (20)       38 2023-07-17 08:29:15.290386 text2vec-1.2.2/setup.cfg
+-rw-r--r--   0 xuming     (501) staff       (20)     1606 2023-06-14 08:11:02.000000 text2vec-1.2.2/setup.py
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-07-17 08:29:15.273036 text2vec-1.2.2/text2vec/
+-rw-r--r--   0 xuming     (501) staff       (20)     1221 2023-06-14 14:22:28.000000 text2vec-1.2.2/text2vec/__init__.py
+-rw-r--r--   0 xuming     (501) staff       (20)     3715 2023-06-14 14:12:57.000000 text2vec-1.2.2/text2vec/bertmatching_dataset.py
+-rw-r--r--   0 xuming     (501) staff       (20)    19099 2023-07-16 04:07:08.000000 text2vec-1.2.2/text2vec/bertmatching_model.py
+-rw-r--r--   0 xuming     (501) staff       (20)     1803 2023-06-14 08:11:02.000000 text2vec-1.2.2/text2vec/bm25.py
+-rw-r--r--   0 xuming     (501) staff       (20)     3377 2023-06-18 04:55:14.000000 text2vec-1.2.2/text2vec/cosent_dataset.py
+-rw-r--r--   0 xuming     (501) staff       (20)    14698 2023-07-16 04:07:08.000000 text2vec-1.2.2/text2vec/cosent_model.py
+-rw-r--r--   0 xuming     (501) staff       (20)     3252 2023-06-14 12:45:18.000000 text2vec-1.2.2/text2vec/ngram.py
+-rw-r--r--   0 xuming     (501) staff       (20)    12614 2023-07-16 04:07:08.000000 text2vec-1.2.2/text2vec/sentence_model.py
+-rw-r--r--   0 xuming     (501) staff       (20)    15952 2023-07-16 04:07:08.000000 text2vec-1.2.2/text2vec/sentencebert_model.py
+-rw-r--r--   0 xuming     (501) staff       (20)     9813 2023-06-20 03:12:38.000000 text2vec-1.2.2/text2vec/similarity.py
+-rw-r--r--   0 xuming     (501) staff       (20)     9136 2021-09-11 10:12:24.000000 text2vec-1.2.2/text2vec/stopwords.txt
+-rw-r--r--   0 xuming     (501) staff       (20)     6358 2023-06-18 04:53:18.000000 text2vec-1.2.2/text2vec/text_matching_dataset.py
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-07-17 08:29:15.287690 text2vec-1.2.2/text2vec/utils/
+-rw-r--r--   0 xuming     (501) staff       (20)        0 2021-09-11 10:12:24.000000 text2vec-1.2.2/text2vec/utils/__init__.py
+-rw-r--r--   0 xuming     (501) staff       (20)     6506 2023-07-17 08:26:35.000000 text2vec-1.2.2/text2vec/utils/distance.py
+-rw-r--r--   0 xuming     (501) staff       (20)    15066 2022-03-11 08:18:40.000000 text2vec-1.2.2/text2vec/utils/get_file.py
+-rw-r--r--   0 xuming     (501) staff       (20)     1595 2023-06-14 13:27:05.000000 text2vec-1.2.2/text2vec/utils/io_util.py
+-rw-r--r--   0 xuming     (501) staff       (20)     5630 2022-02-13 15:54:49.000000 text2vec-1.2.2/text2vec/utils/rank_bm25.py
+-rw-r--r--   0 xuming     (501) staff       (20)      859 2022-02-25 17:04:46.000000 text2vec-1.2.2/text2vec/utils/stats_util.py
+-rw-r--r--   0 xuming     (501) staff       (20)     1916 2022-01-23 04:13:14.000000 text2vec-1.2.2/text2vec/utils/tokenizer.py
+-rw-r--r--   0 xuming     (501) staff       (20)       84 2023-07-17 08:28:26.000000 text2vec-1.2.2/text2vec/version.py
+-rw-r--r--   0 xuming     (501) staff       (20)     6180 2023-06-14 12:45:19.000000 text2vec-1.2.2/text2vec/word2vec.py
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-07-17 08:29:15.279471 text2vec-1.2.2/text2vec.egg-info/
+-rw-r--r--   0 xuming     (501) staff       (20)    51655 2023-07-17 08:29:14.000000 text2vec-1.2.2/text2vec.egg-info/PKG-INFO
+-rw-r--r--   0 xuming     (501) staff       (20)      751 2023-07-17 08:29:14.000000 text2vec-1.2.2/text2vec.egg-info/SOURCES.txt
+-rw-r--r--   0 xuming     (501) staff       (20)        1 2023-07-17 08:29:14.000000 text2vec-1.2.2/text2vec.egg-info/dependency_links.txt
+-rw-r--r--   0 xuming     (501) staff       (20)        1 2022-05-17 02:41:49.000000 text2vec-1.2.2/text2vec.egg-info/not-zip-safe
+-rw-r--r--   0 xuming     (501) staff       (20)       74 2023-07-17 08:29:14.000000 text2vec-1.2.2/text2vec.egg-info/requires.txt
+-rw-r--r--   0 xuming     (501) staff       (20)        9 2023-07-17 08:29:14.000000 text2vec-1.2.2/text2vec.egg-info/top_level.txt
```

### Comparing `text2vec-1.2.1/PKG-INFO` & `text2vec-1.2.2/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: text2vec
-Version: 1.2.1
+Version: 1.2.2
 Summary: Text to vector Tool, encode text
 Home-page: https://github.com/shibing624/text2vec
 Author: XuMing
 Author-email: xuming624@qq.com
 License: Apache License 2.0
 Description: [**ğŸ‡¨ğŸ‡³ä¸­æ–‡**](https://github.com/shibing624/text2vec/blob/master/README.md) | [**ğŸŒEnglish**](https://github.com/shibing624/text2vec/blob/master/README_EN.md) | [**ğŸ“–æ–‡æ¡£/Docs**](https://github.com/shibing624/text2vec/wiki) | [**ğŸ¤–æ¨¡å‹/Models**](https://huggingface.co/shibing624) 
         
@@ -27,54 +27,57 @@
         
         
         **Text2vec**: Text to Vector, Get Sentence Embeddings. æ–‡æœ¬å‘é‡åŒ–ï¼ŒæŠŠæ–‡æœ¬(åŒ…æ‹¬è¯ã€å¥å­ã€æ®µè½)è¡¨å¾ä¸ºå‘é‡çŸ©é˜µã€‚
         
         **text2vec**å®ç°äº†Word2Vecã€RankBM25ã€BERTã€Sentence-BERTã€CoSENTç­‰å¤šç§æ–‡æœ¬è¡¨å¾ã€æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—æ¨¡å‹ï¼Œå¹¶åœ¨æ–‡æœ¬è¯­ä¹‰åŒ¹é…ï¼ˆç›¸ä¼¼åº¦è®¡ç®—ï¼‰ä»»åŠ¡ä¸Šæ¯”è¾ƒäº†å„æ¨¡å‹çš„æ•ˆæœã€‚
         
         ### News
+        [2023/06/22] v1.2.2ç‰ˆæœ¬: å‘å¸ƒäº†å¤šè¯­è¨€åŒ¹é…æ¨¡å‹[shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)ï¼Œç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`ç”¨äººå·¥æŒ‘é€‰åçš„å¤šè¯­è¨€STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-multilingual-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-multilingual-dataset)è®­ç»ƒå¾—åˆ°ï¼Œå¹¶åœ¨ä¸­è‹±æ–‡æµ‹è¯•é›†è¯„ä¼°ç›¸å¯¹äºåŸæ¨¡å‹æ•ˆæœæœ‰æå‡ï¼Œè¯¦è§[Release-v1.2.2](https://github.com/shibing624/text2vec/releases/tag/1.2.2)
+        
         [2023/06/19] v1.2.1ç‰ˆæœ¬: æ›´æ–°äº†ä¸­æ–‡åŒ¹é…æ¨¡å‹`shibing624/text2vec-base-chinese-nli`ä¸ºæ–°ç‰ˆ[shibing624/text2vec-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-sentence)ï¼Œé’ˆå¯¹CoSENTçš„lossè®¡ç®—å¯¹æ’åºæ•æ„Ÿç‰¹ç‚¹ï¼Œäººå·¥æŒ‘é€‰å¹¶æ•´ç†å‡ºé«˜è´¨é‡çš„æœ‰ç›¸å…³æ€§æ’åºçš„STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)ï¼Œåœ¨å„è¯„ä¼°é›†è¡¨ç°ç›¸å¯¹ä¹‹å‰æœ‰æå‡ï¼›å‘å¸ƒäº†é€‚ç”¨äºs2pçš„ä¸­æ–‡åŒ¹é…æ¨¡å‹[shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase)ï¼Œè¯¦è§[Release-v1.2.1](https://github.com/shibing624/text2vec/releases/tag/1.2.1)
         
         [2023/06/15] v1.2.0ç‰ˆæœ¬: å‘å¸ƒäº†ä¸­æ–‡åŒ¹é…æ¨¡å‹[shibing624/text2vec-base-chinese-nli](https://huggingface.co/shibing624/text2vec-base-chinese-nli)ï¼ŒåŸºäº`nghuyong/ernie-3.0-base-zh`æ¨¡å‹ï¼Œä½¿ç”¨äº†ä¸­æ–‡NLIæ•°æ®é›†[shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh)å…¨éƒ¨è¯­æ–™è®­ç»ƒçš„CoSENTæ–‡æœ¬åŒ¹é…æ¨¡å‹ï¼Œåœ¨å„è¯„ä¼°é›†è¡¨ç°æå‡æ˜æ˜¾ï¼Œè¯¦è§[Release-v1.2.0](https://github.com/shibing624/text2vec/releases/tag/1.2.0)
         
         [2022/03/12] v1.1.4ç‰ˆæœ¬: å‘å¸ƒäº†ä¸­æ–‡åŒ¹é…æ¨¡å‹[shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)ï¼ŒåŸºäºä¸­æ–‡STSè®­ç»ƒé›†è®­ç»ƒçš„CoSENTåŒ¹é…æ¨¡å‹ã€‚è¯¦è§[Release-v1.1.4](https://github.com/shibing624/text2vec/releases/tag/1.1.4)
         
         
         **Guide**
-        - [Feature](#Feature)
+        - [Features](#Features)
         - [Evaluation](#Evaluation)
         - [Install](#install)
         - [Usage](#usage)
         - [Contact](#Contact)
-        - [Reference](#reference)
+        - [References](#references)
         
         
-        # Feature
+        ## Features
         ### æ–‡æœ¬å‘é‡è¡¨ç¤ºæ¨¡å‹
         - [Word2Vec](https://github.com/shibing624/text2vec/blob/master/text2vec/word2vec.py)ï¼šé€šè¿‡è…¾è®¯AI Labå¼€æºçš„å¤§è§„æ¨¡é«˜è´¨é‡ä¸­æ–‡[è¯å‘é‡æ•°æ®ï¼ˆ800ä¸‡ä¸­æ–‡è¯è½»é‡ç‰ˆï¼‰](https://pan.baidu.com/s/1La4U4XNFe8s5BJqxPQpeiQ) (æ–‡ä»¶åï¼šlight_Tencent_AILab_ChineseEmbedding.bin å¯†ç : taweï¼‰å®ç°è¯å‘é‡æ£€ç´¢ï¼Œæœ¬é¡¹ç›®å®ç°äº†å¥å­ï¼ˆè¯å‘é‡æ±‚å¹³å‡ï¼‰çš„word2vecå‘é‡è¡¨ç¤º
         - [SBERT(Sentence-BERT)](https://github.com/shibing624/text2vec/blob/master/text2vec/sentencebert_model.py)ï¼šæƒè¡¡æ€§èƒ½å’Œæ•ˆç‡çš„å¥å‘é‡è¡¨ç¤ºæ¨¡å‹ï¼Œè®­ç»ƒæ—¶é€šè¿‡æœ‰ç›‘ç£è®­ç»ƒä¸Šå±‚åˆ†ç±»å‡½æ•°ï¼Œæ–‡æœ¬åŒ¹é…é¢„æµ‹æ—¶ç›´æ¥å¥å­å‘é‡åšä½™å¼¦ï¼Œæœ¬é¡¹ç›®åŸºäºPyTorchå¤ç°äº†Sentence-BERTæ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹
         - [CoSENT(Cosine Sentence)](https://github.com/shibing624/text2vec/blob/master/text2vec/cosent_model.py)ï¼šCoSENTæ¨¡å‹æå‡ºäº†ä¸€ç§æ’åºçš„æŸå¤±å‡½æ•°ï¼Œä½¿è®­ç»ƒè¿‡ç¨‹æ›´è´´è¿‘é¢„æµ‹ï¼Œæ¨¡å‹æ”¶æ•›é€Ÿåº¦å’Œæ•ˆæœæ¯”Sentence-BERTæ›´å¥½ï¼Œæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†CoSENTæ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹
         
         è¯¦ç»†æ–‡æœ¬å‘é‡è¡¨ç¤ºæ–¹æ³•è§wiki: [æ–‡æœ¬å‘é‡è¡¨ç¤ºæ–¹æ³•](https://github.com/shibing624/text2vec/wiki/%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95)
-        # Evaluation
+        ## Evaluation
         
         æ–‡æœ¬åŒ¹é…
         
         #### è‹±æ–‡åŒ¹é…æ•°æ®é›†çš„è¯„æµ‹ç»“æœï¼š
         
         
-        | Arch   | BaseModel                                        | Model                            | English-STS-B | 
-        |:-------|:------------------------------------------------|:-------------------------------------|:-------------:|
-        | GloVe  | glove                                           | Avg_word_embeddings_glove_6B_300d    |     61.77     |
-        | BERT   | bert-base-uncased                               | BERT-base-cls                        |     20.29     |
-        | BERT   | bert-base-uncased                               | BERT-base-first_last_avg             |     59.04     |
-        | BERT   | bert-base-uncased                               | BERT-base-first_last_avg-whiten(NLI) |     63.65     |
-        | SBERT  | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-cls                   |     73.65     |
-        | SBERT  | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-first_last_avg        |     77.96     |
-        | CoSENT | bert-base-uncased                               | CoSENT-base-first_last_avg           |     69.93     |
-        | CoSENT | sentence-transformers/bert-base-nli-mean-tokens | CoSENT-base-nli-first_last_avg       |     79.68     |
+        | Arch   | BaseModel                                        | Model                                                                                                                | English-STS-B | 
+        |:-------|:------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------|:-------------:|
+        | GloVe  | glove                                           | Avg_word_embeddings_glove_6B_300d                                                                                    |     61.77     |
+        | BERT   | bert-base-uncased                               | BERT-base-cls                                                                                                        |     20.29     |
+        | BERT   | bert-base-uncased                               | BERT-base-first_last_avg                                                                                             |     59.04     |
+        | BERT   | bert-base-uncased                               | BERT-base-first_last_avg-whiten(NLI)                                                                                 |     63.65     |
+        | SBERT  | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-cls                                                                                                   |     73.65     |
+        | SBERT  | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-first_last_avg                                                                                        |     77.96     |
+        | CoSENT | bert-base-uncased                               | CoSENT-base-first_last_avg                                                                                           |     69.93     |
+        | CoSENT | sentence-transformers/bert-base-nli-mean-tokens | CoSENT-base-nli-first_last_avg                                                                                       |     79.68     |
+        | CoSENT | sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 | [shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)                |     80.12     |
         
         #### ä¸­æ–‡åŒ¹é…æ•°æ®é›†çš„è¯„æµ‹ç»“æœï¼š
         
         
         | Arch   | BaseModel                    | Model           | ATEC  |  BQ   | LCQMC | PAWSX | STS-B |  Avg  | 
         |:-------|:----------------------------|:--------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|
         | SBERT  | bert-base-chinese           | SBERT-bert-base     | 46.36 | 70.36 | 78.72 | 46.86 | 66.41 | 61.74 |
@@ -83,59 +86,60 @@
         | CoSENT | bert-base-chinese           | CoSENT-bert-base    | 49.74 | 72.38 | 78.69 | 60.00 | 79.27 | 68.01 |
         | CoSENT | hfl/chinese-macbert-base    | CoSENT-macbert-base | 50.39 | 72.93 | 79.17 | 60.86 | 79.30 | 68.53 |
         | CoSENT | hfl/chinese-roberta-wwm-ext | CoSENT-roberta-ext  | 50.81 | 71.45 | 79.31 | 61.56 | 79.96 | 68.61 |
         
         è¯´æ˜ï¼š
         - ç»“æœè¯„æµ‹æŒ‡æ ‡ï¼šspearmanç³»æ•°
         - ä¸ºè¯„æµ‹æ¨¡å‹èƒ½åŠ›ï¼Œç»“æœå‡åªç”¨è¯¥æ•°æ®é›†çš„trainè®­ç»ƒï¼Œåœ¨testä¸Šè¯„ä¼°å¾—åˆ°çš„è¡¨ç°ï¼Œæ²¡ç”¨å¤–éƒ¨æ•°æ®
+        - `SBERT-macbert-base`æ¨¡å‹ï¼Œæ˜¯ç”¨SBertæ–¹æ³•è®­ç»ƒï¼Œè¿è¡Œ[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹
+        - `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`æ¨¡å‹æ˜¯ç”¨SBertè®­ç»ƒï¼Œæ˜¯`paraphrase-MiniLM-L12-v2`æ¨¡å‹çš„å¤šè¯­è¨€ç‰ˆæœ¬ï¼Œæ”¯æŒä¸­æ–‡ã€è‹±æ–‡ç­‰
         
         
         ### Release Models
         - æœ¬é¡¹ç›®releaseæ¨¡å‹çš„ä¸­æ–‡åŒ¹é…è¯„æµ‹ç»“æœï¼š
         
-        | Arch       | BaseModel                         | Model                                                                                                                                             | ATEC  |  BQ   | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-dc |    Avg    |  QPS  |
-        |:-----------|:----------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-------:|:-------:|:---------:|:-----:|
-        | Word2Vec   | word2vec                          | [w2v-light-tencent-chinese](https://ai.tencent.com/ailab/nlp/en/download.html)                                                                    | 20.00 | 31.49 | 59.46 | 2.57  | 55.78 |  55.04  |  20.70  |   35.03   | 23769 |
-        | SBERT      | xlm-roberta-base                  | [sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) | 18.42 | 38.52 | 63.96 | 10.14 | 78.90 |  63.01  |  52.28  |   46.46   | 3138  |
-        | Instructor | hfl/chinese-roberta-wwm-ext       | [moka-ai/m3e-base](https://huggingface.co/moka-ai/m3e-base)                                                                                       | 41.27 | 63.81 | 74.87 | 12.20 | 76.96 |  75.83  |  60.55  |   57.93   | 2980  |
-        | CoSENT     | hfl/chinese-macbert-base          | [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)                                                       | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 |  70.27  |  50.42  |   51.61   | 3008  |
-        | CoSENT     | hfl/chinese-lert-large            | [GanymedeNil/text2vec-large-chinese](https://huggingface.co/GanymedeNil/text2vec-large-chinese)                                                   | 32.61 | 44.59 | 69.30 | 14.51 | 79.44 |  73.01  |  59.04  |   53.12   | 2092  |
-        | CoSENT     | nghuyong/ernie-3.0-base-zh        | [shibing624/text2vec-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-sentence)                                     | 43.37 | 61.43 | 73.48 | 38.90 | 78.25 |  70.60  |  53.08  |   59.87   | 3089  |
-        | CoSENT     | nghuyong/ernie-3.0-base-zh        | [shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase)                                 | 44.89 | 63.58 | 74.24 | 40.90 | 78.93 |  76.70  |  63.30  | **63.08** | 3066  |
+        | Arch       | BaseModel                                                    | Model                                                                                                                                             | ATEC  |  BQ   | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-dc |    Avg    |  QPS  |
+        |:-----------|:-------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-------:|:-------:|:---------:|:-----:|
+        | Word2Vec   | word2vec                                                     | [w2v-light-tencent-chinese](https://ai.tencent.com/ailab/nlp/en/download.html)                                                                    | 20.00 | 31.49 | 59.46 | 2.57  | 55.78 |  55.04  |  20.70  |   35.03   | 23769 |
+        | SBERT      | xlm-roberta-base                                             | [sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) | 18.42 | 38.52 | 63.96 | 10.14 | 78.90 |  63.01  |  52.28  |   46.46   | 3138  |
+        | CoSENT     | hfl/chinese-macbert-base                                     | [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)                                                       | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 |  70.27  |  50.42  |   51.61   | 3008  |
+        | CoSENT     | hfl/chinese-lert-large                                       | [GanymedeNil/text2vec-large-chinese](https://huggingface.co/GanymedeNil/text2vec-large-chinese)                                                   | 32.61 | 44.59 | 69.30 | 14.51 | 79.44 |  73.01  |  59.04  |   53.12   | 2092  |
+        | CoSENT     | nghuyong/ernie-3.0-base-zh                                   | [shibing624/text2vec-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-sentence)                                     | 43.37 | 61.43 | 73.48 | 38.90 | 78.25 |  70.60  |  53.08  |   59.87   | 3089  |
+        | CoSENT     | nghuyong/ernie-3.0-base-zh                                   | [shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase)                                 | 44.89 | 63.58 | 74.24 | 40.90 | 78.93 |  76.70  |  63.30  | **63.08** | 3066  |
+        | CoSENT     | sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2  | [shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)                                             | 32.39 | 50.33 | 65.64 | 32.56 | 74.45 |  68.88  |  51.17  |   53.67   | 3138  |
         
         
         è¯´æ˜ï¼š
         - ç»“æœè¯„æµ‹æŒ‡æ ‡ï¼šspearmanç³»æ•°
-        - æ¨¡å‹è®­ç»ƒå®éªŒæŠ¥å‘Šï¼š[å®éªŒæŠ¥å‘Š](https://github.com/shibing624/text2vec/blob/master/docs/model_report.md)
         - `shibing624/text2vec-base-chinese`æ¨¡å‹ï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`hfl/chinese-macbert-base`åœ¨ä¸­æ–‡STS-Bæ•°æ®è®­ç»ƒå¾—åˆ°ï¼Œå¹¶åœ¨ä¸­æ–‡STS-Bæµ‹è¯•é›†è¯„ä¼°è¾¾åˆ°è¾ƒå¥½æ•ˆæœï¼Œè¿è¡Œ[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹æ–‡ä»¶å·²ç»ä¸Šä¼ HF model hubï¼Œä¸­æ–‡é€šç”¨è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èä½¿ç”¨
         - `shibing624/text2vec-base-chinese-sentence`æ¨¡å‹ï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`nghuyong/ernie-3.0-base-zh`ç”¨äººå·¥æŒ‘é€‰åçš„ä¸­æ–‡STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)è®­ç»ƒå¾—åˆ°ï¼Œå¹¶åœ¨ä¸­æ–‡å„NLIæµ‹è¯•é›†è¯„ä¼°è¾¾åˆ°è¾ƒå¥½æ•ˆæœï¼Œè¿è¡Œ[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹æ–‡ä»¶å·²ç»ä¸Šä¼ HF model hubï¼Œä¸­æ–‡s2s(å¥å­vså¥å­)è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èä½¿ç”¨
         - `shibing624/text2vec-base-chinese-paraphrase`æ¨¡å‹ï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`nghuyong/ernie-3.0-base-zh`ç”¨äººå·¥æŒ‘é€‰åçš„ä¸­æ–‡STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-chinese-paraphrase-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-paraphrase-dataset)ï¼Œæ•°æ®é›†ç›¸å¯¹äº[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)åŠ å…¥äº†s2p(sentence to paraphrase)æ•°æ®ï¼Œå¼ºåŒ–äº†å…¶é•¿æ–‡æœ¬çš„è¡¨å¾èƒ½åŠ›ï¼Œå¹¶åœ¨ä¸­æ–‡å„NLIæµ‹è¯•é›†è¯„ä¼°è¾¾åˆ°SOTAï¼Œè¿è¡Œ[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹æ–‡ä»¶å·²ç»ä¸Šä¼ HF model hubï¼Œä¸­æ–‡s2p(å¥å­vsæ®µè½)è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èä½¿ç”¨
-        - `SBERT-macbert-base`æ¨¡å‹ï¼Œæ˜¯ç”¨SBERTæ–¹æ³•è®­ç»ƒï¼Œè¿è¡Œ[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹
-        - `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`æ¨¡å‹æ˜¯ç”¨SBERTè®­ç»ƒï¼Œæ˜¯`paraphrase-MiniLM-L12-v2`æ¨¡å‹çš„å¤šè¯­è¨€ç‰ˆæœ¬ï¼Œæ”¯æŒä¸­æ–‡ã€è‹±æ–‡ç­‰
+        - `shibing624/text2vec-base-multilingual`æ¨¡å‹ï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`ç”¨äººå·¥æŒ‘é€‰åçš„å¤šè¯­è¨€STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-multilingual-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-multilingual-dataset)è®­ç»ƒå¾—åˆ°ï¼Œå¹¶åœ¨ä¸­è‹±æ–‡æµ‹è¯•é›†è¯„ä¼°ç›¸å¯¹äºåŸæ¨¡å‹æ•ˆæœæœ‰æå‡ï¼Œè¿è¡Œ[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹æ–‡ä»¶å·²ç»ä¸Šä¼ HF model hubï¼Œå¤šè¯­è¨€è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èä½¿ç”¨
         - `w2v-light-tencent-chinese`æ˜¯è…¾è®¯è¯å‘é‡çš„Word2Vecæ¨¡å‹ï¼ŒCPUåŠ è½½ä½¿ç”¨ï¼Œé€‚ç”¨äºä¸­æ–‡å­—é¢åŒ¹é…ä»»åŠ¡å’Œç¼ºå°‘æ•°æ®çš„å†·å¯åŠ¨æƒ…å†µ
         - å„é¢„è®­ç»ƒæ¨¡å‹å‡å¯ä»¥é€šè¿‡transformersè°ƒç”¨ï¼Œå¦‚MacBERTæ¨¡å‹ï¼š`--model_name hfl/chinese-macbert-base` æˆ–è€…robertaæ¨¡å‹ï¼š`--model_name uer/roberta-medium-wwm-chinese-cluecorpussmall`
         - ä¸ºæµ‹è¯„æ¨¡å‹çš„é²æ£’æ€§ï¼ŒåŠ å…¥äº†æœªè®­ç»ƒè¿‡çš„SOHUæµ‹è¯•é›†ï¼Œç”¨äºæµ‹è¯•æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼›ä¸ºè¾¾åˆ°å¼€ç®±å³ç”¨çš„å®ç”¨æ•ˆæœï¼Œä½¿ç”¨äº†æœé›†åˆ°çš„å„ä¸­æ–‡åŒ¹é…æ•°æ®é›†ï¼Œæ•°æ®é›†ä¹Ÿä¸Šä¼ åˆ°HF datasets[é“¾æ¥è§ä¸‹æ–¹](#æ•°æ®é›†)
         - ä¸­æ–‡åŒ¹é…ä»»åŠ¡å®éªŒè¡¨æ˜ï¼Œpoolingæœ€ä¼˜æ˜¯`EncoderType.FIRST_LAST_AVG`å’Œ`EncoderType.MEAN`ï¼Œä¸¤è€…é¢„æµ‹æ•ˆæœå·®å¼‚å¾ˆå°
         - ä¸­æ–‡åŒ¹é…è¯„æµ‹ç»“æœå¤ç°ï¼Œå¯ä»¥ä¸‹è½½ä¸­æ–‡åŒ¹é…æ•°æ®é›†åˆ°`examples/data`ï¼Œè¿è¡Œ[tests/test_model_spearman.py](https://github.com/shibing624/text2vec/blob/master/tests/test_model_spearman.py)ä»£ç å¤ç°è¯„æµ‹ç»“æœ
         - QPSçš„GPUæµ‹è¯•ç¯å¢ƒæ˜¯Tesla V100ï¼Œæ˜¾å­˜32GB
         
-        # Demo
+        æ¨¡å‹è®­ç»ƒå®éªŒæŠ¥å‘Šï¼š[å®éªŒæŠ¥å‘Š](https://github.com/shibing624/text2vec/blob/master/docs/model_report.md)
+        ## Demo
         
         Official Demo: https://www.mulanai.com/product/short_text_sim/
         
         HuggingFace Demo: https://huggingface.co/spaces/shibing624/text2vec
         
         ![](docs/hf.png)
         
         run example: [examples/gradio_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/gradio_demo.py) to see the demo:
         ```shell
         python examples/gradio_demo.py
         ```
         
-        # Install
+        ## Install
         ```shell
         pip install torch # conda install pytorch
         pip install -U text2vec
         ```
         
         or
         
@@ -144,17 +148,17 @@
         pip install -r requirements.txt
         
         git clone https://github.com/shibing624/text2vec.git
         cd text2vec
         pip install --no-deps .
         ```
         
-        # Usage
+        ## Usage
         
-        ## æ–‡æœ¬å‘é‡è¡¨å¾
+        ### æ–‡æœ¬å‘é‡è¡¨å¾
         
         åŸºäº`pretrained model`è®¡ç®—æ–‡æœ¬å‘é‡ï¼š
         
         ```zsh
         >>> from text2vec import SentenceModel
         >>> m = SentenceModel()
         >>> m.encode("å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡")
@@ -194,16 +198,16 @@
         
         
         if __name__ == "__main__":
             # ä¸­æ–‡å¥å‘é‡æ¨¡å‹(CoSENT)ï¼Œä¸­æ–‡è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èï¼Œæ”¯æŒfine-tuneç»§ç»­è®­ç»ƒ
             t2v_model = SentenceModel("shibing624/text2vec-base-chinese")
             compute_emb(t2v_model)
         
-            # æ”¯æŒå¤šè¯­è¨€çš„å¥å‘é‡æ¨¡å‹ï¼ˆSentence-BERTï¼‰ï¼Œè‹±æ–‡è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èï¼Œæ”¯æŒfine-tuneç»§ç»­è®­ç»ƒ
-            sbert_model = SentenceModel("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
+            # æ”¯æŒå¤šè¯­è¨€çš„å¥å‘é‡æ¨¡å‹ï¼ˆCoSENTï¼‰ï¼Œå¤šè¯­è¨€ï¼ˆåŒ…æ‹¬ä¸­è‹±æ–‡ï¼‰è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èï¼Œæ”¯æŒfine-tuneç»§ç»­è®­ç»ƒ
+            sbert_model = SentenceModel("shibing624/text2vec-base-multilingual")
             compute_emb(sbert_model)
         
             # ä¸­æ–‡è¯å‘é‡æ¨¡å‹(word2vec)ï¼Œä¸­æ–‡å­—é¢åŒ¹é…ä»»åŠ¡å’Œå†·å¯åŠ¨é€‚ç”¨
             w2v_model = Word2Vec("w2v-light-tencent-chinese")
             compute_emb(w2v_model)
         
         ```
@@ -220,16 +224,14 @@
         ```
         
         - è¿”å›å€¼`embeddings`æ˜¯`numpy.ndarray`ç±»å‹ï¼Œshapeä¸º`(sentences_size, model_embedding_size)`ï¼Œä¸‰ä¸ªæ¨¡å‹ä»»é€‰ä¸€ç§å³å¯ï¼Œæ¨èç”¨ç¬¬ä¸€ä¸ªã€‚
         - `shibing624/text2vec-base-chinese`æ¨¡å‹æ˜¯CoSENTæ–¹æ³•åœ¨ä¸­æ–‡STS-Bæ•°æ®é›†è®­ç»ƒå¾—åˆ°çš„ï¼Œæ¨¡å‹å·²ç»ä¸Šä¼ åˆ°huggingfaceçš„
         æ¨¡å‹åº“[shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)ï¼Œ
         æ˜¯`text2vec.SentenceModel`æŒ‡å®šçš„é»˜è®¤æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡ä¸Šé¢ç¤ºä¾‹è°ƒç”¨ï¼Œæˆ–è€…å¦‚ä¸‹æ‰€ç¤ºç”¨[transformersåº“](https://github.com/huggingface/transformers)è°ƒç”¨ï¼Œ
         æ¨¡å‹è‡ªåŠ¨ä¸‹è½½åˆ°æœ¬æœºè·¯å¾„ï¼š`~/.cache/huggingface/transformers`
-        - `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`æ¨¡å‹æ˜¯Sentence-BERTçš„å¤šè¯­è¨€å¥å‘é‡æ¨¡å‹ï¼Œ
-        é€‚ç”¨äºé‡Šä¹‰ï¼ˆparaphraseï¼‰è¯†åˆ«ï¼Œæ–‡æœ¬åŒ¹é…ï¼Œé€šè¿‡`text2vec.SentenceModel`å’Œ[sentence-transformersåº“]((https://github.com/UKPLab/sentence-transformers))éƒ½å¯ä»¥è°ƒç”¨è¯¥æ¨¡å‹
         - `w2v-light-tencent-chinese`æ˜¯é€šè¿‡gensimåŠ è½½çš„Word2Vecæ¨¡å‹ï¼Œä½¿ç”¨è…¾è®¯è¯å‘é‡`Tencent_AILab_ChineseEmbedding.tar.gz`è®¡ç®—å„å­—è¯çš„è¯å‘é‡ï¼Œå¥å­å‘é‡é€šè¿‡å•è¯è¯
         å‘é‡å–å¹³å‡å€¼å¾—åˆ°ï¼Œæ¨¡å‹è‡ªåŠ¨ä¸‹è½½åˆ°æœ¬æœºè·¯å¾„ï¼š`~/.text2vec/datasets/light_Tencent_AILab_ChineseEmbedding.bin`
         
         #### Usage (HuggingFace Transformers)
         Without [text2vec](https://github.com/shibing624/text2vec), you can use the model like this: 
         
         First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.
@@ -447,17 +449,17 @@
         from similarities import Similarity
         
         m = Similarity()
         r = m.similarity('å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡', 'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡')
         print(f"similarity score: {float(r)}")  # similarity score: 0.855146050453186
         ```
         
-        # Models
+        ## Models
         
-        ## CoSENT model
+        ### CoSENT model
         
         CoSENTï¼ˆCosine Sentenceï¼‰æ–‡æœ¬åŒ¹é…æ¨¡å‹ï¼Œåœ¨Sentence-BERTä¸Šæ”¹è¿›äº†CosineRankLossçš„å¥å‘é‡æ–¹æ¡ˆ
         
         
         Network structure:
         
         Training:
@@ -488,16 +490,22 @@
         python training_sup_text_matching_model.py --task_name ATEC --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base --output_dir ./outputs/ATEC-cosent
         ```
         
         - åœ¨è‡ªæœ‰ä¸­æ–‡æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹
         
         example: [examples/training_sup_text_matching_model_mydata.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_mydata.py)
         
+        å•å¡è®­ç»ƒï¼š
+        ```shell
+        CUDA_VISIBLE_DEVICES=0 python training_sup_text_matching_model_mydata.py --do_train --do_predict
+        ```
+        
+        å¤šå¡è®­ç»ƒï¼š
         ```shell
-        python training_sup_text_matching_model_mydata.py --do_train --do_predict
+        CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node 2  training_sup_text_matching_model_mydata.py --do_train --do_predict --output_dir outputs/STS-B-text2vec-macbert-v1 --batch_size 64 --fp16 --data_parallel 
         ```
         
         è®­ç»ƒé›†æ ¼å¼å‚è€ƒ[examples/data/STS-B/STS-B.valid.data](https://github.com/shibing624/text2vec/blob/master/examples/data/STS-B/STS-B.valid.data)
         
         ```shell
         sentence1   sentence2   label
         ä¸€ä¸ªå¥³å­©åœ¨ç»™å¥¹çš„å¤´å‘åšå‘å‹ã€‚	ä¸€ä¸ªå¥³å­©åœ¨æ¢³å¤´ã€‚	2
@@ -524,15 +532,15 @@
         
         ```shell
         cd examples
         python training_unsup_text_matching_model_en.py --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-cosent
         ```
         
         
-        ## Sentence-BERT model
+        ### Sentence-BERT model
         
         Sentence-BERTæ–‡æœ¬åŒ¹é…æ¨¡å‹ï¼Œè¡¨å¾å¼å¥å‘é‡è¡¨ç¤ºæ–¹æ¡ˆ
         
         Network structure:
         
         Training:
         
@@ -567,38 +575,38 @@
         example: [examples/training_unsup_text_matching_model_en.py](https://github.com/shibing624/text2vec/blob/master/examples/training_unsup_text_matching_model_en.py)
         
         ```shell
         cd examples
         python training_unsup_text_matching_model_en.py --model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-sbert
         ```
         
-        ## BERT-Match model
+        ### BERT-Match model
         BERTæ–‡æœ¬åŒ¹é…æ¨¡å‹ï¼ŒåŸç”ŸBERTåŒ¹é…ç½‘ç»œç»“æ„ï¼Œäº¤äº’å¼å¥å‘é‡åŒ¹é…æ¨¡å‹
         
         Network structure:
         
         Training and inference:
         
         <img src="docs/bert-fc-train.png" width="300" />
         
         è®­ç»ƒè„šæœ¬åŒä¸Š[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)ã€‚
         
         
-        ## æ¨¡å‹è’¸é¦ï¼ˆModel Distillationï¼‰
+        ### æ¨¡å‹è’¸é¦ï¼ˆModel Distillationï¼‰
         
         ç”±äºtext2vecè®­ç»ƒçš„æ¨¡å‹å¯ä»¥ä½¿ç”¨[sentence-transformers](https://github.com/UKPLab/sentence-transformers)åº“åŠ è½½ï¼Œæ­¤å¤„å¤ç”¨å…¶æ¨¡å‹è’¸é¦æ–¹æ³•[distillation](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/distillation)ã€‚
         
         1. æ¨¡å‹é™ç»´ï¼Œå‚è€ƒ[dimensionality_reduction.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/distillation/dimensionality_reduction.py)ä½¿ç”¨PCAå¯¹æ¨¡å‹è¾“å‡ºembeddingé™ç»´ï¼Œå¯å‡å°‘milvusç­‰å‘é‡æ£€ç´¢æ•°æ®åº“çš„å­˜å‚¨å‹åŠ›ï¼Œè¿˜èƒ½è½»å¾®æå‡æ¨¡å‹æ•ˆæœã€‚
         2. æ¨¡å‹è’¸é¦ï¼Œå‚è€ƒ[model_distillation.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/distillation/model_distillation.py)ä½¿ç”¨è’¸é¦æ–¹æ³•ï¼Œå°†Teacherå¤§æ¨¡å‹è’¸é¦åˆ°æ›´å°‘layerså±‚æ•°çš„studentæ¨¡å‹ä¸­ï¼Œåœ¨æƒè¡¡æ•ˆæœçš„æƒ…å†µä¸‹ï¼Œå¯å¤§å¹…æå‡æ¨¡å‹é¢„æµ‹é€Ÿåº¦ã€‚
         
-        ## æ¨¡å‹éƒ¨ç½²
+        ### æ¨¡å‹éƒ¨ç½²
         
         æä¾›ä¸¤ç§éƒ¨ç½²æ¨¡å‹ï¼Œæ­å»ºæœåŠ¡çš„æ–¹æ³•ï¼š 1ï¼‰åŸºäºJinaæ­å»ºgRPCæœåŠ¡ã€æ¨èã€‘ï¼›2ï¼‰åŸºäºFastAPIæ­å»ºåŸç”ŸHttpæœåŠ¡ã€‚
         
-        ### JinaæœåŠ¡
+        #### JinaæœåŠ¡
         é‡‡ç”¨C/Sæ¨¡å¼æ­å»ºé«˜æ€§èƒ½æœåŠ¡ï¼Œæ”¯æŒdockeräº‘åŸç”Ÿï¼ŒgRPC/HTTP/WebSocketï¼Œæ”¯æŒå¤šä¸ªæ¨¡å‹åŒæ—¶é¢„æµ‹ï¼ŒGPUå¤šå¡å¤„ç†ã€‚
         
         - å®‰è£…ï¼š
         ```pip install jina```
         
         - å¯åŠ¨æœåŠ¡ï¼š
         
@@ -637,15 +645,15 @@
         r = c.post('/', inputs=DocumentArray([Document(text='å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡'), Document(text='èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡')]))
         print(r.embeddings)
         ```
         
         æ‰¹é‡è°ƒç”¨æ–¹æ³•è§example: [examples/jina_client_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/jina_client_demo.py)
         
         
-        ### FastAPIæœåŠ¡
+        #### FastAPIæœåŠ¡
         
         - å®‰è£…ï¼š
         ```pip install fastapi uvicorn```
         
         - å¯åŠ¨æœåŠ¡ï¼š
         
         example: [examples/fastapi_server_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/fastapi_server_demo.py)
@@ -658,15 +666,15 @@
         ```shell
         curl -X 'GET' \
           'http://0.0.0.0:8001/emb?q=hello' \
           -H 'accept: application/json'
         ```
         
         
-        ## æ•°æ®é›†
+        ## Dataset
         
         - æœ¬é¡¹ç›®releaseçš„æ•°æ®é›†ï¼š
         
         | Dataset                    | Introduce                                                                | Download Link                                                                                                                                                                                                                                                                                         |
         |:---------------------------|:-------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
         | shibing624/nli-zh-all      | ä¸­æ–‡è¯­ä¹‰åŒ¹é…æ•°æ®åˆé›†ï¼Œæ•´åˆäº†æ–‡æœ¬æ¨ç†ï¼Œç›¸ä¼¼ï¼Œæ‘˜è¦ï¼Œé—®ç­”ï¼ŒæŒ‡ä»¤å¾®è°ƒç­‰ä»»åŠ¡çš„820ä¸‡é«˜è´¨é‡æ•°æ®ï¼Œå¹¶è½¬åŒ–ä¸ºåŒ¹é…æ ¼å¼æ•°æ®é›†                | [https://huggingface.co/datasets/shibing624/nli-zh-all](https://huggingface.co/datasets/shibing624/nli-zh-all)                                                                                                                                                                                        |
         | shibing624/snli-zh         | ä¸­æ–‡SNLIå’ŒMultiNLIæ•°æ®é›†ï¼Œç¿»è¯‘è‡ªè‹±æ–‡SNLIå’ŒMultiNLI                                    | [https://huggingface.co/datasets/shibing624/snli-zh](https://huggingface.co/datasets/shibing624/snli-zh)                                                                                                                                                                                              |
@@ -677,16 +685,16 @@
         | LCQMC                      | ä¸­æ–‡LCQMC(large-scale Chinese question matching corpus)æ•°æ®é›†ï¼ŒQ-Qpairæ•°æ®é›†      | [LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html)                                                                                                                                                                                                                                               |
         | PAWSX                      | ä¸­æ–‡PAWS(Paraphrase Adversaries from Word Scrambling)æ•°æ®é›†ï¼ŒQ-Qpairæ•°æ®é›†        | [PAWSX](https://arxiv.org/abs/1908.11828)                                                                                                                                                                                                                                                             |
         | STS-B                      | ä¸­æ–‡STS-Bæ•°æ®é›†ï¼Œä¸­æ–‡è‡ªç„¶è¯­è¨€æ¨ç†æ•°æ®é›†ï¼Œä»è‹±æ–‡STS-Bç¿»è¯‘ä¸ºä¸­æ–‡çš„æ•°æ®é›†                                 | [STS-B](https://github.com/pluto-junzeng/CNSD)                                                                                                                                                                                                                                                        |
         
         
         å¸¸ç”¨è‹±æ–‡åŒ¹é…æ•°æ®é›†ï¼š
         
-        - å¤§åé¼é¼çš„multi_nliå’Œsnli: https://huggingface.co/datasets/multi_nli
-        - å¤§åé¼é¼çš„multi_nliå’Œsnli: https://huggingface.co/datasets/snli
+        - è‹±æ–‡åŒ¹é…æ•°æ®é›†ï¼šmulti_nli: https://huggingface.co/datasets/multi_nli
+        - è‹±æ–‡åŒ¹é…æ•°æ®é›†ï¼šsnli: https://huggingface.co/datasets/snli
         - https://huggingface.co/datasets/metaeval/cnli
         - https://huggingface.co/datasets/mteb/stsbenchmark-sts
         - https://huggingface.co/datasets/JeremiahZ/simcse_sup_nli
         - https://huggingface.co/datasets/MoritzLaurer/multilingual-NLI-26lang-2mil7
         
         
         æ•°æ®é›†ä½¿ç”¨ç¤ºä¾‹ï¼š
@@ -721,59 +729,59 @@
         {'sentence1': 'ä¸€ä¸ªå¥³å­©åœ¨ç»™å¥¹çš„å¤´å‘åšå‘å‹ã€‚', 'sentence2': 'ä¸€ä¸ªå¥³å­©åœ¨æ¢³å¤´ã€‚', 'label': 2}
         ```
         
         
         
         
         
-        # Contact
+        ## Contact
         
         - Issue(å»ºè®®)ï¼š[![GitHub issues](https://img.shields.io/github/issues/shibing624/text2vec.svg)](https://github.com/shibing624/text2vec/issues)
         - é‚®ä»¶æˆ‘ï¼šxuming: xuming624@qq.com
         - å¾®ä¿¡æˆ‘ï¼šåŠ æˆ‘*å¾®ä¿¡å·ï¼šxuming624, å¤‡æ³¨ï¼šå§“å-å…¬å¸-NLP* è¿›NLPäº¤æµç¾¤ã€‚
         
         <img src="docs/wechat.jpeg" width="200" />
         
         
-        # Citation
+        ## Citation
         
         å¦‚æœä½ åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†text2vecï¼Œè¯·æŒ‰å¦‚ä¸‹æ ¼å¼å¼•ç”¨ï¼š
         
         APA:
         ```latex
         Xu, M. Text2vec: Text to vector toolkit (Version 1.1.2) [Computer software]. https://github.com/shibing624/text2vec
         ```
         
         BibTeX:
         ```latex
         @misc{Text2vec,
-          author = {Xu, Ming},
+          author = {Ming Xu},
           title = {Text2vec: Text to vector toolkit},
-          year = {2022},
+          year = {2023},
           publisher = {GitHub},
           journal = {GitHub repository},
           howpublished = {\url{https://github.com/shibing624/text2vec}},
         }
         ```
         
-        # License
+        ## License
         
         
         æˆæƒåè®®ä¸º [The Apache License 2.0](LICENSE)ï¼Œå¯å…è´¹ç”¨åšå•†ä¸šç”¨é€”ã€‚è¯·åœ¨äº§å“è¯´æ˜ä¸­é™„åŠ text2vecçš„é“¾æ¥å’Œæˆæƒåè®®ã€‚
         
         
-        # Contribute
+        ## Contribute
         é¡¹ç›®ä»£ç è¿˜å¾ˆç²—ç³™ï¼Œå¦‚æœå¤§å®¶å¯¹ä»£ç æœ‰æ‰€æ”¹è¿›ï¼Œæ¬¢è¿æäº¤å›æœ¬é¡¹ç›®ï¼Œåœ¨æäº¤ä¹‹å‰ï¼Œæ³¨æ„ä»¥ä¸‹ä¸¤ç‚¹ï¼š
         
          - åœ¨`tests`æ·»åŠ ç›¸åº”çš„å•å…ƒæµ‹è¯•
          - ä½¿ç”¨`python -m pytest -v`æ¥è¿è¡Œæ‰€æœ‰å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿æ‰€æœ‰å•æµ‹éƒ½æ˜¯é€šè¿‡çš„
         
         ä¹‹åå³å¯æäº¤PRã€‚
         
-        # Reference
+        ## References
         - [å°†å¥å­è¡¨ç¤ºä¸ºå‘é‡ï¼ˆä¸Šï¼‰ï¼šæ— ç›‘ç£å¥å­è¡¨ç¤ºå­¦ä¹ ï¼ˆsentence embeddingï¼‰](https://www.cnblogs.com/llhthinker/p/10335164.html)
         - [å°†å¥å­è¡¨ç¤ºä¸ºå‘é‡ï¼ˆä¸‹ï¼‰ï¼šæ— ç›‘ç£å¥å­è¡¨ç¤ºå­¦ä¹ ï¼ˆsentence embeddingï¼‰](https://www.cnblogs.com/llhthinker/p/10341841.html)
         - [A Simple but Tough-to-Beat Baseline for Sentence Embeddings[Sanjeev Arora and Yingyu Liang and Tengyu Ma, 2017]](https://openreview.net/forum?id=SyK00v5xx)
         - [å››ç§è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦çš„æ–¹æ³•å¯¹æ¯”[Yves Peirsman]](https://zhuanlan.zhihu.com/p/37104535)
         - [Improvements to BM25 and Language Models Examined](http://www.cs.otago.ac.nz/homepages/andrew/papers/2014-2.pdf)
         - [CoSENTï¼šæ¯”Sentence-BERTæ›´æœ‰æ•ˆçš„å¥å‘é‡æ–¹æ¡ˆ](https://kexue.fm/archives/8847)
         - [è°ˆè°ˆæ–‡æœ¬åŒ¹é…å’Œå¤šè½®æ£€ç´¢](https://zhuanlan.zhihu.com/p/111769969)
```

#### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: text2vec Version: 1.2.1 Summary: Text to vector
+Metadata-Version: 2.1 Name: text2vec Version: 1.2.2 Summary: Text to vector
 Tool, encode text Home-page: https://github.com/shibing624/text2vec Author:
 XuMing Author-email: xuming624@qq.com License: Apache License 2.0 Description:
 [**Ã°ÂŸÂ‡Â¨Ã°ÂŸÂ‡Â³Ã¤Â¸Â­Ã¦Â–Â‡**](https://github.com/shibing624/text2vec/blob/master/
 README.md) | [**Ã°ÂŸÂŒÂEnglish**](https://github.com/shibing624/text2vec/blob/
 master/README_EN.md) | [**Ã°ÂŸÂ“Â–Ã¦Â–Â‡Ã¦Â¡Â£/Docs**](https://github.com/shibing624/
 text2vec/wiki) | [**Ã°ÂŸÂ¤Â–Ã¦Â¨Â¡Ã¥ÂÂ‹/Models**](https://huggingface.co/shibing624)
                                     [Logo]
@@ -17,17 +17,26 @@
 shibing624/text2vec.svg)](https://github.com/shibing624/text2vec/issues) [!
 [Wechat Group](http://vlog.sfyc.ltd/wechat_everyday/
 wxgroup_logo.png?imageView2/0/w/60/h/20)](#Contact) **Text2vec**: Text to
 Vector, Get Sentence Embeddings. Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂÂ‘Ã©Â‡ÂÃ¥ÂŒÂ–Ã¯Â¼ÂŒÃ¦ÂŠÂŠÃ¦Â–Â‡Ã¦ÂœÂ¬
 (Ã¥ÂŒÂ…Ã¦Â‹Â¬Ã¨Â¯ÂÃ£Â€ÂÃ¥ÂÂ¥Ã¥Â­ÂÃ£Â€ÂÃ¦Â®ÂµÃ¨ÂÂ½)Ã¨Â¡Â¨Ã¥Â¾ÂÃ¤Â¸ÂºÃ¥ÂÂ‘Ã©Â‡ÂÃ§ÂŸÂ©Ã©Â˜ÂµÃ£Â€Â‚
 **text2vec**Ã¥Â®ÂÃ§ÂÂ°Ã¤ÂºÂ†Word2VecÃ£Â€ÂRankBM25Ã£Â€ÂBERTÃ£Â€ÂSentence-
 BERTÃ£Â€ÂCoSENTÃ§Â­Â‰Ã¥Â¤ÂšÃ§Â§ÂÃ¦Â–Â‡Ã¦ÂœÂ¬Ã¨Â¡Â¨Ã¥Â¾ÂÃ£Â€ÂÃ¦Â–Â‡Ã¦ÂœÂ¬Ã§Â›Â¸Ã¤Â¼Â¼Ã¥ÂºÂ¦Ã¨Â®Â¡Ã§Â®Â—Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¥Â¹Â¶Ã¥ÂœÂ¨Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¯Â¼ÂˆÃ§Â›Â¸Ã¤Â¼Â¼Ã¥ÂºÂ¦Ã¨Â®Â¡Ã§Â®Â—Ã¯Â¼Â‰Ã¤Â»Â»Ã¥ÂŠÂ¡Ã¤Â¸ÂŠÃ¦Â¯Â”Ã¨Â¾ÂƒÃ¤ÂºÂ†Ã¥ÂÂ„Ã¦Â¨Â¡Ã¥ÂÂ‹Ã§ÂšÂ„Ã¦Â•ÂˆÃ¦ÂÂœÃ£Â€Â‚
-### News [2023/06/19] v1.2.1Ã§Â‰ÂˆÃ¦ÂœÂ¬: Ã¦Â›Â´Ã¦Â–Â°Ã¤ÂºÂ†Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹`shibing624/
-text2vec-base-chinese-nli`Ã¤Â¸ÂºÃ¦Â–Â°Ã§Â‰Âˆ[shibing624/text2vec-base-chinese-sentence]
-(https://huggingface.co/shibing624/text2vec-base-chinese-
+### News [2023/06/22] v1.2.2Ã§Â‰ÂˆÃ¦ÂœÂ¬: Ã¥ÂÂ‘Ã¥Â¸ÂƒÃ¤ÂºÂ†Ã¥Â¤ÂšÃ¨Â¯Â­Ã¨Â¨Â€Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹[shibing624/
+text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-
+multilingual)Ã¯Â¼ÂŒÃ§Â”Â¨CoSENTÃ¦Â–Â¹Ã¦Â³Â•Ã¨Â®Â­Ã§Â»ÂƒÃ¯Â¼ÂŒÃ¥ÂŸÂºÃ¤ÂºÂ`sentence-transformers/
+paraphrase-multilingual-MiniLM-L12-
+v2`Ã§Â”Â¨Ã¤ÂºÂºÃ¥Â·Â¥Ã¦ÂŒÂ‘Ã©Â€Â‰Ã¥ÂÂÃ§ÂšÂ„Ã¥Â¤ÂšÃ¨Â¯Â­Ã¨Â¨Â€STSÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†[shibing624/nli-zh-all/text2vec-
+base-multilingual-dataset](https://huggingface.co/datasets/shibing624/nli-zh-
+all/tree/main/text2vec-base-multilingual-
+dataset)Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â¾Â—Ã¥ÂˆÂ°Ã¯Â¼ÂŒÃ¥Â¹Â¶Ã¥ÂœÂ¨Ã¤Â¸Â­Ã¨Â‹Â±Ã¦Â–Â‡Ã¦ÂµÂ‹Ã¨Â¯Â•Ã©Â›Â†Ã¨Â¯Â„Ã¤Â¼Â°Ã§Â›Â¸Ã¥Â¯Â¹Ã¤ÂºÂÃ¥ÂÂŸÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â•ÂˆÃ¦ÂÂœÃ¦ÂœÂ‰Ã¦ÂÂÃ¥ÂÂ‡Ã¯Â¼ÂŒÃ¨Â¯Â¦Ã¨Â§Â
+[Release-v1.2.2](https://github.com/shibing624/text2vec/releases/tag/1.2.2)
+[2023/06/19] v1.2.1Ã§Â‰ÂˆÃ¦ÂœÂ¬: Ã¦Â›Â´Ã¦Â–Â°Ã¤ÂºÂ†Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹`shibing624/text2vec-
+base-chinese-nli`Ã¤Â¸ÂºÃ¦Â–Â°Ã§Â‰Âˆ[shibing624/text2vec-base-chinese-sentence](https://
+huggingface.co/shibing624/text2vec-base-chinese-
 sentence)Ã¯Â¼ÂŒÃ©Â’ÂˆÃ¥Â¯Â¹CoSENTÃ§ÂšÂ„lossÃ¨Â®Â¡Ã§Â®Â—Ã¥Â¯Â¹Ã¦ÂÂ’Ã¥ÂºÂÃ¦Â•ÂÃ¦Â„ÂŸÃ§Â‰Â¹Ã§Â‚Â¹Ã¯Â¼ÂŒÃ¤ÂºÂºÃ¥Â·Â¥Ã¦ÂŒÂ‘Ã©Â€Â‰Ã¥Â¹Â¶Ã¦Â•Â´Ã§ÂÂ†Ã¥Â‡ÂºÃ©Â«Â˜Ã¨Â´Â¨Ã©Â‡ÂÃ§ÂšÂ„Ã¦ÂœÂ‰Ã§Â›Â¸Ã¥Â…Â³Ã¦Â€Â§Ã¦ÂÂ’Ã¥ÂºÂÃ§ÂšÂ„STSÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†
 [shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://
 huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-
 sentence-
 dataset)Ã¯Â¼ÂŒÃ¥ÂœÂ¨Ã¥ÂÂ„Ã¨Â¯Â„Ã¤Â¼Â°Ã©Â›Â†Ã¨Â¡Â¨Ã§ÂÂ°Ã§Â›Â¸Ã¥Â¯Â¹Ã¤Â¹Â‹Ã¥Â‰ÂÃ¦ÂœÂ‰Ã¦ÂÂÃ¥ÂÂ‡Ã¯Â¼Â›Ã¥ÂÂ‘Ã¥Â¸ÂƒÃ¤ÂºÂ†Ã©Â€Â‚Ã§Â”Â¨Ã¤ÂºÂs2pÃ§ÂšÂ„Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹
 [shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/
 shibing624/text2vec-base-chinese-paraphrase)Ã¯Â¼ÂŒÃ¨Â¯Â¦Ã¨Â§Â[Release-v1.2.1](https://
@@ -38,17 +47,17 @@
 huggingface.co/datasets/shibing624/
 nli_zh)Ã¥Â…Â¨Ã©ÂƒÂ¨Ã¨Â¯Â­Ã¦Â–Â™Ã¨Â®Â­Ã§Â»ÂƒÃ§ÂšÂ„CoSENTÃ¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¥ÂœÂ¨Ã¥ÂÂ„Ã¨Â¯Â„Ã¤Â¼Â°Ã©Â›Â†Ã¨Â¡Â¨Ã§ÂÂ°Ã¦ÂÂÃ¥ÂÂ‡Ã¦Â˜ÂÃ¦Â˜Â¾Ã¯Â¼ÂŒÃ¨Â¯Â¦Ã¨Â§Â
 [Release-v1.2.0](https://github.com/shibing624/text2vec/releases/tag/1.2.0)
 [2022/03/12] v1.1.4Ã§Â‰ÂˆÃ¦ÂœÂ¬: Ã¥ÂÂ‘Ã¥Â¸ÂƒÃ¤ÂºÂ†Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹[shibing624/text2vec-
 base-chinese](https://huggingface.co/shibing624/text2vec-base-
 chinese)Ã¯Â¼ÂŒÃ¥ÂŸÂºÃ¤ÂºÂÃ¤Â¸Â­Ã¦Â–Â‡STSÃ¨Â®Â­Ã§Â»ÂƒÃ©Â›Â†Ã¨Â®Â­Ã§Â»ÂƒÃ§ÂšÂ„CoSENTÃ¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹Ã£Â€Â‚Ã¨Â¯Â¦Ã¨Â§Â
 [Release-v1.1.4](https://github.com/shibing624/text2vec/releases/tag/1.1.4)
-**Guide** - [Feature](#Feature) - [Evaluation](#Evaluation) - [Install]
-(#install) - [Usage](#usage) - [Contact](#Contact) - [Reference](#reference) #
-Feature ### Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂÂ‘Ã©Â‡ÂÃ¨Â¡Â¨Ã§Â¤ÂºÃ¦Â¨Â¡Ã¥ÂÂ‹ - [Word2Vec](https://github.com/
+**Guide** - [Features](#Features) - [Evaluation](#Evaluation) - [Install]
+(#install) - [Usage](#usage) - [Contact](#Contact) - [References](#references)
+## Features ### Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂÂ‘Ã©Â‡ÂÃ¨Â¡Â¨Ã§Â¤ÂºÃ¦Â¨Â¡Ã¥ÂÂ‹ - [Word2Vec](https://github.com/
 shibing624/text2vec/blob/master/text2vec/word2vec.py)Ã¯Â¼ÂšÃ©Â€ÂšÃ¨Â¿Â‡Ã¨Â…Â¾Ã¨Â®Â¯AI
 LabÃ¥Â¼Â€Ã¦ÂºÂÃ§ÂšÂ„Ã¥Â¤Â§Ã¨Â§Â„Ã¦Â¨Â¡Ã©Â«Â˜Ã¨Â´Â¨Ã©Â‡ÂÃ¤Â¸Â­Ã¦Â–Â‡
 [Ã¨Â¯ÂÃ¥ÂÂ‘Ã©Â‡ÂÃ¦Â•Â°Ã¦ÂÂ®Ã¯Â¼Âˆ800Ã¤Â¸Â‡Ã¤Â¸Â­Ã¦Â–Â‡Ã¨Â¯ÂÃ¨Â½Â»Ã©Â‡ÂÃ§Â‰ÂˆÃ¯Â¼Â‰](https://pan.baidu.com/s/
 1La4U4XNFe8s5BJqxPQpeiQ) (Ã¦Â–Â‡Ã¤Â»Â¶Ã¥ÂÂÃ¯Â¼Âšlight_Tencent_AILab_ChineseEmbedding.bin
 Ã¥Â¯Â†Ã§Â Â:
 taweÃ¯Â¼Â‰Ã¥Â®ÂÃ§ÂÂ°Ã¨Â¯ÂÃ¥ÂÂ‘Ã©Â‡ÂÃ¦Â£Â€Ã§Â´Â¢Ã¯Â¼ÂŒÃ¦ÂœÂ¬Ã©Â¡Â¹Ã§Â›Â®Ã¥Â®ÂÃ§ÂÂ°Ã¤ÂºÂ†Ã¥ÂÂ¥Ã¥Â­ÂÃ¯Â¼ÂˆÃ¨Â¯ÂÃ¥ÂÂ‘Ã©Â‡ÂÃ¦Â±Â‚Ã¥Â¹Â³Ã¥ÂÂ‡Ã¯Â¼Â‰Ã§ÂšÂ„word2vecÃ¥ÂÂ‘Ã©Â‡ÂÃ¨Â¡Â¨Ã§Â¤Âº
 - [SBERT(Sentence-BERT)](https://github.com/shibing624/text2vec/blob/master/
@@ -56,67 +65,76 @@
 sentencebert_model.py)Ã¯Â¼ÂšÃ¦ÂÂƒÃ¨Â¡Â¡Ã¦Â€Â§Ã¨ÂƒÂ½Ã¥Â’ÂŒÃ¦Â•ÂˆÃ§ÂÂ‡Ã§ÂšÂ„Ã¥ÂÂ¥Ã¥ÂÂ‘Ã©Â‡ÂÃ¨Â¡Â¨Ã§Â¤ÂºÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¨Â®Â­Ã§Â»ÂƒÃ¦Â—Â¶Ã©Â€ÂšÃ¨Â¿Â‡Ã¦ÂœÂ‰Ã§Â›Â‘Ã§ÂÂ£Ã¨Â®Â­Ã§Â»ÂƒÃ¤Â¸ÂŠÃ¥Â±Â‚Ã¥ÂˆÂ†Ã§Â±Â»Ã¥Â‡Â½Ã¦Â•Â°Ã¯Â¼ÂŒÃ¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂŒÂ¹Ã©Â…ÂÃ©Â¢Â„Ã¦ÂµÂ‹Ã¦Â—Â¶Ã§Â›Â´Ã¦ÂÂ¥Ã¥ÂÂ¥Ã¥Â­ÂÃ¥ÂÂ‘Ã©Â‡ÂÃ¥ÂÂšÃ¤Â½Â™Ã¥Â¼Â¦Ã¯Â¼ÂŒÃ¦ÂœÂ¬Ã©Â¡Â¹Ã§Â›Â®Ã¥ÂŸÂºÃ¤ÂºÂPyTorchÃ¥Â¤ÂÃ§ÂÂ°Ã¤ÂºÂ†Sentence-
 BERTÃ¦Â¨Â¡Ã¥ÂÂ‹Ã§ÂšÂ„Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â’ÂŒÃ©Â¢Â„Ã¦ÂµÂ‹ - [CoSENT(Cosine Sentence)](https://github.com/
 shibing624/text2vec/blob/master/text2vec/
 cosent_model.py)Ã¯Â¼ÂšCoSENTÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¦ÂÂÃ¥Â‡ÂºÃ¤ÂºÂ†Ã¤Â¸Â€Ã§Â§ÂÃ¦ÂÂ’Ã¥ÂºÂÃ§ÂšÂ„Ã¦ÂÂŸÃ¥Â¤Â±Ã¥Â‡Â½Ã¦Â•Â°Ã¯Â¼ÂŒÃ¤Â½Â¿Ã¨Â®Â­Ã§Â»ÂƒÃ¨Â¿Â‡Ã§Â¨Â‹Ã¦Â›Â´Ã¨Â´Â´Ã¨Â¿Â‘Ã©Â¢Â„Ã¦ÂµÂ‹Ã¯Â¼ÂŒÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â”Â¶Ã¦Â•Â›Ã©Â€ÂŸÃ¥ÂºÂ¦Ã¥Â’ÂŒÃ¦Â•ÂˆÃ¦ÂÂœÃ¦Â¯Â”Sentence-
 BERTÃ¦Â›Â´Ã¥Â¥Â½Ã¯Â¼ÂŒÃ¦ÂœÂ¬Ã©Â¡Â¹Ã§Â›Â®Ã¥ÂŸÂºÃ¤ÂºÂPyTorchÃ¥Â®ÂÃ§ÂÂ°Ã¤ÂºÂ†CoSENTÃ¦Â¨Â¡Ã¥ÂÂ‹Ã§ÂšÂ„Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â’ÂŒÃ©Â¢Â„Ã¦ÂµÂ‹
 Ã¨Â¯Â¦Ã§Â»Â†Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂÂ‘Ã©Â‡ÂÃ¨Â¡Â¨Ã§Â¤ÂºÃ¦Â–Â¹Ã¦Â³Â•Ã¨Â§Âwiki: [Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂÂ‘Ã©Â‡ÂÃ¨Â¡Â¨Ã§Â¤ÂºÃ¦Â–Â¹Ã¦Â³Â•](https://
 github.com/shibing624/text2vec/wiki/
-%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95) #
+%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95) ##
 Evaluation Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂŒÂ¹Ã©Â…Â #### Ã¨Â‹Â±Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã§ÂšÂ„Ã¨Â¯Â„Ã¦ÂµÂ‹Ã§Â»Â“Ã¦ÂÂœÃ¯Â¼Âš | Arch |
 BaseModel | Model | English-STS-B | |:-------|:--------------------------------
-----------------|:-------------------------------------|:-------------:| |
+----------------|:-------------------------------------------------------------
+--------------------------------------------------------|:-------------:| |
 GloVe | glove | Avg_word_embeddings_glove_6B_300d | 61.77 | | BERT | bert-base-
 uncased | BERT-base-cls | 20.29 | | BERT | bert-base-uncased | BERT-base-
 first_last_avg | 59.04 | | BERT | bert-base-uncased | BERT-base-first_last_avg-
 whiten(NLI) | 63.65 | | SBERT | sentence-transformers/bert-base-nli-mean-tokens
 | SBERT-base-nli-cls | 73.65 | | SBERT | sentence-transformers/bert-base-nli-
 mean-tokens | SBERT-base-nli-first_last_avg | 77.96 | | CoSENT | bert-base-
 uncased | CoSENT-base-first_last_avg | 69.93 | | CoSENT | sentence-
 transformers/bert-base-nli-mean-tokens | CoSENT-base-nli-first_last_avg | 79.68
-| #### Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã§ÂšÂ„Ã¨Â¯Â„Ã¦ÂµÂ‹Ã§Â»Â“Ã¦ÂÂœÃ¯Â¼Âš | Arch | BaseModel | Model |
-ATEC | BQ | LCQMC | PAWSX | STS-B | Avg | |:-------|:--------------------------
---|:--------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:| |
-SBERT | bert-base-chinese | SBERT-bert-base | 46.36 | 70.36 | 78.72 | 46.86 |
-66.41 | 61.74 | | SBERT | hfl/chinese-macbert-base | SBERT-macbert-base | 47.28
-| 68.63 | 79.42 | 55.59 | 64.82 | 63.15 | | SBERT | hfl/chinese-roberta-wwm-ext
-| SBERT-roberta-ext | 48.29 | 69.99 | 79.22 | 44.10 | 72.42 | 62.80 | | CoSENT
-| bert-base-chinese | CoSENT-bert-base | 49.74 | 72.38 | 78.69 | 60.00 | 79.27
-| 68.01 | | CoSENT | hfl/chinese-macbert-base | CoSENT-macbert-base | 50.39 |
-72.93 | 79.17 | 60.86 | 79.30 | 68.53 | | CoSENT | hfl/chinese-roberta-wwm-ext
-| CoSENT-roberta-ext | 50.81 | 71.45 | 79.31 | 61.56 | 79.96 | 68.61 |
-Ã¨Â¯Â´Ã¦Â˜ÂÃ¯Â¼Âš - Ã§Â»Â“Ã¦ÂÂœÃ¨Â¯Â„Ã¦ÂµÂ‹Ã¦ÂŒÂ‡Ã¦Â Â‡Ã¯Â¼ÂšspearmanÃ§Â³Â»Ã¦Â•Â° -
+| | CoSENT | sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 |
+[shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/
+text2vec-base-multilingual) | 80.12 | ####
+Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã§ÂšÂ„Ã¨Â¯Â„Ã¦ÂµÂ‹Ã§Â»Â“Ã¦ÂÂœÃ¯Â¼Âš | Arch | BaseModel | Model | ATEC | BQ
+| LCQMC | PAWSX | STS-B | Avg | |:-------|:----------------------------|:------
+--------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:| | SBERT | bert-
+base-chinese | SBERT-bert-base | 46.36 | 70.36 | 78.72 | 46.86 | 66.41 | 61.74
+| | SBERT | hfl/chinese-macbert-base | SBERT-macbert-base | 47.28 | 68.63 |
+79.42 | 55.59 | 64.82 | 63.15 | | SBERT | hfl/chinese-roberta-wwm-ext | SBERT-
+roberta-ext | 48.29 | 69.99 | 79.22 | 44.10 | 72.42 | 62.80 | | CoSENT | bert-
+base-chinese | CoSENT-bert-base | 49.74 | 72.38 | 78.69 | 60.00 | 79.27 | 68.01
+| | CoSENT | hfl/chinese-macbert-base | CoSENT-macbert-base | 50.39 | 72.93 |
+79.17 | 60.86 | 79.30 | 68.53 | | CoSENT | hfl/chinese-roberta-wwm-ext |
+CoSENT-roberta-ext | 50.81 | 71.45 | 79.31 | 61.56 | 79.96 | 68.61 | Ã¨Â¯Â´Ã¦Â˜ÂÃ¯Â¼Âš
+- Ã§Â»Â“Ã¦ÂÂœÃ¨Â¯Â„Ã¦ÂµÂ‹Ã¦ÂŒÂ‡Ã¦Â Â‡Ã¯Â¼ÂšspearmanÃ§Â³Â»Ã¦Â•Â° -
 Ã¤Â¸ÂºÃ¨Â¯Â„Ã¦ÂµÂ‹Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨ÂƒÂ½Ã¥ÂŠÂ›Ã¯Â¼ÂŒÃ§Â»Â“Ã¦ÂÂœÃ¥ÂÂ‡Ã¥ÂÂªÃ§Â”Â¨Ã¨Â¯Â¥Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã§ÂšÂ„trainÃ¨Â®Â­Ã§Â»ÂƒÃ¯Â¼ÂŒÃ¥ÂœÂ¨testÃ¤Â¸ÂŠÃ¨Â¯Â„Ã¤Â¼Â°Ã¥Â¾Â—Ã¥ÂˆÂ°Ã§ÂšÂ„Ã¨Â¡Â¨Ã§ÂÂ°Ã¯Â¼ÂŒÃ¦Â²Â¡Ã§Â”Â¨Ã¥Â¤Â–Ã©ÂƒÂ¨Ã¦Â•Â°Ã¦ÂÂ®
-### Release Models - Ã¦ÂœÂ¬Ã©Â¡Â¹Ã§Â›Â®releaseÃ¦Â¨Â¡Ã¥ÂÂ‹Ã§ÂšÂ„Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¨Â¯Â„Ã¦ÂµÂ‹Ã§Â»Â“Ã¦ÂÂœÃ¯Â¼Âš |
-Arch | BaseModel | Model | ATEC | BQ | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-
-dc | Avg | QPS | |:-----------|:----------------------------------|:-----------
+- `SBERT-macbert-base`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¦Â˜Â¯Ã§Â”Â¨SBertÃ¦Â–Â¹Ã¦Â³Â•Ã¨Â®Â­Ã§Â»ÂƒÃ¯Â¼ÂŒÃ¨Â¿ÂÃ¨Â¡ÂŒ[examples/
+training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/
+blob/master/examples/training_sup_text_matching_model.py)Ã¤Â»Â£Ã§Â ÂÃ¥ÂÂ¯Ã¨Â®Â­Ã§Â»ÂƒÃ¦Â¨Â¡Ã¥ÂÂ‹
+- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-
+v2`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â˜Â¯Ã§Â”Â¨SBertÃ¨Â®Â­Ã§Â»ÂƒÃ¯Â¼ÂŒÃ¦Â˜Â¯`paraphrase-MiniLM-L12-
+v2`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã§ÂšÂ„Ã¥Â¤ÂšÃ¨Â¯Â­Ã¨Â¨Â€Ã§Â‰ÂˆÃ¦ÂœÂ¬Ã¯Â¼ÂŒÃ¦Â”Â¯Ã¦ÂŒÂÃ¤Â¸Â­Ã¦Â–Â‡Ã£Â€ÂÃ¨Â‹Â±Ã¦Â–Â‡Ã§Â­Â‰ ### Release Models -
+Ã¦ÂœÂ¬Ã©Â¡Â¹Ã§Â›Â®releaseÃ¦Â¨Â¡Ã¥ÂÂ‹Ã§ÂšÂ„Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¨Â¯Â„Ã¦ÂµÂ‹Ã§Â»Â“Ã¦ÂÂœÃ¯Â¼Âš | Arch | BaseModel | Model
+| ATEC | BQ | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-dc | Avg | QPS | |:-------
+----|:-------------------------------------------------------------|:----------
 -------------------------------------------------------------------------------
---------------------------------------------------------|:-----:|:-----:|:----
+---------------------------------------------------------|:-----:|:-----:|:----
 -:|:-----:|:-----:|:-------:|:-------:|:---------:|:-----:| | Word2Vec |
 word2vec | [w2v-light-tencent-chinese](https://ai.tencent.com/ailab/nlp/en/
 download.html) | 20.00 | 31.49 | 59.46 | 2.57 | 55.78 | 55.04 | 20.70 | 35.03 |
 23769 | | SBERT | xlm-roberta-base | [sentence-transformers/paraphrase-
 multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/
 paraphrase-multilingual-MiniLM-L12-v2) | 18.42 | 38.52 | 63.96 | 10.14 | 78.90
-| 63.01 | 52.28 | 46.46 | 3138 | | Instructor | hfl/chinese-roberta-wwm-ext |
-[moka-ai/m3e-base](https://huggingface.co/moka-ai/m3e-base) | 41.27 | 63.81 |
-74.87 | 12.20 | 76.96 | 75.83 | 60.55 | 57.93 | 2980 | | CoSENT | hfl/chinese-
-macbert-base | [shibing624/text2vec-base-chinese](https://huggingface.co/
-shibing624/text2vec-base-chinese) | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 |
-70.27 | 50.42 | 51.61 | 3008 | | CoSENT | hfl/chinese-lert-large |
-[GanymedeNil/text2vec-large-chinese](https://huggingface.co/GanymedeNil/
-text2vec-large-chinese) | 32.61 | 44.59 | 69.30 | 14.51 | 79.44 | 73.01 | 59.04
-| 53.12 | 2092 | | CoSENT | nghuyong/ernie-3.0-base-zh | [shibing624/text2vec-
-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-
-sentence) | 43.37 | 61.43 | 73.48 | 38.90 | 78.25 | 70.60 | 53.08 | 59.87 |
-3089 | | CoSENT | nghuyong/ernie-3.0-base-zh | [shibing624/text2vec-base-
-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-
-paraphrase) | 44.89 | 63.58 | 74.24 | 40.90 | 78.93 | 76.70 | 63.30 | **63.08**
-| 3066 | Ã¨Â¯Â´Ã¦Â˜ÂÃ¯Â¼Âš - Ã§Â»Â“Ã¦ÂÂœÃ¨Â¯Â„Ã¦ÂµÂ‹Ã¦ÂŒÂ‡Ã¦Â Â‡Ã¯Â¼ÂšspearmanÃ§Â³Â»Ã¦Â•Â° -
-Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â®ÂÃ©ÂªÂŒÃ¦ÂŠÂ¥Ã¥Â‘ÂŠÃ¯Â¼Âš[Ã¥Â®ÂÃ©ÂªÂŒÃ¦ÂŠÂ¥Ã¥Â‘ÂŠ](https://github.com/shibing624/
-text2vec/blob/master/docs/model_report.md) - `shibing624/text2vec-base-
+| 63.01 | 52.28 | 46.46 | 3138 | | CoSENT | hfl/chinese-macbert-base |
+[shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-
+base-chinese) | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 | 70.27 | 50.42 | 51.61 |
+3008 | | CoSENT | hfl/chinese-lert-large | [GanymedeNil/text2vec-large-chinese]
+(https://huggingface.co/GanymedeNil/text2vec-large-chinese) | 32.61 | 44.59 |
+69.30 | 14.51 | 79.44 | 73.01 | 59.04 | 53.12 | 2092 | | CoSENT | nghuyong/
+ernie-3.0-base-zh | [shibing624/text2vec-base-chinese-sentence](https://
+huggingface.co/shibing624/text2vec-base-chinese-sentence) | 43.37 | 61.43 |
+73.48 | 38.90 | 78.25 | 70.60 | 53.08 | 59.87 | 3089 | | CoSENT | nghuyong/
+ernie-3.0-base-zh | [shibing624/text2vec-base-chinese-paraphrase](https://
+huggingface.co/shibing624/text2vec-base-chinese-paraphrase) | 44.89 | 63.58 |
+74.24 | 40.90 | 78.93 | 76.70 | 63.30 | **63.08** | 3066 | | CoSENT | sentence-
+transformers/paraphrase-multilingual-MiniLM-L12-v2 | [shibing624/text2vec-base-
+multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual) |
+32.39 | 50.33 | 65.64 | 32.56 | 74.45 | 68.88 | 51.17 | 53.67 | 3138 |
+Ã¨Â¯Â´Ã¦Â˜ÂÃ¯Â¼Âš - Ã§Â»Â“Ã¦ÂÂœÃ¨Â¯Â„Ã¦ÂµÂ‹Ã¦ÂŒÂ‡Ã¦Â Â‡Ã¯Â¼ÂšspearmanÃ§Â³Â»Ã¦Â•Â° - `shibing624/text2vec-base-
 chinese`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¦Â˜Â¯Ã§Â”Â¨CoSENTÃ¦Â–Â¹Ã¦Â³Â•Ã¨Â®Â­Ã§Â»ÂƒÃ¯Â¼ÂŒÃ¥ÂŸÂºÃ¤ÂºÂ`hfl/chinese-macbert-
 base`Ã¥ÂœÂ¨Ã¤Â¸Â­Ã¦Â–Â‡STS-BÃ¦Â•Â°Ã¦ÂÂ®Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â¾Â—Ã¥ÂˆÂ°Ã¯Â¼ÂŒÃ¥Â¹Â¶Ã¥ÂœÂ¨Ã¤Â¸Â­Ã¦Â–Â‡STS-
 BÃ¦ÂµÂ‹Ã¨Â¯Â•Ã©Â›Â†Ã¨Â¯Â„Ã¤Â¼Â°Ã¨Â¾Â¾Ã¥ÂˆÂ°Ã¨Â¾ÂƒÃ¥Â¥Â½Ã¦Â•ÂˆÃ¦ÂÂœÃ¯Â¼ÂŒÃ¨Â¿ÂÃ¨Â¡ÂŒ[examples/
 training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/
 blob/master/examples/
 training_sup_text_matching_model.py)Ã¤Â»Â£Ã§Â ÂÃ¥ÂÂ¯Ã¨Â®Â­Ã§Â»ÂƒÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â–Â‡Ã¤Â»Â¶Ã¥Â·Â²Ã§Â»ÂÃ¤Â¸ÂŠÃ¤Â¼Â HF
 model hubÃ¯Â¼ÂŒÃ¤Â¸Â­Ã¦Â–Â‡Ã©Â€ÂšÃ§Â”Â¨Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¦ÂÂ¨Ã¨ÂÂÃ¤Â½Â¿Ã§Â”Â¨ - `shibing624/text2vec-
@@ -137,81 +155,83 @@
 [shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://
 huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-
 sentence-dataset)Ã¥ÂŠÂ Ã¥Â…Â¥Ã¤ÂºÂ†s2p(sentence to
 paraphrase)Ã¦Â•Â°Ã¦ÂÂ®Ã¯Â¼ÂŒÃ¥Â¼ÂºÃ¥ÂŒÂ–Ã¤ÂºÂ†Ã¥Â…Â¶Ã©Â•Â¿Ã¦Â–Â‡Ã¦ÂœÂ¬Ã§ÂšÂ„Ã¨Â¡Â¨Ã¥Â¾ÂÃ¨ÂƒÂ½Ã¥ÂŠÂ›Ã¯Â¼ÂŒÃ¥Â¹Â¶Ã¥ÂœÂ¨Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂÂ„NLIÃ¦ÂµÂ‹Ã¨Â¯Â•Ã©Â›Â†Ã¨Â¯Â„Ã¤Â¼Â°Ã¨Â¾Â¾Ã¥ÂˆÂ°SOTAÃ¯Â¼ÂŒÃ¨Â¿ÂÃ¨Â¡ÂŒ
 [examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/
 shibing624/text2vec/blob/master/examples/
 training_sup_text_matching_model_jsonl_data.py)Ã¤Â»Â£Ã§Â ÂÃ¥ÂÂ¯Ã¨Â®Â­Ã§Â»ÂƒÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â–Â‡Ã¤Â»Â¶Ã¥Â·Â²Ã§Â»ÂÃ¤Â¸ÂŠÃ¤Â¼Â HF
-model hubÃ¯Â¼ÂŒÃ¤Â¸Â­Ã¦Â–Â‡s2p(Ã¥ÂÂ¥Ã¥Â­ÂvsÃ¦Â®ÂµÃ¨ÂÂ½)Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¦ÂÂ¨Ã¨ÂÂÃ¤Â½Â¿Ã§Â”Â¨ - `SBERT-
-macbert-base`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¦Â˜Â¯Ã§Â”Â¨SBERTÃ¦Â–Â¹Ã¦Â³Â•Ã¨Â®Â­Ã§Â»ÂƒÃ¯Â¼ÂŒÃ¨Â¿ÂÃ¨Â¡ÂŒ[examples/
-training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/
-blob/master/examples/training_sup_text_matching_model.py)Ã¤Â»Â£Ã§Â ÂÃ¥ÂÂ¯Ã¨Â®Â­Ã§Â»ÂƒÃ¦Â¨Â¡Ã¥ÂÂ‹
-- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-
-v2`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â˜Â¯Ã§Â”Â¨SBERTÃ¨Â®Â­Ã§Â»ÂƒÃ¯Â¼ÂŒÃ¦Â˜Â¯`paraphrase-MiniLM-L12-
-v2`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã§ÂšÂ„Ã¥Â¤ÂšÃ¨Â¯Â­Ã¨Â¨Â€Ã§Â‰ÂˆÃ¦ÂœÂ¬Ã¯Â¼ÂŒÃ¦Â”Â¯Ã¦ÂŒÂÃ¤Â¸Â­Ã¦Â–Â‡Ã£Â€ÂÃ¨Â‹Â±Ã¦Â–Â‡Ã§Â­Â‰ - `w2v-light-tencent-
+model hubÃ¯Â¼ÂŒÃ¤Â¸Â­Ã¦Â–Â‡s2p(Ã¥ÂÂ¥Ã¥Â­ÂvsÃ¦Â®ÂµÃ¨ÂÂ½)Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¦ÂÂ¨Ã¨ÂÂÃ¤Â½Â¿Ã§Â”Â¨ -
+`shibing624/text2vec-base-
+multilingual`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¦Â˜Â¯Ã§Â”Â¨CoSENTÃ¦Â–Â¹Ã¦Â³Â•Ã¨Â®Â­Ã§Â»ÂƒÃ¯Â¼ÂŒÃ¥ÂŸÂºÃ¤ÂºÂ`sentence-transformers/
+paraphrase-multilingual-MiniLM-L12-
+v2`Ã§Â”Â¨Ã¤ÂºÂºÃ¥Â·Â¥Ã¦ÂŒÂ‘Ã©Â€Â‰Ã¥ÂÂÃ§ÂšÂ„Ã¥Â¤ÂšÃ¨Â¯Â­Ã¨Â¨Â€STSÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†[shibing624/nli-zh-all/text2vec-
+base-multilingual-dataset](https://huggingface.co/datasets/shibing624/nli-zh-
+all/tree/main/text2vec-base-multilingual-
+dataset)Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â¾Â—Ã¥ÂˆÂ°Ã¯Â¼ÂŒÃ¥Â¹Â¶Ã¥ÂœÂ¨Ã¤Â¸Â­Ã¨Â‹Â±Ã¦Â–Â‡Ã¦ÂµÂ‹Ã¨Â¯Â•Ã©Â›Â†Ã¨Â¯Â„Ã¤Â¼Â°Ã§Â›Â¸Ã¥Â¯Â¹Ã¤ÂºÂÃ¥ÂÂŸÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â•ÂˆÃ¦ÂÂœÃ¦ÂœÂ‰Ã¦ÂÂÃ¥ÂÂ‡Ã¯Â¼ÂŒÃ¨Â¿ÂÃ¨Â¡ÂŒ
+[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/
+shibing624/text2vec/blob/master/examples/
+training_sup_text_matching_model_jsonl_data.py)Ã¤Â»Â£Ã§Â ÂÃ¥ÂÂ¯Ã¨Â®Â­Ã§Â»ÂƒÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â–Â‡Ã¤Â»Â¶Ã¥Â·Â²Ã§Â»ÂÃ¤Â¸ÂŠÃ¤Â¼Â HF
+model hubÃ¯Â¼ÂŒÃ¥Â¤ÂšÃ¨Â¯Â­Ã¨Â¨Â€Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¦ÂÂ¨Ã¨ÂÂÃ¤Â½Â¿Ã§Â”Â¨ - `w2v-light-tencent-
 chinese`Ã¦Â˜Â¯Ã¨Â…Â¾Ã¨Â®Â¯Ã¨Â¯ÂÃ¥ÂÂ‘Ã©Â‡ÂÃ§ÂšÂ„Word2VecÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒCPUÃ¥ÂŠÂ Ã¨Â½Â½Ã¤Â½Â¿Ã§Â”Â¨Ã¯Â¼ÂŒÃ©Â€Â‚Ã§Â”Â¨Ã¤ÂºÂÃ¤Â¸Â­Ã¦Â–Â‡Ã¥Â­Â—Ã©ÂÂ¢Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¥Â’ÂŒÃ§Â¼ÂºÃ¥Â°Â‘Ã¦Â•Â°Ã¦ÂÂ®Ã§ÂšÂ„Ã¥Â†Â·Ã¥ÂÂ¯Ã¥ÂŠÂ¨Ã¦ÂƒÂ…Ã¥Â†Âµ
 - Ã¥ÂÂ„Ã©Â¢Â„Ã¨Â®Â­Ã§Â»ÂƒÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¥ÂÂ‡Ã¥ÂÂ¯Ã¤Â»Â¥Ã©Â€ÂšÃ¨Â¿Â‡transformersÃ¨Â°ÂƒÃ§Â”Â¨Ã¯Â¼ÂŒÃ¥Â¦Â‚MacBERTÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼Âš`--
 model_name hfl/chinese-macbert-base` Ã¦ÂˆÂ–Ã¨Â€Â…robertaÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼Âš`--model_name uer/
 roberta-medium-wwm-chinese-cluecorpussmall` -
 Ã¤Â¸ÂºÃ¦ÂµÂ‹Ã¨Â¯Â„Ã¦Â¨Â¡Ã¥ÂÂ‹Ã§ÂšÂ„Ã©Â²ÂÃ¦Â£Â’Ã¦Â€Â§Ã¯Â¼ÂŒÃ¥ÂŠÂ Ã¥Â…Â¥Ã¤ÂºÂ†Ã¦ÂœÂªÃ¨Â®Â­Ã§Â»ÂƒÃ¨Â¿Â‡Ã§ÂšÂ„SOHUÃ¦ÂµÂ‹Ã¨Â¯Â•Ã©Â›Â†Ã¯Â¼ÂŒÃ§Â”Â¨Ã¤ÂºÂÃ¦ÂµÂ‹Ã¨Â¯Â•Ã¦Â¨Â¡Ã¥ÂÂ‹Ã§ÂšÂ„Ã¦Â³Â›Ã¥ÂŒÂ–Ã¨ÂƒÂ½Ã¥ÂŠÂ›Ã¯Â¼Â›Ã¤Â¸ÂºÃ¨Â¾Â¾Ã¥ÂˆÂ°Ã¥Â¼Â€Ã§Â®Â±Ã¥ÂÂ³Ã§Â”Â¨Ã§ÂšÂ„Ã¥Â®ÂÃ§Â”Â¨Ã¦Â•ÂˆÃ¦ÂÂœÃ¯Â¼ÂŒÃ¤Â½Â¿Ã§Â”Â¨Ã¤ÂºÂ†Ã¦ÂÂœÃ©Â›Â†Ã¥ÂˆÂ°Ã§ÂšÂ„Ã¥ÂÂ„Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼ÂŒÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¤Â¹ÂŸÃ¤Â¸ÂŠÃ¤Â¼Â Ã¥ÂˆÂ°HF
 datasets[Ã©Â“Â¾Ã¦ÂÂ¥Ã¨Â§ÂÃ¤Â¸Â‹Ã¦Â–Â¹](#Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†) -
 Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¥Â®ÂÃ©ÂªÂŒÃ¨Â¡Â¨Ã¦Â˜ÂÃ¯Â¼ÂŒpoolingÃ¦ÂœÂ€Ã¤Â¼Â˜Ã¦Â˜Â¯`EncoderType.FIRST_LAST_AVG`Ã¥Â’ÂŒ`EncoderType.MEAN`Ã¯Â¼ÂŒÃ¤Â¸Â¤Ã¨Â€Â…Ã©Â¢Â„Ã¦ÂµÂ‹Ã¦Â•ÂˆÃ¦ÂÂœÃ¥Â·Â®Ã¥Â¼Â‚Ã¥Â¾ÂˆÃ¥Â°Â
 -
 Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¨Â¯Â„Ã¦ÂµÂ‹Ã§Â»Â“Ã¦ÂÂœÃ¥Â¤ÂÃ§ÂÂ°Ã¯Â¼ÂŒÃ¥ÂÂ¯Ã¤Â»Â¥Ã¤Â¸Â‹Ã¨Â½Â½Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¥ÂˆÂ°`examples/
 data`Ã¯Â¼ÂŒÃ¨Â¿ÂÃ¨Â¡ÂŒ[tests/test_model_spearman.py](https://github.com/shibing624/
 text2vec/blob/master/tests/test_model_spearman.py)Ã¤Â»Â£Ã§Â ÂÃ¥Â¤ÂÃ§ÂÂ°Ã¨Â¯Â„Ã¦ÂµÂ‹Ã§Â»Â“Ã¦ÂÂœ -
-QPSÃ§ÂšÂ„GPUÃ¦ÂµÂ‹Ã¨Â¯Â•Ã§ÂÂ¯Ã¥Â¢ÂƒÃ¦Â˜Â¯Tesla V100Ã¯Â¼ÂŒÃ¦Â˜Â¾Ã¥Â­Â˜32GB # Demo Official Demo: https://
-www.mulanai.com/product/short_text_sim/ HuggingFace Demo: https://
-huggingface.co/spaces/shibing624/text2vec ![](docs/hf.png) run example:
-[examples/gradio_demo.py](https://github.com/shibing624/text2vec/blob/master/
-examples/gradio_demo.py) to see the demo: ```shell python examples/
-gradio_demo.py ``` # Install ```shell pip install torch # conda install pytorch
-pip install -U text2vec ``` or ```shell pip install torch # conda install
-pytorch pip install -r requirements.txt git clone https://github.com/
-shibing624/text2vec.git cd text2vec pip install --no-deps . ``` # Usage ##
-Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂÂ‘Ã©Â‡ÂÃ¨Â¡Â¨Ã¥Â¾Â Ã¥ÂŸÂºÃ¤ÂºÂ`pretrained model`Ã¨Â®Â¡Ã§Â®Â—Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂÂ‘Ã©Â‡ÂÃ¯Â¼Âš ```zsh >>>
-from text2vec import SentenceModel >>> m = SentenceModel() >>> m.encode
-("Ã¥Â¦Â‚Ã¤Â½Â•Ã¦Â›Â´Ã¦ÂÂ¢Ã¨ÂŠÂ±Ã¥Â‘Â—Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡") Embedding shape: (768,) ``` example:
-[examples/computing_embeddings_demo.py](https://github.com/shibing624/text2vec/
-blob/master/examples/computing_embeddings_demo.py) ```python import sys
-sys.path.append('..') from text2vec import SentenceModel from text2vec import
-Word2Vec def compute_emb(model): # Embed a list of sentences sentences =
-[ 'Ã¥ÂÂ¡', 'Ã©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡', 'Ã¥Â¦Â‚Ã¤Â½Â•Ã¦Â›Â´Ã¦ÂÂ¢Ã¨ÂŠÂ±Ã¥Â‘Â—Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡',
-'Ã¨ÂŠÂ±Ã¥Â‘Â—Ã¦Â›Â´Ã¦Â”Â¹Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡', 'This framework generates embeddings for each
-input sentence', 'Sentences are passed as a list of string.', 'The quick brown
-fox jumps over the lazy dog.' ] sentence_embeddings = model.encode(sentences)
-print(type(sentence_embeddings), sentence_embeddings.shape) # The result is a
-list of sentence embeddings as numpy arrays for sentence, embedding in zip
-(sentences, sentence_embeddings): print("Sentence:", sentence) print("Embedding
-shape:", embedding.shape) print("Embedding head:", embedding[:10]) print() if
-__name__ == "__main__": # Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂÂ¥Ã¥ÂÂ‘Ã©Â‡ÂÃ¦Â¨Â¡Ã¥ÂÂ‹
-(CoSENT)Ã¯Â¼ÂŒÃ¤Â¸Â­Ã¦Â–Â‡Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¦ÂÂ¨Ã¨ÂÂÃ¯Â¼ÂŒÃ¦Â”Â¯Ã¦ÂŒÂfine-tuneÃ§Â»Â§Ã§Â»Â­Ã¨Â®Â­Ã§Â»Âƒ
-t2v_model = SentenceModel("shibing624/text2vec-base-chinese") compute_emb
-(t2v_model) # Ã¦Â”Â¯Ã¦ÂŒÂÃ¥Â¤ÂšÃ¨Â¯Â­Ã¨Â¨Â€Ã§ÂšÂ„Ã¥ÂÂ¥Ã¥ÂÂ‘Ã©Â‡ÂÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂˆSentence-
-BERTÃ¯Â¼Â‰Ã¯Â¼ÂŒÃ¨Â‹Â±Ã¦Â–Â‡Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¦ÂÂ¨Ã¨ÂÂÃ¯Â¼ÂŒÃ¦Â”Â¯Ã¦ÂŒÂfine-tuneÃ§Â»Â§Ã§Â»Â­Ã¨Â®Â­Ã§Â»Âƒ
-sbert_model = SentenceModel("sentence-transformers/paraphrase-multilingual-
-MiniLM-L12-v2") compute_emb(sbert_model) # Ã¤Â¸Â­Ã¦Â–Â‡Ã¨Â¯ÂÃ¥ÂÂ‘Ã©Â‡ÂÃ¦Â¨Â¡Ã¥ÂÂ‹
+QPSÃ§ÂšÂ„GPUÃ¦ÂµÂ‹Ã¨Â¯Â•Ã§ÂÂ¯Ã¥Â¢ÂƒÃ¦Â˜Â¯Tesla V100Ã¯Â¼ÂŒÃ¦Â˜Â¾Ã¥Â­Â˜32GB Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â®ÂÃ©ÂªÂŒÃ¦ÂŠÂ¥Ã¥Â‘ÂŠÃ¯Â¼Âš
+[Ã¥Â®ÂÃ©ÂªÂŒÃ¦ÂŠÂ¥Ã¥Â‘ÂŠ](https://github.com/shibing624/text2vec/blob/master/docs/
+model_report.md) ## Demo Official Demo: https://www.mulanai.com/product/
+short_text_sim/ HuggingFace Demo: https://huggingface.co/spaces/shibing624/
+text2vec ![](docs/hf.png) run example: [examples/gradio_demo.py](https://
+github.com/shibing624/text2vec/blob/master/examples/gradio_demo.py) to see the
+demo: ```shell python examples/gradio_demo.py ``` ## Install ```shell pip
+install torch # conda install pytorch pip install -U text2vec ``` or ```shell
+pip install torch # conda install pytorch pip install -r requirements.txt git
+clone https://github.com/shibing624/text2vec.git cd text2vec pip install --no-
+deps . ``` ## Usage ### Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂÂ‘Ã©Â‡ÂÃ¨Â¡Â¨Ã¥Â¾Â Ã¥ÂŸÂºÃ¤ÂºÂ`pretrained
+model`Ã¨Â®Â¡Ã§Â®Â—Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂÂ‘Ã©Â‡ÂÃ¯Â¼Âš ```zsh >>> from text2vec import SentenceModel >>> m
+= SentenceModel() >>> m.encode("Ã¥Â¦Â‚Ã¤Â½Â•Ã¦Â›Â´Ã¦ÂÂ¢Ã¨ÂŠÂ±Ã¥Â‘Â—Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡") Embedding
+shape: (768,) ``` example: [examples/computing_embeddings_demo.py](https://
+github.com/shibing624/text2vec/blob/master/examples/
+computing_embeddings_demo.py) ```python import sys sys.path.append('..') from
+text2vec import SentenceModel from text2vec import Word2Vec def compute_emb
+(model): # Embed a list of sentences sentences = [ 'Ã¥ÂÂ¡', 'Ã©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡',
+'Ã¥Â¦Â‚Ã¤Â½Â•Ã¦Â›Â´Ã¦ÂÂ¢Ã¨ÂŠÂ±Ã¥Â‘Â—Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡', 'Ã¨ÂŠÂ±Ã¥Â‘Â—Ã¦Â›Â´Ã¦Â”Â¹Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡', 'This
+framework generates embeddings for each input sentence', 'Sentences are passed
+as a list of string.', 'The quick brown fox jumps over the lazy dog.' ]
+sentence_embeddings = model.encode(sentences) print(type(sentence_embeddings),
+sentence_embeddings.shape) # The result is a list of sentence embeddings as
+numpy arrays for sentence, embedding in zip(sentences, sentence_embeddings):
+print("Sentence:", sentence) print("Embedding shape:", embedding.shape) print
+("Embedding head:", embedding[:10]) print() if __name__ == "__main__": #
+Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂÂ¥Ã¥ÂÂ‘Ã©Â‡ÂÃ¦Â¨Â¡Ã¥ÂÂ‹(CoSENT)Ã¯Â¼ÂŒÃ¤Â¸Â­Ã¦Â–Â‡Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¦ÂÂ¨Ã¨ÂÂÃ¯Â¼ÂŒÃ¦Â”Â¯Ã¦ÂŒÂfine-
+tuneÃ§Â»Â§Ã§Â»Â­Ã¨Â®Â­Ã§Â»Âƒ t2v_model = SentenceModel("shibing624/text2vec-base-chinese")
+compute_emb(t2v_model) #
+Ã¦Â”Â¯Ã¦ÂŒÂÃ¥Â¤ÂšÃ¨Â¯Â­Ã¨Â¨Â€Ã§ÂšÂ„Ã¥ÂÂ¥Ã¥ÂÂ‘Ã©Â‡ÂÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂˆCoSENTÃ¯Â¼Â‰Ã¯Â¼ÂŒÃ¥Â¤ÂšÃ¨Â¯Â­Ã¨Â¨Â€Ã¯Â¼ÂˆÃ¥ÂŒÂ…Ã¦Â‹Â¬Ã¤Â¸Â­Ã¨Â‹Â±Ã¦Â–Â‡Ã¯Â¼Â‰Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¦ÂÂ¨Ã¨ÂÂÃ¯Â¼ÂŒÃ¦Â”Â¯Ã¦ÂŒÂfine-
+tuneÃ§Â»Â§Ã§Â»Â­Ã¨Â®Â­Ã§Â»Âƒ sbert_model = SentenceModel("shibing624/text2vec-base-
+multilingual") compute_emb(sbert_model) # Ã¤Â¸Â­Ã¦Â–Â‡Ã¨Â¯ÂÃ¥ÂÂ‘Ã©Â‡ÂÃ¦Â¨Â¡Ã¥ÂÂ‹
 (word2vec)Ã¯Â¼ÂŒÃ¤Â¸Â­Ã¦Â–Â‡Ã¥Â­Â—Ã©ÂÂ¢Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¥Â’ÂŒÃ¥Â†Â·Ã¥ÂÂ¯Ã¥ÂŠÂ¨Ã©Â€Â‚Ã§Â”Â¨ w2v_model = Word2Vec
 ("w2v-light-tencent-chinese") compute_emb(w2v_model) ``` output: ```
 numpy.ndarray'> (7, 768) Sentence: Ã¥ÂÂ¡ Embedding shape: (768,) Sentence:
 Ã©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡ Embedding shape: (768,) ... ``` -
 Ã¨Â¿Â”Ã¥Â›ÂÃ¥Â€Â¼`embeddings`Ã¦Â˜Â¯`numpy.ndarray`Ã§Â±Â»Ã¥ÂÂ‹Ã¯Â¼ÂŒshapeÃ¤Â¸Âº`(sentences_size,
 model_embedding_size)`Ã¯Â¼ÂŒÃ¤Â¸Â‰Ã¤Â¸ÂªÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¤Â»Â»Ã©Â€Â‰Ã¤Â¸Â€Ã§Â§ÂÃ¥ÂÂ³Ã¥ÂÂ¯Ã¯Â¼ÂŒÃ¦ÂÂ¨Ã¨ÂÂÃ§Â”Â¨Ã§Â¬Â¬Ã¤Â¸Â€Ã¤Â¸ÂªÃ£Â€Â‚
 - `shibing624/text2vec-base-chinese`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â˜Â¯CoSENTÃ¦Â–Â¹Ã¦Â³Â•Ã¥ÂœÂ¨Ã¤Â¸Â­Ã¦Â–Â‡STS-
 BÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â¾Â—Ã¥ÂˆÂ°Ã§ÂšÂ„Ã¯Â¼ÂŒÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¥Â·Â²Ã§Â»ÂÃ¤Â¸ÂŠÃ¤Â¼Â Ã¥ÂˆÂ°huggingfaceÃ§ÂšÂ„ Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¥ÂºÂ“
 [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-
 base-chinese)Ã¯Â¼ÂŒ
 Ã¦Â˜Â¯`text2vec.SentenceModel`Ã¦ÂŒÂ‡Ã¥Â®ÂšÃ§ÂšÂ„Ã©Â»Â˜Ã¨Â®Â¤Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¥ÂÂ¯Ã¤Â»Â¥Ã©Â€ÂšÃ¨Â¿Â‡Ã¤Â¸ÂŠÃ©ÂÂ¢Ã§Â¤ÂºÃ¤Â¾Â‹Ã¨Â°ÂƒÃ§Â”Â¨Ã¯Â¼ÂŒÃ¦ÂˆÂ–Ã¨Â€Â…Ã¥Â¦Â‚Ã¤Â¸Â‹Ã¦Â‰Â€Ã§Â¤ÂºÃ§Â”Â¨
 [transformersÃ¥ÂºÂ“](https://github.com/huggingface/transformers)Ã¨Â°ÂƒÃ§Â”Â¨Ã¯Â¼ÂŒ
-Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â‡ÂªÃ¥ÂŠÂ¨Ã¤Â¸Â‹Ã¨Â½Â½Ã¥ÂˆÂ°Ã¦ÂœÂ¬Ã¦ÂœÂºÃ¨Â·Â¯Ã¥Â¾Â„Ã¯Â¼Âš`~/.cache/huggingface/transformers` -
-`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â˜Â¯Sentence-
-BERTÃ§ÂšÂ„Ã¥Â¤ÂšÃ¨Â¯Â­Ã¨Â¨Â€Ã¥ÂÂ¥Ã¥ÂÂ‘Ã©Â‡ÂÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒ
-Ã©Â€Â‚Ã§Â”Â¨Ã¤ÂºÂÃ©Â‡ÂŠÃ¤Â¹Â‰Ã¯Â¼ÂˆparaphraseÃ¯Â¼Â‰Ã¨Â¯Â†Ã¥ÂˆÂ«Ã¯Â¼ÂŒÃ¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂŒÂ¹Ã©Â…ÂÃ¯Â¼ÂŒÃ©Â€ÂšÃ¨Â¿Â‡`text2vec.SentenceModel`Ã¥Â’ÂŒ
-[sentence-transformersÃ¥ÂºÂ“]((https://github.com/UKPLab/sentence-
-transformers))Ã©ÂƒÂ½Ã¥ÂÂ¯Ã¤Â»Â¥Ã¨Â°ÂƒÃ§Â”Â¨Ã¨Â¯Â¥Ã¦Â¨Â¡Ã¥ÂÂ‹ - `w2v-light-tencent-
+Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â‡ÂªÃ¥ÂŠÂ¨Ã¤Â¸Â‹Ã¨Â½Â½Ã¥ÂˆÂ°Ã¦ÂœÂ¬Ã¦ÂœÂºÃ¨Â·Â¯Ã¥Â¾Â„Ã¯Â¼Âš`~/.cache/huggingface/transformers` - `w2v-
+light-tencent-
 chinese`Ã¦Â˜Â¯Ã©Â€ÂšÃ¨Â¿Â‡gensimÃ¥ÂŠÂ Ã¨Â½Â½Ã§ÂšÂ„Word2VecÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¤Â½Â¿Ã§Â”Â¨Ã¨Â…Â¾Ã¨Â®Â¯Ã¨Â¯ÂÃ¥ÂÂ‘Ã©Â‡Â`Tencent_AILab_ChineseEmbedding.tar.gz`Ã¨Â®Â¡Ã§Â®Â—Ã¥ÂÂ„Ã¥Â­Â—Ã¨Â¯ÂÃ§ÂšÂ„Ã¨Â¯ÂÃ¥ÂÂ‘Ã©Â‡ÂÃ¯Â¼ÂŒÃ¥ÂÂ¥Ã¥Â­ÂÃ¥ÂÂ‘Ã©Â‡ÂÃ©Â€ÂšÃ¨Â¿Â‡Ã¥ÂÂ•Ã¨Â¯ÂÃ¨Â¯Â
 Ã¥ÂÂ‘Ã©Â‡ÂÃ¥ÂÂ–Ã¥Â¹Â³Ã¥ÂÂ‡Ã¥Â€Â¼Ã¥Â¾Â—Ã¥ÂˆÂ°Ã¯Â¼ÂŒÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â‡ÂªÃ¥ÂŠÂ¨Ã¤Â¸Â‹Ã¨Â½Â½Ã¥ÂˆÂ°Ã¦ÂœÂ¬Ã¦ÂœÂºÃ¨Â·Â¯Ã¥Â¾Â„Ã¯Â¼Âš`~/.text2vec/
 datasets/light_Tencent_AILab_ChineseEmbedding.bin` #### Usage (HuggingFace
 Transformers) Without [text2vec](https://github.com/shibing624/text2vec), you
 can use the model like this: First, you pass your input through the transformer
 model, then you have to apply the right pooling-operation on-top of the
 contextualized word embeddings. example: [examples/
@@ -322,16 +342,16 @@
 Ã¦Â–Â‡Ã¦ÂœÂ¬Ã§Â›Â¸Ã¤Â¼Â¼Ã¥ÂºÂ¦Ã¨Â®Â¡Ã§Â®Â—Ã¥Â’ÂŒÃ¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦ÂÂœÃ§Â´Â¢Ã¤Â»Â»Ã¥ÂŠÂ¡Ã¯Â¼ÂŒÃ¦ÂÂ¨Ã¨ÂÂÃ¤Â½Â¿Ã§Â”Â¨
 [similaritiesÃ¥ÂºÂ“](https://github.com/shibing624/similarities)
 Ã¯Â¼ÂŒÃ¥Â…Â¼Ã¥Â®Â¹Ã¦ÂœÂ¬Ã©Â¡Â¹Ã§Â›Â®releaseÃ§ÂšÂ„
 Word2vecÃ£Â€ÂSBERTÃ£Â€ÂCosentÃ§Â±Â»Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¨Â¿Â˜Ã¦Â”Â¯Ã¦ÂŒÂÃ¥Â­Â—Ã©ÂÂ¢Ã§Â»Â´Ã¥ÂºÂ¦Ã§Â›Â¸Ã¤Â¼Â¼Ã¥ÂºÂ¦Ã¨Â®Â¡Ã§Â®Â—Ã£Â€ÂÃ¥ÂŒÂ¹Ã©Â…ÂÃ¦ÂÂœÃ§Â´Â¢Ã§Â®Â—Ã¦Â³Â•Ã¯Â¼ÂŒÃ¦Â”Â¯Ã¦ÂŒÂÃ¦Â–Â‡Ã¦ÂœÂ¬Ã£Â€ÂÃ¥Â›Â¾Ã¥ÂƒÂÃ£Â€Â‚
 Ã¥Â®Â‰Ã¨Â£Â…Ã¯Â¼Âš ```pip install -U similarities``` Ã¥ÂÂ¥Ã¥Â­ÂÃ§Â›Â¸Ã¤Â¼Â¼Ã¥ÂºÂ¦Ã¨Â®Â¡Ã§Â®Â—Ã¯Â¼Âš ```python
 from similarities import Similarity m = Similarity() r = m.similarity
 ('Ã¥Â¦Â‚Ã¤Â½Â•Ã¦Â›Â´Ã¦ÂÂ¢Ã¨ÂŠÂ±Ã¥Â‘Â—Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡', 'Ã¨ÂŠÂ±Ã¥Â‘Â—Ã¦Â›Â´Ã¦Â”Â¹Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡') print
-(f"similarity score: {float(r)}") # similarity score: 0.855146050453186 ``` #
-Models ## CoSENT model CoSENTÃ¯Â¼ÂˆCosine
+(f"similarity score: {float(r)}") # similarity score: 0.855146050453186 ``` ##
+Models ### CoSENT model CoSENTÃ¯Â¼ÂˆCosine
 SentenceÃ¯Â¼Â‰Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¥ÂœÂ¨Sentence-
 BERTÃ¤Â¸ÂŠÃ¦Â”Â¹Ã¨Â¿Â›Ã¤ÂºÂ†CosineRankLossÃ§ÂšÂ„Ã¥ÂÂ¥Ã¥ÂÂ‘Ã©Â‡ÂÃ¦Â–Â¹Ã¦Â¡Âˆ Network structure: Training:
 [docs/cosent_train.png] Inference: [docs/inference.png] #### CoSENT
 Ã§Â›Â‘Ã§ÂÂ£Ã¦Â¨Â¡Ã¥ÂÂ‹ Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â’ÂŒÃ©Â¢Â„Ã¦ÂµÂ‹CoSENTÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼Âš - Ã¥ÂœÂ¨Ã¤Â¸Â­Ã¦Â–Â‡STS-
 BÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â’ÂŒÃ¨Â¯Â„Ã¤Â¼Â°`CoSENT`Ã¦Â¨Â¡Ã¥ÂÂ‹ example: [examples/
 training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/
 blob/master/examples/training_sup_text_matching_model.py) ```shell cd examples
@@ -343,21 +363,24 @@
 'PAWSX'Ã¯Â¼ÂŒÃ¥Â…Â·Ã¤Â½Â“Ã¥ÂÂ‚Ã¨Â€ÂƒHuggingFace datasets [https://huggingface.co/datasets/
 shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh) ```shell
 python training_sup_text_matching_model.py --task_name ATEC --model_arch cosent
 --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base -
 -output_dir ./outputs/ATEC-cosent ``` - Ã¥ÂœÂ¨Ã¨Â‡ÂªÃ¦ÂœÂ‰Ã¤Â¸Â­Ã¦Â–Â‡Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¤Â¸ÂŠÃ¨Â®Â­Ã§Â»ÂƒÃ¦Â¨Â¡Ã¥ÂÂ‹
 example: [examples/training_sup_text_matching_model_mydata.py](https://
 github.com/shibing624/text2vec/blob/master/examples/
-training_sup_text_matching_model_mydata.py) ```shell python
-training_sup_text_matching_model_mydata.py --do_train --do_predict ```
-Ã¨Â®Â­Ã§Â»ÂƒÃ©Â›Â†Ã¦Â Â¼Ã¥Â¼ÂÃ¥ÂÂ‚Ã¨Â€Âƒ[examples/data/STS-B/STS-B.valid.data](https://github.com/
-shibing624/text2vec/blob/master/examples/data/STS-B/STS-B.valid.data) ```shell
-sentence1 sentence2 label Ã¤Â¸Â€Ã¤Â¸ÂªÃ¥Â¥Â³Ã¥Â­Â©Ã¥ÂœÂ¨Ã§Â»Â™Ã¥Â¥Â¹Ã§ÂšÂ„Ã¥Â¤Â´Ã¥ÂÂ‘Ã¥ÂÂšÃ¥ÂÂ‘Ã¥ÂÂ‹Ã£Â€Â‚
-Ã¤Â¸Â€Ã¤Â¸ÂªÃ¥Â¥Â³Ã¥Â­Â©Ã¥ÂœÂ¨Ã¦Â¢Â³Ã¥Â¤Â´Ã£Â€Â‚ 2 Ã¤Â¸Â€Ã§Â¾Â¤Ã§Â”Â·Ã¤ÂºÂºÃ¥ÂœÂ¨Ã¦ÂµÂ·Ã¦Â»Â©Ã¤Â¸ÂŠÃ¨Â¸Â¢Ã¨Â¶Â³Ã§ÂÂƒÃ£Â€Â‚
-Ã¤Â¸Â€Ã§Â¾Â¤Ã§Â”Â·Ã¥Â­Â©Ã¥ÂœÂ¨Ã¦ÂµÂ·Ã¦Â»Â©Ã¤Â¸ÂŠÃ¨Â¸Â¢Ã¨Â¶Â³Ã§ÂÂƒÃ£Â€Â‚ 3
+training_sup_text_matching_model_mydata.py) Ã¥ÂÂ•Ã¥ÂÂ¡Ã¨Â®Â­Ã§Â»ÂƒÃ¯Â¼Âš ```shell
+CUDA_VISIBLE_DEVICES=0 python training_sup_text_matching_model_mydata.py --
+do_train --do_predict ``` Ã¥Â¤ÂšÃ¥ÂÂ¡Ã¨Â®Â­Ã§Â»ÂƒÃ¯Â¼Âš ```shell CUDA_VISIBLE_DEVICES=0,1
+torchrun --nproc_per_node 2 training_sup_text_matching_model_mydata.py --
+do_train --do_predict --output_dir outputs/STS-B-text2vec-macbert-v1 --
+batch_size 64 --fp16 --data_parallel ``` Ã¨Â®Â­Ã§Â»ÂƒÃ©Â›Â†Ã¦Â Â¼Ã¥Â¼ÂÃ¥ÂÂ‚Ã¨Â€Âƒ[examples/data/
+STS-B/STS-B.valid.data](https://github.com/shibing624/text2vec/blob/master/
+examples/data/STS-B/STS-B.valid.data) ```shell sentence1 sentence2 label
+Ã¤Â¸Â€Ã¤Â¸ÂªÃ¥Â¥Â³Ã¥Â­Â©Ã¥ÂœÂ¨Ã§Â»Â™Ã¥Â¥Â¹Ã§ÂšÂ„Ã¥Â¤Â´Ã¥ÂÂ‘Ã¥ÂÂšÃ¥ÂÂ‘Ã¥ÂÂ‹Ã£Â€Â‚ Ã¤Â¸Â€Ã¤Â¸ÂªÃ¥Â¥Â³Ã¥Â­Â©Ã¥ÂœÂ¨Ã¦Â¢Â³Ã¥Â¤Â´Ã£Â€Â‚ 2
+Ã¤Â¸Â€Ã§Â¾Â¤Ã§Â”Â·Ã¤ÂºÂºÃ¥ÂœÂ¨Ã¦ÂµÂ·Ã¦Â»Â©Ã¤Â¸ÂŠÃ¨Â¸Â¢Ã¨Â¶Â³Ã§ÂÂƒÃ£Â€Â‚ Ã¤Â¸Â€Ã§Â¾Â¤Ã§Â”Â·Ã¥Â­Â©Ã¥ÂœÂ¨Ã¦ÂµÂ·Ã¦Â»Â©Ã¤Â¸ÂŠÃ¨Â¸Â¢Ã¨Â¶Â³Ã§ÂÂƒÃ£Â€Â‚ 3
 Ã¤Â¸Â€Ã¤Â¸ÂªÃ¥Â¥Â³Ã¤ÂºÂºÃ¥ÂœÂ¨Ã¦ÂµÂ‹Ã©Â‡ÂÃ¥ÂÂ¦Ã¤Â¸Â€Ã¤Â¸ÂªÃ¥Â¥Â³Ã¤ÂºÂºÃ§ÂšÂ„Ã¨Â„ÂšÃ¨Â¸ÂÃ£Â€Â‚
 Ã¥Â¥Â³Ã¤ÂºÂºÃ¦ÂµÂ‹Ã©Â‡ÂÃ¥ÂÂ¦Ã¤Â¸Â€Ã¤Â¸ÂªÃ¥Â¥Â³Ã¤ÂºÂºÃ§ÂšÂ„Ã¨Â„ÂšÃ¨Â¸ÂÃ£Â€Â‚ 5 ```
 `label`Ã¥ÂÂ¯Ã¤Â»Â¥Ã¦Â˜Â¯0Ã¯Â¼ÂŒ1Ã¦Â Â‡Ã§Â­Â¾Ã¯Â¼ÂŒ0Ã¤Â»Â£Ã¨Â¡Â¨Ã¤Â¸Â¤Ã¤Â¸ÂªÃ¥ÂÂ¥Ã¥Â­ÂÃ¤Â¸ÂÃ§Â›Â¸Ã¤Â¼Â¼Ã¯Â¼ÂŒ1Ã¤Â»Â£Ã¨Â¡Â¨Ã§Â›Â¸Ã¤Â¼Â¼Ã¯Â¼Â›Ã¤Â¹ÂŸÃ¥ÂÂ¯Ã¤Â»Â¥Ã¦Â˜Â¯0-
 5Ã§ÂšÂ„Ã¨Â¯Â„Ã¥ÂˆÂ†Ã¯Â¼ÂŒÃ¨Â¯Â„Ã¥ÂˆÂ†Ã¨Â¶ÂŠÃ©Â«Â˜Ã¯Â¼ÂŒÃ¨Â¡Â¨Ã§Â¤ÂºÃ¤Â¸Â¤Ã¤Â¸ÂªÃ¥ÂÂ¥Ã¥Â­ÂÃ¨Â¶ÂŠÃ§Â›Â¸Ã¤Â¼Â¼Ã£Â€Â‚Ã¦Â¨Â¡Ã¥ÂÂ‹Ã©ÂƒÂ½Ã¨ÂƒÂ½Ã¦Â”Â¯Ã¦ÂŒÂÃ£Â€Â‚
 - Ã¥ÂœÂ¨Ã¨Â‹Â±Ã¦Â–Â‡STS-BÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â’ÂŒÃ¨Â¯Â„Ã¤Â¼Â°`CoSENT`Ã¦Â¨Â¡Ã¥ÂÂ‹ example: [examples/
 training_sup_text_matching_model_en.py](https://github.com/shibing624/text2vec/
 blob/master/examples/training_sup_text_matching_model_en.py) ```shell cd
@@ -366,15 +389,15 @@
 output_dir ./outputs/STS-B-en-cosent ``` #### CoSENT Ã¦Â—Â Ã§Â›Â‘Ã§ÂÂ£Ã¦Â¨Â¡Ã¥ÂÂ‹ -
 Ã¥ÂœÂ¨Ã¨Â‹Â±Ã¦Â–Â‡NLIÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¨Â®Â­Ã§Â»Âƒ`CoSENT`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¥ÂœÂ¨STS-BÃ¦ÂµÂ‹Ã¨Â¯Â•Ã©Â›Â†Ã¨Â¯Â„Ã¤Â¼Â°Ã¦Â•ÂˆÃ¦ÂÂœ
 example: [examples/training_unsup_text_matching_model_en.py](https://
 github.com/shibing624/text2vec/blob/master/examples/
 training_unsup_text_matching_model_en.py) ```shell cd examples python
 training_unsup_text_matching_model_en.py --model_arch cosent --do_train --
 do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./
-outputs/STS-B-en-unsup-cosent ``` ## Sentence-BERT model Sentence-
+outputs/STS-B-en-unsup-cosent ``` ### Sentence-BERT model Sentence-
 BERTÃ¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¨Â¡Â¨Ã¥Â¾ÂÃ¥Â¼ÂÃ¥ÂÂ¥Ã¥ÂÂ‘Ã©Â‡ÂÃ¨Â¡Â¨Ã§Â¤ÂºÃ¦Â–Â¹Ã¦Â¡Âˆ Network structure:
 Training: [docs/sbert_train.png] Inference: [docs/sbert_inference.png] ####
 SentenceBERT Ã§Â›Â‘Ã§ÂÂ£Ã¦Â¨Â¡Ã¥ÂÂ‹ - Ã¥ÂœÂ¨Ã¤Â¸Â­Ã¦Â–Â‡STS-BÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â’ÂŒÃ¨Â¯Â„Ã¤Â¼Â°`SBERT`Ã¦Â¨Â¡Ã¥ÂÂ‹
 example: [examples/training_sup_text_matching_model.py](https://github.com/
 shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)
 ```shell cd examples python training_sup_text_matching_model.py --model_arch
 sentencebert --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-
@@ -387,35 +410,35 @@
 uncased --output_dir ./outputs/STS-B-en-sbert ``` #### SentenceBERT
 Ã¦Â—Â Ã§Â›Â‘Ã§ÂÂ£Ã¦Â¨Â¡Ã¥ÂÂ‹ - Ã¥ÂœÂ¨Ã¨Â‹Â±Ã¦Â–Â‡NLIÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¨Â®Â­Ã§Â»Âƒ`SBERT`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¥ÂœÂ¨STS-
 BÃ¦ÂµÂ‹Ã¨Â¯Â•Ã©Â›Â†Ã¨Â¯Â„Ã¤Â¼Â°Ã¦Â•ÂˆÃ¦ÂÂœ example: [examples/
 training_unsup_text_matching_model_en.py](https://github.com/shibing624/
 text2vec/blob/master/examples/training_unsup_text_matching_model_en.py)
 ```shell cd examples python training_unsup_text_matching_model_en.py --
 model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name
-bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-sbert ``` ## BERT-Match
-model
+bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-sbert ``` ### BERT-
+Match model
 BERTÃ¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¥ÂÂŸÃ§Â”ÂŸBERTÃ¥ÂŒÂ¹Ã©Â…ÂÃ§Â½Â‘Ã§Â»ÂœÃ§Â»Â“Ã¦ÂÂ„Ã¯Â¼ÂŒÃ¤ÂºÂ¤Ã¤ÂºÂ’Ã¥Â¼ÂÃ¥ÂÂ¥Ã¥ÂÂ‘Ã©Â‡ÂÃ¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹
 Network structure: Training and inference: [docs/bert-fc-train.png]
 Ã¨Â®Â­Ã§Â»ÂƒÃ¨Â„ÂšÃ¦ÂœÂ¬Ã¥ÂÂŒÃ¤Â¸ÂŠ[examples/training_sup_text_matching_model.py](https://
 github.com/shibing624/text2vec/blob/master/examples/
-training_sup_text_matching_model.py)Ã£Â€Â‚ ## Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â’Â¸Ã©Â¦ÂÃ¯Â¼ÂˆModel DistillationÃ¯Â¼Â‰
-Ã§Â”Â±Ã¤ÂºÂtext2vecÃ¨Â®Â­Ã§Â»ÂƒÃ§ÂšÂ„Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¥ÂÂ¯Ã¤Â»Â¥Ã¤Â½Â¿Ã§Â”Â¨[sentence-transformers](https://
-github.com/UKPLab/sentence-
+training_sup_text_matching_model.py)Ã£Â€Â‚ ### Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â’Â¸Ã©Â¦ÂÃ¯Â¼ÂˆModel
+DistillationÃ¯Â¼Â‰ Ã§Â”Â±Ã¤ÂºÂtext2vecÃ¨Â®Â­Ã§Â»ÂƒÃ§ÂšÂ„Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¥ÂÂ¯Ã¤Â»Â¥Ã¤Â½Â¿Ã§Â”Â¨[sentence-
+transformers](https://github.com/UKPLab/sentence-
 transformers)Ã¥ÂºÂ“Ã¥ÂŠÂ Ã¨Â½Â½Ã¯Â¼ÂŒÃ¦Â­Â¤Ã¥Â¤Â„Ã¥Â¤ÂÃ§Â”Â¨Ã¥Â…Â¶Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â’Â¸Ã©Â¦ÂÃ¦Â–Â¹Ã¦Â³Â•[distillation](https:
 //github.com/UKPLab/sentence-transformers/tree/master/examples/training/
 distillation)Ã£Â€Â‚ 1. Ã¦Â¨Â¡Ã¥ÂÂ‹Ã©Â™ÂÃ§Â»Â´Ã¯Â¼ÂŒÃ¥ÂÂ‚Ã¨Â€Âƒ[dimensionality_reduction.py](https://
 github.com/UKPLab/sentence-transformers/blob/master/examples/training/
 distillation/
 dimensionality_reduction.py)Ã¤Â½Â¿Ã§Â”Â¨PCAÃ¥Â¯Â¹Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â¾Â“Ã¥Â‡ÂºembeddingÃ©Â™ÂÃ§Â»Â´Ã¯Â¼ÂŒÃ¥ÂÂ¯Ã¥Â‡ÂÃ¥Â°Â‘milvusÃ§Â­Â‰Ã¥ÂÂ‘Ã©Â‡ÂÃ¦Â£Â€Ã§Â´Â¢Ã¦Â•Â°Ã¦ÂÂ®Ã¥ÂºÂ“Ã§ÂšÂ„Ã¥Â­Â˜Ã¥Â‚Â¨Ã¥ÂÂ‹Ã¥ÂŠÂ›Ã¯Â¼ÂŒÃ¨Â¿Â˜Ã¨ÂƒÂ½Ã¨Â½Â»Ã¥Â¾Â®Ã¦ÂÂÃ¥ÂÂ‡Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â•ÂˆÃ¦ÂÂœÃ£Â€Â‚
 2. Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â’Â¸Ã©Â¦ÂÃ¯Â¼ÂŒÃ¥ÂÂ‚Ã¨Â€Âƒ[model_distillation.py](https://github.com/UKPLab/
 sentence-transformers/blob/master/examples/training/distillation/
 model_distillation.py)Ã¤Â½Â¿Ã§Â”Â¨Ã¨Â’Â¸Ã©Â¦ÂÃ¦Â–Â¹Ã¦Â³Â•Ã¯Â¼ÂŒÃ¥Â°Â†TeacherÃ¥Â¤Â§Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â’Â¸Ã©Â¦ÂÃ¥ÂˆÂ°Ã¦Â›Â´Ã¥Â°Â‘layersÃ¥Â±Â‚Ã¦Â•Â°Ã§ÂšÂ„studentÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¤Â¸Â­Ã¯Â¼ÂŒÃ¥ÂœÂ¨Ã¦ÂÂƒÃ¨Â¡Â¡Ã¦Â•ÂˆÃ¦ÂÂœÃ§ÂšÂ„Ã¦ÂƒÂ…Ã¥Â†ÂµÃ¤Â¸Â‹Ã¯Â¼ÂŒÃ¥ÂÂ¯Ã¥Â¤Â§Ã¥Â¹Â…Ã¦ÂÂÃ¥ÂÂ‡Ã¦Â¨Â¡Ã¥ÂÂ‹Ã©Â¢Â„Ã¦ÂµÂ‹Ã©Â€ÂŸÃ¥ÂºÂ¦Ã£Â€Â‚
-## Ã¦Â¨Â¡Ã¥ÂÂ‹Ã©ÂƒÂ¨Ã§Â½Â² Ã¦ÂÂÃ¤Â¾Â›Ã¤Â¸Â¤Ã§Â§ÂÃ©ÂƒÂ¨Ã§Â½Â²Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¦ÂÂ­Ã¥Â»ÂºÃ¦ÂœÂÃ¥ÂŠÂ¡Ã§ÂšÂ„Ã¦Â–Â¹Ã¦Â³Â•Ã¯Â¼Âš
+### Ã¦Â¨Â¡Ã¥ÂÂ‹Ã©ÂƒÂ¨Ã§Â½Â² Ã¦ÂÂÃ¤Â¾Â›Ã¤Â¸Â¤Ã§Â§ÂÃ©ÂƒÂ¨Ã§Â½Â²Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¦ÂÂ­Ã¥Â»ÂºÃ¦ÂœÂÃ¥ÂŠÂ¡Ã§ÂšÂ„Ã¦Â–Â¹Ã¦Â³Â•Ã¯Â¼Âš
 1Ã¯Â¼Â‰Ã¥ÂŸÂºÃ¤ÂºÂJinaÃ¦ÂÂ­Ã¥Â»ÂºgRPCÃ¦ÂœÂÃ¥ÂŠÂ¡Ã£Â€ÂÃ¦ÂÂ¨Ã¨ÂÂÃ£Â€Â‘Ã¯Â¼Â›2Ã¯Â¼Â‰Ã¥ÂŸÂºÃ¤ÂºÂFastAPIÃ¦ÂÂ­Ã¥Â»ÂºÃ¥ÂÂŸÃ§Â”ÂŸHttpÃ¦ÂœÂÃ¥ÂŠÂ¡Ã£Â€Â‚
-### JinaÃ¦ÂœÂÃ¥ÂŠÂ¡ Ã©Â‡Â‡Ã§Â”Â¨C/
+#### JinaÃ¦ÂœÂÃ¥ÂŠÂ¡ Ã©Â‡Â‡Ã§Â”Â¨C/
 SÃ¦Â¨Â¡Ã¥Â¼ÂÃ¦ÂÂ­Ã¥Â»ÂºÃ©Â«Â˜Ã¦Â€Â§Ã¨ÂƒÂ½Ã¦ÂœÂÃ¥ÂŠÂ¡Ã¯Â¼ÂŒÃ¦Â”Â¯Ã¦ÂŒÂdockerÃ¤ÂºÂ‘Ã¥ÂÂŸÃ§Â”ÂŸÃ¯Â¼ÂŒgRPC/HTTP/
 WebSocketÃ¯Â¼ÂŒÃ¦Â”Â¯Ã¦ÂŒÂÃ¥Â¤ÂšÃ¤Â¸ÂªÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¥ÂÂŒÃ¦Â—Â¶Ã©Â¢Â„Ã¦ÂµÂ‹Ã¯Â¼ÂŒGPUÃ¥Â¤ÂšÃ¥ÂÂ¡Ã¥Â¤Â„Ã§ÂÂ†Ã£Â€Â‚ - Ã¥Â®Â‰Ã¨Â£Â…Ã¯Â¼Âš
 ```pip install jina``` - Ã¥ÂÂ¯Ã¥ÂŠÂ¨Ã¦ÂœÂÃ¥ÂŠÂ¡Ã¯Â¼Âš example: [examples/
 jina_server_demo.py](examples/jina_server_demo.py) ```python from jina import
 Flow port = 50001 f = Flow(port=port).add( uses='jinahub://Text2vecEncoder',
 uses_with={'model_name': 'shibing624/text2vec-base-chinese'} ) with f: #
 backend server forever f.block() ```
@@ -424,27 +447,27 @@
 Ã¨Â°ÂƒÃ§Â”Â¨Ã¦ÂœÂÃ¥ÂŠÂ¡Ã¯Â¼Âš ```python from jina import Client from docarray import
 Document, DocumentArray port = 50001 c = Client(port=port) data =
 ['Ã¥Â¦Â‚Ã¤Â½Â•Ã¦Â›Â´Ã¦ÂÂ¢Ã¨ÂŠÂ±Ã¥Â‘Â—Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡', 'Ã¨ÂŠÂ±Ã¥Â‘Â—Ã¦Â›Â´Ã¦Â”Â¹Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡'] print
 ("data:", data) print('data embs:') r = c.post('/', inputs=DocumentArray(
 [Document(text='Ã¥Â¦Â‚Ã¤Â½Â•Ã¦Â›Â´Ã¦ÂÂ¢Ã¨ÂŠÂ±Ã¥Â‘Â—Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡'), Document
 (text='Ã¨ÂŠÂ±Ã¥Â‘Â—Ã¦Â›Â´Ã¦Â”Â¹Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡')])) print(r.embeddings) ```
 Ã¦Â‰Â¹Ã©Â‡ÂÃ¨Â°ÂƒÃ§Â”Â¨Ã¦Â–Â¹Ã¦Â³Â•Ã¨Â§Âexample: [examples/jina_client_demo.py](https://
-github.com/shibing624/text2vec/blob/master/examples/jina_client_demo.py) ###
+github.com/shibing624/text2vec/blob/master/examples/jina_client_demo.py) ####
 FastAPIÃ¦ÂœÂÃ¥ÂŠÂ¡ - Ã¥Â®Â‰Ã¨Â£Â…Ã¯Â¼Âš ```pip install fastapi uvicorn``` - Ã¥ÂÂ¯Ã¥ÂŠÂ¨Ã¦ÂœÂÃ¥ÂŠÂ¡Ã¯Â¼Âš
 example: [examples/fastapi_server_demo.py](https://github.com/shibing624/
 text2vec/blob/master/examples/fastapi_server_demo.py) ```shell cd examples
 python fastapi_server_demo.py ``` - Ã¨Â°ÂƒÃ§Â”Â¨Ã¦ÂœÂÃ¥ÂŠÂ¡Ã¯Â¼Âš ```shell curl -X 'GET' \
 'http://0.0.0.0:8001/emb?q=hello' \ -H 'accept: application/json' ``` ##
-Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â† - Ã¦ÂœÂ¬Ã©Â¡Â¹Ã§Â›Â®releaseÃ§ÂšÂ„Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼Âš | Dataset | Introduce | Download
-Link | |:---------------------------|:-----------------------------------------
---------------------------------|:---------------------------------------------
+Dataset - Ã¦ÂœÂ¬Ã©Â¡Â¹Ã§Â›Â®releaseÃ§ÂšÂ„Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼Âš | Dataset | Introduce | Download Link
+| |:---------------------------|:----------------------------------------------
+---------------------------|:--------------------------------------------------
 -------------------------------------------------------------------------------
 -------------------------------------------------------------------------------
 -------------------------------------------------------------------------------
-------------| | shibing624/nli-zh-all |
+-------| | shibing624/nli-zh-all |
 Ã¤Â¸Â­Ã¦Â–Â‡Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã¥ÂÂˆÃ©Â›Â†Ã¯Â¼ÂŒÃ¦Â•Â´Ã¥ÂÂˆÃ¤ÂºÂ†Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¦ÂÂ¨Ã§ÂÂ†Ã¯Â¼ÂŒÃ§Â›Â¸Ã¤Â¼Â¼Ã¯Â¼ÂŒÃ¦Â‘Â˜Ã¨Â¦ÂÃ¯Â¼ÂŒÃ©Â—Â®Ã§Â­Â”Ã¯Â¼ÂŒÃ¦ÂŒÂ‡Ã¤Â»Â¤Ã¥Â¾Â®Ã¨Â°ÂƒÃ§Â­Â‰Ã¤Â»Â»Ã¥ÂŠÂ¡Ã§ÂšÂ„820Ã¤Â¸Â‡Ã©Â«Â˜Ã¨Â´Â¨Ã©Â‡ÂÃ¦Â•Â°Ã¦ÂÂ®Ã¯Â¼ÂŒÃ¥Â¹Â¶Ã¨Â½Â¬Ã¥ÂŒÂ–Ã¤Â¸ÂºÃ¥ÂŒÂ¹Ã©Â…ÂÃ¦Â Â¼Ã¥Â¼ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†
 | [https://huggingface.co/datasets/shibing624/nli-zh-all](https://
 huggingface.co/datasets/shibing624/nli-zh-all) | | shibing624/snli-zh |
 Ã¤Â¸Â­Ã¦Â–Â‡SNLIÃ¥Â’ÂŒMultiNLIÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼ÂŒÃ§Â¿Â»Ã¨Â¯Â‘Ã¨Â‡ÂªÃ¨Â‹Â±Ã¦Â–Â‡SNLIÃ¥Â’ÂŒMultiNLI | [https://
 huggingface.co/datasets/shibing624/snli-zh](https://huggingface.co/datasets/
 shibing624/snli-zh) | | shibing624/nli_zh |
 Ã¤Â¸Â­Ã¦Â–Â‡Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼ÂŒÃ¦Â•Â´Ã¥ÂÂˆÃ¤ÂºÂ†Ã¤Â¸Â­Ã¦Â–Â‡ATECÃ£Â€ÂBQÃ£Â€ÂLCQMCÃ£Â€ÂPAWSXÃ£Â€ÂSTS-
@@ -462,46 +485,46 @@
 info/1037/1162.htm) | | LCQMC | Ã¤Â¸Â­Ã¦Â–Â‡LCQMC(large-scale Chinese question
 matching corpus)Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼ÂŒQ-QpairÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â† | [LCQMC](http://
 icrc.hitsz.edu.cn/Article/show/171.html) | | PAWSX | Ã¤Â¸Â­Ã¦Â–Â‡PAWS(Paraphrase
 Adversaries from Word Scrambling)Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼ÂŒQ-QpairÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â† | [PAWSX](https:/
 /arxiv.org/abs/1908.11828) | | STS-B | Ã¤Â¸Â­Ã¦Â–Â‡STS-
 BÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼ÂŒÃ¤Â¸Â­Ã¦Â–Â‡Ã¨Â‡ÂªÃ§Â„Â¶Ã¨Â¯Â­Ã¨Â¨Â€Ã¦ÂÂ¨Ã§ÂÂ†Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼ÂŒÃ¤Â»ÂÃ¨Â‹Â±Ã¦Â–Â‡STS-
 BÃ§Â¿Â»Ã¨Â¯Â‘Ã¤Â¸ÂºÃ¤Â¸Â­Ã¦Â–Â‡Ã§ÂšÂ„Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â† | [STS-B](https://github.com/pluto-junzeng/CNSD) |
-Ã¥Â¸Â¸Ã§Â”Â¨Ã¨Â‹Â±Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼Âš - Ã¥Â¤Â§Ã¥ÂÂÃ©Â¼ÂÃ©Â¼ÂÃ§ÂšÂ„multi_nliÃ¥Â’ÂŒsnli: https://
-huggingface.co/datasets/multi_nli - Ã¥Â¤Â§Ã¥ÂÂÃ©Â¼ÂÃ©Â¼ÂÃ§ÂšÂ„multi_nliÃ¥Â’ÂŒsnli: https://
+Ã¥Â¸Â¸Ã§Â”Â¨Ã¨Â‹Â±Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼Âš - Ã¨Â‹Â±Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼Âšmulti_nli: https://
+huggingface.co/datasets/multi_nli - Ã¨Â‹Â±Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼Âšsnli: https://
 huggingface.co/datasets/snli - https://huggingface.co/datasets/metaeval/cnli -
 https://huggingface.co/datasets/mteb/stsbenchmark-sts - https://huggingface.co/
 datasets/JeremiahZ/simcse_sup_nli - https://huggingface.co/datasets/
 MoritzLaurer/multilingual-NLI-26lang-2mil7 Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¤Â½Â¿Ã§Â”Â¨Ã§Â¤ÂºÃ¤Â¾Â‹Ã¯Â¼Âš ```shell
 pip install datasets ``` ```python from datasets import load_dataset dataset =
 load_dataset("shibing624/nli_zh", "STS-B") # ATEC or BQ or LCQMC or PAWSX or
 STS-B print(dataset) print(dataset['test'][0]) ``` output: ```shell DatasetDict
 ({ train: Dataset({ features: ['sentence1', 'sentence2', 'label'], num_rows:
 5231 }) validation: Dataset({ features: ['sentence1', 'sentence2', 'label'],
 num_rows: 1458 }) test: Dataset({ features: ['sentence1', 'sentence2',
 'label'], num_rows: 1361 }) }) {'sentence1':
 'Ã¤Â¸Â€Ã¤Â¸ÂªÃ¥Â¥Â³Ã¥Â­Â©Ã¥ÂœÂ¨Ã§Â»Â™Ã¥Â¥Â¹Ã§ÂšÂ„Ã¥Â¤Â´Ã¥ÂÂ‘Ã¥ÂÂšÃ¥ÂÂ‘Ã¥ÂÂ‹Ã£Â€Â‚', 'sentence2':
-'Ã¤Â¸Â€Ã¤Â¸ÂªÃ¥Â¥Â³Ã¥Â­Â©Ã¥ÂœÂ¨Ã¦Â¢Â³Ã¥Â¤Â´Ã£Â€Â‚', 'label': 2} ``` # Contact - Issue(Ã¥Â»ÂºÃ¨Â®Â®)Ã¯Â¼Âš[!
+'Ã¤Â¸Â€Ã¤Â¸ÂªÃ¥Â¥Â³Ã¥Â­Â©Ã¥ÂœÂ¨Ã¦Â¢Â³Ã¥Â¤Â´Ã£Â€Â‚', 'label': 2} ``` ## Contact - Issue(Ã¥Â»ÂºÃ¨Â®Â®)Ã¯Â¼Âš[!
 [GitHub issues](https://img.shields.io/github/issues/shibing624/text2vec.svg)]
 (https://github.com/shibing624/text2vec/issues) - Ã©Â‚Â®Ã¤Â»Â¶Ã¦ÂˆÂ‘Ã¯Â¼Âšxuming:
 xuming624@qq.com - Ã¥Â¾Â®Ã¤Â¿Â¡Ã¦ÂˆÂ‘Ã¯Â¼ÂšÃ¥ÂŠÂ Ã¦ÂˆÂ‘*Ã¥Â¾Â®Ã¤Â¿Â¡Ã¥ÂÂ·Ã¯Â¼Âšxuming624, Ã¥Â¤Â‡Ã¦Â³Â¨Ã¯Â¼ÂšÃ¥Â§Â“Ã¥ÂÂ-
-Ã¥Â…Â¬Ã¥ÂÂ¸-NLP* Ã¨Â¿Â›NLPÃ¤ÂºÂ¤Ã¦ÂµÂÃ§Â¾Â¤Ã£Â€Â‚ [docs/wechat.jpeg] # Citation
+Ã¥Â…Â¬Ã¥ÂÂ¸-NLP* Ã¨Â¿Â›NLPÃ¤ÂºÂ¤Ã¦ÂµÂÃ§Â¾Â¤Ã£Â€Â‚ [docs/wechat.jpeg] ## Citation
 Ã¥Â¦Â‚Ã¦ÂÂœÃ¤Â½Â Ã¥ÂœÂ¨Ã§Â Â”Ã§Â©Â¶Ã¤Â¸Â­Ã¤Â½Â¿Ã§Â”Â¨Ã¤ÂºÂ†text2vecÃ¯Â¼ÂŒÃ¨Â¯Â·Ã¦ÂŒÂ‰Ã¥Â¦Â‚Ã¤Â¸Â‹Ã¦Â Â¼Ã¥Â¼ÂÃ¥Â¼Â•Ã§Â”Â¨Ã¯Â¼Âš APA:
 ```latex Xu, M. Text2vec: Text to vector toolkit (Version 1.1.2) [Computer
 software]. https://github.com/shibing624/text2vec ``` BibTeX: ```latex @misc
-{Text2vec, author = {Xu, Ming}, title = {Text2vec: Text to vector toolkit},
-year = {2022}, publisher = {GitHub}, journal = {GitHub repository},
-howpublished = {\url{https://github.com/shibing624/text2vec}}, } ``` # License
+{Text2vec, author = {Ming Xu}, title = {Text2vec: Text to vector toolkit}, year
+= {2023}, publisher = {GitHub}, journal = {GitHub repository}, howpublished =
+{\url{https://github.com/shibing624/text2vec}}, } ``` ## License
 Ã¦ÂÂˆÃ¦ÂÂƒÃ¥ÂÂÃ¨Â®Â®Ã¤Â¸Âº [The Apache License 2.0]
 (LICENSE)Ã¯Â¼ÂŒÃ¥ÂÂ¯Ã¥Â…ÂÃ¨Â´Â¹Ã§Â”Â¨Ã¥ÂÂšÃ¥Â•Â†Ã¤Â¸ÂšÃ§Â”Â¨Ã©Â€Â”Ã£Â€Â‚Ã¨Â¯Â·Ã¥ÂœÂ¨Ã¤ÂºÂ§Ã¥Â“ÂÃ¨Â¯Â´Ã¦Â˜ÂÃ¤Â¸Â­Ã©Â™Â„Ã¥ÂŠÂ text2vecÃ§ÂšÂ„Ã©Â“Â¾Ã¦ÂÂ¥Ã¥Â’ÂŒÃ¦ÂÂˆÃ¦ÂÂƒÃ¥ÂÂÃ¨Â®Â®Ã£Â€Â‚
-# Contribute
+## Contribute
 Ã©Â¡Â¹Ã§Â›Â®Ã¤Â»Â£Ã§Â ÂÃ¨Â¿Â˜Ã¥Â¾ÂˆÃ§Â²Â—Ã§Â³Â™Ã¯Â¼ÂŒÃ¥Â¦Â‚Ã¦ÂÂœÃ¥Â¤Â§Ã¥Â®Â¶Ã¥Â¯Â¹Ã¤Â»Â£Ã§Â ÂÃ¦ÂœÂ‰Ã¦Â‰Â€Ã¦Â”Â¹Ã¨Â¿Â›Ã¯Â¼ÂŒÃ¦Â¬Â¢Ã¨Â¿ÂÃ¦ÂÂÃ¤ÂºÂ¤Ã¥Â›ÂÃ¦ÂœÂ¬Ã©Â¡Â¹Ã§Â›Â®Ã¯Â¼ÂŒÃ¥ÂœÂ¨Ã¦ÂÂÃ¤ÂºÂ¤Ã¤Â¹Â‹Ã¥Â‰ÂÃ¯Â¼ÂŒÃ¦Â³Â¨Ã¦Â„ÂÃ¤Â»Â¥Ã¤Â¸Â‹Ã¤Â¸Â¤Ã§Â‚Â¹Ã¯Â¼Âš
 - Ã¥ÂœÂ¨`tests`Ã¦Â·Â»Ã¥ÂŠÂ Ã§Â›Â¸Ã¥ÂºÂ”Ã§ÂšÂ„Ã¥ÂÂ•Ã¥Â…ÂƒÃ¦ÂµÂ‹Ã¨Â¯Â• - Ã¤Â½Â¿Ã§Â”Â¨`python -m pytest -
 v`Ã¦ÂÂ¥Ã¨Â¿ÂÃ¨Â¡ÂŒÃ¦Â‰Â€Ã¦ÂœÂ‰Ã¥ÂÂ•Ã¥Â…ÂƒÃ¦ÂµÂ‹Ã¨Â¯Â•Ã¯Â¼ÂŒÃ§Â¡Â®Ã¤Â¿ÂÃ¦Â‰Â€Ã¦ÂœÂ‰Ã¥ÂÂ•Ã¦ÂµÂ‹Ã©ÂƒÂ½Ã¦Â˜Â¯Ã©Â€ÂšÃ¨Â¿Â‡Ã§ÂšÂ„
-Ã¤Â¹Â‹Ã¥ÂÂÃ¥ÂÂ³Ã¥ÂÂ¯Ã¦ÂÂÃ¤ÂºÂ¤PRÃ£Â€Â‚ # Reference -
+Ã¤Â¹Â‹Ã¥ÂÂÃ¥ÂÂ³Ã¥ÂÂ¯Ã¦ÂÂÃ¤ÂºÂ¤PRÃ£Â€Â‚ ## References -
 [Ã¥Â°Â†Ã¥ÂÂ¥Ã¥Â­ÂÃ¨Â¡Â¨Ã§Â¤ÂºÃ¤Â¸ÂºÃ¥ÂÂ‘Ã©Â‡ÂÃ¯Â¼ÂˆÃ¤Â¸ÂŠÃ¯Â¼Â‰Ã¯Â¼ÂšÃ¦Â—Â Ã§Â›Â‘Ã§ÂÂ£Ã¥ÂÂ¥Ã¥Â­ÂÃ¨Â¡Â¨Ã§Â¤ÂºÃ¥Â­Â¦Ã¤Â¹Â Ã¯Â¼Âˆsentence
 embeddingÃ¯Â¼Â‰](https://www.cnblogs.com/llhthinker/p/10335164.html) -
 [Ã¥Â°Â†Ã¥ÂÂ¥Ã¥Â­ÂÃ¨Â¡Â¨Ã§Â¤ÂºÃ¤Â¸ÂºÃ¥ÂÂ‘Ã©Â‡ÂÃ¯Â¼ÂˆÃ¤Â¸Â‹Ã¯Â¼Â‰Ã¯Â¼ÂšÃ¦Â—Â Ã§Â›Â‘Ã§ÂÂ£Ã¥ÂÂ¥Ã¥Â­ÂÃ¨Â¡Â¨Ã§Â¤ÂºÃ¥Â­Â¦Ã¤Â¹Â Ã¯Â¼Âˆsentence
 embeddingÃ¯Â¼Â‰](https://www.cnblogs.com/llhthinker/p/10341841.html) - [A Simple
 but Tough-to-Beat Baseline for Sentence Embeddings[Sanjeev Arora and Yingyu
 Liang and Tengyu Ma, 2017]](https://openreview.net/forum?id=SyK00v5xx) -
 [Ã¥Â›Â›Ã§Â§ÂÃ¨Â®Â¡Ã§Â®Â—Ã¦Â–Â‡Ã¦ÂœÂ¬Ã§Â›Â¸Ã¤Â¼Â¼Ã¥ÂºÂ¦Ã§ÂšÂ„Ã¦Â–Â¹Ã¦Â³Â•Ã¥Â¯Â¹Ã¦Â¯Â”[Yves Peirsman]](https://
```

### Comparing `text2vec-1.2.1/README.md` & `text2vec-1.2.2/README.md`

 * *Files 2% similar despite different names*

```diff
@@ -19,54 +19,57 @@
 
 
 **Text2vec**: Text to Vector, Get Sentence Embeddings. æ–‡æœ¬å‘é‡åŒ–ï¼ŒæŠŠæ–‡æœ¬(åŒ…æ‹¬è¯ã€å¥å­ã€æ®µè½)è¡¨å¾ä¸ºå‘é‡çŸ©é˜µã€‚
 
 **text2vec**å®ç°äº†Word2Vecã€RankBM25ã€BERTã€Sentence-BERTã€CoSENTç­‰å¤šç§æ–‡æœ¬è¡¨å¾ã€æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—æ¨¡å‹ï¼Œå¹¶åœ¨æ–‡æœ¬è¯­ä¹‰åŒ¹é…ï¼ˆç›¸ä¼¼åº¦è®¡ç®—ï¼‰ä»»åŠ¡ä¸Šæ¯”è¾ƒäº†å„æ¨¡å‹çš„æ•ˆæœã€‚
 
 ### News
+[2023/06/22] v1.2.2ç‰ˆæœ¬: å‘å¸ƒäº†å¤šè¯­è¨€åŒ¹é…æ¨¡å‹[shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)ï¼Œç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`ç”¨äººå·¥æŒ‘é€‰åçš„å¤šè¯­è¨€STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-multilingual-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-multilingual-dataset)è®­ç»ƒå¾—åˆ°ï¼Œå¹¶åœ¨ä¸­è‹±æ–‡æµ‹è¯•é›†è¯„ä¼°ç›¸å¯¹äºåŸæ¨¡å‹æ•ˆæœæœ‰æå‡ï¼Œè¯¦è§[Release-v1.2.2](https://github.com/shibing624/text2vec/releases/tag/1.2.2)
+
 [2023/06/19] v1.2.1ç‰ˆæœ¬: æ›´æ–°äº†ä¸­æ–‡åŒ¹é…æ¨¡å‹`shibing624/text2vec-base-chinese-nli`ä¸ºæ–°ç‰ˆ[shibing624/text2vec-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-sentence)ï¼Œé’ˆå¯¹CoSENTçš„lossè®¡ç®—å¯¹æ’åºæ•æ„Ÿç‰¹ç‚¹ï¼Œäººå·¥æŒ‘é€‰å¹¶æ•´ç†å‡ºé«˜è´¨é‡çš„æœ‰ç›¸å…³æ€§æ’åºçš„STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)ï¼Œåœ¨å„è¯„ä¼°é›†è¡¨ç°ç›¸å¯¹ä¹‹å‰æœ‰æå‡ï¼›å‘å¸ƒäº†é€‚ç”¨äºs2pçš„ä¸­æ–‡åŒ¹é…æ¨¡å‹[shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase)ï¼Œè¯¦è§[Release-v1.2.1](https://github.com/shibing624/text2vec/releases/tag/1.2.1)
 
 [2023/06/15] v1.2.0ç‰ˆæœ¬: å‘å¸ƒäº†ä¸­æ–‡åŒ¹é…æ¨¡å‹[shibing624/text2vec-base-chinese-nli](https://huggingface.co/shibing624/text2vec-base-chinese-nli)ï¼ŒåŸºäº`nghuyong/ernie-3.0-base-zh`æ¨¡å‹ï¼Œä½¿ç”¨äº†ä¸­æ–‡NLIæ•°æ®é›†[shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh)å…¨éƒ¨è¯­æ–™è®­ç»ƒçš„CoSENTæ–‡æœ¬åŒ¹é…æ¨¡å‹ï¼Œåœ¨å„è¯„ä¼°é›†è¡¨ç°æå‡æ˜æ˜¾ï¼Œè¯¦è§[Release-v1.2.0](https://github.com/shibing624/text2vec/releases/tag/1.2.0)
 
 [2022/03/12] v1.1.4ç‰ˆæœ¬: å‘å¸ƒäº†ä¸­æ–‡åŒ¹é…æ¨¡å‹[shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)ï¼ŒåŸºäºä¸­æ–‡STSè®­ç»ƒé›†è®­ç»ƒçš„CoSENTåŒ¹é…æ¨¡å‹ã€‚è¯¦è§[Release-v1.1.4](https://github.com/shibing624/text2vec/releases/tag/1.1.4)
 
 
 **Guide**
-- [Feature](#Feature)
+- [Features](#Features)
 - [Evaluation](#Evaluation)
 - [Install](#install)
 - [Usage](#usage)
 - [Contact](#Contact)
-- [Reference](#reference)
+- [References](#references)
 
 
-# Feature
+## Features
 ### æ–‡æœ¬å‘é‡è¡¨ç¤ºæ¨¡å‹
 - [Word2Vec](https://github.com/shibing624/text2vec/blob/master/text2vec/word2vec.py)ï¼šé€šè¿‡è…¾è®¯AI Labå¼€æºçš„å¤§è§„æ¨¡é«˜è´¨é‡ä¸­æ–‡[è¯å‘é‡æ•°æ®ï¼ˆ800ä¸‡ä¸­æ–‡è¯è½»é‡ç‰ˆï¼‰](https://pan.baidu.com/s/1La4U4XNFe8s5BJqxPQpeiQ) (æ–‡ä»¶åï¼šlight_Tencent_AILab_ChineseEmbedding.bin å¯†ç : taweï¼‰å®ç°è¯å‘é‡æ£€ç´¢ï¼Œæœ¬é¡¹ç›®å®ç°äº†å¥å­ï¼ˆè¯å‘é‡æ±‚å¹³å‡ï¼‰çš„word2vecå‘é‡è¡¨ç¤º
 - [SBERT(Sentence-BERT)](https://github.com/shibing624/text2vec/blob/master/text2vec/sentencebert_model.py)ï¼šæƒè¡¡æ€§èƒ½å’Œæ•ˆç‡çš„å¥å‘é‡è¡¨ç¤ºæ¨¡å‹ï¼Œè®­ç»ƒæ—¶é€šè¿‡æœ‰ç›‘ç£è®­ç»ƒä¸Šå±‚åˆ†ç±»å‡½æ•°ï¼Œæ–‡æœ¬åŒ¹é…é¢„æµ‹æ—¶ç›´æ¥å¥å­å‘é‡åšä½™å¼¦ï¼Œæœ¬é¡¹ç›®åŸºäºPyTorchå¤ç°äº†Sentence-BERTæ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹
 - [CoSENT(Cosine Sentence)](https://github.com/shibing624/text2vec/blob/master/text2vec/cosent_model.py)ï¼šCoSENTæ¨¡å‹æå‡ºäº†ä¸€ç§æ’åºçš„æŸå¤±å‡½æ•°ï¼Œä½¿è®­ç»ƒè¿‡ç¨‹æ›´è´´è¿‘é¢„æµ‹ï¼Œæ¨¡å‹æ”¶æ•›é€Ÿåº¦å’Œæ•ˆæœæ¯”Sentence-BERTæ›´å¥½ï¼Œæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†CoSENTæ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹
 
 è¯¦ç»†æ–‡æœ¬å‘é‡è¡¨ç¤ºæ–¹æ³•è§wiki: [æ–‡æœ¬å‘é‡è¡¨ç¤ºæ–¹æ³•](https://github.com/shibing624/text2vec/wiki/%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95)
-# Evaluation
+## Evaluation
 
 æ–‡æœ¬åŒ¹é…
 
 #### è‹±æ–‡åŒ¹é…æ•°æ®é›†çš„è¯„æµ‹ç»“æœï¼š
 
 
-| Arch   | BaseModel                                        | Model                            | English-STS-B | 
-|:-------|:------------------------------------------------|:-------------------------------------|:-------------:|
-| GloVe  | glove                                           | Avg_word_embeddings_glove_6B_300d    |     61.77     |
-| BERT   | bert-base-uncased                               | BERT-base-cls                        |     20.29     |
-| BERT   | bert-base-uncased                               | BERT-base-first_last_avg             |     59.04     |
-| BERT   | bert-base-uncased                               | BERT-base-first_last_avg-whiten(NLI) |     63.65     |
-| SBERT  | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-cls                   |     73.65     |
-| SBERT  | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-first_last_avg        |     77.96     |
-| CoSENT | bert-base-uncased                               | CoSENT-base-first_last_avg           |     69.93     |
-| CoSENT | sentence-transformers/bert-base-nli-mean-tokens | CoSENT-base-nli-first_last_avg       |     79.68     |
+| Arch   | BaseModel                                        | Model                                                                                                                | English-STS-B | 
+|:-------|:------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------|:-------------:|
+| GloVe  | glove                                           | Avg_word_embeddings_glove_6B_300d                                                                                    |     61.77     |
+| BERT   | bert-base-uncased                               | BERT-base-cls                                                                                                        |     20.29     |
+| BERT   | bert-base-uncased                               | BERT-base-first_last_avg                                                                                             |     59.04     |
+| BERT   | bert-base-uncased                               | BERT-base-first_last_avg-whiten(NLI)                                                                                 |     63.65     |
+| SBERT  | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-cls                                                                                                   |     73.65     |
+| SBERT  | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-first_last_avg                                                                                        |     77.96     |
+| CoSENT | bert-base-uncased                               | CoSENT-base-first_last_avg                                                                                           |     69.93     |
+| CoSENT | sentence-transformers/bert-base-nli-mean-tokens | CoSENT-base-nli-first_last_avg                                                                                       |     79.68     |
+| CoSENT | sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 | [shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)                |     80.12     |
 
 #### ä¸­æ–‡åŒ¹é…æ•°æ®é›†çš„è¯„æµ‹ç»“æœï¼š
 
 
 | Arch   | BaseModel                    | Model           | ATEC  |  BQ   | LCQMC | PAWSX | STS-B |  Avg  | 
 |:-------|:----------------------------|:--------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|
 | SBERT  | bert-base-chinese           | SBERT-bert-base     | 46.36 | 70.36 | 78.72 | 46.86 | 66.41 | 61.74 |
@@ -75,59 +78,60 @@
 | CoSENT | bert-base-chinese           | CoSENT-bert-base    | 49.74 | 72.38 | 78.69 | 60.00 | 79.27 | 68.01 |
 | CoSENT | hfl/chinese-macbert-base    | CoSENT-macbert-base | 50.39 | 72.93 | 79.17 | 60.86 | 79.30 | 68.53 |
 | CoSENT | hfl/chinese-roberta-wwm-ext | CoSENT-roberta-ext  | 50.81 | 71.45 | 79.31 | 61.56 | 79.96 | 68.61 |
 
 è¯´æ˜ï¼š
 - ç»“æœè¯„æµ‹æŒ‡æ ‡ï¼šspearmanç³»æ•°
 - ä¸ºè¯„æµ‹æ¨¡å‹èƒ½åŠ›ï¼Œç»“æœå‡åªç”¨è¯¥æ•°æ®é›†çš„trainè®­ç»ƒï¼Œåœ¨testä¸Šè¯„ä¼°å¾—åˆ°çš„è¡¨ç°ï¼Œæ²¡ç”¨å¤–éƒ¨æ•°æ®
+- `SBERT-macbert-base`æ¨¡å‹ï¼Œæ˜¯ç”¨SBertæ–¹æ³•è®­ç»ƒï¼Œè¿è¡Œ[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹
+- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`æ¨¡å‹æ˜¯ç”¨SBertè®­ç»ƒï¼Œæ˜¯`paraphrase-MiniLM-L12-v2`æ¨¡å‹çš„å¤šè¯­è¨€ç‰ˆæœ¬ï¼Œæ”¯æŒä¸­æ–‡ã€è‹±æ–‡ç­‰
 
 
 ### Release Models
 - æœ¬é¡¹ç›®releaseæ¨¡å‹çš„ä¸­æ–‡åŒ¹é…è¯„æµ‹ç»“æœï¼š
 
-| Arch       | BaseModel                         | Model                                                                                                                                             | ATEC  |  BQ   | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-dc |    Avg    |  QPS  |
-|:-----------|:----------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-------:|:-------:|:---------:|:-----:|
-| Word2Vec   | word2vec                          | [w2v-light-tencent-chinese](https://ai.tencent.com/ailab/nlp/en/download.html)                                                                    | 20.00 | 31.49 | 59.46 | 2.57  | 55.78 |  55.04  |  20.70  |   35.03   | 23769 |
-| SBERT      | xlm-roberta-base                  | [sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) | 18.42 | 38.52 | 63.96 | 10.14 | 78.90 |  63.01  |  52.28  |   46.46   | 3138  |
-| Instructor | hfl/chinese-roberta-wwm-ext       | [moka-ai/m3e-base](https://huggingface.co/moka-ai/m3e-base)                                                                                       | 41.27 | 63.81 | 74.87 | 12.20 | 76.96 |  75.83  |  60.55  |   57.93   | 2980  |
-| CoSENT     | hfl/chinese-macbert-base          | [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)                                                       | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 |  70.27  |  50.42  |   51.61   | 3008  |
-| CoSENT     | hfl/chinese-lert-large            | [GanymedeNil/text2vec-large-chinese](https://huggingface.co/GanymedeNil/text2vec-large-chinese)                                                   | 32.61 | 44.59 | 69.30 | 14.51 | 79.44 |  73.01  |  59.04  |   53.12   | 2092  |
-| CoSENT     | nghuyong/ernie-3.0-base-zh        | [shibing624/text2vec-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-sentence)                                     | 43.37 | 61.43 | 73.48 | 38.90 | 78.25 |  70.60  |  53.08  |   59.87   | 3089  |
-| CoSENT     | nghuyong/ernie-3.0-base-zh        | [shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase)                                 | 44.89 | 63.58 | 74.24 | 40.90 | 78.93 |  76.70  |  63.30  | **63.08** | 3066  |
+| Arch       | BaseModel                                                    | Model                                                                                                                                             | ATEC  |  BQ   | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-dc |    Avg    |  QPS  |
+|:-----------|:-------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-------:|:-------:|:---------:|:-----:|
+| Word2Vec   | word2vec                                                     | [w2v-light-tencent-chinese](https://ai.tencent.com/ailab/nlp/en/download.html)                                                                    | 20.00 | 31.49 | 59.46 | 2.57  | 55.78 |  55.04  |  20.70  |   35.03   | 23769 |
+| SBERT      | xlm-roberta-base                                             | [sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) | 18.42 | 38.52 | 63.96 | 10.14 | 78.90 |  63.01  |  52.28  |   46.46   | 3138  |
+| CoSENT     | hfl/chinese-macbert-base                                     | [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)                                                       | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 |  70.27  |  50.42  |   51.61   | 3008  |
+| CoSENT     | hfl/chinese-lert-large                                       | [GanymedeNil/text2vec-large-chinese](https://huggingface.co/GanymedeNil/text2vec-large-chinese)                                                   | 32.61 | 44.59 | 69.30 | 14.51 | 79.44 |  73.01  |  59.04  |   53.12   | 2092  |
+| CoSENT     | nghuyong/ernie-3.0-base-zh                                   | [shibing624/text2vec-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-sentence)                                     | 43.37 | 61.43 | 73.48 | 38.90 | 78.25 |  70.60  |  53.08  |   59.87   | 3089  |
+| CoSENT     | nghuyong/ernie-3.0-base-zh                                   | [shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase)                                 | 44.89 | 63.58 | 74.24 | 40.90 | 78.93 |  76.70  |  63.30  | **63.08** | 3066  |
+| CoSENT     | sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2  | [shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)                                             | 32.39 | 50.33 | 65.64 | 32.56 | 74.45 |  68.88  |  51.17  |   53.67   | 3138  |
 
 
 è¯´æ˜ï¼š
 - ç»“æœè¯„æµ‹æŒ‡æ ‡ï¼šspearmanç³»æ•°
-- æ¨¡å‹è®­ç»ƒå®éªŒæŠ¥å‘Šï¼š[å®éªŒæŠ¥å‘Š](https://github.com/shibing624/text2vec/blob/master/docs/model_report.md)
 - `shibing624/text2vec-base-chinese`æ¨¡å‹ï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`hfl/chinese-macbert-base`åœ¨ä¸­æ–‡STS-Bæ•°æ®è®­ç»ƒå¾—åˆ°ï¼Œå¹¶åœ¨ä¸­æ–‡STS-Bæµ‹è¯•é›†è¯„ä¼°è¾¾åˆ°è¾ƒå¥½æ•ˆæœï¼Œè¿è¡Œ[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹æ–‡ä»¶å·²ç»ä¸Šä¼ HF model hubï¼Œä¸­æ–‡é€šç”¨è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èä½¿ç”¨
 - `shibing624/text2vec-base-chinese-sentence`æ¨¡å‹ï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`nghuyong/ernie-3.0-base-zh`ç”¨äººå·¥æŒ‘é€‰åçš„ä¸­æ–‡STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)è®­ç»ƒå¾—åˆ°ï¼Œå¹¶åœ¨ä¸­æ–‡å„NLIæµ‹è¯•é›†è¯„ä¼°è¾¾åˆ°è¾ƒå¥½æ•ˆæœï¼Œè¿è¡Œ[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹æ–‡ä»¶å·²ç»ä¸Šä¼ HF model hubï¼Œä¸­æ–‡s2s(å¥å­vså¥å­)è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èä½¿ç”¨
 - `shibing624/text2vec-base-chinese-paraphrase`æ¨¡å‹ï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`nghuyong/ernie-3.0-base-zh`ç”¨äººå·¥æŒ‘é€‰åçš„ä¸­æ–‡STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-chinese-paraphrase-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-paraphrase-dataset)ï¼Œæ•°æ®é›†ç›¸å¯¹äº[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)åŠ å…¥äº†s2p(sentence to paraphrase)æ•°æ®ï¼Œå¼ºåŒ–äº†å…¶é•¿æ–‡æœ¬çš„è¡¨å¾èƒ½åŠ›ï¼Œå¹¶åœ¨ä¸­æ–‡å„NLIæµ‹è¯•é›†è¯„ä¼°è¾¾åˆ°SOTAï¼Œè¿è¡Œ[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹æ–‡ä»¶å·²ç»ä¸Šä¼ HF model hubï¼Œä¸­æ–‡s2p(å¥å­vsæ®µè½)è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èä½¿ç”¨
-- `SBERT-macbert-base`æ¨¡å‹ï¼Œæ˜¯ç”¨SBERTæ–¹æ³•è®­ç»ƒï¼Œè¿è¡Œ[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹
-- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`æ¨¡å‹æ˜¯ç”¨SBERTè®­ç»ƒï¼Œæ˜¯`paraphrase-MiniLM-L12-v2`æ¨¡å‹çš„å¤šè¯­è¨€ç‰ˆæœ¬ï¼Œæ”¯æŒä¸­æ–‡ã€è‹±æ–‡ç­‰
+- `shibing624/text2vec-base-multilingual`æ¨¡å‹ï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`ç”¨äººå·¥æŒ‘é€‰åçš„å¤šè¯­è¨€STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-multilingual-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-multilingual-dataset)è®­ç»ƒå¾—åˆ°ï¼Œå¹¶åœ¨ä¸­è‹±æ–‡æµ‹è¯•é›†è¯„ä¼°ç›¸å¯¹äºåŸæ¨¡å‹æ•ˆæœæœ‰æå‡ï¼Œè¿è¡Œ[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹æ–‡ä»¶å·²ç»ä¸Šä¼ HF model hubï¼Œå¤šè¯­è¨€è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èä½¿ç”¨
 - `w2v-light-tencent-chinese`æ˜¯è…¾è®¯è¯å‘é‡çš„Word2Vecæ¨¡å‹ï¼ŒCPUåŠ è½½ä½¿ç”¨ï¼Œé€‚ç”¨äºä¸­æ–‡å­—é¢åŒ¹é…ä»»åŠ¡å’Œç¼ºå°‘æ•°æ®çš„å†·å¯åŠ¨æƒ…å†µ
 - å„é¢„è®­ç»ƒæ¨¡å‹å‡å¯ä»¥é€šè¿‡transformersè°ƒç”¨ï¼Œå¦‚MacBERTæ¨¡å‹ï¼š`--model_name hfl/chinese-macbert-base` æˆ–è€…robertaæ¨¡å‹ï¼š`--model_name uer/roberta-medium-wwm-chinese-cluecorpussmall`
 - ä¸ºæµ‹è¯„æ¨¡å‹çš„é²æ£’æ€§ï¼ŒåŠ å…¥äº†æœªè®­ç»ƒè¿‡çš„SOHUæµ‹è¯•é›†ï¼Œç”¨äºæµ‹è¯•æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼›ä¸ºè¾¾åˆ°å¼€ç®±å³ç”¨çš„å®ç”¨æ•ˆæœï¼Œä½¿ç”¨äº†æœé›†åˆ°çš„å„ä¸­æ–‡åŒ¹é…æ•°æ®é›†ï¼Œæ•°æ®é›†ä¹Ÿä¸Šä¼ åˆ°HF datasets[é“¾æ¥è§ä¸‹æ–¹](#æ•°æ®é›†)
 - ä¸­æ–‡åŒ¹é…ä»»åŠ¡å®éªŒè¡¨æ˜ï¼Œpoolingæœ€ä¼˜æ˜¯`EncoderType.FIRST_LAST_AVG`å’Œ`EncoderType.MEAN`ï¼Œä¸¤è€…é¢„æµ‹æ•ˆæœå·®å¼‚å¾ˆå°
 - ä¸­æ–‡åŒ¹é…è¯„æµ‹ç»“æœå¤ç°ï¼Œå¯ä»¥ä¸‹è½½ä¸­æ–‡åŒ¹é…æ•°æ®é›†åˆ°`examples/data`ï¼Œè¿è¡Œ[tests/test_model_spearman.py](https://github.com/shibing624/text2vec/blob/master/tests/test_model_spearman.py)ä»£ç å¤ç°è¯„æµ‹ç»“æœ
 - QPSçš„GPUæµ‹è¯•ç¯å¢ƒæ˜¯Tesla V100ï¼Œæ˜¾å­˜32GB
 
-# Demo
+æ¨¡å‹è®­ç»ƒå®éªŒæŠ¥å‘Šï¼š[å®éªŒæŠ¥å‘Š](https://github.com/shibing624/text2vec/blob/master/docs/model_report.md)
+## Demo
 
 Official Demo: https://www.mulanai.com/product/short_text_sim/
 
 HuggingFace Demo: https://huggingface.co/spaces/shibing624/text2vec
 
 ![](docs/hf.png)
 
 run example: [examples/gradio_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/gradio_demo.py) to see the demo:
 ```shell
 python examples/gradio_demo.py
 ```
 
-# Install
+## Install
 ```shell
 pip install torch # conda install pytorch
 pip install -U text2vec
 ```
 
 or
 
@@ -136,17 +140,17 @@
 pip install -r requirements.txt
 
 git clone https://github.com/shibing624/text2vec.git
 cd text2vec
 pip install --no-deps .
 ```
 
-# Usage
+## Usage
 
-## æ–‡æœ¬å‘é‡è¡¨å¾
+### æ–‡æœ¬å‘é‡è¡¨å¾
 
 åŸºäº`pretrained model`è®¡ç®—æ–‡æœ¬å‘é‡ï¼š
 
 ```zsh
 >>> from text2vec import SentenceModel
 >>> m = SentenceModel()
 >>> m.encode("å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡")
@@ -186,16 +190,16 @@
 
 
 if __name__ == "__main__":
     # ä¸­æ–‡å¥å‘é‡æ¨¡å‹(CoSENT)ï¼Œä¸­æ–‡è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èï¼Œæ”¯æŒfine-tuneç»§ç»­è®­ç»ƒ
     t2v_model = SentenceModel("shibing624/text2vec-base-chinese")
     compute_emb(t2v_model)
 
-    # æ”¯æŒå¤šè¯­è¨€çš„å¥å‘é‡æ¨¡å‹ï¼ˆSentence-BERTï¼‰ï¼Œè‹±æ–‡è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èï¼Œæ”¯æŒfine-tuneç»§ç»­è®­ç»ƒ
-    sbert_model = SentenceModel("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
+    # æ”¯æŒå¤šè¯­è¨€çš„å¥å‘é‡æ¨¡å‹ï¼ˆCoSENTï¼‰ï¼Œå¤šè¯­è¨€ï¼ˆåŒ…æ‹¬ä¸­è‹±æ–‡ï¼‰è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èï¼Œæ”¯æŒfine-tuneç»§ç»­è®­ç»ƒ
+    sbert_model = SentenceModel("shibing624/text2vec-base-multilingual")
     compute_emb(sbert_model)
 
     # ä¸­æ–‡è¯å‘é‡æ¨¡å‹(word2vec)ï¼Œä¸­æ–‡å­—é¢åŒ¹é…ä»»åŠ¡å’Œå†·å¯åŠ¨é€‚ç”¨
     w2v_model = Word2Vec("w2v-light-tencent-chinese")
     compute_emb(w2v_model)
 
 ```
@@ -212,16 +216,14 @@
 ```
 
 - è¿”å›å€¼`embeddings`æ˜¯`numpy.ndarray`ç±»å‹ï¼Œshapeä¸º`(sentences_size, model_embedding_size)`ï¼Œä¸‰ä¸ªæ¨¡å‹ä»»é€‰ä¸€ç§å³å¯ï¼Œæ¨èç”¨ç¬¬ä¸€ä¸ªã€‚
 - `shibing624/text2vec-base-chinese`æ¨¡å‹æ˜¯CoSENTæ–¹æ³•åœ¨ä¸­æ–‡STS-Bæ•°æ®é›†è®­ç»ƒå¾—åˆ°çš„ï¼Œæ¨¡å‹å·²ç»ä¸Šä¼ åˆ°huggingfaceçš„
 æ¨¡å‹åº“[shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)ï¼Œ
 æ˜¯`text2vec.SentenceModel`æŒ‡å®šçš„é»˜è®¤æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡ä¸Šé¢ç¤ºä¾‹è°ƒç”¨ï¼Œæˆ–è€…å¦‚ä¸‹æ‰€ç¤ºç”¨[transformersåº“](https://github.com/huggingface/transformers)è°ƒç”¨ï¼Œ
 æ¨¡å‹è‡ªåŠ¨ä¸‹è½½åˆ°æœ¬æœºè·¯å¾„ï¼š`~/.cache/huggingface/transformers`
-- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`æ¨¡å‹æ˜¯Sentence-BERTçš„å¤šè¯­è¨€å¥å‘é‡æ¨¡å‹ï¼Œ
-é€‚ç”¨äºé‡Šä¹‰ï¼ˆparaphraseï¼‰è¯†åˆ«ï¼Œæ–‡æœ¬åŒ¹é…ï¼Œé€šè¿‡`text2vec.SentenceModel`å’Œ[sentence-transformersåº“]((https://github.com/UKPLab/sentence-transformers))éƒ½å¯ä»¥è°ƒç”¨è¯¥æ¨¡å‹
 - `w2v-light-tencent-chinese`æ˜¯é€šè¿‡gensimåŠ è½½çš„Word2Vecæ¨¡å‹ï¼Œä½¿ç”¨è…¾è®¯è¯å‘é‡`Tencent_AILab_ChineseEmbedding.tar.gz`è®¡ç®—å„å­—è¯çš„è¯å‘é‡ï¼Œå¥å­å‘é‡é€šè¿‡å•è¯è¯
 å‘é‡å–å¹³å‡å€¼å¾—åˆ°ï¼Œæ¨¡å‹è‡ªåŠ¨ä¸‹è½½åˆ°æœ¬æœºè·¯å¾„ï¼š`~/.text2vec/datasets/light_Tencent_AILab_ChineseEmbedding.bin`
 
 #### Usage (HuggingFace Transformers)
 Without [text2vec](https://github.com/shibing624/text2vec), you can use the model like this: 
 
 First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.
@@ -439,17 +441,17 @@
 from similarities import Similarity
 
 m = Similarity()
 r = m.similarity('å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡', 'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡')
 print(f"similarity score: {float(r)}")  # similarity score: 0.855146050453186
 ```
 
-# Models
+## Models
 
-## CoSENT model
+### CoSENT model
 
 CoSENTï¼ˆCosine Sentenceï¼‰æ–‡æœ¬åŒ¹é…æ¨¡å‹ï¼Œåœ¨Sentence-BERTä¸Šæ”¹è¿›äº†CosineRankLossçš„å¥å‘é‡æ–¹æ¡ˆ
 
 
 Network structure:
 
 Training:
@@ -480,16 +482,22 @@
 python training_sup_text_matching_model.py --task_name ATEC --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base --output_dir ./outputs/ATEC-cosent
 ```
 
 - åœ¨è‡ªæœ‰ä¸­æ–‡æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹
 
 example: [examples/training_sup_text_matching_model_mydata.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_mydata.py)
 
+å•å¡è®­ç»ƒï¼š
+```shell
+CUDA_VISIBLE_DEVICES=0 python training_sup_text_matching_model_mydata.py --do_train --do_predict
+```
+
+å¤šå¡è®­ç»ƒï¼š
 ```shell
-python training_sup_text_matching_model_mydata.py --do_train --do_predict
+CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node 2  training_sup_text_matching_model_mydata.py --do_train --do_predict --output_dir outputs/STS-B-text2vec-macbert-v1 --batch_size 64 --fp16 --data_parallel 
 ```
 
 è®­ç»ƒé›†æ ¼å¼å‚è€ƒ[examples/data/STS-B/STS-B.valid.data](https://github.com/shibing624/text2vec/blob/master/examples/data/STS-B/STS-B.valid.data)
 
 ```shell
 sentence1   sentence2   label
 ä¸€ä¸ªå¥³å­©åœ¨ç»™å¥¹çš„å¤´å‘åšå‘å‹ã€‚	ä¸€ä¸ªå¥³å­©åœ¨æ¢³å¤´ã€‚	2
@@ -516,15 +524,15 @@
 
 ```shell
 cd examples
 python training_unsup_text_matching_model_en.py --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-cosent
 ```
 
 
-## Sentence-BERT model
+### Sentence-BERT model
 
 Sentence-BERTæ–‡æœ¬åŒ¹é…æ¨¡å‹ï¼Œè¡¨å¾å¼å¥å‘é‡è¡¨ç¤ºæ–¹æ¡ˆ
 
 Network structure:
 
 Training:
 
@@ -559,38 +567,38 @@
 example: [examples/training_unsup_text_matching_model_en.py](https://github.com/shibing624/text2vec/blob/master/examples/training_unsup_text_matching_model_en.py)
 
 ```shell
 cd examples
 python training_unsup_text_matching_model_en.py --model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-sbert
 ```
 
-## BERT-Match model
+### BERT-Match model
 BERTæ–‡æœ¬åŒ¹é…æ¨¡å‹ï¼ŒåŸç”ŸBERTåŒ¹é…ç½‘ç»œç»“æ„ï¼Œäº¤äº’å¼å¥å‘é‡åŒ¹é…æ¨¡å‹
 
 Network structure:
 
 Training and inference:
 
 <img src="docs/bert-fc-train.png" width="300" />
 
 è®­ç»ƒè„šæœ¬åŒä¸Š[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)ã€‚
 
 
-## æ¨¡å‹è’¸é¦ï¼ˆModel Distillationï¼‰
+### æ¨¡å‹è’¸é¦ï¼ˆModel Distillationï¼‰
 
 ç”±äºtext2vecè®­ç»ƒçš„æ¨¡å‹å¯ä»¥ä½¿ç”¨[sentence-transformers](https://github.com/UKPLab/sentence-transformers)åº“åŠ è½½ï¼Œæ­¤å¤„å¤ç”¨å…¶æ¨¡å‹è’¸é¦æ–¹æ³•[distillation](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/distillation)ã€‚
 
 1. æ¨¡å‹é™ç»´ï¼Œå‚è€ƒ[dimensionality_reduction.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/distillation/dimensionality_reduction.py)ä½¿ç”¨PCAå¯¹æ¨¡å‹è¾“å‡ºembeddingé™ç»´ï¼Œå¯å‡å°‘milvusç­‰å‘é‡æ£€ç´¢æ•°æ®åº“çš„å­˜å‚¨å‹åŠ›ï¼Œè¿˜èƒ½è½»å¾®æå‡æ¨¡å‹æ•ˆæœã€‚
 2. æ¨¡å‹è’¸é¦ï¼Œå‚è€ƒ[model_distillation.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/distillation/model_distillation.py)ä½¿ç”¨è’¸é¦æ–¹æ³•ï¼Œå°†Teacherå¤§æ¨¡å‹è’¸é¦åˆ°æ›´å°‘layerså±‚æ•°çš„studentæ¨¡å‹ä¸­ï¼Œåœ¨æƒè¡¡æ•ˆæœçš„æƒ…å†µä¸‹ï¼Œå¯å¤§å¹…æå‡æ¨¡å‹é¢„æµ‹é€Ÿåº¦ã€‚
 
-## æ¨¡å‹éƒ¨ç½²
+### æ¨¡å‹éƒ¨ç½²
 
 æä¾›ä¸¤ç§éƒ¨ç½²æ¨¡å‹ï¼Œæ­å»ºæœåŠ¡çš„æ–¹æ³•ï¼š 1ï¼‰åŸºäºJinaæ­å»ºgRPCæœåŠ¡ã€æ¨èã€‘ï¼›2ï¼‰åŸºäºFastAPIæ­å»ºåŸç”ŸHttpæœåŠ¡ã€‚
 
-### JinaæœåŠ¡
+#### JinaæœåŠ¡
 é‡‡ç”¨C/Sæ¨¡å¼æ­å»ºé«˜æ€§èƒ½æœåŠ¡ï¼Œæ”¯æŒdockeräº‘åŸç”Ÿï¼ŒgRPC/HTTP/WebSocketï¼Œæ”¯æŒå¤šä¸ªæ¨¡å‹åŒæ—¶é¢„æµ‹ï¼ŒGPUå¤šå¡å¤„ç†ã€‚
 
 - å®‰è£…ï¼š
 ```pip install jina```
 
 - å¯åŠ¨æœåŠ¡ï¼š
 
@@ -629,15 +637,15 @@
 r = c.post('/', inputs=DocumentArray([Document(text='å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡'), Document(text='èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡')]))
 print(r.embeddings)
 ```
 
 æ‰¹é‡è°ƒç”¨æ–¹æ³•è§example: [examples/jina_client_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/jina_client_demo.py)
 
 
-### FastAPIæœåŠ¡
+#### FastAPIæœåŠ¡
 
 - å®‰è£…ï¼š
 ```pip install fastapi uvicorn```
 
 - å¯åŠ¨æœåŠ¡ï¼š
 
 example: [examples/fastapi_server_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/fastapi_server_demo.py)
@@ -650,15 +658,15 @@
 ```shell
 curl -X 'GET' \
   'http://0.0.0.0:8001/emb?q=hello' \
   -H 'accept: application/json'
 ```
 
 
-## æ•°æ®é›†
+## Dataset
 
 - æœ¬é¡¹ç›®releaseçš„æ•°æ®é›†ï¼š
 
 | Dataset                    | Introduce                                                                | Download Link                                                                                                                                                                                                                                                                                         |
 |:---------------------------|:-------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | shibing624/nli-zh-all      | ä¸­æ–‡è¯­ä¹‰åŒ¹é…æ•°æ®åˆé›†ï¼Œæ•´åˆäº†æ–‡æœ¬æ¨ç†ï¼Œç›¸ä¼¼ï¼Œæ‘˜è¦ï¼Œé—®ç­”ï¼ŒæŒ‡ä»¤å¾®è°ƒç­‰ä»»åŠ¡çš„820ä¸‡é«˜è´¨é‡æ•°æ®ï¼Œå¹¶è½¬åŒ–ä¸ºåŒ¹é…æ ¼å¼æ•°æ®é›†                | [https://huggingface.co/datasets/shibing624/nli-zh-all](https://huggingface.co/datasets/shibing624/nli-zh-all)                                                                                                                                                                                        |
 | shibing624/snli-zh         | ä¸­æ–‡SNLIå’ŒMultiNLIæ•°æ®é›†ï¼Œç¿»è¯‘è‡ªè‹±æ–‡SNLIå’ŒMultiNLI                                    | [https://huggingface.co/datasets/shibing624/snli-zh](https://huggingface.co/datasets/shibing624/snli-zh)                                                                                                                                                                                              |
@@ -669,16 +677,16 @@
 | LCQMC                      | ä¸­æ–‡LCQMC(large-scale Chinese question matching corpus)æ•°æ®é›†ï¼ŒQ-Qpairæ•°æ®é›†      | [LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html)                                                                                                                                                                                                                                               |
 | PAWSX                      | ä¸­æ–‡PAWS(Paraphrase Adversaries from Word Scrambling)æ•°æ®é›†ï¼ŒQ-Qpairæ•°æ®é›†        | [PAWSX](https://arxiv.org/abs/1908.11828)                                                                                                                                                                                                                                                             |
 | STS-B                      | ä¸­æ–‡STS-Bæ•°æ®é›†ï¼Œä¸­æ–‡è‡ªç„¶è¯­è¨€æ¨ç†æ•°æ®é›†ï¼Œä»è‹±æ–‡STS-Bç¿»è¯‘ä¸ºä¸­æ–‡çš„æ•°æ®é›†                                 | [STS-B](https://github.com/pluto-junzeng/CNSD)                                                                                                                                                                                                                                                        |
 
 
 å¸¸ç”¨è‹±æ–‡åŒ¹é…æ•°æ®é›†ï¼š
 
-- å¤§åé¼é¼çš„multi_nliå’Œsnli: https://huggingface.co/datasets/multi_nli
-- å¤§åé¼é¼çš„multi_nliå’Œsnli: https://huggingface.co/datasets/snli
+- è‹±æ–‡åŒ¹é…æ•°æ®é›†ï¼šmulti_nli: https://huggingface.co/datasets/multi_nli
+- è‹±æ–‡åŒ¹é…æ•°æ®é›†ï¼šsnli: https://huggingface.co/datasets/snli
 - https://huggingface.co/datasets/metaeval/cnli
 - https://huggingface.co/datasets/mteb/stsbenchmark-sts
 - https://huggingface.co/datasets/JeremiahZ/simcse_sup_nli
 - https://huggingface.co/datasets/MoritzLaurer/multilingual-NLI-26lang-2mil7
 
 
 æ•°æ®é›†ä½¿ç”¨ç¤ºä¾‹ï¼š
@@ -713,59 +721,59 @@
 {'sentence1': 'ä¸€ä¸ªå¥³å­©åœ¨ç»™å¥¹çš„å¤´å‘åšå‘å‹ã€‚', 'sentence2': 'ä¸€ä¸ªå¥³å­©åœ¨æ¢³å¤´ã€‚', 'label': 2}
 ```
 
 
 
 
 
-# Contact
+## Contact
 
 - Issue(å»ºè®®)ï¼š[![GitHub issues](https://img.shields.io/github/issues/shibing624/text2vec.svg)](https://github.com/shibing624/text2vec/issues)
 - é‚®ä»¶æˆ‘ï¼šxuming: xuming624@qq.com
 - å¾®ä¿¡æˆ‘ï¼šåŠ æˆ‘*å¾®ä¿¡å·ï¼šxuming624, å¤‡æ³¨ï¼šå§“å-å…¬å¸-NLP* è¿›NLPäº¤æµç¾¤ã€‚
 
 <img src="docs/wechat.jpeg" width="200" />
 
 
-# Citation
+## Citation
 
 å¦‚æœä½ åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†text2vecï¼Œè¯·æŒ‰å¦‚ä¸‹æ ¼å¼å¼•ç”¨ï¼š
 
 APA:
 ```latex
 Xu, M. Text2vec: Text to vector toolkit (Version 1.1.2) [Computer software]. https://github.com/shibing624/text2vec
 ```
 
 BibTeX:
 ```latex
 @misc{Text2vec,
-  author = {Xu, Ming},
+  author = {Ming Xu},
   title = {Text2vec: Text to vector toolkit},
-  year = {2022},
+  year = {2023},
   publisher = {GitHub},
   journal = {GitHub repository},
   howpublished = {\url{https://github.com/shibing624/text2vec}},
 }
 ```
 
-# License
+## License
 
 
 æˆæƒåè®®ä¸º [The Apache License 2.0](LICENSE)ï¼Œå¯å…è´¹ç”¨åšå•†ä¸šç”¨é€”ã€‚è¯·åœ¨äº§å“è¯´æ˜ä¸­é™„åŠ text2vecçš„é“¾æ¥å’Œæˆæƒåè®®ã€‚
 
 
-# Contribute
+## Contribute
 é¡¹ç›®ä»£ç è¿˜å¾ˆç²—ç³™ï¼Œå¦‚æœå¤§å®¶å¯¹ä»£ç æœ‰æ‰€æ”¹è¿›ï¼Œæ¬¢è¿æäº¤å›æœ¬é¡¹ç›®ï¼Œåœ¨æäº¤ä¹‹å‰ï¼Œæ³¨æ„ä»¥ä¸‹ä¸¤ç‚¹ï¼š
 
  - åœ¨`tests`æ·»åŠ ç›¸åº”çš„å•å…ƒæµ‹è¯•
  - ä½¿ç”¨`python -m pytest -v`æ¥è¿è¡Œæ‰€æœ‰å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿æ‰€æœ‰å•æµ‹éƒ½æ˜¯é€šè¿‡çš„
 
 ä¹‹åå³å¯æäº¤PRã€‚
 
-# Reference
+## References
 - [å°†å¥å­è¡¨ç¤ºä¸ºå‘é‡ï¼ˆä¸Šï¼‰ï¼šæ— ç›‘ç£å¥å­è¡¨ç¤ºå­¦ä¹ ï¼ˆsentence embeddingï¼‰](https://www.cnblogs.com/llhthinker/p/10335164.html)
 - [å°†å¥å­è¡¨ç¤ºä¸ºå‘é‡ï¼ˆä¸‹ï¼‰ï¼šæ— ç›‘ç£å¥å­è¡¨ç¤ºå­¦ä¹ ï¼ˆsentence embeddingï¼‰](https://www.cnblogs.com/llhthinker/p/10341841.html)
 - [A Simple but Tough-to-Beat Baseline for Sentence Embeddings[Sanjeev Arora and Yingyu Liang and Tengyu Ma, 2017]](https://openreview.net/forum?id=SyK00v5xx)
 - [å››ç§è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦çš„æ–¹æ³•å¯¹æ¯”[Yves Peirsman]](https://zhuanlan.zhihu.com/p/37104535)
 - [Improvements to BM25 and Language Models Examined](http://www.cs.otago.ac.nz/homepages/andrew/papers/2014-2.pdf)
 - [CoSENTï¼šæ¯”Sentence-BERTæ›´æœ‰æ•ˆçš„å¥å‘é‡æ–¹æ¡ˆ](https://kexue.fm/archives/8847)
 - [è°ˆè°ˆæ–‡æœ¬åŒ¹é…å’Œå¤šè½®æ£€ç´¢](https://zhuanlan.zhihu.com/p/111769969)
```

#### html2text {}

```diff
@@ -14,17 +14,26 @@
 shibing624/text2vec.svg)](https://github.com/shibing624/text2vec/issues) [!
 [Wechat Group](http://vlog.sfyc.ltd/wechat_everyday/
 wxgroup_logo.png?imageView2/0/w/60/h/20)](#Contact) **Text2vec**: Text to
 Vector, Get Sentence Embeddings. Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂÂ‘Ã©Â‡ÂÃ¥ÂŒÂ–Ã¯Â¼ÂŒÃ¦ÂŠÂŠÃ¦Â–Â‡Ã¦ÂœÂ¬
 (Ã¥ÂŒÂ…Ã¦Â‹Â¬Ã¨Â¯ÂÃ£Â€ÂÃ¥ÂÂ¥Ã¥Â­ÂÃ£Â€ÂÃ¦Â®ÂµÃ¨ÂÂ½)Ã¨Â¡Â¨Ã¥Â¾ÂÃ¤Â¸ÂºÃ¥ÂÂ‘Ã©Â‡ÂÃ§ÂŸÂ©Ã©Â˜ÂµÃ£Â€Â‚
 **text2vec**Ã¥Â®ÂÃ§ÂÂ°Ã¤ÂºÂ†Word2VecÃ£Â€ÂRankBM25Ã£Â€ÂBERTÃ£Â€ÂSentence-
 BERTÃ£Â€ÂCoSENTÃ§Â­Â‰Ã¥Â¤ÂšÃ§Â§ÂÃ¦Â–Â‡Ã¦ÂœÂ¬Ã¨Â¡Â¨Ã¥Â¾ÂÃ£Â€ÂÃ¦Â–Â‡Ã¦ÂœÂ¬Ã§Â›Â¸Ã¤Â¼Â¼Ã¥ÂºÂ¦Ã¨Â®Â¡Ã§Â®Â—Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¥Â¹Â¶Ã¥ÂœÂ¨Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¯Â¼ÂˆÃ§Â›Â¸Ã¤Â¼Â¼Ã¥ÂºÂ¦Ã¨Â®Â¡Ã§Â®Â—Ã¯Â¼Â‰Ã¤Â»Â»Ã¥ÂŠÂ¡Ã¤Â¸ÂŠÃ¦Â¯Â”Ã¨Â¾ÂƒÃ¤ÂºÂ†Ã¥ÂÂ„Ã¦Â¨Â¡Ã¥ÂÂ‹Ã§ÂšÂ„Ã¦Â•ÂˆÃ¦ÂÂœÃ£Â€Â‚
-### News [2023/06/19] v1.2.1Ã§Â‰ÂˆÃ¦ÂœÂ¬: Ã¦Â›Â´Ã¦Â–Â°Ã¤ÂºÂ†Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹`shibing624/
-text2vec-base-chinese-nli`Ã¤Â¸ÂºÃ¦Â–Â°Ã§Â‰Âˆ[shibing624/text2vec-base-chinese-sentence]
-(https://huggingface.co/shibing624/text2vec-base-chinese-
+### News [2023/06/22] v1.2.2Ã§Â‰ÂˆÃ¦ÂœÂ¬: Ã¥ÂÂ‘Ã¥Â¸ÂƒÃ¤ÂºÂ†Ã¥Â¤ÂšÃ¨Â¯Â­Ã¨Â¨Â€Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹[shibing624/
+text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-
+multilingual)Ã¯Â¼ÂŒÃ§Â”Â¨CoSENTÃ¦Â–Â¹Ã¦Â³Â•Ã¨Â®Â­Ã§Â»ÂƒÃ¯Â¼ÂŒÃ¥ÂŸÂºÃ¤ÂºÂ`sentence-transformers/
+paraphrase-multilingual-MiniLM-L12-
+v2`Ã§Â”Â¨Ã¤ÂºÂºÃ¥Â·Â¥Ã¦ÂŒÂ‘Ã©Â€Â‰Ã¥ÂÂÃ§ÂšÂ„Ã¥Â¤ÂšÃ¨Â¯Â­Ã¨Â¨Â€STSÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†[shibing624/nli-zh-all/text2vec-
+base-multilingual-dataset](https://huggingface.co/datasets/shibing624/nli-zh-
+all/tree/main/text2vec-base-multilingual-
+dataset)Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â¾Â—Ã¥ÂˆÂ°Ã¯Â¼ÂŒÃ¥Â¹Â¶Ã¥ÂœÂ¨Ã¤Â¸Â­Ã¨Â‹Â±Ã¦Â–Â‡Ã¦ÂµÂ‹Ã¨Â¯Â•Ã©Â›Â†Ã¨Â¯Â„Ã¤Â¼Â°Ã§Â›Â¸Ã¥Â¯Â¹Ã¤ÂºÂÃ¥ÂÂŸÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â•ÂˆÃ¦ÂÂœÃ¦ÂœÂ‰Ã¦ÂÂÃ¥ÂÂ‡Ã¯Â¼ÂŒÃ¨Â¯Â¦Ã¨Â§Â
+[Release-v1.2.2](https://github.com/shibing624/text2vec/releases/tag/1.2.2)
+[2023/06/19] v1.2.1Ã§Â‰ÂˆÃ¦ÂœÂ¬: Ã¦Â›Â´Ã¦Â–Â°Ã¤ÂºÂ†Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹`shibing624/text2vec-
+base-chinese-nli`Ã¤Â¸ÂºÃ¦Â–Â°Ã§Â‰Âˆ[shibing624/text2vec-base-chinese-sentence](https://
+huggingface.co/shibing624/text2vec-base-chinese-
 sentence)Ã¯Â¼ÂŒÃ©Â’ÂˆÃ¥Â¯Â¹CoSENTÃ§ÂšÂ„lossÃ¨Â®Â¡Ã§Â®Â—Ã¥Â¯Â¹Ã¦ÂÂ’Ã¥ÂºÂÃ¦Â•ÂÃ¦Â„ÂŸÃ§Â‰Â¹Ã§Â‚Â¹Ã¯Â¼ÂŒÃ¤ÂºÂºÃ¥Â·Â¥Ã¦ÂŒÂ‘Ã©Â€Â‰Ã¥Â¹Â¶Ã¦Â•Â´Ã§ÂÂ†Ã¥Â‡ÂºÃ©Â«Â˜Ã¨Â´Â¨Ã©Â‡ÂÃ§ÂšÂ„Ã¦ÂœÂ‰Ã§Â›Â¸Ã¥Â…Â³Ã¦Â€Â§Ã¦ÂÂ’Ã¥ÂºÂÃ§ÂšÂ„STSÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†
 [shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://
 huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-
 sentence-
 dataset)Ã¯Â¼ÂŒÃ¥ÂœÂ¨Ã¥ÂÂ„Ã¨Â¯Â„Ã¤Â¼Â°Ã©Â›Â†Ã¨Â¡Â¨Ã§ÂÂ°Ã§Â›Â¸Ã¥Â¯Â¹Ã¤Â¹Â‹Ã¥Â‰ÂÃ¦ÂœÂ‰Ã¦ÂÂÃ¥ÂÂ‡Ã¯Â¼Â›Ã¥ÂÂ‘Ã¥Â¸ÂƒÃ¤ÂºÂ†Ã©Â€Â‚Ã§Â”Â¨Ã¤ÂºÂs2pÃ§ÂšÂ„Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹
 [shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/
 shibing624/text2vec-base-chinese-paraphrase)Ã¯Â¼ÂŒÃ¨Â¯Â¦Ã¨Â§Â[Release-v1.2.1](https://
@@ -35,17 +44,17 @@
 huggingface.co/datasets/shibing624/
 nli_zh)Ã¥Â…Â¨Ã©ÂƒÂ¨Ã¨Â¯Â­Ã¦Â–Â™Ã¨Â®Â­Ã§Â»ÂƒÃ§ÂšÂ„CoSENTÃ¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¥ÂœÂ¨Ã¥ÂÂ„Ã¨Â¯Â„Ã¤Â¼Â°Ã©Â›Â†Ã¨Â¡Â¨Ã§ÂÂ°Ã¦ÂÂÃ¥ÂÂ‡Ã¦Â˜ÂÃ¦Â˜Â¾Ã¯Â¼ÂŒÃ¨Â¯Â¦Ã¨Â§Â
 [Release-v1.2.0](https://github.com/shibing624/text2vec/releases/tag/1.2.0)
 [2022/03/12] v1.1.4Ã§Â‰ÂˆÃ¦ÂœÂ¬: Ã¥ÂÂ‘Ã¥Â¸ÂƒÃ¤ÂºÂ†Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹[shibing624/text2vec-
 base-chinese](https://huggingface.co/shibing624/text2vec-base-
 chinese)Ã¯Â¼ÂŒÃ¥ÂŸÂºÃ¤ÂºÂÃ¤Â¸Â­Ã¦Â–Â‡STSÃ¨Â®Â­Ã§Â»ÂƒÃ©Â›Â†Ã¨Â®Â­Ã§Â»ÂƒÃ§ÂšÂ„CoSENTÃ¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹Ã£Â€Â‚Ã¨Â¯Â¦Ã¨Â§Â
 [Release-v1.1.4](https://github.com/shibing624/text2vec/releases/tag/1.1.4)
-**Guide** - [Feature](#Feature) - [Evaluation](#Evaluation) - [Install]
-(#install) - [Usage](#usage) - [Contact](#Contact) - [Reference](#reference) #
-Feature ### Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂÂ‘Ã©Â‡ÂÃ¨Â¡Â¨Ã§Â¤ÂºÃ¦Â¨Â¡Ã¥ÂÂ‹ - [Word2Vec](https://github.com/
+**Guide** - [Features](#Features) - [Evaluation](#Evaluation) - [Install]
+(#install) - [Usage](#usage) - [Contact](#Contact) - [References](#references)
+## Features ### Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂÂ‘Ã©Â‡ÂÃ¨Â¡Â¨Ã§Â¤ÂºÃ¦Â¨Â¡Ã¥ÂÂ‹ - [Word2Vec](https://github.com/
 shibing624/text2vec/blob/master/text2vec/word2vec.py)Ã¯Â¼ÂšÃ©Â€ÂšÃ¨Â¿Â‡Ã¨Â…Â¾Ã¨Â®Â¯AI
 LabÃ¥Â¼Â€Ã¦ÂºÂÃ§ÂšÂ„Ã¥Â¤Â§Ã¨Â§Â„Ã¦Â¨Â¡Ã©Â«Â˜Ã¨Â´Â¨Ã©Â‡ÂÃ¤Â¸Â­Ã¦Â–Â‡
 [Ã¨Â¯ÂÃ¥ÂÂ‘Ã©Â‡ÂÃ¦Â•Â°Ã¦ÂÂ®Ã¯Â¼Âˆ800Ã¤Â¸Â‡Ã¤Â¸Â­Ã¦Â–Â‡Ã¨Â¯ÂÃ¨Â½Â»Ã©Â‡ÂÃ§Â‰ÂˆÃ¯Â¼Â‰](https://pan.baidu.com/s/
 1La4U4XNFe8s5BJqxPQpeiQ) (Ã¦Â–Â‡Ã¤Â»Â¶Ã¥ÂÂÃ¯Â¼Âšlight_Tencent_AILab_ChineseEmbedding.bin
 Ã¥Â¯Â†Ã§Â Â:
 taweÃ¯Â¼Â‰Ã¥Â®ÂÃ§ÂÂ°Ã¨Â¯ÂÃ¥ÂÂ‘Ã©Â‡ÂÃ¦Â£Â€Ã§Â´Â¢Ã¯Â¼ÂŒÃ¦ÂœÂ¬Ã©Â¡Â¹Ã§Â›Â®Ã¥Â®ÂÃ§ÂÂ°Ã¤ÂºÂ†Ã¥ÂÂ¥Ã¥Â­ÂÃ¯Â¼ÂˆÃ¨Â¯ÂÃ¥ÂÂ‘Ã©Â‡ÂÃ¦Â±Â‚Ã¥Â¹Â³Ã¥ÂÂ‡Ã¯Â¼Â‰Ã§ÂšÂ„word2vecÃ¥ÂÂ‘Ã©Â‡ÂÃ¨Â¡Â¨Ã§Â¤Âº
 - [SBERT(Sentence-BERT)](https://github.com/shibing624/text2vec/blob/master/
@@ -53,67 +62,76 @@
 sentencebert_model.py)Ã¯Â¼ÂšÃ¦ÂÂƒÃ¨Â¡Â¡Ã¦Â€Â§Ã¨ÂƒÂ½Ã¥Â’ÂŒÃ¦Â•ÂˆÃ§ÂÂ‡Ã§ÂšÂ„Ã¥ÂÂ¥Ã¥ÂÂ‘Ã©Â‡ÂÃ¨Â¡Â¨Ã§Â¤ÂºÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¨Â®Â­Ã§Â»ÂƒÃ¦Â—Â¶Ã©Â€ÂšÃ¨Â¿Â‡Ã¦ÂœÂ‰Ã§Â›Â‘Ã§ÂÂ£Ã¨Â®Â­Ã§Â»ÂƒÃ¤Â¸ÂŠÃ¥Â±Â‚Ã¥ÂˆÂ†Ã§Â±Â»Ã¥Â‡Â½Ã¦Â•Â°Ã¯Â¼ÂŒÃ¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂŒÂ¹Ã©Â…ÂÃ©Â¢Â„Ã¦ÂµÂ‹Ã¦Â—Â¶Ã§Â›Â´Ã¦ÂÂ¥Ã¥ÂÂ¥Ã¥Â­ÂÃ¥ÂÂ‘Ã©Â‡ÂÃ¥ÂÂšÃ¤Â½Â™Ã¥Â¼Â¦Ã¯Â¼ÂŒÃ¦ÂœÂ¬Ã©Â¡Â¹Ã§Â›Â®Ã¥ÂŸÂºÃ¤ÂºÂPyTorchÃ¥Â¤ÂÃ§ÂÂ°Ã¤ÂºÂ†Sentence-
 BERTÃ¦Â¨Â¡Ã¥ÂÂ‹Ã§ÂšÂ„Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â’ÂŒÃ©Â¢Â„Ã¦ÂµÂ‹ - [CoSENT(Cosine Sentence)](https://github.com/
 shibing624/text2vec/blob/master/text2vec/
 cosent_model.py)Ã¯Â¼ÂšCoSENTÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¦ÂÂÃ¥Â‡ÂºÃ¤ÂºÂ†Ã¤Â¸Â€Ã§Â§ÂÃ¦ÂÂ’Ã¥ÂºÂÃ§ÂšÂ„Ã¦ÂÂŸÃ¥Â¤Â±Ã¥Â‡Â½Ã¦Â•Â°Ã¯Â¼ÂŒÃ¤Â½Â¿Ã¨Â®Â­Ã§Â»ÂƒÃ¨Â¿Â‡Ã§Â¨Â‹Ã¦Â›Â´Ã¨Â´Â´Ã¨Â¿Â‘Ã©Â¢Â„Ã¦ÂµÂ‹Ã¯Â¼ÂŒÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â”Â¶Ã¦Â•Â›Ã©Â€ÂŸÃ¥ÂºÂ¦Ã¥Â’ÂŒÃ¦Â•ÂˆÃ¦ÂÂœÃ¦Â¯Â”Sentence-
 BERTÃ¦Â›Â´Ã¥Â¥Â½Ã¯Â¼ÂŒÃ¦ÂœÂ¬Ã©Â¡Â¹Ã§Â›Â®Ã¥ÂŸÂºÃ¤ÂºÂPyTorchÃ¥Â®ÂÃ§ÂÂ°Ã¤ÂºÂ†CoSENTÃ¦Â¨Â¡Ã¥ÂÂ‹Ã§ÂšÂ„Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â’ÂŒÃ©Â¢Â„Ã¦ÂµÂ‹
 Ã¨Â¯Â¦Ã§Â»Â†Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂÂ‘Ã©Â‡ÂÃ¨Â¡Â¨Ã§Â¤ÂºÃ¦Â–Â¹Ã¦Â³Â•Ã¨Â§Âwiki: [Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂÂ‘Ã©Â‡ÂÃ¨Â¡Â¨Ã§Â¤ÂºÃ¦Â–Â¹Ã¦Â³Â•](https://
 github.com/shibing624/text2vec/wiki/
-%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95) #
+%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95) ##
 Evaluation Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂŒÂ¹Ã©Â…Â #### Ã¨Â‹Â±Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã§ÂšÂ„Ã¨Â¯Â„Ã¦ÂµÂ‹Ã§Â»Â“Ã¦ÂÂœÃ¯Â¼Âš | Arch |
 BaseModel | Model | English-STS-B | |:-------|:--------------------------------
-----------------|:-------------------------------------|:-------------:| |
+----------------|:-------------------------------------------------------------
+--------------------------------------------------------|:-------------:| |
 GloVe | glove | Avg_word_embeddings_glove_6B_300d | 61.77 | | BERT | bert-base-
 uncased | BERT-base-cls | 20.29 | | BERT | bert-base-uncased | BERT-base-
 first_last_avg | 59.04 | | BERT | bert-base-uncased | BERT-base-first_last_avg-
 whiten(NLI) | 63.65 | | SBERT | sentence-transformers/bert-base-nli-mean-tokens
 | SBERT-base-nli-cls | 73.65 | | SBERT | sentence-transformers/bert-base-nli-
 mean-tokens | SBERT-base-nli-first_last_avg | 77.96 | | CoSENT | bert-base-
 uncased | CoSENT-base-first_last_avg | 69.93 | | CoSENT | sentence-
 transformers/bert-base-nli-mean-tokens | CoSENT-base-nli-first_last_avg | 79.68
-| #### Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã§ÂšÂ„Ã¨Â¯Â„Ã¦ÂµÂ‹Ã§Â»Â“Ã¦ÂÂœÃ¯Â¼Âš | Arch | BaseModel | Model |
-ATEC | BQ | LCQMC | PAWSX | STS-B | Avg | |:-------|:--------------------------
---|:--------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:| |
-SBERT | bert-base-chinese | SBERT-bert-base | 46.36 | 70.36 | 78.72 | 46.86 |
-66.41 | 61.74 | | SBERT | hfl/chinese-macbert-base | SBERT-macbert-base | 47.28
-| 68.63 | 79.42 | 55.59 | 64.82 | 63.15 | | SBERT | hfl/chinese-roberta-wwm-ext
-| SBERT-roberta-ext | 48.29 | 69.99 | 79.22 | 44.10 | 72.42 | 62.80 | | CoSENT
-| bert-base-chinese | CoSENT-bert-base | 49.74 | 72.38 | 78.69 | 60.00 | 79.27
-| 68.01 | | CoSENT | hfl/chinese-macbert-base | CoSENT-macbert-base | 50.39 |
-72.93 | 79.17 | 60.86 | 79.30 | 68.53 | | CoSENT | hfl/chinese-roberta-wwm-ext
-| CoSENT-roberta-ext | 50.81 | 71.45 | 79.31 | 61.56 | 79.96 | 68.61 |
-Ã¨Â¯Â´Ã¦Â˜ÂÃ¯Â¼Âš - Ã§Â»Â“Ã¦ÂÂœÃ¨Â¯Â„Ã¦ÂµÂ‹Ã¦ÂŒÂ‡Ã¦Â Â‡Ã¯Â¼ÂšspearmanÃ§Â³Â»Ã¦Â•Â° -
+| | CoSENT | sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 |
+[shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/
+text2vec-base-multilingual) | 80.12 | ####
+Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã§ÂšÂ„Ã¨Â¯Â„Ã¦ÂµÂ‹Ã§Â»Â“Ã¦ÂÂœÃ¯Â¼Âš | Arch | BaseModel | Model | ATEC | BQ
+| LCQMC | PAWSX | STS-B | Avg | |:-------|:----------------------------|:------
+--------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:| | SBERT | bert-
+base-chinese | SBERT-bert-base | 46.36 | 70.36 | 78.72 | 46.86 | 66.41 | 61.74
+| | SBERT | hfl/chinese-macbert-base | SBERT-macbert-base | 47.28 | 68.63 |
+79.42 | 55.59 | 64.82 | 63.15 | | SBERT | hfl/chinese-roberta-wwm-ext | SBERT-
+roberta-ext | 48.29 | 69.99 | 79.22 | 44.10 | 72.42 | 62.80 | | CoSENT | bert-
+base-chinese | CoSENT-bert-base | 49.74 | 72.38 | 78.69 | 60.00 | 79.27 | 68.01
+| | CoSENT | hfl/chinese-macbert-base | CoSENT-macbert-base | 50.39 | 72.93 |
+79.17 | 60.86 | 79.30 | 68.53 | | CoSENT | hfl/chinese-roberta-wwm-ext |
+CoSENT-roberta-ext | 50.81 | 71.45 | 79.31 | 61.56 | 79.96 | 68.61 | Ã¨Â¯Â´Ã¦Â˜ÂÃ¯Â¼Âš
+- Ã§Â»Â“Ã¦ÂÂœÃ¨Â¯Â„Ã¦ÂµÂ‹Ã¦ÂŒÂ‡Ã¦Â Â‡Ã¯Â¼ÂšspearmanÃ§Â³Â»Ã¦Â•Â° -
 Ã¤Â¸ÂºÃ¨Â¯Â„Ã¦ÂµÂ‹Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨ÂƒÂ½Ã¥ÂŠÂ›Ã¯Â¼ÂŒÃ§Â»Â“Ã¦ÂÂœÃ¥ÂÂ‡Ã¥ÂÂªÃ§Â”Â¨Ã¨Â¯Â¥Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã§ÂšÂ„trainÃ¨Â®Â­Ã§Â»ÂƒÃ¯Â¼ÂŒÃ¥ÂœÂ¨testÃ¤Â¸ÂŠÃ¨Â¯Â„Ã¤Â¼Â°Ã¥Â¾Â—Ã¥ÂˆÂ°Ã§ÂšÂ„Ã¨Â¡Â¨Ã§ÂÂ°Ã¯Â¼ÂŒÃ¦Â²Â¡Ã§Â”Â¨Ã¥Â¤Â–Ã©ÂƒÂ¨Ã¦Â•Â°Ã¦ÂÂ®
-### Release Models - Ã¦ÂœÂ¬Ã©Â¡Â¹Ã§Â›Â®releaseÃ¦Â¨Â¡Ã¥ÂÂ‹Ã§ÂšÂ„Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¨Â¯Â„Ã¦ÂµÂ‹Ã§Â»Â“Ã¦ÂÂœÃ¯Â¼Âš |
-Arch | BaseModel | Model | ATEC | BQ | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-
-dc | Avg | QPS | |:-----------|:----------------------------------|:-----------
+- `SBERT-macbert-base`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¦Â˜Â¯Ã§Â”Â¨SBertÃ¦Â–Â¹Ã¦Â³Â•Ã¨Â®Â­Ã§Â»ÂƒÃ¯Â¼ÂŒÃ¨Â¿ÂÃ¨Â¡ÂŒ[examples/
+training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/
+blob/master/examples/training_sup_text_matching_model.py)Ã¤Â»Â£Ã§Â ÂÃ¥ÂÂ¯Ã¨Â®Â­Ã§Â»ÂƒÃ¦Â¨Â¡Ã¥ÂÂ‹
+- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-
+v2`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â˜Â¯Ã§Â”Â¨SBertÃ¨Â®Â­Ã§Â»ÂƒÃ¯Â¼ÂŒÃ¦Â˜Â¯`paraphrase-MiniLM-L12-
+v2`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã§ÂšÂ„Ã¥Â¤ÂšÃ¨Â¯Â­Ã¨Â¨Â€Ã§Â‰ÂˆÃ¦ÂœÂ¬Ã¯Â¼ÂŒÃ¦Â”Â¯Ã¦ÂŒÂÃ¤Â¸Â­Ã¦Â–Â‡Ã£Â€ÂÃ¨Â‹Â±Ã¦Â–Â‡Ã§Â­Â‰ ### Release Models -
+Ã¦ÂœÂ¬Ã©Â¡Â¹Ã§Â›Â®releaseÃ¦Â¨Â¡Ã¥ÂÂ‹Ã§ÂšÂ„Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¨Â¯Â„Ã¦ÂµÂ‹Ã§Â»Â“Ã¦ÂÂœÃ¯Â¼Âš | Arch | BaseModel | Model
+| ATEC | BQ | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-dc | Avg | QPS | |:-------
+----|:-------------------------------------------------------------|:----------
 -------------------------------------------------------------------------------
---------------------------------------------------------|:-----:|:-----:|:----
+---------------------------------------------------------|:-----:|:-----:|:----
 -:|:-----:|:-----:|:-------:|:-------:|:---------:|:-----:| | Word2Vec |
 word2vec | [w2v-light-tencent-chinese](https://ai.tencent.com/ailab/nlp/en/
 download.html) | 20.00 | 31.49 | 59.46 | 2.57 | 55.78 | 55.04 | 20.70 | 35.03 |
 23769 | | SBERT | xlm-roberta-base | [sentence-transformers/paraphrase-
 multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/
 paraphrase-multilingual-MiniLM-L12-v2) | 18.42 | 38.52 | 63.96 | 10.14 | 78.90
-| 63.01 | 52.28 | 46.46 | 3138 | | Instructor | hfl/chinese-roberta-wwm-ext |
-[moka-ai/m3e-base](https://huggingface.co/moka-ai/m3e-base) | 41.27 | 63.81 |
-74.87 | 12.20 | 76.96 | 75.83 | 60.55 | 57.93 | 2980 | | CoSENT | hfl/chinese-
-macbert-base | [shibing624/text2vec-base-chinese](https://huggingface.co/
-shibing624/text2vec-base-chinese) | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 |
-70.27 | 50.42 | 51.61 | 3008 | | CoSENT | hfl/chinese-lert-large |
-[GanymedeNil/text2vec-large-chinese](https://huggingface.co/GanymedeNil/
-text2vec-large-chinese) | 32.61 | 44.59 | 69.30 | 14.51 | 79.44 | 73.01 | 59.04
-| 53.12 | 2092 | | CoSENT | nghuyong/ernie-3.0-base-zh | [shibing624/text2vec-
-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-
-sentence) | 43.37 | 61.43 | 73.48 | 38.90 | 78.25 | 70.60 | 53.08 | 59.87 |
-3089 | | CoSENT | nghuyong/ernie-3.0-base-zh | [shibing624/text2vec-base-
-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-
-paraphrase) | 44.89 | 63.58 | 74.24 | 40.90 | 78.93 | 76.70 | 63.30 | **63.08**
-| 3066 | Ã¨Â¯Â´Ã¦Â˜ÂÃ¯Â¼Âš - Ã§Â»Â“Ã¦ÂÂœÃ¨Â¯Â„Ã¦ÂµÂ‹Ã¦ÂŒÂ‡Ã¦Â Â‡Ã¯Â¼ÂšspearmanÃ§Â³Â»Ã¦Â•Â° -
-Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â®ÂÃ©ÂªÂŒÃ¦ÂŠÂ¥Ã¥Â‘ÂŠÃ¯Â¼Âš[Ã¥Â®ÂÃ©ÂªÂŒÃ¦ÂŠÂ¥Ã¥Â‘ÂŠ](https://github.com/shibing624/
-text2vec/blob/master/docs/model_report.md) - `shibing624/text2vec-base-
+| 63.01 | 52.28 | 46.46 | 3138 | | CoSENT | hfl/chinese-macbert-base |
+[shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-
+base-chinese) | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 | 70.27 | 50.42 | 51.61 |
+3008 | | CoSENT | hfl/chinese-lert-large | [GanymedeNil/text2vec-large-chinese]
+(https://huggingface.co/GanymedeNil/text2vec-large-chinese) | 32.61 | 44.59 |
+69.30 | 14.51 | 79.44 | 73.01 | 59.04 | 53.12 | 2092 | | CoSENT | nghuyong/
+ernie-3.0-base-zh | [shibing624/text2vec-base-chinese-sentence](https://
+huggingface.co/shibing624/text2vec-base-chinese-sentence) | 43.37 | 61.43 |
+73.48 | 38.90 | 78.25 | 70.60 | 53.08 | 59.87 | 3089 | | CoSENT | nghuyong/
+ernie-3.0-base-zh | [shibing624/text2vec-base-chinese-paraphrase](https://
+huggingface.co/shibing624/text2vec-base-chinese-paraphrase) | 44.89 | 63.58 |
+74.24 | 40.90 | 78.93 | 76.70 | 63.30 | **63.08** | 3066 | | CoSENT | sentence-
+transformers/paraphrase-multilingual-MiniLM-L12-v2 | [shibing624/text2vec-base-
+multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual) |
+32.39 | 50.33 | 65.64 | 32.56 | 74.45 | 68.88 | 51.17 | 53.67 | 3138 |
+Ã¨Â¯Â´Ã¦Â˜ÂÃ¯Â¼Âš - Ã§Â»Â“Ã¦ÂÂœÃ¨Â¯Â„Ã¦ÂµÂ‹Ã¦ÂŒÂ‡Ã¦Â Â‡Ã¯Â¼ÂšspearmanÃ§Â³Â»Ã¦Â•Â° - `shibing624/text2vec-base-
 chinese`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¦Â˜Â¯Ã§Â”Â¨CoSENTÃ¦Â–Â¹Ã¦Â³Â•Ã¨Â®Â­Ã§Â»ÂƒÃ¯Â¼ÂŒÃ¥ÂŸÂºÃ¤ÂºÂ`hfl/chinese-macbert-
 base`Ã¥ÂœÂ¨Ã¤Â¸Â­Ã¦Â–Â‡STS-BÃ¦Â•Â°Ã¦ÂÂ®Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â¾Â—Ã¥ÂˆÂ°Ã¯Â¼ÂŒÃ¥Â¹Â¶Ã¥ÂœÂ¨Ã¤Â¸Â­Ã¦Â–Â‡STS-
 BÃ¦ÂµÂ‹Ã¨Â¯Â•Ã©Â›Â†Ã¨Â¯Â„Ã¤Â¼Â°Ã¨Â¾Â¾Ã¥ÂˆÂ°Ã¨Â¾ÂƒÃ¥Â¥Â½Ã¦Â•ÂˆÃ¦ÂÂœÃ¯Â¼ÂŒÃ¨Â¿ÂÃ¨Â¡ÂŒ[examples/
 training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/
 blob/master/examples/
 training_sup_text_matching_model.py)Ã¤Â»Â£Ã§Â ÂÃ¥ÂÂ¯Ã¨Â®Â­Ã§Â»ÂƒÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â–Â‡Ã¤Â»Â¶Ã¥Â·Â²Ã§Â»ÂÃ¤Â¸ÂŠÃ¤Â¼Â HF
 model hubÃ¯Â¼ÂŒÃ¤Â¸Â­Ã¦Â–Â‡Ã©Â€ÂšÃ§Â”Â¨Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¦ÂÂ¨Ã¨ÂÂÃ¤Â½Â¿Ã§Â”Â¨ - `shibing624/text2vec-
@@ -134,81 +152,83 @@
 [shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://
 huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-
 sentence-dataset)Ã¥ÂŠÂ Ã¥Â…Â¥Ã¤ÂºÂ†s2p(sentence to
 paraphrase)Ã¦Â•Â°Ã¦ÂÂ®Ã¯Â¼ÂŒÃ¥Â¼ÂºÃ¥ÂŒÂ–Ã¤ÂºÂ†Ã¥Â…Â¶Ã©Â•Â¿Ã¦Â–Â‡Ã¦ÂœÂ¬Ã§ÂšÂ„Ã¨Â¡Â¨Ã¥Â¾ÂÃ¨ÂƒÂ½Ã¥ÂŠÂ›Ã¯Â¼ÂŒÃ¥Â¹Â¶Ã¥ÂœÂ¨Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂÂ„NLIÃ¦ÂµÂ‹Ã¨Â¯Â•Ã©Â›Â†Ã¨Â¯Â„Ã¤Â¼Â°Ã¨Â¾Â¾Ã¥ÂˆÂ°SOTAÃ¯Â¼ÂŒÃ¨Â¿ÂÃ¨Â¡ÂŒ
 [examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/
 shibing624/text2vec/blob/master/examples/
 training_sup_text_matching_model_jsonl_data.py)Ã¤Â»Â£Ã§Â ÂÃ¥ÂÂ¯Ã¨Â®Â­Ã§Â»ÂƒÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â–Â‡Ã¤Â»Â¶Ã¥Â·Â²Ã§Â»ÂÃ¤Â¸ÂŠÃ¤Â¼Â HF
-model hubÃ¯Â¼ÂŒÃ¤Â¸Â­Ã¦Â–Â‡s2p(Ã¥ÂÂ¥Ã¥Â­ÂvsÃ¦Â®ÂµÃ¨ÂÂ½)Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¦ÂÂ¨Ã¨ÂÂÃ¤Â½Â¿Ã§Â”Â¨ - `SBERT-
-macbert-base`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¦Â˜Â¯Ã§Â”Â¨SBERTÃ¦Â–Â¹Ã¦Â³Â•Ã¨Â®Â­Ã§Â»ÂƒÃ¯Â¼ÂŒÃ¨Â¿ÂÃ¨Â¡ÂŒ[examples/
-training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/
-blob/master/examples/training_sup_text_matching_model.py)Ã¤Â»Â£Ã§Â ÂÃ¥ÂÂ¯Ã¨Â®Â­Ã§Â»ÂƒÃ¦Â¨Â¡Ã¥ÂÂ‹
-- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-
-v2`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â˜Â¯Ã§Â”Â¨SBERTÃ¨Â®Â­Ã§Â»ÂƒÃ¯Â¼ÂŒÃ¦Â˜Â¯`paraphrase-MiniLM-L12-
-v2`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã§ÂšÂ„Ã¥Â¤ÂšÃ¨Â¯Â­Ã¨Â¨Â€Ã§Â‰ÂˆÃ¦ÂœÂ¬Ã¯Â¼ÂŒÃ¦Â”Â¯Ã¦ÂŒÂÃ¤Â¸Â­Ã¦Â–Â‡Ã£Â€ÂÃ¨Â‹Â±Ã¦Â–Â‡Ã§Â­Â‰ - `w2v-light-tencent-
+model hubÃ¯Â¼ÂŒÃ¤Â¸Â­Ã¦Â–Â‡s2p(Ã¥ÂÂ¥Ã¥Â­ÂvsÃ¦Â®ÂµÃ¨ÂÂ½)Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¦ÂÂ¨Ã¨ÂÂÃ¤Â½Â¿Ã§Â”Â¨ -
+`shibing624/text2vec-base-
+multilingual`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¦Â˜Â¯Ã§Â”Â¨CoSENTÃ¦Â–Â¹Ã¦Â³Â•Ã¨Â®Â­Ã§Â»ÂƒÃ¯Â¼ÂŒÃ¥ÂŸÂºÃ¤ÂºÂ`sentence-transformers/
+paraphrase-multilingual-MiniLM-L12-
+v2`Ã§Â”Â¨Ã¤ÂºÂºÃ¥Â·Â¥Ã¦ÂŒÂ‘Ã©Â€Â‰Ã¥ÂÂÃ§ÂšÂ„Ã¥Â¤ÂšÃ¨Â¯Â­Ã¨Â¨Â€STSÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†[shibing624/nli-zh-all/text2vec-
+base-multilingual-dataset](https://huggingface.co/datasets/shibing624/nli-zh-
+all/tree/main/text2vec-base-multilingual-
+dataset)Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â¾Â—Ã¥ÂˆÂ°Ã¯Â¼ÂŒÃ¥Â¹Â¶Ã¥ÂœÂ¨Ã¤Â¸Â­Ã¨Â‹Â±Ã¦Â–Â‡Ã¦ÂµÂ‹Ã¨Â¯Â•Ã©Â›Â†Ã¨Â¯Â„Ã¤Â¼Â°Ã§Â›Â¸Ã¥Â¯Â¹Ã¤ÂºÂÃ¥ÂÂŸÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â•ÂˆÃ¦ÂÂœÃ¦ÂœÂ‰Ã¦ÂÂÃ¥ÂÂ‡Ã¯Â¼ÂŒÃ¨Â¿ÂÃ¨Â¡ÂŒ
+[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/
+shibing624/text2vec/blob/master/examples/
+training_sup_text_matching_model_jsonl_data.py)Ã¤Â»Â£Ã§Â ÂÃ¥ÂÂ¯Ã¨Â®Â­Ã§Â»ÂƒÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â–Â‡Ã¤Â»Â¶Ã¥Â·Â²Ã§Â»ÂÃ¤Â¸ÂŠÃ¤Â¼Â HF
+model hubÃ¯Â¼ÂŒÃ¥Â¤ÂšÃ¨Â¯Â­Ã¨Â¨Â€Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¦ÂÂ¨Ã¨ÂÂÃ¤Â½Â¿Ã§Â”Â¨ - `w2v-light-tencent-
 chinese`Ã¦Â˜Â¯Ã¨Â…Â¾Ã¨Â®Â¯Ã¨Â¯ÂÃ¥ÂÂ‘Ã©Â‡ÂÃ§ÂšÂ„Word2VecÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒCPUÃ¥ÂŠÂ Ã¨Â½Â½Ã¤Â½Â¿Ã§Â”Â¨Ã¯Â¼ÂŒÃ©Â€Â‚Ã§Â”Â¨Ã¤ÂºÂÃ¤Â¸Â­Ã¦Â–Â‡Ã¥Â­Â—Ã©ÂÂ¢Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¥Â’ÂŒÃ§Â¼ÂºÃ¥Â°Â‘Ã¦Â•Â°Ã¦ÂÂ®Ã§ÂšÂ„Ã¥Â†Â·Ã¥ÂÂ¯Ã¥ÂŠÂ¨Ã¦ÂƒÂ…Ã¥Â†Âµ
 - Ã¥ÂÂ„Ã©Â¢Â„Ã¨Â®Â­Ã§Â»ÂƒÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¥ÂÂ‡Ã¥ÂÂ¯Ã¤Â»Â¥Ã©Â€ÂšÃ¨Â¿Â‡transformersÃ¨Â°ÂƒÃ§Â”Â¨Ã¯Â¼ÂŒÃ¥Â¦Â‚MacBERTÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼Âš`--
 model_name hfl/chinese-macbert-base` Ã¦ÂˆÂ–Ã¨Â€Â…robertaÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼Âš`--model_name uer/
 roberta-medium-wwm-chinese-cluecorpussmall` -
 Ã¤Â¸ÂºÃ¦ÂµÂ‹Ã¨Â¯Â„Ã¦Â¨Â¡Ã¥ÂÂ‹Ã§ÂšÂ„Ã©Â²ÂÃ¦Â£Â’Ã¦Â€Â§Ã¯Â¼ÂŒÃ¥ÂŠÂ Ã¥Â…Â¥Ã¤ÂºÂ†Ã¦ÂœÂªÃ¨Â®Â­Ã§Â»ÂƒÃ¨Â¿Â‡Ã§ÂšÂ„SOHUÃ¦ÂµÂ‹Ã¨Â¯Â•Ã©Â›Â†Ã¯Â¼ÂŒÃ§Â”Â¨Ã¤ÂºÂÃ¦ÂµÂ‹Ã¨Â¯Â•Ã¦Â¨Â¡Ã¥ÂÂ‹Ã§ÂšÂ„Ã¦Â³Â›Ã¥ÂŒÂ–Ã¨ÂƒÂ½Ã¥ÂŠÂ›Ã¯Â¼Â›Ã¤Â¸ÂºÃ¨Â¾Â¾Ã¥ÂˆÂ°Ã¥Â¼Â€Ã§Â®Â±Ã¥ÂÂ³Ã§Â”Â¨Ã§ÂšÂ„Ã¥Â®ÂÃ§Â”Â¨Ã¦Â•ÂˆÃ¦ÂÂœÃ¯Â¼ÂŒÃ¤Â½Â¿Ã§Â”Â¨Ã¤ÂºÂ†Ã¦ÂÂœÃ©Â›Â†Ã¥ÂˆÂ°Ã§ÂšÂ„Ã¥ÂÂ„Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼ÂŒÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¤Â¹ÂŸÃ¤Â¸ÂŠÃ¤Â¼Â Ã¥ÂˆÂ°HF
 datasets[Ã©Â“Â¾Ã¦ÂÂ¥Ã¨Â§ÂÃ¤Â¸Â‹Ã¦Â–Â¹](#Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†) -
 Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¥Â®ÂÃ©ÂªÂŒÃ¨Â¡Â¨Ã¦Â˜ÂÃ¯Â¼ÂŒpoolingÃ¦ÂœÂ€Ã¤Â¼Â˜Ã¦Â˜Â¯`EncoderType.FIRST_LAST_AVG`Ã¥Â’ÂŒ`EncoderType.MEAN`Ã¯Â¼ÂŒÃ¤Â¸Â¤Ã¨Â€Â…Ã©Â¢Â„Ã¦ÂµÂ‹Ã¦Â•ÂˆÃ¦ÂÂœÃ¥Â·Â®Ã¥Â¼Â‚Ã¥Â¾ÂˆÃ¥Â°Â
 -
 Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¨Â¯Â„Ã¦ÂµÂ‹Ã§Â»Â“Ã¦ÂÂœÃ¥Â¤ÂÃ§ÂÂ°Ã¯Â¼ÂŒÃ¥ÂÂ¯Ã¤Â»Â¥Ã¤Â¸Â‹Ã¨Â½Â½Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¥ÂˆÂ°`examples/
 data`Ã¯Â¼ÂŒÃ¨Â¿ÂÃ¨Â¡ÂŒ[tests/test_model_spearman.py](https://github.com/shibing624/
 text2vec/blob/master/tests/test_model_spearman.py)Ã¤Â»Â£Ã§Â ÂÃ¥Â¤ÂÃ§ÂÂ°Ã¨Â¯Â„Ã¦ÂµÂ‹Ã§Â»Â“Ã¦ÂÂœ -
-QPSÃ§ÂšÂ„GPUÃ¦ÂµÂ‹Ã¨Â¯Â•Ã§ÂÂ¯Ã¥Â¢ÂƒÃ¦Â˜Â¯Tesla V100Ã¯Â¼ÂŒÃ¦Â˜Â¾Ã¥Â­Â˜32GB # Demo Official Demo: https://
-www.mulanai.com/product/short_text_sim/ HuggingFace Demo: https://
-huggingface.co/spaces/shibing624/text2vec ![](docs/hf.png) run example:
-[examples/gradio_demo.py](https://github.com/shibing624/text2vec/blob/master/
-examples/gradio_demo.py) to see the demo: ```shell python examples/
-gradio_demo.py ``` # Install ```shell pip install torch # conda install pytorch
-pip install -U text2vec ``` or ```shell pip install torch # conda install
-pytorch pip install -r requirements.txt git clone https://github.com/
-shibing624/text2vec.git cd text2vec pip install --no-deps . ``` # Usage ##
-Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂÂ‘Ã©Â‡ÂÃ¨Â¡Â¨Ã¥Â¾Â Ã¥ÂŸÂºÃ¤ÂºÂ`pretrained model`Ã¨Â®Â¡Ã§Â®Â—Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂÂ‘Ã©Â‡ÂÃ¯Â¼Âš ```zsh >>>
-from text2vec import SentenceModel >>> m = SentenceModel() >>> m.encode
-("Ã¥Â¦Â‚Ã¤Â½Â•Ã¦Â›Â´Ã¦ÂÂ¢Ã¨ÂŠÂ±Ã¥Â‘Â—Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡") Embedding shape: (768,) ``` example:
-[examples/computing_embeddings_demo.py](https://github.com/shibing624/text2vec/
-blob/master/examples/computing_embeddings_demo.py) ```python import sys
-sys.path.append('..') from text2vec import SentenceModel from text2vec import
-Word2Vec def compute_emb(model): # Embed a list of sentences sentences =
-[ 'Ã¥ÂÂ¡', 'Ã©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡', 'Ã¥Â¦Â‚Ã¤Â½Â•Ã¦Â›Â´Ã¦ÂÂ¢Ã¨ÂŠÂ±Ã¥Â‘Â—Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡',
-'Ã¨ÂŠÂ±Ã¥Â‘Â—Ã¦Â›Â´Ã¦Â”Â¹Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡', 'This framework generates embeddings for each
-input sentence', 'Sentences are passed as a list of string.', 'The quick brown
-fox jumps over the lazy dog.' ] sentence_embeddings = model.encode(sentences)
-print(type(sentence_embeddings), sentence_embeddings.shape) # The result is a
-list of sentence embeddings as numpy arrays for sentence, embedding in zip
-(sentences, sentence_embeddings): print("Sentence:", sentence) print("Embedding
-shape:", embedding.shape) print("Embedding head:", embedding[:10]) print() if
-__name__ == "__main__": # Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂÂ¥Ã¥ÂÂ‘Ã©Â‡ÂÃ¦Â¨Â¡Ã¥ÂÂ‹
-(CoSENT)Ã¯Â¼ÂŒÃ¤Â¸Â­Ã¦Â–Â‡Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¦ÂÂ¨Ã¨ÂÂÃ¯Â¼ÂŒÃ¦Â”Â¯Ã¦ÂŒÂfine-tuneÃ§Â»Â§Ã§Â»Â­Ã¨Â®Â­Ã§Â»Âƒ
-t2v_model = SentenceModel("shibing624/text2vec-base-chinese") compute_emb
-(t2v_model) # Ã¦Â”Â¯Ã¦ÂŒÂÃ¥Â¤ÂšÃ¨Â¯Â­Ã¨Â¨Â€Ã§ÂšÂ„Ã¥ÂÂ¥Ã¥ÂÂ‘Ã©Â‡ÂÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂˆSentence-
-BERTÃ¯Â¼Â‰Ã¯Â¼ÂŒÃ¨Â‹Â±Ã¦Â–Â‡Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¦ÂÂ¨Ã¨ÂÂÃ¯Â¼ÂŒÃ¦Â”Â¯Ã¦ÂŒÂfine-tuneÃ§Â»Â§Ã§Â»Â­Ã¨Â®Â­Ã§Â»Âƒ
-sbert_model = SentenceModel("sentence-transformers/paraphrase-multilingual-
-MiniLM-L12-v2") compute_emb(sbert_model) # Ã¤Â¸Â­Ã¦Â–Â‡Ã¨Â¯ÂÃ¥ÂÂ‘Ã©Â‡ÂÃ¦Â¨Â¡Ã¥ÂÂ‹
+QPSÃ§ÂšÂ„GPUÃ¦ÂµÂ‹Ã¨Â¯Â•Ã§ÂÂ¯Ã¥Â¢ÂƒÃ¦Â˜Â¯Tesla V100Ã¯Â¼ÂŒÃ¦Â˜Â¾Ã¥Â­Â˜32GB Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â®ÂÃ©ÂªÂŒÃ¦ÂŠÂ¥Ã¥Â‘ÂŠÃ¯Â¼Âš
+[Ã¥Â®ÂÃ©ÂªÂŒÃ¦ÂŠÂ¥Ã¥Â‘ÂŠ](https://github.com/shibing624/text2vec/blob/master/docs/
+model_report.md) ## Demo Official Demo: https://www.mulanai.com/product/
+short_text_sim/ HuggingFace Demo: https://huggingface.co/spaces/shibing624/
+text2vec ![](docs/hf.png) run example: [examples/gradio_demo.py](https://
+github.com/shibing624/text2vec/blob/master/examples/gradio_demo.py) to see the
+demo: ```shell python examples/gradio_demo.py ``` ## Install ```shell pip
+install torch # conda install pytorch pip install -U text2vec ``` or ```shell
+pip install torch # conda install pytorch pip install -r requirements.txt git
+clone https://github.com/shibing624/text2vec.git cd text2vec pip install --no-
+deps . ``` ## Usage ### Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂÂ‘Ã©Â‡ÂÃ¨Â¡Â¨Ã¥Â¾Â Ã¥ÂŸÂºÃ¤ÂºÂ`pretrained
+model`Ã¨Â®Â¡Ã§Â®Â—Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂÂ‘Ã©Â‡ÂÃ¯Â¼Âš ```zsh >>> from text2vec import SentenceModel >>> m
+= SentenceModel() >>> m.encode("Ã¥Â¦Â‚Ã¤Â½Â•Ã¦Â›Â´Ã¦ÂÂ¢Ã¨ÂŠÂ±Ã¥Â‘Â—Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡") Embedding
+shape: (768,) ``` example: [examples/computing_embeddings_demo.py](https://
+github.com/shibing624/text2vec/blob/master/examples/
+computing_embeddings_demo.py) ```python import sys sys.path.append('..') from
+text2vec import SentenceModel from text2vec import Word2Vec def compute_emb
+(model): # Embed a list of sentences sentences = [ 'Ã¥ÂÂ¡', 'Ã©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡',
+'Ã¥Â¦Â‚Ã¤Â½Â•Ã¦Â›Â´Ã¦ÂÂ¢Ã¨ÂŠÂ±Ã¥Â‘Â—Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡', 'Ã¨ÂŠÂ±Ã¥Â‘Â—Ã¦Â›Â´Ã¦Â”Â¹Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡', 'This
+framework generates embeddings for each input sentence', 'Sentences are passed
+as a list of string.', 'The quick brown fox jumps over the lazy dog.' ]
+sentence_embeddings = model.encode(sentences) print(type(sentence_embeddings),
+sentence_embeddings.shape) # The result is a list of sentence embeddings as
+numpy arrays for sentence, embedding in zip(sentences, sentence_embeddings):
+print("Sentence:", sentence) print("Embedding shape:", embedding.shape) print
+("Embedding head:", embedding[:10]) print() if __name__ == "__main__": #
+Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂÂ¥Ã¥ÂÂ‘Ã©Â‡ÂÃ¦Â¨Â¡Ã¥ÂÂ‹(CoSENT)Ã¯Â¼ÂŒÃ¤Â¸Â­Ã¦Â–Â‡Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¦ÂÂ¨Ã¨ÂÂÃ¯Â¼ÂŒÃ¦Â”Â¯Ã¦ÂŒÂfine-
+tuneÃ§Â»Â§Ã§Â»Â­Ã¨Â®Â­Ã§Â»Âƒ t2v_model = SentenceModel("shibing624/text2vec-base-chinese")
+compute_emb(t2v_model) #
+Ã¦Â”Â¯Ã¦ÂŒÂÃ¥Â¤ÂšÃ¨Â¯Â­Ã¨Â¨Â€Ã§ÂšÂ„Ã¥ÂÂ¥Ã¥ÂÂ‘Ã©Â‡ÂÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂˆCoSENTÃ¯Â¼Â‰Ã¯Â¼ÂŒÃ¥Â¤ÂšÃ¨Â¯Â­Ã¨Â¨Â€Ã¯Â¼ÂˆÃ¥ÂŒÂ…Ã¦Â‹Â¬Ã¤Â¸Â­Ã¨Â‹Â±Ã¦Â–Â‡Ã¯Â¼Â‰Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¦ÂÂ¨Ã¨ÂÂÃ¯Â¼ÂŒÃ¦Â”Â¯Ã¦ÂŒÂfine-
+tuneÃ§Â»Â§Ã§Â»Â­Ã¨Â®Â­Ã§Â»Âƒ sbert_model = SentenceModel("shibing624/text2vec-base-
+multilingual") compute_emb(sbert_model) # Ã¤Â¸Â­Ã¦Â–Â‡Ã¨Â¯ÂÃ¥ÂÂ‘Ã©Â‡ÂÃ¦Â¨Â¡Ã¥ÂÂ‹
 (word2vec)Ã¯Â¼ÂŒÃ¤Â¸Â­Ã¦Â–Â‡Ã¥Â­Â—Ã©ÂÂ¢Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¥Â’ÂŒÃ¥Â†Â·Ã¥ÂÂ¯Ã¥ÂŠÂ¨Ã©Â€Â‚Ã§Â”Â¨ w2v_model = Word2Vec
 ("w2v-light-tencent-chinese") compute_emb(w2v_model) ``` output: ```
 numpy.ndarray'> (7, 768) Sentence: Ã¥ÂÂ¡ Embedding shape: (768,) Sentence:
 Ã©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡ Embedding shape: (768,) ... ``` -
 Ã¨Â¿Â”Ã¥Â›ÂÃ¥Â€Â¼`embeddings`Ã¦Â˜Â¯`numpy.ndarray`Ã§Â±Â»Ã¥ÂÂ‹Ã¯Â¼ÂŒshapeÃ¤Â¸Âº`(sentences_size,
 model_embedding_size)`Ã¯Â¼ÂŒÃ¤Â¸Â‰Ã¤Â¸ÂªÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¤Â»Â»Ã©Â€Â‰Ã¤Â¸Â€Ã§Â§ÂÃ¥ÂÂ³Ã¥ÂÂ¯Ã¯Â¼ÂŒÃ¦ÂÂ¨Ã¨ÂÂÃ§Â”Â¨Ã§Â¬Â¬Ã¤Â¸Â€Ã¤Â¸ÂªÃ£Â€Â‚
 - `shibing624/text2vec-base-chinese`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â˜Â¯CoSENTÃ¦Â–Â¹Ã¦Â³Â•Ã¥ÂœÂ¨Ã¤Â¸Â­Ã¦Â–Â‡STS-
 BÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â¾Â—Ã¥ÂˆÂ°Ã§ÂšÂ„Ã¯Â¼ÂŒÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¥Â·Â²Ã§Â»ÂÃ¤Â¸ÂŠÃ¤Â¼Â Ã¥ÂˆÂ°huggingfaceÃ§ÂšÂ„ Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¥ÂºÂ“
 [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-
 base-chinese)Ã¯Â¼ÂŒ
 Ã¦Â˜Â¯`text2vec.SentenceModel`Ã¦ÂŒÂ‡Ã¥Â®ÂšÃ§ÂšÂ„Ã©Â»Â˜Ã¨Â®Â¤Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¥ÂÂ¯Ã¤Â»Â¥Ã©Â€ÂšÃ¨Â¿Â‡Ã¤Â¸ÂŠÃ©ÂÂ¢Ã§Â¤ÂºÃ¤Â¾Â‹Ã¨Â°ÂƒÃ§Â”Â¨Ã¯Â¼ÂŒÃ¦ÂˆÂ–Ã¨Â€Â…Ã¥Â¦Â‚Ã¤Â¸Â‹Ã¦Â‰Â€Ã§Â¤ÂºÃ§Â”Â¨
 [transformersÃ¥ÂºÂ“](https://github.com/huggingface/transformers)Ã¨Â°ÂƒÃ§Â”Â¨Ã¯Â¼ÂŒ
-Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â‡ÂªÃ¥ÂŠÂ¨Ã¤Â¸Â‹Ã¨Â½Â½Ã¥ÂˆÂ°Ã¦ÂœÂ¬Ã¦ÂœÂºÃ¨Â·Â¯Ã¥Â¾Â„Ã¯Â¼Âš`~/.cache/huggingface/transformers` -
-`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â˜Â¯Sentence-
-BERTÃ§ÂšÂ„Ã¥Â¤ÂšÃ¨Â¯Â­Ã¨Â¨Â€Ã¥ÂÂ¥Ã¥ÂÂ‘Ã©Â‡ÂÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒ
-Ã©Â€Â‚Ã§Â”Â¨Ã¤ÂºÂÃ©Â‡ÂŠÃ¤Â¹Â‰Ã¯Â¼ÂˆparaphraseÃ¯Â¼Â‰Ã¨Â¯Â†Ã¥ÂˆÂ«Ã¯Â¼ÂŒÃ¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂŒÂ¹Ã©Â…ÂÃ¯Â¼ÂŒÃ©Â€ÂšÃ¨Â¿Â‡`text2vec.SentenceModel`Ã¥Â’ÂŒ
-[sentence-transformersÃ¥ÂºÂ“]((https://github.com/UKPLab/sentence-
-transformers))Ã©ÂƒÂ½Ã¥ÂÂ¯Ã¤Â»Â¥Ã¨Â°ÂƒÃ§Â”Â¨Ã¨Â¯Â¥Ã¦Â¨Â¡Ã¥ÂÂ‹ - `w2v-light-tencent-
+Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â‡ÂªÃ¥ÂŠÂ¨Ã¤Â¸Â‹Ã¨Â½Â½Ã¥ÂˆÂ°Ã¦ÂœÂ¬Ã¦ÂœÂºÃ¨Â·Â¯Ã¥Â¾Â„Ã¯Â¼Âš`~/.cache/huggingface/transformers` - `w2v-
+light-tencent-
 chinese`Ã¦Â˜Â¯Ã©Â€ÂšÃ¨Â¿Â‡gensimÃ¥ÂŠÂ Ã¨Â½Â½Ã§ÂšÂ„Word2VecÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¤Â½Â¿Ã§Â”Â¨Ã¨Â…Â¾Ã¨Â®Â¯Ã¨Â¯ÂÃ¥ÂÂ‘Ã©Â‡Â`Tencent_AILab_ChineseEmbedding.tar.gz`Ã¨Â®Â¡Ã§Â®Â—Ã¥ÂÂ„Ã¥Â­Â—Ã¨Â¯ÂÃ§ÂšÂ„Ã¨Â¯ÂÃ¥ÂÂ‘Ã©Â‡ÂÃ¯Â¼ÂŒÃ¥ÂÂ¥Ã¥Â­ÂÃ¥ÂÂ‘Ã©Â‡ÂÃ©Â€ÂšÃ¨Â¿Â‡Ã¥ÂÂ•Ã¨Â¯ÂÃ¨Â¯Â
 Ã¥ÂÂ‘Ã©Â‡ÂÃ¥ÂÂ–Ã¥Â¹Â³Ã¥ÂÂ‡Ã¥Â€Â¼Ã¥Â¾Â—Ã¥ÂˆÂ°Ã¯Â¼ÂŒÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â‡ÂªÃ¥ÂŠÂ¨Ã¤Â¸Â‹Ã¨Â½Â½Ã¥ÂˆÂ°Ã¦ÂœÂ¬Ã¦ÂœÂºÃ¨Â·Â¯Ã¥Â¾Â„Ã¯Â¼Âš`~/.text2vec/
 datasets/light_Tencent_AILab_ChineseEmbedding.bin` #### Usage (HuggingFace
 Transformers) Without [text2vec](https://github.com/shibing624/text2vec), you
 can use the model like this: First, you pass your input through the transformer
 model, then you have to apply the right pooling-operation on-top of the
 contextualized word embeddings. example: [examples/
@@ -319,16 +339,16 @@
 Ã¦Â–Â‡Ã¦ÂœÂ¬Ã§Â›Â¸Ã¤Â¼Â¼Ã¥ÂºÂ¦Ã¨Â®Â¡Ã§Â®Â—Ã¥Â’ÂŒÃ¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦ÂÂœÃ§Â´Â¢Ã¤Â»Â»Ã¥ÂŠÂ¡Ã¯Â¼ÂŒÃ¦ÂÂ¨Ã¨ÂÂÃ¤Â½Â¿Ã§Â”Â¨
 [similaritiesÃ¥ÂºÂ“](https://github.com/shibing624/similarities)
 Ã¯Â¼ÂŒÃ¥Â…Â¼Ã¥Â®Â¹Ã¦ÂœÂ¬Ã©Â¡Â¹Ã§Â›Â®releaseÃ§ÂšÂ„
 Word2vecÃ£Â€ÂSBERTÃ£Â€ÂCosentÃ§Â±Â»Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¨Â¿Â˜Ã¦Â”Â¯Ã¦ÂŒÂÃ¥Â­Â—Ã©ÂÂ¢Ã§Â»Â´Ã¥ÂºÂ¦Ã§Â›Â¸Ã¤Â¼Â¼Ã¥ÂºÂ¦Ã¨Â®Â¡Ã§Â®Â—Ã£Â€ÂÃ¥ÂŒÂ¹Ã©Â…ÂÃ¦ÂÂœÃ§Â´Â¢Ã§Â®Â—Ã¦Â³Â•Ã¯Â¼ÂŒÃ¦Â”Â¯Ã¦ÂŒÂÃ¦Â–Â‡Ã¦ÂœÂ¬Ã£Â€ÂÃ¥Â›Â¾Ã¥ÂƒÂÃ£Â€Â‚
 Ã¥Â®Â‰Ã¨Â£Â…Ã¯Â¼Âš ```pip install -U similarities``` Ã¥ÂÂ¥Ã¥Â­ÂÃ§Â›Â¸Ã¤Â¼Â¼Ã¥ÂºÂ¦Ã¨Â®Â¡Ã§Â®Â—Ã¯Â¼Âš ```python
 from similarities import Similarity m = Similarity() r = m.similarity
 ('Ã¥Â¦Â‚Ã¤Â½Â•Ã¦Â›Â´Ã¦ÂÂ¢Ã¨ÂŠÂ±Ã¥Â‘Â—Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡', 'Ã¨ÂŠÂ±Ã¥Â‘Â—Ã¦Â›Â´Ã¦Â”Â¹Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡') print
-(f"similarity score: {float(r)}") # similarity score: 0.855146050453186 ``` #
-Models ## CoSENT model CoSENTÃ¯Â¼ÂˆCosine
+(f"similarity score: {float(r)}") # similarity score: 0.855146050453186 ``` ##
+Models ### CoSENT model CoSENTÃ¯Â¼ÂˆCosine
 SentenceÃ¯Â¼Â‰Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¥ÂœÂ¨Sentence-
 BERTÃ¤Â¸ÂŠÃ¦Â”Â¹Ã¨Â¿Â›Ã¤ÂºÂ†CosineRankLossÃ§ÂšÂ„Ã¥ÂÂ¥Ã¥ÂÂ‘Ã©Â‡ÂÃ¦Â–Â¹Ã¦Â¡Âˆ Network structure: Training:
 [docs/cosent_train.png] Inference: [docs/inference.png] #### CoSENT
 Ã§Â›Â‘Ã§ÂÂ£Ã¦Â¨Â¡Ã¥ÂÂ‹ Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â’ÂŒÃ©Â¢Â„Ã¦ÂµÂ‹CoSENTÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼Âš - Ã¥ÂœÂ¨Ã¤Â¸Â­Ã¦Â–Â‡STS-
 BÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â’ÂŒÃ¨Â¯Â„Ã¤Â¼Â°`CoSENT`Ã¦Â¨Â¡Ã¥ÂÂ‹ example: [examples/
 training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/
 blob/master/examples/training_sup_text_matching_model.py) ```shell cd examples
@@ -340,21 +360,24 @@
 'PAWSX'Ã¯Â¼ÂŒÃ¥Â…Â·Ã¤Â½Â“Ã¥ÂÂ‚Ã¨Â€ÂƒHuggingFace datasets [https://huggingface.co/datasets/
 shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh) ```shell
 python training_sup_text_matching_model.py --task_name ATEC --model_arch cosent
 --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base -
 -output_dir ./outputs/ATEC-cosent ``` - Ã¥ÂœÂ¨Ã¨Â‡ÂªÃ¦ÂœÂ‰Ã¤Â¸Â­Ã¦Â–Â‡Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¤Â¸ÂŠÃ¨Â®Â­Ã§Â»ÂƒÃ¦Â¨Â¡Ã¥ÂÂ‹
 example: [examples/training_sup_text_matching_model_mydata.py](https://
 github.com/shibing624/text2vec/blob/master/examples/
-training_sup_text_matching_model_mydata.py) ```shell python
-training_sup_text_matching_model_mydata.py --do_train --do_predict ```
-Ã¨Â®Â­Ã§Â»ÂƒÃ©Â›Â†Ã¦Â Â¼Ã¥Â¼ÂÃ¥ÂÂ‚Ã¨Â€Âƒ[examples/data/STS-B/STS-B.valid.data](https://github.com/
-shibing624/text2vec/blob/master/examples/data/STS-B/STS-B.valid.data) ```shell
-sentence1 sentence2 label Ã¤Â¸Â€Ã¤Â¸ÂªÃ¥Â¥Â³Ã¥Â­Â©Ã¥ÂœÂ¨Ã§Â»Â™Ã¥Â¥Â¹Ã§ÂšÂ„Ã¥Â¤Â´Ã¥ÂÂ‘Ã¥ÂÂšÃ¥ÂÂ‘Ã¥ÂÂ‹Ã£Â€Â‚
-Ã¤Â¸Â€Ã¤Â¸ÂªÃ¥Â¥Â³Ã¥Â­Â©Ã¥ÂœÂ¨Ã¦Â¢Â³Ã¥Â¤Â´Ã£Â€Â‚ 2 Ã¤Â¸Â€Ã§Â¾Â¤Ã§Â”Â·Ã¤ÂºÂºÃ¥ÂœÂ¨Ã¦ÂµÂ·Ã¦Â»Â©Ã¤Â¸ÂŠÃ¨Â¸Â¢Ã¨Â¶Â³Ã§ÂÂƒÃ£Â€Â‚
-Ã¤Â¸Â€Ã§Â¾Â¤Ã§Â”Â·Ã¥Â­Â©Ã¥ÂœÂ¨Ã¦ÂµÂ·Ã¦Â»Â©Ã¤Â¸ÂŠÃ¨Â¸Â¢Ã¨Â¶Â³Ã§ÂÂƒÃ£Â€Â‚ 3
+training_sup_text_matching_model_mydata.py) Ã¥ÂÂ•Ã¥ÂÂ¡Ã¨Â®Â­Ã§Â»ÂƒÃ¯Â¼Âš ```shell
+CUDA_VISIBLE_DEVICES=0 python training_sup_text_matching_model_mydata.py --
+do_train --do_predict ``` Ã¥Â¤ÂšÃ¥ÂÂ¡Ã¨Â®Â­Ã§Â»ÂƒÃ¯Â¼Âš ```shell CUDA_VISIBLE_DEVICES=0,1
+torchrun --nproc_per_node 2 training_sup_text_matching_model_mydata.py --
+do_train --do_predict --output_dir outputs/STS-B-text2vec-macbert-v1 --
+batch_size 64 --fp16 --data_parallel ``` Ã¨Â®Â­Ã§Â»ÂƒÃ©Â›Â†Ã¦Â Â¼Ã¥Â¼ÂÃ¥ÂÂ‚Ã¨Â€Âƒ[examples/data/
+STS-B/STS-B.valid.data](https://github.com/shibing624/text2vec/blob/master/
+examples/data/STS-B/STS-B.valid.data) ```shell sentence1 sentence2 label
+Ã¤Â¸Â€Ã¤Â¸ÂªÃ¥Â¥Â³Ã¥Â­Â©Ã¥ÂœÂ¨Ã§Â»Â™Ã¥Â¥Â¹Ã§ÂšÂ„Ã¥Â¤Â´Ã¥ÂÂ‘Ã¥ÂÂšÃ¥ÂÂ‘Ã¥ÂÂ‹Ã£Â€Â‚ Ã¤Â¸Â€Ã¤Â¸ÂªÃ¥Â¥Â³Ã¥Â­Â©Ã¥ÂœÂ¨Ã¦Â¢Â³Ã¥Â¤Â´Ã£Â€Â‚ 2
+Ã¤Â¸Â€Ã§Â¾Â¤Ã§Â”Â·Ã¤ÂºÂºÃ¥ÂœÂ¨Ã¦ÂµÂ·Ã¦Â»Â©Ã¤Â¸ÂŠÃ¨Â¸Â¢Ã¨Â¶Â³Ã§ÂÂƒÃ£Â€Â‚ Ã¤Â¸Â€Ã§Â¾Â¤Ã§Â”Â·Ã¥Â­Â©Ã¥ÂœÂ¨Ã¦ÂµÂ·Ã¦Â»Â©Ã¤Â¸ÂŠÃ¨Â¸Â¢Ã¨Â¶Â³Ã§ÂÂƒÃ£Â€Â‚ 3
 Ã¤Â¸Â€Ã¤Â¸ÂªÃ¥Â¥Â³Ã¤ÂºÂºÃ¥ÂœÂ¨Ã¦ÂµÂ‹Ã©Â‡ÂÃ¥ÂÂ¦Ã¤Â¸Â€Ã¤Â¸ÂªÃ¥Â¥Â³Ã¤ÂºÂºÃ§ÂšÂ„Ã¨Â„ÂšÃ¨Â¸ÂÃ£Â€Â‚
 Ã¥Â¥Â³Ã¤ÂºÂºÃ¦ÂµÂ‹Ã©Â‡ÂÃ¥ÂÂ¦Ã¤Â¸Â€Ã¤Â¸ÂªÃ¥Â¥Â³Ã¤ÂºÂºÃ§ÂšÂ„Ã¨Â„ÂšÃ¨Â¸ÂÃ£Â€Â‚ 5 ```
 `label`Ã¥ÂÂ¯Ã¤Â»Â¥Ã¦Â˜Â¯0Ã¯Â¼ÂŒ1Ã¦Â Â‡Ã§Â­Â¾Ã¯Â¼ÂŒ0Ã¤Â»Â£Ã¨Â¡Â¨Ã¤Â¸Â¤Ã¤Â¸ÂªÃ¥ÂÂ¥Ã¥Â­ÂÃ¤Â¸ÂÃ§Â›Â¸Ã¤Â¼Â¼Ã¯Â¼ÂŒ1Ã¤Â»Â£Ã¨Â¡Â¨Ã§Â›Â¸Ã¤Â¼Â¼Ã¯Â¼Â›Ã¤Â¹ÂŸÃ¥ÂÂ¯Ã¤Â»Â¥Ã¦Â˜Â¯0-
 5Ã§ÂšÂ„Ã¨Â¯Â„Ã¥ÂˆÂ†Ã¯Â¼ÂŒÃ¨Â¯Â„Ã¥ÂˆÂ†Ã¨Â¶ÂŠÃ©Â«Â˜Ã¯Â¼ÂŒÃ¨Â¡Â¨Ã§Â¤ÂºÃ¤Â¸Â¤Ã¤Â¸ÂªÃ¥ÂÂ¥Ã¥Â­ÂÃ¨Â¶ÂŠÃ§Â›Â¸Ã¤Â¼Â¼Ã£Â€Â‚Ã¦Â¨Â¡Ã¥ÂÂ‹Ã©ÂƒÂ½Ã¨ÂƒÂ½Ã¦Â”Â¯Ã¦ÂŒÂÃ£Â€Â‚
 - Ã¥ÂœÂ¨Ã¨Â‹Â±Ã¦Â–Â‡STS-BÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â’ÂŒÃ¨Â¯Â„Ã¤Â¼Â°`CoSENT`Ã¦Â¨Â¡Ã¥ÂÂ‹ example: [examples/
 training_sup_text_matching_model_en.py](https://github.com/shibing624/text2vec/
 blob/master/examples/training_sup_text_matching_model_en.py) ```shell cd
@@ -363,15 +386,15 @@
 output_dir ./outputs/STS-B-en-cosent ``` #### CoSENT Ã¦Â—Â Ã§Â›Â‘Ã§ÂÂ£Ã¦Â¨Â¡Ã¥ÂÂ‹ -
 Ã¥ÂœÂ¨Ã¨Â‹Â±Ã¦Â–Â‡NLIÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¨Â®Â­Ã§Â»Âƒ`CoSENT`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¥ÂœÂ¨STS-BÃ¦ÂµÂ‹Ã¨Â¯Â•Ã©Â›Â†Ã¨Â¯Â„Ã¤Â¼Â°Ã¦Â•ÂˆÃ¦ÂÂœ
 example: [examples/training_unsup_text_matching_model_en.py](https://
 github.com/shibing624/text2vec/blob/master/examples/
 training_unsup_text_matching_model_en.py) ```shell cd examples python
 training_unsup_text_matching_model_en.py --model_arch cosent --do_train --
 do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./
-outputs/STS-B-en-unsup-cosent ``` ## Sentence-BERT model Sentence-
+outputs/STS-B-en-unsup-cosent ``` ### Sentence-BERT model Sentence-
 BERTÃ¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¨Â¡Â¨Ã¥Â¾ÂÃ¥Â¼ÂÃ¥ÂÂ¥Ã¥ÂÂ‘Ã©Â‡ÂÃ¨Â¡Â¨Ã§Â¤ÂºÃ¦Â–Â¹Ã¦Â¡Âˆ Network structure:
 Training: [docs/sbert_train.png] Inference: [docs/sbert_inference.png] ####
 SentenceBERT Ã§Â›Â‘Ã§ÂÂ£Ã¦Â¨Â¡Ã¥ÂÂ‹ - Ã¥ÂœÂ¨Ã¤Â¸Â­Ã¦Â–Â‡STS-BÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â’ÂŒÃ¨Â¯Â„Ã¤Â¼Â°`SBERT`Ã¦Â¨Â¡Ã¥ÂÂ‹
 example: [examples/training_sup_text_matching_model.py](https://github.com/
 shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)
 ```shell cd examples python training_sup_text_matching_model.py --model_arch
 sentencebert --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-
@@ -384,35 +407,35 @@
 uncased --output_dir ./outputs/STS-B-en-sbert ``` #### SentenceBERT
 Ã¦Â—Â Ã§Â›Â‘Ã§ÂÂ£Ã¦Â¨Â¡Ã¥ÂÂ‹ - Ã¥ÂœÂ¨Ã¨Â‹Â±Ã¦Â–Â‡NLIÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¨Â®Â­Ã§Â»Âƒ`SBERT`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¥ÂœÂ¨STS-
 BÃ¦ÂµÂ‹Ã¨Â¯Â•Ã©Â›Â†Ã¨Â¯Â„Ã¤Â¼Â°Ã¦Â•ÂˆÃ¦ÂÂœ example: [examples/
 training_unsup_text_matching_model_en.py](https://github.com/shibing624/
 text2vec/blob/master/examples/training_unsup_text_matching_model_en.py)
 ```shell cd examples python training_unsup_text_matching_model_en.py --
 model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name
-bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-sbert ``` ## BERT-Match
-model
+bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-sbert ``` ### BERT-
+Match model
 BERTÃ¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¥ÂÂŸÃ§Â”ÂŸBERTÃ¥ÂŒÂ¹Ã©Â…ÂÃ§Â½Â‘Ã§Â»ÂœÃ§Â»Â“Ã¦ÂÂ„Ã¯Â¼ÂŒÃ¤ÂºÂ¤Ã¤ÂºÂ’Ã¥Â¼ÂÃ¥ÂÂ¥Ã¥ÂÂ‘Ã©Â‡ÂÃ¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹
 Network structure: Training and inference: [docs/bert-fc-train.png]
 Ã¨Â®Â­Ã§Â»ÂƒÃ¨Â„ÂšÃ¦ÂœÂ¬Ã¥ÂÂŒÃ¤Â¸ÂŠ[examples/training_sup_text_matching_model.py](https://
 github.com/shibing624/text2vec/blob/master/examples/
-training_sup_text_matching_model.py)Ã£Â€Â‚ ## Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â’Â¸Ã©Â¦ÂÃ¯Â¼ÂˆModel DistillationÃ¯Â¼Â‰
-Ã§Â”Â±Ã¤ÂºÂtext2vecÃ¨Â®Â­Ã§Â»ÂƒÃ§ÂšÂ„Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¥ÂÂ¯Ã¤Â»Â¥Ã¤Â½Â¿Ã§Â”Â¨[sentence-transformers](https://
-github.com/UKPLab/sentence-
+training_sup_text_matching_model.py)Ã£Â€Â‚ ### Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â’Â¸Ã©Â¦ÂÃ¯Â¼ÂˆModel
+DistillationÃ¯Â¼Â‰ Ã§Â”Â±Ã¤ÂºÂtext2vecÃ¨Â®Â­Ã§Â»ÂƒÃ§ÂšÂ„Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¥ÂÂ¯Ã¤Â»Â¥Ã¤Â½Â¿Ã§Â”Â¨[sentence-
+transformers](https://github.com/UKPLab/sentence-
 transformers)Ã¥ÂºÂ“Ã¥ÂŠÂ Ã¨Â½Â½Ã¯Â¼ÂŒÃ¦Â­Â¤Ã¥Â¤Â„Ã¥Â¤ÂÃ§Â”Â¨Ã¥Â…Â¶Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â’Â¸Ã©Â¦ÂÃ¦Â–Â¹Ã¦Â³Â•[distillation](https:
 //github.com/UKPLab/sentence-transformers/tree/master/examples/training/
 distillation)Ã£Â€Â‚ 1. Ã¦Â¨Â¡Ã¥ÂÂ‹Ã©Â™ÂÃ§Â»Â´Ã¯Â¼ÂŒÃ¥ÂÂ‚Ã¨Â€Âƒ[dimensionality_reduction.py](https://
 github.com/UKPLab/sentence-transformers/blob/master/examples/training/
 distillation/
 dimensionality_reduction.py)Ã¤Â½Â¿Ã§Â”Â¨PCAÃ¥Â¯Â¹Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â¾Â“Ã¥Â‡ÂºembeddingÃ©Â™ÂÃ§Â»Â´Ã¯Â¼ÂŒÃ¥ÂÂ¯Ã¥Â‡ÂÃ¥Â°Â‘milvusÃ§Â­Â‰Ã¥ÂÂ‘Ã©Â‡ÂÃ¦Â£Â€Ã§Â´Â¢Ã¦Â•Â°Ã¦ÂÂ®Ã¥ÂºÂ“Ã§ÂšÂ„Ã¥Â­Â˜Ã¥Â‚Â¨Ã¥ÂÂ‹Ã¥ÂŠÂ›Ã¯Â¼ÂŒÃ¨Â¿Â˜Ã¨ÂƒÂ½Ã¨Â½Â»Ã¥Â¾Â®Ã¦ÂÂÃ¥ÂÂ‡Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â•ÂˆÃ¦ÂÂœÃ£Â€Â‚
 2. Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â’Â¸Ã©Â¦ÂÃ¯Â¼ÂŒÃ¥ÂÂ‚Ã¨Â€Âƒ[model_distillation.py](https://github.com/UKPLab/
 sentence-transformers/blob/master/examples/training/distillation/
 model_distillation.py)Ã¤Â½Â¿Ã§Â”Â¨Ã¨Â’Â¸Ã©Â¦ÂÃ¦Â–Â¹Ã¦Â³Â•Ã¯Â¼ÂŒÃ¥Â°Â†TeacherÃ¥Â¤Â§Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â’Â¸Ã©Â¦ÂÃ¥ÂˆÂ°Ã¦Â›Â´Ã¥Â°Â‘layersÃ¥Â±Â‚Ã¦Â•Â°Ã§ÂšÂ„studentÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¤Â¸Â­Ã¯Â¼ÂŒÃ¥ÂœÂ¨Ã¦ÂÂƒÃ¨Â¡Â¡Ã¦Â•ÂˆÃ¦ÂÂœÃ§ÂšÂ„Ã¦ÂƒÂ…Ã¥Â†ÂµÃ¤Â¸Â‹Ã¯Â¼ÂŒÃ¥ÂÂ¯Ã¥Â¤Â§Ã¥Â¹Â…Ã¦ÂÂÃ¥ÂÂ‡Ã¦Â¨Â¡Ã¥ÂÂ‹Ã©Â¢Â„Ã¦ÂµÂ‹Ã©Â€ÂŸÃ¥ÂºÂ¦Ã£Â€Â‚
-## Ã¦Â¨Â¡Ã¥ÂÂ‹Ã©ÂƒÂ¨Ã§Â½Â² Ã¦ÂÂÃ¤Â¾Â›Ã¤Â¸Â¤Ã§Â§ÂÃ©ÂƒÂ¨Ã§Â½Â²Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¦ÂÂ­Ã¥Â»ÂºÃ¦ÂœÂÃ¥ÂŠÂ¡Ã§ÂšÂ„Ã¦Â–Â¹Ã¦Â³Â•Ã¯Â¼Âš
+### Ã¦Â¨Â¡Ã¥ÂÂ‹Ã©ÂƒÂ¨Ã§Â½Â² Ã¦ÂÂÃ¤Â¾Â›Ã¤Â¸Â¤Ã§Â§ÂÃ©ÂƒÂ¨Ã§Â½Â²Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¦ÂÂ­Ã¥Â»ÂºÃ¦ÂœÂÃ¥ÂŠÂ¡Ã§ÂšÂ„Ã¦Â–Â¹Ã¦Â³Â•Ã¯Â¼Âš
 1Ã¯Â¼Â‰Ã¥ÂŸÂºÃ¤ÂºÂJinaÃ¦ÂÂ­Ã¥Â»ÂºgRPCÃ¦ÂœÂÃ¥ÂŠÂ¡Ã£Â€ÂÃ¦ÂÂ¨Ã¨ÂÂÃ£Â€Â‘Ã¯Â¼Â›2Ã¯Â¼Â‰Ã¥ÂŸÂºÃ¤ÂºÂFastAPIÃ¦ÂÂ­Ã¥Â»ÂºÃ¥ÂÂŸÃ§Â”ÂŸHttpÃ¦ÂœÂÃ¥ÂŠÂ¡Ã£Â€Â‚
-### JinaÃ¦ÂœÂÃ¥ÂŠÂ¡ Ã©Â‡Â‡Ã§Â”Â¨C/
+#### JinaÃ¦ÂœÂÃ¥ÂŠÂ¡ Ã©Â‡Â‡Ã§Â”Â¨C/
 SÃ¦Â¨Â¡Ã¥Â¼ÂÃ¦ÂÂ­Ã¥Â»ÂºÃ©Â«Â˜Ã¦Â€Â§Ã¨ÂƒÂ½Ã¦ÂœÂÃ¥ÂŠÂ¡Ã¯Â¼ÂŒÃ¦Â”Â¯Ã¦ÂŒÂdockerÃ¤ÂºÂ‘Ã¥ÂÂŸÃ§Â”ÂŸÃ¯Â¼ÂŒgRPC/HTTP/
 WebSocketÃ¯Â¼ÂŒÃ¦Â”Â¯Ã¦ÂŒÂÃ¥Â¤ÂšÃ¤Â¸ÂªÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¥ÂÂŒÃ¦Â—Â¶Ã©Â¢Â„Ã¦ÂµÂ‹Ã¯Â¼ÂŒGPUÃ¥Â¤ÂšÃ¥ÂÂ¡Ã¥Â¤Â„Ã§ÂÂ†Ã£Â€Â‚ - Ã¥Â®Â‰Ã¨Â£Â…Ã¯Â¼Âš
 ```pip install jina``` - Ã¥ÂÂ¯Ã¥ÂŠÂ¨Ã¦ÂœÂÃ¥ÂŠÂ¡Ã¯Â¼Âš example: [examples/
 jina_server_demo.py](examples/jina_server_demo.py) ```python from jina import
 Flow port = 50001 f = Flow(port=port).add( uses='jinahub://Text2vecEncoder',
 uses_with={'model_name': 'shibing624/text2vec-base-chinese'} ) with f: #
 backend server forever f.block() ```
@@ -421,27 +444,27 @@
 Ã¨Â°ÂƒÃ§Â”Â¨Ã¦ÂœÂÃ¥ÂŠÂ¡Ã¯Â¼Âš ```python from jina import Client from docarray import
 Document, DocumentArray port = 50001 c = Client(port=port) data =
 ['Ã¥Â¦Â‚Ã¤Â½Â•Ã¦Â›Â´Ã¦ÂÂ¢Ã¨ÂŠÂ±Ã¥Â‘Â—Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡', 'Ã¨ÂŠÂ±Ã¥Â‘Â—Ã¦Â›Â´Ã¦Â”Â¹Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡'] print
 ("data:", data) print('data embs:') r = c.post('/', inputs=DocumentArray(
 [Document(text='Ã¥Â¦Â‚Ã¤Â½Â•Ã¦Â›Â´Ã¦ÂÂ¢Ã¨ÂŠÂ±Ã¥Â‘Â—Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡'), Document
 (text='Ã¨ÂŠÂ±Ã¥Â‘Â—Ã¦Â›Â´Ã¦Â”Â¹Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡')])) print(r.embeddings) ```
 Ã¦Â‰Â¹Ã©Â‡ÂÃ¨Â°ÂƒÃ§Â”Â¨Ã¦Â–Â¹Ã¦Â³Â•Ã¨Â§Âexample: [examples/jina_client_demo.py](https://
-github.com/shibing624/text2vec/blob/master/examples/jina_client_demo.py) ###
+github.com/shibing624/text2vec/blob/master/examples/jina_client_demo.py) ####
 FastAPIÃ¦ÂœÂÃ¥ÂŠÂ¡ - Ã¥Â®Â‰Ã¨Â£Â…Ã¯Â¼Âš ```pip install fastapi uvicorn``` - Ã¥ÂÂ¯Ã¥ÂŠÂ¨Ã¦ÂœÂÃ¥ÂŠÂ¡Ã¯Â¼Âš
 example: [examples/fastapi_server_demo.py](https://github.com/shibing624/
 text2vec/blob/master/examples/fastapi_server_demo.py) ```shell cd examples
 python fastapi_server_demo.py ``` - Ã¨Â°ÂƒÃ§Â”Â¨Ã¦ÂœÂÃ¥ÂŠÂ¡Ã¯Â¼Âš ```shell curl -X 'GET' \
 'http://0.0.0.0:8001/emb?q=hello' \ -H 'accept: application/json' ``` ##
-Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â† - Ã¦ÂœÂ¬Ã©Â¡Â¹Ã§Â›Â®releaseÃ§ÂšÂ„Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼Âš | Dataset | Introduce | Download
-Link | |:---------------------------|:-----------------------------------------
---------------------------------|:---------------------------------------------
+Dataset - Ã¦ÂœÂ¬Ã©Â¡Â¹Ã§Â›Â®releaseÃ§ÂšÂ„Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼Âš | Dataset | Introduce | Download Link
+| |:---------------------------|:----------------------------------------------
+---------------------------|:--------------------------------------------------
 -------------------------------------------------------------------------------
 -------------------------------------------------------------------------------
 -------------------------------------------------------------------------------
-------------| | shibing624/nli-zh-all |
+-------| | shibing624/nli-zh-all |
 Ã¤Â¸Â­Ã¦Â–Â‡Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã¥ÂÂˆÃ©Â›Â†Ã¯Â¼ÂŒÃ¦Â•Â´Ã¥ÂÂˆÃ¤ÂºÂ†Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¦ÂÂ¨Ã§ÂÂ†Ã¯Â¼ÂŒÃ§Â›Â¸Ã¤Â¼Â¼Ã¯Â¼ÂŒÃ¦Â‘Â˜Ã¨Â¦ÂÃ¯Â¼ÂŒÃ©Â—Â®Ã§Â­Â”Ã¯Â¼ÂŒÃ¦ÂŒÂ‡Ã¤Â»Â¤Ã¥Â¾Â®Ã¨Â°ÂƒÃ§Â­Â‰Ã¤Â»Â»Ã¥ÂŠÂ¡Ã§ÂšÂ„820Ã¤Â¸Â‡Ã©Â«Â˜Ã¨Â´Â¨Ã©Â‡ÂÃ¦Â•Â°Ã¦ÂÂ®Ã¯Â¼ÂŒÃ¥Â¹Â¶Ã¨Â½Â¬Ã¥ÂŒÂ–Ã¤Â¸ÂºÃ¥ÂŒÂ¹Ã©Â…ÂÃ¦Â Â¼Ã¥Â¼ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†
 | [https://huggingface.co/datasets/shibing624/nli-zh-all](https://
 huggingface.co/datasets/shibing624/nli-zh-all) | | shibing624/snli-zh |
 Ã¤Â¸Â­Ã¦Â–Â‡SNLIÃ¥Â’ÂŒMultiNLIÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼ÂŒÃ§Â¿Â»Ã¨Â¯Â‘Ã¨Â‡ÂªÃ¨Â‹Â±Ã¦Â–Â‡SNLIÃ¥Â’ÂŒMultiNLI | [https://
 huggingface.co/datasets/shibing624/snli-zh](https://huggingface.co/datasets/
 shibing624/snli-zh) | | shibing624/nli_zh |
 Ã¤Â¸Â­Ã¦Â–Â‡Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼ÂŒÃ¦Â•Â´Ã¥ÂÂˆÃ¤ÂºÂ†Ã¤Â¸Â­Ã¦Â–Â‡ATECÃ£Â€ÂBQÃ£Â€ÂLCQMCÃ£Â€ÂPAWSXÃ£Â€ÂSTS-
@@ -459,46 +482,46 @@
 info/1037/1162.htm) | | LCQMC | Ã¤Â¸Â­Ã¦Â–Â‡LCQMC(large-scale Chinese question
 matching corpus)Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼ÂŒQ-QpairÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â† | [LCQMC](http://
 icrc.hitsz.edu.cn/Article/show/171.html) | | PAWSX | Ã¤Â¸Â­Ã¦Â–Â‡PAWS(Paraphrase
 Adversaries from Word Scrambling)Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼ÂŒQ-QpairÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â† | [PAWSX](https:/
 /arxiv.org/abs/1908.11828) | | STS-B | Ã¤Â¸Â­Ã¦Â–Â‡STS-
 BÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼ÂŒÃ¤Â¸Â­Ã¦Â–Â‡Ã¨Â‡ÂªÃ§Â„Â¶Ã¨Â¯Â­Ã¨Â¨Â€Ã¦ÂÂ¨Ã§ÂÂ†Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼ÂŒÃ¤Â»ÂÃ¨Â‹Â±Ã¦Â–Â‡STS-
 BÃ§Â¿Â»Ã¨Â¯Â‘Ã¤Â¸ÂºÃ¤Â¸Â­Ã¦Â–Â‡Ã§ÂšÂ„Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â† | [STS-B](https://github.com/pluto-junzeng/CNSD) |
-Ã¥Â¸Â¸Ã§Â”Â¨Ã¨Â‹Â±Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼Âš - Ã¥Â¤Â§Ã¥ÂÂÃ©Â¼ÂÃ©Â¼ÂÃ§ÂšÂ„multi_nliÃ¥Â’ÂŒsnli: https://
-huggingface.co/datasets/multi_nli - Ã¥Â¤Â§Ã¥ÂÂÃ©Â¼ÂÃ©Â¼ÂÃ§ÂšÂ„multi_nliÃ¥Â’ÂŒsnli: https://
+Ã¥Â¸Â¸Ã§Â”Â¨Ã¨Â‹Â±Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼Âš - Ã¨Â‹Â±Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼Âšmulti_nli: https://
+huggingface.co/datasets/multi_nli - Ã¨Â‹Â±Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼Âšsnli: https://
 huggingface.co/datasets/snli - https://huggingface.co/datasets/metaeval/cnli -
 https://huggingface.co/datasets/mteb/stsbenchmark-sts - https://huggingface.co/
 datasets/JeremiahZ/simcse_sup_nli - https://huggingface.co/datasets/
 MoritzLaurer/multilingual-NLI-26lang-2mil7 Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¤Â½Â¿Ã§Â”Â¨Ã§Â¤ÂºÃ¤Â¾Â‹Ã¯Â¼Âš ```shell
 pip install datasets ``` ```python from datasets import load_dataset dataset =
 load_dataset("shibing624/nli_zh", "STS-B") # ATEC or BQ or LCQMC or PAWSX or
 STS-B print(dataset) print(dataset['test'][0]) ``` output: ```shell DatasetDict
 ({ train: Dataset({ features: ['sentence1', 'sentence2', 'label'], num_rows:
 5231 }) validation: Dataset({ features: ['sentence1', 'sentence2', 'label'],
 num_rows: 1458 }) test: Dataset({ features: ['sentence1', 'sentence2',
 'label'], num_rows: 1361 }) }) {'sentence1':
 'Ã¤Â¸Â€Ã¤Â¸ÂªÃ¥Â¥Â³Ã¥Â­Â©Ã¥ÂœÂ¨Ã§Â»Â™Ã¥Â¥Â¹Ã§ÂšÂ„Ã¥Â¤Â´Ã¥ÂÂ‘Ã¥ÂÂšÃ¥ÂÂ‘Ã¥ÂÂ‹Ã£Â€Â‚', 'sentence2':
-'Ã¤Â¸Â€Ã¤Â¸ÂªÃ¥Â¥Â³Ã¥Â­Â©Ã¥ÂœÂ¨Ã¦Â¢Â³Ã¥Â¤Â´Ã£Â€Â‚', 'label': 2} ``` # Contact - Issue(Ã¥Â»ÂºÃ¨Â®Â®)Ã¯Â¼Âš[!
+'Ã¤Â¸Â€Ã¤Â¸ÂªÃ¥Â¥Â³Ã¥Â­Â©Ã¥ÂœÂ¨Ã¦Â¢Â³Ã¥Â¤Â´Ã£Â€Â‚', 'label': 2} ``` ## Contact - Issue(Ã¥Â»ÂºÃ¨Â®Â®)Ã¯Â¼Âš[!
 [GitHub issues](https://img.shields.io/github/issues/shibing624/text2vec.svg)]
 (https://github.com/shibing624/text2vec/issues) - Ã©Â‚Â®Ã¤Â»Â¶Ã¦ÂˆÂ‘Ã¯Â¼Âšxuming:
 xuming624@qq.com - Ã¥Â¾Â®Ã¤Â¿Â¡Ã¦ÂˆÂ‘Ã¯Â¼ÂšÃ¥ÂŠÂ Ã¦ÂˆÂ‘*Ã¥Â¾Â®Ã¤Â¿Â¡Ã¥ÂÂ·Ã¯Â¼Âšxuming624, Ã¥Â¤Â‡Ã¦Â³Â¨Ã¯Â¼ÂšÃ¥Â§Â“Ã¥ÂÂ-
-Ã¥Â…Â¬Ã¥ÂÂ¸-NLP* Ã¨Â¿Â›NLPÃ¤ÂºÂ¤Ã¦ÂµÂÃ§Â¾Â¤Ã£Â€Â‚ [docs/wechat.jpeg] # Citation
+Ã¥Â…Â¬Ã¥ÂÂ¸-NLP* Ã¨Â¿Â›NLPÃ¤ÂºÂ¤Ã¦ÂµÂÃ§Â¾Â¤Ã£Â€Â‚ [docs/wechat.jpeg] ## Citation
 Ã¥Â¦Â‚Ã¦ÂÂœÃ¤Â½Â Ã¥ÂœÂ¨Ã§Â Â”Ã§Â©Â¶Ã¤Â¸Â­Ã¤Â½Â¿Ã§Â”Â¨Ã¤ÂºÂ†text2vecÃ¯Â¼ÂŒÃ¨Â¯Â·Ã¦ÂŒÂ‰Ã¥Â¦Â‚Ã¤Â¸Â‹Ã¦Â Â¼Ã¥Â¼ÂÃ¥Â¼Â•Ã§Â”Â¨Ã¯Â¼Âš APA:
 ```latex Xu, M. Text2vec: Text to vector toolkit (Version 1.1.2) [Computer
 software]. https://github.com/shibing624/text2vec ``` BibTeX: ```latex @misc
-{Text2vec, author = {Xu, Ming}, title = {Text2vec: Text to vector toolkit},
-year = {2022}, publisher = {GitHub}, journal = {GitHub repository},
-howpublished = {\url{https://github.com/shibing624/text2vec}}, } ``` # License
+{Text2vec, author = {Ming Xu}, title = {Text2vec: Text to vector toolkit}, year
+= {2023}, publisher = {GitHub}, journal = {GitHub repository}, howpublished =
+{\url{https://github.com/shibing624/text2vec}}, } ``` ## License
 Ã¦ÂÂˆÃ¦ÂÂƒÃ¥ÂÂÃ¨Â®Â®Ã¤Â¸Âº [The Apache License 2.0]
 (LICENSE)Ã¯Â¼ÂŒÃ¥ÂÂ¯Ã¥Â…ÂÃ¨Â´Â¹Ã§Â”Â¨Ã¥ÂÂšÃ¥Â•Â†Ã¤Â¸ÂšÃ§Â”Â¨Ã©Â€Â”Ã£Â€Â‚Ã¨Â¯Â·Ã¥ÂœÂ¨Ã¤ÂºÂ§Ã¥Â“ÂÃ¨Â¯Â´Ã¦Â˜ÂÃ¤Â¸Â­Ã©Â™Â„Ã¥ÂŠÂ text2vecÃ§ÂšÂ„Ã©Â“Â¾Ã¦ÂÂ¥Ã¥Â’ÂŒÃ¦ÂÂˆÃ¦ÂÂƒÃ¥ÂÂÃ¨Â®Â®Ã£Â€Â‚
-# Contribute
+## Contribute
 Ã©Â¡Â¹Ã§Â›Â®Ã¤Â»Â£Ã§Â ÂÃ¨Â¿Â˜Ã¥Â¾ÂˆÃ§Â²Â—Ã§Â³Â™Ã¯Â¼ÂŒÃ¥Â¦Â‚Ã¦ÂÂœÃ¥Â¤Â§Ã¥Â®Â¶Ã¥Â¯Â¹Ã¤Â»Â£Ã§Â ÂÃ¦ÂœÂ‰Ã¦Â‰Â€Ã¦Â”Â¹Ã¨Â¿Â›Ã¯Â¼ÂŒÃ¦Â¬Â¢Ã¨Â¿ÂÃ¦ÂÂÃ¤ÂºÂ¤Ã¥Â›ÂÃ¦ÂœÂ¬Ã©Â¡Â¹Ã§Â›Â®Ã¯Â¼ÂŒÃ¥ÂœÂ¨Ã¦ÂÂÃ¤ÂºÂ¤Ã¤Â¹Â‹Ã¥Â‰ÂÃ¯Â¼ÂŒÃ¦Â³Â¨Ã¦Â„ÂÃ¤Â»Â¥Ã¤Â¸Â‹Ã¤Â¸Â¤Ã§Â‚Â¹Ã¯Â¼Âš
 - Ã¥ÂœÂ¨`tests`Ã¦Â·Â»Ã¥ÂŠÂ Ã§Â›Â¸Ã¥ÂºÂ”Ã§ÂšÂ„Ã¥ÂÂ•Ã¥Â…ÂƒÃ¦ÂµÂ‹Ã¨Â¯Â• - Ã¤Â½Â¿Ã§Â”Â¨`python -m pytest -
 v`Ã¦ÂÂ¥Ã¨Â¿ÂÃ¨Â¡ÂŒÃ¦Â‰Â€Ã¦ÂœÂ‰Ã¥ÂÂ•Ã¥Â…ÂƒÃ¦ÂµÂ‹Ã¨Â¯Â•Ã¯Â¼ÂŒÃ§Â¡Â®Ã¤Â¿ÂÃ¦Â‰Â€Ã¦ÂœÂ‰Ã¥ÂÂ•Ã¦ÂµÂ‹Ã©ÂƒÂ½Ã¦Â˜Â¯Ã©Â€ÂšÃ¨Â¿Â‡Ã§ÂšÂ„
-Ã¤Â¹Â‹Ã¥ÂÂÃ¥ÂÂ³Ã¥ÂÂ¯Ã¦ÂÂÃ¤ÂºÂ¤PRÃ£Â€Â‚ # Reference -
+Ã¤Â¹Â‹Ã¥ÂÂÃ¥ÂÂ³Ã¥ÂÂ¯Ã¦ÂÂÃ¤ÂºÂ¤PRÃ£Â€Â‚ ## References -
 [Ã¥Â°Â†Ã¥ÂÂ¥Ã¥Â­ÂÃ¨Â¡Â¨Ã§Â¤ÂºÃ¤Â¸ÂºÃ¥ÂÂ‘Ã©Â‡ÂÃ¯Â¼ÂˆÃ¤Â¸ÂŠÃ¯Â¼Â‰Ã¯Â¼ÂšÃ¦Â—Â Ã§Â›Â‘Ã§ÂÂ£Ã¥ÂÂ¥Ã¥Â­ÂÃ¨Â¡Â¨Ã§Â¤ÂºÃ¥Â­Â¦Ã¤Â¹Â Ã¯Â¼Âˆsentence
 embeddingÃ¯Â¼Â‰](https://www.cnblogs.com/llhthinker/p/10335164.html) -
 [Ã¥Â°Â†Ã¥ÂÂ¥Ã¥Â­ÂÃ¨Â¡Â¨Ã§Â¤ÂºÃ¤Â¸ÂºÃ¥ÂÂ‘Ã©Â‡ÂÃ¯Â¼ÂˆÃ¤Â¸Â‹Ã¯Â¼Â‰Ã¯Â¼ÂšÃ¦Â—Â Ã§Â›Â‘Ã§ÂÂ£Ã¥ÂÂ¥Ã¥Â­ÂÃ¨Â¡Â¨Ã§Â¤ÂºÃ¥Â­Â¦Ã¤Â¹Â Ã¯Â¼Âˆsentence
 embeddingÃ¯Â¼Â‰](https://www.cnblogs.com/llhthinker/p/10341841.html) - [A Simple
 but Tough-to-Beat Baseline for Sentence Embeddings[Sanjeev Arora and Yingyu
 Liang and Tengyu Ma, 2017]](https://openreview.net/forum?id=SyK00v5xx) -
 [Ã¥Â›Â›Ã§Â§ÂÃ¨Â®Â¡Ã§Â®Â—Ã¦Â–Â‡Ã¦ÂœÂ¬Ã§Â›Â¸Ã¤Â¼Â¼Ã¥ÂºÂ¦Ã§ÂšÂ„Ã¦Â–Â¹Ã¦Â³Â•Ã¥Â¯Â¹Ã¦Â¯Â”[Yves Peirsman]](https://
```

### Comparing `text2vec-1.2.1/setup.py` & `text2vec-1.2.2/setup.py`

 * *Files identical despite different names*

### Comparing `text2vec-1.2.1/text2vec/__init__.py` & `text2vec-1.2.2/text2vec/__init__.py`

 * *Files identical despite different names*

### Comparing `text2vec-1.2.1/text2vec/bertmatching_dataset.py` & `text2vec-1.2.2/text2vec/bertmatching_dataset.py`

 * *Files identical despite different names*

### Comparing `text2vec-1.2.1/text2vec/bertmatching_model.py` & `text2vec-1.2.2/text2vec/bertmatching_model.py`

 * *Files 2% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 
 import math
 import pandas as pd
 import torch
 from loguru import logger
 from torch import nn
 from torch.utils.data import DataLoader, Dataset
-from tqdm.auto import tqdm, trange
+from tqdm import tqdm, trange
 from transformers import BertForSequenceClassification, BertTokenizer
 from transformers.optimization import AdamW, get_linear_schedule_with_warmup
 
 from text2vec.bertmatching_dataset import (
     BertMatchingTestDataset,
     BertMatchingTrainDataset,
     HFBertMatchingTrainDataset,
@@ -102,14 +102,16 @@
             eps: float = 1e-6,
             gradient_accumulation_steps: int = 1,
             max_grad_norm: float = 1.0,
             max_steps: int = -1,
             use_hf_dataset: bool = False,
             hf_dataset_name: str = "STS-B",
             save_model_every_epoch: bool = True,
+            bf16: bool = False,
+            data_parallel: bool = False,
     ):
         """
         Trains the model on 'train_file'
 
         Args:
             train_file: Path to text file containing the text to _train the language model on.
             output_dir: The directory where model files will be saved. If not given, self.args.output_dir will be used.
@@ -124,14 +126,16 @@
             eps (optional): Adam epsilon.
             gradient_accumulation_steps (optional): Number of updates steps to accumulate before performing a backward/update pass.
             max_grad_norm (optional): Max gradient norm.
             max_steps (optional): If > 0: set total number of training steps to perform. Override num_epochs.
             use_hf_dataset (optional): Whether to use the HuggingFace datasets for training.
             hf_dataset_name (optional): Name of the dataset to use for the HuggingFace datasets.
             save_model_every_epoch (optional): Save model checkpoint every epoch.
+            bf16 (optional): Use bfloat16 amp training.
+            data_parallel (optional): Use multi-gpu data parallel training.
         Returns:
             global_step: Number of global steps trained
             training_details: Full training progress scores
         """
         if use_hf_dataset and hf_dataset_name:
             logger.info(
                 f"Train_file will be ignored when use_hf_dataset is True, load HF dataset: {hf_dataset_name}")
@@ -159,14 +163,16 @@
             warmup_ratio=warmup_ratio,
             lr=lr,
             eps=eps,
             gradient_accumulation_steps=gradient_accumulation_steps,
             max_grad_norm=max_grad_norm,
             max_steps=max_steps,
             save_model_every_epoch=save_model_every_epoch,
+            bf16=bf16,
+            data_parallel=data_parallel,
         )
         logger.info(f" Training model done. Saved to {output_dir}.")
 
         return global_step, training_details
 
     def train(
             self,
@@ -181,25 +187,30 @@
             warmup_ratio: float = 0.05,
             lr: float = 2e-5,
             eps: float = 1e-6,
             gradient_accumulation_steps: int = 1,
             max_grad_norm: float = 1.0,
             max_steps: int = -1,
             save_model_every_epoch: bool = True,
+            bf16: bool = False,
+            data_parallel: bool = False,
     ):
         """
         Trains the model on train_dataset.
 
         Utility function to be used by the train_model() method. Not intended to be used directly.
         """
         os.makedirs(output_dir, exist_ok=True)
         logger.debug("Use pytorch device: {}".format(device))
         self.model.bert.to(device)
         set_seed(seed)
 
+        if data_parallel:
+            self.bert = nn.DataParallel(self.bert)
+
         train_dataloader = DataLoader(train_dataset, batch_size=batch_size)  # keep the order of the data, not shuffle
         total_steps = len(train_dataloader) * num_epochs
         param_optimizer = list(self.model.bert.named_parameters())
         no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
         optimizer_grouped_parameters = [
             {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
              'weight_decay': weight_decay},
@@ -263,16 +274,24 @@
                     steps_trained_in_current_epoch -= 1
                     continue
                 inputs, labels = batch
                 labels = labels.to(device)
                 # inputs        [batch, 1, seq_len] -> [batch, seq_len]
                 input_ids = inputs.get('input_ids').squeeze(1).to(device)
                 attention_mask = inputs.get('attention_mask').squeeze(1).to(device)
-                token_type_ids = inputs.get('token_type_ids').squeeze(1).to(device)
-                loss, logits, probs = self.model(input_ids, attention_mask, token_type_ids, labels)
+                token_type_ids = inputs.get('token_type_ids', None)
+                if token_type_ids is not None:
+                    token_type_ids = token_type_ids.squeeze(1).to(self.device)
+
+                if bf16:
+                    with torch.autocast('cuda', dtype=torch.bfloat16):
+                        loss, logits, probs = self.model(input_ids, attention_mask, token_type_ids, labels)
+                else:
+                    loss, logits, probs = self.model(input_ids, attention_mask, token_type_ids, labels)
+                    
                 current_loss = loss.item()
 
                 if verbose:
                     batch_iterator.set_description(
                         f"Epoch: {epoch_number + 1}/{num_epochs}, Batch:{step}/{len(train_dataloader)}, Loss: {current_loss:9.4f}")
 
                 if gradient_accumulation_steps > 1:
```

### Comparing `text2vec-1.2.1/text2vec/bm25.py` & `text2vec-1.2.2/text2vec/bm25.py`

 * *Files identical despite different names*

### Comparing `text2vec-1.2.1/text2vec/cosent_dataset.py` & `text2vec-1.2.2/text2vec/cosent_dataset.py`

 * *Files identical despite different names*

### Comparing `text2vec-1.2.1/text2vec/cosent_model.py` & `text2vec-1.2.2/text2vec/cosent_model.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,21 +1,22 @@
 # -*- coding: utf-8 -*-
 """
 @author:XuMing(xuming624@qq.com)
 @description: Create CoSENT model for text matching task
 """
 
+import math
 import os
 
-import math
 import pandas as pd
 import torch
 from loguru import logger
+from torch import nn
 from torch.utils.data import DataLoader, Dataset
-from tqdm.auto import tqdm, trange
+from tqdm import tqdm, trange
 from transformers.optimization import AdamW, get_linear_schedule_with_warmup
 
 from text2vec.cosent_dataset import CosentTrainDataset, load_cosent_train_data, HFCosentTrainDataset
 from text2vec.sentence_model import SentenceModel
 from text2vec.text_matching_dataset import TextMatchingTestDataset, load_text_matching_test_data, \
     HFTextMatchingTestDataset
 from text2vec.utils.stats_util import set_seed
@@ -59,14 +60,16 @@
             eps: float = 1e-6,
             gradient_accumulation_steps: int = 1,
             max_grad_norm: float = 1.0,
             max_steps: int = -1,
             use_hf_dataset: bool = False,
             hf_dataset_name: str = "STS-B",
             save_model_every_epoch: bool = True,
+            bf16: bool = False,
+            data_parallel: bool = False,
     ):
         """
         Trains the model on 'train_file'
 
         Args:
             train_file: Path to text file containing the text to _train the language model on.
             output_dir: The directory where model files will be saved. If not given, self.args.output_dir will be used.
@@ -81,14 +84,16 @@
             eps (optional): Adam epsilon.
             gradient_accumulation_steps (optional): Number of updates steps to accumulate before performing a backward/update pass.
             max_grad_norm (optional): Max gradient norm.
             max_steps (optional): If > 0: set total number of training steps to perform. Override num_epochs.
             use_hf_dataset (optional): Whether to use the HF dataset.
             hf_dataset_name (optional): Name of the dataset to use for the HuggingFace datasets.
             save_model_every_epoch (optional): Save model checkpoint every epoch.
+            bf16 (optional): Use bfloat16 amp training.
+            data_parallel (optional): Use multi-gpu data parallel training.
         Returns:
             global_step: Number of global steps trained
             training_details: full training progress scores
         """
         if use_hf_dataset and hf_dataset_name:
             logger.info(
                 f"Train_file will be ignored when use_hf_dataset is True, load HF dataset: {hf_dataset_name}")
@@ -115,14 +120,16 @@
             warmup_ratio=warmup_ratio,
             lr=lr,
             eps=eps,
             gradient_accumulation_steps=gradient_accumulation_steps,
             max_grad_norm=max_grad_norm,
             max_steps=max_steps,
             save_model_every_epoch=save_model_every_epoch,
+            bf16=bf16,
+            data_parallel=data_parallel,
         )
         logger.info(f" Training model done. Saved to {output_dir}.")
 
         return global_step, training_details
 
     def calc_loss(self, y_true, y_pred):
         """
@@ -159,25 +166,30 @@
             warmup_ratio: float = 0.05,
             lr: float = 2e-5,
             eps: float = 1e-6,
             gradient_accumulation_steps: int = 1,
             max_grad_norm: float = 1.0,
             max_steps: int = -1,
             save_model_every_epoch: bool = True,
+            bf16: bool = False,
+            data_parallel: bool = False,
     ):
         """
         Trains the model on train_dataset.
 
         Utility function to be used by the train_model() method. Not intended to be used directly.
         """
         os.makedirs(output_dir, exist_ok=True)
         logger.debug("Use device: {}".format(self.device))
         self.bert.to(self.device)
         set_seed(seed)
 
+        if data_parallel:
+            self.bert = nn.DataParallel(self.bert)
+
         train_dataloader = DataLoader(train_dataset, batch_size=batch_size)  # keep the order of the data, not shuffle
         total_steps = len(train_dataloader) * num_epochs
         param_optimizer = list(self.bert.named_parameters())
         no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
         optimizer_grouped_parameters = [
             {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
              'weight_decay': weight_decay},
@@ -241,17 +253,26 @@
                     steps_trained_in_current_epoch -= 1
                     continue
                 inputs, labels = batch
                 labels = labels.to(self.device)
                 # inputs        [batch, 1, seq_len] -> [batch, seq_len]
                 input_ids = inputs.get('input_ids').squeeze(1).to(self.device)
                 attention_mask = inputs.get('attention_mask').squeeze(1).to(self.device)
-                token_type_ids = inputs.get('token_type_ids').squeeze(1).to(self.device)
-                output_embeddings = self.get_sentence_embeddings(input_ids, attention_mask, token_type_ids)
-                loss = self.calc_loss(labels, output_embeddings)
+                token_type_ids = inputs.get('token_type_ids', None)
+                if token_type_ids is not None:
+                    token_type_ids = token_type_ids.squeeze(1).to(self.device)
+
+                if bf16:
+                    with torch.autocast('cuda', dtype=torch.bfloat16):
+                        output_embeddings = self.get_sentence_embeddings(input_ids, attention_mask, token_type_ids)
+                        loss = self.calc_loss(labels, output_embeddings)
+                else:
+                    output_embeddings = self.get_sentence_embeddings(input_ids, attention_mask, token_type_ids)
+                    loss = self.calc_loss(labels, output_embeddings)
+
                 current_loss = loss.item()
                 if verbose:
                     batch_iterator.set_description(
                         f"Epoch: {epoch_number + 1}/{num_epochs}, Batch:{step}/{len(train_dataloader)}, Loss: {current_loss:9.4f}")
 
                 if gradient_accumulation_steps > 1:
                     loss = loss / gradient_accumulation_steps
```

### Comparing `text2vec-1.2.1/text2vec/ngram.py` & `text2vec-1.2.2/text2vec/ngram.py`

 * *Files identical despite different names*

### Comparing `text2vec-1.2.1/text2vec/sentence_model.py` & `text2vec-1.2.2/text2vec/sentence_model.py`

 * *Files 3% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 from enum import Enum
 from typing import List, Union, Optional
 
 import numpy as np
 import torch
 from loguru import logger
 from torch.utils.data import DataLoader, Dataset
-from tqdm.auto import tqdm, trange
+from tqdm import tqdm, trange
 from tqdm.autonotebook import trange
 from transformers import AutoTokenizer, AutoModel
 
 from text2vec.utils.stats_util import compute_spearmanr, compute_pearsonr
 
 os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
 os.environ["TOKENIZERS_PARALLELISM"] = "TRUE"
@@ -89,15 +89,15 @@
         -------
         int or None
             The dimension of the sentence embeddings, or None if it cannot be determined.
         """
         # Use getattr to safely access the out_features attribute of the pooler's dense layer
         return getattr(self.bert.pooler.dense, "out_features", None)
 
-    def get_sentence_embeddings(self, input_ids, attention_mask, token_type_ids):
+    def get_sentence_embeddings(self, input_ids, attention_mask, token_type_ids=None):
         """
         Returns the model output by encoder_type as embeddings.
 
         Utility function for self.bert() method.
         """
         model_output = self.bert(input_ids, attention_mask, token_type_ids, output_hidden_states=True)
 
@@ -223,20 +223,24 @@
         for batch in tqdm(eval_dataloader, disable=False, desc="Running Evaluation"):
             source, target, labels = batch
             labels = labels.to(self.device)
             batch_labels.extend(labels.cpu().numpy())
             # source        [batch, 1, seq_len] -> [batch, seq_len]
             source_input_ids = source.get('input_ids').squeeze(1).to(self.device)
             source_attention_mask = source.get('attention_mask').squeeze(1).to(self.device)
-            source_token_type_ids = source.get('token_type_ids').squeeze(1).to(self.device)
+            source_token_type_ids = source.get('token_type_ids', None)
+            if source_token_type_ids is not None:
+                source_token_type_ids = source_token_type_ids.squeeze(1).to(self.device)
 
             # target        [batch, 1, seq_len] -> [batch, seq_len]
             target_input_ids = target.get('input_ids').squeeze(1).to(self.device)
             target_attention_mask = target.get('attention_mask').squeeze(1).to(self.device)
-            target_token_type_ids = target.get('token_type_ids').squeeze(1).to(self.device)
+            target_token_type_ids = target.get('token_type_ids', None)
+            if target_token_type_ids is not None:
+                target_token_type_ids = target_token_type_ids.squeeze(1).to(self.device)
 
             with torch.no_grad():
                 source_embeddings = self.get_sentence_embeddings(source_input_ids, source_attention_mask,
                                                                  source_token_type_ids)
                 target_embeddings = self.get_sentence_embeddings(target_input_ids, target_attention_mask,
                                                                  target_token_type_ids)
                 preds = torch.cosine_similarity(source_embeddings, target_embeddings)
```

### Comparing `text2vec-1.2.1/text2vec/sentencebert_model.py` & `text2vec-1.2.2/text2vec/sentencebert_model.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,22 +1,22 @@
 # -*- coding: utf-8 -*-
 """
 @author:XuMing(xuming624@qq.com)
 @description: Create Sentence-BERT model for text matching task
 """
 
+import math
 import os
 
-import math
 import pandas as pd
 import torch
 from loguru import logger
 from torch import nn
 from torch.utils.data import DataLoader, Dataset
-from tqdm.auto import tqdm, trange
+from tqdm import tqdm, trange
 from transformers.optimization import AdamW, get_linear_schedule_with_warmup
 
 from text2vec.sentence_model import SentenceModel
 from text2vec.text_matching_dataset import (
     TextMatchingTrainDataset,
     TextMatchingTestDataset,
     load_text_matching_test_data,
@@ -68,14 +68,16 @@
             eps: float = 1e-6,
             gradient_accumulation_steps: int = 1,
             max_grad_norm: float = 1.0,
             max_steps: int = -1,
             use_hf_dataset: bool = False,
             hf_dataset_name: str = "STS-B",
             save_model_every_epoch: bool = True,
+            bf16: bool = False,
+            data_parallel: bool = False,
     ):
         """
         Trains the model on 'train_file'
 
         Args:
             train_file: Path to text file containing the text to _train the language model on.
             output_dir: The directory where model files will be saved. If not given, self.args.output_dir will be used.
@@ -90,14 +92,16 @@
             eps (optional): Adam epsilon.
             gradient_accumulation_steps (optional): Number of updates steps to accumulate before performing a backward/update pass.
             max_grad_norm (optional): Max gradient norm.
             max_steps (optional): If > 0: set total number of training steps to perform. Override num_epochs.
             use_hf_dataset (optional): Whether to use the HuggingFace datasets for training.
             hf_dataset_name (optional): Name of the dataset to use for the HuggingFace datasets.
             save_model_every_epoch (optional): Save model checkpoint every epoch.
+            bf16 (optional): Use bfloat16 amp training.
+            data_parallel (optional): Use multi-gpu data parallel training.
         Returns:
             global_step: Number of global steps trained
             training_details: Full training progress scores
         """
         if use_hf_dataset and hf_dataset_name:
             logger.info(
                 f"Train_file will be ignored when use_hf_dataset is True, load HF dataset: {hf_dataset_name}")
@@ -125,14 +129,16 @@
             warmup_ratio=warmup_ratio,
             lr=lr,
             eps=eps,
             gradient_accumulation_steps=gradient_accumulation_steps,
             max_grad_norm=max_grad_norm,
             max_steps=max_steps,
             save_model_every_epoch=save_model_every_epoch,
+            bf16=bf16,
+            data_parallel=data_parallel,
         )
         logger.info(f" Training model done. Saved to {output_dir}.")
 
         return global_step, training_details
 
     def concat_embeddings(self, source_embeddings, target_embeddings):
         """
@@ -169,25 +175,30 @@
             warmup_ratio: float = 0.05,
             lr: float = 2e-5,
             eps: float = 1e-6,
             gradient_accumulation_steps: int = 1,
             max_grad_norm: float = 1.0,
             max_steps: int = -1,
             save_model_every_epoch: bool = True,
+            bf16: bool = False,
+            data_parallel: bool = False,
     ):
         """
         Trains the model on train_dataset.
 
         Utility function to be used by the train_model() method. Not intended to be used directly.
         """
         os.makedirs(output_dir, exist_ok=True)
         logger.debug("Use pytorch device: {}".format(self.device))
         self.bert.to(self.device)
         set_seed(seed)
 
+        if data_parallel:
+            self.bert = nn.DataParallel(self.bert)
+
         train_dataloader = DataLoader(train_dataset, batch_size=batch_size)  # keep the order of the data, not shuffle
         total_steps = len(train_dataloader) * num_epochs
         param_optimizer = list(self.bert.named_parameters())
         no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
         optimizer_grouped_parameters = [
             {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
              'weight_decay': weight_decay},
@@ -250,28 +261,42 @@
                 if steps_trained_in_current_epoch > 0:
                     steps_trained_in_current_epoch -= 1
                     continue
                 source, target, labels = batch
                 # source        [batch, 1, seq_len] -> [batch, seq_len]
                 source_input_ids = source.get('input_ids').squeeze(1).to(self.device)
                 source_attention_mask = source.get('attention_mask').squeeze(1).to(self.device)
-                source_token_type_ids = source.get('token_type_ids').squeeze(1).to(self.device)
+                source_token_type_ids = source.get('token_type_ids', None)
+                if source_token_type_ids is not None:
+                    source_token_type_ids = source_token_type_ids.squeeze(1).to(self.device)
                 # target        [batch, 1, seq_len] -> [batch, seq_len]
                 target_input_ids = target.get('input_ids').squeeze(1).to(self.device)
                 target_attention_mask = target.get('attention_mask').squeeze(1).to(self.device)
-                target_token_type_ids = target.get('token_type_ids').squeeze(1).to(self.device)
+                target_token_type_ids = target.get('token_type_ids', None)
+                if target_token_type_ids is not None:
+                    target_token_type_ids = target_token_type_ids.squeeze(1).to(self.device)
                 labels = labels.to(self.device)
 
                 # get sentence embeddings of BERT encoder
-                source_embeddings = self.get_sentence_embeddings(source_input_ids, source_attention_mask,
-                                                                 source_token_type_ids)
-                target_embeddings = self.get_sentence_embeddings(target_input_ids, target_attention_mask,
-                                                                 target_token_type_ids)
-                logits = self.concat_embeddings(source_embeddings, target_embeddings)
-                loss = self.calc_loss(labels, logits)
+                if bf16:
+                    with torch.autocast('cuda', dtype=torch.bfloat16):
+                        source_embeddings = self.get_sentence_embeddings(source_input_ids, source_attention_mask,
+                                                                         source_token_type_ids)
+                        target_embeddings = self.get_sentence_embeddings(target_input_ids, target_attention_mask,
+                                                                         target_token_type_ids)
+                        logits = self.concat_embeddings(source_embeddings, target_embeddings)
+                        loss = self.calc_loss(labels, logits)
+                else:
+                    source_embeddings = self.get_sentence_embeddings(source_input_ids, source_attention_mask,
+                                                                     source_token_type_ids)
+                    target_embeddings = self.get_sentence_embeddings(target_input_ids, target_attention_mask,
+                                                                     target_token_type_ids)
+                    logits = self.concat_embeddings(source_embeddings, target_embeddings)
+                    loss = self.calc_loss(labels, logits)
+
                 current_loss = loss.item()
                 if verbose:
                     batch_iterator.set_description(
                         f"Epoch: {epoch_number + 1}/{num_epochs}, Batch:{step}/{len(train_dataloader)}, Loss: {current_loss:9.4f}")
 
                 if gradient_accumulation_steps > 1:
                     loss = loss / gradient_accumulation_steps
```

### Comparing `text2vec-1.2.1/text2vec/similarity.py` & `text2vec-1.2.2/text2vec/similarity.py`

 * *Files identical despite different names*

### Comparing `text2vec-1.2.1/text2vec/stopwords.txt` & `text2vec-1.2.2/text2vec/stopwords.txt`

 * *Files identical despite different names*

### Comparing `text2vec-1.2.1/text2vec/text_matching_dataset.py` & `text2vec-1.2.2/text2vec/text_matching_dataset.py`

 * *Files identical despite different names*

### Comparing `text2vec-1.2.1/text2vec/utils/distance.py` & `text2vec-1.2.2/text2vec/utils/distance.py`

 * *Files 4% similar despite different names*

```diff
@@ -24,14 +24,15 @@
     ä½™å¼¦è·ç¦»
     return cos score
     """
     up = float(np.sum(v1 * v2))
     down = np.linalg.norm(v1) * np.linalg.norm(v2)
     return try_divide(up, down)
 
+
 def hamming_distance(v1, v2):
     n = int(v1, 2) ^ int(v2, 2)
     return bin(n & 0xffffffff).count('1')
 
 
 def euclidean_distance(v1, v2):  # æ¬§æ°è·ç¦»
     return np.sqrt(np.sum(np.square(v1 - v2)))
@@ -41,16 +42,21 @@
     return np.sum(np.abs(v1 - v2))
 
 
 def chebyshev_distance(v1, v2):  # åˆ‡æ¯”é›ªå¤«è·ç¦»
     return np.max(np.abs(v1 - v2))
 
 
-def minkowski_distance(v1, v2):  # é—µå¯å¤«æ–¯åŸºè·ç¦»
-    return np.sqrt(np.sum(np.square(v1 - v2)))
+def minkowski_distance(v1, v2, p=2):
+    """
+    é—µå¯å¤«æ–¯åŸºè·ç¦»
+        p=1 æ›¼å“ˆé¡¿è·ç¦»
+        p=2 æ¬§æ°è·ç¦»
+    """
+    return np.power(np.sum(np.power(np.abs(v1 - v2), p)), 1 / p)
 
 
 def euclidean_distance_standardized(v1, v2):  # æ ‡å‡†åŒ–æ¬§æ°è·ç¦»
     v1_v2 = np.vstack([v1, v2])
     sk_v1_v2 = np.var(v1_v2, axis=0, ddof=1)
     return np.sqrt(((v1 - v2) ** 2 / (sk_v1_v2 + zero_bit * np.ones_like(sk_v1_v2))).sum())
 
@@ -206,15 +212,14 @@
     x = np.array(x).astype(float)
     xr = np.rollaxis(x, axis=axis)
     xr -= np.mean(x, axis=axis)
     xr /= np.std(x, axis=axis)
     return x
 
 
-
 if __name__ == '__main__':
     vec1_test = np.array([1, 38, 17, 32])
     vec2_test = np.array([5, 6, 8, 9])
 
     str1_test = "ä½ åˆ°åº•æ˜¯è°?"
     str2_test = "æ²¡æƒ³åˆ°æˆ‘æ˜¯è°ï¼Œæ˜¯çœŸæ ·å­"
```

### Comparing `text2vec-1.2.1/text2vec/utils/get_file.py` & `text2vec-1.2.2/text2vec/utils/get_file.py`

 * *Files identical despite different names*

### Comparing `text2vec-1.2.1/text2vec/utils/io_util.py` & `text2vec-1.2.2/text2vec/utils/io_util.py`

 * *Files identical despite different names*

### Comparing `text2vec-1.2.1/text2vec/utils/rank_bm25.py` & `text2vec-1.2.2/text2vec/utils/rank_bm25.py`

 * *Files identical despite different names*

### Comparing `text2vec-1.2.1/text2vec/utils/stats_util.py` & `text2vec-1.2.2/text2vec/utils/stats_util.py`

 * *Files identical despite different names*

### Comparing `text2vec-1.2.1/text2vec/utils/tokenizer.py` & `text2vec-1.2.2/text2vec/utils/tokenizer.py`

 * *Files identical despite different names*

### Comparing `text2vec-1.2.1/text2vec/word2vec.py` & `text2vec-1.2.2/text2vec/word2vec.py`

 * *Files identical despite different names*

### Comparing `text2vec-1.2.1/text2vec.egg-info/PKG-INFO` & `text2vec-1.2.2/text2vec.egg-info/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: text2vec
-Version: 1.2.1
+Version: 1.2.2
 Summary: Text to vector Tool, encode text
 Home-page: https://github.com/shibing624/text2vec
 Author: XuMing
 Author-email: xuming624@qq.com
 License: Apache License 2.0
 Description: [**ğŸ‡¨ğŸ‡³ä¸­æ–‡**](https://github.com/shibing624/text2vec/blob/master/README.md) | [**ğŸŒEnglish**](https://github.com/shibing624/text2vec/blob/master/README_EN.md) | [**ğŸ“–æ–‡æ¡£/Docs**](https://github.com/shibing624/text2vec/wiki) | [**ğŸ¤–æ¨¡å‹/Models**](https://huggingface.co/shibing624) 
         
@@ -27,54 +27,57 @@
         
         
         **Text2vec**: Text to Vector, Get Sentence Embeddings. æ–‡æœ¬å‘é‡åŒ–ï¼ŒæŠŠæ–‡æœ¬(åŒ…æ‹¬è¯ã€å¥å­ã€æ®µè½)è¡¨å¾ä¸ºå‘é‡çŸ©é˜µã€‚
         
         **text2vec**å®ç°äº†Word2Vecã€RankBM25ã€BERTã€Sentence-BERTã€CoSENTç­‰å¤šç§æ–‡æœ¬è¡¨å¾ã€æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—æ¨¡å‹ï¼Œå¹¶åœ¨æ–‡æœ¬è¯­ä¹‰åŒ¹é…ï¼ˆç›¸ä¼¼åº¦è®¡ç®—ï¼‰ä»»åŠ¡ä¸Šæ¯”è¾ƒäº†å„æ¨¡å‹çš„æ•ˆæœã€‚
         
         ### News
+        [2023/06/22] v1.2.2ç‰ˆæœ¬: å‘å¸ƒäº†å¤šè¯­è¨€åŒ¹é…æ¨¡å‹[shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)ï¼Œç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`ç”¨äººå·¥æŒ‘é€‰åçš„å¤šè¯­è¨€STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-multilingual-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-multilingual-dataset)è®­ç»ƒå¾—åˆ°ï¼Œå¹¶åœ¨ä¸­è‹±æ–‡æµ‹è¯•é›†è¯„ä¼°ç›¸å¯¹äºåŸæ¨¡å‹æ•ˆæœæœ‰æå‡ï¼Œè¯¦è§[Release-v1.2.2](https://github.com/shibing624/text2vec/releases/tag/1.2.2)
+        
         [2023/06/19] v1.2.1ç‰ˆæœ¬: æ›´æ–°äº†ä¸­æ–‡åŒ¹é…æ¨¡å‹`shibing624/text2vec-base-chinese-nli`ä¸ºæ–°ç‰ˆ[shibing624/text2vec-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-sentence)ï¼Œé’ˆå¯¹CoSENTçš„lossè®¡ç®—å¯¹æ’åºæ•æ„Ÿç‰¹ç‚¹ï¼Œäººå·¥æŒ‘é€‰å¹¶æ•´ç†å‡ºé«˜è´¨é‡çš„æœ‰ç›¸å…³æ€§æ’åºçš„STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)ï¼Œåœ¨å„è¯„ä¼°é›†è¡¨ç°ç›¸å¯¹ä¹‹å‰æœ‰æå‡ï¼›å‘å¸ƒäº†é€‚ç”¨äºs2pçš„ä¸­æ–‡åŒ¹é…æ¨¡å‹[shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase)ï¼Œè¯¦è§[Release-v1.2.1](https://github.com/shibing624/text2vec/releases/tag/1.2.1)
         
         [2023/06/15] v1.2.0ç‰ˆæœ¬: å‘å¸ƒäº†ä¸­æ–‡åŒ¹é…æ¨¡å‹[shibing624/text2vec-base-chinese-nli](https://huggingface.co/shibing624/text2vec-base-chinese-nli)ï¼ŒåŸºäº`nghuyong/ernie-3.0-base-zh`æ¨¡å‹ï¼Œä½¿ç”¨äº†ä¸­æ–‡NLIæ•°æ®é›†[shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh)å…¨éƒ¨è¯­æ–™è®­ç»ƒçš„CoSENTæ–‡æœ¬åŒ¹é…æ¨¡å‹ï¼Œåœ¨å„è¯„ä¼°é›†è¡¨ç°æå‡æ˜æ˜¾ï¼Œè¯¦è§[Release-v1.2.0](https://github.com/shibing624/text2vec/releases/tag/1.2.0)
         
         [2022/03/12] v1.1.4ç‰ˆæœ¬: å‘å¸ƒäº†ä¸­æ–‡åŒ¹é…æ¨¡å‹[shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)ï¼ŒåŸºäºä¸­æ–‡STSè®­ç»ƒé›†è®­ç»ƒçš„CoSENTåŒ¹é…æ¨¡å‹ã€‚è¯¦è§[Release-v1.1.4](https://github.com/shibing624/text2vec/releases/tag/1.1.4)
         
         
         **Guide**
-        - [Feature](#Feature)
+        - [Features](#Features)
         - [Evaluation](#Evaluation)
         - [Install](#install)
         - [Usage](#usage)
         - [Contact](#Contact)
-        - [Reference](#reference)
+        - [References](#references)
         
         
-        # Feature
+        ## Features
         ### æ–‡æœ¬å‘é‡è¡¨ç¤ºæ¨¡å‹
         - [Word2Vec](https://github.com/shibing624/text2vec/blob/master/text2vec/word2vec.py)ï¼šé€šè¿‡è…¾è®¯AI Labå¼€æºçš„å¤§è§„æ¨¡é«˜è´¨é‡ä¸­æ–‡[è¯å‘é‡æ•°æ®ï¼ˆ800ä¸‡ä¸­æ–‡è¯è½»é‡ç‰ˆï¼‰](https://pan.baidu.com/s/1La4U4XNFe8s5BJqxPQpeiQ) (æ–‡ä»¶åï¼šlight_Tencent_AILab_ChineseEmbedding.bin å¯†ç : taweï¼‰å®ç°è¯å‘é‡æ£€ç´¢ï¼Œæœ¬é¡¹ç›®å®ç°äº†å¥å­ï¼ˆè¯å‘é‡æ±‚å¹³å‡ï¼‰çš„word2vecå‘é‡è¡¨ç¤º
         - [SBERT(Sentence-BERT)](https://github.com/shibing624/text2vec/blob/master/text2vec/sentencebert_model.py)ï¼šæƒè¡¡æ€§èƒ½å’Œæ•ˆç‡çš„å¥å‘é‡è¡¨ç¤ºæ¨¡å‹ï¼Œè®­ç»ƒæ—¶é€šè¿‡æœ‰ç›‘ç£è®­ç»ƒä¸Šå±‚åˆ†ç±»å‡½æ•°ï¼Œæ–‡æœ¬åŒ¹é…é¢„æµ‹æ—¶ç›´æ¥å¥å­å‘é‡åšä½™å¼¦ï¼Œæœ¬é¡¹ç›®åŸºäºPyTorchå¤ç°äº†Sentence-BERTæ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹
         - [CoSENT(Cosine Sentence)](https://github.com/shibing624/text2vec/blob/master/text2vec/cosent_model.py)ï¼šCoSENTæ¨¡å‹æå‡ºäº†ä¸€ç§æ’åºçš„æŸå¤±å‡½æ•°ï¼Œä½¿è®­ç»ƒè¿‡ç¨‹æ›´è´´è¿‘é¢„æµ‹ï¼Œæ¨¡å‹æ”¶æ•›é€Ÿåº¦å’Œæ•ˆæœæ¯”Sentence-BERTæ›´å¥½ï¼Œæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†CoSENTæ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹
         
         è¯¦ç»†æ–‡æœ¬å‘é‡è¡¨ç¤ºæ–¹æ³•è§wiki: [æ–‡æœ¬å‘é‡è¡¨ç¤ºæ–¹æ³•](https://github.com/shibing624/text2vec/wiki/%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95)
-        # Evaluation
+        ## Evaluation
         
         æ–‡æœ¬åŒ¹é…
         
         #### è‹±æ–‡åŒ¹é…æ•°æ®é›†çš„è¯„æµ‹ç»“æœï¼š
         
         
-        | Arch   | BaseModel                                        | Model                            | English-STS-B | 
-        |:-------|:------------------------------------------------|:-------------------------------------|:-------------:|
-        | GloVe  | glove                                           | Avg_word_embeddings_glove_6B_300d    |     61.77     |
-        | BERT   | bert-base-uncased                               | BERT-base-cls                        |     20.29     |
-        | BERT   | bert-base-uncased                               | BERT-base-first_last_avg             |     59.04     |
-        | BERT   | bert-base-uncased                               | BERT-base-first_last_avg-whiten(NLI) |     63.65     |
-        | SBERT  | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-cls                   |     73.65     |
-        | SBERT  | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-first_last_avg        |     77.96     |
-        | CoSENT | bert-base-uncased                               | CoSENT-base-first_last_avg           |     69.93     |
-        | CoSENT | sentence-transformers/bert-base-nli-mean-tokens | CoSENT-base-nli-first_last_avg       |     79.68     |
+        | Arch   | BaseModel                                        | Model                                                                                                                | English-STS-B | 
+        |:-------|:------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------|:-------------:|
+        | GloVe  | glove                                           | Avg_word_embeddings_glove_6B_300d                                                                                    |     61.77     |
+        | BERT   | bert-base-uncased                               | BERT-base-cls                                                                                                        |     20.29     |
+        | BERT   | bert-base-uncased                               | BERT-base-first_last_avg                                                                                             |     59.04     |
+        | BERT   | bert-base-uncased                               | BERT-base-first_last_avg-whiten(NLI)                                                                                 |     63.65     |
+        | SBERT  | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-cls                                                                                                   |     73.65     |
+        | SBERT  | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-first_last_avg                                                                                        |     77.96     |
+        | CoSENT | bert-base-uncased                               | CoSENT-base-first_last_avg                                                                                           |     69.93     |
+        | CoSENT | sentence-transformers/bert-base-nli-mean-tokens | CoSENT-base-nli-first_last_avg                                                                                       |     79.68     |
+        | CoSENT | sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 | [shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)                |     80.12     |
         
         #### ä¸­æ–‡åŒ¹é…æ•°æ®é›†çš„è¯„æµ‹ç»“æœï¼š
         
         
         | Arch   | BaseModel                    | Model           | ATEC  |  BQ   | LCQMC | PAWSX | STS-B |  Avg  | 
         |:-------|:----------------------------|:--------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|
         | SBERT  | bert-base-chinese           | SBERT-bert-base     | 46.36 | 70.36 | 78.72 | 46.86 | 66.41 | 61.74 |
@@ -83,59 +86,60 @@
         | CoSENT | bert-base-chinese           | CoSENT-bert-base    | 49.74 | 72.38 | 78.69 | 60.00 | 79.27 | 68.01 |
         | CoSENT | hfl/chinese-macbert-base    | CoSENT-macbert-base | 50.39 | 72.93 | 79.17 | 60.86 | 79.30 | 68.53 |
         | CoSENT | hfl/chinese-roberta-wwm-ext | CoSENT-roberta-ext  | 50.81 | 71.45 | 79.31 | 61.56 | 79.96 | 68.61 |
         
         è¯´æ˜ï¼š
         - ç»“æœè¯„æµ‹æŒ‡æ ‡ï¼šspearmanç³»æ•°
         - ä¸ºè¯„æµ‹æ¨¡å‹èƒ½åŠ›ï¼Œç»“æœå‡åªç”¨è¯¥æ•°æ®é›†çš„trainè®­ç»ƒï¼Œåœ¨testä¸Šè¯„ä¼°å¾—åˆ°çš„è¡¨ç°ï¼Œæ²¡ç”¨å¤–éƒ¨æ•°æ®
+        - `SBERT-macbert-base`æ¨¡å‹ï¼Œæ˜¯ç”¨SBertæ–¹æ³•è®­ç»ƒï¼Œè¿è¡Œ[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹
+        - `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`æ¨¡å‹æ˜¯ç”¨SBertè®­ç»ƒï¼Œæ˜¯`paraphrase-MiniLM-L12-v2`æ¨¡å‹çš„å¤šè¯­è¨€ç‰ˆæœ¬ï¼Œæ”¯æŒä¸­æ–‡ã€è‹±æ–‡ç­‰
         
         
         ### Release Models
         - æœ¬é¡¹ç›®releaseæ¨¡å‹çš„ä¸­æ–‡åŒ¹é…è¯„æµ‹ç»“æœï¼š
         
-        | Arch       | BaseModel                         | Model                                                                                                                                             | ATEC  |  BQ   | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-dc |    Avg    |  QPS  |
-        |:-----------|:----------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-------:|:-------:|:---------:|:-----:|
-        | Word2Vec   | word2vec                          | [w2v-light-tencent-chinese](https://ai.tencent.com/ailab/nlp/en/download.html)                                                                    | 20.00 | 31.49 | 59.46 | 2.57  | 55.78 |  55.04  |  20.70  |   35.03   | 23769 |
-        | SBERT      | xlm-roberta-base                  | [sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) | 18.42 | 38.52 | 63.96 | 10.14 | 78.90 |  63.01  |  52.28  |   46.46   | 3138  |
-        | Instructor | hfl/chinese-roberta-wwm-ext       | [moka-ai/m3e-base](https://huggingface.co/moka-ai/m3e-base)                                                                                       | 41.27 | 63.81 | 74.87 | 12.20 | 76.96 |  75.83  |  60.55  |   57.93   | 2980  |
-        | CoSENT     | hfl/chinese-macbert-base          | [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)                                                       | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 |  70.27  |  50.42  |   51.61   | 3008  |
-        | CoSENT     | hfl/chinese-lert-large            | [GanymedeNil/text2vec-large-chinese](https://huggingface.co/GanymedeNil/text2vec-large-chinese)                                                   | 32.61 | 44.59 | 69.30 | 14.51 | 79.44 |  73.01  |  59.04  |   53.12   | 2092  |
-        | CoSENT     | nghuyong/ernie-3.0-base-zh        | [shibing624/text2vec-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-sentence)                                     | 43.37 | 61.43 | 73.48 | 38.90 | 78.25 |  70.60  |  53.08  |   59.87   | 3089  |
-        | CoSENT     | nghuyong/ernie-3.0-base-zh        | [shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase)                                 | 44.89 | 63.58 | 74.24 | 40.90 | 78.93 |  76.70  |  63.30  | **63.08** | 3066  |
+        | Arch       | BaseModel                                                    | Model                                                                                                                                             | ATEC  |  BQ   | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-dc |    Avg    |  QPS  |
+        |:-----------|:-------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-------:|:-------:|:---------:|:-----:|
+        | Word2Vec   | word2vec                                                     | [w2v-light-tencent-chinese](https://ai.tencent.com/ailab/nlp/en/download.html)                                                                    | 20.00 | 31.49 | 59.46 | 2.57  | 55.78 |  55.04  |  20.70  |   35.03   | 23769 |
+        | SBERT      | xlm-roberta-base                                             | [sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) | 18.42 | 38.52 | 63.96 | 10.14 | 78.90 |  63.01  |  52.28  |   46.46   | 3138  |
+        | CoSENT     | hfl/chinese-macbert-base                                     | [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)                                                       | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 |  70.27  |  50.42  |   51.61   | 3008  |
+        | CoSENT     | hfl/chinese-lert-large                                       | [GanymedeNil/text2vec-large-chinese](https://huggingface.co/GanymedeNil/text2vec-large-chinese)                                                   | 32.61 | 44.59 | 69.30 | 14.51 | 79.44 |  73.01  |  59.04  |   53.12   | 2092  |
+        | CoSENT     | nghuyong/ernie-3.0-base-zh                                   | [shibing624/text2vec-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-sentence)                                     | 43.37 | 61.43 | 73.48 | 38.90 | 78.25 |  70.60  |  53.08  |   59.87   | 3089  |
+        | CoSENT     | nghuyong/ernie-3.0-base-zh                                   | [shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase)                                 | 44.89 | 63.58 | 74.24 | 40.90 | 78.93 |  76.70  |  63.30  | **63.08** | 3066  |
+        | CoSENT     | sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2  | [shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)                                             | 32.39 | 50.33 | 65.64 | 32.56 | 74.45 |  68.88  |  51.17  |   53.67   | 3138  |
         
         
         è¯´æ˜ï¼š
         - ç»“æœè¯„æµ‹æŒ‡æ ‡ï¼šspearmanç³»æ•°
-        - æ¨¡å‹è®­ç»ƒå®éªŒæŠ¥å‘Šï¼š[å®éªŒæŠ¥å‘Š](https://github.com/shibing624/text2vec/blob/master/docs/model_report.md)
         - `shibing624/text2vec-base-chinese`æ¨¡å‹ï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`hfl/chinese-macbert-base`åœ¨ä¸­æ–‡STS-Bæ•°æ®è®­ç»ƒå¾—åˆ°ï¼Œå¹¶åœ¨ä¸­æ–‡STS-Bæµ‹è¯•é›†è¯„ä¼°è¾¾åˆ°è¾ƒå¥½æ•ˆæœï¼Œè¿è¡Œ[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹æ–‡ä»¶å·²ç»ä¸Šä¼ HF model hubï¼Œä¸­æ–‡é€šç”¨è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èä½¿ç”¨
         - `shibing624/text2vec-base-chinese-sentence`æ¨¡å‹ï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`nghuyong/ernie-3.0-base-zh`ç”¨äººå·¥æŒ‘é€‰åçš„ä¸­æ–‡STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)è®­ç»ƒå¾—åˆ°ï¼Œå¹¶åœ¨ä¸­æ–‡å„NLIæµ‹è¯•é›†è¯„ä¼°è¾¾åˆ°è¾ƒå¥½æ•ˆæœï¼Œè¿è¡Œ[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹æ–‡ä»¶å·²ç»ä¸Šä¼ HF model hubï¼Œä¸­æ–‡s2s(å¥å­vså¥å­)è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èä½¿ç”¨
         - `shibing624/text2vec-base-chinese-paraphrase`æ¨¡å‹ï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`nghuyong/ernie-3.0-base-zh`ç”¨äººå·¥æŒ‘é€‰åçš„ä¸­æ–‡STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-chinese-paraphrase-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-paraphrase-dataset)ï¼Œæ•°æ®é›†ç›¸å¯¹äº[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)åŠ å…¥äº†s2p(sentence to paraphrase)æ•°æ®ï¼Œå¼ºåŒ–äº†å…¶é•¿æ–‡æœ¬çš„è¡¨å¾èƒ½åŠ›ï¼Œå¹¶åœ¨ä¸­æ–‡å„NLIæµ‹è¯•é›†è¯„ä¼°è¾¾åˆ°SOTAï¼Œè¿è¡Œ[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹æ–‡ä»¶å·²ç»ä¸Šä¼ HF model hubï¼Œä¸­æ–‡s2p(å¥å­vsæ®µè½)è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èä½¿ç”¨
-        - `SBERT-macbert-base`æ¨¡å‹ï¼Œæ˜¯ç”¨SBERTæ–¹æ³•è®­ç»ƒï¼Œè¿è¡Œ[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹
-        - `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`æ¨¡å‹æ˜¯ç”¨SBERTè®­ç»ƒï¼Œæ˜¯`paraphrase-MiniLM-L12-v2`æ¨¡å‹çš„å¤šè¯­è¨€ç‰ˆæœ¬ï¼Œæ”¯æŒä¸­æ–‡ã€è‹±æ–‡ç­‰
+        - `shibing624/text2vec-base-multilingual`æ¨¡å‹ï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`ç”¨äººå·¥æŒ‘é€‰åçš„å¤šè¯­è¨€STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-multilingual-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-multilingual-dataset)è®­ç»ƒå¾—åˆ°ï¼Œå¹¶åœ¨ä¸­è‹±æ–‡æµ‹è¯•é›†è¯„ä¼°ç›¸å¯¹äºåŸæ¨¡å‹æ•ˆæœæœ‰æå‡ï¼Œè¿è¡Œ[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹æ–‡ä»¶å·²ç»ä¸Šä¼ HF model hubï¼Œå¤šè¯­è¨€è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èä½¿ç”¨
         - `w2v-light-tencent-chinese`æ˜¯è…¾è®¯è¯å‘é‡çš„Word2Vecæ¨¡å‹ï¼ŒCPUåŠ è½½ä½¿ç”¨ï¼Œé€‚ç”¨äºä¸­æ–‡å­—é¢åŒ¹é…ä»»åŠ¡å’Œç¼ºå°‘æ•°æ®çš„å†·å¯åŠ¨æƒ…å†µ
         - å„é¢„è®­ç»ƒæ¨¡å‹å‡å¯ä»¥é€šè¿‡transformersè°ƒç”¨ï¼Œå¦‚MacBERTæ¨¡å‹ï¼š`--model_name hfl/chinese-macbert-base` æˆ–è€…robertaæ¨¡å‹ï¼š`--model_name uer/roberta-medium-wwm-chinese-cluecorpussmall`
         - ä¸ºæµ‹è¯„æ¨¡å‹çš„é²æ£’æ€§ï¼ŒåŠ å…¥äº†æœªè®­ç»ƒè¿‡çš„SOHUæµ‹è¯•é›†ï¼Œç”¨äºæµ‹è¯•æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼›ä¸ºè¾¾åˆ°å¼€ç®±å³ç”¨çš„å®ç”¨æ•ˆæœï¼Œä½¿ç”¨äº†æœé›†åˆ°çš„å„ä¸­æ–‡åŒ¹é…æ•°æ®é›†ï¼Œæ•°æ®é›†ä¹Ÿä¸Šä¼ åˆ°HF datasets[é“¾æ¥è§ä¸‹æ–¹](#æ•°æ®é›†)
         - ä¸­æ–‡åŒ¹é…ä»»åŠ¡å®éªŒè¡¨æ˜ï¼Œpoolingæœ€ä¼˜æ˜¯`EncoderType.FIRST_LAST_AVG`å’Œ`EncoderType.MEAN`ï¼Œä¸¤è€…é¢„æµ‹æ•ˆæœå·®å¼‚å¾ˆå°
         - ä¸­æ–‡åŒ¹é…è¯„æµ‹ç»“æœå¤ç°ï¼Œå¯ä»¥ä¸‹è½½ä¸­æ–‡åŒ¹é…æ•°æ®é›†åˆ°`examples/data`ï¼Œè¿è¡Œ[tests/test_model_spearman.py](https://github.com/shibing624/text2vec/blob/master/tests/test_model_spearman.py)ä»£ç å¤ç°è¯„æµ‹ç»“æœ
         - QPSçš„GPUæµ‹è¯•ç¯å¢ƒæ˜¯Tesla V100ï¼Œæ˜¾å­˜32GB
         
-        # Demo
+        æ¨¡å‹è®­ç»ƒå®éªŒæŠ¥å‘Šï¼š[å®éªŒæŠ¥å‘Š](https://github.com/shibing624/text2vec/blob/master/docs/model_report.md)
+        ## Demo
         
         Official Demo: https://www.mulanai.com/product/short_text_sim/
         
         HuggingFace Demo: https://huggingface.co/spaces/shibing624/text2vec
         
         ![](docs/hf.png)
         
         run example: [examples/gradio_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/gradio_demo.py) to see the demo:
         ```shell
         python examples/gradio_demo.py
         ```
         
-        # Install
+        ## Install
         ```shell
         pip install torch # conda install pytorch
         pip install -U text2vec
         ```
         
         or
         
@@ -144,17 +148,17 @@
         pip install -r requirements.txt
         
         git clone https://github.com/shibing624/text2vec.git
         cd text2vec
         pip install --no-deps .
         ```
         
-        # Usage
+        ## Usage
         
-        ## æ–‡æœ¬å‘é‡è¡¨å¾
+        ### æ–‡æœ¬å‘é‡è¡¨å¾
         
         åŸºäº`pretrained model`è®¡ç®—æ–‡æœ¬å‘é‡ï¼š
         
         ```zsh
         >>> from text2vec import SentenceModel
         >>> m = SentenceModel()
         >>> m.encode("å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡")
@@ -194,16 +198,16 @@
         
         
         if __name__ == "__main__":
             # ä¸­æ–‡å¥å‘é‡æ¨¡å‹(CoSENT)ï¼Œä¸­æ–‡è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èï¼Œæ”¯æŒfine-tuneç»§ç»­è®­ç»ƒ
             t2v_model = SentenceModel("shibing624/text2vec-base-chinese")
             compute_emb(t2v_model)
         
-            # æ”¯æŒå¤šè¯­è¨€çš„å¥å‘é‡æ¨¡å‹ï¼ˆSentence-BERTï¼‰ï¼Œè‹±æ–‡è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èï¼Œæ”¯æŒfine-tuneç»§ç»­è®­ç»ƒ
-            sbert_model = SentenceModel("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
+            # æ”¯æŒå¤šè¯­è¨€çš„å¥å‘é‡æ¨¡å‹ï¼ˆCoSENTï¼‰ï¼Œå¤šè¯­è¨€ï¼ˆåŒ…æ‹¬ä¸­è‹±æ–‡ï¼‰è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èï¼Œæ”¯æŒfine-tuneç»§ç»­è®­ç»ƒ
+            sbert_model = SentenceModel("shibing624/text2vec-base-multilingual")
             compute_emb(sbert_model)
         
             # ä¸­æ–‡è¯å‘é‡æ¨¡å‹(word2vec)ï¼Œä¸­æ–‡å­—é¢åŒ¹é…ä»»åŠ¡å’Œå†·å¯åŠ¨é€‚ç”¨
             w2v_model = Word2Vec("w2v-light-tencent-chinese")
             compute_emb(w2v_model)
         
         ```
@@ -220,16 +224,14 @@
         ```
         
         - è¿”å›å€¼`embeddings`æ˜¯`numpy.ndarray`ç±»å‹ï¼Œshapeä¸º`(sentences_size, model_embedding_size)`ï¼Œä¸‰ä¸ªæ¨¡å‹ä»»é€‰ä¸€ç§å³å¯ï¼Œæ¨èç”¨ç¬¬ä¸€ä¸ªã€‚
         - `shibing624/text2vec-base-chinese`æ¨¡å‹æ˜¯CoSENTæ–¹æ³•åœ¨ä¸­æ–‡STS-Bæ•°æ®é›†è®­ç»ƒå¾—åˆ°çš„ï¼Œæ¨¡å‹å·²ç»ä¸Šä¼ åˆ°huggingfaceçš„
         æ¨¡å‹åº“[shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)ï¼Œ
         æ˜¯`text2vec.SentenceModel`æŒ‡å®šçš„é»˜è®¤æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡ä¸Šé¢ç¤ºä¾‹è°ƒç”¨ï¼Œæˆ–è€…å¦‚ä¸‹æ‰€ç¤ºç”¨[transformersåº“](https://github.com/huggingface/transformers)è°ƒç”¨ï¼Œ
         æ¨¡å‹è‡ªåŠ¨ä¸‹è½½åˆ°æœ¬æœºè·¯å¾„ï¼š`~/.cache/huggingface/transformers`
-        - `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`æ¨¡å‹æ˜¯Sentence-BERTçš„å¤šè¯­è¨€å¥å‘é‡æ¨¡å‹ï¼Œ
-        é€‚ç”¨äºé‡Šä¹‰ï¼ˆparaphraseï¼‰è¯†åˆ«ï¼Œæ–‡æœ¬åŒ¹é…ï¼Œé€šè¿‡`text2vec.SentenceModel`å’Œ[sentence-transformersåº“]((https://github.com/UKPLab/sentence-transformers))éƒ½å¯ä»¥è°ƒç”¨è¯¥æ¨¡å‹
         - `w2v-light-tencent-chinese`æ˜¯é€šè¿‡gensimåŠ è½½çš„Word2Vecæ¨¡å‹ï¼Œä½¿ç”¨è…¾è®¯è¯å‘é‡`Tencent_AILab_ChineseEmbedding.tar.gz`è®¡ç®—å„å­—è¯çš„è¯å‘é‡ï¼Œå¥å­å‘é‡é€šè¿‡å•è¯è¯
         å‘é‡å–å¹³å‡å€¼å¾—åˆ°ï¼Œæ¨¡å‹è‡ªåŠ¨ä¸‹è½½åˆ°æœ¬æœºè·¯å¾„ï¼š`~/.text2vec/datasets/light_Tencent_AILab_ChineseEmbedding.bin`
         
         #### Usage (HuggingFace Transformers)
         Without [text2vec](https://github.com/shibing624/text2vec), you can use the model like this: 
         
         First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.
@@ -447,17 +449,17 @@
         from similarities import Similarity
         
         m = Similarity()
         r = m.similarity('å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡', 'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡')
         print(f"similarity score: {float(r)}")  # similarity score: 0.855146050453186
         ```
         
-        # Models
+        ## Models
         
-        ## CoSENT model
+        ### CoSENT model
         
         CoSENTï¼ˆCosine Sentenceï¼‰æ–‡æœ¬åŒ¹é…æ¨¡å‹ï¼Œåœ¨Sentence-BERTä¸Šæ”¹è¿›äº†CosineRankLossçš„å¥å‘é‡æ–¹æ¡ˆ
         
         
         Network structure:
         
         Training:
@@ -488,16 +490,22 @@
         python training_sup_text_matching_model.py --task_name ATEC --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base --output_dir ./outputs/ATEC-cosent
         ```
         
         - åœ¨è‡ªæœ‰ä¸­æ–‡æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹
         
         example: [examples/training_sup_text_matching_model_mydata.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_mydata.py)
         
+        å•å¡è®­ç»ƒï¼š
+        ```shell
+        CUDA_VISIBLE_DEVICES=0 python training_sup_text_matching_model_mydata.py --do_train --do_predict
+        ```
+        
+        å¤šå¡è®­ç»ƒï¼š
         ```shell
-        python training_sup_text_matching_model_mydata.py --do_train --do_predict
+        CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node 2  training_sup_text_matching_model_mydata.py --do_train --do_predict --output_dir outputs/STS-B-text2vec-macbert-v1 --batch_size 64 --fp16 --data_parallel 
         ```
         
         è®­ç»ƒé›†æ ¼å¼å‚è€ƒ[examples/data/STS-B/STS-B.valid.data](https://github.com/shibing624/text2vec/blob/master/examples/data/STS-B/STS-B.valid.data)
         
         ```shell
         sentence1   sentence2   label
         ä¸€ä¸ªå¥³å­©åœ¨ç»™å¥¹çš„å¤´å‘åšå‘å‹ã€‚	ä¸€ä¸ªå¥³å­©åœ¨æ¢³å¤´ã€‚	2
@@ -524,15 +532,15 @@
         
         ```shell
         cd examples
         python training_unsup_text_matching_model_en.py --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-cosent
         ```
         
         
-        ## Sentence-BERT model
+        ### Sentence-BERT model
         
         Sentence-BERTæ–‡æœ¬åŒ¹é…æ¨¡å‹ï¼Œè¡¨å¾å¼å¥å‘é‡è¡¨ç¤ºæ–¹æ¡ˆ
         
         Network structure:
         
         Training:
         
@@ -567,38 +575,38 @@
         example: [examples/training_unsup_text_matching_model_en.py](https://github.com/shibing624/text2vec/blob/master/examples/training_unsup_text_matching_model_en.py)
         
         ```shell
         cd examples
         python training_unsup_text_matching_model_en.py --model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-sbert
         ```
         
-        ## BERT-Match model
+        ### BERT-Match model
         BERTæ–‡æœ¬åŒ¹é…æ¨¡å‹ï¼ŒåŸç”ŸBERTåŒ¹é…ç½‘ç»œç»“æ„ï¼Œäº¤äº’å¼å¥å‘é‡åŒ¹é…æ¨¡å‹
         
         Network structure:
         
         Training and inference:
         
         <img src="docs/bert-fc-train.png" width="300" />
         
         è®­ç»ƒè„šæœ¬åŒä¸Š[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)ã€‚
         
         
-        ## æ¨¡å‹è’¸é¦ï¼ˆModel Distillationï¼‰
+        ### æ¨¡å‹è’¸é¦ï¼ˆModel Distillationï¼‰
         
         ç”±äºtext2vecè®­ç»ƒçš„æ¨¡å‹å¯ä»¥ä½¿ç”¨[sentence-transformers](https://github.com/UKPLab/sentence-transformers)åº“åŠ è½½ï¼Œæ­¤å¤„å¤ç”¨å…¶æ¨¡å‹è’¸é¦æ–¹æ³•[distillation](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/distillation)ã€‚
         
         1. æ¨¡å‹é™ç»´ï¼Œå‚è€ƒ[dimensionality_reduction.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/distillation/dimensionality_reduction.py)ä½¿ç”¨PCAå¯¹æ¨¡å‹è¾“å‡ºembeddingé™ç»´ï¼Œå¯å‡å°‘milvusç­‰å‘é‡æ£€ç´¢æ•°æ®åº“çš„å­˜å‚¨å‹åŠ›ï¼Œè¿˜èƒ½è½»å¾®æå‡æ¨¡å‹æ•ˆæœã€‚
         2. æ¨¡å‹è’¸é¦ï¼Œå‚è€ƒ[model_distillation.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/distillation/model_distillation.py)ä½¿ç”¨è’¸é¦æ–¹æ³•ï¼Œå°†Teacherå¤§æ¨¡å‹è’¸é¦åˆ°æ›´å°‘layerså±‚æ•°çš„studentæ¨¡å‹ä¸­ï¼Œåœ¨æƒè¡¡æ•ˆæœçš„æƒ…å†µä¸‹ï¼Œå¯å¤§å¹…æå‡æ¨¡å‹é¢„æµ‹é€Ÿåº¦ã€‚
         
-        ## æ¨¡å‹éƒ¨ç½²
+        ### æ¨¡å‹éƒ¨ç½²
         
         æä¾›ä¸¤ç§éƒ¨ç½²æ¨¡å‹ï¼Œæ­å»ºæœåŠ¡çš„æ–¹æ³•ï¼š 1ï¼‰åŸºäºJinaæ­å»ºgRPCæœåŠ¡ã€æ¨èã€‘ï¼›2ï¼‰åŸºäºFastAPIæ­å»ºåŸç”ŸHttpæœåŠ¡ã€‚
         
-        ### JinaæœåŠ¡
+        #### JinaæœåŠ¡
         é‡‡ç”¨C/Sæ¨¡å¼æ­å»ºé«˜æ€§èƒ½æœåŠ¡ï¼Œæ”¯æŒdockeräº‘åŸç”Ÿï¼ŒgRPC/HTTP/WebSocketï¼Œæ”¯æŒå¤šä¸ªæ¨¡å‹åŒæ—¶é¢„æµ‹ï¼ŒGPUå¤šå¡å¤„ç†ã€‚
         
         - å®‰è£…ï¼š
         ```pip install jina```
         
         - å¯åŠ¨æœåŠ¡ï¼š
         
@@ -637,15 +645,15 @@
         r = c.post('/', inputs=DocumentArray([Document(text='å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡'), Document(text='èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡')]))
         print(r.embeddings)
         ```
         
         æ‰¹é‡è°ƒç”¨æ–¹æ³•è§example: [examples/jina_client_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/jina_client_demo.py)
         
         
-        ### FastAPIæœåŠ¡
+        #### FastAPIæœåŠ¡
         
         - å®‰è£…ï¼š
         ```pip install fastapi uvicorn```
         
         - å¯åŠ¨æœåŠ¡ï¼š
         
         example: [examples/fastapi_server_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/fastapi_server_demo.py)
@@ -658,15 +666,15 @@
         ```shell
         curl -X 'GET' \
           'http://0.0.0.0:8001/emb?q=hello' \
           -H 'accept: application/json'
         ```
         
         
-        ## æ•°æ®é›†
+        ## Dataset
         
         - æœ¬é¡¹ç›®releaseçš„æ•°æ®é›†ï¼š
         
         | Dataset                    | Introduce                                                                | Download Link                                                                                                                                                                                                                                                                                         |
         |:---------------------------|:-------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
         | shibing624/nli-zh-all      | ä¸­æ–‡è¯­ä¹‰åŒ¹é…æ•°æ®åˆé›†ï¼Œæ•´åˆäº†æ–‡æœ¬æ¨ç†ï¼Œç›¸ä¼¼ï¼Œæ‘˜è¦ï¼Œé—®ç­”ï¼ŒæŒ‡ä»¤å¾®è°ƒç­‰ä»»åŠ¡çš„820ä¸‡é«˜è´¨é‡æ•°æ®ï¼Œå¹¶è½¬åŒ–ä¸ºåŒ¹é…æ ¼å¼æ•°æ®é›†                | [https://huggingface.co/datasets/shibing624/nli-zh-all](https://huggingface.co/datasets/shibing624/nli-zh-all)                                                                                                                                                                                        |
         | shibing624/snli-zh         | ä¸­æ–‡SNLIå’ŒMultiNLIæ•°æ®é›†ï¼Œç¿»è¯‘è‡ªè‹±æ–‡SNLIå’ŒMultiNLI                                    | [https://huggingface.co/datasets/shibing624/snli-zh](https://huggingface.co/datasets/shibing624/snli-zh)                                                                                                                                                                                              |
@@ -677,16 +685,16 @@
         | LCQMC                      | ä¸­æ–‡LCQMC(large-scale Chinese question matching corpus)æ•°æ®é›†ï¼ŒQ-Qpairæ•°æ®é›†      | [LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html)                                                                                                                                                                                                                                               |
         | PAWSX                      | ä¸­æ–‡PAWS(Paraphrase Adversaries from Word Scrambling)æ•°æ®é›†ï¼ŒQ-Qpairæ•°æ®é›†        | [PAWSX](https://arxiv.org/abs/1908.11828)                                                                                                                                                                                                                                                             |
         | STS-B                      | ä¸­æ–‡STS-Bæ•°æ®é›†ï¼Œä¸­æ–‡è‡ªç„¶è¯­è¨€æ¨ç†æ•°æ®é›†ï¼Œä»è‹±æ–‡STS-Bç¿»è¯‘ä¸ºä¸­æ–‡çš„æ•°æ®é›†                                 | [STS-B](https://github.com/pluto-junzeng/CNSD)                                                                                                                                                                                                                                                        |
         
         
         å¸¸ç”¨è‹±æ–‡åŒ¹é…æ•°æ®é›†ï¼š
         
-        - å¤§åé¼é¼çš„multi_nliå’Œsnli: https://huggingface.co/datasets/multi_nli
-        - å¤§åé¼é¼çš„multi_nliå’Œsnli: https://huggingface.co/datasets/snli
+        - è‹±æ–‡åŒ¹é…æ•°æ®é›†ï¼šmulti_nli: https://huggingface.co/datasets/multi_nli
+        - è‹±æ–‡åŒ¹é…æ•°æ®é›†ï¼šsnli: https://huggingface.co/datasets/snli
         - https://huggingface.co/datasets/metaeval/cnli
         - https://huggingface.co/datasets/mteb/stsbenchmark-sts
         - https://huggingface.co/datasets/JeremiahZ/simcse_sup_nli
         - https://huggingface.co/datasets/MoritzLaurer/multilingual-NLI-26lang-2mil7
         
         
         æ•°æ®é›†ä½¿ç”¨ç¤ºä¾‹ï¼š
@@ -721,59 +729,59 @@
         {'sentence1': 'ä¸€ä¸ªå¥³å­©åœ¨ç»™å¥¹çš„å¤´å‘åšå‘å‹ã€‚', 'sentence2': 'ä¸€ä¸ªå¥³å­©åœ¨æ¢³å¤´ã€‚', 'label': 2}
         ```
         
         
         
         
         
-        # Contact
+        ## Contact
         
         - Issue(å»ºè®®)ï¼š[![GitHub issues](https://img.shields.io/github/issues/shibing624/text2vec.svg)](https://github.com/shibing624/text2vec/issues)
         - é‚®ä»¶æˆ‘ï¼šxuming: xuming624@qq.com
         - å¾®ä¿¡æˆ‘ï¼šåŠ æˆ‘*å¾®ä¿¡å·ï¼šxuming624, å¤‡æ³¨ï¼šå§“å-å…¬å¸-NLP* è¿›NLPäº¤æµç¾¤ã€‚
         
         <img src="docs/wechat.jpeg" width="200" />
         
         
-        # Citation
+        ## Citation
         
         å¦‚æœä½ åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†text2vecï¼Œè¯·æŒ‰å¦‚ä¸‹æ ¼å¼å¼•ç”¨ï¼š
         
         APA:
         ```latex
         Xu, M. Text2vec: Text to vector toolkit (Version 1.1.2) [Computer software]. https://github.com/shibing624/text2vec
         ```
         
         BibTeX:
         ```latex
         @misc{Text2vec,
-          author = {Xu, Ming},
+          author = {Ming Xu},
           title = {Text2vec: Text to vector toolkit},
-          year = {2022},
+          year = {2023},
           publisher = {GitHub},
           journal = {GitHub repository},
           howpublished = {\url{https://github.com/shibing624/text2vec}},
         }
         ```
         
-        # License
+        ## License
         
         
         æˆæƒåè®®ä¸º [The Apache License 2.0](LICENSE)ï¼Œå¯å…è´¹ç”¨åšå•†ä¸šç”¨é€”ã€‚è¯·åœ¨äº§å“è¯´æ˜ä¸­é™„åŠ text2vecçš„é“¾æ¥å’Œæˆæƒåè®®ã€‚
         
         
-        # Contribute
+        ## Contribute
         é¡¹ç›®ä»£ç è¿˜å¾ˆç²—ç³™ï¼Œå¦‚æœå¤§å®¶å¯¹ä»£ç æœ‰æ‰€æ”¹è¿›ï¼Œæ¬¢è¿æäº¤å›æœ¬é¡¹ç›®ï¼Œåœ¨æäº¤ä¹‹å‰ï¼Œæ³¨æ„ä»¥ä¸‹ä¸¤ç‚¹ï¼š
         
          - åœ¨`tests`æ·»åŠ ç›¸åº”çš„å•å…ƒæµ‹è¯•
          - ä½¿ç”¨`python -m pytest -v`æ¥è¿è¡Œæ‰€æœ‰å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿æ‰€æœ‰å•æµ‹éƒ½æ˜¯é€šè¿‡çš„
         
         ä¹‹åå³å¯æäº¤PRã€‚
         
-        # Reference
+        ## References
         - [å°†å¥å­è¡¨ç¤ºä¸ºå‘é‡ï¼ˆä¸Šï¼‰ï¼šæ— ç›‘ç£å¥å­è¡¨ç¤ºå­¦ä¹ ï¼ˆsentence embeddingï¼‰](https://www.cnblogs.com/llhthinker/p/10335164.html)
         - [å°†å¥å­è¡¨ç¤ºä¸ºå‘é‡ï¼ˆä¸‹ï¼‰ï¼šæ— ç›‘ç£å¥å­è¡¨ç¤ºå­¦ä¹ ï¼ˆsentence embeddingï¼‰](https://www.cnblogs.com/llhthinker/p/10341841.html)
         - [A Simple but Tough-to-Beat Baseline for Sentence Embeddings[Sanjeev Arora and Yingyu Liang and Tengyu Ma, 2017]](https://openreview.net/forum?id=SyK00v5xx)
         - [å››ç§è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦çš„æ–¹æ³•å¯¹æ¯”[Yves Peirsman]](https://zhuanlan.zhihu.com/p/37104535)
         - [Improvements to BM25 and Language Models Examined](http://www.cs.otago.ac.nz/homepages/andrew/papers/2014-2.pdf)
         - [CoSENTï¼šæ¯”Sentence-BERTæ›´æœ‰æ•ˆçš„å¥å‘é‡æ–¹æ¡ˆ](https://kexue.fm/archives/8847)
         - [è°ˆè°ˆæ–‡æœ¬åŒ¹é…å’Œå¤šè½®æ£€ç´¢](https://zhuanlan.zhihu.com/p/111769969)
```

#### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: text2vec Version: 1.2.1 Summary: Text to vector
+Metadata-Version: 2.1 Name: text2vec Version: 1.2.2 Summary: Text to vector
 Tool, encode text Home-page: https://github.com/shibing624/text2vec Author:
 XuMing Author-email: xuming624@qq.com License: Apache License 2.0 Description:
 [**Ã°ÂŸÂ‡Â¨Ã°ÂŸÂ‡Â³Ã¤Â¸Â­Ã¦Â–Â‡**](https://github.com/shibing624/text2vec/blob/master/
 README.md) | [**Ã°ÂŸÂŒÂEnglish**](https://github.com/shibing624/text2vec/blob/
 master/README_EN.md) | [**Ã°ÂŸÂ“Â–Ã¦Â–Â‡Ã¦Â¡Â£/Docs**](https://github.com/shibing624/
 text2vec/wiki) | [**Ã°ÂŸÂ¤Â–Ã¦Â¨Â¡Ã¥ÂÂ‹/Models**](https://huggingface.co/shibing624)
                                     [Logo]
@@ -17,17 +17,26 @@
 shibing624/text2vec.svg)](https://github.com/shibing624/text2vec/issues) [!
 [Wechat Group](http://vlog.sfyc.ltd/wechat_everyday/
 wxgroup_logo.png?imageView2/0/w/60/h/20)](#Contact) **Text2vec**: Text to
 Vector, Get Sentence Embeddings. Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂÂ‘Ã©Â‡ÂÃ¥ÂŒÂ–Ã¯Â¼ÂŒÃ¦ÂŠÂŠÃ¦Â–Â‡Ã¦ÂœÂ¬
 (Ã¥ÂŒÂ…Ã¦Â‹Â¬Ã¨Â¯ÂÃ£Â€ÂÃ¥ÂÂ¥Ã¥Â­ÂÃ£Â€ÂÃ¦Â®ÂµÃ¨ÂÂ½)Ã¨Â¡Â¨Ã¥Â¾ÂÃ¤Â¸ÂºÃ¥ÂÂ‘Ã©Â‡ÂÃ§ÂŸÂ©Ã©Â˜ÂµÃ£Â€Â‚
 **text2vec**Ã¥Â®ÂÃ§ÂÂ°Ã¤ÂºÂ†Word2VecÃ£Â€ÂRankBM25Ã£Â€ÂBERTÃ£Â€ÂSentence-
 BERTÃ£Â€ÂCoSENTÃ§Â­Â‰Ã¥Â¤ÂšÃ§Â§ÂÃ¦Â–Â‡Ã¦ÂœÂ¬Ã¨Â¡Â¨Ã¥Â¾ÂÃ£Â€ÂÃ¦Â–Â‡Ã¦ÂœÂ¬Ã§Â›Â¸Ã¤Â¼Â¼Ã¥ÂºÂ¦Ã¨Â®Â¡Ã§Â®Â—Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¥Â¹Â¶Ã¥ÂœÂ¨Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¯Â¼ÂˆÃ§Â›Â¸Ã¤Â¼Â¼Ã¥ÂºÂ¦Ã¨Â®Â¡Ã§Â®Â—Ã¯Â¼Â‰Ã¤Â»Â»Ã¥ÂŠÂ¡Ã¤Â¸ÂŠÃ¦Â¯Â”Ã¨Â¾ÂƒÃ¤ÂºÂ†Ã¥ÂÂ„Ã¦Â¨Â¡Ã¥ÂÂ‹Ã§ÂšÂ„Ã¦Â•ÂˆÃ¦ÂÂœÃ£Â€Â‚
-### News [2023/06/19] v1.2.1Ã§Â‰ÂˆÃ¦ÂœÂ¬: Ã¦Â›Â´Ã¦Â–Â°Ã¤ÂºÂ†Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹`shibing624/
-text2vec-base-chinese-nli`Ã¤Â¸ÂºÃ¦Â–Â°Ã§Â‰Âˆ[shibing624/text2vec-base-chinese-sentence]
-(https://huggingface.co/shibing624/text2vec-base-chinese-
+### News [2023/06/22] v1.2.2Ã§Â‰ÂˆÃ¦ÂœÂ¬: Ã¥ÂÂ‘Ã¥Â¸ÂƒÃ¤ÂºÂ†Ã¥Â¤ÂšÃ¨Â¯Â­Ã¨Â¨Â€Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹[shibing624/
+text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-
+multilingual)Ã¯Â¼ÂŒÃ§Â”Â¨CoSENTÃ¦Â–Â¹Ã¦Â³Â•Ã¨Â®Â­Ã§Â»ÂƒÃ¯Â¼ÂŒÃ¥ÂŸÂºÃ¤ÂºÂ`sentence-transformers/
+paraphrase-multilingual-MiniLM-L12-
+v2`Ã§Â”Â¨Ã¤ÂºÂºÃ¥Â·Â¥Ã¦ÂŒÂ‘Ã©Â€Â‰Ã¥ÂÂÃ§ÂšÂ„Ã¥Â¤ÂšÃ¨Â¯Â­Ã¨Â¨Â€STSÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†[shibing624/nli-zh-all/text2vec-
+base-multilingual-dataset](https://huggingface.co/datasets/shibing624/nli-zh-
+all/tree/main/text2vec-base-multilingual-
+dataset)Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â¾Â—Ã¥ÂˆÂ°Ã¯Â¼ÂŒÃ¥Â¹Â¶Ã¥ÂœÂ¨Ã¤Â¸Â­Ã¨Â‹Â±Ã¦Â–Â‡Ã¦ÂµÂ‹Ã¨Â¯Â•Ã©Â›Â†Ã¨Â¯Â„Ã¤Â¼Â°Ã§Â›Â¸Ã¥Â¯Â¹Ã¤ÂºÂÃ¥ÂÂŸÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â•ÂˆÃ¦ÂÂœÃ¦ÂœÂ‰Ã¦ÂÂÃ¥ÂÂ‡Ã¯Â¼ÂŒÃ¨Â¯Â¦Ã¨Â§Â
+[Release-v1.2.2](https://github.com/shibing624/text2vec/releases/tag/1.2.2)
+[2023/06/19] v1.2.1Ã§Â‰ÂˆÃ¦ÂœÂ¬: Ã¦Â›Â´Ã¦Â–Â°Ã¤ÂºÂ†Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹`shibing624/text2vec-
+base-chinese-nli`Ã¤Â¸ÂºÃ¦Â–Â°Ã§Â‰Âˆ[shibing624/text2vec-base-chinese-sentence](https://
+huggingface.co/shibing624/text2vec-base-chinese-
 sentence)Ã¯Â¼ÂŒÃ©Â’ÂˆÃ¥Â¯Â¹CoSENTÃ§ÂšÂ„lossÃ¨Â®Â¡Ã§Â®Â—Ã¥Â¯Â¹Ã¦ÂÂ’Ã¥ÂºÂÃ¦Â•ÂÃ¦Â„ÂŸÃ§Â‰Â¹Ã§Â‚Â¹Ã¯Â¼ÂŒÃ¤ÂºÂºÃ¥Â·Â¥Ã¦ÂŒÂ‘Ã©Â€Â‰Ã¥Â¹Â¶Ã¦Â•Â´Ã§ÂÂ†Ã¥Â‡ÂºÃ©Â«Â˜Ã¨Â´Â¨Ã©Â‡ÂÃ§ÂšÂ„Ã¦ÂœÂ‰Ã§Â›Â¸Ã¥Â…Â³Ã¦Â€Â§Ã¦ÂÂ’Ã¥ÂºÂÃ§ÂšÂ„STSÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†
 [shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://
 huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-
 sentence-
 dataset)Ã¯Â¼ÂŒÃ¥ÂœÂ¨Ã¥ÂÂ„Ã¨Â¯Â„Ã¤Â¼Â°Ã©Â›Â†Ã¨Â¡Â¨Ã§ÂÂ°Ã§Â›Â¸Ã¥Â¯Â¹Ã¤Â¹Â‹Ã¥Â‰ÂÃ¦ÂœÂ‰Ã¦ÂÂÃ¥ÂÂ‡Ã¯Â¼Â›Ã¥ÂÂ‘Ã¥Â¸ÂƒÃ¤ÂºÂ†Ã©Â€Â‚Ã§Â”Â¨Ã¤ÂºÂs2pÃ§ÂšÂ„Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹
 [shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/
 shibing624/text2vec-base-chinese-paraphrase)Ã¯Â¼ÂŒÃ¨Â¯Â¦Ã¨Â§Â[Release-v1.2.1](https://
@@ -38,17 +47,17 @@
 huggingface.co/datasets/shibing624/
 nli_zh)Ã¥Â…Â¨Ã©ÂƒÂ¨Ã¨Â¯Â­Ã¦Â–Â™Ã¨Â®Â­Ã§Â»ÂƒÃ§ÂšÂ„CoSENTÃ¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¥ÂœÂ¨Ã¥ÂÂ„Ã¨Â¯Â„Ã¤Â¼Â°Ã©Â›Â†Ã¨Â¡Â¨Ã§ÂÂ°Ã¦ÂÂÃ¥ÂÂ‡Ã¦Â˜ÂÃ¦Â˜Â¾Ã¯Â¼ÂŒÃ¨Â¯Â¦Ã¨Â§Â
 [Release-v1.2.0](https://github.com/shibing624/text2vec/releases/tag/1.2.0)
 [2022/03/12] v1.1.4Ã§Â‰ÂˆÃ¦ÂœÂ¬: Ã¥ÂÂ‘Ã¥Â¸ÂƒÃ¤ÂºÂ†Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹[shibing624/text2vec-
 base-chinese](https://huggingface.co/shibing624/text2vec-base-
 chinese)Ã¯Â¼ÂŒÃ¥ÂŸÂºÃ¤ÂºÂÃ¤Â¸Â­Ã¦Â–Â‡STSÃ¨Â®Â­Ã§Â»ÂƒÃ©Â›Â†Ã¨Â®Â­Ã§Â»ÂƒÃ§ÂšÂ„CoSENTÃ¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹Ã£Â€Â‚Ã¨Â¯Â¦Ã¨Â§Â
 [Release-v1.1.4](https://github.com/shibing624/text2vec/releases/tag/1.1.4)
-**Guide** - [Feature](#Feature) - [Evaluation](#Evaluation) - [Install]
-(#install) - [Usage](#usage) - [Contact](#Contact) - [Reference](#reference) #
-Feature ### Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂÂ‘Ã©Â‡ÂÃ¨Â¡Â¨Ã§Â¤ÂºÃ¦Â¨Â¡Ã¥ÂÂ‹ - [Word2Vec](https://github.com/
+**Guide** - [Features](#Features) - [Evaluation](#Evaluation) - [Install]
+(#install) - [Usage](#usage) - [Contact](#Contact) - [References](#references)
+## Features ### Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂÂ‘Ã©Â‡ÂÃ¨Â¡Â¨Ã§Â¤ÂºÃ¦Â¨Â¡Ã¥ÂÂ‹ - [Word2Vec](https://github.com/
 shibing624/text2vec/blob/master/text2vec/word2vec.py)Ã¯Â¼ÂšÃ©Â€ÂšÃ¨Â¿Â‡Ã¨Â…Â¾Ã¨Â®Â¯AI
 LabÃ¥Â¼Â€Ã¦ÂºÂÃ§ÂšÂ„Ã¥Â¤Â§Ã¨Â§Â„Ã¦Â¨Â¡Ã©Â«Â˜Ã¨Â´Â¨Ã©Â‡ÂÃ¤Â¸Â­Ã¦Â–Â‡
 [Ã¨Â¯ÂÃ¥ÂÂ‘Ã©Â‡ÂÃ¦Â•Â°Ã¦ÂÂ®Ã¯Â¼Âˆ800Ã¤Â¸Â‡Ã¤Â¸Â­Ã¦Â–Â‡Ã¨Â¯ÂÃ¨Â½Â»Ã©Â‡ÂÃ§Â‰ÂˆÃ¯Â¼Â‰](https://pan.baidu.com/s/
 1La4U4XNFe8s5BJqxPQpeiQ) (Ã¦Â–Â‡Ã¤Â»Â¶Ã¥ÂÂÃ¯Â¼Âšlight_Tencent_AILab_ChineseEmbedding.bin
 Ã¥Â¯Â†Ã§Â Â:
 taweÃ¯Â¼Â‰Ã¥Â®ÂÃ§ÂÂ°Ã¨Â¯ÂÃ¥ÂÂ‘Ã©Â‡ÂÃ¦Â£Â€Ã§Â´Â¢Ã¯Â¼ÂŒÃ¦ÂœÂ¬Ã©Â¡Â¹Ã§Â›Â®Ã¥Â®ÂÃ§ÂÂ°Ã¤ÂºÂ†Ã¥ÂÂ¥Ã¥Â­ÂÃ¯Â¼ÂˆÃ¨Â¯ÂÃ¥ÂÂ‘Ã©Â‡ÂÃ¦Â±Â‚Ã¥Â¹Â³Ã¥ÂÂ‡Ã¯Â¼Â‰Ã§ÂšÂ„word2vecÃ¥ÂÂ‘Ã©Â‡ÂÃ¨Â¡Â¨Ã§Â¤Âº
 - [SBERT(Sentence-BERT)](https://github.com/shibing624/text2vec/blob/master/
@@ -56,67 +65,76 @@
 sentencebert_model.py)Ã¯Â¼ÂšÃ¦ÂÂƒÃ¨Â¡Â¡Ã¦Â€Â§Ã¨ÂƒÂ½Ã¥Â’ÂŒÃ¦Â•ÂˆÃ§ÂÂ‡Ã§ÂšÂ„Ã¥ÂÂ¥Ã¥ÂÂ‘Ã©Â‡ÂÃ¨Â¡Â¨Ã§Â¤ÂºÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¨Â®Â­Ã§Â»ÂƒÃ¦Â—Â¶Ã©Â€ÂšÃ¨Â¿Â‡Ã¦ÂœÂ‰Ã§Â›Â‘Ã§ÂÂ£Ã¨Â®Â­Ã§Â»ÂƒÃ¤Â¸ÂŠÃ¥Â±Â‚Ã¥ÂˆÂ†Ã§Â±Â»Ã¥Â‡Â½Ã¦Â•Â°Ã¯Â¼ÂŒÃ¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂŒÂ¹Ã©Â…ÂÃ©Â¢Â„Ã¦ÂµÂ‹Ã¦Â—Â¶Ã§Â›Â´Ã¦ÂÂ¥Ã¥ÂÂ¥Ã¥Â­ÂÃ¥ÂÂ‘Ã©Â‡ÂÃ¥ÂÂšÃ¤Â½Â™Ã¥Â¼Â¦Ã¯Â¼ÂŒÃ¦ÂœÂ¬Ã©Â¡Â¹Ã§Â›Â®Ã¥ÂŸÂºÃ¤ÂºÂPyTorchÃ¥Â¤ÂÃ§ÂÂ°Ã¤ÂºÂ†Sentence-
 BERTÃ¦Â¨Â¡Ã¥ÂÂ‹Ã§ÂšÂ„Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â’ÂŒÃ©Â¢Â„Ã¦ÂµÂ‹ - [CoSENT(Cosine Sentence)](https://github.com/
 shibing624/text2vec/blob/master/text2vec/
 cosent_model.py)Ã¯Â¼ÂšCoSENTÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¦ÂÂÃ¥Â‡ÂºÃ¤ÂºÂ†Ã¤Â¸Â€Ã§Â§ÂÃ¦ÂÂ’Ã¥ÂºÂÃ§ÂšÂ„Ã¦ÂÂŸÃ¥Â¤Â±Ã¥Â‡Â½Ã¦Â•Â°Ã¯Â¼ÂŒÃ¤Â½Â¿Ã¨Â®Â­Ã§Â»ÂƒÃ¨Â¿Â‡Ã§Â¨Â‹Ã¦Â›Â´Ã¨Â´Â´Ã¨Â¿Â‘Ã©Â¢Â„Ã¦ÂµÂ‹Ã¯Â¼ÂŒÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â”Â¶Ã¦Â•Â›Ã©Â€ÂŸÃ¥ÂºÂ¦Ã¥Â’ÂŒÃ¦Â•ÂˆÃ¦ÂÂœÃ¦Â¯Â”Sentence-
 BERTÃ¦Â›Â´Ã¥Â¥Â½Ã¯Â¼ÂŒÃ¦ÂœÂ¬Ã©Â¡Â¹Ã§Â›Â®Ã¥ÂŸÂºÃ¤ÂºÂPyTorchÃ¥Â®ÂÃ§ÂÂ°Ã¤ÂºÂ†CoSENTÃ¦Â¨Â¡Ã¥ÂÂ‹Ã§ÂšÂ„Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â’ÂŒÃ©Â¢Â„Ã¦ÂµÂ‹
 Ã¨Â¯Â¦Ã§Â»Â†Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂÂ‘Ã©Â‡ÂÃ¨Â¡Â¨Ã§Â¤ÂºÃ¦Â–Â¹Ã¦Â³Â•Ã¨Â§Âwiki: [Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂÂ‘Ã©Â‡ÂÃ¨Â¡Â¨Ã§Â¤ÂºÃ¦Â–Â¹Ã¦Â³Â•](https://
 github.com/shibing624/text2vec/wiki/
-%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95) #
+%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95) ##
 Evaluation Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂŒÂ¹Ã©Â…Â #### Ã¨Â‹Â±Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã§ÂšÂ„Ã¨Â¯Â„Ã¦ÂµÂ‹Ã§Â»Â“Ã¦ÂÂœÃ¯Â¼Âš | Arch |
 BaseModel | Model | English-STS-B | |:-------|:--------------------------------
-----------------|:-------------------------------------|:-------------:| |
+----------------|:-------------------------------------------------------------
+--------------------------------------------------------|:-------------:| |
 GloVe | glove | Avg_word_embeddings_glove_6B_300d | 61.77 | | BERT | bert-base-
 uncased | BERT-base-cls | 20.29 | | BERT | bert-base-uncased | BERT-base-
 first_last_avg | 59.04 | | BERT | bert-base-uncased | BERT-base-first_last_avg-
 whiten(NLI) | 63.65 | | SBERT | sentence-transformers/bert-base-nli-mean-tokens
 | SBERT-base-nli-cls | 73.65 | | SBERT | sentence-transformers/bert-base-nli-
 mean-tokens | SBERT-base-nli-first_last_avg | 77.96 | | CoSENT | bert-base-
 uncased | CoSENT-base-first_last_avg | 69.93 | | CoSENT | sentence-
 transformers/bert-base-nli-mean-tokens | CoSENT-base-nli-first_last_avg | 79.68
-| #### Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã§ÂšÂ„Ã¨Â¯Â„Ã¦ÂµÂ‹Ã§Â»Â“Ã¦ÂÂœÃ¯Â¼Âš | Arch | BaseModel | Model |
-ATEC | BQ | LCQMC | PAWSX | STS-B | Avg | |:-------|:--------------------------
---|:--------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:| |
-SBERT | bert-base-chinese | SBERT-bert-base | 46.36 | 70.36 | 78.72 | 46.86 |
-66.41 | 61.74 | | SBERT | hfl/chinese-macbert-base | SBERT-macbert-base | 47.28
-| 68.63 | 79.42 | 55.59 | 64.82 | 63.15 | | SBERT | hfl/chinese-roberta-wwm-ext
-| SBERT-roberta-ext | 48.29 | 69.99 | 79.22 | 44.10 | 72.42 | 62.80 | | CoSENT
-| bert-base-chinese | CoSENT-bert-base | 49.74 | 72.38 | 78.69 | 60.00 | 79.27
-| 68.01 | | CoSENT | hfl/chinese-macbert-base | CoSENT-macbert-base | 50.39 |
-72.93 | 79.17 | 60.86 | 79.30 | 68.53 | | CoSENT | hfl/chinese-roberta-wwm-ext
-| CoSENT-roberta-ext | 50.81 | 71.45 | 79.31 | 61.56 | 79.96 | 68.61 |
-Ã¨Â¯Â´Ã¦Â˜ÂÃ¯Â¼Âš - Ã§Â»Â“Ã¦ÂÂœÃ¨Â¯Â„Ã¦ÂµÂ‹Ã¦ÂŒÂ‡Ã¦Â Â‡Ã¯Â¼ÂšspearmanÃ§Â³Â»Ã¦Â•Â° -
+| | CoSENT | sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 |
+[shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/
+text2vec-base-multilingual) | 80.12 | ####
+Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã§ÂšÂ„Ã¨Â¯Â„Ã¦ÂµÂ‹Ã§Â»Â“Ã¦ÂÂœÃ¯Â¼Âš | Arch | BaseModel | Model | ATEC | BQ
+| LCQMC | PAWSX | STS-B | Avg | |:-------|:----------------------------|:------
+--------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:| | SBERT | bert-
+base-chinese | SBERT-bert-base | 46.36 | 70.36 | 78.72 | 46.86 | 66.41 | 61.74
+| | SBERT | hfl/chinese-macbert-base | SBERT-macbert-base | 47.28 | 68.63 |
+79.42 | 55.59 | 64.82 | 63.15 | | SBERT | hfl/chinese-roberta-wwm-ext | SBERT-
+roberta-ext | 48.29 | 69.99 | 79.22 | 44.10 | 72.42 | 62.80 | | CoSENT | bert-
+base-chinese | CoSENT-bert-base | 49.74 | 72.38 | 78.69 | 60.00 | 79.27 | 68.01
+| | CoSENT | hfl/chinese-macbert-base | CoSENT-macbert-base | 50.39 | 72.93 |
+79.17 | 60.86 | 79.30 | 68.53 | | CoSENT | hfl/chinese-roberta-wwm-ext |
+CoSENT-roberta-ext | 50.81 | 71.45 | 79.31 | 61.56 | 79.96 | 68.61 | Ã¨Â¯Â´Ã¦Â˜ÂÃ¯Â¼Âš
+- Ã§Â»Â“Ã¦ÂÂœÃ¨Â¯Â„Ã¦ÂµÂ‹Ã¦ÂŒÂ‡Ã¦Â Â‡Ã¯Â¼ÂšspearmanÃ§Â³Â»Ã¦Â•Â° -
 Ã¤Â¸ÂºÃ¨Â¯Â„Ã¦ÂµÂ‹Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨ÂƒÂ½Ã¥ÂŠÂ›Ã¯Â¼ÂŒÃ§Â»Â“Ã¦ÂÂœÃ¥ÂÂ‡Ã¥ÂÂªÃ§Â”Â¨Ã¨Â¯Â¥Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã§ÂšÂ„trainÃ¨Â®Â­Ã§Â»ÂƒÃ¯Â¼ÂŒÃ¥ÂœÂ¨testÃ¤Â¸ÂŠÃ¨Â¯Â„Ã¤Â¼Â°Ã¥Â¾Â—Ã¥ÂˆÂ°Ã§ÂšÂ„Ã¨Â¡Â¨Ã§ÂÂ°Ã¯Â¼ÂŒÃ¦Â²Â¡Ã§Â”Â¨Ã¥Â¤Â–Ã©ÂƒÂ¨Ã¦Â•Â°Ã¦ÂÂ®
-### Release Models - Ã¦ÂœÂ¬Ã©Â¡Â¹Ã§Â›Â®releaseÃ¦Â¨Â¡Ã¥ÂÂ‹Ã§ÂšÂ„Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¨Â¯Â„Ã¦ÂµÂ‹Ã§Â»Â“Ã¦ÂÂœÃ¯Â¼Âš |
-Arch | BaseModel | Model | ATEC | BQ | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-
-dc | Avg | QPS | |:-----------|:----------------------------------|:-----------
+- `SBERT-macbert-base`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¦Â˜Â¯Ã§Â”Â¨SBertÃ¦Â–Â¹Ã¦Â³Â•Ã¨Â®Â­Ã§Â»ÂƒÃ¯Â¼ÂŒÃ¨Â¿ÂÃ¨Â¡ÂŒ[examples/
+training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/
+blob/master/examples/training_sup_text_matching_model.py)Ã¤Â»Â£Ã§Â ÂÃ¥ÂÂ¯Ã¨Â®Â­Ã§Â»ÂƒÃ¦Â¨Â¡Ã¥ÂÂ‹
+- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-
+v2`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â˜Â¯Ã§Â”Â¨SBertÃ¨Â®Â­Ã§Â»ÂƒÃ¯Â¼ÂŒÃ¦Â˜Â¯`paraphrase-MiniLM-L12-
+v2`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã§ÂšÂ„Ã¥Â¤ÂšÃ¨Â¯Â­Ã¨Â¨Â€Ã§Â‰ÂˆÃ¦ÂœÂ¬Ã¯Â¼ÂŒÃ¦Â”Â¯Ã¦ÂŒÂÃ¤Â¸Â­Ã¦Â–Â‡Ã£Â€ÂÃ¨Â‹Â±Ã¦Â–Â‡Ã§Â­Â‰ ### Release Models -
+Ã¦ÂœÂ¬Ã©Â¡Â¹Ã§Â›Â®releaseÃ¦Â¨Â¡Ã¥ÂÂ‹Ã§ÂšÂ„Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¨Â¯Â„Ã¦ÂµÂ‹Ã§Â»Â“Ã¦ÂÂœÃ¯Â¼Âš | Arch | BaseModel | Model
+| ATEC | BQ | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-dc | Avg | QPS | |:-------
+----|:-------------------------------------------------------------|:----------
 -------------------------------------------------------------------------------
---------------------------------------------------------|:-----:|:-----:|:----
+---------------------------------------------------------|:-----:|:-----:|:----
 -:|:-----:|:-----:|:-------:|:-------:|:---------:|:-----:| | Word2Vec |
 word2vec | [w2v-light-tencent-chinese](https://ai.tencent.com/ailab/nlp/en/
 download.html) | 20.00 | 31.49 | 59.46 | 2.57 | 55.78 | 55.04 | 20.70 | 35.03 |
 23769 | | SBERT | xlm-roberta-base | [sentence-transformers/paraphrase-
 multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/
 paraphrase-multilingual-MiniLM-L12-v2) | 18.42 | 38.52 | 63.96 | 10.14 | 78.90
-| 63.01 | 52.28 | 46.46 | 3138 | | Instructor | hfl/chinese-roberta-wwm-ext |
-[moka-ai/m3e-base](https://huggingface.co/moka-ai/m3e-base) | 41.27 | 63.81 |
-74.87 | 12.20 | 76.96 | 75.83 | 60.55 | 57.93 | 2980 | | CoSENT | hfl/chinese-
-macbert-base | [shibing624/text2vec-base-chinese](https://huggingface.co/
-shibing624/text2vec-base-chinese) | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 |
-70.27 | 50.42 | 51.61 | 3008 | | CoSENT | hfl/chinese-lert-large |
-[GanymedeNil/text2vec-large-chinese](https://huggingface.co/GanymedeNil/
-text2vec-large-chinese) | 32.61 | 44.59 | 69.30 | 14.51 | 79.44 | 73.01 | 59.04
-| 53.12 | 2092 | | CoSENT | nghuyong/ernie-3.0-base-zh | [shibing624/text2vec-
-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-
-sentence) | 43.37 | 61.43 | 73.48 | 38.90 | 78.25 | 70.60 | 53.08 | 59.87 |
-3089 | | CoSENT | nghuyong/ernie-3.0-base-zh | [shibing624/text2vec-base-
-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-
-paraphrase) | 44.89 | 63.58 | 74.24 | 40.90 | 78.93 | 76.70 | 63.30 | **63.08**
-| 3066 | Ã¨Â¯Â´Ã¦Â˜ÂÃ¯Â¼Âš - Ã§Â»Â“Ã¦ÂÂœÃ¨Â¯Â„Ã¦ÂµÂ‹Ã¦ÂŒÂ‡Ã¦Â Â‡Ã¯Â¼ÂšspearmanÃ§Â³Â»Ã¦Â•Â° -
-Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â®ÂÃ©ÂªÂŒÃ¦ÂŠÂ¥Ã¥Â‘ÂŠÃ¯Â¼Âš[Ã¥Â®ÂÃ©ÂªÂŒÃ¦ÂŠÂ¥Ã¥Â‘ÂŠ](https://github.com/shibing624/
-text2vec/blob/master/docs/model_report.md) - `shibing624/text2vec-base-
+| 63.01 | 52.28 | 46.46 | 3138 | | CoSENT | hfl/chinese-macbert-base |
+[shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-
+base-chinese) | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 | 70.27 | 50.42 | 51.61 |
+3008 | | CoSENT | hfl/chinese-lert-large | [GanymedeNil/text2vec-large-chinese]
+(https://huggingface.co/GanymedeNil/text2vec-large-chinese) | 32.61 | 44.59 |
+69.30 | 14.51 | 79.44 | 73.01 | 59.04 | 53.12 | 2092 | | CoSENT | nghuyong/
+ernie-3.0-base-zh | [shibing624/text2vec-base-chinese-sentence](https://
+huggingface.co/shibing624/text2vec-base-chinese-sentence) | 43.37 | 61.43 |
+73.48 | 38.90 | 78.25 | 70.60 | 53.08 | 59.87 | 3089 | | CoSENT | nghuyong/
+ernie-3.0-base-zh | [shibing624/text2vec-base-chinese-paraphrase](https://
+huggingface.co/shibing624/text2vec-base-chinese-paraphrase) | 44.89 | 63.58 |
+74.24 | 40.90 | 78.93 | 76.70 | 63.30 | **63.08** | 3066 | | CoSENT | sentence-
+transformers/paraphrase-multilingual-MiniLM-L12-v2 | [shibing624/text2vec-base-
+multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual) |
+32.39 | 50.33 | 65.64 | 32.56 | 74.45 | 68.88 | 51.17 | 53.67 | 3138 |
+Ã¨Â¯Â´Ã¦Â˜ÂÃ¯Â¼Âš - Ã§Â»Â“Ã¦ÂÂœÃ¨Â¯Â„Ã¦ÂµÂ‹Ã¦ÂŒÂ‡Ã¦Â Â‡Ã¯Â¼ÂšspearmanÃ§Â³Â»Ã¦Â•Â° - `shibing624/text2vec-base-
 chinese`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¦Â˜Â¯Ã§Â”Â¨CoSENTÃ¦Â–Â¹Ã¦Â³Â•Ã¨Â®Â­Ã§Â»ÂƒÃ¯Â¼ÂŒÃ¥ÂŸÂºÃ¤ÂºÂ`hfl/chinese-macbert-
 base`Ã¥ÂœÂ¨Ã¤Â¸Â­Ã¦Â–Â‡STS-BÃ¦Â•Â°Ã¦ÂÂ®Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â¾Â—Ã¥ÂˆÂ°Ã¯Â¼ÂŒÃ¥Â¹Â¶Ã¥ÂœÂ¨Ã¤Â¸Â­Ã¦Â–Â‡STS-
 BÃ¦ÂµÂ‹Ã¨Â¯Â•Ã©Â›Â†Ã¨Â¯Â„Ã¤Â¼Â°Ã¨Â¾Â¾Ã¥ÂˆÂ°Ã¨Â¾ÂƒÃ¥Â¥Â½Ã¦Â•ÂˆÃ¦ÂÂœÃ¯Â¼ÂŒÃ¨Â¿ÂÃ¨Â¡ÂŒ[examples/
 training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/
 blob/master/examples/
 training_sup_text_matching_model.py)Ã¤Â»Â£Ã§Â ÂÃ¥ÂÂ¯Ã¨Â®Â­Ã§Â»ÂƒÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â–Â‡Ã¤Â»Â¶Ã¥Â·Â²Ã§Â»ÂÃ¤Â¸ÂŠÃ¤Â¼Â HF
 model hubÃ¯Â¼ÂŒÃ¤Â¸Â­Ã¦Â–Â‡Ã©Â€ÂšÃ§Â”Â¨Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¦ÂÂ¨Ã¨ÂÂÃ¤Â½Â¿Ã§Â”Â¨ - `shibing624/text2vec-
@@ -137,81 +155,83 @@
 [shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://
 huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-
 sentence-dataset)Ã¥ÂŠÂ Ã¥Â…Â¥Ã¤ÂºÂ†s2p(sentence to
 paraphrase)Ã¦Â•Â°Ã¦ÂÂ®Ã¯Â¼ÂŒÃ¥Â¼ÂºÃ¥ÂŒÂ–Ã¤ÂºÂ†Ã¥Â…Â¶Ã©Â•Â¿Ã¦Â–Â‡Ã¦ÂœÂ¬Ã§ÂšÂ„Ã¨Â¡Â¨Ã¥Â¾ÂÃ¨ÂƒÂ½Ã¥ÂŠÂ›Ã¯Â¼ÂŒÃ¥Â¹Â¶Ã¥ÂœÂ¨Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂÂ„NLIÃ¦ÂµÂ‹Ã¨Â¯Â•Ã©Â›Â†Ã¨Â¯Â„Ã¤Â¼Â°Ã¨Â¾Â¾Ã¥ÂˆÂ°SOTAÃ¯Â¼ÂŒÃ¨Â¿ÂÃ¨Â¡ÂŒ
 [examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/
 shibing624/text2vec/blob/master/examples/
 training_sup_text_matching_model_jsonl_data.py)Ã¤Â»Â£Ã§Â ÂÃ¥ÂÂ¯Ã¨Â®Â­Ã§Â»ÂƒÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â–Â‡Ã¤Â»Â¶Ã¥Â·Â²Ã§Â»ÂÃ¤Â¸ÂŠÃ¤Â¼Â HF
-model hubÃ¯Â¼ÂŒÃ¤Â¸Â­Ã¦Â–Â‡s2p(Ã¥ÂÂ¥Ã¥Â­ÂvsÃ¦Â®ÂµÃ¨ÂÂ½)Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¦ÂÂ¨Ã¨ÂÂÃ¤Â½Â¿Ã§Â”Â¨ - `SBERT-
-macbert-base`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¦Â˜Â¯Ã§Â”Â¨SBERTÃ¦Â–Â¹Ã¦Â³Â•Ã¨Â®Â­Ã§Â»ÂƒÃ¯Â¼ÂŒÃ¨Â¿ÂÃ¨Â¡ÂŒ[examples/
-training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/
-blob/master/examples/training_sup_text_matching_model.py)Ã¤Â»Â£Ã§Â ÂÃ¥ÂÂ¯Ã¨Â®Â­Ã§Â»ÂƒÃ¦Â¨Â¡Ã¥ÂÂ‹
-- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-
-v2`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â˜Â¯Ã§Â”Â¨SBERTÃ¨Â®Â­Ã§Â»ÂƒÃ¯Â¼ÂŒÃ¦Â˜Â¯`paraphrase-MiniLM-L12-
-v2`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã§ÂšÂ„Ã¥Â¤ÂšÃ¨Â¯Â­Ã¨Â¨Â€Ã§Â‰ÂˆÃ¦ÂœÂ¬Ã¯Â¼ÂŒÃ¦Â”Â¯Ã¦ÂŒÂÃ¤Â¸Â­Ã¦Â–Â‡Ã£Â€ÂÃ¨Â‹Â±Ã¦Â–Â‡Ã§Â­Â‰ - `w2v-light-tencent-
+model hubÃ¯Â¼ÂŒÃ¤Â¸Â­Ã¦Â–Â‡s2p(Ã¥ÂÂ¥Ã¥Â­ÂvsÃ¦Â®ÂµÃ¨ÂÂ½)Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¦ÂÂ¨Ã¨ÂÂÃ¤Â½Â¿Ã§Â”Â¨ -
+`shibing624/text2vec-base-
+multilingual`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¦Â˜Â¯Ã§Â”Â¨CoSENTÃ¦Â–Â¹Ã¦Â³Â•Ã¨Â®Â­Ã§Â»ÂƒÃ¯Â¼ÂŒÃ¥ÂŸÂºÃ¤ÂºÂ`sentence-transformers/
+paraphrase-multilingual-MiniLM-L12-
+v2`Ã§Â”Â¨Ã¤ÂºÂºÃ¥Â·Â¥Ã¦ÂŒÂ‘Ã©Â€Â‰Ã¥ÂÂÃ§ÂšÂ„Ã¥Â¤ÂšÃ¨Â¯Â­Ã¨Â¨Â€STSÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†[shibing624/nli-zh-all/text2vec-
+base-multilingual-dataset](https://huggingface.co/datasets/shibing624/nli-zh-
+all/tree/main/text2vec-base-multilingual-
+dataset)Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â¾Â—Ã¥ÂˆÂ°Ã¯Â¼ÂŒÃ¥Â¹Â¶Ã¥ÂœÂ¨Ã¤Â¸Â­Ã¨Â‹Â±Ã¦Â–Â‡Ã¦ÂµÂ‹Ã¨Â¯Â•Ã©Â›Â†Ã¨Â¯Â„Ã¤Â¼Â°Ã§Â›Â¸Ã¥Â¯Â¹Ã¤ÂºÂÃ¥ÂÂŸÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â•ÂˆÃ¦ÂÂœÃ¦ÂœÂ‰Ã¦ÂÂÃ¥ÂÂ‡Ã¯Â¼ÂŒÃ¨Â¿ÂÃ¨Â¡ÂŒ
+[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/
+shibing624/text2vec/blob/master/examples/
+training_sup_text_matching_model_jsonl_data.py)Ã¤Â»Â£Ã§Â ÂÃ¥ÂÂ¯Ã¨Â®Â­Ã§Â»ÂƒÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â–Â‡Ã¤Â»Â¶Ã¥Â·Â²Ã§Â»ÂÃ¤Â¸ÂŠÃ¤Â¼Â HF
+model hubÃ¯Â¼ÂŒÃ¥Â¤ÂšÃ¨Â¯Â­Ã¨Â¨Â€Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¦ÂÂ¨Ã¨ÂÂÃ¤Â½Â¿Ã§Â”Â¨ - `w2v-light-tencent-
 chinese`Ã¦Â˜Â¯Ã¨Â…Â¾Ã¨Â®Â¯Ã¨Â¯ÂÃ¥ÂÂ‘Ã©Â‡ÂÃ§ÂšÂ„Word2VecÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒCPUÃ¥ÂŠÂ Ã¨Â½Â½Ã¤Â½Â¿Ã§Â”Â¨Ã¯Â¼ÂŒÃ©Â€Â‚Ã§Â”Â¨Ã¤ÂºÂÃ¤Â¸Â­Ã¦Â–Â‡Ã¥Â­Â—Ã©ÂÂ¢Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¥Â’ÂŒÃ§Â¼ÂºÃ¥Â°Â‘Ã¦Â•Â°Ã¦ÂÂ®Ã§ÂšÂ„Ã¥Â†Â·Ã¥ÂÂ¯Ã¥ÂŠÂ¨Ã¦ÂƒÂ…Ã¥Â†Âµ
 - Ã¥ÂÂ„Ã©Â¢Â„Ã¨Â®Â­Ã§Â»ÂƒÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¥ÂÂ‡Ã¥ÂÂ¯Ã¤Â»Â¥Ã©Â€ÂšÃ¨Â¿Â‡transformersÃ¨Â°ÂƒÃ§Â”Â¨Ã¯Â¼ÂŒÃ¥Â¦Â‚MacBERTÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼Âš`--
 model_name hfl/chinese-macbert-base` Ã¦ÂˆÂ–Ã¨Â€Â…robertaÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼Âš`--model_name uer/
 roberta-medium-wwm-chinese-cluecorpussmall` -
 Ã¤Â¸ÂºÃ¦ÂµÂ‹Ã¨Â¯Â„Ã¦Â¨Â¡Ã¥ÂÂ‹Ã§ÂšÂ„Ã©Â²ÂÃ¦Â£Â’Ã¦Â€Â§Ã¯Â¼ÂŒÃ¥ÂŠÂ Ã¥Â…Â¥Ã¤ÂºÂ†Ã¦ÂœÂªÃ¨Â®Â­Ã§Â»ÂƒÃ¨Â¿Â‡Ã§ÂšÂ„SOHUÃ¦ÂµÂ‹Ã¨Â¯Â•Ã©Â›Â†Ã¯Â¼ÂŒÃ§Â”Â¨Ã¤ÂºÂÃ¦ÂµÂ‹Ã¨Â¯Â•Ã¦Â¨Â¡Ã¥ÂÂ‹Ã§ÂšÂ„Ã¦Â³Â›Ã¥ÂŒÂ–Ã¨ÂƒÂ½Ã¥ÂŠÂ›Ã¯Â¼Â›Ã¤Â¸ÂºÃ¨Â¾Â¾Ã¥ÂˆÂ°Ã¥Â¼Â€Ã§Â®Â±Ã¥ÂÂ³Ã§Â”Â¨Ã§ÂšÂ„Ã¥Â®ÂÃ§Â”Â¨Ã¦Â•ÂˆÃ¦ÂÂœÃ¯Â¼ÂŒÃ¤Â½Â¿Ã§Â”Â¨Ã¤ÂºÂ†Ã¦ÂÂœÃ©Â›Â†Ã¥ÂˆÂ°Ã§ÂšÂ„Ã¥ÂÂ„Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼ÂŒÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¤Â¹ÂŸÃ¤Â¸ÂŠÃ¤Â¼Â Ã¥ÂˆÂ°HF
 datasets[Ã©Â“Â¾Ã¦ÂÂ¥Ã¨Â§ÂÃ¤Â¸Â‹Ã¦Â–Â¹](#Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†) -
 Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¥Â®ÂÃ©ÂªÂŒÃ¨Â¡Â¨Ã¦Â˜ÂÃ¯Â¼ÂŒpoolingÃ¦ÂœÂ€Ã¤Â¼Â˜Ã¦Â˜Â¯`EncoderType.FIRST_LAST_AVG`Ã¥Â’ÂŒ`EncoderType.MEAN`Ã¯Â¼ÂŒÃ¤Â¸Â¤Ã¨Â€Â…Ã©Â¢Â„Ã¦ÂµÂ‹Ã¦Â•ÂˆÃ¦ÂÂœÃ¥Â·Â®Ã¥Â¼Â‚Ã¥Â¾ÂˆÃ¥Â°Â
 -
 Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¨Â¯Â„Ã¦ÂµÂ‹Ã§Â»Â“Ã¦ÂÂœÃ¥Â¤ÂÃ§ÂÂ°Ã¯Â¼ÂŒÃ¥ÂÂ¯Ã¤Â»Â¥Ã¤Â¸Â‹Ã¨Â½Â½Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¥ÂˆÂ°`examples/
 data`Ã¯Â¼ÂŒÃ¨Â¿ÂÃ¨Â¡ÂŒ[tests/test_model_spearman.py](https://github.com/shibing624/
 text2vec/blob/master/tests/test_model_spearman.py)Ã¤Â»Â£Ã§Â ÂÃ¥Â¤ÂÃ§ÂÂ°Ã¨Â¯Â„Ã¦ÂµÂ‹Ã§Â»Â“Ã¦ÂÂœ -
-QPSÃ§ÂšÂ„GPUÃ¦ÂµÂ‹Ã¨Â¯Â•Ã§ÂÂ¯Ã¥Â¢ÂƒÃ¦Â˜Â¯Tesla V100Ã¯Â¼ÂŒÃ¦Â˜Â¾Ã¥Â­Â˜32GB # Demo Official Demo: https://
-www.mulanai.com/product/short_text_sim/ HuggingFace Demo: https://
-huggingface.co/spaces/shibing624/text2vec ![](docs/hf.png) run example:
-[examples/gradio_demo.py](https://github.com/shibing624/text2vec/blob/master/
-examples/gradio_demo.py) to see the demo: ```shell python examples/
-gradio_demo.py ``` # Install ```shell pip install torch # conda install pytorch
-pip install -U text2vec ``` or ```shell pip install torch # conda install
-pytorch pip install -r requirements.txt git clone https://github.com/
-shibing624/text2vec.git cd text2vec pip install --no-deps . ``` # Usage ##
-Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂÂ‘Ã©Â‡ÂÃ¨Â¡Â¨Ã¥Â¾Â Ã¥ÂŸÂºÃ¤ÂºÂ`pretrained model`Ã¨Â®Â¡Ã§Â®Â—Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂÂ‘Ã©Â‡ÂÃ¯Â¼Âš ```zsh >>>
-from text2vec import SentenceModel >>> m = SentenceModel() >>> m.encode
-("Ã¥Â¦Â‚Ã¤Â½Â•Ã¦Â›Â´Ã¦ÂÂ¢Ã¨ÂŠÂ±Ã¥Â‘Â—Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡") Embedding shape: (768,) ``` example:
-[examples/computing_embeddings_demo.py](https://github.com/shibing624/text2vec/
-blob/master/examples/computing_embeddings_demo.py) ```python import sys
-sys.path.append('..') from text2vec import SentenceModel from text2vec import
-Word2Vec def compute_emb(model): # Embed a list of sentences sentences =
-[ 'Ã¥ÂÂ¡', 'Ã©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡', 'Ã¥Â¦Â‚Ã¤Â½Â•Ã¦Â›Â´Ã¦ÂÂ¢Ã¨ÂŠÂ±Ã¥Â‘Â—Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡',
-'Ã¨ÂŠÂ±Ã¥Â‘Â—Ã¦Â›Â´Ã¦Â”Â¹Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡', 'This framework generates embeddings for each
-input sentence', 'Sentences are passed as a list of string.', 'The quick brown
-fox jumps over the lazy dog.' ] sentence_embeddings = model.encode(sentences)
-print(type(sentence_embeddings), sentence_embeddings.shape) # The result is a
-list of sentence embeddings as numpy arrays for sentence, embedding in zip
-(sentences, sentence_embeddings): print("Sentence:", sentence) print("Embedding
-shape:", embedding.shape) print("Embedding head:", embedding[:10]) print() if
-__name__ == "__main__": # Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂÂ¥Ã¥ÂÂ‘Ã©Â‡ÂÃ¦Â¨Â¡Ã¥ÂÂ‹
-(CoSENT)Ã¯Â¼ÂŒÃ¤Â¸Â­Ã¦Â–Â‡Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¦ÂÂ¨Ã¨ÂÂÃ¯Â¼ÂŒÃ¦Â”Â¯Ã¦ÂŒÂfine-tuneÃ§Â»Â§Ã§Â»Â­Ã¨Â®Â­Ã§Â»Âƒ
-t2v_model = SentenceModel("shibing624/text2vec-base-chinese") compute_emb
-(t2v_model) # Ã¦Â”Â¯Ã¦ÂŒÂÃ¥Â¤ÂšÃ¨Â¯Â­Ã¨Â¨Â€Ã§ÂšÂ„Ã¥ÂÂ¥Ã¥ÂÂ‘Ã©Â‡ÂÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂˆSentence-
-BERTÃ¯Â¼Â‰Ã¯Â¼ÂŒÃ¨Â‹Â±Ã¦Â–Â‡Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¦ÂÂ¨Ã¨ÂÂÃ¯Â¼ÂŒÃ¦Â”Â¯Ã¦ÂŒÂfine-tuneÃ§Â»Â§Ã§Â»Â­Ã¨Â®Â­Ã§Â»Âƒ
-sbert_model = SentenceModel("sentence-transformers/paraphrase-multilingual-
-MiniLM-L12-v2") compute_emb(sbert_model) # Ã¤Â¸Â­Ã¦Â–Â‡Ã¨Â¯ÂÃ¥ÂÂ‘Ã©Â‡ÂÃ¦Â¨Â¡Ã¥ÂÂ‹
+QPSÃ§ÂšÂ„GPUÃ¦ÂµÂ‹Ã¨Â¯Â•Ã§ÂÂ¯Ã¥Â¢ÂƒÃ¦Â˜Â¯Tesla V100Ã¯Â¼ÂŒÃ¦Â˜Â¾Ã¥Â­Â˜32GB Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â®ÂÃ©ÂªÂŒÃ¦ÂŠÂ¥Ã¥Â‘ÂŠÃ¯Â¼Âš
+[Ã¥Â®ÂÃ©ÂªÂŒÃ¦ÂŠÂ¥Ã¥Â‘ÂŠ](https://github.com/shibing624/text2vec/blob/master/docs/
+model_report.md) ## Demo Official Demo: https://www.mulanai.com/product/
+short_text_sim/ HuggingFace Demo: https://huggingface.co/spaces/shibing624/
+text2vec ![](docs/hf.png) run example: [examples/gradio_demo.py](https://
+github.com/shibing624/text2vec/blob/master/examples/gradio_demo.py) to see the
+demo: ```shell python examples/gradio_demo.py ``` ## Install ```shell pip
+install torch # conda install pytorch pip install -U text2vec ``` or ```shell
+pip install torch # conda install pytorch pip install -r requirements.txt git
+clone https://github.com/shibing624/text2vec.git cd text2vec pip install --no-
+deps . ``` ## Usage ### Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂÂ‘Ã©Â‡ÂÃ¨Â¡Â¨Ã¥Â¾Â Ã¥ÂŸÂºÃ¤ÂºÂ`pretrained
+model`Ã¨Â®Â¡Ã§Â®Â—Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂÂ‘Ã©Â‡ÂÃ¯Â¼Âš ```zsh >>> from text2vec import SentenceModel >>> m
+= SentenceModel() >>> m.encode("Ã¥Â¦Â‚Ã¤Â½Â•Ã¦Â›Â´Ã¦ÂÂ¢Ã¨ÂŠÂ±Ã¥Â‘Â—Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡") Embedding
+shape: (768,) ``` example: [examples/computing_embeddings_demo.py](https://
+github.com/shibing624/text2vec/blob/master/examples/
+computing_embeddings_demo.py) ```python import sys sys.path.append('..') from
+text2vec import SentenceModel from text2vec import Word2Vec def compute_emb
+(model): # Embed a list of sentences sentences = [ 'Ã¥ÂÂ¡', 'Ã©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡',
+'Ã¥Â¦Â‚Ã¤Â½Â•Ã¦Â›Â´Ã¦ÂÂ¢Ã¨ÂŠÂ±Ã¥Â‘Â—Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡', 'Ã¨ÂŠÂ±Ã¥Â‘Â—Ã¦Â›Â´Ã¦Â”Â¹Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡', 'This
+framework generates embeddings for each input sentence', 'Sentences are passed
+as a list of string.', 'The quick brown fox jumps over the lazy dog.' ]
+sentence_embeddings = model.encode(sentences) print(type(sentence_embeddings),
+sentence_embeddings.shape) # The result is a list of sentence embeddings as
+numpy arrays for sentence, embedding in zip(sentences, sentence_embeddings):
+print("Sentence:", sentence) print("Embedding shape:", embedding.shape) print
+("Embedding head:", embedding[:10]) print() if __name__ == "__main__": #
+Ã¤Â¸Â­Ã¦Â–Â‡Ã¥ÂÂ¥Ã¥ÂÂ‘Ã©Â‡ÂÃ¦Â¨Â¡Ã¥ÂÂ‹(CoSENT)Ã¯Â¼ÂŒÃ¤Â¸Â­Ã¦Â–Â‡Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¦ÂÂ¨Ã¨ÂÂÃ¯Â¼ÂŒÃ¦Â”Â¯Ã¦ÂŒÂfine-
+tuneÃ§Â»Â§Ã§Â»Â­Ã¨Â®Â­Ã§Â»Âƒ t2v_model = SentenceModel("shibing624/text2vec-base-chinese")
+compute_emb(t2v_model) #
+Ã¦Â”Â¯Ã¦ÂŒÂÃ¥Â¤ÂšÃ¨Â¯Â­Ã¨Â¨Â€Ã§ÂšÂ„Ã¥ÂÂ¥Ã¥ÂÂ‘Ã©Â‡ÂÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂˆCoSENTÃ¯Â¼Â‰Ã¯Â¼ÂŒÃ¥Â¤ÂšÃ¨Â¯Â­Ã¨Â¨Â€Ã¯Â¼ÂˆÃ¥ÂŒÂ…Ã¦Â‹Â¬Ã¤Â¸Â­Ã¨Â‹Â±Ã¦Â–Â‡Ã¯Â¼Â‰Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¦ÂÂ¨Ã¨ÂÂÃ¯Â¼ÂŒÃ¦Â”Â¯Ã¦ÂŒÂfine-
+tuneÃ§Â»Â§Ã§Â»Â­Ã¨Â®Â­Ã§Â»Âƒ sbert_model = SentenceModel("shibing624/text2vec-base-
+multilingual") compute_emb(sbert_model) # Ã¤Â¸Â­Ã¦Â–Â‡Ã¨Â¯ÂÃ¥ÂÂ‘Ã©Â‡ÂÃ¦Â¨Â¡Ã¥ÂÂ‹
 (word2vec)Ã¯Â¼ÂŒÃ¤Â¸Â­Ã¦Â–Â‡Ã¥Â­Â—Ã©ÂÂ¢Ã¥ÂŒÂ¹Ã©Â…ÂÃ¤Â»Â»Ã¥ÂŠÂ¡Ã¥Â’ÂŒÃ¥Â†Â·Ã¥ÂÂ¯Ã¥ÂŠÂ¨Ã©Â€Â‚Ã§Â”Â¨ w2v_model = Word2Vec
 ("w2v-light-tencent-chinese") compute_emb(w2v_model) ``` output: ```
 numpy.ndarray'> (7, 768) Sentence: Ã¥ÂÂ¡ Embedding shape: (768,) Sentence:
 Ã©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡ Embedding shape: (768,) ... ``` -
 Ã¨Â¿Â”Ã¥Â›ÂÃ¥Â€Â¼`embeddings`Ã¦Â˜Â¯`numpy.ndarray`Ã§Â±Â»Ã¥ÂÂ‹Ã¯Â¼ÂŒshapeÃ¤Â¸Âº`(sentences_size,
 model_embedding_size)`Ã¯Â¼ÂŒÃ¤Â¸Â‰Ã¤Â¸ÂªÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¤Â»Â»Ã©Â€Â‰Ã¤Â¸Â€Ã§Â§ÂÃ¥ÂÂ³Ã¥ÂÂ¯Ã¯Â¼ÂŒÃ¦ÂÂ¨Ã¨ÂÂÃ§Â”Â¨Ã§Â¬Â¬Ã¤Â¸Â€Ã¤Â¸ÂªÃ£Â€Â‚
 - `shibing624/text2vec-base-chinese`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â˜Â¯CoSENTÃ¦Â–Â¹Ã¦Â³Â•Ã¥ÂœÂ¨Ã¤Â¸Â­Ã¦Â–Â‡STS-
 BÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â¾Â—Ã¥ÂˆÂ°Ã§ÂšÂ„Ã¯Â¼ÂŒÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¥Â·Â²Ã§Â»ÂÃ¤Â¸ÂŠÃ¤Â¼Â Ã¥ÂˆÂ°huggingfaceÃ§ÂšÂ„ Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¥ÂºÂ“
 [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-
 base-chinese)Ã¯Â¼ÂŒ
 Ã¦Â˜Â¯`text2vec.SentenceModel`Ã¦ÂŒÂ‡Ã¥Â®ÂšÃ§ÂšÂ„Ã©Â»Â˜Ã¨Â®Â¤Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¥ÂÂ¯Ã¤Â»Â¥Ã©Â€ÂšÃ¨Â¿Â‡Ã¤Â¸ÂŠÃ©ÂÂ¢Ã§Â¤ÂºÃ¤Â¾Â‹Ã¨Â°ÂƒÃ§Â”Â¨Ã¯Â¼ÂŒÃ¦ÂˆÂ–Ã¨Â€Â…Ã¥Â¦Â‚Ã¤Â¸Â‹Ã¦Â‰Â€Ã§Â¤ÂºÃ§Â”Â¨
 [transformersÃ¥ÂºÂ“](https://github.com/huggingface/transformers)Ã¨Â°ÂƒÃ§Â”Â¨Ã¯Â¼ÂŒ
-Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â‡ÂªÃ¥ÂŠÂ¨Ã¤Â¸Â‹Ã¨Â½Â½Ã¥ÂˆÂ°Ã¦ÂœÂ¬Ã¦ÂœÂºÃ¨Â·Â¯Ã¥Â¾Â„Ã¯Â¼Âš`~/.cache/huggingface/transformers` -
-`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â˜Â¯Sentence-
-BERTÃ§ÂšÂ„Ã¥Â¤ÂšÃ¨Â¯Â­Ã¨Â¨Â€Ã¥ÂÂ¥Ã¥ÂÂ‘Ã©Â‡ÂÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒ
-Ã©Â€Â‚Ã§Â”Â¨Ã¤ÂºÂÃ©Â‡ÂŠÃ¤Â¹Â‰Ã¯Â¼ÂˆparaphraseÃ¯Â¼Â‰Ã¨Â¯Â†Ã¥ÂˆÂ«Ã¯Â¼ÂŒÃ¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂŒÂ¹Ã©Â…ÂÃ¯Â¼ÂŒÃ©Â€ÂšÃ¨Â¿Â‡`text2vec.SentenceModel`Ã¥Â’ÂŒ
-[sentence-transformersÃ¥ÂºÂ“]((https://github.com/UKPLab/sentence-
-transformers))Ã©ÂƒÂ½Ã¥ÂÂ¯Ã¤Â»Â¥Ã¨Â°ÂƒÃ§Â”Â¨Ã¨Â¯Â¥Ã¦Â¨Â¡Ã¥ÂÂ‹ - `w2v-light-tencent-
+Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â‡ÂªÃ¥ÂŠÂ¨Ã¤Â¸Â‹Ã¨Â½Â½Ã¥ÂˆÂ°Ã¦ÂœÂ¬Ã¦ÂœÂºÃ¨Â·Â¯Ã¥Â¾Â„Ã¯Â¼Âš`~/.cache/huggingface/transformers` - `w2v-
+light-tencent-
 chinese`Ã¦Â˜Â¯Ã©Â€ÂšÃ¨Â¿Â‡gensimÃ¥ÂŠÂ Ã¨Â½Â½Ã§ÂšÂ„Word2VecÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¤Â½Â¿Ã§Â”Â¨Ã¨Â…Â¾Ã¨Â®Â¯Ã¨Â¯ÂÃ¥ÂÂ‘Ã©Â‡Â`Tencent_AILab_ChineseEmbedding.tar.gz`Ã¨Â®Â¡Ã§Â®Â—Ã¥ÂÂ„Ã¥Â­Â—Ã¨Â¯ÂÃ§ÂšÂ„Ã¨Â¯ÂÃ¥ÂÂ‘Ã©Â‡ÂÃ¯Â¼ÂŒÃ¥ÂÂ¥Ã¥Â­ÂÃ¥ÂÂ‘Ã©Â‡ÂÃ©Â€ÂšÃ¨Â¿Â‡Ã¥ÂÂ•Ã¨Â¯ÂÃ¨Â¯Â
 Ã¥ÂÂ‘Ã©Â‡ÂÃ¥ÂÂ–Ã¥Â¹Â³Ã¥ÂÂ‡Ã¥Â€Â¼Ã¥Â¾Â—Ã¥ÂˆÂ°Ã¯Â¼ÂŒÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â‡ÂªÃ¥ÂŠÂ¨Ã¤Â¸Â‹Ã¨Â½Â½Ã¥ÂˆÂ°Ã¦ÂœÂ¬Ã¦ÂœÂºÃ¨Â·Â¯Ã¥Â¾Â„Ã¯Â¼Âš`~/.text2vec/
 datasets/light_Tencent_AILab_ChineseEmbedding.bin` #### Usage (HuggingFace
 Transformers) Without [text2vec](https://github.com/shibing624/text2vec), you
 can use the model like this: First, you pass your input through the transformer
 model, then you have to apply the right pooling-operation on-top of the
 contextualized word embeddings. example: [examples/
@@ -322,16 +342,16 @@
 Ã¦Â–Â‡Ã¦ÂœÂ¬Ã§Â›Â¸Ã¤Â¼Â¼Ã¥ÂºÂ¦Ã¨Â®Â¡Ã§Â®Â—Ã¥Â’ÂŒÃ¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦ÂÂœÃ§Â´Â¢Ã¤Â»Â»Ã¥ÂŠÂ¡Ã¯Â¼ÂŒÃ¦ÂÂ¨Ã¨ÂÂÃ¤Â½Â¿Ã§Â”Â¨
 [similaritiesÃ¥ÂºÂ“](https://github.com/shibing624/similarities)
 Ã¯Â¼ÂŒÃ¥Â…Â¼Ã¥Â®Â¹Ã¦ÂœÂ¬Ã©Â¡Â¹Ã§Â›Â®releaseÃ§ÂšÂ„
 Word2vecÃ£Â€ÂSBERTÃ£Â€ÂCosentÃ§Â±Â»Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¨Â¿Â˜Ã¦Â”Â¯Ã¦ÂŒÂÃ¥Â­Â—Ã©ÂÂ¢Ã§Â»Â´Ã¥ÂºÂ¦Ã§Â›Â¸Ã¤Â¼Â¼Ã¥ÂºÂ¦Ã¨Â®Â¡Ã§Â®Â—Ã£Â€ÂÃ¥ÂŒÂ¹Ã©Â…ÂÃ¦ÂÂœÃ§Â´Â¢Ã§Â®Â—Ã¦Â³Â•Ã¯Â¼ÂŒÃ¦Â”Â¯Ã¦ÂŒÂÃ¦Â–Â‡Ã¦ÂœÂ¬Ã£Â€ÂÃ¥Â›Â¾Ã¥ÂƒÂÃ£Â€Â‚
 Ã¥Â®Â‰Ã¨Â£Â…Ã¯Â¼Âš ```pip install -U similarities``` Ã¥ÂÂ¥Ã¥Â­ÂÃ§Â›Â¸Ã¤Â¼Â¼Ã¥ÂºÂ¦Ã¨Â®Â¡Ã§Â®Â—Ã¯Â¼Âš ```python
 from similarities import Similarity m = Similarity() r = m.similarity
 ('Ã¥Â¦Â‚Ã¤Â½Â•Ã¦Â›Â´Ã¦ÂÂ¢Ã¨ÂŠÂ±Ã¥Â‘Â—Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡', 'Ã¨ÂŠÂ±Ã¥Â‘Â—Ã¦Â›Â´Ã¦Â”Â¹Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡') print
-(f"similarity score: {float(r)}") # similarity score: 0.855146050453186 ``` #
-Models ## CoSENT model CoSENTÃ¯Â¼ÂˆCosine
+(f"similarity score: {float(r)}") # similarity score: 0.855146050453186 ``` ##
+Models ### CoSENT model CoSENTÃ¯Â¼ÂˆCosine
 SentenceÃ¯Â¼Â‰Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¥ÂœÂ¨Sentence-
 BERTÃ¤Â¸ÂŠÃ¦Â”Â¹Ã¨Â¿Â›Ã¤ÂºÂ†CosineRankLossÃ§ÂšÂ„Ã¥ÂÂ¥Ã¥ÂÂ‘Ã©Â‡ÂÃ¦Â–Â¹Ã¦Â¡Âˆ Network structure: Training:
 [docs/cosent_train.png] Inference: [docs/inference.png] #### CoSENT
 Ã§Â›Â‘Ã§ÂÂ£Ã¦Â¨Â¡Ã¥ÂÂ‹ Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â’ÂŒÃ©Â¢Â„Ã¦ÂµÂ‹CoSENTÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼Âš - Ã¥ÂœÂ¨Ã¤Â¸Â­Ã¦Â–Â‡STS-
 BÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â’ÂŒÃ¨Â¯Â„Ã¤Â¼Â°`CoSENT`Ã¦Â¨Â¡Ã¥ÂÂ‹ example: [examples/
 training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/
 blob/master/examples/training_sup_text_matching_model.py) ```shell cd examples
@@ -343,21 +363,24 @@
 'PAWSX'Ã¯Â¼ÂŒÃ¥Â…Â·Ã¤Â½Â“Ã¥ÂÂ‚Ã¨Â€ÂƒHuggingFace datasets [https://huggingface.co/datasets/
 shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh) ```shell
 python training_sup_text_matching_model.py --task_name ATEC --model_arch cosent
 --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base -
 -output_dir ./outputs/ATEC-cosent ``` - Ã¥ÂœÂ¨Ã¨Â‡ÂªÃ¦ÂœÂ‰Ã¤Â¸Â­Ã¦Â–Â‡Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¤Â¸ÂŠÃ¨Â®Â­Ã§Â»ÂƒÃ¦Â¨Â¡Ã¥ÂÂ‹
 example: [examples/training_sup_text_matching_model_mydata.py](https://
 github.com/shibing624/text2vec/blob/master/examples/
-training_sup_text_matching_model_mydata.py) ```shell python
-training_sup_text_matching_model_mydata.py --do_train --do_predict ```
-Ã¨Â®Â­Ã§Â»ÂƒÃ©Â›Â†Ã¦Â Â¼Ã¥Â¼ÂÃ¥ÂÂ‚Ã¨Â€Âƒ[examples/data/STS-B/STS-B.valid.data](https://github.com/
-shibing624/text2vec/blob/master/examples/data/STS-B/STS-B.valid.data) ```shell
-sentence1 sentence2 label Ã¤Â¸Â€Ã¤Â¸ÂªÃ¥Â¥Â³Ã¥Â­Â©Ã¥ÂœÂ¨Ã§Â»Â™Ã¥Â¥Â¹Ã§ÂšÂ„Ã¥Â¤Â´Ã¥ÂÂ‘Ã¥ÂÂšÃ¥ÂÂ‘Ã¥ÂÂ‹Ã£Â€Â‚
-Ã¤Â¸Â€Ã¤Â¸ÂªÃ¥Â¥Â³Ã¥Â­Â©Ã¥ÂœÂ¨Ã¦Â¢Â³Ã¥Â¤Â´Ã£Â€Â‚ 2 Ã¤Â¸Â€Ã§Â¾Â¤Ã§Â”Â·Ã¤ÂºÂºÃ¥ÂœÂ¨Ã¦ÂµÂ·Ã¦Â»Â©Ã¤Â¸ÂŠÃ¨Â¸Â¢Ã¨Â¶Â³Ã§ÂÂƒÃ£Â€Â‚
-Ã¤Â¸Â€Ã§Â¾Â¤Ã§Â”Â·Ã¥Â­Â©Ã¥ÂœÂ¨Ã¦ÂµÂ·Ã¦Â»Â©Ã¤Â¸ÂŠÃ¨Â¸Â¢Ã¨Â¶Â³Ã§ÂÂƒÃ£Â€Â‚ 3
+training_sup_text_matching_model_mydata.py) Ã¥ÂÂ•Ã¥ÂÂ¡Ã¨Â®Â­Ã§Â»ÂƒÃ¯Â¼Âš ```shell
+CUDA_VISIBLE_DEVICES=0 python training_sup_text_matching_model_mydata.py --
+do_train --do_predict ``` Ã¥Â¤ÂšÃ¥ÂÂ¡Ã¨Â®Â­Ã§Â»ÂƒÃ¯Â¼Âš ```shell CUDA_VISIBLE_DEVICES=0,1
+torchrun --nproc_per_node 2 training_sup_text_matching_model_mydata.py --
+do_train --do_predict --output_dir outputs/STS-B-text2vec-macbert-v1 --
+batch_size 64 --fp16 --data_parallel ``` Ã¨Â®Â­Ã§Â»ÂƒÃ©Â›Â†Ã¦Â Â¼Ã¥Â¼ÂÃ¥ÂÂ‚Ã¨Â€Âƒ[examples/data/
+STS-B/STS-B.valid.data](https://github.com/shibing624/text2vec/blob/master/
+examples/data/STS-B/STS-B.valid.data) ```shell sentence1 sentence2 label
+Ã¤Â¸Â€Ã¤Â¸ÂªÃ¥Â¥Â³Ã¥Â­Â©Ã¥ÂœÂ¨Ã§Â»Â™Ã¥Â¥Â¹Ã§ÂšÂ„Ã¥Â¤Â´Ã¥ÂÂ‘Ã¥ÂÂšÃ¥ÂÂ‘Ã¥ÂÂ‹Ã£Â€Â‚ Ã¤Â¸Â€Ã¤Â¸ÂªÃ¥Â¥Â³Ã¥Â­Â©Ã¥ÂœÂ¨Ã¦Â¢Â³Ã¥Â¤Â´Ã£Â€Â‚ 2
+Ã¤Â¸Â€Ã§Â¾Â¤Ã§Â”Â·Ã¤ÂºÂºÃ¥ÂœÂ¨Ã¦ÂµÂ·Ã¦Â»Â©Ã¤Â¸ÂŠÃ¨Â¸Â¢Ã¨Â¶Â³Ã§ÂÂƒÃ£Â€Â‚ Ã¤Â¸Â€Ã§Â¾Â¤Ã§Â”Â·Ã¥Â­Â©Ã¥ÂœÂ¨Ã¦ÂµÂ·Ã¦Â»Â©Ã¤Â¸ÂŠÃ¨Â¸Â¢Ã¨Â¶Â³Ã§ÂÂƒÃ£Â€Â‚ 3
 Ã¤Â¸Â€Ã¤Â¸ÂªÃ¥Â¥Â³Ã¤ÂºÂºÃ¥ÂœÂ¨Ã¦ÂµÂ‹Ã©Â‡ÂÃ¥ÂÂ¦Ã¤Â¸Â€Ã¤Â¸ÂªÃ¥Â¥Â³Ã¤ÂºÂºÃ§ÂšÂ„Ã¨Â„ÂšÃ¨Â¸ÂÃ£Â€Â‚
 Ã¥Â¥Â³Ã¤ÂºÂºÃ¦ÂµÂ‹Ã©Â‡ÂÃ¥ÂÂ¦Ã¤Â¸Â€Ã¤Â¸ÂªÃ¥Â¥Â³Ã¤ÂºÂºÃ§ÂšÂ„Ã¨Â„ÂšÃ¨Â¸ÂÃ£Â€Â‚ 5 ```
 `label`Ã¥ÂÂ¯Ã¤Â»Â¥Ã¦Â˜Â¯0Ã¯Â¼ÂŒ1Ã¦Â Â‡Ã§Â­Â¾Ã¯Â¼ÂŒ0Ã¤Â»Â£Ã¨Â¡Â¨Ã¤Â¸Â¤Ã¤Â¸ÂªÃ¥ÂÂ¥Ã¥Â­ÂÃ¤Â¸ÂÃ§Â›Â¸Ã¤Â¼Â¼Ã¯Â¼ÂŒ1Ã¤Â»Â£Ã¨Â¡Â¨Ã§Â›Â¸Ã¤Â¼Â¼Ã¯Â¼Â›Ã¤Â¹ÂŸÃ¥ÂÂ¯Ã¤Â»Â¥Ã¦Â˜Â¯0-
 5Ã§ÂšÂ„Ã¨Â¯Â„Ã¥ÂˆÂ†Ã¯Â¼ÂŒÃ¨Â¯Â„Ã¥ÂˆÂ†Ã¨Â¶ÂŠÃ©Â«Â˜Ã¯Â¼ÂŒÃ¨Â¡Â¨Ã§Â¤ÂºÃ¤Â¸Â¤Ã¤Â¸ÂªÃ¥ÂÂ¥Ã¥Â­ÂÃ¨Â¶ÂŠÃ§Â›Â¸Ã¤Â¼Â¼Ã£Â€Â‚Ã¦Â¨Â¡Ã¥ÂÂ‹Ã©ÂƒÂ½Ã¨ÂƒÂ½Ã¦Â”Â¯Ã¦ÂŒÂÃ£Â€Â‚
 - Ã¥ÂœÂ¨Ã¨Â‹Â±Ã¦Â–Â‡STS-BÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â’ÂŒÃ¨Â¯Â„Ã¤Â¼Â°`CoSENT`Ã¦Â¨Â¡Ã¥ÂÂ‹ example: [examples/
 training_sup_text_matching_model_en.py](https://github.com/shibing624/text2vec/
 blob/master/examples/training_sup_text_matching_model_en.py) ```shell cd
@@ -366,15 +389,15 @@
 output_dir ./outputs/STS-B-en-cosent ``` #### CoSENT Ã¦Â—Â Ã§Â›Â‘Ã§ÂÂ£Ã¦Â¨Â¡Ã¥ÂÂ‹ -
 Ã¥ÂœÂ¨Ã¨Â‹Â±Ã¦Â–Â‡NLIÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¨Â®Â­Ã§Â»Âƒ`CoSENT`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¥ÂœÂ¨STS-BÃ¦ÂµÂ‹Ã¨Â¯Â•Ã©Â›Â†Ã¨Â¯Â„Ã¤Â¼Â°Ã¦Â•ÂˆÃ¦ÂÂœ
 example: [examples/training_unsup_text_matching_model_en.py](https://
 github.com/shibing624/text2vec/blob/master/examples/
 training_unsup_text_matching_model_en.py) ```shell cd examples python
 training_unsup_text_matching_model_en.py --model_arch cosent --do_train --
 do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./
-outputs/STS-B-en-unsup-cosent ``` ## Sentence-BERT model Sentence-
+outputs/STS-B-en-unsup-cosent ``` ### Sentence-BERT model Sentence-
 BERTÃ¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¨Â¡Â¨Ã¥Â¾ÂÃ¥Â¼ÂÃ¥ÂÂ¥Ã¥ÂÂ‘Ã©Â‡ÂÃ¨Â¡Â¨Ã§Â¤ÂºÃ¦Â–Â¹Ã¦Â¡Âˆ Network structure:
 Training: [docs/sbert_train.png] Inference: [docs/sbert_inference.png] ####
 SentenceBERT Ã§Â›Â‘Ã§ÂÂ£Ã¦Â¨Â¡Ã¥ÂÂ‹ - Ã¥ÂœÂ¨Ã¤Â¸Â­Ã¦Â–Â‡STS-BÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¨Â®Â­Ã§Â»ÂƒÃ¥Â’ÂŒÃ¨Â¯Â„Ã¤Â¼Â°`SBERT`Ã¦Â¨Â¡Ã¥ÂÂ‹
 example: [examples/training_sup_text_matching_model.py](https://github.com/
 shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)
 ```shell cd examples python training_sup_text_matching_model.py --model_arch
 sentencebert --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-
@@ -387,35 +410,35 @@
 uncased --output_dir ./outputs/STS-B-en-sbert ``` #### SentenceBERT
 Ã¦Â—Â Ã§Â›Â‘Ã§ÂÂ£Ã¦Â¨Â¡Ã¥ÂÂ‹ - Ã¥ÂœÂ¨Ã¨Â‹Â±Ã¦Â–Â‡NLIÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¨Â®Â­Ã§Â»Âƒ`SBERT`Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¥ÂœÂ¨STS-
 BÃ¦ÂµÂ‹Ã¨Â¯Â•Ã©Â›Â†Ã¨Â¯Â„Ã¤Â¼Â°Ã¦Â•ÂˆÃ¦ÂÂœ example: [examples/
 training_unsup_text_matching_model_en.py](https://github.com/shibing624/
 text2vec/blob/master/examples/training_unsup_text_matching_model_en.py)
 ```shell cd examples python training_unsup_text_matching_model_en.py --
 model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name
-bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-sbert ``` ## BERT-Match
-model
+bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-sbert ``` ### BERT-
+Match model
 BERTÃ¦Â–Â‡Ã¦ÂœÂ¬Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¥ÂÂŸÃ§Â”ÂŸBERTÃ¥ÂŒÂ¹Ã©Â…ÂÃ§Â½Â‘Ã§Â»ÂœÃ§Â»Â“Ã¦ÂÂ„Ã¯Â¼ÂŒÃ¤ÂºÂ¤Ã¤ÂºÂ’Ã¥Â¼ÂÃ¥ÂÂ¥Ã¥ÂÂ‘Ã©Â‡ÂÃ¥ÂŒÂ¹Ã©Â…ÂÃ¦Â¨Â¡Ã¥ÂÂ‹
 Network structure: Training and inference: [docs/bert-fc-train.png]
 Ã¨Â®Â­Ã§Â»ÂƒÃ¨Â„ÂšÃ¦ÂœÂ¬Ã¥ÂÂŒÃ¤Â¸ÂŠ[examples/training_sup_text_matching_model.py](https://
 github.com/shibing624/text2vec/blob/master/examples/
-training_sup_text_matching_model.py)Ã£Â€Â‚ ## Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â’Â¸Ã©Â¦ÂÃ¯Â¼ÂˆModel DistillationÃ¯Â¼Â‰
-Ã§Â”Â±Ã¤ÂºÂtext2vecÃ¨Â®Â­Ã§Â»ÂƒÃ§ÂšÂ„Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¥ÂÂ¯Ã¤Â»Â¥Ã¤Â½Â¿Ã§Â”Â¨[sentence-transformers](https://
-github.com/UKPLab/sentence-
+training_sup_text_matching_model.py)Ã£Â€Â‚ ### Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â’Â¸Ã©Â¦ÂÃ¯Â¼ÂˆModel
+DistillationÃ¯Â¼Â‰ Ã§Â”Â±Ã¤ÂºÂtext2vecÃ¨Â®Â­Ã§Â»ÂƒÃ§ÂšÂ„Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¥ÂÂ¯Ã¤Â»Â¥Ã¤Â½Â¿Ã§Â”Â¨[sentence-
+transformers](https://github.com/UKPLab/sentence-
 transformers)Ã¥ÂºÂ“Ã¥ÂŠÂ Ã¨Â½Â½Ã¯Â¼ÂŒÃ¦Â­Â¤Ã¥Â¤Â„Ã¥Â¤ÂÃ§Â”Â¨Ã¥Â…Â¶Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â’Â¸Ã©Â¦ÂÃ¦Â–Â¹Ã¦Â³Â•[distillation](https:
 //github.com/UKPLab/sentence-transformers/tree/master/examples/training/
 distillation)Ã£Â€Â‚ 1. Ã¦Â¨Â¡Ã¥ÂÂ‹Ã©Â™ÂÃ§Â»Â´Ã¯Â¼ÂŒÃ¥ÂÂ‚Ã¨Â€Âƒ[dimensionality_reduction.py](https://
 github.com/UKPLab/sentence-transformers/blob/master/examples/training/
 distillation/
 dimensionality_reduction.py)Ã¤Â½Â¿Ã§Â”Â¨PCAÃ¥Â¯Â¹Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â¾Â“Ã¥Â‡ÂºembeddingÃ©Â™ÂÃ§Â»Â´Ã¯Â¼ÂŒÃ¥ÂÂ¯Ã¥Â‡ÂÃ¥Â°Â‘milvusÃ§Â­Â‰Ã¥ÂÂ‘Ã©Â‡ÂÃ¦Â£Â€Ã§Â´Â¢Ã¦Â•Â°Ã¦ÂÂ®Ã¥ÂºÂ“Ã§ÂšÂ„Ã¥Â­Â˜Ã¥Â‚Â¨Ã¥ÂÂ‹Ã¥ÂŠÂ›Ã¯Â¼ÂŒÃ¨Â¿Â˜Ã¨ÂƒÂ½Ã¨Â½Â»Ã¥Â¾Â®Ã¦ÂÂÃ¥ÂÂ‡Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¦Â•ÂˆÃ¦ÂÂœÃ£Â€Â‚
 2. Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â’Â¸Ã©Â¦ÂÃ¯Â¼ÂŒÃ¥ÂÂ‚Ã¨Â€Âƒ[model_distillation.py](https://github.com/UKPLab/
 sentence-transformers/blob/master/examples/training/distillation/
 model_distillation.py)Ã¤Â½Â¿Ã§Â”Â¨Ã¨Â’Â¸Ã©Â¦ÂÃ¦Â–Â¹Ã¦Â³Â•Ã¯Â¼ÂŒÃ¥Â°Â†TeacherÃ¥Â¤Â§Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¨Â’Â¸Ã©Â¦ÂÃ¥ÂˆÂ°Ã¦Â›Â´Ã¥Â°Â‘layersÃ¥Â±Â‚Ã¦Â•Â°Ã§ÂšÂ„studentÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¤Â¸Â­Ã¯Â¼ÂŒÃ¥ÂœÂ¨Ã¦ÂÂƒÃ¨Â¡Â¡Ã¦Â•ÂˆÃ¦ÂÂœÃ§ÂšÂ„Ã¦ÂƒÂ…Ã¥Â†ÂµÃ¤Â¸Â‹Ã¯Â¼ÂŒÃ¥ÂÂ¯Ã¥Â¤Â§Ã¥Â¹Â…Ã¦ÂÂÃ¥ÂÂ‡Ã¦Â¨Â¡Ã¥ÂÂ‹Ã©Â¢Â„Ã¦ÂµÂ‹Ã©Â€ÂŸÃ¥ÂºÂ¦Ã£Â€Â‚
-## Ã¦Â¨Â¡Ã¥ÂÂ‹Ã©ÂƒÂ¨Ã§Â½Â² Ã¦ÂÂÃ¤Â¾Â›Ã¤Â¸Â¤Ã§Â§ÂÃ©ÂƒÂ¨Ã§Â½Â²Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¦ÂÂ­Ã¥Â»ÂºÃ¦ÂœÂÃ¥ÂŠÂ¡Ã§ÂšÂ„Ã¦Â–Â¹Ã¦Â³Â•Ã¯Â¼Âš
+### Ã¦Â¨Â¡Ã¥ÂÂ‹Ã©ÂƒÂ¨Ã§Â½Â² Ã¦ÂÂÃ¤Â¾Â›Ã¤Â¸Â¤Ã§Â§ÂÃ©ÂƒÂ¨Ã§Â½Â²Ã¦Â¨Â¡Ã¥ÂÂ‹Ã¯Â¼ÂŒÃ¦ÂÂ­Ã¥Â»ÂºÃ¦ÂœÂÃ¥ÂŠÂ¡Ã§ÂšÂ„Ã¦Â–Â¹Ã¦Â³Â•Ã¯Â¼Âš
 1Ã¯Â¼Â‰Ã¥ÂŸÂºÃ¤ÂºÂJinaÃ¦ÂÂ­Ã¥Â»ÂºgRPCÃ¦ÂœÂÃ¥ÂŠÂ¡Ã£Â€ÂÃ¦ÂÂ¨Ã¨ÂÂÃ£Â€Â‘Ã¯Â¼Â›2Ã¯Â¼Â‰Ã¥ÂŸÂºÃ¤ÂºÂFastAPIÃ¦ÂÂ­Ã¥Â»ÂºÃ¥ÂÂŸÃ§Â”ÂŸHttpÃ¦ÂœÂÃ¥ÂŠÂ¡Ã£Â€Â‚
-### JinaÃ¦ÂœÂÃ¥ÂŠÂ¡ Ã©Â‡Â‡Ã§Â”Â¨C/
+#### JinaÃ¦ÂœÂÃ¥ÂŠÂ¡ Ã©Â‡Â‡Ã§Â”Â¨C/
 SÃ¦Â¨Â¡Ã¥Â¼ÂÃ¦ÂÂ­Ã¥Â»ÂºÃ©Â«Â˜Ã¦Â€Â§Ã¨ÂƒÂ½Ã¦ÂœÂÃ¥ÂŠÂ¡Ã¯Â¼ÂŒÃ¦Â”Â¯Ã¦ÂŒÂdockerÃ¤ÂºÂ‘Ã¥ÂÂŸÃ§Â”ÂŸÃ¯Â¼ÂŒgRPC/HTTP/
 WebSocketÃ¯Â¼ÂŒÃ¦Â”Â¯Ã¦ÂŒÂÃ¥Â¤ÂšÃ¤Â¸ÂªÃ¦Â¨Â¡Ã¥ÂÂ‹Ã¥ÂÂŒÃ¦Â—Â¶Ã©Â¢Â„Ã¦ÂµÂ‹Ã¯Â¼ÂŒGPUÃ¥Â¤ÂšÃ¥ÂÂ¡Ã¥Â¤Â„Ã§ÂÂ†Ã£Â€Â‚ - Ã¥Â®Â‰Ã¨Â£Â…Ã¯Â¼Âš
 ```pip install jina``` - Ã¥ÂÂ¯Ã¥ÂŠÂ¨Ã¦ÂœÂÃ¥ÂŠÂ¡Ã¯Â¼Âš example: [examples/
 jina_server_demo.py](examples/jina_server_demo.py) ```python from jina import
 Flow port = 50001 f = Flow(port=port).add( uses='jinahub://Text2vecEncoder',
 uses_with={'model_name': 'shibing624/text2vec-base-chinese'} ) with f: #
 backend server forever f.block() ```
@@ -424,27 +447,27 @@
 Ã¨Â°ÂƒÃ§Â”Â¨Ã¦ÂœÂÃ¥ÂŠÂ¡Ã¯Â¼Âš ```python from jina import Client from docarray import
 Document, DocumentArray port = 50001 c = Client(port=port) data =
 ['Ã¥Â¦Â‚Ã¤Â½Â•Ã¦Â›Â´Ã¦ÂÂ¢Ã¨ÂŠÂ±Ã¥Â‘Â—Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡', 'Ã¨ÂŠÂ±Ã¥Â‘Â—Ã¦Â›Â´Ã¦Â”Â¹Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡'] print
 ("data:", data) print('data embs:') r = c.post('/', inputs=DocumentArray(
 [Document(text='Ã¥Â¦Â‚Ã¤Â½Â•Ã¦Â›Â´Ã¦ÂÂ¢Ã¨ÂŠÂ±Ã¥Â‘Â—Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡'), Document
 (text='Ã¨ÂŠÂ±Ã¥Â‘Â—Ã¦Â›Â´Ã¦Â”Â¹Ã§Â»Â‘Ã¥Â®ÂšÃ©Â“Â¶Ã¨Â¡ÂŒÃ¥ÂÂ¡')])) print(r.embeddings) ```
 Ã¦Â‰Â¹Ã©Â‡ÂÃ¨Â°ÂƒÃ§Â”Â¨Ã¦Â–Â¹Ã¦Â³Â•Ã¨Â§Âexample: [examples/jina_client_demo.py](https://
-github.com/shibing624/text2vec/blob/master/examples/jina_client_demo.py) ###
+github.com/shibing624/text2vec/blob/master/examples/jina_client_demo.py) ####
 FastAPIÃ¦ÂœÂÃ¥ÂŠÂ¡ - Ã¥Â®Â‰Ã¨Â£Â…Ã¯Â¼Âš ```pip install fastapi uvicorn``` - Ã¥ÂÂ¯Ã¥ÂŠÂ¨Ã¦ÂœÂÃ¥ÂŠÂ¡Ã¯Â¼Âš
 example: [examples/fastapi_server_demo.py](https://github.com/shibing624/
 text2vec/blob/master/examples/fastapi_server_demo.py) ```shell cd examples
 python fastapi_server_demo.py ``` - Ã¨Â°ÂƒÃ§Â”Â¨Ã¦ÂœÂÃ¥ÂŠÂ¡Ã¯Â¼Âš ```shell curl -X 'GET' \
 'http://0.0.0.0:8001/emb?q=hello' \ -H 'accept: application/json' ``` ##
-Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â† - Ã¦ÂœÂ¬Ã©Â¡Â¹Ã§Â›Â®releaseÃ§ÂšÂ„Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼Âš | Dataset | Introduce | Download
-Link | |:---------------------------|:-----------------------------------------
---------------------------------|:---------------------------------------------
+Dataset - Ã¦ÂœÂ¬Ã©Â¡Â¹Ã§Â›Â®releaseÃ§ÂšÂ„Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼Âš | Dataset | Introduce | Download Link
+| |:---------------------------|:----------------------------------------------
+---------------------------|:--------------------------------------------------
 -------------------------------------------------------------------------------
 -------------------------------------------------------------------------------
 -------------------------------------------------------------------------------
-------------| | shibing624/nli-zh-all |
+-------| | shibing624/nli-zh-all |
 Ã¤Â¸Â­Ã¦Â–Â‡Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã¥ÂÂˆÃ©Â›Â†Ã¯Â¼ÂŒÃ¦Â•Â´Ã¥ÂÂˆÃ¤ÂºÂ†Ã¦Â–Â‡Ã¦ÂœÂ¬Ã¦ÂÂ¨Ã§ÂÂ†Ã¯Â¼ÂŒÃ§Â›Â¸Ã¤Â¼Â¼Ã¯Â¼ÂŒÃ¦Â‘Â˜Ã¨Â¦ÂÃ¯Â¼ÂŒÃ©Â—Â®Ã§Â­Â”Ã¯Â¼ÂŒÃ¦ÂŒÂ‡Ã¤Â»Â¤Ã¥Â¾Â®Ã¨Â°ÂƒÃ§Â­Â‰Ã¤Â»Â»Ã¥ÂŠÂ¡Ã§ÂšÂ„820Ã¤Â¸Â‡Ã©Â«Â˜Ã¨Â´Â¨Ã©Â‡ÂÃ¦Â•Â°Ã¦ÂÂ®Ã¯Â¼ÂŒÃ¥Â¹Â¶Ã¨Â½Â¬Ã¥ÂŒÂ–Ã¤Â¸ÂºÃ¥ÂŒÂ¹Ã©Â…ÂÃ¦Â Â¼Ã¥Â¼ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†
 | [https://huggingface.co/datasets/shibing624/nli-zh-all](https://
 huggingface.co/datasets/shibing624/nli-zh-all) | | shibing624/snli-zh |
 Ã¤Â¸Â­Ã¦Â–Â‡SNLIÃ¥Â’ÂŒMultiNLIÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼ÂŒÃ§Â¿Â»Ã¨Â¯Â‘Ã¨Â‡ÂªÃ¨Â‹Â±Ã¦Â–Â‡SNLIÃ¥Â’ÂŒMultiNLI | [https://
 huggingface.co/datasets/shibing624/snli-zh](https://huggingface.co/datasets/
 shibing624/snli-zh) | | shibing624/nli_zh |
 Ã¤Â¸Â­Ã¦Â–Â‡Ã¨Â¯Â­Ã¤Â¹Â‰Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼ÂŒÃ¦Â•Â´Ã¥ÂÂˆÃ¤ÂºÂ†Ã¤Â¸Â­Ã¦Â–Â‡ATECÃ£Â€ÂBQÃ£Â€ÂLCQMCÃ£Â€ÂPAWSXÃ£Â€ÂSTS-
@@ -462,46 +485,46 @@
 info/1037/1162.htm) | | LCQMC | Ã¤Â¸Â­Ã¦Â–Â‡LCQMC(large-scale Chinese question
 matching corpus)Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼ÂŒQ-QpairÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â† | [LCQMC](http://
 icrc.hitsz.edu.cn/Article/show/171.html) | | PAWSX | Ã¤Â¸Â­Ã¦Â–Â‡PAWS(Paraphrase
 Adversaries from Word Scrambling)Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼ÂŒQ-QpairÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â† | [PAWSX](https:/
 /arxiv.org/abs/1908.11828) | | STS-B | Ã¤Â¸Â­Ã¦Â–Â‡STS-
 BÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼ÂŒÃ¤Â¸Â­Ã¦Â–Â‡Ã¨Â‡ÂªÃ§Â„Â¶Ã¨Â¯Â­Ã¨Â¨Â€Ã¦ÂÂ¨Ã§ÂÂ†Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼ÂŒÃ¤Â»ÂÃ¨Â‹Â±Ã¦Â–Â‡STS-
 BÃ§Â¿Â»Ã¨Â¯Â‘Ã¤Â¸ÂºÃ¤Â¸Â­Ã¦Â–Â‡Ã§ÂšÂ„Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â† | [STS-B](https://github.com/pluto-junzeng/CNSD) |
-Ã¥Â¸Â¸Ã§Â”Â¨Ã¨Â‹Â±Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼Âš - Ã¥Â¤Â§Ã¥ÂÂÃ©Â¼ÂÃ©Â¼ÂÃ§ÂšÂ„multi_nliÃ¥Â’ÂŒsnli: https://
-huggingface.co/datasets/multi_nli - Ã¥Â¤Â§Ã¥ÂÂÃ©Â¼ÂÃ©Â¼ÂÃ§ÂšÂ„multi_nliÃ¥Â’ÂŒsnli: https://
+Ã¥Â¸Â¸Ã§Â”Â¨Ã¨Â‹Â±Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼Âš - Ã¨Â‹Â±Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼Âšmulti_nli: https://
+huggingface.co/datasets/multi_nli - Ã¨Â‹Â±Ã¦Â–Â‡Ã¥ÂŒÂ¹Ã©Â…ÂÃ¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¯Â¼Âšsnli: https://
 huggingface.co/datasets/snli - https://huggingface.co/datasets/metaeval/cnli -
 https://huggingface.co/datasets/mteb/stsbenchmark-sts - https://huggingface.co/
 datasets/JeremiahZ/simcse_sup_nli - https://huggingface.co/datasets/
 MoritzLaurer/multilingual-NLI-26lang-2mil7 Ã¦Â•Â°Ã¦ÂÂ®Ã©Â›Â†Ã¤Â½Â¿Ã§Â”Â¨Ã§Â¤ÂºÃ¤Â¾Â‹Ã¯Â¼Âš ```shell
 pip install datasets ``` ```python from datasets import load_dataset dataset =
 load_dataset("shibing624/nli_zh", "STS-B") # ATEC or BQ or LCQMC or PAWSX or
 STS-B print(dataset) print(dataset['test'][0]) ``` output: ```shell DatasetDict
 ({ train: Dataset({ features: ['sentence1', 'sentence2', 'label'], num_rows:
 5231 }) validation: Dataset({ features: ['sentence1', 'sentence2', 'label'],
 num_rows: 1458 }) test: Dataset({ features: ['sentence1', 'sentence2',
 'label'], num_rows: 1361 }) }) {'sentence1':
 'Ã¤Â¸Â€Ã¤Â¸ÂªÃ¥Â¥Â³Ã¥Â­Â©Ã¥ÂœÂ¨Ã§Â»Â™Ã¥Â¥Â¹Ã§ÂšÂ„Ã¥Â¤Â´Ã¥ÂÂ‘Ã¥ÂÂšÃ¥ÂÂ‘Ã¥ÂÂ‹Ã£Â€Â‚', 'sentence2':
-'Ã¤Â¸Â€Ã¤Â¸ÂªÃ¥Â¥Â³Ã¥Â­Â©Ã¥ÂœÂ¨Ã¦Â¢Â³Ã¥Â¤Â´Ã£Â€Â‚', 'label': 2} ``` # Contact - Issue(Ã¥Â»ÂºÃ¨Â®Â®)Ã¯Â¼Âš[!
+'Ã¤Â¸Â€Ã¤Â¸ÂªÃ¥Â¥Â³Ã¥Â­Â©Ã¥ÂœÂ¨Ã¦Â¢Â³Ã¥Â¤Â´Ã£Â€Â‚', 'label': 2} ``` ## Contact - Issue(Ã¥Â»ÂºÃ¨Â®Â®)Ã¯Â¼Âš[!
 [GitHub issues](https://img.shields.io/github/issues/shibing624/text2vec.svg)]
 (https://github.com/shibing624/text2vec/issues) - Ã©Â‚Â®Ã¤Â»Â¶Ã¦ÂˆÂ‘Ã¯Â¼Âšxuming:
 xuming624@qq.com - Ã¥Â¾Â®Ã¤Â¿Â¡Ã¦ÂˆÂ‘Ã¯Â¼ÂšÃ¥ÂŠÂ Ã¦ÂˆÂ‘*Ã¥Â¾Â®Ã¤Â¿Â¡Ã¥ÂÂ·Ã¯Â¼Âšxuming624, Ã¥Â¤Â‡Ã¦Â³Â¨Ã¯Â¼ÂšÃ¥Â§Â“Ã¥ÂÂ-
-Ã¥Â…Â¬Ã¥ÂÂ¸-NLP* Ã¨Â¿Â›NLPÃ¤ÂºÂ¤Ã¦ÂµÂÃ§Â¾Â¤Ã£Â€Â‚ [docs/wechat.jpeg] # Citation
+Ã¥Â…Â¬Ã¥ÂÂ¸-NLP* Ã¨Â¿Â›NLPÃ¤ÂºÂ¤Ã¦ÂµÂÃ§Â¾Â¤Ã£Â€Â‚ [docs/wechat.jpeg] ## Citation
 Ã¥Â¦Â‚Ã¦ÂÂœÃ¤Â½Â Ã¥ÂœÂ¨Ã§Â Â”Ã§Â©Â¶Ã¤Â¸Â­Ã¤Â½Â¿Ã§Â”Â¨Ã¤ÂºÂ†text2vecÃ¯Â¼ÂŒÃ¨Â¯Â·Ã¦ÂŒÂ‰Ã¥Â¦Â‚Ã¤Â¸Â‹Ã¦Â Â¼Ã¥Â¼ÂÃ¥Â¼Â•Ã§Â”Â¨Ã¯Â¼Âš APA:
 ```latex Xu, M. Text2vec: Text to vector toolkit (Version 1.1.2) [Computer
 software]. https://github.com/shibing624/text2vec ``` BibTeX: ```latex @misc
-{Text2vec, author = {Xu, Ming}, title = {Text2vec: Text to vector toolkit},
-year = {2022}, publisher = {GitHub}, journal = {GitHub repository},
-howpublished = {\url{https://github.com/shibing624/text2vec}}, } ``` # License
+{Text2vec, author = {Ming Xu}, title = {Text2vec: Text to vector toolkit}, year
+= {2023}, publisher = {GitHub}, journal = {GitHub repository}, howpublished =
+{\url{https://github.com/shibing624/text2vec}}, } ``` ## License
 Ã¦ÂÂˆÃ¦ÂÂƒÃ¥ÂÂÃ¨Â®Â®Ã¤Â¸Âº [The Apache License 2.0]
 (LICENSE)Ã¯Â¼ÂŒÃ¥ÂÂ¯Ã¥Â…ÂÃ¨Â´Â¹Ã§Â”Â¨Ã¥ÂÂšÃ¥Â•Â†Ã¤Â¸ÂšÃ§Â”Â¨Ã©Â€Â”Ã£Â€Â‚Ã¨Â¯Â·Ã¥ÂœÂ¨Ã¤ÂºÂ§Ã¥Â“ÂÃ¨Â¯Â´Ã¦Â˜ÂÃ¤Â¸Â­Ã©Â™Â„Ã¥ÂŠÂ text2vecÃ§ÂšÂ„Ã©Â“Â¾Ã¦ÂÂ¥Ã¥Â’ÂŒÃ¦ÂÂˆÃ¦ÂÂƒÃ¥ÂÂÃ¨Â®Â®Ã£Â€Â‚
-# Contribute
+## Contribute
 Ã©Â¡Â¹Ã§Â›Â®Ã¤Â»Â£Ã§Â ÂÃ¨Â¿Â˜Ã¥Â¾ÂˆÃ§Â²Â—Ã§Â³Â™Ã¯Â¼ÂŒÃ¥Â¦Â‚Ã¦ÂÂœÃ¥Â¤Â§Ã¥Â®Â¶Ã¥Â¯Â¹Ã¤Â»Â£Ã§Â ÂÃ¦ÂœÂ‰Ã¦Â‰Â€Ã¦Â”Â¹Ã¨Â¿Â›Ã¯Â¼ÂŒÃ¦Â¬Â¢Ã¨Â¿ÂÃ¦ÂÂÃ¤ÂºÂ¤Ã¥Â›ÂÃ¦ÂœÂ¬Ã©Â¡Â¹Ã§Â›Â®Ã¯Â¼ÂŒÃ¥ÂœÂ¨Ã¦ÂÂÃ¤ÂºÂ¤Ã¤Â¹Â‹Ã¥Â‰ÂÃ¯Â¼ÂŒÃ¦Â³Â¨Ã¦Â„ÂÃ¤Â»Â¥Ã¤Â¸Â‹Ã¤Â¸Â¤Ã§Â‚Â¹Ã¯Â¼Âš
 - Ã¥ÂœÂ¨`tests`Ã¦Â·Â»Ã¥ÂŠÂ Ã§Â›Â¸Ã¥ÂºÂ”Ã§ÂšÂ„Ã¥ÂÂ•Ã¥Â…ÂƒÃ¦ÂµÂ‹Ã¨Â¯Â• - Ã¤Â½Â¿Ã§Â”Â¨`python -m pytest -
 v`Ã¦ÂÂ¥Ã¨Â¿ÂÃ¨Â¡ÂŒÃ¦Â‰Â€Ã¦ÂœÂ‰Ã¥ÂÂ•Ã¥Â…ÂƒÃ¦ÂµÂ‹Ã¨Â¯Â•Ã¯Â¼ÂŒÃ§Â¡Â®Ã¤Â¿ÂÃ¦Â‰Â€Ã¦ÂœÂ‰Ã¥ÂÂ•Ã¦ÂµÂ‹Ã©ÂƒÂ½Ã¦Â˜Â¯Ã©Â€ÂšÃ¨Â¿Â‡Ã§ÂšÂ„
-Ã¤Â¹Â‹Ã¥ÂÂÃ¥ÂÂ³Ã¥ÂÂ¯Ã¦ÂÂÃ¤ÂºÂ¤PRÃ£Â€Â‚ # Reference -
+Ã¤Â¹Â‹Ã¥ÂÂÃ¥ÂÂ³Ã¥ÂÂ¯Ã¦ÂÂÃ¤ÂºÂ¤PRÃ£Â€Â‚ ## References -
 [Ã¥Â°Â†Ã¥ÂÂ¥Ã¥Â­ÂÃ¨Â¡Â¨Ã§Â¤ÂºÃ¤Â¸ÂºÃ¥ÂÂ‘Ã©Â‡ÂÃ¯Â¼ÂˆÃ¤Â¸ÂŠÃ¯Â¼Â‰Ã¯Â¼ÂšÃ¦Â—Â Ã§Â›Â‘Ã§ÂÂ£Ã¥ÂÂ¥Ã¥Â­ÂÃ¨Â¡Â¨Ã§Â¤ÂºÃ¥Â­Â¦Ã¤Â¹Â Ã¯Â¼Âˆsentence
 embeddingÃ¯Â¼Â‰](https://www.cnblogs.com/llhthinker/p/10335164.html) -
 [Ã¥Â°Â†Ã¥ÂÂ¥Ã¥Â­ÂÃ¨Â¡Â¨Ã§Â¤ÂºÃ¤Â¸ÂºÃ¥ÂÂ‘Ã©Â‡ÂÃ¯Â¼ÂˆÃ¤Â¸Â‹Ã¯Â¼Â‰Ã¯Â¼ÂšÃ¦Â—Â Ã§Â›Â‘Ã§ÂÂ£Ã¥ÂÂ¥Ã¥Â­ÂÃ¨Â¡Â¨Ã§Â¤ÂºÃ¥Â­Â¦Ã¤Â¹Â Ã¯Â¼Âˆsentence
 embeddingÃ¯Â¼Â‰](https://www.cnblogs.com/llhthinker/p/10341841.html) - [A Simple
 but Tough-to-Beat Baseline for Sentence Embeddings[Sanjeev Arora and Yingyu
 Liang and Tengyu Ma, 2017]](https://openreview.net/forum?id=SyK00v5xx) -
 [Ã¥Â›Â›Ã§Â§ÂÃ¨Â®Â¡Ã§Â®Â—Ã¦Â–Â‡Ã¦ÂœÂ¬Ã§Â›Â¸Ã¤Â¼Â¼Ã¥ÂºÂ¦Ã§ÂšÂ„Ã¦Â–Â¹Ã¦Â³Â•Ã¥Â¯Â¹Ã¦Â¯Â”[Yves Peirsman]](https://
```

### Comparing `text2vec-1.2.1/text2vec.egg-info/SOURCES.txt` & `text2vec-1.2.2/text2vec.egg-info/SOURCES.txt`

 * *Files identical despite different names*

