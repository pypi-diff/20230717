# Comparing `tmp/text2vec-1.2.1.tar.gz` & `tmp/text2vec-1.2.2.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "text2vec-1.2.1.tar", last modified: Wed Jun 21 03:03:37 2023, max compression
+gzip compressed data, was "text2vec-1.2.2.tar", last modified: Mon Jul 17 08:29:15 2023, max compression
```

## Comparing `text2vec-1.2.1.tar` & `text2vec-1.2.2.tar`

### file list

```diff
@@ -1,35 +1,35 @@
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-06-21 03:03:37.425662 text2vec-1.2.1/
--rw-r--r--   0 xuming     (501) staff       (20)    49036 2023-06-21 03:03:37.425133 text2vec-1.2.1/PKG-INFO
--rw-r--r--   0 xuming     (501) staff       (20)    42020 2023-06-21 02:19:56.000000 text2vec-1.2.1/README.md
--rw-r--r--   0 xuming     (501) staff       (20)       38 2023-06-21 03:03:37.425895 text2vec-1.2.1/setup.cfg
--rw-r--r--   0 xuming     (501) staff       (20)     1606 2023-06-14 08:11:02.000000 text2vec-1.2.1/setup.py
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-06-21 03:03:37.386479 text2vec-1.2.1/text2vec/
--rw-r--r--   0 xuming     (501) staff       (20)     1221 2023-06-14 14:22:28.000000 text2vec-1.2.1/text2vec/__init__.py
--rw-r--r--   0 xuming     (501) staff       (20)     3715 2023-06-14 14:12:57.000000 text2vec-1.2.1/text2vec/bertmatching_dataset.py
--rw-r--r--   0 xuming     (501) staff       (20)    18322 2023-06-15 02:51:06.000000 text2vec-1.2.1/text2vec/bertmatching_model.py
--rw-r--r--   0 xuming     (501) staff       (20)     1803 2023-06-14 08:11:02.000000 text2vec-1.2.1/text2vec/bm25.py
--rw-r--r--   0 xuming     (501) staff       (20)     3377 2023-06-18 04:55:14.000000 text2vec-1.2.1/text2vec/cosent_dataset.py
--rw-r--r--   0 xuming     (501) staff       (20)    13840 2023-06-15 02:51:06.000000 text2vec-1.2.1/text2vec/cosent_model.py
--rw-r--r--   0 xuming     (501) staff       (20)     3252 2023-06-14 12:45:18.000000 text2vec-1.2.1/text2vec/ngram.py
--rw-r--r--   0 xuming     (501) staff       (20)    12378 2023-06-20 03:07:55.000000 text2vec-1.2.1/text2vec/sentence_model.py
--rw-r--r--   0 xuming     (501) staff       (20)    14566 2023-06-15 02:51:06.000000 text2vec-1.2.1/text2vec/sentencebert_model.py
--rw-r--r--   0 xuming     (501) staff       (20)     9813 2023-06-20 03:12:38.000000 text2vec-1.2.1/text2vec/similarity.py
--rw-r--r--   0 xuming     (501) staff       (20)     9136 2021-09-11 10:12:24.000000 text2vec-1.2.1/text2vec/stopwords.txt
--rw-r--r--   0 xuming     (501) staff       (20)     6358 2023-06-18 04:53:18.000000 text2vec-1.2.1/text2vec/text_matching_dataset.py
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-06-21 03:03:37.421648 text2vec-1.2.1/text2vec/utils/
--rw-r--r--   0 xuming     (501) staff       (20)        0 2021-09-11 10:12:24.000000 text2vec-1.2.1/text2vec/utils/__init__.py
--rw-r--r--   0 xuming     (501) staff       (20)     6413 2022-01-21 13:50:54.000000 text2vec-1.2.1/text2vec/utils/distance.py
--rw-r--r--   0 xuming     (501) staff       (20)    15066 2022-03-11 08:18:40.000000 text2vec-1.2.1/text2vec/utils/get_file.py
--rw-r--r--   0 xuming     (501) staff       (20)     1595 2023-06-14 13:27:05.000000 text2vec-1.2.1/text2vec/utils/io_util.py
--rw-r--r--   0 xuming     (501) staff       (20)     5630 2022-02-13 15:54:49.000000 text2vec-1.2.1/text2vec/utils/rank_bm25.py
--rw-r--r--   0 xuming     (501) staff       (20)      859 2022-02-25 17:04:46.000000 text2vec-1.2.1/text2vec/utils/stats_util.py
--rw-r--r--   0 xuming     (501) staff       (20)     1916 2022-01-23 04:13:14.000000 text2vec-1.2.1/text2vec/utils/tokenizer.py
--rw-r--r--   0 xuming     (501) staff       (20)       84 2023-06-21 03:03:19.000000 text2vec-1.2.1/text2vec/version.py
--rw-r--r--   0 xuming     (501) staff       (20)     6180 2023-06-14 12:45:19.000000 text2vec-1.2.1/text2vec/word2vec.py
-drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-06-21 03:03:37.413588 text2vec-1.2.1/text2vec.egg-info/
--rw-r--r--   0 xuming     (501) staff       (20)    49036 2023-06-21 03:03:37.000000 text2vec-1.2.1/text2vec.egg-info/PKG-INFO
--rw-r--r--   0 xuming     (501) staff       (20)      751 2023-06-21 03:03:37.000000 text2vec-1.2.1/text2vec.egg-info/SOURCES.txt
--rw-r--r--   0 xuming     (501) staff       (20)        1 2023-06-21 03:03:37.000000 text2vec-1.2.1/text2vec.egg-info/dependency_links.txt
--rw-r--r--   0 xuming     (501) staff       (20)        1 2022-05-17 02:41:49.000000 text2vec-1.2.1/text2vec.egg-info/not-zip-safe
--rw-r--r--   0 xuming     (501) staff       (20)       74 2023-06-21 03:03:37.000000 text2vec-1.2.1/text2vec.egg-info/requires.txt
--rw-r--r--   0 xuming     (501) staff       (20)        9 2023-06-21 03:03:37.000000 text2vec-1.2.1/text2vec.egg-info/top_level.txt
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-07-17 08:29:15.290018 text2vec-1.2.2/
+-rw-r--r--   0 xuming     (501) staff       (20)    51655 2023-07-17 08:29:15.289376 text2vec-1.2.2/PKG-INFO
+-rw-r--r--   0 xuming     (501) staff       (20)    44575 2023-07-16 07:50:04.000000 text2vec-1.2.2/README.md
+-rw-r--r--   0 xuming     (501) staff       (20)       38 2023-07-17 08:29:15.290386 text2vec-1.2.2/setup.cfg
+-rw-r--r--   0 xuming     (501) staff       (20)     1606 2023-06-14 08:11:02.000000 text2vec-1.2.2/setup.py
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-07-17 08:29:15.273036 text2vec-1.2.2/text2vec/
+-rw-r--r--   0 xuming     (501) staff       (20)     1221 2023-06-14 14:22:28.000000 text2vec-1.2.2/text2vec/__init__.py
+-rw-r--r--   0 xuming     (501) staff       (20)     3715 2023-06-14 14:12:57.000000 text2vec-1.2.2/text2vec/bertmatching_dataset.py
+-rw-r--r--   0 xuming     (501) staff       (20)    19099 2023-07-16 04:07:08.000000 text2vec-1.2.2/text2vec/bertmatching_model.py
+-rw-r--r--   0 xuming     (501) staff       (20)     1803 2023-06-14 08:11:02.000000 text2vec-1.2.2/text2vec/bm25.py
+-rw-r--r--   0 xuming     (501) staff       (20)     3377 2023-06-18 04:55:14.000000 text2vec-1.2.2/text2vec/cosent_dataset.py
+-rw-r--r--   0 xuming     (501) staff       (20)    14698 2023-07-16 04:07:08.000000 text2vec-1.2.2/text2vec/cosent_model.py
+-rw-r--r--   0 xuming     (501) staff       (20)     3252 2023-06-14 12:45:18.000000 text2vec-1.2.2/text2vec/ngram.py
+-rw-r--r--   0 xuming     (501) staff       (20)    12614 2023-07-16 04:07:08.000000 text2vec-1.2.2/text2vec/sentence_model.py
+-rw-r--r--   0 xuming     (501) staff       (20)    15952 2023-07-16 04:07:08.000000 text2vec-1.2.2/text2vec/sentencebert_model.py
+-rw-r--r--   0 xuming     (501) staff       (20)     9813 2023-06-20 03:12:38.000000 text2vec-1.2.2/text2vec/similarity.py
+-rw-r--r--   0 xuming     (501) staff       (20)     9136 2021-09-11 10:12:24.000000 text2vec-1.2.2/text2vec/stopwords.txt
+-rw-r--r--   0 xuming     (501) staff       (20)     6358 2023-06-18 04:53:18.000000 text2vec-1.2.2/text2vec/text_matching_dataset.py
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-07-17 08:29:15.287690 text2vec-1.2.2/text2vec/utils/
+-rw-r--r--   0 xuming     (501) staff       (20)        0 2021-09-11 10:12:24.000000 text2vec-1.2.2/text2vec/utils/__init__.py
+-rw-r--r--   0 xuming     (501) staff       (20)     6506 2023-07-17 08:26:35.000000 text2vec-1.2.2/text2vec/utils/distance.py
+-rw-r--r--   0 xuming     (501) staff       (20)    15066 2022-03-11 08:18:40.000000 text2vec-1.2.2/text2vec/utils/get_file.py
+-rw-r--r--   0 xuming     (501) staff       (20)     1595 2023-06-14 13:27:05.000000 text2vec-1.2.2/text2vec/utils/io_util.py
+-rw-r--r--   0 xuming     (501) staff       (20)     5630 2022-02-13 15:54:49.000000 text2vec-1.2.2/text2vec/utils/rank_bm25.py
+-rw-r--r--   0 xuming     (501) staff       (20)      859 2022-02-25 17:04:46.000000 text2vec-1.2.2/text2vec/utils/stats_util.py
+-rw-r--r--   0 xuming     (501) staff       (20)     1916 2022-01-23 04:13:14.000000 text2vec-1.2.2/text2vec/utils/tokenizer.py
+-rw-r--r--   0 xuming     (501) staff       (20)       84 2023-07-17 08:28:26.000000 text2vec-1.2.2/text2vec/version.py
+-rw-r--r--   0 xuming     (501) staff       (20)     6180 2023-06-14 12:45:19.000000 text2vec-1.2.2/text2vec/word2vec.py
+drwxr-xr-x   0 xuming     (501) staff       (20)        0 2023-07-17 08:29:15.279471 text2vec-1.2.2/text2vec.egg-info/
+-rw-r--r--   0 xuming     (501) staff       (20)    51655 2023-07-17 08:29:14.000000 text2vec-1.2.2/text2vec.egg-info/PKG-INFO
+-rw-r--r--   0 xuming     (501) staff       (20)      751 2023-07-17 08:29:14.000000 text2vec-1.2.2/text2vec.egg-info/SOURCES.txt
+-rw-r--r--   0 xuming     (501) staff       (20)        1 2023-07-17 08:29:14.000000 text2vec-1.2.2/text2vec.egg-info/dependency_links.txt
+-rw-r--r--   0 xuming     (501) staff       (20)        1 2022-05-17 02:41:49.000000 text2vec-1.2.2/text2vec.egg-info/not-zip-safe
+-rw-r--r--   0 xuming     (501) staff       (20)       74 2023-07-17 08:29:14.000000 text2vec-1.2.2/text2vec.egg-info/requires.txt
+-rw-r--r--   0 xuming     (501) staff       (20)        9 2023-07-17 08:29:14.000000 text2vec-1.2.2/text2vec.egg-info/top_level.txt
```

### Comparing `text2vec-1.2.1/PKG-INFO` & `text2vec-1.2.2/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: text2vec
-Version: 1.2.1
+Version: 1.2.2
 Summary: Text to vector Tool, encode text
 Home-page: https://github.com/shibing624/text2vec
 Author: XuMing
 Author-email: xuming624@qq.com
 License: Apache License 2.0
 Description: [**üá®üá≥‰∏≠Êñá**](https://github.com/shibing624/text2vec/blob/master/README.md) | [**üåêEnglish**](https://github.com/shibing624/text2vec/blob/master/README_EN.md) | [**üìñÊñáÊ°£/Docs**](https://github.com/shibing624/text2vec/wiki) | [**ü§ñÊ®°Âûã/Models**](https://huggingface.co/shibing624) 
         
@@ -27,54 +27,57 @@
         
         
         **Text2vec**: Text to Vector, Get Sentence Embeddings. ÊñáÊú¨ÂêëÈáèÂåñÔºåÊääÊñáÊú¨(ÂåÖÊã¨ËØç„ÄÅÂè•Â≠ê„ÄÅÊÆµËêΩ)Ë°®ÂæÅ‰∏∫ÂêëÈáèÁü©Èòµ„ÄÇ
         
         **text2vec**ÂÆûÁé∞‰∫ÜWord2Vec„ÄÅRankBM25„ÄÅBERT„ÄÅSentence-BERT„ÄÅCoSENTÁ≠âÂ§öÁßçÊñáÊú¨Ë°®ÂæÅ„ÄÅÊñáÊú¨Áõ∏‰ººÂ∫¶ËÆ°ÁÆóÊ®°ÂûãÔºåÂπ∂Âú®ÊñáÊú¨ËØ≠‰πâÂåπÈÖçÔºàÁõ∏‰ººÂ∫¶ËÆ°ÁÆóÔºâ‰ªªÂä°‰∏äÊØîËæÉ‰∫ÜÂêÑÊ®°ÂûãÁöÑÊïàÊûú„ÄÇ
         
         ### News
+        [2023/06/22] v1.2.2ÁâàÊú¨: ÂèëÂ∏É‰∫ÜÂ§öËØ≠Ë®ÄÂåπÈÖçÊ®°Âûã[shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)ÔºåÁî®CoSENTÊñπÊ≥ïËÆ≠ÁªÉÔºåÂü∫‰∫é`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`Áî®‰∫∫Â∑•ÊåëÈÄâÂêéÁöÑÂ§öËØ≠Ë®ÄSTSÊï∞ÊçÆÈõÜ[shibing624/nli-zh-all/text2vec-base-multilingual-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-multilingual-dataset)ËÆ≠ÁªÉÂæóÂà∞ÔºåÂπ∂Âú®‰∏≠Ëã±ÊñáÊµãËØïÈõÜËØÑ‰º∞Áõ∏ÂØπ‰∫éÂéüÊ®°ÂûãÊïàÊûúÊúâÊèêÂçáÔºåËØ¶ËßÅ[Release-v1.2.2](https://github.com/shibing624/text2vec/releases/tag/1.2.2)
+        
         [2023/06/19] v1.2.1ÁâàÊú¨: Êõ¥Êñ∞‰∫Ü‰∏≠ÊñáÂåπÈÖçÊ®°Âûã`shibing624/text2vec-base-chinese-nli`‰∏∫Êñ∞Áâà[shibing624/text2vec-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-sentence)ÔºåÈíàÂØπCoSENTÁöÑlossËÆ°ÁÆóÂØπÊéíÂ∫èÊïèÊÑüÁâπÁÇπÔºå‰∫∫Â∑•ÊåëÈÄâÂπ∂Êï¥ÁêÜÂá∫È´òË¥®ÈáèÁöÑÊúâÁõ∏ÂÖ≥ÊÄßÊéíÂ∫èÁöÑSTSÊï∞ÊçÆÈõÜ[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)ÔºåÂú®ÂêÑËØÑ‰º∞ÈõÜË°®Áé∞Áõ∏ÂØπ‰πãÂâçÊúâÊèêÂçáÔºõÂèëÂ∏É‰∫ÜÈÄÇÁî®‰∫és2pÁöÑ‰∏≠ÊñáÂåπÈÖçÊ®°Âûã[shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase)ÔºåËØ¶ËßÅ[Release-v1.2.1](https://github.com/shibing624/text2vec/releases/tag/1.2.1)
         
         [2023/06/15] v1.2.0ÁâàÊú¨: ÂèëÂ∏É‰∫Ü‰∏≠ÊñáÂåπÈÖçÊ®°Âûã[shibing624/text2vec-base-chinese-nli](https://huggingface.co/shibing624/text2vec-base-chinese-nli)ÔºåÂü∫‰∫é`nghuyong/ernie-3.0-base-zh`Ê®°ÂûãÔºå‰ΩøÁî®‰∫Ü‰∏≠ÊñáNLIÊï∞ÊçÆÈõÜ[shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh)ÂÖ®ÈÉ®ËØ≠ÊñôËÆ≠ÁªÉÁöÑCoSENTÊñáÊú¨ÂåπÈÖçÊ®°ÂûãÔºåÂú®ÂêÑËØÑ‰º∞ÈõÜË°®Áé∞ÊèêÂçáÊòéÊòæÔºåËØ¶ËßÅ[Release-v1.2.0](https://github.com/shibing624/text2vec/releases/tag/1.2.0)
         
         [2022/03/12] v1.1.4ÁâàÊú¨: ÂèëÂ∏É‰∫Ü‰∏≠ÊñáÂåπÈÖçÊ®°Âûã[shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)ÔºåÂü∫‰∫é‰∏≠ÊñáSTSËÆ≠ÁªÉÈõÜËÆ≠ÁªÉÁöÑCoSENTÂåπÈÖçÊ®°Âûã„ÄÇËØ¶ËßÅ[Release-v1.1.4](https://github.com/shibing624/text2vec/releases/tag/1.1.4)
         
         
         **Guide**
-        - [Feature](#Feature)
+        - [Features](#Features)
         - [Evaluation](#Evaluation)
         - [Install](#install)
         - [Usage](#usage)
         - [Contact](#Contact)
-        - [Reference](#reference)
+        - [References](#references)
         
         
-        # Feature
+        ## Features
         ### ÊñáÊú¨ÂêëÈáèË°®Á§∫Ê®°Âûã
         - [Word2Vec](https://github.com/shibing624/text2vec/blob/master/text2vec/word2vec.py)ÔºöÈÄöËøáËÖæËÆØAI LabÂºÄÊ∫êÁöÑÂ§ßËßÑÊ®°È´òË¥®Èáè‰∏≠Êñá[ËØçÂêëÈáèÊï∞ÊçÆÔºà800‰∏á‰∏≠ÊñáËØçËΩªÈáèÁâàÔºâ](https://pan.baidu.com/s/1La4U4XNFe8s5BJqxPQpeiQ) (Êñá‰ª∂ÂêçÔºölight_Tencent_AILab_ChineseEmbedding.bin ÂØÜÁ†Å: taweÔºâÂÆûÁé∞ËØçÂêëÈáèÊ£ÄÁ¥¢ÔºåÊú¨È°πÁõÆÂÆûÁé∞‰∫ÜÂè•Â≠êÔºàËØçÂêëÈáèÊ±ÇÂπ≥ÂùáÔºâÁöÑword2vecÂêëÈáèË°®Á§∫
         - [SBERT(Sentence-BERT)](https://github.com/shibing624/text2vec/blob/master/text2vec/sentencebert_model.py)ÔºöÊùÉË°°ÊÄßËÉΩÂíåÊïàÁéáÁöÑÂè•ÂêëÈáèË°®Á§∫Ê®°ÂûãÔºåËÆ≠ÁªÉÊó∂ÈÄöËøáÊúâÁõëÁù£ËÆ≠ÁªÉ‰∏äÂ±ÇÂàÜÁ±ªÂáΩÊï∞ÔºåÊñáÊú¨ÂåπÈÖçÈ¢ÑÊµãÊó∂Áõ¥Êé•Âè•Â≠êÂêëÈáèÂÅö‰ΩôÂº¶ÔºåÊú¨È°πÁõÆÂü∫‰∫éPyTorchÂ§çÁé∞‰∫ÜSentence-BERTÊ®°ÂûãÁöÑËÆ≠ÁªÉÂíåÈ¢ÑÊµã
         - [CoSENT(Cosine Sentence)](https://github.com/shibing624/text2vec/blob/master/text2vec/cosent_model.py)ÔºöCoSENTÊ®°ÂûãÊèêÂá∫‰∫Ü‰∏ÄÁßçÊéíÂ∫èÁöÑÊçüÂ§±ÂáΩÊï∞Ôºå‰ΩøËÆ≠ÁªÉËøáÁ®ãÊõ¥Ë¥¥ËøëÈ¢ÑÊµãÔºåÊ®°ÂûãÊî∂ÊïõÈÄüÂ∫¶ÂíåÊïàÊûúÊØîSentence-BERTÊõ¥Â•ΩÔºåÊú¨È°πÁõÆÂü∫‰∫éPyTorchÂÆûÁé∞‰∫ÜCoSENTÊ®°ÂûãÁöÑËÆ≠ÁªÉÂíåÈ¢ÑÊµã
         
         ËØ¶ÁªÜÊñáÊú¨ÂêëÈáèË°®Á§∫ÊñπÊ≥ïËßÅwiki: [ÊñáÊú¨ÂêëÈáèË°®Á§∫ÊñπÊ≥ï](https://github.com/shibing624/text2vec/wiki/%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95)
-        # Evaluation
+        ## Evaluation
         
         ÊñáÊú¨ÂåπÈÖç
         
         #### Ëã±ÊñáÂåπÈÖçÊï∞ÊçÆÈõÜÁöÑËØÑÊµãÁªìÊûúÔºö
         
         
-        | Arch   | BaseModel                                        | Model                            | English-STS-B | 
-        |:-------|:------------------------------------------------|:-------------------------------------|:-------------:|
-        | GloVe  | glove                                           | Avg_word_embeddings_glove_6B_300d    |     61.77     |
-        | BERT   | bert-base-uncased                               | BERT-base-cls                        |     20.29     |
-        | BERT   | bert-base-uncased                               | BERT-base-first_last_avg             |     59.04     |
-        | BERT   | bert-base-uncased                               | BERT-base-first_last_avg-whiten(NLI) |     63.65     |
-        | SBERT  | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-cls                   |     73.65     |
-        | SBERT  | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-first_last_avg        |     77.96     |
-        | CoSENT | bert-base-uncased                               | CoSENT-base-first_last_avg           |     69.93     |
-        | CoSENT | sentence-transformers/bert-base-nli-mean-tokens | CoSENT-base-nli-first_last_avg       |     79.68     |
+        | Arch   | BaseModel                                        | Model                                                                                                                | English-STS-B | 
+        |:-------|:------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------|:-------------:|
+        | GloVe  | glove                                           | Avg_word_embeddings_glove_6B_300d                                                                                    |     61.77     |
+        | BERT   | bert-base-uncased                               | BERT-base-cls                                                                                                        |     20.29     |
+        | BERT   | bert-base-uncased                               | BERT-base-first_last_avg                                                                                             |     59.04     |
+        | BERT   | bert-base-uncased                               | BERT-base-first_last_avg-whiten(NLI)                                                                                 |     63.65     |
+        | SBERT  | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-cls                                                                                                   |     73.65     |
+        | SBERT  | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-first_last_avg                                                                                        |     77.96     |
+        | CoSENT | bert-base-uncased                               | CoSENT-base-first_last_avg                                                                                           |     69.93     |
+        | CoSENT | sentence-transformers/bert-base-nli-mean-tokens | CoSENT-base-nli-first_last_avg                                                                                       |     79.68     |
+        | CoSENT | sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 | [shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)                |     80.12     |
         
         #### ‰∏≠ÊñáÂåπÈÖçÊï∞ÊçÆÈõÜÁöÑËØÑÊµãÁªìÊûúÔºö
         
         
         | Arch   | BaseModel                    | Model           | ATEC  |  BQ   | LCQMC | PAWSX | STS-B |  Avg  | 
         |:-------|:----------------------------|:--------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|
         | SBERT  | bert-base-chinese           | SBERT-bert-base     | 46.36 | 70.36 | 78.72 | 46.86 | 66.41 | 61.74 |
@@ -83,59 +86,60 @@
         | CoSENT | bert-base-chinese           | CoSENT-bert-base    | 49.74 | 72.38 | 78.69 | 60.00 | 79.27 | 68.01 |
         | CoSENT | hfl/chinese-macbert-base    | CoSENT-macbert-base | 50.39 | 72.93 | 79.17 | 60.86 | 79.30 | 68.53 |
         | CoSENT | hfl/chinese-roberta-wwm-ext | CoSENT-roberta-ext  | 50.81 | 71.45 | 79.31 | 61.56 | 79.96 | 68.61 |
         
         ËØ¥ÊòéÔºö
         - ÁªìÊûúËØÑÊµãÊåáÊ†áÔºöspearmanÁ≥ªÊï∞
         - ‰∏∫ËØÑÊµãÊ®°ÂûãËÉΩÂäõÔºåÁªìÊûúÂùáÂè™Áî®ËØ•Êï∞ÊçÆÈõÜÁöÑtrainËÆ≠ÁªÉÔºåÂú®test‰∏äËØÑ‰º∞ÂæóÂà∞ÁöÑË°®Áé∞ÔºåÊ≤°Áî®Â§ñÈÉ®Êï∞ÊçÆ
+        - `SBERT-macbert-base`Ê®°ÂûãÔºåÊòØÁî®SBertÊñπÊ≥ïËÆ≠ÁªÉÔºåËøêË°å[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)‰ª£Á†ÅÂèØËÆ≠ÁªÉÊ®°Âûã
+        - `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`Ê®°ÂûãÊòØÁî®SBertËÆ≠ÁªÉÔºåÊòØ`paraphrase-MiniLM-L12-v2`Ê®°ÂûãÁöÑÂ§öËØ≠Ë®ÄÁâàÊú¨ÔºåÊîØÊåÅ‰∏≠Êñá„ÄÅËã±ÊñáÁ≠â
         
         
         ### Release Models
         - Êú¨È°πÁõÆreleaseÊ®°ÂûãÁöÑ‰∏≠ÊñáÂåπÈÖçËØÑÊµãÁªìÊûúÔºö
         
-        | Arch       | BaseModel                         | Model                                                                                                                                             | ATEC  |  BQ   | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-dc |    Avg    |  QPS  |
-        |:-----------|:----------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-------:|:-------:|:---------:|:-----:|
-        | Word2Vec   | word2vec                          | [w2v-light-tencent-chinese](https://ai.tencent.com/ailab/nlp/en/download.html)                                                                    | 20.00 | 31.49 | 59.46 | 2.57  | 55.78 |  55.04  |  20.70  |   35.03   | 23769 |
-        | SBERT      | xlm-roberta-base                  | [sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) | 18.42 | 38.52 | 63.96 | 10.14 | 78.90 |  63.01  |  52.28  |   46.46   | 3138  |
-        | Instructor | hfl/chinese-roberta-wwm-ext       | [moka-ai/m3e-base](https://huggingface.co/moka-ai/m3e-base)                                                                                       | 41.27 | 63.81 | 74.87 | 12.20 | 76.96 |  75.83  |  60.55  |   57.93   | 2980  |
-        | CoSENT     | hfl/chinese-macbert-base          | [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)                                                       | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 |  70.27  |  50.42  |   51.61   | 3008  |
-        | CoSENT     | hfl/chinese-lert-large            | [GanymedeNil/text2vec-large-chinese](https://huggingface.co/GanymedeNil/text2vec-large-chinese)                                                   | 32.61 | 44.59 | 69.30 | 14.51 | 79.44 |  73.01  |  59.04  |   53.12   | 2092  |
-        | CoSENT     | nghuyong/ernie-3.0-base-zh        | [shibing624/text2vec-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-sentence)                                     | 43.37 | 61.43 | 73.48 | 38.90 | 78.25 |  70.60  |  53.08  |   59.87   | 3089  |
-        | CoSENT     | nghuyong/ernie-3.0-base-zh        | [shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase)                                 | 44.89 | 63.58 | 74.24 | 40.90 | 78.93 |  76.70  |  63.30  | **63.08** | 3066  |
+        | Arch       | BaseModel                                                    | Model                                                                                                                                             | ATEC  |  BQ   | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-dc |    Avg    |  QPS  |
+        |:-----------|:-------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-------:|:-------:|:---------:|:-----:|
+        | Word2Vec   | word2vec                                                     | [w2v-light-tencent-chinese](https://ai.tencent.com/ailab/nlp/en/download.html)                                                                    | 20.00 | 31.49 | 59.46 | 2.57  | 55.78 |  55.04  |  20.70  |   35.03   | 23769 |
+        | SBERT      | xlm-roberta-base                                             | [sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) | 18.42 | 38.52 | 63.96 | 10.14 | 78.90 |  63.01  |  52.28  |   46.46   | 3138  |
+        | CoSENT     | hfl/chinese-macbert-base                                     | [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)                                                       | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 |  70.27  |  50.42  |   51.61   | 3008  |
+        | CoSENT     | hfl/chinese-lert-large                                       | [GanymedeNil/text2vec-large-chinese](https://huggingface.co/GanymedeNil/text2vec-large-chinese)                                                   | 32.61 | 44.59 | 69.30 | 14.51 | 79.44 |  73.01  |  59.04  |   53.12   | 2092  |
+        | CoSENT     | nghuyong/ernie-3.0-base-zh                                   | [shibing624/text2vec-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-sentence)                                     | 43.37 | 61.43 | 73.48 | 38.90 | 78.25 |  70.60  |  53.08  |   59.87   | 3089  |
+        | CoSENT     | nghuyong/ernie-3.0-base-zh                                   | [shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase)                                 | 44.89 | 63.58 | 74.24 | 40.90 | 78.93 |  76.70  |  63.30  | **63.08** | 3066  |
+        | CoSENT     | sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2  | [shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)                                             | 32.39 | 50.33 | 65.64 | 32.56 | 74.45 |  68.88  |  51.17  |   53.67   | 3138  |
         
         
         ËØ¥ÊòéÔºö
         - ÁªìÊûúËØÑÊµãÊåáÊ†áÔºöspearmanÁ≥ªÊï∞
-        - Ê®°ÂûãËÆ≠ÁªÉÂÆûÈ™åÊä•ÂëäÔºö[ÂÆûÈ™åÊä•Âëä](https://github.com/shibing624/text2vec/blob/master/docs/model_report.md)
         - `shibing624/text2vec-base-chinese`Ê®°ÂûãÔºåÊòØÁî®CoSENTÊñπÊ≥ïËÆ≠ÁªÉÔºåÂü∫‰∫é`hfl/chinese-macbert-base`Âú®‰∏≠ÊñáSTS-BÊï∞ÊçÆËÆ≠ÁªÉÂæóÂà∞ÔºåÂπ∂Âú®‰∏≠ÊñáSTS-BÊµãËØïÈõÜËØÑ‰º∞ËææÂà∞ËæÉÂ•ΩÊïàÊûúÔºåËøêË°å[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)‰ª£Á†ÅÂèØËÆ≠ÁªÉÊ®°ÂûãÔºåÊ®°ÂûãÊñá‰ª∂Â∑≤Áªè‰∏ä‰º†HF model hubÔºå‰∏≠ÊñáÈÄöÁî®ËØ≠‰πâÂåπÈÖç‰ªªÂä°Êé®Ëçê‰ΩøÁî®
         - `shibing624/text2vec-base-chinese-sentence`Ê®°ÂûãÔºåÊòØÁî®CoSENTÊñπÊ≥ïËÆ≠ÁªÉÔºåÂü∫‰∫é`nghuyong/ernie-3.0-base-zh`Áî®‰∫∫Â∑•ÊåëÈÄâÂêéÁöÑ‰∏≠ÊñáSTSÊï∞ÊçÆÈõÜ[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)ËÆ≠ÁªÉÂæóÂà∞ÔºåÂπ∂Âú®‰∏≠ÊñáÂêÑNLIÊµãËØïÈõÜËØÑ‰º∞ËææÂà∞ËæÉÂ•ΩÊïàÊûúÔºåËøêË°å[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)‰ª£Á†ÅÂèØËÆ≠ÁªÉÊ®°ÂûãÔºåÊ®°ÂûãÊñá‰ª∂Â∑≤Áªè‰∏ä‰º†HF model hubÔºå‰∏≠Êñás2s(Âè•Â≠êvsÂè•Â≠ê)ËØ≠‰πâÂåπÈÖç‰ªªÂä°Êé®Ëçê‰ΩøÁî®
         - `shibing624/text2vec-base-chinese-paraphrase`Ê®°ÂûãÔºåÊòØÁî®CoSENTÊñπÊ≥ïËÆ≠ÁªÉÔºåÂü∫‰∫é`nghuyong/ernie-3.0-base-zh`Áî®‰∫∫Â∑•ÊåëÈÄâÂêéÁöÑ‰∏≠ÊñáSTSÊï∞ÊçÆÈõÜ[shibing624/nli-zh-all/text2vec-base-chinese-paraphrase-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-paraphrase-dataset)ÔºåÊï∞ÊçÆÈõÜÁõ∏ÂØπ‰∫é[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)Âä†ÂÖ•‰∫Üs2p(sentence to paraphrase)Êï∞ÊçÆÔºåÂº∫Âåñ‰∫ÜÂÖ∂ÈïøÊñáÊú¨ÁöÑË°®ÂæÅËÉΩÂäõÔºåÂπ∂Âú®‰∏≠ÊñáÂêÑNLIÊµãËØïÈõÜËØÑ‰º∞ËææÂà∞SOTAÔºåËøêË°å[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)‰ª£Á†ÅÂèØËÆ≠ÁªÉÊ®°ÂûãÔºåÊ®°ÂûãÊñá‰ª∂Â∑≤Áªè‰∏ä‰º†HF model hubÔºå‰∏≠Êñás2p(Âè•Â≠êvsÊÆµËêΩ)ËØ≠‰πâÂåπÈÖç‰ªªÂä°Êé®Ëçê‰ΩøÁî®
-        - `SBERT-macbert-base`Ê®°ÂûãÔºåÊòØÁî®SBERTÊñπÊ≥ïËÆ≠ÁªÉÔºåËøêË°å[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)‰ª£Á†ÅÂèØËÆ≠ÁªÉÊ®°Âûã
-        - `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`Ê®°ÂûãÊòØÁî®SBERTËÆ≠ÁªÉÔºåÊòØ`paraphrase-MiniLM-L12-v2`Ê®°ÂûãÁöÑÂ§öËØ≠Ë®ÄÁâàÊú¨ÔºåÊîØÊåÅ‰∏≠Êñá„ÄÅËã±ÊñáÁ≠â
+        - `shibing624/text2vec-base-multilingual`Ê®°ÂûãÔºåÊòØÁî®CoSENTÊñπÊ≥ïËÆ≠ÁªÉÔºåÂü∫‰∫é`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`Áî®‰∫∫Â∑•ÊåëÈÄâÂêéÁöÑÂ§öËØ≠Ë®ÄSTSÊï∞ÊçÆÈõÜ[shibing624/nli-zh-all/text2vec-base-multilingual-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-multilingual-dataset)ËÆ≠ÁªÉÂæóÂà∞ÔºåÂπ∂Âú®‰∏≠Ëã±ÊñáÊµãËØïÈõÜËØÑ‰º∞Áõ∏ÂØπ‰∫éÂéüÊ®°ÂûãÊïàÊûúÊúâÊèêÂçáÔºåËøêË°å[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)‰ª£Á†ÅÂèØËÆ≠ÁªÉÊ®°ÂûãÔºåÊ®°ÂûãÊñá‰ª∂Â∑≤Áªè‰∏ä‰º†HF model hubÔºåÂ§öËØ≠Ë®ÄËØ≠‰πâÂåπÈÖç‰ªªÂä°Êé®Ëçê‰ΩøÁî®
         - `w2v-light-tencent-chinese`ÊòØËÖæËÆØËØçÂêëÈáèÁöÑWord2VecÊ®°ÂûãÔºåCPUÂä†ËΩΩ‰ΩøÁî®ÔºåÈÄÇÁî®‰∫é‰∏≠ÊñáÂ≠óÈù¢ÂåπÈÖç‰ªªÂä°ÂíåÁº∫Â∞ëÊï∞ÊçÆÁöÑÂÜ∑ÂêØÂä®ÊÉÖÂÜµ
         - ÂêÑÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÂùáÂèØ‰ª•ÈÄöËøátransformersË∞ÉÁî®ÔºåÂ¶ÇMacBERTÊ®°ÂûãÔºö`--model_name hfl/chinese-macbert-base` ÊàñËÄÖrobertaÊ®°ÂûãÔºö`--model_name uer/roberta-medium-wwm-chinese-cluecorpussmall`
         - ‰∏∫ÊµãËØÑÊ®°ÂûãÁöÑÈ≤ÅÊ£íÊÄßÔºåÂä†ÂÖ•‰∫ÜÊú™ËÆ≠ÁªÉËøáÁöÑSOHUÊµãËØïÈõÜÔºåÁî®‰∫éÊµãËØïÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÔºõ‰∏∫ËææÂà∞ÂºÄÁÆ±Âç≥Áî®ÁöÑÂÆûÁî®ÊïàÊûúÔºå‰ΩøÁî®‰∫ÜÊêúÈõÜÂà∞ÁöÑÂêÑ‰∏≠ÊñáÂåπÈÖçÊï∞ÊçÆÈõÜÔºåÊï∞ÊçÆÈõÜ‰πü‰∏ä‰º†Âà∞HF datasets[ÈìæÊé•ËßÅ‰∏ãÊñπ](#Êï∞ÊçÆÈõÜ)
         - ‰∏≠ÊñáÂåπÈÖç‰ªªÂä°ÂÆûÈ™åË°®ÊòéÔºåpoolingÊúÄ‰ºòÊòØ`EncoderType.FIRST_LAST_AVG`Âíå`EncoderType.MEAN`Ôºå‰∏§ËÄÖÈ¢ÑÊµãÊïàÊûúÂ∑ÆÂºÇÂæàÂ∞è
         - ‰∏≠ÊñáÂåπÈÖçËØÑÊµãÁªìÊûúÂ§çÁé∞ÔºåÂèØ‰ª•‰∏ãËΩΩ‰∏≠ÊñáÂåπÈÖçÊï∞ÊçÆÈõÜÂà∞`examples/data`ÔºåËøêË°å[tests/test_model_spearman.py](https://github.com/shibing624/text2vec/blob/master/tests/test_model_spearman.py)‰ª£Á†ÅÂ§çÁé∞ËØÑÊµãÁªìÊûú
         - QPSÁöÑGPUÊµãËØïÁéØÂ¢ÉÊòØTesla V100ÔºåÊòæÂ≠ò32GB
         
-        # Demo
+        Ê®°ÂûãËÆ≠ÁªÉÂÆûÈ™åÊä•ÂëäÔºö[ÂÆûÈ™åÊä•Âëä](https://github.com/shibing624/text2vec/blob/master/docs/model_report.md)
+        ## Demo
         
         Official Demo: https://www.mulanai.com/product/short_text_sim/
         
         HuggingFace Demo: https://huggingface.co/spaces/shibing624/text2vec
         
         ![](docs/hf.png)
         
         run example: [examples/gradio_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/gradio_demo.py) to see the demo:
         ```shell
         python examples/gradio_demo.py
         ```
         
-        # Install
+        ## Install
         ```shell
         pip install torch # conda install pytorch
         pip install -U text2vec
         ```
         
         or
         
@@ -144,17 +148,17 @@
         pip install -r requirements.txt
         
         git clone https://github.com/shibing624/text2vec.git
         cd text2vec
         pip install --no-deps .
         ```
         
-        # Usage
+        ## Usage
         
-        ## ÊñáÊú¨ÂêëÈáèË°®ÂæÅ
+        ### ÊñáÊú¨ÂêëÈáèË°®ÂæÅ
         
         Âü∫‰∫é`pretrained model`ËÆ°ÁÆóÊñáÊú¨ÂêëÈáèÔºö
         
         ```zsh
         >>> from text2vec import SentenceModel
         >>> m = SentenceModel()
         >>> m.encode("Â¶Ç‰ΩïÊõ¥Êç¢Ëä±ÂëóÁªëÂÆöÈì∂Ë°åÂç°")
@@ -194,16 +198,16 @@
         
         
         if __name__ == "__main__":
             # ‰∏≠ÊñáÂè•ÂêëÈáèÊ®°Âûã(CoSENT)Ôºå‰∏≠ÊñáËØ≠‰πâÂåπÈÖç‰ªªÂä°Êé®ËçêÔºåÊîØÊåÅfine-tuneÁªßÁª≠ËÆ≠ÁªÉ
             t2v_model = SentenceModel("shibing624/text2vec-base-chinese")
             compute_emb(t2v_model)
         
-            # ÊîØÊåÅÂ§öËØ≠Ë®ÄÁöÑÂè•ÂêëÈáèÊ®°ÂûãÔºàSentence-BERTÔºâÔºåËã±ÊñáËØ≠‰πâÂåπÈÖç‰ªªÂä°Êé®ËçêÔºåÊîØÊåÅfine-tuneÁªßÁª≠ËÆ≠ÁªÉ
-            sbert_model = SentenceModel("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
+            # ÊîØÊåÅÂ§öËØ≠Ë®ÄÁöÑÂè•ÂêëÈáèÊ®°ÂûãÔºàCoSENTÔºâÔºåÂ§öËØ≠Ë®ÄÔºàÂåÖÊã¨‰∏≠Ëã±ÊñáÔºâËØ≠‰πâÂåπÈÖç‰ªªÂä°Êé®ËçêÔºåÊîØÊåÅfine-tuneÁªßÁª≠ËÆ≠ÁªÉ
+            sbert_model = SentenceModel("shibing624/text2vec-base-multilingual")
             compute_emb(sbert_model)
         
             # ‰∏≠ÊñáËØçÂêëÈáèÊ®°Âûã(word2vec)Ôºå‰∏≠ÊñáÂ≠óÈù¢ÂåπÈÖç‰ªªÂä°ÂíåÂÜ∑ÂêØÂä®ÈÄÇÁî®
             w2v_model = Word2Vec("w2v-light-tencent-chinese")
             compute_emb(w2v_model)
         
         ```
@@ -220,16 +224,14 @@
         ```
         
         - ËøîÂõûÂÄº`embeddings`ÊòØ`numpy.ndarray`Á±ªÂûãÔºåshape‰∏∫`(sentences_size, model_embedding_size)`Ôºå‰∏â‰∏™Ê®°Âûã‰ªªÈÄâ‰∏ÄÁßçÂç≥ÂèØÔºåÊé®ËçêÁî®Á¨¨‰∏Ä‰∏™„ÄÇ
         - `shibing624/text2vec-base-chinese`Ê®°ÂûãÊòØCoSENTÊñπÊ≥ïÂú®‰∏≠ÊñáSTS-BÊï∞ÊçÆÈõÜËÆ≠ÁªÉÂæóÂà∞ÁöÑÔºåÊ®°ÂûãÂ∑≤Áªè‰∏ä‰º†Âà∞huggingfaceÁöÑ
         Ê®°ÂûãÂ∫ì[shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)Ôºå
         ÊòØ`text2vec.SentenceModel`ÊåáÂÆöÁöÑÈªòËÆ§Ê®°ÂûãÔºåÂèØ‰ª•ÈÄöËøá‰∏äÈù¢Á§∫‰æãË∞ÉÁî®ÔºåÊàñËÄÖÂ¶Ç‰∏ãÊâÄÁ§∫Áî®[transformersÂ∫ì](https://github.com/huggingface/transformers)Ë∞ÉÁî®Ôºå
         Ê®°ÂûãËá™Âä®‰∏ãËΩΩÂà∞Êú¨Êú∫Ë∑ØÂæÑÔºö`~/.cache/huggingface/transformers`
-        - `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`Ê®°ÂûãÊòØSentence-BERTÁöÑÂ§öËØ≠Ë®ÄÂè•ÂêëÈáèÊ®°ÂûãÔºå
-        ÈÄÇÁî®‰∫éÈáä‰πâÔºàparaphraseÔºâËØÜÂà´ÔºåÊñáÊú¨ÂåπÈÖçÔºåÈÄöËøá`text2vec.SentenceModel`Âíå[sentence-transformersÂ∫ì]((https://github.com/UKPLab/sentence-transformers))ÈÉΩÂèØ‰ª•Ë∞ÉÁî®ËØ•Ê®°Âûã
         - `w2v-light-tencent-chinese`ÊòØÈÄöËøágensimÂä†ËΩΩÁöÑWord2VecÊ®°ÂûãÔºå‰ΩøÁî®ËÖæËÆØËØçÂêëÈáè`Tencent_AILab_ChineseEmbedding.tar.gz`ËÆ°ÁÆóÂêÑÂ≠óËØçÁöÑËØçÂêëÈáèÔºåÂè•Â≠êÂêëÈáèÈÄöËøáÂçïËØçËØç
         ÂêëÈáèÂèñÂπ≥ÂùáÂÄºÂæóÂà∞ÔºåÊ®°ÂûãËá™Âä®‰∏ãËΩΩÂà∞Êú¨Êú∫Ë∑ØÂæÑÔºö`~/.text2vec/datasets/light_Tencent_AILab_ChineseEmbedding.bin`
         
         #### Usage (HuggingFace Transformers)
         Without [text2vec](https://github.com/shibing624/text2vec), you can use the model like this: 
         
         First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.
@@ -447,17 +449,17 @@
         from similarities import Similarity
         
         m = Similarity()
         r = m.similarity('Â¶Ç‰ΩïÊõ¥Êç¢Ëä±ÂëóÁªëÂÆöÈì∂Ë°åÂç°', 'Ëä±ÂëóÊõ¥ÊîπÁªëÂÆöÈì∂Ë°åÂç°')
         print(f"similarity score: {float(r)}")  # similarity score: 0.855146050453186
         ```
         
-        # Models
+        ## Models
         
-        ## CoSENT model
+        ### CoSENT model
         
         CoSENTÔºàCosine SentenceÔºâÊñáÊú¨ÂåπÈÖçÊ®°ÂûãÔºåÂú®Sentence-BERT‰∏äÊîπËøõ‰∫ÜCosineRankLossÁöÑÂè•ÂêëÈáèÊñπÊ°à
         
         
         Network structure:
         
         Training:
@@ -488,16 +490,22 @@
         python training_sup_text_matching_model.py --task_name ATEC --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base --output_dir ./outputs/ATEC-cosent
         ```
         
         - Âú®Ëá™Êúâ‰∏≠ÊñáÊï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÊ®°Âûã
         
         example: [examples/training_sup_text_matching_model_mydata.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_mydata.py)
         
+        ÂçïÂç°ËÆ≠ÁªÉÔºö
+        ```shell
+        CUDA_VISIBLE_DEVICES=0 python training_sup_text_matching_model_mydata.py --do_train --do_predict
+        ```
+        
+        Â§öÂç°ËÆ≠ÁªÉÔºö
         ```shell
-        python training_sup_text_matching_model_mydata.py --do_train --do_predict
+        CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node 2  training_sup_text_matching_model_mydata.py --do_train --do_predict --output_dir outputs/STS-B-text2vec-macbert-v1 --batch_size 64 --fp16 --data_parallel 
         ```
         
         ËÆ≠ÁªÉÈõÜÊ†ºÂºèÂèÇËÄÉ[examples/data/STS-B/STS-B.valid.data](https://github.com/shibing624/text2vec/blob/master/examples/data/STS-B/STS-B.valid.data)
         
         ```shell
         sentence1   sentence2   label
         ‰∏Ä‰∏™Â•≥Â≠©Âú®ÁªôÂ•πÁöÑÂ§¥ÂèëÂÅöÂèëÂûã„ÄÇ	‰∏Ä‰∏™Â•≥Â≠©Âú®Ê¢≥Â§¥„ÄÇ	2
@@ -524,15 +532,15 @@
         
         ```shell
         cd examples
         python training_unsup_text_matching_model_en.py --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-cosent
         ```
         
         
-        ## Sentence-BERT model
+        ### Sentence-BERT model
         
         Sentence-BERTÊñáÊú¨ÂåπÈÖçÊ®°ÂûãÔºåË°®ÂæÅÂºèÂè•ÂêëÈáèË°®Á§∫ÊñπÊ°à
         
         Network structure:
         
         Training:
         
@@ -567,38 +575,38 @@
         example: [examples/training_unsup_text_matching_model_en.py](https://github.com/shibing624/text2vec/blob/master/examples/training_unsup_text_matching_model_en.py)
         
         ```shell
         cd examples
         python training_unsup_text_matching_model_en.py --model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-sbert
         ```
         
-        ## BERT-Match model
+        ### BERT-Match model
         BERTÊñáÊú¨ÂåπÈÖçÊ®°ÂûãÔºåÂéüÁîüBERTÂåπÈÖçÁΩëÁªúÁªìÊûÑÔºå‰∫§‰∫íÂºèÂè•ÂêëÈáèÂåπÈÖçÊ®°Âûã
         
         Network structure:
         
         Training and inference:
         
         <img src="docs/bert-fc-train.png" width="300" />
         
         ËÆ≠ÁªÉËÑöÊú¨Âêå‰∏ä[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)„ÄÇ
         
         
-        ## Ê®°ÂûãËí∏È¶èÔºàModel DistillationÔºâ
+        ### Ê®°ÂûãËí∏È¶èÔºàModel DistillationÔºâ
         
         Áî±‰∫étext2vecËÆ≠ÁªÉÁöÑÊ®°ÂûãÂèØ‰ª•‰ΩøÁî®[sentence-transformers](https://github.com/UKPLab/sentence-transformers)Â∫ìÂä†ËΩΩÔºåÊ≠§Â§ÑÂ§çÁî®ÂÖ∂Ê®°ÂûãËí∏È¶èÊñπÊ≥ï[distillation](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/distillation)„ÄÇ
         
         1. Ê®°ÂûãÈôçÁª¥ÔºåÂèÇËÄÉ[dimensionality_reduction.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/distillation/dimensionality_reduction.py)‰ΩøÁî®PCAÂØπÊ®°ÂûãËæìÂá∫embeddingÈôçÁª¥ÔºåÂèØÂáèÂ∞ëmilvusÁ≠âÂêëÈáèÊ£ÄÁ¥¢Êï∞ÊçÆÂ∫ìÁöÑÂ≠òÂÇ®ÂéãÂäõÔºåËøòËÉΩËΩªÂæÆÊèêÂçáÊ®°ÂûãÊïàÊûú„ÄÇ
         2. Ê®°ÂûãËí∏È¶èÔºåÂèÇËÄÉ[model_distillation.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/distillation/model_distillation.py)‰ΩøÁî®Ëí∏È¶èÊñπÊ≥ïÔºåÂ∞ÜTeacherÂ§ßÊ®°ÂûãËí∏È¶èÂà∞Êõ¥Â∞ëlayersÂ±ÇÊï∞ÁöÑstudentÊ®°Âûã‰∏≠ÔºåÂú®ÊùÉË°°ÊïàÊûúÁöÑÊÉÖÂÜµ‰∏ãÔºåÂèØÂ§ßÂπÖÊèêÂçáÊ®°ÂûãÈ¢ÑÊµãÈÄüÂ∫¶„ÄÇ
         
-        ## Ê®°ÂûãÈÉ®ÁΩ≤
+        ### Ê®°ÂûãÈÉ®ÁΩ≤
         
         Êèê‰æõ‰∏§ÁßçÈÉ®ÁΩ≤Ê®°ÂûãÔºåÊê≠Âª∫ÊúçÂä°ÁöÑÊñπÊ≥ïÔºö 1ÔºâÂü∫‰∫éJinaÊê≠Âª∫gRPCÊúçÂä°„ÄêÊé®Ëçê„ÄëÔºõ2ÔºâÂü∫‰∫éFastAPIÊê≠Âª∫ÂéüÁîüHttpÊúçÂä°„ÄÇ
         
-        ### JinaÊúçÂä°
+        #### JinaÊúçÂä°
         ÈááÁî®C/SÊ®°ÂºèÊê≠Âª∫È´òÊÄßËÉΩÊúçÂä°ÔºåÊîØÊåÅdocker‰∫ëÂéüÁîüÔºågRPC/HTTP/WebSocketÔºåÊîØÊåÅÂ§ö‰∏™Ê®°ÂûãÂêåÊó∂È¢ÑÊµãÔºåGPUÂ§öÂç°Â§ÑÁêÜ„ÄÇ
         
         - ÂÆâË£ÖÔºö
         ```pip install jina```
         
         - ÂêØÂä®ÊúçÂä°Ôºö
         
@@ -637,15 +645,15 @@
         r = c.post('/', inputs=DocumentArray([Document(text='Â¶Ç‰ΩïÊõ¥Êç¢Ëä±ÂëóÁªëÂÆöÈì∂Ë°åÂç°'), Document(text='Ëä±ÂëóÊõ¥ÊîπÁªëÂÆöÈì∂Ë°åÂç°')]))
         print(r.embeddings)
         ```
         
         ÊâπÈáèË∞ÉÁî®ÊñπÊ≥ïËßÅexample: [examples/jina_client_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/jina_client_demo.py)
         
         
-        ### FastAPIÊúçÂä°
+        #### FastAPIÊúçÂä°
         
         - ÂÆâË£ÖÔºö
         ```pip install fastapi uvicorn```
         
         - ÂêØÂä®ÊúçÂä°Ôºö
         
         example: [examples/fastapi_server_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/fastapi_server_demo.py)
@@ -658,15 +666,15 @@
         ```shell
         curl -X 'GET' \
           'http://0.0.0.0:8001/emb?q=hello' \
           -H 'accept: application/json'
         ```
         
         
-        ## Êï∞ÊçÆÈõÜ
+        ## Dataset
         
         - Êú¨È°πÁõÆreleaseÁöÑÊï∞ÊçÆÈõÜÔºö
         
         | Dataset                    | Introduce                                                                | Download Link                                                                                                                                                                                                                                                                                         |
         |:---------------------------|:-------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
         | shibing624/nli-zh-all      | ‰∏≠ÊñáËØ≠‰πâÂåπÈÖçÊï∞ÊçÆÂêàÈõÜÔºåÊï¥Âêà‰∫ÜÊñáÊú¨Êé®ÁêÜÔºåÁõ∏‰ººÔºåÊëòË¶ÅÔºåÈóÆÁ≠îÔºåÊåá‰ª§ÂæÆË∞ÉÁ≠â‰ªªÂä°ÁöÑ820‰∏áÈ´òË¥®ÈáèÊï∞ÊçÆÔºåÂπ∂ËΩ¨Âåñ‰∏∫ÂåπÈÖçÊ†ºÂºèÊï∞ÊçÆÈõÜ                | [https://huggingface.co/datasets/shibing624/nli-zh-all](https://huggingface.co/datasets/shibing624/nli-zh-all)                                                                                                                                                                                        |
         | shibing624/snli-zh         | ‰∏≠ÊñáSNLIÂíåMultiNLIÊï∞ÊçÆÈõÜÔºåÁøªËØëËá™Ëã±ÊñáSNLIÂíåMultiNLI                                    | [https://huggingface.co/datasets/shibing624/snli-zh](https://huggingface.co/datasets/shibing624/snli-zh)                                                                                                                                                                                              |
@@ -677,16 +685,16 @@
         | LCQMC                      | ‰∏≠ÊñáLCQMC(large-scale Chinese question matching corpus)Êï∞ÊçÆÈõÜÔºåQ-QpairÊï∞ÊçÆÈõÜ      | [LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html)                                                                                                                                                                                                                                               |
         | PAWSX                      | ‰∏≠ÊñáPAWS(Paraphrase Adversaries from Word Scrambling)Êï∞ÊçÆÈõÜÔºåQ-QpairÊï∞ÊçÆÈõÜ        | [PAWSX](https://arxiv.org/abs/1908.11828)                                                                                                                                                                                                                                                             |
         | STS-B                      | ‰∏≠ÊñáSTS-BÊï∞ÊçÆÈõÜÔºå‰∏≠ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÊé®ÁêÜÊï∞ÊçÆÈõÜÔºå‰ªéËã±ÊñáSTS-BÁøªËØë‰∏∫‰∏≠ÊñáÁöÑÊï∞ÊçÆÈõÜ                                 | [STS-B](https://github.com/pluto-junzeng/CNSD)                                                                                                                                                                                                                                                        |
         
         
         Â∏∏Áî®Ëã±ÊñáÂåπÈÖçÊï∞ÊçÆÈõÜÔºö
         
-        - Â§ßÂêçÈºéÈºéÁöÑmulti_nliÂíåsnli: https://huggingface.co/datasets/multi_nli
-        - Â§ßÂêçÈºéÈºéÁöÑmulti_nliÂíåsnli: https://huggingface.co/datasets/snli
+        - Ëã±ÊñáÂåπÈÖçÊï∞ÊçÆÈõÜÔºömulti_nli: https://huggingface.co/datasets/multi_nli
+        - Ëã±ÊñáÂåπÈÖçÊï∞ÊçÆÈõÜÔºösnli: https://huggingface.co/datasets/snli
         - https://huggingface.co/datasets/metaeval/cnli
         - https://huggingface.co/datasets/mteb/stsbenchmark-sts
         - https://huggingface.co/datasets/JeremiahZ/simcse_sup_nli
         - https://huggingface.co/datasets/MoritzLaurer/multilingual-NLI-26lang-2mil7
         
         
         Êï∞ÊçÆÈõÜ‰ΩøÁî®Á§∫‰æãÔºö
@@ -721,59 +729,59 @@
         {'sentence1': '‰∏Ä‰∏™Â•≥Â≠©Âú®ÁªôÂ•πÁöÑÂ§¥ÂèëÂÅöÂèëÂûã„ÄÇ', 'sentence2': '‰∏Ä‰∏™Â•≥Â≠©Âú®Ê¢≥Â§¥„ÄÇ', 'label': 2}
         ```
         
         
         
         
         
-        # Contact
+        ## Contact
         
         - Issue(Âª∫ËÆÆ)Ôºö[![GitHub issues](https://img.shields.io/github/issues/shibing624/text2vec.svg)](https://github.com/shibing624/text2vec/issues)
         - ÈÇÆ‰ª∂ÊàëÔºöxuming: xuming624@qq.com
         - ÂæÆ‰ø°ÊàëÔºöÂä†Êàë*ÂæÆ‰ø°Âè∑Ôºöxuming624, Â§áÊ≥®ÔºöÂßìÂêç-ÂÖ¨Âè∏-NLP* ËøõNLP‰∫§ÊµÅÁæ§„ÄÇ
         
         <img src="docs/wechat.jpeg" width="200" />
         
         
-        # Citation
+        ## Citation
         
         Â¶ÇÊûú‰Ω†Âú®Á†îÁ©∂‰∏≠‰ΩøÁî®‰∫Ütext2vecÔºåËØ∑ÊåâÂ¶Ç‰∏ãÊ†ºÂºèÂºïÁî®Ôºö
         
         APA:
         ```latex
         Xu, M. Text2vec: Text to vector toolkit (Version 1.1.2) [Computer software]. https://github.com/shibing624/text2vec
         ```
         
         BibTeX:
         ```latex
         @misc{Text2vec,
-          author = {Xu, Ming},
+          author = {Ming Xu},
           title = {Text2vec: Text to vector toolkit},
-          year = {2022},
+          year = {2023},
           publisher = {GitHub},
           journal = {GitHub repository},
           howpublished = {\url{https://github.com/shibing624/text2vec}},
         }
         ```
         
-        # License
+        ## License
         
         
         ÊéàÊùÉÂçèËÆÆ‰∏∫ [The Apache License 2.0](LICENSE)ÔºåÂèØÂÖçË¥πÁî®ÂÅöÂïÜ‰∏öÁî®ÈÄî„ÄÇËØ∑Âú®‰∫ßÂìÅËØ¥Êòé‰∏≠ÈôÑÂä†text2vecÁöÑÈìæÊé•ÂíåÊéàÊùÉÂçèËÆÆ„ÄÇ
         
         
-        # Contribute
+        ## Contribute
         È°πÁõÆ‰ª£Á†ÅËøòÂæàÁ≤óÁ≥ôÔºåÂ¶ÇÊûúÂ§ßÂÆ∂ÂØπ‰ª£Á†ÅÊúâÊâÄÊîπËøõÔºåÊ¨¢ËøéÊèê‰∫§ÂõûÊú¨È°πÁõÆÔºåÂú®Êèê‰∫§‰πãÂâçÔºåÊ≥®ÊÑè‰ª•‰∏ã‰∏§ÁÇπÔºö
         
          - Âú®`tests`Ê∑ªÂä†Áõ∏Â∫îÁöÑÂçïÂÖÉÊµãËØï
          - ‰ΩøÁî®`python -m pytest -v`Êù•ËøêË°åÊâÄÊúâÂçïÂÖÉÊµãËØïÔºåÁ°Æ‰øùÊâÄÊúâÂçïÊµãÈÉΩÊòØÈÄöËøáÁöÑ
         
         ‰πãÂêéÂç≥ÂèØÊèê‰∫§PR„ÄÇ
         
-        # Reference
+        ## References
         - [Â∞ÜÂè•Â≠êË°®Á§∫‰∏∫ÂêëÈáèÔºà‰∏äÔºâÔºöÊó†ÁõëÁù£Âè•Â≠êË°®Á§∫Â≠¶‰π†Ôºàsentence embeddingÔºâ](https://www.cnblogs.com/llhthinker/p/10335164.html)
         - [Â∞ÜÂè•Â≠êË°®Á§∫‰∏∫ÂêëÈáèÔºà‰∏ãÔºâÔºöÊó†ÁõëÁù£Âè•Â≠êË°®Á§∫Â≠¶‰π†Ôºàsentence embeddingÔºâ](https://www.cnblogs.com/llhthinker/p/10341841.html)
         - [A Simple but Tough-to-Beat Baseline for Sentence Embeddings[Sanjeev Arora and Yingyu Liang and Tengyu Ma, 2017]](https://openreview.net/forum?id=SyK00v5xx)
         - [ÂõõÁßçËÆ°ÁÆóÊñáÊú¨Áõ∏‰ººÂ∫¶ÁöÑÊñπÊ≥ïÂØπÊØî[Yves Peirsman]](https://zhuanlan.zhihu.com/p/37104535)
         - [Improvements to BM25 and Language Models Examined](http://www.cs.otago.ac.nz/homepages/andrew/papers/2014-2.pdf)
         - [CoSENTÔºöÊØîSentence-BERTÊõ¥ÊúâÊïàÁöÑÂè•ÂêëÈáèÊñπÊ°à](https://kexue.fm/archives/8847)
         - [Ë∞àË∞àÊñáÊú¨ÂåπÈÖçÂíåÂ§öËΩÆÊ£ÄÁ¥¢](https://zhuanlan.zhihu.com/p/111769969)
```

#### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: text2vec Version: 1.2.1 Summary: Text to vector
+Metadata-Version: 2.1 Name: text2vec Version: 1.2.2 Summary: Text to vector
 Tool, encode text Home-page: https://github.com/shibing624/text2vec Author:
 XuMing Author-email: xuming624@qq.com License: Apache License 2.0 Description:
 [**√∞¬ü¬á¬®√∞¬ü¬á¬≥√§¬∏¬≠√¶¬ñ¬á**](https://github.com/shibing624/text2vec/blob/master/
 README.md) | [**√∞¬ü¬å¬êEnglish**](https://github.com/shibing624/text2vec/blob/
 master/README_EN.md) | [**√∞¬ü¬ì¬ñ√¶¬ñ¬á√¶¬°¬£/Docs**](https://github.com/shibing624/
 text2vec/wiki) | [**√∞¬ü¬§¬ñ√¶¬®¬°√•¬û¬ã/Models**](https://huggingface.co/shibing624)
                                     [Logo]
@@ -17,17 +17,26 @@
 shibing624/text2vec.svg)](https://github.com/shibing624/text2vec/issues) [!
 [Wechat Group](http://vlog.sfyc.ltd/wechat_everyday/
 wxgroup_logo.png?imageView2/0/w/60/h/20)](#Contact) **Text2vec**: Text to
 Vector, Get Sentence Embeddings. √¶¬ñ¬á√¶¬ú¬¨√•¬ê¬ë√©¬á¬è√•¬å¬ñ√Ø¬º¬å√¶¬ä¬ä√¶¬ñ¬á√¶¬ú¬¨
 (√•¬å¬Ö√¶¬ã¬¨√®¬Ø¬ç√£¬Ä¬Å√•¬è¬•√•¬≠¬ê√£¬Ä¬Å√¶¬Æ¬µ√®¬ê¬Ω)√®¬°¬®√•¬æ¬Å√§¬∏¬∫√•¬ê¬ë√©¬á¬è√ß¬ü¬©√©¬ò¬µ√£¬Ä¬Ç
 **text2vec**√•¬Æ¬û√ß¬é¬∞√§¬∫¬ÜWord2Vec√£¬Ä¬ÅRankBM25√£¬Ä¬ÅBERT√£¬Ä¬ÅSentence-
 BERT√£¬Ä¬ÅCoSENT√ß¬≠¬â√•¬§¬ö√ß¬ß¬ç√¶¬ñ¬á√¶¬ú¬¨√®¬°¬®√•¬æ¬Å√£¬Ä¬Å√¶¬ñ¬á√¶¬ú¬¨√ß¬õ¬∏√§¬º¬º√•¬∫¬¶√®¬Æ¬°√ß¬Æ¬ó√¶¬®¬°√•¬û¬ã√Ø¬º¬å√•¬π¬∂√•¬ú¬®√¶¬ñ¬á√¶¬ú¬¨√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√Ø¬º¬à√ß¬õ¬∏√§¬º¬º√•¬∫¬¶√®¬Æ¬°√ß¬Æ¬ó√Ø¬º¬â√§¬ª¬ª√•¬ä¬°√§¬∏¬ä√¶¬Ø¬î√®¬æ¬É√§¬∫¬Ü√•¬ê¬Ñ√¶¬®¬°√•¬û¬ã√ß¬ö¬Ñ√¶¬ï¬à√¶¬û¬ú√£¬Ä¬Ç
-### News [2023/06/19] v1.2.1√ß¬â¬à√¶¬ú¬¨: √¶¬õ¬¥√¶¬ñ¬∞√§¬∫¬Ü√§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã`shibing624/
-text2vec-base-chinese-nli`√§¬∏¬∫√¶¬ñ¬∞√ß¬â¬à[shibing624/text2vec-base-chinese-sentence]
-(https://huggingface.co/shibing624/text2vec-base-chinese-
+### News [2023/06/22] v1.2.2√ß¬â¬à√¶¬ú¬¨: √•¬è¬ë√•¬∏¬É√§¬∫¬Ü√•¬§¬ö√®¬Ø¬≠√®¬®¬Ä√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã[shibing624/
+text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-
+multilingual)√Ø¬º¬å√ß¬î¬®CoSENT√¶¬ñ¬π√¶¬≥¬ï√®¬Æ¬≠√ß¬ª¬É√Ø¬º¬å√•¬ü¬∫√§¬∫¬é`sentence-transformers/
+paraphrase-multilingual-MiniLM-L12-
+v2`√ß¬î¬®√§¬∫¬∫√•¬∑¬•√¶¬å¬ë√©¬Ä¬â√•¬ê¬é√ß¬ö¬Ñ√•¬§¬ö√®¬Ø¬≠√®¬®¬ÄSTS√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü[shibing624/nli-zh-all/text2vec-
+base-multilingual-dataset](https://huggingface.co/datasets/shibing624/nli-zh-
+all/tree/main/text2vec-base-multilingual-
+dataset)√®¬Æ¬≠√ß¬ª¬É√•¬æ¬ó√•¬à¬∞√Ø¬º¬å√•¬π¬∂√•¬ú¬®√§¬∏¬≠√®¬ã¬±√¶¬ñ¬á√¶¬µ¬ã√®¬Ø¬ï√©¬õ¬Ü√®¬Ø¬Ñ√§¬º¬∞√ß¬õ¬∏√•¬Ø¬π√§¬∫¬é√•¬é¬ü√¶¬®¬°√•¬û¬ã√¶¬ï¬à√¶¬û¬ú√¶¬ú¬â√¶¬è¬ê√•¬ç¬á√Ø¬º¬å√®¬Ø¬¶√®¬ß¬Å
+[Release-v1.2.2](https://github.com/shibing624/text2vec/releases/tag/1.2.2)
+[2023/06/19] v1.2.1√ß¬â¬à√¶¬ú¬¨: √¶¬õ¬¥√¶¬ñ¬∞√§¬∫¬Ü√§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã`shibing624/text2vec-
+base-chinese-nli`√§¬∏¬∫√¶¬ñ¬∞√ß¬â¬à[shibing624/text2vec-base-chinese-sentence](https://
+huggingface.co/shibing624/text2vec-base-chinese-
 sentence)√Ø¬º¬å√©¬í¬à√•¬Ø¬πCoSENT√ß¬ö¬Ñloss√®¬Æ¬°√ß¬Æ¬ó√•¬Ø¬π√¶¬é¬í√•¬∫¬è√¶¬ï¬è√¶¬Ñ¬ü√ß¬â¬π√ß¬Ç¬π√Ø¬º¬å√§¬∫¬∫√•¬∑¬•√¶¬å¬ë√©¬Ä¬â√•¬π¬∂√¶¬ï¬¥√ß¬ê¬Ü√•¬á¬∫√©¬´¬ò√®¬¥¬®√©¬á¬è√ß¬ö¬Ñ√¶¬ú¬â√ß¬õ¬∏√•¬Ö¬≥√¶¬Ä¬ß√¶¬é¬í√•¬∫¬è√ß¬ö¬ÑSTS√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü
 [shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://
 huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-
 sentence-
 dataset)√Ø¬º¬å√•¬ú¬®√•¬ê¬Ñ√®¬Ø¬Ñ√§¬º¬∞√©¬õ¬Ü√®¬°¬®√ß¬é¬∞√ß¬õ¬∏√•¬Ø¬π√§¬π¬ã√•¬â¬ç√¶¬ú¬â√¶¬è¬ê√•¬ç¬á√Ø¬º¬õ√•¬è¬ë√•¬∏¬É√§¬∫¬Ü√©¬Ä¬Ç√ß¬î¬®√§¬∫¬és2p√ß¬ö¬Ñ√§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã
 [shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/
 shibing624/text2vec-base-chinese-paraphrase)√Ø¬º¬å√®¬Ø¬¶√®¬ß¬Å[Release-v1.2.1](https://
@@ -38,17 +47,17 @@
 huggingface.co/datasets/shibing624/
 nli_zh)√•¬Ö¬®√©¬É¬®√®¬Ø¬≠√¶¬ñ¬ô√®¬Æ¬≠√ß¬ª¬É√ß¬ö¬ÑCoSENT√¶¬ñ¬á√¶¬ú¬¨√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã√Ø¬º¬å√•¬ú¬®√•¬ê¬Ñ√®¬Ø¬Ñ√§¬º¬∞√©¬õ¬Ü√®¬°¬®√ß¬é¬∞√¶¬è¬ê√•¬ç¬á√¶¬ò¬é√¶¬ò¬æ√Ø¬º¬å√®¬Ø¬¶√®¬ß¬Å
 [Release-v1.2.0](https://github.com/shibing624/text2vec/releases/tag/1.2.0)
 [2022/03/12] v1.1.4√ß¬â¬à√¶¬ú¬¨: √•¬è¬ë√•¬∏¬É√§¬∫¬Ü√§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã[shibing624/text2vec-
 base-chinese](https://huggingface.co/shibing624/text2vec-base-
 chinese)√Ø¬º¬å√•¬ü¬∫√§¬∫¬é√§¬∏¬≠√¶¬ñ¬áSTS√®¬Æ¬≠√ß¬ª¬É√©¬õ¬Ü√®¬Æ¬≠√ß¬ª¬É√ß¬ö¬ÑCoSENT√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã√£¬Ä¬Ç√®¬Ø¬¶√®¬ß¬Å
 [Release-v1.1.4](https://github.com/shibing624/text2vec/releases/tag/1.1.4)
-**Guide** - [Feature](#Feature) - [Evaluation](#Evaluation) - [Install]
-(#install) - [Usage](#usage) - [Contact](#Contact) - [Reference](#reference) #
-Feature ### √¶¬ñ¬á√¶¬ú¬¨√•¬ê¬ë√©¬á¬è√®¬°¬®√ß¬§¬∫√¶¬®¬°√•¬û¬ã - [Word2Vec](https://github.com/
+**Guide** - [Features](#Features) - [Evaluation](#Evaluation) - [Install]
+(#install) - [Usage](#usage) - [Contact](#Contact) - [References](#references)
+## Features ### √¶¬ñ¬á√¶¬ú¬¨√•¬ê¬ë√©¬á¬è√®¬°¬®√ß¬§¬∫√¶¬®¬°√•¬û¬ã - [Word2Vec](https://github.com/
 shibing624/text2vec/blob/master/text2vec/word2vec.py)√Ø¬º¬ö√©¬Ä¬ö√®¬ø¬á√®¬Ö¬æ√®¬Æ¬ØAI
 Lab√•¬º¬Ä√¶¬∫¬ê√ß¬ö¬Ñ√•¬§¬ß√®¬ß¬Ñ√¶¬®¬°√©¬´¬ò√®¬¥¬®√©¬á¬è√§¬∏¬≠√¶¬ñ¬á
 [√®¬Ø¬ç√•¬ê¬ë√©¬á¬è√¶¬ï¬∞√¶¬ç¬Æ√Ø¬º¬à800√§¬∏¬á√§¬∏¬≠√¶¬ñ¬á√®¬Ø¬ç√®¬Ω¬ª√©¬á¬è√ß¬â¬à√Ø¬º¬â](https://pan.baidu.com/s/
 1La4U4XNFe8s5BJqxPQpeiQ) (√¶¬ñ¬á√§¬ª¬∂√•¬ê¬ç√Ø¬º¬ölight_Tencent_AILab_ChineseEmbedding.bin
 √•¬Ø¬Ü√ß¬†¬Å:
 tawe√Ø¬º¬â√•¬Æ¬û√ß¬é¬∞√®¬Ø¬ç√•¬ê¬ë√©¬á¬è√¶¬£¬Ä√ß¬¥¬¢√Ø¬º¬å√¶¬ú¬¨√©¬°¬π√ß¬õ¬Æ√•¬Æ¬û√ß¬é¬∞√§¬∫¬Ü√•¬è¬•√•¬≠¬ê√Ø¬º¬à√®¬Ø¬ç√•¬ê¬ë√©¬á¬è√¶¬±¬Ç√•¬π¬≥√•¬ù¬á√Ø¬º¬â√ß¬ö¬Ñword2vec√•¬ê¬ë√©¬á¬è√®¬°¬®√ß¬§¬∫
 - [SBERT(Sentence-BERT)](https://github.com/shibing624/text2vec/blob/master/
@@ -56,67 +65,76 @@
 sentencebert_model.py)√Ø¬º¬ö√¶¬ù¬É√®¬°¬°√¶¬Ä¬ß√®¬É¬Ω√•¬í¬å√¶¬ï¬à√ß¬é¬á√ß¬ö¬Ñ√•¬è¬•√•¬ê¬ë√©¬á¬è√®¬°¬®√ß¬§¬∫√¶¬®¬°√•¬û¬ã√Ø¬º¬å√®¬Æ¬≠√ß¬ª¬É√¶¬ó¬∂√©¬Ä¬ö√®¬ø¬á√¶¬ú¬â√ß¬õ¬ë√ß¬ù¬£√®¬Æ¬≠√ß¬ª¬É√§¬∏¬ä√•¬±¬Ç√•¬à¬Ü√ß¬±¬ª√•¬á¬Ω√¶¬ï¬∞√Ø¬º¬å√¶¬ñ¬á√¶¬ú¬¨√•¬å¬π√©¬Ö¬ç√©¬¢¬Ñ√¶¬µ¬ã√¶¬ó¬∂√ß¬õ¬¥√¶¬é¬•√•¬è¬•√•¬≠¬ê√•¬ê¬ë√©¬á¬è√•¬Å¬ö√§¬Ω¬ô√•¬º¬¶√Ø¬º¬å√¶¬ú¬¨√©¬°¬π√ß¬õ¬Æ√•¬ü¬∫√§¬∫¬éPyTorch√•¬§¬ç√ß¬é¬∞√§¬∫¬ÜSentence-
 BERT√¶¬®¬°√•¬û¬ã√ß¬ö¬Ñ√®¬Æ¬≠√ß¬ª¬É√•¬í¬å√©¬¢¬Ñ√¶¬µ¬ã - [CoSENT(Cosine Sentence)](https://github.com/
 shibing624/text2vec/blob/master/text2vec/
 cosent_model.py)√Ø¬º¬öCoSENT√¶¬®¬°√•¬û¬ã√¶¬è¬ê√•¬á¬∫√§¬∫¬Ü√§¬∏¬Ä√ß¬ß¬ç√¶¬é¬í√•¬∫¬è√ß¬ö¬Ñ√¶¬ç¬ü√•¬§¬±√•¬á¬Ω√¶¬ï¬∞√Ø¬º¬å√§¬Ω¬ø√®¬Æ¬≠√ß¬ª¬É√®¬ø¬á√ß¬®¬ã√¶¬õ¬¥√®¬¥¬¥√®¬ø¬ë√©¬¢¬Ñ√¶¬µ¬ã√Ø¬º¬å√¶¬®¬°√•¬û¬ã√¶¬î¬∂√¶¬ï¬õ√©¬Ä¬ü√•¬∫¬¶√•¬í¬å√¶¬ï¬à√¶¬û¬ú√¶¬Ø¬îSentence-
 BERT√¶¬õ¬¥√•¬•¬Ω√Ø¬º¬å√¶¬ú¬¨√©¬°¬π√ß¬õ¬Æ√•¬ü¬∫√§¬∫¬éPyTorch√•¬Æ¬û√ß¬é¬∞√§¬∫¬ÜCoSENT√¶¬®¬°√•¬û¬ã√ß¬ö¬Ñ√®¬Æ¬≠√ß¬ª¬É√•¬í¬å√©¬¢¬Ñ√¶¬µ¬ã
 √®¬Ø¬¶√ß¬ª¬Ü√¶¬ñ¬á√¶¬ú¬¨√•¬ê¬ë√©¬á¬è√®¬°¬®√ß¬§¬∫√¶¬ñ¬π√¶¬≥¬ï√®¬ß¬Åwiki: [√¶¬ñ¬á√¶¬ú¬¨√•¬ê¬ë√©¬á¬è√®¬°¬®√ß¬§¬∫√¶¬ñ¬π√¶¬≥¬ï](https://
 github.com/shibing624/text2vec/wiki/
-%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95) #
+%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95) ##
 Evaluation √¶¬ñ¬á√¶¬ú¬¨√•¬å¬π√©¬Ö¬ç #### √®¬ã¬±√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√ß¬ö¬Ñ√®¬Ø¬Ñ√¶¬µ¬ã√ß¬ª¬ì√¶¬û¬ú√Ø¬º¬ö | Arch |
 BaseModel | Model | English-STS-B | |:-------|:--------------------------------
-----------------|:-------------------------------------|:-------------:| |
+----------------|:-------------------------------------------------------------
+--------------------------------------------------------|:-------------:| |
 GloVe | glove | Avg_word_embeddings_glove_6B_300d | 61.77 | | BERT | bert-base-
 uncased | BERT-base-cls | 20.29 | | BERT | bert-base-uncased | BERT-base-
 first_last_avg | 59.04 | | BERT | bert-base-uncased | BERT-base-first_last_avg-
 whiten(NLI) | 63.65 | | SBERT | sentence-transformers/bert-base-nli-mean-tokens
 | SBERT-base-nli-cls | 73.65 | | SBERT | sentence-transformers/bert-base-nli-
 mean-tokens | SBERT-base-nli-first_last_avg | 77.96 | | CoSENT | bert-base-
 uncased | CoSENT-base-first_last_avg | 69.93 | | CoSENT | sentence-
 transformers/bert-base-nli-mean-tokens | CoSENT-base-nli-first_last_avg | 79.68
-| #### √§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√ß¬ö¬Ñ√®¬Ø¬Ñ√¶¬µ¬ã√ß¬ª¬ì√¶¬û¬ú√Ø¬º¬ö | Arch | BaseModel | Model |
-ATEC | BQ | LCQMC | PAWSX | STS-B | Avg | |:-------|:--------------------------
---|:--------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:| |
-SBERT | bert-base-chinese | SBERT-bert-base | 46.36 | 70.36 | 78.72 | 46.86 |
-66.41 | 61.74 | | SBERT | hfl/chinese-macbert-base | SBERT-macbert-base | 47.28
-| 68.63 | 79.42 | 55.59 | 64.82 | 63.15 | | SBERT | hfl/chinese-roberta-wwm-ext
-| SBERT-roberta-ext | 48.29 | 69.99 | 79.22 | 44.10 | 72.42 | 62.80 | | CoSENT
-| bert-base-chinese | CoSENT-bert-base | 49.74 | 72.38 | 78.69 | 60.00 | 79.27
-| 68.01 | | CoSENT | hfl/chinese-macbert-base | CoSENT-macbert-base | 50.39 |
-72.93 | 79.17 | 60.86 | 79.30 | 68.53 | | CoSENT | hfl/chinese-roberta-wwm-ext
-| CoSENT-roberta-ext | 50.81 | 71.45 | 79.31 | 61.56 | 79.96 | 68.61 |
-√®¬Ø¬¥√¶¬ò¬é√Ø¬º¬ö - √ß¬ª¬ì√¶¬û¬ú√®¬Ø¬Ñ√¶¬µ¬ã√¶¬å¬á√¶¬†¬á√Ø¬º¬öspearman√ß¬≥¬ª√¶¬ï¬∞ -
+| | CoSENT | sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 |
+[shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/
+text2vec-base-multilingual) | 80.12 | ####
+√§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√ß¬ö¬Ñ√®¬Ø¬Ñ√¶¬µ¬ã√ß¬ª¬ì√¶¬û¬ú√Ø¬º¬ö | Arch | BaseModel | Model | ATEC | BQ
+| LCQMC | PAWSX | STS-B | Avg | |:-------|:----------------------------|:------
+--------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:| | SBERT | bert-
+base-chinese | SBERT-bert-base | 46.36 | 70.36 | 78.72 | 46.86 | 66.41 | 61.74
+| | SBERT | hfl/chinese-macbert-base | SBERT-macbert-base | 47.28 | 68.63 |
+79.42 | 55.59 | 64.82 | 63.15 | | SBERT | hfl/chinese-roberta-wwm-ext | SBERT-
+roberta-ext | 48.29 | 69.99 | 79.22 | 44.10 | 72.42 | 62.80 | | CoSENT | bert-
+base-chinese | CoSENT-bert-base | 49.74 | 72.38 | 78.69 | 60.00 | 79.27 | 68.01
+| | CoSENT | hfl/chinese-macbert-base | CoSENT-macbert-base | 50.39 | 72.93 |
+79.17 | 60.86 | 79.30 | 68.53 | | CoSENT | hfl/chinese-roberta-wwm-ext |
+CoSENT-roberta-ext | 50.81 | 71.45 | 79.31 | 61.56 | 79.96 | 68.61 | √®¬Ø¬¥√¶¬ò¬é√Ø¬º¬ö
+- √ß¬ª¬ì√¶¬û¬ú√®¬Ø¬Ñ√¶¬µ¬ã√¶¬å¬á√¶¬†¬á√Ø¬º¬öspearman√ß¬≥¬ª√¶¬ï¬∞ -
 √§¬∏¬∫√®¬Ø¬Ñ√¶¬µ¬ã√¶¬®¬°√•¬û¬ã√®¬É¬Ω√•¬ä¬õ√Ø¬º¬å√ß¬ª¬ì√¶¬û¬ú√•¬ù¬á√•¬è¬™√ß¬î¬®√®¬Ø¬•√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√ß¬ö¬Ñtrain√®¬Æ¬≠√ß¬ª¬É√Ø¬º¬å√•¬ú¬®test√§¬∏¬ä√®¬Ø¬Ñ√§¬º¬∞√•¬æ¬ó√•¬à¬∞√ß¬ö¬Ñ√®¬°¬®√ß¬é¬∞√Ø¬º¬å√¶¬≤¬°√ß¬î¬®√•¬§¬ñ√©¬É¬®√¶¬ï¬∞√¶¬ç¬Æ
-### Release Models - √¶¬ú¬¨√©¬°¬π√ß¬õ¬Ærelease√¶¬®¬°√•¬û¬ã√ß¬ö¬Ñ√§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√®¬Ø¬Ñ√¶¬µ¬ã√ß¬ª¬ì√¶¬û¬ú√Ø¬º¬ö |
-Arch | BaseModel | Model | ATEC | BQ | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-
-dc | Avg | QPS | |:-----------|:----------------------------------|:-----------
+- `SBERT-macbert-base`√¶¬®¬°√•¬û¬ã√Ø¬º¬å√¶¬ò¬Ø√ß¬î¬®SBert√¶¬ñ¬π√¶¬≥¬ï√®¬Æ¬≠√ß¬ª¬É√Ø¬º¬å√®¬ø¬ê√®¬°¬å[examples/
+training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/
+blob/master/examples/training_sup_text_matching_model.py)√§¬ª¬£√ß¬†¬Å√•¬è¬Ø√®¬Æ¬≠√ß¬ª¬É√¶¬®¬°√•¬û¬ã
+- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-
+v2`√¶¬®¬°√•¬û¬ã√¶¬ò¬Ø√ß¬î¬®SBert√®¬Æ¬≠√ß¬ª¬É√Ø¬º¬å√¶¬ò¬Ø`paraphrase-MiniLM-L12-
+v2`√¶¬®¬°√•¬û¬ã√ß¬ö¬Ñ√•¬§¬ö√®¬Ø¬≠√®¬®¬Ä√ß¬â¬à√¶¬ú¬¨√Ø¬º¬å√¶¬î¬Ø√¶¬å¬Å√§¬∏¬≠√¶¬ñ¬á√£¬Ä¬Å√®¬ã¬±√¶¬ñ¬á√ß¬≠¬â ### Release Models -
+√¶¬ú¬¨√©¬°¬π√ß¬õ¬Ærelease√¶¬®¬°√•¬û¬ã√ß¬ö¬Ñ√§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√®¬Ø¬Ñ√¶¬µ¬ã√ß¬ª¬ì√¶¬û¬ú√Ø¬º¬ö | Arch | BaseModel | Model
+| ATEC | BQ | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-dc | Avg | QPS | |:-------
+----|:-------------------------------------------------------------|:----------
 -------------------------------------------------------------------------------
---------------------------------------------------------|:-----:|:-----:|:----
+---------------------------------------------------------|:-----:|:-----:|:----
 -:|:-----:|:-----:|:-------:|:-------:|:---------:|:-----:| | Word2Vec |
 word2vec | [w2v-light-tencent-chinese](https://ai.tencent.com/ailab/nlp/en/
 download.html) | 20.00 | 31.49 | 59.46 | 2.57 | 55.78 | 55.04 | 20.70 | 35.03 |
 23769 | | SBERT | xlm-roberta-base | [sentence-transformers/paraphrase-
 multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/
 paraphrase-multilingual-MiniLM-L12-v2) | 18.42 | 38.52 | 63.96 | 10.14 | 78.90
-| 63.01 | 52.28 | 46.46 | 3138 | | Instructor | hfl/chinese-roberta-wwm-ext |
-[moka-ai/m3e-base](https://huggingface.co/moka-ai/m3e-base) | 41.27 | 63.81 |
-74.87 | 12.20 | 76.96 | 75.83 | 60.55 | 57.93 | 2980 | | CoSENT | hfl/chinese-
-macbert-base | [shibing624/text2vec-base-chinese](https://huggingface.co/
-shibing624/text2vec-base-chinese) | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 |
-70.27 | 50.42 | 51.61 | 3008 | | CoSENT | hfl/chinese-lert-large |
-[GanymedeNil/text2vec-large-chinese](https://huggingface.co/GanymedeNil/
-text2vec-large-chinese) | 32.61 | 44.59 | 69.30 | 14.51 | 79.44 | 73.01 | 59.04
-| 53.12 | 2092 | | CoSENT | nghuyong/ernie-3.0-base-zh | [shibing624/text2vec-
-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-
-sentence) | 43.37 | 61.43 | 73.48 | 38.90 | 78.25 | 70.60 | 53.08 | 59.87 |
-3089 | | CoSENT | nghuyong/ernie-3.0-base-zh | [shibing624/text2vec-base-
-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-
-paraphrase) | 44.89 | 63.58 | 74.24 | 40.90 | 78.93 | 76.70 | 63.30 | **63.08**
-| 3066 | √®¬Ø¬¥√¶¬ò¬é√Ø¬º¬ö - √ß¬ª¬ì√¶¬û¬ú√®¬Ø¬Ñ√¶¬µ¬ã√¶¬å¬á√¶¬†¬á√Ø¬º¬öspearman√ß¬≥¬ª√¶¬ï¬∞ -
-√¶¬®¬°√•¬û¬ã√®¬Æ¬≠√ß¬ª¬É√•¬Æ¬û√©¬™¬å√¶¬ä¬•√•¬ë¬ä√Ø¬º¬ö[√•¬Æ¬û√©¬™¬å√¶¬ä¬•√•¬ë¬ä](https://github.com/shibing624/
-text2vec/blob/master/docs/model_report.md) - `shibing624/text2vec-base-
+| 63.01 | 52.28 | 46.46 | 3138 | | CoSENT | hfl/chinese-macbert-base |
+[shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-
+base-chinese) | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 | 70.27 | 50.42 | 51.61 |
+3008 | | CoSENT | hfl/chinese-lert-large | [GanymedeNil/text2vec-large-chinese]
+(https://huggingface.co/GanymedeNil/text2vec-large-chinese) | 32.61 | 44.59 |
+69.30 | 14.51 | 79.44 | 73.01 | 59.04 | 53.12 | 2092 | | CoSENT | nghuyong/
+ernie-3.0-base-zh | [shibing624/text2vec-base-chinese-sentence](https://
+huggingface.co/shibing624/text2vec-base-chinese-sentence) | 43.37 | 61.43 |
+73.48 | 38.90 | 78.25 | 70.60 | 53.08 | 59.87 | 3089 | | CoSENT | nghuyong/
+ernie-3.0-base-zh | [shibing624/text2vec-base-chinese-paraphrase](https://
+huggingface.co/shibing624/text2vec-base-chinese-paraphrase) | 44.89 | 63.58 |
+74.24 | 40.90 | 78.93 | 76.70 | 63.30 | **63.08** | 3066 | | CoSENT | sentence-
+transformers/paraphrase-multilingual-MiniLM-L12-v2 | [shibing624/text2vec-base-
+multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual) |
+32.39 | 50.33 | 65.64 | 32.56 | 74.45 | 68.88 | 51.17 | 53.67 | 3138 |
+√®¬Ø¬¥√¶¬ò¬é√Ø¬º¬ö - √ß¬ª¬ì√¶¬û¬ú√®¬Ø¬Ñ√¶¬µ¬ã√¶¬å¬á√¶¬†¬á√Ø¬º¬öspearman√ß¬≥¬ª√¶¬ï¬∞ - `shibing624/text2vec-base-
 chinese`√¶¬®¬°√•¬û¬ã√Ø¬º¬å√¶¬ò¬Ø√ß¬î¬®CoSENT√¶¬ñ¬π√¶¬≥¬ï√®¬Æ¬≠√ß¬ª¬É√Ø¬º¬å√•¬ü¬∫√§¬∫¬é`hfl/chinese-macbert-
 base`√•¬ú¬®√§¬∏¬≠√¶¬ñ¬áSTS-B√¶¬ï¬∞√¶¬ç¬Æ√®¬Æ¬≠√ß¬ª¬É√•¬æ¬ó√•¬à¬∞√Ø¬º¬å√•¬π¬∂√•¬ú¬®√§¬∏¬≠√¶¬ñ¬áSTS-
 B√¶¬µ¬ã√®¬Ø¬ï√©¬õ¬Ü√®¬Ø¬Ñ√§¬º¬∞√®¬æ¬æ√•¬à¬∞√®¬æ¬É√•¬•¬Ω√¶¬ï¬à√¶¬û¬ú√Ø¬º¬å√®¬ø¬ê√®¬°¬å[examples/
 training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/
 blob/master/examples/
 training_sup_text_matching_model.py)√§¬ª¬£√ß¬†¬Å√•¬è¬Ø√®¬Æ¬≠√ß¬ª¬É√¶¬®¬°√•¬û¬ã√Ø¬º¬å√¶¬®¬°√•¬û¬ã√¶¬ñ¬á√§¬ª¬∂√•¬∑¬≤√ß¬ª¬è√§¬∏¬ä√§¬º¬†HF
 model hub√Ø¬º¬å√§¬∏¬≠√¶¬ñ¬á√©¬Ä¬ö√ß¬î¬®√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√¶¬é¬®√®¬ç¬ê√§¬Ω¬ø√ß¬î¬® - `shibing624/text2vec-
@@ -137,81 +155,83 @@
 [shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://
 huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-
 sentence-dataset)√•¬ä¬†√•¬Ö¬•√§¬∫¬Üs2p(sentence to
 paraphrase)√¶¬ï¬∞√¶¬ç¬Æ√Ø¬º¬å√•¬º¬∫√•¬å¬ñ√§¬∫¬Ü√•¬Ö¬∂√©¬ï¬ø√¶¬ñ¬á√¶¬ú¬¨√ß¬ö¬Ñ√®¬°¬®√•¬æ¬Å√®¬É¬Ω√•¬ä¬õ√Ø¬º¬å√•¬π¬∂√•¬ú¬®√§¬∏¬≠√¶¬ñ¬á√•¬ê¬ÑNLI√¶¬µ¬ã√®¬Ø¬ï√©¬õ¬Ü√®¬Ø¬Ñ√§¬º¬∞√®¬æ¬æ√•¬à¬∞SOTA√Ø¬º¬å√®¬ø¬ê√®¬°¬å
 [examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/
 shibing624/text2vec/blob/master/examples/
 training_sup_text_matching_model_jsonl_data.py)√§¬ª¬£√ß¬†¬Å√•¬è¬Ø√®¬Æ¬≠√ß¬ª¬É√¶¬®¬°√•¬û¬ã√Ø¬º¬å√¶¬®¬°√•¬û¬ã√¶¬ñ¬á√§¬ª¬∂√•¬∑¬≤√ß¬ª¬è√§¬∏¬ä√§¬º¬†HF
-model hub√Ø¬º¬å√§¬∏¬≠√¶¬ñ¬ás2p(√•¬è¬•√•¬≠¬êvs√¶¬Æ¬µ√®¬ê¬Ω)√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√¶¬é¬®√®¬ç¬ê√§¬Ω¬ø√ß¬î¬® - `SBERT-
-macbert-base`√¶¬®¬°√•¬û¬ã√Ø¬º¬å√¶¬ò¬Ø√ß¬î¬®SBERT√¶¬ñ¬π√¶¬≥¬ï√®¬Æ¬≠√ß¬ª¬É√Ø¬º¬å√®¬ø¬ê√®¬°¬å[examples/
-training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/
-blob/master/examples/training_sup_text_matching_model.py)√§¬ª¬£√ß¬†¬Å√•¬è¬Ø√®¬Æ¬≠√ß¬ª¬É√¶¬®¬°√•¬û¬ã
-- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-
-v2`√¶¬®¬°√•¬û¬ã√¶¬ò¬Ø√ß¬î¬®SBERT√®¬Æ¬≠√ß¬ª¬É√Ø¬º¬å√¶¬ò¬Ø`paraphrase-MiniLM-L12-
-v2`√¶¬®¬°√•¬û¬ã√ß¬ö¬Ñ√•¬§¬ö√®¬Ø¬≠√®¬®¬Ä√ß¬â¬à√¶¬ú¬¨√Ø¬º¬å√¶¬î¬Ø√¶¬å¬Å√§¬∏¬≠√¶¬ñ¬á√£¬Ä¬Å√®¬ã¬±√¶¬ñ¬á√ß¬≠¬â - `w2v-light-tencent-
+model hub√Ø¬º¬å√§¬∏¬≠√¶¬ñ¬ás2p(√•¬è¬•√•¬≠¬êvs√¶¬Æ¬µ√®¬ê¬Ω)√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√¶¬é¬®√®¬ç¬ê√§¬Ω¬ø√ß¬î¬® -
+`shibing624/text2vec-base-
+multilingual`√¶¬®¬°√•¬û¬ã√Ø¬º¬å√¶¬ò¬Ø√ß¬î¬®CoSENT√¶¬ñ¬π√¶¬≥¬ï√®¬Æ¬≠√ß¬ª¬É√Ø¬º¬å√•¬ü¬∫√§¬∫¬é`sentence-transformers/
+paraphrase-multilingual-MiniLM-L12-
+v2`√ß¬î¬®√§¬∫¬∫√•¬∑¬•√¶¬å¬ë√©¬Ä¬â√•¬ê¬é√ß¬ö¬Ñ√•¬§¬ö√®¬Ø¬≠√®¬®¬ÄSTS√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü[shibing624/nli-zh-all/text2vec-
+base-multilingual-dataset](https://huggingface.co/datasets/shibing624/nli-zh-
+all/tree/main/text2vec-base-multilingual-
+dataset)√®¬Æ¬≠√ß¬ª¬É√•¬æ¬ó√•¬à¬∞√Ø¬º¬å√•¬π¬∂√•¬ú¬®√§¬∏¬≠√®¬ã¬±√¶¬ñ¬á√¶¬µ¬ã√®¬Ø¬ï√©¬õ¬Ü√®¬Ø¬Ñ√§¬º¬∞√ß¬õ¬∏√•¬Ø¬π√§¬∫¬é√•¬é¬ü√¶¬®¬°√•¬û¬ã√¶¬ï¬à√¶¬û¬ú√¶¬ú¬â√¶¬è¬ê√•¬ç¬á√Ø¬º¬å√®¬ø¬ê√®¬°¬å
+[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/
+shibing624/text2vec/blob/master/examples/
+training_sup_text_matching_model_jsonl_data.py)√§¬ª¬£√ß¬†¬Å√•¬è¬Ø√®¬Æ¬≠√ß¬ª¬É√¶¬®¬°√•¬û¬ã√Ø¬º¬å√¶¬®¬°√•¬û¬ã√¶¬ñ¬á√§¬ª¬∂√•¬∑¬≤√ß¬ª¬è√§¬∏¬ä√§¬º¬†HF
+model hub√Ø¬º¬å√•¬§¬ö√®¬Ø¬≠√®¬®¬Ä√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√¶¬é¬®√®¬ç¬ê√§¬Ω¬ø√ß¬î¬® - `w2v-light-tencent-
 chinese`√¶¬ò¬Ø√®¬Ö¬æ√®¬Æ¬Ø√®¬Ø¬ç√•¬ê¬ë√©¬á¬è√ß¬ö¬ÑWord2Vec√¶¬®¬°√•¬û¬ã√Ø¬º¬åCPU√•¬ä¬†√®¬Ω¬Ω√§¬Ω¬ø√ß¬î¬®√Ø¬º¬å√©¬Ä¬Ç√ß¬î¬®√§¬∫¬é√§¬∏¬≠√¶¬ñ¬á√•¬≠¬ó√©¬ù¬¢√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√•¬í¬å√ß¬º¬∫√•¬∞¬ë√¶¬ï¬∞√¶¬ç¬Æ√ß¬ö¬Ñ√•¬Ü¬∑√•¬ê¬Ø√•¬ä¬®√¶¬É¬Ö√•¬Ü¬µ
 - √•¬ê¬Ñ√©¬¢¬Ñ√®¬Æ¬≠√ß¬ª¬É√¶¬®¬°√•¬û¬ã√•¬ù¬á√•¬è¬Ø√§¬ª¬•√©¬Ä¬ö√®¬ø¬átransformers√®¬∞¬É√ß¬î¬®√Ø¬º¬å√•¬¶¬ÇMacBERT√¶¬®¬°√•¬û¬ã√Ø¬º¬ö`--
 model_name hfl/chinese-macbert-base` √¶¬à¬ñ√®¬Ä¬Öroberta√¶¬®¬°√•¬û¬ã√Ø¬º¬ö`--model_name uer/
 roberta-medium-wwm-chinese-cluecorpussmall` -
 √§¬∏¬∫√¶¬µ¬ã√®¬Ø¬Ñ√¶¬®¬°√•¬û¬ã√ß¬ö¬Ñ√©¬≤¬Å√¶¬£¬í√¶¬Ä¬ß√Ø¬º¬å√•¬ä¬†√•¬Ö¬•√§¬∫¬Ü√¶¬ú¬™√®¬Æ¬≠√ß¬ª¬É√®¬ø¬á√ß¬ö¬ÑSOHU√¶¬µ¬ã√®¬Ø¬ï√©¬õ¬Ü√Ø¬º¬å√ß¬î¬®√§¬∫¬é√¶¬µ¬ã√®¬Ø¬ï√¶¬®¬°√•¬û¬ã√ß¬ö¬Ñ√¶¬≥¬õ√•¬å¬ñ√®¬É¬Ω√•¬ä¬õ√Ø¬º¬õ√§¬∏¬∫√®¬æ¬æ√•¬à¬∞√•¬º¬Ä√ß¬Æ¬±√•¬ç¬≥√ß¬î¬®√ß¬ö¬Ñ√•¬Æ¬û√ß¬î¬®√¶¬ï¬à√¶¬û¬ú√Ø¬º¬å√§¬Ω¬ø√ß¬î¬®√§¬∫¬Ü√¶¬ê¬ú√©¬õ¬Ü√•¬à¬∞√ß¬ö¬Ñ√•¬ê¬Ñ√§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬å√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√§¬π¬ü√§¬∏¬ä√§¬º¬†√•¬à¬∞HF
 datasets[√©¬ì¬æ√¶¬é¬•√®¬ß¬Å√§¬∏¬ã√¶¬ñ¬π](#√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü) -
 √§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√•¬Æ¬û√©¬™¬å√®¬°¬®√¶¬ò¬é√Ø¬º¬åpooling√¶¬ú¬Ä√§¬º¬ò√¶¬ò¬Ø`EncoderType.FIRST_LAST_AVG`√•¬í¬å`EncoderType.MEAN`√Ø¬º¬å√§¬∏¬§√®¬Ä¬Ö√©¬¢¬Ñ√¶¬µ¬ã√¶¬ï¬à√¶¬û¬ú√•¬∑¬Æ√•¬º¬Ç√•¬æ¬à√•¬∞¬è
 -
 √§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√®¬Ø¬Ñ√¶¬µ¬ã√ß¬ª¬ì√¶¬û¬ú√•¬§¬ç√ß¬é¬∞√Ø¬º¬å√•¬è¬Ø√§¬ª¬•√§¬∏¬ã√®¬Ω¬Ω√§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√•¬à¬∞`examples/
 data`√Ø¬º¬å√®¬ø¬ê√®¬°¬å[tests/test_model_spearman.py](https://github.com/shibing624/
 text2vec/blob/master/tests/test_model_spearman.py)√§¬ª¬£√ß¬†¬Å√•¬§¬ç√ß¬é¬∞√®¬Ø¬Ñ√¶¬µ¬ã√ß¬ª¬ì√¶¬û¬ú -
-QPS√ß¬ö¬ÑGPU√¶¬µ¬ã√®¬Ø¬ï√ß¬é¬Ø√•¬¢¬É√¶¬ò¬ØTesla V100√Ø¬º¬å√¶¬ò¬æ√•¬≠¬ò32GB # Demo Official Demo: https://
-www.mulanai.com/product/short_text_sim/ HuggingFace Demo: https://
-huggingface.co/spaces/shibing624/text2vec ![](docs/hf.png) run example:
-[examples/gradio_demo.py](https://github.com/shibing624/text2vec/blob/master/
-examples/gradio_demo.py) to see the demo: ```shell python examples/
-gradio_demo.py ``` # Install ```shell pip install torch # conda install pytorch
-pip install -U text2vec ``` or ```shell pip install torch # conda install
-pytorch pip install -r requirements.txt git clone https://github.com/
-shibing624/text2vec.git cd text2vec pip install --no-deps . ``` # Usage ##
-√¶¬ñ¬á√¶¬ú¬¨√•¬ê¬ë√©¬á¬è√®¬°¬®√•¬æ¬Å √•¬ü¬∫√§¬∫¬é`pretrained model`√®¬Æ¬°√ß¬Æ¬ó√¶¬ñ¬á√¶¬ú¬¨√•¬ê¬ë√©¬á¬è√Ø¬º¬ö ```zsh >>>
-from text2vec import SentenceModel >>> m = SentenceModel() >>> m.encode
-("√•¬¶¬Ç√§¬Ω¬ï√¶¬õ¬¥√¶¬ç¬¢√®¬ä¬±√•¬ë¬ó√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°") Embedding shape: (768,) ``` example:
-[examples/computing_embeddings_demo.py](https://github.com/shibing624/text2vec/
-blob/master/examples/computing_embeddings_demo.py) ```python import sys
-sys.path.append('..') from text2vec import SentenceModel from text2vec import
-Word2Vec def compute_emb(model): # Embed a list of sentences sentences =
-[ '√•¬ç¬°', '√©¬ì¬∂√®¬°¬å√•¬ç¬°', '√•¬¶¬Ç√§¬Ω¬ï√¶¬õ¬¥√¶¬ç¬¢√®¬ä¬±√•¬ë¬ó√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°',
-'√®¬ä¬±√•¬ë¬ó√¶¬õ¬¥√¶¬î¬π√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°', 'This framework generates embeddings for each
-input sentence', 'Sentences are passed as a list of string.', 'The quick brown
-fox jumps over the lazy dog.' ] sentence_embeddings = model.encode(sentences)
-print(type(sentence_embeddings), sentence_embeddings.shape) # The result is a
-list of sentence embeddings as numpy arrays for sentence, embedding in zip
-(sentences, sentence_embeddings): print("Sentence:", sentence) print("Embedding
-shape:", embedding.shape) print("Embedding head:", embedding[:10]) print() if
-__name__ == "__main__": # √§¬∏¬≠√¶¬ñ¬á√•¬è¬•√•¬ê¬ë√©¬á¬è√¶¬®¬°√•¬û¬ã
-(CoSENT)√Ø¬º¬å√§¬∏¬≠√¶¬ñ¬á√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√¶¬é¬®√®¬ç¬ê√Ø¬º¬å√¶¬î¬Ø√¶¬å¬Åfine-tune√ß¬ª¬ß√ß¬ª¬≠√®¬Æ¬≠√ß¬ª¬É
-t2v_model = SentenceModel("shibing624/text2vec-base-chinese") compute_emb
-(t2v_model) # √¶¬î¬Ø√¶¬å¬Å√•¬§¬ö√®¬Ø¬≠√®¬®¬Ä√ß¬ö¬Ñ√•¬è¬•√•¬ê¬ë√©¬á¬è√¶¬®¬°√•¬û¬ã√Ø¬º¬àSentence-
-BERT√Ø¬º¬â√Ø¬º¬å√®¬ã¬±√¶¬ñ¬á√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√¶¬é¬®√®¬ç¬ê√Ø¬º¬å√¶¬î¬Ø√¶¬å¬Åfine-tune√ß¬ª¬ß√ß¬ª¬≠√®¬Æ¬≠√ß¬ª¬É
-sbert_model = SentenceModel("sentence-transformers/paraphrase-multilingual-
-MiniLM-L12-v2") compute_emb(sbert_model) # √§¬∏¬≠√¶¬ñ¬á√®¬Ø¬ç√•¬ê¬ë√©¬á¬è√¶¬®¬°√•¬û¬ã
+QPS√ß¬ö¬ÑGPU√¶¬µ¬ã√®¬Ø¬ï√ß¬é¬Ø√•¬¢¬É√¶¬ò¬ØTesla V100√Ø¬º¬å√¶¬ò¬æ√•¬≠¬ò32GB √¶¬®¬°√•¬û¬ã√®¬Æ¬≠√ß¬ª¬É√•¬Æ¬û√©¬™¬å√¶¬ä¬•√•¬ë¬ä√Ø¬º¬ö
+[√•¬Æ¬û√©¬™¬å√¶¬ä¬•√•¬ë¬ä](https://github.com/shibing624/text2vec/blob/master/docs/
+model_report.md) ## Demo Official Demo: https://www.mulanai.com/product/
+short_text_sim/ HuggingFace Demo: https://huggingface.co/spaces/shibing624/
+text2vec ![](docs/hf.png) run example: [examples/gradio_demo.py](https://
+github.com/shibing624/text2vec/blob/master/examples/gradio_demo.py) to see the
+demo: ```shell python examples/gradio_demo.py ``` ## Install ```shell pip
+install torch # conda install pytorch pip install -U text2vec ``` or ```shell
+pip install torch # conda install pytorch pip install -r requirements.txt git
+clone https://github.com/shibing624/text2vec.git cd text2vec pip install --no-
+deps . ``` ## Usage ### √¶¬ñ¬á√¶¬ú¬¨√•¬ê¬ë√©¬á¬è√®¬°¬®√•¬æ¬Å √•¬ü¬∫√§¬∫¬é`pretrained
+model`√®¬Æ¬°√ß¬Æ¬ó√¶¬ñ¬á√¶¬ú¬¨√•¬ê¬ë√©¬á¬è√Ø¬º¬ö ```zsh >>> from text2vec import SentenceModel >>> m
+= SentenceModel() >>> m.encode("√•¬¶¬Ç√§¬Ω¬ï√¶¬õ¬¥√¶¬ç¬¢√®¬ä¬±√•¬ë¬ó√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°") Embedding
+shape: (768,) ``` example: [examples/computing_embeddings_demo.py](https://
+github.com/shibing624/text2vec/blob/master/examples/
+computing_embeddings_demo.py) ```python import sys sys.path.append('..') from
+text2vec import SentenceModel from text2vec import Word2Vec def compute_emb
+(model): # Embed a list of sentences sentences = [ '√•¬ç¬°', '√©¬ì¬∂√®¬°¬å√•¬ç¬°',
+'√•¬¶¬Ç√§¬Ω¬ï√¶¬õ¬¥√¶¬ç¬¢√®¬ä¬±√•¬ë¬ó√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°', '√®¬ä¬±√•¬ë¬ó√¶¬õ¬¥√¶¬î¬π√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°', 'This
+framework generates embeddings for each input sentence', 'Sentences are passed
+as a list of string.', 'The quick brown fox jumps over the lazy dog.' ]
+sentence_embeddings = model.encode(sentences) print(type(sentence_embeddings),
+sentence_embeddings.shape) # The result is a list of sentence embeddings as
+numpy arrays for sentence, embedding in zip(sentences, sentence_embeddings):
+print("Sentence:", sentence) print("Embedding shape:", embedding.shape) print
+("Embedding head:", embedding[:10]) print() if __name__ == "__main__": #
+√§¬∏¬≠√¶¬ñ¬á√•¬è¬•√•¬ê¬ë√©¬á¬è√¶¬®¬°√•¬û¬ã(CoSENT)√Ø¬º¬å√§¬∏¬≠√¶¬ñ¬á√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√¶¬é¬®√®¬ç¬ê√Ø¬º¬å√¶¬î¬Ø√¶¬å¬Åfine-
+tune√ß¬ª¬ß√ß¬ª¬≠√®¬Æ¬≠√ß¬ª¬É t2v_model = SentenceModel("shibing624/text2vec-base-chinese")
+compute_emb(t2v_model) #
+√¶¬î¬Ø√¶¬å¬Å√•¬§¬ö√®¬Ø¬≠√®¬®¬Ä√ß¬ö¬Ñ√•¬è¬•√•¬ê¬ë√©¬á¬è√¶¬®¬°√•¬û¬ã√Ø¬º¬àCoSENT√Ø¬º¬â√Ø¬º¬å√•¬§¬ö√®¬Ø¬≠√®¬®¬Ä√Ø¬º¬à√•¬å¬Ö√¶¬ã¬¨√§¬∏¬≠√®¬ã¬±√¶¬ñ¬á√Ø¬º¬â√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√¶¬é¬®√®¬ç¬ê√Ø¬º¬å√¶¬î¬Ø√¶¬å¬Åfine-
+tune√ß¬ª¬ß√ß¬ª¬≠√®¬Æ¬≠√ß¬ª¬É sbert_model = SentenceModel("shibing624/text2vec-base-
+multilingual") compute_emb(sbert_model) # √§¬∏¬≠√¶¬ñ¬á√®¬Ø¬ç√•¬ê¬ë√©¬á¬è√¶¬®¬°√•¬û¬ã
 (word2vec)√Ø¬º¬å√§¬∏¬≠√¶¬ñ¬á√•¬≠¬ó√©¬ù¬¢√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√•¬í¬å√•¬Ü¬∑√•¬ê¬Ø√•¬ä¬®√©¬Ä¬Ç√ß¬î¬® w2v_model = Word2Vec
 ("w2v-light-tencent-chinese") compute_emb(w2v_model) ``` output: ```
 numpy.ndarray'> (7, 768) Sentence: √•¬ç¬° Embedding shape: (768,) Sentence:
 √©¬ì¬∂√®¬°¬å√•¬ç¬° Embedding shape: (768,) ... ``` -
 √®¬ø¬î√•¬õ¬û√•¬Ä¬º`embeddings`√¶¬ò¬Ø`numpy.ndarray`√ß¬±¬ª√•¬û¬ã√Ø¬º¬åshape√§¬∏¬∫`(sentences_size,
 model_embedding_size)`√Ø¬º¬å√§¬∏¬â√§¬∏¬™√¶¬®¬°√•¬û¬ã√§¬ª¬ª√©¬Ä¬â√§¬∏¬Ä√ß¬ß¬ç√•¬ç¬≥√•¬è¬Ø√Ø¬º¬å√¶¬é¬®√®¬ç¬ê√ß¬î¬®√ß¬¨¬¨√§¬∏¬Ä√§¬∏¬™√£¬Ä¬Ç
 - `shibing624/text2vec-base-chinese`√¶¬®¬°√•¬û¬ã√¶¬ò¬ØCoSENT√¶¬ñ¬π√¶¬≥¬ï√•¬ú¬®√§¬∏¬≠√¶¬ñ¬áSTS-
 B√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√®¬Æ¬≠√ß¬ª¬É√•¬æ¬ó√•¬à¬∞√ß¬ö¬Ñ√Ø¬º¬å√¶¬®¬°√•¬û¬ã√•¬∑¬≤√ß¬ª¬è√§¬∏¬ä√§¬º¬†√•¬à¬∞huggingface√ß¬ö¬Ñ √¶¬®¬°√•¬û¬ã√•¬∫¬ì
 [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-
 base-chinese)√Ø¬º¬å
 √¶¬ò¬Ø`text2vec.SentenceModel`√¶¬å¬á√•¬Æ¬ö√ß¬ö¬Ñ√©¬ª¬ò√®¬Æ¬§√¶¬®¬°√•¬û¬ã√Ø¬º¬å√•¬è¬Ø√§¬ª¬•√©¬Ä¬ö√®¬ø¬á√§¬∏¬ä√©¬ù¬¢√ß¬§¬∫√§¬æ¬ã√®¬∞¬É√ß¬î¬®√Ø¬º¬å√¶¬à¬ñ√®¬Ä¬Ö√•¬¶¬Ç√§¬∏¬ã√¶¬â¬Ä√ß¬§¬∫√ß¬î¬®
 [transformers√•¬∫¬ì](https://github.com/huggingface/transformers)√®¬∞¬É√ß¬î¬®√Ø¬º¬å
-√¶¬®¬°√•¬û¬ã√®¬á¬™√•¬ä¬®√§¬∏¬ã√®¬Ω¬Ω√•¬à¬∞√¶¬ú¬¨√¶¬ú¬∫√®¬∑¬Ø√•¬æ¬Ñ√Ø¬º¬ö`~/.cache/huggingface/transformers` -
-`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`√¶¬®¬°√•¬û¬ã√¶¬ò¬ØSentence-
-BERT√ß¬ö¬Ñ√•¬§¬ö√®¬Ø¬≠√®¬®¬Ä√•¬è¬•√•¬ê¬ë√©¬á¬è√¶¬®¬°√•¬û¬ã√Ø¬º¬å
-√©¬Ä¬Ç√ß¬î¬®√§¬∫¬é√©¬á¬ä√§¬π¬â√Ø¬º¬àparaphrase√Ø¬º¬â√®¬Ø¬Ü√•¬à¬´√Ø¬º¬å√¶¬ñ¬á√¶¬ú¬¨√•¬å¬π√©¬Ö¬ç√Ø¬º¬å√©¬Ä¬ö√®¬ø¬á`text2vec.SentenceModel`√•¬í¬å
-[sentence-transformers√•¬∫¬ì]((https://github.com/UKPLab/sentence-
-transformers))√©¬É¬Ω√•¬è¬Ø√§¬ª¬•√®¬∞¬É√ß¬î¬®√®¬Ø¬•√¶¬®¬°√•¬û¬ã - `w2v-light-tencent-
+√¶¬®¬°√•¬û¬ã√®¬á¬™√•¬ä¬®√§¬∏¬ã√®¬Ω¬Ω√•¬à¬∞√¶¬ú¬¨√¶¬ú¬∫√®¬∑¬Ø√•¬æ¬Ñ√Ø¬º¬ö`~/.cache/huggingface/transformers` - `w2v-
+light-tencent-
 chinese`√¶¬ò¬Ø√©¬Ä¬ö√®¬ø¬ágensim√•¬ä¬†√®¬Ω¬Ω√ß¬ö¬ÑWord2Vec√¶¬®¬°√•¬û¬ã√Ø¬º¬å√§¬Ω¬ø√ß¬î¬®√®¬Ö¬æ√®¬Æ¬Ø√®¬Ø¬ç√•¬ê¬ë√©¬á¬è`Tencent_AILab_ChineseEmbedding.tar.gz`√®¬Æ¬°√ß¬Æ¬ó√•¬ê¬Ñ√•¬≠¬ó√®¬Ø¬ç√ß¬ö¬Ñ√®¬Ø¬ç√•¬ê¬ë√©¬á¬è√Ø¬º¬å√•¬è¬•√•¬≠¬ê√•¬ê¬ë√©¬á¬è√©¬Ä¬ö√®¬ø¬á√•¬ç¬ï√®¬Ø¬ç√®¬Ø¬ç
 √•¬ê¬ë√©¬á¬è√•¬è¬ñ√•¬π¬≥√•¬ù¬á√•¬Ä¬º√•¬æ¬ó√•¬à¬∞√Ø¬º¬å√¶¬®¬°√•¬û¬ã√®¬á¬™√•¬ä¬®√§¬∏¬ã√®¬Ω¬Ω√•¬à¬∞√¶¬ú¬¨√¶¬ú¬∫√®¬∑¬Ø√•¬æ¬Ñ√Ø¬º¬ö`~/.text2vec/
 datasets/light_Tencent_AILab_ChineseEmbedding.bin` #### Usage (HuggingFace
 Transformers) Without [text2vec](https://github.com/shibing624/text2vec), you
 can use the model like this: First, you pass your input through the transformer
 model, then you have to apply the right pooling-operation on-top of the
 contextualized word embeddings. example: [examples/
@@ -322,16 +342,16 @@
 √¶¬ñ¬á√¶¬ú¬¨√ß¬õ¬∏√§¬º¬º√•¬∫¬¶√®¬Æ¬°√ß¬Æ¬ó√•¬í¬å√¶¬ñ¬á√¶¬ú¬¨√•¬å¬π√©¬Ö¬ç√¶¬ê¬ú√ß¬¥¬¢√§¬ª¬ª√•¬ä¬°√Ø¬º¬å√¶¬é¬®√®¬ç¬ê√§¬Ω¬ø√ß¬î¬®
 [similarities√•¬∫¬ì](https://github.com/shibing624/similarities)
 √Ø¬º¬å√•¬Ö¬º√•¬Æ¬π√¶¬ú¬¨√©¬°¬π√ß¬õ¬Ærelease√ß¬ö¬Ñ
 Word2vec√£¬Ä¬ÅSBERT√£¬Ä¬ÅCosent√ß¬±¬ª√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã√Ø¬º¬å√®¬ø¬ò√¶¬î¬Ø√¶¬å¬Å√•¬≠¬ó√©¬ù¬¢√ß¬ª¬¥√•¬∫¬¶√ß¬õ¬∏√§¬º¬º√•¬∫¬¶√®¬Æ¬°√ß¬Æ¬ó√£¬Ä¬Å√•¬å¬π√©¬Ö¬ç√¶¬ê¬ú√ß¬¥¬¢√ß¬Æ¬ó√¶¬≥¬ï√Ø¬º¬å√¶¬î¬Ø√¶¬å¬Å√¶¬ñ¬á√¶¬ú¬¨√£¬Ä¬Å√•¬õ¬æ√•¬É¬è√£¬Ä¬Ç
 √•¬Æ¬â√®¬£¬Ö√Ø¬º¬ö ```pip install -U similarities``` √•¬è¬•√•¬≠¬ê√ß¬õ¬∏√§¬º¬º√•¬∫¬¶√®¬Æ¬°√ß¬Æ¬ó√Ø¬º¬ö ```python
 from similarities import Similarity m = Similarity() r = m.similarity
 ('√•¬¶¬Ç√§¬Ω¬ï√¶¬õ¬¥√¶¬ç¬¢√®¬ä¬±√•¬ë¬ó√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°', '√®¬ä¬±√•¬ë¬ó√¶¬õ¬¥√¶¬î¬π√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°') print
-(f"similarity score: {float(r)}") # similarity score: 0.855146050453186 ``` #
-Models ## CoSENT model CoSENT√Ø¬º¬àCosine
+(f"similarity score: {float(r)}") # similarity score: 0.855146050453186 ``` ##
+Models ### CoSENT model CoSENT√Ø¬º¬àCosine
 Sentence√Ø¬º¬â√¶¬ñ¬á√¶¬ú¬¨√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã√Ø¬º¬å√•¬ú¬®Sentence-
 BERT√§¬∏¬ä√¶¬î¬π√®¬ø¬õ√§¬∫¬ÜCosineRankLoss√ß¬ö¬Ñ√•¬è¬•√•¬ê¬ë√©¬á¬è√¶¬ñ¬π√¶¬°¬à Network structure: Training:
 [docs/cosent_train.png] Inference: [docs/inference.png] #### CoSENT
 √ß¬õ¬ë√ß¬ù¬£√¶¬®¬°√•¬û¬ã √®¬Æ¬≠√ß¬ª¬É√•¬í¬å√©¬¢¬Ñ√¶¬µ¬ãCoSENT√¶¬®¬°√•¬û¬ã√Ø¬º¬ö - √•¬ú¬®√§¬∏¬≠√¶¬ñ¬áSTS-
 B√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√®¬Æ¬≠√ß¬ª¬É√•¬í¬å√®¬Ø¬Ñ√§¬º¬∞`CoSENT`√¶¬®¬°√•¬û¬ã example: [examples/
 training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/
 blob/master/examples/training_sup_text_matching_model.py) ```shell cd examples
@@ -343,21 +363,24 @@
 'PAWSX'√Ø¬º¬å√•¬Ö¬∑√§¬Ω¬ì√•¬è¬Ç√®¬Ä¬ÉHuggingFace datasets [https://huggingface.co/datasets/
 shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh) ```shell
 python training_sup_text_matching_model.py --task_name ATEC --model_arch cosent
 --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base -
 -output_dir ./outputs/ATEC-cosent ``` - √•¬ú¬®√®¬á¬™√¶¬ú¬â√§¬∏¬≠√¶¬ñ¬á√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√§¬∏¬ä√®¬Æ¬≠√ß¬ª¬É√¶¬®¬°√•¬û¬ã
 example: [examples/training_sup_text_matching_model_mydata.py](https://
 github.com/shibing624/text2vec/blob/master/examples/
-training_sup_text_matching_model_mydata.py) ```shell python
-training_sup_text_matching_model_mydata.py --do_train --do_predict ```
-√®¬Æ¬≠√ß¬ª¬É√©¬õ¬Ü√¶¬†¬º√•¬º¬è√•¬è¬Ç√®¬Ä¬É[examples/data/STS-B/STS-B.valid.data](https://github.com/
-shibing624/text2vec/blob/master/examples/data/STS-B/STS-B.valid.data) ```shell
-sentence1 sentence2 label √§¬∏¬Ä√§¬∏¬™√•¬•¬≥√•¬≠¬©√•¬ú¬®√ß¬ª¬ô√•¬•¬π√ß¬ö¬Ñ√•¬§¬¥√•¬è¬ë√•¬Å¬ö√•¬è¬ë√•¬û¬ã√£¬Ä¬Ç
-√§¬∏¬Ä√§¬∏¬™√•¬•¬≥√•¬≠¬©√•¬ú¬®√¶¬¢¬≥√•¬§¬¥√£¬Ä¬Ç 2 √§¬∏¬Ä√ß¬æ¬§√ß¬î¬∑√§¬∫¬∫√•¬ú¬®√¶¬µ¬∑√¶¬ª¬©√§¬∏¬ä√®¬∏¬¢√®¬∂¬≥√ß¬ê¬É√£¬Ä¬Ç
-√§¬∏¬Ä√ß¬æ¬§√ß¬î¬∑√•¬≠¬©√•¬ú¬®√¶¬µ¬∑√¶¬ª¬©√§¬∏¬ä√®¬∏¬¢√®¬∂¬≥√ß¬ê¬É√£¬Ä¬Ç 3
+training_sup_text_matching_model_mydata.py) √•¬ç¬ï√•¬ç¬°√®¬Æ¬≠√ß¬ª¬É√Ø¬º¬ö ```shell
+CUDA_VISIBLE_DEVICES=0 python training_sup_text_matching_model_mydata.py --
+do_train --do_predict ``` √•¬§¬ö√•¬ç¬°√®¬Æ¬≠√ß¬ª¬É√Ø¬º¬ö ```shell CUDA_VISIBLE_DEVICES=0,1
+torchrun --nproc_per_node 2 training_sup_text_matching_model_mydata.py --
+do_train --do_predict --output_dir outputs/STS-B-text2vec-macbert-v1 --
+batch_size 64 --fp16 --data_parallel ``` √®¬Æ¬≠√ß¬ª¬É√©¬õ¬Ü√¶¬†¬º√•¬º¬è√•¬è¬Ç√®¬Ä¬É[examples/data/
+STS-B/STS-B.valid.data](https://github.com/shibing624/text2vec/blob/master/
+examples/data/STS-B/STS-B.valid.data) ```shell sentence1 sentence2 label
+√§¬∏¬Ä√§¬∏¬™√•¬•¬≥√•¬≠¬©√•¬ú¬®√ß¬ª¬ô√•¬•¬π√ß¬ö¬Ñ√•¬§¬¥√•¬è¬ë√•¬Å¬ö√•¬è¬ë√•¬û¬ã√£¬Ä¬Ç √§¬∏¬Ä√§¬∏¬™√•¬•¬≥√•¬≠¬©√•¬ú¬®√¶¬¢¬≥√•¬§¬¥√£¬Ä¬Ç 2
+√§¬∏¬Ä√ß¬æ¬§√ß¬î¬∑√§¬∫¬∫√•¬ú¬®√¶¬µ¬∑√¶¬ª¬©√§¬∏¬ä√®¬∏¬¢√®¬∂¬≥√ß¬ê¬É√£¬Ä¬Ç √§¬∏¬Ä√ß¬æ¬§√ß¬î¬∑√•¬≠¬©√•¬ú¬®√¶¬µ¬∑√¶¬ª¬©√§¬∏¬ä√®¬∏¬¢√®¬∂¬≥√ß¬ê¬É√£¬Ä¬Ç 3
 √§¬∏¬Ä√§¬∏¬™√•¬•¬≥√§¬∫¬∫√•¬ú¬®√¶¬µ¬ã√©¬á¬è√•¬è¬¶√§¬∏¬Ä√§¬∏¬™√•¬•¬≥√§¬∫¬∫√ß¬ö¬Ñ√®¬Ñ¬ö√®¬∏¬ù√£¬Ä¬Ç
 √•¬•¬≥√§¬∫¬∫√¶¬µ¬ã√©¬á¬è√•¬è¬¶√§¬∏¬Ä√§¬∏¬™√•¬•¬≥√§¬∫¬∫√ß¬ö¬Ñ√®¬Ñ¬ö√®¬∏¬ù√£¬Ä¬Ç 5 ```
 `label`√•¬è¬Ø√§¬ª¬•√¶¬ò¬Ø0√Ø¬º¬å1√¶¬†¬á√ß¬≠¬æ√Ø¬º¬å0√§¬ª¬£√®¬°¬®√§¬∏¬§√§¬∏¬™√•¬è¬•√•¬≠¬ê√§¬∏¬ç√ß¬õ¬∏√§¬º¬º√Ø¬º¬å1√§¬ª¬£√®¬°¬®√ß¬õ¬∏√§¬º¬º√Ø¬º¬õ√§¬π¬ü√•¬è¬Ø√§¬ª¬•√¶¬ò¬Ø0-
 5√ß¬ö¬Ñ√®¬Ø¬Ñ√•¬à¬Ü√Ø¬º¬å√®¬Ø¬Ñ√•¬à¬Ü√®¬∂¬ä√©¬´¬ò√Ø¬º¬å√®¬°¬®√ß¬§¬∫√§¬∏¬§√§¬∏¬™√•¬è¬•√•¬≠¬ê√®¬∂¬ä√ß¬õ¬∏√§¬º¬º√£¬Ä¬Ç√¶¬®¬°√•¬û¬ã√©¬É¬Ω√®¬É¬Ω√¶¬î¬Ø√¶¬å¬Å√£¬Ä¬Ç
 - √•¬ú¬®√®¬ã¬±√¶¬ñ¬áSTS-B√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√®¬Æ¬≠√ß¬ª¬É√•¬í¬å√®¬Ø¬Ñ√§¬º¬∞`CoSENT`√¶¬®¬°√•¬û¬ã example: [examples/
 training_sup_text_matching_model_en.py](https://github.com/shibing624/text2vec/
 blob/master/examples/training_sup_text_matching_model_en.py) ```shell cd
@@ -366,15 +389,15 @@
 output_dir ./outputs/STS-B-en-cosent ``` #### CoSENT √¶¬ó¬†√ß¬õ¬ë√ß¬ù¬£√¶¬®¬°√•¬û¬ã -
 √•¬ú¬®√®¬ã¬±√¶¬ñ¬áNLI√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√®¬Æ¬≠√ß¬ª¬É`CoSENT`√¶¬®¬°√•¬û¬ã√Ø¬º¬å√•¬ú¬®STS-B√¶¬µ¬ã√®¬Ø¬ï√©¬õ¬Ü√®¬Ø¬Ñ√§¬º¬∞√¶¬ï¬à√¶¬û¬ú
 example: [examples/training_unsup_text_matching_model_en.py](https://
 github.com/shibing624/text2vec/blob/master/examples/
 training_unsup_text_matching_model_en.py) ```shell cd examples python
 training_unsup_text_matching_model_en.py --model_arch cosent --do_train --
 do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./
-outputs/STS-B-en-unsup-cosent ``` ## Sentence-BERT model Sentence-
+outputs/STS-B-en-unsup-cosent ``` ### Sentence-BERT model Sentence-
 BERT√¶¬ñ¬á√¶¬ú¬¨√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã√Ø¬º¬å√®¬°¬®√•¬æ¬Å√•¬º¬è√•¬è¬•√•¬ê¬ë√©¬á¬è√®¬°¬®√ß¬§¬∫√¶¬ñ¬π√¶¬°¬à Network structure:
 Training: [docs/sbert_train.png] Inference: [docs/sbert_inference.png] ####
 SentenceBERT √ß¬õ¬ë√ß¬ù¬£√¶¬®¬°√•¬û¬ã - √•¬ú¬®√§¬∏¬≠√¶¬ñ¬áSTS-B√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√®¬Æ¬≠√ß¬ª¬É√•¬í¬å√®¬Ø¬Ñ√§¬º¬∞`SBERT`√¶¬®¬°√•¬û¬ã
 example: [examples/training_sup_text_matching_model.py](https://github.com/
 shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)
 ```shell cd examples python training_sup_text_matching_model.py --model_arch
 sentencebert --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-
@@ -387,35 +410,35 @@
 uncased --output_dir ./outputs/STS-B-en-sbert ``` #### SentenceBERT
 √¶¬ó¬†√ß¬õ¬ë√ß¬ù¬£√¶¬®¬°√•¬û¬ã - √•¬ú¬®√®¬ã¬±√¶¬ñ¬áNLI√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√®¬Æ¬≠√ß¬ª¬É`SBERT`√¶¬®¬°√•¬û¬ã√Ø¬º¬å√•¬ú¬®STS-
 B√¶¬µ¬ã√®¬Ø¬ï√©¬õ¬Ü√®¬Ø¬Ñ√§¬º¬∞√¶¬ï¬à√¶¬û¬ú example: [examples/
 training_unsup_text_matching_model_en.py](https://github.com/shibing624/
 text2vec/blob/master/examples/training_unsup_text_matching_model_en.py)
 ```shell cd examples python training_unsup_text_matching_model_en.py --
 model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name
-bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-sbert ``` ## BERT-Match
-model
+bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-sbert ``` ### BERT-
+Match model
 BERT√¶¬ñ¬á√¶¬ú¬¨√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã√Ø¬º¬å√•¬é¬ü√ß¬î¬üBERT√•¬å¬π√©¬Ö¬ç√ß¬Ω¬ë√ß¬ª¬ú√ß¬ª¬ì√¶¬û¬Ñ√Ø¬º¬å√§¬∫¬§√§¬∫¬í√•¬º¬è√•¬è¬•√•¬ê¬ë√©¬á¬è√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã
 Network structure: Training and inference: [docs/bert-fc-train.png]
 √®¬Æ¬≠√ß¬ª¬É√®¬Ñ¬ö√¶¬ú¬¨√•¬ê¬å√§¬∏¬ä[examples/training_sup_text_matching_model.py](https://
 github.com/shibing624/text2vec/blob/master/examples/
-training_sup_text_matching_model.py)√£¬Ä¬Ç ## √¶¬®¬°√•¬û¬ã√®¬í¬∏√©¬¶¬è√Ø¬º¬àModel Distillation√Ø¬º¬â
-√ß¬î¬±√§¬∫¬étext2vec√®¬Æ¬≠√ß¬ª¬É√ß¬ö¬Ñ√¶¬®¬°√•¬û¬ã√•¬è¬Ø√§¬ª¬•√§¬Ω¬ø√ß¬î¬®[sentence-transformers](https://
-github.com/UKPLab/sentence-
+training_sup_text_matching_model.py)√£¬Ä¬Ç ### √¶¬®¬°√•¬û¬ã√®¬í¬∏√©¬¶¬è√Ø¬º¬àModel
+Distillation√Ø¬º¬â √ß¬î¬±√§¬∫¬étext2vec√®¬Æ¬≠√ß¬ª¬É√ß¬ö¬Ñ√¶¬®¬°√•¬û¬ã√•¬è¬Ø√§¬ª¬•√§¬Ω¬ø√ß¬î¬®[sentence-
+transformers](https://github.com/UKPLab/sentence-
 transformers)√•¬∫¬ì√•¬ä¬†√®¬Ω¬Ω√Ø¬º¬å√¶¬≠¬§√•¬§¬Ñ√•¬§¬ç√ß¬î¬®√•¬Ö¬∂√¶¬®¬°√•¬û¬ã√®¬í¬∏√©¬¶¬è√¶¬ñ¬π√¶¬≥¬ï[distillation](https:
 //github.com/UKPLab/sentence-transformers/tree/master/examples/training/
 distillation)√£¬Ä¬Ç 1. √¶¬®¬°√•¬û¬ã√©¬ô¬ç√ß¬ª¬¥√Ø¬º¬å√•¬è¬Ç√®¬Ä¬É[dimensionality_reduction.py](https://
 github.com/UKPLab/sentence-transformers/blob/master/examples/training/
 distillation/
 dimensionality_reduction.py)√§¬Ω¬ø√ß¬î¬®PCA√•¬Ø¬π√¶¬®¬°√•¬û¬ã√®¬æ¬ì√•¬á¬∫embedding√©¬ô¬ç√ß¬ª¬¥√Ø¬º¬å√•¬è¬Ø√•¬á¬è√•¬∞¬ëmilvus√ß¬≠¬â√•¬ê¬ë√©¬á¬è√¶¬£¬Ä√ß¬¥¬¢√¶¬ï¬∞√¶¬ç¬Æ√•¬∫¬ì√ß¬ö¬Ñ√•¬≠¬ò√•¬Ç¬®√•¬é¬ã√•¬ä¬õ√Ø¬º¬å√®¬ø¬ò√®¬É¬Ω√®¬Ω¬ª√•¬æ¬Æ√¶¬è¬ê√•¬ç¬á√¶¬®¬°√•¬û¬ã√¶¬ï¬à√¶¬û¬ú√£¬Ä¬Ç
 2. √¶¬®¬°√•¬û¬ã√®¬í¬∏√©¬¶¬è√Ø¬º¬å√•¬è¬Ç√®¬Ä¬É[model_distillation.py](https://github.com/UKPLab/
 sentence-transformers/blob/master/examples/training/distillation/
 model_distillation.py)√§¬Ω¬ø√ß¬î¬®√®¬í¬∏√©¬¶¬è√¶¬ñ¬π√¶¬≥¬ï√Ø¬º¬å√•¬∞¬ÜTeacher√•¬§¬ß√¶¬®¬°√•¬û¬ã√®¬í¬∏√©¬¶¬è√•¬à¬∞√¶¬õ¬¥√•¬∞¬ëlayers√•¬±¬Ç√¶¬ï¬∞√ß¬ö¬Ñstudent√¶¬®¬°√•¬û¬ã√§¬∏¬≠√Ø¬º¬å√•¬ú¬®√¶¬ù¬É√®¬°¬°√¶¬ï¬à√¶¬û¬ú√ß¬ö¬Ñ√¶¬É¬Ö√•¬Ü¬µ√§¬∏¬ã√Ø¬º¬å√•¬è¬Ø√•¬§¬ß√•¬π¬Ö√¶¬è¬ê√•¬ç¬á√¶¬®¬°√•¬û¬ã√©¬¢¬Ñ√¶¬µ¬ã√©¬Ä¬ü√•¬∫¬¶√£¬Ä¬Ç
-## √¶¬®¬°√•¬û¬ã√©¬É¬®√ß¬Ω¬≤ √¶¬è¬ê√§¬æ¬õ√§¬∏¬§√ß¬ß¬ç√©¬É¬®√ß¬Ω¬≤√¶¬®¬°√•¬û¬ã√Ø¬º¬å√¶¬ê¬≠√•¬ª¬∫√¶¬ú¬ç√•¬ä¬°√ß¬ö¬Ñ√¶¬ñ¬π√¶¬≥¬ï√Ø¬º¬ö
+### √¶¬®¬°√•¬û¬ã√©¬É¬®√ß¬Ω¬≤ √¶¬è¬ê√§¬æ¬õ√§¬∏¬§√ß¬ß¬ç√©¬É¬®√ß¬Ω¬≤√¶¬®¬°√•¬û¬ã√Ø¬º¬å√¶¬ê¬≠√•¬ª¬∫√¶¬ú¬ç√•¬ä¬°√ß¬ö¬Ñ√¶¬ñ¬π√¶¬≥¬ï√Ø¬º¬ö
 1√Ø¬º¬â√•¬ü¬∫√§¬∫¬éJina√¶¬ê¬≠√•¬ª¬∫gRPC√¶¬ú¬ç√•¬ä¬°√£¬Ä¬ê√¶¬é¬®√®¬ç¬ê√£¬Ä¬ë√Ø¬º¬õ2√Ø¬º¬â√•¬ü¬∫√§¬∫¬éFastAPI√¶¬ê¬≠√•¬ª¬∫√•¬é¬ü√ß¬î¬üHttp√¶¬ú¬ç√•¬ä¬°√£¬Ä¬Ç
-### Jina√¶¬ú¬ç√•¬ä¬° √©¬á¬á√ß¬î¬®C/
+#### Jina√¶¬ú¬ç√•¬ä¬° √©¬á¬á√ß¬î¬®C/
 S√¶¬®¬°√•¬º¬è√¶¬ê¬≠√•¬ª¬∫√©¬´¬ò√¶¬Ä¬ß√®¬É¬Ω√¶¬ú¬ç√•¬ä¬°√Ø¬º¬å√¶¬î¬Ø√¶¬å¬Ådocker√§¬∫¬ë√•¬é¬ü√ß¬î¬ü√Ø¬º¬ågRPC/HTTP/
 WebSocket√Ø¬º¬å√¶¬î¬Ø√¶¬å¬Å√•¬§¬ö√§¬∏¬™√¶¬®¬°√•¬û¬ã√•¬ê¬å√¶¬ó¬∂√©¬¢¬Ñ√¶¬µ¬ã√Ø¬º¬åGPU√•¬§¬ö√•¬ç¬°√•¬§¬Ñ√ß¬ê¬Ü√£¬Ä¬Ç - √•¬Æ¬â√®¬£¬Ö√Ø¬º¬ö
 ```pip install jina``` - √•¬ê¬Ø√•¬ä¬®√¶¬ú¬ç√•¬ä¬°√Ø¬º¬ö example: [examples/
 jina_server_demo.py](examples/jina_server_demo.py) ```python from jina import
 Flow port = 50001 f = Flow(port=port).add( uses='jinahub://Text2vecEncoder',
 uses_with={'model_name': 'shibing624/text2vec-base-chinese'} ) with f: #
 backend server forever f.block() ```
@@ -424,27 +447,27 @@
 √®¬∞¬É√ß¬î¬®√¶¬ú¬ç√•¬ä¬°√Ø¬º¬ö ```python from jina import Client from docarray import
 Document, DocumentArray port = 50001 c = Client(port=port) data =
 ['√•¬¶¬Ç√§¬Ω¬ï√¶¬õ¬¥√¶¬ç¬¢√®¬ä¬±√•¬ë¬ó√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°', '√®¬ä¬±√•¬ë¬ó√¶¬õ¬¥√¶¬î¬π√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°'] print
 ("data:", data) print('data embs:') r = c.post('/', inputs=DocumentArray(
 [Document(text='√•¬¶¬Ç√§¬Ω¬ï√¶¬õ¬¥√¶¬ç¬¢√®¬ä¬±√•¬ë¬ó√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°'), Document
 (text='√®¬ä¬±√•¬ë¬ó√¶¬õ¬¥√¶¬î¬π√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°')])) print(r.embeddings) ```
 √¶¬â¬π√©¬á¬è√®¬∞¬É√ß¬î¬®√¶¬ñ¬π√¶¬≥¬ï√®¬ß¬Åexample: [examples/jina_client_demo.py](https://
-github.com/shibing624/text2vec/blob/master/examples/jina_client_demo.py) ###
+github.com/shibing624/text2vec/blob/master/examples/jina_client_demo.py) ####
 FastAPI√¶¬ú¬ç√•¬ä¬° - √•¬Æ¬â√®¬£¬Ö√Ø¬º¬ö ```pip install fastapi uvicorn``` - √•¬ê¬Ø√•¬ä¬®√¶¬ú¬ç√•¬ä¬°√Ø¬º¬ö
 example: [examples/fastapi_server_demo.py](https://github.com/shibing624/
 text2vec/blob/master/examples/fastapi_server_demo.py) ```shell cd examples
 python fastapi_server_demo.py ``` - √®¬∞¬É√ß¬î¬®√¶¬ú¬ç√•¬ä¬°√Ø¬º¬ö ```shell curl -X 'GET' \
 'http://0.0.0.0:8001/emb?q=hello' \ -H 'accept: application/json' ``` ##
-√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü - √¶¬ú¬¨√©¬°¬π√ß¬õ¬Ærelease√ß¬ö¬Ñ√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬ö | Dataset | Introduce | Download
-Link | |:---------------------------|:-----------------------------------------
---------------------------------|:---------------------------------------------
+Dataset - √¶¬ú¬¨√©¬°¬π√ß¬õ¬Ærelease√ß¬ö¬Ñ√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬ö | Dataset | Introduce | Download Link
+| |:---------------------------|:----------------------------------------------
+---------------------------|:--------------------------------------------------
 -------------------------------------------------------------------------------
 -------------------------------------------------------------------------------
 -------------------------------------------------------------------------------
-------------| | shibing624/nli-zh-all |
+-------| | shibing624/nli-zh-all |
 √§¬∏¬≠√¶¬ñ¬á√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√•¬ê¬à√©¬õ¬Ü√Ø¬º¬å√¶¬ï¬¥√•¬ê¬à√§¬∫¬Ü√¶¬ñ¬á√¶¬ú¬¨√¶¬é¬®√ß¬ê¬Ü√Ø¬º¬å√ß¬õ¬∏√§¬º¬º√Ø¬º¬å√¶¬ë¬ò√®¬¶¬Å√Ø¬º¬å√©¬ó¬Æ√ß¬≠¬î√Ø¬º¬å√¶¬å¬á√§¬ª¬§√•¬æ¬Æ√®¬∞¬É√ß¬≠¬â√§¬ª¬ª√•¬ä¬°√ß¬ö¬Ñ820√§¬∏¬á√©¬´¬ò√®¬¥¬®√©¬á¬è√¶¬ï¬∞√¶¬ç¬Æ√Ø¬º¬å√•¬π¬∂√®¬Ω¬¨√•¬å¬ñ√§¬∏¬∫√•¬å¬π√©¬Ö¬ç√¶¬†¬º√•¬º¬è√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü
 | [https://huggingface.co/datasets/shibing624/nli-zh-all](https://
 huggingface.co/datasets/shibing624/nli-zh-all) | | shibing624/snli-zh |
 √§¬∏¬≠√¶¬ñ¬áSNLI√•¬í¬åMultiNLI√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬å√ß¬ø¬ª√®¬Ø¬ë√®¬á¬™√®¬ã¬±√¶¬ñ¬áSNLI√•¬í¬åMultiNLI | [https://
 huggingface.co/datasets/shibing624/snli-zh](https://huggingface.co/datasets/
 shibing624/snli-zh) | | shibing624/nli_zh |
 √§¬∏¬≠√¶¬ñ¬á√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬å√¶¬ï¬¥√•¬ê¬à√§¬∫¬Ü√§¬∏¬≠√¶¬ñ¬áATEC√£¬Ä¬ÅBQ√£¬Ä¬ÅLCQMC√£¬Ä¬ÅPAWSX√£¬Ä¬ÅSTS-
@@ -462,46 +485,46 @@
 info/1037/1162.htm) | | LCQMC | √§¬∏¬≠√¶¬ñ¬áLCQMC(large-scale Chinese question
 matching corpus)√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬åQ-Qpair√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü | [LCQMC](http://
 icrc.hitsz.edu.cn/Article/show/171.html) | | PAWSX | √§¬∏¬≠√¶¬ñ¬áPAWS(Paraphrase
 Adversaries from Word Scrambling)√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬åQ-Qpair√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü | [PAWSX](https:/
 /arxiv.org/abs/1908.11828) | | STS-B | √§¬∏¬≠√¶¬ñ¬áSTS-
 B√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬å√§¬∏¬≠√¶¬ñ¬á√®¬á¬™√ß¬Ñ¬∂√®¬Ø¬≠√®¬®¬Ä√¶¬é¬®√ß¬ê¬Ü√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬å√§¬ª¬é√®¬ã¬±√¶¬ñ¬áSTS-
 B√ß¬ø¬ª√®¬Ø¬ë√§¬∏¬∫√§¬∏¬≠√¶¬ñ¬á√ß¬ö¬Ñ√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü | [STS-B](https://github.com/pluto-junzeng/CNSD) |
-√•¬∏¬∏√ß¬î¬®√®¬ã¬±√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬ö - √•¬§¬ß√•¬ê¬ç√©¬º¬é√©¬º¬é√ß¬ö¬Ñmulti_nli√•¬í¬åsnli: https://
-huggingface.co/datasets/multi_nli - √•¬§¬ß√•¬ê¬ç√©¬º¬é√©¬º¬é√ß¬ö¬Ñmulti_nli√•¬í¬åsnli: https://
+√•¬∏¬∏√ß¬î¬®√®¬ã¬±√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬ö - √®¬ã¬±√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬ömulti_nli: https://
+huggingface.co/datasets/multi_nli - √®¬ã¬±√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬ösnli: https://
 huggingface.co/datasets/snli - https://huggingface.co/datasets/metaeval/cnli -
 https://huggingface.co/datasets/mteb/stsbenchmark-sts - https://huggingface.co/
 datasets/JeremiahZ/simcse_sup_nli - https://huggingface.co/datasets/
 MoritzLaurer/multilingual-NLI-26lang-2mil7 √¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√§¬Ω¬ø√ß¬î¬®√ß¬§¬∫√§¬æ¬ã√Ø¬º¬ö ```shell
 pip install datasets ``` ```python from datasets import load_dataset dataset =
 load_dataset("shibing624/nli_zh", "STS-B") # ATEC or BQ or LCQMC or PAWSX or
 STS-B print(dataset) print(dataset['test'][0]) ``` output: ```shell DatasetDict
 ({ train: Dataset({ features: ['sentence1', 'sentence2', 'label'], num_rows:
 5231 }) validation: Dataset({ features: ['sentence1', 'sentence2', 'label'],
 num_rows: 1458 }) test: Dataset({ features: ['sentence1', 'sentence2',
 'label'], num_rows: 1361 }) }) {'sentence1':
 '√§¬∏¬Ä√§¬∏¬™√•¬•¬≥√•¬≠¬©√•¬ú¬®√ß¬ª¬ô√•¬•¬π√ß¬ö¬Ñ√•¬§¬¥√•¬è¬ë√•¬Å¬ö√•¬è¬ë√•¬û¬ã√£¬Ä¬Ç', 'sentence2':
-'√§¬∏¬Ä√§¬∏¬™√•¬•¬≥√•¬≠¬©√•¬ú¬®√¶¬¢¬≥√•¬§¬¥√£¬Ä¬Ç', 'label': 2} ``` # Contact - Issue(√•¬ª¬∫√®¬Æ¬Æ)√Ø¬º¬ö[!
+'√§¬∏¬Ä√§¬∏¬™√•¬•¬≥√•¬≠¬©√•¬ú¬®√¶¬¢¬≥√•¬§¬¥√£¬Ä¬Ç', 'label': 2} ``` ## Contact - Issue(√•¬ª¬∫√®¬Æ¬Æ)√Ø¬º¬ö[!
 [GitHub issues](https://img.shields.io/github/issues/shibing624/text2vec.svg)]
 (https://github.com/shibing624/text2vec/issues) - √©¬Ç¬Æ√§¬ª¬∂√¶¬à¬ë√Ø¬º¬öxuming:
 xuming624@qq.com - √•¬æ¬Æ√§¬ø¬°√¶¬à¬ë√Ø¬º¬ö√•¬ä¬†√¶¬à¬ë*√•¬æ¬Æ√§¬ø¬°√•¬è¬∑√Ø¬º¬öxuming624, √•¬§¬á√¶¬≥¬®√Ø¬º¬ö√•¬ß¬ì√•¬ê¬ç-
-√•¬Ö¬¨√•¬è¬∏-NLP* √®¬ø¬õNLP√§¬∫¬§√¶¬µ¬Å√ß¬æ¬§√£¬Ä¬Ç [docs/wechat.jpeg] # Citation
+√•¬Ö¬¨√•¬è¬∏-NLP* √®¬ø¬õNLP√§¬∫¬§√¶¬µ¬Å√ß¬æ¬§√£¬Ä¬Ç [docs/wechat.jpeg] ## Citation
 √•¬¶¬Ç√¶¬û¬ú√§¬Ω¬†√•¬ú¬®√ß¬†¬î√ß¬©¬∂√§¬∏¬≠√§¬Ω¬ø√ß¬î¬®√§¬∫¬Ütext2vec√Ø¬º¬å√®¬Ø¬∑√¶¬å¬â√•¬¶¬Ç√§¬∏¬ã√¶¬†¬º√•¬º¬è√•¬º¬ï√ß¬î¬®√Ø¬º¬ö APA:
 ```latex Xu, M. Text2vec: Text to vector toolkit (Version 1.1.2) [Computer
 software]. https://github.com/shibing624/text2vec ``` BibTeX: ```latex @misc
-{Text2vec, author = {Xu, Ming}, title = {Text2vec: Text to vector toolkit},
-year = {2022}, publisher = {GitHub}, journal = {GitHub repository},
-howpublished = {\url{https://github.com/shibing624/text2vec}}, } ``` # License
+{Text2vec, author = {Ming Xu}, title = {Text2vec: Text to vector toolkit}, year
+= {2023}, publisher = {GitHub}, journal = {GitHub repository}, howpublished =
+{\url{https://github.com/shibing624/text2vec}}, } ``` ## License
 √¶¬é¬à√¶¬ù¬É√•¬ç¬è√®¬Æ¬Æ√§¬∏¬∫ [The Apache License 2.0]
 (LICENSE)√Ø¬º¬å√•¬è¬Ø√•¬Ö¬ç√®¬¥¬π√ß¬î¬®√•¬Å¬ö√•¬ï¬Ü√§¬∏¬ö√ß¬î¬®√©¬Ä¬î√£¬Ä¬Ç√®¬Ø¬∑√•¬ú¬®√§¬∫¬ß√•¬ì¬Å√®¬Ø¬¥√¶¬ò¬é√§¬∏¬≠√©¬ô¬Ñ√•¬ä¬†text2vec√ß¬ö¬Ñ√©¬ì¬æ√¶¬é¬•√•¬í¬å√¶¬é¬à√¶¬ù¬É√•¬ç¬è√®¬Æ¬Æ√£¬Ä¬Ç
-# Contribute
+## Contribute
 √©¬°¬π√ß¬õ¬Æ√§¬ª¬£√ß¬†¬Å√®¬ø¬ò√•¬æ¬à√ß¬≤¬ó√ß¬≥¬ô√Ø¬º¬å√•¬¶¬Ç√¶¬û¬ú√•¬§¬ß√•¬Æ¬∂√•¬Ø¬π√§¬ª¬£√ß¬†¬Å√¶¬ú¬â√¶¬â¬Ä√¶¬î¬π√®¬ø¬õ√Ø¬º¬å√¶¬¨¬¢√®¬ø¬é√¶¬è¬ê√§¬∫¬§√•¬õ¬û√¶¬ú¬¨√©¬°¬π√ß¬õ¬Æ√Ø¬º¬å√•¬ú¬®√¶¬è¬ê√§¬∫¬§√§¬π¬ã√•¬â¬ç√Ø¬º¬å√¶¬≥¬®√¶¬Ñ¬è√§¬ª¬•√§¬∏¬ã√§¬∏¬§√ß¬Ç¬π√Ø¬º¬ö
 - √•¬ú¬®`tests`√¶¬∑¬ª√•¬ä¬†√ß¬õ¬∏√•¬∫¬î√ß¬ö¬Ñ√•¬ç¬ï√•¬Ö¬É√¶¬µ¬ã√®¬Ø¬ï - √§¬Ω¬ø√ß¬î¬®`python -m pytest -
 v`√¶¬ù¬•√®¬ø¬ê√®¬°¬å√¶¬â¬Ä√¶¬ú¬â√•¬ç¬ï√•¬Ö¬É√¶¬µ¬ã√®¬Ø¬ï√Ø¬º¬å√ß¬°¬Æ√§¬ø¬ù√¶¬â¬Ä√¶¬ú¬â√•¬ç¬ï√¶¬µ¬ã√©¬É¬Ω√¶¬ò¬Ø√©¬Ä¬ö√®¬ø¬á√ß¬ö¬Ñ
-√§¬π¬ã√•¬ê¬é√•¬ç¬≥√•¬è¬Ø√¶¬è¬ê√§¬∫¬§PR√£¬Ä¬Ç # Reference -
+√§¬π¬ã√•¬ê¬é√•¬ç¬≥√•¬è¬Ø√¶¬è¬ê√§¬∫¬§PR√£¬Ä¬Ç ## References -
 [√•¬∞¬Ü√•¬è¬•√•¬≠¬ê√®¬°¬®√ß¬§¬∫√§¬∏¬∫√•¬ê¬ë√©¬á¬è√Ø¬º¬à√§¬∏¬ä√Ø¬º¬â√Ø¬º¬ö√¶¬ó¬†√ß¬õ¬ë√ß¬ù¬£√•¬è¬•√•¬≠¬ê√®¬°¬®√ß¬§¬∫√•¬≠¬¶√§¬π¬†√Ø¬º¬àsentence
 embedding√Ø¬º¬â](https://www.cnblogs.com/llhthinker/p/10335164.html) -
 [√•¬∞¬Ü√•¬è¬•√•¬≠¬ê√®¬°¬®√ß¬§¬∫√§¬∏¬∫√•¬ê¬ë√©¬á¬è√Ø¬º¬à√§¬∏¬ã√Ø¬º¬â√Ø¬º¬ö√¶¬ó¬†√ß¬õ¬ë√ß¬ù¬£√•¬è¬•√•¬≠¬ê√®¬°¬®√ß¬§¬∫√•¬≠¬¶√§¬π¬†√Ø¬º¬àsentence
 embedding√Ø¬º¬â](https://www.cnblogs.com/llhthinker/p/10341841.html) - [A Simple
 but Tough-to-Beat Baseline for Sentence Embeddings[Sanjeev Arora and Yingyu
 Liang and Tengyu Ma, 2017]](https://openreview.net/forum?id=SyK00v5xx) -
 [√•¬õ¬õ√ß¬ß¬ç√®¬Æ¬°√ß¬Æ¬ó√¶¬ñ¬á√¶¬ú¬¨√ß¬õ¬∏√§¬º¬º√•¬∫¬¶√ß¬ö¬Ñ√¶¬ñ¬π√¶¬≥¬ï√•¬Ø¬π√¶¬Ø¬î[Yves Peirsman]](https://
```

### Comparing `text2vec-1.2.1/README.md` & `text2vec-1.2.2/README.md`

 * *Files 2% similar despite different names*

```diff
@@ -19,54 +19,57 @@
 
 
 **Text2vec**: Text to Vector, Get Sentence Embeddings. ÊñáÊú¨ÂêëÈáèÂåñÔºåÊääÊñáÊú¨(ÂåÖÊã¨ËØç„ÄÅÂè•Â≠ê„ÄÅÊÆµËêΩ)Ë°®ÂæÅ‰∏∫ÂêëÈáèÁü©Èòµ„ÄÇ
 
 **text2vec**ÂÆûÁé∞‰∫ÜWord2Vec„ÄÅRankBM25„ÄÅBERT„ÄÅSentence-BERT„ÄÅCoSENTÁ≠âÂ§öÁßçÊñáÊú¨Ë°®ÂæÅ„ÄÅÊñáÊú¨Áõ∏‰ººÂ∫¶ËÆ°ÁÆóÊ®°ÂûãÔºåÂπ∂Âú®ÊñáÊú¨ËØ≠‰πâÂåπÈÖçÔºàÁõ∏‰ººÂ∫¶ËÆ°ÁÆóÔºâ‰ªªÂä°‰∏äÊØîËæÉ‰∫ÜÂêÑÊ®°ÂûãÁöÑÊïàÊûú„ÄÇ
 
 ### News
+[2023/06/22] v1.2.2ÁâàÊú¨: ÂèëÂ∏É‰∫ÜÂ§öËØ≠Ë®ÄÂåπÈÖçÊ®°Âûã[shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)ÔºåÁî®CoSENTÊñπÊ≥ïËÆ≠ÁªÉÔºåÂü∫‰∫é`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`Áî®‰∫∫Â∑•ÊåëÈÄâÂêéÁöÑÂ§öËØ≠Ë®ÄSTSÊï∞ÊçÆÈõÜ[shibing624/nli-zh-all/text2vec-base-multilingual-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-multilingual-dataset)ËÆ≠ÁªÉÂæóÂà∞ÔºåÂπ∂Âú®‰∏≠Ëã±ÊñáÊµãËØïÈõÜËØÑ‰º∞Áõ∏ÂØπ‰∫éÂéüÊ®°ÂûãÊïàÊûúÊúâÊèêÂçáÔºåËØ¶ËßÅ[Release-v1.2.2](https://github.com/shibing624/text2vec/releases/tag/1.2.2)
+
 [2023/06/19] v1.2.1ÁâàÊú¨: Êõ¥Êñ∞‰∫Ü‰∏≠ÊñáÂåπÈÖçÊ®°Âûã`shibing624/text2vec-base-chinese-nli`‰∏∫Êñ∞Áâà[shibing624/text2vec-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-sentence)ÔºåÈíàÂØπCoSENTÁöÑlossËÆ°ÁÆóÂØπÊéíÂ∫èÊïèÊÑüÁâπÁÇπÔºå‰∫∫Â∑•ÊåëÈÄâÂπ∂Êï¥ÁêÜÂá∫È´òË¥®ÈáèÁöÑÊúâÁõ∏ÂÖ≥ÊÄßÊéíÂ∫èÁöÑSTSÊï∞ÊçÆÈõÜ[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)ÔºåÂú®ÂêÑËØÑ‰º∞ÈõÜË°®Áé∞Áõ∏ÂØπ‰πãÂâçÊúâÊèêÂçáÔºõÂèëÂ∏É‰∫ÜÈÄÇÁî®‰∫és2pÁöÑ‰∏≠ÊñáÂåπÈÖçÊ®°Âûã[shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase)ÔºåËØ¶ËßÅ[Release-v1.2.1](https://github.com/shibing624/text2vec/releases/tag/1.2.1)
 
 [2023/06/15] v1.2.0ÁâàÊú¨: ÂèëÂ∏É‰∫Ü‰∏≠ÊñáÂåπÈÖçÊ®°Âûã[shibing624/text2vec-base-chinese-nli](https://huggingface.co/shibing624/text2vec-base-chinese-nli)ÔºåÂü∫‰∫é`nghuyong/ernie-3.0-base-zh`Ê®°ÂûãÔºå‰ΩøÁî®‰∫Ü‰∏≠ÊñáNLIÊï∞ÊçÆÈõÜ[shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh)ÂÖ®ÈÉ®ËØ≠ÊñôËÆ≠ÁªÉÁöÑCoSENTÊñáÊú¨ÂåπÈÖçÊ®°ÂûãÔºåÂú®ÂêÑËØÑ‰º∞ÈõÜË°®Áé∞ÊèêÂçáÊòéÊòæÔºåËØ¶ËßÅ[Release-v1.2.0](https://github.com/shibing624/text2vec/releases/tag/1.2.0)
 
 [2022/03/12] v1.1.4ÁâàÊú¨: ÂèëÂ∏É‰∫Ü‰∏≠ÊñáÂåπÈÖçÊ®°Âûã[shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)ÔºåÂü∫‰∫é‰∏≠ÊñáSTSËÆ≠ÁªÉÈõÜËÆ≠ÁªÉÁöÑCoSENTÂåπÈÖçÊ®°Âûã„ÄÇËØ¶ËßÅ[Release-v1.1.4](https://github.com/shibing624/text2vec/releases/tag/1.1.4)
 
 
 **Guide**
-- [Feature](#Feature)
+- [Features](#Features)
 - [Evaluation](#Evaluation)
 - [Install](#install)
 - [Usage](#usage)
 - [Contact](#Contact)
-- [Reference](#reference)
+- [References](#references)
 
 
-# Feature
+## Features
 ### ÊñáÊú¨ÂêëÈáèË°®Á§∫Ê®°Âûã
 - [Word2Vec](https://github.com/shibing624/text2vec/blob/master/text2vec/word2vec.py)ÔºöÈÄöËøáËÖæËÆØAI LabÂºÄÊ∫êÁöÑÂ§ßËßÑÊ®°È´òË¥®Èáè‰∏≠Êñá[ËØçÂêëÈáèÊï∞ÊçÆÔºà800‰∏á‰∏≠ÊñáËØçËΩªÈáèÁâàÔºâ](https://pan.baidu.com/s/1La4U4XNFe8s5BJqxPQpeiQ) (Êñá‰ª∂ÂêçÔºölight_Tencent_AILab_ChineseEmbedding.bin ÂØÜÁ†Å: taweÔºâÂÆûÁé∞ËØçÂêëÈáèÊ£ÄÁ¥¢ÔºåÊú¨È°πÁõÆÂÆûÁé∞‰∫ÜÂè•Â≠êÔºàËØçÂêëÈáèÊ±ÇÂπ≥ÂùáÔºâÁöÑword2vecÂêëÈáèË°®Á§∫
 - [SBERT(Sentence-BERT)](https://github.com/shibing624/text2vec/blob/master/text2vec/sentencebert_model.py)ÔºöÊùÉË°°ÊÄßËÉΩÂíåÊïàÁéáÁöÑÂè•ÂêëÈáèË°®Á§∫Ê®°ÂûãÔºåËÆ≠ÁªÉÊó∂ÈÄöËøáÊúâÁõëÁù£ËÆ≠ÁªÉ‰∏äÂ±ÇÂàÜÁ±ªÂáΩÊï∞ÔºåÊñáÊú¨ÂåπÈÖçÈ¢ÑÊµãÊó∂Áõ¥Êé•Âè•Â≠êÂêëÈáèÂÅö‰ΩôÂº¶ÔºåÊú¨È°πÁõÆÂü∫‰∫éPyTorchÂ§çÁé∞‰∫ÜSentence-BERTÊ®°ÂûãÁöÑËÆ≠ÁªÉÂíåÈ¢ÑÊµã
 - [CoSENT(Cosine Sentence)](https://github.com/shibing624/text2vec/blob/master/text2vec/cosent_model.py)ÔºöCoSENTÊ®°ÂûãÊèêÂá∫‰∫Ü‰∏ÄÁßçÊéíÂ∫èÁöÑÊçüÂ§±ÂáΩÊï∞Ôºå‰ΩøËÆ≠ÁªÉËøáÁ®ãÊõ¥Ë¥¥ËøëÈ¢ÑÊµãÔºåÊ®°ÂûãÊî∂ÊïõÈÄüÂ∫¶ÂíåÊïàÊûúÊØîSentence-BERTÊõ¥Â•ΩÔºåÊú¨È°πÁõÆÂü∫‰∫éPyTorchÂÆûÁé∞‰∫ÜCoSENTÊ®°ÂûãÁöÑËÆ≠ÁªÉÂíåÈ¢ÑÊµã
 
 ËØ¶ÁªÜÊñáÊú¨ÂêëÈáèË°®Á§∫ÊñπÊ≥ïËßÅwiki: [ÊñáÊú¨ÂêëÈáèË°®Á§∫ÊñπÊ≥ï](https://github.com/shibing624/text2vec/wiki/%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95)
-# Evaluation
+## Evaluation
 
 ÊñáÊú¨ÂåπÈÖç
 
 #### Ëã±ÊñáÂåπÈÖçÊï∞ÊçÆÈõÜÁöÑËØÑÊµãÁªìÊûúÔºö
 
 
-| Arch   | BaseModel                                        | Model                            | English-STS-B | 
-|:-------|:------------------------------------------------|:-------------------------------------|:-------------:|
-| GloVe  | glove                                           | Avg_word_embeddings_glove_6B_300d    |     61.77     |
-| BERT   | bert-base-uncased                               | BERT-base-cls                        |     20.29     |
-| BERT   | bert-base-uncased                               | BERT-base-first_last_avg             |     59.04     |
-| BERT   | bert-base-uncased                               | BERT-base-first_last_avg-whiten(NLI) |     63.65     |
-| SBERT  | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-cls                   |     73.65     |
-| SBERT  | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-first_last_avg        |     77.96     |
-| CoSENT | bert-base-uncased                               | CoSENT-base-first_last_avg           |     69.93     |
-| CoSENT | sentence-transformers/bert-base-nli-mean-tokens | CoSENT-base-nli-first_last_avg       |     79.68     |
+| Arch   | BaseModel                                        | Model                                                                                                                | English-STS-B | 
+|:-------|:------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------|:-------------:|
+| GloVe  | glove                                           | Avg_word_embeddings_glove_6B_300d                                                                                    |     61.77     |
+| BERT   | bert-base-uncased                               | BERT-base-cls                                                                                                        |     20.29     |
+| BERT   | bert-base-uncased                               | BERT-base-first_last_avg                                                                                             |     59.04     |
+| BERT   | bert-base-uncased                               | BERT-base-first_last_avg-whiten(NLI)                                                                                 |     63.65     |
+| SBERT  | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-cls                                                                                                   |     73.65     |
+| SBERT  | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-first_last_avg                                                                                        |     77.96     |
+| CoSENT | bert-base-uncased                               | CoSENT-base-first_last_avg                                                                                           |     69.93     |
+| CoSENT | sentence-transformers/bert-base-nli-mean-tokens | CoSENT-base-nli-first_last_avg                                                                                       |     79.68     |
+| CoSENT | sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 | [shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)                |     80.12     |
 
 #### ‰∏≠ÊñáÂåπÈÖçÊï∞ÊçÆÈõÜÁöÑËØÑÊµãÁªìÊûúÔºö
 
 
 | Arch   | BaseModel                    | Model           | ATEC  |  BQ   | LCQMC | PAWSX | STS-B |  Avg  | 
 |:-------|:----------------------------|:--------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|
 | SBERT  | bert-base-chinese           | SBERT-bert-base     | 46.36 | 70.36 | 78.72 | 46.86 | 66.41 | 61.74 |
@@ -75,59 +78,60 @@
 | CoSENT | bert-base-chinese           | CoSENT-bert-base    | 49.74 | 72.38 | 78.69 | 60.00 | 79.27 | 68.01 |
 | CoSENT | hfl/chinese-macbert-base    | CoSENT-macbert-base | 50.39 | 72.93 | 79.17 | 60.86 | 79.30 | 68.53 |
 | CoSENT | hfl/chinese-roberta-wwm-ext | CoSENT-roberta-ext  | 50.81 | 71.45 | 79.31 | 61.56 | 79.96 | 68.61 |
 
 ËØ¥ÊòéÔºö
 - ÁªìÊûúËØÑÊµãÊåáÊ†áÔºöspearmanÁ≥ªÊï∞
 - ‰∏∫ËØÑÊµãÊ®°ÂûãËÉΩÂäõÔºåÁªìÊûúÂùáÂè™Áî®ËØ•Êï∞ÊçÆÈõÜÁöÑtrainËÆ≠ÁªÉÔºåÂú®test‰∏äËØÑ‰º∞ÂæóÂà∞ÁöÑË°®Áé∞ÔºåÊ≤°Áî®Â§ñÈÉ®Êï∞ÊçÆ
+- `SBERT-macbert-base`Ê®°ÂûãÔºåÊòØÁî®SBertÊñπÊ≥ïËÆ≠ÁªÉÔºåËøêË°å[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)‰ª£Á†ÅÂèØËÆ≠ÁªÉÊ®°Âûã
+- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`Ê®°ÂûãÊòØÁî®SBertËÆ≠ÁªÉÔºåÊòØ`paraphrase-MiniLM-L12-v2`Ê®°ÂûãÁöÑÂ§öËØ≠Ë®ÄÁâàÊú¨ÔºåÊîØÊåÅ‰∏≠Êñá„ÄÅËã±ÊñáÁ≠â
 
 
 ### Release Models
 - Êú¨È°πÁõÆreleaseÊ®°ÂûãÁöÑ‰∏≠ÊñáÂåπÈÖçËØÑÊµãÁªìÊûúÔºö
 
-| Arch       | BaseModel                         | Model                                                                                                                                             | ATEC  |  BQ   | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-dc |    Avg    |  QPS  |
-|:-----------|:----------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-------:|:-------:|:---------:|:-----:|
-| Word2Vec   | word2vec                          | [w2v-light-tencent-chinese](https://ai.tencent.com/ailab/nlp/en/download.html)                                                                    | 20.00 | 31.49 | 59.46 | 2.57  | 55.78 |  55.04  |  20.70  |   35.03   | 23769 |
-| SBERT      | xlm-roberta-base                  | [sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) | 18.42 | 38.52 | 63.96 | 10.14 | 78.90 |  63.01  |  52.28  |   46.46   | 3138  |
-| Instructor | hfl/chinese-roberta-wwm-ext       | [moka-ai/m3e-base](https://huggingface.co/moka-ai/m3e-base)                                                                                       | 41.27 | 63.81 | 74.87 | 12.20 | 76.96 |  75.83  |  60.55  |   57.93   | 2980  |
-| CoSENT     | hfl/chinese-macbert-base          | [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)                                                       | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 |  70.27  |  50.42  |   51.61   | 3008  |
-| CoSENT     | hfl/chinese-lert-large            | [GanymedeNil/text2vec-large-chinese](https://huggingface.co/GanymedeNil/text2vec-large-chinese)                                                   | 32.61 | 44.59 | 69.30 | 14.51 | 79.44 |  73.01  |  59.04  |   53.12   | 2092  |
-| CoSENT     | nghuyong/ernie-3.0-base-zh        | [shibing624/text2vec-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-sentence)                                     | 43.37 | 61.43 | 73.48 | 38.90 | 78.25 |  70.60  |  53.08  |   59.87   | 3089  |
-| CoSENT     | nghuyong/ernie-3.0-base-zh        | [shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase)                                 | 44.89 | 63.58 | 74.24 | 40.90 | 78.93 |  76.70  |  63.30  | **63.08** | 3066  |
+| Arch       | BaseModel                                                    | Model                                                                                                                                             | ATEC  |  BQ   | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-dc |    Avg    |  QPS  |
+|:-----------|:-------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-------:|:-------:|:---------:|:-----:|
+| Word2Vec   | word2vec                                                     | [w2v-light-tencent-chinese](https://ai.tencent.com/ailab/nlp/en/download.html)                                                                    | 20.00 | 31.49 | 59.46 | 2.57  | 55.78 |  55.04  |  20.70  |   35.03   | 23769 |
+| SBERT      | xlm-roberta-base                                             | [sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) | 18.42 | 38.52 | 63.96 | 10.14 | 78.90 |  63.01  |  52.28  |   46.46   | 3138  |
+| CoSENT     | hfl/chinese-macbert-base                                     | [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)                                                       | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 |  70.27  |  50.42  |   51.61   | 3008  |
+| CoSENT     | hfl/chinese-lert-large                                       | [GanymedeNil/text2vec-large-chinese](https://huggingface.co/GanymedeNil/text2vec-large-chinese)                                                   | 32.61 | 44.59 | 69.30 | 14.51 | 79.44 |  73.01  |  59.04  |   53.12   | 2092  |
+| CoSENT     | nghuyong/ernie-3.0-base-zh                                   | [shibing624/text2vec-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-sentence)                                     | 43.37 | 61.43 | 73.48 | 38.90 | 78.25 |  70.60  |  53.08  |   59.87   | 3089  |
+| CoSENT     | nghuyong/ernie-3.0-base-zh                                   | [shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase)                                 | 44.89 | 63.58 | 74.24 | 40.90 | 78.93 |  76.70  |  63.30  | **63.08** | 3066  |
+| CoSENT     | sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2  | [shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)                                             | 32.39 | 50.33 | 65.64 | 32.56 | 74.45 |  68.88  |  51.17  |   53.67   | 3138  |
 
 
 ËØ¥ÊòéÔºö
 - ÁªìÊûúËØÑÊµãÊåáÊ†áÔºöspearmanÁ≥ªÊï∞
-- Ê®°ÂûãËÆ≠ÁªÉÂÆûÈ™åÊä•ÂëäÔºö[ÂÆûÈ™åÊä•Âëä](https://github.com/shibing624/text2vec/blob/master/docs/model_report.md)
 - `shibing624/text2vec-base-chinese`Ê®°ÂûãÔºåÊòØÁî®CoSENTÊñπÊ≥ïËÆ≠ÁªÉÔºåÂü∫‰∫é`hfl/chinese-macbert-base`Âú®‰∏≠ÊñáSTS-BÊï∞ÊçÆËÆ≠ÁªÉÂæóÂà∞ÔºåÂπ∂Âú®‰∏≠ÊñáSTS-BÊµãËØïÈõÜËØÑ‰º∞ËææÂà∞ËæÉÂ•ΩÊïàÊûúÔºåËøêË°å[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)‰ª£Á†ÅÂèØËÆ≠ÁªÉÊ®°ÂûãÔºåÊ®°ÂûãÊñá‰ª∂Â∑≤Áªè‰∏ä‰º†HF model hubÔºå‰∏≠ÊñáÈÄöÁî®ËØ≠‰πâÂåπÈÖç‰ªªÂä°Êé®Ëçê‰ΩøÁî®
 - `shibing624/text2vec-base-chinese-sentence`Ê®°ÂûãÔºåÊòØÁî®CoSENTÊñπÊ≥ïËÆ≠ÁªÉÔºåÂü∫‰∫é`nghuyong/ernie-3.0-base-zh`Áî®‰∫∫Â∑•ÊåëÈÄâÂêéÁöÑ‰∏≠ÊñáSTSÊï∞ÊçÆÈõÜ[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)ËÆ≠ÁªÉÂæóÂà∞ÔºåÂπ∂Âú®‰∏≠ÊñáÂêÑNLIÊµãËØïÈõÜËØÑ‰º∞ËææÂà∞ËæÉÂ•ΩÊïàÊûúÔºåËøêË°å[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)‰ª£Á†ÅÂèØËÆ≠ÁªÉÊ®°ÂûãÔºåÊ®°ÂûãÊñá‰ª∂Â∑≤Áªè‰∏ä‰º†HF model hubÔºå‰∏≠Êñás2s(Âè•Â≠êvsÂè•Â≠ê)ËØ≠‰πâÂåπÈÖç‰ªªÂä°Êé®Ëçê‰ΩøÁî®
 - `shibing624/text2vec-base-chinese-paraphrase`Ê®°ÂûãÔºåÊòØÁî®CoSENTÊñπÊ≥ïËÆ≠ÁªÉÔºåÂü∫‰∫é`nghuyong/ernie-3.0-base-zh`Áî®‰∫∫Â∑•ÊåëÈÄâÂêéÁöÑ‰∏≠ÊñáSTSÊï∞ÊçÆÈõÜ[shibing624/nli-zh-all/text2vec-base-chinese-paraphrase-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-paraphrase-dataset)ÔºåÊï∞ÊçÆÈõÜÁõ∏ÂØπ‰∫é[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)Âä†ÂÖ•‰∫Üs2p(sentence to paraphrase)Êï∞ÊçÆÔºåÂº∫Âåñ‰∫ÜÂÖ∂ÈïøÊñáÊú¨ÁöÑË°®ÂæÅËÉΩÂäõÔºåÂπ∂Âú®‰∏≠ÊñáÂêÑNLIÊµãËØïÈõÜËØÑ‰º∞ËææÂà∞SOTAÔºåËøêË°å[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)‰ª£Á†ÅÂèØËÆ≠ÁªÉÊ®°ÂûãÔºåÊ®°ÂûãÊñá‰ª∂Â∑≤Áªè‰∏ä‰º†HF model hubÔºå‰∏≠Êñás2p(Âè•Â≠êvsÊÆµËêΩ)ËØ≠‰πâÂåπÈÖç‰ªªÂä°Êé®Ëçê‰ΩøÁî®
-- `SBERT-macbert-base`Ê®°ÂûãÔºåÊòØÁî®SBERTÊñπÊ≥ïËÆ≠ÁªÉÔºåËøêË°å[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)‰ª£Á†ÅÂèØËÆ≠ÁªÉÊ®°Âûã
-- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`Ê®°ÂûãÊòØÁî®SBERTËÆ≠ÁªÉÔºåÊòØ`paraphrase-MiniLM-L12-v2`Ê®°ÂûãÁöÑÂ§öËØ≠Ë®ÄÁâàÊú¨ÔºåÊîØÊåÅ‰∏≠Êñá„ÄÅËã±ÊñáÁ≠â
+- `shibing624/text2vec-base-multilingual`Ê®°ÂûãÔºåÊòØÁî®CoSENTÊñπÊ≥ïËÆ≠ÁªÉÔºåÂü∫‰∫é`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`Áî®‰∫∫Â∑•ÊåëÈÄâÂêéÁöÑÂ§öËØ≠Ë®ÄSTSÊï∞ÊçÆÈõÜ[shibing624/nli-zh-all/text2vec-base-multilingual-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-multilingual-dataset)ËÆ≠ÁªÉÂæóÂà∞ÔºåÂπ∂Âú®‰∏≠Ëã±ÊñáÊµãËØïÈõÜËØÑ‰º∞Áõ∏ÂØπ‰∫éÂéüÊ®°ÂûãÊïàÊûúÊúâÊèêÂçáÔºåËøêË°å[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)‰ª£Á†ÅÂèØËÆ≠ÁªÉÊ®°ÂûãÔºåÊ®°ÂûãÊñá‰ª∂Â∑≤Áªè‰∏ä‰º†HF model hubÔºåÂ§öËØ≠Ë®ÄËØ≠‰πâÂåπÈÖç‰ªªÂä°Êé®Ëçê‰ΩøÁî®
 - `w2v-light-tencent-chinese`ÊòØËÖæËÆØËØçÂêëÈáèÁöÑWord2VecÊ®°ÂûãÔºåCPUÂä†ËΩΩ‰ΩøÁî®ÔºåÈÄÇÁî®‰∫é‰∏≠ÊñáÂ≠óÈù¢ÂåπÈÖç‰ªªÂä°ÂíåÁº∫Â∞ëÊï∞ÊçÆÁöÑÂÜ∑ÂêØÂä®ÊÉÖÂÜµ
 - ÂêÑÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÂùáÂèØ‰ª•ÈÄöËøátransformersË∞ÉÁî®ÔºåÂ¶ÇMacBERTÊ®°ÂûãÔºö`--model_name hfl/chinese-macbert-base` ÊàñËÄÖrobertaÊ®°ÂûãÔºö`--model_name uer/roberta-medium-wwm-chinese-cluecorpussmall`
 - ‰∏∫ÊµãËØÑÊ®°ÂûãÁöÑÈ≤ÅÊ£íÊÄßÔºåÂä†ÂÖ•‰∫ÜÊú™ËÆ≠ÁªÉËøáÁöÑSOHUÊµãËØïÈõÜÔºåÁî®‰∫éÊµãËØïÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÔºõ‰∏∫ËææÂà∞ÂºÄÁÆ±Âç≥Áî®ÁöÑÂÆûÁî®ÊïàÊûúÔºå‰ΩøÁî®‰∫ÜÊêúÈõÜÂà∞ÁöÑÂêÑ‰∏≠ÊñáÂåπÈÖçÊï∞ÊçÆÈõÜÔºåÊï∞ÊçÆÈõÜ‰πü‰∏ä‰º†Âà∞HF datasets[ÈìæÊé•ËßÅ‰∏ãÊñπ](#Êï∞ÊçÆÈõÜ)
 - ‰∏≠ÊñáÂåπÈÖç‰ªªÂä°ÂÆûÈ™åË°®ÊòéÔºåpoolingÊúÄ‰ºòÊòØ`EncoderType.FIRST_LAST_AVG`Âíå`EncoderType.MEAN`Ôºå‰∏§ËÄÖÈ¢ÑÊµãÊïàÊûúÂ∑ÆÂºÇÂæàÂ∞è
 - ‰∏≠ÊñáÂåπÈÖçËØÑÊµãÁªìÊûúÂ§çÁé∞ÔºåÂèØ‰ª•‰∏ãËΩΩ‰∏≠ÊñáÂåπÈÖçÊï∞ÊçÆÈõÜÂà∞`examples/data`ÔºåËøêË°å[tests/test_model_spearman.py](https://github.com/shibing624/text2vec/blob/master/tests/test_model_spearman.py)‰ª£Á†ÅÂ§çÁé∞ËØÑÊµãÁªìÊûú
 - QPSÁöÑGPUÊµãËØïÁéØÂ¢ÉÊòØTesla V100ÔºåÊòæÂ≠ò32GB
 
-# Demo
+Ê®°ÂûãËÆ≠ÁªÉÂÆûÈ™åÊä•ÂëäÔºö[ÂÆûÈ™åÊä•Âëä](https://github.com/shibing624/text2vec/blob/master/docs/model_report.md)
+## Demo
 
 Official Demo: https://www.mulanai.com/product/short_text_sim/
 
 HuggingFace Demo: https://huggingface.co/spaces/shibing624/text2vec
 
 ![](docs/hf.png)
 
 run example: [examples/gradio_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/gradio_demo.py) to see the demo:
 ```shell
 python examples/gradio_demo.py
 ```
 
-# Install
+## Install
 ```shell
 pip install torch # conda install pytorch
 pip install -U text2vec
 ```
 
 or
 
@@ -136,17 +140,17 @@
 pip install -r requirements.txt
 
 git clone https://github.com/shibing624/text2vec.git
 cd text2vec
 pip install --no-deps .
 ```
 
-# Usage
+## Usage
 
-## ÊñáÊú¨ÂêëÈáèË°®ÂæÅ
+### ÊñáÊú¨ÂêëÈáèË°®ÂæÅ
 
 Âü∫‰∫é`pretrained model`ËÆ°ÁÆóÊñáÊú¨ÂêëÈáèÔºö
 
 ```zsh
 >>> from text2vec import SentenceModel
 >>> m = SentenceModel()
 >>> m.encode("Â¶Ç‰ΩïÊõ¥Êç¢Ëä±ÂëóÁªëÂÆöÈì∂Ë°åÂç°")
@@ -186,16 +190,16 @@
 
 
 if __name__ == "__main__":
     # ‰∏≠ÊñáÂè•ÂêëÈáèÊ®°Âûã(CoSENT)Ôºå‰∏≠ÊñáËØ≠‰πâÂåπÈÖç‰ªªÂä°Êé®ËçêÔºåÊîØÊåÅfine-tuneÁªßÁª≠ËÆ≠ÁªÉ
     t2v_model = SentenceModel("shibing624/text2vec-base-chinese")
     compute_emb(t2v_model)
 
-    # ÊîØÊåÅÂ§öËØ≠Ë®ÄÁöÑÂè•ÂêëÈáèÊ®°ÂûãÔºàSentence-BERTÔºâÔºåËã±ÊñáËØ≠‰πâÂåπÈÖç‰ªªÂä°Êé®ËçêÔºåÊîØÊåÅfine-tuneÁªßÁª≠ËÆ≠ÁªÉ
-    sbert_model = SentenceModel("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
+    # ÊîØÊåÅÂ§öËØ≠Ë®ÄÁöÑÂè•ÂêëÈáèÊ®°ÂûãÔºàCoSENTÔºâÔºåÂ§öËØ≠Ë®ÄÔºàÂåÖÊã¨‰∏≠Ëã±ÊñáÔºâËØ≠‰πâÂåπÈÖç‰ªªÂä°Êé®ËçêÔºåÊîØÊåÅfine-tuneÁªßÁª≠ËÆ≠ÁªÉ
+    sbert_model = SentenceModel("shibing624/text2vec-base-multilingual")
     compute_emb(sbert_model)
 
     # ‰∏≠ÊñáËØçÂêëÈáèÊ®°Âûã(word2vec)Ôºå‰∏≠ÊñáÂ≠óÈù¢ÂåπÈÖç‰ªªÂä°ÂíåÂÜ∑ÂêØÂä®ÈÄÇÁî®
     w2v_model = Word2Vec("w2v-light-tencent-chinese")
     compute_emb(w2v_model)
 
 ```
@@ -212,16 +216,14 @@
 ```
 
 - ËøîÂõûÂÄº`embeddings`ÊòØ`numpy.ndarray`Á±ªÂûãÔºåshape‰∏∫`(sentences_size, model_embedding_size)`Ôºå‰∏â‰∏™Ê®°Âûã‰ªªÈÄâ‰∏ÄÁßçÂç≥ÂèØÔºåÊé®ËçêÁî®Á¨¨‰∏Ä‰∏™„ÄÇ
 - `shibing624/text2vec-base-chinese`Ê®°ÂûãÊòØCoSENTÊñπÊ≥ïÂú®‰∏≠ÊñáSTS-BÊï∞ÊçÆÈõÜËÆ≠ÁªÉÂæóÂà∞ÁöÑÔºåÊ®°ÂûãÂ∑≤Áªè‰∏ä‰º†Âà∞huggingfaceÁöÑ
 Ê®°ÂûãÂ∫ì[shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)Ôºå
 ÊòØ`text2vec.SentenceModel`ÊåáÂÆöÁöÑÈªòËÆ§Ê®°ÂûãÔºåÂèØ‰ª•ÈÄöËøá‰∏äÈù¢Á§∫‰æãË∞ÉÁî®ÔºåÊàñËÄÖÂ¶Ç‰∏ãÊâÄÁ§∫Áî®[transformersÂ∫ì](https://github.com/huggingface/transformers)Ë∞ÉÁî®Ôºå
 Ê®°ÂûãËá™Âä®‰∏ãËΩΩÂà∞Êú¨Êú∫Ë∑ØÂæÑÔºö`~/.cache/huggingface/transformers`
-- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`Ê®°ÂûãÊòØSentence-BERTÁöÑÂ§öËØ≠Ë®ÄÂè•ÂêëÈáèÊ®°ÂûãÔºå
-ÈÄÇÁî®‰∫éÈáä‰πâÔºàparaphraseÔºâËØÜÂà´ÔºåÊñáÊú¨ÂåπÈÖçÔºåÈÄöËøá`text2vec.SentenceModel`Âíå[sentence-transformersÂ∫ì]((https://github.com/UKPLab/sentence-transformers))ÈÉΩÂèØ‰ª•Ë∞ÉÁî®ËØ•Ê®°Âûã
 - `w2v-light-tencent-chinese`ÊòØÈÄöËøágensimÂä†ËΩΩÁöÑWord2VecÊ®°ÂûãÔºå‰ΩøÁî®ËÖæËÆØËØçÂêëÈáè`Tencent_AILab_ChineseEmbedding.tar.gz`ËÆ°ÁÆóÂêÑÂ≠óËØçÁöÑËØçÂêëÈáèÔºåÂè•Â≠êÂêëÈáèÈÄöËøáÂçïËØçËØç
 ÂêëÈáèÂèñÂπ≥ÂùáÂÄºÂæóÂà∞ÔºåÊ®°ÂûãËá™Âä®‰∏ãËΩΩÂà∞Êú¨Êú∫Ë∑ØÂæÑÔºö`~/.text2vec/datasets/light_Tencent_AILab_ChineseEmbedding.bin`
 
 #### Usage (HuggingFace Transformers)
 Without [text2vec](https://github.com/shibing624/text2vec), you can use the model like this: 
 
 First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.
@@ -439,17 +441,17 @@
 from similarities import Similarity
 
 m = Similarity()
 r = m.similarity('Â¶Ç‰ΩïÊõ¥Êç¢Ëä±ÂëóÁªëÂÆöÈì∂Ë°åÂç°', 'Ëä±ÂëóÊõ¥ÊîπÁªëÂÆöÈì∂Ë°åÂç°')
 print(f"similarity score: {float(r)}")  # similarity score: 0.855146050453186
 ```
 
-# Models
+## Models
 
-## CoSENT model
+### CoSENT model
 
 CoSENTÔºàCosine SentenceÔºâÊñáÊú¨ÂåπÈÖçÊ®°ÂûãÔºåÂú®Sentence-BERT‰∏äÊîπËøõ‰∫ÜCosineRankLossÁöÑÂè•ÂêëÈáèÊñπÊ°à
 
 
 Network structure:
 
 Training:
@@ -480,16 +482,22 @@
 python training_sup_text_matching_model.py --task_name ATEC --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base --output_dir ./outputs/ATEC-cosent
 ```
 
 - Âú®Ëá™Êúâ‰∏≠ÊñáÊï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÊ®°Âûã
 
 example: [examples/training_sup_text_matching_model_mydata.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_mydata.py)
 
+ÂçïÂç°ËÆ≠ÁªÉÔºö
+```shell
+CUDA_VISIBLE_DEVICES=0 python training_sup_text_matching_model_mydata.py --do_train --do_predict
+```
+
+Â§öÂç°ËÆ≠ÁªÉÔºö
 ```shell
-python training_sup_text_matching_model_mydata.py --do_train --do_predict
+CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node 2  training_sup_text_matching_model_mydata.py --do_train --do_predict --output_dir outputs/STS-B-text2vec-macbert-v1 --batch_size 64 --fp16 --data_parallel 
 ```
 
 ËÆ≠ÁªÉÈõÜÊ†ºÂºèÂèÇËÄÉ[examples/data/STS-B/STS-B.valid.data](https://github.com/shibing624/text2vec/blob/master/examples/data/STS-B/STS-B.valid.data)
 
 ```shell
 sentence1   sentence2   label
 ‰∏Ä‰∏™Â•≥Â≠©Âú®ÁªôÂ•πÁöÑÂ§¥ÂèëÂÅöÂèëÂûã„ÄÇ	‰∏Ä‰∏™Â•≥Â≠©Âú®Ê¢≥Â§¥„ÄÇ	2
@@ -516,15 +524,15 @@
 
 ```shell
 cd examples
 python training_unsup_text_matching_model_en.py --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-cosent
 ```
 
 
-## Sentence-BERT model
+### Sentence-BERT model
 
 Sentence-BERTÊñáÊú¨ÂåπÈÖçÊ®°ÂûãÔºåË°®ÂæÅÂºèÂè•ÂêëÈáèË°®Á§∫ÊñπÊ°à
 
 Network structure:
 
 Training:
 
@@ -559,38 +567,38 @@
 example: [examples/training_unsup_text_matching_model_en.py](https://github.com/shibing624/text2vec/blob/master/examples/training_unsup_text_matching_model_en.py)
 
 ```shell
 cd examples
 python training_unsup_text_matching_model_en.py --model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-sbert
 ```
 
-## BERT-Match model
+### BERT-Match model
 BERTÊñáÊú¨ÂåπÈÖçÊ®°ÂûãÔºåÂéüÁîüBERTÂåπÈÖçÁΩëÁªúÁªìÊûÑÔºå‰∫§‰∫íÂºèÂè•ÂêëÈáèÂåπÈÖçÊ®°Âûã
 
 Network structure:
 
 Training and inference:
 
 <img src="docs/bert-fc-train.png" width="300" />
 
 ËÆ≠ÁªÉËÑöÊú¨Âêå‰∏ä[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)„ÄÇ
 
 
-## Ê®°ÂûãËí∏È¶èÔºàModel DistillationÔºâ
+### Ê®°ÂûãËí∏È¶èÔºàModel DistillationÔºâ
 
 Áî±‰∫étext2vecËÆ≠ÁªÉÁöÑÊ®°ÂûãÂèØ‰ª•‰ΩøÁî®[sentence-transformers](https://github.com/UKPLab/sentence-transformers)Â∫ìÂä†ËΩΩÔºåÊ≠§Â§ÑÂ§çÁî®ÂÖ∂Ê®°ÂûãËí∏È¶èÊñπÊ≥ï[distillation](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/distillation)„ÄÇ
 
 1. Ê®°ÂûãÈôçÁª¥ÔºåÂèÇËÄÉ[dimensionality_reduction.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/distillation/dimensionality_reduction.py)‰ΩøÁî®PCAÂØπÊ®°ÂûãËæìÂá∫embeddingÈôçÁª¥ÔºåÂèØÂáèÂ∞ëmilvusÁ≠âÂêëÈáèÊ£ÄÁ¥¢Êï∞ÊçÆÂ∫ìÁöÑÂ≠òÂÇ®ÂéãÂäõÔºåËøòËÉΩËΩªÂæÆÊèêÂçáÊ®°ÂûãÊïàÊûú„ÄÇ
 2. Ê®°ÂûãËí∏È¶èÔºåÂèÇËÄÉ[model_distillation.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/distillation/model_distillation.py)‰ΩøÁî®Ëí∏È¶èÊñπÊ≥ïÔºåÂ∞ÜTeacherÂ§ßÊ®°ÂûãËí∏È¶èÂà∞Êõ¥Â∞ëlayersÂ±ÇÊï∞ÁöÑstudentÊ®°Âûã‰∏≠ÔºåÂú®ÊùÉË°°ÊïàÊûúÁöÑÊÉÖÂÜµ‰∏ãÔºåÂèØÂ§ßÂπÖÊèêÂçáÊ®°ÂûãÈ¢ÑÊµãÈÄüÂ∫¶„ÄÇ
 
-## Ê®°ÂûãÈÉ®ÁΩ≤
+### Ê®°ÂûãÈÉ®ÁΩ≤
 
 Êèê‰æõ‰∏§ÁßçÈÉ®ÁΩ≤Ê®°ÂûãÔºåÊê≠Âª∫ÊúçÂä°ÁöÑÊñπÊ≥ïÔºö 1ÔºâÂü∫‰∫éJinaÊê≠Âª∫gRPCÊúçÂä°„ÄêÊé®Ëçê„ÄëÔºõ2ÔºâÂü∫‰∫éFastAPIÊê≠Âª∫ÂéüÁîüHttpÊúçÂä°„ÄÇ
 
-### JinaÊúçÂä°
+#### JinaÊúçÂä°
 ÈááÁî®C/SÊ®°ÂºèÊê≠Âª∫È´òÊÄßËÉΩÊúçÂä°ÔºåÊîØÊåÅdocker‰∫ëÂéüÁîüÔºågRPC/HTTP/WebSocketÔºåÊîØÊåÅÂ§ö‰∏™Ê®°ÂûãÂêåÊó∂È¢ÑÊµãÔºåGPUÂ§öÂç°Â§ÑÁêÜ„ÄÇ
 
 - ÂÆâË£ÖÔºö
 ```pip install jina```
 
 - ÂêØÂä®ÊúçÂä°Ôºö
 
@@ -629,15 +637,15 @@
 r = c.post('/', inputs=DocumentArray([Document(text='Â¶Ç‰ΩïÊõ¥Êç¢Ëä±ÂëóÁªëÂÆöÈì∂Ë°åÂç°'), Document(text='Ëä±ÂëóÊõ¥ÊîπÁªëÂÆöÈì∂Ë°åÂç°')]))
 print(r.embeddings)
 ```
 
 ÊâπÈáèË∞ÉÁî®ÊñπÊ≥ïËßÅexample: [examples/jina_client_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/jina_client_demo.py)
 
 
-### FastAPIÊúçÂä°
+#### FastAPIÊúçÂä°
 
 - ÂÆâË£ÖÔºö
 ```pip install fastapi uvicorn```
 
 - ÂêØÂä®ÊúçÂä°Ôºö
 
 example: [examples/fastapi_server_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/fastapi_server_demo.py)
@@ -650,15 +658,15 @@
 ```shell
 curl -X 'GET' \
   'http://0.0.0.0:8001/emb?q=hello' \
   -H 'accept: application/json'
 ```
 
 
-## Êï∞ÊçÆÈõÜ
+## Dataset
 
 - Êú¨È°πÁõÆreleaseÁöÑÊï∞ÊçÆÈõÜÔºö
 
 | Dataset                    | Introduce                                                                | Download Link                                                                                                                                                                                                                                                                                         |
 |:---------------------------|:-------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | shibing624/nli-zh-all      | ‰∏≠ÊñáËØ≠‰πâÂåπÈÖçÊï∞ÊçÆÂêàÈõÜÔºåÊï¥Âêà‰∫ÜÊñáÊú¨Êé®ÁêÜÔºåÁõ∏‰ººÔºåÊëòË¶ÅÔºåÈóÆÁ≠îÔºåÊåá‰ª§ÂæÆË∞ÉÁ≠â‰ªªÂä°ÁöÑ820‰∏áÈ´òË¥®ÈáèÊï∞ÊçÆÔºåÂπ∂ËΩ¨Âåñ‰∏∫ÂåπÈÖçÊ†ºÂºèÊï∞ÊçÆÈõÜ                | [https://huggingface.co/datasets/shibing624/nli-zh-all](https://huggingface.co/datasets/shibing624/nli-zh-all)                                                                                                                                                                                        |
 | shibing624/snli-zh         | ‰∏≠ÊñáSNLIÂíåMultiNLIÊï∞ÊçÆÈõÜÔºåÁøªËØëËá™Ëã±ÊñáSNLIÂíåMultiNLI                                    | [https://huggingface.co/datasets/shibing624/snli-zh](https://huggingface.co/datasets/shibing624/snli-zh)                                                                                                                                                                                              |
@@ -669,16 +677,16 @@
 | LCQMC                      | ‰∏≠ÊñáLCQMC(large-scale Chinese question matching corpus)Êï∞ÊçÆÈõÜÔºåQ-QpairÊï∞ÊçÆÈõÜ      | [LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html)                                                                                                                                                                                                                                               |
 | PAWSX                      | ‰∏≠ÊñáPAWS(Paraphrase Adversaries from Word Scrambling)Êï∞ÊçÆÈõÜÔºåQ-QpairÊï∞ÊçÆÈõÜ        | [PAWSX](https://arxiv.org/abs/1908.11828)                                                                                                                                                                                                                                                             |
 | STS-B                      | ‰∏≠ÊñáSTS-BÊï∞ÊçÆÈõÜÔºå‰∏≠ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÊé®ÁêÜÊï∞ÊçÆÈõÜÔºå‰ªéËã±ÊñáSTS-BÁøªËØë‰∏∫‰∏≠ÊñáÁöÑÊï∞ÊçÆÈõÜ                                 | [STS-B](https://github.com/pluto-junzeng/CNSD)                                                                                                                                                                                                                                                        |
 
 
 Â∏∏Áî®Ëã±ÊñáÂåπÈÖçÊï∞ÊçÆÈõÜÔºö
 
-- Â§ßÂêçÈºéÈºéÁöÑmulti_nliÂíåsnli: https://huggingface.co/datasets/multi_nli
-- Â§ßÂêçÈºéÈºéÁöÑmulti_nliÂíåsnli: https://huggingface.co/datasets/snli
+- Ëã±ÊñáÂåπÈÖçÊï∞ÊçÆÈõÜÔºömulti_nli: https://huggingface.co/datasets/multi_nli
+- Ëã±ÊñáÂåπÈÖçÊï∞ÊçÆÈõÜÔºösnli: https://huggingface.co/datasets/snli
 - https://huggingface.co/datasets/metaeval/cnli
 - https://huggingface.co/datasets/mteb/stsbenchmark-sts
 - https://huggingface.co/datasets/JeremiahZ/simcse_sup_nli
 - https://huggingface.co/datasets/MoritzLaurer/multilingual-NLI-26lang-2mil7
 
 
 Êï∞ÊçÆÈõÜ‰ΩøÁî®Á§∫‰æãÔºö
@@ -713,59 +721,59 @@
 {'sentence1': '‰∏Ä‰∏™Â•≥Â≠©Âú®ÁªôÂ•πÁöÑÂ§¥ÂèëÂÅöÂèëÂûã„ÄÇ', 'sentence2': '‰∏Ä‰∏™Â•≥Â≠©Âú®Ê¢≥Â§¥„ÄÇ', 'label': 2}
 ```
 
 
 
 
 
-# Contact
+## Contact
 
 - Issue(Âª∫ËÆÆ)Ôºö[![GitHub issues](https://img.shields.io/github/issues/shibing624/text2vec.svg)](https://github.com/shibing624/text2vec/issues)
 - ÈÇÆ‰ª∂ÊàëÔºöxuming: xuming624@qq.com
 - ÂæÆ‰ø°ÊàëÔºöÂä†Êàë*ÂæÆ‰ø°Âè∑Ôºöxuming624, Â§áÊ≥®ÔºöÂßìÂêç-ÂÖ¨Âè∏-NLP* ËøõNLP‰∫§ÊµÅÁæ§„ÄÇ
 
 <img src="docs/wechat.jpeg" width="200" />
 
 
-# Citation
+## Citation
 
 Â¶ÇÊûú‰Ω†Âú®Á†îÁ©∂‰∏≠‰ΩøÁî®‰∫Ütext2vecÔºåËØ∑ÊåâÂ¶Ç‰∏ãÊ†ºÂºèÂºïÁî®Ôºö
 
 APA:
 ```latex
 Xu, M. Text2vec: Text to vector toolkit (Version 1.1.2) [Computer software]. https://github.com/shibing624/text2vec
 ```
 
 BibTeX:
 ```latex
 @misc{Text2vec,
-  author = {Xu, Ming},
+  author = {Ming Xu},
   title = {Text2vec: Text to vector toolkit},
-  year = {2022},
+  year = {2023},
   publisher = {GitHub},
   journal = {GitHub repository},
   howpublished = {\url{https://github.com/shibing624/text2vec}},
 }
 ```
 
-# License
+## License
 
 
 ÊéàÊùÉÂçèËÆÆ‰∏∫ [The Apache License 2.0](LICENSE)ÔºåÂèØÂÖçË¥πÁî®ÂÅöÂïÜ‰∏öÁî®ÈÄî„ÄÇËØ∑Âú®‰∫ßÂìÅËØ¥Êòé‰∏≠ÈôÑÂä†text2vecÁöÑÈìæÊé•ÂíåÊéàÊùÉÂçèËÆÆ„ÄÇ
 
 
-# Contribute
+## Contribute
 È°πÁõÆ‰ª£Á†ÅËøòÂæàÁ≤óÁ≥ôÔºåÂ¶ÇÊûúÂ§ßÂÆ∂ÂØπ‰ª£Á†ÅÊúâÊâÄÊîπËøõÔºåÊ¨¢ËøéÊèê‰∫§ÂõûÊú¨È°πÁõÆÔºåÂú®Êèê‰∫§‰πãÂâçÔºåÊ≥®ÊÑè‰ª•‰∏ã‰∏§ÁÇπÔºö
 
  - Âú®`tests`Ê∑ªÂä†Áõ∏Â∫îÁöÑÂçïÂÖÉÊµãËØï
  - ‰ΩøÁî®`python -m pytest -v`Êù•ËøêË°åÊâÄÊúâÂçïÂÖÉÊµãËØïÔºåÁ°Æ‰øùÊâÄÊúâÂçïÊµãÈÉΩÊòØÈÄöËøáÁöÑ
 
 ‰πãÂêéÂç≥ÂèØÊèê‰∫§PR„ÄÇ
 
-# Reference
+## References
 - [Â∞ÜÂè•Â≠êË°®Á§∫‰∏∫ÂêëÈáèÔºà‰∏äÔºâÔºöÊó†ÁõëÁù£Âè•Â≠êË°®Á§∫Â≠¶‰π†Ôºàsentence embeddingÔºâ](https://www.cnblogs.com/llhthinker/p/10335164.html)
 - [Â∞ÜÂè•Â≠êË°®Á§∫‰∏∫ÂêëÈáèÔºà‰∏ãÔºâÔºöÊó†ÁõëÁù£Âè•Â≠êË°®Á§∫Â≠¶‰π†Ôºàsentence embeddingÔºâ](https://www.cnblogs.com/llhthinker/p/10341841.html)
 - [A Simple but Tough-to-Beat Baseline for Sentence Embeddings[Sanjeev Arora and Yingyu Liang and Tengyu Ma, 2017]](https://openreview.net/forum?id=SyK00v5xx)
 - [ÂõõÁßçËÆ°ÁÆóÊñáÊú¨Áõ∏‰ººÂ∫¶ÁöÑÊñπÊ≥ïÂØπÊØî[Yves Peirsman]](https://zhuanlan.zhihu.com/p/37104535)
 - [Improvements to BM25 and Language Models Examined](http://www.cs.otago.ac.nz/homepages/andrew/papers/2014-2.pdf)
 - [CoSENTÔºöÊØîSentence-BERTÊõ¥ÊúâÊïàÁöÑÂè•ÂêëÈáèÊñπÊ°à](https://kexue.fm/archives/8847)
 - [Ë∞àË∞àÊñáÊú¨ÂåπÈÖçÂíåÂ§öËΩÆÊ£ÄÁ¥¢](https://zhuanlan.zhihu.com/p/111769969)
```

#### html2text {}

```diff
@@ -14,17 +14,26 @@
 shibing624/text2vec.svg)](https://github.com/shibing624/text2vec/issues) [!
 [Wechat Group](http://vlog.sfyc.ltd/wechat_everyday/
 wxgroup_logo.png?imageView2/0/w/60/h/20)](#Contact) **Text2vec**: Text to
 Vector, Get Sentence Embeddings. √¶¬ñ¬á√¶¬ú¬¨√•¬ê¬ë√©¬á¬è√•¬å¬ñ√Ø¬º¬å√¶¬ä¬ä√¶¬ñ¬á√¶¬ú¬¨
 (√•¬å¬Ö√¶¬ã¬¨√®¬Ø¬ç√£¬Ä¬Å√•¬è¬•√•¬≠¬ê√£¬Ä¬Å√¶¬Æ¬µ√®¬ê¬Ω)√®¬°¬®√•¬æ¬Å√§¬∏¬∫√•¬ê¬ë√©¬á¬è√ß¬ü¬©√©¬ò¬µ√£¬Ä¬Ç
 **text2vec**√•¬Æ¬û√ß¬é¬∞√§¬∫¬ÜWord2Vec√£¬Ä¬ÅRankBM25√£¬Ä¬ÅBERT√£¬Ä¬ÅSentence-
 BERT√£¬Ä¬ÅCoSENT√ß¬≠¬â√•¬§¬ö√ß¬ß¬ç√¶¬ñ¬á√¶¬ú¬¨√®¬°¬®√•¬æ¬Å√£¬Ä¬Å√¶¬ñ¬á√¶¬ú¬¨√ß¬õ¬∏√§¬º¬º√•¬∫¬¶√®¬Æ¬°√ß¬Æ¬ó√¶¬®¬°√•¬û¬ã√Ø¬º¬å√•¬π¬∂√•¬ú¬®√¶¬ñ¬á√¶¬ú¬¨√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√Ø¬º¬à√ß¬õ¬∏√§¬º¬º√•¬∫¬¶√®¬Æ¬°√ß¬Æ¬ó√Ø¬º¬â√§¬ª¬ª√•¬ä¬°√§¬∏¬ä√¶¬Ø¬î√®¬æ¬É√§¬∫¬Ü√•¬ê¬Ñ√¶¬®¬°√•¬û¬ã√ß¬ö¬Ñ√¶¬ï¬à√¶¬û¬ú√£¬Ä¬Ç
-### News [2023/06/19] v1.2.1√ß¬â¬à√¶¬ú¬¨: √¶¬õ¬¥√¶¬ñ¬∞√§¬∫¬Ü√§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã`shibing624/
-text2vec-base-chinese-nli`√§¬∏¬∫√¶¬ñ¬∞√ß¬â¬à[shibing624/text2vec-base-chinese-sentence]
-(https://huggingface.co/shibing624/text2vec-base-chinese-
+### News [2023/06/22] v1.2.2√ß¬â¬à√¶¬ú¬¨: √•¬è¬ë√•¬∏¬É√§¬∫¬Ü√•¬§¬ö√®¬Ø¬≠√®¬®¬Ä√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã[shibing624/
+text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-
+multilingual)√Ø¬º¬å√ß¬î¬®CoSENT√¶¬ñ¬π√¶¬≥¬ï√®¬Æ¬≠√ß¬ª¬É√Ø¬º¬å√•¬ü¬∫√§¬∫¬é`sentence-transformers/
+paraphrase-multilingual-MiniLM-L12-
+v2`√ß¬î¬®√§¬∫¬∫√•¬∑¬•√¶¬å¬ë√©¬Ä¬â√•¬ê¬é√ß¬ö¬Ñ√•¬§¬ö√®¬Ø¬≠√®¬®¬ÄSTS√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü[shibing624/nli-zh-all/text2vec-
+base-multilingual-dataset](https://huggingface.co/datasets/shibing624/nli-zh-
+all/tree/main/text2vec-base-multilingual-
+dataset)√®¬Æ¬≠√ß¬ª¬É√•¬æ¬ó√•¬à¬∞√Ø¬º¬å√•¬π¬∂√•¬ú¬®√§¬∏¬≠√®¬ã¬±√¶¬ñ¬á√¶¬µ¬ã√®¬Ø¬ï√©¬õ¬Ü√®¬Ø¬Ñ√§¬º¬∞√ß¬õ¬∏√•¬Ø¬π√§¬∫¬é√•¬é¬ü√¶¬®¬°√•¬û¬ã√¶¬ï¬à√¶¬û¬ú√¶¬ú¬â√¶¬è¬ê√•¬ç¬á√Ø¬º¬å√®¬Ø¬¶√®¬ß¬Å
+[Release-v1.2.2](https://github.com/shibing624/text2vec/releases/tag/1.2.2)
+[2023/06/19] v1.2.1√ß¬â¬à√¶¬ú¬¨: √¶¬õ¬¥√¶¬ñ¬∞√§¬∫¬Ü√§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã`shibing624/text2vec-
+base-chinese-nli`√§¬∏¬∫√¶¬ñ¬∞√ß¬â¬à[shibing624/text2vec-base-chinese-sentence](https://
+huggingface.co/shibing624/text2vec-base-chinese-
 sentence)√Ø¬º¬å√©¬í¬à√•¬Ø¬πCoSENT√ß¬ö¬Ñloss√®¬Æ¬°√ß¬Æ¬ó√•¬Ø¬π√¶¬é¬í√•¬∫¬è√¶¬ï¬è√¶¬Ñ¬ü√ß¬â¬π√ß¬Ç¬π√Ø¬º¬å√§¬∫¬∫√•¬∑¬•√¶¬å¬ë√©¬Ä¬â√•¬π¬∂√¶¬ï¬¥√ß¬ê¬Ü√•¬á¬∫√©¬´¬ò√®¬¥¬®√©¬á¬è√ß¬ö¬Ñ√¶¬ú¬â√ß¬õ¬∏√•¬Ö¬≥√¶¬Ä¬ß√¶¬é¬í√•¬∫¬è√ß¬ö¬ÑSTS√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü
 [shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://
 huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-
 sentence-
 dataset)√Ø¬º¬å√•¬ú¬®√•¬ê¬Ñ√®¬Ø¬Ñ√§¬º¬∞√©¬õ¬Ü√®¬°¬®√ß¬é¬∞√ß¬õ¬∏√•¬Ø¬π√§¬π¬ã√•¬â¬ç√¶¬ú¬â√¶¬è¬ê√•¬ç¬á√Ø¬º¬õ√•¬è¬ë√•¬∏¬É√§¬∫¬Ü√©¬Ä¬Ç√ß¬î¬®√§¬∫¬és2p√ß¬ö¬Ñ√§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã
 [shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/
 shibing624/text2vec-base-chinese-paraphrase)√Ø¬º¬å√®¬Ø¬¶√®¬ß¬Å[Release-v1.2.1](https://
@@ -35,17 +44,17 @@
 huggingface.co/datasets/shibing624/
 nli_zh)√•¬Ö¬®√©¬É¬®√®¬Ø¬≠√¶¬ñ¬ô√®¬Æ¬≠√ß¬ª¬É√ß¬ö¬ÑCoSENT√¶¬ñ¬á√¶¬ú¬¨√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã√Ø¬º¬å√•¬ú¬®√•¬ê¬Ñ√®¬Ø¬Ñ√§¬º¬∞√©¬õ¬Ü√®¬°¬®√ß¬é¬∞√¶¬è¬ê√•¬ç¬á√¶¬ò¬é√¶¬ò¬æ√Ø¬º¬å√®¬Ø¬¶√®¬ß¬Å
 [Release-v1.2.0](https://github.com/shibing624/text2vec/releases/tag/1.2.0)
 [2022/03/12] v1.1.4√ß¬â¬à√¶¬ú¬¨: √•¬è¬ë√•¬∏¬É√§¬∫¬Ü√§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã[shibing624/text2vec-
 base-chinese](https://huggingface.co/shibing624/text2vec-base-
 chinese)√Ø¬º¬å√•¬ü¬∫√§¬∫¬é√§¬∏¬≠√¶¬ñ¬áSTS√®¬Æ¬≠√ß¬ª¬É√©¬õ¬Ü√®¬Æ¬≠√ß¬ª¬É√ß¬ö¬ÑCoSENT√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã√£¬Ä¬Ç√®¬Ø¬¶√®¬ß¬Å
 [Release-v1.1.4](https://github.com/shibing624/text2vec/releases/tag/1.1.4)
-**Guide** - [Feature](#Feature) - [Evaluation](#Evaluation) - [Install]
-(#install) - [Usage](#usage) - [Contact](#Contact) - [Reference](#reference) #
-Feature ### √¶¬ñ¬á√¶¬ú¬¨√•¬ê¬ë√©¬á¬è√®¬°¬®√ß¬§¬∫√¶¬®¬°√•¬û¬ã - [Word2Vec](https://github.com/
+**Guide** - [Features](#Features) - [Evaluation](#Evaluation) - [Install]
+(#install) - [Usage](#usage) - [Contact](#Contact) - [References](#references)
+## Features ### √¶¬ñ¬á√¶¬ú¬¨√•¬ê¬ë√©¬á¬è√®¬°¬®√ß¬§¬∫√¶¬®¬°√•¬û¬ã - [Word2Vec](https://github.com/
 shibing624/text2vec/blob/master/text2vec/word2vec.py)√Ø¬º¬ö√©¬Ä¬ö√®¬ø¬á√®¬Ö¬æ√®¬Æ¬ØAI
 Lab√•¬º¬Ä√¶¬∫¬ê√ß¬ö¬Ñ√•¬§¬ß√®¬ß¬Ñ√¶¬®¬°√©¬´¬ò√®¬¥¬®√©¬á¬è√§¬∏¬≠√¶¬ñ¬á
 [√®¬Ø¬ç√•¬ê¬ë√©¬á¬è√¶¬ï¬∞√¶¬ç¬Æ√Ø¬º¬à800√§¬∏¬á√§¬∏¬≠√¶¬ñ¬á√®¬Ø¬ç√®¬Ω¬ª√©¬á¬è√ß¬â¬à√Ø¬º¬â](https://pan.baidu.com/s/
 1La4U4XNFe8s5BJqxPQpeiQ) (√¶¬ñ¬á√§¬ª¬∂√•¬ê¬ç√Ø¬º¬ölight_Tencent_AILab_ChineseEmbedding.bin
 √•¬Ø¬Ü√ß¬†¬Å:
 tawe√Ø¬º¬â√•¬Æ¬û√ß¬é¬∞√®¬Ø¬ç√•¬ê¬ë√©¬á¬è√¶¬£¬Ä√ß¬¥¬¢√Ø¬º¬å√¶¬ú¬¨√©¬°¬π√ß¬õ¬Æ√•¬Æ¬û√ß¬é¬∞√§¬∫¬Ü√•¬è¬•√•¬≠¬ê√Ø¬º¬à√®¬Ø¬ç√•¬ê¬ë√©¬á¬è√¶¬±¬Ç√•¬π¬≥√•¬ù¬á√Ø¬º¬â√ß¬ö¬Ñword2vec√•¬ê¬ë√©¬á¬è√®¬°¬®√ß¬§¬∫
 - [SBERT(Sentence-BERT)](https://github.com/shibing624/text2vec/blob/master/
@@ -53,67 +62,76 @@
 sentencebert_model.py)√Ø¬º¬ö√¶¬ù¬É√®¬°¬°√¶¬Ä¬ß√®¬É¬Ω√•¬í¬å√¶¬ï¬à√ß¬é¬á√ß¬ö¬Ñ√•¬è¬•√•¬ê¬ë√©¬á¬è√®¬°¬®√ß¬§¬∫√¶¬®¬°√•¬û¬ã√Ø¬º¬å√®¬Æ¬≠√ß¬ª¬É√¶¬ó¬∂√©¬Ä¬ö√®¬ø¬á√¶¬ú¬â√ß¬õ¬ë√ß¬ù¬£√®¬Æ¬≠√ß¬ª¬É√§¬∏¬ä√•¬±¬Ç√•¬à¬Ü√ß¬±¬ª√•¬á¬Ω√¶¬ï¬∞√Ø¬º¬å√¶¬ñ¬á√¶¬ú¬¨√•¬å¬π√©¬Ö¬ç√©¬¢¬Ñ√¶¬µ¬ã√¶¬ó¬∂√ß¬õ¬¥√¶¬é¬•√•¬è¬•√•¬≠¬ê√•¬ê¬ë√©¬á¬è√•¬Å¬ö√§¬Ω¬ô√•¬º¬¶√Ø¬º¬å√¶¬ú¬¨√©¬°¬π√ß¬õ¬Æ√•¬ü¬∫√§¬∫¬éPyTorch√•¬§¬ç√ß¬é¬∞√§¬∫¬ÜSentence-
 BERT√¶¬®¬°√•¬û¬ã√ß¬ö¬Ñ√®¬Æ¬≠√ß¬ª¬É√•¬í¬å√©¬¢¬Ñ√¶¬µ¬ã - [CoSENT(Cosine Sentence)](https://github.com/
 shibing624/text2vec/blob/master/text2vec/
 cosent_model.py)√Ø¬º¬öCoSENT√¶¬®¬°√•¬û¬ã√¶¬è¬ê√•¬á¬∫√§¬∫¬Ü√§¬∏¬Ä√ß¬ß¬ç√¶¬é¬í√•¬∫¬è√ß¬ö¬Ñ√¶¬ç¬ü√•¬§¬±√•¬á¬Ω√¶¬ï¬∞√Ø¬º¬å√§¬Ω¬ø√®¬Æ¬≠√ß¬ª¬É√®¬ø¬á√ß¬®¬ã√¶¬õ¬¥√®¬¥¬¥√®¬ø¬ë√©¬¢¬Ñ√¶¬µ¬ã√Ø¬º¬å√¶¬®¬°√•¬û¬ã√¶¬î¬∂√¶¬ï¬õ√©¬Ä¬ü√•¬∫¬¶√•¬í¬å√¶¬ï¬à√¶¬û¬ú√¶¬Ø¬îSentence-
 BERT√¶¬õ¬¥√•¬•¬Ω√Ø¬º¬å√¶¬ú¬¨√©¬°¬π√ß¬õ¬Æ√•¬ü¬∫√§¬∫¬éPyTorch√•¬Æ¬û√ß¬é¬∞√§¬∫¬ÜCoSENT√¶¬®¬°√•¬û¬ã√ß¬ö¬Ñ√®¬Æ¬≠√ß¬ª¬É√•¬í¬å√©¬¢¬Ñ√¶¬µ¬ã
 √®¬Ø¬¶√ß¬ª¬Ü√¶¬ñ¬á√¶¬ú¬¨√•¬ê¬ë√©¬á¬è√®¬°¬®√ß¬§¬∫√¶¬ñ¬π√¶¬≥¬ï√®¬ß¬Åwiki: [√¶¬ñ¬á√¶¬ú¬¨√•¬ê¬ë√©¬á¬è√®¬°¬®√ß¬§¬∫√¶¬ñ¬π√¶¬≥¬ï](https://
 github.com/shibing624/text2vec/wiki/
-%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95) #
+%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95) ##
 Evaluation √¶¬ñ¬á√¶¬ú¬¨√•¬å¬π√©¬Ö¬ç #### √®¬ã¬±√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√ß¬ö¬Ñ√®¬Ø¬Ñ√¶¬µ¬ã√ß¬ª¬ì√¶¬û¬ú√Ø¬º¬ö | Arch |
 BaseModel | Model | English-STS-B | |:-------|:--------------------------------
-----------------|:-------------------------------------|:-------------:| |
+----------------|:-------------------------------------------------------------
+--------------------------------------------------------|:-------------:| |
 GloVe | glove | Avg_word_embeddings_glove_6B_300d | 61.77 | | BERT | bert-base-
 uncased | BERT-base-cls | 20.29 | | BERT | bert-base-uncased | BERT-base-
 first_last_avg | 59.04 | | BERT | bert-base-uncased | BERT-base-first_last_avg-
 whiten(NLI) | 63.65 | | SBERT | sentence-transformers/bert-base-nli-mean-tokens
 | SBERT-base-nli-cls | 73.65 | | SBERT | sentence-transformers/bert-base-nli-
 mean-tokens | SBERT-base-nli-first_last_avg | 77.96 | | CoSENT | bert-base-
 uncased | CoSENT-base-first_last_avg | 69.93 | | CoSENT | sentence-
 transformers/bert-base-nli-mean-tokens | CoSENT-base-nli-first_last_avg | 79.68
-| #### √§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√ß¬ö¬Ñ√®¬Ø¬Ñ√¶¬µ¬ã√ß¬ª¬ì√¶¬û¬ú√Ø¬º¬ö | Arch | BaseModel | Model |
-ATEC | BQ | LCQMC | PAWSX | STS-B | Avg | |:-------|:--------------------------
---|:--------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:| |
-SBERT | bert-base-chinese | SBERT-bert-base | 46.36 | 70.36 | 78.72 | 46.86 |
-66.41 | 61.74 | | SBERT | hfl/chinese-macbert-base | SBERT-macbert-base | 47.28
-| 68.63 | 79.42 | 55.59 | 64.82 | 63.15 | | SBERT | hfl/chinese-roberta-wwm-ext
-| SBERT-roberta-ext | 48.29 | 69.99 | 79.22 | 44.10 | 72.42 | 62.80 | | CoSENT
-| bert-base-chinese | CoSENT-bert-base | 49.74 | 72.38 | 78.69 | 60.00 | 79.27
-| 68.01 | | CoSENT | hfl/chinese-macbert-base | CoSENT-macbert-base | 50.39 |
-72.93 | 79.17 | 60.86 | 79.30 | 68.53 | | CoSENT | hfl/chinese-roberta-wwm-ext
-| CoSENT-roberta-ext | 50.81 | 71.45 | 79.31 | 61.56 | 79.96 | 68.61 |
-√®¬Ø¬¥√¶¬ò¬é√Ø¬º¬ö - √ß¬ª¬ì√¶¬û¬ú√®¬Ø¬Ñ√¶¬µ¬ã√¶¬å¬á√¶¬†¬á√Ø¬º¬öspearman√ß¬≥¬ª√¶¬ï¬∞ -
+| | CoSENT | sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 |
+[shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/
+text2vec-base-multilingual) | 80.12 | ####
+√§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√ß¬ö¬Ñ√®¬Ø¬Ñ√¶¬µ¬ã√ß¬ª¬ì√¶¬û¬ú√Ø¬º¬ö | Arch | BaseModel | Model | ATEC | BQ
+| LCQMC | PAWSX | STS-B | Avg | |:-------|:----------------------------|:------
+--------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:| | SBERT | bert-
+base-chinese | SBERT-bert-base | 46.36 | 70.36 | 78.72 | 46.86 | 66.41 | 61.74
+| | SBERT | hfl/chinese-macbert-base | SBERT-macbert-base | 47.28 | 68.63 |
+79.42 | 55.59 | 64.82 | 63.15 | | SBERT | hfl/chinese-roberta-wwm-ext | SBERT-
+roberta-ext | 48.29 | 69.99 | 79.22 | 44.10 | 72.42 | 62.80 | | CoSENT | bert-
+base-chinese | CoSENT-bert-base | 49.74 | 72.38 | 78.69 | 60.00 | 79.27 | 68.01
+| | CoSENT | hfl/chinese-macbert-base | CoSENT-macbert-base | 50.39 | 72.93 |
+79.17 | 60.86 | 79.30 | 68.53 | | CoSENT | hfl/chinese-roberta-wwm-ext |
+CoSENT-roberta-ext | 50.81 | 71.45 | 79.31 | 61.56 | 79.96 | 68.61 | √®¬Ø¬¥√¶¬ò¬é√Ø¬º¬ö
+- √ß¬ª¬ì√¶¬û¬ú√®¬Ø¬Ñ√¶¬µ¬ã√¶¬å¬á√¶¬†¬á√Ø¬º¬öspearman√ß¬≥¬ª√¶¬ï¬∞ -
 √§¬∏¬∫√®¬Ø¬Ñ√¶¬µ¬ã√¶¬®¬°√•¬û¬ã√®¬É¬Ω√•¬ä¬õ√Ø¬º¬å√ß¬ª¬ì√¶¬û¬ú√•¬ù¬á√•¬è¬™√ß¬î¬®√®¬Ø¬•√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√ß¬ö¬Ñtrain√®¬Æ¬≠√ß¬ª¬É√Ø¬º¬å√•¬ú¬®test√§¬∏¬ä√®¬Ø¬Ñ√§¬º¬∞√•¬æ¬ó√•¬à¬∞√ß¬ö¬Ñ√®¬°¬®√ß¬é¬∞√Ø¬º¬å√¶¬≤¬°√ß¬î¬®√•¬§¬ñ√©¬É¬®√¶¬ï¬∞√¶¬ç¬Æ
-### Release Models - √¶¬ú¬¨√©¬°¬π√ß¬õ¬Ærelease√¶¬®¬°√•¬û¬ã√ß¬ö¬Ñ√§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√®¬Ø¬Ñ√¶¬µ¬ã√ß¬ª¬ì√¶¬û¬ú√Ø¬º¬ö |
-Arch | BaseModel | Model | ATEC | BQ | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-
-dc | Avg | QPS | |:-----------|:----------------------------------|:-----------
+- `SBERT-macbert-base`√¶¬®¬°√•¬û¬ã√Ø¬º¬å√¶¬ò¬Ø√ß¬î¬®SBert√¶¬ñ¬π√¶¬≥¬ï√®¬Æ¬≠√ß¬ª¬É√Ø¬º¬å√®¬ø¬ê√®¬°¬å[examples/
+training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/
+blob/master/examples/training_sup_text_matching_model.py)√§¬ª¬£√ß¬†¬Å√•¬è¬Ø√®¬Æ¬≠√ß¬ª¬É√¶¬®¬°√•¬û¬ã
+- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-
+v2`√¶¬®¬°√•¬û¬ã√¶¬ò¬Ø√ß¬î¬®SBert√®¬Æ¬≠√ß¬ª¬É√Ø¬º¬å√¶¬ò¬Ø`paraphrase-MiniLM-L12-
+v2`√¶¬®¬°√•¬û¬ã√ß¬ö¬Ñ√•¬§¬ö√®¬Ø¬≠√®¬®¬Ä√ß¬â¬à√¶¬ú¬¨√Ø¬º¬å√¶¬î¬Ø√¶¬å¬Å√§¬∏¬≠√¶¬ñ¬á√£¬Ä¬Å√®¬ã¬±√¶¬ñ¬á√ß¬≠¬â ### Release Models -
+√¶¬ú¬¨√©¬°¬π√ß¬õ¬Ærelease√¶¬®¬°√•¬û¬ã√ß¬ö¬Ñ√§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√®¬Ø¬Ñ√¶¬µ¬ã√ß¬ª¬ì√¶¬û¬ú√Ø¬º¬ö | Arch | BaseModel | Model
+| ATEC | BQ | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-dc | Avg | QPS | |:-------
+----|:-------------------------------------------------------------|:----------
 -------------------------------------------------------------------------------
---------------------------------------------------------|:-----:|:-----:|:----
+---------------------------------------------------------|:-----:|:-----:|:----
 -:|:-----:|:-----:|:-------:|:-------:|:---------:|:-----:| | Word2Vec |
 word2vec | [w2v-light-tencent-chinese](https://ai.tencent.com/ailab/nlp/en/
 download.html) | 20.00 | 31.49 | 59.46 | 2.57 | 55.78 | 55.04 | 20.70 | 35.03 |
 23769 | | SBERT | xlm-roberta-base | [sentence-transformers/paraphrase-
 multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/
 paraphrase-multilingual-MiniLM-L12-v2) | 18.42 | 38.52 | 63.96 | 10.14 | 78.90
-| 63.01 | 52.28 | 46.46 | 3138 | | Instructor | hfl/chinese-roberta-wwm-ext |
-[moka-ai/m3e-base](https://huggingface.co/moka-ai/m3e-base) | 41.27 | 63.81 |
-74.87 | 12.20 | 76.96 | 75.83 | 60.55 | 57.93 | 2980 | | CoSENT | hfl/chinese-
-macbert-base | [shibing624/text2vec-base-chinese](https://huggingface.co/
-shibing624/text2vec-base-chinese) | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 |
-70.27 | 50.42 | 51.61 | 3008 | | CoSENT | hfl/chinese-lert-large |
-[GanymedeNil/text2vec-large-chinese](https://huggingface.co/GanymedeNil/
-text2vec-large-chinese) | 32.61 | 44.59 | 69.30 | 14.51 | 79.44 | 73.01 | 59.04
-| 53.12 | 2092 | | CoSENT | nghuyong/ernie-3.0-base-zh | [shibing624/text2vec-
-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-
-sentence) | 43.37 | 61.43 | 73.48 | 38.90 | 78.25 | 70.60 | 53.08 | 59.87 |
-3089 | | CoSENT | nghuyong/ernie-3.0-base-zh | [shibing624/text2vec-base-
-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-
-paraphrase) | 44.89 | 63.58 | 74.24 | 40.90 | 78.93 | 76.70 | 63.30 | **63.08**
-| 3066 | √®¬Ø¬¥√¶¬ò¬é√Ø¬º¬ö - √ß¬ª¬ì√¶¬û¬ú√®¬Ø¬Ñ√¶¬µ¬ã√¶¬å¬á√¶¬†¬á√Ø¬º¬öspearman√ß¬≥¬ª√¶¬ï¬∞ -
-√¶¬®¬°√•¬û¬ã√®¬Æ¬≠√ß¬ª¬É√•¬Æ¬û√©¬™¬å√¶¬ä¬•√•¬ë¬ä√Ø¬º¬ö[√•¬Æ¬û√©¬™¬å√¶¬ä¬•√•¬ë¬ä](https://github.com/shibing624/
-text2vec/blob/master/docs/model_report.md) - `shibing624/text2vec-base-
+| 63.01 | 52.28 | 46.46 | 3138 | | CoSENT | hfl/chinese-macbert-base |
+[shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-
+base-chinese) | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 | 70.27 | 50.42 | 51.61 |
+3008 | | CoSENT | hfl/chinese-lert-large | [GanymedeNil/text2vec-large-chinese]
+(https://huggingface.co/GanymedeNil/text2vec-large-chinese) | 32.61 | 44.59 |
+69.30 | 14.51 | 79.44 | 73.01 | 59.04 | 53.12 | 2092 | | CoSENT | nghuyong/
+ernie-3.0-base-zh | [shibing624/text2vec-base-chinese-sentence](https://
+huggingface.co/shibing624/text2vec-base-chinese-sentence) | 43.37 | 61.43 |
+73.48 | 38.90 | 78.25 | 70.60 | 53.08 | 59.87 | 3089 | | CoSENT | nghuyong/
+ernie-3.0-base-zh | [shibing624/text2vec-base-chinese-paraphrase](https://
+huggingface.co/shibing624/text2vec-base-chinese-paraphrase) | 44.89 | 63.58 |
+74.24 | 40.90 | 78.93 | 76.70 | 63.30 | **63.08** | 3066 | | CoSENT | sentence-
+transformers/paraphrase-multilingual-MiniLM-L12-v2 | [shibing624/text2vec-base-
+multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual) |
+32.39 | 50.33 | 65.64 | 32.56 | 74.45 | 68.88 | 51.17 | 53.67 | 3138 |
+√®¬Ø¬¥√¶¬ò¬é√Ø¬º¬ö - √ß¬ª¬ì√¶¬û¬ú√®¬Ø¬Ñ√¶¬µ¬ã√¶¬å¬á√¶¬†¬á√Ø¬º¬öspearman√ß¬≥¬ª√¶¬ï¬∞ - `shibing624/text2vec-base-
 chinese`√¶¬®¬°√•¬û¬ã√Ø¬º¬å√¶¬ò¬Ø√ß¬î¬®CoSENT√¶¬ñ¬π√¶¬≥¬ï√®¬Æ¬≠√ß¬ª¬É√Ø¬º¬å√•¬ü¬∫√§¬∫¬é`hfl/chinese-macbert-
 base`√•¬ú¬®√§¬∏¬≠√¶¬ñ¬áSTS-B√¶¬ï¬∞√¶¬ç¬Æ√®¬Æ¬≠√ß¬ª¬É√•¬æ¬ó√•¬à¬∞√Ø¬º¬å√•¬π¬∂√•¬ú¬®√§¬∏¬≠√¶¬ñ¬áSTS-
 B√¶¬µ¬ã√®¬Ø¬ï√©¬õ¬Ü√®¬Ø¬Ñ√§¬º¬∞√®¬æ¬æ√•¬à¬∞√®¬æ¬É√•¬•¬Ω√¶¬ï¬à√¶¬û¬ú√Ø¬º¬å√®¬ø¬ê√®¬°¬å[examples/
 training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/
 blob/master/examples/
 training_sup_text_matching_model.py)√§¬ª¬£√ß¬†¬Å√•¬è¬Ø√®¬Æ¬≠√ß¬ª¬É√¶¬®¬°√•¬û¬ã√Ø¬º¬å√¶¬®¬°√•¬û¬ã√¶¬ñ¬á√§¬ª¬∂√•¬∑¬≤√ß¬ª¬è√§¬∏¬ä√§¬º¬†HF
 model hub√Ø¬º¬å√§¬∏¬≠√¶¬ñ¬á√©¬Ä¬ö√ß¬î¬®√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√¶¬é¬®√®¬ç¬ê√§¬Ω¬ø√ß¬î¬® - `shibing624/text2vec-
@@ -134,81 +152,83 @@
 [shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://
 huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-
 sentence-dataset)√•¬ä¬†√•¬Ö¬•√§¬∫¬Üs2p(sentence to
 paraphrase)√¶¬ï¬∞√¶¬ç¬Æ√Ø¬º¬å√•¬º¬∫√•¬å¬ñ√§¬∫¬Ü√•¬Ö¬∂√©¬ï¬ø√¶¬ñ¬á√¶¬ú¬¨√ß¬ö¬Ñ√®¬°¬®√•¬æ¬Å√®¬É¬Ω√•¬ä¬õ√Ø¬º¬å√•¬π¬∂√•¬ú¬®√§¬∏¬≠√¶¬ñ¬á√•¬ê¬ÑNLI√¶¬µ¬ã√®¬Ø¬ï√©¬õ¬Ü√®¬Ø¬Ñ√§¬º¬∞√®¬æ¬æ√•¬à¬∞SOTA√Ø¬º¬å√®¬ø¬ê√®¬°¬å
 [examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/
 shibing624/text2vec/blob/master/examples/
 training_sup_text_matching_model_jsonl_data.py)√§¬ª¬£√ß¬†¬Å√•¬è¬Ø√®¬Æ¬≠√ß¬ª¬É√¶¬®¬°√•¬û¬ã√Ø¬º¬å√¶¬®¬°√•¬û¬ã√¶¬ñ¬á√§¬ª¬∂√•¬∑¬≤√ß¬ª¬è√§¬∏¬ä√§¬º¬†HF
-model hub√Ø¬º¬å√§¬∏¬≠√¶¬ñ¬ás2p(√•¬è¬•√•¬≠¬êvs√¶¬Æ¬µ√®¬ê¬Ω)√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√¶¬é¬®√®¬ç¬ê√§¬Ω¬ø√ß¬î¬® - `SBERT-
-macbert-base`√¶¬®¬°√•¬û¬ã√Ø¬º¬å√¶¬ò¬Ø√ß¬î¬®SBERT√¶¬ñ¬π√¶¬≥¬ï√®¬Æ¬≠√ß¬ª¬É√Ø¬º¬å√®¬ø¬ê√®¬°¬å[examples/
-training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/
-blob/master/examples/training_sup_text_matching_model.py)√§¬ª¬£√ß¬†¬Å√•¬è¬Ø√®¬Æ¬≠√ß¬ª¬É√¶¬®¬°√•¬û¬ã
-- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-
-v2`√¶¬®¬°√•¬û¬ã√¶¬ò¬Ø√ß¬î¬®SBERT√®¬Æ¬≠√ß¬ª¬É√Ø¬º¬å√¶¬ò¬Ø`paraphrase-MiniLM-L12-
-v2`√¶¬®¬°√•¬û¬ã√ß¬ö¬Ñ√•¬§¬ö√®¬Ø¬≠√®¬®¬Ä√ß¬â¬à√¶¬ú¬¨√Ø¬º¬å√¶¬î¬Ø√¶¬å¬Å√§¬∏¬≠√¶¬ñ¬á√£¬Ä¬Å√®¬ã¬±√¶¬ñ¬á√ß¬≠¬â - `w2v-light-tencent-
+model hub√Ø¬º¬å√§¬∏¬≠√¶¬ñ¬ás2p(√•¬è¬•√•¬≠¬êvs√¶¬Æ¬µ√®¬ê¬Ω)√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√¶¬é¬®√®¬ç¬ê√§¬Ω¬ø√ß¬î¬® -
+`shibing624/text2vec-base-
+multilingual`√¶¬®¬°√•¬û¬ã√Ø¬º¬å√¶¬ò¬Ø√ß¬î¬®CoSENT√¶¬ñ¬π√¶¬≥¬ï√®¬Æ¬≠√ß¬ª¬É√Ø¬º¬å√•¬ü¬∫√§¬∫¬é`sentence-transformers/
+paraphrase-multilingual-MiniLM-L12-
+v2`√ß¬î¬®√§¬∫¬∫√•¬∑¬•√¶¬å¬ë√©¬Ä¬â√•¬ê¬é√ß¬ö¬Ñ√•¬§¬ö√®¬Ø¬≠√®¬®¬ÄSTS√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü[shibing624/nli-zh-all/text2vec-
+base-multilingual-dataset](https://huggingface.co/datasets/shibing624/nli-zh-
+all/tree/main/text2vec-base-multilingual-
+dataset)√®¬Æ¬≠√ß¬ª¬É√•¬æ¬ó√•¬à¬∞√Ø¬º¬å√•¬π¬∂√•¬ú¬®√§¬∏¬≠√®¬ã¬±√¶¬ñ¬á√¶¬µ¬ã√®¬Ø¬ï√©¬õ¬Ü√®¬Ø¬Ñ√§¬º¬∞√ß¬õ¬∏√•¬Ø¬π√§¬∫¬é√•¬é¬ü√¶¬®¬°√•¬û¬ã√¶¬ï¬à√¶¬û¬ú√¶¬ú¬â√¶¬è¬ê√•¬ç¬á√Ø¬º¬å√®¬ø¬ê√®¬°¬å
+[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/
+shibing624/text2vec/blob/master/examples/
+training_sup_text_matching_model_jsonl_data.py)√§¬ª¬£√ß¬†¬Å√•¬è¬Ø√®¬Æ¬≠√ß¬ª¬É√¶¬®¬°√•¬û¬ã√Ø¬º¬å√¶¬®¬°√•¬û¬ã√¶¬ñ¬á√§¬ª¬∂√•¬∑¬≤√ß¬ª¬è√§¬∏¬ä√§¬º¬†HF
+model hub√Ø¬º¬å√•¬§¬ö√®¬Ø¬≠√®¬®¬Ä√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√¶¬é¬®√®¬ç¬ê√§¬Ω¬ø√ß¬î¬® - `w2v-light-tencent-
 chinese`√¶¬ò¬Ø√®¬Ö¬æ√®¬Æ¬Ø√®¬Ø¬ç√•¬ê¬ë√©¬á¬è√ß¬ö¬ÑWord2Vec√¶¬®¬°√•¬û¬ã√Ø¬º¬åCPU√•¬ä¬†√®¬Ω¬Ω√§¬Ω¬ø√ß¬î¬®√Ø¬º¬å√©¬Ä¬Ç√ß¬î¬®√§¬∫¬é√§¬∏¬≠√¶¬ñ¬á√•¬≠¬ó√©¬ù¬¢√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√•¬í¬å√ß¬º¬∫√•¬∞¬ë√¶¬ï¬∞√¶¬ç¬Æ√ß¬ö¬Ñ√•¬Ü¬∑√•¬ê¬Ø√•¬ä¬®√¶¬É¬Ö√•¬Ü¬µ
 - √•¬ê¬Ñ√©¬¢¬Ñ√®¬Æ¬≠√ß¬ª¬É√¶¬®¬°√•¬û¬ã√•¬ù¬á√•¬è¬Ø√§¬ª¬•√©¬Ä¬ö√®¬ø¬átransformers√®¬∞¬É√ß¬î¬®√Ø¬º¬å√•¬¶¬ÇMacBERT√¶¬®¬°√•¬û¬ã√Ø¬º¬ö`--
 model_name hfl/chinese-macbert-base` √¶¬à¬ñ√®¬Ä¬Öroberta√¶¬®¬°√•¬û¬ã√Ø¬º¬ö`--model_name uer/
 roberta-medium-wwm-chinese-cluecorpussmall` -
 √§¬∏¬∫√¶¬µ¬ã√®¬Ø¬Ñ√¶¬®¬°√•¬û¬ã√ß¬ö¬Ñ√©¬≤¬Å√¶¬£¬í√¶¬Ä¬ß√Ø¬º¬å√•¬ä¬†√•¬Ö¬•√§¬∫¬Ü√¶¬ú¬™√®¬Æ¬≠√ß¬ª¬É√®¬ø¬á√ß¬ö¬ÑSOHU√¶¬µ¬ã√®¬Ø¬ï√©¬õ¬Ü√Ø¬º¬å√ß¬î¬®√§¬∫¬é√¶¬µ¬ã√®¬Ø¬ï√¶¬®¬°√•¬û¬ã√ß¬ö¬Ñ√¶¬≥¬õ√•¬å¬ñ√®¬É¬Ω√•¬ä¬õ√Ø¬º¬õ√§¬∏¬∫√®¬æ¬æ√•¬à¬∞√•¬º¬Ä√ß¬Æ¬±√•¬ç¬≥√ß¬î¬®√ß¬ö¬Ñ√•¬Æ¬û√ß¬î¬®√¶¬ï¬à√¶¬û¬ú√Ø¬º¬å√§¬Ω¬ø√ß¬î¬®√§¬∫¬Ü√¶¬ê¬ú√©¬õ¬Ü√•¬à¬∞√ß¬ö¬Ñ√•¬ê¬Ñ√§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬å√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√§¬π¬ü√§¬∏¬ä√§¬º¬†√•¬à¬∞HF
 datasets[√©¬ì¬æ√¶¬é¬•√®¬ß¬Å√§¬∏¬ã√¶¬ñ¬π](#√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü) -
 √§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√•¬Æ¬û√©¬™¬å√®¬°¬®√¶¬ò¬é√Ø¬º¬åpooling√¶¬ú¬Ä√§¬º¬ò√¶¬ò¬Ø`EncoderType.FIRST_LAST_AVG`√•¬í¬å`EncoderType.MEAN`√Ø¬º¬å√§¬∏¬§√®¬Ä¬Ö√©¬¢¬Ñ√¶¬µ¬ã√¶¬ï¬à√¶¬û¬ú√•¬∑¬Æ√•¬º¬Ç√•¬æ¬à√•¬∞¬è
 -
 √§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√®¬Ø¬Ñ√¶¬µ¬ã√ß¬ª¬ì√¶¬û¬ú√•¬§¬ç√ß¬é¬∞√Ø¬º¬å√•¬è¬Ø√§¬ª¬•√§¬∏¬ã√®¬Ω¬Ω√§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√•¬à¬∞`examples/
 data`√Ø¬º¬å√®¬ø¬ê√®¬°¬å[tests/test_model_spearman.py](https://github.com/shibing624/
 text2vec/blob/master/tests/test_model_spearman.py)√§¬ª¬£√ß¬†¬Å√•¬§¬ç√ß¬é¬∞√®¬Ø¬Ñ√¶¬µ¬ã√ß¬ª¬ì√¶¬û¬ú -
-QPS√ß¬ö¬ÑGPU√¶¬µ¬ã√®¬Ø¬ï√ß¬é¬Ø√•¬¢¬É√¶¬ò¬ØTesla V100√Ø¬º¬å√¶¬ò¬æ√•¬≠¬ò32GB # Demo Official Demo: https://
-www.mulanai.com/product/short_text_sim/ HuggingFace Demo: https://
-huggingface.co/spaces/shibing624/text2vec ![](docs/hf.png) run example:
-[examples/gradio_demo.py](https://github.com/shibing624/text2vec/blob/master/
-examples/gradio_demo.py) to see the demo: ```shell python examples/
-gradio_demo.py ``` # Install ```shell pip install torch # conda install pytorch
-pip install -U text2vec ``` or ```shell pip install torch # conda install
-pytorch pip install -r requirements.txt git clone https://github.com/
-shibing624/text2vec.git cd text2vec pip install --no-deps . ``` # Usage ##
-√¶¬ñ¬á√¶¬ú¬¨√•¬ê¬ë√©¬á¬è√®¬°¬®√•¬æ¬Å √•¬ü¬∫√§¬∫¬é`pretrained model`√®¬Æ¬°√ß¬Æ¬ó√¶¬ñ¬á√¶¬ú¬¨√•¬ê¬ë√©¬á¬è√Ø¬º¬ö ```zsh >>>
-from text2vec import SentenceModel >>> m = SentenceModel() >>> m.encode
-("√•¬¶¬Ç√§¬Ω¬ï√¶¬õ¬¥√¶¬ç¬¢√®¬ä¬±√•¬ë¬ó√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°") Embedding shape: (768,) ``` example:
-[examples/computing_embeddings_demo.py](https://github.com/shibing624/text2vec/
-blob/master/examples/computing_embeddings_demo.py) ```python import sys
-sys.path.append('..') from text2vec import SentenceModel from text2vec import
-Word2Vec def compute_emb(model): # Embed a list of sentences sentences =
-[ '√•¬ç¬°', '√©¬ì¬∂√®¬°¬å√•¬ç¬°', '√•¬¶¬Ç√§¬Ω¬ï√¶¬õ¬¥√¶¬ç¬¢√®¬ä¬±√•¬ë¬ó√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°',
-'√®¬ä¬±√•¬ë¬ó√¶¬õ¬¥√¶¬î¬π√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°', 'This framework generates embeddings for each
-input sentence', 'Sentences are passed as a list of string.', 'The quick brown
-fox jumps over the lazy dog.' ] sentence_embeddings = model.encode(sentences)
-print(type(sentence_embeddings), sentence_embeddings.shape) # The result is a
-list of sentence embeddings as numpy arrays for sentence, embedding in zip
-(sentences, sentence_embeddings): print("Sentence:", sentence) print("Embedding
-shape:", embedding.shape) print("Embedding head:", embedding[:10]) print() if
-__name__ == "__main__": # √§¬∏¬≠√¶¬ñ¬á√•¬è¬•√•¬ê¬ë√©¬á¬è√¶¬®¬°√•¬û¬ã
-(CoSENT)√Ø¬º¬å√§¬∏¬≠√¶¬ñ¬á√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√¶¬é¬®√®¬ç¬ê√Ø¬º¬å√¶¬î¬Ø√¶¬å¬Åfine-tune√ß¬ª¬ß√ß¬ª¬≠√®¬Æ¬≠√ß¬ª¬É
-t2v_model = SentenceModel("shibing624/text2vec-base-chinese") compute_emb
-(t2v_model) # √¶¬î¬Ø√¶¬å¬Å√•¬§¬ö√®¬Ø¬≠√®¬®¬Ä√ß¬ö¬Ñ√•¬è¬•√•¬ê¬ë√©¬á¬è√¶¬®¬°√•¬û¬ã√Ø¬º¬àSentence-
-BERT√Ø¬º¬â√Ø¬º¬å√®¬ã¬±√¶¬ñ¬á√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√¶¬é¬®√®¬ç¬ê√Ø¬º¬å√¶¬î¬Ø√¶¬å¬Åfine-tune√ß¬ª¬ß√ß¬ª¬≠√®¬Æ¬≠√ß¬ª¬É
-sbert_model = SentenceModel("sentence-transformers/paraphrase-multilingual-
-MiniLM-L12-v2") compute_emb(sbert_model) # √§¬∏¬≠√¶¬ñ¬á√®¬Ø¬ç√•¬ê¬ë√©¬á¬è√¶¬®¬°√•¬û¬ã
+QPS√ß¬ö¬ÑGPU√¶¬µ¬ã√®¬Ø¬ï√ß¬é¬Ø√•¬¢¬É√¶¬ò¬ØTesla V100√Ø¬º¬å√¶¬ò¬æ√•¬≠¬ò32GB √¶¬®¬°√•¬û¬ã√®¬Æ¬≠√ß¬ª¬É√•¬Æ¬û√©¬™¬å√¶¬ä¬•√•¬ë¬ä√Ø¬º¬ö
+[√•¬Æ¬û√©¬™¬å√¶¬ä¬•√•¬ë¬ä](https://github.com/shibing624/text2vec/blob/master/docs/
+model_report.md) ## Demo Official Demo: https://www.mulanai.com/product/
+short_text_sim/ HuggingFace Demo: https://huggingface.co/spaces/shibing624/
+text2vec ![](docs/hf.png) run example: [examples/gradio_demo.py](https://
+github.com/shibing624/text2vec/blob/master/examples/gradio_demo.py) to see the
+demo: ```shell python examples/gradio_demo.py ``` ## Install ```shell pip
+install torch # conda install pytorch pip install -U text2vec ``` or ```shell
+pip install torch # conda install pytorch pip install -r requirements.txt git
+clone https://github.com/shibing624/text2vec.git cd text2vec pip install --no-
+deps . ``` ## Usage ### √¶¬ñ¬á√¶¬ú¬¨√•¬ê¬ë√©¬á¬è√®¬°¬®√•¬æ¬Å √•¬ü¬∫√§¬∫¬é`pretrained
+model`√®¬Æ¬°√ß¬Æ¬ó√¶¬ñ¬á√¶¬ú¬¨√•¬ê¬ë√©¬á¬è√Ø¬º¬ö ```zsh >>> from text2vec import SentenceModel >>> m
+= SentenceModel() >>> m.encode("√•¬¶¬Ç√§¬Ω¬ï√¶¬õ¬¥√¶¬ç¬¢√®¬ä¬±√•¬ë¬ó√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°") Embedding
+shape: (768,) ``` example: [examples/computing_embeddings_demo.py](https://
+github.com/shibing624/text2vec/blob/master/examples/
+computing_embeddings_demo.py) ```python import sys sys.path.append('..') from
+text2vec import SentenceModel from text2vec import Word2Vec def compute_emb
+(model): # Embed a list of sentences sentences = [ '√•¬ç¬°', '√©¬ì¬∂√®¬°¬å√•¬ç¬°',
+'√•¬¶¬Ç√§¬Ω¬ï√¶¬õ¬¥√¶¬ç¬¢√®¬ä¬±√•¬ë¬ó√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°', '√®¬ä¬±√•¬ë¬ó√¶¬õ¬¥√¶¬î¬π√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°', 'This
+framework generates embeddings for each input sentence', 'Sentences are passed
+as a list of string.', 'The quick brown fox jumps over the lazy dog.' ]
+sentence_embeddings = model.encode(sentences) print(type(sentence_embeddings),
+sentence_embeddings.shape) # The result is a list of sentence embeddings as
+numpy arrays for sentence, embedding in zip(sentences, sentence_embeddings):
+print("Sentence:", sentence) print("Embedding shape:", embedding.shape) print
+("Embedding head:", embedding[:10]) print() if __name__ == "__main__": #
+√§¬∏¬≠√¶¬ñ¬á√•¬è¬•√•¬ê¬ë√©¬á¬è√¶¬®¬°√•¬û¬ã(CoSENT)√Ø¬º¬å√§¬∏¬≠√¶¬ñ¬á√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√¶¬é¬®√®¬ç¬ê√Ø¬º¬å√¶¬î¬Ø√¶¬å¬Åfine-
+tune√ß¬ª¬ß√ß¬ª¬≠√®¬Æ¬≠√ß¬ª¬É t2v_model = SentenceModel("shibing624/text2vec-base-chinese")
+compute_emb(t2v_model) #
+√¶¬î¬Ø√¶¬å¬Å√•¬§¬ö√®¬Ø¬≠√®¬®¬Ä√ß¬ö¬Ñ√•¬è¬•√•¬ê¬ë√©¬á¬è√¶¬®¬°√•¬û¬ã√Ø¬º¬àCoSENT√Ø¬º¬â√Ø¬º¬å√•¬§¬ö√®¬Ø¬≠√®¬®¬Ä√Ø¬º¬à√•¬å¬Ö√¶¬ã¬¨√§¬∏¬≠√®¬ã¬±√¶¬ñ¬á√Ø¬º¬â√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√¶¬é¬®√®¬ç¬ê√Ø¬º¬å√¶¬î¬Ø√¶¬å¬Åfine-
+tune√ß¬ª¬ß√ß¬ª¬≠√®¬Æ¬≠√ß¬ª¬É sbert_model = SentenceModel("shibing624/text2vec-base-
+multilingual") compute_emb(sbert_model) # √§¬∏¬≠√¶¬ñ¬á√®¬Ø¬ç√•¬ê¬ë√©¬á¬è√¶¬®¬°√•¬û¬ã
 (word2vec)√Ø¬º¬å√§¬∏¬≠√¶¬ñ¬á√•¬≠¬ó√©¬ù¬¢√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√•¬í¬å√•¬Ü¬∑√•¬ê¬Ø√•¬ä¬®√©¬Ä¬Ç√ß¬î¬® w2v_model = Word2Vec
 ("w2v-light-tencent-chinese") compute_emb(w2v_model) ``` output: ```
 numpy.ndarray'> (7, 768) Sentence: √•¬ç¬° Embedding shape: (768,) Sentence:
 √©¬ì¬∂√®¬°¬å√•¬ç¬° Embedding shape: (768,) ... ``` -
 √®¬ø¬î√•¬õ¬û√•¬Ä¬º`embeddings`√¶¬ò¬Ø`numpy.ndarray`√ß¬±¬ª√•¬û¬ã√Ø¬º¬åshape√§¬∏¬∫`(sentences_size,
 model_embedding_size)`√Ø¬º¬å√§¬∏¬â√§¬∏¬™√¶¬®¬°√•¬û¬ã√§¬ª¬ª√©¬Ä¬â√§¬∏¬Ä√ß¬ß¬ç√•¬ç¬≥√•¬è¬Ø√Ø¬º¬å√¶¬é¬®√®¬ç¬ê√ß¬î¬®√ß¬¨¬¨√§¬∏¬Ä√§¬∏¬™√£¬Ä¬Ç
 - `shibing624/text2vec-base-chinese`√¶¬®¬°√•¬û¬ã√¶¬ò¬ØCoSENT√¶¬ñ¬π√¶¬≥¬ï√•¬ú¬®√§¬∏¬≠√¶¬ñ¬áSTS-
 B√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√®¬Æ¬≠√ß¬ª¬É√•¬æ¬ó√•¬à¬∞√ß¬ö¬Ñ√Ø¬º¬å√¶¬®¬°√•¬û¬ã√•¬∑¬≤√ß¬ª¬è√§¬∏¬ä√§¬º¬†√•¬à¬∞huggingface√ß¬ö¬Ñ √¶¬®¬°√•¬û¬ã√•¬∫¬ì
 [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-
 base-chinese)√Ø¬º¬å
 √¶¬ò¬Ø`text2vec.SentenceModel`√¶¬å¬á√•¬Æ¬ö√ß¬ö¬Ñ√©¬ª¬ò√®¬Æ¬§√¶¬®¬°√•¬û¬ã√Ø¬º¬å√•¬è¬Ø√§¬ª¬•√©¬Ä¬ö√®¬ø¬á√§¬∏¬ä√©¬ù¬¢√ß¬§¬∫√§¬æ¬ã√®¬∞¬É√ß¬î¬®√Ø¬º¬å√¶¬à¬ñ√®¬Ä¬Ö√•¬¶¬Ç√§¬∏¬ã√¶¬â¬Ä√ß¬§¬∫√ß¬î¬®
 [transformers√•¬∫¬ì](https://github.com/huggingface/transformers)√®¬∞¬É√ß¬î¬®√Ø¬º¬å
-√¶¬®¬°√•¬û¬ã√®¬á¬™√•¬ä¬®√§¬∏¬ã√®¬Ω¬Ω√•¬à¬∞√¶¬ú¬¨√¶¬ú¬∫√®¬∑¬Ø√•¬æ¬Ñ√Ø¬º¬ö`~/.cache/huggingface/transformers` -
-`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`√¶¬®¬°√•¬û¬ã√¶¬ò¬ØSentence-
-BERT√ß¬ö¬Ñ√•¬§¬ö√®¬Ø¬≠√®¬®¬Ä√•¬è¬•√•¬ê¬ë√©¬á¬è√¶¬®¬°√•¬û¬ã√Ø¬º¬å
-√©¬Ä¬Ç√ß¬î¬®√§¬∫¬é√©¬á¬ä√§¬π¬â√Ø¬º¬àparaphrase√Ø¬º¬â√®¬Ø¬Ü√•¬à¬´√Ø¬º¬å√¶¬ñ¬á√¶¬ú¬¨√•¬å¬π√©¬Ö¬ç√Ø¬º¬å√©¬Ä¬ö√®¬ø¬á`text2vec.SentenceModel`√•¬í¬å
-[sentence-transformers√•¬∫¬ì]((https://github.com/UKPLab/sentence-
-transformers))√©¬É¬Ω√•¬è¬Ø√§¬ª¬•√®¬∞¬É√ß¬î¬®√®¬Ø¬•√¶¬®¬°√•¬û¬ã - `w2v-light-tencent-
+√¶¬®¬°√•¬û¬ã√®¬á¬™√•¬ä¬®√§¬∏¬ã√®¬Ω¬Ω√•¬à¬∞√¶¬ú¬¨√¶¬ú¬∫√®¬∑¬Ø√•¬æ¬Ñ√Ø¬º¬ö`~/.cache/huggingface/transformers` - `w2v-
+light-tencent-
 chinese`√¶¬ò¬Ø√©¬Ä¬ö√®¬ø¬ágensim√•¬ä¬†√®¬Ω¬Ω√ß¬ö¬ÑWord2Vec√¶¬®¬°√•¬û¬ã√Ø¬º¬å√§¬Ω¬ø√ß¬î¬®√®¬Ö¬æ√®¬Æ¬Ø√®¬Ø¬ç√•¬ê¬ë√©¬á¬è`Tencent_AILab_ChineseEmbedding.tar.gz`√®¬Æ¬°√ß¬Æ¬ó√•¬ê¬Ñ√•¬≠¬ó√®¬Ø¬ç√ß¬ö¬Ñ√®¬Ø¬ç√•¬ê¬ë√©¬á¬è√Ø¬º¬å√•¬è¬•√•¬≠¬ê√•¬ê¬ë√©¬á¬è√©¬Ä¬ö√®¬ø¬á√•¬ç¬ï√®¬Ø¬ç√®¬Ø¬ç
 √•¬ê¬ë√©¬á¬è√•¬è¬ñ√•¬π¬≥√•¬ù¬á√•¬Ä¬º√•¬æ¬ó√•¬à¬∞√Ø¬º¬å√¶¬®¬°√•¬û¬ã√®¬á¬™√•¬ä¬®√§¬∏¬ã√®¬Ω¬Ω√•¬à¬∞√¶¬ú¬¨√¶¬ú¬∫√®¬∑¬Ø√•¬æ¬Ñ√Ø¬º¬ö`~/.text2vec/
 datasets/light_Tencent_AILab_ChineseEmbedding.bin` #### Usage (HuggingFace
 Transformers) Without [text2vec](https://github.com/shibing624/text2vec), you
 can use the model like this: First, you pass your input through the transformer
 model, then you have to apply the right pooling-operation on-top of the
 contextualized word embeddings. example: [examples/
@@ -319,16 +339,16 @@
 √¶¬ñ¬á√¶¬ú¬¨√ß¬õ¬∏√§¬º¬º√•¬∫¬¶√®¬Æ¬°√ß¬Æ¬ó√•¬í¬å√¶¬ñ¬á√¶¬ú¬¨√•¬å¬π√©¬Ö¬ç√¶¬ê¬ú√ß¬¥¬¢√§¬ª¬ª√•¬ä¬°√Ø¬º¬å√¶¬é¬®√®¬ç¬ê√§¬Ω¬ø√ß¬î¬®
 [similarities√•¬∫¬ì](https://github.com/shibing624/similarities)
 √Ø¬º¬å√•¬Ö¬º√•¬Æ¬π√¶¬ú¬¨√©¬°¬π√ß¬õ¬Ærelease√ß¬ö¬Ñ
 Word2vec√£¬Ä¬ÅSBERT√£¬Ä¬ÅCosent√ß¬±¬ª√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã√Ø¬º¬å√®¬ø¬ò√¶¬î¬Ø√¶¬å¬Å√•¬≠¬ó√©¬ù¬¢√ß¬ª¬¥√•¬∫¬¶√ß¬õ¬∏√§¬º¬º√•¬∫¬¶√®¬Æ¬°√ß¬Æ¬ó√£¬Ä¬Å√•¬å¬π√©¬Ö¬ç√¶¬ê¬ú√ß¬¥¬¢√ß¬Æ¬ó√¶¬≥¬ï√Ø¬º¬å√¶¬î¬Ø√¶¬å¬Å√¶¬ñ¬á√¶¬ú¬¨√£¬Ä¬Å√•¬õ¬æ√•¬É¬è√£¬Ä¬Ç
 √•¬Æ¬â√®¬£¬Ö√Ø¬º¬ö ```pip install -U similarities``` √•¬è¬•√•¬≠¬ê√ß¬õ¬∏√§¬º¬º√•¬∫¬¶√®¬Æ¬°√ß¬Æ¬ó√Ø¬º¬ö ```python
 from similarities import Similarity m = Similarity() r = m.similarity
 ('√•¬¶¬Ç√§¬Ω¬ï√¶¬õ¬¥√¶¬ç¬¢√®¬ä¬±√•¬ë¬ó√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°', '√®¬ä¬±√•¬ë¬ó√¶¬õ¬¥√¶¬î¬π√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°') print
-(f"similarity score: {float(r)}") # similarity score: 0.855146050453186 ``` #
-Models ## CoSENT model CoSENT√Ø¬º¬àCosine
+(f"similarity score: {float(r)}") # similarity score: 0.855146050453186 ``` ##
+Models ### CoSENT model CoSENT√Ø¬º¬àCosine
 Sentence√Ø¬º¬â√¶¬ñ¬á√¶¬ú¬¨√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã√Ø¬º¬å√•¬ú¬®Sentence-
 BERT√§¬∏¬ä√¶¬î¬π√®¬ø¬õ√§¬∫¬ÜCosineRankLoss√ß¬ö¬Ñ√•¬è¬•√•¬ê¬ë√©¬á¬è√¶¬ñ¬π√¶¬°¬à Network structure: Training:
 [docs/cosent_train.png] Inference: [docs/inference.png] #### CoSENT
 √ß¬õ¬ë√ß¬ù¬£√¶¬®¬°√•¬û¬ã √®¬Æ¬≠√ß¬ª¬É√•¬í¬å√©¬¢¬Ñ√¶¬µ¬ãCoSENT√¶¬®¬°√•¬û¬ã√Ø¬º¬ö - √•¬ú¬®√§¬∏¬≠√¶¬ñ¬áSTS-
 B√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√®¬Æ¬≠√ß¬ª¬É√•¬í¬å√®¬Ø¬Ñ√§¬º¬∞`CoSENT`√¶¬®¬°√•¬û¬ã example: [examples/
 training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/
 blob/master/examples/training_sup_text_matching_model.py) ```shell cd examples
@@ -340,21 +360,24 @@
 'PAWSX'√Ø¬º¬å√•¬Ö¬∑√§¬Ω¬ì√•¬è¬Ç√®¬Ä¬ÉHuggingFace datasets [https://huggingface.co/datasets/
 shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh) ```shell
 python training_sup_text_matching_model.py --task_name ATEC --model_arch cosent
 --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base -
 -output_dir ./outputs/ATEC-cosent ``` - √•¬ú¬®√®¬á¬™√¶¬ú¬â√§¬∏¬≠√¶¬ñ¬á√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√§¬∏¬ä√®¬Æ¬≠√ß¬ª¬É√¶¬®¬°√•¬û¬ã
 example: [examples/training_sup_text_matching_model_mydata.py](https://
 github.com/shibing624/text2vec/blob/master/examples/
-training_sup_text_matching_model_mydata.py) ```shell python
-training_sup_text_matching_model_mydata.py --do_train --do_predict ```
-√®¬Æ¬≠√ß¬ª¬É√©¬õ¬Ü√¶¬†¬º√•¬º¬è√•¬è¬Ç√®¬Ä¬É[examples/data/STS-B/STS-B.valid.data](https://github.com/
-shibing624/text2vec/blob/master/examples/data/STS-B/STS-B.valid.data) ```shell
-sentence1 sentence2 label √§¬∏¬Ä√§¬∏¬™√•¬•¬≥√•¬≠¬©√•¬ú¬®√ß¬ª¬ô√•¬•¬π√ß¬ö¬Ñ√•¬§¬¥√•¬è¬ë√•¬Å¬ö√•¬è¬ë√•¬û¬ã√£¬Ä¬Ç
-√§¬∏¬Ä√§¬∏¬™√•¬•¬≥√•¬≠¬©√•¬ú¬®√¶¬¢¬≥√•¬§¬¥√£¬Ä¬Ç 2 √§¬∏¬Ä√ß¬æ¬§√ß¬î¬∑√§¬∫¬∫√•¬ú¬®√¶¬µ¬∑√¶¬ª¬©√§¬∏¬ä√®¬∏¬¢√®¬∂¬≥√ß¬ê¬É√£¬Ä¬Ç
-√§¬∏¬Ä√ß¬æ¬§√ß¬î¬∑√•¬≠¬©√•¬ú¬®√¶¬µ¬∑√¶¬ª¬©√§¬∏¬ä√®¬∏¬¢√®¬∂¬≥√ß¬ê¬É√£¬Ä¬Ç 3
+training_sup_text_matching_model_mydata.py) √•¬ç¬ï√•¬ç¬°√®¬Æ¬≠√ß¬ª¬É√Ø¬º¬ö ```shell
+CUDA_VISIBLE_DEVICES=0 python training_sup_text_matching_model_mydata.py --
+do_train --do_predict ``` √•¬§¬ö√•¬ç¬°√®¬Æ¬≠√ß¬ª¬É√Ø¬º¬ö ```shell CUDA_VISIBLE_DEVICES=0,1
+torchrun --nproc_per_node 2 training_sup_text_matching_model_mydata.py --
+do_train --do_predict --output_dir outputs/STS-B-text2vec-macbert-v1 --
+batch_size 64 --fp16 --data_parallel ``` √®¬Æ¬≠√ß¬ª¬É√©¬õ¬Ü√¶¬†¬º√•¬º¬è√•¬è¬Ç√®¬Ä¬É[examples/data/
+STS-B/STS-B.valid.data](https://github.com/shibing624/text2vec/blob/master/
+examples/data/STS-B/STS-B.valid.data) ```shell sentence1 sentence2 label
+√§¬∏¬Ä√§¬∏¬™√•¬•¬≥√•¬≠¬©√•¬ú¬®√ß¬ª¬ô√•¬•¬π√ß¬ö¬Ñ√•¬§¬¥√•¬è¬ë√•¬Å¬ö√•¬è¬ë√•¬û¬ã√£¬Ä¬Ç √§¬∏¬Ä√§¬∏¬™√•¬•¬≥√•¬≠¬©√•¬ú¬®√¶¬¢¬≥√•¬§¬¥√£¬Ä¬Ç 2
+√§¬∏¬Ä√ß¬æ¬§√ß¬î¬∑√§¬∫¬∫√•¬ú¬®√¶¬µ¬∑√¶¬ª¬©√§¬∏¬ä√®¬∏¬¢√®¬∂¬≥√ß¬ê¬É√£¬Ä¬Ç √§¬∏¬Ä√ß¬æ¬§√ß¬î¬∑√•¬≠¬©√•¬ú¬®√¶¬µ¬∑√¶¬ª¬©√§¬∏¬ä√®¬∏¬¢√®¬∂¬≥√ß¬ê¬É√£¬Ä¬Ç 3
 √§¬∏¬Ä√§¬∏¬™√•¬•¬≥√§¬∫¬∫√•¬ú¬®√¶¬µ¬ã√©¬á¬è√•¬è¬¶√§¬∏¬Ä√§¬∏¬™√•¬•¬≥√§¬∫¬∫√ß¬ö¬Ñ√®¬Ñ¬ö√®¬∏¬ù√£¬Ä¬Ç
 √•¬•¬≥√§¬∫¬∫√¶¬µ¬ã√©¬á¬è√•¬è¬¶√§¬∏¬Ä√§¬∏¬™√•¬•¬≥√§¬∫¬∫√ß¬ö¬Ñ√®¬Ñ¬ö√®¬∏¬ù√£¬Ä¬Ç 5 ```
 `label`√•¬è¬Ø√§¬ª¬•√¶¬ò¬Ø0√Ø¬º¬å1√¶¬†¬á√ß¬≠¬æ√Ø¬º¬å0√§¬ª¬£√®¬°¬®√§¬∏¬§√§¬∏¬™√•¬è¬•√•¬≠¬ê√§¬∏¬ç√ß¬õ¬∏√§¬º¬º√Ø¬º¬å1√§¬ª¬£√®¬°¬®√ß¬õ¬∏√§¬º¬º√Ø¬º¬õ√§¬π¬ü√•¬è¬Ø√§¬ª¬•√¶¬ò¬Ø0-
 5√ß¬ö¬Ñ√®¬Ø¬Ñ√•¬à¬Ü√Ø¬º¬å√®¬Ø¬Ñ√•¬à¬Ü√®¬∂¬ä√©¬´¬ò√Ø¬º¬å√®¬°¬®√ß¬§¬∫√§¬∏¬§√§¬∏¬™√•¬è¬•√•¬≠¬ê√®¬∂¬ä√ß¬õ¬∏√§¬º¬º√£¬Ä¬Ç√¶¬®¬°√•¬û¬ã√©¬É¬Ω√®¬É¬Ω√¶¬î¬Ø√¶¬å¬Å√£¬Ä¬Ç
 - √•¬ú¬®√®¬ã¬±√¶¬ñ¬áSTS-B√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√®¬Æ¬≠√ß¬ª¬É√•¬í¬å√®¬Ø¬Ñ√§¬º¬∞`CoSENT`√¶¬®¬°√•¬û¬ã example: [examples/
 training_sup_text_matching_model_en.py](https://github.com/shibing624/text2vec/
 blob/master/examples/training_sup_text_matching_model_en.py) ```shell cd
@@ -363,15 +386,15 @@
 output_dir ./outputs/STS-B-en-cosent ``` #### CoSENT √¶¬ó¬†√ß¬õ¬ë√ß¬ù¬£√¶¬®¬°√•¬û¬ã -
 √•¬ú¬®√®¬ã¬±√¶¬ñ¬áNLI√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√®¬Æ¬≠√ß¬ª¬É`CoSENT`√¶¬®¬°√•¬û¬ã√Ø¬º¬å√•¬ú¬®STS-B√¶¬µ¬ã√®¬Ø¬ï√©¬õ¬Ü√®¬Ø¬Ñ√§¬º¬∞√¶¬ï¬à√¶¬û¬ú
 example: [examples/training_unsup_text_matching_model_en.py](https://
 github.com/shibing624/text2vec/blob/master/examples/
 training_unsup_text_matching_model_en.py) ```shell cd examples python
 training_unsup_text_matching_model_en.py --model_arch cosent --do_train --
 do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./
-outputs/STS-B-en-unsup-cosent ``` ## Sentence-BERT model Sentence-
+outputs/STS-B-en-unsup-cosent ``` ### Sentence-BERT model Sentence-
 BERT√¶¬ñ¬á√¶¬ú¬¨√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã√Ø¬º¬å√®¬°¬®√•¬æ¬Å√•¬º¬è√•¬è¬•√•¬ê¬ë√©¬á¬è√®¬°¬®√ß¬§¬∫√¶¬ñ¬π√¶¬°¬à Network structure:
 Training: [docs/sbert_train.png] Inference: [docs/sbert_inference.png] ####
 SentenceBERT √ß¬õ¬ë√ß¬ù¬£√¶¬®¬°√•¬û¬ã - √•¬ú¬®√§¬∏¬≠√¶¬ñ¬áSTS-B√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√®¬Æ¬≠√ß¬ª¬É√•¬í¬å√®¬Ø¬Ñ√§¬º¬∞`SBERT`√¶¬®¬°√•¬û¬ã
 example: [examples/training_sup_text_matching_model.py](https://github.com/
 shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)
 ```shell cd examples python training_sup_text_matching_model.py --model_arch
 sentencebert --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-
@@ -384,35 +407,35 @@
 uncased --output_dir ./outputs/STS-B-en-sbert ``` #### SentenceBERT
 √¶¬ó¬†√ß¬õ¬ë√ß¬ù¬£√¶¬®¬°√•¬û¬ã - √•¬ú¬®√®¬ã¬±√¶¬ñ¬áNLI√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√®¬Æ¬≠√ß¬ª¬É`SBERT`√¶¬®¬°√•¬û¬ã√Ø¬º¬å√•¬ú¬®STS-
 B√¶¬µ¬ã√®¬Ø¬ï√©¬õ¬Ü√®¬Ø¬Ñ√§¬º¬∞√¶¬ï¬à√¶¬û¬ú example: [examples/
 training_unsup_text_matching_model_en.py](https://github.com/shibing624/
 text2vec/blob/master/examples/training_unsup_text_matching_model_en.py)
 ```shell cd examples python training_unsup_text_matching_model_en.py --
 model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name
-bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-sbert ``` ## BERT-Match
-model
+bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-sbert ``` ### BERT-
+Match model
 BERT√¶¬ñ¬á√¶¬ú¬¨√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã√Ø¬º¬å√•¬é¬ü√ß¬î¬üBERT√•¬å¬π√©¬Ö¬ç√ß¬Ω¬ë√ß¬ª¬ú√ß¬ª¬ì√¶¬û¬Ñ√Ø¬º¬å√§¬∫¬§√§¬∫¬í√•¬º¬è√•¬è¬•√•¬ê¬ë√©¬á¬è√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã
 Network structure: Training and inference: [docs/bert-fc-train.png]
 √®¬Æ¬≠√ß¬ª¬É√®¬Ñ¬ö√¶¬ú¬¨√•¬ê¬å√§¬∏¬ä[examples/training_sup_text_matching_model.py](https://
 github.com/shibing624/text2vec/blob/master/examples/
-training_sup_text_matching_model.py)√£¬Ä¬Ç ## √¶¬®¬°√•¬û¬ã√®¬í¬∏√©¬¶¬è√Ø¬º¬àModel Distillation√Ø¬º¬â
-√ß¬î¬±√§¬∫¬étext2vec√®¬Æ¬≠√ß¬ª¬É√ß¬ö¬Ñ√¶¬®¬°√•¬û¬ã√•¬è¬Ø√§¬ª¬•√§¬Ω¬ø√ß¬î¬®[sentence-transformers](https://
-github.com/UKPLab/sentence-
+training_sup_text_matching_model.py)√£¬Ä¬Ç ### √¶¬®¬°√•¬û¬ã√®¬í¬∏√©¬¶¬è√Ø¬º¬àModel
+Distillation√Ø¬º¬â √ß¬î¬±√§¬∫¬étext2vec√®¬Æ¬≠√ß¬ª¬É√ß¬ö¬Ñ√¶¬®¬°√•¬û¬ã√•¬è¬Ø√§¬ª¬•√§¬Ω¬ø√ß¬î¬®[sentence-
+transformers](https://github.com/UKPLab/sentence-
 transformers)√•¬∫¬ì√•¬ä¬†√®¬Ω¬Ω√Ø¬º¬å√¶¬≠¬§√•¬§¬Ñ√•¬§¬ç√ß¬î¬®√•¬Ö¬∂√¶¬®¬°√•¬û¬ã√®¬í¬∏√©¬¶¬è√¶¬ñ¬π√¶¬≥¬ï[distillation](https:
 //github.com/UKPLab/sentence-transformers/tree/master/examples/training/
 distillation)√£¬Ä¬Ç 1. √¶¬®¬°√•¬û¬ã√©¬ô¬ç√ß¬ª¬¥√Ø¬º¬å√•¬è¬Ç√®¬Ä¬É[dimensionality_reduction.py](https://
 github.com/UKPLab/sentence-transformers/blob/master/examples/training/
 distillation/
 dimensionality_reduction.py)√§¬Ω¬ø√ß¬î¬®PCA√•¬Ø¬π√¶¬®¬°√•¬û¬ã√®¬æ¬ì√•¬á¬∫embedding√©¬ô¬ç√ß¬ª¬¥√Ø¬º¬å√•¬è¬Ø√•¬á¬è√•¬∞¬ëmilvus√ß¬≠¬â√•¬ê¬ë√©¬á¬è√¶¬£¬Ä√ß¬¥¬¢√¶¬ï¬∞√¶¬ç¬Æ√•¬∫¬ì√ß¬ö¬Ñ√•¬≠¬ò√•¬Ç¬®√•¬é¬ã√•¬ä¬õ√Ø¬º¬å√®¬ø¬ò√®¬É¬Ω√®¬Ω¬ª√•¬æ¬Æ√¶¬è¬ê√•¬ç¬á√¶¬®¬°√•¬û¬ã√¶¬ï¬à√¶¬û¬ú√£¬Ä¬Ç
 2. √¶¬®¬°√•¬û¬ã√®¬í¬∏√©¬¶¬è√Ø¬º¬å√•¬è¬Ç√®¬Ä¬É[model_distillation.py](https://github.com/UKPLab/
 sentence-transformers/blob/master/examples/training/distillation/
 model_distillation.py)√§¬Ω¬ø√ß¬î¬®√®¬í¬∏√©¬¶¬è√¶¬ñ¬π√¶¬≥¬ï√Ø¬º¬å√•¬∞¬ÜTeacher√•¬§¬ß√¶¬®¬°√•¬û¬ã√®¬í¬∏√©¬¶¬è√•¬à¬∞√¶¬õ¬¥√•¬∞¬ëlayers√•¬±¬Ç√¶¬ï¬∞√ß¬ö¬Ñstudent√¶¬®¬°√•¬û¬ã√§¬∏¬≠√Ø¬º¬å√•¬ú¬®√¶¬ù¬É√®¬°¬°√¶¬ï¬à√¶¬û¬ú√ß¬ö¬Ñ√¶¬É¬Ö√•¬Ü¬µ√§¬∏¬ã√Ø¬º¬å√•¬è¬Ø√•¬§¬ß√•¬π¬Ö√¶¬è¬ê√•¬ç¬á√¶¬®¬°√•¬û¬ã√©¬¢¬Ñ√¶¬µ¬ã√©¬Ä¬ü√•¬∫¬¶√£¬Ä¬Ç
-## √¶¬®¬°√•¬û¬ã√©¬É¬®√ß¬Ω¬≤ √¶¬è¬ê√§¬æ¬õ√§¬∏¬§√ß¬ß¬ç√©¬É¬®√ß¬Ω¬≤√¶¬®¬°√•¬û¬ã√Ø¬º¬å√¶¬ê¬≠√•¬ª¬∫√¶¬ú¬ç√•¬ä¬°√ß¬ö¬Ñ√¶¬ñ¬π√¶¬≥¬ï√Ø¬º¬ö
+### √¶¬®¬°√•¬û¬ã√©¬É¬®√ß¬Ω¬≤ √¶¬è¬ê√§¬æ¬õ√§¬∏¬§√ß¬ß¬ç√©¬É¬®√ß¬Ω¬≤√¶¬®¬°√•¬û¬ã√Ø¬º¬å√¶¬ê¬≠√•¬ª¬∫√¶¬ú¬ç√•¬ä¬°√ß¬ö¬Ñ√¶¬ñ¬π√¶¬≥¬ï√Ø¬º¬ö
 1√Ø¬º¬â√•¬ü¬∫√§¬∫¬éJina√¶¬ê¬≠√•¬ª¬∫gRPC√¶¬ú¬ç√•¬ä¬°√£¬Ä¬ê√¶¬é¬®√®¬ç¬ê√£¬Ä¬ë√Ø¬º¬õ2√Ø¬º¬â√•¬ü¬∫√§¬∫¬éFastAPI√¶¬ê¬≠√•¬ª¬∫√•¬é¬ü√ß¬î¬üHttp√¶¬ú¬ç√•¬ä¬°√£¬Ä¬Ç
-### Jina√¶¬ú¬ç√•¬ä¬° √©¬á¬á√ß¬î¬®C/
+#### Jina√¶¬ú¬ç√•¬ä¬° √©¬á¬á√ß¬î¬®C/
 S√¶¬®¬°√•¬º¬è√¶¬ê¬≠√•¬ª¬∫√©¬´¬ò√¶¬Ä¬ß√®¬É¬Ω√¶¬ú¬ç√•¬ä¬°√Ø¬º¬å√¶¬î¬Ø√¶¬å¬Ådocker√§¬∫¬ë√•¬é¬ü√ß¬î¬ü√Ø¬º¬ågRPC/HTTP/
 WebSocket√Ø¬º¬å√¶¬î¬Ø√¶¬å¬Å√•¬§¬ö√§¬∏¬™√¶¬®¬°√•¬û¬ã√•¬ê¬å√¶¬ó¬∂√©¬¢¬Ñ√¶¬µ¬ã√Ø¬º¬åGPU√•¬§¬ö√•¬ç¬°√•¬§¬Ñ√ß¬ê¬Ü√£¬Ä¬Ç - √•¬Æ¬â√®¬£¬Ö√Ø¬º¬ö
 ```pip install jina``` - √•¬ê¬Ø√•¬ä¬®√¶¬ú¬ç√•¬ä¬°√Ø¬º¬ö example: [examples/
 jina_server_demo.py](examples/jina_server_demo.py) ```python from jina import
 Flow port = 50001 f = Flow(port=port).add( uses='jinahub://Text2vecEncoder',
 uses_with={'model_name': 'shibing624/text2vec-base-chinese'} ) with f: #
 backend server forever f.block() ```
@@ -421,27 +444,27 @@
 √®¬∞¬É√ß¬î¬®√¶¬ú¬ç√•¬ä¬°√Ø¬º¬ö ```python from jina import Client from docarray import
 Document, DocumentArray port = 50001 c = Client(port=port) data =
 ['√•¬¶¬Ç√§¬Ω¬ï√¶¬õ¬¥√¶¬ç¬¢√®¬ä¬±√•¬ë¬ó√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°', '√®¬ä¬±√•¬ë¬ó√¶¬õ¬¥√¶¬î¬π√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°'] print
 ("data:", data) print('data embs:') r = c.post('/', inputs=DocumentArray(
 [Document(text='√•¬¶¬Ç√§¬Ω¬ï√¶¬õ¬¥√¶¬ç¬¢√®¬ä¬±√•¬ë¬ó√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°'), Document
 (text='√®¬ä¬±√•¬ë¬ó√¶¬õ¬¥√¶¬î¬π√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°')])) print(r.embeddings) ```
 √¶¬â¬π√©¬á¬è√®¬∞¬É√ß¬î¬®√¶¬ñ¬π√¶¬≥¬ï√®¬ß¬Åexample: [examples/jina_client_demo.py](https://
-github.com/shibing624/text2vec/blob/master/examples/jina_client_demo.py) ###
+github.com/shibing624/text2vec/blob/master/examples/jina_client_demo.py) ####
 FastAPI√¶¬ú¬ç√•¬ä¬° - √•¬Æ¬â√®¬£¬Ö√Ø¬º¬ö ```pip install fastapi uvicorn``` - √•¬ê¬Ø√•¬ä¬®√¶¬ú¬ç√•¬ä¬°√Ø¬º¬ö
 example: [examples/fastapi_server_demo.py](https://github.com/shibing624/
 text2vec/blob/master/examples/fastapi_server_demo.py) ```shell cd examples
 python fastapi_server_demo.py ``` - √®¬∞¬É√ß¬î¬®√¶¬ú¬ç√•¬ä¬°√Ø¬º¬ö ```shell curl -X 'GET' \
 'http://0.0.0.0:8001/emb?q=hello' \ -H 'accept: application/json' ``` ##
-√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü - √¶¬ú¬¨√©¬°¬π√ß¬õ¬Ærelease√ß¬ö¬Ñ√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬ö | Dataset | Introduce | Download
-Link | |:---------------------------|:-----------------------------------------
---------------------------------|:---------------------------------------------
+Dataset - √¶¬ú¬¨√©¬°¬π√ß¬õ¬Ærelease√ß¬ö¬Ñ√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬ö | Dataset | Introduce | Download Link
+| |:---------------------------|:----------------------------------------------
+---------------------------|:--------------------------------------------------
 -------------------------------------------------------------------------------
 -------------------------------------------------------------------------------
 -------------------------------------------------------------------------------
-------------| | shibing624/nli-zh-all |
+-------| | shibing624/nli-zh-all |
 √§¬∏¬≠√¶¬ñ¬á√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√•¬ê¬à√©¬õ¬Ü√Ø¬º¬å√¶¬ï¬¥√•¬ê¬à√§¬∫¬Ü√¶¬ñ¬á√¶¬ú¬¨√¶¬é¬®√ß¬ê¬Ü√Ø¬º¬å√ß¬õ¬∏√§¬º¬º√Ø¬º¬å√¶¬ë¬ò√®¬¶¬Å√Ø¬º¬å√©¬ó¬Æ√ß¬≠¬î√Ø¬º¬å√¶¬å¬á√§¬ª¬§√•¬æ¬Æ√®¬∞¬É√ß¬≠¬â√§¬ª¬ª√•¬ä¬°√ß¬ö¬Ñ820√§¬∏¬á√©¬´¬ò√®¬¥¬®√©¬á¬è√¶¬ï¬∞√¶¬ç¬Æ√Ø¬º¬å√•¬π¬∂√®¬Ω¬¨√•¬å¬ñ√§¬∏¬∫√•¬å¬π√©¬Ö¬ç√¶¬†¬º√•¬º¬è√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü
 | [https://huggingface.co/datasets/shibing624/nli-zh-all](https://
 huggingface.co/datasets/shibing624/nli-zh-all) | | shibing624/snli-zh |
 √§¬∏¬≠√¶¬ñ¬áSNLI√•¬í¬åMultiNLI√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬å√ß¬ø¬ª√®¬Ø¬ë√®¬á¬™√®¬ã¬±√¶¬ñ¬áSNLI√•¬í¬åMultiNLI | [https://
 huggingface.co/datasets/shibing624/snli-zh](https://huggingface.co/datasets/
 shibing624/snli-zh) | | shibing624/nli_zh |
 √§¬∏¬≠√¶¬ñ¬á√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬å√¶¬ï¬¥√•¬ê¬à√§¬∫¬Ü√§¬∏¬≠√¶¬ñ¬áATEC√£¬Ä¬ÅBQ√£¬Ä¬ÅLCQMC√£¬Ä¬ÅPAWSX√£¬Ä¬ÅSTS-
@@ -459,46 +482,46 @@
 info/1037/1162.htm) | | LCQMC | √§¬∏¬≠√¶¬ñ¬áLCQMC(large-scale Chinese question
 matching corpus)√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬åQ-Qpair√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü | [LCQMC](http://
 icrc.hitsz.edu.cn/Article/show/171.html) | | PAWSX | √§¬∏¬≠√¶¬ñ¬áPAWS(Paraphrase
 Adversaries from Word Scrambling)√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬åQ-Qpair√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü | [PAWSX](https:/
 /arxiv.org/abs/1908.11828) | | STS-B | √§¬∏¬≠√¶¬ñ¬áSTS-
 B√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬å√§¬∏¬≠√¶¬ñ¬á√®¬á¬™√ß¬Ñ¬∂√®¬Ø¬≠√®¬®¬Ä√¶¬é¬®√ß¬ê¬Ü√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬å√§¬ª¬é√®¬ã¬±√¶¬ñ¬áSTS-
 B√ß¬ø¬ª√®¬Ø¬ë√§¬∏¬∫√§¬∏¬≠√¶¬ñ¬á√ß¬ö¬Ñ√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü | [STS-B](https://github.com/pluto-junzeng/CNSD) |
-√•¬∏¬∏√ß¬î¬®√®¬ã¬±√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬ö - √•¬§¬ß√•¬ê¬ç√©¬º¬é√©¬º¬é√ß¬ö¬Ñmulti_nli√•¬í¬åsnli: https://
-huggingface.co/datasets/multi_nli - √•¬§¬ß√•¬ê¬ç√©¬º¬é√©¬º¬é√ß¬ö¬Ñmulti_nli√•¬í¬åsnli: https://
+√•¬∏¬∏√ß¬î¬®√®¬ã¬±√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬ö - √®¬ã¬±√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬ömulti_nli: https://
+huggingface.co/datasets/multi_nli - √®¬ã¬±√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬ösnli: https://
 huggingface.co/datasets/snli - https://huggingface.co/datasets/metaeval/cnli -
 https://huggingface.co/datasets/mteb/stsbenchmark-sts - https://huggingface.co/
 datasets/JeremiahZ/simcse_sup_nli - https://huggingface.co/datasets/
 MoritzLaurer/multilingual-NLI-26lang-2mil7 √¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√§¬Ω¬ø√ß¬î¬®√ß¬§¬∫√§¬æ¬ã√Ø¬º¬ö ```shell
 pip install datasets ``` ```python from datasets import load_dataset dataset =
 load_dataset("shibing624/nli_zh", "STS-B") # ATEC or BQ or LCQMC or PAWSX or
 STS-B print(dataset) print(dataset['test'][0]) ``` output: ```shell DatasetDict
 ({ train: Dataset({ features: ['sentence1', 'sentence2', 'label'], num_rows:
 5231 }) validation: Dataset({ features: ['sentence1', 'sentence2', 'label'],
 num_rows: 1458 }) test: Dataset({ features: ['sentence1', 'sentence2',
 'label'], num_rows: 1361 }) }) {'sentence1':
 '√§¬∏¬Ä√§¬∏¬™√•¬•¬≥√•¬≠¬©√•¬ú¬®√ß¬ª¬ô√•¬•¬π√ß¬ö¬Ñ√•¬§¬¥√•¬è¬ë√•¬Å¬ö√•¬è¬ë√•¬û¬ã√£¬Ä¬Ç', 'sentence2':
-'√§¬∏¬Ä√§¬∏¬™√•¬•¬≥√•¬≠¬©√•¬ú¬®√¶¬¢¬≥√•¬§¬¥√£¬Ä¬Ç', 'label': 2} ``` # Contact - Issue(√•¬ª¬∫√®¬Æ¬Æ)√Ø¬º¬ö[!
+'√§¬∏¬Ä√§¬∏¬™√•¬•¬≥√•¬≠¬©√•¬ú¬®√¶¬¢¬≥√•¬§¬¥√£¬Ä¬Ç', 'label': 2} ``` ## Contact - Issue(√•¬ª¬∫√®¬Æ¬Æ)√Ø¬º¬ö[!
 [GitHub issues](https://img.shields.io/github/issues/shibing624/text2vec.svg)]
 (https://github.com/shibing624/text2vec/issues) - √©¬Ç¬Æ√§¬ª¬∂√¶¬à¬ë√Ø¬º¬öxuming:
 xuming624@qq.com - √•¬æ¬Æ√§¬ø¬°√¶¬à¬ë√Ø¬º¬ö√•¬ä¬†√¶¬à¬ë*√•¬æ¬Æ√§¬ø¬°√•¬è¬∑√Ø¬º¬öxuming624, √•¬§¬á√¶¬≥¬®√Ø¬º¬ö√•¬ß¬ì√•¬ê¬ç-
-√•¬Ö¬¨√•¬è¬∏-NLP* √®¬ø¬õNLP√§¬∫¬§√¶¬µ¬Å√ß¬æ¬§√£¬Ä¬Ç [docs/wechat.jpeg] # Citation
+√•¬Ö¬¨√•¬è¬∏-NLP* √®¬ø¬õNLP√§¬∫¬§√¶¬µ¬Å√ß¬æ¬§√£¬Ä¬Ç [docs/wechat.jpeg] ## Citation
 √•¬¶¬Ç√¶¬û¬ú√§¬Ω¬†√•¬ú¬®√ß¬†¬î√ß¬©¬∂√§¬∏¬≠√§¬Ω¬ø√ß¬î¬®√§¬∫¬Ütext2vec√Ø¬º¬å√®¬Ø¬∑√¶¬å¬â√•¬¶¬Ç√§¬∏¬ã√¶¬†¬º√•¬º¬è√•¬º¬ï√ß¬î¬®√Ø¬º¬ö APA:
 ```latex Xu, M. Text2vec: Text to vector toolkit (Version 1.1.2) [Computer
 software]. https://github.com/shibing624/text2vec ``` BibTeX: ```latex @misc
-{Text2vec, author = {Xu, Ming}, title = {Text2vec: Text to vector toolkit},
-year = {2022}, publisher = {GitHub}, journal = {GitHub repository},
-howpublished = {\url{https://github.com/shibing624/text2vec}}, } ``` # License
+{Text2vec, author = {Ming Xu}, title = {Text2vec: Text to vector toolkit}, year
+= {2023}, publisher = {GitHub}, journal = {GitHub repository}, howpublished =
+{\url{https://github.com/shibing624/text2vec}}, } ``` ## License
 √¶¬é¬à√¶¬ù¬É√•¬ç¬è√®¬Æ¬Æ√§¬∏¬∫ [The Apache License 2.0]
 (LICENSE)√Ø¬º¬å√•¬è¬Ø√•¬Ö¬ç√®¬¥¬π√ß¬î¬®√•¬Å¬ö√•¬ï¬Ü√§¬∏¬ö√ß¬î¬®√©¬Ä¬î√£¬Ä¬Ç√®¬Ø¬∑√•¬ú¬®√§¬∫¬ß√•¬ì¬Å√®¬Ø¬¥√¶¬ò¬é√§¬∏¬≠√©¬ô¬Ñ√•¬ä¬†text2vec√ß¬ö¬Ñ√©¬ì¬æ√¶¬é¬•√•¬í¬å√¶¬é¬à√¶¬ù¬É√•¬ç¬è√®¬Æ¬Æ√£¬Ä¬Ç
-# Contribute
+## Contribute
 √©¬°¬π√ß¬õ¬Æ√§¬ª¬£√ß¬†¬Å√®¬ø¬ò√•¬æ¬à√ß¬≤¬ó√ß¬≥¬ô√Ø¬º¬å√•¬¶¬Ç√¶¬û¬ú√•¬§¬ß√•¬Æ¬∂√•¬Ø¬π√§¬ª¬£√ß¬†¬Å√¶¬ú¬â√¶¬â¬Ä√¶¬î¬π√®¬ø¬õ√Ø¬º¬å√¶¬¨¬¢√®¬ø¬é√¶¬è¬ê√§¬∫¬§√•¬õ¬û√¶¬ú¬¨√©¬°¬π√ß¬õ¬Æ√Ø¬º¬å√•¬ú¬®√¶¬è¬ê√§¬∫¬§√§¬π¬ã√•¬â¬ç√Ø¬º¬å√¶¬≥¬®√¶¬Ñ¬è√§¬ª¬•√§¬∏¬ã√§¬∏¬§√ß¬Ç¬π√Ø¬º¬ö
 - √•¬ú¬®`tests`√¶¬∑¬ª√•¬ä¬†√ß¬õ¬∏√•¬∫¬î√ß¬ö¬Ñ√•¬ç¬ï√•¬Ö¬É√¶¬µ¬ã√®¬Ø¬ï - √§¬Ω¬ø√ß¬î¬®`python -m pytest -
 v`√¶¬ù¬•√®¬ø¬ê√®¬°¬å√¶¬â¬Ä√¶¬ú¬â√•¬ç¬ï√•¬Ö¬É√¶¬µ¬ã√®¬Ø¬ï√Ø¬º¬å√ß¬°¬Æ√§¬ø¬ù√¶¬â¬Ä√¶¬ú¬â√•¬ç¬ï√¶¬µ¬ã√©¬É¬Ω√¶¬ò¬Ø√©¬Ä¬ö√®¬ø¬á√ß¬ö¬Ñ
-√§¬π¬ã√•¬ê¬é√•¬ç¬≥√•¬è¬Ø√¶¬è¬ê√§¬∫¬§PR√£¬Ä¬Ç # Reference -
+√§¬π¬ã√•¬ê¬é√•¬ç¬≥√•¬è¬Ø√¶¬è¬ê√§¬∫¬§PR√£¬Ä¬Ç ## References -
 [√•¬∞¬Ü√•¬è¬•√•¬≠¬ê√®¬°¬®√ß¬§¬∫√§¬∏¬∫√•¬ê¬ë√©¬á¬è√Ø¬º¬à√§¬∏¬ä√Ø¬º¬â√Ø¬º¬ö√¶¬ó¬†√ß¬õ¬ë√ß¬ù¬£√•¬è¬•√•¬≠¬ê√®¬°¬®√ß¬§¬∫√•¬≠¬¶√§¬π¬†√Ø¬º¬àsentence
 embedding√Ø¬º¬â](https://www.cnblogs.com/llhthinker/p/10335164.html) -
 [√•¬∞¬Ü√•¬è¬•√•¬≠¬ê√®¬°¬®√ß¬§¬∫√§¬∏¬∫√•¬ê¬ë√©¬á¬è√Ø¬º¬à√§¬∏¬ã√Ø¬º¬â√Ø¬º¬ö√¶¬ó¬†√ß¬õ¬ë√ß¬ù¬£√•¬è¬•√•¬≠¬ê√®¬°¬®√ß¬§¬∫√•¬≠¬¶√§¬π¬†√Ø¬º¬àsentence
 embedding√Ø¬º¬â](https://www.cnblogs.com/llhthinker/p/10341841.html) - [A Simple
 but Tough-to-Beat Baseline for Sentence Embeddings[Sanjeev Arora and Yingyu
 Liang and Tengyu Ma, 2017]](https://openreview.net/forum?id=SyK00v5xx) -
 [√•¬õ¬õ√ß¬ß¬ç√®¬Æ¬°√ß¬Æ¬ó√¶¬ñ¬á√¶¬ú¬¨√ß¬õ¬∏√§¬º¬º√•¬∫¬¶√ß¬ö¬Ñ√¶¬ñ¬π√¶¬≥¬ï√•¬Ø¬π√¶¬Ø¬î[Yves Peirsman]](https://
```

### Comparing `text2vec-1.2.1/setup.py` & `text2vec-1.2.2/setup.py`

 * *Files identical despite different names*

### Comparing `text2vec-1.2.1/text2vec/__init__.py` & `text2vec-1.2.2/text2vec/__init__.py`

 * *Files identical despite different names*

### Comparing `text2vec-1.2.1/text2vec/bertmatching_dataset.py` & `text2vec-1.2.2/text2vec/bertmatching_dataset.py`

 * *Files identical despite different names*

### Comparing `text2vec-1.2.1/text2vec/bertmatching_model.py` & `text2vec-1.2.2/text2vec/bertmatching_model.py`

 * *Files 2% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 
 import math
 import pandas as pd
 import torch
 from loguru import logger
 from torch import nn
 from torch.utils.data import DataLoader, Dataset
-from tqdm.auto import tqdm, trange
+from tqdm import tqdm, trange
 from transformers import BertForSequenceClassification, BertTokenizer
 from transformers.optimization import AdamW, get_linear_schedule_with_warmup
 
 from text2vec.bertmatching_dataset import (
     BertMatchingTestDataset,
     BertMatchingTrainDataset,
     HFBertMatchingTrainDataset,
@@ -102,14 +102,16 @@
             eps: float = 1e-6,
             gradient_accumulation_steps: int = 1,
             max_grad_norm: float = 1.0,
             max_steps: int = -1,
             use_hf_dataset: bool = False,
             hf_dataset_name: str = "STS-B",
             save_model_every_epoch: bool = True,
+            bf16: bool = False,
+            data_parallel: bool = False,
     ):
         """
         Trains the model on 'train_file'
 
         Args:
             train_file: Path to text file containing the text to _train the language model on.
             output_dir: The directory where model files will be saved. If not given, self.args.output_dir will be used.
@@ -124,14 +126,16 @@
             eps (optional): Adam epsilon.
             gradient_accumulation_steps (optional): Number of updates steps to accumulate before performing a backward/update pass.
             max_grad_norm (optional): Max gradient norm.
             max_steps (optional): If > 0: set total number of training steps to perform. Override num_epochs.
             use_hf_dataset (optional): Whether to use the HuggingFace datasets for training.
             hf_dataset_name (optional): Name of the dataset to use for the HuggingFace datasets.
             save_model_every_epoch (optional): Save model checkpoint every epoch.
+            bf16 (optional): Use bfloat16 amp training.
+            data_parallel (optional): Use multi-gpu data parallel training.
         Returns:
             global_step: Number of global steps trained
             training_details: Full training progress scores
         """
         if use_hf_dataset and hf_dataset_name:
             logger.info(
                 f"Train_file will be ignored when use_hf_dataset is True, load HF dataset: {hf_dataset_name}")
@@ -159,14 +163,16 @@
             warmup_ratio=warmup_ratio,
             lr=lr,
             eps=eps,
             gradient_accumulation_steps=gradient_accumulation_steps,
             max_grad_norm=max_grad_norm,
             max_steps=max_steps,
             save_model_every_epoch=save_model_every_epoch,
+            bf16=bf16,
+            data_parallel=data_parallel,
         )
         logger.info(f" Training model done. Saved to {output_dir}.")
 
         return global_step, training_details
 
     def train(
             self,
@@ -181,25 +187,30 @@
             warmup_ratio: float = 0.05,
             lr: float = 2e-5,
             eps: float = 1e-6,
             gradient_accumulation_steps: int = 1,
             max_grad_norm: float = 1.0,
             max_steps: int = -1,
             save_model_every_epoch: bool = True,
+            bf16: bool = False,
+            data_parallel: bool = False,
     ):
         """
         Trains the model on train_dataset.
 
         Utility function to be used by the train_model() method. Not intended to be used directly.
         """
         os.makedirs(output_dir, exist_ok=True)
         logger.debug("Use pytorch device: {}".format(device))
         self.model.bert.to(device)
         set_seed(seed)
 
+        if data_parallel:
+            self.bert = nn.DataParallel(self.bert)
+
         train_dataloader = DataLoader(train_dataset, batch_size=batch_size)  # keep the order of the data, not shuffle
         total_steps = len(train_dataloader) * num_epochs
         param_optimizer = list(self.model.bert.named_parameters())
         no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
         optimizer_grouped_parameters = [
             {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
              'weight_decay': weight_decay},
@@ -263,16 +274,24 @@
                     steps_trained_in_current_epoch -= 1
                     continue
                 inputs, labels = batch
                 labels = labels.to(device)
                 # inputs        [batch, 1, seq_len] -> [batch, seq_len]
                 input_ids = inputs.get('input_ids').squeeze(1).to(device)
                 attention_mask = inputs.get('attention_mask').squeeze(1).to(device)
-                token_type_ids = inputs.get('token_type_ids').squeeze(1).to(device)
-                loss, logits, probs = self.model(input_ids, attention_mask, token_type_ids, labels)
+                token_type_ids = inputs.get('token_type_ids', None)
+                if token_type_ids is not None:
+                    token_type_ids = token_type_ids.squeeze(1).to(self.device)
+
+                if bf16:
+                    with torch.autocast('cuda', dtype=torch.bfloat16):
+                        loss, logits, probs = self.model(input_ids, attention_mask, token_type_ids, labels)
+                else:
+                    loss, logits, probs = self.model(input_ids, attention_mask, token_type_ids, labels)
+                    
                 current_loss = loss.item()
 
                 if verbose:
                     batch_iterator.set_description(
                         f"Epoch: {epoch_number + 1}/{num_epochs}, Batch:{step}/{len(train_dataloader)}, Loss: {current_loss:9.4f}")
 
                 if gradient_accumulation_steps > 1:
```

### Comparing `text2vec-1.2.1/text2vec/bm25.py` & `text2vec-1.2.2/text2vec/bm25.py`

 * *Files identical despite different names*

### Comparing `text2vec-1.2.1/text2vec/cosent_dataset.py` & `text2vec-1.2.2/text2vec/cosent_dataset.py`

 * *Files identical despite different names*

### Comparing `text2vec-1.2.1/text2vec/cosent_model.py` & `text2vec-1.2.2/text2vec/cosent_model.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,21 +1,22 @@
 # -*- coding: utf-8 -*-
 """
 @author:XuMing(xuming624@qq.com)
 @description: Create CoSENT model for text matching task
 """
 
+import math
 import os
 
-import math
 import pandas as pd
 import torch
 from loguru import logger
+from torch import nn
 from torch.utils.data import DataLoader, Dataset
-from tqdm.auto import tqdm, trange
+from tqdm import tqdm, trange
 from transformers.optimization import AdamW, get_linear_schedule_with_warmup
 
 from text2vec.cosent_dataset import CosentTrainDataset, load_cosent_train_data, HFCosentTrainDataset
 from text2vec.sentence_model import SentenceModel
 from text2vec.text_matching_dataset import TextMatchingTestDataset, load_text_matching_test_data, \
     HFTextMatchingTestDataset
 from text2vec.utils.stats_util import set_seed
@@ -59,14 +60,16 @@
             eps: float = 1e-6,
             gradient_accumulation_steps: int = 1,
             max_grad_norm: float = 1.0,
             max_steps: int = -1,
             use_hf_dataset: bool = False,
             hf_dataset_name: str = "STS-B",
             save_model_every_epoch: bool = True,
+            bf16: bool = False,
+            data_parallel: bool = False,
     ):
         """
         Trains the model on 'train_file'
 
         Args:
             train_file: Path to text file containing the text to _train the language model on.
             output_dir: The directory where model files will be saved. If not given, self.args.output_dir will be used.
@@ -81,14 +84,16 @@
             eps (optional): Adam epsilon.
             gradient_accumulation_steps (optional): Number of updates steps to accumulate before performing a backward/update pass.
             max_grad_norm (optional): Max gradient norm.
             max_steps (optional): If > 0: set total number of training steps to perform. Override num_epochs.
             use_hf_dataset (optional): Whether to use the HF dataset.
             hf_dataset_name (optional): Name of the dataset to use for the HuggingFace datasets.
             save_model_every_epoch (optional): Save model checkpoint every epoch.
+            bf16 (optional): Use bfloat16 amp training.
+            data_parallel (optional): Use multi-gpu data parallel training.
         Returns:
             global_step: Number of global steps trained
             training_details: full training progress scores
         """
         if use_hf_dataset and hf_dataset_name:
             logger.info(
                 f"Train_file will be ignored when use_hf_dataset is True, load HF dataset: {hf_dataset_name}")
@@ -115,14 +120,16 @@
             warmup_ratio=warmup_ratio,
             lr=lr,
             eps=eps,
             gradient_accumulation_steps=gradient_accumulation_steps,
             max_grad_norm=max_grad_norm,
             max_steps=max_steps,
             save_model_every_epoch=save_model_every_epoch,
+            bf16=bf16,
+            data_parallel=data_parallel,
         )
         logger.info(f" Training model done. Saved to {output_dir}.")
 
         return global_step, training_details
 
     def calc_loss(self, y_true, y_pred):
         """
@@ -159,25 +166,30 @@
             warmup_ratio: float = 0.05,
             lr: float = 2e-5,
             eps: float = 1e-6,
             gradient_accumulation_steps: int = 1,
             max_grad_norm: float = 1.0,
             max_steps: int = -1,
             save_model_every_epoch: bool = True,
+            bf16: bool = False,
+            data_parallel: bool = False,
     ):
         """
         Trains the model on train_dataset.
 
         Utility function to be used by the train_model() method. Not intended to be used directly.
         """
         os.makedirs(output_dir, exist_ok=True)
         logger.debug("Use device: {}".format(self.device))
         self.bert.to(self.device)
         set_seed(seed)
 
+        if data_parallel:
+            self.bert = nn.DataParallel(self.bert)
+
         train_dataloader = DataLoader(train_dataset, batch_size=batch_size)  # keep the order of the data, not shuffle
         total_steps = len(train_dataloader) * num_epochs
         param_optimizer = list(self.bert.named_parameters())
         no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
         optimizer_grouped_parameters = [
             {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
              'weight_decay': weight_decay},
@@ -241,17 +253,26 @@
                     steps_trained_in_current_epoch -= 1
                     continue
                 inputs, labels = batch
                 labels = labels.to(self.device)
                 # inputs        [batch, 1, seq_len] -> [batch, seq_len]
                 input_ids = inputs.get('input_ids').squeeze(1).to(self.device)
                 attention_mask = inputs.get('attention_mask').squeeze(1).to(self.device)
-                token_type_ids = inputs.get('token_type_ids').squeeze(1).to(self.device)
-                output_embeddings = self.get_sentence_embeddings(input_ids, attention_mask, token_type_ids)
-                loss = self.calc_loss(labels, output_embeddings)
+                token_type_ids = inputs.get('token_type_ids', None)
+                if token_type_ids is not None:
+                    token_type_ids = token_type_ids.squeeze(1).to(self.device)
+
+                if bf16:
+                    with torch.autocast('cuda', dtype=torch.bfloat16):
+                        output_embeddings = self.get_sentence_embeddings(input_ids, attention_mask, token_type_ids)
+                        loss = self.calc_loss(labels, output_embeddings)
+                else:
+                    output_embeddings = self.get_sentence_embeddings(input_ids, attention_mask, token_type_ids)
+                    loss = self.calc_loss(labels, output_embeddings)
+
                 current_loss = loss.item()
                 if verbose:
                     batch_iterator.set_description(
                         f"Epoch: {epoch_number + 1}/{num_epochs}, Batch:{step}/{len(train_dataloader)}, Loss: {current_loss:9.4f}")
 
                 if gradient_accumulation_steps > 1:
                     loss = loss / gradient_accumulation_steps
```

### Comparing `text2vec-1.2.1/text2vec/ngram.py` & `text2vec-1.2.2/text2vec/ngram.py`

 * *Files identical despite different names*

### Comparing `text2vec-1.2.1/text2vec/sentence_model.py` & `text2vec-1.2.2/text2vec/sentence_model.py`

 * *Files 3% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 from enum import Enum
 from typing import List, Union, Optional
 
 import numpy as np
 import torch
 from loguru import logger
 from torch.utils.data import DataLoader, Dataset
-from tqdm.auto import tqdm, trange
+from tqdm import tqdm, trange
 from tqdm.autonotebook import trange
 from transformers import AutoTokenizer, AutoModel
 
 from text2vec.utils.stats_util import compute_spearmanr, compute_pearsonr
 
 os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
 os.environ["TOKENIZERS_PARALLELISM"] = "TRUE"
@@ -89,15 +89,15 @@
         -------
         int or None
             The dimension of the sentence embeddings, or None if it cannot be determined.
         """
         # Use getattr to safely access the out_features attribute of the pooler's dense layer
         return getattr(self.bert.pooler.dense, "out_features", None)
 
-    def get_sentence_embeddings(self, input_ids, attention_mask, token_type_ids):
+    def get_sentence_embeddings(self, input_ids, attention_mask, token_type_ids=None):
         """
         Returns the model output by encoder_type as embeddings.
 
         Utility function for self.bert() method.
         """
         model_output = self.bert(input_ids, attention_mask, token_type_ids, output_hidden_states=True)
 
@@ -223,20 +223,24 @@
         for batch in tqdm(eval_dataloader, disable=False, desc="Running Evaluation"):
             source, target, labels = batch
             labels = labels.to(self.device)
             batch_labels.extend(labels.cpu().numpy())
             # source        [batch, 1, seq_len] -> [batch, seq_len]
             source_input_ids = source.get('input_ids').squeeze(1).to(self.device)
             source_attention_mask = source.get('attention_mask').squeeze(1).to(self.device)
-            source_token_type_ids = source.get('token_type_ids').squeeze(1).to(self.device)
+            source_token_type_ids = source.get('token_type_ids', None)
+            if source_token_type_ids is not None:
+                source_token_type_ids = source_token_type_ids.squeeze(1).to(self.device)
 
             # target        [batch, 1, seq_len] -> [batch, seq_len]
             target_input_ids = target.get('input_ids').squeeze(1).to(self.device)
             target_attention_mask = target.get('attention_mask').squeeze(1).to(self.device)
-            target_token_type_ids = target.get('token_type_ids').squeeze(1).to(self.device)
+            target_token_type_ids = target.get('token_type_ids', None)
+            if target_token_type_ids is not None:
+                target_token_type_ids = target_token_type_ids.squeeze(1).to(self.device)
 
             with torch.no_grad():
                 source_embeddings = self.get_sentence_embeddings(source_input_ids, source_attention_mask,
                                                                  source_token_type_ids)
                 target_embeddings = self.get_sentence_embeddings(target_input_ids, target_attention_mask,
                                                                  target_token_type_ids)
                 preds = torch.cosine_similarity(source_embeddings, target_embeddings)
```

### Comparing `text2vec-1.2.1/text2vec/sentencebert_model.py` & `text2vec-1.2.2/text2vec/sentencebert_model.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,22 +1,22 @@
 # -*- coding: utf-8 -*-
 """
 @author:XuMing(xuming624@qq.com)
 @description: Create Sentence-BERT model for text matching task
 """
 
+import math
 import os
 
-import math
 import pandas as pd
 import torch
 from loguru import logger
 from torch import nn
 from torch.utils.data import DataLoader, Dataset
-from tqdm.auto import tqdm, trange
+from tqdm import tqdm, trange
 from transformers.optimization import AdamW, get_linear_schedule_with_warmup
 
 from text2vec.sentence_model import SentenceModel
 from text2vec.text_matching_dataset import (
     TextMatchingTrainDataset,
     TextMatchingTestDataset,
     load_text_matching_test_data,
@@ -68,14 +68,16 @@
             eps: float = 1e-6,
             gradient_accumulation_steps: int = 1,
             max_grad_norm: float = 1.0,
             max_steps: int = -1,
             use_hf_dataset: bool = False,
             hf_dataset_name: str = "STS-B",
             save_model_every_epoch: bool = True,
+            bf16: bool = False,
+            data_parallel: bool = False,
     ):
         """
         Trains the model on 'train_file'
 
         Args:
             train_file: Path to text file containing the text to _train the language model on.
             output_dir: The directory where model files will be saved. If not given, self.args.output_dir will be used.
@@ -90,14 +92,16 @@
             eps (optional): Adam epsilon.
             gradient_accumulation_steps (optional): Number of updates steps to accumulate before performing a backward/update pass.
             max_grad_norm (optional): Max gradient norm.
             max_steps (optional): If > 0: set total number of training steps to perform. Override num_epochs.
             use_hf_dataset (optional): Whether to use the HuggingFace datasets for training.
             hf_dataset_name (optional): Name of the dataset to use for the HuggingFace datasets.
             save_model_every_epoch (optional): Save model checkpoint every epoch.
+            bf16 (optional): Use bfloat16 amp training.
+            data_parallel (optional): Use multi-gpu data parallel training.
         Returns:
             global_step: Number of global steps trained
             training_details: Full training progress scores
         """
         if use_hf_dataset and hf_dataset_name:
             logger.info(
                 f"Train_file will be ignored when use_hf_dataset is True, load HF dataset: {hf_dataset_name}")
@@ -125,14 +129,16 @@
             warmup_ratio=warmup_ratio,
             lr=lr,
             eps=eps,
             gradient_accumulation_steps=gradient_accumulation_steps,
             max_grad_norm=max_grad_norm,
             max_steps=max_steps,
             save_model_every_epoch=save_model_every_epoch,
+            bf16=bf16,
+            data_parallel=data_parallel,
         )
         logger.info(f" Training model done. Saved to {output_dir}.")
 
         return global_step, training_details
 
     def concat_embeddings(self, source_embeddings, target_embeddings):
         """
@@ -169,25 +175,30 @@
             warmup_ratio: float = 0.05,
             lr: float = 2e-5,
             eps: float = 1e-6,
             gradient_accumulation_steps: int = 1,
             max_grad_norm: float = 1.0,
             max_steps: int = -1,
             save_model_every_epoch: bool = True,
+            bf16: bool = False,
+            data_parallel: bool = False,
     ):
         """
         Trains the model on train_dataset.
 
         Utility function to be used by the train_model() method. Not intended to be used directly.
         """
         os.makedirs(output_dir, exist_ok=True)
         logger.debug("Use pytorch device: {}".format(self.device))
         self.bert.to(self.device)
         set_seed(seed)
 
+        if data_parallel:
+            self.bert = nn.DataParallel(self.bert)
+
         train_dataloader = DataLoader(train_dataset, batch_size=batch_size)  # keep the order of the data, not shuffle
         total_steps = len(train_dataloader) * num_epochs
         param_optimizer = list(self.bert.named_parameters())
         no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
         optimizer_grouped_parameters = [
             {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
              'weight_decay': weight_decay},
@@ -250,28 +261,42 @@
                 if steps_trained_in_current_epoch > 0:
                     steps_trained_in_current_epoch -= 1
                     continue
                 source, target, labels = batch
                 # source        [batch, 1, seq_len] -> [batch, seq_len]
                 source_input_ids = source.get('input_ids').squeeze(1).to(self.device)
                 source_attention_mask = source.get('attention_mask').squeeze(1).to(self.device)
-                source_token_type_ids = source.get('token_type_ids').squeeze(1).to(self.device)
+                source_token_type_ids = source.get('token_type_ids', None)
+                if source_token_type_ids is not None:
+                    source_token_type_ids = source_token_type_ids.squeeze(1).to(self.device)
                 # target        [batch, 1, seq_len] -> [batch, seq_len]
                 target_input_ids = target.get('input_ids').squeeze(1).to(self.device)
                 target_attention_mask = target.get('attention_mask').squeeze(1).to(self.device)
-                target_token_type_ids = target.get('token_type_ids').squeeze(1).to(self.device)
+                target_token_type_ids = target.get('token_type_ids', None)
+                if target_token_type_ids is not None:
+                    target_token_type_ids = target_token_type_ids.squeeze(1).to(self.device)
                 labels = labels.to(self.device)
 
                 # get sentence embeddings of BERT encoder
-                source_embeddings = self.get_sentence_embeddings(source_input_ids, source_attention_mask,
-                                                                 source_token_type_ids)
-                target_embeddings = self.get_sentence_embeddings(target_input_ids, target_attention_mask,
-                                                                 target_token_type_ids)
-                logits = self.concat_embeddings(source_embeddings, target_embeddings)
-                loss = self.calc_loss(labels, logits)
+                if bf16:
+                    with torch.autocast('cuda', dtype=torch.bfloat16):
+                        source_embeddings = self.get_sentence_embeddings(source_input_ids, source_attention_mask,
+                                                                         source_token_type_ids)
+                        target_embeddings = self.get_sentence_embeddings(target_input_ids, target_attention_mask,
+                                                                         target_token_type_ids)
+                        logits = self.concat_embeddings(source_embeddings, target_embeddings)
+                        loss = self.calc_loss(labels, logits)
+                else:
+                    source_embeddings = self.get_sentence_embeddings(source_input_ids, source_attention_mask,
+                                                                     source_token_type_ids)
+                    target_embeddings = self.get_sentence_embeddings(target_input_ids, target_attention_mask,
+                                                                     target_token_type_ids)
+                    logits = self.concat_embeddings(source_embeddings, target_embeddings)
+                    loss = self.calc_loss(labels, logits)
+
                 current_loss = loss.item()
                 if verbose:
                     batch_iterator.set_description(
                         f"Epoch: {epoch_number + 1}/{num_epochs}, Batch:{step}/{len(train_dataloader)}, Loss: {current_loss:9.4f}")
 
                 if gradient_accumulation_steps > 1:
                     loss = loss / gradient_accumulation_steps
```

### Comparing `text2vec-1.2.1/text2vec/similarity.py` & `text2vec-1.2.2/text2vec/similarity.py`

 * *Files identical despite different names*

### Comparing `text2vec-1.2.1/text2vec/stopwords.txt` & `text2vec-1.2.2/text2vec/stopwords.txt`

 * *Files identical despite different names*

### Comparing `text2vec-1.2.1/text2vec/text_matching_dataset.py` & `text2vec-1.2.2/text2vec/text_matching_dataset.py`

 * *Files identical despite different names*

### Comparing `text2vec-1.2.1/text2vec/utils/distance.py` & `text2vec-1.2.2/text2vec/utils/distance.py`

 * *Files 4% similar despite different names*

```diff
@@ -24,14 +24,15 @@
     ‰ΩôÂº¶Ë∑ùÁ¶ª
     return cos score
     """
     up = float(np.sum(v1 * v2))
     down = np.linalg.norm(v1) * np.linalg.norm(v2)
     return try_divide(up, down)
 
+
 def hamming_distance(v1, v2):
     n = int(v1, 2) ^ int(v2, 2)
     return bin(n & 0xffffffff).count('1')
 
 
 def euclidean_distance(v1, v2):  # Ê¨ßÊ∞èË∑ùÁ¶ª
     return np.sqrt(np.sum(np.square(v1 - v2)))
@@ -41,16 +42,21 @@
     return np.sum(np.abs(v1 - v2))
 
 
 def chebyshev_distance(v1, v2):  # ÂàáÊØîÈõ™Â§´Ë∑ùÁ¶ª
     return np.max(np.abs(v1 - v2))
 
 
-def minkowski_distance(v1, v2):  # ÈóµÂèØÂ§´ÊñØÂü∫Ë∑ùÁ¶ª
-    return np.sqrt(np.sum(np.square(v1 - v2)))
+def minkowski_distance(v1, v2, p=2):
+    """
+    ÈóµÂèØÂ§´ÊñØÂü∫Ë∑ùÁ¶ª
+        p=1 ÊõºÂìàÈ°øË∑ùÁ¶ª
+        p=2 Ê¨ßÊ∞èË∑ùÁ¶ª
+    """
+    return np.power(np.sum(np.power(np.abs(v1 - v2), p)), 1 / p)
 
 
 def euclidean_distance_standardized(v1, v2):  # Ê†áÂáÜÂåñÊ¨ßÊ∞èË∑ùÁ¶ª
     v1_v2 = np.vstack([v1, v2])
     sk_v1_v2 = np.var(v1_v2, axis=0, ddof=1)
     return np.sqrt(((v1 - v2) ** 2 / (sk_v1_v2 + zero_bit * np.ones_like(sk_v1_v2))).sum())
 
@@ -206,15 +212,14 @@
     x = np.array(x).astype(float)
     xr = np.rollaxis(x, axis=axis)
     xr -= np.mean(x, axis=axis)
     xr /= np.std(x, axis=axis)
     return x
 
 
-
 if __name__ == '__main__':
     vec1_test = np.array([1, 38, 17, 32])
     vec2_test = np.array([5, 6, 8, 9])
 
     str1_test = "‰Ω†Âà∞Â∫ïÊòØË∞Å?"
     str2_test = "Ê≤°ÊÉ≥Âà∞ÊàëÊòØË∞ÅÔºåÊòØÁúüÊ†∑Â≠ê"
```

### Comparing `text2vec-1.2.1/text2vec/utils/get_file.py` & `text2vec-1.2.2/text2vec/utils/get_file.py`

 * *Files identical despite different names*

### Comparing `text2vec-1.2.1/text2vec/utils/io_util.py` & `text2vec-1.2.2/text2vec/utils/io_util.py`

 * *Files identical despite different names*

### Comparing `text2vec-1.2.1/text2vec/utils/rank_bm25.py` & `text2vec-1.2.2/text2vec/utils/rank_bm25.py`

 * *Files identical despite different names*

### Comparing `text2vec-1.2.1/text2vec/utils/stats_util.py` & `text2vec-1.2.2/text2vec/utils/stats_util.py`

 * *Files identical despite different names*

### Comparing `text2vec-1.2.1/text2vec/utils/tokenizer.py` & `text2vec-1.2.2/text2vec/utils/tokenizer.py`

 * *Files identical despite different names*

### Comparing `text2vec-1.2.1/text2vec/word2vec.py` & `text2vec-1.2.2/text2vec/word2vec.py`

 * *Files identical despite different names*

### Comparing `text2vec-1.2.1/text2vec.egg-info/PKG-INFO` & `text2vec-1.2.2/text2vec.egg-info/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: text2vec
-Version: 1.2.1
+Version: 1.2.2
 Summary: Text to vector Tool, encode text
 Home-page: https://github.com/shibing624/text2vec
 Author: XuMing
 Author-email: xuming624@qq.com
 License: Apache License 2.0
 Description: [**üá®üá≥‰∏≠Êñá**](https://github.com/shibing624/text2vec/blob/master/README.md) | [**üåêEnglish**](https://github.com/shibing624/text2vec/blob/master/README_EN.md) | [**üìñÊñáÊ°£/Docs**](https://github.com/shibing624/text2vec/wiki) | [**ü§ñÊ®°Âûã/Models**](https://huggingface.co/shibing624) 
         
@@ -27,54 +27,57 @@
         
         
         **Text2vec**: Text to Vector, Get Sentence Embeddings. ÊñáÊú¨ÂêëÈáèÂåñÔºåÊääÊñáÊú¨(ÂåÖÊã¨ËØç„ÄÅÂè•Â≠ê„ÄÅÊÆµËêΩ)Ë°®ÂæÅ‰∏∫ÂêëÈáèÁü©Èòµ„ÄÇ
         
         **text2vec**ÂÆûÁé∞‰∫ÜWord2Vec„ÄÅRankBM25„ÄÅBERT„ÄÅSentence-BERT„ÄÅCoSENTÁ≠âÂ§öÁßçÊñáÊú¨Ë°®ÂæÅ„ÄÅÊñáÊú¨Áõ∏‰ººÂ∫¶ËÆ°ÁÆóÊ®°ÂûãÔºåÂπ∂Âú®ÊñáÊú¨ËØ≠‰πâÂåπÈÖçÔºàÁõ∏‰ººÂ∫¶ËÆ°ÁÆóÔºâ‰ªªÂä°‰∏äÊØîËæÉ‰∫ÜÂêÑÊ®°ÂûãÁöÑÊïàÊûú„ÄÇ
         
         ### News
+        [2023/06/22] v1.2.2ÁâàÊú¨: ÂèëÂ∏É‰∫ÜÂ§öËØ≠Ë®ÄÂåπÈÖçÊ®°Âûã[shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)ÔºåÁî®CoSENTÊñπÊ≥ïËÆ≠ÁªÉÔºåÂü∫‰∫é`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`Áî®‰∫∫Â∑•ÊåëÈÄâÂêéÁöÑÂ§öËØ≠Ë®ÄSTSÊï∞ÊçÆÈõÜ[shibing624/nli-zh-all/text2vec-base-multilingual-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-multilingual-dataset)ËÆ≠ÁªÉÂæóÂà∞ÔºåÂπ∂Âú®‰∏≠Ëã±ÊñáÊµãËØïÈõÜËØÑ‰º∞Áõ∏ÂØπ‰∫éÂéüÊ®°ÂûãÊïàÊûúÊúâÊèêÂçáÔºåËØ¶ËßÅ[Release-v1.2.2](https://github.com/shibing624/text2vec/releases/tag/1.2.2)
+        
         [2023/06/19] v1.2.1ÁâàÊú¨: Êõ¥Êñ∞‰∫Ü‰∏≠ÊñáÂåπÈÖçÊ®°Âûã`shibing624/text2vec-base-chinese-nli`‰∏∫Êñ∞Áâà[shibing624/text2vec-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-sentence)ÔºåÈíàÂØπCoSENTÁöÑlossËÆ°ÁÆóÂØπÊéíÂ∫èÊïèÊÑüÁâπÁÇπÔºå‰∫∫Â∑•ÊåëÈÄâÂπ∂Êï¥ÁêÜÂá∫È´òË¥®ÈáèÁöÑÊúâÁõ∏ÂÖ≥ÊÄßÊéíÂ∫èÁöÑSTSÊï∞ÊçÆÈõÜ[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)ÔºåÂú®ÂêÑËØÑ‰º∞ÈõÜË°®Áé∞Áõ∏ÂØπ‰πãÂâçÊúâÊèêÂçáÔºõÂèëÂ∏É‰∫ÜÈÄÇÁî®‰∫és2pÁöÑ‰∏≠ÊñáÂåπÈÖçÊ®°Âûã[shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase)ÔºåËØ¶ËßÅ[Release-v1.2.1](https://github.com/shibing624/text2vec/releases/tag/1.2.1)
         
         [2023/06/15] v1.2.0ÁâàÊú¨: ÂèëÂ∏É‰∫Ü‰∏≠ÊñáÂåπÈÖçÊ®°Âûã[shibing624/text2vec-base-chinese-nli](https://huggingface.co/shibing624/text2vec-base-chinese-nli)ÔºåÂü∫‰∫é`nghuyong/ernie-3.0-base-zh`Ê®°ÂûãÔºå‰ΩøÁî®‰∫Ü‰∏≠ÊñáNLIÊï∞ÊçÆÈõÜ[shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh)ÂÖ®ÈÉ®ËØ≠ÊñôËÆ≠ÁªÉÁöÑCoSENTÊñáÊú¨ÂåπÈÖçÊ®°ÂûãÔºåÂú®ÂêÑËØÑ‰º∞ÈõÜË°®Áé∞ÊèêÂçáÊòéÊòæÔºåËØ¶ËßÅ[Release-v1.2.0](https://github.com/shibing624/text2vec/releases/tag/1.2.0)
         
         [2022/03/12] v1.1.4ÁâàÊú¨: ÂèëÂ∏É‰∫Ü‰∏≠ÊñáÂåπÈÖçÊ®°Âûã[shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)ÔºåÂü∫‰∫é‰∏≠ÊñáSTSËÆ≠ÁªÉÈõÜËÆ≠ÁªÉÁöÑCoSENTÂåπÈÖçÊ®°Âûã„ÄÇËØ¶ËßÅ[Release-v1.1.4](https://github.com/shibing624/text2vec/releases/tag/1.1.4)
         
         
         **Guide**
-        - [Feature](#Feature)
+        - [Features](#Features)
         - [Evaluation](#Evaluation)
         - [Install](#install)
         - [Usage](#usage)
         - [Contact](#Contact)
-        - [Reference](#reference)
+        - [References](#references)
         
         
-        # Feature
+        ## Features
         ### ÊñáÊú¨ÂêëÈáèË°®Á§∫Ê®°Âûã
         - [Word2Vec](https://github.com/shibing624/text2vec/blob/master/text2vec/word2vec.py)ÔºöÈÄöËøáËÖæËÆØAI LabÂºÄÊ∫êÁöÑÂ§ßËßÑÊ®°È´òË¥®Èáè‰∏≠Êñá[ËØçÂêëÈáèÊï∞ÊçÆÔºà800‰∏á‰∏≠ÊñáËØçËΩªÈáèÁâàÔºâ](https://pan.baidu.com/s/1La4U4XNFe8s5BJqxPQpeiQ) (Êñá‰ª∂ÂêçÔºölight_Tencent_AILab_ChineseEmbedding.bin ÂØÜÁ†Å: taweÔºâÂÆûÁé∞ËØçÂêëÈáèÊ£ÄÁ¥¢ÔºåÊú¨È°πÁõÆÂÆûÁé∞‰∫ÜÂè•Â≠êÔºàËØçÂêëÈáèÊ±ÇÂπ≥ÂùáÔºâÁöÑword2vecÂêëÈáèË°®Á§∫
         - [SBERT(Sentence-BERT)](https://github.com/shibing624/text2vec/blob/master/text2vec/sentencebert_model.py)ÔºöÊùÉË°°ÊÄßËÉΩÂíåÊïàÁéáÁöÑÂè•ÂêëÈáèË°®Á§∫Ê®°ÂûãÔºåËÆ≠ÁªÉÊó∂ÈÄöËøáÊúâÁõëÁù£ËÆ≠ÁªÉ‰∏äÂ±ÇÂàÜÁ±ªÂáΩÊï∞ÔºåÊñáÊú¨ÂåπÈÖçÈ¢ÑÊµãÊó∂Áõ¥Êé•Âè•Â≠êÂêëÈáèÂÅö‰ΩôÂº¶ÔºåÊú¨È°πÁõÆÂü∫‰∫éPyTorchÂ§çÁé∞‰∫ÜSentence-BERTÊ®°ÂûãÁöÑËÆ≠ÁªÉÂíåÈ¢ÑÊµã
         - [CoSENT(Cosine Sentence)](https://github.com/shibing624/text2vec/blob/master/text2vec/cosent_model.py)ÔºöCoSENTÊ®°ÂûãÊèêÂá∫‰∫Ü‰∏ÄÁßçÊéíÂ∫èÁöÑÊçüÂ§±ÂáΩÊï∞Ôºå‰ΩøËÆ≠ÁªÉËøáÁ®ãÊõ¥Ë¥¥ËøëÈ¢ÑÊµãÔºåÊ®°ÂûãÊî∂ÊïõÈÄüÂ∫¶ÂíåÊïàÊûúÊØîSentence-BERTÊõ¥Â•ΩÔºåÊú¨È°πÁõÆÂü∫‰∫éPyTorchÂÆûÁé∞‰∫ÜCoSENTÊ®°ÂûãÁöÑËÆ≠ÁªÉÂíåÈ¢ÑÊµã
         
         ËØ¶ÁªÜÊñáÊú¨ÂêëÈáèË°®Á§∫ÊñπÊ≥ïËßÅwiki: [ÊñáÊú¨ÂêëÈáèË°®Á§∫ÊñπÊ≥ï](https://github.com/shibing624/text2vec/wiki/%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95)
-        # Evaluation
+        ## Evaluation
         
         ÊñáÊú¨ÂåπÈÖç
         
         #### Ëã±ÊñáÂåπÈÖçÊï∞ÊçÆÈõÜÁöÑËØÑÊµãÁªìÊûúÔºö
         
         
-        | Arch   | BaseModel                                        | Model                            | English-STS-B | 
-        |:-------|:------------------------------------------------|:-------------------------------------|:-------------:|
-        | GloVe  | glove                                           | Avg_word_embeddings_glove_6B_300d    |     61.77     |
-        | BERT   | bert-base-uncased                               | BERT-base-cls                        |     20.29     |
-        | BERT   | bert-base-uncased                               | BERT-base-first_last_avg             |     59.04     |
-        | BERT   | bert-base-uncased                               | BERT-base-first_last_avg-whiten(NLI) |     63.65     |
-        | SBERT  | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-cls                   |     73.65     |
-        | SBERT  | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-first_last_avg        |     77.96     |
-        | CoSENT | bert-base-uncased                               | CoSENT-base-first_last_avg           |     69.93     |
-        | CoSENT | sentence-transformers/bert-base-nli-mean-tokens | CoSENT-base-nli-first_last_avg       |     79.68     |
+        | Arch   | BaseModel                                        | Model                                                                                                                | English-STS-B | 
+        |:-------|:------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------|:-------------:|
+        | GloVe  | glove                                           | Avg_word_embeddings_glove_6B_300d                                                                                    |     61.77     |
+        | BERT   | bert-base-uncased                               | BERT-base-cls                                                                                                        |     20.29     |
+        | BERT   | bert-base-uncased                               | BERT-base-first_last_avg                                                                                             |     59.04     |
+        | BERT   | bert-base-uncased                               | BERT-base-first_last_avg-whiten(NLI)                                                                                 |     63.65     |
+        | SBERT  | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-cls                                                                                                   |     73.65     |
+        | SBERT  | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-first_last_avg                                                                                        |     77.96     |
+        | CoSENT | bert-base-uncased                               | CoSENT-base-first_last_avg                                                                                           |     69.93     |
+        | CoSENT | sentence-transformers/bert-base-nli-mean-tokens | CoSENT-base-nli-first_last_avg                                                                                       |     79.68     |
+        | CoSENT | sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 | [shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)                |     80.12     |
         
         #### ‰∏≠ÊñáÂåπÈÖçÊï∞ÊçÆÈõÜÁöÑËØÑÊµãÁªìÊûúÔºö
         
         
         | Arch   | BaseModel                    | Model           | ATEC  |  BQ   | LCQMC | PAWSX | STS-B |  Avg  | 
         |:-------|:----------------------------|:--------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|
         | SBERT  | bert-base-chinese           | SBERT-bert-base     | 46.36 | 70.36 | 78.72 | 46.86 | 66.41 | 61.74 |
@@ -83,59 +86,60 @@
         | CoSENT | bert-base-chinese           | CoSENT-bert-base    | 49.74 | 72.38 | 78.69 | 60.00 | 79.27 | 68.01 |
         | CoSENT | hfl/chinese-macbert-base    | CoSENT-macbert-base | 50.39 | 72.93 | 79.17 | 60.86 | 79.30 | 68.53 |
         | CoSENT | hfl/chinese-roberta-wwm-ext | CoSENT-roberta-ext  | 50.81 | 71.45 | 79.31 | 61.56 | 79.96 | 68.61 |
         
         ËØ¥ÊòéÔºö
         - ÁªìÊûúËØÑÊµãÊåáÊ†áÔºöspearmanÁ≥ªÊï∞
         - ‰∏∫ËØÑÊµãÊ®°ÂûãËÉΩÂäõÔºåÁªìÊûúÂùáÂè™Áî®ËØ•Êï∞ÊçÆÈõÜÁöÑtrainËÆ≠ÁªÉÔºåÂú®test‰∏äËØÑ‰º∞ÂæóÂà∞ÁöÑË°®Áé∞ÔºåÊ≤°Áî®Â§ñÈÉ®Êï∞ÊçÆ
+        - `SBERT-macbert-base`Ê®°ÂûãÔºåÊòØÁî®SBertÊñπÊ≥ïËÆ≠ÁªÉÔºåËøêË°å[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)‰ª£Á†ÅÂèØËÆ≠ÁªÉÊ®°Âûã
+        - `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`Ê®°ÂûãÊòØÁî®SBertËÆ≠ÁªÉÔºåÊòØ`paraphrase-MiniLM-L12-v2`Ê®°ÂûãÁöÑÂ§öËØ≠Ë®ÄÁâàÊú¨ÔºåÊîØÊåÅ‰∏≠Êñá„ÄÅËã±ÊñáÁ≠â
         
         
         ### Release Models
         - Êú¨È°πÁõÆreleaseÊ®°ÂûãÁöÑ‰∏≠ÊñáÂåπÈÖçËØÑÊµãÁªìÊûúÔºö
         
-        | Arch       | BaseModel                         | Model                                                                                                                                             | ATEC  |  BQ   | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-dc |    Avg    |  QPS  |
-        |:-----------|:----------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-------:|:-------:|:---------:|:-----:|
-        | Word2Vec   | word2vec                          | [w2v-light-tencent-chinese](https://ai.tencent.com/ailab/nlp/en/download.html)                                                                    | 20.00 | 31.49 | 59.46 | 2.57  | 55.78 |  55.04  |  20.70  |   35.03   | 23769 |
-        | SBERT      | xlm-roberta-base                  | [sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) | 18.42 | 38.52 | 63.96 | 10.14 | 78.90 |  63.01  |  52.28  |   46.46   | 3138  |
-        | Instructor | hfl/chinese-roberta-wwm-ext       | [moka-ai/m3e-base](https://huggingface.co/moka-ai/m3e-base)                                                                                       | 41.27 | 63.81 | 74.87 | 12.20 | 76.96 |  75.83  |  60.55  |   57.93   | 2980  |
-        | CoSENT     | hfl/chinese-macbert-base          | [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)                                                       | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 |  70.27  |  50.42  |   51.61   | 3008  |
-        | CoSENT     | hfl/chinese-lert-large            | [GanymedeNil/text2vec-large-chinese](https://huggingface.co/GanymedeNil/text2vec-large-chinese)                                                   | 32.61 | 44.59 | 69.30 | 14.51 | 79.44 |  73.01  |  59.04  |   53.12   | 2092  |
-        | CoSENT     | nghuyong/ernie-3.0-base-zh        | [shibing624/text2vec-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-sentence)                                     | 43.37 | 61.43 | 73.48 | 38.90 | 78.25 |  70.60  |  53.08  |   59.87   | 3089  |
-        | CoSENT     | nghuyong/ernie-3.0-base-zh        | [shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase)                                 | 44.89 | 63.58 | 74.24 | 40.90 | 78.93 |  76.70  |  63.30  | **63.08** | 3066  |
+        | Arch       | BaseModel                                                    | Model                                                                                                                                             | ATEC  |  BQ   | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-dc |    Avg    |  QPS  |
+        |:-----------|:-------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-------:|:-------:|:---------:|:-----:|
+        | Word2Vec   | word2vec                                                     | [w2v-light-tencent-chinese](https://ai.tencent.com/ailab/nlp/en/download.html)                                                                    | 20.00 | 31.49 | 59.46 | 2.57  | 55.78 |  55.04  |  20.70  |   35.03   | 23769 |
+        | SBERT      | xlm-roberta-base                                             | [sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) | 18.42 | 38.52 | 63.96 | 10.14 | 78.90 |  63.01  |  52.28  |   46.46   | 3138  |
+        | CoSENT     | hfl/chinese-macbert-base                                     | [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)                                                       | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 |  70.27  |  50.42  |   51.61   | 3008  |
+        | CoSENT     | hfl/chinese-lert-large                                       | [GanymedeNil/text2vec-large-chinese](https://huggingface.co/GanymedeNil/text2vec-large-chinese)                                                   | 32.61 | 44.59 | 69.30 | 14.51 | 79.44 |  73.01  |  59.04  |   53.12   | 2092  |
+        | CoSENT     | nghuyong/ernie-3.0-base-zh                                   | [shibing624/text2vec-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-sentence)                                     | 43.37 | 61.43 | 73.48 | 38.90 | 78.25 |  70.60  |  53.08  |   59.87   | 3089  |
+        | CoSENT     | nghuyong/ernie-3.0-base-zh                                   | [shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase)                                 | 44.89 | 63.58 | 74.24 | 40.90 | 78.93 |  76.70  |  63.30  | **63.08** | 3066  |
+        | CoSENT     | sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2  | [shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)                                             | 32.39 | 50.33 | 65.64 | 32.56 | 74.45 |  68.88  |  51.17  |   53.67   | 3138  |
         
         
         ËØ¥ÊòéÔºö
         - ÁªìÊûúËØÑÊµãÊåáÊ†áÔºöspearmanÁ≥ªÊï∞
-        - Ê®°ÂûãËÆ≠ÁªÉÂÆûÈ™åÊä•ÂëäÔºö[ÂÆûÈ™åÊä•Âëä](https://github.com/shibing624/text2vec/blob/master/docs/model_report.md)
         - `shibing624/text2vec-base-chinese`Ê®°ÂûãÔºåÊòØÁî®CoSENTÊñπÊ≥ïËÆ≠ÁªÉÔºåÂü∫‰∫é`hfl/chinese-macbert-base`Âú®‰∏≠ÊñáSTS-BÊï∞ÊçÆËÆ≠ÁªÉÂæóÂà∞ÔºåÂπ∂Âú®‰∏≠ÊñáSTS-BÊµãËØïÈõÜËØÑ‰º∞ËææÂà∞ËæÉÂ•ΩÊïàÊûúÔºåËøêË°å[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)‰ª£Á†ÅÂèØËÆ≠ÁªÉÊ®°ÂûãÔºåÊ®°ÂûãÊñá‰ª∂Â∑≤Áªè‰∏ä‰º†HF model hubÔºå‰∏≠ÊñáÈÄöÁî®ËØ≠‰πâÂåπÈÖç‰ªªÂä°Êé®Ëçê‰ΩøÁî®
         - `shibing624/text2vec-base-chinese-sentence`Ê®°ÂûãÔºåÊòØÁî®CoSENTÊñπÊ≥ïËÆ≠ÁªÉÔºåÂü∫‰∫é`nghuyong/ernie-3.0-base-zh`Áî®‰∫∫Â∑•ÊåëÈÄâÂêéÁöÑ‰∏≠ÊñáSTSÊï∞ÊçÆÈõÜ[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)ËÆ≠ÁªÉÂæóÂà∞ÔºåÂπ∂Âú®‰∏≠ÊñáÂêÑNLIÊµãËØïÈõÜËØÑ‰º∞ËææÂà∞ËæÉÂ•ΩÊïàÊûúÔºåËøêË°å[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)‰ª£Á†ÅÂèØËÆ≠ÁªÉÊ®°ÂûãÔºåÊ®°ÂûãÊñá‰ª∂Â∑≤Áªè‰∏ä‰º†HF model hubÔºå‰∏≠Êñás2s(Âè•Â≠êvsÂè•Â≠ê)ËØ≠‰πâÂåπÈÖç‰ªªÂä°Êé®Ëçê‰ΩøÁî®
         - `shibing624/text2vec-base-chinese-paraphrase`Ê®°ÂûãÔºåÊòØÁî®CoSENTÊñπÊ≥ïËÆ≠ÁªÉÔºåÂü∫‰∫é`nghuyong/ernie-3.0-base-zh`Áî®‰∫∫Â∑•ÊåëÈÄâÂêéÁöÑ‰∏≠ÊñáSTSÊï∞ÊçÆÈõÜ[shibing624/nli-zh-all/text2vec-base-chinese-paraphrase-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-paraphrase-dataset)ÔºåÊï∞ÊçÆÈõÜÁõ∏ÂØπ‰∫é[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)Âä†ÂÖ•‰∫Üs2p(sentence to paraphrase)Êï∞ÊçÆÔºåÂº∫Âåñ‰∫ÜÂÖ∂ÈïøÊñáÊú¨ÁöÑË°®ÂæÅËÉΩÂäõÔºåÂπ∂Âú®‰∏≠ÊñáÂêÑNLIÊµãËØïÈõÜËØÑ‰º∞ËææÂà∞SOTAÔºåËøêË°å[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)‰ª£Á†ÅÂèØËÆ≠ÁªÉÊ®°ÂûãÔºåÊ®°ÂûãÊñá‰ª∂Â∑≤Áªè‰∏ä‰º†HF model hubÔºå‰∏≠Êñás2p(Âè•Â≠êvsÊÆµËêΩ)ËØ≠‰πâÂåπÈÖç‰ªªÂä°Êé®Ëçê‰ΩøÁî®
-        - `SBERT-macbert-base`Ê®°ÂûãÔºåÊòØÁî®SBERTÊñπÊ≥ïËÆ≠ÁªÉÔºåËøêË°å[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)‰ª£Á†ÅÂèØËÆ≠ÁªÉÊ®°Âûã
-        - `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`Ê®°ÂûãÊòØÁî®SBERTËÆ≠ÁªÉÔºåÊòØ`paraphrase-MiniLM-L12-v2`Ê®°ÂûãÁöÑÂ§öËØ≠Ë®ÄÁâàÊú¨ÔºåÊîØÊåÅ‰∏≠Êñá„ÄÅËã±ÊñáÁ≠â
+        - `shibing624/text2vec-base-multilingual`Ê®°ÂûãÔºåÊòØÁî®CoSENTÊñπÊ≥ïËÆ≠ÁªÉÔºåÂü∫‰∫é`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`Áî®‰∫∫Â∑•ÊåëÈÄâÂêéÁöÑÂ§öËØ≠Ë®ÄSTSÊï∞ÊçÆÈõÜ[shibing624/nli-zh-all/text2vec-base-multilingual-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-multilingual-dataset)ËÆ≠ÁªÉÂæóÂà∞ÔºåÂπ∂Âú®‰∏≠Ëã±ÊñáÊµãËØïÈõÜËØÑ‰º∞Áõ∏ÂØπ‰∫éÂéüÊ®°ÂûãÊïàÊûúÊúâÊèêÂçáÔºåËøêË°å[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)‰ª£Á†ÅÂèØËÆ≠ÁªÉÊ®°ÂûãÔºåÊ®°ÂûãÊñá‰ª∂Â∑≤Áªè‰∏ä‰º†HF model hubÔºåÂ§öËØ≠Ë®ÄËØ≠‰πâÂåπÈÖç‰ªªÂä°Êé®Ëçê‰ΩøÁî®
         - `w2v-light-tencent-chinese`ÊòØËÖæËÆØËØçÂêëÈáèÁöÑWord2VecÊ®°ÂûãÔºåCPUÂä†ËΩΩ‰ΩøÁî®ÔºåÈÄÇÁî®‰∫é‰∏≠ÊñáÂ≠óÈù¢ÂåπÈÖç‰ªªÂä°ÂíåÁº∫Â∞ëÊï∞ÊçÆÁöÑÂÜ∑ÂêØÂä®ÊÉÖÂÜµ
         - ÂêÑÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÂùáÂèØ‰ª•ÈÄöËøátransformersË∞ÉÁî®ÔºåÂ¶ÇMacBERTÊ®°ÂûãÔºö`--model_name hfl/chinese-macbert-base` ÊàñËÄÖrobertaÊ®°ÂûãÔºö`--model_name uer/roberta-medium-wwm-chinese-cluecorpussmall`
         - ‰∏∫ÊµãËØÑÊ®°ÂûãÁöÑÈ≤ÅÊ£íÊÄßÔºåÂä†ÂÖ•‰∫ÜÊú™ËÆ≠ÁªÉËøáÁöÑSOHUÊµãËØïÈõÜÔºåÁî®‰∫éÊµãËØïÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÔºõ‰∏∫ËææÂà∞ÂºÄÁÆ±Âç≥Áî®ÁöÑÂÆûÁî®ÊïàÊûúÔºå‰ΩøÁî®‰∫ÜÊêúÈõÜÂà∞ÁöÑÂêÑ‰∏≠ÊñáÂåπÈÖçÊï∞ÊçÆÈõÜÔºåÊï∞ÊçÆÈõÜ‰πü‰∏ä‰º†Âà∞HF datasets[ÈìæÊé•ËßÅ‰∏ãÊñπ](#Êï∞ÊçÆÈõÜ)
         - ‰∏≠ÊñáÂåπÈÖç‰ªªÂä°ÂÆûÈ™åË°®ÊòéÔºåpoolingÊúÄ‰ºòÊòØ`EncoderType.FIRST_LAST_AVG`Âíå`EncoderType.MEAN`Ôºå‰∏§ËÄÖÈ¢ÑÊµãÊïàÊûúÂ∑ÆÂºÇÂæàÂ∞è
         - ‰∏≠ÊñáÂåπÈÖçËØÑÊµãÁªìÊûúÂ§çÁé∞ÔºåÂèØ‰ª•‰∏ãËΩΩ‰∏≠ÊñáÂåπÈÖçÊï∞ÊçÆÈõÜÂà∞`examples/data`ÔºåËøêË°å[tests/test_model_spearman.py](https://github.com/shibing624/text2vec/blob/master/tests/test_model_spearman.py)‰ª£Á†ÅÂ§çÁé∞ËØÑÊµãÁªìÊûú
         - QPSÁöÑGPUÊµãËØïÁéØÂ¢ÉÊòØTesla V100ÔºåÊòæÂ≠ò32GB
         
-        # Demo
+        Ê®°ÂûãËÆ≠ÁªÉÂÆûÈ™åÊä•ÂëäÔºö[ÂÆûÈ™åÊä•Âëä](https://github.com/shibing624/text2vec/blob/master/docs/model_report.md)
+        ## Demo
         
         Official Demo: https://www.mulanai.com/product/short_text_sim/
         
         HuggingFace Demo: https://huggingface.co/spaces/shibing624/text2vec
         
         ![](docs/hf.png)
         
         run example: [examples/gradio_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/gradio_demo.py) to see the demo:
         ```shell
         python examples/gradio_demo.py
         ```
         
-        # Install
+        ## Install
         ```shell
         pip install torch # conda install pytorch
         pip install -U text2vec
         ```
         
         or
         
@@ -144,17 +148,17 @@
         pip install -r requirements.txt
         
         git clone https://github.com/shibing624/text2vec.git
         cd text2vec
         pip install --no-deps .
         ```
         
-        # Usage
+        ## Usage
         
-        ## ÊñáÊú¨ÂêëÈáèË°®ÂæÅ
+        ### ÊñáÊú¨ÂêëÈáèË°®ÂæÅ
         
         Âü∫‰∫é`pretrained model`ËÆ°ÁÆóÊñáÊú¨ÂêëÈáèÔºö
         
         ```zsh
         >>> from text2vec import SentenceModel
         >>> m = SentenceModel()
         >>> m.encode("Â¶Ç‰ΩïÊõ¥Êç¢Ëä±ÂëóÁªëÂÆöÈì∂Ë°åÂç°")
@@ -194,16 +198,16 @@
         
         
         if __name__ == "__main__":
             # ‰∏≠ÊñáÂè•ÂêëÈáèÊ®°Âûã(CoSENT)Ôºå‰∏≠ÊñáËØ≠‰πâÂåπÈÖç‰ªªÂä°Êé®ËçêÔºåÊîØÊåÅfine-tuneÁªßÁª≠ËÆ≠ÁªÉ
             t2v_model = SentenceModel("shibing624/text2vec-base-chinese")
             compute_emb(t2v_model)
         
-            # ÊîØÊåÅÂ§öËØ≠Ë®ÄÁöÑÂè•ÂêëÈáèÊ®°ÂûãÔºàSentence-BERTÔºâÔºåËã±ÊñáËØ≠‰πâÂåπÈÖç‰ªªÂä°Êé®ËçêÔºåÊîØÊåÅfine-tuneÁªßÁª≠ËÆ≠ÁªÉ
-            sbert_model = SentenceModel("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
+            # ÊîØÊåÅÂ§öËØ≠Ë®ÄÁöÑÂè•ÂêëÈáèÊ®°ÂûãÔºàCoSENTÔºâÔºåÂ§öËØ≠Ë®ÄÔºàÂåÖÊã¨‰∏≠Ëã±ÊñáÔºâËØ≠‰πâÂåπÈÖç‰ªªÂä°Êé®ËçêÔºåÊîØÊåÅfine-tuneÁªßÁª≠ËÆ≠ÁªÉ
+            sbert_model = SentenceModel("shibing624/text2vec-base-multilingual")
             compute_emb(sbert_model)
         
             # ‰∏≠ÊñáËØçÂêëÈáèÊ®°Âûã(word2vec)Ôºå‰∏≠ÊñáÂ≠óÈù¢ÂåπÈÖç‰ªªÂä°ÂíåÂÜ∑ÂêØÂä®ÈÄÇÁî®
             w2v_model = Word2Vec("w2v-light-tencent-chinese")
             compute_emb(w2v_model)
         
         ```
@@ -220,16 +224,14 @@
         ```
         
         - ËøîÂõûÂÄº`embeddings`ÊòØ`numpy.ndarray`Á±ªÂûãÔºåshape‰∏∫`(sentences_size, model_embedding_size)`Ôºå‰∏â‰∏™Ê®°Âûã‰ªªÈÄâ‰∏ÄÁßçÂç≥ÂèØÔºåÊé®ËçêÁî®Á¨¨‰∏Ä‰∏™„ÄÇ
         - `shibing624/text2vec-base-chinese`Ê®°ÂûãÊòØCoSENTÊñπÊ≥ïÂú®‰∏≠ÊñáSTS-BÊï∞ÊçÆÈõÜËÆ≠ÁªÉÂæóÂà∞ÁöÑÔºåÊ®°ÂûãÂ∑≤Áªè‰∏ä‰º†Âà∞huggingfaceÁöÑ
         Ê®°ÂûãÂ∫ì[shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)Ôºå
         ÊòØ`text2vec.SentenceModel`ÊåáÂÆöÁöÑÈªòËÆ§Ê®°ÂûãÔºåÂèØ‰ª•ÈÄöËøá‰∏äÈù¢Á§∫‰æãË∞ÉÁî®ÔºåÊàñËÄÖÂ¶Ç‰∏ãÊâÄÁ§∫Áî®[transformersÂ∫ì](https://github.com/huggingface/transformers)Ë∞ÉÁî®Ôºå
         Ê®°ÂûãËá™Âä®‰∏ãËΩΩÂà∞Êú¨Êú∫Ë∑ØÂæÑÔºö`~/.cache/huggingface/transformers`
-        - `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`Ê®°ÂûãÊòØSentence-BERTÁöÑÂ§öËØ≠Ë®ÄÂè•ÂêëÈáèÊ®°ÂûãÔºå
-        ÈÄÇÁî®‰∫éÈáä‰πâÔºàparaphraseÔºâËØÜÂà´ÔºåÊñáÊú¨ÂåπÈÖçÔºåÈÄöËøá`text2vec.SentenceModel`Âíå[sentence-transformersÂ∫ì]((https://github.com/UKPLab/sentence-transformers))ÈÉΩÂèØ‰ª•Ë∞ÉÁî®ËØ•Ê®°Âûã
         - `w2v-light-tencent-chinese`ÊòØÈÄöËøágensimÂä†ËΩΩÁöÑWord2VecÊ®°ÂûãÔºå‰ΩøÁî®ËÖæËÆØËØçÂêëÈáè`Tencent_AILab_ChineseEmbedding.tar.gz`ËÆ°ÁÆóÂêÑÂ≠óËØçÁöÑËØçÂêëÈáèÔºåÂè•Â≠êÂêëÈáèÈÄöËøáÂçïËØçËØç
         ÂêëÈáèÂèñÂπ≥ÂùáÂÄºÂæóÂà∞ÔºåÊ®°ÂûãËá™Âä®‰∏ãËΩΩÂà∞Êú¨Êú∫Ë∑ØÂæÑÔºö`~/.text2vec/datasets/light_Tencent_AILab_ChineseEmbedding.bin`
         
         #### Usage (HuggingFace Transformers)
         Without [text2vec](https://github.com/shibing624/text2vec), you can use the model like this: 
         
         First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.
@@ -447,17 +449,17 @@
         from similarities import Similarity
         
         m = Similarity()
         r = m.similarity('Â¶Ç‰ΩïÊõ¥Êç¢Ëä±ÂëóÁªëÂÆöÈì∂Ë°åÂç°', 'Ëä±ÂëóÊõ¥ÊîπÁªëÂÆöÈì∂Ë°åÂç°')
         print(f"similarity score: {float(r)}")  # similarity score: 0.855146050453186
         ```
         
-        # Models
+        ## Models
         
-        ## CoSENT model
+        ### CoSENT model
         
         CoSENTÔºàCosine SentenceÔºâÊñáÊú¨ÂåπÈÖçÊ®°ÂûãÔºåÂú®Sentence-BERT‰∏äÊîπËøõ‰∫ÜCosineRankLossÁöÑÂè•ÂêëÈáèÊñπÊ°à
         
         
         Network structure:
         
         Training:
@@ -488,16 +490,22 @@
         python training_sup_text_matching_model.py --task_name ATEC --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base --output_dir ./outputs/ATEC-cosent
         ```
         
         - Âú®Ëá™Êúâ‰∏≠ÊñáÊï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÊ®°Âûã
         
         example: [examples/training_sup_text_matching_model_mydata.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_mydata.py)
         
+        ÂçïÂç°ËÆ≠ÁªÉÔºö
+        ```shell
+        CUDA_VISIBLE_DEVICES=0 python training_sup_text_matching_model_mydata.py --do_train --do_predict
+        ```
+        
+        Â§öÂç°ËÆ≠ÁªÉÔºö
         ```shell
-        python training_sup_text_matching_model_mydata.py --do_train --do_predict
+        CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node 2  training_sup_text_matching_model_mydata.py --do_train --do_predict --output_dir outputs/STS-B-text2vec-macbert-v1 --batch_size 64 --fp16 --data_parallel 
         ```
         
         ËÆ≠ÁªÉÈõÜÊ†ºÂºèÂèÇËÄÉ[examples/data/STS-B/STS-B.valid.data](https://github.com/shibing624/text2vec/blob/master/examples/data/STS-B/STS-B.valid.data)
         
         ```shell
         sentence1   sentence2   label
         ‰∏Ä‰∏™Â•≥Â≠©Âú®ÁªôÂ•πÁöÑÂ§¥ÂèëÂÅöÂèëÂûã„ÄÇ	‰∏Ä‰∏™Â•≥Â≠©Âú®Ê¢≥Â§¥„ÄÇ	2
@@ -524,15 +532,15 @@
         
         ```shell
         cd examples
         python training_unsup_text_matching_model_en.py --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-cosent
         ```
         
         
-        ## Sentence-BERT model
+        ### Sentence-BERT model
         
         Sentence-BERTÊñáÊú¨ÂåπÈÖçÊ®°ÂûãÔºåË°®ÂæÅÂºèÂè•ÂêëÈáèË°®Á§∫ÊñπÊ°à
         
         Network structure:
         
         Training:
         
@@ -567,38 +575,38 @@
         example: [examples/training_unsup_text_matching_model_en.py](https://github.com/shibing624/text2vec/blob/master/examples/training_unsup_text_matching_model_en.py)
         
         ```shell
         cd examples
         python training_unsup_text_matching_model_en.py --model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-sbert
         ```
         
-        ## BERT-Match model
+        ### BERT-Match model
         BERTÊñáÊú¨ÂåπÈÖçÊ®°ÂûãÔºåÂéüÁîüBERTÂåπÈÖçÁΩëÁªúÁªìÊûÑÔºå‰∫§‰∫íÂºèÂè•ÂêëÈáèÂåπÈÖçÊ®°Âûã
         
         Network structure:
         
         Training and inference:
         
         <img src="docs/bert-fc-train.png" width="300" />
         
         ËÆ≠ÁªÉËÑöÊú¨Âêå‰∏ä[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)„ÄÇ
         
         
-        ## Ê®°ÂûãËí∏È¶èÔºàModel DistillationÔºâ
+        ### Ê®°ÂûãËí∏È¶èÔºàModel DistillationÔºâ
         
         Áî±‰∫étext2vecËÆ≠ÁªÉÁöÑÊ®°ÂûãÂèØ‰ª•‰ΩøÁî®[sentence-transformers](https://github.com/UKPLab/sentence-transformers)Â∫ìÂä†ËΩΩÔºåÊ≠§Â§ÑÂ§çÁî®ÂÖ∂Ê®°ÂûãËí∏È¶èÊñπÊ≥ï[distillation](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/distillation)„ÄÇ
         
         1. Ê®°ÂûãÈôçÁª¥ÔºåÂèÇËÄÉ[dimensionality_reduction.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/distillation/dimensionality_reduction.py)‰ΩøÁî®PCAÂØπÊ®°ÂûãËæìÂá∫embeddingÈôçÁª¥ÔºåÂèØÂáèÂ∞ëmilvusÁ≠âÂêëÈáèÊ£ÄÁ¥¢Êï∞ÊçÆÂ∫ìÁöÑÂ≠òÂÇ®ÂéãÂäõÔºåËøòËÉΩËΩªÂæÆÊèêÂçáÊ®°ÂûãÊïàÊûú„ÄÇ
         2. Ê®°ÂûãËí∏È¶èÔºåÂèÇËÄÉ[model_distillation.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/distillation/model_distillation.py)‰ΩøÁî®Ëí∏È¶èÊñπÊ≥ïÔºåÂ∞ÜTeacherÂ§ßÊ®°ÂûãËí∏È¶èÂà∞Êõ¥Â∞ëlayersÂ±ÇÊï∞ÁöÑstudentÊ®°Âûã‰∏≠ÔºåÂú®ÊùÉË°°ÊïàÊûúÁöÑÊÉÖÂÜµ‰∏ãÔºåÂèØÂ§ßÂπÖÊèêÂçáÊ®°ÂûãÈ¢ÑÊµãÈÄüÂ∫¶„ÄÇ
         
-        ## Ê®°ÂûãÈÉ®ÁΩ≤
+        ### Ê®°ÂûãÈÉ®ÁΩ≤
         
         Êèê‰æõ‰∏§ÁßçÈÉ®ÁΩ≤Ê®°ÂûãÔºåÊê≠Âª∫ÊúçÂä°ÁöÑÊñπÊ≥ïÔºö 1ÔºâÂü∫‰∫éJinaÊê≠Âª∫gRPCÊúçÂä°„ÄêÊé®Ëçê„ÄëÔºõ2ÔºâÂü∫‰∫éFastAPIÊê≠Âª∫ÂéüÁîüHttpÊúçÂä°„ÄÇ
         
-        ### JinaÊúçÂä°
+        #### JinaÊúçÂä°
         ÈááÁî®C/SÊ®°ÂºèÊê≠Âª∫È´òÊÄßËÉΩÊúçÂä°ÔºåÊîØÊåÅdocker‰∫ëÂéüÁîüÔºågRPC/HTTP/WebSocketÔºåÊîØÊåÅÂ§ö‰∏™Ê®°ÂûãÂêåÊó∂È¢ÑÊµãÔºåGPUÂ§öÂç°Â§ÑÁêÜ„ÄÇ
         
         - ÂÆâË£ÖÔºö
         ```pip install jina```
         
         - ÂêØÂä®ÊúçÂä°Ôºö
         
@@ -637,15 +645,15 @@
         r = c.post('/', inputs=DocumentArray([Document(text='Â¶Ç‰ΩïÊõ¥Êç¢Ëä±ÂëóÁªëÂÆöÈì∂Ë°åÂç°'), Document(text='Ëä±ÂëóÊõ¥ÊîπÁªëÂÆöÈì∂Ë°åÂç°')]))
         print(r.embeddings)
         ```
         
         ÊâπÈáèË∞ÉÁî®ÊñπÊ≥ïËßÅexample: [examples/jina_client_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/jina_client_demo.py)
         
         
-        ### FastAPIÊúçÂä°
+        #### FastAPIÊúçÂä°
         
         - ÂÆâË£ÖÔºö
         ```pip install fastapi uvicorn```
         
         - ÂêØÂä®ÊúçÂä°Ôºö
         
         example: [examples/fastapi_server_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/fastapi_server_demo.py)
@@ -658,15 +666,15 @@
         ```shell
         curl -X 'GET' \
           'http://0.0.0.0:8001/emb?q=hello' \
           -H 'accept: application/json'
         ```
         
         
-        ## Êï∞ÊçÆÈõÜ
+        ## Dataset
         
         - Êú¨È°πÁõÆreleaseÁöÑÊï∞ÊçÆÈõÜÔºö
         
         | Dataset                    | Introduce                                                                | Download Link                                                                                                                                                                                                                                                                                         |
         |:---------------------------|:-------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
         | shibing624/nli-zh-all      | ‰∏≠ÊñáËØ≠‰πâÂåπÈÖçÊï∞ÊçÆÂêàÈõÜÔºåÊï¥Âêà‰∫ÜÊñáÊú¨Êé®ÁêÜÔºåÁõ∏‰ººÔºåÊëòË¶ÅÔºåÈóÆÁ≠îÔºåÊåá‰ª§ÂæÆË∞ÉÁ≠â‰ªªÂä°ÁöÑ820‰∏áÈ´òË¥®ÈáèÊï∞ÊçÆÔºåÂπ∂ËΩ¨Âåñ‰∏∫ÂåπÈÖçÊ†ºÂºèÊï∞ÊçÆÈõÜ                | [https://huggingface.co/datasets/shibing624/nli-zh-all](https://huggingface.co/datasets/shibing624/nli-zh-all)                                                                                                                                                                                        |
         | shibing624/snli-zh         | ‰∏≠ÊñáSNLIÂíåMultiNLIÊï∞ÊçÆÈõÜÔºåÁøªËØëËá™Ëã±ÊñáSNLIÂíåMultiNLI                                    | [https://huggingface.co/datasets/shibing624/snli-zh](https://huggingface.co/datasets/shibing624/snli-zh)                                                                                                                                                                                              |
@@ -677,16 +685,16 @@
         | LCQMC                      | ‰∏≠ÊñáLCQMC(large-scale Chinese question matching corpus)Êï∞ÊçÆÈõÜÔºåQ-QpairÊï∞ÊçÆÈõÜ      | [LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html)                                                                                                                                                                                                                                               |
         | PAWSX                      | ‰∏≠ÊñáPAWS(Paraphrase Adversaries from Word Scrambling)Êï∞ÊçÆÈõÜÔºåQ-QpairÊï∞ÊçÆÈõÜ        | [PAWSX](https://arxiv.org/abs/1908.11828)                                                                                                                                                                                                                                                             |
         | STS-B                      | ‰∏≠ÊñáSTS-BÊï∞ÊçÆÈõÜÔºå‰∏≠ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÊé®ÁêÜÊï∞ÊçÆÈõÜÔºå‰ªéËã±ÊñáSTS-BÁøªËØë‰∏∫‰∏≠ÊñáÁöÑÊï∞ÊçÆÈõÜ                                 | [STS-B](https://github.com/pluto-junzeng/CNSD)                                                                                                                                                                                                                                                        |
         
         
         Â∏∏Áî®Ëã±ÊñáÂåπÈÖçÊï∞ÊçÆÈõÜÔºö
         
-        - Â§ßÂêçÈºéÈºéÁöÑmulti_nliÂíåsnli: https://huggingface.co/datasets/multi_nli
-        - Â§ßÂêçÈºéÈºéÁöÑmulti_nliÂíåsnli: https://huggingface.co/datasets/snli
+        - Ëã±ÊñáÂåπÈÖçÊï∞ÊçÆÈõÜÔºömulti_nli: https://huggingface.co/datasets/multi_nli
+        - Ëã±ÊñáÂåπÈÖçÊï∞ÊçÆÈõÜÔºösnli: https://huggingface.co/datasets/snli
         - https://huggingface.co/datasets/metaeval/cnli
         - https://huggingface.co/datasets/mteb/stsbenchmark-sts
         - https://huggingface.co/datasets/JeremiahZ/simcse_sup_nli
         - https://huggingface.co/datasets/MoritzLaurer/multilingual-NLI-26lang-2mil7
         
         
         Êï∞ÊçÆÈõÜ‰ΩøÁî®Á§∫‰æãÔºö
@@ -721,59 +729,59 @@
         {'sentence1': '‰∏Ä‰∏™Â•≥Â≠©Âú®ÁªôÂ•πÁöÑÂ§¥ÂèëÂÅöÂèëÂûã„ÄÇ', 'sentence2': '‰∏Ä‰∏™Â•≥Â≠©Âú®Ê¢≥Â§¥„ÄÇ', 'label': 2}
         ```
         
         
         
         
         
-        # Contact
+        ## Contact
         
         - Issue(Âª∫ËÆÆ)Ôºö[![GitHub issues](https://img.shields.io/github/issues/shibing624/text2vec.svg)](https://github.com/shibing624/text2vec/issues)
         - ÈÇÆ‰ª∂ÊàëÔºöxuming: xuming624@qq.com
         - ÂæÆ‰ø°ÊàëÔºöÂä†Êàë*ÂæÆ‰ø°Âè∑Ôºöxuming624, Â§áÊ≥®ÔºöÂßìÂêç-ÂÖ¨Âè∏-NLP* ËøõNLP‰∫§ÊµÅÁæ§„ÄÇ
         
         <img src="docs/wechat.jpeg" width="200" />
         
         
-        # Citation
+        ## Citation
         
         Â¶ÇÊûú‰Ω†Âú®Á†îÁ©∂‰∏≠‰ΩøÁî®‰∫Ütext2vecÔºåËØ∑ÊåâÂ¶Ç‰∏ãÊ†ºÂºèÂºïÁî®Ôºö
         
         APA:
         ```latex
         Xu, M. Text2vec: Text to vector toolkit (Version 1.1.2) [Computer software]. https://github.com/shibing624/text2vec
         ```
         
         BibTeX:
         ```latex
         @misc{Text2vec,
-          author = {Xu, Ming},
+          author = {Ming Xu},
           title = {Text2vec: Text to vector toolkit},
-          year = {2022},
+          year = {2023},
           publisher = {GitHub},
           journal = {GitHub repository},
           howpublished = {\url{https://github.com/shibing624/text2vec}},
         }
         ```
         
-        # License
+        ## License
         
         
         ÊéàÊùÉÂçèËÆÆ‰∏∫ [The Apache License 2.0](LICENSE)ÔºåÂèØÂÖçË¥πÁî®ÂÅöÂïÜ‰∏öÁî®ÈÄî„ÄÇËØ∑Âú®‰∫ßÂìÅËØ¥Êòé‰∏≠ÈôÑÂä†text2vecÁöÑÈìæÊé•ÂíåÊéàÊùÉÂçèËÆÆ„ÄÇ
         
         
-        # Contribute
+        ## Contribute
         È°πÁõÆ‰ª£Á†ÅËøòÂæàÁ≤óÁ≥ôÔºåÂ¶ÇÊûúÂ§ßÂÆ∂ÂØπ‰ª£Á†ÅÊúâÊâÄÊîπËøõÔºåÊ¨¢ËøéÊèê‰∫§ÂõûÊú¨È°πÁõÆÔºåÂú®Êèê‰∫§‰πãÂâçÔºåÊ≥®ÊÑè‰ª•‰∏ã‰∏§ÁÇπÔºö
         
          - Âú®`tests`Ê∑ªÂä†Áõ∏Â∫îÁöÑÂçïÂÖÉÊµãËØï
          - ‰ΩøÁî®`python -m pytest -v`Êù•ËøêË°åÊâÄÊúâÂçïÂÖÉÊµãËØïÔºåÁ°Æ‰øùÊâÄÊúâÂçïÊµãÈÉΩÊòØÈÄöËøáÁöÑ
         
         ‰πãÂêéÂç≥ÂèØÊèê‰∫§PR„ÄÇ
         
-        # Reference
+        ## References
         - [Â∞ÜÂè•Â≠êË°®Á§∫‰∏∫ÂêëÈáèÔºà‰∏äÔºâÔºöÊó†ÁõëÁù£Âè•Â≠êË°®Á§∫Â≠¶‰π†Ôºàsentence embeddingÔºâ](https://www.cnblogs.com/llhthinker/p/10335164.html)
         - [Â∞ÜÂè•Â≠êË°®Á§∫‰∏∫ÂêëÈáèÔºà‰∏ãÔºâÔºöÊó†ÁõëÁù£Âè•Â≠êË°®Á§∫Â≠¶‰π†Ôºàsentence embeddingÔºâ](https://www.cnblogs.com/llhthinker/p/10341841.html)
         - [A Simple but Tough-to-Beat Baseline for Sentence Embeddings[Sanjeev Arora and Yingyu Liang and Tengyu Ma, 2017]](https://openreview.net/forum?id=SyK00v5xx)
         - [ÂõõÁßçËÆ°ÁÆóÊñáÊú¨Áõ∏‰ººÂ∫¶ÁöÑÊñπÊ≥ïÂØπÊØî[Yves Peirsman]](https://zhuanlan.zhihu.com/p/37104535)
         - [Improvements to BM25 and Language Models Examined](http://www.cs.otago.ac.nz/homepages/andrew/papers/2014-2.pdf)
         - [CoSENTÔºöÊØîSentence-BERTÊõ¥ÊúâÊïàÁöÑÂè•ÂêëÈáèÊñπÊ°à](https://kexue.fm/archives/8847)
         - [Ë∞àË∞àÊñáÊú¨ÂåπÈÖçÂíåÂ§öËΩÆÊ£ÄÁ¥¢](https://zhuanlan.zhihu.com/p/111769969)
```

#### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: text2vec Version: 1.2.1 Summary: Text to vector
+Metadata-Version: 2.1 Name: text2vec Version: 1.2.2 Summary: Text to vector
 Tool, encode text Home-page: https://github.com/shibing624/text2vec Author:
 XuMing Author-email: xuming624@qq.com License: Apache License 2.0 Description:
 [**√∞¬ü¬á¬®√∞¬ü¬á¬≥√§¬∏¬≠√¶¬ñ¬á**](https://github.com/shibing624/text2vec/blob/master/
 README.md) | [**√∞¬ü¬å¬êEnglish**](https://github.com/shibing624/text2vec/blob/
 master/README_EN.md) | [**√∞¬ü¬ì¬ñ√¶¬ñ¬á√¶¬°¬£/Docs**](https://github.com/shibing624/
 text2vec/wiki) | [**√∞¬ü¬§¬ñ√¶¬®¬°√•¬û¬ã/Models**](https://huggingface.co/shibing624)
                                     [Logo]
@@ -17,17 +17,26 @@
 shibing624/text2vec.svg)](https://github.com/shibing624/text2vec/issues) [!
 [Wechat Group](http://vlog.sfyc.ltd/wechat_everyday/
 wxgroup_logo.png?imageView2/0/w/60/h/20)](#Contact) **Text2vec**: Text to
 Vector, Get Sentence Embeddings. √¶¬ñ¬á√¶¬ú¬¨√•¬ê¬ë√©¬á¬è√•¬å¬ñ√Ø¬º¬å√¶¬ä¬ä√¶¬ñ¬á√¶¬ú¬¨
 (√•¬å¬Ö√¶¬ã¬¨√®¬Ø¬ç√£¬Ä¬Å√•¬è¬•√•¬≠¬ê√£¬Ä¬Å√¶¬Æ¬µ√®¬ê¬Ω)√®¬°¬®√•¬æ¬Å√§¬∏¬∫√•¬ê¬ë√©¬á¬è√ß¬ü¬©√©¬ò¬µ√£¬Ä¬Ç
 **text2vec**√•¬Æ¬û√ß¬é¬∞√§¬∫¬ÜWord2Vec√£¬Ä¬ÅRankBM25√£¬Ä¬ÅBERT√£¬Ä¬ÅSentence-
 BERT√£¬Ä¬ÅCoSENT√ß¬≠¬â√•¬§¬ö√ß¬ß¬ç√¶¬ñ¬á√¶¬ú¬¨√®¬°¬®√•¬æ¬Å√£¬Ä¬Å√¶¬ñ¬á√¶¬ú¬¨√ß¬õ¬∏√§¬º¬º√•¬∫¬¶√®¬Æ¬°√ß¬Æ¬ó√¶¬®¬°√•¬û¬ã√Ø¬º¬å√•¬π¬∂√•¬ú¬®√¶¬ñ¬á√¶¬ú¬¨√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√Ø¬º¬à√ß¬õ¬∏√§¬º¬º√•¬∫¬¶√®¬Æ¬°√ß¬Æ¬ó√Ø¬º¬â√§¬ª¬ª√•¬ä¬°√§¬∏¬ä√¶¬Ø¬î√®¬æ¬É√§¬∫¬Ü√•¬ê¬Ñ√¶¬®¬°√•¬û¬ã√ß¬ö¬Ñ√¶¬ï¬à√¶¬û¬ú√£¬Ä¬Ç
-### News [2023/06/19] v1.2.1√ß¬â¬à√¶¬ú¬¨: √¶¬õ¬¥√¶¬ñ¬∞√§¬∫¬Ü√§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã`shibing624/
-text2vec-base-chinese-nli`√§¬∏¬∫√¶¬ñ¬∞√ß¬â¬à[shibing624/text2vec-base-chinese-sentence]
-(https://huggingface.co/shibing624/text2vec-base-chinese-
+### News [2023/06/22] v1.2.2√ß¬â¬à√¶¬ú¬¨: √•¬è¬ë√•¬∏¬É√§¬∫¬Ü√•¬§¬ö√®¬Ø¬≠√®¬®¬Ä√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã[shibing624/
+text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-
+multilingual)√Ø¬º¬å√ß¬î¬®CoSENT√¶¬ñ¬π√¶¬≥¬ï√®¬Æ¬≠√ß¬ª¬É√Ø¬º¬å√•¬ü¬∫√§¬∫¬é`sentence-transformers/
+paraphrase-multilingual-MiniLM-L12-
+v2`√ß¬î¬®√§¬∫¬∫√•¬∑¬•√¶¬å¬ë√©¬Ä¬â√•¬ê¬é√ß¬ö¬Ñ√•¬§¬ö√®¬Ø¬≠√®¬®¬ÄSTS√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü[shibing624/nli-zh-all/text2vec-
+base-multilingual-dataset](https://huggingface.co/datasets/shibing624/nli-zh-
+all/tree/main/text2vec-base-multilingual-
+dataset)√®¬Æ¬≠√ß¬ª¬É√•¬æ¬ó√•¬à¬∞√Ø¬º¬å√•¬π¬∂√•¬ú¬®√§¬∏¬≠√®¬ã¬±√¶¬ñ¬á√¶¬µ¬ã√®¬Ø¬ï√©¬õ¬Ü√®¬Ø¬Ñ√§¬º¬∞√ß¬õ¬∏√•¬Ø¬π√§¬∫¬é√•¬é¬ü√¶¬®¬°√•¬û¬ã√¶¬ï¬à√¶¬û¬ú√¶¬ú¬â√¶¬è¬ê√•¬ç¬á√Ø¬º¬å√®¬Ø¬¶√®¬ß¬Å
+[Release-v1.2.2](https://github.com/shibing624/text2vec/releases/tag/1.2.2)
+[2023/06/19] v1.2.1√ß¬â¬à√¶¬ú¬¨: √¶¬õ¬¥√¶¬ñ¬∞√§¬∫¬Ü√§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã`shibing624/text2vec-
+base-chinese-nli`√§¬∏¬∫√¶¬ñ¬∞√ß¬â¬à[shibing624/text2vec-base-chinese-sentence](https://
+huggingface.co/shibing624/text2vec-base-chinese-
 sentence)√Ø¬º¬å√©¬í¬à√•¬Ø¬πCoSENT√ß¬ö¬Ñloss√®¬Æ¬°√ß¬Æ¬ó√•¬Ø¬π√¶¬é¬í√•¬∫¬è√¶¬ï¬è√¶¬Ñ¬ü√ß¬â¬π√ß¬Ç¬π√Ø¬º¬å√§¬∫¬∫√•¬∑¬•√¶¬å¬ë√©¬Ä¬â√•¬π¬∂√¶¬ï¬¥√ß¬ê¬Ü√•¬á¬∫√©¬´¬ò√®¬¥¬®√©¬á¬è√ß¬ö¬Ñ√¶¬ú¬â√ß¬õ¬∏√•¬Ö¬≥√¶¬Ä¬ß√¶¬é¬í√•¬∫¬è√ß¬ö¬ÑSTS√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü
 [shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://
 huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-
 sentence-
 dataset)√Ø¬º¬å√•¬ú¬®√•¬ê¬Ñ√®¬Ø¬Ñ√§¬º¬∞√©¬õ¬Ü√®¬°¬®√ß¬é¬∞√ß¬õ¬∏√•¬Ø¬π√§¬π¬ã√•¬â¬ç√¶¬ú¬â√¶¬è¬ê√•¬ç¬á√Ø¬º¬õ√•¬è¬ë√•¬∏¬É√§¬∫¬Ü√©¬Ä¬Ç√ß¬î¬®√§¬∫¬és2p√ß¬ö¬Ñ√§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã
 [shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/
 shibing624/text2vec-base-chinese-paraphrase)√Ø¬º¬å√®¬Ø¬¶√®¬ß¬Å[Release-v1.2.1](https://
@@ -38,17 +47,17 @@
 huggingface.co/datasets/shibing624/
 nli_zh)√•¬Ö¬®√©¬É¬®√®¬Ø¬≠√¶¬ñ¬ô√®¬Æ¬≠√ß¬ª¬É√ß¬ö¬ÑCoSENT√¶¬ñ¬á√¶¬ú¬¨√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã√Ø¬º¬å√•¬ú¬®√•¬ê¬Ñ√®¬Ø¬Ñ√§¬º¬∞√©¬õ¬Ü√®¬°¬®√ß¬é¬∞√¶¬è¬ê√•¬ç¬á√¶¬ò¬é√¶¬ò¬æ√Ø¬º¬å√®¬Ø¬¶√®¬ß¬Å
 [Release-v1.2.0](https://github.com/shibing624/text2vec/releases/tag/1.2.0)
 [2022/03/12] v1.1.4√ß¬â¬à√¶¬ú¬¨: √•¬è¬ë√•¬∏¬É√§¬∫¬Ü√§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã[shibing624/text2vec-
 base-chinese](https://huggingface.co/shibing624/text2vec-base-
 chinese)√Ø¬º¬å√•¬ü¬∫√§¬∫¬é√§¬∏¬≠√¶¬ñ¬áSTS√®¬Æ¬≠√ß¬ª¬É√©¬õ¬Ü√®¬Æ¬≠√ß¬ª¬É√ß¬ö¬ÑCoSENT√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã√£¬Ä¬Ç√®¬Ø¬¶√®¬ß¬Å
 [Release-v1.1.4](https://github.com/shibing624/text2vec/releases/tag/1.1.4)
-**Guide** - [Feature](#Feature) - [Evaluation](#Evaluation) - [Install]
-(#install) - [Usage](#usage) - [Contact](#Contact) - [Reference](#reference) #
-Feature ### √¶¬ñ¬á√¶¬ú¬¨√•¬ê¬ë√©¬á¬è√®¬°¬®√ß¬§¬∫√¶¬®¬°√•¬û¬ã - [Word2Vec](https://github.com/
+**Guide** - [Features](#Features) - [Evaluation](#Evaluation) - [Install]
+(#install) - [Usage](#usage) - [Contact](#Contact) - [References](#references)
+## Features ### √¶¬ñ¬á√¶¬ú¬¨√•¬ê¬ë√©¬á¬è√®¬°¬®√ß¬§¬∫√¶¬®¬°√•¬û¬ã - [Word2Vec](https://github.com/
 shibing624/text2vec/blob/master/text2vec/word2vec.py)√Ø¬º¬ö√©¬Ä¬ö√®¬ø¬á√®¬Ö¬æ√®¬Æ¬ØAI
 Lab√•¬º¬Ä√¶¬∫¬ê√ß¬ö¬Ñ√•¬§¬ß√®¬ß¬Ñ√¶¬®¬°√©¬´¬ò√®¬¥¬®√©¬á¬è√§¬∏¬≠√¶¬ñ¬á
 [√®¬Ø¬ç√•¬ê¬ë√©¬á¬è√¶¬ï¬∞√¶¬ç¬Æ√Ø¬º¬à800√§¬∏¬á√§¬∏¬≠√¶¬ñ¬á√®¬Ø¬ç√®¬Ω¬ª√©¬á¬è√ß¬â¬à√Ø¬º¬â](https://pan.baidu.com/s/
 1La4U4XNFe8s5BJqxPQpeiQ) (√¶¬ñ¬á√§¬ª¬∂√•¬ê¬ç√Ø¬º¬ölight_Tencent_AILab_ChineseEmbedding.bin
 √•¬Ø¬Ü√ß¬†¬Å:
 tawe√Ø¬º¬â√•¬Æ¬û√ß¬é¬∞√®¬Ø¬ç√•¬ê¬ë√©¬á¬è√¶¬£¬Ä√ß¬¥¬¢√Ø¬º¬å√¶¬ú¬¨√©¬°¬π√ß¬õ¬Æ√•¬Æ¬û√ß¬é¬∞√§¬∫¬Ü√•¬è¬•√•¬≠¬ê√Ø¬º¬à√®¬Ø¬ç√•¬ê¬ë√©¬á¬è√¶¬±¬Ç√•¬π¬≥√•¬ù¬á√Ø¬º¬â√ß¬ö¬Ñword2vec√•¬ê¬ë√©¬á¬è√®¬°¬®√ß¬§¬∫
 - [SBERT(Sentence-BERT)](https://github.com/shibing624/text2vec/blob/master/
@@ -56,67 +65,76 @@
 sentencebert_model.py)√Ø¬º¬ö√¶¬ù¬É√®¬°¬°√¶¬Ä¬ß√®¬É¬Ω√•¬í¬å√¶¬ï¬à√ß¬é¬á√ß¬ö¬Ñ√•¬è¬•√•¬ê¬ë√©¬á¬è√®¬°¬®√ß¬§¬∫√¶¬®¬°√•¬û¬ã√Ø¬º¬å√®¬Æ¬≠√ß¬ª¬É√¶¬ó¬∂√©¬Ä¬ö√®¬ø¬á√¶¬ú¬â√ß¬õ¬ë√ß¬ù¬£√®¬Æ¬≠√ß¬ª¬É√§¬∏¬ä√•¬±¬Ç√•¬à¬Ü√ß¬±¬ª√•¬á¬Ω√¶¬ï¬∞√Ø¬º¬å√¶¬ñ¬á√¶¬ú¬¨√•¬å¬π√©¬Ö¬ç√©¬¢¬Ñ√¶¬µ¬ã√¶¬ó¬∂√ß¬õ¬¥√¶¬é¬•√•¬è¬•√•¬≠¬ê√•¬ê¬ë√©¬á¬è√•¬Å¬ö√§¬Ω¬ô√•¬º¬¶√Ø¬º¬å√¶¬ú¬¨√©¬°¬π√ß¬õ¬Æ√•¬ü¬∫√§¬∫¬éPyTorch√•¬§¬ç√ß¬é¬∞√§¬∫¬ÜSentence-
 BERT√¶¬®¬°√•¬û¬ã√ß¬ö¬Ñ√®¬Æ¬≠√ß¬ª¬É√•¬í¬å√©¬¢¬Ñ√¶¬µ¬ã - [CoSENT(Cosine Sentence)](https://github.com/
 shibing624/text2vec/blob/master/text2vec/
 cosent_model.py)√Ø¬º¬öCoSENT√¶¬®¬°√•¬û¬ã√¶¬è¬ê√•¬á¬∫√§¬∫¬Ü√§¬∏¬Ä√ß¬ß¬ç√¶¬é¬í√•¬∫¬è√ß¬ö¬Ñ√¶¬ç¬ü√•¬§¬±√•¬á¬Ω√¶¬ï¬∞√Ø¬º¬å√§¬Ω¬ø√®¬Æ¬≠√ß¬ª¬É√®¬ø¬á√ß¬®¬ã√¶¬õ¬¥√®¬¥¬¥√®¬ø¬ë√©¬¢¬Ñ√¶¬µ¬ã√Ø¬º¬å√¶¬®¬°√•¬û¬ã√¶¬î¬∂√¶¬ï¬õ√©¬Ä¬ü√•¬∫¬¶√•¬í¬å√¶¬ï¬à√¶¬û¬ú√¶¬Ø¬îSentence-
 BERT√¶¬õ¬¥√•¬•¬Ω√Ø¬º¬å√¶¬ú¬¨√©¬°¬π√ß¬õ¬Æ√•¬ü¬∫√§¬∫¬éPyTorch√•¬Æ¬û√ß¬é¬∞√§¬∫¬ÜCoSENT√¶¬®¬°√•¬û¬ã√ß¬ö¬Ñ√®¬Æ¬≠√ß¬ª¬É√•¬í¬å√©¬¢¬Ñ√¶¬µ¬ã
 √®¬Ø¬¶√ß¬ª¬Ü√¶¬ñ¬á√¶¬ú¬¨√•¬ê¬ë√©¬á¬è√®¬°¬®√ß¬§¬∫√¶¬ñ¬π√¶¬≥¬ï√®¬ß¬Åwiki: [√¶¬ñ¬á√¶¬ú¬¨√•¬ê¬ë√©¬á¬è√®¬°¬®√ß¬§¬∫√¶¬ñ¬π√¶¬≥¬ï](https://
 github.com/shibing624/text2vec/wiki/
-%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95) #
+%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95) ##
 Evaluation √¶¬ñ¬á√¶¬ú¬¨√•¬å¬π√©¬Ö¬ç #### √®¬ã¬±√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√ß¬ö¬Ñ√®¬Ø¬Ñ√¶¬µ¬ã√ß¬ª¬ì√¶¬û¬ú√Ø¬º¬ö | Arch |
 BaseModel | Model | English-STS-B | |:-------|:--------------------------------
-----------------|:-------------------------------------|:-------------:| |
+----------------|:-------------------------------------------------------------
+--------------------------------------------------------|:-------------:| |
 GloVe | glove | Avg_word_embeddings_glove_6B_300d | 61.77 | | BERT | bert-base-
 uncased | BERT-base-cls | 20.29 | | BERT | bert-base-uncased | BERT-base-
 first_last_avg | 59.04 | | BERT | bert-base-uncased | BERT-base-first_last_avg-
 whiten(NLI) | 63.65 | | SBERT | sentence-transformers/bert-base-nli-mean-tokens
 | SBERT-base-nli-cls | 73.65 | | SBERT | sentence-transformers/bert-base-nli-
 mean-tokens | SBERT-base-nli-first_last_avg | 77.96 | | CoSENT | bert-base-
 uncased | CoSENT-base-first_last_avg | 69.93 | | CoSENT | sentence-
 transformers/bert-base-nli-mean-tokens | CoSENT-base-nli-first_last_avg | 79.68
-| #### √§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√ß¬ö¬Ñ√®¬Ø¬Ñ√¶¬µ¬ã√ß¬ª¬ì√¶¬û¬ú√Ø¬º¬ö | Arch | BaseModel | Model |
-ATEC | BQ | LCQMC | PAWSX | STS-B | Avg | |:-------|:--------------------------
---|:--------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:| |
-SBERT | bert-base-chinese | SBERT-bert-base | 46.36 | 70.36 | 78.72 | 46.86 |
-66.41 | 61.74 | | SBERT | hfl/chinese-macbert-base | SBERT-macbert-base | 47.28
-| 68.63 | 79.42 | 55.59 | 64.82 | 63.15 | | SBERT | hfl/chinese-roberta-wwm-ext
-| SBERT-roberta-ext | 48.29 | 69.99 | 79.22 | 44.10 | 72.42 | 62.80 | | CoSENT
-| bert-base-chinese | CoSENT-bert-base | 49.74 | 72.38 | 78.69 | 60.00 | 79.27
-| 68.01 | | CoSENT | hfl/chinese-macbert-base | CoSENT-macbert-base | 50.39 |
-72.93 | 79.17 | 60.86 | 79.30 | 68.53 | | CoSENT | hfl/chinese-roberta-wwm-ext
-| CoSENT-roberta-ext | 50.81 | 71.45 | 79.31 | 61.56 | 79.96 | 68.61 |
-√®¬Ø¬¥√¶¬ò¬é√Ø¬º¬ö - √ß¬ª¬ì√¶¬û¬ú√®¬Ø¬Ñ√¶¬µ¬ã√¶¬å¬á√¶¬†¬á√Ø¬º¬öspearman√ß¬≥¬ª√¶¬ï¬∞ -
+| | CoSENT | sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 |
+[shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/
+text2vec-base-multilingual) | 80.12 | ####
+√§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√ß¬ö¬Ñ√®¬Ø¬Ñ√¶¬µ¬ã√ß¬ª¬ì√¶¬û¬ú√Ø¬º¬ö | Arch | BaseModel | Model | ATEC | BQ
+| LCQMC | PAWSX | STS-B | Avg | |:-------|:----------------------------|:------
+--------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:| | SBERT | bert-
+base-chinese | SBERT-bert-base | 46.36 | 70.36 | 78.72 | 46.86 | 66.41 | 61.74
+| | SBERT | hfl/chinese-macbert-base | SBERT-macbert-base | 47.28 | 68.63 |
+79.42 | 55.59 | 64.82 | 63.15 | | SBERT | hfl/chinese-roberta-wwm-ext | SBERT-
+roberta-ext | 48.29 | 69.99 | 79.22 | 44.10 | 72.42 | 62.80 | | CoSENT | bert-
+base-chinese | CoSENT-bert-base | 49.74 | 72.38 | 78.69 | 60.00 | 79.27 | 68.01
+| | CoSENT | hfl/chinese-macbert-base | CoSENT-macbert-base | 50.39 | 72.93 |
+79.17 | 60.86 | 79.30 | 68.53 | | CoSENT | hfl/chinese-roberta-wwm-ext |
+CoSENT-roberta-ext | 50.81 | 71.45 | 79.31 | 61.56 | 79.96 | 68.61 | √®¬Ø¬¥√¶¬ò¬é√Ø¬º¬ö
+- √ß¬ª¬ì√¶¬û¬ú√®¬Ø¬Ñ√¶¬µ¬ã√¶¬å¬á√¶¬†¬á√Ø¬º¬öspearman√ß¬≥¬ª√¶¬ï¬∞ -
 √§¬∏¬∫√®¬Ø¬Ñ√¶¬µ¬ã√¶¬®¬°√•¬û¬ã√®¬É¬Ω√•¬ä¬õ√Ø¬º¬å√ß¬ª¬ì√¶¬û¬ú√•¬ù¬á√•¬è¬™√ß¬î¬®√®¬Ø¬•√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√ß¬ö¬Ñtrain√®¬Æ¬≠√ß¬ª¬É√Ø¬º¬å√•¬ú¬®test√§¬∏¬ä√®¬Ø¬Ñ√§¬º¬∞√•¬æ¬ó√•¬à¬∞√ß¬ö¬Ñ√®¬°¬®√ß¬é¬∞√Ø¬º¬å√¶¬≤¬°√ß¬î¬®√•¬§¬ñ√©¬É¬®√¶¬ï¬∞√¶¬ç¬Æ
-### Release Models - √¶¬ú¬¨√©¬°¬π√ß¬õ¬Ærelease√¶¬®¬°√•¬û¬ã√ß¬ö¬Ñ√§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√®¬Ø¬Ñ√¶¬µ¬ã√ß¬ª¬ì√¶¬û¬ú√Ø¬º¬ö |
-Arch | BaseModel | Model | ATEC | BQ | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-
-dc | Avg | QPS | |:-----------|:----------------------------------|:-----------
+- `SBERT-macbert-base`√¶¬®¬°√•¬û¬ã√Ø¬º¬å√¶¬ò¬Ø√ß¬î¬®SBert√¶¬ñ¬π√¶¬≥¬ï√®¬Æ¬≠√ß¬ª¬É√Ø¬º¬å√®¬ø¬ê√®¬°¬å[examples/
+training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/
+blob/master/examples/training_sup_text_matching_model.py)√§¬ª¬£√ß¬†¬Å√•¬è¬Ø√®¬Æ¬≠√ß¬ª¬É√¶¬®¬°√•¬û¬ã
+- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-
+v2`√¶¬®¬°√•¬û¬ã√¶¬ò¬Ø√ß¬î¬®SBert√®¬Æ¬≠√ß¬ª¬É√Ø¬º¬å√¶¬ò¬Ø`paraphrase-MiniLM-L12-
+v2`√¶¬®¬°√•¬û¬ã√ß¬ö¬Ñ√•¬§¬ö√®¬Ø¬≠√®¬®¬Ä√ß¬â¬à√¶¬ú¬¨√Ø¬º¬å√¶¬î¬Ø√¶¬å¬Å√§¬∏¬≠√¶¬ñ¬á√£¬Ä¬Å√®¬ã¬±√¶¬ñ¬á√ß¬≠¬â ### Release Models -
+√¶¬ú¬¨√©¬°¬π√ß¬õ¬Ærelease√¶¬®¬°√•¬û¬ã√ß¬ö¬Ñ√§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√®¬Ø¬Ñ√¶¬µ¬ã√ß¬ª¬ì√¶¬û¬ú√Ø¬º¬ö | Arch | BaseModel | Model
+| ATEC | BQ | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-dc | Avg | QPS | |:-------
+----|:-------------------------------------------------------------|:----------
 -------------------------------------------------------------------------------
---------------------------------------------------------|:-----:|:-----:|:----
+---------------------------------------------------------|:-----:|:-----:|:----
 -:|:-----:|:-----:|:-------:|:-------:|:---------:|:-----:| | Word2Vec |
 word2vec | [w2v-light-tencent-chinese](https://ai.tencent.com/ailab/nlp/en/
 download.html) | 20.00 | 31.49 | 59.46 | 2.57 | 55.78 | 55.04 | 20.70 | 35.03 |
 23769 | | SBERT | xlm-roberta-base | [sentence-transformers/paraphrase-
 multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/
 paraphrase-multilingual-MiniLM-L12-v2) | 18.42 | 38.52 | 63.96 | 10.14 | 78.90
-| 63.01 | 52.28 | 46.46 | 3138 | | Instructor | hfl/chinese-roberta-wwm-ext |
-[moka-ai/m3e-base](https://huggingface.co/moka-ai/m3e-base) | 41.27 | 63.81 |
-74.87 | 12.20 | 76.96 | 75.83 | 60.55 | 57.93 | 2980 | | CoSENT | hfl/chinese-
-macbert-base | [shibing624/text2vec-base-chinese](https://huggingface.co/
-shibing624/text2vec-base-chinese) | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 |
-70.27 | 50.42 | 51.61 | 3008 | | CoSENT | hfl/chinese-lert-large |
-[GanymedeNil/text2vec-large-chinese](https://huggingface.co/GanymedeNil/
-text2vec-large-chinese) | 32.61 | 44.59 | 69.30 | 14.51 | 79.44 | 73.01 | 59.04
-| 53.12 | 2092 | | CoSENT | nghuyong/ernie-3.0-base-zh | [shibing624/text2vec-
-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-
-sentence) | 43.37 | 61.43 | 73.48 | 38.90 | 78.25 | 70.60 | 53.08 | 59.87 |
-3089 | | CoSENT | nghuyong/ernie-3.0-base-zh | [shibing624/text2vec-base-
-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-
-paraphrase) | 44.89 | 63.58 | 74.24 | 40.90 | 78.93 | 76.70 | 63.30 | **63.08**
-| 3066 | √®¬Ø¬¥√¶¬ò¬é√Ø¬º¬ö - √ß¬ª¬ì√¶¬û¬ú√®¬Ø¬Ñ√¶¬µ¬ã√¶¬å¬á√¶¬†¬á√Ø¬º¬öspearman√ß¬≥¬ª√¶¬ï¬∞ -
-√¶¬®¬°√•¬û¬ã√®¬Æ¬≠√ß¬ª¬É√•¬Æ¬û√©¬™¬å√¶¬ä¬•√•¬ë¬ä√Ø¬º¬ö[√•¬Æ¬û√©¬™¬å√¶¬ä¬•√•¬ë¬ä](https://github.com/shibing624/
-text2vec/blob/master/docs/model_report.md) - `shibing624/text2vec-base-
+| 63.01 | 52.28 | 46.46 | 3138 | | CoSENT | hfl/chinese-macbert-base |
+[shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-
+base-chinese) | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 | 70.27 | 50.42 | 51.61 |
+3008 | | CoSENT | hfl/chinese-lert-large | [GanymedeNil/text2vec-large-chinese]
+(https://huggingface.co/GanymedeNil/text2vec-large-chinese) | 32.61 | 44.59 |
+69.30 | 14.51 | 79.44 | 73.01 | 59.04 | 53.12 | 2092 | | CoSENT | nghuyong/
+ernie-3.0-base-zh | [shibing624/text2vec-base-chinese-sentence](https://
+huggingface.co/shibing624/text2vec-base-chinese-sentence) | 43.37 | 61.43 |
+73.48 | 38.90 | 78.25 | 70.60 | 53.08 | 59.87 | 3089 | | CoSENT | nghuyong/
+ernie-3.0-base-zh | [shibing624/text2vec-base-chinese-paraphrase](https://
+huggingface.co/shibing624/text2vec-base-chinese-paraphrase) | 44.89 | 63.58 |
+74.24 | 40.90 | 78.93 | 76.70 | 63.30 | **63.08** | 3066 | | CoSENT | sentence-
+transformers/paraphrase-multilingual-MiniLM-L12-v2 | [shibing624/text2vec-base-
+multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual) |
+32.39 | 50.33 | 65.64 | 32.56 | 74.45 | 68.88 | 51.17 | 53.67 | 3138 |
+√®¬Ø¬¥√¶¬ò¬é√Ø¬º¬ö - √ß¬ª¬ì√¶¬û¬ú√®¬Ø¬Ñ√¶¬µ¬ã√¶¬å¬á√¶¬†¬á√Ø¬º¬öspearman√ß¬≥¬ª√¶¬ï¬∞ - `shibing624/text2vec-base-
 chinese`√¶¬®¬°√•¬û¬ã√Ø¬º¬å√¶¬ò¬Ø√ß¬î¬®CoSENT√¶¬ñ¬π√¶¬≥¬ï√®¬Æ¬≠√ß¬ª¬É√Ø¬º¬å√•¬ü¬∫√§¬∫¬é`hfl/chinese-macbert-
 base`√•¬ú¬®√§¬∏¬≠√¶¬ñ¬áSTS-B√¶¬ï¬∞√¶¬ç¬Æ√®¬Æ¬≠√ß¬ª¬É√•¬æ¬ó√•¬à¬∞√Ø¬º¬å√•¬π¬∂√•¬ú¬®√§¬∏¬≠√¶¬ñ¬áSTS-
 B√¶¬µ¬ã√®¬Ø¬ï√©¬õ¬Ü√®¬Ø¬Ñ√§¬º¬∞√®¬æ¬æ√•¬à¬∞√®¬æ¬É√•¬•¬Ω√¶¬ï¬à√¶¬û¬ú√Ø¬º¬å√®¬ø¬ê√®¬°¬å[examples/
 training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/
 blob/master/examples/
 training_sup_text_matching_model.py)√§¬ª¬£√ß¬†¬Å√•¬è¬Ø√®¬Æ¬≠√ß¬ª¬É√¶¬®¬°√•¬û¬ã√Ø¬º¬å√¶¬®¬°√•¬û¬ã√¶¬ñ¬á√§¬ª¬∂√•¬∑¬≤√ß¬ª¬è√§¬∏¬ä√§¬º¬†HF
 model hub√Ø¬º¬å√§¬∏¬≠√¶¬ñ¬á√©¬Ä¬ö√ß¬î¬®√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√¶¬é¬®√®¬ç¬ê√§¬Ω¬ø√ß¬î¬® - `shibing624/text2vec-
@@ -137,81 +155,83 @@
 [shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://
 huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-
 sentence-dataset)√•¬ä¬†√•¬Ö¬•√§¬∫¬Üs2p(sentence to
 paraphrase)√¶¬ï¬∞√¶¬ç¬Æ√Ø¬º¬å√•¬º¬∫√•¬å¬ñ√§¬∫¬Ü√•¬Ö¬∂√©¬ï¬ø√¶¬ñ¬á√¶¬ú¬¨√ß¬ö¬Ñ√®¬°¬®√•¬æ¬Å√®¬É¬Ω√•¬ä¬õ√Ø¬º¬å√•¬π¬∂√•¬ú¬®√§¬∏¬≠√¶¬ñ¬á√•¬ê¬ÑNLI√¶¬µ¬ã√®¬Ø¬ï√©¬õ¬Ü√®¬Ø¬Ñ√§¬º¬∞√®¬æ¬æ√•¬à¬∞SOTA√Ø¬º¬å√®¬ø¬ê√®¬°¬å
 [examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/
 shibing624/text2vec/blob/master/examples/
 training_sup_text_matching_model_jsonl_data.py)√§¬ª¬£√ß¬†¬Å√•¬è¬Ø√®¬Æ¬≠√ß¬ª¬É√¶¬®¬°√•¬û¬ã√Ø¬º¬å√¶¬®¬°√•¬û¬ã√¶¬ñ¬á√§¬ª¬∂√•¬∑¬≤√ß¬ª¬è√§¬∏¬ä√§¬º¬†HF
-model hub√Ø¬º¬å√§¬∏¬≠√¶¬ñ¬ás2p(√•¬è¬•√•¬≠¬êvs√¶¬Æ¬µ√®¬ê¬Ω)√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√¶¬é¬®√®¬ç¬ê√§¬Ω¬ø√ß¬î¬® - `SBERT-
-macbert-base`√¶¬®¬°√•¬û¬ã√Ø¬º¬å√¶¬ò¬Ø√ß¬î¬®SBERT√¶¬ñ¬π√¶¬≥¬ï√®¬Æ¬≠√ß¬ª¬É√Ø¬º¬å√®¬ø¬ê√®¬°¬å[examples/
-training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/
-blob/master/examples/training_sup_text_matching_model.py)√§¬ª¬£√ß¬†¬Å√•¬è¬Ø√®¬Æ¬≠√ß¬ª¬É√¶¬®¬°√•¬û¬ã
-- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-
-v2`√¶¬®¬°√•¬û¬ã√¶¬ò¬Ø√ß¬î¬®SBERT√®¬Æ¬≠√ß¬ª¬É√Ø¬º¬å√¶¬ò¬Ø`paraphrase-MiniLM-L12-
-v2`√¶¬®¬°√•¬û¬ã√ß¬ö¬Ñ√•¬§¬ö√®¬Ø¬≠√®¬®¬Ä√ß¬â¬à√¶¬ú¬¨√Ø¬º¬å√¶¬î¬Ø√¶¬å¬Å√§¬∏¬≠√¶¬ñ¬á√£¬Ä¬Å√®¬ã¬±√¶¬ñ¬á√ß¬≠¬â - `w2v-light-tencent-
+model hub√Ø¬º¬å√§¬∏¬≠√¶¬ñ¬ás2p(√•¬è¬•√•¬≠¬êvs√¶¬Æ¬µ√®¬ê¬Ω)√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√¶¬é¬®√®¬ç¬ê√§¬Ω¬ø√ß¬î¬® -
+`shibing624/text2vec-base-
+multilingual`√¶¬®¬°√•¬û¬ã√Ø¬º¬å√¶¬ò¬Ø√ß¬î¬®CoSENT√¶¬ñ¬π√¶¬≥¬ï√®¬Æ¬≠√ß¬ª¬É√Ø¬º¬å√•¬ü¬∫√§¬∫¬é`sentence-transformers/
+paraphrase-multilingual-MiniLM-L12-
+v2`√ß¬î¬®√§¬∫¬∫√•¬∑¬•√¶¬å¬ë√©¬Ä¬â√•¬ê¬é√ß¬ö¬Ñ√•¬§¬ö√®¬Ø¬≠√®¬®¬ÄSTS√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü[shibing624/nli-zh-all/text2vec-
+base-multilingual-dataset](https://huggingface.co/datasets/shibing624/nli-zh-
+all/tree/main/text2vec-base-multilingual-
+dataset)√®¬Æ¬≠√ß¬ª¬É√•¬æ¬ó√•¬à¬∞√Ø¬º¬å√•¬π¬∂√•¬ú¬®√§¬∏¬≠√®¬ã¬±√¶¬ñ¬á√¶¬µ¬ã√®¬Ø¬ï√©¬õ¬Ü√®¬Ø¬Ñ√§¬º¬∞√ß¬õ¬∏√•¬Ø¬π√§¬∫¬é√•¬é¬ü√¶¬®¬°√•¬û¬ã√¶¬ï¬à√¶¬û¬ú√¶¬ú¬â√¶¬è¬ê√•¬ç¬á√Ø¬º¬å√®¬ø¬ê√®¬°¬å
+[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/
+shibing624/text2vec/blob/master/examples/
+training_sup_text_matching_model_jsonl_data.py)√§¬ª¬£√ß¬†¬Å√•¬è¬Ø√®¬Æ¬≠√ß¬ª¬É√¶¬®¬°√•¬û¬ã√Ø¬º¬å√¶¬®¬°√•¬û¬ã√¶¬ñ¬á√§¬ª¬∂√•¬∑¬≤√ß¬ª¬è√§¬∏¬ä√§¬º¬†HF
+model hub√Ø¬º¬å√•¬§¬ö√®¬Ø¬≠√®¬®¬Ä√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√¶¬é¬®√®¬ç¬ê√§¬Ω¬ø√ß¬î¬® - `w2v-light-tencent-
 chinese`√¶¬ò¬Ø√®¬Ö¬æ√®¬Æ¬Ø√®¬Ø¬ç√•¬ê¬ë√©¬á¬è√ß¬ö¬ÑWord2Vec√¶¬®¬°√•¬û¬ã√Ø¬º¬åCPU√•¬ä¬†√®¬Ω¬Ω√§¬Ω¬ø√ß¬î¬®√Ø¬º¬å√©¬Ä¬Ç√ß¬î¬®√§¬∫¬é√§¬∏¬≠√¶¬ñ¬á√•¬≠¬ó√©¬ù¬¢√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√•¬í¬å√ß¬º¬∫√•¬∞¬ë√¶¬ï¬∞√¶¬ç¬Æ√ß¬ö¬Ñ√•¬Ü¬∑√•¬ê¬Ø√•¬ä¬®√¶¬É¬Ö√•¬Ü¬µ
 - √•¬ê¬Ñ√©¬¢¬Ñ√®¬Æ¬≠√ß¬ª¬É√¶¬®¬°√•¬û¬ã√•¬ù¬á√•¬è¬Ø√§¬ª¬•√©¬Ä¬ö√®¬ø¬átransformers√®¬∞¬É√ß¬î¬®√Ø¬º¬å√•¬¶¬ÇMacBERT√¶¬®¬°√•¬û¬ã√Ø¬º¬ö`--
 model_name hfl/chinese-macbert-base` √¶¬à¬ñ√®¬Ä¬Öroberta√¶¬®¬°√•¬û¬ã√Ø¬º¬ö`--model_name uer/
 roberta-medium-wwm-chinese-cluecorpussmall` -
 √§¬∏¬∫√¶¬µ¬ã√®¬Ø¬Ñ√¶¬®¬°√•¬û¬ã√ß¬ö¬Ñ√©¬≤¬Å√¶¬£¬í√¶¬Ä¬ß√Ø¬º¬å√•¬ä¬†√•¬Ö¬•√§¬∫¬Ü√¶¬ú¬™√®¬Æ¬≠√ß¬ª¬É√®¬ø¬á√ß¬ö¬ÑSOHU√¶¬µ¬ã√®¬Ø¬ï√©¬õ¬Ü√Ø¬º¬å√ß¬î¬®√§¬∫¬é√¶¬µ¬ã√®¬Ø¬ï√¶¬®¬°√•¬û¬ã√ß¬ö¬Ñ√¶¬≥¬õ√•¬å¬ñ√®¬É¬Ω√•¬ä¬õ√Ø¬º¬õ√§¬∏¬∫√®¬æ¬æ√•¬à¬∞√•¬º¬Ä√ß¬Æ¬±√•¬ç¬≥√ß¬î¬®√ß¬ö¬Ñ√•¬Æ¬û√ß¬î¬®√¶¬ï¬à√¶¬û¬ú√Ø¬º¬å√§¬Ω¬ø√ß¬î¬®√§¬∫¬Ü√¶¬ê¬ú√©¬õ¬Ü√•¬à¬∞√ß¬ö¬Ñ√•¬ê¬Ñ√§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬å√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√§¬π¬ü√§¬∏¬ä√§¬º¬†√•¬à¬∞HF
 datasets[√©¬ì¬æ√¶¬é¬•√®¬ß¬Å√§¬∏¬ã√¶¬ñ¬π](#√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü) -
 √§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√•¬Æ¬û√©¬™¬å√®¬°¬®√¶¬ò¬é√Ø¬º¬åpooling√¶¬ú¬Ä√§¬º¬ò√¶¬ò¬Ø`EncoderType.FIRST_LAST_AVG`√•¬í¬å`EncoderType.MEAN`√Ø¬º¬å√§¬∏¬§√®¬Ä¬Ö√©¬¢¬Ñ√¶¬µ¬ã√¶¬ï¬à√¶¬û¬ú√•¬∑¬Æ√•¬º¬Ç√•¬æ¬à√•¬∞¬è
 -
 √§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√®¬Ø¬Ñ√¶¬µ¬ã√ß¬ª¬ì√¶¬û¬ú√•¬§¬ç√ß¬é¬∞√Ø¬º¬å√•¬è¬Ø√§¬ª¬•√§¬∏¬ã√®¬Ω¬Ω√§¬∏¬≠√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√•¬à¬∞`examples/
 data`√Ø¬º¬å√®¬ø¬ê√®¬°¬å[tests/test_model_spearman.py](https://github.com/shibing624/
 text2vec/blob/master/tests/test_model_spearman.py)√§¬ª¬£√ß¬†¬Å√•¬§¬ç√ß¬é¬∞√®¬Ø¬Ñ√¶¬µ¬ã√ß¬ª¬ì√¶¬û¬ú -
-QPS√ß¬ö¬ÑGPU√¶¬µ¬ã√®¬Ø¬ï√ß¬é¬Ø√•¬¢¬É√¶¬ò¬ØTesla V100√Ø¬º¬å√¶¬ò¬æ√•¬≠¬ò32GB # Demo Official Demo: https://
-www.mulanai.com/product/short_text_sim/ HuggingFace Demo: https://
-huggingface.co/spaces/shibing624/text2vec ![](docs/hf.png) run example:
-[examples/gradio_demo.py](https://github.com/shibing624/text2vec/blob/master/
-examples/gradio_demo.py) to see the demo: ```shell python examples/
-gradio_demo.py ``` # Install ```shell pip install torch # conda install pytorch
-pip install -U text2vec ``` or ```shell pip install torch # conda install
-pytorch pip install -r requirements.txt git clone https://github.com/
-shibing624/text2vec.git cd text2vec pip install --no-deps . ``` # Usage ##
-√¶¬ñ¬á√¶¬ú¬¨√•¬ê¬ë√©¬á¬è√®¬°¬®√•¬æ¬Å √•¬ü¬∫√§¬∫¬é`pretrained model`√®¬Æ¬°√ß¬Æ¬ó√¶¬ñ¬á√¶¬ú¬¨√•¬ê¬ë√©¬á¬è√Ø¬º¬ö ```zsh >>>
-from text2vec import SentenceModel >>> m = SentenceModel() >>> m.encode
-("√•¬¶¬Ç√§¬Ω¬ï√¶¬õ¬¥√¶¬ç¬¢√®¬ä¬±√•¬ë¬ó√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°") Embedding shape: (768,) ``` example:
-[examples/computing_embeddings_demo.py](https://github.com/shibing624/text2vec/
-blob/master/examples/computing_embeddings_demo.py) ```python import sys
-sys.path.append('..') from text2vec import SentenceModel from text2vec import
-Word2Vec def compute_emb(model): # Embed a list of sentences sentences =
-[ '√•¬ç¬°', '√©¬ì¬∂√®¬°¬å√•¬ç¬°', '√•¬¶¬Ç√§¬Ω¬ï√¶¬õ¬¥√¶¬ç¬¢√®¬ä¬±√•¬ë¬ó√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°',
-'√®¬ä¬±√•¬ë¬ó√¶¬õ¬¥√¶¬î¬π√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°', 'This framework generates embeddings for each
-input sentence', 'Sentences are passed as a list of string.', 'The quick brown
-fox jumps over the lazy dog.' ] sentence_embeddings = model.encode(sentences)
-print(type(sentence_embeddings), sentence_embeddings.shape) # The result is a
-list of sentence embeddings as numpy arrays for sentence, embedding in zip
-(sentences, sentence_embeddings): print("Sentence:", sentence) print("Embedding
-shape:", embedding.shape) print("Embedding head:", embedding[:10]) print() if
-__name__ == "__main__": # √§¬∏¬≠√¶¬ñ¬á√•¬è¬•√•¬ê¬ë√©¬á¬è√¶¬®¬°√•¬û¬ã
-(CoSENT)√Ø¬º¬å√§¬∏¬≠√¶¬ñ¬á√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√¶¬é¬®√®¬ç¬ê√Ø¬º¬å√¶¬î¬Ø√¶¬å¬Åfine-tune√ß¬ª¬ß√ß¬ª¬≠√®¬Æ¬≠√ß¬ª¬É
-t2v_model = SentenceModel("shibing624/text2vec-base-chinese") compute_emb
-(t2v_model) # √¶¬î¬Ø√¶¬å¬Å√•¬§¬ö√®¬Ø¬≠√®¬®¬Ä√ß¬ö¬Ñ√•¬è¬•√•¬ê¬ë√©¬á¬è√¶¬®¬°√•¬û¬ã√Ø¬º¬àSentence-
-BERT√Ø¬º¬â√Ø¬º¬å√®¬ã¬±√¶¬ñ¬á√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√¶¬é¬®√®¬ç¬ê√Ø¬º¬å√¶¬î¬Ø√¶¬å¬Åfine-tune√ß¬ª¬ß√ß¬ª¬≠√®¬Æ¬≠√ß¬ª¬É
-sbert_model = SentenceModel("sentence-transformers/paraphrase-multilingual-
-MiniLM-L12-v2") compute_emb(sbert_model) # √§¬∏¬≠√¶¬ñ¬á√®¬Ø¬ç√•¬ê¬ë√©¬á¬è√¶¬®¬°√•¬û¬ã
+QPS√ß¬ö¬ÑGPU√¶¬µ¬ã√®¬Ø¬ï√ß¬é¬Ø√•¬¢¬É√¶¬ò¬ØTesla V100√Ø¬º¬å√¶¬ò¬æ√•¬≠¬ò32GB √¶¬®¬°√•¬û¬ã√®¬Æ¬≠√ß¬ª¬É√•¬Æ¬û√©¬™¬å√¶¬ä¬•√•¬ë¬ä√Ø¬º¬ö
+[√•¬Æ¬û√©¬™¬å√¶¬ä¬•√•¬ë¬ä](https://github.com/shibing624/text2vec/blob/master/docs/
+model_report.md) ## Demo Official Demo: https://www.mulanai.com/product/
+short_text_sim/ HuggingFace Demo: https://huggingface.co/spaces/shibing624/
+text2vec ![](docs/hf.png) run example: [examples/gradio_demo.py](https://
+github.com/shibing624/text2vec/blob/master/examples/gradio_demo.py) to see the
+demo: ```shell python examples/gradio_demo.py ``` ## Install ```shell pip
+install torch # conda install pytorch pip install -U text2vec ``` or ```shell
+pip install torch # conda install pytorch pip install -r requirements.txt git
+clone https://github.com/shibing624/text2vec.git cd text2vec pip install --no-
+deps . ``` ## Usage ### √¶¬ñ¬á√¶¬ú¬¨√•¬ê¬ë√©¬á¬è√®¬°¬®√•¬æ¬Å √•¬ü¬∫√§¬∫¬é`pretrained
+model`√®¬Æ¬°√ß¬Æ¬ó√¶¬ñ¬á√¶¬ú¬¨√•¬ê¬ë√©¬á¬è√Ø¬º¬ö ```zsh >>> from text2vec import SentenceModel >>> m
+= SentenceModel() >>> m.encode("√•¬¶¬Ç√§¬Ω¬ï√¶¬õ¬¥√¶¬ç¬¢√®¬ä¬±√•¬ë¬ó√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°") Embedding
+shape: (768,) ``` example: [examples/computing_embeddings_demo.py](https://
+github.com/shibing624/text2vec/blob/master/examples/
+computing_embeddings_demo.py) ```python import sys sys.path.append('..') from
+text2vec import SentenceModel from text2vec import Word2Vec def compute_emb
+(model): # Embed a list of sentences sentences = [ '√•¬ç¬°', '√©¬ì¬∂√®¬°¬å√•¬ç¬°',
+'√•¬¶¬Ç√§¬Ω¬ï√¶¬õ¬¥√¶¬ç¬¢√®¬ä¬±√•¬ë¬ó√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°', '√®¬ä¬±√•¬ë¬ó√¶¬õ¬¥√¶¬î¬π√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°', 'This
+framework generates embeddings for each input sentence', 'Sentences are passed
+as a list of string.', 'The quick brown fox jumps over the lazy dog.' ]
+sentence_embeddings = model.encode(sentences) print(type(sentence_embeddings),
+sentence_embeddings.shape) # The result is a list of sentence embeddings as
+numpy arrays for sentence, embedding in zip(sentences, sentence_embeddings):
+print("Sentence:", sentence) print("Embedding shape:", embedding.shape) print
+("Embedding head:", embedding[:10]) print() if __name__ == "__main__": #
+√§¬∏¬≠√¶¬ñ¬á√•¬è¬•√•¬ê¬ë√©¬á¬è√¶¬®¬°√•¬û¬ã(CoSENT)√Ø¬º¬å√§¬∏¬≠√¶¬ñ¬á√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√¶¬é¬®√®¬ç¬ê√Ø¬º¬å√¶¬î¬Ø√¶¬å¬Åfine-
+tune√ß¬ª¬ß√ß¬ª¬≠√®¬Æ¬≠√ß¬ª¬É t2v_model = SentenceModel("shibing624/text2vec-base-chinese")
+compute_emb(t2v_model) #
+√¶¬î¬Ø√¶¬å¬Å√•¬§¬ö√®¬Ø¬≠√®¬®¬Ä√ß¬ö¬Ñ√•¬è¬•√•¬ê¬ë√©¬á¬è√¶¬®¬°√•¬û¬ã√Ø¬º¬àCoSENT√Ø¬º¬â√Ø¬º¬å√•¬§¬ö√®¬Ø¬≠√®¬®¬Ä√Ø¬º¬à√•¬å¬Ö√¶¬ã¬¨√§¬∏¬≠√®¬ã¬±√¶¬ñ¬á√Ø¬º¬â√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√¶¬é¬®√®¬ç¬ê√Ø¬º¬å√¶¬î¬Ø√¶¬å¬Åfine-
+tune√ß¬ª¬ß√ß¬ª¬≠√®¬Æ¬≠√ß¬ª¬É sbert_model = SentenceModel("shibing624/text2vec-base-
+multilingual") compute_emb(sbert_model) # √§¬∏¬≠√¶¬ñ¬á√®¬Ø¬ç√•¬ê¬ë√©¬á¬è√¶¬®¬°√•¬û¬ã
 (word2vec)√Ø¬º¬å√§¬∏¬≠√¶¬ñ¬á√•¬≠¬ó√©¬ù¬¢√•¬å¬π√©¬Ö¬ç√§¬ª¬ª√•¬ä¬°√•¬í¬å√•¬Ü¬∑√•¬ê¬Ø√•¬ä¬®√©¬Ä¬Ç√ß¬î¬® w2v_model = Word2Vec
 ("w2v-light-tencent-chinese") compute_emb(w2v_model) ``` output: ```
 numpy.ndarray'> (7, 768) Sentence: √•¬ç¬° Embedding shape: (768,) Sentence:
 √©¬ì¬∂√®¬°¬å√•¬ç¬° Embedding shape: (768,) ... ``` -
 √®¬ø¬î√•¬õ¬û√•¬Ä¬º`embeddings`√¶¬ò¬Ø`numpy.ndarray`√ß¬±¬ª√•¬û¬ã√Ø¬º¬åshape√§¬∏¬∫`(sentences_size,
 model_embedding_size)`√Ø¬º¬å√§¬∏¬â√§¬∏¬™√¶¬®¬°√•¬û¬ã√§¬ª¬ª√©¬Ä¬â√§¬∏¬Ä√ß¬ß¬ç√•¬ç¬≥√•¬è¬Ø√Ø¬º¬å√¶¬é¬®√®¬ç¬ê√ß¬î¬®√ß¬¨¬¨√§¬∏¬Ä√§¬∏¬™√£¬Ä¬Ç
 - `shibing624/text2vec-base-chinese`√¶¬®¬°√•¬û¬ã√¶¬ò¬ØCoSENT√¶¬ñ¬π√¶¬≥¬ï√•¬ú¬®√§¬∏¬≠√¶¬ñ¬áSTS-
 B√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√®¬Æ¬≠√ß¬ª¬É√•¬æ¬ó√•¬à¬∞√ß¬ö¬Ñ√Ø¬º¬å√¶¬®¬°√•¬û¬ã√•¬∑¬≤√ß¬ª¬è√§¬∏¬ä√§¬º¬†√•¬à¬∞huggingface√ß¬ö¬Ñ √¶¬®¬°√•¬û¬ã√•¬∫¬ì
 [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-
 base-chinese)√Ø¬º¬å
 √¶¬ò¬Ø`text2vec.SentenceModel`√¶¬å¬á√•¬Æ¬ö√ß¬ö¬Ñ√©¬ª¬ò√®¬Æ¬§√¶¬®¬°√•¬û¬ã√Ø¬º¬å√•¬è¬Ø√§¬ª¬•√©¬Ä¬ö√®¬ø¬á√§¬∏¬ä√©¬ù¬¢√ß¬§¬∫√§¬æ¬ã√®¬∞¬É√ß¬î¬®√Ø¬º¬å√¶¬à¬ñ√®¬Ä¬Ö√•¬¶¬Ç√§¬∏¬ã√¶¬â¬Ä√ß¬§¬∫√ß¬î¬®
 [transformers√•¬∫¬ì](https://github.com/huggingface/transformers)√®¬∞¬É√ß¬î¬®√Ø¬º¬å
-√¶¬®¬°√•¬û¬ã√®¬á¬™√•¬ä¬®√§¬∏¬ã√®¬Ω¬Ω√•¬à¬∞√¶¬ú¬¨√¶¬ú¬∫√®¬∑¬Ø√•¬æ¬Ñ√Ø¬º¬ö`~/.cache/huggingface/transformers` -
-`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`√¶¬®¬°√•¬û¬ã√¶¬ò¬ØSentence-
-BERT√ß¬ö¬Ñ√•¬§¬ö√®¬Ø¬≠√®¬®¬Ä√•¬è¬•√•¬ê¬ë√©¬á¬è√¶¬®¬°√•¬û¬ã√Ø¬º¬å
-√©¬Ä¬Ç√ß¬î¬®√§¬∫¬é√©¬á¬ä√§¬π¬â√Ø¬º¬àparaphrase√Ø¬º¬â√®¬Ø¬Ü√•¬à¬´√Ø¬º¬å√¶¬ñ¬á√¶¬ú¬¨√•¬å¬π√©¬Ö¬ç√Ø¬º¬å√©¬Ä¬ö√®¬ø¬á`text2vec.SentenceModel`√•¬í¬å
-[sentence-transformers√•¬∫¬ì]((https://github.com/UKPLab/sentence-
-transformers))√©¬É¬Ω√•¬è¬Ø√§¬ª¬•√®¬∞¬É√ß¬î¬®√®¬Ø¬•√¶¬®¬°√•¬û¬ã - `w2v-light-tencent-
+√¶¬®¬°√•¬û¬ã√®¬á¬™√•¬ä¬®√§¬∏¬ã√®¬Ω¬Ω√•¬à¬∞√¶¬ú¬¨√¶¬ú¬∫√®¬∑¬Ø√•¬æ¬Ñ√Ø¬º¬ö`~/.cache/huggingface/transformers` - `w2v-
+light-tencent-
 chinese`√¶¬ò¬Ø√©¬Ä¬ö√®¬ø¬ágensim√•¬ä¬†√®¬Ω¬Ω√ß¬ö¬ÑWord2Vec√¶¬®¬°√•¬û¬ã√Ø¬º¬å√§¬Ω¬ø√ß¬î¬®√®¬Ö¬æ√®¬Æ¬Ø√®¬Ø¬ç√•¬ê¬ë√©¬á¬è`Tencent_AILab_ChineseEmbedding.tar.gz`√®¬Æ¬°√ß¬Æ¬ó√•¬ê¬Ñ√•¬≠¬ó√®¬Ø¬ç√ß¬ö¬Ñ√®¬Ø¬ç√•¬ê¬ë√©¬á¬è√Ø¬º¬å√•¬è¬•√•¬≠¬ê√•¬ê¬ë√©¬á¬è√©¬Ä¬ö√®¬ø¬á√•¬ç¬ï√®¬Ø¬ç√®¬Ø¬ç
 √•¬ê¬ë√©¬á¬è√•¬è¬ñ√•¬π¬≥√•¬ù¬á√•¬Ä¬º√•¬æ¬ó√•¬à¬∞√Ø¬º¬å√¶¬®¬°√•¬û¬ã√®¬á¬™√•¬ä¬®√§¬∏¬ã√®¬Ω¬Ω√•¬à¬∞√¶¬ú¬¨√¶¬ú¬∫√®¬∑¬Ø√•¬æ¬Ñ√Ø¬º¬ö`~/.text2vec/
 datasets/light_Tencent_AILab_ChineseEmbedding.bin` #### Usage (HuggingFace
 Transformers) Without [text2vec](https://github.com/shibing624/text2vec), you
 can use the model like this: First, you pass your input through the transformer
 model, then you have to apply the right pooling-operation on-top of the
 contextualized word embeddings. example: [examples/
@@ -322,16 +342,16 @@
 √¶¬ñ¬á√¶¬ú¬¨√ß¬õ¬∏√§¬º¬º√•¬∫¬¶√®¬Æ¬°√ß¬Æ¬ó√•¬í¬å√¶¬ñ¬á√¶¬ú¬¨√•¬å¬π√©¬Ö¬ç√¶¬ê¬ú√ß¬¥¬¢√§¬ª¬ª√•¬ä¬°√Ø¬º¬å√¶¬é¬®√®¬ç¬ê√§¬Ω¬ø√ß¬î¬®
 [similarities√•¬∫¬ì](https://github.com/shibing624/similarities)
 √Ø¬º¬å√•¬Ö¬º√•¬Æ¬π√¶¬ú¬¨√©¬°¬π√ß¬õ¬Ærelease√ß¬ö¬Ñ
 Word2vec√£¬Ä¬ÅSBERT√£¬Ä¬ÅCosent√ß¬±¬ª√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã√Ø¬º¬å√®¬ø¬ò√¶¬î¬Ø√¶¬å¬Å√•¬≠¬ó√©¬ù¬¢√ß¬ª¬¥√•¬∫¬¶√ß¬õ¬∏√§¬º¬º√•¬∫¬¶√®¬Æ¬°√ß¬Æ¬ó√£¬Ä¬Å√•¬å¬π√©¬Ö¬ç√¶¬ê¬ú√ß¬¥¬¢√ß¬Æ¬ó√¶¬≥¬ï√Ø¬º¬å√¶¬î¬Ø√¶¬å¬Å√¶¬ñ¬á√¶¬ú¬¨√£¬Ä¬Å√•¬õ¬æ√•¬É¬è√£¬Ä¬Ç
 √•¬Æ¬â√®¬£¬Ö√Ø¬º¬ö ```pip install -U similarities``` √•¬è¬•√•¬≠¬ê√ß¬õ¬∏√§¬º¬º√•¬∫¬¶√®¬Æ¬°√ß¬Æ¬ó√Ø¬º¬ö ```python
 from similarities import Similarity m = Similarity() r = m.similarity
 ('√•¬¶¬Ç√§¬Ω¬ï√¶¬õ¬¥√¶¬ç¬¢√®¬ä¬±√•¬ë¬ó√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°', '√®¬ä¬±√•¬ë¬ó√¶¬õ¬¥√¶¬î¬π√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°') print
-(f"similarity score: {float(r)}") # similarity score: 0.855146050453186 ``` #
-Models ## CoSENT model CoSENT√Ø¬º¬àCosine
+(f"similarity score: {float(r)}") # similarity score: 0.855146050453186 ``` ##
+Models ### CoSENT model CoSENT√Ø¬º¬àCosine
 Sentence√Ø¬º¬â√¶¬ñ¬á√¶¬ú¬¨√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã√Ø¬º¬å√•¬ú¬®Sentence-
 BERT√§¬∏¬ä√¶¬î¬π√®¬ø¬õ√§¬∫¬ÜCosineRankLoss√ß¬ö¬Ñ√•¬è¬•√•¬ê¬ë√©¬á¬è√¶¬ñ¬π√¶¬°¬à Network structure: Training:
 [docs/cosent_train.png] Inference: [docs/inference.png] #### CoSENT
 √ß¬õ¬ë√ß¬ù¬£√¶¬®¬°√•¬û¬ã √®¬Æ¬≠√ß¬ª¬É√•¬í¬å√©¬¢¬Ñ√¶¬µ¬ãCoSENT√¶¬®¬°√•¬û¬ã√Ø¬º¬ö - √•¬ú¬®√§¬∏¬≠√¶¬ñ¬áSTS-
 B√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√®¬Æ¬≠√ß¬ª¬É√•¬í¬å√®¬Ø¬Ñ√§¬º¬∞`CoSENT`√¶¬®¬°√•¬û¬ã example: [examples/
 training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/
 blob/master/examples/training_sup_text_matching_model.py) ```shell cd examples
@@ -343,21 +363,24 @@
 'PAWSX'√Ø¬º¬å√•¬Ö¬∑√§¬Ω¬ì√•¬è¬Ç√®¬Ä¬ÉHuggingFace datasets [https://huggingface.co/datasets/
 shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh) ```shell
 python training_sup_text_matching_model.py --task_name ATEC --model_arch cosent
 --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base -
 -output_dir ./outputs/ATEC-cosent ``` - √•¬ú¬®√®¬á¬™√¶¬ú¬â√§¬∏¬≠√¶¬ñ¬á√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√§¬∏¬ä√®¬Æ¬≠√ß¬ª¬É√¶¬®¬°√•¬û¬ã
 example: [examples/training_sup_text_matching_model_mydata.py](https://
 github.com/shibing624/text2vec/blob/master/examples/
-training_sup_text_matching_model_mydata.py) ```shell python
-training_sup_text_matching_model_mydata.py --do_train --do_predict ```
-√®¬Æ¬≠√ß¬ª¬É√©¬õ¬Ü√¶¬†¬º√•¬º¬è√•¬è¬Ç√®¬Ä¬É[examples/data/STS-B/STS-B.valid.data](https://github.com/
-shibing624/text2vec/blob/master/examples/data/STS-B/STS-B.valid.data) ```shell
-sentence1 sentence2 label √§¬∏¬Ä√§¬∏¬™√•¬•¬≥√•¬≠¬©√•¬ú¬®√ß¬ª¬ô√•¬•¬π√ß¬ö¬Ñ√•¬§¬¥√•¬è¬ë√•¬Å¬ö√•¬è¬ë√•¬û¬ã√£¬Ä¬Ç
-√§¬∏¬Ä√§¬∏¬™√•¬•¬≥√•¬≠¬©√•¬ú¬®√¶¬¢¬≥√•¬§¬¥√£¬Ä¬Ç 2 √§¬∏¬Ä√ß¬æ¬§√ß¬î¬∑√§¬∫¬∫√•¬ú¬®√¶¬µ¬∑√¶¬ª¬©√§¬∏¬ä√®¬∏¬¢√®¬∂¬≥√ß¬ê¬É√£¬Ä¬Ç
-√§¬∏¬Ä√ß¬æ¬§√ß¬î¬∑√•¬≠¬©√•¬ú¬®√¶¬µ¬∑√¶¬ª¬©√§¬∏¬ä√®¬∏¬¢√®¬∂¬≥√ß¬ê¬É√£¬Ä¬Ç 3
+training_sup_text_matching_model_mydata.py) √•¬ç¬ï√•¬ç¬°√®¬Æ¬≠√ß¬ª¬É√Ø¬º¬ö ```shell
+CUDA_VISIBLE_DEVICES=0 python training_sup_text_matching_model_mydata.py --
+do_train --do_predict ``` √•¬§¬ö√•¬ç¬°√®¬Æ¬≠√ß¬ª¬É√Ø¬º¬ö ```shell CUDA_VISIBLE_DEVICES=0,1
+torchrun --nproc_per_node 2 training_sup_text_matching_model_mydata.py --
+do_train --do_predict --output_dir outputs/STS-B-text2vec-macbert-v1 --
+batch_size 64 --fp16 --data_parallel ``` √®¬Æ¬≠√ß¬ª¬É√©¬õ¬Ü√¶¬†¬º√•¬º¬è√•¬è¬Ç√®¬Ä¬É[examples/data/
+STS-B/STS-B.valid.data](https://github.com/shibing624/text2vec/blob/master/
+examples/data/STS-B/STS-B.valid.data) ```shell sentence1 sentence2 label
+√§¬∏¬Ä√§¬∏¬™√•¬•¬≥√•¬≠¬©√•¬ú¬®√ß¬ª¬ô√•¬•¬π√ß¬ö¬Ñ√•¬§¬¥√•¬è¬ë√•¬Å¬ö√•¬è¬ë√•¬û¬ã√£¬Ä¬Ç √§¬∏¬Ä√§¬∏¬™√•¬•¬≥√•¬≠¬©√•¬ú¬®√¶¬¢¬≥√•¬§¬¥√£¬Ä¬Ç 2
+√§¬∏¬Ä√ß¬æ¬§√ß¬î¬∑√§¬∫¬∫√•¬ú¬®√¶¬µ¬∑√¶¬ª¬©√§¬∏¬ä√®¬∏¬¢√®¬∂¬≥√ß¬ê¬É√£¬Ä¬Ç √§¬∏¬Ä√ß¬æ¬§√ß¬î¬∑√•¬≠¬©√•¬ú¬®√¶¬µ¬∑√¶¬ª¬©√§¬∏¬ä√®¬∏¬¢√®¬∂¬≥√ß¬ê¬É√£¬Ä¬Ç 3
 √§¬∏¬Ä√§¬∏¬™√•¬•¬≥√§¬∫¬∫√•¬ú¬®√¶¬µ¬ã√©¬á¬è√•¬è¬¶√§¬∏¬Ä√§¬∏¬™√•¬•¬≥√§¬∫¬∫√ß¬ö¬Ñ√®¬Ñ¬ö√®¬∏¬ù√£¬Ä¬Ç
 √•¬•¬≥√§¬∫¬∫√¶¬µ¬ã√©¬á¬è√•¬è¬¶√§¬∏¬Ä√§¬∏¬™√•¬•¬≥√§¬∫¬∫√ß¬ö¬Ñ√®¬Ñ¬ö√®¬∏¬ù√£¬Ä¬Ç 5 ```
 `label`√•¬è¬Ø√§¬ª¬•√¶¬ò¬Ø0√Ø¬º¬å1√¶¬†¬á√ß¬≠¬æ√Ø¬º¬å0√§¬ª¬£√®¬°¬®√§¬∏¬§√§¬∏¬™√•¬è¬•√•¬≠¬ê√§¬∏¬ç√ß¬õ¬∏√§¬º¬º√Ø¬º¬å1√§¬ª¬£√®¬°¬®√ß¬õ¬∏√§¬º¬º√Ø¬º¬õ√§¬π¬ü√•¬è¬Ø√§¬ª¬•√¶¬ò¬Ø0-
 5√ß¬ö¬Ñ√®¬Ø¬Ñ√•¬à¬Ü√Ø¬º¬å√®¬Ø¬Ñ√•¬à¬Ü√®¬∂¬ä√©¬´¬ò√Ø¬º¬å√®¬°¬®√ß¬§¬∫√§¬∏¬§√§¬∏¬™√•¬è¬•√•¬≠¬ê√®¬∂¬ä√ß¬õ¬∏√§¬º¬º√£¬Ä¬Ç√¶¬®¬°√•¬û¬ã√©¬É¬Ω√®¬É¬Ω√¶¬î¬Ø√¶¬å¬Å√£¬Ä¬Ç
 - √•¬ú¬®√®¬ã¬±√¶¬ñ¬áSTS-B√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√®¬Æ¬≠√ß¬ª¬É√•¬í¬å√®¬Ø¬Ñ√§¬º¬∞`CoSENT`√¶¬®¬°√•¬û¬ã example: [examples/
 training_sup_text_matching_model_en.py](https://github.com/shibing624/text2vec/
 blob/master/examples/training_sup_text_matching_model_en.py) ```shell cd
@@ -366,15 +389,15 @@
 output_dir ./outputs/STS-B-en-cosent ``` #### CoSENT √¶¬ó¬†√ß¬õ¬ë√ß¬ù¬£√¶¬®¬°√•¬û¬ã -
 √•¬ú¬®√®¬ã¬±√¶¬ñ¬áNLI√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√®¬Æ¬≠√ß¬ª¬É`CoSENT`√¶¬®¬°√•¬û¬ã√Ø¬º¬å√•¬ú¬®STS-B√¶¬µ¬ã√®¬Ø¬ï√©¬õ¬Ü√®¬Ø¬Ñ√§¬º¬∞√¶¬ï¬à√¶¬û¬ú
 example: [examples/training_unsup_text_matching_model_en.py](https://
 github.com/shibing624/text2vec/blob/master/examples/
 training_unsup_text_matching_model_en.py) ```shell cd examples python
 training_unsup_text_matching_model_en.py --model_arch cosent --do_train --
 do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./
-outputs/STS-B-en-unsup-cosent ``` ## Sentence-BERT model Sentence-
+outputs/STS-B-en-unsup-cosent ``` ### Sentence-BERT model Sentence-
 BERT√¶¬ñ¬á√¶¬ú¬¨√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã√Ø¬º¬å√®¬°¬®√•¬æ¬Å√•¬º¬è√•¬è¬•√•¬ê¬ë√©¬á¬è√®¬°¬®√ß¬§¬∫√¶¬ñ¬π√¶¬°¬à Network structure:
 Training: [docs/sbert_train.png] Inference: [docs/sbert_inference.png] ####
 SentenceBERT √ß¬õ¬ë√ß¬ù¬£√¶¬®¬°√•¬û¬ã - √•¬ú¬®√§¬∏¬≠√¶¬ñ¬áSTS-B√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√®¬Æ¬≠√ß¬ª¬É√•¬í¬å√®¬Ø¬Ñ√§¬º¬∞`SBERT`√¶¬®¬°√•¬û¬ã
 example: [examples/training_sup_text_matching_model.py](https://github.com/
 shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)
 ```shell cd examples python training_sup_text_matching_model.py --model_arch
 sentencebert --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-
@@ -387,35 +410,35 @@
 uncased --output_dir ./outputs/STS-B-en-sbert ``` #### SentenceBERT
 √¶¬ó¬†√ß¬õ¬ë√ß¬ù¬£√¶¬®¬°√•¬û¬ã - √•¬ú¬®√®¬ã¬±√¶¬ñ¬áNLI√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√®¬Æ¬≠√ß¬ª¬É`SBERT`√¶¬®¬°√•¬û¬ã√Ø¬º¬å√•¬ú¬®STS-
 B√¶¬µ¬ã√®¬Ø¬ï√©¬õ¬Ü√®¬Ø¬Ñ√§¬º¬∞√¶¬ï¬à√¶¬û¬ú example: [examples/
 training_unsup_text_matching_model_en.py](https://github.com/shibing624/
 text2vec/blob/master/examples/training_unsup_text_matching_model_en.py)
 ```shell cd examples python training_unsup_text_matching_model_en.py --
 model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name
-bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-sbert ``` ## BERT-Match
-model
+bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-sbert ``` ### BERT-
+Match model
 BERT√¶¬ñ¬á√¶¬ú¬¨√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã√Ø¬º¬å√•¬é¬ü√ß¬î¬üBERT√•¬å¬π√©¬Ö¬ç√ß¬Ω¬ë√ß¬ª¬ú√ß¬ª¬ì√¶¬û¬Ñ√Ø¬º¬å√§¬∫¬§√§¬∫¬í√•¬º¬è√•¬è¬•√•¬ê¬ë√©¬á¬è√•¬å¬π√©¬Ö¬ç√¶¬®¬°√•¬û¬ã
 Network structure: Training and inference: [docs/bert-fc-train.png]
 √®¬Æ¬≠√ß¬ª¬É√®¬Ñ¬ö√¶¬ú¬¨√•¬ê¬å√§¬∏¬ä[examples/training_sup_text_matching_model.py](https://
 github.com/shibing624/text2vec/blob/master/examples/
-training_sup_text_matching_model.py)√£¬Ä¬Ç ## √¶¬®¬°√•¬û¬ã√®¬í¬∏√©¬¶¬è√Ø¬º¬àModel Distillation√Ø¬º¬â
-√ß¬î¬±√§¬∫¬étext2vec√®¬Æ¬≠√ß¬ª¬É√ß¬ö¬Ñ√¶¬®¬°√•¬û¬ã√•¬è¬Ø√§¬ª¬•√§¬Ω¬ø√ß¬î¬®[sentence-transformers](https://
-github.com/UKPLab/sentence-
+training_sup_text_matching_model.py)√£¬Ä¬Ç ### √¶¬®¬°√•¬û¬ã√®¬í¬∏√©¬¶¬è√Ø¬º¬àModel
+Distillation√Ø¬º¬â √ß¬î¬±√§¬∫¬étext2vec√®¬Æ¬≠√ß¬ª¬É√ß¬ö¬Ñ√¶¬®¬°√•¬û¬ã√•¬è¬Ø√§¬ª¬•√§¬Ω¬ø√ß¬î¬®[sentence-
+transformers](https://github.com/UKPLab/sentence-
 transformers)√•¬∫¬ì√•¬ä¬†√®¬Ω¬Ω√Ø¬º¬å√¶¬≠¬§√•¬§¬Ñ√•¬§¬ç√ß¬î¬®√•¬Ö¬∂√¶¬®¬°√•¬û¬ã√®¬í¬∏√©¬¶¬è√¶¬ñ¬π√¶¬≥¬ï[distillation](https:
 //github.com/UKPLab/sentence-transformers/tree/master/examples/training/
 distillation)√£¬Ä¬Ç 1. √¶¬®¬°√•¬û¬ã√©¬ô¬ç√ß¬ª¬¥√Ø¬º¬å√•¬è¬Ç√®¬Ä¬É[dimensionality_reduction.py](https://
 github.com/UKPLab/sentence-transformers/blob/master/examples/training/
 distillation/
 dimensionality_reduction.py)√§¬Ω¬ø√ß¬î¬®PCA√•¬Ø¬π√¶¬®¬°√•¬û¬ã√®¬æ¬ì√•¬á¬∫embedding√©¬ô¬ç√ß¬ª¬¥√Ø¬º¬å√•¬è¬Ø√•¬á¬è√•¬∞¬ëmilvus√ß¬≠¬â√•¬ê¬ë√©¬á¬è√¶¬£¬Ä√ß¬¥¬¢√¶¬ï¬∞√¶¬ç¬Æ√•¬∫¬ì√ß¬ö¬Ñ√•¬≠¬ò√•¬Ç¬®√•¬é¬ã√•¬ä¬õ√Ø¬º¬å√®¬ø¬ò√®¬É¬Ω√®¬Ω¬ª√•¬æ¬Æ√¶¬è¬ê√•¬ç¬á√¶¬®¬°√•¬û¬ã√¶¬ï¬à√¶¬û¬ú√£¬Ä¬Ç
 2. √¶¬®¬°√•¬û¬ã√®¬í¬∏√©¬¶¬è√Ø¬º¬å√•¬è¬Ç√®¬Ä¬É[model_distillation.py](https://github.com/UKPLab/
 sentence-transformers/blob/master/examples/training/distillation/
 model_distillation.py)√§¬Ω¬ø√ß¬î¬®√®¬í¬∏√©¬¶¬è√¶¬ñ¬π√¶¬≥¬ï√Ø¬º¬å√•¬∞¬ÜTeacher√•¬§¬ß√¶¬®¬°√•¬û¬ã√®¬í¬∏√©¬¶¬è√•¬à¬∞√¶¬õ¬¥√•¬∞¬ëlayers√•¬±¬Ç√¶¬ï¬∞√ß¬ö¬Ñstudent√¶¬®¬°√•¬û¬ã√§¬∏¬≠√Ø¬º¬å√•¬ú¬®√¶¬ù¬É√®¬°¬°√¶¬ï¬à√¶¬û¬ú√ß¬ö¬Ñ√¶¬É¬Ö√•¬Ü¬µ√§¬∏¬ã√Ø¬º¬å√•¬è¬Ø√•¬§¬ß√•¬π¬Ö√¶¬è¬ê√•¬ç¬á√¶¬®¬°√•¬û¬ã√©¬¢¬Ñ√¶¬µ¬ã√©¬Ä¬ü√•¬∫¬¶√£¬Ä¬Ç
-## √¶¬®¬°√•¬û¬ã√©¬É¬®√ß¬Ω¬≤ √¶¬è¬ê√§¬æ¬õ√§¬∏¬§√ß¬ß¬ç√©¬É¬®√ß¬Ω¬≤√¶¬®¬°√•¬û¬ã√Ø¬º¬å√¶¬ê¬≠√•¬ª¬∫√¶¬ú¬ç√•¬ä¬°√ß¬ö¬Ñ√¶¬ñ¬π√¶¬≥¬ï√Ø¬º¬ö
+### √¶¬®¬°√•¬û¬ã√©¬É¬®√ß¬Ω¬≤ √¶¬è¬ê√§¬æ¬õ√§¬∏¬§√ß¬ß¬ç√©¬É¬®√ß¬Ω¬≤√¶¬®¬°√•¬û¬ã√Ø¬º¬å√¶¬ê¬≠√•¬ª¬∫√¶¬ú¬ç√•¬ä¬°√ß¬ö¬Ñ√¶¬ñ¬π√¶¬≥¬ï√Ø¬º¬ö
 1√Ø¬º¬â√•¬ü¬∫√§¬∫¬éJina√¶¬ê¬≠√•¬ª¬∫gRPC√¶¬ú¬ç√•¬ä¬°√£¬Ä¬ê√¶¬é¬®√®¬ç¬ê√£¬Ä¬ë√Ø¬º¬õ2√Ø¬º¬â√•¬ü¬∫√§¬∫¬éFastAPI√¶¬ê¬≠√•¬ª¬∫√•¬é¬ü√ß¬î¬üHttp√¶¬ú¬ç√•¬ä¬°√£¬Ä¬Ç
-### Jina√¶¬ú¬ç√•¬ä¬° √©¬á¬á√ß¬î¬®C/
+#### Jina√¶¬ú¬ç√•¬ä¬° √©¬á¬á√ß¬î¬®C/
 S√¶¬®¬°√•¬º¬è√¶¬ê¬≠√•¬ª¬∫√©¬´¬ò√¶¬Ä¬ß√®¬É¬Ω√¶¬ú¬ç√•¬ä¬°√Ø¬º¬å√¶¬î¬Ø√¶¬å¬Ådocker√§¬∫¬ë√•¬é¬ü√ß¬î¬ü√Ø¬º¬ågRPC/HTTP/
 WebSocket√Ø¬º¬å√¶¬î¬Ø√¶¬å¬Å√•¬§¬ö√§¬∏¬™√¶¬®¬°√•¬û¬ã√•¬ê¬å√¶¬ó¬∂√©¬¢¬Ñ√¶¬µ¬ã√Ø¬º¬åGPU√•¬§¬ö√•¬ç¬°√•¬§¬Ñ√ß¬ê¬Ü√£¬Ä¬Ç - √•¬Æ¬â√®¬£¬Ö√Ø¬º¬ö
 ```pip install jina``` - √•¬ê¬Ø√•¬ä¬®√¶¬ú¬ç√•¬ä¬°√Ø¬º¬ö example: [examples/
 jina_server_demo.py](examples/jina_server_demo.py) ```python from jina import
 Flow port = 50001 f = Flow(port=port).add( uses='jinahub://Text2vecEncoder',
 uses_with={'model_name': 'shibing624/text2vec-base-chinese'} ) with f: #
 backend server forever f.block() ```
@@ -424,27 +447,27 @@
 √®¬∞¬É√ß¬î¬®√¶¬ú¬ç√•¬ä¬°√Ø¬º¬ö ```python from jina import Client from docarray import
 Document, DocumentArray port = 50001 c = Client(port=port) data =
 ['√•¬¶¬Ç√§¬Ω¬ï√¶¬õ¬¥√¶¬ç¬¢√®¬ä¬±√•¬ë¬ó√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°', '√®¬ä¬±√•¬ë¬ó√¶¬õ¬¥√¶¬î¬π√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°'] print
 ("data:", data) print('data embs:') r = c.post('/', inputs=DocumentArray(
 [Document(text='√•¬¶¬Ç√§¬Ω¬ï√¶¬õ¬¥√¶¬ç¬¢√®¬ä¬±√•¬ë¬ó√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°'), Document
 (text='√®¬ä¬±√•¬ë¬ó√¶¬õ¬¥√¶¬î¬π√ß¬ª¬ë√•¬Æ¬ö√©¬ì¬∂√®¬°¬å√•¬ç¬°')])) print(r.embeddings) ```
 √¶¬â¬π√©¬á¬è√®¬∞¬É√ß¬î¬®√¶¬ñ¬π√¶¬≥¬ï√®¬ß¬Åexample: [examples/jina_client_demo.py](https://
-github.com/shibing624/text2vec/blob/master/examples/jina_client_demo.py) ###
+github.com/shibing624/text2vec/blob/master/examples/jina_client_demo.py) ####
 FastAPI√¶¬ú¬ç√•¬ä¬° - √•¬Æ¬â√®¬£¬Ö√Ø¬º¬ö ```pip install fastapi uvicorn``` - √•¬ê¬Ø√•¬ä¬®√¶¬ú¬ç√•¬ä¬°√Ø¬º¬ö
 example: [examples/fastapi_server_demo.py](https://github.com/shibing624/
 text2vec/blob/master/examples/fastapi_server_demo.py) ```shell cd examples
 python fastapi_server_demo.py ``` - √®¬∞¬É√ß¬î¬®√¶¬ú¬ç√•¬ä¬°√Ø¬º¬ö ```shell curl -X 'GET' \
 'http://0.0.0.0:8001/emb?q=hello' \ -H 'accept: application/json' ``` ##
-√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü - √¶¬ú¬¨√©¬°¬π√ß¬õ¬Ærelease√ß¬ö¬Ñ√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬ö | Dataset | Introduce | Download
-Link | |:---------------------------|:-----------------------------------------
---------------------------------|:---------------------------------------------
+Dataset - √¶¬ú¬¨√©¬°¬π√ß¬õ¬Ærelease√ß¬ö¬Ñ√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬ö | Dataset | Introduce | Download Link
+| |:---------------------------|:----------------------------------------------
+---------------------------|:--------------------------------------------------
 -------------------------------------------------------------------------------
 -------------------------------------------------------------------------------
 -------------------------------------------------------------------------------
-------------| | shibing624/nli-zh-all |
+-------| | shibing624/nli-zh-all |
 √§¬∏¬≠√¶¬ñ¬á√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√•¬ê¬à√©¬õ¬Ü√Ø¬º¬å√¶¬ï¬¥√•¬ê¬à√§¬∫¬Ü√¶¬ñ¬á√¶¬ú¬¨√¶¬é¬®√ß¬ê¬Ü√Ø¬º¬å√ß¬õ¬∏√§¬º¬º√Ø¬º¬å√¶¬ë¬ò√®¬¶¬Å√Ø¬º¬å√©¬ó¬Æ√ß¬≠¬î√Ø¬º¬å√¶¬å¬á√§¬ª¬§√•¬æ¬Æ√®¬∞¬É√ß¬≠¬â√§¬ª¬ª√•¬ä¬°√ß¬ö¬Ñ820√§¬∏¬á√©¬´¬ò√®¬¥¬®√©¬á¬è√¶¬ï¬∞√¶¬ç¬Æ√Ø¬º¬å√•¬π¬∂√®¬Ω¬¨√•¬å¬ñ√§¬∏¬∫√•¬å¬π√©¬Ö¬ç√¶¬†¬º√•¬º¬è√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü
 | [https://huggingface.co/datasets/shibing624/nli-zh-all](https://
 huggingface.co/datasets/shibing624/nli-zh-all) | | shibing624/snli-zh |
 √§¬∏¬≠√¶¬ñ¬áSNLI√•¬í¬åMultiNLI√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬å√ß¬ø¬ª√®¬Ø¬ë√®¬á¬™√®¬ã¬±√¶¬ñ¬áSNLI√•¬í¬åMultiNLI | [https://
 huggingface.co/datasets/shibing624/snli-zh](https://huggingface.co/datasets/
 shibing624/snli-zh) | | shibing624/nli_zh |
 √§¬∏¬≠√¶¬ñ¬á√®¬Ø¬≠√§¬π¬â√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬å√¶¬ï¬¥√•¬ê¬à√§¬∫¬Ü√§¬∏¬≠√¶¬ñ¬áATEC√£¬Ä¬ÅBQ√£¬Ä¬ÅLCQMC√£¬Ä¬ÅPAWSX√£¬Ä¬ÅSTS-
@@ -462,46 +485,46 @@
 info/1037/1162.htm) | | LCQMC | √§¬∏¬≠√¶¬ñ¬áLCQMC(large-scale Chinese question
 matching corpus)√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬åQ-Qpair√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü | [LCQMC](http://
 icrc.hitsz.edu.cn/Article/show/171.html) | | PAWSX | √§¬∏¬≠√¶¬ñ¬áPAWS(Paraphrase
 Adversaries from Word Scrambling)√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬åQ-Qpair√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü | [PAWSX](https:/
 /arxiv.org/abs/1908.11828) | | STS-B | √§¬∏¬≠√¶¬ñ¬áSTS-
 B√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬å√§¬∏¬≠√¶¬ñ¬á√®¬á¬™√ß¬Ñ¬∂√®¬Ø¬≠√®¬®¬Ä√¶¬é¬®√ß¬ê¬Ü√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬å√§¬ª¬é√®¬ã¬±√¶¬ñ¬áSTS-
 B√ß¬ø¬ª√®¬Ø¬ë√§¬∏¬∫√§¬∏¬≠√¶¬ñ¬á√ß¬ö¬Ñ√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü | [STS-B](https://github.com/pluto-junzeng/CNSD) |
-√•¬∏¬∏√ß¬î¬®√®¬ã¬±√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬ö - √•¬§¬ß√•¬ê¬ç√©¬º¬é√©¬º¬é√ß¬ö¬Ñmulti_nli√•¬í¬åsnli: https://
-huggingface.co/datasets/multi_nli - √•¬§¬ß√•¬ê¬ç√©¬º¬é√©¬º¬é√ß¬ö¬Ñmulti_nli√•¬í¬åsnli: https://
+√•¬∏¬∏√ß¬î¬®√®¬ã¬±√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬ö - √®¬ã¬±√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬ömulti_nli: https://
+huggingface.co/datasets/multi_nli - √®¬ã¬±√¶¬ñ¬á√•¬å¬π√©¬Ö¬ç√¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√Ø¬º¬ösnli: https://
 huggingface.co/datasets/snli - https://huggingface.co/datasets/metaeval/cnli -
 https://huggingface.co/datasets/mteb/stsbenchmark-sts - https://huggingface.co/
 datasets/JeremiahZ/simcse_sup_nli - https://huggingface.co/datasets/
 MoritzLaurer/multilingual-NLI-26lang-2mil7 √¶¬ï¬∞√¶¬ç¬Æ√©¬õ¬Ü√§¬Ω¬ø√ß¬î¬®√ß¬§¬∫√§¬æ¬ã√Ø¬º¬ö ```shell
 pip install datasets ``` ```python from datasets import load_dataset dataset =
 load_dataset("shibing624/nli_zh", "STS-B") # ATEC or BQ or LCQMC or PAWSX or
 STS-B print(dataset) print(dataset['test'][0]) ``` output: ```shell DatasetDict
 ({ train: Dataset({ features: ['sentence1', 'sentence2', 'label'], num_rows:
 5231 }) validation: Dataset({ features: ['sentence1', 'sentence2', 'label'],
 num_rows: 1458 }) test: Dataset({ features: ['sentence1', 'sentence2',
 'label'], num_rows: 1361 }) }) {'sentence1':
 '√§¬∏¬Ä√§¬∏¬™√•¬•¬≥√•¬≠¬©√•¬ú¬®√ß¬ª¬ô√•¬•¬π√ß¬ö¬Ñ√•¬§¬¥√•¬è¬ë√•¬Å¬ö√•¬è¬ë√•¬û¬ã√£¬Ä¬Ç', 'sentence2':
-'√§¬∏¬Ä√§¬∏¬™√•¬•¬≥√•¬≠¬©√•¬ú¬®√¶¬¢¬≥√•¬§¬¥√£¬Ä¬Ç', 'label': 2} ``` # Contact - Issue(√•¬ª¬∫√®¬Æ¬Æ)√Ø¬º¬ö[!
+'√§¬∏¬Ä√§¬∏¬™√•¬•¬≥√•¬≠¬©√•¬ú¬®√¶¬¢¬≥√•¬§¬¥√£¬Ä¬Ç', 'label': 2} ``` ## Contact - Issue(√•¬ª¬∫√®¬Æ¬Æ)√Ø¬º¬ö[!
 [GitHub issues](https://img.shields.io/github/issues/shibing624/text2vec.svg)]
 (https://github.com/shibing624/text2vec/issues) - √©¬Ç¬Æ√§¬ª¬∂√¶¬à¬ë√Ø¬º¬öxuming:
 xuming624@qq.com - √•¬æ¬Æ√§¬ø¬°√¶¬à¬ë√Ø¬º¬ö√•¬ä¬†√¶¬à¬ë*√•¬æ¬Æ√§¬ø¬°√•¬è¬∑√Ø¬º¬öxuming624, √•¬§¬á√¶¬≥¬®√Ø¬º¬ö√•¬ß¬ì√•¬ê¬ç-
-√•¬Ö¬¨√•¬è¬∏-NLP* √®¬ø¬õNLP√§¬∫¬§√¶¬µ¬Å√ß¬æ¬§√£¬Ä¬Ç [docs/wechat.jpeg] # Citation
+√•¬Ö¬¨√•¬è¬∏-NLP* √®¬ø¬õNLP√§¬∫¬§√¶¬µ¬Å√ß¬æ¬§√£¬Ä¬Ç [docs/wechat.jpeg] ## Citation
 √•¬¶¬Ç√¶¬û¬ú√§¬Ω¬†√•¬ú¬®√ß¬†¬î√ß¬©¬∂√§¬∏¬≠√§¬Ω¬ø√ß¬î¬®√§¬∫¬Ütext2vec√Ø¬º¬å√®¬Ø¬∑√¶¬å¬â√•¬¶¬Ç√§¬∏¬ã√¶¬†¬º√•¬º¬è√•¬º¬ï√ß¬î¬®√Ø¬º¬ö APA:
 ```latex Xu, M. Text2vec: Text to vector toolkit (Version 1.1.2) [Computer
 software]. https://github.com/shibing624/text2vec ``` BibTeX: ```latex @misc
-{Text2vec, author = {Xu, Ming}, title = {Text2vec: Text to vector toolkit},
-year = {2022}, publisher = {GitHub}, journal = {GitHub repository},
-howpublished = {\url{https://github.com/shibing624/text2vec}}, } ``` # License
+{Text2vec, author = {Ming Xu}, title = {Text2vec: Text to vector toolkit}, year
+= {2023}, publisher = {GitHub}, journal = {GitHub repository}, howpublished =
+{\url{https://github.com/shibing624/text2vec}}, } ``` ## License
 √¶¬é¬à√¶¬ù¬É√•¬ç¬è√®¬Æ¬Æ√§¬∏¬∫ [The Apache License 2.0]
 (LICENSE)√Ø¬º¬å√•¬è¬Ø√•¬Ö¬ç√®¬¥¬π√ß¬î¬®√•¬Å¬ö√•¬ï¬Ü√§¬∏¬ö√ß¬î¬®√©¬Ä¬î√£¬Ä¬Ç√®¬Ø¬∑√•¬ú¬®√§¬∫¬ß√•¬ì¬Å√®¬Ø¬¥√¶¬ò¬é√§¬∏¬≠√©¬ô¬Ñ√•¬ä¬†text2vec√ß¬ö¬Ñ√©¬ì¬æ√¶¬é¬•√•¬í¬å√¶¬é¬à√¶¬ù¬É√•¬ç¬è√®¬Æ¬Æ√£¬Ä¬Ç
-# Contribute
+## Contribute
 √©¬°¬π√ß¬õ¬Æ√§¬ª¬£√ß¬†¬Å√®¬ø¬ò√•¬æ¬à√ß¬≤¬ó√ß¬≥¬ô√Ø¬º¬å√•¬¶¬Ç√¶¬û¬ú√•¬§¬ß√•¬Æ¬∂√•¬Ø¬π√§¬ª¬£√ß¬†¬Å√¶¬ú¬â√¶¬â¬Ä√¶¬î¬π√®¬ø¬õ√Ø¬º¬å√¶¬¨¬¢√®¬ø¬é√¶¬è¬ê√§¬∫¬§√•¬õ¬û√¶¬ú¬¨√©¬°¬π√ß¬õ¬Æ√Ø¬º¬å√•¬ú¬®√¶¬è¬ê√§¬∫¬§√§¬π¬ã√•¬â¬ç√Ø¬º¬å√¶¬≥¬®√¶¬Ñ¬è√§¬ª¬•√§¬∏¬ã√§¬∏¬§√ß¬Ç¬π√Ø¬º¬ö
 - √•¬ú¬®`tests`√¶¬∑¬ª√•¬ä¬†√ß¬õ¬∏√•¬∫¬î√ß¬ö¬Ñ√•¬ç¬ï√•¬Ö¬É√¶¬µ¬ã√®¬Ø¬ï - √§¬Ω¬ø√ß¬î¬®`python -m pytest -
 v`√¶¬ù¬•√®¬ø¬ê√®¬°¬å√¶¬â¬Ä√¶¬ú¬â√•¬ç¬ï√•¬Ö¬É√¶¬µ¬ã√®¬Ø¬ï√Ø¬º¬å√ß¬°¬Æ√§¬ø¬ù√¶¬â¬Ä√¶¬ú¬â√•¬ç¬ï√¶¬µ¬ã√©¬É¬Ω√¶¬ò¬Ø√©¬Ä¬ö√®¬ø¬á√ß¬ö¬Ñ
-√§¬π¬ã√•¬ê¬é√•¬ç¬≥√•¬è¬Ø√¶¬è¬ê√§¬∫¬§PR√£¬Ä¬Ç # Reference -
+√§¬π¬ã√•¬ê¬é√•¬ç¬≥√•¬è¬Ø√¶¬è¬ê√§¬∫¬§PR√£¬Ä¬Ç ## References -
 [√•¬∞¬Ü√•¬è¬•√•¬≠¬ê√®¬°¬®√ß¬§¬∫√§¬∏¬∫√•¬ê¬ë√©¬á¬è√Ø¬º¬à√§¬∏¬ä√Ø¬º¬â√Ø¬º¬ö√¶¬ó¬†√ß¬õ¬ë√ß¬ù¬£√•¬è¬•√•¬≠¬ê√®¬°¬®√ß¬§¬∫√•¬≠¬¶√§¬π¬†√Ø¬º¬àsentence
 embedding√Ø¬º¬â](https://www.cnblogs.com/llhthinker/p/10335164.html) -
 [√•¬∞¬Ü√•¬è¬•√•¬≠¬ê√®¬°¬®√ß¬§¬∫√§¬∏¬∫√•¬ê¬ë√©¬á¬è√Ø¬º¬à√§¬∏¬ã√Ø¬º¬â√Ø¬º¬ö√¶¬ó¬†√ß¬õ¬ë√ß¬ù¬£√•¬è¬•√•¬≠¬ê√®¬°¬®√ß¬§¬∫√•¬≠¬¶√§¬π¬†√Ø¬º¬àsentence
 embedding√Ø¬º¬â](https://www.cnblogs.com/llhthinker/p/10341841.html) - [A Simple
 but Tough-to-Beat Baseline for Sentence Embeddings[Sanjeev Arora and Yingyu
 Liang and Tengyu Ma, 2017]](https://openreview.net/forum?id=SyK00v5xx) -
 [√•¬õ¬õ√ß¬ß¬ç√®¬Æ¬°√ß¬Æ¬ó√¶¬ñ¬á√¶¬ú¬¨√ß¬õ¬∏√§¬º¬º√•¬∫¬¶√ß¬ö¬Ñ√¶¬ñ¬π√¶¬≥¬ï√•¬Ø¬π√¶¬Ø¬î[Yves Peirsman]](https://
```

### Comparing `text2vec-1.2.1/text2vec.egg-info/SOURCES.txt` & `text2vec-1.2.2/text2vec.egg-info/SOURCES.txt`

 * *Files identical despite different names*

