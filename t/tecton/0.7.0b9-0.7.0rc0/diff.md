# Comparing `tmp/tecton-0.7.0b9.tar.gz` & `tmp/tecton-0.7.0rc0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "tecton-0.7.0b9.tar", last modified: Thu Mar 30 19:24:57 2023, max compression
+gzip compressed data, was "tecton-0.7.0rc0.tar", last modified: Fri Jul 14 21:47:32 2023, max compression
```

## Comparing `tecton-0.7.0b9.tar` & `tecton-0.7.0rc0.tar`

### file list

```diff
@@ -1,522 +1,555 @@
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.562433 tecton-0.7.0b9/
--rwxr-xr-x   0 nobody   (65534) nogroup  (65534)      159 2023-03-30 19:24:54.000000 tecton-0.7.0b9/MANIFEST.in
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     3827 2023-03-30 19:24:57.562433 tecton-0.7.0b9/PKG-INFO
--rwxr-xr-x   0 nobody   (65534) nogroup  (65534)     3143 2023-03-30 19:24:54.000000 tecton-0.7.0b9/README.md
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.514429 tecton-0.7.0b9/protoc_gen_swagger/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/protoc_gen_swagger/__init__.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.514429 tecton-0.7.0b9/protoc_gen_swagger/options/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/protoc_gen_swagger/options/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2657 2023-03-30 19:24:54.000000 tecton-0.7.0b9/protoc_gen_swagger/options/annotations_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    16494 2023-03-30 19:24:54.000000 tecton-0.7.0b9/protoc_gen_swagger/options/openapiv2_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)       38 2023-03-30 19:24:57.562433 tecton-0.7.0b9/setup.cfg
--rwxr-xr-x   0 nobody   (65534) nogroup  (65534)     1997 2023-03-30 19:24:54.000000 tecton-0.7.0b9/setup.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.514429 tecton-0.7.0b9/tecton/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     3055 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/__init__.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.518430 tecton-0.7.0b9/tecton/_internals/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    11203 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/analytics.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2190 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/athena_api.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1197 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/data_frame_helper.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     6128 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/delete_keys_api.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     7859 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/display.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      943 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/env_utils.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    25363 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/errors.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      536 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/fco.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      845 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/find_spark.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    12977 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/materialization_api.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      694 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/metadata_service.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.518430 tecton-0.7.0b9/tecton/_internals/metadata_service_impl/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/metadata_service_impl/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      421 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/metadata_service_impl/auth_lib.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      195 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/metadata_service_impl/base_stub.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     3170 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/metadata_service_impl/error_lib.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     3122 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/metadata_service_impl/http_client.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1786 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/metadata_service_impl/request_lib.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1990 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/metadata_service_impl/response.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2511 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/metadata_service_impl/service_calls.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      197 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/metadata_service_impl/trace.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    13751 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/query_helper.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.518430 tecton-0.7.0b9/tecton/_internals/repo/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/repo/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    11216 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/repo/function_serialization.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    10766 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/rewrite.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    15738 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/run_api.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     8634 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/sdk_decorators.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     8711 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/snowflake_api.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    24254 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/spark_api.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2842 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/spark_utils.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     5971 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/time_utils.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2008 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/type_utils.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    15447 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/utils.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     6197 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/validations_api.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)       36 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_internals/workspace_utils.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      276 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/_stamp.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      790 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/aggregation_functions.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.518430 tecton-0.7.0b9/tecton/cli/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/cli/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     8927 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/cli/access_control.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     4055 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/cli/api_key.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    29966 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/cli/cli.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     4315 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/cli/cli_utils.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     5957 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/cli/command.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2614 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/cli/common.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    16440 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/cli/engine.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    25059 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/cli/engine_renderer.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     7948 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/cli/error_utils.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1663 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/cli/printer.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     5814 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/cli/service_account.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     6468 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/cli/workspace.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2450 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/fco_listers.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.522430 tecton-0.7.0b9/tecton/framework/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/framework/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     6580 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/framework/base_tecton_object.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      809 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/framework/compute_mode.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    83902 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/framework/configs.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    17157 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/framework/data_frame.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    31192 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/framework/data_source.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    10624 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/framework/dataset.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     7570 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/framework/entity.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    33317 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/framework/feature_service.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)   157104 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/framework/feature_view.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     3854 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/framework/filtered_source.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    22597 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/framework/transformation.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      673 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/framework/utils.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1821 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/framework/validation_mode.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    20947 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/framework/workspace.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.522430 tecton-0.7.0b9/tecton/identities/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/identities/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1493 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/identities/api_keys.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     3938 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/identities/credentials.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    10448 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/identities/okta.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1658 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/pytest_tecton.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      381 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/run_api_consts.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     3036 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/snowflake_context.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2772 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/tecton_context.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2543 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/types.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.522430 tecton-0.7.0b9/tecton/vendor/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/__init__.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.522430 tecton-0.7.0b9/tecton/vendor/dill/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/dill/__init__.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.522430 tecton-0.7.0b9/tecton/vendor/dill/dill/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     7211 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/dill/dill/__diff.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     3634 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/dill/dill/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    54753 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/dill/dill/_dill.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    20428 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/dill/dill/_objects.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    11822 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/dill/dill/detect.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     7444 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/dill/dill/info.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      755 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/dill/dill/objtypes.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     4467 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/dill/dill/pointers.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      719 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/dill/dill/settings.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    44906 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/dill/dill/source.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     8238 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/dill/dill/temp.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.522430 tecton-0.7.0b9/tecton/vendor/pyspark/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/__init__.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.526430 tecton-0.7.0b9/tecton/vendor/pyspark/py4j/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      111 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/py4j/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    26941 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/py4j/clientserver.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2329 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/py4j/compat.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     4306 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/py4j/finalizer.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    18289 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/py4j/java_collections.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    96550 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/py4j/java_gateway.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    14805 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/py4j/protocol.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     4361 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/py4j/signals.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)       23 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/py4j/version.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.526430 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     4777 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2282 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/_globals.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     9940 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/accumulators.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     7386 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/broadcast.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.526430 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/cloudpickle/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      201 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/cloudpickle/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    30084 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/cloudpickle/cloudpickle.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    29668 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/cloudpickle/cloudpickle_fast.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      354 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/cloudpickle/compat.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     7831 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/conf.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    54613 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/context.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     7619 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/daemon.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1891 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/files.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     3969 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/find_spark_home.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     6654 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/install.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    10025 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/java_gateway.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     3994 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/join.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.530431 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1530 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    10846 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/base.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)   126616 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/classification.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    62498 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/clustering.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     4294 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/common.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    34149 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/evaluation.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)   212823 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/feature.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    17053 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/fpm.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     4603 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/functions.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     8092 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/image.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.530431 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/linalg/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    39791 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/linalg/__init__.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.530431 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/param/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    18532 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/param/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     9556 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/param/_shared_params_code_gen.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    22295 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/param/shared.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    13656 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/pipeline.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    23213 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/recommendation.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    91533 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/regression.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    18317 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/stat.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    13108 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/tree.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    57402 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/tuning.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    22016 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/util.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    15371 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/wrapper.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.530431 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1372 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    28695 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/classification.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    39889 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/clustering.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     4976 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/common.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    20443 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/evaluation.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    28134 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/feature.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     7028 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/fpm.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.530431 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/linalg/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    46744 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/linalg/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    57731 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/linalg/distributed.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    19517 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/random.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    12101 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/recommendation.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    32842 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/regression.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.534431 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/stat/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1969 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/stat/KernelDensity.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1309 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/stat/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    14081 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/stat/_statistics.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1289 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/stat/distribution.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2301 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/stat/test.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    25342 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/tree.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    21149 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/util.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     5755 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/profiler.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.534431 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/python/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/python/__init__.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.534431 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/python/pyspark/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/python/pyspark/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2473 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/python/pyspark/shell.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)   109373 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/rdd.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     4250 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/rddsampler.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.534431 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/resource/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1317 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/resource/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1590 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/resource/information.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     7097 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/resource/profile.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    10946 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/resource/requests.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1224 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/resultiterable.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    20586 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/serializers.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2473 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/shell.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    27959 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/shuffle.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.534431 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2590 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/__init__.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.534431 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/avro/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      809 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/avro/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     5993 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/avro/functions.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    12110 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/catalog.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    29664 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/column.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2999 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/conf.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    23884 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/context.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    99829 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/dataframe.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)   157099 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/functions.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    10681 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/group.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.538431 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/pandas/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      959 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/pandas/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    21163 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/pandas/conversion.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    28130 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/pandas/functions.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    14683 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/pandas/group_ops.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     3806 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/pandas/map_ops.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    12308 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/pandas/serializers.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     6324 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/pandas/typehints.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    13225 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/pandas/types.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2745 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/pandas/utils.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    77056 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/readwriter.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    31188 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/session.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    69121 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/streaming.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    55203 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/types.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    20068 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/udf.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     6976 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/utils.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    12863 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/window.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     5149 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/statcounter.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     3756 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/status.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2785 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/storagelevel.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.538431 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/streaming/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1007 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/streaming/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    16448 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/streaming/context.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    27879 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/streaming/dstream.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     5436 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/streaming/kinesis.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2333 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/streaming/listener.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     5614 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/streaming/util.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     9189 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/taskcontext.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2679 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/traceback_utils.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    11623 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/util.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)       20 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/version.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    27875 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/worker.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1334 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/vendor_dill.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1448 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/vendor/vendor_pyspark.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1185 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton/version.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.514429 tecton-0.7.0b9/tecton.egg-info/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     3827 2023-03-30 19:24:56.000000 tecton-0.7.0b9/tecton.egg-info/PKG-INFO
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    17884 2023-03-30 19:24:56.000000 tecton-0.7.0b9/tecton.egg-info/SOURCES.txt
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        1 2023-03-30 19:24:56.000000 tecton-0.7.0b9/tecton.egg-info/dependency_links.txt
--rw-r--r--   0 nobody   (65534) nogroup  (65534)       96 2023-03-30 19:24:56.000000 tecton-0.7.0b9/tecton.egg-info/entry_points.txt
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      745 2023-03-30 19:24:56.000000 tecton-0.7.0b9/tecton.egg-info/requires.txt
--rw-r--r--   0 nobody   (65534) nogroup  (65534)       95 2023-03-30 19:24:56.000000 tecton-0.7.0b9/tecton.egg-info/top_level.txt
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.538431 tecton-0.7.0b9/tecton_athena/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_athena/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    14614 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_athena/athena_session.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     8209 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_athena/data_catalog_helper.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     5682 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_athena/odfv_helper.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    16733 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_athena/pipeline_helper.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    21428 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_athena/sql_helper.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.538431 tecton-0.7.0b9/tecton_athena/templates/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_athena/templates/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1956 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_athena/templates/create_table.sql
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      267 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_athena/templates/data_source.sql
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     9544 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_athena/templates/historical_features.sql
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      672 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_athena/templates/materialization_tile.sql
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     3863 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_athena/templates/run_full_aggregation.sql
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      503 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_athena/templates/run_partial_aggregation.sql
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1046 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_athena/templates/time_limit.sql
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      277 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_athena/templates/transformation_pipeline.sql
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      656 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_athena/templates_utils.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.542431 tecton-0.7.0b9/tecton_core/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    10493 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/aggregation_utils.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    18433 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/conf.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     6080 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/data_types.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     4647 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/errors.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     4778 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/fco_container.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    15218 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/feature_definition_wrapper.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     8508 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/feature_set_config.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2820 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/feature_view_utils.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      302 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/filter_context.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2263 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/function_deserialization.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      623 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/id_helper.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1304 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/logger.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     7353 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/materialization_context.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     6633 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/offline_store.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1395 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/online_serving_index.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     5905 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/pipeline_common.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     4367 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/pipeline_sql_builder.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.542431 tecton-0.7.0b9/tecton_core/query/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/query/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     5273 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/query/aggregation_plans.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    24787 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/query/builder.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     8386 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/query/node_interface.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    65452 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/query/nodes.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      496 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/query/rewrite.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     9100 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/query/sql_compat.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      195 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/query_consts.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     4764 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/repo_file_handler.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      765 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/schema.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     5523 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/schema_derivation_utils.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.542431 tecton-0.7.0b9/tecton_core/specs/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      675 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/specs/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    34465 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/specs/data_source_spec.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1418 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/specs/entity_spec.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     5529 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/specs/feature_service_spec.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    29070 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/specs/feature_view_spec.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     3779 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/specs/tecton_object_spec.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2581 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/specs/transformation_spec.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     4811 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/specs/utils.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     6453 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/time_utils.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.542431 tecton-0.7.0b9/tecton_core/vendor/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/vendor/__init__.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.542431 tecton-0.7.0b9/tecton_core/vendor/treelib/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1594 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/vendor/treelib/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      943 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/vendor/treelib/exceptions.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1575 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/vendor/treelib/misc.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     9595 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/vendor/treelib/node.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1265 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/vendor/treelib/plugins.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    37459 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/vendor/treelib/tree.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1275 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_core/vendor/vendor_treelib.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.542431 tecton-0.7.0b9/tecton_proto/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/__init__.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.542431 tecton-0.7.0b9/tecton_proto/amplitude/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/amplitude/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     4278 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/amplitude/amplitude_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     3093 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/amplitude/client_logging_pb2.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.542431 tecton-0.7.0b9/tecton_proto/api/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/api/__init__.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.542431 tecton-0.7.0b9/tecton_proto/api/featureservice/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/api/featureservice/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    38057 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/api/featureservice/feature_service_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     8623 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/api/featureservice/feature_service_request_pb2.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.546432 tecton-0.7.0b9/tecton_proto/args/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/args/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2864 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/args/basic_info_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1897 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/args/data_source_config_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    17024 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/args/data_source_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2596 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/args/diff_options_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     6198 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/args/diff_test_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2481 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/args/entity_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2415 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/args/fco_args_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     4030 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/args/feature_service_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    30162 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/args/feature_view_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     5930 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/args/pipeline_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1694 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/args/repo_metadata_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     4097 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/args/transformation_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1637 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/args/user_defined_function_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1893 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/args/version_constraints_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     6767 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/args/virtual_data_source_pb2.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.546432 tecton-0.7.0b9/tecton_proto/auth/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/auth/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1372 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/auth/acl_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    15940 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/auth/authorization_service_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2935 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/auth/principal_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1814 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/auth/resource_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2917 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/auth/service_pb2.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.546432 tecton-0.7.0b9/tecton_proto/cli/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/cli/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     3075 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/cli/repo_diff_pb2.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.546432 tecton-0.7.0b9/tecton_proto/common/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/common/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2661 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/common/aggregation_function_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1717 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/common/analytics_options_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1672 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/common/column_type_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1373 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/common/data_source_type_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2204 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/common/data_type_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1626 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/common/fco_locator_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1286 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/common/framework_version_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1196 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/common/id_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1150 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/common/pair_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1368 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/common/schema_container_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2070 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/common/schema_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1276 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/common/secret_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1381 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/common/spark_schema_pb2.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.546432 tecton-0.7.0b9/tecton_proto/consumption/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/consumption/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2807 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/consumption/consumption_pb2.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.550432 tecton-0.7.0b9/tecton_proto/data/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/data/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     6005 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/data/batch_data_source_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1921 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/data/entity_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2848 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/data/fco_metadata_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2758 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/data/fco_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     4220 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/data/feature_service_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1487 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/data/feature_store_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    14378 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/data/feature_view_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2121 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/data/freshness_status_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2124 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/data/fv_materialization_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1366 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/data/hive_metastore_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1971 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/data/internal_spark_cluster_status_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     5367 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/data/materialization_status_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2316 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/data/onboarding_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    11550 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/data/remote_spark_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     3365 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/data/saved_feature_data_frame_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     4541 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/data/serving_status_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     9668 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/data/state_update_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     3945 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/data/stream_data_source_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1828 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/data/summary_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1933 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/data/tecton_api_key_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2358 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/data/transformation_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     9336 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/data/user_deployment_settings_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1650 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/data/user_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2979 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/data/virtual_data_source_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1527 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/data/workspace_pb2.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.550432 tecton-0.7.0b9/tecton_proto/databricks_api/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/databricks_api/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     7943 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/databricks_api/clusters_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1889 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/databricks_api/dbfs_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1276 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/databricks_api/error_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     3593 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/databricks_api/execution_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1501 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/databricks_api/instance_profiles_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     8112 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/databricks_api/jobs_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1912 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/databricks_api/libraries_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1937 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/databricks_api/permissions_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1253 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/databricks_api/scim_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2391 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/databricks_api/secrets_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1708 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/databricks_api/workspace_pb2.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.550432 tecton-0.7.0b9/tecton_proto/dataobs/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/dataobs/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2490 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/dataobs/config_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2409 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/dataobs/expectation_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     3691 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/dataobs/metric_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     5532 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/dataobs/validation_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     3569 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/dataobs/validation_task_params_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     4048 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/dataobs/validation_task_pb2.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.550432 tecton-0.7.0b9/tecton_proto/feature_server/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/feature_server/__init__.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.550432 tecton-0.7.0b9/tecton_proto/feature_server/configuration/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/feature_server/configuration/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    12602 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/feature_server/configuration/feature_server_configuration_pb2.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.550432 tecton-0.7.0b9/tecton_proto/materialization/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/materialization/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     6373 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/materialization/job_metadata_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2598 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/materialization/materialization_states_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2655 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/materialization/spark_cluster_pb2.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.554432 tecton-0.7.0b9/tecton_proto/materializationjobservice/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/materializationjobservice/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     9600 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/materializationjobservice/materialization_job_service_pb2.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.554432 tecton-0.7.0b9/tecton_proto/metadataservice/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/metadataservice/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2776 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/metadataservice/http_over_grpc_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)   109537 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/metadataservice/metadata_service_pb2.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.554432 tecton-0.7.0b9/tecton_proto/online_store/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/online_store/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     3450 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/online_store/feature_value_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1306 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/online_store/status_entry_pb2.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.554432 tecton-0.7.0b9/tecton_proto/spark_api/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/spark_api/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1195 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/spark_api/error_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     7176 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/spark_api/jobs_pb2.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.554432 tecton-0.7.0b9/tecton_proto/spark_common/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/spark_common/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     6558 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/spark_common/clusters_pb2.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2386 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/spark_common/libraries_pb2.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.554432 tecton-0.7.0b9/tecton_proto/validation/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/validation/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     4609 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_proto/validation/validator_pb2.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.554432 tecton-0.7.0b9/tecton_snowflake/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_snowflake/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    29291 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_snowflake/pipeline_helper.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     6514 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_snowflake/schema_derivation_utils.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1993 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_snowflake/snowflake_type_utils.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    29771 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_snowflake/sql_helper.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.554432 tecton-0.7.0b9/tecton_snowflake/templates/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_snowflake/templates/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      909 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_snowflake/templates/copier_macro.sql
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      383 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_snowflake/templates/create_temp_table_for_bfv.sql
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      350 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_snowflake/templates/create_temp_table_for_bwafv.sql
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      255 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_snowflake/templates/data_source.sql
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      698 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_snowflake/templates/delete_staged_files.sql
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     3069 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_snowflake/templates/historical_features.sql
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     7311 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_snowflake/templates/historical_features_macros.sql
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1549 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_snowflake/templates/materialization_tile.sql
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2241 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_snowflake/templates/materialized_feature_view.sql
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      126 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_snowflake/templates/online_store_copier.sql
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     6740 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_snowflake/templates/run_full_aggregation.sql
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      583 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_snowflake/templates/run_partial_aggregation.sql
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      569 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_snowflake/templates/time_limit.sql
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      213 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_snowflake/templates/transformation_pipeline.sql
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      806 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_snowflake/templates_utils.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      249 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_snowflake/utils.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.558433 tecton-0.7.0b9/tecton_spark/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      454 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_spark/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    14566 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_spark/aggregation_plans.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1704 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_spark/data_observability.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      926 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_spark/data_source_credentials.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    34514 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_spark/data_source_helper.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      403 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_spark/errors_spark.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      890 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_spark/feature_view_spark_utils.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2926 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_spark/ingest_utils.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.558433 tecton-0.7.0b9/tecton_spark/jars/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_spark/jars/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)  1567067 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_spark/jars/tecton-udfs-spark-3.jar
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     4180 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_spark/materialization_plan.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    22007 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_spark/offline_store.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    10610 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_spark/partial_aggregations.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    29804 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_spark/pipeline_helper.py
-drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:57.562433 tecton-0.7.0b9/tecton_spark/query/
--rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_spark/query/__init__.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     3395 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_spark/query/data_source.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     8467 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_spark/query/filter.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    18311 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_spark/query/join.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1229 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_spark/query/node.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     3603 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_spark/query/pipeline.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     5862 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_spark/query/projection.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     5066 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_spark/query/translate.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1052 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_spark/request_context.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)    11008 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_spark/schema_derivation_utils.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     4852 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_spark/schema_spark_utils.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     2150 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_spark/spark_helper.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     1708 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_spark/spark_schema_wrapper.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     4850 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_spark/time_utils.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)      414 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_spark/udf_jar.py
--rw-r--r--   0 nobody   (65534) nogroup  (65534)     3243 2023-03-30 19:24:54.000000 tecton-0.7.0b9/tecton_spark/udfs.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.617556 tecton-0.7.0rc0/
+-rwxr-xr-x   0 nobody   (65534) nogroup  (65534)      159 2023-07-14 21:47:27.000000 tecton-0.7.0rc0/MANIFEST.in
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3890 2023-07-14 21:47:32.617556 tecton-0.7.0rc0/PKG-INFO
+-rwxr-xr-x   0 nobody   (65534) nogroup  (65534)     3143 2023-07-14 21:47:27.000000 tecton-0.7.0rc0/README.md
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.561551 tecton-0.7.0rc0/protoc_gen_swagger/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/protoc_gen_swagger/__init__.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.561551 tecton-0.7.0rc0/protoc_gen_swagger/options/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/protoc_gen_swagger/options/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2657 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/protoc_gen_swagger/options/annotations_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    16494 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/protoc_gen_swagger/options/openapiv2_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)       38 2023-07-14 21:47:32.621557 tecton-0.7.0rc0/setup.cfg
+-rwxr-xr-x   0 nobody   (65534) nogroup  (65534)     2094 2023-07-14 21:47:27.000000 tecton-0.7.0rc0/setup.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.561551 tecton-0.7.0rc0/tecton/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     5361 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/__init__.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.565551 tecton-0.7.0rc0/tecton/_internals/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    11143 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/analytics.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2259 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/athena_api.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1169 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/data_frame_helper.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     6181 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/delete_keys_api.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     7925 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/display.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      944 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/env_utils.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    26511 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/errors.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      881 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/find_spark.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2901 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/ingest_utils.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    13125 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/materialization_api.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      666 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/metadata_service.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.569552 tecton-0.7.0rc0/tecton/_internals/metadata_service_impl/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/metadata_service_impl/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      421 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/metadata_service_impl/auth_lib.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      195 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/metadata_service_impl/base_stub.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3078 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/metadata_service_impl/error_lib.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3115 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/metadata_service_impl/http_client.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1782 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/metadata_service_impl/request_lib.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1990 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/metadata_service_impl/response.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2628 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/metadata_service_impl/service_calls.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      198 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/metadata_service_impl/trace.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     4147 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/mock_source_utils.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    16135 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/query_helper.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.569552 tecton-0.7.0rc0/tecton/_internals/repo/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/repo/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    11417 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/repo/function_serialization.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    10712 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/rewrite.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    15439 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/run_api.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    12582 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/sdk_decorators.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     9019 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/snowflake_api.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    26066 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/spark_api.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2823 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/spark_utils.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     6101 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/time_utils.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2273 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/type_utils.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    15669 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/utils.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     4991 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/validations_api.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)       36 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_internals/workspace_utils.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      276 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/_stamp.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2238 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/aggregation_functions.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.569552 tecton-0.7.0rc0/tecton/cli/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/cli/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    15083 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/cli/access_control.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1574 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/cli/api_key.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    34602 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/cli/cli.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     4315 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/cli/cli_utils.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     6569 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/cli/command.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3009 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/cli/common.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    14945 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/cli/engine.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    25411 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/cli/engine_renderer.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3348 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/cli/environment.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     7683 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/cli/error_utils.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1664 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/cli/printer.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     5832 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/cli/service_account.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1312 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/cli/user.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     6859 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/cli/workspace.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      670 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/cli/workspace_utils.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2450 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/fco_listers.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.573552 tecton-0.7.0rc0/tecton/framework/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/framework/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     7294 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/framework/base_tecton_object.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      810 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/framework/compute_mode.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    89131 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/framework/configs.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    20716 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/framework/data_frame.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    31660 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/framework/data_source.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    10597 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/framework/dataset.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     7485 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/framework/entity.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    34112 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/framework/feature_service.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)   165862 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/framework/feature_view.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3854 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/framework/filtered_source.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    21844 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/framework/transformation.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      673 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/framework/utils.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2091 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/framework/validation_mode.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    22879 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/framework/workspace.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.573552 tecton-0.7.0rc0/tecton/identities/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/identities/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1521 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/identities/api_keys.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3888 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/identities/credentials.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    10498 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/identities/okta.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2138 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/pytest_tecton.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      381 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/run_api_consts.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3009 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/snowflake_context.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2730 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/tecton_context.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3422 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/types.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.573552 tecton-0.7.0rc0/tecton/vendor/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/__init__.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.573552 tecton-0.7.0rc0/tecton/vendor/dill/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/dill/__init__.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.573552 tecton-0.7.0rc0/tecton/vendor/dill/dill/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     7211 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/dill/dill/__diff.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3634 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/dill/dill/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    54753 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/dill/dill/_dill.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    20428 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/dill/dill/_objects.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    11822 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/dill/dill/detect.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     7444 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/dill/dill/info.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      755 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/dill/dill/objtypes.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     4467 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/dill/dill/pointers.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      719 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/dill/dill/settings.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    44906 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/dill/dill/source.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     8238 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/dill/dill/temp.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.573552 tecton-0.7.0rc0/tecton/vendor/pyspark/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/__init__.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.573552 tecton-0.7.0rc0/tecton/vendor/pyspark/py4j/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      111 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/py4j/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    26941 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/py4j/clientserver.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2329 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/py4j/compat.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     4306 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/py4j/finalizer.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    18289 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/py4j/java_collections.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    96550 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/py4j/java_gateway.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    14805 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/py4j/protocol.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     4361 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/py4j/signals.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)       23 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/py4j/version.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.577552 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     4777 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2282 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/_globals.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     9940 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/accumulators.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     7386 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/broadcast.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.577552 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/cloudpickle/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      201 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/cloudpickle/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    30084 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/cloudpickle/cloudpickle.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    29668 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/cloudpickle/cloudpickle_fast.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      354 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/cloudpickle/compat.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     7831 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/conf.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    54613 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/context.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     7619 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/daemon.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1891 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/files.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3969 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/find_spark_home.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     6654 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/install.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    10025 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/java_gateway.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3994 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/join.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.581553 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1530 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    10846 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/base.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)   126616 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/classification.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    62498 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/clustering.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     4294 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/common.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    34149 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/evaluation.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)   212823 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/feature.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    17053 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/fpm.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     4603 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/functions.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     8092 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/image.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.581553 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/linalg/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    39791 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/linalg/__init__.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.581553 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/param/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    18532 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/param/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     9556 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/param/_shared_params_code_gen.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    22295 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/param/shared.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    13656 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/pipeline.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    23213 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/recommendation.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    91533 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/regression.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    18317 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/stat.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    13108 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/tree.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    57402 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/tuning.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    22016 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/util.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    15371 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/wrapper.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.581553 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1372 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    28695 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/classification.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    39889 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/clustering.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     4976 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/common.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    20443 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/evaluation.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    28134 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/feature.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     7028 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/fpm.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.585553 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/linalg/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    46744 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/linalg/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    57731 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/linalg/distributed.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    19517 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/random.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    12101 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/recommendation.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    32842 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/regression.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.585553 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/stat/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1969 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/stat/KernelDensity.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1309 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/stat/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    14081 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/stat/_statistics.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1289 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/stat/distribution.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2301 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/stat/test.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    25342 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/tree.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    21149 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/util.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     5755 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/profiler.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.585553 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/python/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/python/__init__.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.585553 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/python/pyspark/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/python/pyspark/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2473 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/python/pyspark/shell.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)   109373 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/rdd.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     4250 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/rddsampler.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.585553 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/resource/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1317 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/resource/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1590 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/resource/information.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     7097 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/resource/profile.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    10946 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/resource/requests.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1224 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/resultiterable.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    20586 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/serializers.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2473 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/shell.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    27959 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/shuffle.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.585553 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2590 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/__init__.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.585553 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/avro/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      809 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/avro/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     5993 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/avro/functions.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    12110 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/catalog.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    29664 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/column.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2999 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/conf.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    23884 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/context.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    99829 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/dataframe.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)   157099 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/functions.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    10681 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/group.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.589554 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/pandas/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      959 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/pandas/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    21163 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/pandas/conversion.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    28130 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/pandas/functions.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    14683 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/pandas/group_ops.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3806 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/pandas/map_ops.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    12308 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/pandas/serializers.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     6324 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/pandas/typehints.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    13225 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/pandas/types.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2745 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/pandas/utils.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    77056 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/readwriter.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    31188 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/session.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    69121 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/streaming.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    55203 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/types.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    20068 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/udf.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     6976 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/utils.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    12863 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/window.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     5149 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/statcounter.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3756 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/status.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2785 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/storagelevel.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.589554 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/streaming/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1007 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/streaming/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    16448 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/streaming/context.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    27879 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/streaming/dstream.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     5436 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/streaming/kinesis.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2333 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/streaming/listener.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     5614 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/streaming/util.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     9189 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/taskcontext.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2679 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/traceback_utils.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    11623 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/util.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)       20 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/version.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    27875 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/worker.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1334 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/vendor_dill.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1448 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/vendor/vendor_pyspark.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1185 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton/version.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.561551 tecton-0.7.0rc0/tecton.egg-info/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3890 2023-07-14 21:47:30.000000 tecton-0.7.0rc0/tecton.egg-info/PKG-INFO
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    19035 2023-07-14 21:47:31.000000 tecton-0.7.0rc0/tecton.egg-info/SOURCES.txt
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        1 2023-07-14 21:47:30.000000 tecton-0.7.0rc0/tecton.egg-info/dependency_links.txt
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)       96 2023-07-14 21:47:30.000000 tecton-0.7.0rc0/tecton.egg-info/entry_points.txt
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      833 2023-07-14 21:47:31.000000 tecton-0.7.0rc0/tecton.egg-info/requires.txt
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)       95 2023-07-14 21:47:31.000000 tecton-0.7.0rc0/tecton.egg-info/top_level.txt
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.589554 tecton-0.7.0rc0/tecton_athena/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_athena/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    14608 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_athena/athena_session.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     8482 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_athena/data_catalog_helper.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     5747 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_athena/odfv_helper.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     6356 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_athena/pipeline_helper.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.589554 tecton-0.7.0rc0/tecton_athena/query/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_athena/query/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2291 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_athena/query/translate.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    17764 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_athena/sql_helper.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.589554 tecton-0.7.0rc0/tecton_athena/templates/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_athena/templates/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1956 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_athena/templates/create_table.sql
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      267 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_athena/templates/data_source.sql
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     9544 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_athena/templates/historical_features.sql
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      672 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_athena/templates/materialization_tile.sql
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3863 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_athena/templates/run_full_aggregation.sql
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      503 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_athena/templates/run_partial_aggregation.sql
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1046 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_athena/templates/time_limit.sql
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      277 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_athena/templates/transformation_pipeline.sql
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      656 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_athena/templates_utils.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.593554 tecton-0.7.0rc0/tecton_core/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    13329 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/aggregation_utils.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    19520 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/conf.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     7206 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/data_types.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     5415 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/errors.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     4818 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/fco_container.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    17300 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/feature_definition_wrapper.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     9765 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/feature_set_config.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2450 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/feature_view_utils.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      302 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/filter_context.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2300 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/function_deserialization.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      623 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/id_helper.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     7393 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/materialization_context.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     6916 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/offline_store.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1395 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/online_serving_index.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     6304 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/pipeline_common.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     4393 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/pipeline_sql_builder.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.593554 tecton-0.7.0rc0/tecton_core/query/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/query/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    10765 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/query/aggregation_plans.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    26314 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/query/builder.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     8446 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/query/node_interface.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    66482 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/query/nodes.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.593554 tecton-0.7.0rc0/tecton_core/query/pandas/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/query/pandas/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2677 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/query/pandas/node.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     8340 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/query/pandas/nodes.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      267 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/query/pandas/sql.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      610 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/query/rewrite.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     9133 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/query/sql_compat.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      397 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/query_consts.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     4835 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/repo_file_handler.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      867 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/request_context.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1961 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/schema.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     5562 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/schema_derivation_utils.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.597554 tecton-0.7.0rc0/tecton_core/specs/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      660 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/specs/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    38770 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/specs/data_source_spec.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1258 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/specs/entity_spec.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     5274 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/specs/feature_service_spec.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    30873 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/specs/feature_view_spec.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     4440 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/specs/tecton_object_spec.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2440 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/specs/transformation_spec.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     4912 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/specs/utils.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     6872 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/time_utils.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.597554 tecton-0.7.0rc0/tecton_core/vendor/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/vendor/__init__.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.597554 tecton-0.7.0rc0/tecton_core/vendor/treelib/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1594 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/vendor/treelib/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      943 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/vendor/treelib/exceptions.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1575 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/vendor/treelib/misc.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     9595 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/vendor/treelib/node.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1265 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/vendor/treelib/plugins.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    37469 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/vendor/treelib/tree.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1275 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_core/vendor/vendor_treelib.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.597554 tecton-0.7.0rc0/tecton_proto/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/__init__.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.597554 tecton-0.7.0rc0/tecton_proto/amplitude/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/amplitude/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     4278 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/amplitude/amplitude_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3093 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/amplitude/client_logging_pb2.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.597554 tecton-0.7.0rc0/tecton_proto/api/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/api/__init__.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.597554 tecton-0.7.0rc0/tecton_proto/api/featureservice/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/api/featureservice/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    85094 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/api/featureservice/feature_service_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     9207 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/api/featureservice/feature_service_request_pb2.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.601555 tecton-0.7.0rc0/tecton_proto/args/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/args/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2864 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/args/basic_info_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1897 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/args/data_source_config_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    17760 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/args/data_source_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3520 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/args/diff_options_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     6990 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/args/diff_test_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2481 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/args/entity_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2415 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/args/fco_args_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     4122 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/args/feature_service_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    33904 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/args/feature_view_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     6463 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/args/pipeline_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1694 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/args/repo_metadata_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     4071 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/args/transformation_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1637 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/args/user_defined_function_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1893 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/args/version_constraints_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     7076 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/args/virtual_data_source_pb2.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.601555 tecton-0.7.0rc0/tecton_proto/auth/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/auth/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1372 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/auth/acl_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    19265 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/auth/authorization_service_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2935 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/auth/principal_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1922 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/auth/resource_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3027 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/auth/resource_role_assignments_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3544 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/auth/service_pb2.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.601555 tecton-0.7.0rc0/tecton_proto/canary/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/canary/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1208 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/canary/type_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2195 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/canary/update_pb2.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.601555 tecton-0.7.0rc0/tecton_proto/cli/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/cli/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3075 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/cli/repo_diff_pb2.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.605555 tecton-0.7.0rc0/tecton_proto/common/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/common/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3459 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/common/aggregation_function_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1717 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/common/analytics_options_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1672 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/common/column_type_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1339 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/common/container_image_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1373 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/common/data_source_type_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2431 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/common/data_type_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1735 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/common/fco_locator_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1286 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/common/framework_version_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1196 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/common/id_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1150 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/common/pair_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1368 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/common/schema_container_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2070 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/common/schema_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1276 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/common/secret_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1381 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/common/spark_schema_pb2.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.605555 tecton-0.7.0rc0/tecton_proto/consumption/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/consumption/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3289 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/consumption/consumption_pb2.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.605555 tecton-0.7.0rc0/tecton_proto/data/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/data/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     6382 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/data/batch_data_source_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1589 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/data/data_source_access_config_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1921 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/data/entity_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2848 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/data/fco_metadata_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2758 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/data/fco_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     4497 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/data/feature_service_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1487 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/data/feature_store_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    15392 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/data/feature_view_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2121 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/data/freshness_status_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2124 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/data/fv_materialization_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1366 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/data/hive_metastore_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1971 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/data/internal_spark_cluster_status_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     5638 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/data/materialization_status_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1752 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/data/odfv_compute_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2316 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/data/onboarding_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1907 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/data/principal_group_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2696 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/data/remote_compute_environment_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    12187 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/data/remote_spark_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3468 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/data/saved_feature_data_frame_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     4541 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/data/serving_status_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     9668 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/data/state_update_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3945 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/data/stream_data_source_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1828 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/data/summary_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2142 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/data/tecton_api_key_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2358 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/data/transformation_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     9697 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/data/user_deployment_settings_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1650 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/data/user_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2979 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/data/virtual_data_source_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1743 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/data/workspace_pb2.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.609556 tecton-0.7.0rc0/tecton_proto/databricks_api/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/databricks_api/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     8185 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/databricks_api/clusters_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1889 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/databricks_api/dbfs_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1276 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/databricks_api/error_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3593 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/databricks_api/execution_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1501 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/databricks_api/instance_profiles_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     8112 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/databricks_api/jobs_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1912 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/databricks_api/libraries_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1937 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/databricks_api/permissions_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1253 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/databricks_api/scim_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2391 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/databricks_api/secrets_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1708 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/databricks_api/workspace_pb2.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.609556 tecton-0.7.0rc0/tecton_proto/dataobs/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/dataobs/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2490 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/dataobs/config_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2553 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/dataobs/expectation_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3691 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/dataobs/metric_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     5748 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/dataobs/validation_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3569 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/dataobs/validation_task_params_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     4629 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/dataobs/validation_task_pb2.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.609556 tecton-0.7.0rc0/tecton_proto/feature_server/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/feature_server/__init__.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.609556 tecton-0.7.0rc0/tecton_proto/feature_server/configuration/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/feature_server/configuration/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    13426 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/feature_server/configuration/feature_server_configuration_pb2.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.609556 tecton-0.7.0rc0/tecton_proto/materialization/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/materialization/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     7654 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/materialization/job_metadata_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2598 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/materialization/materialization_states_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    13314 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/materialization/materialization_task_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     5790 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/materialization/params_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2691 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/materialization/spark_cluster_pb2.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.609556 tecton-0.7.0rc0/tecton_proto/materializationjobservice/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/materializationjobservice/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    14321 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/materializationjobservice/materialization_job_service_pb2.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.609556 tecton-0.7.0rc0/tecton_proto/metadataservice/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/metadataservice/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2776 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/metadataservice/http_over_grpc_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)   107952 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/metadataservice/metadata_service_pb2.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.609556 tecton-0.7.0rc0/tecton_proto/online_store/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/online_store/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     4353 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/online_store/feature_value_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1306 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/online_store/status_entry_pb2.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.609556 tecton-0.7.0rc0/tecton_proto/online_store_writer/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/online_store_writer/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2401 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/online_store_writer/config_pb2.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.609556 tecton-0.7.0rc0/tecton_proto/remoteenvironmentservice/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/remoteenvironmentservice/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     8041 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/remoteenvironmentservice/remote_environment_service_pb2.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.613556 tecton-0.7.0rc0/tecton_proto/snowflake/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/snowflake/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1246 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/snowflake/location_pb2.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.613556 tecton-0.7.0rc0/tecton_proto/spark_api/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/spark_api/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1195 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/spark_api/error_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     7263 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/spark_api/jobs_pb2.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.613556 tecton-0.7.0rc0/tecton_proto/spark_common/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/spark_common/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     7339 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/spark_common/clusters_pb2.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2386 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/spark_common/libraries_pb2.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.613556 tecton-0.7.0rc0/tecton_proto/validation/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/validation/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     4609 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_proto/validation/validator_pb2.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.613556 tecton-0.7.0rc0/tecton_snowflake/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_snowflake/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    29228 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_snowflake/pipeline_helper.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     7376 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_snowflake/schema_derivation_utils.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2273 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_snowflake/snowflake_type_utils.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    32077 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_snowflake/sql_helper.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.613556 tecton-0.7.0rc0/tecton_snowflake/templates/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_snowflake/templates/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1046 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_snowflake/templates/copier_macro.sql
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      396 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_snowflake/templates/create_temp_table_for_bfv.sql
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      363 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_snowflake/templates/create_temp_table_for_bwafv.sql
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      255 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_snowflake/templates/data_source.sql
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1248 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_snowflake/templates/delete_orphaned_schemas.sql
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      698 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_snowflake/templates/delete_staged_files.sql
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3069 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_snowflake/templates/historical_features.sql
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     7892 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_snowflake/templates/historical_features_macros.sql
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1795 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_snowflake/templates/materialization_tile.sql
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2575 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_snowflake/templates/materialized_feature_view.sql
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      937 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_snowflake/templates/offline_materialization_macros.sql
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      126 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_snowflake/templates/online_store_copier.sql
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     6695 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_snowflake/templates/run_full_aggregation.sql
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      583 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_snowflake/templates/run_partial_aggregation.sql
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      569 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_snowflake/templates/time_limit.sql
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      213 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_snowflake/templates/transformation_pipeline.sql
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      806 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_snowflake/templates_utils.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      249 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_snowflake/utils.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.617556 tecton-0.7.0rc0/tecton_spark/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      465 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_spark/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    25652 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_spark/aggregation_plans.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2090 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_spark/data_observability.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      926 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_spark/data_source_credentials.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    35854 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_spark/data_source_helper.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      425 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_spark/errors_spark.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      890 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_spark/feature_view_spark_utils.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.617556 tecton-0.7.0rc0/tecton_spark/jars/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_spark/jars/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)  1604241 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_spark/jars/tecton-udfs-spark-3.jar
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     4039 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_spark/materialization_plan.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    22766 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_spark/offline_store.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    11048 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_spark/partial_aggregations.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    34132 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_spark/pipeline_helper.py
+drwxr-xr-x   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:32.617556 tecton-0.7.0rc0/tecton_spark/query/
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)        0 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_spark/query/__init__.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3054 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_spark/query/data_source.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     8463 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_spark/query/filter.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    18310 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_spark/query/join.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1229 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_spark/query/node.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     4229 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_spark/query/pipeline.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     5861 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_spark/query/projection.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     5087 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_spark/query/translate.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)    11252 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_spark/schema_derivation_utils.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     5410 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_spark/schema_spark_utils.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     2135 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_spark/spark_helper.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     1677 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_spark/spark_schema_wrapper.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     4842 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_spark/time_utils.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)      393 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_spark/udf_jar.py
+-rw-r--r--   0 nobody   (65534) nogroup  (65534)     3257 2023-07-14 21:47:28.000000 tecton-0.7.0rc0/tecton_spark/udfs.py
```

### Comparing `tecton-0.7.0b9/PKG-INFO` & `tecton-0.7.0rc0/PKG-INFO`

 * *Files 3% similar despite different names*

```diff
@@ -1,27 +1,29 @@
 Metadata-Version: 2.1
 Name: tecton
-Version: 0.7.0b9
+Version: 0.7.0rc0
 Summary: Tecton Python SDK
 Home-page: https://tecton.ai
 Author: Tecton, Inc.
 Author-email: support@tecton.ai
 License: Tecton Proprietary
 Classifier: Programming Language :: Python :: 3
 Classifier: Operating System :: OS Independent
 Classifier: License :: Other/Proprietary License
-Requires-Python: >=3.7.*
+Requires-Python: >=3.7
 Description-Content-Type: text/markdown
 Provides-Extra: databricks-connect
 Provides-Extra: databricks-connect9
 Provides-Extra: databricks-connect10
+Provides-Extra: databricks-connect11
 Provides-Extra: pyspark
 Provides-Extra: pyspark3
 Provides-Extra: pyspark3.1
 Provides-Extra: pyspark3.2
+Provides-Extra: pyspark3.3
 Provides-Extra: snowflake
 Provides-Extra: snowpark
 Provides-Extra: athena
 
 ![logo](https://s3.us-west-2.amazonaws.com/tecton.ai.public/documentation/pypi/tecton-logo.svg)
 
 Tecton is the fastest way to build operational machine learning applications. It helps automate real-time decision making like fraud detection, product recommendations, and search result ranking in production applications.
```

### Comparing `tecton-0.7.0b9/README.md` & `tecton-0.7.0rc0/README.md`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/protoc_gen_swagger/options/annotations_pb2.py` & `tecton-0.7.0rc0/protoc_gen_swagger/options/annotations_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/protoc_gen_swagger/options/openapiv2_pb2.py` & `tecton-0.7.0rc0/protoc_gen_swagger/options/openapiv2_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/setup.py` & `tecton-0.7.0rc0/setup.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,23 +10,23 @@
     packages.append(root.replace("/", "."))
 for root, _, files in os.walk("protoc_gen_swagger"):
   if any([f.endswith("_pb2.py") for f in files]):
     packages.append(root.replace("/", "."))
 
 setuptools.setup(
     classifiers=['Programming Language :: Python :: 3', 'Operating System :: OS Independent', 'License :: Other/Proprietary License'],
-    python_requires='>=3.7.*',
+    python_requires='>=3.7',
     author='Tecton, Inc.',
     author_email='support@tecton.ai',
     url='https://tecton.ai',
     license='Tecton Proprietary',
     include_package_data=True,
     description='Tecton Python SDK',
     entry_points={'console_scripts': ['tecton=tecton.cli.cli:main'], 'pytest11': ['pytest_tecton=tecton.pytest_tecton']},
     name='tecton',
-    version='0.7.0b9',
+    version='0.7.0rc0',
     long_description=long_description,
     long_description_content_type='text/markdown',
-    install_requires=['attrs>=21.3.0', 'boto3', 'googleapis-common-protos~=1.52', 'jinja2~=3.0.3', 'numpy~=1.16', 'pathspec', 'pendulum~=2.1', 'protobuf~=3.20.0', 'pypika~=0.48.9', 'pytimeparse', 'pandas~=1.0', 'texttable', 'requests', 'colorama~=0.4', 'tqdm~=4.41', 'yaspin<3,>=0.16', 'typing-extensions~=4.1', 'pygments>=2.7.4', 'pytest', 'click~=8.0', 'typeguard~=2.0', 'sqlparse', 'semantic_version'],
-    extras_require={'databricks-connect': ['databricks-connect[sql]~=9.1.23'], 'databricks-connect9': ['databricks-connect[sql]~=9.1.23'], 'databricks-connect10': ['databricks-connect[sql]~=10.4.12'], 'pyspark': ['pyspark[sql]~=3.1.2'], 'pyspark3': ['pyspark[sql]~=3.1.2'], 'pyspark3.1': ['pyspark[sql]~=3.1.2'], 'pyspark3.2': ['pyspark[sql]~=3.2.1'], 'snowflake': ['snowflake-connector-python[pandas]~=2.8'], 'snowpark': ['snowflake-snowpark-python[pandas]~=1.0.0'], 'athena': ['awswrangler~=2.15']},
+    install_requires=['attrs>=21.3.0', 'boto3', 'googleapis-common-protos~=1.52', 'jinja2~=3.0', 'numpy~=1.16', 'pathspec', 'pendulum~=2.1', 'protobuf>=3.20.0', 'pypika~=0.48.9', 'pytimeparse', 'pandas~=1.0', 'texttable', 'requests', 'colorama~=0.4', 'tqdm~=4.41', 'yaspin<3,>=0.16', 'typing-extensions~=4.1', 'pygments>=2.7.4', 'pytest', 'click~=8.0', 'typeguard~=2.0', 'sqlparse', 'semantic_version'],
+    extras_require={'databricks-connect': ['databricks-connect[sql]~=10.4.12'], 'databricks-connect9': ['databricks-connect[sql]~=9.1.23'], 'databricks-connect10': ['databricks-connect[sql]~=10.4.12'], 'databricks-connect11': ['databricks-connect[sql]~=11.3.12'], 'pyspark': ['pyspark[sql]~=3.2.1'], 'pyspark3': ['pyspark[sql]~=3.2.1'], 'pyspark3.1': ['pyspark[sql]~=3.1.2'], 'pyspark3.2': ['pyspark[sql]~=3.2.1'], 'pyspark3.3': ['pyspark[sql]~=3.3.2'], 'snowflake': ['snowflake-connector-python[pandas]~=2.8'], 'snowpark': ['snowflake-snowpark-python[pandas]~=1.0'], 'athena': ['awswrangler~=2.15']},
     packages=packages,
 )
```

### Comparing `tecton-0.7.0b9/tecton/_internals/analytics.py` & `tecton-0.7.0rc0/tecton/_internals/analytics.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,30 +7,28 @@
 from concurrent.futures import ThreadPoolExecutor
 from dataclasses import dataclass
 from typing import Dict
 from typing import Optional
 
 import pendulum
 
-from tecton import conf
 from tecton import version
 from tecton._internals import metadata_service
 from tecton._internals.env_utils import get_current_username
-from tecton_core.logger import get_logger
-from tecton_core.logger import get_logging_level
+from tecton_core import conf
 from tecton_proto.amplitude import client_logging_pb2
 from tecton_proto.metadataservice.metadata_service_pb2 import IngestAnalyticsRequest
 from tecton_proto.metadataservice.metadata_service_pb2 import IngestClientLogsRequest
 from tecton_proto.metadataservice.metadata_service_pb2 import QueryStateUpdateResponse
 from tecton_proto.metadataservice.metadata_service_pb2 import QueryStateUpdateResponseV2
 
 
 SINGLE_POSITIONAL_RETURN_VALUE_NAME = "0"
 
-logger = get_logger("AnalyticsLogger")
+logger = logging.getLogger(__name__)
 
 
 @dataclass
 class StateUpdateEventMetrics:
     """Metrics to log to Amplitude for tecton plan, apply, delete."""
 
     num_total_fcos: int
@@ -118,15 +116,15 @@
         NOTE: do not use `conf.py` here as this will increase the latency for SDK calls
         """
         request = IngestClientLogsRequest()
         data = request.sdk_method_invocation
         data.trace_id = trace_id
         data.time.FromDatetime(pendulum.now("UTC"))
         data.user_id = get_current_username()
-        data.log_level = logging.getLevelName(get_logging_level())
+        data.log_level = logging.getLevelName(logging.getLogger("tecton").getEffectiveLevel())
         data.sdk_version = version.get_semantic_version() or ""
         data.python_version = platform.python_version()
         if obj:
             method_class = obj if isinstance(obj, type) else type(obj)
             data.class_name = method_class.__name__
             data.method_name = method.__name__
             # Checking the type of the object causes a dependency conflict.
```

### Comparing `tecton-0.7.0b9/tecton/_internals/athena_api.py` & `tecton-0.7.0rc0/tecton/_internals/athena_api.py`

 * *Files 20% similar despite different names*

```diff
@@ -6,14 +6,15 @@
 
 from tecton.framework.data_frame import TectonDataFrame
 from tecton_athena import sql_helper
 from tecton_core.errors import TectonAthenaNotImplementedError
 from tecton_core.feature_set_config import FeatureSetConfig
 from tecton_core.time_utils import get_timezone_aware_datetime
 
+
 TEMP_INPUT_PREFIX = "_TT_TEMP_INPUT_"
 
 
 def get_historical_features(
     feature_set_config: FeatureSetConfig,
     spine: Optional[Union[pandas.DataFrame, str]] = None,
     timestamp_key: Optional[str] = None,
@@ -22,25 +23,28 @@
     save: bool = False,
     save_as: Optional[str] = None,
     start_time: datetime = None,
     end_time: datetime = None,
     entities: Optional[Union[pandas.DataFrame]] = None,
 ) -> TectonDataFrame:
     if from_source:
-        raise TectonAthenaNotImplementedError(
-            "Retrieving features directly from data sources (i.e. using from_source=True) is not supported with Athena retrieval. Use from_source=False and feature views that have offline materialization enabled."
-        )
+        msg = "Retrieving features directly from data sources (i.e. using from_source=True) is not supported with Athena retrieval. Use from_source=False and feature views that have offline materialization enabled."
+        raise TectonAthenaNotImplementedError(msg)
     if save or save_as is not None:
-        raise TectonAthenaNotImplementedError("save is not supported for Athena")
+        msg = "save is not supported for Athena"
+        raise TectonAthenaNotImplementedError(msg)
     if timestamp_key is None and spine is not None:
-        raise TectonAthenaNotImplementedError("timestamp_key must be specified")
+        msg = "timestamp_key must be specified"
+        raise TectonAthenaNotImplementedError(msg)
     if entities is not None:
-        raise TectonAthenaNotImplementedError("entities is not supported right now")
+        msg = "entities is not supported right now"
+        raise TectonAthenaNotImplementedError(msg)
     if spine is not None and (start_time or end_time):
-        raise TectonAthenaNotImplementedError("If a spine is provided, start_time and end_time must not be provided")
+        msg = "If a spine is provided, start_time and end_time must not be provided"
+        raise TectonAthenaNotImplementedError(msg)
 
     start_time = get_timezone_aware_datetime(start_time)
     end_time = get_timezone_aware_datetime(end_time)
 
     return TectonDataFrame._create(
         sql_helper.get_historical_features(
             spine=spine,
```

### Comparing `tecton-0.7.0b9/tecton/_internals/data_frame_helper.py` & `tecton-0.7.0rc0/tecton/_internals/data_frame_helper.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,17 +1,16 @@
+import logging
 from typing import Optional
 
 import pendulum
 from pyspark.sql import DataFrame
 from pyspark.sql import functions
 
-from tecton_core.logger import get_logger
 
-
-logger = get_logger("Query Builder")
+logger = logging.getLogger(__name__)
 
 
 def _get_time_limits_of_dataframe(df: DataFrame, time_key: str) -> Optional[pendulum.Period]:
     """The returned range is inclusive at the beginning & exclusive at the end: [start, end)."""
     # Fetch lower and upper time bound of the spine so that we can demand the individual feature definitions
     # to limit the amount of data they fetch from the raw data sources.
     # Returns None if df is empty.
```

### Comparing `tecton-0.7.0b9/tecton/_internals/delete_keys_api.py` & `tecton-0.7.0rc0/tecton/_internals/delete_keys_api.py`

 * *Files 4% similar despite different names*

```diff
@@ -5,24 +5,24 @@
 import pandas as pd
 from pyspark import sql as pyspark_sql
 from pyspark.sql import functions as pyspark_functions
 from pyspark.sql import types as pyspark_types
 
 from tecton import tecton_context
 from tecton._internals import errors
+from tecton._internals import ingest_utils
 from tecton._internals import metadata_service
 from tecton._internals import spark_api
 from tecton._internals import utils
 from tecton_core import feature_definition_wrapper
 from tecton_core import id_helper
 from tecton_proto.common import fco_locator_pb2
 from tecton_proto.common import id_pb2
 from tecton_proto.metadataservice import metadata_service_pb2
 from tecton_proto.online_store import feature_value_pb2
-from tecton_spark import ingest_utils
 
 
 def delete_keys(
     online: bool,
     offline: bool,
     keys: Union[pyspark_sql.DataFrame, pd.DataFrame],
     feature_definition: feature_definition_wrapper.FeatureDefinitionWrapper,
@@ -94,15 +94,16 @@
     )
     return metadata_service.instance().GetDeleteEntitiesInfo(info_request)
 
 
 def _get_keys_spark_df(keys: Union[pyspark_sql.DataFrame, pd.DataFrame], view_schema) -> pyspark_sql.DataFrame:
     if isinstance(keys, pd.DataFrame):
         if len(keys) == 0:
-            raise errors.EMPTY_ARGUMENT("join_keys")
+            msg = "join_keys"
+            raise errors.EMPTY_ARGUMENT(msg)
         if len(keys.columns[keys.columns.duplicated()]):
             raise errors.DUPLICATED_COLS_IN_KEYS(", ".join(list(keys.columns)))
         return ingest_utils.convert_pandas_to_spark_df(keys, view_schema)
     elif isinstance(keys, pyspark_sql.DataFrame):
         return keys
     else:
         raise errors.INVALID_JOIN_KEY_TYPE(type(keys))
@@ -132,14 +133,15 @@
             if isinstance(item, int):
                 ret.feature_values.add().int64_value = item
             elif isinstance(item, str):
                 ret.feature_values.add().string_value = item
             elif item is None:
                 ret.feature_values.add().null_value.CopyFrom(feature_value_pb2.NullValue())
             else:
-                raise Exception(f"Unknown type: {type(item)}")
+                msg = f"Unknown type: {type(item)}"
+                raise Exception(msg)
         return b64encode(ret.SerializeToString()).decode()
 
     serialize = pyspark_functions.udf(serialize_fn, pyspark_types.StringType())
     return spark_keys_df.select(pyspark_functions.struct(*join_keys).alias("join_keys_array")).select(
         serialize("join_keys_array")
     )
```

### Comparing `tecton-0.7.0b9/tecton/_internals/display.py` & `tecton-0.7.0rc0/tecton/_internals/display.py`

 * *Files 2% similar despite different names*

```diff
@@ -81,17 +81,16 @@
 
         :param headings: N-element list of heading names.
         :param rows: List of N-element tuples containing each row's data.
         :max_width: Optional max-width for the table. Setting to zero creates an unlimited width table.
         """
         for row_index, row in enumerate(rows):
             if len(row) != len(headings):
-                raise ValueError(
-                    f"malformed dimensions: headings has length {len(headings)}, but row {row_index} has lenght {len(row)}"
-                )
+                msg = f"malformed dimensions: headings has length {len(headings)}, but row {row_index} has length {len(row)}"
+                raise ValueError(msg)
 
         text_table = Texttable()
         if max_width is not None:
             text_table.set_max_width(max_width)
         text_table.add_rows(rows, header=False)
         text_table.header(headings)
         text_table.set_deco(Texttable.HEADER)
@@ -111,15 +110,16 @@
         """
         Returns a Displayable based on key-value properties.
 
         :param items: List of 2-element tuples.
         """
         for row_index, item in enumerate(items):
             if len(item) != 2:
-                raise ValueError(f"row {row_index} has length {len(item)}, but should be 2")
+                msg = f"row {row_index} has length {len(item)}, but should be 2"
+                raise ValueError(msg)
 
         text_table = Texttable()
         text_table.add_rows(items, header=False)
         text_table.set_deco(Texttable.BORDER | Texttable.VLINES | Texttable.HLINES)
 
         # build HTML version
         html = cls._table_template.render(items=items)
@@ -154,15 +154,16 @@
             elif isinstance(v, dict):
                 # Render nested tables
                 value = cls._draw_nested_table_from_dict(v)
                 table_items.append((display_name, value))
             elif isinstance(v, str):
                 table_items.append((display_name, v))
             else:
-                raise TypeError(f"Unsupported type {type(v)}")
+                msg = f"Unsupported type {type(v)}"
+                raise TypeError(msg)
 
         text_table = Texttable()
         text_table.add_rows(table_items, header=False)
         text_table.set_deco(Texttable.BORDER | Texttable.VLINES | Texttable.HLINES)
         text_table.set_cols_valign(["m", "m"])
 
         # build HTML version
@@ -216,15 +217,16 @@
                 table_items.append((display_name, value))
             elif isinstance(v, dict):
                 value = str(cls._draw_nested_table_from_dict(v))
                 table_items.append((display_name, value))
             elif isinstance(v, str):
                 table_items.append((display_name, v))
             else:
-                raise TypeError(f"Unsupported type {type(v)}")
+                msg = f"Unsupported type {type(v)}"
+                raise TypeError(msg)
 
         text_table = Texttable()
         text_table.add_rows(table_items, header=False)
         text_table.set_deco(Texttable.HLINES)
         return text_table.draw()
```

### Comparing `tecton-0.7.0b9/tecton/_internals/env_utils.py` & `tecton-0.7.0rc0/tecton/_internals/env_utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 import getpass
 from typing import Optional
 
+
 _current_username = None
 
 
 def _get_databricks_user() -> Optional[str]:
     try:
         import IPython
```

### Comparing `tecton-0.7.0b9/tecton/_internals/errors.py` & `tecton-0.7.0rc0/tecton/_internals/errors.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,65 +1,98 @@
-from typing import Iterable
 from typing import List
 from typing import Set
 
 from tecton._internals.utils import KEY_DELETION_MAX
 from tecton_core.errors import TectonInternalError
 from tecton_core.errors import TectonValidationError
 
+
 # Generic
-INTERNAL_ERROR = lambda message: TectonInternalError(
-    f"We seem to have encountered an error. Please contact support for assistance. Error details: {message}"
-)
-MDS_INACCESSIBLE = lambda host_port: TectonInternalError(
-    f"Failed to connect to Tecton at {host_port}, please check your connectivity or contact support"
-)
-VALIDATION_ERROR_FROM_MDS = lambda message, trace_id: TectonValidationError(f"{message}, trace ID: {trace_id}")
+def INTERNAL_ERROR(message):
+    return TectonInternalError(
+        f"We seem to have encountered an error. Please contact support for assistance. Error details: {message}"
+    )
 
-INTERNAL_ERROR_FROM_MDS = lambda message, trace_id: TectonInternalError(
-    f"Internal Tecton server error, please contact support with error details: {message}, trace ID: {trace_id}"
-)
 
-INVALID_SPINE_TYPE = lambda t: TectonValidationError(
-    f"Invalid type of spine '{t}'. Spine must be an instance of [pyspark.sql.dataframe.DataFrame, " "pandas.DataFrame]."
-)
-UNSUPPORTED_OPERATION = lambda op, reason: TectonValidationError(f"Operation '{op}' is not supported: {reason}")
-INVALID_SPINE_TIME_KEY_TYPE_SPARK = lambda t: TectonValidationError(
-    f"Invalid type of timestamp_key column in the given spine. Expected TimestampType, got {t}"
-)
-INVALID_SPINE_TIME_KEY_TYPE_PANDAS = lambda t: TectonValidationError(
-    f"Invalid type of timestamp_key column in the given spine. Expected datetime, got {t}"
-)
-MISSING_SPINE_COLUMN = lambda param, col, existing_cols: TectonValidationError(
-    f"{param} column is missing from the spine. Expected to find '{col}' among available spine columns: '{', '.join(existing_cols)}'."
-)
-MISSING_REQUEST_DATA_IN_SPINE = lambda key, existing_cols: TectonValidationError(
-    f"Request context key '{key}' not found in spine schema. Expected to find '{key}' among available spine columns: '{', '.join(existing_cols)}'."
-)
-NONEXISTENT_WORKSPACE = lambda name, workspaces: TectonValidationError(
-    f'Workspace "{name}" not found. Available workspaces: {workspaces}'
-)
-INCORRECT_MATERIALIZATION_ENABLED_FLAG = lambda user_set_bool, server_side_bool: TectonValidationError(
-    f"'is_live={user_set_bool}' argument does not match the value on the server: {server_side_bool}"
-)
-UNSUPPORTED_OPERATION_IN_DEVELOPMENT_WORKSPACE = lambda op: TectonValidationError(
-    f"Operation '{op}' is not supported in a development workspace"
-)
-INVALID_JOIN_KEYS_TYPE = lambda t: TectonValidationError(
-    f"Invalid type for join_keys. Expected Dict[str, Union[int, str, bytes]], got {t}"
-)
-INVALID_REQUEST_DATA_TYPE = lambda t: TectonValidationError(
-    f"Invalid type for request_data. Expected Dict[str, Union[int, str, bytes, float]], got {t}"
-)
-INVALID_REQUEST_CONTEXT_TYPE = lambda t: TectonValidationError(
-    f"Invalid type for request_context_map. Expected Dict[str, Union[int, str, bytes, float]], got {t}"
-)
-FV_NEEDS_TO_BE_BATCH_TRIGGER_MANUAL = lambda fv_name: TectonValidationError(
-    f"batch_trigger of Feature View {fv_name} needs to be BatchTriggerType.MANUAL"
-)
+def MDS_INACCESSIBLE(host_port):
+    return TectonInternalError(
+        f"Failed to connect to Tecton at {host_port}, please check your connectivity or contact support"
+    )
+
+
+def VALIDATION_ERROR_FROM_MDS(message, trace_id):
+    return TectonValidationError(f"{message}, trace ID: {trace_id}")
+
+
+def INTERNAL_ERROR_FROM_MDS(message, trace_id):
+    return TectonInternalError(
+        f"Internal Tecton server error, please contact support with error details: {message}, trace ID: {trace_id}"
+    )
+
+
+def INVALID_SPINE_TYPE(t):
+    return TectonValidationError(
+        f"Invalid type of spine '{t}'. Spine must be an instance of [pyspark.sql.dataframe.DataFrame, pandas.DataFrame]."
+    )
+
+
+def UNSUPPORTED_OPERATION(op, reason):
+    return TectonValidationError(f"Operation '{op}' is not supported: {reason}")
+
+
+def INVALID_SPINE_TIME_KEY_TYPE_SPARK(t):
+    return TectonValidationError(
+        f"Invalid type of timestamp_key column in the given spine. Expected TimestampType, got {t}"
+    )
+
+
+def INVALID_SPINE_TIME_KEY_TYPE_PANDAS(t):
+    return TectonValidationError(f"Invalid type of timestamp_key column in the given spine. Expected datetime, got {t}")
+
+
+def MISSING_SPINE_COLUMN(param, col, existing_cols):
+    return TectonValidationError(
+        f"{param} column is missing from the spine. Expected to find '{col}' among available spine columns: '{', '.join(existing_cols)}'."
+    )
+
+
+def MISSING_REQUEST_DATA_IN_SPINE(key, existing_cols):
+    return TectonValidationError(
+        f"Request context key '{key}' not found in spine schema. Expected to find '{key}' among available spine columns: '{', '.join(existing_cols)}'."
+    )
+
+
+def NONEXISTENT_WORKSPACE(name, workspaces):
+    return TectonValidationError(f'Workspace "{name}" not found. Available workspaces: {workspaces}')
+
+
+def INCORRECT_MATERIALIZATION_ENABLED_FLAG(user_set_bool, server_side_bool):
+    return TectonValidationError(
+        f"'is_live={user_set_bool}' argument does not match the value on the server: {server_side_bool}"
+    )
+
+
+def UNSUPPORTED_OPERATION_IN_DEVELOPMENT_WORKSPACE(op):
+    return TectonValidationError(f"Operation '{op}' is not supported in a development workspace")
+
+
+def INVALID_JOIN_KEYS_TYPE(t):
+    return TectonValidationError(f"Invalid type for join_keys. Expected Dict[str, Union[int, str, bytes]], got {t}")
+
+
+def INVALID_REQUEST_DATA_TYPE(t):
+    return TectonValidationError(
+        f"Invalid type for request_data. Expected Dict[str, Union[int, str, bytes, float]], got {t}"
+    )
+
+
+def INVALID_REQUEST_CONTEXT_TYPE(t):
+    return TectonValidationError(
+        f"Invalid type for request_context_map. Expected Dict[str, Union[int, str, bytes, float]], got {t}"
+    )
 
 
 def INVALID_INDIVIDUAL_JOIN_KEY_TYPE(key: str, type_str: str):
     return TectonValidationError(
         f"Invalid type for join_key '{key}'. Expected either type int, str, or bytes, got {type_str}"
     )
 
@@ -94,91 +127,111 @@
     )
 
 
 def UNKNOWN_REQUEST_CONTEXT_KEY(keys, key):
     return TectonValidationError(f"Unknown request context key '{key}', expected one of: {keys}")
 
 
-FV_TIME_KEY_MISSING = lambda fv_name: TectonValidationError(
-    f"Argument 'timestamp_key' is required for the feature definition '{fv_name}'"
-)
-FV_NO_MATERIALIZED_DATA = lambda fv_name: TectonValidationError(
-    f"Feature definition '{fv_name}' doesn't have any materialized data. "
-    "Materialization jobs may not have updated the offline feature store yet. "
-    "Please monitor using materialization_status() or use from_source=True to compute from source data."
-)
+def FV_TIME_KEY_MISSING(fv_name):
+    return TectonValidationError(f"Argument 'timestamp_key' is required for the feature definition '{fv_name}'")
+
+
+def FV_NO_MATERIALIZED_DATA(fv_name):
+    return TectonValidationError(
+        f"Feature definition '{fv_name}' doesn't have any materialized data. Materialization jobs may not have updated the offline feature store yet. Please monitor using materialization_status() or use from_source=True to compute from source data."
+    )
+
 
 FV_NOT_SUPPORTED_PREVIEW = TectonValidationError(
     "This method cannot be used with this type of Feature Definition. Please use get_historical_features(spine=spine)."
 )
 FD_PREVIEW_NO_MATERIALIZED_OFFLINE_DATA = TectonValidationError(
-    f"No materialized offline data found. If this Feature Definition was recently created,"
+    "No materialized offline data found. If this Feature Definition was recently created,"
     + " its materialization backfill may still be in progress. This can be monitored using materialization_status()."
     + " In the meantime, you can set use_materialized_data=False on preview() to compute features directly from data sources."
 )
 FV_GET_FEATURE_DF_NO_SPINE = TectonValidationError("get_feature_dataframe() requires a 'spine' argument.")
 
 
-ODFV_GET_MATERIALIZED_FEATURES_FROM_DEVELOPMENT_WORKSPACE = lambda odfv_name, workspace: TectonValidationError(
-    f"On-Demand Feature View {odfv_name} is in workspace {workspace}, which is a development workspace (does not have materialization enabled). "
-    "Please use from_source=True when retrieving features or alternatively use a live workspace and configure offline materialization for all dependent Feature Views."
-)
+def ODFV_GET_MATERIALIZED_FEATURES_FROM_DEVELOPMENT_WORKSPACE(odfv_name, workspace):
+    return TectonValidationError(
+        f"On-Demand Feature View {odfv_name} is in workspace {workspace}, which is a development workspace (does not have materialization enabled). Please use from_source=True when retrieving features or alternatively use a live workspace and configure offline materialization for all dependent Feature Views."
+    )
 
-FD_GET_MATERIALIZED_FEATURES_FROM_DEVELOPMENT_WORKSPACE = lambda fd_name, workspace: TectonValidationError(
-    f"Feature Definition {fd_name} is in workspace {workspace}, which is a development workspace (does not have materialization enabled). "
-    "Please use from_source=True when getting features (not applicable for Feature Tables) or "
-    "alternatively configure offline materialization for this Feature Definition in a live workspace."
-)
 
-FEATURE_TABLE_GET_MATERIALIZED_FEATURES_FROM_DEVELOPMENT_WORKSPACE = lambda ft_name, workspace: TectonValidationError(
-    f"Feature Table {ft_name} is in workspace {workspace}, which is a development workspace (does not have materialization enabled). "
-    "Please apply this Feature Table to a live workspace and ingest some features before using with get_historical_features()."
-)
+def FD_GET_MATERIALIZED_FEATURES_FROM_DEVELOPMENT_WORKSPACE(fd_name, workspace):
+    return TectonValidationError(
+        f"Feature Definition {fd_name} is in workspace {workspace}, which is a development workspace (does not have materialization enabled). Please use from_source=True when getting features (not applicable for Feature Tables) or alternatively configure offline materialization for this Feature Definition in a live workspace."
+    )
 
-FEATURE_TABLE_GET_ONLINE_FEATURES_FROM_DEVELOPMENT_WORKSPACE = lambda ft_name, workspace: TectonValidationError(
-    f"Feature Table {ft_name} is in workspace {workspace}, which is a development workspace (does not have materialization enabled). "
-    "Please apply this Feature Table to a live workspace and ingest some features before using with get_online_features()."
-)
 
-FEATURE_TABLE_GET_MATERIALIZED_FEATURES_OFFLINE_FALSE = lambda ft_name: TectonValidationError(
-    f"Feature Table {ft_name} does not have offline materialization enabled, i.e. offline=True. Cannot retrieve offline feature if offline materializaiton is not enabled."
-)
+def FEATURE_TABLE_GET_MATERIALIZED_FEATURES_FROM_DEVELOPMENT_WORKSPACE(ft_name, workspace):
+    return TectonValidationError(
+        f"Feature Table {ft_name} is in workspace {workspace}, which is a development workspace (does not have materialization enabled). Please apply this Feature Table to a live workspace and ingest some features before using with get_historical_features()."
+    )
 
-FD_GET_MATERIALIZED_FEATURES_FROM_LOCAL_OBJECT = lambda fv_name, fco_name: TectonValidationError(
-    f"{fco_name} {fv_name} is defined locally, i.e. it has not been applied to a Tecton workspace. "
-    "In order to force fetching data from the Offline store (i.e. from_source=False) this Feature View must be applied to a Live workspace and have materialization enabled (i.e. offline=True)."
-)
 
-FD_GET_MATERIALIZED_FEATURES_FROM_DEVELOPMENT_WORKSPACE_GFD = lambda fv_name, workspace: TectonValidationError(
-    f"Feature View {fv_name} is in workspace {workspace}, which is a development workspace (does not have materialization enabled). "
-    "In order to force fetching data from the Offline store (i.e. from_source=False) this Feature View must be applied to a Live workspace and have materialization enabled (i.e. offline=True)."
-)
+def FEATURE_TABLE_GET_ONLINE_FEATURES_FROM_DEVELOPMENT_WORKSPACE(ft_name, workspace):
+    return TectonValidationError(
+        f"Feature Table {ft_name} is in workspace {workspace}, which is a development workspace (does not have materialization enabled). Please apply this Feature Table to a live workspace and ingest some features before using with get_online_features()."
+    )
 
-FV_WITH_INC_BACKFILLS_GET_MATERIALIZED_FEATURES_FROM_DEVELOPMENT_WORKSPACE = lambda fv_name, workspace: TectonValidationError(
-    f"Feature view {fv_name} uses incremental backfills and is in workspace {workspace}, which is a development workspace (does not have materialization enabled). "
-    + "Computing features from source is not supported for Batch Feature Views with incremental_backfills set to True. "
-    + "Enable offline materialization for this feature view in a live workspace to use `get_historical_features()`, or use `run()` to test this feature view without materializing data."
-)
 
-FV_WITH_INC_BACKFILLS_GET_MATERIALIZED_FEATURES_IN_LOCAL_MODE = lambda fv_name: TectonValidationError(
-    f"Feature view {fv_name} uses incremental backfills and is locally defined which means materialization is not enabled."
-    + "Computing features from source is not supported for Batch Feature Views with incremental_backfills set to True. "
-    + "Apply and enable offline materialization for this feature view in a live workspace to use `get_historical_features()`, or use `run()` to test this feature view locally in a notebook."
-)
+def FEATURE_TABLE_GET_MATERIALIZED_FEATURES_OFFLINE_FALSE(ft_name):
+    return TectonValidationError(
+        f"Feature Table {ft_name} does not have offline materialization enabled, i.e. offline=True. Cannot retrieve offline feature if offline materializaiton is not enabled."
+    )
 
-FD_GET_FEATURES_MATERIALIZATION_DISABLED = lambda fd_name: TectonValidationError(
-    f"Feature View {fd_name} does not have offline materialization turned on. "
-    "In order to force fetching data from the Offline store (i.e. from_source=False) this Feature View must be applied to a Live workspace and have materialization enabled (i.e. offline=True)."
-)
 
-FV_GET_FEATURES_MATERIALIZATION_DISABLED_GFD = lambda fv_name: TectonValidationError(
-    f"Feature View {fv_name} does not have offline materialization turned on. "
-    f"Try calling this function with 'use_materialized_data=False' "
-    "or alternatively configure offline materialization for this Feature View."
-)
+def FD_GET_MATERIALIZED_FEATURES_FROM_LOCAL_OBJECT(fv_name, fco_name):
+    return TectonValidationError(
+        f"{fco_name} {fv_name} is defined locally, i.e. it has not been applied to a Tecton workspace. In order to force fetching data from the Offline store (i.e. from_source=False) this Feature View must be applied to a Live workspace and have materialization enabled (i.e. offline=True)."
+    )
+
+
+def FD_GET_MATERIALIZED_FEATURES_FROM_DEVELOPMENT_WORKSPACE_GFD(fv_name, workspace):
+    return TectonValidationError(
+        f"Feature View {fv_name} is in workspace {workspace}, which is a development workspace (does not have materialization enabled). In order to force fetching data from the Offline store (i.e. from_source=False) this Feature View must be applied to a Live workspace and have materialization enabled (i.e. offline=True)."
+    )
+
+
+def FV_WITH_INC_BACKFILLS_GET_MATERIALIZED_FEATURES_FROM_DEVELOPMENT_WORKSPACE(fv_name, workspace):
+    return TectonValidationError(
+        f"Feature view {fv_name} uses incremental backfills and is in workspace {workspace}, which is a development workspace (does not have materialization enabled). "
+        + "Computing features from source is not supported for Batch Feature Views with incremental_backfills set to True. "
+        + "Enable offline materialization for this feature view in a live workspace to use `get_historical_features()`, or use `run()` to test this feature view without materializing data."
+    )
+
+
+def FV_WITH_INC_BACKFILLS_GET_MATERIALIZED_FEATURES_IN_LOCAL_MODE(fv_name):
+    return TectonValidationError(
+        f"Feature view {fv_name} uses incremental backfills and is locally defined which means materialization is not enabled."
+        + "Computing features from source is not supported for Batch Feature Views with incremental_backfills set to True. "
+        + "Apply and enable offline materialization for this feature view in a live workspace to use `get_historical_features()`, or use `run()` to test this feature view locally in a notebook."
+    )
+
+
+def FV_WITH_INC_BACKFILLS_GET_MATERIALIZED_FEATURES_MOCK_DATA(fv_name: str):
+    return TectonValidationError(
+        f"Feature view {fv_name} uses incremental backfills and is locally defined which means materialization is not enabled."
+        + "Computing features from mock data is not supported for Batch Feature Views with incremental_backfills set to True."
+        + "Apply and enable offline materialization for this feature view in a live workspace to use `get_historical_features()`, or use `run()` to test this feature view."
+    )
+
+
+def FD_GET_FEATURES_MATERIALIZATION_DISABLED(fd_name):
+    return TectonValidationError(
+        f"Feature View {fd_name} does not have offline materialization turned on. In order to force fetching data from the Offline store (i.e. from_source=False) this Feature View must be applied to a Live workspace and have materialization enabled (i.e. offline=True)."
+    )
+
+
+def FV_GET_FEATURES_MATERIALIZATION_DISABLED_GFD(fv_name):
+    return TectonValidationError(
+        f"Feature View {fv_name} does not have offline materialization turned on. Try calling this function with 'use_materialized_data=False' or alternatively configure offline materialization for this Feature View."
+    )
 
 
 # DataSources
 DS_STREAM_PREVIEW_ON_NON_STREAM = TectonValidationError("'start_stream_preview' called on non-streaming data source")
 
 DS_DATAFRAME_NO_TIMESTAMP = TectonValidationError(
     "Cannot find timestamp column for this data source. Please call 'get_dataframe' without parameters 'start_time' or 'end_time'."
@@ -215,55 +268,52 @@
 
 FS_API_KEY_MISSING = TectonValidationError(
     "API key is required for online feature requests, but was not found in the environment. Please generate a key and set TECTON_API_KEY "
     + "using https://docs.tecton.ai/v2/examples/fetch-real-time-features.html#generating-an-api-key"
 )
 
 
-# Entity
-def FV_INVALID_MOCK_SOURCES(mock_sources_keys: Iterable[str], fn_param: Iterable[str]):
-    mock_sources_keys = mock_sources_keys if mock_sources_keys else set()
+def FV_INVALID_MOCK_SOURCES(mock_sources_keys: List[str], fv_params: List[str]):
     return TectonValidationError(
-        f"Mock sources' keys {mock_sources_keys} do not match FeatureView's parameters {fn_param}"
+        f"Mock sources {mock_sources_keys} do not match the Feature View's input parameters {fv_params}"
     )
 
 
-def FV_INVALID_MOCK_INPUTS(mock_inputs: Set[str], inputs: Set[str]):
-    input_names = str(mock_inputs) if mock_inputs else "{}"
-    return TectonValidationError(f"Mock input names {input_names} do not match FeatureView's inputs {inputs}")
+def FV_INVALID_MOCK_INPUTS(mock_inputs: List[str], inputs: List[str]):
+    return TectonValidationError(f"Mock input {mock_inputs} do not match FeatureView's inputs {inputs}")
 
 
 def FV_INVALID_MOCK_INPUTS_NUM_ROWS(num_rows: List[int]):
     return TectonValidationError(
         f"Number of rows are not equal across all mock_inputs. Number of rows found are: {str(num_rows)}."
     )
 
 
-def FV_INVALID_MOCK_INPUT_SCHEMA(input_name: str, mock_columns: Set[str], expected_columns: Set[str]):
-    return TectonValidationError(
-        f"mock_inputs['{input_name}'] has mismatch schema columns {mock_columns}, expected {expected_columns}"
-    )
-
-
 def FV_UNSUPPORTED_ARG(invalid_arg_name: str):
     return TectonValidationError(f"Argument '{invalid_arg_name}' is not supported for this FeatureView type.")
 
 
 def FV_INVALID_ARG_VALUE(arg_name: str, value: str, expected: str):
     return TectonValidationError(f"Invalid argument value '{arg_name}={value}', supported value(s): '{expected}'")
 
 
 def FV_INVALID_ARG_COMBO(arg_names: List[str]):
     return TectonValidationError(f"Invalid argument combinations; {str(arg_names)} cannot be used together.")
 
 
-FT_UNABLE_TO_ACCESS_SOURCE_DATA = lambda fv_name: TectonValidationError(
-    f"The source data for FeatureTable {fv_name} does not exist. "
-    "Please use from_source=False when calling this function."
-)
+def FT_UNABLE_TO_ACCESS_SOURCE_DATA(fv_name):
+    return TectonValidationError(
+        f"The source data for FeatureTable {fv_name} does not exist. Please use from_source=False when calling this function."
+    )
+
+
+def UNVALIDATED_FEATURE_VIEWS_FROM_SOURCE_FALSE(fv_name):
+    return TectonValidationError(
+        f"Feature view {fv_name} is not validated since `TECTON_VALIDATION_MODE=skip`. When calling `get_historical_features` on unvalidated feature views, `from_source` cannot be set to False. Please remove the `from_source` parameter from your call or set `from_source=True`."
+    )
 
 
 class InvalidTransformationMode(TectonValidationError):
     def __init__(self, name: str, got: str, allowed_modes: List[str]):
         super().__init__(f"Transformation mode for '{name}' got '{got}', must be one of: {', '.join(allowed_modes)}")
 
 
@@ -312,19 +362,19 @@
 
 FS_GET_ONLINE_FEATURES_REQUIRED_ARGS = TectonValidationError(
     "get_online_features requires at least one of join_keys or request_data"
 )
 
 
 def GET_ONLINE_FEATURES_MISSING_REQUEST_KEY(keys: Set[str]):
-    return TectonValidationError(f"The following required keys are missing in request_data: " + ", ".join(keys))
+    return TectonValidationError("The following required keys are missing in request_data: " + ", ".join(keys))
 
 
 def GET_FEATURE_VECTOR_MISSING_REQUEST_KEY(keys: Set[str]):
-    return TectonValidationError(f"The following required keys are missing in request_context_map: " + ", ".join(keys))
+    return TectonValidationError("The following required keys are missing in request_context_map: " + ", ".join(keys))
 
 
 def GET_ONLINE_FEATURES_FS_NO_REQUEST_DATA(keys: List[str]):
     return TectonValidationError(
         "get_online_features requires the 'request_data' argument for this Feature Service since it contains an On-Demand Feature View. "
         + "Expected the following request data keys: "
         + ", ".join(keys)
@@ -349,98 +399,129 @@
 def GET_FEATURE_VECTOR_FV_NO_REQUEST_DATA(keys: List[str]):
     return TectonValidationError(
         "get_feature_vector requires the 'request_context_map' argument for On-Demand Feature Views. Expected the following request context keys: "
         + ", ".join(keys)
     )
 
 
-ODFV_WITH_FT_INPUT_DEV_WORKSPACE = lambda ft_name: TectonValidationError(
-    f"This On-Demand Feature View has a Feature Table, {ft_name}, as an input. Feature Table feature retrieval cannot be used in a dev workspace. Please apply the Feature Table to a live workspace to retrieve features."
-)
+def ODFV_WITH_FT_INPUT_DEV_WORKSPACE(ft_name):
+    return TectonValidationError(
+        f"This On-Demand Feature View has a Feature Table, {ft_name}, as an input. Feature Table feature retrieval cannot be used in a dev workspace. Please apply the Feature Table to a live workspace to retrieve features."
+    )
 
-ODFV_WITH_FT_INPUT_LOCAL_MODE = lambda ft_name: TectonValidationError(
-    f"This On-Demand Feature View has a Feature Table, {ft_name}, as an input. Feature Table feature retrieval cannot be used when the Feature Table is locally defined. Please apply the Feature Table to a live workspace to retrieve features."
-)
 
-ODFV_WITH_FT_INPUT_FROM_SOURCE = lambda ft_name: TectonValidationError(
-    f"This On-Demand Feature View has a Feature Table, {ft_name}, as an input. Feature Table features must be retrieved from the offline store, i.e. with from_source=False."
-)
+def ODFV_WITH_FT_INPUT_LOCAL_MODE(ft_name):
+    return TectonValidationError(
+        f"This On-Demand Feature View has a Feature Table, {ft_name}, as an input. Feature Table feature retrieval cannot be used when the Feature Table is locally defined. Please apply the Feature Table to a live workspace to retrieve features."
+    )
 
-ODFV_WITH_UNMATERIALIZED_FV_INPUT_FROM_SOURCE_FALSE = lambda fv_name: TectonValidationError(
-    f"This On-Demand Feature View has a Feature View, {fv_name}, as an input, which does not have materialization enabled (offline=False). Either retrieve features with from_source=True (not applicable for Feature Tables) or enable offline materialization for the input feature views."
-)
 
-LOCAL_ODFV_WITH_DEV_WORKSPACE_FV_INPUT_FROM_SOURCE_FALSE = lambda fv_name: TectonValidationError(
-    f"This On-Demand Feature View has a Feature View, {fv_name}, as an input, which belongs to a dev workspace and therefore does not have materialization enabled. Either retrieve features with from_source=True (not applicable for Feature Tables) or apply this Feature View to a live workspace with offline=True."
-)
+def ODFV_WITH_FT_INPUT_FROM_SOURCE(ft_name):
+    return TectonValidationError(
+        f"This On-Demand Feature View has a Feature Table, {ft_name}, as an input. Feature Table features must be retrieved from the offline store, i.e. with from_source=False."
+    )
+
+
+def ODFV_WITH_UNMATERIALIZED_FV_INPUT_FROM_SOURCE_FALSE(fv_name):
+    return TectonValidationError(
+        f"This On-Demand Feature View has a Feature View, {fv_name}, as an input, which does not have materialization enabled (offline=False). Either retrieve features with from_source=True (not applicable for Feature Tables) or enable offline materialization for the input feature views."
+    )
+
+
+def LOCAL_ODFV_WITH_DEV_WORKSPACE_FV_INPUT_FROM_SOURCE_FALSE(fv_name):
+    return TectonValidationError(
+        f"This On-Demand Feature View has a Feature View, {fv_name}, as an input, which belongs to a dev workspace and therefore does not have materialization enabled. Either retrieve features with from_source=True (not applicable for Feature Tables) or apply this Feature View to a live workspace with offline=True."
+    )
+
+
+def LOCAL_ODFV_WITH_LOCAL_FV_INPUT_FROM_SOURCE_FALSE(fv_name):
+    return TectonValidationError(
+        f"This On-Demand Feature View has a Feature View, {fv_name}, as an input, that is locally defined and therefore does not have materialization enabled. Either retrieve features with from_source=True or apply this Feature View to a live workspace with offline=True."
+    )
 
-LOCAL_ODFV_WITH_LOCAL_FV_INPUT_FROM_SOURCE_FALSE = lambda fv_name: TectonValidationError(
-    f"This On-Demand Feature View has a Feature View, {fv_name}, as an input, that is locally defined and therefore does not have materialization enabled. Either retrieve features with from_source=True or apply this Feature View to a live workspace with offline=True."
-)
 
 FROM_SOURCE_WITH_FT = TectonValidationError(
-    f"Computing features from source is not supported for Feature Tables. Try calling this method with from_source=False."
+    "Computing features from source is not supported for Feature Tables. Try calling this method with from_source=False."
 )
 
 USE_MATERIALIZED_DATA_WITH_FT = TectonValidationError(
-    f"Computing features from source is not supported for Feature Tables. Try calling this method with use_materialized_data=True."
+    "Computing features from source is not supported for Feature Tables. Try calling this method with use_materialized_data=True."
 )
 
 
 FS_WITH_FT_DEVELOPMENT_WORKSPACE = TectonValidationError(
-    f"This Feature Service contains a Feature Table and fetching historical features for Feature Tables is not supported in a development workspace. This method is only supported in live workspaces."
+    "This Feature Service contains a Feature Table and fetching historical features for Feature Tables is not supported in a development workspace. This method is only supported in live workspaces."
 )
 
 
 FV_WITH_FT_DEVELOPMENT_WORKSPACE = TectonValidationError(
-    f"This Feature View has a Feature Table input and fetching historical features for Feature Tables is not supported in a development workspace. This method is only supported in live workspaces."
+    "This Feature View has a Feature Table input and fetching historical features for Feature Tables is not supported in a development workspace. This method is only supported in live workspaces."
 )
 
 
 # Backfill Config Validation
 BFC_MODE_SINGLE_REQUIRED_FEATURE_END_TIME_WHEN_START_TIME_SET = TectonValidationError(
     "feature_end_time is required when feature_start_time is set, for a FeatureView with "
     + "single_batch_schedule_interval_per_job backfill mode."
 )
 
 BFC_MODE_SINGLE_INVALID_FEATURE_TIME_RANGE = TectonValidationError(
     "Run with single_batch_schedule_interval_per_job backfill mode only supports time range equal to batch_schedule"
 )
 
-INCORRECT_KEYS = lambda keys, join_keys: TectonValidationError(
-    f"Requested keys to be deleted ({keys}) do not match the expected join keys ({join_keys})."
-)
 
-NO_STORE_SELECTED = TectonValidationError(f"One of online or offline store must be selected.")
+def INCORRECT_KEYS(keys, join_keys):
+    return TectonValidationError(
+        f"Requested keys to be deleted ({keys}) do not match the expected join keys ({join_keys})."
+    )
+
+
+NO_STORE_SELECTED = TectonValidationError("One of online or offline store must be selected.")
 
 TOO_MANY_KEYS = TectonValidationError(f"Max number of keys to be deleted is {KEY_DELETION_MAX}.")
 
 OFFLINE_STORE_NOT_SUPPORTED = TectonValidationError(
     "Only DeltaLake is supported for entity deletion in offline feature stores."
 )
 
 FV_UNSUPPORTED_AGGREGATION = TectonValidationError(
-    f"Argument 'aggregation_level' is not supported for Feature Views with `aggregations` not specified."
-)
-INVALID_JOIN_KEY_TYPE = lambda t: TectonValidationError(
-    f"Invalid type of join keys '{t}'. Keys must be an instance of [pyspark.sql.dataframe.DataFrame, "
-    "pandas.DataFrame]."
+    "Argument 'aggregation_level' is not supported for Feature Views with `aggregations` not specified."
 )
-DUPLICATED_COLS_IN_KEYS = lambda t: TectonValidationError(f"Argument keys {t} have duplicated column names. ")
 
-MISSING_SNOWFAKE_CONNECTION_REQUIREMENTS = lambda param: TectonValidationError(
-    f"Snowflake connection is missing the variable {param}. Please ensure the following parameters are set when creating your snowflake connection:  database, warehouse, and schema. "
-)
+
+def INVALID_JOIN_KEY_TYPE(t):
+    return TectonValidationError(
+        f"Invalid type of join keys '{t}'. Keys must be an instance of [pyspark.sql.dataframe.DataFrame, pandas.DataFrame]."
+    )
+
+
+def DUPLICATED_COLS_IN_KEYS(t):
+    return TectonValidationError(f"Argument keys {t} have duplicated column names. ")
+
+
+def MISSING_SNOWFAKE_CONNECTION_REQUIREMENTS(param):
+    return TectonValidationError(
+        f"Snowflake connection is missing the variable {param}. Please ensure the following parameters are set when creating your snowflake connection:  database, warehouse, and schema. "
+    )
+
 
 ATHENA_COMPUTE_ONLY_SUPPORTED_IN_LIVE_WORKSPACE = TectonValidationError(
     "Athena compute can only be used in live workspaces. Current workspace is not live. Please unset ALPHA_ATHENA_COMPUTE_ENABLED or switch to a live workspace."
 )
 
 ATHENA_COMPUTE_NOT_SUPPORTED_IN_LOCAL_MODE = TectonValidationError(
-    "Athena compute can only be used in on applied Tecton objects in live workspaces. Using a locally definied Tecton object is not currently supported with Athena."
+    "Athena compute can only be used in on applied Tecton objects in live workspaces. Using a locally defined Tecton object is not currently supported with Athena."
+)
+
+ATHENA_COMPUTE_MOCK_SOURCES_UNSUPPORTED = TectonValidationError(
+    "Athena compute can only be used with materialized data in live workspaces. Using mock data is not supported with Athena."
+)
+
+SNOWFLAKE_COMPUTE_MOCK_SOURCES_UNSUPPORTED = TectonValidationError(
+    "Using mock data in `get_historical_features` is not supported with Snowflake."
 )
 
 
 # Notebook Development
 def TECTON_OBJECT_REQUIRES_VALIDATION(function_name: str, class_name: str, fco_name: str):
     return TectonValidationError(
         f"{class_name} '{fco_name}' must be validated before `{function_name}` can be called. Call `validate()` to validate this object. If you'd like to enable automatic validation, you can use `tecton.set_validation_mode('auto')` or set the environment variable `TECTON_VALIDATION_MODE=auto`. Note that validation requires connected compute (Spark/Snowflake/etc.) and makes requests to your Tecton instance's API."
```

### Comparing `tecton-0.7.0b9/tecton/_internals/find_spark.py` & `tecton-0.7.0rc0/tecton/_internals/find_spark.py`

 * *Files 14% similar despite different names*

```diff
@@ -8,13 +8,15 @@
     spec = importlib.util.find_spec("pyspark")
     if spec:
         # pyspark exists in the system path
         return
     tecton_spec = importlib.util.find_spec("tecton")
     vendor_path = Path(tecton_spec.origin).parent / "vendor/pyspark"
     if not vendor_path.exists():
-        raise Exception(f"Could not locate system pyspark or vendored pyspark (looked in {vendor_path})")
+        msg = f"Could not locate system pyspark or vendored pyspark (looked in {vendor_path})"
+        raise Exception(msg)
     # insert at the beginning. This is important because although we already know pyspark isn't anywhere in the path,
     # we've also vendored the corresponding version of py4j
     sys.path.insert(0, str(vendor_path))
-    if importlib.util.find_spec("pyspark") == None:
-        raise Exception(f"Could not import vendored pyspark (added path {vendor_path})")
+    if importlib.util.find_spec("pyspark") is None:
+        msg = f"Could not import vendored pyspark (added path {vendor_path})"
+        raise Exception(msg)
```

### Comparing `tecton-0.7.0b9/tecton/_internals/materialization_api.py` & `tecton-0.7.0rc0/tecton/_internals/materialization_api.py`

 * *Files 2% similar despite different names*

```diff
@@ -247,25 +247,28 @@
     while True:
         job_data = get_materialization_job(feature_view, workspace, job_id)
         run_state = job_data.state
 
         if run_state == "SUCCESS":
             return job_data
         elif timeout and ((datetime.now() - wait_start_time) > timeout):
-            raise MaterializationTimeoutException(f"job {job_id} timed out, last job state {run_state}")
+            msg = f"job {job_id} timed out, last job state {run_state}"
+            raise MaterializationTimeoutException(msg)
         elif run_state == "RUNNING":
             time.sleep(60)
         else:
-            raise MaterializationJobFailedException(f"job {job_id} failed, last job state {run_state}")
+            msg = f"job {job_id} failed, last job state {run_state}"
+            raise MaterializationJobFailedException(msg)
 
 
-def get_materialization_status_response(id_proto: id_pb2.Id) -> MaterializationStatus:
+def get_materialization_status_response(id_proto: id_pb2.Id, workspace: str) -> MaterializationStatus:
     """Returns MaterializationStatus proto for the FeatureView."""
     request = GetMaterializationStatusRequest()
     request.feature_package_id.CopyFrom(id_proto)
+    request.workspace = workspace
 
     response = metadata_service.instance().GetMaterializationStatus(request)
     return response.materialization_status
 
 
 def _create_materialization_table(column_names: List[str], materialization_status_rows: List[List]) -> Displayable:
     # Setting `max_width=0` creates a table with an unlimited width.
@@ -273,28 +276,28 @@
     # Align columns in the middle horizontally
     table._text_table.set_cols_align(["c" for _ in range(len(column_names))])
 
     return table
 
 
 def get_materialization_status_for_display(
-    id_proto: id_pb2.Id, verbose: bool, limit: int, sort_columns: Optional[str], errors_only: bool
+    id_proto: id_pb2.Id, workspace: str, verbose: bool, limit: int, sort_columns: Optional[str], errors_only: bool
 ) -> Displayable:
-    materialization_attempts = get_materialization_status_response(id_proto).materialization_attempts
+    materialization_attempts = get_materialization_status_response(id_proto, workspace).materialization_attempts
     column_names, materialization_status_rows = utils.format_materialization_attempts(
         materialization_attempts, verbose, limit, sort_columns, errors_only
     )
 
     return _create_materialization_table(column_names, materialization_status_rows)
 
 
 def get_deletion_status_for_display(
-    id_proto: id_pb2.Id, verbose: bool, limit: int, sort_columns: Optional[str], errors_only: bool
+    id_proto: id_pb2.Id, workspace: str, verbose: bool, limit: int, sort_columns: Optional[str], errors_only: bool
 ) -> Displayable:
-    materialization_attempts = get_materialization_status_response(id_proto).materialization_attempts
+    materialization_attempts = get_materialization_status_response(id_proto, workspace).materialization_attempts
     deletion_attempts = [
         attempt
         for attempt in materialization_attempts
         if attempt.data_source_type == DataSourceType.DATA_SOURCE_TYPE_DELETION
     ]
     column_names, materialization_status_rows = utils.format_materialization_attempts(
         deletion_attempts, verbose, limit, sort_columns, errors_only
```

### Comparing `tecton-0.7.0b9/tecton/_internals/metadata_service_impl/error_lib.py` & `tecton-0.7.0rc0/tecton/_internals/metadata_service_impl/error_lib.py`

 * *Files 12% similar despite different names*

```diff
@@ -4,15 +4,17 @@
 from tecton._internals import errors
 from tecton._internals.metadata_service_impl import auth_lib
 from tecton._internals.metadata_service_impl import trace
 from tecton_core.errors import FailedPreconditionError
 from tecton_core.errors import TectonAPIInaccessibleError
 from tecton_core.errors import TectonAPIValidationError
 from tecton_core.errors import TectonNotFoundError
-from tecton_core.logger import get_logging_level
+
+
+logger = logging.getLogger(__name__)
 
 
 class gRPCStatus(enum.Enum):
     """gRPC response status codes.
 
     Status codes are replicated here to avoid importing the `grpc.StatusCode` enum class,
     which requires the grpcio library.
@@ -54,33 +56,28 @@
     if status_code == gRPCStatus.INVALID_ARGUMENT.value:
         raise TectonAPIValidationError(details)
 
     if status_code == gRPCStatus.FAILED_PRECONDITION.value:
         raise FailedPreconditionError(details)
 
     if status_code == gRPCStatus.UNAUTHENTICATED.value:
-        raise PermissionError(
-            f"Tecton credentials are invalid, not configured, or expired ({details}). "
-            "To authenticate using an API key, set TECTON_API_KEY in your environment or use tecton.set_credentials(tecton_api_key=<key>). "
-            "To authenticate using the CLI, run `tecton login`."
-        )
+        msg = f"Tecton credentials are invalid, not configured, or expired ({details}). To authenticate using an API key, set TECTON_API_KEY in your environment or use tecton.set_credentials(tecton_api_key=<key>). To authenticate using the CLI, run `tecton login`."
+        raise PermissionError(msg)
 
     if status_code == gRPCStatus.PERMISSION_DENIED.value:
         if not auth_lib.request_has_token():
             # Remove this case in https://tecton.atlassian.net/browse/TEC-9107
-            raise PermissionError(
-                "Tecton credentials have insufficient permissions. "
-                "To authenticate using an API key, set TECTON_API_KEY in your environment or use tecton.set_credentials(tecton_api_key=<key>). "
-                "To authenticate using the CLI, run `tecton login`."
-            )
-        elif details != None and "InvalidToken" in details:
+            msg = "Tecton credentials have insufficient permissions. To authenticate using an API key, set TECTON_API_KEY in your environment or use tecton.set_credentials(tecton_api_key=<key>). To authenticate using the CLI, run `tecton login`."
+            raise PermissionError(msg)
+        elif details is not None and "InvalidToken" in details:
             # Remove this case in https://tecton.atlassian.net/browse/TEC-9107
-            raise PermissionError(f"Configured Tecton credentials are not valid ({details}).")
+            msg = f"Configured Tecton credentials are not valid ({details})."
+            raise PermissionError(msg)
         else:
-            raise PermissionError(f"Insufficient permissions ({details}).")
+            msg = f"Insufficient permissions ({details})."
+            raise PermissionError(msg)
     if status_code == gRPCStatus.NOT_FOUND.value:
         raise TectonNotFoundError(details)
 
-    if get_logging_level() < logging.INFO:
-        raise Exception(f"Unknown MDS exception. code={status_code}, details={details}")
+    logger.debug(f"Unknown MDS exception. code={status_code}, details={details}")
 
     raise errors.INTERNAL_ERROR_FROM_MDS(details, trace.get_trace_id())
```

### Comparing `tecton-0.7.0b9/tecton/_internals/metadata_service_impl/http_client.py` & `tecton-0.7.0rc0/tecton/_internals/metadata_service_impl/http_client.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,21 +1,22 @@
 import base64
+import logging
 from collections import defaultdict
 
 import requests
 
 from tecton._internals.metadata_service_impl import base_stub
 from tecton._internals.metadata_service_impl import error_lib
 from tecton._internals.metadata_service_impl import request_lib
 from tecton._internals.metadata_service_impl.response import MDSResponse
-from tecton._internals.metadata_service_impl.service_calls import get_method_name_to_grpc_call
 from tecton._internals.metadata_service_impl.service_calls import GrpcCall
-from tecton_core.logger import get_logger
+from tecton._internals.metadata_service_impl.service_calls import get_method_name_to_grpc_call
+
 
-logger = get_logger("metadata_service")
+logger = logging.getLogger(__name__)
 
 requests_session = requests.Session()
 
 
 class PureHTTPStub(base_stub.BaseStub):
     def __init__(self):
         """
@@ -30,15 +31,16 @@
         """
         Transforms methods called directly on PureHTTPStub to _InternalHTTPStub requests.
         E.g. PureHTTPStub::SomeMethod(request) is transformed to _InternalHTTPStub::execute('SomeMethod', request).
 
         An AttributeError is raised if the method invoked does not match a MetadataService RPC method.
         """
         if method_name not in self._method_name_to_grpc_call:
-            raise AttributeError(f"Nonexistent MetadataService method: {method_name}")
+            msg = f"Nonexistent MetadataService method: {method_name}"
+            raise AttributeError(msg)
 
         def method(request, timeout_sec: float = 300.0):
             return self._stub_obj.execute(self._method_name_to_grpc_call[method_name], request, timeout_sec)
 
         return method
 
     def close(self):
```

### Comparing `tecton-0.7.0b9/tecton/_internals/metadata_service_impl/request_lib.py` & `tecton-0.7.0rc0/tecton/_internals/metadata_service_impl/request_lib.py`

 * *Files 6% similar despite different names*

```diff
@@ -45,12 +45,11 @@
 
 def request_url() -> str:
     """
     :return: A Validated API service URL.
     """
     api_service = conf.get_or_none("API_SERVICE")
     if not api_service:
-        raise errors.TectonAPIValidationError(
-            "API_SERVICE not set. Please configure API_SERVICE or use tecton.set_credentials(tecton_url=<url>)"
-        )
+        msg = "API_SERVICE not set. Please configure API_SERVICE or use tecton.set_credentials(tecton_url=<url>)"
+        raise errors.TectonAPIValidationError(msg)
     conf.validate_api_service_url(api_service)
     return urllib.parse.urljoin(api_service + "/", "proxy")
```

### Comparing `tecton-0.7.0b9/tecton/_internals/metadata_service_impl/response.py` & `tecton-0.7.0rc0/tecton/_internals/metadata_service_impl/response.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/_internals/metadata_service_impl/service_calls.py` & `tecton-0.7.0rc0/tecton/_internals/metadata_service_impl/service_calls.py`

 * *Files 5% similar despite different names*

```diff
@@ -8,16 +8,23 @@
 from typing import Dict
 
 from google.protobuf.empty_pb2 import Empty
 
 from tecton_proto.auth import authorization_service_pb2
 from tecton_proto.materializationjobservice import materialization_job_service_pb2
 from tecton_proto.metadataservice import metadata_service_pb2
+from tecton_proto.remoteenvironmentservice import remote_environment_service_pb2
 
-grpc_service_modules = [metadata_service_pb2, materialization_job_service_pb2, authorization_service_pb2]
+
+grpc_service_modules = [
+    metadata_service_pb2,
+    materialization_job_service_pb2,
+    authorization_service_pb2,
+    remote_environment_service_pb2,
+]
 
 
 @dataclass
 class GrpcCall:
     """
     grpc call data representation
 
@@ -50,11 +57,10 @@
                 else:
                     response_deserializer = getattr(module, method.output_type.name).FromString
                 grpc_method = f"/{descriptor.full_name}/{method.name}"
                 grpc_call = GrpcCall(grpc_method, request_serializer, response_deserializer)
                 if method_name_to_call.get(method.name) is None:
                     method_name_to_call[method.name] = grpc_call
                 else:
-                    raise Exception(
-                        f"Method name collision for method {method.name} in service {service_name} and {method_name_to_call.get(method.name).method}"
-                    )
+                    msg = f"Method name collision for method {method.name} in service {service_name} and {method_name_to_call.get(method.name).method}"
+                    raise Exception(msg)
     return method_name_to_call
```

### Comparing `tecton-0.7.0b9/tecton/_internals/query_helper.py` & `tecton-0.7.0rc0/tecton/_internals/query_helper.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,7 +1,8 @@
+import math
 from datetime import datetime
 from json import JSONDecodeError
 from typing import Dict
 from typing import Mapping
 from typing import Optional
 from typing import Union
 from urllib.parse import urljoin
@@ -12,27 +13,27 @@
 from google.protobuf.json_format import MessageToDict
 from google.protobuf.json_format import MessageToJson
 from google.protobuf.json_format import Parse
 from google.protobuf.struct_pb2 import Value
 from requests.exceptions import HTTPError
 
 import tecton
-from tecton import conf
 from tecton._internals import errors
 from tecton.framework.data_frame import FeatureVector
+from tecton_core import conf
 from tecton_core import data_types
+from tecton_core.request_context import RequestContext
 from tecton_proto.api.featureservice.feature_service_pb2 import FeatureServerComplexDataType
 from tecton_proto.api.featureservice.feature_service_pb2 import FeatureServerDataType
 from tecton_proto.api.featureservice.feature_service_pb2 import GetFeaturesResponse
 from tecton_proto.api.featureservice.feature_service_pb2 import GetFeaturesResult
 from tecton_proto.api.featureservice.feature_service_pb2 import Metadata
 from tecton_proto.api.featureservice.feature_service_pb2 import QueryFeaturesRequest
 from tecton_proto.api.featureservice.feature_service_pb2 import QueryFeaturesResponse
 from tecton_proto.api.featureservice.feature_service_request_pb2 import GetFeaturesRequest
-from tecton_spark.request_context import RequestContext
 
 
 TYPE_BOOLEAN = "boolean"
 TYPE_FLOAT64 = "float64"
 TYPE_INT64 = "int64"
 TYPE_STRING = "string"
 TYPE_STRING_ARRAY = "string_array"
@@ -192,15 +193,16 @@
     def _detailed_http_raise_for_status(self, http_response):
         try:
             http_response.raise_for_status()
         except HTTPError as e:
             try:
                 details = http_response.json()
             except JSONDecodeError as json_e:
-                raise errors.FS_BACKEND_ERROR(f"unable to process response ({http_response.status_code} error)")
+                msg = f"unable to process response ({http_response.status_code} error)"
+                raise errors.FS_BACKEND_ERROR(msg)
 
             # Include the actual error message details in the exception
             # if available.
             if "message" in details and "code" in details:
                 raise errors.FS_BACKEND_ERROR(details["message"])
             else:
                 # Otherwise just throw the original error.
@@ -241,66 +243,107 @@
             # `FeatureServerComplexDataType.fields`.
             struct = {}
             for i, field in enumerate(data_type.fields):
                 python_value = self._pb_to_python_value(val.values[i], field.data_type)
                 if python_value is not None:
                     struct[field.name] = python_value
             return struct
+        elif data_type.type == FeatureServerDataType.map:
+            return {k: self._pb_to_python_value(v, data_type.value_type) for k, v in val.fields.items()}
         else:
-            raise Exception(
-                f"Unexpected type '{data_type}' - Expected float64, int64, string, boolean, array, or struct."
-            )
+            msg = f"Unexpected type '{data_type}' - Expected float64, int64, string, boolean, array, or struct."
+            raise Exception(msg)
 
     def _python_to_pb_value(self, key: str, api_value: "Value", python_value: Union[int, np.int_, str, bool]):
         """Converts a single value from a python type to a protobuf wrapped value. Infer based on python type."""
 
         # NB. bool is a subclass of int in Python for some weird reason so we check for it first
         if isinstance(python_value, bool):
             api_value.bool_value = python_value
         elif isinstance(python_value, int):
             api_value.string_value = str(python_value)
         elif isinstance(python_value, np.int_):
             api_value.string_value = str(python_value)
         elif isinstance(python_value, str):
             api_value.string_value = python_value
         else:
-            raise NotImplementedError(
-                f"Found type '{type(python_value).__name__} for {key}' - Expected one of int, str, bool"
-            )
+            msg = f"Found type '{type(python_value).__name__} for {key}' - Expected one of int, str, bool"
+            raise NotImplementedError(msg)
 
     def _request_context_to_pb_value(
         self,
         key: str,
         api_value: "Value",
-        python_value: Union[int, np.int_, str, bool, float, list],
+        python_value: Union[int, np.int_, str, bool, float, list, dict],
         data_type: data_types.DataType,
     ):
         if python_value is None:
             api_value.null_value = True
         elif data_type == data_types.BoolType():
             if not isinstance(python_value, bool):
-                raise TypeError(f"Invalid type for {key}: expected bool, got {type(python_value).__name__}")
+                msg = f"Invalid type for {key}: expected bool, got {type(python_value).__name__}"
+                raise TypeError(msg)
             api_value.bool_value = python_value
         elif data_type == data_types.Int64Type():
             if not isinstance(python_value, (int, np.int_)):
-                raise TypeError(
-                    f"Invalid type for {key}: expected int or numpy.int_, got {type(python_value).__name__}"
-                )
+                msg = f"Invalid type for {key}: expected int or numpy.int_, got {type(python_value).__name__}"
+                raise TypeError(msg)
             api_value.string_value = str(python_value)
         elif data_type == data_types.StringType():
             if not isinstance(python_value, str):
-                raise TypeError(f"Invalid type for {key}: expected str, got {type(python_value).__name__}")
+                msg = f"Invalid type for {key}: expected str, got {type(python_value).__name__}"
+                raise TypeError(msg)
             api_value.string_value = python_value
         elif data_type in (data_types.Float32Type(), data_types.Float64Type()):
             if not isinstance(python_value, (int, float, np.int_)):
-                raise TypeError(f"Invalid type for {key}: expected int or float, got {type(python_value).__name__}")
-            api_value.number_value = python_value
+                msg = f"Invalid type for {key}: expected int or float, got {type(python_value).__name__}"
+                raise TypeError(msg)
+            if python_value == float("inf"):
+                api_value.string_value = "Infinity"
+            elif python_value == float("-inf"):
+                api_value.string_value = "-Infinity"
+            elif math.isnan(python_value):
+                api_value.string_value = "NaN"
+            else:
+                api_value.number_value = python_value
         elif isinstance(data_type, data_types.ArrayType):
             if not isinstance(python_value, list):
-                raise TypeError(f"Invalid type for {key}: expected list, got {type(python_value).__name__}")
+                msg = f"Invalid type for {key}: expected list, got {type(python_value).__name__}"
+                raise TypeError(msg)
             api_value.list_value.SetInParent()  # Needed in the case of empty arrays.
             for item in python_value:
                 list_value = api_value.list_value.values.add()
                 self._request_context_to_pb_value(key + ".elementType", list_value, item, data_type.element_type)
+        elif isinstance(data_type, data_types.StructType):
+            if isinstance(python_value, list):
+                if len(python_value) != len(data_type.fields):
+                    msg = f"Inconsistent number of fields for {key}(a Struct type): expected {len(data_type.fields)} fields, got {len(python_value)} fields"
+                    raise TypeError(msg)
+                api_value.list_value.SetInParent()  # Needed in the case of empty arrays.
+                for i, item in enumerate(python_value):
+                    list_value = api_value.list_value.values.add()
+                    self._request_context_to_pb_value(
+                        f"{key}.{data_type.fields[i].name}", list_value, item, data_type.fields[i].data_type
+                    )
+            elif isinstance(python_value, dict):
+                for field in data_type.fields:
+                    self._request_context_to_pb_value(
+                        f"{key}.{field.name}",
+                        api_value.struct_value.fields[field.name],
+                        python_value.get(field.name),
+                        field.data_type,
+                    )
+            else:
+                msg = f"Invalid type for {key}: expected dict or list, got {type(python_value).__name__}"
+                raise TypeError(msg)
+        elif isinstance(data_type, data_types.MapType):
+            if not isinstance(python_value, dict):
+                msg = f"Invalid type for {key}: expected dict, got {type(python_value).__name__}"
+                raise TypeError(msg)
+            for k, v in python_value.items():
+                self._request_context_to_pb_value(
+                    key + ".value_type", api_value.struct_value.fields[k], v, data_type.value_type
+                )
         else:
             # should never happen
-            raise NotImplementedError(f"Data type {data_type} not supported")
+            msg = f"Data type {data_type} not supported"
+            raise NotImplementedError(msg)
```

### Comparing `tecton-0.7.0b9/tecton/_internals/repo/function_serialization.py` & `tecton-0.7.0rc0/tecton/_internals/repo/function_serialization.py`

 * *Files 3% similar despite different names*

```diff
@@ -12,31 +12,31 @@
 references to instantiated non-builtin types with a repr that can't be exec'd
 
 users referring to a renamed version of an imported module has some wonky
 behavior, for example `import json` then `foo=json` then referring to
 `foo` in the func to be serialized.
 """
 import inspect
+import logging
 from collections import defaultdict
 from types import FunctionType
 from types import ModuleType
 from typing import Callable
 
 from tecton.vendor.dill.dill.detect import freevars
 from tecton.vendor.dill.dill.detect import globalvars
 from tecton_core import conf
 from tecton_core import repo_file_handler
 from tecton_core.errors import TectonValidationError
-from tecton_core.logger import get_logger
 from tecton_proto.args import user_defined_function_pb2
 
 
 WINDOW_UNBOUNDED_PRECEDING = "unbounded_preceding"
 
-logger = get_logger("function_serialization")
+logger = logging.getLogger(__name__)
 
 
 def to_proto(transform: Callable) -> user_defined_function_pb2.UserDefinedFunction:
     if hasattr(transform, "_code"):
         code = transform._code
     else:
         code = _getsource(transform)
@@ -48,17 +48,16 @@
 
 BANNED_INDIRECT_IMPORTS = ["materialization_context", "tecton_sliding_window_udf", "WINDOW_UNBOUNDED_PRECEDING"]
 
 
 def _validate_code(code):
     for banned_import in BANNED_INDIRECT_IMPORTS:
         if f"tecton.{banned_import}" in code:
-            raise Exception(
-                f"Cannot serialize `tecton.{banned_import}`. Please use an import like `from tecton import {banned_import}`. Note that even comments may trigger this if your code contains `tecton.{banned_import}`"
-            )
+            msg = f"Cannot serialize `tecton.{banned_import}`. Please use an import like `from tecton import {banned_import}`. Note that even comments may trigger this if your code contains `tecton.{banned_import}`"
+            raise Exception(msg)
 
 
 # If these are in the path to a module, we don't expect the function definitions to be inside a feature repo, but we still want to inline them
 # integration_tests/ is added as a hack because our integration tests aren't run in a feature repo. But unless a customer is importing from a python package that has a directory named integration_tests, it
 # shouldn't cause any wrong behavior.
 # IMPORTANT: Include backslash paths for windows.
 TECTON_TRANSFORMATION_WHITELIST = ["tecton/compat", "tecton\compat", "integration_tests/"]
@@ -81,17 +80,22 @@
     forced_behavior = conf.get_or_none("TECTON_FORCE_FUNCTION_SERIALIZATION")
     if forced_behavior is not None:
         if forced_behavior.lower() == "true":
             return True
         elif forced_behavior.lower() == "false":
             return False
         else:
-            raise ValueError("TECTON_FORCE_FUNCTION_SERIALIZATION should be 'true', 'false', or unset.")
+            msg = "TECTON_FORCE_FUNCTION_SERIALIZATION should be 'true', 'false', or unset."
+            raise ValueError(msg)
 
     func_module = inspect.getmodule(func)
+    if func_module is None:
+        # Used by NDD with EMR. In EMR pyspark notebook, 'inspect.getmodule' returns None for functions defined in main scope.
+        return False
+
     if func_module.__name__ == "__main__":
         # This function was defined in the main module (i.e. probably in a notebook or repl). Do not attempt to
         # serialize it.
         return False
 
     func_file = inspect.getfile(func_module)
 
@@ -100,17 +104,16 @@
         return True
 
     if repo_file_handler.is_file_in_a_tecton_repo(func_file):
         # If a function is in a Tecton repo (i.e. an ancestor directory has a .tecton file), then always attemp to
         # serialize it. In notebook development, this will occur if an FCO is imported into the notebook from a repo.
         repo_file_handler.ensure_prepare_repo(func_file)
         if func_file not in repo_file_handler.repo_files_set():
-            raise TectonValidationError(
-                f'Error during function serialization. The file "{func_file}" is in a Tecton repo but not in the cached repo data. Importing Tecton objects from multiple repos is not supported.'
-            )
+            msg = f'Error during function serialization. The file "{func_file}" is in a Tecton repo but not in the cached repo data. Importing Tecton objects from multiple repos is not supported.'
+            raise TectonValidationError(msg)
         return True
     else:
         # This function is defined outside of the main module, but not in a Tecton repo. Do not attempt to serialize it.
         return False
 
 
 # UNDONE:
@@ -177,15 +180,15 @@
         seen_key = str(name) + str(obj)
         if seen_args.get(seen_key) is True:
             return
         seen_args[seen_key] = True
 
         has_spark = True
         try:
-            import pyspark
+            import pyspark  # noqa: F401
         except ImportError:
             has_spark = False
 
         is_spark_struct = False
         is_unbound_materialization_context = False
         is_tecton_sliding_window_udf = False
         if has_spark:
@@ -199,26 +202,26 @@
             from tecton_spark.udfs import tecton_sliding_window_udf
 
             is_tecton_sliding_window_udf = obj is tecton_sliding_window_udf
 
         # Confusingly classes are subtypes of 'type'; non-classes are not
         if isinstance(obj, type):
             if obj.__module__ == "__main__":
-                raise Exception(f"Cannot serialize class {obj.__name__} from module __main__")
+                msg = f"Cannot serialize class {obj.__name__} from module __main__"
+                raise Exception(msg)
             imports[obj.__module__].add(obj.__name__)
 
         elif isinstance(obj, FunctionType):
             process_functiontype(name, obj, imports, modules, code_lines, seen_args)
         elif isinstance(obj, ModuleType):
             module_file = inspect.getfile(obj)
             # skip this check for functions written by Tecton
             if not is_tecton_defined(func_file) and module_file in repo_file_handler.repo_files_set():
-                raise Exception(
-                    f"Cannot serialize usage of {obj.__name__}. Instead, directly import objects from the module, e.g. `from {obj.__name__} import obj`"
-                )
+                msg = f"Cannot serialize usage of {obj.__name__}. Instead, directly import objects from the module, e.g. `from {obj.__name__} import obj`"
+                raise Exception(msg)
             if f"{obj.__package__}.{name}" == obj.__name__:
                 imports[obj.__package__].add(name)
             else:
                 modules.add(obj.__name__)
         elif is_spark_struct:
             _add_codeline(f"{name} = StructType.fromJson(json.loads('{obj.json()}'))")
             modules.add("json")
@@ -231,15 +234,16 @@
             imports["tecton_spark.time_utils"].add("WINDOW_UNBOUNDED_PRECEDING")
         else:
             try:
                 repr_str = f"{name}={repr(obj)}"
                 exec(repr_str)
                 _add_codeline(repr_str)
             except Exception:
-                raise Exception(f"Cannot evaluate object {obj} of type '{type(obj)}' for serialization")
+                msg = f"Cannot evaluate object {obj} of type '{type(obj)}' for serialization"
+                raise Exception(msg)
 
     recurse(func.__name__, func, imports, modules, code_lines, seen_args, write_codelines=True)
 
     for module in sorted(imports):
         import_line = f"from {module} import "
         import_line += ", ".join(sorted(imports[module]))
         code_lines.insert(0, import_line)
```

### Comparing `tecton-0.7.0b9/tecton/_internals/rewrite.py` & `tecton-0.7.0rc0/tecton/_internals/rewrite.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,15 +1,14 @@
 from typing import Dict
 from typing import List
 from typing import Optional
 from typing import Type
 
 import attrs
 import pendulum
-from pyspark.sql import SparkSession
 
 from tecton._internals import time_utils
 from tecton_core import conf
 from tecton_core.pipeline_common import get_time_window_from_data_source_node
 from tecton_core.query.node_interface import NodeRef
 from tecton_core.query.node_interface import QueryNode
 from tecton_core.query.nodes import AddAnchorTimeNode
@@ -25,32 +24,15 @@
 from tecton_core.query.nodes import OfflineStoreScanNode
 from tecton_core.query.nodes import PartialAggNode
 from tecton_core.query.nodes import RenameColsNode
 from tecton_core.query.nodes import RespectFeatureStartTimeNode
 from tecton_core.query.nodes import SelectDistinctNode
 from tecton_core.query.nodes import UserSpecifiedDataNode
 from tecton_core.query.rewrite import Rewrite
-
-
-def tree_contains(tree: NodeRef, node_type: Type[QueryNode]) -> bool:
-    """Returns True if the tree contains a NodeRef of the given type, False otherwise."""
-    if isinstance(tree.node, node_type):
-        return True
-
-    return any(tree_contains(subtree, node_type) for subtree in tree.inputs)
-
-
-def find_node_of_type(tree: NodeRef, node_type: Type[QueryNode]) -> Optional[NodeRef]:
-    """Returns the NodeRef (i.e. a subtree) of the given type, or None if no such NodeRef exists.
-
-    Raises an AssertionError if there are multiple NodeRefs of the given type.
-    """
-    nodes = _find_all_nodes_of_type(tree, node_type)
-    assert len(nodes) < 2, f"Expected at most one node of type {node_type}, found {len(nodes)}"
-    return nodes[0] if len(nodes) == 1 else None
+from tecton_core.query.rewrite import tree_contains
 
 
 def _find_all_nodes_of_type(tree: NodeRef, node_type: Type[QueryNode]) -> List[NodeRef]:
     """Returns a list of all NodeRefs of the given type."""
     nodes = []
 
     if isinstance(tree.node, node_type):
@@ -86,22 +68,20 @@
             for i in tree.inputs:
                 self.rewrite(i)
 
 
 class SpineEntityPushdown(Rewrite):
     """Filters the original feature data with respect to the entities contained in a spine.
 
-    Requires a Spark session since the spine must be evaluated.
-
     This should be applied to AsofJoinNodes and AsofJoinFullAggNodes since both of have sorts (as part of windows), for
     which we would like to minimize memory usage.
     """
 
-    def __init__(self, spark: SparkSession):
-        self.spark = spark
+    def __init__(self):
+        pass
 
     def rewrite(self, tree: NodeRef):
         if isinstance(tree.node, AsofJoinNode):
             self.rewrite_asof(tree)
         elif isinstance(tree.node, AsofJoinFullAggNode):
             self.rewrite_asof_full_agg(tree)
         else:
@@ -137,22 +117,25 @@
         if isinstance(node, can_be_pushed_down):
             self.pushdown_entities(node.input_node, spine, join_cols)
         else:
             entities_node = SelectDistinctNode(spine, join_cols).as_ref()
             tree.node = EntityFilterNode(node.as_ref(), entities_node, join_cols)
 
 
-# TODO: genericize this so it can be applied to non-spark. Right now we depend on directly being able to read dataframe from spark to get time limits.
 class SpineTimePushdown(Rewrite):
     """
-    We actually evaluate the spine to get the time ranges for this rewrite, so we need a spark session to do so.
+    Adds time filter ranges based on the time limits of the spine.
+
+    For example, if the spine in a fv.ghf() has time range [start, end], then the rewrite might apply that time range
+    as a filter to any FeatureViewPipelineNodes, OfflineStoreScanNodes, or DataSourceScanNodes that are part of the
+    feature data (with the appropriate modifications to the time range to account for factors such as batch schedule,
+    data delay, etc.).
     """
 
-    def __init__(self, spark: Optional[SparkSession]):
-        self.spark = spark
+    def __init__(self):
         # The spine time limits are the same for all spines used throughout the query, so we only calculate once.
         self.spine_time_limits: Optional[pendulum.Period] = None
 
     def rewrite(self, tree: NodeRef):
         if isinstance(tree.node, AsofJoinNode):
             self.rewrite_asof(tree)
         elif isinstance(tree.node, AsofJoinFullAggNode):
@@ -168,22 +151,28 @@
         # to make the naming match up with the aggregate case (in which the spine is not on the left).
         if self.spine_time_limits is None:
             cur_node = node.left_container.node.node
             self.spine_time_limits = _get_spine_time_limits(cur_node)
 
         self.pushdown_time_range(node.right_container.node, self.spine_time_limits)
 
-    # Compute the time limits from the node, and push down the time limits to its input
     def rewrite_asof_full_agg(self, tree: NodeRef):
+        """Computes the spine time limits and pushes them down to all relevant nodes in the partial aggregates."""
         node = tree.node
 
-        # We only want to rewrite for a user specified spine.
-        spine_node = find_node_of_type(node.spine, UserSpecifiedDataNode)
-        if spine_node is None:
+        # For an AsofFullAggJoinNode, the spine is the left side of the join and the partial aggregates are the right.
+        # The rewrite should only be applied when (a) the correct time range can be determined from the spine and (b)
+        # specific time filters have not yet been applied to the partial aggregates.
+        # Condition (a) means that the spine must contain a UserSpecifiedDataNode.
+        # Condition (b) means that the fv.run and fv.ghf(time range) cases should be avoided, since the time ranges will
+        # already have been applied to the partial aggregates. Both of these cases use a PartialAggNode as a fake spine,
+        # so they can be avoided by skipping the rewrite if the spine contains a PartialAggNode.
+        if tree_contains(node.spine, PartialAggNode) or not tree_contains(node.spine, UserSpecifiedDataNode):
             return
+
         if self.spine_time_limits is None:
             self.spine_time_limits = _get_spine_time_limits(node)
         self.pushdown_time_range(node.partial_agg_node, self.spine_time_limits)
 
     # Push down and convert spine time filter to either raw data or feature time filter at the DataSourceScanNode or OfflineStoreScanNode.
     # Nodes that do not affect the correlation with the spine time range are enumerated in the can_be_pushed_down list.
     def pushdown_time_range(self, tree: NodeRef, spine_time_limits: pendulum.Period):
@@ -217,22 +206,22 @@
                             n.node.ds_node,
                         )
                         if data_time_filter is not None:
                             n.node = attrs.evolve(
                                 n.node, start_time=data_time_filter.start, end_time=data_time_filter.end
                             )
             elif isinstance(node, OfflineStoreScanNode):
-                tree.node = attrs.evolve(node, time_filter=feature_time_limits)
+                tree.node = attrs.evolve(node, partition_time_filter=feature_time_limits)
 
 
 # Mutates the input
-def rewrite_tree_for_spine(tree: NodeRef, spark: Optional[SparkSession]):
+def rewrite_tree_for_spine(tree: NodeRef):
     if not conf.get_bool("QUERY_REWRITE_ENABLED"):
         return
-    rewrites = [SpineTimePushdown(spark), SpineEntityPushdown(spark)]
+    rewrites = [SpineTimePushdown(), SpineEntityPushdown()]
     for rewrite in rewrites:
         rewrite.rewrite(tree)
 
 
 def _get_spine_user_specified_data_node(cur_node: QueryNode) -> Optional[UserSpecifiedDataNode]:
     if isinstance(cur_node, UserSpecifiedDataNode):
         return cur_node
```

### Comparing `tecton-0.7.0b9/tecton/_internals/run_api.py` & `tecton-0.7.0rc0/tecton/_internals/run_api.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,7 +1,8 @@
+import logging
 import tempfile
 from datetime import datetime
 from datetime import timedelta
 from typing import Any
 from typing import Dict
 from typing import Optional
 from typing import Union
@@ -9,49 +10,48 @@
 import pandas
 import pendulum
 from pyspark.sql import DataFrame
 from pyspark.sql.streaming import StreamingQuery
 
 import tecton
 from tecton._internals import errors
+from tecton._internals import mock_source_utils
 from tecton._internals.rewrite import MockDataRewrite
-from tecton.framework import data_frame
 from tecton.run_api_consts import AGGREGATION_LEVEL_DISABLED
 from tecton.run_api_consts import AGGREGATION_LEVEL_FULL
 from tecton.run_api_consts import AGGREGATION_LEVEL_PARTIAL
 from tecton.run_api_consts import DEFAULT_AGGREGATION_TILES_WINDOW_END_COLUMN_NAME
 from tecton.run_api_consts import DEFAULT_AGGREGATION_TILES_WINDOW_START_COLUMN_NAME
 from tecton.run_api_consts import SUPPORTED_AGGREGATION_LEVEL_VALUES
 from tecton.tecton_context import TectonContext
-from tecton_core import logger as logger_lib
 from tecton_core import time_utils
+from tecton_core.errors import TectonValidationError
 from tecton_core.feature_definition_wrapper import FeatureDefinitionWrapper as FeatureDefinition
 from tecton_core.feature_definition_wrapper import FrameworkVersion
 from tecton_core.query.builder import ANCHOR_TIME
+from tecton_core.query.builder import WINDOW_END_COLUMN_NAME
 from tecton_core.query.builder import build_get_full_agg_features
 from tecton_core.query.builder import build_materialization_querytree
 from tecton_core.query.builder import build_pipeline_querytree
-from tecton_core.query.builder import WINDOW_END_COLUMN_NAME
 from tecton_core.query.node_interface import NodeRef
 from tecton_core.query.nodes import AddAnchorTimeNode
 from tecton_core.query.nodes import ConvertEpochToTimestampNode
 from tecton_core.query.nodes import RenameColsNode
-from tecton_core.query.nodes import UserSpecifiedDataNode
 from tecton_core.query.sql_compat import default_case
 from tecton_proto.args import pipeline_pb2
-from tecton_proto.args.feature_view_pb2 import BackfillConfigMode
 from tecton_proto.data import feature_view_pb2
 from tecton_spark import materialization_plan
 from tecton_spark.partial_aggregations import partial_aggregate_column_renames
-from tecton_spark.pipeline_helper import get_all_input_ds_id_map
 from tecton_spark.pipeline_helper import get_all_input_keys
+from tecton_spark.pipeline_helper import get_fco_ids_to_input_keys
 from tecton_spark.pipeline_helper import run_mock_odfv_pipeline
 from tecton_spark.spark_helper import check_spark_version
 
-logger = logger_lib.get_logger("RunApi")
+
+logger = logging.getLogger(__name__)
 
 
 def maybe_warn_incorrect_time_range_size(
     fd: FeatureDefinition, start_time: datetime, end_time: datetime, aggregation_level: Optional[str]
 ):
     time_range = end_time - start_time
     if fd.is_temporal_aggregate:
@@ -80,26 +80,26 @@
         )
 
 
 def run_batch(
     fd: FeatureDefinition,
     feature_start_time: datetime,
     feature_end_time: datetime,
-    mock_inputs: Dict[str, Union[pandas.DataFrame, DataFrame]],
+    mock_data_sources: Dict[str, NodeRef],
     framework_version: FrameworkVersion,
     aggregation_level: Optional[str],
 ) -> "tecton.framework.data_frame.TectonDataFrame":
     if not fd.is_on_demand:
         check_spark_version(fd.fv_spec.batch_cluster_config)
 
     return _querytree_run_batch(
         fd=fd,
         feature_start_time=feature_start_time,
         feature_end_time=feature_end_time,
-        mock_inputs=mock_inputs,
+        mock_data_sources=mock_data_sources,
         framework_version=framework_version,
         aggregation_level=aggregation_level,
     )
 
 
 def _build_run_batch_querytree(
     fd: FeatureDefinition,
@@ -158,45 +158,36 @@
             return build_pipeline_querytree(fd, for_stream=False, feature_data_time_limits=feature_time_limits_aligned)
         elif aggregation_level == AGGREGATION_LEVEL_FULL:
             qt = build_get_full_agg_features(
                 fd,
                 from_source=True,
                 feature_data_time_limits=feature_time_limits_aligned,
                 aggregation_anchor_time=feature_end_time,
+                respect_feature_start_time=False,
             )
             return qt
 
-    raise Exception("Unsupported batch query tree")
+    msg = "Unsupported batch query tree"
+    raise Exception(msg)
 
 
 def _querytree_run_batch(
     fd: FeatureDefinition,
     feature_start_time: datetime,
     feature_end_time: datetime,
-    mock_inputs: Dict[str, Union[pandas.DataFrame, DataFrame]],
+    mock_data_sources: Dict[str, NodeRef],
     framework_version: FrameworkVersion,
     aggregation_level: Optional[str],
 ) -> "tecton.framework.data_frame.DataFrame":
     aggregation_level = validate_and_get_aggregation_level(fd, aggregation_level)
 
-    # Validate that mock_inputs' keys.
-    validate_batch_mock_inputs_keys(mock_inputs, fd)
-
     feature_time_limits_aligned = pendulum.period(feature_start_time, feature_end_time)
 
     qt = _build_run_batch_querytree(fd, feature_end_time, feature_time_limits_aligned, aggregation_level)
 
-    input_ds_id_map = get_all_input_ds_id_map(fd.pipeline.root)
-    user_data_node_metadata = {"timestamp_key": fd.timestamp_key}
-    mock_data_sources = {
-        input_ds_id_map[key]: NodeRef(
-            UserSpecifiedDataNode(data_frame.TectonDataFrame._create(mock_inputs[key]), user_data_node_metadata)
-        )
-        for key in mock_inputs.keys()
-    }
     MockDataRewrite(mock_data_sources).rewrite(qt)
 
     return tecton.framework.data_frame.TectonDataFrame._create(qt)
 
 
 def run_stream(fd: FeatureDefinition, output_temp_table: str) -> StreamingQuery:
     check_spark_version(fd.fv_spec.stream_cluster_config)
@@ -219,15 +210,15 @@
     fd: FeatureDefinition, fv_name: str, mock_inputs: Dict[str, Union[Dict[str, Any], pandas.DataFrame, DataFrame]]
 ) -> Union[Dict[str, Any], "tecton.framework.data_frame.TectonDataFrame"]:
     for key in mock_inputs:
         if isinstance(mock_inputs[key], DataFrame):
             mock_inputs[key] = mock_inputs[key].toPandas()
 
     # Validate that all the mock_inputs matchs with FV inputs, and that num rows match across all mock_inputs.
-    validate_ondemand_mock_inputs_keys(mock_inputs, fd.pipeline)
+    validate_ondemand_mock_inputs(mock_inputs, fd.pipeline, fd)
 
     # Execute ODFV pipeline to get output DataFrame.
     output = run_mock_odfv_pipeline(
         pipeline=fd.pipeline,
         transformations=fd.transformations,
         name=fv_name,
         mock_inputs=mock_inputs,
@@ -243,87 +234,86 @@
     if aggregation_level is None:
         if fd.is_temporal_aggregate:
             aggregation_level = AGGREGATION_LEVEL_FULL
         else:
             aggregation_level = AGGREGATION_LEVEL_DISABLED
 
     if aggregation_level not in SUPPORTED_AGGREGATION_LEVEL_VALUES:
-        raise errors.FV_INVALID_ARG_VALUE(
-            "aggregation_level", str(aggregation_level), str(SUPPORTED_AGGREGATION_LEVEL_VALUES)
-        )
+        msg = "aggregation_level"
+        raise errors.FV_INVALID_ARG_VALUE(msg, str(aggregation_level), str(SUPPORTED_AGGREGATION_LEVEL_VALUES))
 
     return aggregation_level
 
 
-# For single-batch-schedule-interval-per-job backfill, validate the followings.
-# - Only support single-tile run.
-# - Don't allow passing `feature_start_time` without feature_end_time since it may be confusing that the tile time
-#   range goes into the future.
-def _validate_feature_time_for_backfill_config(
-    fv_proto: feature_view_pb2.FeatureView,
-    feature_start_time: Optional[Union[pendulum.DateTime, datetime]],
-    feature_end_time: Optional[Union[pendulum.DateTime, datetime]],
-    feature_time_limits_aligned: pendulum.Period,
-):
-    # TODO(raviphol): Use is_incremental_backfill once D9614 is landed.
-    if not fv_proto.HasField("temporal"):
-        return
-    if not fv_proto.temporal.HasField("backfill_config"):
-        return
-    backfill_config_mode = fv_proto.temporal.backfill_config.mode
-    if backfill_config_mode is not BackfillConfigMode.BACKFILL_CONFIG_MODE_SINGLE_BATCH_SCHEDULE_INTERVAL_PER_JOB:
-        return
-
-    if feature_start_time and not feature_end_time:
-        raise errors.BFC_MODE_SINGLE_REQUIRED_FEATURE_END_TIME_WHEN_START_TIME_SET
-
-    schedule_interval_seconds = fv_proto.materialization_params.schedule_interval.ToSeconds()
-    if schedule_interval_seconds == 0:
-        raise errors.INTERNAL_ERROR("Materialization schedule interval not found.")
-
-    num_tile = feature_time_limits_aligned.in_seconds() // schedule_interval_seconds
-    if num_tile > 1:
-        raise errors.BFC_MODE_SINGLE_INVALID_FEATURE_TIME_RANGE
-
-
-# Validate that mock_inputs keys are a subset of data sources.
-def validate_batch_mock_inputs_keys(mock_inputs, fd: FeatureDefinition):
-    expected_input_names = get_all_input_keys(fd.pipeline.root)
-    mock_inputs_keys = set(mock_inputs.keys())
-    if not mock_inputs_keys.issubset(expected_input_names):
-        raise errors.FV_INVALID_MOCK_INPUTS(mock_inputs_keys, expected_input_names)
-
-
 # Validate that mock_inputs keys are exact match with expected inputs.
-def validate_ondemand_mock_inputs_keys(
+def validate_ondemand_mock_inputs(
     mock_inputs: Dict[str, Union[Dict[str, Any], pandas.DataFrame]],
     pipeline: pipeline_pb2.Pipeline,
+    odfv_fd: Optional[FeatureDefinition] = None,
 ):
+    """Validate the mock_inputs for `run` or `test_run` match expected inputs.
+
+    If a feature definition is passed, this will additionally check inputs against batch feature view schemas.
+    """
     expected_input_names = get_all_input_keys(pipeline.root)
     mock_inputs_keys = set(mock_inputs.keys())
     if mock_inputs_keys != expected_input_names:
-        raise errors.FV_INVALID_MOCK_INPUTS(mock_inputs_keys, expected_input_names)
+        raise errors.FV_INVALID_MOCK_INPUTS(list(mock_inputs_keys), list(expected_input_names))
 
     # Get num row for all FV mock_inputs with DF types, to validate that they match.
     input_df_row_counts = set()
     for input in mock_inputs.values():
         if isinstance(input, pandas.DataFrame):
             input_df_row_counts.add(len(input.index))
     if len(input_df_row_counts) > 1:
         raise errors.FV_INVALID_MOCK_INPUTS_NUM_ROWS(input_df_row_counts)
 
+    # Can only validate batch feature view inputs if the schema is known
+    if odfv_fd is not None:
+        validate_mock_bfv_inputs_to_odfv(mock_inputs, odfv_fd)
+
+
+def validate_mock_bfv_inputs_to_odfv(
+    mock_inputs: Dict[str, Union[Dict[str, Any], pandas.DataFrame]], odfv_fd: FeatureDefinition
+):
+    """Validate the mock data used to represent batch feature view materialized data for `run()` and `test_run()`.
+
+    Unlike other mock data checks, because there's no way for us to easily identify what features are needed in the
+    ODFV, we expect all features in the upstream BFV to be present.
+    """
+    from tecton_core import feature_set_config
 
-# Check that schema of each mock inputs matches with data sources.
-def _validate_input_dataframe_schema(input_name, dataframe: DataFrame, spark_schema):
-    columns = sorted(dataframe.columns)
-    expected_column_names = sorted([field.name for field in spark_schema.fields])
-
-    # Validate mock input's schema against expected schema.
-    if not expected_column_names == columns:
-        raise errors.FV_INVALID_MOCK_INPUT_SCHEMA(input_name, set(columns), set(expected_column_names))
+    # Extract input names (from pipeline) and map to feature view schemas (from FDW)
+    dependent_fvs = feature_set_config.find_dependent_feature_set_items(
+        odfv_fd.fco_container, odfv_fd.pipeline.root, visited_inputs={}, fv_id=odfv_fd.id
+    )
+    fv_ids_to_fvs = {fv.feature_definition.id: fv for fv in dependent_fvs}
+    fco_ids_to_input_keys = get_fco_ids_to_input_keys(odfv_fd.pipeline.root)
+    input_name_to_features = {}
+    for fv_id, fv in fv_ids_to_fvs.items():
+        input_name = fco_ids_to_input_keys[fv_id]
+        input_name_to_features[input_name] = fv.features
+
+    # Validate some required fields in the mock data schemas.
+    for key, mock_df in mock_inputs.items():
+        if key not in input_name_to_features:
+            continue
+        input_columns = mock_source_utils.get_pandas_or_spark_df_or_dict_columns(mock_df)
+        # Check columns match BFV
+        dependent_columns = input_name_to_features[key]
+        unexpected_columns = [col for col in input_columns if col not in dependent_columns]
+        if len(unexpected_columns) > 0:
+            msg = f"Unexpected columns: {unexpected_columns} found in mock inputs. Expected columns from the Feature View {key}, such as: '{dependent_columns}'"
+            raise TectonValidationError(msg)
+        missing_columns = [col for col in dependent_columns if col not in input_columns]
+        if len(missing_columns) > 0:
+            logger.warning(
+                f"ODFV {odfv_fd.name} has a dependency on the Feature View {key}. Features '{missing_columns}' of this "
+                f"Feature View are not found. Available columns: '{input_columns}'"
+            )
 
 
 def _get_ds_by_id(data_sources, id: str):
     for ds in data_sources:
         if ds.id == id:
             return ds
     return None
```

### Comparing `tecton-0.7.0b9/tecton/_internals/snowflake_api.py` & `tecton-0.7.0rc0/tecton/_internals/snowflake_api.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,17 +1,18 @@
+import typing
 from datetime import datetime
 from typing import Dict
 from typing import List
 from typing import Optional
 from typing import Union
 
 import pandas
 import pendulum
 
-from tecton._internals.run_api import validate_batch_mock_inputs_keys
+from tecton._internals.mock_source_utils import validate_batch_mock_inputs
 from tecton.framework.data_frame import TectonDataFrame
 from tecton.snowflake_context import SnowflakeContext
 from tecton_core import conf
 from tecton_core import schema_derivation_utils as core_schema_derivation_utils
 from tecton_core import specs
 from tecton_core.errors import TectonSnowflakeNotImplementedError
 from tecton_core.feature_definition_wrapper import FeatureDefinitionWrapper as FeatureDefinition
@@ -22,14 +23,18 @@
 from tecton_proto.args import virtual_data_source_pb2 as virtual_data_source__args_pb2
 from tecton_proto.common import schema_pb2
 from tecton_proto.common import spark_schema_pb2
 from tecton_snowflake import schema_derivation_utils
 from tecton_snowflake import sql_helper
 
 
+if typing.TYPE_CHECKING:
+    import snowflake.snowpark
+
+
 def get_historical_features(
     feature_set_config: FeatureSetConfig,
     spine: Optional[Union["snowflake.snowpark.DataFrame", pandas.DataFrame, TectonDataFrame, str]] = None,
     timestamp_key: Optional[str] = None,
     include_feature_view_timestamp_columns: bool = False,
     from_source: Optional[bool] = None,
     save: bool = False,
@@ -38,15 +43,16 @@
     end_time: Optional[datetime] = None,
     entities: Optional[Union["snowflake.snowpark.DataFrame", pandas.DataFrame, TectonDataFrame]] = None,
     append_prefix: bool = True,  # Whether to append the prefix to the feature column name
 ) -> TectonDataFrame:
     # TODO(TEC-6991): Dataset doesn't really work with snowflake as it has spark dependency.
     # Need to rewrite it with snowflake context or remove this param for snowflake.
     if save or save_as is not None:
-        raise TectonSnowflakeNotImplementedError("save is not supported for Snowflake")
+        msg = "save is not supported for Snowflake"
+        raise TectonSnowflakeNotImplementedError(msg)
 
     start_time = get_timezone_aware_datetime(start_time)
     end_time = get_timezone_aware_datetime(end_time)
 
     if conf.get_bool("ALPHA_SNOWFLAKE_SNOWPARK_ENABLED"):
         if entities is not None:
             # Convert entities to a snowflake dataframe
@@ -67,17 +73,19 @@
                 entities=entities,
                 append_prefix=append_prefix,
                 from_source=from_source,
             )
         )
     else:
         if timestamp_key is None and spine is not None:
-            raise TectonSnowflakeNotImplementedError("timestamp_key must be specified using Snowflake without Snowpark")
+            msg = "timestamp_key must be specified using Snowflake without Snowpark"
+            raise TectonSnowflakeNotImplementedError(msg)
         if entities is not None:
-            raise TectonSnowflakeNotImplementedError("entities is only supported for Snowflake with Snowpark enabled")
+            msg = "entities is only supported for Snowflake with Snowpark enabled"
+            raise TectonSnowflakeNotImplementedError(msg)
         return TectonDataFrame._create(
             sql_helper.get_historical_features(
                 spine=spine,
                 connection=SnowflakeContext.get_instance().get_connection(),
                 timestamp_key=timestamp_key,
                 feature_set_config=feature_set_config,
                 include_feature_view_timestamp_columns=include_feature_view_timestamp_columns,
@@ -92,27 +100,26 @@
 def run_batch(
     fd: FeatureDefinition,
     mock_inputs: Dict[str, pandas.DataFrame],
     feature_start_time: datetime,
     feature_end_time: datetime,
     aggregation_level: Optional[str],
 ) -> TectonDataFrame:
-    validate_batch_mock_inputs_keys(mock_inputs, fd)
+    validate_batch_mock_inputs(mock_inputs, fd)
     mock_sql_inputs = None
 
     feature_start_time = get_timezone_aware_datetime(feature_start_time)
     feature_end_time = get_timezone_aware_datetime(feature_end_time)
 
     if fd.is_temporal_aggregate:
         for feature in fd.fv_spec.aggregate_features:
             aggregate_function = sql_helper.AGGREGATION_PLANS[feature.function]
             if not aggregate_function:
-                raise TectonSnowflakeNotImplementedError(
-                    f"Unsupported aggregation function {feature.function} in snowflake pipeline"
-                )
+                msg = f"Unsupported aggregation function {feature.function} in snowflake pipeline"
+                raise TectonSnowflakeNotImplementedError(msg)
 
     session = None
     connection = None
     if conf.get_bool("ALPHA_SNOWFLAKE_SNOWPARK_ENABLED"):
         session = SnowflakeContext.get_instance().get_session()
     else:
         connection = SnowflakeContext.get_instance().get_connection()
@@ -150,15 +157,16 @@
 
 def get_dataframe_for_data_source(
     data_source: specs.DataSourceSpec,
     start_time: datetime = None,
     end_time: datetime = None,
 ) -> TectonDataFrame:
     if not conf.get_bool("ALPHA_SNOWFLAKE_SNOWPARK_ENABLED"):
-        raise TectonSnowflakeNotImplementedError("get_dataframe is only supported with Snowpark enabled")
+        msg = "get_dataframe is only supported with Snowpark enabled"
+        raise TectonSnowflakeNotImplementedError(msg)
 
     start_time = get_timezone_aware_datetime(start_time)
     end_time = get_timezone_aware_datetime(end_time)
 
     session = SnowflakeContext.get_instance().get_session()
     return TectonDataFrame._create_with_snowflake(
         sql_helper.get_dataframe_for_data_source(session, data_source.batch_source, start_time, end_time)
@@ -166,28 +174,33 @@
 
 
 # For notebook driven development
 def derive_batch_schema(
     ds_args: virtual_data_source__args_pb2.VirtualDataSourceArgs,
 ) -> spark_schema_pb2.SparkSchema:
     if not ds_args.HasField("snowflake_ds_config"):
-        raise ValueError(f"Invalid batch source args: {ds_args}")
+        msg = f"Invalid batch source args: {ds_args}"
+        raise ValueError(msg)
 
     connection = SnowflakeContext.get_instance().get_connection()
     return schema_derivation_utils.get_snowflake_schema(ds_args, connection)
 
 
 def derive_view_schema_for_feature_view(
     feature_view_args: feature_view__args_pb2.FeatureViewArgs,
     transformation_specs: List[specs.TransformationSpec],
     data_source_specs: List[specs.DataSourceSpec],
 ) -> schema_pb2.Schema:
     connection = SnowflakeContext.get_instance().get_connection()
+    session = None
+    if conf.get_bool("ALPHA_SNOWFLAKE_SNOWPARK_ENABLED"):
+        session = SnowflakeContext.get_instance().get_session()
+
     return schema_derivation_utils.get_feature_view_view_schema(
-        feature_view_args, transformation_specs, data_source_specs, connection
+        feature_view_args, transformation_specs, data_source_specs, connection, session
     )
 
 
 def derive_materialization_schema_for_feature_view(
     view_schema: schema_pb2.Schema,
     feature_view_args: feature_view__args_pb2.FeatureViewArgs,
 ) -> schema_pb2.Schema:
```

### Comparing `tecton-0.7.0b9/tecton/_internals/spark_api.py` & `tecton-0.7.0rc0/tecton/_internals/spark_api.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,7 +1,8 @@
+import logging
 import tempfile
 from datetime import datetime
 from typing import Callable
 from typing import Dict
 from typing import List
 from typing import Optional
 from typing import Sequence
@@ -11,83 +12,95 @@
 import pendulum
 from pyspark import sql as pyspark_sql
 from pyspark.sql import streaming as pyspark_streaming
 
 from tecton import tecton_context
 from tecton import types as sdk_types
 from tecton._internals import errors
+from tecton._internals import ingest_utils
+from tecton._internals import rewrite
 from tecton._internals import time_utils
 from tecton._internals import type_utils
 from tecton._internals import utils
 from tecton.framework.data_frame import TectonDataFrame
 from tecton.framework.dataset import Dataset
 from tecton.tecton_context import TectonContext
 from tecton_core import conf
+from tecton_core import data_types
 from tecton_core import errors as core_errors
 from tecton_core import feature_set_config
 from tecton_core import materialization_context
 from tecton_core import query_consts
 from tecton_core import schema
 from tecton_core import schema_derivation_utils as core_schema_derivation_utils
 from tecton_core import specs
 from tecton_core.feature_definition_wrapper import FeatureDefinitionWrapper
 from tecton_core.feature_set_config import FeatureDefinitionAndJoinConfig
-from tecton_core.logger import get_logger
 from tecton_core.query import builder
+from tecton_core.query import node_interface
 from tecton_core.query import nodes
 from tecton_core.query import sql_compat
 from tecton_core.time_utils import align_time_downwards
 from tecton_proto.args import feature_view_pb2
 from tecton_proto.args import virtual_data_source_pb2 as virtual_data_source__args_pb2
 from tecton_proto.common import schema_pb2
 from tecton_proto.common import spark_schema_pb2
 from tecton_proto.data import feature_view_pb2 as feature_view__data_pb2
 from tecton_spark import data_source_helper
-from tecton_spark import ingest_utils
 from tecton_spark import schema_derivation_utils
 from tecton_spark import schema_spark_utils
 from tecton_spark import spark_schema_wrapper
 from tecton_spark.spark_helper import check_spark_version
 from tecton_spark.time_utils import convert_epoch_to_datetime
 from tecton_spark.time_utils import convert_timestamp_to_epoch
 
 
-logger = get_logger("spark_api")
+logger = logging.getLogger(__name__)
+
+_CHECKPOINT_DIRECTORIES: List[tempfile.TemporaryDirectory] = []
 
 
 def get_historical_features_for_feature_service(
     feature_service_spec: specs.FeatureServiceSpec,
     feature_set_config: feature_set_config.FeatureSetConfig,
-    spine: Optional[Union[pyspark_sql.DataFrame, pd.DataFrame, TectonDataFrame]],
+    spine: Union[pyspark_sql.DataFrame, pd.DataFrame, TectonDataFrame],
     timestamp_key: Optional[str],
     from_source: bool,
     save: bool,
     save_as: Optional[str],
 ) -> TectonDataFrame:
-    if not isinstance(spine, TectonDataFrame):
-        spine = TectonDataFrame._create(spine)
     timestamp_required = spine is not None and any(
         [_should_infer_timestamp_of_spine(fd, timestamp_key) for fd in feature_set_config.feature_definitions]
     )
 
     if timestamp_required:
         timestamp_key = timestamp_key or utils.infer_timestamp(spine)
+
+    if isinstance(spine, pd.DataFrame):
+        spine_schema = feature_set_config.spine_schema
+        if timestamp_key is not None:
+            spine_schema += schema.Schema.from_dict({timestamp_key: data_types.TimestampType()})
+        spine = TectonDataFrame._create_from_pandas_with_schema(spine, spine_schema)
+    elif not isinstance(spine, TectonDataFrame):
+        spine = TectonDataFrame._create(spine)
+
     if spine:
         utils.validate_spine_dataframe(spine, timestamp_key, feature_set_config.request_context_keys)
 
     user_data_node_metadata = {}
     # TODO: Create a SpineNode with a param of timestamp_key instead of using UserSpecifiedNode.
     if timestamp_key:
         user_data_node_metadata["timestamp_key"] = timestamp_key
     tree = builder.build_feature_set_config_querytree(
         feature_set_config,
         nodes.UserSpecifiedDataNode(spine, user_data_node_metadata).as_ref(),
         timestamp_key,
         from_source,
     )
+
     df = TectonDataFrame._create(tree)
     if save or save_as is not None:
         return Dataset._create(
             df=df,
             save_as=save_as,
             workspace=feature_service_spec.workspace,
             feature_service_id=feature_service_spec.id,
@@ -104,43 +117,52 @@
     timestamp_key: Optional[str],
     start_time: Optional[Union[pendulum.DateTime, datetime]],
     end_time: Optional[Union[pendulum.DateTime, datetime]],
     entities: Optional[Union[pyspark_sql.DataFrame, pd.DataFrame, TectonDataFrame]],
     from_source: bool,
     save: bool,
     save_as: Optional[str],
+    mock_data_sources: Dict[str, pyspark_sql.DataFrame],
 ) -> TectonDataFrame:
     if not feature_definition.is_on_demand:
         check_spark_version(feature_definition.fv_spec.batch_cluster_config)
 
     if spine is not None:
-        if not isinstance(spine, TectonDataFrame):
-            spine = TectonDataFrame._create(spine)
         if _should_infer_timestamp_of_spine(feature_definition, timestamp_key):
             timestamp_key = utils.infer_timestamp(spine)
-
-        df = _point_in_time_get_historical_features_for_feature_definition(
+        if isinstance(spine, pd.DataFrame):
+            fd_schema = feature_definition.spine_schema
+            if timestamp_key is not None:
+                fd_schema += schema.Schema.from_dict({timestamp_key: data_types.TimestampType()})
+            spine = TectonDataFrame._create_from_pandas_with_schema(spine, schema=fd_schema)
+        elif not isinstance(spine, TectonDataFrame):
+            spine = TectonDataFrame._create(spine)
+        qt = _point_in_time_get_historical_features_for_feature_definition(
             feature_definition, spine, timestamp_key, from_source
         )
     else:
         if entities is not None:
             if not isinstance(entities, TectonDataFrame):
                 entities = TectonDataFrame._create(entities)
             assert set(entities._dataframe.columns).issubset(
                 set(feature_definition.join_keys)
             ), f"Entities should only contain columns that can be used as Join Keys: {feature_definition.join_keys}"
 
-        df = _time_range_get_historical_features_for_feature_definition(
+        qt = _time_range_get_historical_features_for_feature_definition(
             feature_definition,
             start_time=start_time,
             end_time=end_time,
             entities=entities,
             from_source=from_source,
         )
 
+    rewrite.MockDataRewrite(mock_data_sources).rewrite(qt)
+
+    df = TectonDataFrame._create(qt)
+
     if save or save_as is not None:
         return Dataset._create(
             df=df,
             save_as=save_as,
             workspace=feature_definition.workspace,
             feature_definition_id=feature_definition.id,
             spine=spine,
@@ -165,33 +187,34 @@
 
 
 def _point_in_time_get_historical_features_for_feature_definition(
     feature_definition: FeatureDefinitionWrapper,
     spine: TectonDataFrame,
     timestamp_key: Optional[str],
     from_source: bool,
-) -> TectonDataFrame:
+) -> node_interface.NodeRef:
     if feature_definition.is_on_demand:
         utils.validate_spine_dataframe(spine, timestamp_key, feature_definition.request_context_keys)
     else:
         utils.validate_spine_dataframe(spine, timestamp_key)
 
     dac = FeatureDefinitionAndJoinConfig.from_feature_definition(feature_definition)
     user_data_node_metadata = {}
     if timestamp_key:
         user_data_node_metadata["timestamp_key"] = timestamp_key
-    return TectonDataFrame._create(
-        builder.build_spine_join_querytree(
-            dac,
-            nodes.UserSpecifiedDataNode(spine, user_data_node_metadata).as_ref(),
-            timestamp_key,
-            from_source,
-        )
+
+    qt = builder.build_spine_join_querytree(
+        dac,
+        nodes.UserSpecifiedDataNode(spine, user_data_node_metadata).as_ref(),
+        timestamp_key,
+        from_source,
     )
 
+    return qt
+
 
 def _most_recent_tile_end_time(fd: FeatureDefinitionWrapper, timestamp: datetime) -> int:
     """Computes the most recent tile end time which is ready to be computed.
 
     :param timestamp: The timestamp in python datetime format
     :return: The timestamp in seconds of the greatest ready tile end time <= timestamp.
     """
@@ -204,15 +227,15 @@
 
 def _time_range_get_historical_features_for_feature_definition(
     fd: FeatureDefinitionWrapper,
     entities: TectonDataFrame,
     start_time: Optional[Union[pendulum.DateTime, datetime]],
     end_time: Optional[Union[pendulum.DateTime, datetime]],
     from_source: bool,
-) -> TectonDataFrame:
+) -> node_interface.NodeRef:
     if start_time is not None and isinstance(start_time, datetime):
         start_time = pendulum.instance(start_time)
     if end_time is not None and isinstance(end_time, datetime):
         end_time = pendulum.instance(end_time)
 
     if start_time is not None and fd.feature_start_timestamp is not None and start_time < fd.feature_start_timestamp:
         logger.warning(
@@ -280,15 +303,16 @@
 
     qt = nodes.FeatureTimeFilterNode(
         qt,
         feature_data_time_limits=time_range,
         policy=feature_view__data_pb2.MaterializationTimeRangePolicy.MATERIALIZATION_TIME_RANGE_POLICY_FILTER_TO_RANGE,
         timestamp_field=fd.timestamp_key,
     ).as_ref()
-    return TectonDataFrame._create(qt)
+
+    return qt
 
 
 def get_dataframe_for_data_source(
     data_source: specs.DataSourceSpec,
     start_time: datetime,
     end_time: datetime,
     apply_translator: bool,
@@ -324,22 +348,28 @@
     data_source: specs.DataSourceSpec,
     table_name: str,
     apply_translator: bool,
     option_overrides: Optional[Dict[str, str]],
 ) -> pyspark_streaming.StreamingQuery:
     df = get_stream_preview_dataframe(data_source, apply_translator, option_overrides)
 
-    with tempfile.TemporaryDirectory() as d:
-        return (
-            df.writeStream.format("memory")
-            .queryName(table_name)
-            .option("checkpointLocation", d)
-            .outputMode("append")
-            .start()
-        )
+    # Set a tempdir checkpointLocation. This is needed for the stream preview to work in EMR notebooks. The
+    # TemporaryDirectory object handles cleaning up the temporary directory when it is destroyed, so add the object to
+    # a global list that will be cleaned up with the program exits. (This isn't guaranteed - but it's not the end of
+    # the world if we leak some temporary directories.)
+    d = tempfile.TemporaryDirectory()
+    _CHECKPOINT_DIRECTORIES.append(d)
+
+    return (
+        df.writeStream.format("memory")
+        .queryName(table_name)
+        .option("checkpointLocation", d)
+        .outputMode("append")
+        .start()
+    )
 
 
 def get_stream_preview_dataframe(
     data_source: specs.DataSourceSpec, apply_translator: bool, option_overrides: Optional[Dict[str, str]]
 ) -> pyspark_sql.DataFrame:
     """
     Helper function that allows start_stream_preview() to be unit tested, since we can't easily unit test writing
@@ -400,23 +430,34 @@
             spark=spark,
             table=ds_args.hive_ds_config.table,
             database=ds_args.hive_ds_config.database,
             post_processor=batch_post_processor,
             timestamp_field=ds_args.hive_ds_config.common_args.timestamp_field,
             timestamp_format=ds_args.hive_ds_config.timestamp_format,
         )
+    elif ds_args.HasField("unity_ds_config"):
+        return schema_derivation_utils.get_unity_table_schema(
+            spark=spark,
+            catalog=ds_args.unity_ds_config.catalog,
+            schema=ds_args.unity_ds_config.schema,
+            table=ds_args.unity_ds_config.table,
+            post_processor=batch_post_processor,
+            timestamp_field=ds_args.unity_ds_config.common_args.timestamp_field,
+            timestamp_format=ds_args.unity_ds_config.timestamp_format,
+        )
     elif ds_args.HasField("spark_batch_config"):
         return schema_derivation_utils.get_batch_data_source_function_schema(
             spark=spark,
             data_source_function=batch_data_source_function,
             supports_time_filtering=ds_args.spark_batch_config.supports_time_filtering,
         )
     elif ds_args.HasField("redshift_ds_config"):
         if not ds_args.redshift_ds_config.HasField("endpoint"):
-            raise core_errors.DS_ARGS_MISSING_FIELD("redshift", "endpoint")
+            msg = "redshift"
+            raise core_errors.DS_ARGS_MISSING_FIELD(msg, "endpoint")
 
         has_table = ds_args.redshift_ds_config.HasField("table") and ds_args.redshift_ds_config.table
         has_query = ds_args.redshift_ds_config.HasField("query") and ds_args.redshift_ds_config.query
         if (has_table and has_query) or (not has_table and not has_query):
             raise core_errors.REDSHIFT_DS_EITHER_TABLE_OR_QUERY
         temp_s3 = conf.get_or_none("SPARK_REDSHIFT_TEMP_DIR")
         if temp_s3 is None:
@@ -428,21 +469,25 @@
             table=ds_args.redshift_ds_config.table,
             query=ds_args.redshift_ds_config.query,
             temp_s3=temp_s3,
             post_processor=batch_post_processor,
         )
     elif ds_args.HasField("snowflake_ds_config"):
         if not ds_args.snowflake_ds_config.HasField("url"):
-            raise core_errors.DS_ARGS_MISSING_FIELD("snowflake", "url")
+            msg = "snowflake"
+            raise core_errors.DS_ARGS_MISSING_FIELD(msg, "url")
         if not ds_args.snowflake_ds_config.HasField("database"):
-            raise core_errors.DS_ARGS_MISSING_FIELD("snowflake", "database")
+            msg = "snowflake"
+            raise core_errors.DS_ARGS_MISSING_FIELD(msg, "database")
         if not ds_args.snowflake_ds_config.HasField("schema"):
-            raise core_errors.DS_ARGS_MISSING_FIELD("snowflake", "schema")
+            msg = "snowflake"
+            raise core_errors.DS_ARGS_MISSING_FIELD(msg, "schema")
         if not ds_args.snowflake_ds_config.HasField("warehouse"):
-            raise core_errors.DS_ARGS_MISSING_FIELD("snowflake", "warehouse")
+            msg = "snowflake"
+            raise core_errors.DS_ARGS_MISSING_FIELD(msg, "warehouse")
 
         return schema_derivation_utils.get_snowflake_schema(
             spark=spark,
             url=ds_args.snowflake_ds_config.url,
             database=ds_args.snowflake_ds_config.database,
             schema=ds_args.snowflake_ds_config.schema,
             warehouse=ds_args.snowflake_ds_config.warehouse,
@@ -474,15 +519,16 @@
             schema_uri=schema_uri,
             schema_override=schema_override,
             post_processor=batch_post_processor,
             timestamp_col=timestamp_column,
             timestmap_format=timestamp_format,
         )
     else:
-        raise ValueError(f"Invalid batch source args: {ds_args}")
+        msg = f"Invalid batch source args: {ds_args}"
+        raise ValueError(msg)
 
 
 def derive_stream_schema(
     ds_args: virtual_data_source__args_pb2.VirtualDataSourceArgs,
     stream_post_processor: Optional[Callable],
     stream_data_source_function: Optional[Callable],
 ) -> spark_schema_pb2.SparkSchema:
@@ -492,15 +538,16 @@
             spark, ds_args.kinesis_ds_config.stream_name, stream_post_processor
         )
     elif ds_args.HasField("kafka_ds_config"):
         return schema_derivation_utils.get_kafka_schema(spark, stream_post_processor)
     elif ds_args.HasField("spark_stream_config"):
         return schema_derivation_utils.get_stream_data_source_function_schema(spark, stream_data_source_function)
     else:
-        raise ValueError(f"Invalid stream source args: {ds_args}")
+        msg = f"Invalid stream source args: {ds_args}"
+        raise ValueError(msg)
 
 
 _TRANSFORMATION_RUN_TEMP_VIEW_PREFIX = "_tecton_transformation_run_"
 CONST_TYPE = Union[str, int, float, bool]
 
 
 def run_transformation_mode_spark_sql(
@@ -548,17 +595,16 @@
     if upload_url:
         ingest_utils.upload_df_pandas(upload_url, df)
     elif df_path:
         spark_df = ingest_utils.convert_pandas_to_spark_df(df, view_schema)
         spark_df.write.parquet(df_path)
 
 
-def get_request_schema_from_spark_schema(spark_schema_proto: spark_schema_pb2.SparkSchema) -> List[sdk_types.Field]:
-    """Convert SparkSchema from pipeline proto into a list of Tecton Fields."""
-    wrapper = spark_schema_wrapper.SparkSchemaWrapper.from_proto(spark_schema_proto)
-    columns_and_types = schema_spark_utils.schema_from_spark(wrapper.unwrap()).column_name_and_data_types()
+def get_request_schema_from_tecton_schema(tecton_schema: schema_pb2.Schema) -> List[sdk_types.Field]:
+    """Convert TectonSchema into a list of Tecton Fields."""
+    columns_and_types = schema.Schema(tecton_schema).column_name_and_data_types()
     request_schema = []
     for c_and_t in columns_and_types:
         name = c_and_t[0]
         data_type = type_utils.sdk_type_from_tecton_type(c_and_t[1])
         request_schema.append(sdk_types.Field(name, data_type))
     return request_schema
```

### Comparing `tecton-0.7.0b9/tecton/_internals/spark_utils.py` & `tecton-0.7.0rc0/tecton/_internals/spark_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,14 +1,14 @@
+import logging
 import os
 import sys
 
 from pyspark.sql import SparkSession
 
-from tecton import conf
-from tecton_core.logger import get_logger
+from tecton_core import conf
 from tecton_spark.udf_jar import get_udf_jar_path
 
 
 # Environment variables to propagate to the executor
 EXECUTOR_ENV_VARIABLES = [
     "AWS_ACCESS_KEY_ID",
     "AWS_SECRET_ACCESS_KEY",
@@ -16,15 +16,15 @@
     "HIVE_METASTORE_PORT",
     "HIVE_METASTORE_USERNAME",
     "HIVE_METASTORE_DATABASE",
     "HIVE_METASTORE_PASSWORD",
 ]
 
 
-logger = get_logger("SparkUtils")
+logger = logging.getLogger(__name__)
 
 
 def _get_basic_spark_session_builder():
     builder = SparkSession.builder
 
     if conf.get_or_none("AWS_ACCESS_KEY_ID") is not None:
         builder = builder.config("spark.hadoop.fs.s3a.access.key", conf.get_or_none("AWS_ACCESS_KEY_ID"))
```

### Comparing `tecton-0.7.0b9/tecton/_internals/time_utils.py` & `tecton-0.7.0rc0/tecton/_internals/time_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -27,15 +27,16 @@
         return _feature_table_get_feature_data_time_limits(fd, spine_time_limits)
     elif fd.is_temporal:
         return _temporal_fv_get_feature_data_time_limits(fd, spine_time_limits)
     elif fd.is_temporal_aggregate:
         return _temporal_agg_fv_get_feature_data_time_limits(fd, spine_time_limits)
 
     # Should never happen!
-    raise Exception("Feature definition must be a feature table, temporal FV, or temporal agg FV")
+    msg = "Feature definition must be a feature table, temporal FV, or temporal agg FV"
+    raise Exception(msg)
 
 
 def _feature_table_get_feature_data_time_limits(
     fd: FeatureDefinition,
     spine_time_limits: pendulum.Period,
 ) -> pendulum.Period:
     """Feature data time limits for a feature table.
@@ -45,14 +46,16 @@
 
     start_time = spine_time_limits.start
     end_time = spine_time_limits.end
 
     # Subtract by `serving_ttl` to accommodate for enough feature data for the feature expiration
     if fd.serving_ttl:
         start_time = start_time - fd.serving_ttl
+    else:
+        start_time = pendulum.from_timestamp(0)
 
     return end_time - start_time
 
 
 def _align_left(timestamp: pendulum.DateTime, min_scheduling_interval: pendulum.Duration) -> pendulum.DateTime:
     # min scheduling interval may be zero for continuous
     if min_scheduling_interval.total_seconds() > 0:
@@ -90,14 +93,16 @@
 
     start_time = spine_time_limits.start
     end_time = spine_time_limits.end
 
     # Subtract by `serving_ttl` to accommodate for enough feature data for the feature expiration
     if fd.serving_ttl:
         start_time = start_time - fd.serving_ttl
+    else:
+        start_time = pendulum.from_timestamp(0)
 
     # Respect feature_start_time if it's set.
     if fd.feature_start_timestamp:
         start_time = max(start_time, fd.feature_start_timestamp)
 
     start_time = _align_left(start_time, fd.min_scheduling_interval)
```

### Comparing `tecton-0.7.0b9/tecton/_internals/type_utils.py` & `tecton-0.7.0rc0/tecton/_internals/type_utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -21,15 +21,15 @@
 
 def to_spark_schema_wrapper(field_list: List[sdk_types.Field]) -> SparkSchemaWrapper:
     s = spark_types.StructType([field.spark_type() for field in field_list])
     return SparkSchemaWrapper(s)
 
 
 def sdk_type_from_tecton_type(
-    data_type: Union[tecton_types.DataType, tecton_types.StructType]
+    data_type: Union[tecton_types.DataType, tecton_types.StructType, tecton_types.ArrayType, tecton_types.MapType]
 ) -> Union[sdk_types.DataType, sdk_types.Array, sdk_types.Struct]:
     if isinstance(data_type, tecton_types.Int64Type):
         return sdk_types.Int64
     elif isinstance(data_type, tecton_types.Float32Type):
         return sdk_types.Float32
     elif isinstance(data_type, tecton_types.Float64Type):
         return sdk_types.Float64
@@ -40,9 +40,14 @@
     elif isinstance(data_type, tecton_types.TimestampType):
         return sdk_types.Timestamp
     elif isinstance(data_type, tecton_types.ArrayType):
         return sdk_types.Array(sdk_type_from_tecton_type(data_type.element_type))
     elif isinstance(data_type, tecton_types.StructType):
         fields = [sdk_types.Field(field.name, sdk_type_from_tecton_type(field.data_type)) for field in data_type.fields]
         return sdk_types.Struct(fields)
+    elif isinstance(data_type, tecton_types.MapType):
+        return sdk_types.Map(
+            sdk_type_from_tecton_type(data_type.key_type), sdk_type_from_tecton_type(data_type.value_type)
+        )
     else:
-        raise NotImplementedError(f"{data_type} is not a recognized data types.")
+        msg = f"{data_type} is not a recognized data types."
+        raise NotImplementedError(msg)
```

### Comparing `tecton-0.7.0b9/tecton/_internals/utils.py` & `tecton-0.7.0rc0/tecton/_internals/utils.py`

 * *Files 3% similar despite different names*

```diff
@@ -32,41 +32,46 @@
 from tecton_proto.data.materialization_status_pb2 import MaterializationAttemptStatus
 from tecton_proto.data.materialization_status_pb2 import MaterializationStatusState
 from tecton_proto.metadataservice.metadata_service_pb2 import GetFeatureFreshnessRequest
 from tecton_proto.metadataservice.metadata_service_pb2 import GetWorkspaceRequest
 from tecton_proto.metadataservice.metadata_service_pb2 import QueryFeatureViewsRequest
 from tecton_spark.schema_spark_utils import schema_from_spark
 
+
 _TIME_FORMAT = "%Y-%m-%d %H:%M:%S %Z"
 KEY_DELETION_MAX = 500000
 
 
 def validate_join_keys(join_keys: List[str]):
     """
     Validates that `join_keys` is not empty and has non-empty distinct values.
 
     :raises TectonValidationError: if `join_keys` is invalid.
     """
     if not join_keys:
-        raise errors.EMPTY_ARGUMENT("join_keys")
+        msg = "join_keys"
+        raise errors.EMPTY_ARGUMENT(msg)
 
     if "" in join_keys:
-        raise errors.EMPTY_ELEMENT_IN_ARGUMENT("join_keys")
+        msg = "join_keys"
+        raise errors.EMPTY_ELEMENT_IN_ARGUMENT(msg)
 
     if len(join_keys) > len(set(join_keys)):
-        raise errors.DUPLICATED_ELEMENTS_IN_ARGUMENT("join_keys")
+        msg = "join_keys"
+        raise errors.DUPLICATED_ELEMENTS_IN_ARGUMENT(msg)
 
 
 def validate_spine_dataframe(
     spine: DataframeWrapper, timestamp_key: Optional[str], request_context_keys: List[str] = None
 ):
     spine_df = spine._dataframe
     if timestamp_key:
         if timestamp_key not in spine_df.columns:
-            raise errors.MISSING_SPINE_COLUMN("timestamp_key", timestamp_key, spine_df.columns)
+            msg = "timestamp_key"
+            raise errors.MISSING_SPINE_COLUMN(msg, timestamp_key, spine_df.columns)
         if isinstance(spine_df, pysparkDF):
             data_type = spine_df.schema[timestamp_key].dataType
             if not isinstance(data_type, TimestampType):
                 raise errors.INVALID_SPINE_TIME_KEY_TYPE_SPARK(data_type)
         elif isinstance(spine_df, pd.DataFrame):
             dtypes = dict(spine_df.dtypes)
             data_type = dtypes[timestamp_key]
@@ -134,15 +139,16 @@
     if sort_columns:
         keys = [k.upper() for k in sort_columns.split(",")]
         indices = []
         for key in keys:
             try:
                 indices.append(column_names.index(key))
             except ValueError:
-                raise ValueError(f"Unknown sort key {key}, should be one of: {','.join(column_names)}")
+                msg = f"Unknown sort key {key}, should be one of: {','.join(column_names)}"
+                raise ValueError(msg)
         materialization_status_rows.sort(key=lambda r: [r[i] for i in indices])
 
     return column_names, materialization_status_rows
 
 
 def _materialization_status_state_name(state: MaterializationStatusState) -> str:
     state_name = MaterializationStatusState.Name(state)
@@ -224,27 +230,30 @@
         ret = 0
         for child in node.transformation_node.inputs:
             ret = ret + get_num_dependent_fv(child.node, visited_inputs)
         return ret
     return 0
 
 
-def infer_timestamp(spine: DataframeWrapper) -> Optional[str]:
-    spine_df = spine._dataframe
-    dtypes = dict(spine_df.dtypes)
+def infer_timestamp(spine: Union[pd.DataFrame, pysparkDF, DataframeWrapper]) -> Optional[str]:
+    if isinstance(spine, DataframeWrapper):
+        spine = spine._dataframe
+    dtypes = dict(spine.dtypes)
 
-    if isinstance(spine_df, pd.DataFrame):
+    if isinstance(spine, pd.DataFrame):
         timestamp_cols = [(k, v) for (k, v) in dtypes.items() if pd.api.types.is_datetime64_any_dtype(v)]
-    elif isinstance(spine_df, pysparkDF):
+    elif isinstance(spine, pysparkDF):
         timestamp_cols = [(k, v) for (k, v) in dtypes.items() if v == "timestamp"]
     else:
-        raise TectonValidationError(f"Unexpected data type for spine: {type(spine_df)}")
+        msg = f"Unexpected data type for spine: {type(spine)}"
+        raise TectonValidationError(msg)
 
     if len(timestamp_cols) > 1 or len(timestamp_cols) == 0:
-        raise TectonValidationError(f"Could not infer timestamp keys from {dtypes}; please specify explicitly")
+        msg = f"Could not infer timestamp keys from {dtypes}; please specify explicitly"
+        raise TectonValidationError(msg)
     return timestamp_cols[0][0]
 
 
 def can_be_stale(ff_proto: FreshnessStatus) -> bool:
     return (
         ff_proto.expected_freshness.seconds > 0 and ff_proto.freshness.seconds > 0 and ff_proto.materialization_enabled
     )
@@ -340,15 +349,16 @@
 def validate_entity_deletion_keys_dataframe(df: pysparkDF, join_keys: List[str], view_schema: Schema):
     if len(set(df.columns)) != len(df.columns):
         raise errors.DUPLICATED_COLS_IN_KEYS(", ".join(list(df.columns)))
     row_count = df.count()
     if row_count > KEY_DELETION_MAX:
         raise errors.TOO_MANY_KEYS
     if row_count == 0:
-        raise errors.EMPTY_ARGUMENT("join_keys")
+        msg = "join_keys"
+        raise errors.EMPTY_ARGUMENT(msg)
     if set(df.columns) != set(join_keys):
         raise errors.INCORRECT_KEYS(", ".join(list(df.columns)), ", ".join(join_keys))
     df_columns = schema_from_spark(df.schema).column_name_and_data_types()
     fv_columns = view_schema.column_name_and_data_types()
     for df_column in df_columns:
         if df_column not in fv_columns:
             raise INGEST_COLUMN_TYPE_MISMATCH(
```

### Comparing `tecton-0.7.0b9/tecton/cli/cli.py` & `tecton-0.7.0rc0/tecton/cli/cli.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 import glob
 import importlib
 import io
+import logging
 import os
 import platform
 import sys
 import tarfile
 from dataclasses import dataclass
 from pathlib import Path
 from types import ModuleType
@@ -12,58 +13,65 @@
 from typing import List
 from typing import Optional
 from typing import Tuple
 
 import click
 import pytest
 import requests
+from click.shell_completion import shell_complete
 from google.protobuf.empty_pb2 import Empty
 from yaspin.spinners import Spinners
 
 import tecton
-from .cli_utils import bold
-from .cli_utils import confirm_or_exit
-from .cli_utils import pprint_attr_obj
-from .cli_utils import pprint_dict
-from .cli_utils import print_version_msg
 from tecton import tecton_context
 from tecton._internals import metadata_service
 from tecton._internals import sdk_decorators
 from tecton._internals.analytics import StateUpdateResponse
 from tecton._internals.display import Displayable
-from tecton._internals.fco import Fco
 from tecton._internals.utils import format_freshness_table
 from tecton._internals.utils import format_materialization_attempts
 from tecton._internals.utils import get_all_freshness
 from tecton._internals.workspace_utils import PROD_WORKSPACE_NAME_CLIENT
 from tecton.cli import access_control
 from tecton.cli import api_key
 from tecton.cli import common
+from tecton.cli import environment
 from tecton.cli import printer
 from tecton.cli import service_account
+from tecton.cli import user
 from tecton.cli import workspace
-from tecton.cli.command import _cluster_url
+from tecton.cli import workspace_utils
 from tecton.cli.command import TectonCommand
 from tecton.cli.command import TectonGroup
+from tecton.cli.command import _cluster_url
 from tecton.cli.engine import dump_local_state
 from tecton.cli.engine import update_tecton_state
 from tecton.cli.error_utils import pretty_error
+from tecton.cli.workspace import WorkspaceType
 from tecton.cli.workspace import _switch_to_workspace
+from tecton.framework import base_tecton_object
 from tecton.identities import credentials
 from tecton.identities import okta
 from tecton_core import repo_file_handler
 from tecton_core.errors import TectonAPIInaccessibleError
 from tecton_core.errors import TectonValidationError
 from tecton_core.fco_container import FcoContainer
 from tecton_core.id_helper import IdHelper
 from tecton_proto.metadataservice.metadata_service_pb2 import GetFeatureViewRequest
 from tecton_proto.metadataservice.metadata_service_pb2 import GetMaterializationStatusRequest
 from tecton_proto.metadataservice.metadata_service_pb2 import GetRestoreInfoRequest
 from tecton_proto.metadataservice.metadata_service_pb2 import GetStateUpdateLogRequest
 
+from .cli_utils import bold
+from .cli_utils import confirm_or_exit
+from .cli_utils import pprint_attr_obj
+from .cli_utils import pprint_dict
+from .cli_utils import print_version_msg
+
+
 CONTEXT_SETTINGS = {
     "max_content_width": 160,
     "help_option_names": ["-h", "--help"],
 }
 
 _CLIENT_VERSION_INFO_RESPONSE_HEADER = "x-tecton-client-version-info"
 _CLIENT_VERSION_WARNING_RESPONSE_HEADER = "x-tecton-client-version-warning"
@@ -71,78 +79,170 @@
 
 @click.group(name="tecton", context_settings=CONTEXT_SETTINGS, cls=TectonGroup)
 @click.option("--verbose/--no-verbose", default=False, help="Be verbose")
 @click.option("--debug/--no-debug", default=False, help="Enable debug info.")
 @click.pass_context
 def cli(ctx, verbose, debug):
     "Tecton command-line tool."
-    from tecton_core.logger import set_logging_level
-    import logging
-
-    set_logging_level(logging.ERROR)
     sdk_decorators.disable_sdk_public_method_decorator()
 
+    logging_level = logging.DEBUG if debug else logging.WARNING
+    logging.basicConfig(
+        level=logging_level,
+        stream=sys.stderr,
+        format="%(levelname)s(%(name)s): %(message)s",
+    )
+
     # add cwd to path
     sys.path.append("")
 
 
 @cli.command(requires_auth=False)
 def version():
     """Print version."""
     tecton.version.summary()
 
 
+@cli.command(requires_auth=False)
+@click.option("--zsh", default=False, is_flag=True, help="Generate a zsh tab completion script.")
+@click.option("--bash", default=False, is_flag=True, help="Generate a bash tab completion script.")
+@click.option("--fish", default=False, is_flag=True, help="Generate a fish tab completion script.")
+def completion(zsh, bash, fish):
+    """Generates a shell script to set up tab completion for Tecton. Zsh, bash, and fish shells are supported.
+
+    See typical usage examples below:
+
+    zsh:
+
+        # Generate and save the Tecton auto-complete script.
+
+        tecton completion --zsh > ~/.tecton-complete.zsh
+
+        # Enable zsh auto-completion. (Not needed if you already have auto-complete enabled, e.g. are using oh-my-zsh.)
+
+        echo 'autoload -Uz compinit && compinit' >> ~/.zshrc
+
+        # Add sourcing the script into your .zshrc.
+
+        echo '. ~/.tecton-complete.zsh' >> ~/.zshrc
+
+    bash:
+
+        # Generate and save the Tecton auto-complete script.
+
+        tecton completion --bash > ~/.tecton-complete.bash
+
+        # Add sourcing the script into your .bashrc.
+
+        echo '. ~/.tecton-complete.bash' >> ~/.bashrc
+
+    fish:
+
+        # Generate and save the Tecton auto-complete script to your fish configs.
+
+        tecton completion --fish > ~/.config/fish/completions/tecton.fish
+    """
+    true_count = sum([zsh, bash, fish])
+    if true_count != 1:
+        msg = "Please set exactly one of --zsh, --bash, or --fish to generate a script for your shell environment."
+        raise SystemExit(msg)
+
+    if zsh:
+        instruction = "zsh_source"
+    elif bash:
+        instruction = "bash_source"
+    elif fish:
+        instruction = "fish_source"
+
+    status_code = shell_complete(
+        cli, ctx_args={}, prog_name="tecton", complete_var="_TECTON_COMPLETE", instruction=instruction
+    )
+    sys.exit(status_code)
+
+
 class EngineCommand(TectonCommand):
     def __init__(
         self,
         *args,
         apply: bool,
         upgrade_all: bool = False,
         destroy: bool = False,
         allows_suppress_recreates: bool = False,
         has_plan_id: bool = False,
         **kwargs,
     ):
         @click.pass_context
         def callback(
             ctx,
+            yes,
             safety_checks,
+            no_safety_checks,
             json_out,
             suppress_warnings,
             server_side_rendering,
+            workspace,  # Not used but it needs to be here to match params list.
             suppress_recreates=False,
             plan_id=None,
             skip_tests=None,
         ):
             args = EngineArgs(
                 skip_tests=skip_tests,
-                no_safety_checks=(not safety_checks),
                 json_out=json_out,
+                no_safety_checks=yes or no_safety_checks,
                 suppress_warnings=suppress_warnings,
                 server_side_rendering=server_side_rendering,
                 debug=common.get_debug(ctx),
             )
 
             assert not (plan_id and suppress_recreates), (
                 "The flag --suppress-recreates is only used when computing a new plan. If the plan passed "
                 "in using --plan-id was already computed using --suppress-recreates, that behavior persists "
                 "as part of the plan."
             )
 
+            if yes and safety_checks:
+                msg = "The flag --yes is an alias for --no-safety-checks, and so the flags --yes and --safety-checks cannot be used together"
+                raise TectonValidationError(msg)
+
+            if safety_checks or no_safety_checks:
+                print_version_msg(
+                    "The flags --safety-checks and --no-safety-checks are deprecated and will be removed "
+                    "in 0.8. Remove --safety-checks or use --yes instead of --no-safety-checks.",
+                    True,
+                )
+
             if plan_id:
                 args.plan_id = plan_id
             if suppress_recreates:
                 args.suppress_recreates = suppress_recreates
 
             return run_engine(args, apply=apply, upgrade_all=upgrade_all, destroy=destroy)
 
         params = [
             # TODO(Add help)
             click.Option(
-                ["--safety-checks/--no-safety-checks"], default=True, help="Disable interactive safety checks."
+                ["--yes", "-y"],
+                is_flag=True,
+                default=False,
+                help="Disable interactive safety checks.",
+            ),
+            # TODO(deprecate_after=0.8) --no-safety-checks will be replaced with --yes
+            # --safety-checks and --no-safety-checks is split up so we can know when a user has explicitly set these
+            # flags in order to issue a warning
+            click.Option(
+                ["--safety-checks"],
+                is_flag=True,
+                default=False,
+                help="Disable interactive safety checks.",
+            ),
+            click.Option(
+                ["--no-safety-checks"],
+                is_flag=True,
+                default=False,
+                help="Disable interactive safety checks.",
             ),
             click.Option(
                 ["--json-out"],
                 default="",
                 help="Output the tecton state update diff (as JSON) to the file path provided.",
             ),
             click.Option(
@@ -153,14 +253,20 @@
             ),
             click.Option(
                 ["--server-side-rendering/--no-server-side-rendering"],
                 default=True,
                 help="Render tecton plan output on the server side instead of client side.",
                 hidden=True,
             ),
+            click.Option(
+                ["--workspace"],
+                default=None,
+                type=WorkspaceType(),
+                help="Name of the target workspace that tecton state update request applies to.",
+            ),
         ]
         if not destroy:
             params.append(
                 click.Option(
                     ["--skip-tests/--no-skip-tests"],
                     default=False,
                     help="Disable running tests.",
@@ -182,15 +288,17 @@
 
         super().__init__(*args, callback=callback, params=params, uses_workspace=True, **kwargs)
 
 
 cli.add_command(api_key.api_key)
 cli.add_command(service_account.service_account)
 cli.add_command(access_control.access_control)
+cli.add_command(user.user)
 cli.add_command(workspace.workspace)
+cli.add_command(environment.environment)
 cli.add_command(
     EngineCommand(
         name="plan",
         apply=False,
         allows_suppress_recreates=True,
         help="Compare your local feature definitions with remote state and *show* the plan to bring them in sync.",
     )
@@ -310,17 +418,17 @@
         sys.exit(1)
     except Exception as e:
         before_error()
         pretty_error(Path(file_path), py_files, exception=e, repo_root=repo_root, error_message=e.args[0], debug=debug)
         sys.exit(1)
 
 
-def collect_top_level_objects(py_files: List[Path], repo_root: Path, debug: bool, pretty_errors: bool) -> List[Fco]:
-    from tecton._internals.fco import _ALL_FCOS
-
+def collect_top_level_objects(
+    py_files: List[Path], repo_root: Path, debug: bool, pretty_errors: bool
+) -> List[base_tecton_object.BaseTectonObject]:
     modules = [py_path_to_module(p, repo_root) for p in py_files]
 
     with printer.safe_yaspin(Spinners.earth, text="Importing feature repository modules") as sp:
         for file_path, module_path in zip(py_files, modules):
             sp.text = f"Processing feature repository module {module_path}"
 
             if pretty_errors:
@@ -337,18 +445,18 @@
 
         num_modules = len(modules)
         sp.text = (
             f"Imported {num_modules} Python {plural(num_modules, 'module', 'modules')} from the feature repository"
         )
         sp.ok(printer.safe_string("✅"))
 
-        return list(_ALL_FCOS.values())
+        return list(base_tecton_object._LOCAL_TECTON_OBJECTS)
 
 
-def prepare_args(debug: bool) -> Tuple[List[Fco], str, List[Path]]:
+def prepare_args(debug: bool) -> Tuple[List[base_tecton_object.BaseTectonObject], str, List[Path]]:
     repo_file_handler.ensure_prepare_repo()
     repo_files = repo_file_handler.repo_files()
     repo_root = repo_file_handler.repo_root()
 
     py_files = [p for p in repo_files if p.suffix == ".py"]
     os.chdir(repo_root)
 
@@ -477,23 +585,23 @@
     plan_id = None
     if hasattr(args, "plan_id"):
         plan_id = args.plan_id
     suppress_recreates = False
     if hasattr(args, "suppress_recreates") and args.suppress_recreates:
         suppress_recreates = True
 
-    if destroy:
-        top_level_objects: List[Fco] = []
+    if destroy or plan_id:
+        # There is no need to run tests when destroying a repo or when a plan_id is provided.
+        top_level_objects: List[base_tecton_object.BaseTectonObject] = []
         repo_root = None
         repo_files: List[Path] = []
     else:
         top_level_objects, repo_root, repo_files = prepare_args(args.debug)
-        # We only skip tests if the user passes a skip_tests flag, or if we have a plan_id,
-        # because in that case we’ve tested before generating the plan_id
-        if args.skip_tests == False and plan_id is None:
+
+        if args.skip_tests == False:
             run_tests(args.debug)
 
     # When using server-side plan rendering, use no colors on Windows
     # or if NO_COLOR is set
     no_color = platform.system() == "Windows" or _no_color_convention()
 
     return update_tecton_state(
@@ -592,17 +700,19 @@
         for f in repo_files:
             os.remove(f)
 
     # Extract the feature repo.
     with tarfile.open(fileobj=io.BytesIO(tar_response.content), mode="r|gz") as tar:
         for entry in tar:
             if os.path.isabs(entry.name) or ".." in entry.name:
-                raise ValueError("Illegal tar archive entry")
+                msg = "Illegal tar archive entry"
+                raise ValueError(msg)
             elif os.path.exists(root / Path(entry.name)):
-                raise ValueError(f"tecton restore would overwrite an unexpected file: {entry.name}")
+                msg = f"tecton restore would overwrite an unexpected file: {entry.name}"
+                raise ValueError(msg)
             tar.extract(entry, path=root)
     printer.safe_print("Success")
 
 
 @cli.command(uses_workspace=True)
 @click.option("--limit", default=10, type=int, help="Number of log entries to return.")
 def log(limit):
@@ -629,30 +739,32 @@
     help="Manually require user to open browser and paste login token. Needed when using the Tecton CLI in a headless environment.",
 )
 @click.option("--okta-session-token", default=None, hidden=True, required=False)
 def login(tecton_url: Optional[str], manual: bool, okta_session_token: Optional[str]):
     """Log in and authenticate Tecton CLI.
 
     The Tecton URL may be optionally passed on the command line as TECTON_URL, otherwise you will be prompted."""
-    from urllib.parse import urlparse, urljoin
-    from tecton import conf
+    from urllib.parse import urljoin
+    from urllib.parse import urlparse
+
+    from tecton_core import conf
 
     host = _cluster_url()
 
     if tecton_url is None:
         printer.safe_print("Enter configuration. Press enter to use current value")
         prompt = "Tecton Cluster URL [%s]: " % (host or "no current value. example: https://yourco.tecton.ai")
         new_host = input(prompt).strip()
         if new_host:
             host = new_host
     else:
         host = tecton_url
     try:
         urlparse(host)
-    except:
+    except Exception:
         printer.safe_print("Tecton Cluster URL must be a valid URL")
         sys.exit(1)
     # add this check for now since it can be hard to debug if you don't specify https and API_SERVICE fails
     if host is None or not (host.startswith("https://") or host.startswith("http://localhost:")):
         if host is not None and "//" not in host:
             host = f"https://{host}"
         else:
@@ -702,18 +814,26 @@
 
     conf.save_tecton_configs()
     conf.save_okta_tokens(access_token, access_token_expiration, refresh_token)
     printer.safe_print(f"✅ Updated configuration at {conf._LOCAL_TECTON_CONFIG_FILE}")
 
 
 @cli.command()
-def freshness():
+@click.option(
+    "--workspace",
+    default=None,
+    type=WorkspaceType(),
+    help="Name of the target workspace that tecton state update request applies to.",
+)
+def freshness(workspace):
     """Feature freshness for Feature Views in the current workspace."""
     # TODO: use GetAllFeatureFreshnessRequest once we implement Chronosphere based API.
-    freshness_statuses = get_all_freshness(tecton_context.get_current_workspace())
+    workspace_name = workspace if workspace else tecton_context.get_current_workspace()
+    workspace_utils.check_workspace_exists(workspace_name)
+    freshness_statuses = get_all_freshness(workspace_name)
     num_fvs = len(freshness_statuses)
     if num_fvs == 0:
         printer.safe_print("No Feature Views found in this workspace.")
         return
 
     printer.safe_print(format_freshness_table(freshness_statuses))
 
@@ -736,43 +856,53 @@
             "Description": profile.description,
             "Created by": profile.created_by,
         }
         pprint_dict(service_account, colwidth=19)
     else:
         pprint_dict({"Tecton Endpoint": _cluster_url()}, colwidth=16)
         printer.safe_print(
-            f"Tecton credentials are not configured or have expired. Run `tecton login` or set an "
-            f"API Key to authenticate"
+            "Tecton credentials are not configured or have expired. Run `tecton login` or set an "
+            "API Key to authenticate"
         )
 
 
 @cli.command()
 @click.pass_context
 @click.argument("feature_view_name")
 @click.option("--limit", default=100, type=int, help="Set the maximum limit of results.")
 @click.option("--errors-only/--no-errors-only", default=False, help="Only show errors.")
-def materialization_status(ctx, feature_view_name, limit, errors_only):
+@click.option(
+    "--workspace",
+    default=None,
+    type=WorkspaceType(),
+    help="Name of the target workspace that tecton state update request applies to.",
+)
+def materialization_status(ctx, feature_view_name, limit, errors_only, workspace):
     """Show materialization status information for a FeatureView with FEATURE_VIEW_NAME in the current workspace.
 
     Prepend the --verbose flag for more information."""
+
     # Fetch FeatureView
+    workspace_name = workspace if workspace else tecton_context.get_current_workspace()
+    workspace_utils.check_workspace_exists(workspace_name)
     fvRequest = GetFeatureViewRequest()
     fvRequest.version_specifier = feature_view_name
-    fvRequest.workspace = tecton_context.get_current_workspace()
+    fvRequest.workspace = workspace_name
     fvResponse = metadata_service.instance().GetFeatureView(fvRequest)
     fco_container = FcoContainer.from_proto(fvResponse.fco_container)
     fv_spec = fco_container.get_single_root()
     if fv_spec is None:
         printer.safe_print(f"Feature view '{feature_view_name}' not found.")
         sys.exit(1)
     fv_id = IdHelper.from_string(fv_spec.id)
 
     # Fetch Materialization Status
     statusRequest = GetMaterializationStatusRequest()
     statusRequest.feature_package_id.CopyFrom(fv_id)
+    statusRequest.workspace = workspace_name
     statusResponse = metadata_service.instance().GetMaterializationStatus(statusRequest)
 
     column_names, materialization_status_rows = format_materialization_attempts(
         statusResponse.materialization_status.materialization_attempts,
         verbose=common.get_verbose(ctx),
         limit=limit,
         errors_only=errors_only,
```

### Comparing `tecton-0.7.0b9/tecton/cli/cli_utils.py` & `tecton-0.7.0rc0/tecton/cli/cli_utils.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/cli/command.py` & `tecton-0.7.0rc0/tecton/cli/command.py`

 * *Files 5% similar despite different names*

```diff
@@ -9,21 +9,23 @@
 import pendulum
 
 from tecton import tecton_context
 from tecton._internals.analytics import AnalyticsLogger
 from tecton._internals.analytics import StateUpdateEventMetrics
 from tecton._internals.analytics import StateUpdateResponse
 from tecton.cli import printer
+from tecton.cli import workspace_utils
 from tecton_core import conf
 
+
 analytics = AnalyticsLogger()
 
 
 def _cluster_url() -> Optional[str]:
-    from tecton import conf
+    from tecton_core import conf
 
     api_service = conf.get_or_none("API_SERVICE")
     if api_service:
         # API_SERVICE URLs of the form <subdomain>.tecton.ai/api are expected so this check
         # ensures an internal DNS address isn't being used or an invalid path is specified.
         if api_service.endswith("/api") and "ingress" not in api_service:
             return api_service[: -len("/api")]
@@ -51,14 +53,22 @@
             cur = ctx
             # The top level context is `cli` which we don't want to include.
             while cur:
                 command_names.append(cur.command.name)
                 cur = cur.parent
             command_names.reverse()
 
+            # TODO(TEC-14547): Move `workspace` param usages to callback function.
+            is_create_workspace_command = ctx.parent.info_name == "workspace" and ctx.info_name == "create"
+            has_workspace_option = ctx.params.get("workspace", None) is not None
+            if has_workspace_option and not is_create_workspace_command:
+                workspace_name = ctx.params["workspace"]
+                workspace_utils.check_workspace_exists(workspace_name)
+                conf.set("TECTON_WORKSPACE", workspace_name)
+
             # Do not try logging events if cluster has never be configured or if user is trying to log in,
             # otherwise the CLI either won't be able to find the MDS or auth token might have expired
             if cluster_configured:
                 if uses_workspace:
                     printer.safe_print(f'Using workspace "{tecton_context.get_current_workspace()}" on cluster {host}')
                 start_time = pendulum.now("UTC")
                 state_update_event = None
@@ -130,18 +140,22 @@
     :arg hide_input Whether to hide user input in the prompt, as for passwords. Additionally, if a default is set,
          it will be obfuscated with `*` in the prompt.
     """
     if "prompt" not in kwargs:
         kwargs["prompt"] = True
     lower_key = key.lower().replace("_", "-")
     param_decls = param_decls or [f"--{lower_key}"]
-    default = lambda: conf.get_or_none(key)
+
     if hide_input:
         kwargs["type"] = HiddenValueType()
         default = HiddenValueType().convert(conf.get_or_none(key))
+    else:
+
+        def default():
+            return conf.get_or_none(key)
 
     def decorator(f):
         @click.option(*param_decls, **kwargs, hide_input=hide_input, default=default)
         @functools.wraps(f)
         def wrapper(*args, **kwargs):
             def unwrap(v):
                 return v.value if isinstance(v, HiddenValue) else v
```

### Comparing `tecton-0.7.0b9/tecton/cli/common.py` & `tecton-0.7.0rc0/tecton/cli/common.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,22 +1,23 @@
 import inspect
+import logging
 import re
 import site
 from pathlib import Path
 
-from tecton import conf
-from tecton_core import logger as logger_lib
+from tecton_core import conf
 from tecton_proto.args import repo_metadata_pb2
 from tecton_proto.common import id_pb2
 
+
 # Matches frame strings such as "<string>"
 SKIP_FRAME_REGEX = re.compile("\<.*\>")
 
 
-logger = logger_lib.get_logger(__name__)
+logger = logging.getLogger(__name__)
 
 
 def get_current_workspace():
     logger.warn(
         "`tecton.cli.common.get_current_workspace` is deprecated and will be removed in 0.7. Use `tecton.get_current_workspace()` instead."
     )
     return conf.get_or_none("TECTON_WORKSPACE")
@@ -29,21 +30,29 @@
     - This function assumed it is being called from the constructor of an FCO
     - inspect.stack() returns the call stack (starting with this function)
     - Walk up the stack frames until the first file within a tecton repo (a child of .tecton) is found
     - The first valid tecton repo file is considered the filename of the FCO.
     """
     from tecton_core.repo_file_handler import _maybe_get_repo_root
 
-    excluded_site_pkgs = site.getsitepackages() + [site.getusersitepackages()]
-
     source_info = repo_metadata_pb2.SourceInfo(fco_id=fco_id)
     repo_root = _maybe_get_repo_root()
     if not repo_root:
         return source_info
 
+    # 'getsitepackages' and 'getusersitepackages' are not avaiable in some python envs such as EMR notebook with
+    # Python 3.7.
+    if not (hasattr(site, "getsitepackages") and hasattr(site, "getusersitepackages")):
+        logger.warn(
+            "Python 'site' pakcage doesn't contain 'getsitepackages' or 'getusersitepackages' methods. SourceInfo is not going to be populated."
+        )
+        return source_info
+
+    excluded_site_pkgs = site.getsitepackages() + [site.getusersitepackages()]
+
     frames = inspect.stack()
     repo_root_path = Path(repo_root)
     for frame in frames:
         if SKIP_FRAME_REGEX.match(frame.frame.f_code.co_filename) is not None:
             continue
         frame_path = Path(frame.frame.f_code.co_filename).resolve()
         if not frame_path.exists():
```

### Comparing `tecton-0.7.0b9/tecton/cli/engine.py` & `tecton-0.7.0rc0/tecton/cli/engine.py`

 * *Files 13% similar despite different names*

```diff
@@ -2,100 +2,72 @@
 import os
 import sys
 import tarfile
 import time
 from pathlib import Path
 from typing import List
 from typing import Optional
+from typing import Sequence
 from typing import Tuple
 from typing import Union
 
 import requests
 from google.protobuf.json_format import MessageToJson
 from yaspin.spinners import Spinners
 
 import tecton
-from .cli_utils import confirm_or_exit
-from .error_utils import format_server_errors
 from tecton._internals import metadata_service
 from tecton._internals.analytics import StateUpdateEventMetrics
 from tecton._internals.analytics import StateUpdateResponse
 from tecton._internals.utils import is_live_workspace
 from tecton.cli import printer
 from tecton.cli.engine_renderer import ThickPlanRenderingClient
 from tecton.cli.engine_renderer import ThinPlanRenderingClient
 from tecton.framework import base_tecton_object
 from tecton_core.errors import TectonAPIValidationError
 from tecton_core.errors import TectonInternalError
 from tecton_core.feature_definition_wrapper import FrameworkVersion
 from tecton_core.id_helper import IdHelper
-from tecton_proto.args.entity_pb2 import EntityArgs
 from tecton_proto.args.fco_args_pb2 import FcoArgs
-from tecton_proto.args.feature_service_pb2 import FeatureServiceArgs
-from tecton_proto.args.feature_view_pb2 import FeatureViewArgs
 from tecton_proto.args.repo_metadata_pb2 import FeatureRepoSourceInfo
-from tecton_proto.args.transformation_pb2 import TransformationArgs
-from tecton_proto.args.virtual_data_source_pb2 import VirtualDataSourceArgs
+from tecton_proto.data.state_update_pb2 import StateUpdateRequest
 from tecton_proto.data.state_update_pb2 import ValidationMessage
 from tecton_proto.metadataservice.metadata_service_pb2 import ApplyStateUpdateRequest
 from tecton_proto.metadataservice.metadata_service_pb2 import NewStateUpdateRequest
 from tecton_proto.metadataservice.metadata_service_pb2 import NewStateUpdateRequestV2
 from tecton_proto.metadataservice.metadata_service_pb2 import QueryStateUpdateRequest
 from tecton_proto.metadataservice.metadata_service_pb2 import QueryStateUpdateRequestV2
 from tecton_proto.metadataservice.metadata_service_pb2 import QueryStateUpdateResponse
 from tecton_proto.metadataservice.metadata_service_pb2 import QueryStateUpdateResponseV2
 
+from .cli_utils import confirm_or_exit
+from .error_utils import format_server_errors
+
 
-def get_declared_fco_args(objects) -> Tuple[List[FcoArgs], FeatureRepoSourceInfo]:
+def get_declared_fco_args(
+    objects: Sequence[base_tecton_object.BaseTectonObject],
+) -> Tuple[List[FcoArgs], FeatureRepoSourceInfo]:
     all_args = []
     repo_source_info = FeatureRepoSourceInfo()
 
     for fco_obj in objects:
-        # TODO(TEC-12828): Clean this logic up and improve type annotations.
-        if isinstance(fco_obj, base_tecton_object.BaseTectonObject):
-            all_args.append(fco_obj._build_args())
-            repo_source_info.source_info.append(fco_obj._source_info)
-        else:
-            fco_args = FcoArgs()
-            args = fco_obj._args
-            source_info = fco_obj._source_info
-            if isinstance(args, VirtualDataSourceArgs):
-                source_info.fco_id.CopyFrom(args.virtual_data_source_id)
-                fco_args.virtual_data_source.CopyFrom(args)
-            elif isinstance(args, EntityArgs):
-                source_info.fco_id.CopyFrom(args.entity_id)
-                fco_args.entity.CopyFrom(args)
-            elif isinstance(args, TransformationArgs):
-                source_info.fco_id.CopyFrom(args.transformation_id)
-                fco_args.transformation.CopyFrom(args)
-            elif isinstance(args, FeatureViewArgs):
-                source_info.fco_id.CopyFrom(args.feature_view_id)
-                fco_args.feature_view.CopyFrom(args)
-            elif isinstance(args, FeatureServiceArgs):
-                source_info.fco_id.CopyFrom(args.feature_service_id)
-                fco_args.feature_service.CopyFrom(args)
-            else:
-                raise RuntimeError(f"Unknown object {fco_obj}")
-
-            all_args.append(fco_args)
-
-            source_info.CopyFrom(source_info)
-            repo_source_info.source_info.append(source_info)
+        all_args.append(fco_obj._build_args())
+        repo_source_info.source_info.append(fco_obj._source_info)
 
     return all_args, repo_source_info
 
 
-def dump_local_state(objects):
+def dump_local_state(objects: base_tecton_object.BaseTectonObject):
     with printer.safe_yaspin(Spinners.earth, text="Collecting local feature declarations") as sp:
         fco_args, repo_source_info = get_declared_fco_args(objects)
         sp.ok(printer.safe_string("✅"))
 
-    request_plan = NewStateUpdateRequest()
-    request_plan.request.fco_args.extend(fco_args)
-    request_plan.request.repo_source_info.CopyFrom(repo_source_info)
+    request_plan = NewStateUpdateRequest(
+        request=StateUpdateRequest(fco_args=fco_args, repo_source_info=repo_source_info)
+    )
     printer.safe_print(MessageToJson(request_plan, including_default_value_fields=True))
 
 
 # upload tar.gz of python files to url via PUT request
 def upload_files(repo_files: List[Path], repo_root, url: str):
     tar_bytes = io.BytesIO()
     with tarfile.open(fileobj=tar_bytes, mode="w|gz") as targz:
@@ -104,15 +76,15 @@
     for _ in range(3):
         try:
             r = requests.put(url, data=tar_bytes.getbuffer())
             if r.status_code != 200:
                 # We will get 403 (forbidden) when the signed url expires.
                 if r.status_code == 403:
                     printer.safe_print(
-                        f"\nUploading feature repo failed due to expired session. Please retry the command."
+                        "\nUploading feature repo failed due to expired session. Please retry the command."
                     )
                 else:
                     printer.safe_print(f"\nUploading feature repo failed with reason: {r.reason}")
                 sys.exit(1)
             return
         except requests.RequestException as e:
             last_error = e
@@ -210,17 +182,17 @@
     else:
         if query_response.validation_result.errors:
             return query_response.validation_result.errors
     return []
 
 
 def update_tecton_state(
-    objects,
+    objects: List[base_tecton_object.BaseTectonObject],
     repo_files: List[Path],
-    repo_root: str,
+    repo_root: Optional[str],
     apply,
     debug,
     interactive,
     upgrade_all: bool,
     workspace_name: str,
     suppress_warnings: bool = False,
     suppress_recreates: bool = False,
@@ -229,15 +201,15 @@
     plan_id: Optional[str] = None,
     no_color: bool = False,  # used for plan rendering on server-side
     enable_server_side_rendering: bool = True,
 ) -> StateUpdateResponse:
     # In debug mode we compute the plan synchronously, do not save it in the database, and do not allow to apply it.
     # Primary goal is allowing local development/debugging plans against remote clusters in read-only mode.
     assert not (debug and apply), "Cannot apply in debug mode"
-    json_out = json_out_path != None
+    json_out = json_out_path is not None
 
     if apply and plan_id:
         # Applying an existing plan, so skip preparing args.
         state_id = IdHelper.from_string(plan_id)
         request_query = QueryStateUpdateRequest()
         request_query.state_id.CopyFrom(state_id)
         request_query.workspace = workspace_name
@@ -258,15 +230,15 @@
 
         if response_query.error:
             printer.safe_print(response_query.error)
             return StateUpdateResponse.from_error_message(response_query.error, suppress_recreates)
         if len(_get_validation_errors(response_query, enable_server_side_rendering)) > 0:
             # Cannot pretty-print validation result using format_server_errors(), because collected local objects
             # might have changed since this plan was generated, so can't accurately match with this plan's FCOs.
-            message = f"Cannot apply plan because it had errors."
+            message = "Cannot apply plan because it had errors."
             printer.safe_print(message)
             return StateUpdateResponse.from_error_message(message, suppress_recreates)
 
     else:
         with printer.safe_yaspin(Spinners.earth, text="Collecting local feature declarations") as sp:
             fco_args, repo_source_info = get_declared_fco_args(objects)
             sp.ok(printer.safe_string("✅"))
```

### Comparing `tecton-0.7.0b9/tecton/cli/engine_renderer.py` & `tecton-0.7.0rc0/tecton/cli/engine_renderer.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,16 +9,14 @@
 from typing import Tuple
 from typing import Union
 
 from colorama import Fore
 from colorama import Style
 from google.protobuf.json_format import MessageToJson
 
-from .cli_utils import code_diff
-from .cli_utils import human_fco_type
 from tecton._internals import errors
 from tecton.cli import printer
 from tecton.cli.command import _cluster_url
 from tecton_core.id_helper import IdHelper
 from tecton_proto.args.diff_options_pb2 import FcoPropertyRenderingType
 from tecton_proto.args.entity_pb2 import EntityArgs
 from tecton_proto.args.feature_service_pb2 import FeatureServiceArgs
@@ -29,14 +27,17 @@
 from tecton_proto.cli import repo_diff_pb2
 from tecton_proto.common.data_source_type_pb2 import DataSourceType
 from tecton_proto.common.id_pb2 import Id
 from tecton_proto.data import state_update_pb2
 from tecton_proto.data.state_update_pb2 import FcoTransitionSideEffectStreamRestartType
 from tecton_proto.metadataservice.metadata_service_pb2 import QueryStateUpdateResponseV2
 
+from .cli_utils import code_diff
+from .cli_utils import human_fco_type
+
 
 UnwrappedFcoArgs = Union[
     VirtualDataSourceArgs,
     EntityArgs,
     TransformationArgs,
     FeatureViewArgs,
     FeatureServiceArgs,
@@ -190,15 +191,15 @@
         printer.safe_print(
             Style.NORMAL,
             f"View your plan in the Web UI: {_cluster_url()}/app/{self.workspace_name}/plan-summary/{IdHelper.to_string(self.plan_id)}",
             Style.RESET_ALL,
         )
 
         if num_printed_warnings_total > 0 and not self.suppress_warnings:
-            printer.safe_print(Style.BRIGHT, f"⚠️  Objects in plan contain warnings.", Style.RESET_ALL)
+            printer.safe_print(Style.BRIGHT, "⚠️  Objects in plan contain warnings.", Style.RESET_ALL)
 
         if self.recreates_suppressed and self.is_live_workspace:
             num_stream_restarts = 0
             num_streams_checkpoints_invalidated = 0
             for item in self.diffs:
                 _, transition_type, _, _ = self._parse_plan_item(item)
                 if transition_type == "UPDATE" and item.HasField("transition_side_effects"):
@@ -207,18 +208,18 @@
                         num_stream_restarts += 1
                     if "Checkpoints Invalidated" in update_type_transition:
                         num_streams_checkpoints_invalidated += 1
 
             printer.safe_print(
                 Style.BRIGHT,
                 Fore.RED,
-                f"\n⚠️  ⚠️  ⚠️  WARNING: This plan was computed with --suppress-recreates which will force-apply "
-                f"changes without recreating objects or rematerializing data. Feature schemas are unchanged, "
-                f"but please triple check your plan output as this could cause changes in feature semantics. "
-                f"Refer to https://docs.tecton.ai/ for full docs on --suppress-recreates.",
+                "\n⚠️  ⚠️  ⚠️  WARNING: This plan was computed with --suppress-recreates which will force-apply "
+                "changes without recreating objects or rematerializing data. Feature schemas are unchanged, "
+                "but please triple check your plan output as this could cause changes in feature semantics. "
+                "Refer to https://docs.tecton.ai/ for full docs on --suppress-recreates.",
                 Style.RESET_ALL,
             )
 
             if self.is_live_workspace and num_stream_restarts > 0:
                 warning_message = f"\n{num_stream_restarts} streaming materialization job(s) will be restarted. "
                 if num_streams_checkpoints_invalidated > 0:
                     warning_message += (
@@ -392,14 +393,17 @@
 
         diff_indent = 4
         for diff_item in item.diff:
             property_name_colwidth = max(len(diff_item.property_name) + 2, colwidth + 1)
             if diff_item.rendering_type == FcoPropertyRenderingType.FCO_PROPERTY_RENDERING_TYPE_HIDDEN:
                 continue
             if len(diff_item.val_existing) + len(diff_item.val_declared) < 80:
+                if diff_item.rendering_type == FcoPropertyRenderingType.FCO_PROPERTY_RENDERING_TYPE_REDACTED:
+                    diff_item.val_existing = "[REDACTED]" if diff_item.val_existing else diff_item.val_existing
+                    diff_item.val_declared = "[REDACTED]" if diff_item.val_declared else diff_item.val_declared
                 msg = (
                     (diff_item.property_name + ": ").ljust(property_name_colwidth)
                     + diff_item.val_existing
                     + " -> "
                     + diff_item.val_declared
                 )
                 printer.safe_print((diff_indent * " ") + color + msg + Fore.RESET)
@@ -458,15 +462,16 @@
     def _get_tecton_object_type(self, fco_diff: state_update_pb2.FcoDiff) -> repo_diff_pb2.TectonObjectType:
         if fco_diff.type == state_update_pb2.FcoTransitionType.CREATE:
             fco_type = fco_diff.declared_args.WhichOneof("args")
         else:
             fco_type = fco_diff.existing_args.WhichOneof("args")
 
         if fco_type not in _FCO_TYPE_TO_OBJECT_TYPE:
-            raise errors.INTERNAL_ERROR(f"Error computing Tecton object type from FCO type: {fco_type}.")
+            msg = f"Error computing Tecton object type from FCO type: {fco_type}."
+            raise errors.INTERNAL_ERROR(msg)
 
         return _FCO_TYPE_TO_OBJECT_TYPE[fco_type]
 
     def _update_side_effects_transition_display(self, item: state_update_pb2.FcoDiff) -> str:
         stream_restart_type = FcoTransitionSideEffectStreamRestartType.Name(
             item.transition_side_effects.stream_restart_type
         )
```

### Comparing `tecton-0.7.0b9/tecton/cli/error_utils.py` & `tecton-0.7.0rc0/tecton/cli/error_utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,26 +1,28 @@
 import ast
 import traceback
 from dataclasses import dataclass
 from pathlib import Path
 from typing import List
 from typing import Optional
+from typing import Sequence
 from typing import Tuple
 
 from colorama import Fore
 from pygments import highlight
 from pygments.formatters import TerminalFormatter
 from pygments.lexers import PythonLexer
 
-from .cli_utils import bold
 from tecton.cli import printer
 from tecton.framework import base_tecton_object
 from tecton_core.id_helper import IdHelper
 from tecton_proto.data.state_update_pb2 import ValidationMessage
 
+from .cli_utils import bold
+
 
 # Fallback: if extract_code_block failed to parse the file (maybe due to a syntax error in user
 # code), just try to unintelligently extract some lines around lineno.
 def extract_lines(file_path: Path, lineno: int) -> Tuple[Optional[str], Optional[int]]:
     with open(file_path) as f:
         lines = list(f.read().splitlines())
 
@@ -137,15 +139,16 @@
                     lineno=line_no,
                     code_block=code,
                     code_block_start_line=start_line,
                 )
             )
 
     # relative path formatting helper
-    relp = lambda x: str(x.relative_to(repo_root))
+    def relp(x):
+        return str(x.relative_to(repo_root))
 
     if pretty_frames:
         filename = pretty_frames[-1].file_path
         lineno = pretty_frames[-1].lineno
         printer.safe_print(
             Fore.RED
             + f"Error while processing {bold(relp(filename))}, at line {bold(str(lineno))}: {error_message}"
@@ -169,46 +172,41 @@
         printer.safe_print(highlight(pretty_frame.code_block, PythonLexer(), tf))
 
     if error_details:
         printer.safe_print("=================== Error: ===============================")
         printer.safe_print(error_details)
 
 
-# TODO(TEC-12828): Clean up unified/declarative fork and add type annotations.
-def format_validation_location_lite(obj, repo_root: str) -> None:
-    obj_name = obj.info.name if isinstance(obj, base_tecton_object.BaseTectonObject) else obj.name
+def format_validation_location_lite(obj: base_tecton_object.BaseTectonObject, repo_root: str) -> None:
     printer.safe_print(
         " " * 4,
-        f"in {obj.__class__.__name__} {obj_name} declared in {bold(obj._source_info.source_filename + ':' + str(obj._source_info.source_lineno))}",
+        f"in {obj.__class__.__name__} {obj.name} declared in {bold(obj._source_info.source_filename + ':' + str(obj._source_info.source_lineno))}",
     )
 
 
-# TODO(TEC-12828): Clean up unified/declarative fork and add type annotations.
-def format_validation_location_fancy(obj, repo_root: str) -> None:
+def format_validation_location_fancy(obj: base_tecton_object.BaseTectonObject, repo_root: str) -> None:
     filename = obj._source_info.source_filename
     file_path = Path(repo_root) / filename
     lineno = int(obj._source_info.source_lineno)
 
     code, start_line = extract_code_block(file_path, lineno)
     tf = TerminalFormatter(bg="dark", linenos=True)
     tf._lineno = start_line
-
-    obj_name = obj.info.name if isinstance(obj, base_tecton_object.BaseTectonObject) else obj.name
-
     printer.safe_print(
-        f"=================== {obj.__class__.__name__} {obj_name} declared in {bold(filename)} ==================="
+        f"=================== {obj.__class__.__name__} {obj.name} declared in {bold(filename)} ==================="
     )
     printer.safe_print(highlight(code, PythonLexer(), tf))
 
 
-# TODO(TEC-12828): Add objects type annotation here.
-def format_server_errors(messages: List[ValidationMessage], objects, repo_root: str):
+def format_server_errors(
+    messages: List[ValidationMessage], objects: Sequence[base_tecton_object.BaseTectonObject], repo_root: str
+):
     obj_by_id = {}
     for fco_obj in objects:
-        obj_by_id[IdHelper.to_string(fco_obj._id)] = fco_obj
+        obj_by_id[fco_obj.id] = fco_obj
 
     for i, m in enumerate(messages):
         printer.safe_print(Fore.RED + m.message + Fore.RESET)
         for fco_ref in m.fco_refs:
             obj_id = IdHelper.to_string(fco_ref.fco_id)
             obj = obj_by_id[obj_id]
```

### Comparing `tecton-0.7.0b9/tecton/cli/printer.py` & `tecton-0.7.0rc0/tecton/cli/printer.py`

 * *Files 0% similar despite different names*

```diff
@@ -2,14 +2,15 @@
 import platform
 import re
 import string
 
 from yaspin import yaspin
 from yaspin.spinners import Spinners
 
+
 """
 A drop-in wrapper around `print` and `yaspin` that's primarily used to filter rich output
 (i.e. emojis) from CLI output.
 
 By default, rich output is disabled in non-Mac environments or if TECTON_RICH_OUTPUT != "1".
 """
```

### Comparing `tecton-0.7.0b9/tecton/cli/service_account.py` & `tecton-0.7.0rc0/tecton/cli/service_account.py`

 * *Files 1% similar despite different names*

```diff
@@ -62,15 +62,16 @@
 @click.option("-d", "--description", help="An optional, human readable description for this Service Account")
 @click.argument("id", required=True)
 def update(id, name, description):
     """Update the name or description of a Service Account."""
     request = UpdateServiceAccountRequest(id=id)
 
     if name is None and description is None:
-        raise click.ClickException("Please mention the field to update using --name or --description.")
+        msg = "Please mention the field to update using --name or --description."
+        raise click.ClickException(msg)
 
     if name:
         request.name = name
 
     if description is not None:
         request.description = description
```

### Comparing `tecton-0.7.0b9/tecton/cli/workspace.py` & `tecton-0.7.0rc0/tecton/cli/workspace.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,43 +1,55 @@
+import logging
 import sys
 
 import click
+from click import shell_completion
 from colorama import Fore
 
-from tecton import conf
 from tecton import tecton_context
 from tecton._internals import metadata_service
 from tecton._internals.utils import is_live_workspace
 from tecton._internals.workspace_utils import PROD_WORKSPACE_NAME_CLIENT
 from tecton.cli import printer
+from tecton.cli import workspace_utils
 from tecton.cli.command import TectonGroup
 from tecton.cli.engine import update_tecton_state
+from tecton_core import conf
 from tecton_proto.metadataservice.metadata_service_pb2 import CreateWorkspaceRequest
 from tecton_proto.metadataservice.metadata_service_pb2 import DeleteWorkspaceRequest
 from tecton_proto.metadataservice.metadata_service_pb2 import GetWorkspaceRequest
 from tecton_proto.metadataservice.metadata_service_pb2 import ListWorkspacesRequest
 
 
+logger = logging.getLogger(__name__)
+
+
+class WorkspaceType(click.ParamType):
+    name = "workspace"
+
+    def shell_complete(self, ctx, param, incomplete):
+        try:
+            workspace_names = {w.name for w in _list_workspaces()}
+        except (Exception, SystemExit) as e:
+            logger.error(f"\nTab-completion failed with error: {e}")
+            return []
+
+        return [shell_completion.CompletionItem(name) for name in workspace_names if name.startswith(incomplete)]
+
+
 @click.group("workspace", cls=TectonGroup)
 def workspace():
     """Manipulate a tecton workspace."""
 
 
 @workspace.command()
-@click.argument("workspace")
+@click.argument("workspace", type=WorkspaceType())
 def select(workspace):
     """Select WORKSPACE."""
-    # validate
-    workspace_names = {w.name for w in _list_workspaces()}
-    if workspace not in workspace_names:
-        printer.safe_print(
-            f'Workspace "{workspace}" not found. Run `tecton workspace list` to see list of available workspaces.'
-        )
-        sys.exit(1)
-
+    workspace_utils.check_workspace_exists(workspace)
     _switch_to_workspace(workspace)
 
 
 @workspace.command()
 def list():
     """List available workspaces."""
     current_workspace = tecton_context.get_current_workspace()
@@ -101,15 +113,15 @@
 Running "tecton plan" will compare your local repository
 against the remote repository, which is initially empty.
     """
     )
 
 
 @workspace.command()
-@click.argument("workspace")
+@click.argument("workspace", type=WorkspaceType())
 @click.option("--yes", "-y", is_flag=True)
 def delete(workspace, yes):
     """Delete workspace named WORKSPACE."""
     # validate
     if workspace == PROD_WORKSPACE_NAME_CLIENT:
         printer.safe_print(f"Deleting workspace '{PROD_WORKSPACE_NAME_CLIENT}' not allowed.")
         sys.exit(1)
```

### Comparing `tecton-0.7.0b9/tecton/fco_listers.py` & `tecton-0.7.0rc0/tecton/fco_listers.py`

 * *Files 8% similar despite different names*

```diff
@@ -17,52 +17,45 @@
     request = ListWorkspacesRequest()
     response = metadata_service.instance().ListWorkspaces(request)
     return sorted([workspace.name for workspace in response.workspaces])
 
 
 @sdk_public_method
 def list_feature_views(workspace_name: Optional[str] = None):
-    raise errors.TectonValidationError(
-        'list_feature_views() must be called from a Workspace object. E.g. tecton.get_workspace("<workspace>").list_feature_views().'
-    )
+    msg = 'list_feature_views() must be called from a Workspace object. E.g. tecton.get_workspace("<workspace>").list_feature_views().'
+    raise errors.TectonValidationError(msg)
 
 
 @sdk_public_method
 def list_feature_tables(workspace_name: Optional[str] = None):
-    raise errors.TectonValidationError(
-        'list_feature_tables() must be called from a Workspace object. E.g. tecton.get_workspace("<workspace>").list_feature_tables().'
-    )
+    msg = 'list_feature_tables() must be called from a Workspace object. E.g. tecton.get_workspace("<workspace>").list_feature_tables().'
+    raise errors.TectonValidationError(msg)
 
 
 @sdk_public_method
 def list_feature_services(workspace_name: Optional[str] = None):
-    raise errors.TectonValidationError(
-        'list_feature_services() must be called from a Workspace object. E.g. tecton.get_workspace("<workspace>").list_feature_services().'
-    )
+    msg = 'list_feature_services() must be called from a Workspace object. E.g. tecton.get_workspace("<workspace>").list_feature_services().'
+    raise errors.TectonValidationError(msg)
 
 
 @sdk_public_method
 def list_transformations(workspace_name: Optional[str] = None):
-    raise errors.TectonValidationError(
-        'list_transformations() must be called from a Workspace object. E.g. tecton.get_workspace("<workspace>").list_transformations().'
-    )
+    msg = 'list_transformations() must be called from a Workspace object. E.g. tecton.get_workspace("<workspace>").list_transformations().'
+    raise errors.TectonValidationError(msg)
 
 
 @sdk_public_method
 def list_entities(workspace_name: Optional[str] = None):
-    raise errors.TectonValidationError(
-        'list_entities() must be called from a Workspace object. E.g. tecton.get_workspace("<workspace>").list_entities().'
-    )
+    msg = 'list_entities() must be called from a Workspace object. E.g. tecton.get_workspace("<workspace>").list_entities().'
+    raise errors.TectonValidationError(msg)
 
 
 @sdk_public_method
 def list_data_sources(workspace_name: Optional[str] = None):
-    raise errors.TectonValidationError(
-        'list_data_sources() must be called from a Workspace object. E.g. tecton.get_workspace("<workspace>").list_data_sources().'
-    )
+    msg = 'list_data_sources() must be called from a Workspace object. E.g. tecton.get_workspace("<workspace>").list_data_sources().'
+    raise errors.TectonValidationError(msg)
 
 
 @sdk_public_method
 def list_datasets(workspace_name: Optional[str] = None):
-    raise errors.TectonValidationError(
-        'list_datasets() must be called from a Workspace object. E.g. tecton.get_workspace("<workspace>").list_datasets().'
-    )
+    msg = 'list_datasets() must be called from a Workspace object. E.g. tecton.get_workspace("<workspace>").list_datasets().'
+    raise errors.TectonValidationError(msg)
```

### Comparing `tecton-0.7.0b9/tecton/framework/base_tecton_object.py` & `tecton-0.7.0rc0/tecton/framework/base_tecton_object.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 import datetime
-import logging
 from typing import Dict
 from typing import Optional
+from typing import Set
 
 import attrs
 from typeguard import typechecked
 
 from tecton._internals import errors
 from tecton._internals import sdk_decorators
 from tecton.framework import compute_mode
@@ -13,56 +13,59 @@
 from tecton_core import specs
 from tecton_proto.args import basic_info_pb2
 from tecton_proto.args import fco_args_pb2
 from tecton_proto.args import repo_metadata_pb2
 from tecton_proto.common import id_pb2
 from tecton_proto.validation import validator_pb2
 
-# Use the shared tecton.validations logger for print validation messages.
-validation_logger = logging.getLogger("tecton.validations")
+
+_LOCAL_TECTON_OBJECTS: Set["BaseTectonObject"] = set()
 
 
 @attrs.frozen
 class TectonObjectInfo:
     """A public SDK dataclass containing common metadata used for all Tecton Objects."""
 
     id: str
     name: str
     description: Optional[str]
     tags: Dict[str, str]
     owner: Optional[str]
     workspace: Optional[str]
     created_at: Optional[datetime.datetime] = attrs.field(repr=False)
+    defined_in: Optional[str] = attrs.field(repr=False)
     _is_local_object: bool = attrs.field(repr=False)
 
     @classmethod
     @typechecked
     def from_args_proto(cls, basic_info: basic_info_pb2.BasicInfo, id: id_pb2.Id) -> "TectonObjectInfo":
         return cls(
             id=id_helper.IdHelper.to_string(id),
             name=basic_info.name,
             description=basic_info.description if basic_info.HasField("description") else None,
             tags=dict(basic_info.tags),
             owner=basic_info.owner if basic_info.HasField("owner") else None,
             created_at=None,  # created_at is only filled for remote (i.e. applied) Tecton objects.
             workspace=None,  # workspace is only filled for remote (i.e. applied) Tecton objects.
+            defined_in=None,  # defined_in is only filled for remote (i.e. applied) Tecton objects.
             is_local_object=True,
         )
 
     @classmethod
     @typechecked
     def from_spec(cls, spec: specs.TectonObjectSpec) -> "TectonObjectInfo":
         return cls(
             id=spec.id,
             name=spec.name,
             description=spec.metadata.description,
             tags=spec.metadata.tags,
             owner=spec.metadata.owner,
             created_at=spec.metadata.created_at,
             workspace=spec.workspace,
+            defined_in=spec.metadata.defined_in,
             is_local_object=spec.is_local_object,
         )
 
     @property
     def _id_proto(self) -> id_pb2.Id:
         return id_helper.IdHelper.from_string(self.id)
 
@@ -112,15 +115,16 @@
         (e.g. ``my_ds = BatchSource(name="my_ds", ...)``) may need to be validated before some of their methods can be called,
         e.g. ``my_feature_view.get_historical_features()``.
         """
         if compute_mode.get_compute_mode() == compute_mode.ComputeMode.ATHENA:
             raise errors.ATHENA_COMPUTE_NOT_SUPPORTED_IN_LOCAL_MODE
 
         if self._is_valid:
-            validation_logger.info(f"{self.__class__.__name__} '{self.name}': Successfully validated. (Cached)")
+            # TODO(TEC-14146): Use the logging module instead of print statements.
+            print(f"{self.__class__.__name__} '{self.name}': Successfully validated. (Cached)")
         else:
             self._validate()
 
     def _build_args(self) -> fco_args_pb2.FcoArgs:
         """Returns a copy of the args as a FcoArgs proto for plan/apply logic."""
         raise NotImplementedError
 
@@ -155,20 +159,33 @@
 
     @property
     def workspace(self) -> Optional[str]:
         """Returns the workspace that this Tecton object belongs to. `None` for locally defined objects."""
         return self.info.workspace
 
     @property
+    def created_at(self) -> Optional[datetime.datetime]:
+        """Returns the time that this Tecton object was created or last updated. `None` for locally defined objects."""
+        return self.info.created_at
+
+    @property
+    def defined_in(self) -> Optional[str]:
+        """The repo filename where this object was declared. `None` for locally defined objects."""
+        return self.info.defined_in
+
+    @property
     def _is_local_object(self) -> bool:
         """Returns True if the object was defined locally, i.e. was not applied and fetched from the Tecton backend."""
         return self.info._is_local_object
 
     @property
     def _id_proto(self) -> id_pb2.Id:
         """Returns the proto version of the Tecton object id."""
         return self.info._id_proto
 
-    @property
-    # TODO(jake): Remove this property after deleting declarative code.
-    def _id(self) -> id_pb2.Id:
-        return self.info._id_proto
+
+def _register_local_object(obj: BaseTectonObject) -> None:
+    """Register the tecton object to the set of global tecton objects.
+
+    The global set is used to collect objects during `tecton apply`.
+    """
+    _LOCAL_TECTON_OBJECTS.add(obj)
```

### Comparing `tecton-0.7.0b9/tecton/framework/compute_mode.py` & `tecton-0.7.0rc0/tecton/framework/compute_mode.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 from enum import Enum
 
-from tecton import conf
+from tecton_core import conf
 
 
 class ComputeMode(str, Enum):
     SPARK = "spark"
     SNOWFLAKE = "snowflake"
     ATHENA = "athena"
 
@@ -16,10 +16,9 @@
     if conf.get_bool("ALPHA_SNOWFLAKE_COMPUTE_ENABLED") or compute_mode == ComputeMode.SNOWFLAKE:
         return ComputeMode.SNOWFLAKE
     elif conf.get_bool("ALPHA_ATHENA_COMPUTE_ENABLED") or compute_mode == ComputeMode.ATHENA:
         return ComputeMode.ATHENA
     elif compute_mode == ComputeMode.SPARK:
         return ComputeMode.SPARK
     else:
-        raise ValueError(
-            f"Invalid Tecton compute mode: {compute_mode}. Must be one of {[[e.value for e in ComputeMode]]}"
-        )
+        msg = f"Invalid Tecton compute mode: {compute_mode}. Must be one of {[[e.value for e in ComputeMode]]}"
+        raise ValueError(msg)
```

### Comparing `tecton-0.7.0b9/tecton/framework/configs.py` & `tecton-0.7.0rc0/tecton/framework/configs.py`

 * *Files 2% similar despite different names*

```diff
@@ -82,24 +82,27 @@
 
 
 @attrs.define(auto_attribs=True)
 class EMRJsonClusterConfig:
     """Configuration used to specify materialization clusters using json on EMR.
 
     This class describes the attributes of the new clusters which are created in EMR during
-    materialization jobs. This is an alpha feature. Contact Tecton Support before using.
+    materialization jobs. Please find more details in `User Guide`_.
 
     :param json: A JSON string used to directly configure the cluster used in materialization.
+
+    .. _User Guide: https://docs.tecton.ai/docs/materializing-features/configuring-job-clusters-via-json#configuring-emr-job-clusters
     """
 
     json: Optional[str] = None
 
     def _to_cluster_proto(self) -> feature_view_pb2.ClusterConfig:
         if self.json is None:
-            raise ValueError("Missing input JSON string")
+            msg = "Missing input JSON string"
+            raise ValueError(msg)
         return feature_view_pb2.ClusterConfig(json_emr=_to_json_cluster_proto(self.json))
 
 
 @attrs.define(auto_attribs=True)
 class EMRClusterConfig:
     """Configuration used to specify materialization cluster options on EMR.
 
@@ -179,22 +182,20 @@
 
     def _to_proto(self) -> feature_view_pb2.NewClusterConfig:
         proto = feature_view_pb2.NewClusterConfig()
         if self.instance_type:
             proto.instance_type = self.instance_type
         if self.instance_availability:
             if self.instance_availability not in EMR_SUPPORTED_AVAILABILITY:
-                raise ValueError(
-                    f"Instance availability {self.instance_availability} is not supported. Choose one of {EMR_SUPPORTED_AVAILABILITY}"
-                )
+                msg = f"Instance availability {self.instance_availability} is not supported. Choose one of {EMR_SUPPORTED_AVAILABILITY}"
+                raise ValueError(msg)
             proto.instance_availability = self.instance_availability
         if self.emr_version not in EMR_SUPPORTED_SPARK:
-            raise ValueError(
-                f"EMR version {self.emr_version} is not supported. Supported versions: {EMR_SUPPORTED_SPARK}"
-            )
+            msg = f"EMR version {self.emr_version} is not supported. Supported versions: {EMR_SUPPORTED_SPARK}"
+            raise ValueError(msg)
         if self.number_of_workers:
             proto.number_of_workers = self.number_of_workers
         if self.first_on_demand:
             proto.first_on_demand = self.first_on_demand
         if self.root_volume_size_in_gb:
             proto.root_volume_size_in_gb = self.root_volume_size_in_gb
         if self.extra_pip_dependencies:
@@ -207,28 +208,50 @@
         return proto
 
     def _to_cluster_proto(self) -> feature_view_pb2.ClusterConfig:
         return feature_view_pb2.ClusterConfig(new_emr=self._to_proto())
 
 
 @attrs.define(auto_attribs=True)
+class DataprocJsonClusterConfig:
+    """Configuration used to specify materialization clusters using json on Dataproc.
+
+    This class describes the attributes of the new clusters which are created in Dataproc during
+    materialization jobs. This feature is only available for private preview.
+
+    :param json: A JSON string used to directly configure the cluster used in materialization.
+    """
+
+    json: Optional[str] = None
+
+    def _to_cluster_proto(self) -> feature_view_pb2.ClusterConfig:
+        if self.json is None:
+            msg = "Missing input JSON string"
+            raise ValueError(msg)
+        return feature_view_pb2.ClusterConfig(json_dataproc=_to_json_cluster_proto(self.json))
+
+
+@attrs.define(auto_attribs=True)
 class DatabricksJsonClusterConfig:
     """Configuration used to specify materialization clusters using json on Databricks.
 
     This class describes the attributes of the new clusters which are created in Databricks during
-    materialization jobs. This is an alpha feature. Contact Tecton Support before using.
+    materialization jobs. Please find more details in `User Guide`_.
 
     :param json: A JSON string used to directly configure the cluster used in materialization.
+
+    .. _User Guide: https://docs.tecton.ai/docs/materializing-features/configuring-job-clusters-via-json#configuring-databricks-job-clusters
     """
 
     json: Optional[str] = None
 
     def _to_cluster_proto(self) -> feature_view_pb2.ClusterConfig:
         if self.json is None:
-            raise ValueError("Missing input JSON string")
+            msg = "Missing input JSON string"
+            raise ValueError(msg)
         return feature_view_pb2.ClusterConfig(json_databricks=_to_json_cluster_proto(self.json))
 
 
 @attrs.define(auto_attribs=True)
 class DatabricksClusterConfig:
     """Configuration used to specify materialization cluster options on Databricks.
 
@@ -239,16 +262,14 @@
         Additionally, Graviton instances such as the m6g family are not supported. If not specified, a value determined by the Tecton backend is used.
     :param instance_availability: Instance availability for the cluster : "spot", "on_demand", or "spot_with_fallback".
         If not specified, default is spot.
     :param first_on_demand: The first `first_on_demand` nodes of the cluster will use on_demand instances. The rest will use the type specified by instance_availability.
         If first_on_demand >= 1, the driver node use on_demand instance type.
     :param number_of_workers: Number of instances for the materialization job. If not specified, a value determined by the Tecton backend is used.
         If set to 0 then jobs will be run in single-node clusters.
-    :param root_volume_size_in_gb: Size of the root volume in GB per instance for the materialization job.
-        If not specified, a value determined by the Tecton backend is used.
     :param extra_pip_dependencies: Extra pip dependencies to be installed on the materialization cluster. Must be PyPI packages, or wheels/eggs in S3 or DBFS.
     :param spark_config: Map of Spark configuration options and their respective values that will be passed to the
         FeatureView materialization Spark cluster.
     :param dbr_version: DBR version of the cluster. Supported versions include 9.1.x-scala2.12, 10.4.x-scala2.12, and 11.3.x-scala2.12. (Default: 10.4.x-scala2.12)
 
     Note on ``extra_pip_dependencies``: This is a list of packages that will be installed during materialization.
     To use PyPI packages, specify the package name and optionally the version, e.g. ``"tensorflow"`` or ``"tensorflow==2.2.0"``.
@@ -299,47 +320,43 @@
 
     """
 
     instance_type: Optional[str] = None
     instance_availability: Optional[str] = None
     number_of_workers: Optional[int] = None
     first_on_demand: Optional[int] = None
-    root_volume_size_in_gb: Optional[int] = None
     extra_pip_dependencies: Optional[List[str]] = None
     spark_config: Optional[Dict[str, str]] = None
     dbr_version: str = DEFAULT_SPARK_VERSIONS["databricks_spark_version"]
 
     def _to_proto(self) -> feature_view_pb2.NewClusterConfig:
         proto = feature_view_pb2.NewClusterConfig()
         if self.instance_type:
             proto.instance_type = self.instance_type
         if self.instance_availability:
             if self.instance_availability not in DATABRICKS_SUPPORTED_AVAILABILITY:
-                raise ValueError(
-                    f"Instance availability {self.instance_availability} is not supported. Choose {AVAILABILITY_SPOT}, {AVAILABILITY_ON_DEMAND} or {AVAILABILITY_SPOT_FALLBACK}"
-                )
+                msg = f"Instance availability {self.instance_availability} is not supported. Choose {AVAILABILITY_SPOT}, {AVAILABILITY_ON_DEMAND} or {AVAILABILITY_SPOT_FALLBACK}"
+                raise ValueError(msg)
             proto.instance_availability = self.instance_availability
         if self.dbr_version not in DATABRICKS_SUPPORTED_SPARK:
-            raise ValueError(
-                f"Databricks version {self.dbr_version} is not supported. Supported versions: {DATABRICKS_SUPPORTED_SPARK}"
-            )
+            msg = f"Databricks version {self.dbr_version} is not supported. Supported versions: {DATABRICKS_SUPPORTED_SPARK}"
+            raise ValueError(msg)
 
         if self.number_of_workers is not None:
             proto.number_of_workers = self.number_of_workers
-        if self.root_volume_size_in_gb:
-            proto.root_volume_size_in_gb = self.root_volume_size_in_gb
         if self.first_on_demand:
             proto.first_on_demand = self.first_on_demand
         if self.extra_pip_dependencies:
             # Pretty easy to do e.g. extra_pip_dependencies="tensorflow" by mistake and end up with
             # [t, e, n, s, o, r, f, l, o, w] as a list of dependencies passed to the Spark job.
             #
             # Since this is annoying to debug, we check for that here.
             if isinstance(self.extra_pip_dependencies, str):
-                raise ValueError("extra_pip_dependencies must be a list")
+                msg = "extra_pip_dependencies must be a list"
+                raise ValueError(msg)
             proto.extra_pip_dependencies.extend(self.extra_pip_dependencies)
         if self.spark_config:
             spark_config = SparkConfigWrapper(self.spark_config)._to_proto()
             proto.spark_config.CopyFrom(spark_config)
         proto.pinned_spark_version = self.dbr_version
 
         return proto
@@ -426,39 +443,35 @@
         return store_config
 
 
 @attrs.define(auto_attribs=True)
 class RedisConfig:
     """(Config Class) RedisConfig Class.
 
-    This class describes the attributes of Redis based online feature store for the feature definition.
-    Currently there are no attributes for this class.
-    Users can specify online_store = RedisConfig()
-    Note : Your Tecton deployment needs to be connected to Redis before you can use this configuration option.
-    Please contact Tecton support for details.
+    This class describes the attributes of Redis-based online feature store for the feature definition.
+    Note : Your Tecton deployment needs to be connected to Redis before you can use this configuration option. See https://docs.tecton.ai/docs/setting-up-tecton/setting-up-other-components/connecting-redis-as-an-online-store for details and please contact Tecton Support if you need assistance.
+
+    :param primary_endpoint: Primary endpoint for the Redis Cluster. This is optional and if absent, Tecton will use the default Redis Cluster configured for your deployment.
+    :param authentication_token: Authentication token for the Redis Cluster, must be provided if primary_endpoint is present.
     """
 
-    endpoint: Optional[str] = None
-    tls_enabled: Optional[bool] = None
-    auth_token: Optional[str] = None
-    cluster_mode_enabled: Optional[bool] = None
+    primary_endpoint: Optional[str] = None
+    authentication_token: Optional[str] = None
 
     def _to_proto(self):
-        store_config = feature_view_pb2.OnlineStoreConfig()
-        store_config.redis.enabled = True
-        store_config.redis.SetInParent()
-        if self.endpoint:
-            store_config.redis.primary_endpoint = self.endpoint
-        if self.tls_enabled:
-            store_config.redis.tls_enabled = self.tls_enabled
-        if self.auth_token:
-            store_config.redis.authentication_token = self.auth_token
-        if self.cluster_mode_enabled:
-            store_config.redis.cluster_enabled = self.cluster_mode_enabled
-        return store_config
+        # TODO(TEC-13889): Remove the tls_enabled field from proto and backend as this should always be set to true
+        tls_enabled = True if self.authentication_token else False
+        return feature_view_pb2.OnlineStoreConfig(
+            redis=feature_view_pb2.RedisOnlineStore(
+                enabled=True,
+                primary_endpoint=self.primary_endpoint,
+                authentication_token=self.authentication_token,
+                tls_enabled=tls_enabled,
+            )
+        )
 
 
 @attrs.define(auto_attribs=True)
 class MonitoringConfig:
     """Configuration used to specify monitoring options.
 
     This class describes the FeatureView materialization freshness and alerting configurations. Requires
@@ -1175,14 +1188,96 @@
     def data_delay(self):
         return self._data_delay
 
     def _merge_batch_args(self, data_source_args: virtual_data_source_pb2.VirtualDataSourceArgs):
         data_source_args.hive_ds_config.CopyFrom(self._args)
 
 
+class UnityConfig(BaseBatchConfig):
+    """
+    Configuration used to reference a Unity table.
+
+    The UnityConfig class is used to create a reference to a Unity Table.
+
+    This class is used as an input to a :class:`BatchSource`'s parameter ``batch_config``. Declaring this configuration
+    class alone will not register a Data Source. Instead, declare as a part of ``BatchSource`` that takes this configuration
+    class instance as a parameter.
+    """
+
+    def __init__(
+        self,
+        catalog: str,
+        schema: str,
+        table: str,
+        timestamp_field: Optional[str] = None,
+        timestamp_format: Optional[str] = None,
+        datetime_partition_columns: Optional[List[DatetimePartitionColumn]] = None,
+        post_processor: Optional[Callable] = None,
+        data_delay: datetime.timedelta = datetime.timedelta(seconds=0),
+    ):
+        """
+        Instantiates a new UnityConfig.
+
+        :param catalog: A catalog registered in Unity
+        :param schema: A schema registered in Unity
+        :param table: A table registered in Unity
+        :param timestamp_field: The timestamp column in this data source that should be used by `FilteredSource`
+                                    to filter data from this source, before any feature view transformations are applied.
+                                    Only required if this source is used with `FilteredSource`.
+        :param timestamp_format: Format of string-encoded timestamp column (e.g. "yyyy-MM-dd'T'hh:mm:ss.SSS'Z'").
+                                 If the timestamp string cannot be parsed with this format, Tecton will fallback and attempt to
+                                 use the default timestamp parser.
+        :param datetime_partition_columns: List of DatetimePartitionColumn the raw data is partitioned by, otherwise None.
+        :param post_processor: Python user defined function ``f(DataFrame) -> DataFrame`` that takes in raw
+                                     PySpark data source DataFrame and translates it to the DataFrame to be
+                                     consumed by the Feature View.
+        :param data_delay: By default, incremental materialization jobs run immediately at the end of the
+                                    batch schedule period. This parameter configures how long they wait after the end
+                                    of the period before starting, typically to ensure that all data has landed.
+                                    For example, if a feature view has a `batch_schedule` of 1 day and one of
+                                    the data source inputs has `data_delay=timedelta(hours=1)` set, then
+                                    incremental materialization jobs will run at `01:00` UTC.
+
+        :return: A UnityConfig class instance.
+
+        """
+        self._args = data_source_pb2.UnityDataSourceArgs(
+            catalog=catalog,
+            schema=schema,
+            table=table,
+            timestamp_format=timestamp_format,
+            common_args=data_source_pb2.BatchDataSourceCommonArgs(
+                data_delay=time_utils.timedelta_to_proto(data_delay),
+                timestamp_field=timestamp_field,
+            ),
+        )
+
+        if datetime_partition_columns:
+            for column in datetime_partition_columns:
+                column_args = data_source_pb2.DatetimePartitionColumnArgs(
+                    column_name=column.column_name,
+                    datepart=column.datepart,
+                    zero_padded=column.zero_padded,
+                    format_string=column.format_string,
+                )
+                self._args.datetime_partition_columns.append(column_args)
+        if post_processor is not None and function_serialization.should_serialize_function(post_processor):
+            self._args.common_args.post_processor.CopyFrom(function_serialization.to_proto(post_processor))
+
+        self._data_delay = data_delay
+        self.post_processor = post_processor
+
+    @property
+    def data_delay(self):
+        return self._data_delay
+
+    def _merge_batch_args(self, data_source_args: virtual_data_source_pb2.VirtualDataSourceArgs):
+        data_source_args.unity_ds_config.CopyFrom(self._args)
+
+
 class SnowflakeConfig(BaseBatchConfig):
     """
     Configuration used to reference a Snowflake table or query.
 
     The SnowflakeConfig class is used to create a reference to a Snowflake table. You can also create a
     reference to a query on one or more tables, which will be registered in Tecton in a similar way as a view
     is registered in other data systems.
@@ -1336,17 +1431,19 @@
                                                   query="SELECT timestamp as ts, created, user_id, ad_id, duration"
                                                         "FROM ad_serving_features")
         """
         self._args = args = data_source_pb2.RedshiftDataSourceArgs()
         args.endpoint = endpoint
 
         if table and query:
-            raise AssertionError(f"Should only specify one of table and query sources for redshift")
+            msg = "Should only specify one of table and query sources for redshift"
+            raise AssertionError(msg)
         if not table and not query:
-            raise AssertionError(f"Missing both table and query sources for redshift, exactly one must be present")
+            msg = "Missing both table and query sources for redshift, exactly one must be present"
+            raise AssertionError(msg)
 
         if table:
             args.table = table
         else:
             args.query = query
 
         if post_processor is not None and function_serialization.should_serialize_function(post_processor):
@@ -1407,21 +1504,19 @@
                                     ``end_time`` set.
 
         :return: A SparkBatchConfig class instance.
         """
         params = list(inspect.signature(data_source_function).parameters)
         function_name = data_source_function.__name__
         if supports_time_filtering and params != ["spark", "filter_context"]:
-            raise AssertionError(
-                f"Data Source Function {function_name}'s required signature is `{function_name}(spark, filter_context)` when supports_time_filtering is True"
-            )
+            msg = f"Data Source Function {function_name}'s required signature is `{function_name}(spark, filter_context)` when supports_time_filtering is True"
+            raise AssertionError(msg)
         elif not supports_time_filtering and params != ["spark"]:
-            raise AssertionError(
-                f"Data Source Function {function_name}'s required signature is `{function_name}(spark)`"
-            )
+            msg = f"Data Source Function {function_name}'s required signature is `{function_name}(spark)`"
+            raise AssertionError(msg)
 
         self._args = data_source_pb2.SparkBatchConfigArgs()
         if function_serialization.should_serialize_function(data_source_function):
             self._args.data_source_function.CopyFrom(function_serialization.to_proto(data_source_function))
         self._args.data_delay.FromTimedelta(data_delay)
         self._args.supports_time_filtering = supports_time_filtering
         self._data_delay = data_delay
@@ -1528,17 +1623,16 @@
                     a streaming ``DataFrame``.
 
         :return: A SparkStreamConfig class instance.
         """
         params = list(inspect.signature(data_source_function).parameters)
         function_name = data_source_function.__name__
         if params != ["spark"]:
-            raise AssertionError(
-                f"Data Source Function {function_name}'s required signature is `{function_name}(spark)`"
-            )
+            msg = f"Data Source Function {function_name}'s required signature is `{function_name}(spark)`"
+            raise AssertionError(msg)
 
         self._args = data_source_pb2.SparkStreamConfigArgs()
         if function_serialization.should_serialize_function(data_source_function):
             self._args.data_source_function.CopyFrom(function_serialization.to_proto(data_source_function))
         self.data_source_function = data_source_function
 
     def _merge_stream_args(self, data_source_args: virtual_data_source_pb2.VirtualDataSourceArgs):
@@ -1665,25 +1759,29 @@
         proto.column = self.column
 
         if isinstance(self.function, str):
             proto.function = self.function
         elif isinstance(self.function, AggregationFunction):
             proto.function = self.function.name
             for k, v in self.function.params.items():
-                assert isinstance(v, int)
-                proto.function_params[k].CopyFrom(feature_view_pb2.ParamValue(int64_value=v))
+                if isinstance(v, int):
+                    proto.function_params[k].CopyFrom(feature_view_pb2.ParamValue(int64_value=v))
+                else:
+                    proto.function_params[k].CopyFrom(feature_view_pb2.ParamValue(double_value=v))
         else:
-            raise TypeError(f"Invalid function type: {type(self.function)}")
+            msg = f"Invalid function type: {type(self.function)}"
+            raise TypeError(msg)
 
         proto.time_window.FromTimedelta(self.time_window)
 
         if self.name:
             proto.name = self.name
         else:
             proto.name = feature_view_utils.construct_aggregation_output_feature_name(
                 proto.column,
-                feature_view_utils.resolve_function_name(proto.function, proto.function_params),
+                proto.function,
+                proto.function_params,
                 proto.time_window,
                 time_utils.timedelta_to_proto(aggregation_interval),
                 is_continuous,
             )
         return proto
```

### Comparing `tecton-0.7.0b9/tecton/framework/data_frame.py` & `tecton-0.7.0rc0/tecton/framework/data_frame.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,8 +1,10 @@
+import logging
 import time
+import typing
 from datetime import datetime
 from typing import Any
 from typing import Dict
 from typing import List
 from typing import Optional
 from typing import Union
 
@@ -15,28 +17,41 @@
 from tecton import snowflake_context
 from tecton._internals.data_frame_helper import _get_time_limits_of_dataframe
 from tecton._internals.rewrite import rewrite_tree_for_spine
 from tecton._internals.sdk_decorators import sdk_public_method
 from tecton._internals.utils import get_time_limits_of_pandas_dataframe
 from tecton_athena import athena_session
 from tecton_athena.data_catalog_helper import register_feature_view_as_athena_table_if_necessary
+from tecton_athena.query.translate import AthenaSqlExecutor
+from tecton_athena.query.translate import athena_convert
 from tecton_core import conf
-from tecton_core.logger import get_logger
+from tecton_core import data_types
 from tecton_core.query.node_interface import DataframeWrapper
 from tecton_core.query.node_interface import NodeRef
 from tecton_core.query.node_interface import recurse_query_tree
+from tecton_core.query.nodes import MultiOdfvPipelineNode
 from tecton_core.query.nodes import OfflineStoreScanNode
+from tecton_core.query.nodes import RenameColsNode
 from tecton_core.query.nodes import UserSpecifiedDataNode
+from tecton_core.query.rewrite import tree_contains
+from tecton_core.schema import Schema
+from tecton_spark import schema_spark_utils
 from tecton_spark.query import translate
 
-logger = get_logger("Dataframe")
+
+if typing.TYPE_CHECKING:
+    import snowflake.snowpark
+
+logger = logging.getLogger(__name__)
 
 # We have to use Any here because snowflake.snowpark.DataFrame is not a direct dependency of the SDK.
 snowpark_dataframe = Any
 
+_internal_index_column = "_tecton_internal_index_col"
+
 
 def set_pandas_timezone_from_spark(pandas_df):
     """Match pandas timezone to that of Spark, s.t. the timestamps are correctly displayed."""
     from tecton.tecton_context import TectonContext
 
     tz = TectonContext.get_instance()._spark.conf.get("spark.sql.session.timeZone")
     for col in pandas_df.columns:
@@ -126,14 +141,20 @@
     # TODO: Change the type to snowflake.snowpark.DataFrame, currently it will
     # fail type checking for our auto generated doc.
     _snowflake_df: Optional[Any] = None
     # should already by optimized
     _querytree: Optional[NodeRef] = attr.field(default=None, repr=lambda x: "TectonQueryTree")
     _temp_table_registered: Optional[set] = None
 
+    # _schema is the schema of TectonDataFrame. It is present only if the TectonDataFrame is built from a pandas
+    # dataframe as a spine and it is used when converting a pandas datafarme to a spark dataframe. Note the _schema only
+    # contains the name and data type for those columns Tecton manages. If the spine contains extra columns like `label`
+    # etc, those columns are converted to their default spark data types.
+    _schema: Optional[Schema] = None
+
     @sdk_public_method
     def explain(
         self,
         node_id: bool = True,
         name: bool = True,
         description: bool = True,
         columns: bool = False,
@@ -144,17 +165,19 @@
             node_id: If True, the unique id associated with each node will be rendered.
             name: If True, the class names of the nodes will be rendered.
             description: If True, the actions of the nodes will be rendered.
             columns: If True, the columns of each node will be rendered as an appendix after tree itself.
         """
         if self._querytree:
             if not name and not description:
-                raise RuntimeError("At least one of 'name' or 'description' must be True.")
+                msg = "At least one of 'name' or 'description' must be True."
+                raise RuntimeError(msg)
             if columns and not node_id:
-                raise RuntimeError("Can only show columns if 'node_id' is True.")
+                msg = "Can only show columns if 'node_id' is True."
+                raise RuntimeError(msg)
             print(self._querytree.pretty_str(node_id=node_id, name=name, description=description, columns=columns))
         else:
             print("Explain is only available for TectonDataFrames backed by a query tree.")
 
     @sdk_public_method
     def to_spark(self) -> pyspark.sql.DataFrame:
         """Returns data as a Spark DataFrame.
@@ -166,15 +189,37 @@
         else:
             from tecton.tecton_context import TectonContext
 
             tc = TectonContext.get_instance()
             if self._querytree is not None:
                 return translate.spark_convert(self._querytree).to_dataframe(tc._spark)
             elif self._pandas_df is not None:
-                return tc._spark.createDataFrame(self._pandas_df)
+                if self._schema is not None:
+                    extra_columns = list(set(self._pandas_df.columns) - set(self._schema.column_names()))
+                    if len(extra_columns) == 0:
+                        pdf = self._pandas_df[self._schema.column_names()]
+                        return tc._spark.createDataFrame(pdf, schema=schema_spark_utils.schema_to_spark(self._schema))
+                    else:
+                        # If there are extra columns beyond Tecton's management sopce, it splits the spine into two
+                        # parts:
+                        #   1. sub_df_1, which contains Tecton managed columns, is built with explicit schema.
+                        #   2. sub_df_2, which contains those extra columns, is built with spark default schema(no
+                        #      explicit schema passed in.
+                        # Eventually these two parts are joined together using an internal index column, which is
+                        # dropped afterwards. Note the join operation isn't expensive here given it is backed by a
+                        # pandas dataframe that is already loaded into memory.
+                        pdf = self._pandas_df.rename_axis(_internal_index_column).reset_index()
+                        pdf_schema = self._schema + Schema.from_dict({_internal_index_column: data_types.Int64Type()})
+                        sub_df_1 = tc._spark.createDataFrame(
+                            pdf[pdf_schema.column_names()], schema=schema_spark_utils.schema_to_spark(pdf_schema)
+                        )
+                        sub_df_2 = tc._spark.createDataFrame(pdf[[_internal_index_column] + extra_columns])
+                        return sub_df_1.join(sub_df_2, on=_internal_index_column).drop(_internal_index_column)
+                else:
+                    return tc._spark.createDataFrame(self._pandas_df)
             else:
                 raise NotImplementedError
 
     @sdk_public_method
     def to_pandas(self) -> pandas.DataFrame:
         """Returns data as a Pandas DataFrame.
 
@@ -189,63 +234,79 @@
             return set_pandas_timezone_from_spark(self._spark_df.toPandas())
 
         if self._snowflake_df is not None:
             return self._snowflake_df.to_pandas(statement_params={"SF_PARTNER": "tecton-ai"})
 
         if self._querytree is not None:
             if conf.get_or_none("SQL_DIALECT") == "athena":
+                self._register_tables()
                 session = athena_session.get_session()
-                sql = self._to_sql(create_temp_views=True)
+                recurse_query_tree(
+                    self._querytree,
+                    lambda node: register_feature_view_as_athena_table_if_necessary(
+                        node.feature_definition_wrapper, session
+                    )
+                    if isinstance(node, OfflineStoreScanNode)
+                    else None,
+                )
                 views = self._get_querytree_views()
                 for view_name, view_sql in views:
                     session.create_view(view_sql, view_name)
                 try:
-                    ret = session.read_sql(sql)
+                    athena_sql_executor = AthenaSqlExecutor(session)
+                    df = athena_convert(self._querytree, athena_sql_executor).to_dataframe()
                 finally:
                     # A tree and its subtree can share the same temp tables. If to_pandas() is called
                     # concurrently, tables can be deleted when they are still being used by the other tree.
                     self._drop_temp_tables()
                     for view_name, _ in views:
                         session.delete_view_if_exists(view_name)
-                return ret
+                return df
             else:
                 return set_pandas_timezone_from_spark(self.to_spark().toPandas())
 
     def _to_sql(self, create_temp_views: bool = False):
         if self._querytree is not None:
-            self._temp_table_registered = self._temp_table_registered or set()
             # prevent registering the same spine multiple times.
             if create_temp_views:
-
-                def maybe_register_temp_table(node):
-                    if isinstance(node, UserSpecifiedDataNode):
-                        tmp_table_name = node.data._temp_table_name
-                        if tmp_table_name not in self._temp_table_registered:
-                            node.data._register_temp_table()
-                        self._temp_table_registered.add(tmp_table_name)
-
-                recurse_query_tree(
-                    self._querytree,
-                    maybe_register_temp_table,
-                )
-            # Register Athena table
-            if conf.get_or_none("SQL_DIALECT") == "athena":
-                session = athena_session.get_session()
-                recurse_query_tree(
-                    self._querytree,
-                    lambda node: register_feature_view_as_athena_table_if_necessary(
-                        node.feature_definition_wrapper, session
-                    )
-                    if isinstance(node, OfflineStoreScanNode)
-                    else None,
-                )
-            return self._querytree.to_sql()
+                self._register_tables()
+            if tree_contains(self._querytree, MultiOdfvPipelineNode):
+                # SQL is not available for ODFVs. Showing SQL only for the subtree below the ODFV pipeline
+                subtree = self.get_sql_node(self._querytree)
+                return subtree.to_sql()
+            else:
+                return self._querytree.to_sql()
         else:
             raise NotImplementedError
 
+    def get_sql_node(self, tree: NodeRef):
+        # Returns the first node from which SQL can be generated(subtree below ODFV pipeline)
+        can_be_pushed = (
+            MultiOdfvPipelineNode,
+            RenameColsNode,
+        )
+        if isinstance(tree.node, can_be_pushed):
+            return self.get_sql_node(tree.node.input_node)
+        return tree
+
+    def _register_tables(self):
+        self._temp_table_registered = self._temp_table_registered or set()
+
+        def maybe_register_temp_table(node):
+            if isinstance(node, UserSpecifiedDataNode):
+                tmp_table_name = node.data._temp_table_name
+                if tmp_table_name not in self._temp_table_registered:
+                    node.data._register_temp_table()
+                self._temp_table_registered.add(tmp_table_name)
+
+        recurse_query_tree(
+            self._querytree,
+            maybe_register_temp_table,
+        )
+
     def _get_querytree_views(self):
         qt_views = []
         recurse_query_tree(self._querytree, lambda node: qt_views.extend(node.get_sql_views()))
         return qt_views
 
     # TODO(TEC-11097): Deprecate this method after version 0.7.0
     @sdk_public_method
@@ -265,15 +326,16 @@
         assert self._pandas_df is not None
 
         from tecton.snowflake_context import SnowflakeContext
 
         if conf.get_bool("ALPHA_SNOWFLAKE_SNOWPARK_ENABLED"):
             return SnowflakeContext.get_instance().get_session().createDataFrame(self._pandas_df)
         else:
-            raise Exception("to_snowflake() is only available with Snowpark enabled")
+            msg = "to_snowflake() is only available with Snowpark enabled"
+            raise Exception(msg)
 
     @sdk_public_method
     def to_snowpark(self) -> snowpark_dataframe:
         """
         Returns data as a Snowpark DataFrame.
 
         :return: A Snowpark DataFrame.
@@ -284,15 +346,16 @@
         assert self._pandas_df is not None
 
         from tecton.snowflake_context import SnowflakeContext
 
         if conf.get_bool("ALPHA_SNOWFLAKE_SNOWPARK_ENABLED"):
             return SnowflakeContext.get_instance().get_session().createDataFrame(self._pandas_df)
         else:
-            raise Exception("to_snowpark() is only available with Snowpark enabled")
+            msg = "to_snowpark() is only available with Snowpark enabled"
+            raise Exception(msg)
 
     def get_time_range(self, timestamp_key):
         if conf.get_or_none("SQL_DIALECT") == "snowflake":
             raise NotImplementedError
         if conf.get_or_none("SQL_DIALECT") == "athena":
             return get_time_limits_of_pandas_dataframe(self.to_pandas(), timestamp_key)
         else:
@@ -306,42 +369,45 @@
     ):
         """Creates a Tecton DataFrame from a Spark or Pandas DataFrame."""
         if isinstance(df, pandas.DataFrame):
             return cls(spark_df=None, pandas_df=df, snowflake_df=None)
         elif isinstance(df, pyspark.sql.DataFrame):
             return cls(spark_df=df, pandas_df=None, snowflake_df=None)
         elif isinstance(df, NodeRef):
-            # TODO: Refactor to not have branching here.
             if rewrite:
-                if conf.get_or_none("SQL_DIALECT") == "athena":
-                    rewrite_tree_for_spine(df, None)
-                else:
-                    from tecton.tecton_context import TectonContext
-
-                    spark = TectonContext.get_instance()._spark
-                    rewrite_tree_for_spine(df, spark)
+                rewrite_tree_for_spine(df)
             return cls(spark_df=None, pandas_df=None, snowflake_df=None, querytree=df)
 
-        raise TypeError(f"DataFrame must be of type pandas.DataFrame or pyspark.sql.Dataframe, not {type(df)}")
+        msg = f"DataFrame must be of type pandas.DataFrame or pyspark.sql.Dataframe, not {type(df)}"
+        raise TypeError(msg)
+
+    @classmethod
+    def _create_from_pandas_with_schema(cls, df: pandas.DataFrame, schema: Schema):
+        if isinstance(df, pandas.DataFrame):
+            return cls(spark_df=None, pandas_df=df, snowflake_df=None, schema=schema)
+        msg = f"DataFrame must be pandas.DataFrame when using _create_from_pandas, not {type(df)}"
+        raise TypeError(msg)
 
     @classmethod
     # This should be merged into _create once snowpark is installed with pip
     def _create_with_snowflake(cls, df: "snowflake.snowpark.DataFrame"):
         """Creates a Tecton DataFrame from a Snowflake DataFrame."""
         from snowflake.snowpark import DataFrame as SnowflakeDataFrame
 
         if isinstance(df, SnowflakeDataFrame):
             return cls(spark_df=None, pandas_df=None, snowflake_df=df)
 
-        raise TypeError(f"DataFrame must be of type snowflake.snowpark.Dataframe, not {type(df)}")
+        msg = f"DataFrame must be of type snowflake.snowpark.Dataframe, not {type(df)}"
+        raise TypeError(msg)
 
     def subtree(self, node_id: int) -> "TectonDataFrame":
         """Creates a TectonDataFrame from a subtree of prior querytree labeled by a node id in .explain()."""
         if not self._querytree:
-            raise RuntimeError("Cannot construct a TectonDataFrame from a node id.")
+            msg = "Cannot construct a TectonDataFrame from a node id."
+            raise RuntimeError(msg)
 
         tree = self._querytree.create_tree()
 
         # Do not apply rewrites again as they should have already been applied when generating the query tree for this
         # TectonDataFrame.
         return TectonDataFrame._create(NodeRef(tree.get_node(node_id).data), rewrite=False)
```

### Comparing `tecton-0.7.0b9/tecton/framework/data_source.py` & `tecton-0.7.0rc0/tecton/framework/data_source.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,67 +1,68 @@
 from __future__ import annotations
 
 import datetime
+import logging
 from typing import Dict
 from typing import List
 from typing import Optional
 from typing import Union
 
 import attrs
 from pyspark.sql import dataframe as pyspark_dataframe
 from pyspark.sql import streaming as pyspark_streaming
 from typeguard import typechecked
 
 from tecton import types
 from tecton._internals import display
 from tecton._internals import errors
-from tecton._internals import fco as internal_fco
 from tecton._internals import metadata_service
 from tecton._internals import sdk_decorators
 from tecton._internals import snowflake_api
 from tecton._internals import spark_api
 from tecton._internals import type_utils
 from tecton._internals import validations_api
 from tecton.framework import base_tecton_object
 from tecton.framework import compute_mode
 from tecton.framework import configs
 from tecton.framework import data_frame
 from tecton_core import id_helper
-from tecton_core import logger as logger_lib
 from tecton_core import specs
 from tecton_proto.args import basic_info_pb2
 from tecton_proto.args import fco_args_pb2
 from tecton_proto.args import virtual_data_source_pb2 as virtual_data_source__args_pb2
 from tecton_proto.common import data_source_type_pb2
 from tecton_proto.common import fco_locator_pb2
 from tecton_proto.common import framework_version_pb2
 from tecton_proto.common import schema_container_pb2
 from tecton_proto.metadataservice import metadata_service_pb2
 from tecton_proto.validation import validator_pb2
 from tecton_spark import spark_schema_wrapper
 
+
 BatchConfigType = Union[
     configs.FileConfig,
     configs.HiveConfig,
     configs.RedshiftConfig,
     configs.SnowflakeConfig,
     configs.SparkBatchConfig,
+    configs.UnityConfig,
 ]
 
 StreamConfigType = Union[
     configs.KinesisConfig,
     configs.KafkaConfig,
     configs.SparkStreamConfig,
 ]
 
-logger = logger_lib.get_logger("DataSource")
+logger = logging.getLogger(__name__)
 
 
 @attrs.define(eq=False)
-class DataSource(base_tecton_object.BaseTectonObject, internal_fco.Fco):
+class DataSource(base_tecton_object.BaseTectonObject):
     """Base class for Data Source classes."""
 
     # A data source spec, i.e. a dataclass representation of the Tecton object that is used in most functional use
     # cases, e.g. constructing queries. Set only after the object has been validated. Remote objects, i.e. applied
     # objects fetched from the backend, are assumed valid.
     _spec: Optional[specs.DataSourceSpec] = attrs.field(repr=False)
 
@@ -86,30 +87,34 @@
                 virtual_data_source=validator_pb2.VirtualDataSourceValidationArgs(
                     args=self._args,
                     batch_schema=self._args_supplement.batch_schema,
                     stream_schema=self._args_supplement.stream_schema,
                 )
             )
         else:
-            return validator_pb2.FcoValidationArgs(virtual_data_source=self._spec.data_proto.validation_args)
+            return self._spec.validation_args
 
     @property
     def _is_valid(self) -> bool:
         return self._spec is not None
 
     def _validate(self, indentation_level: int = 0) -> None:
         if self._is_valid:
             return
 
         try:
             self._derive_schemas(indentation_level)
-        except Exception as e:
-            raise errors.TectonValidationError(
-                f"An error occured when attempting to run {self.__class__.__name__} '{self.name}' during validation. See the chained exception for details."
-            ) from e
+        except Exception:
+            # Use logger.exception() to print/log the validation error message followed by the exception trace and then
+            # re-raise. This approach is preferred to exception chaining because it's a better notebook UX - especially
+            # for EMR notebooks, where chained exceptions are not rendered.
+            logger.exception(
+                f"An error occured when attempting to run {self.__class__.__name__} '{self.name}' during validation."
+            )
+            raise
 
         validations_api.run_backend_validation_and_assert_valid(
             self,
             validator_pb2.ValidationRequest(validation_args=[self._build_fco_validation_args()]),
             indentation_level,
         )
 
@@ -183,26 +188,29 @@
         if self._spec is not None:
             return self._spec.batch_source.data_delay if self._spec.batch_source is not None else None
 
         # This args data delay utility is needed so that feature view args can be constructed from unvalidated
         # data sources objects.
         if self._args.HasField("hive_ds_config"):
             return self._args.hive_ds_config.common_args.data_delay.ToTimedelta()
+        elif self._args.HasField("unity_ds_config"):
+            return self._args.unity_ds_config.common_args.data_delay.ToTimedelta()
         elif self._args.HasField("spark_batch_config"):
             return self._args.spark_batch_config.data_delay.ToTimedelta()
         elif self._args.HasField("redshift_ds_config"):
             return self._args.redshift_ds_config.common_args.data_delay.ToTimedelta()
         elif self._args.HasField("snowflake_ds_config"):
             return self._args.snowflake_ds_config.common_args.data_delay.ToTimedelta()
         elif self._args.HasField("file_ds_config"):
             return self._args.file_ds_config.common_args.data_delay.ToTimedelta()
         elif self._args.type == data_source_type_pb2.DataSourceType.PUSH_NO_BATCH:
             return None
         else:
-            raise ValueError(f"Invalid batch source args: {self._args}")
+            msg = f"Invalid batch source args: {self._args}"
+            raise ValueError(msg)
 
     @property
     @sdk_decorators.sdk_public_method
     # TODO(deprecate_after=0.6): Deprecate this method.
     def is_streaming(self) -> bool:
         """Deprecated. Use `isinstance(fv, StreamSource)` instead."""
         logger.warn("is_streaming is deprecated and will be removed in 0.7. Use isinstance(fv, StreamSource) instead.")
@@ -248,18 +256,18 @@
 
         :param name: A unique name of the DataSource.
         :param description: A human-readable description.
         :param tags: Tags associated with this Tecton Data Source (key-value pairs of arbitrary metadata).
         :param owner: Owner name (typically the email of the primary maintainer).
         :param prevent_destroy: If True, this Tecton object will be blocked from being deleted or re-created (i.e. a
             destructive update) during tecton plan/apply. To remove or update this object, `prevent_destroy` must be
-            first set to False via a separate tecton apply. `prevent_destroy` can be used to prevent accidental changes
-            such as inadvertantly deleting a Feature Service used in production or recreating a Feature View that
-            triggers expensive rematerialization jobs. `prevent_destroy` also blocks changes to dependent Tecton objects
-            that would trigger a recreate of the tagged object, e.g. if `prevent_destroy` is set on a Feature Service,
+            set to False via the same tecton apply or a separate tecton apply. `prevent_destroy` can be used to prevent
+            accidental changes such as inadvertantly deleting a Feature Service used in production or recreating a Feature
+            View that triggers expensive rematerialization jobs. `prevent_destroy` also blocks changes to dependent Tecton
+            objects that would trigger a recreate of the tagged object, e.g. if `prevent_destroy` is set on a Feature Service,
             that will also prevent deletions or re-creates of Feature Views used in that service. `prevent_destroy` is
             only enforced in live (i.e. non-dev) workspaces.
         :param batch_config: BatchConfig object containing the configuration of the Batch Data Source to be included
             in this Data Source.
         """
         from tecton.cli import common as cli_common
 
@@ -279,15 +287,15 @@
             info=info,
             spec=None,
             args=ds_args,
             source_info=source_info,
             args_supplement=_build_args_supplement(batch_config, None),
         )
 
-        internal_fco.Fco._register(self)
+        base_tecton_object._register_local_object(self)
 
     @classmethod
     @typechecked
     def _from_spec(cls, spec: specs.DataSourceSpec) -> "BatchSource":
         """Create a BatchSource from directly from a spec. Specs are assumed valid and will not be re-validated."""
         info = base_tecton_object.TectonObjectInfo.from_spec(spec)
         obj = cls.__new__(cls)  # Instantiate the object. Does not call init.
@@ -389,15 +397,15 @@
 
         :param name: A unique name of the DataSource.
         :param description: A human-readable description.
         :param tags: Tags associated with this Tecton Object (key-value pairs of arbitrary metadata).
         :param owner: Owner name (typically the email of the primary maintainer).
         :param prevent_destroy: If True, this Tecton object will be blocked from being deleted or re-created (i.e. a
             destructive update) during tecton plan/apply. To remove or update this object, `prevent_destroy` must be
-            first set to False via a separate tecton apply. `prevent_destroy` can be used to prevent accidental changes
+            set to False via the same tecton apply or a separate tecton apply. `prevent_destroy` can be used to prevent accidental changes
             such as inadvertantly deleting a Feature Service used in production or recreating a Feature View that
             triggers expensive rematerialization jobs. `prevent_destroy` also blocks changes to dependent Tecton objects
             that would trigger a recreate of the tagged object, e.g. if `prevent_destroy` is set on a Feature Service,
             that will also prevent deletions or re-creates of Feature Views used in that service. `prevent_destroy` is
             only enforced in live (i.e. non-dev) workspaces.
         :param batch_config: BatchConfig object containing the configuration of the Batch Data Source that backs this
             Tecton Stream Source.
@@ -422,15 +430,15 @@
             info=info,
             spec=None,
             args=ds_args,
             source_info=source_info,
             args_supplement=_build_args_supplement(batch_config, stream_config),
         )
 
-        internal_fco.Fco._register(self)
+        base_tecton_object._register_local_object(self)
 
     @classmethod
     @typechecked
     def _from_spec(cls, spec: specs.DataSourceSpec) -> "StreamSource":
         """Create a StreamSource from directly from a spec. Specs are assumed valid and will not be re-validated."""
         info = base_tecton_object.TectonObjectInfo.from_spec(spec)
         obj = cls.__new__(cls)  # Instantiate the object. Does not call init.
@@ -537,15 +545,15 @@
         :param name: A unique name of the DataSource.
         :param schema: A schema for the PushSource
         :param description: A human-readable description.
         :param tags: Tags associated with this Tecton Object (key-value pairs of arbitrary metadata).
         :param owner: Owner name (typically the email of the primary maintainer).
         :param prevent_destroy: If True, this Tecton object will be blocked from being deleted or re-created (i.e. a
             destructive update) during tecton plan/apply. To remove or update this object, `prevent_destroy` must be
-            first set to False via a separate tecton apply. `prevent_destroy` can be used to prevent accidental changes
+            set to False via the same tecton apply or a separate tecton apply. `prevent_destroy` can be used to prevent accidental changes
             such as inadvertantly deleting a Feature Service used in production or recreating a Feature View that
             triggers expensive rematerialization jobs. `prevent_destroy` also blocks changes to dependent Tecton objects
             that would trigger a recreate of the tagged object, e.g. if `prevent_destroy` is set on a Feature Service,
             that will also prevent deletions or re-creates of Feature Views used in that service. `prevent_destroy` is
             only enforced in live (i.e. non-dev) workspaces.
         :param batch_config: An optional BatchConfig object containing the configuration of the Batch Data Source that backs
             this Tecton Push Source. The Batch Source's schema must contain a super-set of all the columns defined in the Push Source schema.
@@ -575,15 +583,15 @@
             info=info,
             spec=None,
             args=ds_args,
             source_info=source_info,
             args_supplement=_build_args_supplement(batch_config, None),
         )
 
-        internal_fco.Fco._register(self)
+        base_tecton_object._register_local_object(self)
 
     @classmethod
     @typechecked
     def _from_spec(cls, spec: specs.DataSourceSpec) -> "PushSource":
         """Create a PushSource from directly from a spec. Specs are assumed valid and will not be re-validated."""
         info = base_tecton_object.TectonObjectInfo.from_spec(spec)
         obj = cls.__new__(cls)  # Instantiate the object. Does not call init.
@@ -624,37 +632,41 @@
         return BatchSource._from_spec(data_source_spec)
     elif data_source_spec.type in (
         data_source_type_pb2.DataSourceType.PUSH_WITH_BATCH,
         data_source_type_pb2.DataSourceType.PUSH_NO_BATCH,
     ):
         return PushSource._from_spec(data_source_spec)
     else:
-        raise ValueError(f"Unexpected Data Source Type. Spec: {data_source_spec}")
+        msg = f"Unexpected Data Source Type. Spec: {data_source_spec}"
+        raise ValueError(msg)
 
 
 def _build_args_supplement(
     batch_config: Optional[BatchConfigType], stream_config: Optional[StreamConfigType]
 ) -> specs.DataSourceSpecArgsSupplement:
     supplement = specs.DataSourceSpecArgsSupplement()
     if isinstance(
         batch_config,
         (
             configs.FileConfig,
             configs.HiveConfig,
             configs.RedshiftConfig,
             configs.SnowflakeConfig,
+            configs.UnityConfig,
         ),
     ):
         supplement.batch_post_processor = batch_config.post_processor
     elif isinstance(batch_config, configs.SparkBatchConfig):
         supplement.batch_data_source_function = batch_config.data_source_function
     elif batch_config is not None:
-        raise TypeError(f"Unexpected batch_config type: {batch_config}")
+        msg = f"Unexpected batch_config type: {batch_config}"
+        raise TypeError(msg)
 
     if isinstance(stream_config, (configs.KinesisConfig, configs.KafkaConfig)):
         supplement.stream_post_processor = stream_config.post_processor
     elif isinstance(stream_config, configs.SparkStreamConfig):
         supplement.stream_data_source_function = stream_config.data_source_function
     elif stream_config is not None:
-        raise TypeError(f"Unexpected stream_config type: {stream_config}")
+        msg = f"Unexpected stream_config type: {stream_config}"
+        raise TypeError(msg)
 
     return supplement
```

### Comparing `tecton-0.7.0b9/tecton/framework/dataset.py` & `tecton-0.7.0rc0/tecton/framework/dataset.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 import base64
 import json
+import logging
 from typing import Optional
 from typing import Union
 
 import pandas
 import pyspark
 from google.protobuf.json_format import MessageToJson
 from pyspark.sql.types import StructType
@@ -11,22 +12,22 @@
 from tecton._internals import errors
 from tecton._internals import metadata_service
 from tecton._internals.display import Displayable
 from tecton._internals.sdk_decorators import sdk_public_method
 from tecton.framework.data_frame import TectonDataFrame
 from tecton.tecton_context import TectonContext
 from tecton_core.id_helper import IdHelper
-from tecton_core.logger import get_logger
 from tecton_proto.data.saved_feature_data_frame_pb2 import SavedFeatureDataFrame
 from tecton_proto.data.saved_feature_data_frame_pb2 import SavedFeatureDataFrameType
 from tecton_proto.metadataservice.metadata_service_pb2 import ArchiveSavedFeatureDataFrameRequest
 from tecton_proto.metadataservice.metadata_service_pb2 import CreateSavedFeatureDataFrameRequest
 from tecton_spark.spark_schema_wrapper import SparkSchemaWrapper
 
-logger = get_logger("FeatureService")
+
+logger = logging.getLogger(__name__)
 
 
 class Dataset(TectonDataFrame):
     """
     Dataset class.
 
     Persisted data consisting of entity & request keys, timestamps, and calculated features. Datasets are
@@ -231,17 +232,16 @@
             f"{type(self).__name__}(name='{self.name}', "
             + f"{source_str}, created_at='{self._proto.info.created_at.ToJsonString()}')"
         )
 
 
 @sdk_public_method
 def get_dataset(name: str, workspace_name: Optional[str] = None):
-    raise errors.TectonValidationError(
-        'get_dataset must be called from a Workspace object. E.g. tecton.get_workspace("<workspace>").get_dataset("<dataset>").'
-    )
+    msg = 'get_dataset must be called from a Workspace object. E.g. tecton.get_workspace("<workspace>").get_dataset("<dataset>").'
+    raise errors.TectonValidationError(msg)
 
 
 def _convert_logged_df_schema(spark_df):
     if spark_df is None:
         return
     # Note: _partition column is not used right now, but in future
     # it can be used to optimize time-range access of this dataframe
```

### Comparing `tecton-0.7.0b9/tecton/framework/entity.py` & `tecton-0.7.0rc0/tecton/framework/entity.py`

 * *Files 6% similar despite different names*

```diff
@@ -3,15 +3,14 @@
 from typing import Optional
 from typing import Union
 
 import attrs
 from typeguard import typechecked
 
 from tecton._internals import display
-from tecton._internals import fco as internal_fco
 from tecton._internals import metadata_service
 from tecton._internals import sdk_decorators
 from tecton._internals import validations_api
 from tecton.framework import base_tecton_object
 from tecton_core import feature_definition_wrapper
 from tecton_core import id_helper
 from tecton_core import specs
@@ -20,15 +19,15 @@
 from tecton_proto.args import fco_args_pb2
 from tecton_proto.common import fco_locator_pb2
 from tecton_proto.metadataservice import metadata_service_pb2
 from tecton_proto.validation import validator_pb2
 
 
 @attrs.define(eq=False)
-class Entity(base_tecton_object.BaseTectonObject, internal_fco.Fco):
+class Entity(base_tecton_object.BaseTectonObject):
     """A Tecton Entity, used to organize and join features.
 
     An Entity is a class that represents an Entity that is being modeled in Tecton. Entities are used to index and
     organize features - a :class:`FeatureView` contains at least one Entity.
 
     Entities contain metadata about *join keys*, which represent the columns that are used to join features together.
 
@@ -70,15 +69,15 @@
 
         :param name: Unique name for the new entity.
         :param description: Short description of the new entity.
         :param tags: Tags associated with this Tecton Object (key-value pairs of arbitrary metadata).
         :param owner: Owner name (typically the email of the primary maintainer).
         :param prevent_destroy: If True, this Tecton object will be blocked from being deleted or re-created (i.e. a
             destructive update) during tecton plan/apply. To remove or update this object, `prevent_destroy` must be
-            first set to False via a separate tecton apply. `prevent_destroy` can be used to prevent accidental changes
+            set to False via the same tecton apply or a separate tecton apply. `prevent_destroy` can be used to prevent accidental changes
             such as inadvertantly deleting a Feature Service used in production or recreating a Feature View that
             triggers expensive rematerialization jobs. `prevent_destroy` also blocks changes to dependent Tecton objects
             that would trigger a recreate of the tagged object, e.g. if `prevent_destroy` is set on a Feature Service,
             that will also prevent deletions or re-creates of Feature Views used in that service. `prevent_destroy` is
             only enforced in live (i.e. non-dev) workspaces.
         :param join_keys: Names of columns that uniquely identify the entity in FeatureView's SQL statement
             for which features should be aggregated. Defaults to using ``name`` as the entity's join key.
@@ -100,15 +99,15 @@
             join_keys=resolved_join_keys,
             version=feature_definition_wrapper.FrameworkVersion.FWV5.value,
             prevent_destroy=prevent_destroy,
         )
         info = base_tecton_object.TectonObjectInfo.from_args_proto(args.info, args.entity_id)
         source_info = cli_common.construct_fco_source_info(args.entity_id)
         self.__attrs_init__(info=info, spec=None, args=args, source_info=source_info)
-        internal_fco.Fco._register(self)
+        base_tecton_object._register_local_object(self)
 
     @classmethod
     @typechecked
     def _from_spec(cls, spec: specs.EntitySpec) -> "Entity":
         """Create an Entity from directly from a spec. Specs are assumed valid and will not be re-validated."""
         info = base_tecton_object.TectonObjectInfo.from_spec(spec)
         obj = cls.__new__(cls)  # Instantiate the object. Does not call init.
@@ -123,15 +122,15 @@
         if self.info._is_local_object:
             return validator_pb2.FcoValidationArgs(
                 entity=validator_pb2.EntityValidationArgs(
                     args=self._args,
                 )
             )
         else:
-            return validator_pb2.FcoValidationArgs(entity=self._spec.data_proto.validation_args)
+            return self._spec.validation_args
 
     @property
     def _is_valid(self) -> bool:
         return self._spec is not None
 
     @property
     def join_keys(self) -> List[str]:
```

### Comparing `tecton-0.7.0b9/tecton/framework/feature_service.py` & `tecton-0.7.0rc0/tecton/framework/feature_service.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 import json
+import logging
 import time
 import urllib
 from typing import Dict
 from typing import List
 from typing import Mapping
 from typing import Optional
 from typing import Set
@@ -14,51 +15,50 @@
 import pandas
 import pendulum
 import requests
 from google.protobuf import json_format
 from pyspark.sql import dataframe as pyspark_dataframe
 from typeguard import typechecked
 
-from tecton import conf
 from tecton._internals import athena_api
 from tecton._internals import display
 from tecton._internals import errors
-from tecton._internals import fco as internal_fco
 from tecton._internals import metadata_service
 from tecton._internals import query_helper
 from tecton._internals import sdk_decorators
 from tecton._internals import snowflake_api
 from tecton._internals import spark_api
 from tecton._internals import utils as internal_utils
 from tecton._internals import validations_api
 from tecton.framework import base_tecton_object
 from tecton.framework import compute_mode
 from tecton.framework import configs
 from tecton.framework import data_frame as tecton_dataframe
 from tecton.framework import feature_view as framework_feature_view
+from tecton_core import conf
 from tecton_core import fco_container
 from tecton_core import feature_definition_wrapper
 from tecton_core import feature_set_config
 from tecton_core import id_helper
-from tecton_core import logger as logger_lib
 from tecton_core import query_consts
+from tecton_core import request_context
 from tecton_core import specs
 from tecton_proto.api.featureservice import feature_service_pb2 as feature_service__api_pb2
 from tecton_proto.args import basic_info_pb2
 from tecton_proto.args import fco_args_pb2
 from tecton_proto.args import feature_service_pb2 as feature_service__args_pb2
 from tecton_proto.metadataservice import metadata_service_pb2
 from tecton_proto.validation import validator_pb2
-from tecton_spark import request_context
 
-logger = logger_lib.get_logger("FeatureService")
+
+logger = logging.getLogger(__name__)
 
 
 @attrs.define(eq=False)
-class FeatureService(base_tecton_object.BaseTectonObject, internal_fco.Fco):
+class FeatureService(base_tecton_object.BaseTectonObject):
     """A Tecton Feature Service.
 
     In Tecton, a Feature Service exposes an API for accessing a set of FeatureViews.
 
     Once deployed in production, each model has one associated Feature Service that serves the model its features. A
     Feature Service contains a list of the Feature Views associated with a model. It also includes user-provided
     metadata such as name, description, and owner that Tecton uses to organize feature data.
@@ -122,71 +122,73 @@
         description: Optional[str] = None,
         tags: Optional[Dict[str, str]] = None,
         owner: Optional[str] = None,
         prevent_destroy: bool = False,
         online_serving_enabled: bool = True,
         features: List[Union[framework_feature_view.FeatureReference, framework_feature_view.FeatureView]] = None,
         logging: Optional[configs.LoggingConfig] = None,
+        on_demand_environment: Optional[str] = None,
     ):
         """
         Instantiates a new FeatureService.
 
         :param name: A unique name for the Feature Service.
         :param description: A human-readable description.
         :param tags: Tags associated with this Tecton Object (key-value pairs of arbitrary metadata).
         :param owner: Owner name (typically the email of the primary maintainer).
         :param prevent_destroy: If True, this Tecton object will be blocked from being deleted or re-created (i.e. a
-            destructive update) during tecton plan/apply. To remove or update this object, `prevent_destroy` must be
-            first set to False via a separate tecton apply. `prevent_destroy` can be used to prevent accidental changes
+            destructive update) during tecton plan/apply. To remove or update this object, `prevent_destroy` must be set to False
+            via the same tecton apply or a separate tecton apply. `prevent_destroy` can be used to prevent accidental changes
             such as inadvertantly deleting a Feature Service used in production or recreating a Feature View that
             triggers expensive rematerialization jobs. `prevent_destroy` also blocks changes to dependent Tecton objects
             that would trigger a recreate of the tagged object, e.g. if `prevent_destroy` is set on a Feature Service,
             that will also prevent deletions or re-creates of Feature Views used in that service. `prevent_destroy` is
             only enforced in live (i.e. non-dev) workspaces.
         :param online_serving_enabled: (Optional, default True) If True, users can send realtime requests
             to this FeatureService, and only FeatureViews with online materialization enabled can be added
             to this FeatureService.
         :param features: The list of FeatureView or FeatureReference that this FeatureService will serve.
         :param logging: A configuration for logging feature requests sent to this Feature Service.
+        :param on_demand_environment: Configuration for where on demand features should be computed online.
         """
         from tecton.cli import common as cli_common
 
         feature_references = []
         for feature in features:
             if isinstance(feature, framework_feature_view.FeatureReference):
                 feature_references.append(feature)
             elif isinstance(feature, framework_feature_view.FeatureView):
                 feature_references.append(framework_feature_view.FeatureReference(feature_definition=feature))
             else:
-                raise TypeError(
-                    f"Object in FeatureService.features with an invalid type: {type(feature)}. Should be of type FeatureReference or Feature View."
-                )
+                msg = f"Object in FeatureService.features with an invalid type: {type(feature)}. Should be of type FeatureReference or Feature View."
+                raise TypeError(msg)
 
         feature_packages = [_feature_reference_to_feature_service_feature_package(ref) for ref in feature_references]
 
         args = feature_service__args_pb2.FeatureServiceArgs(
             feature_service_id=id_helper.IdHelper.generate_id(),
             info=basic_info_pb2.BasicInfo(name=name, description=description, tags=tags, owner=owner),
             prevent_destroy=prevent_destroy,
             online_serving_enabled=online_serving_enabled,
             feature_packages=feature_packages,
             version=feature_definition_wrapper.FrameworkVersion.FWV5.value,
-            logging=logging._to_proto() if logging is not None else None,
+            logging=logging._to_proto() if logging else None,
+            on_demand_environment=on_demand_environment,
         )
         info = base_tecton_object.TectonObjectInfo.from_args_proto(args.info, args.feature_service_id)
         source_info = cli_common.construct_fco_source_info(args.feature_service_id)
         self.__attrs_init__(
             info=info,
             spec=None,
             args=args,
             source_info=source_info,
             feature_references=feature_references,
             feature_set_config=None,
         )
-        internal_fco.Fco._register(self)
+        base_tecton_object._register_local_object(self)
 
     @classmethod
     @typechecked
     def _from_spec(cls, spec: specs.FeatureServiceSpec, fco_container: fco_container.FcoContainer) -> "FeatureService":
         """Create a Feature Service from directly from a spec. Specs are assumed valid and will not be re-validated."""
         feature_set_config_ = feature_set_config.FeatureSetConfig.from_feature_service_spec(spec, fco_container)
         info = base_tecton_object.TectonObjectInfo.from_spec(spec)
@@ -226,17 +228,15 @@
         if self.info._is_local_object:
             return validator_pb2.FcoValidationArgs(
                 feature_service=validator_pb2.FeatureServiceValidationArgs(
                     args=self._args,
                 )
             )
         else:
-            return validator_pb2.FcoValidationArgs(
-                feature_service=self._spec.data_proto.feature_service.validation_args
-            )
+            return self._spec.validation_args
 
     def _get_dependent_objects(self, include_indirect_dependencies: bool) -> List[base_tecton_object.BaseTectonObject]:
         dependent_objects = []
         for fv in self._feature_definitions:
             if include_indirect_dependencies:
                 dependent_objects.extend(fv._get_dependent_objects(include_indirect_dependencies=True) + [fv])
             else:
@@ -313,27 +313,41 @@
             curl_str = "curl -X POST " + service_url + '\\\n-H "' + curl_header + "\"-d\\\n'" + curl_params_json + "'"
             summary_items.append(("Example cURL", curl_str))
         return display.Displayable.from_properties(items=summary_items)
 
     @property
     @sdk_decorators.sdk_public_method
     def features(self) -> List[framework_feature_view.FeatureReference]:
-        """Returns the list of feature references included in this feature service.
+        """Returns the list of feature references included in this Feature Service.
 
         FeatureReferences are references to Feature Views/Tables that may select a subset of features, override the
         Feature View/Table namespace, or re-map join-keys.
         """
         return list(self._feature_references)
 
+    @property
+    @sdk_decorators.sdk_public_method
+    def feature_views(self) -> Set[framework_feature_view.FeatureView]:
+        """Returns the set of Feature Views directly depended on by this Feature Service.
+
+        A single Feature View may be included multiple times in a Feature Service under different namespaces. See the
+        FeatureReference documentation. This method dedupes those Feature Views.
+        """
+
+        # Dedupe by ids.
+        # TODO(jake): Tecton objects should probably be hashable based on their id.
+        return set({ref.feature_definition.id: ref.feature_definition for ref in self._feature_references}.values())
+
     @sdk_decorators.sdk_public_method(requires_validation=True)
     def get_feature_columns(self) -> List[str]:
         """Returns the list of all feature columns included in this feature service."""
         all_features = self._feature_set_config.features
         return [feature for feature in all_features if not feature.startswith(query_consts.UDF_INTERNAL)]
 
+    # TODO(follow-up PR): Clean up this method. (Holding off to minimize backport.)
     @property
     def _feature_definitions(self) -> Set[framework_feature_view.FeatureView]:
         """Returns the set of unique Feature Definitions directly depended on by this Feature Service.
 
         A single Feature Definition may be included multiple times in a Feature Service under different namespaces.
         This method dedupes those.
         """
@@ -446,17 +460,16 @@
                 from_source=from_source,
                 save=save,
                 save_as=save_as,
                 feature_set_config=self._feature_set_config,
             )
 
         if isinstance(spine, str):
-            raise TypeError(
-                "When using spark compute, `spine` must be one of (pyspark.sql.dataframe.DataFrame, pandas.core.frame.DataFrame, tecton.framework.data_frame.TectonDataFrame); got str instead."
-            )
+            msg = "When using spark compute, `spine` must be one of (pyspark.sql.dataframe.DataFrame, pandas.core.frame.DataFrame, tecton.framework.data_frame.TectonDataFrame); got str instead."
+            raise TypeError(msg)
 
         return spark_api.get_historical_features_for_feature_service(
             feature_service_spec=self._spec,
             feature_set_config=self._feature_set_config,
             spine=spine,
             timestamp_key=timestamp_key,
             from_source=from_source,
@@ -508,20 +521,20 @@
         :return: A :class:`tecton.FeatureVector` of the results.
         """
         # Default to empty dicts.
         join_keys = join_keys or {}
         request_data = request_data or {}
 
         if not internal_utils.is_live_workspace(self.workspace):
-            raise errors.UNSUPPORTED_OPERATION_IN_DEVELOPMENT_WORKSPACE("get_online_features")
+            msg = "get_online_features"
+            raise errors.UNSUPPORTED_OPERATION_IN_DEVELOPMENT_WORKSPACE(msg)
 
         if not self._spec.online_serving_enabled:
-            raise errors.UNSUPPORTED_OPERATION(
-                "get_online_features", "online_serving_enabled was not defined for this Feature Service."
-            )
+            msg = "get_online_features"
+            raise errors.UNSUPPORTED_OPERATION(msg, "online_serving_enabled was not defined for this Feature Service.")
         if not join_keys and not request_data:
             raise errors.FS_GET_ONLINE_FEATURES_REQUIRED_ARGS
 
         fs_contains_non_odfvs = not all(fd.is_on_demand for fd in self._feature_set_config.feature_definitions)
         if fs_contains_non_odfvs and not join_keys:
             raise errors.GET_ONLINE_FEATURES_FS_JOIN_KEYS
 
@@ -549,20 +562,20 @@
         of the included FeatureViews. Returns a Tecton :class:`TectonDataFrame` of all matched records.
 
         :param join_keys: Query join keys, i.e., a union of join keys in the ``online_serving_index`` of all
             enclosed FeatureViews.
         :return: A Tecton :class:`TectonDataFrame`
         """
         if not internal_utils.is_live_workspace(self.workspace):
-            raise errors.UNSUPPORTED_OPERATION_IN_DEVELOPMENT_WORKSPACE("query_features")
+            msg = "query_features"
+            raise errors.UNSUPPORTED_OPERATION_IN_DEVELOPMENT_WORKSPACE(msg)
 
         if not self._spec.online_serving_enabled:
-            raise errors.UNSUPPORTED_OPERATION(
-                "query_features", "online_serving_enabled was not defined for this Feature Service."
-            )
+            msg = "query_features"
+            raise errors.UNSUPPORTED_OPERATION(msg, "online_serving_enabled was not defined for this Feature Service.")
 
         return query_helper._QueryHelper(self.workspace, feature_service_name=self.name).query_features(join_keys)
 
     @property
     def _request_context(self) -> request_context.RequestContext:
         merged_context = request_context.RequestContext({})
         for fv in self._feature_definitions:
@@ -579,15 +592,16 @@
         The FeatureService is considered ready once every FeatureView that has been added to it
         has had at least once successful materialization run.
 
         :param timeout: The timeout to wait. Defaults to 15 minutes.
         :param wait_for_materialization: If False, does not wait for batch materialization to complete.
         """
         if not internal_utils.is_live_workspace(self.workspace):
-            raise errors.UNSUPPORTED_OPERATION_IN_DEVELOPMENT_WORKSPACE("_wait_until_ready")
+            msg = "_wait_until_ready"
+            raise errors.UNSUPPORTED_OPERATION_IN_DEVELOPMENT_WORKSPACE(msg)
 
         if timeout is None:
             timeout = pendulum.Duration(minutes=15)
 
         deadline = pendulum.now() + timeout
 
         has_been_not_ready = False
@@ -604,21 +618,21 @@
             )
             details = http_response.json()
             if http_response.status_code == 404:
                 # FeatureService is not ready to serve
                 if verbose:
                     logger.info(f" Waiting for FeatureService to be ready to serve ({details['message']})")
                 else:
-                    logger.info(f" Waiting for FeatureService to be ready to serve")
+                    logger.info(" Waiting for FeatureService to be ready to serve")
             elif http_response.status_code == 200:
                 # FeatureService is ready
                 if verbose:
                     logger.info(f"wait_until_ready: Ready! Response={http_response.text}")
                 else:
-                    logger.info(f"wait_until_ready: Ready!")
+                    logger.info("wait_until_ready: Ready!")
                 # Extra wait time due to different FS hosts being potentially out-of-sync in picking up the latest state
                 if has_been_not_ready:
                     time.sleep(20)
                 return
             else:
                 http_response.raise_for_status()
                 return
```

### Comparing `tecton-0.7.0b9/tecton/framework/feature_view.py` & `tecton-0.7.0rc0/tecton/framework/feature_view.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 from __future__ import annotations
 
 import datetime
 import enum
 import inspect
+import logging
 from typing import Any
 from typing import Callable
 from typing import Dict
 from typing import List
 from typing import Mapping
 from typing import Optional
 from typing import Sequence
@@ -23,17 +24,17 @@
 
 from tecton import tecton_context
 from tecton import types
 from tecton._internals import athena_api
 from tecton._internals import delete_keys_api
 from tecton._internals import display
 from tecton._internals import errors
-from tecton._internals import fco as internal_fco
 from tecton._internals import materialization_api
 from tecton._internals import metadata_service
+from tecton._internals import mock_source_utils
 from tecton._internals import query_helper
 from tecton._internals import run_api
 from tecton._internals import sdk_decorators
 from tecton._internals import snowflake_api
 from tecton._internals import spark_api
 from tecton._internals import type_utils
 from tecton._internals import utils as internal_utils
@@ -43,53 +44,53 @@
 from tecton.framework import configs
 from tecton.framework import data_frame as tecton_dataframe
 from tecton.framework import data_source as framework_data_source
 from tecton.framework import entity as framework_entity
 from tecton.framework import filtered_source
 from tecton.framework import transformation as framework_transformation
 from tecton.framework import utils
-from tecton.framework.base_tecton_object import TectonObjectInfo  # nopycln: import - Needed for SDK docs generation.
+from tecton.framework.validation_mode import ValidationMode
 from tecton_core import aggregation_utils
+from tecton_core import conf
 from tecton_core import errors as core_errors
 from tecton_core import fco_container
 from tecton_core import feature_definition_wrapper
 from tecton_core import feature_set_config
 from tecton_core import id_helper
-from tecton_core import logger as logger_lib
 from tecton_core import materialization_context
 from tecton_core import pipeline_common
+from tecton_core import request_context
 from tecton_core import schema_derivation_utils
 from tecton_core import specs
 from tecton_core import time_utils
 from tecton_core.errors import TectonValidationError
 from tecton_proto.args import basic_info_pb2
 from tecton_proto.args import fco_args_pb2
 from tecton_proto.args import feature_service_pb2
 from tecton_proto.args import feature_view_pb2 as feature_view__args_pb2
 from tecton_proto.args import pipeline_pb2
-from tecton_proto.args import repo_metadata_pb2  # nopycln: import - Needed for SDK docs generation.
 from tecton_proto.common import data_source_type_pb2
 from tecton_proto.common import fco_locator_pb2
 from tecton_proto.common import id_pb2
 from tecton_proto.common.data_source_type_pb2 import DataSourceType
 from tecton_proto.data import materialization_status_pb2
 from tecton_proto.metadataservice import metadata_service_pb2
 from tecton_proto.validation import validator_pb2
 from tecton_spark import pipeline_helper
-from tecton_spark import request_context
 from tecton_spark import spark_schema_wrapper
 
+
 # FilteredSource start offsets smaller (more negative) than this offset will be considered UNBOUNDED_PRECEEDING.
 MIN_START_OFFSET = datetime.timedelta(days=-365 * 100)  # 100 years
 
 # This is the mode used when the feature view decorator is used on a pipeline function, i.e. one that only contains
 # references to transformations and constants.
 PIPELINE_MODE = "pipeline"
 
-logger = logger_lib.get_logger("FeatureView")
+logger = logging.getLogger(__name__)
 
 
 # Create a parallel enum class since Python proto extensions do not use an enum class.
 # Keep up-to-date with StreamProcessingMode from tecton_proto/args/feature_view.proto.
 class StreamProcessingMode(enum.Enum):
     TIME_INTERVAL = feature_view__args_pb2.StreamProcessingMode.STREAM_PROCESSING_MODE_TIME_INTERVAL
     CONTINUOUS = feature_view__args_pb2.StreamProcessingMode.STREAM_PROCESSING_MODE_CONTINUOUS
@@ -124,27 +125,43 @@
     def _print_from_source_message(self, count: int, single: str, message: str):
         if count > 0:
             feature_view_text = internal_utils.plural(count, f"{single} is", f"{single}s are")
             logger.info(f"{count} {feature_view_text} {message}")
 
     def display(self):
         self._print_from_source_message(
-            self.offline_store_count, "Feature View", "being computed from data in the offline store."
+            self.offline_store_count, "Feature View", "being read from data in the offline store."
         )
         self._print_from_source_message(
             self.from_source_count, "Feature View", "being computed directly from raw data sources."
         )
         self._print_from_source_message(
             self.feature_table_count, "Feature Table", "being loaded from the offline store."
         )
         self._print_from_source_message(self.on_demand_count, "On-Demand Feature View", "being computed ad hoc.")
 
 
+def _to_pyspark_mocks(
+    mock_inputs: Dict[str, Union[pyspark_dataframe.DataFrame, pandas.DataFrame]]
+) -> Dict[str, pyspark_dataframe.DataFrame]:
+    pyspark_mock_inputs = {}
+    for input_name, mock_data in mock_inputs.items():
+        if isinstance(mock_data, pyspark_dataframe.DataFrame):
+            pyspark_mock_inputs[input_name] = mock_data
+        elif isinstance(mock_data, pandas.DataFrame):
+            spark = tecton_context.TectonContext.get_instance()._spark
+            pyspark_mock_inputs[input_name] = spark.createDataFrame(mock_data)
+        else:
+            msg = f"Unexpected mock source type for kwarg {input_name}: {type(mock_data)}"
+            raise TypeError(msg)
+    return pyspark_mock_inputs
+
+
 @attrs.define(eq=False)
-class FeatureView(base_tecton_object.BaseTectonObject, internal_fco.Fco):
+class FeatureView(base_tecton_object.BaseTectonObject):
     """Base class for Feature View classes (including Feature Tables)."""
 
     # A FeatureDefinitionWrapper instance, which contains the Feature View spec for this Feature View and dependent
     # FCO specs (e.g. Data Source specs). Set only after the object has been validated. Remote objects, i.e. applied
     # objects fetched from the backend, are assumed valid.
     _feature_definition: Optional[feature_definition_wrapper.FeatureDefinitionWrapper] = attrs.field(repr=False)
 
@@ -170,15 +187,15 @@
                 feature_view=validator_pb2.FeatureViewValidationArgs(
                     args=self._args,
                     view_schema=self._args_supplement.view_schema,
                     materialization_schema=self._args_supplement.materialization_schema,
                 )
             )
         else:
-            return validator_pb2.FcoValidationArgs(feature_view=self._spec.data_proto.validation_args)
+            return self._spec.validation_args
 
     @property
     def _is_valid(self) -> bool:
         return self._spec is not None
 
     def _derive_schemas(self) -> None:
         raise NotImplementedError
@@ -232,17 +249,16 @@
         """Returns a wildcard join key column name if it exists; Otherwise returns None."""
         wildcard_keys = set(self.join_keys) - set(self.online_serving_index)
         if len(wildcard_keys) == 0:
             return None
         elif len(wildcard_keys) == 1:
             return list(wildcard_keys)[0]
         else:
-            raise ValueError(
-                "The online serving index must either be equal to join_keys or only be missing a single key."
-            )
+            msg = "The online serving index must either be equal to join_keys or only be missing a single key."
+            raise ValueError(msg)
 
     @property
     @sdk_decorators.sdk_public_method
     @sdk_decorators.assert_remote_object
     def url(self) -> str:
         """Returns a link to the Tecton Web UI."""
         return self._feature_definition.fv_spec.url
@@ -255,18 +271,22 @@
     def _validate(self, indentation_level: int = 0) -> None:
         if self._is_valid:
             return
 
         self._validate_dependencies(indentation_level)
         try:
             self._derive_schemas()
-        except Exception as e:
-            raise errors.TectonValidationError(
-                f"An error occured when attempting to run {self.__class__.__name__} '{self.name}' during validation. See the chained exception for details."
-            ) from e
+        except Exception:
+            # Use logger.exception() to print/log the validation error message followed by the exception trace and then
+            # re-raise. This approach is preferred to exception chaining because it's a better notebook UX - especially
+            # for EMR notebooks, where chained exceptions are not rendered.
+            logger.exception(
+                f"An error occured when attempting to run {self.__class__.__name__} '{self.name}' during validation."
+            )
+            raise
 
         validations_api.run_backend_validation_and_assert_valid(
             self,
             validator_pb2.ValidationRequest(
                 validation_args=[
                     dependent_obj._build_fco_validation_args()
                     for dependent_obj in self._get_dependent_objects(include_indirect_dependencies=True)
@@ -311,15 +331,15 @@
     @sdk_decorators.assert_remote_object
     def list_materialization_jobs(self) -> List[materialization_api.MaterializationJobData]:
         return materialization_api.list_materialization_jobs(self.name, self.workspace)
 
     @sdk_decorators.assert_remote_object
     def _get_materialization_status(self) -> materialization_status_pb2.MaterializationStatus:
         # TODO(TEC-13080): delete this private method when integration tests no longer use it.
-        return materialization_api.get_materialization_status_response(self._spec.id_proto)
+        return materialization_api.get_materialization_status_response(self._spec.id_proto, self.workspace)
 
     @sdk_decorators.assert_remote_object
     def _get_serving_status(self):
         request = metadata_service_pb2.GetServingStatusRequest(
             feature_package_id=id_helper.IdHelper.from_string(self._feature_definition.id), workspace=self.workspace
         )
         return metadata_service.instance().GetServingStatus(request)
@@ -338,15 +358,16 @@
                 name="feature_service",
                 features=[
                     my_feature_view[["my_feature_1", "my_feature_2"]]
                 ],
             )
         """
         if not isinstance(features, list):
-            raise TypeError("The `features` field must be a list")
+            msg = "The `features` field must be a list"
+            raise TypeError(msg)
 
         return FeatureReference(feature_definition=self, features=features)
 
     def with_name(self, namespace: str) -> FeatureReference:
         """Rename a Feature View used in a Feature Service.
 
         .. code-block:: python
@@ -490,25 +511,37 @@
             materialization_schema = snowflake_api.derive_materialization_schema_for_feature_view(
                 view_schema, self._args
             )
         else:
             push_sources = [
                 ds_spec
                 for ds_spec in data_source_specs
-                if ds_spec.type in (DataSourceType.PUSH_NO_BATCH, DataSourceType.PUSH_WITH_BATCH)
+                if ds_spec.type
+                in (
+                    data_source_type_pb2.DataSourceType.PUSH_NO_BATCH,
+                    data_source_type_pb2.DataSourceType.PUSH_WITH_BATCH,
+                )
             ]
             is_push_source = len(push_sources) > 0
 
             if is_push_source:
-                assert len(push_sources) == 1, "If there is a Push Source, there should be exactly one data source."
-                ds_spec = push_sources[0]
-
-                push_source_schema = ds_spec.schema.tecton_schema
-                schema_derivation_utils.populate_schema_with_derived_fields(push_source_schema)
-                view_schema = push_source_schema
+                # SFVs with Push Sources should be able to specify a Schema override
+                if len(transformation_specs) > 0:
+                    assert self._args.materialized_feature_view_args.HasField(
+                        "schema"
+                    ), f"A schema must be specified for {self.info.name}"
+                    view_schema = self._args.materialized_feature_view_args.schema
+                else:
+                    # If there's no override, there's no transformations, and we can use the push source schema
+                    assert len(push_sources) == 1, "If there is a Push Source, there should be exactly one data source."
+                    ds_spec = push_sources[0]
+
+                    push_source_schema = ds_spec.schema.tecton_schema
+                    schema_derivation_utils.populate_schema_with_derived_fields(push_source_schema)
+                    view_schema = push_source_schema
                 materialization_schema = spark_api.derive_materialization_schema_for_feature_view(
                     view_schema, self._args
                 )
             else:
                 view_schema = spark_api.derive_view_schema_for_feature_view(
                     self._args, transformation_specs, data_source_specs
                 )
@@ -555,29 +588,32 @@
             from_source = not fd.materialization_enabled or not fd.writes_to_offline_store
 
         if from_source:
             return QuerySources(from_source_count=1)
         else:
             return QuerySources(offline_store_count=1)
 
-    @sdk_decorators.sdk_public_method(requires_validation=True)
+    @sdk_decorators.sdk_public_method(
+        requires_validation=True, supports_skip_validation=(compute_mode.ComputeMode.SPARK,)
+    )
     def get_historical_features(
         self,
         spine: Optional[
             Union[pyspark_dataframe.DataFrame, pandas.DataFrame, tecton_dataframe.TectonDataFrame, str]
         ] = None,
         timestamp_key: Optional[str] = None,
         start_time: Optional[datetime.datetime] = None,
         end_time: Optional[datetime.datetime] = None,
         entities: Optional[
             Union[pyspark_dataframe.DataFrame, pandas.DataFrame, tecton_dataframe.TectonDataFrame]
         ] = None,
         from_source: Optional[bool] = None,
         save: bool = False,
         save_as: Optional[str] = None,
+        mock_inputs: Optional[Dict[str, Union[pandas.DataFrame, pyspark_dataframe.DataFrame]]] = None,
     ) -> tecton_dataframe.TectonDataFrame:
         """Returns a Tecton :class:`TectonDataFrame` of historical values for this feature view.
 
         By default (i.e. ``from_source=None``), this method fetches feature values from the Offline Store for Feature
         Views that have offline materialization enabled and otherwise computes feature values on the fly from raw data.
 
         If no arguments are passed in, all feature values for this feature view will be returned in a Tecton DataFrame.
@@ -615,14 +651,21 @@
         :type from_source: bool
         :param save: Whether to persist the DataFrame as a Dataset object. Default is False.
         :type save: bool
         :param save_as: Name to save the DataFrame as.
             If unspecified and save=True, a name will be generated.
         :type save_as: str
 
+        :param mock_inputs: mock sources that should be used instead of fetching directly from raw data
+            sources. The keys of the dictionary should match the feature view's function parameters. For feature views with multiple sources, mocking some data sources and using raw data for others is supported.
+        :type mock_inputs: Optional[Dict[str, Union[pandas.DataFrame, pyspark_dataframe.DataFrame]]]
+
+
+
+
         Examples:
             A FeatureView :py:mod:`fv` with join key :py:mod:`user_id`.
 
             1) :py:mod:`fv.get_historical_features(spine)` where :py:mod:`spine=pandas.Dataframe({'user_id': [1,2,3],
             'date': [datetime(...), datetime(...), datetime(...)]})`
             Fetch historical features from the offline store for users 1, 2, and 3 for the specified timestamps in the spine.
 
@@ -633,29 +676,37 @@
             Fetch historical features from the offline store for users 1, 2, and 3 for the specified timestamps in the 'date_1' column in the spine.
 
             4) :py:mod:`fv.get_historical_features(start_time=datetime(...), end_time=datetime(...))`
             Fetch all historical features from the offline store in the time range specified by `start_time` and `end_time`.
 
         :return: A Tecton :class:`TectonDataFrame`.
         """
-        sources = self._check_can_query_from_source(from_source)
-        sources.display()
+
+        if self._is_valid:
+            # NOTE: we don't want to call `_check_can_query_from_source` on
+            # unvalidated feature view. We implement our own checks for
+            # unvalidated feature views after constructing the unvalidated
+            # feature definition.
+            sources = self._check_can_query_from_source(from_source)
+            sources.display()
 
         if spine is None and timestamp_key is not None:
             raise errors.GET_HISTORICAL_FEATURES_WRONG_PARAMS(["timestamp_key"], "the spine parameter is not provided")
 
         if spine is not None and (start_time is not None or end_time is not None or entities is not None):
             raise errors.GET_HISTORICAL_FEATURES_WRONG_PARAMS(
                 ["start_time", "end_time", "entities"], "the spine parameter is provided"
             )
 
         if start_time is not None and end_time is not None and start_time >= end_time:
             raise core_errors.START_TIME_NOT_BEFORE_END_TIME(start_time, end_time)
 
         if compute_mode.get_compute_mode() == compute_mode.ComputeMode.SNOWFLAKE:
+            if mock_inputs:
+                raise errors.SNOWFLAKE_COMPUTE_MOCK_SOURCES_UNSUPPORTED
             return snowflake_api.get_historical_features(
                 spine=spine,
                 timestamp_key=timestamp_key,
                 start_time=start_time,
                 end_time=end_time,
                 entities=entities,
                 from_source=from_source,
@@ -664,46 +715,77 @@
                 feature_set_config=self._construct_feature_set_config(),
                 append_prefix=False,
             )
 
         if compute_mode.get_compute_mode() == compute_mode.ComputeMode.ATHENA:
             if self.info.workspace is None or not self._feature_definition.materialization_enabled:
                 raise errors.ATHENA_COMPUTE_ONLY_SUPPORTED_IN_LIVE_WORKSPACE
-
+            if mock_inputs:
+                raise errors.ATHENA_COMPUTE_MOCK_SOURCES_UNSUPPORTED
             return athena_api.get_historical_features(
                 spine=spine,
                 timestamp_key=timestamp_key,
                 start_time=start_time,
                 end_time=end_time,
                 entities=entities,
                 from_source=from_source,
                 save=save,
                 save_as=save_as,
                 feature_set_config=self._construct_feature_set_config(),
             )
 
+        mock_data_sources = {}
+        if mock_inputs or not self._is_valid:
+            if self._feature_definition is None:
+                pyspark_mock_inputs = {}
+                if mock_inputs:
+                    pyspark_mock_inputs = _to_pyspark_mocks(mock_inputs)
+
+                # NOTE: when unvalidated we are requiring all the sources to be specified.
+                # technically it's 'possible' that we can only require the ones
+                # that are from unvalidated sources.
+                feature_definition = self._create_unvalidated_feature_definition(pyspark_mock_inputs)
+
+                if from_source == False:
+                    raise errors.UNVALIDATED_FEATURE_VIEWS_FROM_SOURCE_FALSE(self.name)
+
+                if feature_definition.is_incremental_backfill:
+                    raise errors.FV_WITH_INC_BACKFILLS_GET_MATERIALIZED_FEATURES_MOCK_DATA(self.name)
+            else:
+                feature_definition = self._feature_definition
+
+            mock_data_sources = mock_source_utils.convert_mock_inputs_to_mock_sources(feature_definition, mock_inputs)
+        else:
+            feature_definition = self._feature_definition
+
         return spark_api.get_historical_features_for_feature_definition(
-            feature_definition=self._feature_definition,
+            feature_definition=feature_definition,
             spine=spine,
             timestamp_key=timestamp_key,
             start_time=start_time,
             end_time=end_time,
             entities=entities,
             from_source=from_source,
             save=save,
             save_as=save_as,
+            mock_data_sources=mock_data_sources,
         )
 
-    @sdk_decorators.sdk_public_method(requires_validation=True, validation_error_message=errors.RUN_REQUIRES_VALIDATION)
+    @sdk_decorators.sdk_public_method(
+        requires_validation=True,
+        validation_error_message=errors.RUN_REQUIRES_VALIDATION,
+        supports_skip_validation=(compute_mode.ComputeMode.SPARK,),
+    )
     def run(
         self,
         start_time: datetime.datetime,
         end_time: datetime.datetime,
         aggregation_level: Optional[str] = None,
-        **mock_sources: Union[pandas.DataFrame, pyspark_dataframe.DataFrame],
+        mock_inputs: Optional[Dict[str, Union[pandas.DataFrame, pyspark_dataframe.DataFrame]]] = None,
+        **mock_inputs_kwargs: Union[pandas.DataFrame, pyspark_dataframe.DataFrame],
     ) -> tecton_dataframe.TectonDataFrame:
         """Run the FeatureView. Supports transforming data directly from raw data sources or using mock data.
 
         To run the feature view with data from raw data sources, the environment must have access to the data sources.
 
         :param start_time: The start time of the time window to materialize.
         :param end_time: The end time of the time window to materialize.
@@ -716,15 +798,20 @@
 
                 3) The tiles from #2 are combined to form the final feature values. The number of tiles that are combined is based off of the time_window of the aggregation.
 
             For testing and debugging purposes, to see the output of #1, use ``aggregation_level="disabled"``. For #2, use ``aggregation_level="partial"``. For #3, use ``aggregation_level="full"``.
 
             ``aggregation_level="full"`` is the default behavior.
 
-        :param \*\*mock_sources: kwargs for mock sources that should be used instead of fetching directly from raw data
+
+        :param mock_inputs: mock sources that should be used instead of fetching directly from raw data
+            sources. The keys of the dictionary should match the feature view's function parameters. For feature views with multiple sources, mocking some data sources and using raw data for others is supported.
+        :type mock_inputs: Optional[Dict[str, Union[pandas.DataFrame, pyspark_dataframe.DataFrame]]]
+
+        :param \*\*mock_inputs_kwargs: kwargs for mock sources that should be used instead of fetching directly from raw data
             sources. The keys should match the feature view's function parameters. For feature views with multiple sources, mocking some data sources and using raw data for others is supported.
 
         :return: A tecton DataFrame of the results.
 
         Example:
 
         .. code-block:: python
@@ -751,59 +838,77 @@
             aggregate_feature_view = tecton.get_workspace("my_workspace").get_feature_view("my_aggregate_feature_view")
 
             result = aggregate_feature_view.run(
               start_time=datetime(2022, 5, 1),
               end_time=datetime(2022, 5, 2),
               aggregation_level="full")  # or "partial" or "disabled"
         """
-        if self._feature_definition.is_temporal and aggregation_level is not None:
-            raise errors.FV_UNSUPPORTED_AGGREGATION
 
-        # TODO(TEC-12514) Add support for run method for PushSource without batch_config
-        if self._spec.data_source_type == data_source_type_pb2.DataSourceType.PUSH_NO_BATCH:
-            raise TectonValidationError(
-                f"The `run()` method is currently unsupported for Feature Views that are backed by a PushSource without a batch_config"
-            )
+        if mock_inputs:
+            resolved_mock_inputs = mock_inputs
+            if len(mock_inputs_kwargs) > 0:
+                msg = "Mock sources cannot be configured using both the mock_inputs dictionary and using the kwargs to the `run` call."
+                raise TectonValidationError(msg)
+        else:
+            resolved_mock_inputs = mock_inputs_kwargs
+
+        if any(ds._data_source_type == DataSourceType.PUSH_NO_BATCH for ds in self.sources):
+            msg = "The `run()` method is currently unsupported for Feature Views that are backed by a PushSource without a batch_config"
+            raise TectonValidationError(msg)
 
         if start_time is None or end_time is None:
-            raise TypeError("run() requires start_time and end_time to be set.")
+            msg = "run() requires start_time and end_time to be set."
+            raise TypeError(msg)
 
         if start_time >= end_time:
             raise core_errors.START_TIME_NOT_BEFORE_END_TIME(start_time, end_time)
 
-        aggregation_level = run_api.validate_and_get_aggregation_level(self._feature_definition, aggregation_level)
+        if self._feature_definition is None:
+            pyspark_mock_inputs = _to_pyspark_mocks(resolved_mock_inputs)
+            feature_definition = self._create_unvalidated_feature_definition(pyspark_mock_inputs)
+        else:
+            feature_definition = self._feature_definition
+
+        if feature_definition.is_temporal and aggregation_level is not None:
+            raise errors.FV_UNSUPPORTED_AGGREGATION
 
-        run_api.maybe_warn_incorrect_time_range_size(self._feature_definition, start_time, end_time, aggregation_level)
+        aggregation_level = run_api.validate_and_get_aggregation_level(feature_definition, aggregation_level)
+
+        run_api.maybe_warn_incorrect_time_range_size(feature_definition, start_time, end_time, aggregation_level)
 
         if compute_mode.get_compute_mode() == compute_mode.ComputeMode.SNOWFLAKE:
             return snowflake_api.run_batch(
                 fd=self._feature_definition,
                 feature_start_time=start_time,
                 feature_end_time=end_time,
-                mock_inputs=mock_sources,
+                mock_inputs=resolved_mock_inputs,
                 aggregation_level=aggregation_level,
             )
 
+        mock_data_sources = mock_source_utils.convert_mock_inputs_to_mock_sources(
+            feature_definition, resolved_mock_inputs
+        )
+
         return run_api.run_batch(
-            self._feature_definition,
+            feature_definition,
             start_time,
             end_time,
-            mock_sources,
+            mock_data_sources,
             feature_definition_wrapper.FrameworkVersion.FWV5,
             aggregation_level=aggregation_level,
         )
 
     @sdk_decorators.sdk_public_method
     @sdk_decorators.assert_local_object(error_message=errors.CANNOT_USE_LOCAL_RUN_ON_REMOTE_OBJECT)
     def test_run(
         self,
         start_time: datetime.datetime,
         end_time: datetime.datetime,
         aggregation_level: Optional[str] = None,
-        **mock_sources: Union[pandas.DataFrame, pyspark_dataframe.DataFrame],
+        **mock_inputs: Union[pandas.DataFrame, pyspark_dataframe.DataFrame],
     ) -> tecton_dataframe.TectonDataFrame:
         """Run the FeatureView using mock data sources. This requires a local spark session.
 
         Unlike :py:func:`run`, :py:func:`test_run` is intended for unit testing. It will not make calls to your
         connected Tecton cluster or attempt to read real data.
 
         :param start_time: The start time of the time window to materialize.
@@ -818,15 +923,15 @@
 
                 3) The tiles from #2 are combined to form the final feature values. The number of tiles that are combined is based off of the time_window of the aggregation.
 
             For testing and debugging purposes, to see the output of #1, use ``aggregation_level="disabled"``. For #2, use ``aggregation_level="partial"``. For #3, use ``aggregation_level="full"``.
 
             ``aggregation_level="full"`` is the default behavior.
 
-        :param \*\*mock_sources: kwargs with expected same keys as the FeatureView's inputs parameter. Each input name
+        :param \*\*mock_inputs: kwargs with expected same keys as the FeatureView's inputs parameter. Each input name
             maps to a Spark DataFrame that should be evaluated for that node in the pipeline.
 
         Example:
 
         .. code-block:: python
 
             from datetime import datetime, timedelta
@@ -842,15 +947,15 @@
                     "user_id": ["user_1", "user_2", "user_3", "user_4"],
                     "signup_timestamp": [datetime(2022, 5, 1)] * 4,
                     "cc_num": [1000000000000000, 4000000000000000, 5000000000000000, 6000000000000000],
                 })
                 input_spark_df = tecton_pytest_spark_session.createDataFrame(input_pandas_df)
 
                 # Simulate materializing features for May 1st.
-                output = user_credit_card_issuer.run(
+                output = user_credit_card_issuer.test_run(
                     start_time=datetime(2022, 5, 1),
                     end_time=datetime(2022, 5, 2),
                     fraud_users_batch=input_spark_df)
 
                 actual = output.to_pandas()
 
                 expected = pandas.DataFrame({
@@ -860,70 +965,40 @@
                 })
 
                 pandas.testing.assert_frame_equal(actual, expected)
 
 
         :return: A :class:`tecton.TectonDataFrame` object.
         """
-        data_source_type = self.sources[0]._data_source_type
-        # TODO(TEC-12514) Add support for test_run method for PushSource without batch_config
-        if data_source_type == DataSourceType.PUSH_NO_BATCH:
-            raise TectonValidationError(
-                f"The `test_run()` method is currently unsupported for Feature Views that are backed by a PushSource without a batch_config"
+        # We set `TECTON_VALIDATION_MODE` here for full backwards compatibility
+        # in case someone is not using the `tecton` pytest plugin.
+        with conf._temporary_set("TECTON_VALIDATION_MODE", ValidationMode.SKIP.value):
+            return self.run(
+                start_time=start_time,
+                end_time=end_time,
+                aggregation_level=aggregation_level,
+                mock_inputs=mock_inputs,
             )
 
-        if start_time is None or end_time is None:
-            raise TypeError("test_run() requires start_time and end_time to be set.")
-
-        if start_time >= end_time:
-            raise core_errors.START_TIME_NOT_BEFORE_END_TIME(start_time, end_time)
-
-        # Convert pandas dataframes to pyspark dataframes so that we can use the pyspark schemas for mock schema
-        # derivation.
-        pyspark_mock_sources = {}
-        for input_name, mock_data in mock_sources.items():
-            if isinstance(mock_data, pyspark_dataframe.DataFrame):
-                pyspark_mock_sources[input_name] = mock_data
-            elif isinstance(mock_data, pandas.DataFrame):
-                spark = tecton_context.TectonContext.get_instance()._spark
-                pyspark_mock_sources[input_name] = spark.createDataFrame(mock_data)
-            else:
-                raise TypeError(f"Unexpected mock source type for kwarg {input_name}: {type(mock_data)}")
-
-        feature_definition = self._create_unvalidated_feature_definition(pyspark_mock_sources)
-
-        aggregation_level = run_api.validate_and_get_aggregation_level(feature_definition, aggregation_level)
-
-        run_api.maybe_warn_incorrect_time_range_size(feature_definition, start_time, end_time, aggregation_level)
-
-        return run_api.run_batch(
-            feature_definition,
-            start_time,
-            end_time,
-            pyspark_mock_sources,
-            feature_definition_wrapper.FrameworkVersion.FWV5,
-            aggregation_level=aggregation_level,
-        )
-
     @sdk_decorators.assert_local_object
     def _create_unvalidated_feature_definition(
-        self, mock_sources: Dict[str, pyspark_dataframe.DataFrame]
+        self, mock_inputs: Dict[str, pyspark_dataframe.DataFrame]
     ) -> feature_definition_wrapper.FeatureDefinitionWrapper:
         """Create an unvalidated feature definition. Used for user unit testing, where backend validation is unavailable."""
-        input_name_to_ds_ids = pipeline_helper.get_all_input_ds_id_map(self._args.pipeline.root)
+        input_name_to_ds_ids = pipeline_common.get_input_name_to_ds_id_map(self._args.pipeline)
 
-        if set(input_name_to_ds_ids.keys()) != set(mock_sources.keys()):
-            raise errors.FV_INVALID_MOCK_SOURCES(mock_sources.keys(), input_name_to_ds_ids.keys())
+        if set(input_name_to_ds_ids.keys()) != set(mock_inputs.keys()):
+            raise errors.FV_INVALID_MOCK_SOURCES(list(mock_inputs.keys()), list(input_name_to_ds_ids.keys()))
 
-        # It's possible to have the same data source included twice in mock_sources under two different input names, but
+        # It's possible to have the same data source included twice in mock_inputs under two different input names, but
         # we only need to generate an unvalidated spec once for data source. (Putting two conflicting specs with the
         # same id in the fco container will lead to erros.)
         ds_id_to_mock_df = {}
         for input_name, ds_id in input_name_to_ds_ids.items():
-            mock_df = mock_sources[input_name]
+            mock_df = mock_inputs[input_name]
             ds_id_to_mock_df[ds_id] = mock_df
 
         data_source_specs = []
         for source in self.sources:
             mock_df = ds_id_to_mock_df[source.info.id]
             data_source_specs.append(source._create_unvalidated_spec(mock_df))
 
@@ -963,15 +1038,16 @@
 
             2) :py:mod:`fv.get_online_features(join_keys={'user_id': 1}, include_join_keys_in_respone=True)`
             Fetch the latest features from the online store for user 1 and include the join key information (user_id=1) in the returned FeatureVector.
 
         :return: A :class:`tecton.FeatureVector` of the results.
         """
         if not self._feature_definition.writes_to_online_store:
-            raise errors.UNSUPPORTED_OPERATION("get_online_features", "online=True is not set for this FeatureView.")
+            msg = "get_online_features"
+            raise errors.UNSUPPORTED_OPERATION(msg, "online=True is not set for this FeatureView.")
 
         return query_helper._QueryHelper(self.info.workspace, feature_view_name=self.info.name).get_feature_vector(
             join_keys,
             include_join_keys_in_response,
             request_context_map={},
             request_context_schema=request_context.RequestContext({}),
         )
@@ -1045,15 +1121,15 @@
         return max([source.data_delay for source in self.sources]) or datetime.timedelta(0)
 
     @property
     @sdk_decorators.sdk_public_method
     # TODO(deprecate_after=0.6): Deprecate this method.
     def max_data_delay(self) -> datetime.timedelta:
         """Deprecated. Use max_source_data_delay instead."""
-        logger.warn(
+        logger.warning(
             "max_data_delay is deprecated and will be removed in 0.7. Please use max_source_data_delay instead."
         )
         return self.max_source_data_delay
 
     @sdk_decorators.sdk_public_method
     @sdk_decorators.assert_remote_object
     def delete_keys(
@@ -1072,50 +1148,50 @@
         :param keys: The Dataframe to be deleted. Must conform to the FeatureView join keys.
         :param online: (Optional, default=True) Whether or not to delete from the online store.
         :param offline: (Optional, default=True) Whether or not to delete from the offline store.
         :return: None if deletion job was created successfully.
         """
         is_live_workspace = internal_utils.is_live_workspace(self.info.workspace)
         if not is_live_workspace:
-            raise errors.UNSUPPORTED_OPERATION_IN_DEVELOPMENT_WORKSPACE("delete_keys")
+            msg = "delete_keys"
+            raise errors.UNSUPPORTED_OPERATION_IN_DEVELOPMENT_WORKSPACE(msg)
 
         return delete_keys_api.delete_keys(online, offline, keys, self._feature_definition)
 
     @sdk_decorators.sdk_public_method
     @sdk_decorators.assert_remote_object
-    def materialization_status(self, verbose=False, limit=1000, sort_columns=None, errors_only=False):
+    def materialization_status(
+        self, verbose=False, limit=1000, sort_columns=None, errors_only=False
+    ) -> display.Displayable:
         """Displays materialization information for the FeatureView, which may include past jobs, scheduled jobs,
         and job failures.
 
         This method returns different information depending on the type of FeatureView.
         :param verbose: If set to true, method will display additional low level materialization information,
         useful for debugging.
         :param limit: Maximum number of jobs to return.
         :param sort_columns: A comma-separated list of column names by which to sort the rows.
         :param errors_only: If set to true, method will only return jobs that failed with an error.
         """
         return materialization_api.get_materialization_status_for_display(
-            self._spec.id_proto, verbose, limit, sort_columns, errors_only
+            self._spec.id_proto, self.workspace, verbose, limit, sort_columns, errors_only
         )
 
     @sdk_decorators.sdk_public_method
     @sdk_decorators.documented_by(materialization_api.trigger_materialization_job)
     @sdk_decorators.assert_remote_object
     def trigger_materialization_job(
         self,
         start_time: datetime.datetime,
         end_time: datetime.datetime,
         online: bool,
         offline: bool,
         use_tecton_managed_retries: bool = True,
         overwrite: bool = False,
     ) -> str:
-        if self.batch_trigger != BatchTriggerType.MANUAL:
-            raise errors.FV_NEEDS_TO_BE_BATCH_TRIGGER_MANUAL(self.name)
-
         return materialization_api.trigger_materialization_job(
             self.name, self.workspace, start_time, end_time, online, offline, use_tecton_managed_retries, overwrite
         )
 
     @sdk_decorators.sdk_public_method
     @sdk_decorators.documented_by(materialization_api.wait_for_materialization_job)
     @sdk_decorators.assert_remote_object
@@ -1124,26 +1200,26 @@
         job_id: str,
         timeout: Optional[datetime.timedelta] = None,
     ) -> materialization_api.MaterializationJobData:
         return materialization_api.wait_for_materialization_job(self.name, self.workspace, job_id, timeout)
 
     @sdk_decorators.sdk_public_method
     @sdk_decorators.assert_remote_object
-    def deletion_status(self, verbose=False, limit=1000, sort_columns=None, errors_only=False):
+    def deletion_status(self, verbose=False, limit=1000, sort_columns=None, errors_only=False) -> display.Displayable:
         """Displays information for deletion jobs created with the delete_keys() method,which may include past jobs,
         scheduled jobs, and job failures.
 
         :param verbose: If set to true, method will display additional low level deletion information,
             useful for debugging.
         :param limit: Maximum number of jobs to return.
         :param sort_columns: A comma-separated list of column names by which to sort the rows.
-        :param: errors_only: If set to true, method will only return jobs that failed with an error.
+        :param errors_only: If set to true, method will only return jobs that failed with an error.
         """
         return materialization_api.get_deletion_status_for_display(
-            self._spec.id_proto, verbose, limit, sort_columns, errors_only
+            self._spec.id_proto, self.workspace, verbose, limit, sort_columns, errors_only
         )
 
 
 @attrs.define(eq=False)
 class BatchFeatureView(MaterializedFeatureView):
     """A Tecton Batch Feature View, used for materializing features on a batch schedule from a BatchSource.
 
@@ -1171,32 +1247,36 @@
         mode: str,
         aggregation_interval: Optional[datetime.timedelta] = None,
         aggregations: Optional[Sequence[configs.Aggregation]] = None,
         online: bool = False,
         offline: bool = False,
         ttl: Optional[datetime.timedelta] = None,
         feature_start_time: Optional[datetime.datetime] = None,
+        manual_trigger_backfill_end_time: Optional[datetime.datetime] = None,
         batch_trigger: BatchTriggerType = BatchTriggerType.SCHEDULED,
         batch_schedule: Optional[datetime.timedelta] = None,
         online_serving_index: Optional[Sequence[str]] = None,
         batch_compute: Optional[
             Union[
                 configs.DatabricksClusterConfig,
                 configs.EMRClusterConfig,
                 configs.DatabricksJsonClusterConfig,
+                configs.DataprocJsonClusterConfig,
                 configs.EMRJsonClusterConfig,
             ]
         ] = None,
         offline_store: Optional[Union[configs.ParquetConfig, configs.DeltaConfig]] = configs.ParquetConfig(),
         online_store: Optional[Union[configs.DynamoConfig, configs.RedisConfig]] = None,
         monitor_freshness: bool = False,
+        data_quality_enabled: Optional[bool] = None,
+        skip_default_expectations: Optional[bool] = None,
         expected_feature_freshness: Optional[datetime.timedelta] = None,
         alert_email: Optional[str] = None,
         timestamp_field: Optional[str] = None,
-        max_batch_aggregation_interval: Optional[datetime.timedelta] = None,
+        max_backfill_interval: Optional[datetime.timedelta] = None,
         incremental_backfills: bool = False,
     ):
         from tecton.cli import common as cli_common
 
         if mode == PIPELINE_MODE:
             pipeline_function = feature_view_function
         else:
@@ -1230,28 +1310,31 @@
             offline_store=offline_store,
             online_store=online_store,
             aggregation_interval=aggregation_interval,
             stream_processing_mode=stream_processing_mode,
             aggregations=aggregations,
             ttl=ttl,
             feature_start_time=feature_start_time,
+            manual_trigger_backfill_end_time=manual_trigger_backfill_end_time,
             batch_trigger=batch_trigger,
             batch_schedule=batch_schedule,
             online_serving_index=online_serving_index,
             batch_compute=batch_compute,
             stream_compute=None,
             monitor_freshness=monitor_freshness,
+            data_quality_enabled=data_quality_enabled,
+            skip_default_expectations=skip_default_expectations,
             expected_feature_freshness=expected_feature_freshness,
             alert_email=alert_email,
             description=description,
             owner=owner,
             tags=tags,
             timestamp_field=timestamp_field,
             data_source_type=data_source_type_pb2.DataSourceType.BATCH,
-            max_batch_aggregation_interval=max_batch_aggregation_interval,
+            max_backfill_interval=max_backfill_interval,
             output_stream=None,
             incremental_backfills=incremental_backfills,
         )
 
         info = base_tecton_object.TectonObjectInfo.from_args_proto(args.info, args.feature_view_id)
 
         data_sources = tuple(
@@ -1265,15 +1348,15 @@
             args=args,
             source_info=source_info,
             sources=data_sources,
             entities=tuple(entities),
             transformations=tuple(pipeline_root.transformations),
             args_supplement=None,
         )
-        internal_fco.Fco._register(self)
+        base_tecton_object._register_local_object(self)
 
 
 @typechecked
 def batch_feature_view(
     *,
     name: Optional[str] = None,
     description: Optional[str] = None,
@@ -1285,31 +1368,36 @@
     entities: Sequence[framework_entity.Entity],
     aggregation_interval: Optional[datetime.timedelta] = None,
     aggregations: Optional[Sequence[configs.Aggregation]] = None,
     online: bool = False,
     offline: bool = False,
     ttl: Optional[datetime.timedelta] = None,
     feature_start_time: Optional[datetime.datetime] = None,
+    manual_trigger_backfill_end_time: Optional[datetime.datetime] = None,
     batch_trigger: BatchTriggerType = BatchTriggerType.SCHEDULED,
     batch_schedule: Optional[datetime.timedelta] = None,
     online_serving_index: Optional[Sequence[str]] = None,
     batch_compute: Optional[
         Union[
             configs.DatabricksClusterConfig,
             configs.EMRClusterConfig,
             configs.DatabricksJsonClusterConfig,
+            configs.DataprocJsonClusterConfig,
             configs.EMRJsonClusterConfig,
         ]
     ] = None,
     offline_store: Optional[Union[configs.ParquetConfig, configs.DeltaConfig]] = configs.ParquetConfig(),
     online_store: Optional[Union[configs.DynamoConfig, configs.RedisConfig]] = None,
     monitor_freshness: bool = False,
+    data_quality_enabled: Optional[bool] = None,
+    skip_default_expectations: Optional[bool] = None,
     expected_feature_freshness: Optional[datetime.timedelta] = None,
     alert_email: Optional[str] = None,
     timestamp_field: Optional[str] = None,
+    max_backfill_interval: Optional[datetime.timedelta] = None,
     max_batch_aggregation_interval: Optional[datetime.timedelta] = None,
     incremental_backfills: bool = False,
 ):
     """Declare a Batch Feature View.
 
     :param mode: Whether the annotated function is a pipeline function ("pipeline" mode) or a transformation function ("spark_sql", "pyspark", "snowflake_sql", "snowpark", or "athena" mode).
         For the non-pipeline mode, an inferred transformation will also be registered.
@@ -1328,41 +1416,47 @@
         The default value is ``BatchTriggerType.SCHEDULED``, where Tecton will run materialization jobs based on the
         schedule defined by the ``batch_schedule`` parameter. If set to ``BatchTriggerType.MANUAL``, then batch
         materialization jobs must be explicitly initiated by the user through either the Tecton SDK or Airflow operator.
     :param batch_compute: Configuration for the batch materialization cluster.
     :param offline_store: Configuration for how data is written to the offline feature store.
     :param online_store: Configuration for how data is written to the online feature store.
     :param monitor_freshness: If true, enables monitoring when feature data is materialized to the online feature store.
+    :param data_quality_enabled: If false, disables data quality metric computation and data quality dashboard.
+    :param skip_default_expectations: If true, skips validating default expectations on the feature data.
     :param expected_feature_freshness: Threshold used to determine if recently materialized feature data is stale. Data is stale if ``now - most_recent_feature_value_timestamp > expected_feature_freshness``. For feature views using Tecton aggregations, data is stale if ``now - round_up_to_aggregation_interval(most_recent_feature_value_timestamp) > expected_feature_freshness``. Where ``round_up_to_aggregation_interval()`` rounds up the feature timestamp to the end of the ``aggregation_interval``. Value must be at least 2 times ``aggregation_interval``. If not specified, a value determined by the Tecton backend is used.
     :param alert_email: Email that alerts for this FeatureView will be sent to.
     :param description: A human readable description.
     :param owner: Owner name (typically the email of the primary maintainer).
     :param tags: Tags associated with this Tecton Object (key-value pairs of arbitrary metadata).
     :param prevent_destroy: If True, this Tecton object will be blocked from being deleted or re-created (i.e. a
-        destructive update) during tecton plan/apply. To remove or update this object, ``prevent_destroy`` must be
-        first set to False via a separate tecton apply. ``prevent_destroy`` can be used to prevent accidental changes
+        destructive update) during tecton plan/apply. To remove or update this object, ``prevent_destroy`` must be set to
+        False via the same tecton apply or a separate tecton apply. ``prevent_destroy`` can be used to prevent accidental changes
         such as inadvertantly deleting a Feature Service used in production or recreating a Feature View that
         triggers expensive rematerialization jobs. ``prevent_destroy`` also blocks changes to dependent Tecton objects
         that would trigger a recreate of the tagged object, e.g. if ``prevent_destroy`` is set on a Feature Service,
         that will also prevent deletions or re-creates of Feature Views used in that service. ``prevent_destroy`` is
         only enforced in live (i.e. non-dev) workspaces.
     :param timestamp_field: The column name that refers to the timestamp for records that are produced by the
         feature view. This parameter is optional if exactly one column is a Timestamp type. This parameter is
         required if using Tecton on Snowflake without Snowpark.
     :param name: Unique, human friendly name that identifies the FeatureView. Defaults to the function name.
-    :param max_batch_aggregation_interval: (Advanced) The time interval for which each backfill job will run to materialize
+    :param max_batch_aggregation_interval: Deprecated. Use max_backfill_interval instead, which has the exact same usage.
+    :param max_backfill_interval: (Advanced) The time interval for which each backfill job will run to materialize
         feature data. This affects the number of backfill jobs that will run, which is
-        (`<feature registration time>` - `feature_start_time`) / `max_batch_aggregation_interval`.
-        Configuring the `max_batch_aggregation_interval` parameter appropriately will help to optimize large backfill jobs.
+        (`<feature registration time>` - `feature_start_time`) / `max_backfill_interval`.
+        Configuring the `max_backfill_interval` parameter appropriately will help to optimize large backfill jobs.
         If this parameter is not specified, then 10 backfill jobs will run (the default).
 
     :param incremental_backfills: If set to `True`, the feature view will be backfilled one interval at a time as
         if it had been updated "incrementally" since its feature_start_time. For example, if ``batch_schedule`` is 1 day
         and ``feature_start_time`` is 1 year prior to the current time, then the backfill will run 365 separate
         materialization jobs to fill the historical feature data.
+    :param manual_trigger_backfill_end_time: If set, Tecton will schedule backfill materialization jobs for this feature
+        view up to this time. Materialization jobs after this point must be triggered manually. (This param is only valid
+        to set if BatchTriggerType is MANUAL.)
     :return: An object of type :class:`tecton.BatchFeatureView`.
 
     Example BatchFeatureView declaration:
 
     .. code-block:: python
 
         from datetime import datetime
@@ -1430,14 +1524,20 @@
                     AMOUNT,
                     TIMESTAMP
                 FROM
                     {credit_scores_batch}
             '''
     """
 
+    if max_batch_aggregation_interval is not None:
+        assert max_backfill_interval is None, "Cannot set both max_backfill_interval and max_batch_aggregation_interval"
+        logger.warning(
+            "FeatureView.max_batch_aggregation_interval is deprecated. Please use max_backfill_interval instead. max_backfill_interval has the same semantics and is just a new name."
+        )
+
     def decorator(feature_view_function):
         return BatchFeatureView(
             name=name or feature_view_function.__name__,
             description=description,
             owner=owner,
             tags=tags,
             prevent_destroy=prevent_destroy,
@@ -1447,25 +1547,28 @@
             entities=entities,
             aggregation_interval=aggregation_interval,
             aggregations=aggregations,
             online=online,
             offline=offline,
             ttl=ttl,
             feature_start_time=feature_start_time,
+            manual_trigger_backfill_end_time=manual_trigger_backfill_end_time,
             batch_trigger=batch_trigger,
             batch_schedule=batch_schedule,
             online_serving_index=online_serving_index,
             batch_compute=batch_compute,
             offline_store=offline_store,
             online_store=online_store,
             monitor_freshness=monitor_freshness,
+            data_quality_enabled=data_quality_enabled,
+            skip_default_expectations=skip_default_expectations,
             expected_feature_freshness=expected_feature_freshness,
             alert_email=alert_email,
             timestamp_field=timestamp_field,
-            max_batch_aggregation_interval=max_batch_aggregation_interval,
+            max_backfill_interval=max_backfill_interval or max_batch_aggregation_interval,
             incremental_backfills=incremental_backfills,
         )
 
     return decorator
 
 
 @attrs.define(eq=False)
@@ -1500,57 +1603,61 @@
         aggregations: Optional[Sequence[configs.Aggregation]] = None,
         stream_processing_mode: Optional[StreamProcessingMode] = None,
         aggregation_mode: Optional[AggregationMode] = None,
         online: bool = False,
         offline: bool = False,
         ttl: Optional[datetime.timedelta] = None,
         feature_start_time: Optional[datetime.datetime] = None,
+        manual_trigger_backfill_end_time: Optional[datetime.datetime] = None,
         batch_trigger: BatchTriggerType = None,
         batch_schedule: Optional[datetime.timedelta] = None,
         online_serving_index: Optional[Sequence[str]] = None,
         batch_compute: Optional[
             Union[
                 configs.DatabricksClusterConfig,
                 configs.EMRClusterConfig,
                 configs.DatabricksJsonClusterConfig,
+                configs.DataprocJsonClusterConfig,
                 configs.EMRJsonClusterConfig,
             ]
         ] = None,
         stream_compute: Optional[
             Union[
                 configs.DatabricksClusterConfig,
                 configs.EMRClusterConfig,
                 configs.DatabricksJsonClusterConfig,
+                configs.DataprocJsonClusterConfig,
                 configs.EMRJsonClusterConfig,
             ]
         ] = None,
         offline_store: Optional[Union[configs.ParquetConfig, configs.DeltaConfig]] = configs.ParquetConfig(),
         online_store: Optional[Union[configs.DynamoConfig, configs.RedisConfig]] = None,
         monitor_freshness: bool = False,
+        data_quality_enabled: Optional[bool] = None,
+        skip_default_expectations: Optional[bool] = None,
         expected_feature_freshness: Optional[datetime.timedelta] = None,
         alert_email: Optional[str] = None,
         timestamp_field: Optional[str] = None,
-        max_batch_aggregation_interval: Optional[datetime.timedelta] = None,
+        max_backfill_interval: Optional[datetime.timedelta] = None,
         output_stream: Optional[configs.OutputStream] = None,
         schema: Optional[List[types.Field]] = None,
     ):
         """Construct a StreamFeatureView.
 
         `init` should not be used directly, and instead :py:func:`tecton.stream_feature_view` decorator is recommended.
         """
         from tecton.cli import common as cli_common
 
         if aggregation_mode is not None:
-            logger.warn(
-                f"StreamFeatureView '{name}' sets aggregation_mode, which is deprecated and will be removed in tecton 0.7. Use stream_processing_mode=StreamProcessingMode.CONTINUOUS (or TIME_INTEVAL) instead."
+            logger.warning(
+                f"StreamFeatureView '{name}' sets aggregation_mode, which is deprecated and will be removed in tecton 0.7. Use stream_processing_mode=StreamProcessingMode.CONTINUOUS (or TIME_INTERVAL) instead."
             )
             if stream_processing_mode is not None:
-                raise core_errors.TectonValidationError(
-                    "Cannot set both aggregation_mode and stream_processing_mode. aggregation_mode is deprecated. Use stream_processing_mode instead."
-                )
+                msg = "Cannot set both aggregation_mode and stream_processing_mode. aggregation_mode is deprecated. Use stream_processing_mode instead."
+                raise core_errors.TectonValidationError(msg)
             stream_processing_mode = aggregation_mode
 
         data_source = source.source if isinstance(source, filtered_source.FilteredSource) else source
         has_push_source = isinstance(data_source, framework_data_source.PushSource)
         _validate_fv_input_count([source], feature_view_function)
 
         if has_push_source:
@@ -1600,28 +1707,31 @@
             offline_store=offline_store,
             online_store=online_store,
             aggregation_interval=aggregation_interval,
             stream_processing_mode=stream_processing_mode_,
             aggregations=aggregations,
             ttl=ttl,
             feature_start_time=feature_start_time,
+            manual_trigger_backfill_end_time=manual_trigger_backfill_end_time,
             batch_trigger=batch_trigger_,
             batch_schedule=batch_schedule,
             online_serving_index=online_serving_index,
             batch_compute=batch_compute,
             stream_compute=stream_compute,
             monitor_freshness=monitor_freshness,
+            data_quality_enabled=data_quality_enabled,
+            skip_default_expectations=skip_default_expectations,
             expected_feature_freshness=expected_feature_freshness,
             alert_email=alert_email,
             description=description,
             owner=owner,
             tags=tags,
             timestamp_field=timestamp_field,
             data_source_type=data_source_type,
-            max_batch_aggregation_interval=max_batch_aggregation_interval,
+            max_backfill_interval=max_backfill_interval,
             output_stream=output_stream,
             incremental_backfills=False,
             schema=schema,
         )
 
         info = base_tecton_object.TectonObjectInfo.from_args_proto(args.info, args.feature_view_id)
 
@@ -1634,15 +1744,15 @@
             args=args,
             source_info=source_info,
             sources=data_sources,
             entities=tuple(entities),
             transformations=tuple(pipeline_root.transformations),
             args_supplement=None,
         )
-        internal_fco.Fco._register(self)
+        base_tecton_object._register_local_object(self)
 
     @sdk_decorators.sdk_public_method(requires_validation=True)
     def run_stream(self, output_temp_table: str) -> streaming.StreamingQuery:
         """Starts a streaming job to keep writting the output records of this FeatureView to a temporary table.
 
         The job will be running until the execution is terminated.
 
@@ -1677,39 +1787,45 @@
     aggregations: Optional[Sequence[configs.Aggregation]] = None,
     stream_processing_mode: Optional[StreamProcessingMode] = None,
     aggregation_mode: Optional[AggregationMode] = None,
     online: bool = False,
     offline: bool = False,
     ttl: Optional[datetime.timedelta] = None,
     feature_start_time: Optional[datetime.datetime] = None,
+    manual_trigger_backfill_end_time: Optional[datetime.datetime] = None,
     batch_trigger: BatchTriggerType = BatchTriggerType.SCHEDULED,
     batch_schedule: Optional[datetime.timedelta] = None,
     online_serving_index: Optional[Sequence[str]] = None,
     batch_compute: Optional[
         Union[
             configs.DatabricksClusterConfig,
             configs.EMRClusterConfig,
             configs.DatabricksJsonClusterConfig,
+            configs.DataprocJsonClusterConfig,
             configs.EMRJsonClusterConfig,
         ]
     ] = None,
     stream_compute: Optional[
         Union[
             configs.DatabricksClusterConfig,
             configs.EMRClusterConfig,
             configs.DatabricksJsonClusterConfig,
+            configs.DataprocJsonClusterConfig,
             configs.EMRJsonClusterConfig,
         ]
     ] = None,
     offline_store: Optional[Union[configs.ParquetConfig, configs.DeltaConfig]] = configs.ParquetConfig(),
     online_store: Optional[Union[configs.DynamoConfig, configs.RedisConfig]] = None,
     monitor_freshness: bool = False,
+    data_quality_enabled: Optional[bool] = None,
+    skip_default_expectations: Optional[bool] = None,
     expected_feature_freshness: Optional[datetime.timedelta] = None,
     alert_email: Optional[str] = None,
     timestamp_field: Optional[str] = None,
+    max_backfill_interval: Optional[datetime.timedelta] = None,
     max_batch_aggregation_interval: Optional[datetime.timedelta] = None,
     output_stream: Optional[configs.OutputStream] = None,
     schema: Optional[List[types.Field]] = None,
 ):
     """Declare a Stream Feature View.
 
     :param mode: Whether the annotated function is a pipeline function ("pipeline" mode) or a transformation function ("spark_sql" or "pyspark" mode).
@@ -1734,37 +1850,41 @@
     :param batch_schedule: The interval at which batch materialization should be scheduled.
     :param online_serving_index: (Advanced) Defines the set of join keys that will be indexed and queryable during online serving.
     :param batch_compute: Batch materialization cluster configuration.
     :param stream_compute: Streaming materialization cluster configuration.
     :param offline_store: Configuration for how data is written to the offline feature store.
     :param online_store: Configuration for how data is written to the online feature store.
     :param monitor_freshness: If true, enables monitoring when feature data is materialized to the online feature store.
+    :param data_quality_enabled: If false, disables data quality metric computation and data quality dashboard.
+    :param skip_default_expectations: If true, skips validating default expectations on the feature data.
     :param expected_feature_freshness: Threshold used to determine if recently materialized feature data is stale. Data is stale if ``now - most_recent_feature_value_timestamp > expected_feature_freshness``. For feature views using Tecton aggregations, data is stale if ``now - round_up_to_aggregation_interval(most_recent_feature_value_timestamp) > expected_feature_freshness``. Where ``round_up_to_aggregation_interval()`` rounds up the feature timestamp to the end of the ``aggregation_interval``. Value must be at least 2 times ``aggregation_interval``. If not specified, a value determined by the Tecton backend is used.
     :param alert_email: Email that alerts for this FeatureView will be sent to.
     :param description: A human readable description.
     :param owner: Owner name (typically the email of the primary maintainer).
     :param tags: Tags associated with this Tecton Object (key-value pairs of arbitrary metadata).
     :param prevent_destroy: If True, this Tecton object will be blocked from being deleted or re-created (i.e. a
-        destructive update) during tecton plan/apply. To remove or update this object, ``prevent_destroy`` must be
-        first set to False via a separate tecton apply. ``prevent_destroy`` can be used to prevent accidental changes
+        destructive update) during tecton plan/apply. To remove or update this object, ``prevent_destroy`` must be set
+        to False via the same tecton apply or a separate tecton apply. ``prevent_destroy`` can be used to prevent accidental changes
         such as inadvertantly deleting a Feature Service used in production or recreating a Feature View that
         triggers expensive rematerialization jobs. ``prevent_destroy`` also blocks changes to dependent Tecton objects
         that would trigger a recreate of the tagged object, e.g. if ``prevent_destroy`` is set on a Feature Service,
         that will also prevent deletions or re-creates of Feature Views used in that service. ``prevent_destroy`` is
         only enforced in live (i.e. non-dev) workspaces.
     :param timestamp_field: The column name that refers to the timestamp for records that are produced by the
         feature view. This parameter is optional if exactly one column is a Timestamp type.
     :param name: Unique, human friendly name that identifies the FeatureView. Defaults to the function name.
-    :param max_batch_aggregation_interval: (Advanced) The time interval for which each backfill job will run to materialize
+    :param max_batch_aggregation_interval: Deprecated. Use max_backfill_interval instead, which has the exact same usage.
+    :param max_backfill_interval: (Advanced) The time interval for which each backfill job will run to materialize
         feature data. This affects the number of backfill jobs that will run, which is
-        (`<feature registration time>` - `feature_start_time`) / `max_batch_aggregation_interval`.
-        Configuring the `max_batch_aggregation_interval` parameter appropriately will help to optimize large backfill jobs.
+        (`<feature registration time>` - `feature_start_time`) / `max_backfill_interval`.
+        Configuring the `max_backfill_interval` parameter appropriately will help to optimize large backfill jobs.
         If this parameter is not specified, then 10 backfill jobs will run (the default).
     :param output_stream: Configuration for a stream to write feature outputs to, specified as a :class:`tecton.framework.conifgs.KinesisOutputStream` or :class:`tecton.framework.configs.KafkaOutputStream`.
     :param schema: Tecton schema specifying the expected output of the feature view. This should only be specified when defining stream feature views with push sources and transformations.
+    :param manual_trigger_backfill_end_time: If set, Tecton will schedule backfill materialization jobs for this feature view up to this time. Materialization jobs after this point must be triggered manually. (This param is only valid to set if BatchTriggerType is MANUAL.)
     :return: An object of type :class:`tecton.StreamFeatureView`.
 
     Example `StreamFeatureView` declaration:
 
     .. code-block:: python
 
         from datetime import datetime, timedelta
@@ -1831,14 +1951,20 @@
                     AMOUNT,
                     TIMESTAMP
                 FROM
                     {transactions_stream}
                 '''
     """
 
+    if max_batch_aggregation_interval is not None:
+        assert max_backfill_interval is None, "Cannot set both max_backfill_interval and max_batch_aggregation_interval"
+        logger.warning(
+            "FeatureView.max_batch_aggregation_interval is deprecated. Please use max_backfill_interval instead. max_backfill_interval has the same semantics and is just a new name."
+        )
+
     def decorator(feature_view_function):
         return StreamFeatureView(
             name=name or feature_view_function.__name__,
             description=description,
             owner=owner,
             tags=tags,
             prevent_destroy=prevent_destroy,
@@ -1850,26 +1976,29 @@
             aggregations=aggregations,
             stream_processing_mode=stream_processing_mode,
             aggregation_mode=aggregation_mode,
             online=online,
             offline=offline,
             ttl=ttl,
             feature_start_time=feature_start_time,
+            manual_trigger_backfill_end_time=manual_trigger_backfill_end_time,
             batch_trigger=batch_trigger,
             batch_schedule=batch_schedule,
             online_serving_index=online_serving_index,
             batch_compute=batch_compute,
             stream_compute=stream_compute,
             offline_store=offline_store,
             online_store=online_store,
             monitor_freshness=monitor_freshness,
+            data_quality_enabled=data_quality_enabled,
+            skip_default_expectations=skip_default_expectations,
             expected_feature_freshness=expected_feature_freshness,
             alert_email=alert_email,
             timestamp_field=timestamp_field,
-            max_batch_aggregation_interval=max_batch_aggregation_interval,
+            max_backfill_interval=max_backfill_interval or max_batch_aggregation_interval,
             output_stream=output_stream,
             schema=schema,
         )
 
     return decorator
 
 
@@ -1924,52 +2053,53 @@
         name: str,
         description: Optional[str] = None,
         owner: Optional[str] = None,
         tags: Optional[Dict[str, str]] = None,
         prevent_destroy: bool = False,
         entities: List[framework_entity.Entity],
         schema: List[types.Field],
-        ttl: datetime.timedelta,
+        ttl: Optional[datetime.timedelta] = None,
         online: bool = False,
         offline: bool = False,
         offline_store: configs.DeltaConfig = configs.DeltaConfig(),
         online_store: Optional[Union[configs.DynamoConfig, configs.RedisConfig]] = None,
         batch_compute: Optional[
             Union[
                 configs.DatabricksClusterConfig,
                 configs.EMRClusterConfig,
                 configs.DatabricksJsonClusterConfig,
+                configs.DataprocJsonClusterConfig,
                 configs.EMRJsonClusterConfig,
             ]
         ] = None,
         online_serving_index: Optional[List[str]] = None,
     ):
         """Instantiate a new FeatureTable.
 
         :param name: Unique, human friendly name that identifies the FeatureTable.
         :param description: A human readable description.
         :param owner: Owner name (typically the email of the primary maintainer).
         :param tags: Tags associated with this Tecton Object (key-value pairs of arbitrary metadata).
         :param prevent_destroy: If True, this Tecton object will be blocked from being deleted or re-created (i.e. a
-            destructive update) during tecton plan/apply. To remove or update this object, ``prevent_destroy`` must be
-            first set to False via a separate tecton apply. ``prevent_destroy`` can be used to prevent accidental changes
+            destructive update) during tecton plan/apply. To remove or update this object, ``prevent_destroy`` must
+            be set to False via the same tecton apply or a separate tecton apply. ``prevent_destroy`` can be used to prevent accidental changes
             such as inadvertantly deleting a Feature Service used in production or recreating a Feature View that
             triggers expensive rematerialization jobs. ``prevent_destroy`` also blocks changes to dependent Tecton objects
             that would trigger a recreate of the tagged object, e.g. if ``prevent_destroy`` is set on a Feature Service,
             that will also prevent deletions or re-creates of Feature Views used in that service. ``prevent_destroy`` is
             only enforced in live (i.e. non-dev) workspaces.
         :param entities: A list of Entity objects, used to organize features.
         :param schema: A schema for the FeatureTable. Supported types are: Int64, Float64, String, Bool and Array with Int64, Float32, Float64 and String typed elements. Additionally you must have exactly one Timestamp typed column for the feature timestamp.
         :param ttl: The TTL (or "look back window") for features defined by this feature table. This parameter determines how long features will live in the online store and how far to  "look back" relative to a training example's timestamp when generating offline training sets. Shorter TTLs improve performance and reduce costs.
         :param online: Enable writing to online feature store. (Default: False)
         :param offline: Enable writing to offline feature store. (Default: False)
         :param offline_store: Configuration for how data is written to the offline feature store.
         :param online_store: Configuration for how data is written to the online feature store.
         :param batch_compute: Configuration for batch materialization clusters. Should be one of:
-            [:class:`EMRClusterConfig`, :class:`DatabricksClusterConfig`, :class:`EMRJsonClusterConfig`, :class:`DatabricksJsonClusterConfig`]
+            [:class:`EMRClusterConfig`, :class:`DatabricksClusterConfig`, :class:`EMRJsonClusterConfig`, :class:`DatabricksJsonClusterConfig`, :class:`DataprocJsonClusterConfig`]
         :param online_serving_index: (Advanced) Defines the set of join keys that will be indexed and queryable during online serving.
             Defaults to the complete set of join keys. Up to one join key may be omitted. If one key is omitted, online requests to a Feature Service will
             return all feature vectors that match the specified join keys.
         """
         from tecton.cli import common as cli_common
 
         if isinstance(schema, list):
@@ -2019,15 +2149,15 @@
             info=info,
             feature_definition=None,
             args=args,
             source_info=source_info,
             entities=tuple(entities),
             args_supplement=None,
         )
-        internal_fco.Fco._register(self)
+        base_tecton_object._register_local_object(self)
 
     @classmethod
     @typechecked
     def _from_spec(cls, spec: specs.FeatureTableSpec, fco_container_: fco_container.FcoContainer) -> "FeatureTable":
         """Create a FeatureTable from directly from a spec. Specs are assumed valid and will not be re-validated."""
         feature_definition = feature_definition_wrapper.FeatureDefinitionWrapper(spec, fco_container_)
         info = base_tecton_object.TectonObjectInfo.from_spec(spec)
@@ -2123,14 +2253,15 @@
         :type end_time: Union[pendulum.DateTime, datetime.datetime]
         :param save: Whether to persist the DataFrame as a Dataset object. Default is False.
         :type save: bool
         :param save_as: name to save the DataFrame as.
                 If unspecified and save=True, a name will be generated.
         :type save_as: str
 
+
         Examples:
             A FeatureTable :py:mod:`ft` with join key :py:mod:`user_id`.
 
             1) :py:mod:`ft.get_historical_features(spine)` where :py:mod:`spine=pandas.Dataframe({'user_id': [1,2,3], 'date': [datetime(...), datetime(...), datetime(...)]})`
             Fetch historical features from the offline store for users 1, 2, and 3 for the specified timestamps in the spine.
 
             2) :py:mod:`ft.get_historical_features(spine, save_as='my_dataset)` where :py:mod:`spine=pandas.Dataframe({'user_id': [1,2,3], 'date': [datetime(...), datetime(...), datetime(...)]})`
@@ -2178,14 +2309,15 @@
             timestamp_key=timestamp_key,
             start_time=start_time,
             end_time=end_time,
             entities=entities,
             from_source=False,
             save=save,
             save_as=save_as,
+            mock_data_sources={},
         )
 
     @sdk_decorators.sdk_public_method
     @sdk_decorators.assert_remote_object
     def get_online_features(
         self,
         join_keys: Mapping[str, Union[int, np.int_, str, bytes]],
@@ -2209,15 +2341,16 @@
         """
         if not self._feature_definition.materialization_enabled:
             raise errors.FEATURE_TABLE_GET_ONLINE_FEATURES_FROM_DEVELOPMENT_WORKSPACE(
                 self.info.name, self.info.workspace
             )
 
         if not self._feature_definition.writes_to_online_store:
-            raise errors.UNSUPPORTED_OPERATION("get_online_features", "online=True was not set for this Feature Table.")
+            msg = "get_online_features"
+            raise errors.UNSUPPORTED_OPERATION(msg, "online=True was not set for this Feature Table.")
 
         return query_helper._QueryHelper(self.info.workspace, feature_view_name=self.info.name).get_feature_vector(
             join_keys,
             include_join_keys_in_response,
             request_context_map={},
             request_context_schema=request_context.RequestContext({}),
         )
@@ -2230,15 +2363,16 @@
         This method kicks off a materialization job to write the data into the offline and online store, depending on
         the Feature Table configuration.
 
         :param df: The Dataframe to be ingested. Has to conform to the FeatureTable schema.
         """
 
         if not self._feature_definition.materialization_enabled:
-            raise errors.UNSUPPORTED_OPERATION_IN_DEVELOPMENT_WORKSPACE("ingest")
+            msg = "ingest"
+            raise errors.UNSUPPORTED_OPERATION_IN_DEVELOPMENT_WORKSPACE(msg)
 
         get_upload_info_request = metadata_service_pb2.GetNewIngestDataframeInfoRequest(
             feature_definition_id=self._spec.id_proto
         )
         upload_info_response = metadata_service.instance().GetNewIngestDataframeInfo(get_upload_info_request)
 
         df_path = upload_info_response.df_path
@@ -2273,31 +2407,34 @@
         :param keys: The Dataframe to be deleted. Must conform to the FeatureTable join keys.
         :param online: (Optional, default=True) Whether or not to delete from the online store.
         :param offline: (Optional, default=True) Whether or not to delete from the offline store.
         :return: None if deletion job was created successfully.
         """
         is_live_workspace = internal_utils.is_live_workspace(self.info.workspace)
         if not is_live_workspace:
-            raise errors.UNSUPPORTED_OPERATION_IN_DEVELOPMENT_WORKSPACE("delete_keys")
+            msg = "delete_keys"
+            raise errors.UNSUPPORTED_OPERATION_IN_DEVELOPMENT_WORKSPACE(msg)
 
         return delete_keys_api.delete_keys(online, offline, keys, self._feature_definition)
 
     @sdk_decorators.documented_by(MaterializedFeatureView.materialization_status)
     @sdk_decorators.assert_remote_object
-    def materialization_status(self, verbose=False, limit=1000, sort_columns=None, errors_only=False):
+    def materialization_status(
+        self, verbose=False, limit=1000, sort_columns=None, errors_only=False
+    ) -> display.Displayable:
         return materialization_api.get_materialization_status_for_display(
-            self._spec.id_proto, verbose, limit, sort_columns, errors_only
+            self._spec.id_proto, self.workspace, verbose, limit, sort_columns, errors_only
         )
 
     @sdk_decorators.sdk_public_method
     @sdk_decorators.documented_by(MaterializedFeatureView.deletion_status)
     @sdk_decorators.assert_remote_object
-    def deletion_status(self, verbose=False, limit=1000, sort_columns=None, errors_only=False):
+    def deletion_status(self, verbose=False, limit=1000, sort_columns=None, errors_only=False) -> display.Displayable:
         return materialization_api.get_deletion_status_for_display(
-            self._spec.id_proto, verbose, limit, sort_columns, errors_only
+            self._spec.id_proto, self.workspace, verbose, limit, sort_columns, errors_only
         )
 
 
 @attrs.define(eq=False)
 class OnDemandFeatureView(FeatureView):
     """A Tecton On-Demand Feature View.
 
@@ -2322,14 +2459,15 @@
         owner: Optional[str] = None,
         tags: Optional[Dict[str, str]] = None,
         prevent_destroy: bool = False,
         mode: str,
         sources: List[Union[configs.RequestSource, FeatureView, "FeatureReference"]],
         schema: List[types.Field],
         feature_view_function: Callable,
+        environments: Optional[List[str]] = None,
     ):
         from tecton.cli import common as cli_common
 
         _validate_fv_input_count(sources, feature_view_function)
 
         if mode == PIPELINE_MODE:
             pipeline_function = feature_view_function
@@ -2343,22 +2481,27 @@
             def pipeline_function(**kwargs):
                 return inferred_transform(**kwargs)
 
         pipeline_root = _build_pipeline_for_odfv(name, feature_view_function, pipeline_function, sources)
 
         spark_schema_wrapper = type_utils.to_spark_schema_wrapper(schema)
 
+        on_demand_args = feature_view__args_pb2.OnDemandArgs(
+            schema=spark_schema_wrapper.to_proto(),
+            environments=environments if environments else [],
+        )
+
         args = feature_view__args_pb2.FeatureViewArgs(
             feature_view_id=id_helper.IdHelper.generate_id(),
             info=basic_info_pb2.BasicInfo(name=name, description=description, tags=tags, owner=owner),
             prevent_destroy=prevent_destroy,
             version=feature_definition_wrapper.FrameworkVersion.FWV5.value,
             pipeline=pipeline_pb2.Pipeline(root=pipeline_root.node_proto),
             feature_view_type=feature_view__args_pb2.FeatureViewType.FEATURE_VIEW_TYPE_ON_DEMAND,
-            on_demand_args=feature_view__args_pb2.OnDemandArgs(schema=spark_schema_wrapper.to_proto()),
+            on_demand_args=on_demand_args,
         )
 
         info = base_tecton_object.TectonObjectInfo.from_args_proto(args.info, args.feature_view_id)
         source_info = cli_common.construct_fco_source_info(args.feature_view_id)
 
         references_and_request_sources = []
         for source in sources:
@@ -2373,15 +2516,15 @@
             args=args,
             source_info=source_info,
             sources=tuple(references_and_request_sources),
             transformations=tuple(pipeline_root.transformations),
             args_supplement=None,
         )
 
-        internal_fco.Fco._register(self)
+        base_tecton_object._register_local_object(self)
 
     @classmethod
     @typechecked
     def _from_spec(
         cls, spec: specs.OnDemandFeatureViewSpec, fco_container_: fco_container.FcoContainer
     ) -> "FeatureView":
         """Create a FeatureView from directly from a spec. Specs are assumed valid and will not be re-validated."""
@@ -2476,22 +2619,22 @@
         """
         # Snowflake compute uses the same code for run_ondemand as Spark.
         return run_api.run_ondemand(self._feature_definition, self.info.name, mock_inputs)
 
     @sdk_decorators.sdk_public_method
     @sdk_decorators.assert_local_object(error_message=errors.CANNOT_USE_LOCAL_RUN_ON_REMOTE_OBJECT)
     def test_run(
-        self, **mock_sources: Union[Dict[str, Any], pandas.DataFrame]
+        self, **mock_inputs: Union[Dict[str, Any], pandas.DataFrame]
     ) -> Union[Dict[str, Any], pandas.DataFrame]:
         """Run the OnDemandFeatureView using mock sources.
 
         Unlike :py:func:`run`, :py:func:`test_run` is intended for unit testing. It will not make calls to your
         connected Tecton cluster to validate the OnDemandFeatureView.
 
-        :param mock_sources: Required. Keyword args with the same expected keys
+        :param mock_inputs: Required. Keyword args with the same expected keys
             as the OnDemandFeatureView's inputs parameters.
             For the "python" mode, each input must be a Dictionary representing a single row.
             For the "pandas" mode, each input must be a DataFrame with all of them containing the
             same number of rows and matching row ordering.
 
         Example:
             .. code-block:: python
@@ -2505,21 +2648,20 @@
                     return {'transaction_amount_is_high': transaction_request['amount'] > 10000}
 
                 # Test using `run` API.
                 result = transaction_amount_is_high.test_run(transaction_request={'amount': 100})
 
         :return: A `Dict` object for the "python" mode and a `pandas.DataFrame` object for the "pandas" mode".
         """
-        run_api.validate_ondemand_mock_inputs_keys(mock_sources, self._args.pipeline)
+        # TODO(adchia): Validate batch feature inputs here against BFV schema
+        run_api.validate_ondemand_mock_inputs(mock_inputs, self._args.pipeline)
 
         transformation_specs = [transformation._create_unvalidated_spec() for transformation in self.transformations]
 
-        return pipeline_helper.run_mock_odfv_pipeline(
-            self._args.pipeline, transformation_specs, self.name, mock_sources
-        )
+        return pipeline_helper.run_mock_odfv_pipeline(self._args.pipeline, transformation_specs, self.name, mock_inputs)
 
     @sdk_decorators.sdk_public_method(requires_validation=True)
     def get_historical_features(
         self,
         spine: Union[pyspark_dataframe.DataFrame, pandas.DataFrame, tecton_dataframe.TectonDataFrame, str],
         timestamp_key: Optional[str] = None,
         from_source: Optional[bool] = None,
@@ -2608,14 +2750,15 @@
             timestamp_key=timestamp_key,
             start_time=None,
             end_time=None,
             entities=None,
             from_source=from_source,
             save=save,
             save_as=save_as,
+            mock_data_sources={},
         )
 
     @sdk_decorators.sdk_public_method
     @sdk_decorators.assert_remote_object
     def get_online_features(
         self,
         join_keys: Optional[Mapping[str, Union[int, np.int_, str, bytes]]] = None,
@@ -2687,29 +2830,30 @@
     description: Optional[str] = None,
     owner: Optional[str] = None,
     tags: Optional[Dict[str, str]] = None,
     prevent_destroy: bool = False,
     mode: str,
     sources: List[Union[configs.RequestSource, FeatureView, "FeatureReference"]],
     schema: List[types.Field],
+    environments: Optional[List[str]] = None,
 ):
     """
     Declare an On-Demand Feature View
 
     :param mode: Whether the annotated function is a pipeline function ("pipeline" mode) or a transformation function ("python" or "pandas" mode).
         For the non-pipeline mode, an inferred transformation will also be registered.
     :param sources: The data source inputs to the feature view. An input can be a RequestSource or a materialized Feature View.
     :param schema: Tecton schema matching the expected output of the transformation.
     :param description: A human readable description.
     :param owner: Owner name (typically the email of the primary maintainer).
     :param tags: Tags associated with this Tecton Object (key-value pairs of arbitrary metadata).
     :param name: Unique, human friendly name that identifies the FeatureView. Defaults to the function name.
     :param prevent_destroy: If True, this Tecton object will be blocked from being deleted or re-created (i.e. a
         destructive update) during tecton plan/apply. To remove or update this object, ``prevent_destroy`` must be
-        first set to False via a separate tecton apply. ``prevent_destroy`` can be used to prevent accidental changes
+        set to False via the same tecton apply or a separate tecton apply. ``prevent_destroy`` can be used to prevent accidental changes
         such as inadvertantly deleting a Feature Service used in production or recreating a Feature View that
         triggers expensive rematerialization jobs. ``prevent_destroy`` also blocks changes to dependent Tecton objects
         that would trigger a recreate of the tagged object, e.g. if ``prevent_destroy`` is set on a Feature Service,
         that will also prevent deletions or re-creates of Feature Views used in that service. ``prevent_destroy`` is
         only enforced in live (i.e. non-dev) workspaces.
     :return: An object of type :class:`tecton.OnDemandFeatureView`.
 
@@ -2782,14 +2926,15 @@
             owner=owner,
             tags=tags,
             prevent_destroy=prevent_destroy,
             mode=mode,
             sources=sources,
             schema=schema,
             feature_view_function=feature_view_function,
+            environments=environments,
         )
 
     return decorator
 
 
 @attrs.define
 class FeatureReference:
@@ -2845,15 +2990,15 @@
             namespace=namespace,
             features=features,
             override_join_keys=override_join_keys,
         )
 
     @property
     def id(self) -> id_pb2.Id:
-        return self.feature_definition._id
+        return self.feature_definition._id_proto
 
     @sdk_decorators.documented_by(FeatureView.with_name)
     def with_name(self, namespace: str) -> "FeatureReference":
         self.namespace = namespace
         return self
 
     @sdk_decorators.documented_by(FeatureView.with_join_key_map)
@@ -2940,24 +3085,25 @@
             return BatchFeatureView._from_spec(feature_view_spec, fco_container_)
         if feature_view_spec.data_source_type in (
             data_source_type_pb2.DataSourceType.STREAM_WITH_BATCH,
             data_source_type_pb2.DataSourceType.PUSH_WITH_BATCH,
             data_source_type_pb2.DataSourceType.PUSH_NO_BATCH,
         ):
             return StreamFeatureView._from_spec(feature_view_spec, fco_container_)
-    raise errors.INTERNAL_ERROR("Missing or unsupported FeatureView type.")
+    msg = "Missing or unsupported FeatureView type."
+    raise errors.INTERNAL_ERROR(msg)
 
 
 def _build_odfv_sources_from_spec(
     spec: specs.OnDemandFeatureViewSpec, fco_container_: fco_container.FcoContainer
 ) -> Tuple[Union[FeatureReference, configs.RequestSource], ...]:
     sources = []
     request_context = pipeline_common.find_request_context(spec.pipeline.root)
     if request_context is not None:
-        request_schema = spark_api.get_request_schema_from_spark_schema(request_context.schema)
+        request_schema = spark_api.get_request_schema_from_tecton_schema(request_context.tecton_schema)
         sources.append(configs.RequestSource(schema=request_schema))
 
     feature_view_nodes = pipeline_common.get_all_feature_view_nodes(spec.pipeline)
     for node in feature_view_nodes:
         fv_spec = fco_container_.get_by_id_proto(node.feature_view_node.feature_view_id)
         fv = feature_view_from_spec(fv_spec, fco_container_)
         override_join_keys = {
@@ -2996,41 +3142,38 @@
 ) -> framework_transformation.PipelineNodeWrapper:
     if isinstance(source, FeatureView):
         source = FeatureReference(feature_definition=source)
 
     pipeline_node = pipeline_pb2.PipelineNode()
     if isinstance(source, framework_data_source.DataSource):
         node = pipeline_pb2.DataSourceNode(
-            virtual_data_source_id=source._id,
+            virtual_data_source_id=source._id_proto,
             window_unbounded=True,
             schedule_offset=time_utils.timedelta_to_proto(source.data_delay),
             input_name=input_name,
         )
         pipeline_node.data_source_node.CopyFrom(node)
     elif isinstance(source, filtered_source.FilteredSource):
         node = pipeline_pb2.DataSourceNode(
-            virtual_data_source_id=source.source._id,
+            virtual_data_source_id=source.source._id_proto,
             schedule_offset=time_utils.timedelta_to_proto(source.source.data_delay),
             input_name=input_name,
         )
         if source.start_time_offset <= MIN_START_OFFSET:
             node.window_unbounded_preceding = True
         else:
             node.start_time_offset.FromTimedelta(source.start_time_offset)
 
         pipeline_node.data_source_node.CopyFrom(node)
     elif isinstance(source, configs.RequestSource):
         request_schema = source.schema
-        if isinstance(request_schema, List):
-            wrapper = type_utils.to_spark_schema_wrapper(request_schema)
-        else:
-            wrapper = spark_schema_wrapper.SparkSchemaWrapper(request_schema)
+        schema_proto = type_utils.to_tecton_schema(request_schema)
 
         node = pipeline_pb2.RequestDataSourceNode(
-            request_context=pipeline_pb2.RequestContext(schema=wrapper.to_proto()),
+            request_context=pipeline_pb2.RequestContext(tecton_schema=schema_proto),
             input_name=input_name,
         )
         pipeline_node.request_data_source_node.CopyFrom(node)
     elif isinstance(source, FeatureReference):
         override_join_keys = None
         if source.override_join_keys:
             override_join_keys = [
@@ -3045,15 +3188,16 @@
                 override_join_keys=override_join_keys,
                 namespace=source.namespace,
                 features=source.features,
             ),
         )
         pipeline_node.feature_view_node.CopyFrom(node)
     else:
-        raise TypeError(f"Invalid source type: {type(source)}")
+        msg = f"Invalid source type: {type(source)}"
+        raise TypeError(msg)
     return framework_transformation.PipelineNodeWrapper(node_proto=pipeline_node)
 
 
 def _transformation_to_pipeline_node(
     pipeline_function: Callable,
     params_to_sources: Dict[
         str,
@@ -3073,15 +3217,16 @@
 
 def _test_binding_user_function(fn, inputs):
     # this function binds the top-level pipeline function only, for transformation binding, see transformation.__call__
     pipeline_signature = inspect.signature(fn)
     try:
         pipeline_signature.bind(**inputs)
     except TypeError as e:
-        raise TypeError(f"while binding inputs to pipeline function, TypeError: {e}")
+        msg = f"while binding inputs to pipeline function, TypeError: {e}"
+        raise TypeError(msg)
 
 
 def _sources_to_pipeline_nodes(
     params_to_sources: Dict[
         str,
         Union[
             framework_data_source.DataSource,
@@ -3133,51 +3278,56 @@
     offline: bool,
     offline_store: Union[configs.ParquetConfig, configs.DeltaConfig],
     online_store: Optional[Union[configs.DynamoConfig, configs.RedisConfig]],
     aggregation_interval: Optional[datetime.timedelta],
     aggregations: Optional[Sequence[configs.Aggregation]],
     ttl: Optional[datetime.timedelta],
     feature_start_time: Optional[datetime.datetime],
+    manual_trigger_backfill_end_time: Optional[datetime.datetime],
     batch_schedule: Optional[datetime.timedelta],
     online_serving_index: Optional[Sequence[str]],
     batch_compute: Optional[
         Union[
             configs.DatabricksClusterConfig,
             configs.EMRClusterConfig,
             configs.DatabricksJsonClusterConfig,
+            configs.DataprocJsonClusterConfig,
             configs.EMRJsonClusterConfig,
         ]
     ],
     stream_compute: Optional[
         Union[
             configs.DatabricksClusterConfig,
             configs.EMRClusterConfig,
             configs.DatabricksJsonClusterConfig,
+            configs.DataprocJsonClusterConfig,
             configs.EMRJsonClusterConfig,
         ]
     ],
     monitor_freshness: bool,
+    data_quality_enabled: Optional[bool],
+    skip_default_expectations: Optional[bool],
     expected_feature_freshness: Optional[datetime.timedelta],
     alert_email: Optional[str],
     description: Optional[str],
     owner: Optional[str],
     tags: Optional[Dict[str, str]],
     feature_view_type: feature_view__args_pb2.FeatureViewType.ValueType,
     timestamp_field: Optional[str],
     data_source_type: data_source_type_pb2.DataSourceType.ValueType,
     incremental_backfills: bool,
     prevent_destroy: bool,
     stream_processing_mode: Optional[StreamProcessingMode] = None,
-    max_batch_aggregation_interval: Optional[datetime.timedelta] = None,
+    max_backfill_interval: Optional[datetime.timedelta] = None,
     output_stream: Optional[configs.OutputStream] = None,
     batch_trigger: Optional[BatchTriggerType] = None,
     schema: Optional[List[types.Field]] = None,
 ) -> feature_view__args_pb2.FeatureViewArgs:
     """Build feature view args proto for materialized feature views (i.e. batch and stream feature views)."""
-    batch_compute_proto = batch_compute._to_cluster_proto() if batch_compute else None
+    batch_compute_proto = batch_compute._to_cluster_proto() if batch_compute else _build_default_cluster_config()
 
     stream_compute_proto = None
     if stream_compute:
         stream_compute_proto = stream_compute._to_cluster_proto()
     elif data_source_type == DataSourceType.STREAM_WITH_BATCH:
         stream_compute_proto = _build_default_cluster_config()
 
@@ -3188,36 +3338,40 @@
         is_continuous = stream_processing_mode == StreamProcessingMode.CONTINUOUS
         if aggregation_interval is None:
             aggregation_interval = datetime.timedelta(seconds=0)
 
         aggregation_protos = [agg._to_proto(aggregation_interval, is_continuous) for agg in aggregations]
 
     schema_proto = type_utils.to_tecton_schema(schema) if schema else None
-
     return feature_view__args_pb2.FeatureViewArgs(
         feature_view_id=id_helper.IdHelper.generate_id(),
         version=feature_definition_wrapper.FrameworkVersion.FWV5.value,
         info=basic_info_pb2.BasicInfo(name=name, description=description, owner=owner, tags=tags),
         prevent_destroy=prevent_destroy,
         entities=[
-            feature_view__args_pb2.EntityKeyOverride(entity_id=entity._id, join_keys=entity.join_keys)
+            feature_view__args_pb2.EntityKeyOverride(entity_id=entity._id_proto, join_keys=entity.join_keys)
             for entity in entities
         ],
         pipeline=pipeline,
         feature_view_type=feature_view_type,
         online_serving_index=online_serving_index if online_serving_index else None,
         online_enabled=online,
         offline_enabled=offline,
+        data_quality_config=feature_view__args_pb2.DataQualityConfig(
+            data_quality_enabled=data_quality_enabled,
+            skip_default_expectations=skip_default_expectations,
+        ),
         materialized_feature_view_args=feature_view__args_pb2.MaterializedFeatureViewArgs(
             timestamp_field=timestamp_field,
             feature_start_time=time_utils.datetime_to_proto(feature_start_time),
+            manual_trigger_backfill_end_time=time_utils.datetime_to_proto(manual_trigger_backfill_end_time),
             batch_schedule=time_utils.timedelta_to_proto(batch_schedule),
             offline_store=offline_store._to_proto(),
             online_store=online_store._to_proto() if online_store else None,
-            max_batch_aggregation_interval=time_utils.timedelta_to_proto(max_batch_aggregation_interval),
+            max_backfill_interval=time_utils.timedelta_to_proto(max_backfill_interval),
             monitoring=monitoring._to_proto() if monitoring else None,
             data_source_type=data_source_type,
             incremental_backfills=incremental_backfills,
             batch_trigger=batch_trigger.value,
             output_stream=output_stream._to_proto() if output_stream else None,
             batch_compute=batch_compute_proto,
             stream_compute=stream_compute_proto,
```

### Comparing `tecton-0.7.0b9/tecton/framework/filtered_source.py` & `tecton-0.7.0rc0/tecton/framework/filtered_source.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/framework/transformation.py` & `tecton-0.7.0rc0/tecton/framework/transformation.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,15 +9,14 @@
 import pandas as pd
 import pyspark
 from google.protobuf import empty_pb2
 from typeguard import typechecked
 
 from tecton._internals import display
 from tecton._internals import errors
-from tecton._internals import fco as internal_fco
 from tecton._internals import metadata_service
 from tecton._internals import sdk_decorators
 from tecton._internals import spark_api
 from tecton._internals import validations_api
 from tecton._internals.repo import function_serialization
 from tecton.cli import common as cli_common
 from tecton.framework import base_tecton_object
@@ -36,15 +35,14 @@
 from tecton_proto.validation import validator_pb2
 
 
 SPARK_SQL_MODE = "spark_sql"
 PYSPARK_MODE = "pyspark"
 SNOWFLAKE_SQL_MODE = "snowflake_sql"
 SNOWPARK_MODE = "snowpark"
-ATHENA_MODE = "athena"
 PANDAS_MODE = "pandas"
 PYTHON_MODE = "python"
 
 
 class Constant:
     """
     Wraps a const value that can be used as arguments to a Pipeline functions.
@@ -128,15 +126,15 @@
             node=input.node_proto,
         )
         self.node_proto.transformation_node.inputs.append(input_proto)
         self.transformations.update(input.transformations)
 
 
 @attrs.define(eq=False)
-class Transformation(base_tecton_object.BaseTectonObject, internal_fco.Fco):
+class Transformation(base_tecton_object.BaseTectonObject):
     """A Tecton Transformation. Transformations are used encapsulate and share transformation logic between Feature Views.
 
     Use the :py:func:`tecton.transformation` decorator to create a Transformation.
     """
 
     # A Tecton "args" proto. Only set if this object was defined locally, i.e. this object was not applied
     # and fetched from the Tecton backend.
@@ -195,15 +193,15 @@
         self.__attrs_init__(
             info=info,
             spec=None,
             args=args,
             source_info=source_info,
             user_function=user_function,
         )
-        internal_fco.Fco._register(self)
+        base_tecton_object._register_local_object(self)
 
     @classmethod
     @typechecked
     def _from_spec(cls, spec: specs.TransformationSpec) -> "Transformation":
         """Create a Transformation from directly from a spec. Specs are assumed valid and will not be re-validated."""
         info = base_tecton_object.TectonObjectInfo.from_spec(spec)
         obj = cls.__new__(cls)  # Instantiate the object. Does not call init.
@@ -218,15 +216,15 @@
         if self.info._is_local_object:
             return validator_pb2.FcoValidationArgs(
                 transformation=validator_pb2.TransformationValidationArgs(
                     args=self._args,
                 )
             )
         else:
-            return validator_pb2.FcoValidationArgs(transformation=self._spec.data_proto.validation_args)
+            return self._spec.validation_args
 
     @property
     def _is_valid(self) -> bool:
         return self._spec is not None
 
     def __call__(self, *args, **kwargs) -> PipelineNodeWrapper:
         """Override the user defined transformation function.
@@ -239,15 +237,16 @@
             transformations=set([self]),
         )
         user_function = self._spec.user_function if self._spec is not None else self._user_function
 
         try:
             bound_user_function = signature(user_function).bind(*args, **kwargs)
         except TypeError as e:
-            raise TypeError(f"while binding inputs to function {self.info.name}, TypeError: {e}")
+            msg = f"while binding inputs to function {self.info.name}, TypeError: {e}"
+            raise TypeError(msg)
 
         materialization_context_count = 0
         # Construct input nodes from args for the user function
         for i, arg in enumerate(args):
             input_node_wrapper = PipelineNodeWrapper.create_from_arg(arg, self.info.name)
             node_wrapper.add_transformation_input(input_node_wrapper, arg_index=i)
             if isinstance(arg, materialization_context.UnboundMaterializationContext):
@@ -266,20 +265,20 @@
                 if param.name in bound_user_function.arguments:
                     # the user passed in context explicitly, so no need to double register
                     continue
                 input_node_wrapper = PipelineNodeWrapper.create_from_arg(param.default, self.info.name)
                 node_wrapper.add_transformation_input(input_node_wrapper, arg_name=param.name)
                 materialization_context_count += 1
             elif param.default is materialization_context:
-                raise Exception(
-                    "It seems you passed in tecton.materialization_context. Did you mean tecton.materialization_context()?"
-                )
+                msg = "It seems you passed in tecton.materialization_context. Did you mean tecton.materialization_context()?"
+                raise Exception(msg)
 
         if materialization_context_count > 1:
-            raise Exception(f"Only 1 materialization_context can be passed into transformation {self.info.name}")
+            msg = f"Only 1 materialization_context can be passed into transformation {self.info.name}"
+            raise Exception(msg)
 
         return node_wrapper
 
     @property
     def transformer(self):
         """The user function for this transformation."""
         if self._spec is None:
@@ -296,15 +295,16 @@
             indentation_level,
         )
         self._spec = specs.TransformationSpec.from_args_proto(self._args, self._user_function)
 
     def _on_demand_run(self, *inputs: pd.DataFrame) -> data_frame.TectonDataFrame:
         for df in inputs:
             if not isinstance(df, pd.DataFrame):
-                raise TypeError(f"Input must be of type pandas.DataFrame, but was {type(df)}.")
+                msg = f"Input must be of type pandas.DataFrame, but was {type(df)}."
+                raise TypeError(msg)
 
         return data_frame.TectonDataFrame._create(self.transformer(*inputs))
 
     @sdk_decorators.sdk_public_method
     @typechecked
     def run(
         self,
@@ -330,15 +330,16 @@
             return spark_api.run_transformation_mode_spark_sql(
                 *inputs, transformer=self.transformer, context=context, transformation_name=self.info.name
             )
         elif transformation_mode == transformation__args_proto.TransformationMode.TRANSFORMATION_MODE_PYSPARK:
             return spark_api.run_transformation_mode_pyspark(*inputs, transformer=self.transformer, context=context)
         elif transformation_mode == transformation__args_proto.TransformationMode.TRANSFORMATION_MODE_PANDAS:
             return self._on_demand_run(*inputs)
-        raise RuntimeError(f"{transformation_mode} does not support `run(...)`")
+        msg = f"{transformation_mode} does not support `run(...)`"
+        raise RuntimeError(msg)
 
     @sdk_decorators.sdk_public_method
     @sdk_decorators.assert_remote_object
     def summary(self):
         """Displays a human readable summary of this Transformation."""
         request = metadata_service_pb2.GetTransformationSummaryRequest(
             fco_locator=fco_locator_pb2.FcoLocator(id=self._spec.id_proto, workspace=self._spec.workspace)
@@ -346,35 +347,29 @@
         response = metadata_service.instance().GetTransformationSummary(request)
         return display.Displayable.from_fco_summary(response.fco_summary)
 
     @sdk_decorators.assert_local_object
     def _create_unvalidated_spec(self) -> specs.TransformationSpec:
         """Create an unvalidated spec. Used for user unit testing, where backend validation is unavailable."""
         if not self._args.HasField("user_function"):
-            raise core_errors.TectonValidationError(
-                f"test_run() requires using a fully serializable Tecton object to ensure testing accuracy, but the"
-                f" Python code for Transformation '{self.name}' was not successfully serialized. This is probably"
-                f" because the object was defined outside in a Tecton repo (e.g. in a notebook or repl). The object"
-                f" should be defined in a *.py file under a directory containing a .tecton file."
-            )
+            msg = f"test_run() requires using a fully serializable Tecton object to ensure testing accuracy, but the Python code for Transformation '{self.name}' was not successfully serialized. This is probably because the object was defined outside in a Tecton repo (e.g. in a notebook or repl). The object should be defined in a *.py file under a directory containing a .tecton file."
+            raise core_errors.TectonValidationError(msg)
 
         return specs.TransformationSpec.from_args_proto(self._args, user_function=None)
 
 
 def _get_transformation_mode_enum(mode: str, name: str) -> transformation__args_proto.TransformationMode.ValueType:
     if mode == SPARK_SQL_MODE:
         return transformation__args_proto.TransformationMode.TRANSFORMATION_MODE_SPARK_SQL
     elif mode == PYSPARK_MODE:
         return transformation__args_proto.TransformationMode.TRANSFORMATION_MODE_PYSPARK
     elif mode == SNOWFLAKE_SQL_MODE:
         return transformation__args_proto.TransformationMode.TRANSFORMATION_MODE_SNOWFLAKE_SQL
     elif mode == SNOWPARK_MODE:
         return transformation__args_proto.TransformationMode.TRANSFORMATION_MODE_SNOWPARK
-    elif mode == ATHENA_MODE:
-        return transformation__args_proto.TransformationMode.TRANSFORMATION_MODE_ATHENA
     elif mode == PANDAS_MODE:
         return transformation__args_proto.TransformationMode.TRANSFORMATION_MODE_PANDAS
     elif mode == PYTHON_MODE:
         return transformation__args_proto.TransformationMode.TRANSFORMATION_MODE_PYTHON
     else:
         raise errors.InvalidTransformationMode(
             name,
@@ -390,22 +385,22 @@
     description: Optional[str] = None,
     owner: Optional[str] = None,
     tags: Optional[Dict[str, str]] = None,
     prevent_destroy: bool = False,
 ):
     """Declares a Transformation that wraps a user function. Transformations are assembled in a pipeline function of a Feature View.
 
-    :param mode: The mode for this transformation must be one of "spark_sql", "pyspark", "snowflake_sql", "snowpark", "athena", "pandas" or "python".
+    :param mode: The mode for this transformation must be one of "spark_sql", "pyspark", "snowflake_sql", "snowpark", "pandas" or "python".
     :param name: Unique, human friendly name that identifies the Transformation. Defaults to the function name.
     :param description: A human readable description.
     :param owner: Owner name (typically the email of the primary maintainer).
     :param tags: Tags associated with this Tecton Object (key-value pairs of arbitrary metadata).
     :param prevent_destroy: If True, this Tecton object will be blocked from being deleted or re-created (i.e. a
-        destructive update) during tecton plan/apply. To remove or update this object, `prevent_destroy` must be
-        first set to False via a separate tecton apply. `prevent_destroy` can be used to prevent accidental changes
+        destructive update) during tecton plan/apply. To remove or update this object, `prevent_destroy` must be set to
+        False via the same tecton apply or a separate tecton apply. `prevent_destroy` can be used to prevent accidental changes
         such as inadvertantly deleting a Feature Service used in production or recreating a Feature View that
         triggers expensive rematerialization jobs. `prevent_destroy` also blocks changes to dependent Tecton objects
         that would trigger a recreate of the tagged object, e.g. if `prevent_destroy` is set on a Feature Service,
         that will also prevent deletions or re-creates of Feature Views used in that service. `prevent_destroy` is
         only enforced in live (i.e. non-dev) workspaces.
     :return: A wrapped transformation
 
@@ -424,25 +419,14 @@
                 return f'''
                     SELECT
                         *,
                         split({column_to_split}, {delimiter}) AS {new_column_name}
                     FROM {input_data}
                 '''
 
-             # Create an Athena transformation.
-             @transformation(mode="athena",
-                             description="Create new column by splitting the string in an existing column")
-             def str_split(input_data, column_to_split, new_column_name, delimiter):
-                 return f'''
-                     SELECT
-                         *,
-                         split({column_to_split}, '{delimiter}') AS {new_column_name}
-                     FROM {input_data}
-                 '''
-
             # Create a PySpark transformation.
             @transformation(mode="pyspark",
                             description="Add a new column 'user_has_good_credit' if score is > 670")
             def user_has_good_credit_transformation(credit_scores):
                 from pyspark.sql import functions as F
 
                 (df = credit_scores.withColumn("user_has_good_credit",
```

### Comparing `tecton-0.7.0b9/tecton/framework/utils.py` & `tecton-0.7.0rc0/tecton/framework/utils.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/framework/validation_mode.py` & `tecton-0.7.0rc0/tecton/framework/validation_mode.py`

 * *Files 5% similar despite different names*

```diff
@@ -2,14 +2,15 @@
 
 from tecton_core import conf
 
 
 class ValidationMode(str, Enum):
     EXPLICIT = "explicit"
     AUTOMATIC = "auto"
+    SKIP = "skip"
 
 
 def set_validation_mode(mode: ValidationMode):
     """Convenience utility to set the Tecton object validation mode for the lifetime of the Python process.
 
     Must be either "explicit" (tecton.ValidationMode.EXPLICIT) or "auto" (tecton.ValidationMode.AUTOMATIC). "explicit"
     is the default.
@@ -44,16 +45,22 @@
         )
 
         credit_scores_batch.validate()
 
         df = credit_scores_batch.get_dataframe()
 
 
+    In "skip" mode, some methods like `run` and `get_historical_features` can
+    skip backend validation. This is primarily used for unit testing and is
+    typically automatically configured by Tecton.
+
     Note: Tecton objects fetched from the Tecton backend have already been validated during `tecton plan` and do
     not need to be re-validated.
     """
     if mode is None or mode.lower() not in (
         ValidationMode.AUTOMATIC,
         ValidationMode.EXPLICIT,
+        ValidationMode.SKIP,
     ):
-        raise ValueError(f"Mode should be one of 'auto' or 'explicit', got {mode}")
+        msg = f"Mode should be one of 'auto' or 'explicit', got {mode}"
+        raise ValueError(msg)
     conf.set("TECTON_VALIDATION_MODE", mode)
```

### Comparing `tecton-0.7.0b9/tecton/framework/workspace.py` & `tecton-0.7.0rc0/tecton/framework/workspace.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,7 +1,8 @@
+import logging
 from typing import List
 from typing import Optional
 from typing import Union
 
 from tecton._internals import errors
 from tecton._internals import metadata_service
 from tecton._internals.display import Displayable
@@ -12,18 +13,20 @@
 from tecton._internals.utils import is_live_workspace
 from tecton.framework import data_source as framework_data_source
 from tecton.framework import entity as framework_entity
 from tecton.framework import feature_service as framework_feature_service
 from tecton.framework import feature_view as framework_feature_view
 from tecton.framework import transformation as framework_transformation
 from tecton.framework.dataset import Dataset
+from tecton_core import conf
 from tecton_core import specs
 from tecton_core.fco_container import FcoContainer
 from tecton_core.feature_definition_wrapper import FrameworkVersion
-from tecton_core.logger import get_logger
+from tecton_proto.data import fco_pb2
+from tecton_proto.metadataservice.metadata_service_pb2 import ArchiveSavedFeatureDataFrameRequest
 from tecton_proto.metadataservice.metadata_service_pb2 import GetAllEntitiesRequest
 from tecton_proto.metadataservice.metadata_service_pb2 import GetAllFeatureServicesRequest
 from tecton_proto.metadataservice.metadata_service_pb2 import GetAllSavedFeatureDataFramesRequest
 from tecton_proto.metadataservice.metadata_service_pb2 import GetAllTransformationsRequest
 from tecton_proto.metadataservice.metadata_service_pb2 import GetAllVirtualDataSourcesRequest
 from tecton_proto.metadataservice.metadata_service_pb2 import GetEntityRequest
 from tecton_proto.metadataservice.metadata_service_pb2 import GetFeatureServiceRequest
@@ -31,15 +34,15 @@
 from tecton_proto.metadataservice.metadata_service_pb2 import GetSavedFeatureDataFrameRequest
 from tecton_proto.metadataservice.metadata_service_pb2 import GetTransformationRequest
 from tecton_proto.metadataservice.metadata_service_pb2 import GetVirtualDataSourceRequest
 from tecton_proto.metadataservice.metadata_service_pb2 import ListWorkspacesRequest
 from tecton_proto.metadataservice.metadata_service_pb2 import QueryFeatureViewsRequest
 
 
-logger = get_logger("Workspace")
+logger = logging.getLogger(__name__)
 
 
 class Workspace:
     """
     Workspace class.
 
     This class represents a Workspace. The Workspace class is used to fetch Tecton Objects, which are stored in a Workspace.
@@ -122,15 +125,15 @@
     @sdk_public_method
     # TODO(TEC-13678): consolidate with tecton.list_workspaces() and remove this
     def get_all(self) -> List["Workspace"]:
         """Deprecated. Use tecton.list_workspaces() instead. Returns a list of all registered Workspaces.
 
         :return: A list of Workspace objects.
         """
-        logger.warn("get_all() is deprecated. Use tecton.list_workspaces() instead.")
+        logger.warning("get_all() is deprecated. Use tecton.list_workspaces() instead.")
         request = ListWorkspacesRequest()
         response = metadata_service.instance().ListWorkspaces(request)
         workspaces = [
             Workspace(ws.name, _is_live=ws.capabilities.materializable, _validate=False) for ws in response.workspaces
         ]
 
         # Return live workspaces first (alphabetical), then development workspaces.
@@ -163,77 +166,83 @@
 
     @sdk_public_method
     def get_feature_view(self, name: str) -> framework_feature_view.FeatureView:
         """Returns a Feature View that has been applied to a workspace.
 
         :param name: The name of the Feature View to retrieve.
         """
-        request = GetFeatureViewRequest(version_specifier=name, workspace=self.workspace)
+
+        request = GetFeatureViewRequest(
+            version_specifier=name,
+            workspace=self.workspace,
+            run_object_version_check=not conf.get_bool("SKIP_OBJECT_VERSION_CHECK"),
+        )
         response = metadata_service.instance().GetFeatureView(request)
         fco_container = FcoContainer.from_proto(response.fco_container)
         feature_view_spec = fco_container.get_single_root()
 
         if feature_view_spec is None:
-            raise errors.TectonValidationError(
-                f"Feature View '{name}' not found. Try running `workspace.list_feature_views()` to view all registered Feature Views."
-            )
+            msg = f"Feature View '{name}' not found. Try running `workspace.list_feature_views()` to view all registered Feature Views."
+            raise errors.TectonValidationError(msg)
 
         assert isinstance(feature_view_spec, specs.FeatureViewSpec)
 
         if isinstance(feature_view_spec, specs.FeatureTableSpec):
-            raise errors.TectonValidationError(
-                f"Feature View '{name}' not found. Did you mean workspace.get_feature_table('{name}')?"
-            )
+            msg = f"Feature View '{name}' not found. Did you mean workspace.get_feature_table('{name}')?"
+            raise errors.TectonValidationError(msg)
 
         return framework_feature_view.feature_view_from_spec(feature_view_spec, fco_container)
 
     @sdk_public_method
     def get_feature_table(self, name: str) -> framework_feature_view.FeatureTable:
         """Returns a Feature Table that has been applied to a workspace.
 
         :param name: The name of the Feature Table to retrieve.
         """
-        request = GetFeatureViewRequest(version_specifier=name, workspace=self.workspace)
+        request = GetFeatureViewRequest(
+            version_specifier=name,
+            workspace=self.workspace,
+            run_object_version_check=not conf.get_bool("SKIP_OBJECT_VERSION_CHECK"),
+        )
         response = metadata_service.instance().GetFeatureView(request)
         fco_container = FcoContainer.from_proto(response.fco_container)
         feature_table_spec = fco_container.get_single_root()
 
         if feature_table_spec is None:
-            raise errors.TectonValidationError(
-                f"Feature Table '{name}' not found. Try running `workspace.list_feature_tables()` to view all registered Feature Tables."
-            )
+            msg = f"Feature Table '{name}' not found. Try running `workspace.list_feature_tables()` to view all registered Feature Tables."
+            raise errors.TectonValidationError(msg)
 
         assert isinstance(feature_table_spec, specs.FeatureViewSpec)
-        ft_proto = feature_table_spec.data_proto
+        if not isinstance(feature_table_spec, specs.FeatureTableSpec):
+            msg = f"Feature Table '{name}' not found. Did you mean workspace.get_feature_view('{name}')?"
+            raise errors.TectonValidationError(msg)
 
-        if not ft_proto.HasField("feature_table"):
-            raise errors.TectonValidationError(
-                f"Feature Table '{name}' not found. Did you mean workspace.get_feature_view('{name}')?"
-            )
-
-        if ft_proto.fco_metadata.framework_version != FrameworkVersion.FWV5.value:
+        if feature_table_spec.metadata.framework_version != FrameworkVersion.FWV5.value:
             raise errors.UNSUPPORTED_FRAMEWORK_VERSION
 
         return framework_feature_view.FeatureTable._from_spec(feature_table_spec, fco_container)
 
     @sdk_public_method
     def get_feature_service(self, name: str) -> framework_feature_service.FeatureService:
         """Returns a Feature Service that has been applied to a workspace.
 
         :param name: The name of the Feature Service to retrieve.
         """
-        request = GetFeatureServiceRequest(service_reference=name, workspace=self.workspace)
+        request = GetFeatureServiceRequest(
+            service_reference=name,
+            workspace=self.workspace,
+            run_object_version_check=not conf.get_bool("SKIP_OBJECT_VERSION_CHECK"),
+        )
         response = metadata_service.instance().GetFeatureService(request)
         fco_container = FcoContainer.from_proto(response.fco_container)
         feature_service_spec = fco_container.get_single_root()
 
         if feature_service_spec is None:
-            raise errors.TectonValidationError(
-                f"Feature Service '{name}' not found. Try running `workspace.list_feature_services()` to view all registered Feature Services."
-            )
+            msg = f"Feature Service '{name}' not found. Try running `workspace.list_feature_services()` to view all registered Feature Services."
+            raise errors.TectonValidationError(msg)
 
         assert isinstance(feature_service_spec, specs.FeatureServiceSpec)
 
         return framework_feature_service.FeatureService._from_spec(feature_service_spec, fco_container)
 
     @sdk_public_method
     def get_data_source(
@@ -243,69 +252,77 @@
         framework_data_source.BatchSource,
         framework_data_source.StreamSource,
     ]:
         """Returns a Data Source that has been applied to a workspace.
 
         :param name: The name of the Data Source to retrieve.
         """
-        request = GetVirtualDataSourceRequest(name=name, workspace=self.workspace)
+        request = GetVirtualDataSourceRequest(
+            name=name,
+            workspace=self.workspace,
+            run_object_version_check=not conf.get_bool("SKIP_OBJECT_VERSION_CHECK"),
+        )
         response = metadata_service.instance().GetVirtualDataSource(request)
         fco_container = FcoContainer.from_proto(response.fco_container)
         data_source_spec = fco_container.get_single_root()
 
         if data_source_spec is None:
-            raise errors.TectonValidationError(
-                f"Data Source '{name}' not found. Try running `workspace.list_data_sources()` to view all registered Data Sources."
-            )
+            msg = f"Data Source '{name}' not found. Try running `workspace.list_data_sources()` to view all registered Data Sources."
+            raise errors.TectonValidationError(msg)
 
         assert isinstance(data_source_spec, specs.DataSourceSpec)
-        ds_proto = data_source_spec.data_proto
 
-        if ds_proto.fco_metadata.framework_version != FrameworkVersion.FWV5.value:
+        if data_source_spec.metadata.framework_version != FrameworkVersion.FWV5.value:
             raise errors.UNSUPPORTED_FRAMEWORK_VERSION
 
         return framework_data_source.data_source_from_spec(data_source_spec)
 
     @sdk_public_method
     def get_entity(self, name: str) -> framework_entity.Entity:
         """Returns an Entity that has been applied to a workspace.
 
         :param name: The name of the Entity to retrieve.
         """
 
-        request = GetEntityRequest(name=name, workspace=self.workspace)
+        request = GetEntityRequest(
+            name=name,
+            workspace=self.workspace,
+            run_object_version_check=not conf.get_bool("SKIP_OBJECT_VERSION_CHECK"),
+        )
         response = metadata_service.instance().GetEntity(request)
 
         fco_container = FcoContainer.from_proto(response.fco_container)
         entity_spec = fco_container.get_single_root()
 
         if entity_spec is None:
-            raise errors.TectonValidationError(
-                f"Entity '{name}' not found. Try running `workspace.list_entities()` to view all registered Entities."
-            )
+            msg = f"Entity '{name}' not found. Try running `workspace.list_entities()` to view all registered Entities."
+            raise errors.TectonValidationError(msg)
 
         assert isinstance(entity_spec, specs.EntitySpec)
 
         return framework_entity.Entity._from_spec(entity_spec)
 
     @sdk_public_method
     def get_transformation(self, name: str) -> framework_transformation.Transformation:
         """Returns a Transformation that has been applied to a workspace.
 
         :param name: The name of the Transformation to retrieve.
         """
-        request = GetTransformationRequest(name=name, workspace=self.workspace)
+        request = GetTransformationRequest(
+            name=name,
+            workspace=self.workspace,
+            run_object_version_check=not conf.get_bool("SKIP_OBJECT_VERSION_CHECK"),
+        )
         response = metadata_service.instance().GetTransformation(request)
         fco_container = FcoContainer.from_proto(response.fco_container)
         transformation_spec = fco_container.get_single_root()
 
         if transformation_spec is None:
-            raise errors.TectonValidationError(
-                f"Transformation '{name}' not found. Try running `workspace.list_transformations()` to view all registered Transformations."
-            )
+            msg = f"Transformation '{name}' not found. Try running `workspace.list_transformations()` to view all registered Transformations."
+            raise errors.TectonValidationError(msg)
 
         assert isinstance(transformation_spec, specs.TransformationSpec)
         return framework_transformation.Transformation._from_spec(transformation_spec)
 
     @sdk_public_method
     def get_dataset(self, name: str) -> Dataset:
         """Returns a Dataset that has been saved to this workspace.
@@ -314,14 +331,28 @@
         """
         request = GetSavedFeatureDataFrameRequest(saved_feature_dataframe_name=name, workspace=self.workspace)
         response = metadata_service.instance().GetSavedFeatureDataFrame(request)
 
         return Dataset._from_proto(response.saved_feature_dataframe)
 
     @sdk_public_method
+    def delete_dataset(self, name: str):
+        """Deletes a Dataset that has been saved to this workspace.
+
+        :param name: The name of the Dataset to delete.
+        """
+        request = GetSavedFeatureDataFrameRequest(saved_feature_dataframe_name=name, workspace=self.workspace)
+        response = metadata_service.instance().GetSavedFeatureDataFrame(request)
+
+        request = ArchiveSavedFeatureDataFrameRequest()
+        request.saved_feature_dataframe_id.CopyFrom(response.saved_feature_dataframe.saved_feature_dataframe_id)
+        metadata_service.instance().ArchiveSavedFeatureDataFrame(request)
+        print(f"Dataset {response.saved_feature_dataframe.info.name} deleted")
+
+    @sdk_public_method
     def list_datasets(self) -> List[str]:
         """Returns a list of all saved Datasets within a workspace.
 
         :return: List of strings of dataset names"""
         request = GetAllSavedFeatureDataFramesRequest(workspace=self.workspace)
         response = metadata_service.instance().GetAllSavedFeatureDataFrames(request)
         return sorted([sfdf.info.name for sfdf in response.saved_feature_dataframes])
@@ -329,18 +360,22 @@
     @sdk_public_method
     def list_feature_views(self) -> List[str]:
         """Returns a list of all registered Feature Views within a workspace.
 
         :return: List of strings of feature view names"""
         request = QueryFeatureViewsRequest(workspace=self.workspace)
         response = metadata_service.instance().QueryFeatureViews(request)
-        fco_container = FcoContainer.from_proto(response.fco_container)
-        return sorted(
-            [spec.name for spec in fco_container.get_root_fcos() if not isinstance(spec, specs.FeatureTableSpec)]
-        )
+
+        def feature_view_filter(fco: fco_pb2.Fco) -> bool:
+            return fco.HasField("feature_view") and not fco.feature_view.HasField("feature_table")
+
+        # Do not extract these data protos into "specs" since that runs function deserialization and other work which
+        # is not needed here and may throw errors if there is a bad definition.
+        feature_view_fcos = filter(feature_view_filter, response.fco_container.fcos)
+        return sorted([fco.feature_view.fco_metadata.name for fco in feature_view_fcos])
 
     @sdk_public_method
     def list_feature_services(self) -> List[str]:
         """Returns a list of all registered Feature Services within a workspace.
 
         :return: List of strings of feature service names"""
         request = GetAllFeatureServicesRequest()
@@ -378,16 +413,22 @@
     @sdk_public_method
     def list_feature_tables(self) -> List[str]:
         """Returns a list of all registered Feature Tables within a workspace.
 
         :return: List of strings of feature table names"""
         request = QueryFeatureViewsRequest(workspace=self.workspace)
         response = metadata_service.instance().QueryFeatureViews(request)
-        fco_container = FcoContainer.from_proto(response.fco_container)
-        return sorted([spec.name for spec in fco_container.get_root_fcos() if isinstance(spec, specs.FeatureTableSpec)])
+
+        def feature_table_filter(fco: fco_pb2.Fco) -> bool:
+            return fco.HasField("feature_view") and fco.feature_view.HasField("feature_table")
+
+        # Do not extract these data protos into "specs" since that runs function deserialization and other work which
+        # is not needed here and may throw errors if there is a bad definition.
+        feature_table_fcos = filter(feature_table_filter, response.fco_container.fcos)
+        return sorted([fco.feature_view.fco_metadata.name for fco in feature_table_fcos])
 
     @sdk_public_method
     def get_feature_freshness(self) -> Union[Displayable, str]:
         """Returns feature freshness status for Feature Views and Tables.
 
         :return: Displayable containing freshness statuses for all features. Use `to_dict()` for a parseable representation.
         """
@@ -396,57 +437,71 @@
         if len(freshness_statuses) == 0:
             return "No Feature Views found in this workspace"
         return format_freshness_table(freshness_statuses)
 
 
 @sdk_public_method
 @documented_by(Workspace.get)
-def get_workspace(name: str):
+def get_workspace(name: str) -> Workspace:
     return Workspace.get(name)
 
 
 @sdk_public_method
-# Deprecated methods - kept around so that there is a helpful error message. Can be cleaned up after the 0.6 cut.
-def get_feature_service(name: str, workspace_name: Optional[str] = None):
-    raise errors.TectonValidationError(
-        'get_feature_service must be called from a Workspace object. E.g. tecton.get_workspace("<workspace>").get_feature_service("<feature service>").'
-    )
+def get_feature_service(name: str, workspace: str) -> framework_feature_service.FeatureService:
+    """Returns a Feature Service that has been applied to a workspace.
+
+    :param name: The name of the Feature Service to retrieve.
+    :param workspace: The name of the workspace.
+    """
+    return get_workspace(workspace).get_feature_service(name)
 
 
 @sdk_public_method
-# Deprecated methods - kept around so that there is a helpful error message. Can be cleaned up after the 0.6 cut.
-def get_feature_table(ft_reference: str, workspace_name: Optional[str] = None):
-    raise errors.TectonValidationError(
-        'get_feature_table must be called from a Workspace object. E.g. tecton.get_workspace("<workspace>").get_feature_table("<feature table>").'
-    )
+def get_feature_table(name: str, workspace: str) -> framework_feature_view.FeatureTable:
+    """Returns a Feature Table that has been applied to a workspace.
+
+    :param name: The name of the Feature Table to retrieve.
+    :param workspace: The name of the workspace.
+    """
+    return get_workspace(workspace).get_feature_table(name)
 
 
 @sdk_public_method
-# Deprecated methods - kept around so that there is a helpful error message. Can be cleaned up after the 0.6 cut.
-def get_feature_view(fv_reference: str, workspace_name: Optional[str] = None):
-    raise errors.TectonValidationError(
-        'get_feature_view must be called from a Workspace object. E.g. tecton.get_workspace("<workspace>").get_feature_view("<feature view>").'
-    )
+def get_feature_view(name: str, workspace: str) -> framework_feature_view.FeatureView:
+    """Returns a Feature View that has been applied to a workspace.
+
+    :param name: The name of the Feature View to retrieve.
+    :param workspace: The name of the workspace.
+    """
+    return get_workspace(workspace).get_feature_view(name)
 
 
 @sdk_public_method
-# Deprecated methods - kept around so that there is a helpful error message. Can be cleaned up after the 0.6 cut.
-def get_entity(name: str, workspace_name: Optional[str] = None):
-    raise errors.TectonValidationError(
-        'get_entity must be called from a Workspace object. E.g. tecton.get_workspace("<workspace>").get_entity("<entity>").'
-    )
+def get_entity(name: str, workspace: str) -> framework_entity.Entity:
+    """Returns an Entity that has been applied to a workspace.
+
+    :param name: The name of the Entity to retrieve.
+    :param workspace: The name of the workspace.
+    """
+    return get_workspace(workspace).get_entity(name)
 
 
 @sdk_public_method
-# Deprecated methods - kept around so that there is a helpful error message. Can be cleaned up after the 0.6 cut.
-def get_transformation(name, workspace_name: Optional[str] = None):
-    raise errors.TectonValidationError(
-        'get_transformation must be called from a Workspace object. E.g. tecton.get_workspace("<workspace>").get_transformation("<transformation>").'
-    )
+def get_transformation(name: str, workspace: str) -> framework_transformation.Transformation:
+    """Returns a Transformation that has been applied to a workspace.
+
+    :param name: The name of the Transformation to retrieve.
+    :param workspace: The name of the workspace.
+    """
+    return get_workspace(workspace).get_transformation(name)
 
 
 @sdk_public_method
-# Deprecated methods - kept around so that there is a helpful error message. Can be cleaned up after the 0.6 cut.
-def get_data_source(name, workspace_name: Optional[str] = None):
-    raise errors.TectonValidationError(
-        'get_data_source must be called from a Workspace object. E.g. tecton.get_workspace("<workspace>").get_data_source("<data source>").'
-    )
+def get_data_source(
+    name: str, workspace: str
+) -> Union[framework_data_source.PushSource, framework_data_source.BatchSource, framework_data_source.StreamSource,]:
+    """Returns a Data Source that has been applied to a workspace.
+
+    :param name: The name of the Data Source to retrieve.
+    :param workspace: The name of the workspace.
+    """
+    return get_workspace(workspace).get_data_source(name)
```

### Comparing `tecton-0.7.0b9/tecton/identities/api_keys.py` & `tecton-0.7.0rc0/tecton/identities/api_keys.py`

 * *Files 2% similar despite different names*

```diff
@@ -27,16 +27,17 @@
 
 
 def delete(id):
     """Delete an API key by its ID."""
     request = DeleteApiKeyRequest()
     try:
         id_proto = IdHelper.from_string(id)
-    except:
-        raise TectonValidationError("Invalid format for ID")
+    except Exception:
+        msg = "Invalid format for ID"
+        raise TectonValidationError(msg)
     request.id.CopyFrom(id_proto)
     return metadata_service.instance().DeleteApiKey(request)
 
 
 def list():
     """List active API keys."""
     request = ListApiKeysRequest()
```

### Comparing `tecton-0.7.0b9/tecton/identities/credentials.py` & `tecton-0.7.0rc0/tecton/identities/credentials.py`

 * *Files 6% similar despite different names*

```diff
@@ -46,37 +46,32 @@
 
 
 def test_credentials() -> None:
     """Test credentials and throw an exception if unauthenticated."""
     # First, check if a Tecton URL is configured.
     api_service = conf.get_or_none("API_SERVICE")
     if not api_service:
-        raise errors.TectonAPIInaccessibleError(
-            "Tecton URL not set. Please configure API_SERVICE or use tecton.set_credentials(tecton_url=<url>)."
-        )
+        msg = "Tecton URL not set. Please configure API_SERVICE or use tecton.set_credentials(tecton_url=<url>)."
+        raise errors.TectonAPIInaccessibleError(msg)
     tecton_url = api_service[:-4] if api_service.endswith("/api") else api_service
 
     # Next, determine how the user is authenticated (Okta or Service Account).
     profile = who_am_i()
     auth_mode = None
     if isinstance(profile, ServiceAccountProfile):
         auth_mode = f"Service Account {profile.id} ({profile.name})"
     elif isinstance(profile, okta.UserProfile):
         auth_mode = f"User Profile {profile.email}"
     else:
         # profile can be None if TECTON_API_KEY is set, but invalid.
         if conf.get_or_none("TECTON_API_KEY"):
-            raise errors.TectonAPIInaccessibleError(
-                f"Invalid TECTON_API_KEY configured for {tecton_url}. "
-                "Please update TECTON_API_KEY or use tecton.set_credentials(tecton_api_key=<key>)."
-            )
-        raise errors.FailedPreconditionError(
-            f"No user profile or service account configured for {tecton_url}. "
-            "Please configure TECTON_API_KEY or use tecton.set_credentials(tecton_api_key=<key>)."
-        )
+            msg = f"Invalid TECTON_API_KEY configured for {tecton_url}. Please update TECTON_API_KEY or use tecton.set_credentials(tecton_api_key=<key>)."
+            raise errors.TectonAPIInaccessibleError(msg)
+        msg = f"No user profile or service account configured for {tecton_url}. Please configure TECTON_API_KEY or use tecton.set_credentials(tecton_api_key=<key>)."
+        raise errors.FailedPreconditionError(msg)
 
     print(f"Successfully authenticated with {tecton_url} using {auth_mode}.")
 
 
 def who_am_i() -> Optional[Union[ServiceAccountProfile, okta.UserProfile]]:
     """Introspect the current User or API Key used to authenticate with Tecton.
```

### Comparing `tecton-0.7.0b9/tecton/identities/okta.py` & `tecton-0.7.0rc0/tecton/identities/okta.py`

 * *Files 1% similar despite different names*

```diff
@@ -14,14 +14,15 @@
 
 import attrs
 import click
 import requests
 
 from tecton_core import conf
 
+
 AUTH_SERVER = "https://login.tecton.ai/oauth2/default/.well-known/oauth-authorization-server"
 USER_INFO_ENDPOINT = "https://login.tecton.ai/oauth2/default/v1/userinfo"
 
 # this must match the uri that the okta application was configured with
 OKTA_EXPECTED_PORTS = [10003, 10013, 10023]
 
 # our http server will populate this when it receives the callback from okta
@@ -67,16 +68,16 @@
             "grant_type": "refresh_token",
             "client_id": conf.get_or_none("CLI_CLIENT_ID"),
             "scope": "openid profile email",
         }
         try:
             return get_tokens_helper(self.metadata["token_endpoint"], params)
         except requests.RequestException as e:
-            print("Authorization token expired. Please reauthenticate with `tecton login`", file=sys.stderr)
-            sys.exit(1)
+            msg = "Authorization token expired. Please reauthenticate with `tecton login`"
+            raise SystemExit(msg)
 
 
 class AuthFlowType(enum.Enum):
     # Spin up local http server that can intercept the okta callback and avoid having to
     # copy & paste
     BROWSER_HANDS_FREE = 1
 
@@ -115,14 +116,15 @@
         if self.auth_flow_type == AuthFlowType.SESSION_TOKEN:
             params["sessionToken"] = self.okta_session_token
             params["response_mode"] = "query"
         authorize_url = f"{self.metadata['authorization_endpoint']}?{urlencode(params)}"
         return authorize_url, state
 
     def _browser_auth(self, code_verifier: str) -> Tuple[str, str]:
+        e: Optional[OSError] = None
         for port in OKTA_EXPECTED_PORTS:
             try:
                 httpd = socketserver.TCPServer(("", int(port)), OktaCallbackReceivingServer)
                 break
             except OSError as e:
                 # socket in use, try other port
                 if e.errno == 48:
@@ -130,19 +132,20 @@
                 else:
                     print(
                         "Encountered error with authorization callback. Your environment may not support automatic login; try running `tecton login --manual` instead.",
                         file=sys.stderr,
                     )
                     sys.exit(1)
         else:
-            raise e
+            if e:
+                raise e
 
         redirect_uri = f"http://localhost:{port}/authorization/callback"
         authorize_url, state = self.build_authorization_url(code_verifier, redirect_uri)
-        print(f"Requesting authorization for Tecton CLI via browser. ")
+        print("Requesting authorization for Tecton CLI via browser. ")
         time.sleep(2)
         try:
             print("If browser doesn't open automatically, use `tecton login --manual`")
             click.launch(authorize_url)
             httpd.handle_request()
         finally:
             httpd.server_close()
```

### Comparing `tecton-0.7.0b9/tecton/pytest_tecton.py` & `tecton-0.7.0rc0/tecton/pytest_tecton.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,13 +1,24 @@
 import pytest
 
 from tecton import tecton_context
+from tecton.framework import validation_mode
+from tecton_core import conf
 from tecton_spark.udf_jar import get_udf_jar_path
 
 
+def pytest_configure():
+    # Set validation mode to 'skip' in unit test mode.
+    validation_mode.set_validation_mode(validation_mode.ValidationMode.SKIP.value)
+
+    # Force function serialization in test mode so it's as representative to
+    # what Tecton will run in materialization.
+    conf.set("TECTON_FORCE_FUNCTION_SERIALIZATION", "true")
+
+
 @pytest.fixture(scope="session")
 def tecton_pytest_spark_session():
     try:
         from pyspark.sql import SparkSession
     except ImportError:
         pytest.fail("Cannot create a SparkSession if `pyspark` is not installed.")
 
@@ -21,14 +32,15 @@
         spark = (
             SparkSession.builder.appName("tecton_pytest_spark_session")
             .config("spark.jars", get_udf_jar_path())
             # This short-circuit's Spark's attempt to auto-detect a hostname for the master address, which can lead to
             # errors on hosts with "unusual" hostnames that Spark believes are invalid.
             .config("spark.driver.host", "localhost")
             .config("spark.ui.enabled", "false")
+            .config("spark.sql.session.timeZone", "UTC")
             .getOrCreate()
         )
     except Exception as e:
         # Unfortunately we can't do much better than parsing the error message since Spark raises `Exception` rather than a more specific type.
         if str(e) == "Java gateway process exited before sending its port number":
             pytest.fail(
                 "Failed to start Java process for Spark, perhaps Java isn't installed or the 'JAVA_HOME' environment variable is not set?"
```

### Comparing `tecton-0.7.0b9/tecton/tecton_context.py` & `tecton-0.7.0rc0/tecton/tecton_context.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,20 +1,20 @@
+import logging
 from typing import Any
 from typing import Dict
 from typing import Optional
 
 from pyspark.sql import SparkSession
 
 from tecton._internals.sdk_decorators import sdk_public_method
 from tecton._internals.spark_utils import get_or_create_spark_session
 from tecton_core import conf
-from tecton_core import logger as logger_lib
 
 
-logger = logger_lib.get_logger("TectonContext")
+logger = logging.getLogger(__name__)
 
 
 class TectonContext:
     """
     Execute Spark SQL queries; access various utils.
     """
 
@@ -52,15 +52,15 @@
         """
         Create the singleton instance of TectonContext from the provided spark session.
         """
         cls._current_context_instance = instance
 
     @classmethod
     def _generate_and_set_new_instance(cls) -> "TectonContext":
-        logger.debug(f"Generating new Spark session")
+        logger.debug("Generating new Spark session")
         spark = get_or_create_spark_session(
             cls._config.get("custom_spark_options"),
         )
         cls._current_context_instance = cls(spark)
         return cls._current_context_instance
 
     def _get_spark(self) -> SparkSession:
```

### Comparing `tecton-0.7.0b9/tecton/types.py` & `tecton-0.7.0rc0/tecton/types.py`

 * *Files 17% similar despite different names*

```diff
@@ -18,15 +18,15 @@
     Timestamp = "Timestamp", spark_types.TimestampType(), tecton_types.TimestampType()
 
     def __repr__(self):
         return self.name
 
 
 class Array:
-    def __init__(self, element_type: Union[DataType, "Array", "Struct"]):
+    def __init__(self, element_type: Union[DataType, "Array", "Struct", "Map"]):
         self.element_type = element_type
 
     @property
     def spark_type(self) -> spark_types.ArrayType:
         return spark_types.ArrayType(self.element_type.spark_type)
 
     @property
@@ -46,15 +46,15 @@
 
 
 @typechecked
 class Field:
     def __init__(
         self,
         name: str,
-        dtype: Union[DataType, Array, "Struct"],
+        dtype: Union[DataType, Array, "Struct", "Map"],
     ):
         self.name = name
         self.dtype = dtype
 
     def spark_type(self) -> spark_types.StructField:
         return spark_types.StructField(self.name, self.dtype.spark_type)
 
@@ -78,7 +78,29 @@
     @property
     def tecton_type(self) -> tecton_types.StructType:
         struct_fields = [field.tecton_type() for field in self.fields]
         return tecton_types.StructType(struct_fields)
 
     def __repr__(self):
         return f"Struct({self.fields})"
+
+
+@typechecked
+class Map:
+    # key_type only allows String as of 07/07/2023. From type annotations perspective, we allow all types to be passed
+    # in here so users could receive better error message from MDS instead of python type checking error.
+    def __init__(
+        self, key_type: Union[DataType, Array, Struct, "Map"], value_type: Union[DataType, Array, Struct, "Map"]
+    ):
+        self.key_type = key_type
+        self.value_type = value_type
+
+    @property
+    def spark_type(self) -> spark_types.MapType:
+        return spark_types.MapType(self.key_type.spark_type, self.value_type.spark_type)
+
+    @property
+    def tecton_type(self) -> tecton_types.MapType:
+        return tecton_types.MapType(self.key_type.tecton_type, self.value_type.tecton_type)
+
+    def __repr__(self):
+        return f"Map({repr(self.key_type)}, {repr(self.value_type)})"
```

### Comparing `tecton-0.7.0b9/tecton/vendor/dill/dill/__diff.py` & `tecton-0.7.0rc0/tecton/vendor/dill/dill/__diff.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/dill/dill/__init__.py` & `tecton-0.7.0rc0/tecton/vendor/dill/dill/__init__.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/dill/dill/_dill.py` & `tecton-0.7.0rc0/tecton/vendor/dill/dill/_dill.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/dill/dill/_objects.py` & `tecton-0.7.0rc0/tecton/vendor/dill/dill/_objects.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/dill/dill/detect.py` & `tecton-0.7.0rc0/tecton/vendor/dill/dill/detect.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/dill/dill/info.py` & `tecton-0.7.0rc0/tecton/vendor/dill/dill/info.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/dill/dill/objtypes.py` & `tecton-0.7.0rc0/tecton/vendor/dill/dill/objtypes.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/dill/dill/pointers.py` & `tecton-0.7.0rc0/tecton/vendor/dill/dill/pointers.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/dill/dill/settings.py` & `tecton-0.7.0rc0/tecton/vendor/dill/dill/settings.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/dill/dill/source.py` & `tecton-0.7.0rc0/tecton/vendor/dill/dill/source.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/dill/dill/temp.py` & `tecton-0.7.0rc0/tecton/vendor/dill/dill/temp.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/py4j/clientserver.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/py4j/clientserver.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/py4j/compat.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/py4j/compat.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/py4j/finalizer.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/py4j/finalizer.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/py4j/java_collections.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/py4j/java_collections.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/py4j/java_gateway.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/py4j/java_gateway.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/py4j/protocol.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/py4j/protocol.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/py4j/signals.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/py4j/signals.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/__init__.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/__init__.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/_globals.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/_globals.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/accumulators.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/accumulators.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/broadcast.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/broadcast.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/cloudpickle/cloudpickle.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/cloudpickle/cloudpickle.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/cloudpickle/cloudpickle_fast.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/cloudpickle/cloudpickle_fast.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/conf.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/conf.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/context.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/context.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/daemon.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/daemon.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/files.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/files.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/find_spark_home.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/find_spark_home.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/install.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/install.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/java_gateway.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/java_gateway.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/join.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/join.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/__init__.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/__init__.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/base.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/base.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/classification.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/classification.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/clustering.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/clustering.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/common.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/common.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/evaluation.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/evaluation.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/feature.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/feature.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/fpm.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/fpm.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/functions.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/functions.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/image.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/image.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/linalg/__init__.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/linalg/__init__.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/param/__init__.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/param/__init__.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/param/_shared_params_code_gen.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/param/_shared_params_code_gen.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/param/shared.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/param/shared.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/pipeline.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/pipeline.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/recommendation.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/recommendation.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/regression.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/regression.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/stat.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/stat.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/tree.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/tree.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/tuning.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/tuning.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/util.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/util.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/ml/wrapper.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/ml/wrapper.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/__init__.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/__init__.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/classification.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/classification.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/clustering.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/clustering.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/common.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/common.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/evaluation.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/evaluation.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/feature.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/feature.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/fpm.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/fpm.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/linalg/__init__.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/linalg/__init__.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/linalg/distributed.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/linalg/distributed.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/random.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/random.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/recommendation.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/recommendation.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/regression.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/regression.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/stat/KernelDensity.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/stat/KernelDensity.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/stat/__init__.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/stat/__init__.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/stat/_statistics.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/stat/_statistics.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/stat/distribution.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/stat/distribution.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/stat/test.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/stat/test.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/tree.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/tree.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/mllib/util.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/mllib/util.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/profiler.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/profiler.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/python/pyspark/shell.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/python/pyspark/shell.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/rdd.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/rdd.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/rddsampler.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/rddsampler.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/resource/__init__.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/resource/__init__.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/resource/information.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/resource/information.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/resource/profile.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/resource/profile.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/resource/requests.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/resource/requests.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/resultiterable.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/resultiterable.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/serializers.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/serializers.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/shell.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/shell.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/shuffle.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/shuffle.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/__init__.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/__init__.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/avro/__init__.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/avro/__init__.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/avro/functions.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/avro/functions.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/catalog.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/catalog.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/column.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/column.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/conf.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/conf.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/context.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/context.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/dataframe.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/dataframe.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/functions.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/functions.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/group.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/group.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/pandas/__init__.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/pandas/__init__.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/pandas/conversion.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/pandas/conversion.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/pandas/functions.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/pandas/functions.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/pandas/group_ops.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/pandas/group_ops.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/pandas/map_ops.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/pandas/map_ops.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/pandas/serializers.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/pandas/serializers.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/pandas/typehints.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/pandas/typehints.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/pandas/types.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/pandas/types.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/pandas/utils.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/pandas/utils.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/readwriter.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/readwriter.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/session.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/session.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/streaming.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/streaming.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/types.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/types.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/udf.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/udf.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/utils.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/utils.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/sql/window.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/sql/window.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/statcounter.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/statcounter.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/status.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/status.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/storagelevel.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/storagelevel.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/streaming/__init__.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/streaming/__init__.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/streaming/context.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/streaming/context.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/streaming/dstream.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/streaming/dstream.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/streaming/kinesis.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/streaming/kinesis.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/streaming/listener.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/streaming/listener.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/streaming/util.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/streaming/util.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/taskcontext.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/taskcontext.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/traceback_utils.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/traceback_utils.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/util.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/util.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/pyspark/pyspark/worker.py` & `tecton-0.7.0rc0/tecton/vendor/pyspark/pyspark/worker.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/vendor_dill.py` & `tecton-0.7.0rc0/tecton/vendor/vendor_dill.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/vendor/vendor_pyspark.py` & `tecton-0.7.0rc0/tecton/vendor/vendor_pyspark.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton/version.py` & `tecton-0.7.0rc0/tecton/version.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton.egg-info/PKG-INFO` & `tecton-0.7.0rc0/tecton.egg-info/PKG-INFO`

 * *Files 3% similar despite different names*

```diff
@@ -1,27 +1,29 @@
 Metadata-Version: 2.1
 Name: tecton
-Version: 0.7.0b9
+Version: 0.7.0rc0
 Summary: Tecton Python SDK
 Home-page: https://tecton.ai
 Author: Tecton, Inc.
 Author-email: support@tecton.ai
 License: Tecton Proprietary
 Classifier: Programming Language :: Python :: 3
 Classifier: Operating System :: OS Independent
 Classifier: License :: Other/Proprietary License
-Requires-Python: >=3.7.*
+Requires-Python: >=3.7
 Description-Content-Type: text/markdown
 Provides-Extra: databricks-connect
 Provides-Extra: databricks-connect9
 Provides-Extra: databricks-connect10
+Provides-Extra: databricks-connect11
 Provides-Extra: pyspark
 Provides-Extra: pyspark3
 Provides-Extra: pyspark3.1
 Provides-Extra: pyspark3.2
+Provides-Extra: pyspark3.3
 Provides-Extra: snowflake
 Provides-Extra: snowpark
 Provides-Extra: athena
 
 ![logo](https://s3.us-west-2.amazonaws.com/tecton.ai.public/documentation/pypi/tecton-logo.svg)
 
 Tecton is the fastest way to build operational machine learning applications. It helps automate real-time decision making like fraud detection, product recommendations, and search result ranking in production applications.
```

### Comparing `tecton-0.7.0b9/tecton.egg-info/SOURCES.txt` & `tecton-0.7.0rc0/tecton.egg-info/SOURCES.txt`

 * *Files 5% similar despite different names*

```diff
@@ -25,18 +25,19 @@
 tecton/_internals/analytics.py
 tecton/_internals/athena_api.py
 tecton/_internals/data_frame_helper.py
 tecton/_internals/delete_keys_api.py
 tecton/_internals/display.py
 tecton/_internals/env_utils.py
 tecton/_internals/errors.py
-tecton/_internals/fco.py
 tecton/_internals/find_spark.py
+tecton/_internals/ingest_utils.py
 tecton/_internals/materialization_api.py
 tecton/_internals/metadata_service.py
+tecton/_internals/mock_source_utils.py
 tecton/_internals/query_helper.py
 tecton/_internals/rewrite.py
 tecton/_internals/run_api.py
 tecton/_internals/sdk_decorators.py
 tecton/_internals/snowflake_api.py
 tecton/_internals/spark_api.py
 tecton/_internals/spark_utils.py
@@ -61,18 +62,21 @@
 tecton/cli/api_key.py
 tecton/cli/cli.py
 tecton/cli/cli_utils.py
 tecton/cli/command.py
 tecton/cli/common.py
 tecton/cli/engine.py
 tecton/cli/engine_renderer.py
+tecton/cli/environment.py
 tecton/cli/error_utils.py
 tecton/cli/printer.py
 tecton/cli/service_account.py
+tecton/cli/user.py
 tecton/cli/workspace.py
+tecton/cli/workspace_utils.py
 tecton/framework/__init__.py
 tecton/framework/base_tecton_object.py
 tecton/framework/compute_mode.py
 tecton/framework/configs.py
 tecton/framework/data_frame.py
 tecton/framework/data_source.py
 tecton/framework/dataset.py
@@ -227,14 +231,16 @@
 tecton_athena/__init__.py
 tecton_athena/athena_session.py
 tecton_athena/data_catalog_helper.py
 tecton_athena/odfv_helper.py
 tecton_athena/pipeline_helper.py
 tecton_athena/sql_helper.py
 tecton_athena/templates_utils.py
+tecton_athena/query/__init__.py
+tecton_athena/query/translate.py
 tecton_athena/templates/__init__.py
 tecton_athena/templates/create_table.sql
 tecton_athena/templates/data_source.sql
 tecton_athena/templates/historical_features.sql
 tecton_athena/templates/materialization_tile.sql
 tecton_athena/templates/run_full_aggregation.sql
 tecton_athena/templates/run_partial_aggregation.sql
@@ -248,32 +254,36 @@
 tecton_core/fco_container.py
 tecton_core/feature_definition_wrapper.py
 tecton_core/feature_set_config.py
 tecton_core/feature_view_utils.py
 tecton_core/filter_context.py
 tecton_core/function_deserialization.py
 tecton_core/id_helper.py
-tecton_core/logger.py
 tecton_core/materialization_context.py
 tecton_core/offline_store.py
 tecton_core/online_serving_index.py
 tecton_core/pipeline_common.py
 tecton_core/pipeline_sql_builder.py
 tecton_core/query_consts.py
 tecton_core/repo_file_handler.py
+tecton_core/request_context.py
 tecton_core/schema.py
 tecton_core/schema_derivation_utils.py
 tecton_core/time_utils.py
 tecton_core/query/__init__.py
 tecton_core/query/aggregation_plans.py
 tecton_core/query/builder.py
 tecton_core/query/node_interface.py
 tecton_core/query/nodes.py
 tecton_core/query/rewrite.py
 tecton_core/query/sql_compat.py
+tecton_core/query/pandas/__init__.py
+tecton_core/query/pandas/node.py
+tecton_core/query/pandas/nodes.py
+tecton_core/query/pandas/sql.py
 tecton_core/specs/__init__.py
 tecton_core/specs/data_source_spec.py
 tecton_core/specs/entity_spec.py
 tecton_core/specs/feature_service_spec.py
 tecton_core/specs/feature_view_spec.py
 tecton_core/specs/tecton_object_spec.py
 tecton_core/specs/transformation_spec.py
@@ -311,47 +321,56 @@
 tecton_proto/args/version_constraints_pb2.py
 tecton_proto/args/virtual_data_source_pb2.py
 tecton_proto/auth/__init__.py
 tecton_proto/auth/acl_pb2.py
 tecton_proto/auth/authorization_service_pb2.py
 tecton_proto/auth/principal_pb2.py
 tecton_proto/auth/resource_pb2.py
+tecton_proto/auth/resource_role_assignments_pb2.py
 tecton_proto/auth/service_pb2.py
+tecton_proto/canary/__init__.py
+tecton_proto/canary/type_pb2.py
+tecton_proto/canary/update_pb2.py
 tecton_proto/cli/__init__.py
 tecton_proto/cli/repo_diff_pb2.py
 tecton_proto/common/__init__.py
 tecton_proto/common/aggregation_function_pb2.py
 tecton_proto/common/analytics_options_pb2.py
 tecton_proto/common/column_type_pb2.py
+tecton_proto/common/container_image_pb2.py
 tecton_proto/common/data_source_type_pb2.py
 tecton_proto/common/data_type_pb2.py
 tecton_proto/common/fco_locator_pb2.py
 tecton_proto/common/framework_version_pb2.py
 tecton_proto/common/id_pb2.py
 tecton_proto/common/pair_pb2.py
 tecton_proto/common/schema_container_pb2.py
 tecton_proto/common/schema_pb2.py
 tecton_proto/common/secret_pb2.py
 tecton_proto/common/spark_schema_pb2.py
 tecton_proto/consumption/__init__.py
 tecton_proto/consumption/consumption_pb2.py
 tecton_proto/data/__init__.py
 tecton_proto/data/batch_data_source_pb2.py
+tecton_proto/data/data_source_access_config_pb2.py
 tecton_proto/data/entity_pb2.py
 tecton_proto/data/fco_metadata_pb2.py
 tecton_proto/data/fco_pb2.py
 tecton_proto/data/feature_service_pb2.py
 tecton_proto/data/feature_store_pb2.py
 tecton_proto/data/feature_view_pb2.py
 tecton_proto/data/freshness_status_pb2.py
 tecton_proto/data/fv_materialization_pb2.py
 tecton_proto/data/hive_metastore_pb2.py
 tecton_proto/data/internal_spark_cluster_status_pb2.py
 tecton_proto/data/materialization_status_pb2.py
+tecton_proto/data/odfv_compute_pb2.py
 tecton_proto/data/onboarding_pb2.py
+tecton_proto/data/principal_group_pb2.py
+tecton_proto/data/remote_compute_environment_pb2.py
 tecton_proto/data/remote_spark_pb2.py
 tecton_proto/data/saved_feature_data_frame_pb2.py
 tecton_proto/data/serving_status_pb2.py
 tecton_proto/data/state_update_pb2.py
 tecton_proto/data/stream_data_source_pb2.py
 tecton_proto/data/summary_pb2.py
 tecton_proto/data/tecton_api_key_pb2.py
@@ -381,23 +400,31 @@
 tecton_proto/dataobs/validation_task_pb2.py
 tecton_proto/feature_server/__init__.py
 tecton_proto/feature_server/configuration/__init__.py
 tecton_proto/feature_server/configuration/feature_server_configuration_pb2.py
 tecton_proto/materialization/__init__.py
 tecton_proto/materialization/job_metadata_pb2.py
 tecton_proto/materialization/materialization_states_pb2.py
+tecton_proto/materialization/materialization_task_pb2.py
+tecton_proto/materialization/params_pb2.py
 tecton_proto/materialization/spark_cluster_pb2.py
 tecton_proto/materializationjobservice/__init__.py
 tecton_proto/materializationjobservice/materialization_job_service_pb2.py
 tecton_proto/metadataservice/__init__.py
 tecton_proto/metadataservice/http_over_grpc_pb2.py
 tecton_proto/metadataservice/metadata_service_pb2.py
 tecton_proto/online_store/__init__.py
 tecton_proto/online_store/feature_value_pb2.py
 tecton_proto/online_store/status_entry_pb2.py
+tecton_proto/online_store_writer/__init__.py
+tecton_proto/online_store_writer/config_pb2.py
+tecton_proto/remoteenvironmentservice/__init__.py
+tecton_proto/remoteenvironmentservice/remote_environment_service_pb2.py
+tecton_proto/snowflake/__init__.py
+tecton_proto/snowflake/location_pb2.py
 tecton_proto/spark_api/__init__.py
 tecton_proto/spark_api/error_pb2.py
 tecton_proto/spark_api/jobs_pb2.py
 tecton_proto/spark_common/__init__.py
 tecton_proto/spark_common/clusters_pb2.py
 tecton_proto/spark_common/libraries_pb2.py
 tecton_proto/validation/__init__.py
@@ -410,37 +437,37 @@
 tecton_snowflake/templates_utils.py
 tecton_snowflake/utils.py
 tecton_snowflake/templates/__init__.py
 tecton_snowflake/templates/copier_macro.sql
 tecton_snowflake/templates/create_temp_table_for_bfv.sql
 tecton_snowflake/templates/create_temp_table_for_bwafv.sql
 tecton_snowflake/templates/data_source.sql
+tecton_snowflake/templates/delete_orphaned_schemas.sql
 tecton_snowflake/templates/delete_staged_files.sql
 tecton_snowflake/templates/historical_features.sql
 tecton_snowflake/templates/historical_features_macros.sql
 tecton_snowflake/templates/materialization_tile.sql
 tecton_snowflake/templates/materialized_feature_view.sql
+tecton_snowflake/templates/offline_materialization_macros.sql
 tecton_snowflake/templates/online_store_copier.sql
 tecton_snowflake/templates/run_full_aggregation.sql
 tecton_snowflake/templates/run_partial_aggregation.sql
 tecton_snowflake/templates/time_limit.sql
 tecton_snowflake/templates/transformation_pipeline.sql
 tecton_spark/__init__.py
 tecton_spark/aggregation_plans.py
 tecton_spark/data_observability.py
 tecton_spark/data_source_credentials.py
 tecton_spark/data_source_helper.py
 tecton_spark/errors_spark.py
 tecton_spark/feature_view_spark_utils.py
-tecton_spark/ingest_utils.py
 tecton_spark/materialization_plan.py
 tecton_spark/offline_store.py
 tecton_spark/partial_aggregations.py
 tecton_spark/pipeline_helper.py
-tecton_spark/request_context.py
 tecton_spark/schema_derivation_utils.py
 tecton_spark/schema_spark_utils.py
 tecton_spark/spark_helper.py
 tecton_spark/spark_schema_wrapper.py
 tecton_spark/time_utils.py
 tecton_spark/udf_jar.py
 tecton_spark/udfs.py
```

### Comparing `tecton-0.7.0b9/tecton.egg-info/requires.txt` & `tecton-0.7.0rc0/tecton.egg-info/requires.txt`

 * *Files 22% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 attrs>=21.3.0
 boto3
 googleapis-common-protos~=1.52
-jinja2~=3.0.3
+jinja2~=3.0
 numpy~=1.16
 pathspec
 pendulum~=2.1
-protobuf~=3.20.0
+protobuf>=3.20.0
 pypika~=0.48.9
 pytimeparse
 pandas~=1.0
 texttable
 requests
 colorama~=0.4
 tqdm~=4.41
@@ -22,32 +22,38 @@
 sqlparse
 semantic_version
 
 [athena]
 awswrangler~=2.15
 
 [databricks-connect]
-databricks-connect[sql]~=9.1.23
+databricks-connect[sql]~=10.4.12
 
 [databricks-connect10]
 databricks-connect[sql]~=10.4.12
 
+[databricks-connect11]
+databricks-connect[sql]~=11.3.12
+
 [databricks-connect9]
 databricks-connect[sql]~=9.1.23
 
 [pyspark]
-pyspark[sql]~=3.1.2
+pyspark[sql]~=3.2.1
 
 [pyspark3]
-pyspark[sql]~=3.1.2
+pyspark[sql]~=3.2.1
 
 [pyspark3.1]
 pyspark[sql]~=3.1.2
 
 [pyspark3.2]
 pyspark[sql]~=3.2.1
 
+[pyspark3.3]
+pyspark[sql]~=3.3.2
+
 [snowflake]
 snowflake-connector-python[pandas]~=2.8
 
 [snowpark]
-snowflake-snowpark-python[pandas]~=1.0.0
+snowflake-snowpark-python[pandas]~=1.0
```

### Comparing `tecton-0.7.0b9/tecton_athena/athena_session.py` & `tecton-0.7.0rc0/tecton_athena/athena_session.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 import dataclasses
 import datetime
 import hashlib
 import json
+import logging
 import os
 import secrets
 import tempfile
 from dataclasses import dataclass
 from typing import Any
 from typing import Dict
 from typing import Iterator
@@ -14,19 +15,19 @@
 from urllib.parse import urlparse
 
 import boto3
 import pandas
 
 from tecton_athena.templates_utils import load_template
 from tecton_core import conf
-from tecton_core import logger as logger_lib
 from tecton_core.errors import TectonAthenaValidationError
 from tecton_spark import offline_store
 
-logger = logger_lib.get_logger("AthenaSession")
+
+logger = logging.getLogger(__name__)
 
 # In some cases, strings in Pandas DF are actually represented as "object" types.
 # Hence the sketchy 'object' -> 'string' map
 PANDAS_TO_HIVE_TYPES = {
     "string": "string",
     "object": "string",
     "int64": "bigint",
@@ -113,17 +114,16 @@
     @property
     def _wr(self):
         try:
             import awswrangler
 
             return awswrangler
         except ModuleNotFoundError:
-            raise Exception(
-                "Athena Session cannot be initialized. Python module awswrangler not found. Did you forget to pip install tecton[athena]?"
-            )
+            msg = "Athena Session cannot be initialized. Python module awswrangler not found. Did you forget to pip install tecton[athena]?"
+            raise Exception(msg)
 
     def _get_athena_s3_path(self):
         s3_path = self.config.s3_path or conf.get_or_none("ATHENA_S3_PATH")
         if s3_path is not None:
             # Configuration always takes precedent and can be set at any time
             self._athena_s3_path = s3_path
         elif self._athena_s3_path is None:
@@ -132,15 +132,16 @@
             self._athena_s3_path = self._wr.athena.create_athena_bucket(boto3_session=self.config.boto3_session)
 
         if self._athena_s3_path.endswith("/"):
             # Drop "/" - calling function expects a path without trailing "/"
             self._athena_s3_path = self._athena_s3_path[0:-1]
 
         if not self._athena_s3_path.startswith("s3"):
-            raise TectonAthenaValidationError(f"Provided S3 path does not start with s3://. Provided value: {s3_path}")
+            msg = f"Provided S3 path does not start with s3://. Provided value: {s3_path}"
+            raise TectonAthenaValidationError(msg)
 
         return self._athena_s3_path
 
     def delete_table_if_exists(self, database: str, table: str):
         return self._wr.catalog.delete_table_if_exists(
             database=database, table=table, boto3_session=self.config.boto3_session
         )
@@ -186,15 +187,15 @@
             )
             # Some customers need to use bucket, but others may not have permissions to do so.
             # Fallback to use client when permission is not there.
             try:
                 s3 = self.config.boto3_session.resource("s3")
                 bucket = s3.Bucket(s3_bucket)
                 bucket.upload_file(f.name, s3_object_key, ExtraArgs=extra_args)
-            except:
+            except Exception:
                 s3_client.upload_file(f.name, s3_bucket, s3_object_key, ExtraArgs=extra_args)
             return s3_path_without_file
 
     def _create_athena_table_from_s3_path(self, s3_path: str, table_name: str, hive_columns: dict) -> str:
         athena_database = self._get_athena_database()
         query = CREATE_TABLE_TEMPLATE.render(
             database=athena_database,
@@ -216,15 +217,16 @@
         column_types = {}
         for k, v in pandas_df.dtypes.to_dict().items():
             if "datetime64" in v.name:
                 column_types[k] = "timestamp"
                 continue
             type_name = v.name.lower()
             if type_name not in PANDAS_TO_HIVE_TYPES:
-                raise Exception(f"Pandas Type {type_name} not supported. Mapping to Hive Type not found.")
+                msg = f"Pandas Type {type_name} not supported. Mapping to Hive Type not found."
+                raise Exception(msg)
             column_types[k] = PANDAS_TO_HIVE_TYPES[type_name]
         return column_types
 
     def write_pandas(self, df: pandas.DataFrame, table_name: str) -> str:
         s3_full_path = self._upload_pandas_to_s3(df)
         hive_columns = self._pandas_columns_to_hive_columns(df)
 
@@ -303,21 +305,19 @@
                 f"Glue table {table_spec.database}.{table_spec.table} hash matches expectation ({table_spec.stable_hash}). No update needed."
             )
             if table_spec.data_format == "delta":
                 # required for partition discovery
                 self.repair_table(table_spec.database, table_spec.table)
             return
         if existing_table_spec_version is None:
-            raise TectonAthenaValidationError(
-                f"Glue table {table_spec.database}.{table_spec.table} registration doesn't meet expectations but cannot be updated because it doesn't seem to have been registered by Tecton. Please drop the table to have Tecton manage its registration."
-            )
+            msg = f"Glue table {table_spec.database}.{table_spec.table} registration doesn't meet expectations but cannot be updated because it doesn't seem to have been registered by Tecton. Please drop the table to have Tecton manage its registration."
+            raise TectonAthenaValidationError(msg)
         if existing_table_spec_version > AthenaTableCreationSpec.SPEC_VERSION:
-            raise TectonAthenaValidationError(
-                f"Glue table {table_spec.database}.{table_spec.table} registration doesn't meet expectations but cannot be updated because it was registered with a newer version of Tecton. Please upgrade the SDK. Found Spec version: {existing_table_spec_version}. SDK Spec version: {AthenaTableCreationSpec.SPEC_VERSION}"
-            )
+            msg = f"Glue table {table_spec.database}.{table_spec.table} registration doesn't meet expectations but cannot be updated because it was registered with a newer version of Tecton. Please upgrade the SDK. Found Spec version: {existing_table_spec_version}. SDK Spec version: {AthenaTableCreationSpec.SPEC_VERSION}"
+            raise TectonAthenaValidationError(msg)
         logger.info(
             f"Glue table {table_spec.database}.{table_spec.table} registration needs to be updated. Dropping existing table..."
         )
         self.delete_table_if_exists(table_spec.database, table_spec.table)
         table_create_result = self.create_table(table_spec)
         return table_create_result
```

### Comparing `tecton-0.7.0b9/tecton_athena/data_catalog_helper.py` & `tecton-0.7.0rc0/tecton_athena/data_catalog_helper.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,31 +1,33 @@
+import logging
 from datetime import timedelta
 
 import pandas
 import pendulum
 
 import tecton_spark.offline_store
-from tecton_athena.athena_session import AthenaSession
-from tecton_athena.athena_session import AthenaTableCreationSpec
 from tecton_athena.athena_session import PARTITION_TYPE_DATESTR
 from tecton_athena.athena_session import PARTITION_TYPE_UNIX_EPOCH_NS
-from tecton_core import logger as logger_lib
+from tecton_athena.athena_session import AthenaSession
+from tecton_athena.athena_session import AthenaTableCreationSpec
+from tecton_core import conf
 from tecton_core.data_types import ArrayType
 from tecton_core.data_types import BoolType
 from tecton_core.data_types import Float64Type
 from tecton_core.data_types import Int32Type
 from tecton_core.data_types import Int64Type
 from tecton_core.data_types import StringType
 from tecton_core.data_types import TimestampType
 from tecton_core.errors import TectonAthenaNotImplementedError
 from tecton_core.feature_definition_wrapper import FeatureDefinitionWrapper as FeatureDefinition
 from tecton_core.offline_store import get_offline_store_partition_params
 from tecton_proto.data.feature_store_pb2 import FeatureStoreFormatVersion
 
-logger = logger_lib.get_logger("DataCatalogHelper")
+
+logger = logging.getLogger(__name__)
 
 SECONDS_TO_NANOSECONDS = 1000 * 1000 * 1000
 
 TECTON_DATA_TYPE_TO_HIVE_TYPES = {
     StringType(): "string",
     Float64Type(): "double",
     Int32Type(): "integer",
@@ -93,15 +95,16 @@
     partition_params = get_offline_store_partition_params(feature_definition)
     if store_type == "delta":
         partition_by = partition_params.partition_by
 
         partition_by_interval_timedelta = partition_params.partition_interval.as_timedelta()
         if partition_by_interval_timedelta % timedelta(days=1) != timedelta(0):
             # We only support daily partitions now
-            raise TectonAthenaNotImplementedError("Athena reader currently only supports daily delta partitions")
+            msg = "Athena reader currently only supports daily delta partitions"
+            raise TectonAthenaNotImplementedError(msg)
 
         # we do not set partition_by_interval, because this is used by
         # projection in AthenaCreateTableSpec. The default value is "1"
         # , because the default unit is in days because we use "date" type
 
         partition_type = PARTITION_TYPE_DATESTR
         partition_by_type = "date"
@@ -124,30 +127,34 @@
         )
         partition_by_range_to = int(
             pendulum.now("utc").add(years=1).end_of("year").timestamp() * SECONDS_TO_NANOSECONDS
         )
         # The partition size should really be configured explicitly in the proto offline store config
         partition_by_interval = int(partition_by_interval_timedelta.total_seconds() * SECONDS_TO_NANOSECONDS)
     else:
-        raise Exception("Unexpected offline store config")
+        msg = "Unexpected offline store config"
+        raise Exception(msg)
 
     materialization_schema = feature_definition.materialization_schema
     hive_columns = {}
     for col, data_type in materialization_schema.column_name_and_data_types():
         assert data_type in TECTON_DATA_TYPE_TO_HIVE_TYPES, f"Unexpected data type {data_type}"
         hive_columns[col] = TECTON_DATA_TYPE_TO_HIVE_TYPES[data_type]
 
     if feature_definition.is_temporal_aggregate:
         if tecton_spark.offline_store.ANCHOR_TIME not in hive_columns:
-            raise Exception("Expected to find _anchor_time in materialized FeatureView schema")
+            msg = "Expected to find _anchor_time in materialized FeatureView schema"
+            raise Exception(msg)
         hive_columns[
             tecton_spark.offline_store.ANCHOR_TIME
         ] = "bigint"  # We have a bug somewhere - the materialization_schema indicates that the spark type for anchor_time is 4-byte integer. But we need 8 bytes.
 
-    if feature_definition.is_continuous:
+    # Athena querytree expects the registered table to match the materialization schema.
+    # We can remove this special handling when non QT is removed.
+    if feature_definition.is_continuous and conf.get_or_none("ALPHA_ATHENA_COMPUTE_ENABLED"):
         if "timestamp" in hive_columns:
             # Special madness about continuous aggregates
             # The parquet files contain both a `timestamp` and an `_anchor_time` field. Same value in both, just a different type
             del hive_columns["timestamp"]
 
     if partition_by in hive_columns:
         del hive_columns[partition_by]
@@ -173,12 +180,13 @@
 
     return athena_table_spec
 
 
 def generate_sql_table_from_pandas_df(df: pandas.DataFrame, table_name: str, session: AthenaSession) -> str:
     """Generate a TABLE from pandas.DataFrame. Returns the sql query to select * from the table"""
     if session is None:
-        raise ValueError("Session must be provided")
+        msg = "Session must be provided"
+        raise ValueError(msg)
 
     session.sql(f"DROP TABLE IF EXISTS {table_name}")
     table_expr = session.write_pandas(df, table_name)
     return f"SELECT * FROM {table_expr}"
```

### Comparing `tecton-0.7.0b9/tecton_athena/odfv_helper.py` & `tecton-0.7.0rc0/tecton_athena/odfv_helper.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,21 +1,24 @@
+import logging
 from typing import Dict
 from typing import List
 
 import pandas
 
 from tecton_athena import pipeline_helper
 from tecton_core import feature_set_config
-from tecton_core import logger as logger_lib
 from tecton_core.errors import TectonValidationError
 from tecton_core.feature_definition_wrapper import FeatureDefinitionWrapper as FeatureDefinition
 from tecton_core.feature_set_config import FeatureSetConfig
 from tecton_proto.args.transformation_pb2 import TransformationMode
 
-logger = logger_lib.get_logger("AthenaConnector")
+
+logger = logging.getLogger(__name__)
+
+# Staging all of the following for deletion once Athena migration to querytree is complete
 
 
 def get_odfvs_from_feature_set_config(feature_set_config: FeatureSetConfig):
     feature_set_items = feature_set_config.definitions_and_configs
     odfvs = [fd.feature_definition for fd in feature_set_items if fd.feature_definition.is_on_demand]
     return odfvs
 
@@ -28,56 +31,54 @@
     odfv_transformation_node = odfv.pipeline.root.transformation_node
 
     for input in odfv_transformation_node.inputs:
         input_name = input.arg_name
         input_df = None
 
         if input.node.HasField("request_data_source_node"):
-            request_context_schema = input.node.request_data_source_node.request_context.schema
-            request_context_fields = [f.name for f in request_context_schema.fields]
+            request_context_schema = input.node.request_data_source_node.request_context.tecton_schema
+            request_context_fields = [c.name for c in request_context_schema.columns]
 
             for f in request_context_fields:
                 if f not in data_df.columns:
-                    raise TectonValidationError(
-                        f"ODFV {odfv.name} has a dependency on the Request Data Source named '{input_name}'. Field {f} of this Request Data Source is not found in the spine. Available columns: {list(data_df.columns)}"
-                    )
+                    msg = f"ODFV {odfv.name} has a dependency on the Request Data Source named '{input_name}'. Field {f} of this Request Data Source is not found in the spine. Available columns: {list(data_df.columns)}"
+                    raise TectonValidationError(msg)
 
             input_df = data_df[request_context_fields]
         elif input.node.HasField("feature_view_node"):
             fv_features = feature_set_config.find_dependent_feature_set_items(
                 odfv.fco_container, input.node, {}, odfv.id
             )[0]
             select_columns_and_rename_map = {}
             for f in fv_features.features:
                 column_name = f"{fv_features.namespace}__{f}"
                 mapped_name = f
                 select_columns_and_rename_map[column_name] = mapped_name
 
             for f in select_columns_and_rename_map.keys():
                 if f not in data_df.columns:
-                    raise TectonValidationError(
-                        f"ODFV {odfv.name} has a dependency on the Feature View '{input_name}'. Feature {f} of this Feature View is not found in the retrieved historical data. Available columns: {list(data_df.columns)}"
-                    )
+                    msg = f"ODFV {odfv.name} has a dependency on the Feature View '{input_name}'. Feature {f} of this Feature View is not found in the retrieved historical data. Available columns: {list(data_df.columns)}"
+                    raise TectonValidationError(msg)
             # Let's select all of the features of the input FV from data_df
             input_df = data_df.rename(columns=select_columns_and_rename_map)[[*select_columns_and_rename_map.values()]]
         else:
-            raise Exception(f"Unexpected input found ({input_name}) on ODFV {odfv.name}")
+            msg = f"Unexpected input found ({input_name}) on ODFV {odfv.name}"
+            raise Exception(msg)
 
         odfv_invocation_inputs[input_name] = input_df
 
     return odfv_invocation_inputs
 
 
 def _run_odfv(data_df: pandas.DataFrame, odfv: FeatureDefinition) -> pandas.DataFrame:
     transformation_mode = odfv.transformations[0].transformation_mode
 
     odfv_pipeline = pipeline_helper.build_odfv_execution_pipeline(
         pipeline=odfv.pipeline, transformations=odfv.transformations, name=odfv.name
     )
-
     if transformation_mode == TransformationMode.TRANSFORMATION_MODE_PANDAS:
         odfv_inputs = _extract_inputs_for_odfv_from_data(data_df, odfv)
         odfv_result_df = odfv_pipeline.execute_with_inputs(odfv_inputs)
         return odfv_result_df
     elif transformation_mode == TransformationMode.TRANSFORMATION_MODE_PYTHON:
         odfv_inputs = _extract_inputs_for_odfv_from_data(data_df, odfv)
 
@@ -103,15 +104,16 @@
             for input_name in odfv_inputs.keys():
                 row_odfv_inputs[input_name] = odfv_inputs[input_name][row_index]
 
             odfv_result_dict = odfv_pipeline.execute_with_inputs(row_odfv_inputs)
             odfv_result_list.append(odfv_result_dict)
         return pandas.DataFrame.from_dict(odfv_result_list)
     else:
-        raise TectonValidationError(f"ODFV {odfv.name} has an unexpected transformation mode: {transformation_mode}")
+        msg = f"ODFV {odfv.name} has an unexpected transformation mode: {transformation_mode}"
+        raise TectonValidationError(msg)
 
 
 def run_and_append_on_demand_features_to_historical_data(data_df: pandas.DataFrame, odfvs: List[FeatureDefinition]):
     for odfv in odfvs:
         odfv_result_df = _run_odfv(data_df, odfv)
         # We're performing an index based merge between the historical data and the ODFV results
         # That's safe to do because the ODFV function isn't expected to change the order of rows
```

### Comparing `tecton-0.7.0b9/tecton_athena/sql_helper.py` & `tecton-0.7.0rc0/tecton_athena/sql_helper.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,46 +1,42 @@
-from collections import defaultdict
+import logging
 from dataclasses import dataclass
 from datetime import datetime
 from datetime import timedelta
 from typing import Dict
 from typing import List
 from typing import Optional
-from typing import Set
 from typing import Tuple
 from typing import Union
 
 import numpy
 import pandas
 import pendulum
 import sqlparse
 
 import tecton_spark.offline_store
 from tecton_athena import odfv_helper
 from tecton_athena.athena_session import get_session
-from tecton_athena.data_catalog_helper import generate_sql_table_from_pandas_df
 from tecton_athena.data_catalog_helper import PARTITION_TYPE_DATESTR
 from tecton_athena.data_catalog_helper import PARTITION_TYPE_UNIX_EPOCH_NS
+from tecton_athena.data_catalog_helper import generate_sql_table_from_pandas_df
 from tecton_athena.data_catalog_helper import register_feature_view_as_athena_table_if_necessary
-from tecton_athena.pipeline_helper import pipeline_to_sql_string
 from tecton_athena.templates_utils import load_template
-from tecton_core import logger as logger_lib
 from tecton_core import time_utils
 from tecton_core.errors import START_TIME_NOT_BEFORE_END_TIME
 from tecton_core.errors import TectonAthenaNotImplementedError
 from tecton_core.errors import TectonValidationError
 from tecton_core.feature_definition_wrapper import FeatureDefinitionWrapper as FeatureDefinition
 from tecton_core.feature_set_config import FeatureSetConfig
-from tecton_core.materialization_context import BoundMaterializationContext
 from tecton_core.offline_store import window_size_seconds
 from tecton_proto.common import aggregation_function_pb2 as afpb
 from tecton_proto.data import feature_view_pb2
 
 
-logger = logger_lib.get_logger("AthenaConnector")
+logger = logging.getLogger(__name__)
 
 SECONDS_TO_NANOSECONDS = 1000 * 1000 * 1000
 
 HISTORICAL_FEATURES_TEMPLATE = load_template("historical_features.sql")
 TIME_LIMIT_TEMPLATE = load_template("time_limit.sql")
 MATERIALIZATION_TILE_TEMPLATE = load_template("materialization_tile.sql")
 PARTIAL_AGGREGATION_TEMPLATE = load_template("run_partial_aggregation.sql")
@@ -147,30 +143,32 @@
 
         if fd.is_on_demand:
             continue
 
         offline_store_type = fd.offline_store_config.WhichOneof("store_type")
 
         if offline_store_type not in ("parquet", "delta"):
-            raise TectonValidationError(f"Offline store {offline_store_type} is not supported")
+            msg = f"Offline store {offline_store_type} is not supported"
+            raise TectonValidationError(msg)
 
         # Change the feature view name if it's for internal udf use.
         if item.namespace.startswith("_udf_internal"):
             name = item.namespace.upper()
         else:
             name = fd.name
 
         join_keys = {key: value for key, value in item.join_keys}
         features = [
             col_name
             for col_name in fd.view_schema.column_names()
             if col_name not in (list(join_keys.keys()) + [fd.timestamp_key])
         ]
         if len(fd.online_serving_index.join_keys) != len(fd.join_keys):
-            raise TectonAthenaNotImplementedError("Wildcard is not supported for Athena")
+            msg = "Wildcard is not supported for Athena"
+            raise TectonAthenaNotImplementedError(msg)
 
         if spine_info is not None:
             feature_start_time, feature_end_time = _get_feature_selection_time_bounds_from_spine_time_range(
                 feature_definition=fd, spine_min_ts=spine_info.min_ts, spine_max_ts=spine_info.max_ts
             )
         else:
             feature_start_time, feature_end_time = _get_feature_selection_time_bounds(
@@ -222,15 +220,16 @@
         dt = pendulum.from_timestamp(time_utils.align_epoch_downwards(unix_timestamp_seconds, partition_size_seconds))
         partition_format = tecton_spark.offline_store._timestamp_formats(partition_size).python_format
         return dt.strftime(partition_format)
     elif partition_type == PARTITION_TYPE_UNIX_EPOCH_NS:
         unix_timestamp_ns = int(timestamp.timestamp() * SECONDS_TO_NANOSECONDS)
         return time_utils.align_epoch_downwards(unix_timestamp_ns, partition_size_seconds * SECONDS_TO_NANOSECONDS)
     else:
-        raise Exception("Unexpected partition_type")
+        msg = "Unexpected partition_type"
+        raise Exception(msg)
 
 
 def _ensure_time_is_pendulum(time):
     if time is None:
         return None
 
     if isinstance(time, pendulum.DateTime):
@@ -238,15 +237,16 @@
 
     if isinstance(time, (int, float)):
         return pendulum.from_timestamp(time)
 
     if isinstance(time, datetime):
         return pendulum.instance(time)
 
-    raise Exception(f"Unexpected Time type {type(time)}")
+    msg = f"Unexpected Time type {type(time)}"
+    raise Exception(msg)
 
 
 # Todo: Refactor. Fairly ugly function
 def _feature_view_select_all_sql(feature_definition: FeatureDefinition, start_time, end_time):
     athena_table = register_feature_view_as_athena_table_if_necessary(feature_definition, session=get_session())
 
     source = None
@@ -305,17 +305,16 @@
     # Currently only work with full aggregation.
     spine: Optional[str] = None,
     spine_timestamp_key: Optional[str] = None,
     spine_keys: Optional[List[str]] = None,
     mock_sql_inputs: Optional[Dict[str, str]] = None,
 ) -> str:
     if not feature_definition.writes_to_offline_store:
-        raise Exception(
-            f"FeatureView {feature_definition.name} does not have offline materialization enabled. Cannot proceed. Please enable offline materialization."
-        )
+        msg = f"FeatureView {feature_definition.name} does not have offline materialization enabled. Cannot proceed. Please enable offline materialization."
+        raise Exception(msg)
 
     materialized_sql = _feature_view_select_all_sql(
         feature_definition, start_time=feature_start_time, end_time=feature_end_time
     )
 
     if feature_definition.is_temporal_aggregate:
         if aggregation_level == "full":
@@ -328,15 +327,16 @@
                 spine=spine,
                 spine_timestamp_key=spine_timestamp_key,
                 spine_keys=spine_keys,
                 batch_schedule=int(feature_definition.batch_materialization_schedule.total_seconds()),
             )
             return _format_sql(aggregated_sql_str)
         else:
-            raise ValueError(f"Unsupported aggregation level: {aggregation_level}")
+            msg = f"Unsupported aggregation level: {aggregation_level}"
+            raise ValueError(msg)
 
     else:
         return _format_sql(materialized_sql)
 
 
 @dataclass
 class _SpineInfo:
@@ -365,19 +365,21 @@
         expected_spine_columns = set([c.lower() for c in expected_spine_keys])
         expected_spine_columns.add(expected_timestamp_key.lower())
 
         non_key_columns_in_spine = spine_columns - expected_spine_columns
         missing_columns_in_spine = expected_spine_columns - spine_columns
 
         if len(missing_columns_in_spine) > 0:
-            raise Exception(f"Expected to find the following columns in the spine: {missing_columns_in_spine}")
+            msg = f"Expected to find the following columns in the spine: {missing_columns_in_spine}"
+            raise Exception(msg)
 
         spine_timestamp_type = spine.dtypes[expected_timestamp_key].type
         if spine_timestamp_type != pandas.Timestamp and spine_timestamp_type != numpy.datetime64:
-            raise Exception(f"Spine timestamp column must be of type Timestamp. It's of type {spine_timestamp_type}")
+            msg = f"Spine timestamp column must be of type Timestamp. It's of type {spine_timestamp_type}"
+            raise Exception(msg)
 
         spine_min_ts = pendulum.instance(spine[expected_timestamp_key].min())
         spine_max_ts = pendulum.instance(spine[expected_timestamp_key].max())
 
         spine_table_name = get_session().get_spine_temp_table_name()
         generate_sql_table_from_pandas_df(df=spine, session=get_session(), table_name=spine_table_name)
         return _SpineInfo(
@@ -385,15 +387,16 @@
             timestamp_key=expected_timestamp_key,
             min_ts=spine_min_ts,
             max_ts=spine_max_ts,
             spine_keys=expected_spine_keys,
             contains_non_join_key_columns=len(non_key_columns_in_spine) > 0,
         )
     else:
-        raise TectonAthenaNotImplementedError("Only pandas based spines are currently supported")
+        msg = "Only pandas based spines are currently supported"
+        raise TectonAthenaNotImplementedError(msg)
 
 
 def _drop_internal_columns_from_ghf_result(data_df: pandas.DataFrame) -> pandas.DataFrame:
     internal_columns = [c for c in data_df.columns if c.startswith("_udf_internal")]
     return data_df.drop(columns=internal_columns)
 
 
@@ -425,105 +428,7 @@
         # ODFVs dependent FVs may have pulled in internal columns that need to be dropped
         final_feature_data_df = _drop_internal_columns_from_ghf_result(internal_feature_data_df)
 
         return final_feature_data_df
     finally:
         if spine_table:
             get_session().sql(f"DROP TABLE IF EXISTS {spine_table}")
-
-
-def generate_run_batch_sql(
-    fd: FeatureDefinition,
-    # start is inclusive and end is exclusive
-    feature_start_time: datetime,
-    feature_end_time: datetime,
-    aggregation_level: str,
-    mock_sql_inputs: Optional[Dict[str, str]],
-) -> str:
-    materialization_context = BoundMaterializationContext._create_internal(
-        pendulum.instance(feature_start_time), pendulum.instance(feature_end_time), fd.batch_materialization_schedule
-    )
-
-    pipeline_sql = pipeline_to_sql_string(
-        pipeline=fd.pipeline,
-        data_sources=fd.data_sources,
-        transformations=fd.transformations,
-        materialization_context=materialization_context,
-        mock_sql_inputs=mock_sql_inputs,
-    )
-    materialized_sql = get_materialization_query(
-        feature_definition=fd,
-        source_sql=pipeline_sql,
-        feature_start_time=feature_start_time,
-        feature_end_time=feature_end_time,
-    )
-
-    if not fd.is_temporal_aggregate:
-        return materialized_sql
-    else:
-        if aggregation_level == "disabled":
-            sql_str = TIME_LIMIT_TEMPLATE.render(
-                source=pipeline_sql,
-                timestamp_key=fd.timestamp_key,
-                start_time=feature_start_time,
-                end_time=feature_end_time,
-            )
-        elif aggregation_level == "partial":
-            # Rename the output columns, and add tile start/end time columns
-            sql_str = PARTIAL_AGGREGATION_TEMPLATE.render(
-                source=materialized_sql,
-                join_keys=fd.join_keys,
-                aggregations=_get_feature_view_aggregations(fd),
-                slide_interval=fd.aggregate_slide_interval,
-                slide_interval_string=fd.get_aggregate_slide_interval_string,
-                timestamp_key=fd.timestamp_key,
-            )
-        elif aggregation_level == "full":
-            sql_str = FULL_AGGREGATION_TEMPLATE.render(
-                source=materialized_sql,
-                join_keys=fd.join_keys,
-                aggregation=fd.trailing_time_window_aggregation,
-                timestamp_key=fd.timestamp_key,
-                name=fd.name,
-                batch_schedule=int(fd.batch_materialization_schedule.total_seconds()),
-            )
-        else:
-            raise ValueError(f"Unsupported aggregation level: {aggregation_level}")
-
-        return _format_sql(sql_str)
-
-
-def get_materialization_query(
-    fd: FeatureDefinition,
-    sql_query: str,
-    feature_start_time: datetime,
-    feature_end_time: datetime,
-):
-    if fd.is_temporal_aggregate:
-        sql_query = MATERIALIZATION_TILE_TEMPLATE.render(
-            source=sql_query,
-            join_keys=fd.join_keys,
-            aggregations=_get_feature_view_aggregations(fd),
-            slide_interval=fd.aggregate_slide_interval,
-            timestamp_key=fd.timestamp_key,
-        )
-    return _format_sql(
-        TIME_LIMIT_TEMPLATE.render(
-            source=sql_query,
-            timestamp_key=fd.timestamp_key,
-            start_time=feature_start_time,
-            end_time=feature_end_time,
-        )
-    )
-
-
-def _get_feature_view_aggregations(fd: FeatureDefinition) -> Dict[str, Set[str]]:
-    aggregations = defaultdict(set)
-    for agg_feature in fd.fv_spec.aggregate_features:
-        aggregate_function = AGGREGATION_PLANS[agg_feature.function]
-        aggregations[agg_feature.input_feature_name].update(aggregate_function)
-
-    # Need to order the functions for deterministic results.
-    for key, value in aggregations.items():
-        aggregations[key] = sorted(value)
-
-    return aggregations
```

### Comparing `tecton-0.7.0b9/tecton_athena/templates/create_table.sql` & `tecton-0.7.0rc0/tecton_athena/templates/create_table.sql`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_athena/templates/historical_features.sql` & `tecton-0.7.0rc0/tecton_athena/templates/historical_features.sql`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_athena/templates/materialization_tile.sql` & `tecton-0.7.0rc0/tecton_athena/templates/materialization_tile.sql`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_athena/templates/run_full_aggregation.sql` & `tecton-0.7.0rc0/tecton_athena/templates/run_full_aggregation.sql`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_athena/templates/time_limit.sql` & `tecton-0.7.0rc0/tecton_athena/templates/time_limit.sql`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_athena/templates_utils.py` & `tecton-0.7.0rc0/tecton_athena/templates_utils.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_core/conf.py` & `tecton-0.7.0rc0/tecton_core/conf.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,34 +1,32 @@
+import builtins
+import contextlib
 import json
 import logging
 import os
 import sys
 from enum import Enum
 from pathlib import Path
 from typing import Any
 from typing import Dict
 from typing import Iterable
 from typing import List
 from typing import Optional
 
 from tecton_core import errors
 
+
 logging.getLogger("boto3").setLevel(logging.ERROR)
 logging.getLogger("botocore").setLevel(logging.ERROR)
 
 _ConfigSettings = Dict[str, Any]
 
 _CONFIG_OVERRIDES: _ConfigSettings = {}
 
 
-# Tecton conf unfortunately defines a top-level "set" method which shadows the "set" data type. Before defining the
-# set method, get a reference to the set data type.
-_set_type = set
-
-
 class ConfSource(Enum):
     # (Always supported) This key can be overriden in the Python runtime.
     SESSION_OVERRIDE = 1
     # (Always supported) This key can be read from the local environment.
     OS_ENV = 2
     # This key can be written to and read from the tecton config file.
     LOCAL_TECTON_CONFIG = 3
@@ -90,14 +88,27 @@
     del _CONFIG_OVERRIDES[key]
 
 
 def _set(key, value):
     _CONFIG_OVERRIDES[key] = value
 
 
+@contextlib.contextmanager
+def _temporary_set(key, value):
+    curr_val = _CONFIG_OVERRIDES.get(key)
+    _CONFIG_OVERRIDES[key] = value
+    try:
+        yield
+    finally:
+        if curr_val:
+            _CONFIG_OVERRIDES[key] = curr_val
+        else:
+            del _CONFIG_OVERRIDES[key]
+
+
 def _does_key_have_valid_prefix(key) -> bool:
     for prefix in _VALID_KEY_PREFIXES:
         if key.startswith(prefix):
             return True
     return False
 
 
@@ -106,15 +117,16 @@
     _Debugger.preamble(key)
 
     if key in _VALID_KEYS_TO_ALLOWED_SOURCES:
         allowed_sources = _VALID_KEYS_TO_ALLOWED_SOURCES[key]
     elif _does_key_have_valid_prefix(key):
         allowed_sources = DEFAULT_ALLOWED_SOURCES
     else:
-        raise errors.TectonInternalError(f"Tried accessing invalid configuration key '{key}'")
+        msg = f"Tried accessing invalid configuration key '{key}'"
+        raise errors.TectonInternalError(msg)
 
     # Session-scoped override.
     if ConfSource.SESSION_OVERRIDE in allowed_sources:
         val = _CONFIG_OVERRIDES.get(key)
         _Debugger.print(ConfSource.SESSION_OVERRIDE, key, val)
         if val is not None:  # NOTE: check explicitly against None so we can set a value to False or ""
             return val
@@ -147,22 +159,28 @@
         if val is not None:  # NOTE: check explicitly against None so we can set a value to False or ""
             return val
 
     if _get_runtime_env() == TectonEnv.UNKNOWN:
         # Fallback attempt to set env if user has not set it.
         _set_tecton_runtime_env()
 
+    # NOTE: although originally intended for internal use. Some customers have
+    # found this configuration or have required this behavior. Care should be
+    # exercised in changing any behavior here to ensure no customer breakages.
     if ConfSource.DATABRICKS_SECRET in allowed_sources and not _get_runtime_only("TECTON_CONF_DISABLE_DBUTILS"):
         # Databricks secrets
         for scope in _get_secret_scopes():
             value = _get_from_db_secrets(key, scope)
             _Debugger.print(ConfSource.DATABRICKS_SECRET, key, value, details=f"{scope}:{key}")
             if value is not None:  # NOTE: check explicitly against None so we can set a value to False or ""
                 return value
 
+    # NOTE: although originally intended for internal use. Some customers have
+    # found this configuration or have required this behavior. Care should be
+    # exercised in changing any behavior here to ensure no customer breakages.
     if ConfSource.AWS_SECRET_MANAGER in allowed_sources and not _get_runtime_only("TECTON_CONF_DISABLE_AWS_SECRETS"):
         # AWS secret manager
         for scope in _get_secret_scopes():
             value = _get_from_secretsmanager(key, scope)
             _Debugger.print(ConfSource.AWS_SECRET_MANAGER, key, value, details=f"{scope}/{key}")
             if value is not None:  # NOTE: check explicitly against None so we can set a value to False or ""
                 return value
@@ -175,21 +193,22 @@
     _Debugger.print(ConfSource.NOT_FOUND, key)
     return None
 
 
 def _get_runtime_only(key) -> Optional[str]:
     """An alternate _get() that will look up only from runtime sources. Used to avoid infinite loops."""
     if key not in _VALID_KEYS_TO_ALLOWED_SOURCES:
-        raise errors.TectonInternalError(f"_get_runtime_only should only used with valid keys. {key}")
+        msg = f"_get_runtime_only should only used with valid keys. {key}"
+        raise errors.TectonInternalError(msg)
 
     allowed_sources = _VALID_KEYS_TO_ALLOWED_SOURCES[key]
-    if _set_type(allowed_sources) != _set_type(RUNTIME_ALLOWED_SOURCES):
-        raise errors.TectonInternalError(
-            f"_get_runtime_only should only used with keys that only allow run time sources."
-        )
+    # Use `builtins.set` since we shadowed the `set` built-in in this module.
+    if builtins.set(allowed_sources) != builtins.set(RUNTIME_ALLOWED_SOURCES):
+        msg = "_get_runtime_only should only used with keys that only allow run time sources."
+        raise errors.TectonInternalError(msg)
 
     # Session-scoped override.
     val = _CONFIG_OVERRIDES.get(key)
     if val is not None:  # NOTE: check explicitly against None so we can set a value to False or ""
         return val
 
     # Environment variable.
@@ -207,32 +226,35 @@
 def get_or_none(key) -> Optional[str]:
     return _get(key)
 
 
 def get_or_raise(key) -> str:
     val = _get(key)
     if val is None:
-        raise errors.TectonInternalError(f"{key} not set")
+        msg = f"{key} not set"
+        raise errors.TectonInternalError(msg)
     return val
 
 
 def get_bool(key) -> bool:
     val = _get(key)
     if val is None:
         return False
     # bit of a hack for if people set a boolean value in a local override
     if isinstance(val, bool):
         return val
     if not isinstance(val, str):
-        raise ValueError(f"{key} should be an instance of str, not {type(val)}")
+        msg = f"{key} should be an instance of str, not {type(val)}"
+        raise ValueError(msg)
     if val.lower() in {"yes", "true"}:
         return True
     if val.lower() in {"no", "false"}:
         return False
-    raise ValueError(f"{key} should be 'true' or 'false', not {val}")
+    msg = f"{key} should be 'true' or 'false', not {val}"
+    raise ValueError(msg)
 
 
 # Internal
 
 _LOCAL_TECTON_CONFIG_FILE = Path(os.environ.get("TECTON_CONFIG_PATH", Path.home() / ".tecton/config"))
 _LOCAL_TECTON_TOKENS_FILE = _LOCAL_TECTON_CONFIG_FILE.with_suffix(".tokens")
 
@@ -270,14 +292,15 @@
     "TECTON_CONF_DISABLE_DBUTILS": RUNTIME_ALLOWED_SOURCES,
     "TECTON_CONF_DISABLE_AWS_SECRETS": RUNTIME_ALLOWED_SOURCES,
     "TECTON_DEBUG": RUNTIME_ALLOWED_SOURCES,
     "TECTON_RUNTIME_ENV": RUNTIME_ALLOWED_SOURCES,
     "TECTON_RUNTIME_MODE": RUNTIME_ALLOWED_SOURCES,
     "TECTON_VALIDATION_MODE": RUNTIME_ALLOWED_SOURCES,
     "TECTON_FORCE_FUNCTION_SERIALIZATION": RUNTIME_ALLOWED_SOURCES,
+    "SKIP_OBJECT_VERSION_CHECK": RUNTIME_ALLOWED_SOURCES,
     "ALPHA_ATHENA_COMPUTE_ENABLED": DEFAULT_ALLOWED_SOURCES,
     "SQL_DIALECT": DEFAULT_ALLOWED_SOURCES,
     "ATHENA_S3_PATH": DEFAULT_ALLOWED_SOURCES,
     "ATHENA_DATABASE": DEFAULT_ALLOWED_SOURCES,
     "ENABLE_TEMPO": DEFAULT_ALLOWED_SOURCES,
     "QUERY_REWRITE_ENABLED": DEFAULT_ALLOWED_SOURCES,
     "ALPHA_SNOWFLAKE_SNOWPARK_ENABLED": DEFAULT_ALLOWED_SOURCES,
@@ -293,28 +316,30 @@
     "TECTON_API_KEY": DEFAULT_ALLOWED_SOURCES,
     "QUERYTREE_SHORT_SQL_ENABLED": RUNTIME_ALLOWED_SOURCES,
     "REDSHIFT_USER": DEFAULT_ALLOWED_SOURCES,
     "REDSHIFT_PASSWORD": DEFAULT_ALLOWED_SOURCES,
     "SKIP_FEATURE_TIMESTAMP_VALIDATION": DEFAULT_ALLOWED_SOURCES,
     "SNOWFLAKE_ACCOUNT_IDENTIFIER": DEFAULT_ALLOWED_SOURCES,
     "SNOWFLAKE_DEBUG": DEFAULT_ALLOWED_SOURCES,
-    "SNOWFLAKE_SHORT_SQL_ENABLED": DEFAULT_ALLOWED_SOURCES,  # Whether to break up long SQL statements into multiple queries for Snowflake
+    "SNOWFLAKE_SHORT_SQL_ENABLED": DEFAULT_ALLOWED_SOURCES,  # Whether to break up long SQL statements into temporary views for Snowflake
+    "SNOWFLAKE_TEMP_TABLE_ENABLED": DEFAULT_ALLOWED_SOURCES,  # Whether to break up long SQL statements with temporary tables for Snowflake, takes precedence over SNOWFLAKE_SHORT_SQL_ENABLED
     "SNOWFLAKE_USER": DEFAULT_ALLOWED_SOURCES,
     "SNOWFLAKE_PASSWORD": DEFAULT_ALLOWED_SOURCES,
     "SNOWFLAKE_WAREHOUSE": DEFAULT_ALLOWED_SOURCES,
     "SNOWFLAKE_DATABASE": DEFAULT_ALLOWED_SOURCES,
+    "REDIS_AUTH_TOKEN": DEFAULT_ALLOWED_SOURCES,
 }
 
 _VALID_KEY_PREFIXES = ["SECRET_"]
 
 
 def _snowpark_enabled():
     ## Try to import snowpark, if it fails, return False
     try:
-        import snowflake.snowpark
+        import snowflake.snowpark  # noqa: F401
 
         return "true"
     except ImportError:
         return "false"
 
 
 _DEFAULTS = {
@@ -467,17 +492,16 @@
 
 def validate_api_service_url(url: str):
     """Validate Tecton API URL.
     Returns nothing for valid URLs or raises an error."""
     if "localhost" in url or "ingress" in url:
         return
     if not url.endswith("/api"):
-        raise errors.TectonAPIValidationError(
-            f'Tecton API URL ("{url}") should be formatted "https://<deployment>.tecton.ai/api"'
-        )
+        msg = f'Tecton API URL ("{url}") should be formatted "https://<deployment>.tecton.ai/api"'
+        raise errors.TectonAPIValidationError(msg)
 
 
 # Config values written to and read from the local .tecton/config file.
 _LOCAL_TECTON_CONFIG: _ConfigSettings = {}
 
 # Config values read from the local .tecton/config.tokens file.
 _LOCAL_TECTON_TOKENS: _ConfigSettings = {}
```

### Comparing `tecton-0.7.0b9/tecton_core/data_types.py` & `tecton-0.7.0rc0/tecton_core/data_types.py`

 * *Files 10% similar despite different names*

```diff
@@ -137,14 +137,20 @@
     @property
     def element_type(self) -> DataType:
         return self._element_type
 
     def __str__(self) -> str:
         return f"Array({self._element_type})"
 
+    # TODO: have this work for SQL types
+    @property
+    def sql_type(self) -> SqlTypes:
+        # Used for safe typecasting in sql-based compute platforms
+        raise NotImplementedError
+
 
 # Note StructField does not inherit from DataType. This is because it is not directly convertable to a data type proto.
 class StructField:
     def __init__(self, name: str, data_type: DataType) -> None:
         self._name = name
         self._data_type = data_type
 
@@ -178,14 +184,40 @@
     def fields(self) -> List[StructField]:
         return self._fields
 
     def __str__(self) -> str:
         fields_string = ", ".join(str(field) for field in self._fields)
         return f"Struct({fields_string})"
 
+    # TODO: have this work for SQL types
+    @property
+    def sql_type(self) -> SqlTypes:
+        # Used for safe typecasting in sql-based compute platforms
+        raise NotImplementedError
+
+
+class MapType(DataType):
+    def __init__(self, key_type: DataType, value_type: DataType):
+        self._key_type = key_type
+        self._value_type = value_type
+
+    @property
+    def proto(self) -> data_type_pb2.DataType:
+        return data_type_pb2.DataType(
+            type=data_type_pb2.DATA_TYPE_MAP, map_key_type=self._key_type.proto, map_value_type=self._value_type.proto
+        )
+
+    @property
+    def key_type(self) -> DataType:
+        return self._key_type
+
+    @property
+    def value_type(self) -> DataType:
+        return self._value_type
+
 
 def data_type_from_proto(proto: data_type_pb2.DataType) -> DataType:
     """
     Factory method to creata a DataType python class from a `tecton_proto.common.DataType` proto.
     """
     assert proto
     assert proto.type
@@ -207,9 +239,12 @@
     elif proto.type == data_type_pb2.DATA_TYPE_ARRAY:
         assert proto.array_element_type
         element_type = data_type_from_proto(proto.array_element_type)
         return ArrayType(element_type)
     elif proto.type == data_type_pb2.DATA_TYPE_STRUCT:
         fields = [StructField(field.name, data_type_from_proto(field.data_type)) for field in proto.struct_fields]
         return StructType(fields)
+    elif proto.type == data_type_pb2.DATA_TYPE_MAP:
+        return MapType(data_type_from_proto(proto.map_key_type), data_type_from_proto(proto.map_value_type))
     else:
-        raise ValueError(f"Unexpected data type {proto}")
+        msg = f"Unexpected data type {proto}"
+        raise ValueError(msg)
```

### Comparing `tecton-0.7.0b9/tecton_core/errors.py` & `tecton-0.7.0rc0/tecton_core/errors.py`

 * *Files 5% similar despite different names*

```diff
@@ -74,14 +74,21 @@
 def UDF_ERROR(error: Exception):
     return TectonValidationError(
         "UDF Error: please review and ensure the correctness of your feature definition and the input data passed in. Otherwise please contact Tecton Support for assistance."
         + f" Running the transformation resulted in the following error: {type(error).__name__}: {str(error)} "
     )
 
 
+def UDF_TYPE_ERROR(error: Exception):
+    return TectonValidationError(
+        "UDF Type Error: please ensure that your UDF correctly handles the typing of row values. Make sure to cast dataframe values to the correct type and ensure that you are handling"
+        + f" null column values correctly in your UDF. Running the transformation resulted in the following error: {type(error).__name__}: {str(error)} "
+    )
+
+
 def INVALID_SPINE_SQL(error: Exception):
     return TectonValidationError(
         f"Invalid SQL: please review your SQL for the spine passed in. Received error: {type(error).__name__}: {str(error)} "
     )
 
 
 def START_TIME_NOT_BEFORE_END_TIME(start_time: datetime, end_time: datetime):
@@ -111,23 +118,32 @@
 class TectonAthenaNotImplementedError(NotImplementedError):
     """
     Exception that indicates a feature is not yet implemented with Athena compute.
     """
 
 
 FV_BFC_SINGLE_FROM_SOURCE = TectonValidationError(
-    f"Computing features from source is not supported for Batch Feature Views with incremental_backfills set to True. "
+    "Computing features from source is not supported for Batch Feature Views with incremental_backfills set to True. "
     + "Enable offline materialization for this feature view in a live workspace to use `get_historical_features()`. Alternatively, use `run()` to test this feature view without materializing data."
 )
-FV_NEEDS_TO_BE_MATERIALIZED = lambda fv_name: TectonValidationError(
-    f"Feature View '{fv_name}' has not been configured for materialization. "
-    + "Please use from_source=True when getting features or "
-    + "configure offline materialization for this Feature View in a live workspace."
-)
+
+
+def FV_NEEDS_TO_BE_MATERIALIZED(fv_name):
+    return TectonValidationError(
+        f"Feature View '{fv_name}' has not been configured for materialization. "
+        + "Please use from_source=True when getting features or "
+        + "configure offline materialization for this Feature View in a live workspace."
+    )
+
 
 FT_DF_TOO_LARGE = TectonValidationError(
     "Dataframe too large for a single ingestion, consider splitting into smaller ones"
 )
 
 
 def FT_UPLOAD_FAILED(reason):
     return TectonValidationError(f"Failed to upload dataframe: {reason}")
+
+
+SNOWFLAKE_CONNECTION_NOT_SET = TectonValidationError(
+    "Snowflake connection not configuered. Please set Snowflake connection using tecton.snowflake_context.set_connection(connection). https://docs.tecton.ai/docs/setting-up-tecton/connecting-to-a-data-platform/tecton-on-snowflake/connecting-notebooks-to-snowflake"
+)
```

### Comparing `tecton-0.7.0b9/tecton_core/fco_container.py` & `tecton-0.7.0rc0/tecton_core/fco_container.py`

 * *Files 2% similar despite different names*

```diff
@@ -68,15 +68,16 @@
         :return: The root TectonObjectSpec for the container or None. Errors if len(root_ids) > 1
         """
 
         num_root_ids = len(self._root_ids)
         if num_root_ids == 0:
             return None
         elif num_root_ids > 1:
-            raise ValueError(f"Expected a single result but got {num_root_ids}")
+            msg = f"Expected a single result but got {num_root_ids}"
+            raise ValueError(msg)
         else:
             return self.get_by_id(self._root_ids[0])
 
     @typechecked
     def get_root_fcos(self) -> List[specs.TectonObjectSpec]:
         """
         :return: All root TectonObjectSpec for the container.
@@ -96,15 +97,16 @@
     elif fco.HasField("transformation"):
         return specs.TransformationSpec.from_data_proto(fco.transformation, deserialize_funcs_to_main)
     elif fco.HasField("feature_view"):
         return specs.create_feature_view_spec_from_data_proto(fco.feature_view)
     elif fco.HasField("feature_service"):
         return specs.FeatureServiceSpec.from_data_proto(fco.feature_service)
     else:
-        raise ValueError(f"Unexpected fco type: {fco}")
+        msg = f"Unexpected fco type: {fco}"
+        raise ValueError(msg)
 
 
 def _wrap_data_fco(inner_proto) -> fco_pb2.Fco:
     fco = fco_pb2.Fco()
     if isinstance(inner_proto, DataSourceProto):
         fco.virtual_data_source.CopyFrom(inner_proto)
     elif isinstance(inner_proto, Transformation):
```

### Comparing `tecton-0.7.0b9/tecton_core/feature_definition_wrapper.py` & `tecton-0.7.0rc0/tecton_core/feature_definition_wrapper.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 import enum
+import logging
 from typing import Dict
 from typing import List
 from typing import Optional
 
 import attrs
 import pendulum
 from google.protobuf import duration_pb2
@@ -10,27 +11,29 @@
 
 from tecton_core import pipeline_common
 from tecton_core import specs
 from tecton_core import time_utils
 from tecton_core.fco_container import FcoContainer
 from tecton_core.feature_view_utils import CONTINUOUS_MODE_BATCH_INTERVAL
 from tecton_core.id_helper import IdHelper
-from tecton_core.logger import get_logger
 from tecton_core.online_serving_index import OnlineServingIndex
 from tecton_core.schema import Schema
+from tecton_core.specs import utils
 from tecton_proto.args.feature_view_pb2 import OfflineFeatureStoreConfig
 from tecton_proto.args.pipeline_pb2 import DataSourceNode
 from tecton_proto.args.pipeline_pb2 import Pipeline
 from tecton_proto.args.pipeline_pb2 import PipelineNode
 from tecton_proto.common import data_source_type_pb2
+from tecton_proto.common import schema_pb2
 from tecton_proto.common.framework_version_pb2 import FrameworkVersion as FrameworkVersionProto
 from tecton_proto.data import feature_view_pb2
 from tecton_proto.data.feature_view_pb2 import OfflineStoreParams
 
-logger = get_logger("FeatureDefinitionWrapper")
+
+logger = logging.getLogger(__name__)
 
 
 # Create a parallel enum class since Python proto extensions do not use an enum class.
 # Keep up-to-date with FrameworkVersion from tecton_proto/args/version_constraints.proto.
 class FrameworkVersion(enum.Enum):
     UNSPECIFIED = FrameworkVersionProto.UNSPECIFIED
     FWV3 = FrameworkVersionProto.FWV3
@@ -40,27 +43,19 @@
 @attrs.define(frozen=True)
 class FeatureDefinitionWrapper:
     """A container for a Feature View spec and its dependent specs, i.e. data sources, transformations, and entities."""
 
     fv_spec: specs.FeatureViewSpec
     fco_container: FcoContainer
 
-    # As part of the notebook development project, FeatureDefinitionWrapper contains both a feature view data
-    # proto and a feature view spec that represent the same feature view.
-    # TODO(jake): Finish migrating from feature view proto to feature view spec.
-    fv: Optional[feature_view_pb2.FeatureView]
-
     @typechecked
     def __init__(self, feature_view_spec: specs.FeatureViewSpec, fco_container: FcoContainer):
-        # As part of the notebookd development project, FeatureDefinitionWrapper contains both a feature view data
-        # proto and a feature view spec that represent the same feature view.
         return self.__attrs_init__(  # type: ignore
             fv_spec=feature_view_spec,
             fco_container=fco_container,
-            fv=feature_view_spec.data_proto,
         )
 
     @property
     def id(self) -> str:
         return self.fv_spec.id
 
     @property
@@ -135,14 +130,36 @@
         return self.time_key
 
     @property
     def join_keys(self) -> List[str]:
         return list(self.fv_spec.join_keys)
 
     @property
+    def join_keys_schema(self) -> Schema:
+        if self.is_on_demand:
+            # For ODFV, we need to extract its dependent materialized FeatureViewSpec and join key override mapping to correctly build the spine schema.
+            all_fv_nodes = pipeline_common.get_all_feature_view_nodes(self.pipeline)
+            dependent_fv_specs = [
+                self.fco_container.get_by_id_proto(node.feature_view_node.feature_view_id) for node in all_fv_nodes
+            ]
+            jk_overrides = [
+                [
+                    utils.JoinKeyMappingSpec(
+                        spine_column_name=override_join_key.spine_column,
+                        feature_view_column_name=override_join_key.feature_column,
+                    )
+                    for override_join_key in node.feature_view_node.feature_view.override_join_keys
+                ]
+                for node in all_fv_nodes
+            ]
+            return self.fv_spec.join_key_schema(zip(dependent_fv_specs, jk_overrides))
+        else:
+            return self.fv_spec.join_key_schema()
+
+    @property
     def online_serving_index(self) -> OnlineServingIndex:
         return OnlineServingIndex(list(self.fv_spec.online_serving_keys))
 
     @property
     def wildcard_join_key(self) -> Optional[str]:
         """
         Returns a wildcard join key column name for the feature view if it exists;
@@ -193,57 +210,58 @@
             return self.fv_spec.batch_schedule
         else:
             return None
 
     @property
     def batch_materialization_schedule(self) -> pendulum.Duration:
         if not isinstance(self.fv_spec, specs.MaterializedFeatureViewSpec):
-            raise TypeError(
-                f"Feature definition with type {type(self.fv_spec)} does not have a batch_materialization_schedule."
-            )
+            msg = f"Feature definition with type {type(self.fv_spec)} does not have a batch_materialization_schedule."
+            raise TypeError(msg)
 
         if self.fv_spec.batch_schedule is not None:
             return self.fv_spec.batch_schedule
         elif self.fv_spec.is_continuous:
             return time_utils.proto_to_duration(CONTINUOUS_MODE_BATCH_INTERVAL)
         elif self.fv_spec.slide_interval is not None:
             return self.fv_spec.slide_interval
         else:
-            raise ValueError("Materialized feature view must have a batch_materialization_schedule.")
+            msg = "Materialized feature view must have a batch_materialization_schedule."
+            raise ValueError(msg)
 
     @property
     def offline_store_params(self) -> Optional[OfflineStoreParams]:
         return self.fv_spec.offline_store_params
 
     @property
     def max_source_data_delay(self) -> pendulum.Duration:
         if not isinstance(self.fv_spec, specs.MaterializedFeatureViewSpec):
-            raise TypeError(f"Feature definition with type {type(self.fv_spec)} does not have max_source_data_delay.")
+            msg = f"Feature definition with type {type(self.fv_spec)} does not have max_source_data_delay."
+            raise TypeError(msg)
         return self.fv_spec.max_source_data_delay
 
     @property
     def materialization_start_timestamp(self) -> pendulum.datetime:
         if not isinstance(self.fv_spec, specs.MaterializedFeatureViewSpec):
-            raise TypeError(
-                f"Feature definition with type {type(self.fv_spec)} does not have a materialization_start_timestamp."
-            )
+            msg = f"Feature definition with type {type(self.fv_spec)} does not have a materialization_start_timestamp."
+            raise TypeError(msg)
 
         return self.fv_spec.materialization_start_time
 
     @property
     def feature_start_timestamp(self) -> Optional[pendulum.datetime]:
         if not isinstance(self.fv_spec, specs.MaterializedFeatureViewSpec) or self.fv_spec.feature_start_time is None:
             return None
 
         return self.fv_spec.feature_start_time
 
     @property
     def time_range_policy(self) -> feature_view_pb2.MaterializationTimeRangePolicy:
         if isinstance(self.fv_spec, specs.OnDemandFeatureViewSpec) or self.fv_spec.time_range_policy is None:
-            raise ValueError("No materialization time range policy set for this feature view.")
+            msg = "No materialization time range policy set for this feature view."
+            raise ValueError(msg)
 
         return self.fv_spec.time_range_policy
 
     @property
     def data_source_ids(self) -> List[str]:
         if self.pipeline is None:
             return []
@@ -252,22 +270,35 @@
         return [IdHelper.to_string(node.virtual_data_source_id) for node in nodes]
 
     @property
     def data_sources(self) -> List[specs.DataSourceSpec]:
         ds_ids = self.data_source_ids
         return self.fco_container.get_by_ids(ds_ids)
 
+    def get_data_source_with_input_name(self, input_name) -> specs.DataSourceSpec:
+        """Get the data source spec that uses `input_name` for the feature view transformation."""
+        input_name_to_ds_id = pipeline_common.get_input_name_to_ds_id_map(self.pipeline)
+
+        if input_name not in input_name_to_ds_id:
+            msg = (
+                f"Feature view '{self.name}' does not have an input data source with the parameter name '{input_name}'"
+            )
+            raise KeyError(msg)
+
+        return self.fco_container.get_by_id(input_name_to_ds_id[input_name])
+
     @property
     def get_tile_interval(self) -> pendulum.Duration:
         if self.is_temporal_aggregate:
             return self.fv_spec.slide_interval
         elif self.is_temporal:
             return self.fv_spec.batch_schedule
 
-        raise ValueError(f"Invalid invocation on unsupported FeatureView type")
+        msg = "Invalid invocation on unsupported FeatureView type"
+        raise ValueError(msg)
 
     @property
     def get_batch_schedule_for_version(self) -> int:
         return time_utils.convert_timedelta_for_version(
             self.fv_spec.batch_schedule, self.get_feature_store_format_version
         )
 
@@ -278,36 +309,40 @@
                 self.fv_spec.slide_interval, self.get_feature_store_format_version
             )
         elif self.is_temporal:
             return time_utils.convert_timedelta_for_version(
                 self.fv_spec.batch_schedule, self.get_feature_store_format_version
             )
 
-        raise TypeError(f"Invalid invocation on unsupported FeatureView type")
+        msg = "Invalid invocation on unsupported FeatureView type"
+        raise TypeError(msg)
 
     @property
     def get_aggregate_slide_interval_string(self) -> str:
         if not self.is_temporal_aggregate:
-            raise TypeError(f"Invalid invocation on unsupported FeatureView type")
+            msg = "Invalid invocation on unsupported FeatureView type"
+            raise TypeError(msg)
 
         return self.fv_spec.slide_interval_string
 
     @property
     def aggregate_slide_interval(self) -> duration_pb2.Duration:
         if not self.is_temporal_aggregate:
-            raise TypeError(f"Invalid invocation on unsupported FeatureView type")
+            msg = "Invalid invocation on unsupported FeatureView type"
+            raise TypeError(msg)
 
         duration = duration_pb2.Duration()
         duration.FromTimedelta(self.fv_spec.slide_interval)
         return duration
 
     @property
     def materialized_data_path(self) -> str:
         if isinstance(self.fv_spec, specs.OnDemandFeatureViewSpec) or self.fv_spec.materialized_data_path is None:
-            raise ValueError("No materialized data path available.")
+            msg = "No materialized data path available."
+            raise ValueError(msg)
 
         return self.fv_spec.materialized_data_path
 
     @property
     def max_aggregation_window(self) -> Optional[int]:
         if not self.is_temporal_aggregate:
             return None
@@ -354,22 +389,38 @@
 
     @property
     def workspace(self) -> str:
         return self.fv_spec.workspace
 
     @property
     def request_context_keys(self) -> List[str]:
-        if self.pipeline is None:
+        rc_schema = self.request_context_schema
+        if rc_schema is not None:
+            return rc_schema.column_names()
+        else:
             return []
 
+    @property
+    def request_context_schema(self) -> Schema:
+        if self.pipeline is None:
+            return Schema(schema_pb2.Schema())
+
         request_context = pipeline_common.find_request_context(self.pipeline.root)
         if request_context:
-            return [field.name for field in request_context.schema.fields]
+            return Schema(request_context.tecton_schema)
         else:
-            return []
+            return Schema(schema_pb2.Schema())
+
+    # Returns the schema of the spine that can be used to query feature values. Note the actual spine user passes in
+    # could contain extra columns that are not used by Tecton, and returned schema doesn't include these columns. For
+    # details about how to build spine_schema for different FeatureView, see `spine_schema` method defined in
+    # FeatureViewSpec and its children classes.
+    @property
+    def spine_schema(self) -> Schema:
+        return self.join_keys_schema + self.request_context_schema
 
     @property
     def pipeline(self) -> Optional[Pipeline]:
         if isinstance(self.fv_spec, (specs.MaterializedFeatureViewSpec, specs.OnDemandFeatureViewSpec)):
             return self.fv_spec.pipeline
         else:
             # Feature Tables do not have pipelines.
```

### Comparing `tecton-0.7.0b9/tecton_core/feature_set_config.py` & `tecton-0.7.0rc0/tecton_core/feature_set_config.py`

 * *Files 5% similar despite different names*

```diff
@@ -4,31 +4,41 @@
 
 import attrs
 
 from tecton_core import specs
 from tecton_core.fco_container import FcoContainer
 from tecton_core.feature_definition_wrapper import FeatureDefinitionWrapper
 from tecton_core.query_consts import UDF_INTERNAL
+from tecton_core.schema import Schema
 from tecton_proto.args import pipeline_pb2
+from tecton_proto.common import schema_pb2
 
 
 @attrs.frozen
 class _Key:
     name: str
     namespace: str
 
 
 @attrs.frozen
 class FeatureDefinitionAndJoinConfig:
     """
-    A Feature Definition Wrapper and its associated join configuration.
-
-    :param feature_definition: A FeatureDefinitionWrapper.
-    :param join_keys: The mapping from FeatureService's join keys to FeatureView/FeatureTable's join keys.
+    A Feature Definition Wrapper and its associated join configuration,
+    potentially sub-selecting output features based on a FeatureSetItem spec
+    (from the feature service).
+
+    :param feature_definition: A FeatureDefinitionWrapper, reflecting the
+        underlying spec.
+    :param name: The name of the feature definition.
+    :param join_keys: The mapping from FeatureService's join keys to
+        FeatureView/FeatureTable's join keys.
     :param namespace: The namespace.
+    :param features: The output features. Note this can differ from the
+        FeatureDefinitionWrapper's features if the FeatureService sub-selects
+        features from the feature view.
     """
 
     feature_definition: FeatureDefinitionWrapper
     name: str
     # Not a dict to account for multi mapping, though we may not handle multi mapping properly everywhere
     join_keys: List[Tuple[str, str]]
     namespace: str
@@ -64,14 +74,27 @@
             namespace=feature_set_item.namespace,
             features=list(feature_set_item.feature_columns),
         )
 
     def _key(self) -> _Key:
         return _Key(namespace=self.namespace or "", name=self.name)
 
+    # Returns the spine schema of this FeatureDefinitionAndJoinConfig. It collects the join key schema and request
+    # context schema from its FeatureDefinitionWrapper.
+    @property
+    def spine_schema(self) -> Schema:
+        spine_schema_dict = self.feature_definition.spine_schema.to_dict()
+        return Schema.from_dict(
+            {
+                spine_key: spine_schema_dict[fd_key]
+                for spine_key, fd_key in self.join_keys
+                if fd_key != self.feature_definition.wildcard_join_key
+            }
+        )
+
 
 @attrs.define
 class FeatureSetConfig:
     """A wrapper over a list of FeatureDefinitionAndJoinConfigs.
 
     Used for Feature Service and Feature Definition query construction. Needed for Feature Definition queries because
     some Feature Definitions (namely ODFVs with FV inputs) may require specs from multiple Feature Definitions to
@@ -113,14 +136,21 @@
 
     @property
     def request_context_keys(self) -> List[str]:
         all_keys = sum([fd.request_context_keys for fd in self.feature_definitions], [])
         unique_keys = list(set(all_keys))
         return unique_keys
 
+    @property
+    def spine_schema(self) -> Schema:
+        spine_schema = Schema(schema_pb2.Schema())
+        for dac in self.definitions_and_configs:
+            spine_schema += dac.spine_schema
+        return spine_schema
+
     @staticmethod
     def _get_full_feature_names(config: FeatureDefinitionAndJoinConfig) -> List[str]:
         return [
             config.namespace + config.feature_definition.namespace_separator + feature if config.namespace else feature
             for feature in config.features
         ]
```

### Comparing `tecton-0.7.0b9/tecton_core/function_deserialization.py` & `tecton-0.7.0rc0/tecton_core/function_deserialization.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 import sys
 
 from tecton_core import materialization_context
 from tecton_core.errors import TectonValidationError
 from tecton_proto.args.user_defined_function_pb2 import UserDefinedFunction
 
+
 # TODO(deprecated_after=0.5): handle backward-compatibility for builtin transformations that did not use tecton.materialization_context
 # but instead directly accessed tecton_spark.materialization_context
 sys.modules["tecton_spark.materialization_context"] = materialization_context
 
 
 def from_proto(serialized_transform: UserDefinedFunction, globals_=None, locals_=None):
     """
@@ -20,26 +21,28 @@
     assert serialized_transform.HasField("body") and serialized_transform.HasField(
         "name"
     ), "Invalid UserDefinedFunction."
 
     try:
         exec(serialized_transform.body, globals_, locals_)
     except NameError as e:
+        msg = "Failed to serialize function. Please note that all imports must be in the body of the function (not top-level) and type annotations cannot require imports. Additionally, be cautious of variables that shadow other variables. See https://docs.tecton.ai/v2/overviews/framework/transformations.html for more details."
         raise TectonValidationError(
-            "Failed to serialize function. Please note that all imports must be in the body of the function (not top-level) and type annotations cannot require imports. Additionally, be cautious of variables that shadow other variables. See https://docs.tecton.ai/v2/overviews/framework/transformations.html for more details.",
+            msg,
             e,
         )
 
     # Return function pointer
     try:
         fn = eval(serialized_transform.name, globals_, locals_)
         fn._code = serialized_transform.body
         return fn
     except Exception as e:
-        raise ValueError("Invalid transform") from e
+        msg = "Invalid transform"
+        raise ValueError(msg) from e
 
 
 # This version of function deserialization uses the "main scope".
 # This has historically been the behavior of function deserialization.
 # Generally this should be avoided since it can cause hard to debug issues,
 # e.g. two helper functions of the same name can shadow each other. This global
 # scope can also cause issues since it allows for users to access imported
```

### Comparing `tecton-0.7.0b9/tecton_core/id_helper.py` & `tecton-0.7.0rc0/tecton_core/id_helper.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_core/materialization_context.py` & `tecton-0.7.0rc0/tecton_core/materialization_context.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,17 +1,22 @@
+import typing
 from dataclasses import dataclass
 from typing import Optional
 from typing import Union
 
 import pendulum
 from typeguard import typechecked
 
 from tecton_core.errors import TectonValidationError
 
 
+if typing.TYPE_CHECKING:
+    import pyspark
+
+
 @dataclass
 class BaseMaterializationContext:
     _feature_start_time_DONT_ACCESS_DIRECTLY: pendulum.DateTime
     _feature_end_time_DONT_ACCESS_DIRECTLY: pendulum.DateTime
     _batch_schedule_DONT_ACCESS_DIRECTLY: pendulum.Duration
 
     @property
@@ -72,41 +77,36 @@
 class UnboundMaterializationContext(BaseMaterializationContext):
     """
     This is only meant for instantiation in transformation default args. Using it directly will fail.
     """
 
     @property
     def batch_schedule(self):
-        raise TectonValidationError(
-            "tecton.materialization_context() must be passed in via a kwarg default only. Instantiation in function body is not allowed."
-        )
+        msg = "tecton.materialization_context() must be passed in via a kwarg default only. Instantiation in function body is not allowed."
+        raise TectonValidationError(msg)
 
     @property
     def start_time(self):
-        raise TectonValidationError(
-            "tecton.materialization_context() must be passed in via a kwarg default only. Instantiation in function body is not allowed."
-        )
+        msg = "tecton.materialization_context() must be passed in via a kwarg default only. Instantiation in function body is not allowed."
+        raise TectonValidationError(msg)
 
     @property
     def end_time(self):
-        raise TectonValidationError(
-            "tecton.materialization_context() must be passed in via a kwarg default only. Instantiation in function body is not allowed."
-        )
+        msg = "tecton.materialization_context() must be passed in via a kwarg default only. Instantiation in function body is not allowed."
+        raise TectonValidationError(msg)
 
     @property
     def feature_start_time(self):
-        raise TectonValidationError(
-            "tecton.materialization_context() must be passed in via a kwarg default only. Instantiation in function body is not allowed."
-        )
+        msg = "tecton.materialization_context() must be passed in via a kwarg default only. Instantiation in function body is not allowed."
+        raise TectonValidationError(msg)
 
     @property
     def feature_end_time(self):
-        raise TectonValidationError(
-            "tecton.materialization_context() must be passed in via a kwarg default only. Instantiation in function body is not allowed."
-        )
+        msg = "tecton.materialization_context() must be passed in via a kwarg default only. Instantiation in function body is not allowed."
+        raise TectonValidationError(msg)
 
 
 @dataclass
 class BoundMaterializationContext(BaseMaterializationContext):
     @classmethod
     def create(cls, feature_start_time, feature_end_time):
         # user facing version
```

### Comparing `tecton-0.7.0b9/tecton_core/offline_store.py` & `tecton-0.7.0rc0/tecton_core/offline_store.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,21 +1,23 @@
+import datetime
 import enum
 from datetime import timedelta
 from typing import Union
 
 import attrs
 import pendulum
 
 from tecton_core import time_utils
 from tecton_core.errors import TectonInternalError
 from tecton_core.feature_definition_wrapper import FeatureDefinitionWrapper as FeatureDefinition
 from tecton_core.query_consts import ANCHOR_TIME
 from tecton_proto.data.feature_view_pb2 import DeltaOfflineStoreVersion
 from tecton_proto.data.feature_view_pb2 import ParquetOfflineStoreVersion
 
+
 TIME_PARTITION = "time_partition"
 SECONDS_TO_NANOSECONDS = 1000 * 1000 * 1000
 CONTINUOUS_PARTITION_SIZE_SECONDS = 86400
 
 
 class PartitionType(str, enum.Enum):
     DATE_STR = "DateString"
@@ -53,15 +55,16 @@
         else ParquetOfflineStoreVersion.PARQUET_OFFLINE_STORE_VERSION_1
     )
     if offline_store_version == ParquetOfflineStoreVersion.PARQUET_OFFLINE_STORE_VERSION_1:
         return TIME_PARTITION if fd.is_continuous else ANCHOR_TIME
     elif offline_store_version == ParquetOfflineStoreVersion.PARQUET_OFFLINE_STORE_VERSION_2:
         return TIME_PARTITION
     else:
-        raise TectonInternalError("unsupported offline store version")
+        msg = "unsupported offline store version"
+        raise TectonInternalError(msg)
 
 
 def partition_size_for_delta(fd: FeatureDefinition) -> pendulum.Duration:
     if fd.offline_store_params is not None:
         return pendulum.Duration(
             seconds=fd.offline_store_params.delta.time_partition_size.ToTimedelta().total_seconds()
         )
@@ -82,24 +85,24 @@
 def _check_supported_offline_store_version(fd: FeatureDefinition):
     if fd.offline_store_params is None:
         return
     if (
         fd.offline_store_params.HasField("delta")
         and fd.offline_store_params.delta.version not in DELTA_SUPPORTED_VERSIONS
     ):
-        raise TectonInternalError(
+        msg = (
             f"Unsupported offline store version {fd.offline_store_params.delta.version}. Try upgrading your Tecton SDK."
         )
+        raise TectonInternalError(msg)
     if (
         fd.offline_store_params.HasField("parquet")
         and fd.offline_store_params.parquet.version not in PARQUET_SUPPORTED_VERSIONS
     ):
-        raise TectonInternalError(
-            f"Unsupported offline store version {fd.offline_store_params.parquet.version}. Try upgrading your Tecton SDK."
-        )
+        msg = f"Unsupported offline store version {fd.offline_store_params.parquet.version}. Try upgrading your Tecton SDK."
+        raise TectonInternalError(msg)
 
 
 def get_offline_store_partition_params(feature_definition: FeatureDefinition) -> OfflineStorePartitionParams:
     # Examples of how our offline store is partitioned
     ### BWAFV on Delta
     # Partition Column: time_partition
     # Materialized Columns: _anchor_time, [join_keys], [feature_columns]
@@ -126,43 +129,44 @@
         partition_type = PartitionType.DATE_STR
         partition_interval = partition_size_for_delta(feature_definition)
     elif store_type == "parquet":
         partition_by = partition_col_for_parquet(feature_definition)
         partition_type = PartitionType.EPOCH
         partition_interval = partition_size_for_parquet(feature_definition)
     else:
-        raise Exception("Unexpected offline store config")
+        msg = "Unexpected offline store config"
+        raise Exception(msg)
     return OfflineStorePartitionParams(partition_by, partition_type, partition_interval)
 
 
 def timestamp_to_partition_date_str(timestamp: pendulum.DateTime, partition_params: OfflineStorePartitionParams) -> str:
     partition_interval_timedelta = partition_params.partition_interval.as_timedelta()
     aligned_time = time_utils.align_time_downwards(timestamp, partition_interval_timedelta)
     partition_format = _timestamp_formats(partition_interval_timedelta).python_format
     return aligned_time.strftime(partition_format)
 
 
 def timestamp_to_partition_epoch(
     timestamp: pendulum.DateTime,
     partition_params: OfflineStorePartitionParams,
-    is_continuous: bool,
     feature_store_format_version: int,
 ) -> int:
-    if is_continuous:
-        aligned_time = timestamp
-    else:
-        aligned_time = time_utils.align_time_downwards(timestamp, partition_params.partition_interval.as_timedelta())
+    aligned_time = time_utils.align_time_downwards(timestamp, partition_params.partition_interval.as_timedelta())
+    # align_time_downwards returns the time without tzinfo. convert_timestamp_for_version calls timestamp() which
+    # treats naive datetime instances as local time. This can cause an issue if local time is not in UTC.
+    aligned_time = aligned_time.replace(tzinfo=datetime.timezone.utc)
     return time_utils.convert_timestamp_for_version(aligned_time, feature_store_format_version)
 
 
 def window_size_seconds(window: Union[timedelta, pendulum.Duration]):
     if isinstance(window, pendulum.Duration):
         window = window.as_timedelta()
     if window % timedelta(seconds=1) != timedelta(0):
-        raise AssertionError(f"partition_size is not a round number of seconds: {window}")
+        msg = f"partition_size is not a round number of seconds: {window}"
+        raise AssertionError(msg)
     return int(window.total_seconds())
 
 
 def _timestamp_formats(partition_size: timedelta):
     if partition_size % timedelta(days=1) == timedelta(0):
         return TimestampFormats(spark_format="yyyy-MM-dd", python_format="%Y-%m-%d")
     else:
```

### Comparing `tecton-0.7.0b9/tecton_core/online_serving_index.py` & `tecton-0.7.0rc0/tecton_core/online_serving_index.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_core/pipeline_common.py` & `tecton-0.7.0rc0/tecton_core/pipeline_common.py`

 * *Files 4% similar despite different names*

```diff
@@ -4,33 +4,34 @@
 from typing import Optional
 from typing import Union
 
 import pandas
 import pendulum
 
 from tecton_core import time_utils
+from tecton_core.id_helper import IdHelper
 from tecton_proto.args.pipeline_pb2 import ConstantNode
 from tecton_proto.args.pipeline_pb2 import DataSourceNode
 from tecton_proto.args.pipeline_pb2 import Input as InputProto
 from tecton_proto.args.pipeline_pb2 import Pipeline
 from tecton_proto.args.pipeline_pb2 import PipelineNode
 from tecton_proto.args.pipeline_pb2 import RequestContext as RequestContextProto
 
+
 CONSTANT_TYPE = Optional[Union[str, int, float, bool]]
 CONSTANT_TYPE_OBJECTS = (str, int, float, bool)
 
 
 def _make_mode_to_type() -> Dict[str, Any]:
     lookup: Dict[str, Any] = {
         "pandas": pandas.DataFrame,
         "python": Dict,
         "pipeline": PipelineNode,
         "spark_sql": str,
         "snowflake_sql": str,
-        "athena": str,
     }
     try:
         import pyspark.sql
 
         lookup["pyspark"] = pyspark.sql.DataFrame
     except ImportError:
         pass
@@ -53,15 +54,16 @@
         return int(constant_node.int_const)
     elif constant_node.HasField("float_const"):
         return float(constant_node.float_const)
     elif constant_node.HasField("bool_const"):
         return constant_node.bool_const
     elif constant_node.HasField("null_const"):
         return None
-    raise KeyError(f"Unknown ConstantNode type: {constant_node}")
+    msg = f"Unknown ConstantNode type: {constant_node}"
+    raise KeyError(msg)
 
 
 def get_keyword_inputs(transformation_node) -> Dict[str, InputProto]:
     """Returns the keyword inputs of transformation_node in a dict."""
     return {
         node_input.arg_name: node_input for node_input in transformation_node.inputs if node_input.HasField("arg_name")
     }
@@ -80,21 +82,19 @@
             break
     expected_type = MODE_TO_TYPE_LOOKUP[mode]
     actual_type = type(result)
 
     if isinstance(result, expected_type):
         return
     elif possible_mode is not None and possible_mode in supported_modes:
-        raise TypeError(
-            f"Transformation function {object_name} with mode '{mode}' is expected to return result with type {expected_type}, but returns result with type {actual_type} instead. Did you mean to set mode='{possible_mode}'?"
-        )
+        msg = f"Transformation function {object_name} with mode '{mode}' is expected to return result with type {expected_type}, but returns result with type {actual_type} instead. Did you mean to set mode='{possible_mode}'?"
+        raise TypeError(msg)
     else:
-        raise TypeError(
-            f"Transformation function {object_name} with mode {mode} is expected to return result with type {expected_type}, but returns result with type {actual_type} instead."
-        )
+        msg = f"Transformation function {object_name} with mode {mode} is expected to return result with type {expected_type}, but returns result with type {actual_type} instead."
+        raise TypeError(msg)
 
 
 def get_time_window_from_data_source_node(
     feature_time_limits: Optional[pendulum.Period],
     schedule_interval: Optional[pendulum.Duration],
     data_source_node: DataSourceNode,
 ) -> Optional[pendulum.Period]:
@@ -124,14 +124,23 @@
         for child in node.transformation_node.inputs:
             rc = find_request_context(child.node)
             if rc is not None:
                 return rc
     return None
 
 
+def get_input_name_to_ds_id_map(pipeline: Pipeline) -> Dict[str, str]:
+    """Return a map from input name to data source id for the pipeline."""
+    data_source_nodes = get_all_data_source_nodes(pipeline)
+    return {
+        node.data_source_node.input_name: IdHelper.to_string(node.data_source_node.virtual_data_source_id)
+        for node in data_source_nodes
+    }
+
+
 def get_all_feature_view_nodes(pipeline: Pipeline) -> List[PipelineNode]:
     """Returns all feature view nodes from the provided pipeline."""
     return [node for node in get_all_pipeline_nodes(pipeline.root) if node.HasField("feature_view_node")]
 
 
 def get_all_data_source_nodes(pipeline: Pipeline) -> List[PipelineNode]:
     """Returns all data source nodes from the provided pipeline."""
```

### Comparing `tecton-0.7.0b9/tecton_core/pipeline_sql_builder.py` & `tecton-0.7.0rc0/tecton_core/pipeline_sql_builder.py`

 * *Files 2% similar despite different names*

```diff
@@ -80,15 +80,16 @@
             node_value = self._node_to_value(transformation_input.node)
             if transformation_input.HasField("arg_index"):
                 assert len(args) == transformation_input.arg_index
                 args.append(node_value)
             elif transformation_input.HasField("arg_name"):
                 kwargs[transformation_input.arg_name] = node_value
             else:
-                raise KeyError(f"Unknown argument type for Input node: {transformation_input}")
+                msg = f"Unknown argument type for Input node: {transformation_input}"
+                raise KeyError(msg)
         transformation = self.id_to_transformation[IdHelper.to_string(transformation_node.transformation_id)]
         user_function = transformation.user_function
         sql = user_function(*args, **kwargs)
         return sql, unique_node_alias(pipeline_node)
 
 
 def unique_node_alias(node: PipelineNode) -> str:
```

### Comparing `tecton-0.7.0b9/tecton_core/query/builder.py` & `tecton-0.7.0rc0/tecton_core/query/builder.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 from datetime import datetime
 from typing import Dict
+from typing import List
 from typing import Optional
 from typing import Set
 from typing import Tuple
 
 import pendulum
 
 from tecton_core import errors
@@ -25,15 +26,15 @@
 from tecton_core.query.nodes import AsofWildcardExplodeNode
 from tecton_core.query.nodes import ConvertEpochToTimestampNode
 from tecton_core.query.nodes import DataSourceScanNode
 from tecton_core.query.nodes import FeatureTimeFilterNode
 from tecton_core.query.nodes import FeatureViewPipelineNode
 from tecton_core.query.nodes import JoinNode
 from tecton_core.query.nodes import MetricsCollectorNode
-from tecton_core.query.nodes import OdfvPipelineNode
+from tecton_core.query.nodes import MultiOdfvPipelineNode
 from tecton_core.query.nodes import OfflineStoreScanNode
 from tecton_core.query.nodes import PartialAggNode
 from tecton_core.query.nodes import RenameColsNode
 from tecton_core.query.nodes import RespectFeatureStartTimeNode
 from tecton_core.query.nodes import RespectTTLNode
 from tecton_core.query.nodes import SelectDistinctNode
 from tecton_core.query.nodes import StreamWatermarkNode
@@ -41,14 +42,15 @@
 from tecton_core.query.sql_compat import default_case
 from tecton_core.query_consts import ANCHOR_TIME
 from tecton_core.query_consts import EFFECTIVE_TIMESTAMP
 from tecton_core.query_consts import EXPIRATION_TIMESTAMP
 from tecton_core.query_consts import UDF_INTERNAL
 from tecton_proto.args.pipeline_pb2 import DataSourceNode as ProtoDataSourceNode
 
+
 WINDOW_END_COLUMN_NAME = "tile_end_time"
 
 
 def build_datasource_scan_node(
     ds: specs.DataSourceSpec,
     for_stream: bool,
     start_time: Optional[datetime] = None,
@@ -180,15 +182,16 @@
             tree,
             fdw=fdw,
             window_start_column_name=anchor_time_field,
             window_end_column_name=window_end_column_name,
             aggregation_anchor_time=aggregation_anchor_time,
         ).as_ref()
     else:
-        raise Exception("unexpected FV type")
+        msg = "unexpected FV type"
+        raise Exception(msg)
     return tree
 
 
 def build_get_features(
     fdw: FeatureDefinitionWrapper,
     from_source: Optional[bool],
     feature_data_time_limits: Optional[pendulum.Period] = None,
@@ -207,16 +210,23 @@
     if from_source is None:
         from_source = not fdw.materialization_enabled or not fdw.writes_to_offline_store
 
     if from_source is False:
         assert not aggregation_anchor_time, "aggregation anchor time is not allowed when fetching features from source"
         if not fdw.materialization_enabled or not fdw.writes_to_offline_store:
             raise errors.FV_NEEDS_TO_BE_MATERIALIZED(fdw.name)
-        return OfflineStoreScanNode(feature_definition_wrapper=fdw, time_filter=feature_data_time_limits).as_ref()
+        return OfflineStoreScanNode(
+            feature_definition_wrapper=fdw, partition_time_filter=feature_data_time_limits
+        ).as_ref()
     else:
+        # TODO(sanika) uncomment when athena has been migrated to querytree
+        # if conf.get_or_none("SQL_DIALECT") == "athena":
+        #     raise errors.TectonAthenaValidationError(
+        #         "Features can only be read from the offline store when Athena compute is enabled. Please set from_source = False"
+        #     )
         # TODO(TEC-13005)
         # TODO(pooja): raise an appropriate error here for push source
         if fdw.is_incremental_backfill:
             raise errors.FV_BFC_SINGLE_FROM_SOURCE
         return build_materialization_querytree(
             fdw,
             for_stream=False,
@@ -224,15 +234,14 @@
             aggregation_anchor_time=aggregation_anchor_time,
         )
 
 
 def build_get_full_agg_features(
     fdw: FeatureDefinitionWrapper,
     from_source: Optional[bool],
-    spine: Optional[NodeRef] = None,
     feature_data_time_limits: Optional[pendulum.Period] = None,
     respect_feature_start_time: bool = True,
     aggregation_anchor_time: Optional[datetime] = None,
     show_effective_time: bool = False,
 ):
     partial_aggs = build_get_features(
         fdw,
@@ -366,21 +375,23 @@
     for feature in dac.features:
         if UDF_INTERNAL in feature:
             internal_cols.add(feature)
 
 
 # Construct each wildcard materialized fvtree by joining against distinct set of join keys.
 # Then, outer join these using WildcardJoinNode which performs an outer join while handling null-valued features properly.
-def _build_wild_fv_subtree(spine_node, fv_dacs, spine_time_field, from_source):
+def _build_wild_fv_subtree(spine_node, fv_dacs, spine_time_field, from_source) -> NodeRef:
     newtree = None
     for dac in fv_dacs:
         fdw = dac.feature_definition
 
-        # TODO(TEC-12324): re-implement this as a rewrite or remove.
         subspine_join_keys = [jk[0] for jk in dac.join_keys if jk[0] != fdw.wildcard_join_key]
+        # SelectDistinctNode is needed for correctness in order to filter out rows with duplicate join keys before
+        # retrieving feature values. This avoids exploding wildcard rows when there are duplicates in both the spine and the
+        # feature view tree.
         subspine = SelectDistinctNode(spine_node, subspine_join_keys + [spine_time_field]).as_ref()
         fvtree = build_spine_join_querytree(dac, subspine, spine_time_field, from_source)
         if len(dac.features) < len(fdw.features):
             fvtree = RenameColsNode(
                 fvtree,
                 drop=[f"{fdw.name}{fdw.namespace_separator}{f}" for f in fdw.features if f not in dac.features],
             ).as_ref()
@@ -398,53 +409,60 @@
     newtree = spine_node
     internal_cols = set()
     for dac in fv_dacs:
         fdw = dac.feature_definition
         _update_internal_cols(fdw, dac, internal_cols)
 
         subspine_join_keys = [jk[0] for jk in dac.join_keys]
-        # TODO(TEC-12324): re-implement this as a rewrite or remove.
+        # SelectDistinctNode is needed for correctness in the case that there are duplicate rows in the spine. The
+        # alternative considered was to add a row_id as a hash of the row or a monotonically increasing id, however the
+        # row_id as a hash is not unique for duplicate rows and a monotonically increasing id is non-deterministic.
         subspine = SelectDistinctNode(spine_node, subspine_join_keys + [spine_time_field]).as_ref()
         fvtree = build_spine_join_querytree(dac, subspine, spine_time_field, from_source)
         if len(dac.features) < len(fdw.features):
             fvtree = RenameColsNode(
                 fvtree,
                 drop=[f"{fdw.name}{fdw.namespace_separator}{f}" for f in fdw.features if f not in dac.features],
             ).as_ref()
         newtree = JoinNode(newtree, fvtree, how="inner", join_cols=subspine_join_keys + [spine_time_field]).as_ref()
     return newtree, internal_cols
 
 
 # Compute odfvs via udf on the parent (not using joins)
-def _build_odfv_subtree(parent_tree, odfv_dacs):
-    # do all on-demand next
+def _build_odfv_subtree(parent_tree: NodeRef, odfv_dacs: List[FeatureDefinitionAndJoinConfig]):
     newtree = parent_tree
+    feature_definitions_namespaces = [(dac.feature_definition, dac.namespace) for dac in odfv_dacs]
+    newtree = MultiOdfvPipelineNode(newtree, feature_definitions_namespaces).as_ref()
+
+    # Compute the union of the features to be computed
+    dac_features = set()
+    fdw_features = set()
     for dac in odfv_dacs:
-        fdw = dac.feature_definition
-        newtree = OdfvPipelineNode(newtree, fdw, dac.namespace).as_ref()
-        if len(dac.features) < len(fdw.features):
-            drop_map = {
-                f"{dac.namespace}{fdw.namespace_separator}{f}": None for f in fdw.features if f not in dac.features
-            }
-            newtree = RenameColsNode(
-                newtree,
-                drop=[f"{dac.namespace}{fdw.namespace_separator}{f}" for f in fdw.features if f not in dac.features],
-            ).as_ref()
+        feature_prefix = f"{dac.namespace}{dac.feature_definition.namespace_separator}"
+        dac_features.update({f"{feature_prefix}{f}" for f in dac.features})
+        fdw_features.update({f"{feature_prefix}{f}" for f in dac.feature_definition.features})
+
+    # Drop features if user queried a subset via feature services
+    if len(dac_features) < len(fdw_features):
+        newtree = RenameColsNode(
+            newtree,
+            drop=[namespaced_feat for namespaced_feat in fdw_features if namespaced_feat not in dac_features],
+        ).as_ref()
     return newtree
 
 
 # Construct each materialized fvtree by joining against distinct set of join keys.
 # Then, join the full spine against each of those.
 # Finally, compute odfvs via udf on top of the result (not using joins)
 def build_feature_set_config_querytree(
     fsc: FeatureSetConfig, spine_node: NodeRef, spine_time_field: str, from_source: Optional[bool]
 ) -> NodeRef:
-    odfv_dacs = []
-    wildcard_dacs = []
-    normal_fv_dacs = []
+    odfv_dacs: List[FeatureDefinitionAndJoinConfig] = []
+    wildcard_dacs: List[FeatureDefinitionAndJoinConfig] = []
+    normal_fv_dacs: List[FeatureDefinitionAndJoinConfig] = []
 
     for dac in fsc.definitions_and_configs:
         if dac.feature_definition.is_on_demand:
             odfv_dacs.append(dac)
         elif dac.feature_definition.wildcard_join_key is not None:
             if dac.feature_definition.wildcard_join_key in spine_node.columns:
                 # Despite this being a wildcard FV, since we have the wildcard
@@ -460,53 +478,55 @@
     else:
         newtree = spine_node
 
     internal_cols = set()
     if normal_fv_dacs:
         newtree, internal_cols = _build_standard_fv_subtree(newtree, normal_fv_dacs, spine_time_field, from_source)
 
-    newtree = _build_odfv_subtree(newtree, odfv_dacs)
+    if odfv_dacs:
+        newtree = _build_odfv_subtree(newtree, odfv_dacs)
 
     # drop all internal cols
     if len(internal_cols) > 0:
         newtree = RenameColsNode(newtree, drop=list(internal_cols)).as_ref()
 
     return newtree
 
 
 def _build_spine_query_tree_temporal_or_feature_table(
     spine_node: NodeRef, dac: FeatureDefinitionAndJoinConfig, data_delay_seconds: int, from_source: Optional[bool]
 ):
-    TIMESTAMP_PLUS_TTL = default_case("_timestamp_plus_ttl")
     fdw = dac.feature_definition
+    TIMESTAMP_PLUS_TTL = default_case("_timestamp_plus_ttl")
     base = build_get_features(fdw, from_source=from_source)
     batch_schedule_seconds = 0 if fdw.is_feature_table else fdw.batch_materialization_schedule.in_seconds()
     base = AddEffectiveTimestampNode(
         base,
         timestamp_field=fdw.timestamp_key,
         effective_timestamp_name=default_case(EFFECTIVE_TIMESTAMP),
         batch_schedule_seconds=batch_schedule_seconds,
         data_delay_seconds=data_delay_seconds,
         is_stream=fdw.is_stream,
         is_temporal_aggregate=False,
     ).as_ref()
-    base = AddDurationNode(
-        base, timestamp_field=fdw.timestamp_key, duration=fdw.serving_ttl, new_column_name=TIMESTAMP_PLUS_TTL
-    ).as_ref()
-    # Calculate effective expiration time = window(feature_time + ttl, batch_schedule).end + data_delay
-    batch_schedule_seconds = 0 if fdw.is_feature_table else fdw.batch_materialization_schedule.in_seconds()
-    base = AddEffectiveTimestampNode(
-        base,
-        timestamp_field=TIMESTAMP_PLUS_TTL,
-        effective_timestamp_name=default_case(EXPIRATION_TIMESTAMP),
-        batch_schedule_seconds=batch_schedule_seconds,
-        data_delay_seconds=data_delay_seconds,
-        is_stream=fdw.is_stream,
-        is_temporal_aggregate=False,
-    ).as_ref()
+    if fdw.serving_ttl is not None:
+        base = AddDurationNode(
+            base, timestamp_field=fdw.timestamp_key, duration=fdw.serving_ttl, new_column_name=TIMESTAMP_PLUS_TTL
+        ).as_ref()
+        # Calculate effective expiration time = window(feature_time + ttl, batch_schedule).end + data_delay
+        batch_schedule_seconds = 0 if fdw.is_feature_table else fdw.batch_materialization_schedule.in_seconds()
+        base = AddEffectiveTimestampNode(
+            base,
+            timestamp_field=TIMESTAMP_PLUS_TTL,
+            effective_timestamp_name=default_case(EXPIRATION_TIMESTAMP),
+            batch_schedule_seconds=batch_schedule_seconds,
+            data_delay_seconds=data_delay_seconds,
+            is_stream=fdw.is_stream,
+            is_temporal_aggregate=False,
+        ).as_ref()
     rightside_join_prefix = default_case("_tecton_right")
     join_prefixed_feature_names = [f"{rightside_join_prefix}_{f}" for f in fdw.features]
     # we can't just ask for the correct right_prefix to begin with because the asofJoin always sticks an extra underscore in between
     rename_map: Dict[str, Optional[str]] = {
         f"{rightside_join_prefix}_{f}": f"{dac.namespace}{fdw.namespace_separator}{f}"
         for f in fdw.features
         if f in dac.features
@@ -519,37 +539,39 @@
             rename_map[f"{rightside_join_prefix}_{f}"] = f"{dac.namespace}{fdw.namespace_separator}{f}"
 
     expiration_timestamp_col = f"{rightside_join_prefix}_{default_case(EXPIRATION_TIMESTAMP)}"
 
     cols_to_drop.append(f"{rightside_join_prefix}_{fdw.timestamp_key}")
     cols_to_drop.append(f"{rightside_join_prefix}_{default_case(ANCHOR_TIME)}")
     cols_to_drop.append(f"{rightside_join_prefix}_{default_case(EFFECTIVE_TIMESTAMP)}")
-    cols_to_drop.append(f"{rightside_join_prefix}_{default_case(TIMESTAMP_PLUS_TTL)}")
-    cols_to_drop.append(expiration_timestamp_col)
+    if fdw.serving_ttl is not None:
+        cols_to_drop.append(f"{rightside_join_prefix}_{default_case(TIMESTAMP_PLUS_TTL)}")
+        cols_to_drop.append(expiration_timestamp_col)
 
     if fdw.feature_start_timestamp is not None:
         base = RespectFeatureStartTimeNode(
             base, fdw.timestamp_key, fdw.feature_start_timestamp, fdw.features, fdw.get_feature_store_format_version
         ).as_ref()
 
     if fdw.wildcard_join_key is not None and fdw.wildcard_join_key not in spine_node.columns:
         # Need to shallow copy base so that the left and right side are separate
         base_copy = NodeRef.shallow_copy(base)
         spine_node = AsofWildcardExplodeNode(
             spine_node, fdw.timestamp_key, base_copy, EFFECTIVE_TIMESTAMP, fdw
         ).as_ref()
 
-    join = AsofJoinNode(
+    base = AsofJoinNode(
         left_container=AsofJoinInputContainer(spine_node, fdw.timestamp_key),
         right_container=AsofJoinInputContainer(
             base,
             timestamp_field=fdw.timestamp_key,
             effective_timestamp_field=default_case(EFFECTIVE_TIMESTAMP),
             prefix=rightside_join_prefix,
             schema=fdw.view_schema,
         ),
         join_cols=fdw.join_keys,
     ).as_ref()
 
-    ttl_node = RespectTTLNode(join, fdw.timestamp_key, expiration_timestamp_col, join_prefixed_feature_names).as_ref()
+    if fdw.serving_ttl is not None:
+        base = RespectTTLNode(base, fdw.timestamp_key, expiration_timestamp_col, join_prefixed_feature_names).as_ref()
     # remove anchor cols/dupe timestamp cols
-    return RenameColsNode(ttl_node, mapping=rename_map, drop=cols_to_drop).as_ref()
+    return RenameColsNode(base, mapping=rename_map, drop=cols_to_drop).as_ref()
```

### Comparing `tecton-0.7.0b9/tecton_core/query/node_interface.py` & `tecton-0.7.0rc0/tecton_core/query/node_interface.py`

 * *Files 1% similar despite different names*

```diff
@@ -10,14 +10,15 @@
 
 import attrs
 import pypika
 import sqlparse
 
 from tecton_core.vendor.treelib import Tree
 
+
 INDENT_BLOCK = "  "
 
 
 @dataclass
 class NodeRef:
     """
     Used so we can more easily modify the QueryTree by inserting and removing nodes, e.g.
@@ -256,12 +257,15 @@
     @property
     def _temp_table_name(self):
         """
         Gets the temp view name registered by register()
         """
         return f"TMP_TABLE_{id(self._dataframe)}"
 
+    def to_spark(self):
+        raise NotImplementedError
+
 
 def recurse_query_tree(node_ref: NodeRef, f: Callable):
     f(node_ref.node)
     for child in node_ref.inputs:
         recurse_query_tree(child, f)
```

### Comparing `tecton-0.7.0b9/tecton_core/query/nodes.py` & `tecton-0.7.0rc0/tecton_core/query/nodes.py`

 * *Files 1% similar despite different names*

```diff
@@ -6,50 +6,50 @@
 from typing import Optional
 from typing import Tuple
 from typing import Union
 
 import attrs
 import pendulum
 import pypika
+from pypika import NULL
 from pypika import AliasedQuery
-from pypika import analytics as an
 from pypika import Case
 from pypika import Database
 from pypika import Field
-from pypika import NULL
-from pypika import Query
 from pypika import Table
+from pypika import analytics as an
 from pypika.functions import Cast
 from pypika.terms import LiteralValue
 from pypika.terms import Term
 
 from tecton_core import conf
 from tecton_core import specs
 from tecton_core import time_utils
+from tecton_core.errors import TectonAthenaNotImplementedError
 from tecton_core.feature_definition_wrapper import FeatureDefinitionWrapper
 from tecton_core.materialization_context import BoundMaterializationContext
-from tecton_core.offline_store import get_offline_store_partition_params
-from tecton_core.offline_store import PartitionType
 from tecton_core.offline_store import TIME_PARTITION
+from tecton_core.offline_store import PartitionType
+from tecton_core.offline_store import get_offline_store_partition_params
 from tecton_core.offline_store import timestamp_to_partition_date_str
 from tecton_core.offline_store import timestamp_to_partition_epoch
 from tecton_core.pipeline_sql_builder import PipelineSqlBuilder
 from tecton_core.query.aggregation_plans import AGGREGATION_PLANS
 from tecton_core.query.aggregation_plans import QueryWindowSpec
 from tecton_core.query.node_interface import DataframeWrapper
 from tecton_core.query.node_interface import NodeRef
 from tecton_core.query.node_interface import QueryNode
+from tecton_core.query.sql_compat import CustomQuery
+from tecton_core.query.sql_compat import LastValue
+from tecton_core.query.sql_compat import Query
 from tecton_core.query.sql_compat import convert_epoch_seconds_to_feature_store_format_version
 from tecton_core.query.sql_compat import convert_epoch_term_in_seconds
-from tecton_core.query.sql_compat import CustomQuery
 from tecton_core.query.sql_compat import date_add
 from tecton_core.query.sql_compat import default_case
 from tecton_core.query.sql_compat import from_unixtime
-from tecton_core.query.sql_compat import LastValue
-from tecton_core.query.sql_compat import Query
 from tecton_core.query.sql_compat import struct
 from tecton_core.query.sql_compat import struct_extract
 from tecton_core.query.sql_compat import to_timestamp
 from tecton_core.query.sql_compat import to_unixtime
 from tecton_core.query_consts import ANCHOR_TIME
 from tecton_core.schema import Schema
 from tecton_core.time_utils import convert_duration_to_seconds
@@ -57,40 +57,39 @@
 from tecton_core.time_utils import convert_timedelta_for_version
 from tecton_proto.args.pipeline_pb2 import DataSourceNode
 from tecton_proto.common.data_source_type_pb2 import DataSourceType
 from tecton_proto.data.feature_view_pb2 import MaterializationTimeRangePolicy
 
 
 @attrs.frozen
-class OdfvPipelineNode(QueryNode):
+class MultiOdfvPipelineNode(QueryNode):
     """
-    Evaluates an odfv pipeline on top of an input containing columns prefixed '_udf_internal' to be used as dependent feature view inputs. The _udf_internal contract is
-    documented in pipeline_helper.py
-    The input may also have other feature values. This ensures we can match multiple odfv features to the right rows based on request context without joining them.
-    In order to make this possible, a namespace is also passed through at this point to ensure the odfv features do not conflict with other features.
+    Evaluates multiple ODFVs:
+        - Dependent feature view columns are prefixed `udf_internal` (query_constants.UDF_INTERNAL).
+        - Each ODFV has a namespace to ensure their features do not conflict with other features
     """
 
     input_node: NodeRef
-    feature_definition_wrapper: FeatureDefinitionWrapper
-    namespace: str
+    feature_definition_wrappers_namespaces: List[Tuple[FeatureDefinitionWrapper, str]]
 
     @property
     def columns(self) -> Tuple[str, ...]:
-        sep = self.feature_definition_wrapper.namespace_separator
-        return tuple(
-            list(self.input_node.columns)
-            + [f"{self.namespace}{sep}{name}" for name in self.feature_definition_wrapper.view_schema.column_names()]
-        )
+        output_columns = list(self.input_node.columns)
+        for fdw, namespace in self.feature_definition_wrappers_namespaces:
+            sep = fdw.namespace_separator
+            output_columns += [f"{namespace}{sep}{name}" for name in fdw.view_schema.column_names()]
+        return tuple(output_columns)
 
     @property
     def inputs(self) -> Tuple[NodeRef, ...]:
         return (self.input_node,)
 
     def as_str(self):
-        return f"Evaluate on-demand feature view pipeline '{self.feature_definition_wrapper.name}'"
+        fdw_names = [fdw.name for fdw, _ in self.feature_definition_wrappers_namespaces]
+        return f"Evaluate multiple on-demand feature views in pipeline '{fdw_names}'"
 
     def _to_query(self) -> pypika.Query:
         raise NotImplementedError
 
 
 @attrs.frozen
 class FeatureViewPipelineNode(QueryNode):
@@ -102,15 +101,16 @@
 
     @property
     def columns(self) -> Tuple[str, ...]:
         return self.feature_definition_wrapper.view_schema.column_names()
 
     @property
     def schedule_interval(self) -> pendulum.Duration:
-        # Note: elsewhere we set this to pendulum.Duration(seconds=fv_proto.materialization_params.schedule_interval.ToSeconds())
+        # Note: elsewhere we set this to
+        # pendulum.Duration(seconds=fv_proto.materialization_params.schedule_interval.ToSeconds())
         # but that seemed wrong for bwafv
         return self.feature_definition_wrapper.batch_materialization_schedule
 
     @property
     def inputs(self) -> Tuple[NodeRef, ...]:
         return tuple(self.inputs_map.values())
 
@@ -145,18 +145,18 @@
             query, alias = t_node
             res = res.with_(CustomQuery(query), alias)
         return res
 
 
 @attrs.frozen
 class DataSourceScanNode(QueryNode):
-    """Scans a data source and applies the given time range filter.
+    """Scans a batch data source and applies the given time range filter, or reads a stream source.
 
     Attributes:
-        ds: The data source to be scanned.
+        ds: The data source to be scanned or read.
         ds_node: The DataSourceNode (proto object, not QueryNode) corresponding to the data source. Used for rewrites.
         is_stream: If True, the data source is a stream source.
         start_time: The start time to be applied.
         end_time: The end time to be applied.
     """
 
     ds: specs.DataSourceSpec
@@ -177,33 +177,35 @@
         else:
             raise NotImplementedError
 
     # MyPy has a known issue on validators https://mypy.readthedocs.io/en/stable/additional_features.html#id1
     @is_stream.validator  # type: ignore
     def check_no_time_filter(self, _, is_stream: bool):
         if is_stream and (self.start_time is not None or self.end_time is not None):
-            raise ValueError("Raw data filtering cannot be run on a stream source")
+            msg = "Raw data filtering cannot be run on a stream source"
+            raise ValueError(msg)
 
     @property
     def inputs(self) -> Tuple[NodeRef, ...]:
         return tuple()
 
     def as_str(self):
         verb = "Read stream source" if self.is_stream else "Scan data source"
         s = f"{verb} '{self.ds.name}'"
         if self.start_time and self.end_time:
             s += f" and apply time range filter [{self.start_time}, {self.end_time})"
         elif self.start_time:
             s += f" and filter by start time {self.start_time}"
         elif self.end_time:
             s += f" and filter by end time {self.end_time}"
-        else:
+        elif not self.is_stream:
+            # No need to warn for stream sources since they don't support time range filtering.
             s += ". WARNING: there is no time range filter so all rows will be returned. This can be very inefficient."
         if self.start_time and self.end_time and self.start_time >= self.end_time:
-            s += f". WARNING: since start time >= end time, no rows will be returned."
+            s += ". WARNING: since start time >= end time, no rows will be returned."
         return s
 
     def _to_query(self) -> pypika.Query:
         if self.is_stream:
             raise NotImplementedError
         source = self.ds.batch_source
         if hasattr(source, "table") and source.table:
@@ -245,34 +247,35 @@
     def _to_query(self) -> pypika.Query:
         raise NotImplementedError
 
 
 @attrs.frozen
 class OfflineStoreScanNode(QueryNode):
     """
-    Fetch values from offline store
+    Fetch values from offline store. Note that time_filter only applies to partitions, not
+    actual row timestamps, so you may have rows outside the time_filter range.
     """
 
     feature_definition_wrapper: FeatureDefinitionWrapper
-    time_filter: Optional[pendulum.Period] = None
+    partition_time_filter: Optional[pendulum.Period] = None
 
     @property
     def columns(self) -> Tuple[str, ...]:
         offline_store_config = self.feature_definition_wrapper.offline_store_config
         store_type = offline_store_config.WhichOneof("store_type")
         cols = self.feature_definition_wrapper.materialization_schema.column_names()
         if self.feature_definition_wrapper.is_temporal and store_type == "parquet":
             # anchor time is not included in m13n schema for bfv/sfv
             cols.append(default_case(ANCHOR_TIME))
         return cols
 
     def as_str(self):
         s = f"Scan offline store for '{self.feature_definition_wrapper.name}'"
-        if self.time_filter:
-            s += f" with feature time limits [{self.time_filter.start}, {self.time_filter.end}]"
+        if self.partition_time_filter:
+            s += f" with feature time limits [{self.partition_time_filter.start}, {self.partition_time_filter.end}]"
         return s
 
     @property
     def inputs(self) -> Tuple[NodeRef, ...]:
         return tuple()
 
     @property
@@ -288,44 +291,46 @@
             q = q.where(w)
         return q
 
     def _get_partition_filters(self) -> List[Term]:
         # Whenever the partition filtering logic is changed, also make sure the changes are applied to the spark
         # version in tecton_spark/offline_store.py
 
-        if not self.time_filter:
+        if not self.partition_time_filter:
             return []
         partition_params = get_offline_store_partition_params(self.feature_definition_wrapper)
         partition_col = Field(partition_params.partition_by)
         if partition_params.partition_by == ANCHOR_TIME or (
             partition_params.partition_by == TIME_PARTITION and partition_params.partition_type == PartitionType.EPOCH
         ):
             partition_col = Cast(partition_col, "bigint")
 
         partition_filters = []
         partition_lower_bound = None
         partition_upper_bound = None
         if partition_params.partition_type == PartitionType.DATE_STR:
-            if self.time_filter.start:
-                partition_lower_bound = timestamp_to_partition_date_str(self.time_filter.start, partition_params)
-            if self.time_filter.end:
-                partition_upper_bound = timestamp_to_partition_date_str(self.time_filter.end, partition_params)
+            if self.partition_time_filter.start:
+                partition_lower_bound = timestamp_to_partition_date_str(
+                    self.partition_time_filter.start, partition_params
+                )
+            if self.partition_time_filter.end:
+                partition_upper_bound = timestamp_to_partition_date_str(
+                    self.partition_time_filter.end, partition_params
+                )
         elif partition_params.partition_type == PartitionType.EPOCH:
-            if self.time_filter.start:
+            if self.partition_time_filter.start:
                 partition_lower_bound = timestamp_to_partition_epoch(
-                    self.time_filter.start,
+                    self.partition_time_filter.start,
                     partition_params,
-                    self.feature_definition_wrapper.is_continuous,
                     self.feature_definition_wrapper.get_feature_store_format_version,
                 )
-            if self.time_filter.end:
+            if self.partition_time_filter.end:
                 partition_upper_bound = timestamp_to_partition_epoch(
-                    self.time_filter.end,
+                    self.partition_time_filter.end,
                     partition_params,
-                    self.feature_definition_wrapper.is_continuous,
                     self.feature_definition_wrapper.get_feature_store_format_version,
                 )
 
         if partition_lower_bound:
             partition_filters.append(partition_col >= partition_lower_bound)
         if partition_upper_bound:
             partition_filters.append(partition_col <= partition_upper_bound)
@@ -363,15 +368,15 @@
     right: NodeRef
     join_cols: List[str]
     how: str
 
     @property
     def columns(self) -> Tuple[str, ...]:
         right_nonjoin_cols = set(self.right.columns) - set(self.join_cols)
-        return tuple(list(self.left.columns) + list(right_nonjoin_cols))
+        return tuple(list(self.left.columns) + sorted(list(right_nonjoin_cols)))
 
     @property
     def inputs(self) -> Tuple[NodeRef, ...]:
         return (self.left, self.right)
 
     @property
     def input_names(self) -> Optional[List[str]]:
@@ -393,15 +398,16 @@
         elif self.how == "left":
             join_q = join_q.left_join(right_q)
         elif self.how == "right":
             join_q = join_q.right_join(right_q)
         elif self.how == "outer":
             join_q = join_q.outer_join(right_q)
         else:
-            raise NotImplementedError(f"Join Type {self.how} has not been implemented")
+            msg = f"Join Type {self.how} has not been implemented"
+            raise NotImplementedError(msg)
         return join_q.using(*self.join_cols).select("*")
 
     def get_sql_views(self) -> List[Tuple[str, str]]:
         if not conf.get_bool("QUERYTREE_SHORT_SQL_ENABLED"):
             return []
         view_sql = self.right.node.to_sql()
         return [(self._get_view_name(), view_sql)]
@@ -432,15 +438,15 @@
         return tuple(list(self.left.columns) + list(right_nonjoin_cols))
 
     @property
     def inputs(self) -> Tuple[NodeRef, ...]:
         return (self.left, self.right)
 
     def as_str(self, verbose: bool):
-        return f"Outer join (include nulls)" + (f" on {self.join_cols}:" if verbose else ":")
+        return "Outer join (include nulls)" + (f" on {self.join_cols}:" if verbose else ":")
 
     def _to_query(self) -> pypika.Query:
         raise NotImplementedError
 
 
 @attrs.frozen
 class EntityFilterNode(QueryNode):
@@ -578,15 +584,15 @@
             [LiteralValue(False).as_(IS_LEFT)]
             + [Field(x) for x in common_cols]
             + [NULL.as_(x) for x in left_nonjoin_cols]
             + [struct(right_nonjoin_cols).as_(self._right_struct_col)]
         )
         left_df = Query().from_(left_df).select(*left_full_cols)
         right_df = Query().from_(right_df).select(*right_full_cols)
-        union = left_df.union(right_df)
+        union = left_df.union_all(right_df)
         right_window_funcs = []
         # Also order by IS_LEFT because we want spine rows to be after feature rows if
         # timestamps are the same
         order_by_fields = timestamp_join_cols + [IS_LEFT]
         right_window_funcs.append(
             LastValue(Field(self._right_struct_col))
             .over(*[Field(x) for x in self.join_cols])
@@ -625,15 +631,15 @@
         return (self.spine, self.partial_agg_node)
 
     @property
     def input_names(self) -> Optional[List[str]]:
         return ["spine", "partial_aggregates"]
 
     def as_str(self):
-        return f"Spine asof join partial aggregates, where the join condition is partial_aggregates._anchor_time <= spine._anchor_time and partial aggregates are rolled up to compute full aggregates"
+        return "Spine asof join partial aggregates, where the join condition is partial_aggregates._anchor_time <= spine._anchor_time and partial aggregates are rolled up to compute full aggregates"
 
     @property
     def columns(self) -> Tuple[str, ...]:
         return tuple(list(self.spine.columns) + self.fdw.features)
 
     def _get_aggregations(self, window_order_col: str, partition_cols: List[str]) -> List[Term]:
         time_aggregation = self.fdw.trailing_time_window_aggregation
@@ -649,14 +655,17 @@
                 tile_interval = self.fdw.get_tile_interval_for_version
             earliest_anchor_time = (
                 convert_timedelta_for_version(window_duration, feature_store_format_version) - tile_interval
             )
             # Adjust earliest_anchor_time by * 2 + 1 to account for the changes to TECTON_WINDOW_ORDER_COL
             earliest_anchor_time = an.Preceding(earliest_anchor_time * 2 + 1)
             aggregation_plan = AGGREGATION_PLANS.get(feature.function)
+            if conf.get_or_none("SQL_DIALECT") == "athena" and aggregation_plan is None:
+                msg = f"{feature.function} is not implemented in Athena"
+                raise TectonAthenaNotImplementedError(msg)
             names = aggregation_plan.materialized_column_names(feature.input_feature_name)
             query_window_spec = QueryWindowSpec(
                 partition_cols=partition_cols,
                 order_by_col=window_order_col,
                 range_start=earliest_anchor_time,
                 range_end=an.CURRENT_ROW,
             )
@@ -671,38 +680,36 @@
         join_keys = self.fdw.join_keys
         timestamp_join_cols = [ANCHOR_TIME]
         common_cols = join_keys + timestamp_join_cols
         left_nonjoin_cols = list(set(self.spine.node.columns) - set(common_cols))
         left_prefix = "_tecton_left"
         right_nonjoin_cols = list(set(self.partial_agg_node.node.columns) - set(join_keys + timestamp_join_cols))
         IS_LEFT = "_tecton_is_left"
-
         # Since the spine and feature rows are unioned together, the spine rows must be ordered after the feature rows
         # when they have the same ANCHOR_TIME for window aggregation to be correct. Window aggregation does not allow
         # ordering using two columns when range between is used. So we adjust the spine row ANCHOR_TIME by * 2 + 1, and the
         # feature row ANCHOR_TIME by * 2. Range between values will also be adjusted due to these changes.
         TECTON_WINDOW_ORDER_COL = "_tecton_window_order_col"
         left_full_cols = (
             [LiteralValue(True).as_(IS_LEFT)]
             + [Field(x) for x in common_cols]
             + [Field(x).as_(f"{left_prefix}_{x}") for x in left_nonjoin_cols]
             + [NULL.as_(x) for x in right_nonjoin_cols]
-            + [(Field(ANCHOR_TIME) * 2 + 1).as_(TECTON_WINDOW_ORDER_COL)]
+            + [(Cast(Field(ANCHOR_TIME) * 2 + 1, "bigint")).as_(TECTON_WINDOW_ORDER_COL)]
         )
         right_full_cols = (
             [LiteralValue(False).as_(IS_LEFT)]
             + [Field(x) for x in common_cols]
             + [NULL.as_(f"{left_prefix}_{x}") for x in left_nonjoin_cols]
             + [Field(x) for x in right_nonjoin_cols]
-            + [(Field(ANCHOR_TIME) * 2).as_(TECTON_WINDOW_ORDER_COL)]
+            + [(Cast(Field(ANCHOR_TIME) * 2, "bigint")).as_(TECTON_WINDOW_ORDER_COL)]
         )
         left_df = Query().from_(left_df).select(*left_full_cols)
         right_df = Query().from_(right_df).select(*right_full_cols)
-        union = left_df.union(right_df)
-
+        union = left_df.union_all(right_df)
         aggregations = self._get_aggregations(TECTON_WINDOW_ORDER_COL, join_keys)
         output_columns = (
             common_cols
             + [Field(f"{left_prefix}_{x}").as_(x) for x in left_nonjoin_cols]
             + aggregations
             + [Field(IS_LEFT)]
         )
@@ -742,15 +749,15 @@
         return (self.left, self.right)
 
     @property
     def input_names(self) -> Optional[List[str]]:
         return ["left", "right"]
 
     def as_str(self):
-        return f"Left asof wildcard match and explode right, where the join condition is left._anchor_time - ttl + 1 < right._anchor_time <= left._anchor_time."
+        return "Left asof wildcard match and explode right, where the join condition is left._anchor_time - ttl + 1 < right._anchor_time <= left._anchor_time."
 
     @property
     def columns(self) -> Tuple[str, ...]:
         return tuple(list(self.left.columns) + [self.fdw.wildcard_join_key])
 
     def _to_query(self) -> pypika.Query:
         raise NotImplementedError
@@ -788,15 +795,16 @@
         if self.window_end_column_name is not None and not self.fdw.is_continuous:
             cols.append(self.window_end_column_name)
         return cols
 
     @fdw.validator
     def check_is_aggregate(self, _, value):
         if not value.is_temporal_aggregate:
-            raise ValueError("Cannot construct a PartialAggNode using a non-aggregate feature view.")
+            msg = "Cannot construct a PartialAggNode using a non-aggregate feature view."
+            raise ValueError(msg)
 
     @property
     def inputs(self) -> Tuple[NodeRef, ...]:
         return (self.input_node,)
 
     def as_str(self):
         actions = [
@@ -848,17 +856,27 @@
 
             select_cols = agg_cols + join_cols + [window_start.as_(self.window_start_column_name)]
             group_by_cols = join_cols + [window_start]
             group_by_cols.append(window_end)
             q = q.groupby(*group_by_cols)
             if self.window_end_column_name:
                 select_cols.append(window_end.as_(self.window_end_column_name))
+
         else:
             # Continuous
-            select_cols = agg_cols + join_cols + [timestamp_field, Field(time_aggregation.time_key).as_(ANCHOR_TIME)]
+            select_cols = (
+                agg_cols
+                + join_cols
+                + [
+                    timestamp_field,
+                    convert_epoch_seconds_to_feature_store_format_version(
+                        to_unixtime(Field(time_aggregation.time_key)), self.fdw.get_feature_store_format_version
+                    ).as_(ANCHOR_TIME),
+                ]
+            )
         res = q.select(*select_cols)
         return res
 
     def _get_partial_agg_columns(self):
         time_aggregation = self.fdw.trailing_time_window_aggregation
         agg_cols = []
         output_columns = set()
@@ -996,30 +1014,30 @@
             )
         elif self.is_stream:
             return (
                 base
                 + f"Since '{self.name} is a stream feature view with aggregations in time interval mode, feature data "
                 + "is stored in tiles. Each tile has size equal to the tile interval, which is "
                 + f"{convert_duration_to_seconds(self.tile_interval, self.feature_store_format_version)} seconds. "
-                + f"The anchor time column contains the start time of the most recent tile available for retrieval. "
+                + "The anchor time column contains the start time of the most recent tile available for retrieval. "
                 + f"It is calculated as '{self.timestamp_field}' - ('{self.timestamp_field}' % tile_interval) "
-                + f"- tile_interval."
+                + "- tile_interval."
             )
         else:
             if self.data_delay_seconds > 0:
                 data_delay_seconds = f"Let T = '{self.timestamp_field}' - data_delay where data_delay = {self.data_delay_seconds} seconds. "
             else:
                 data_delay_seconds = f"Let T be the timestamp column '{self.timestamp_field}'. "
 
             return (
                 base
                 + f"Since '{self.name}' is a batch feature view with aggregations, feature data is stored in tiles. "
                 + "Each tile has size equal to the tile interval, which is "
                 f"{convert_duration_to_seconds(self.tile_interval, self.feature_store_format_version)} seconds. "
-                + f"The anchor time column contains the start time of the most recent tile available for retrieval. "
+                + "The anchor time column contains the start time of the most recent tile available for retrieval. "
                 + data_delay_seconds
                 + f"The anchor time column is calculated as T - (T % batch_schedule) - tile_interval where batch_schedule = "
                 f"{convert_duration_to_seconds(self.batch_schedule, self.feature_store_format_version)} seconds."
             )
 
     def _to_query(self) -> pypika.Query:
         uid = self.input_node.node.__class__.__name__ + "_" + str(id(self.input_node.node))
@@ -1105,15 +1123,16 @@
 
     @mapping.validator  # type: ignore
     def check_non_null_keys(self, _, value):
         if value is None:
             return
         for k in value.keys():
             if k is None:
-                raise ValueError(f"RenameColsNode mapping should only contain non-null keys. Mapping={value}")
+                msg = f"RenameColsNode mapping should only contain non-null keys. Mapping={value}"
+                raise ValueError(msg)
 
     @property
     def columns(self) -> Tuple[str, ...]:
         cols = self.input_node.columns
         assert cols is not None, self.input_node
         newcols = []
         for col in cols:
@@ -1499,20 +1518,21 @@
                 timestamp_col = date_add("millisecond", -1, timestamp_col)
             effective_timestamp = from_unixtime(
                 to_unixtime(timestamp_col)
                 - (to_unixtime(timestamp_col) % self.batch_schedule_seconds)
                 + self.batch_schedule_seconds
                 + self.data_delay_seconds
             )
-        return (
-            Query()
-            .with_(input_query, uid)
-            .from_(AliasedQuery(uid))
-            .select("*", effective_timestamp.as_(self.effective_timestamp_name))
-        )
+        fields = []
+        for col in self.columns:
+            if col == self.effective_timestamp_name:
+                fields.append(effective_timestamp.as_(self.effective_timestamp_name))
+            else:
+                fields.append(Field(col))
+        return Query().with_(input_query, uid).from_(AliasedQuery(uid)).select(*fields)
 
 
 @attrs.frozen
 class AddDurationNode(QueryNode):
     """Adds a duration to a timestamp field"""
 
     input_node: NodeRef
```

### Comparing `tecton-0.7.0b9/tecton_core/query/sql_compat.py` & `tecton-0.7.0rc0/tecton_core/query/sql_compat.py`

 * *Files 2% similar despite different names*

```diff
@@ -2,19 +2,19 @@
 from typing import Any
 from typing import Dict
 from typing import List
 from typing import Optional
 
 import pypika
 from pypika import AliasedQuery
-from pypika import analytics
 from pypika import CustomFunction
 from pypika import Field
 from pypika import Interval
 from pypika import Table
+from pypika import analytics
 from pypika.dialects import MySQLQuery
 from pypika.dialects import PostgreSQLQuery
 from pypika.functions import Cast
 from pypika.functions import DateAdd
 from pypika.queries import Selectable
 from pypika.terms import AnalyticFunction
 from pypika.terms import Function
@@ -114,16 +114,17 @@
 
 def dialect() -> Dialect:
     # TODO(Daryl) - infer the sql dialect from the fv itself,
     # We just use the conf for now as athena transformations are still under development.
     d = conf.get_or_none("SQL_DIALECT")
     try:
         return Dialect(d)
-    except:
-        raise Exception(f"Unsupported sql dialect: set SQL_DIALECT to {[x.value for x in Dialect]}")
+    except Exception:
+        msg = f"Unsupported sql dialect: set SQL_DIALECT to {[x.value for x in Dialect]}"
+        raise Exception(msg)
 
 
 def Query():
     if dialect() == Dialect.SNOWFLAKE:
         return CaseSensitiveSnowflakeQuery
     elif dialect() == Dialect.SPARK:
         # Spark (similar to HiveSQL) uses backticks like MySQL
@@ -239,15 +240,15 @@
     Converts an epoch term to a timestamp column
     :param timestamp: epoch column [V0 : Seconds, V1 : Nanoseconds]
     :param time_stamp_feature_store_format_version: Feature Store Format Version
     :return: epoch in seconds
     """
     if time_stamp_feature_store_format_version == FeatureStoreFormatVersion.FEATURE_STORE_FORMAT_VERSION_DEFAULT:
         return timestamp
-    return timestamp / 1e9
+    return timestamp / int(1e9)
 
 
 def convert_epoch_seconds_to_feature_store_format_version(
     timestamp: Term, feature_store_format_version: FeatureStoreFormatVersion
 ):
     if feature_store_format_version == FeatureStoreFormatVersion.FEATURE_STORE_FORMAT_VERSION_DEFAULT:
         return timestamp
```

### Comparing `tecton-0.7.0b9/tecton_core/repo_file_handler.py` & `tecton-0.7.0rc0/tecton_core/repo_file_handler.py`

 * *Files 5% similar despite different names*

```diff
@@ -37,38 +37,42 @@
 def ensure_prepare_repo(file_in_repo: Optional[str] = None) -> None:
     repo_data = get_repo_data()
     if repo_data:
         # repo is already prepared
         return
     root = _maybe_get_repo_root(file_in_repo)
     if root is None:
-        raise Exception(f"Feature repository root not found. Run `tecton init` to set it.")
+        msg = "Feature repository root not found. Run `tecton init` to set it."
+        raise Exception(msg)
     paths = get_repo_files(root)
     file_set = {str(f) for f in paths}
     set_repo_data(RepoData(paths=paths, file_set=file_set, root=root))
 
 
 def repo_files() -> List[Path]:
     repo_data = get_repo_data()
     if repo_data is None:
-        raise Exception("Repo is not prepared")
+        msg = "Repo is not prepared"
+        raise Exception(msg)
     return repo_data.paths
 
 
 def repo_files_set() -> Set[str]:
     repo_data = get_repo_data()
     if repo_data is None:
-        raise Exception("Repo is not prepared")
+        msg = "Repo is not prepared"
+        raise Exception(msg)
     return repo_data.file_set
 
 
 def repo_root() -> str:
     repo_data = get_repo_data()
     if repo_data is None:
-        raise Exception("Repo is not prepared")
+        msg = "Repo is not prepared"
+        raise Exception(msg)
     return repo_data.root
 
 
 def _fake_init_for_testing(root: str = "") -> None:
     fake_root = Path(root)
     set_repo_data(RepoData(paths=[fake_root], file_set={str(fake_root)}, root=str(fake_root)))
```

### Comparing `tecton-0.7.0b9/tecton_core/schema_derivation_utils.py` & `tecton-0.7.0rc0/tecton_core/schema_derivation_utils.py`

 * *Files 10% similar despite different names*

```diff
@@ -26,35 +26,33 @@
         timestamp_key = feature_view_args.materialized_feature_view_args.timestamp_field
     else:
         timestamp_fields = [
             column for column in view_schema.columns if column.offline_data_type == tecton_types.TimestampType().proto
         ]
 
         if len(timestamp_fields) != 1:
-            raise errors.TectonValidationError(
-                "The timestamp_field must be set on the Feature View or the feature view transformation output should contain only one and only one column of type Timestamp"
-            )
+            msg = "The timestamp_field must be set on the Feature View or the feature view transformation output should contain only one and only one column of type Timestamp"
+            raise errors.TectonValidationError(msg)
         timestamp_key = timestamp_fields[0].name
 
     view_schema_column_names = [column.name for column in view_schema.columns]
     if timestamp_key not in view_schema_column_names:
-        raise errors.TectonValidationError(
-            f"Timestamp key '{timestamp_key}' not found in view schema. View schema has columns: {', '.join(view_schema_column_names)}"
-        )
+        msg = f"Timestamp key '{timestamp_key}' not found in view schema. View schema has columns: {', '.join(view_schema_column_names)}"
+        raise errors.TectonValidationError(msg)
     return timestamp_key
 
 
 def populate_schema_with_derived_fields(schema: schema_pb2.Schema) -> None:
     """Copies the behavior of populateSchemaWithDerivedFields in FeatureViewUtils.kt.
 
     Should only be applied to the schemas of Push Sources, which are expected to have the offline_data_type field set.
     """
     for column in schema.columns:
         assert column.offline_data_type is not None
-        data_type = tecton_types.data_type_from_proto(column.offline_data_type)
+        tecton_types.data_type_from_proto(column.offline_data_type)
         column.feature_server_data_type.CopyFrom(column.offline_data_type)
 
 
 def compute_aggregate_materialization_schema_from_view_schema(
     view_schema: schema_pb2.Schema,
     feature_view_args: feature_view_pb2.FeatureViewArgs,
     is_spark: bool,
@@ -64,34 +62,34 @@
 
     # Add join key columns from view schema to materializaton schema.
     join_keys = []
     for entity in feature_view_args.entities:
         join_keys.extend(entity.join_keys)
     for join_key in join_keys:
         if join_key not in view_schema_column_map:
-            raise errors.TectonValidationError(
-                f"Join key {join_key} not found in view schema. View schema has columns {','.join(view_schema_column_map.keys())}"
-            )
+            msg = f"Join key {join_key} not found in view schema. View schema has columns {','.join(view_schema_column_map.keys())}"
+            raise errors.TectonValidationError(msg)
         materialization_schema_columns.append(view_schema_column_map[join_key])
 
     # Add columns for aggregate features.
     added = []
     for aggregation in feature_view_args.materialized_feature_view_args.aggregations:
         if aggregation.column not in view_schema_column_map:
-            raise errors.TectonValidationError(
-                f"Column {aggregation.column} used for aggregations not found in view schema. View schema has columns {','.join(view_schema_column_map.keys())}"
-            )
+            msg = f"Column {aggregation.column} used for aggregations not found in view schema. View schema has columns {','.join(view_schema_column_map.keys())}"
+            raise errors.TectonValidationError(msg)
         input_column = view_schema_column_map[aggregation.column]
 
-        is_continous = (
+        is_continuous = (
             feature_view_args.materialized_feature_view_args.stream_processing_mode
             == feature_view_pb2.StreamProcessingMode.STREAM_PROCESSING_MODE_CONTINUOUS
         )
         prefixes = aggregation_utils.get_materialization_aggregation_column_prefixes(
-            aggregation.function.lower(), aggregation.function_params, is_continous
+            aggregation_utils.get_aggregation_enum_from_string(aggregation.function.lower()),
+            aggregation.function_params,
+            is_continuous,
         )
         for prefix in prefixes:
             materialization_column_name = aggregation_utils.get_materialization_column_name(prefix, input_column.name)
 
             tecton_type = aggregation_utils.aggregation_prefix_to_tecton_type(prefix)
             if tecton_type is None:
                 tecton_type = tecton_types.data_type_from_proto(input_column.offline_data_type)
```

### Comparing `tecton-0.7.0b9/tecton_core/specs/__init__.py` & `tecton-0.7.0rc0/tecton_core/specs/__init__.py`

 * *Files 18% similar despite different names*

```diff
@@ -2,14 +2,14 @@
 
 Specs provide a unified, frozen (i.e. immutable), and more useful abstraction over args and data protos for use within
 the Python SDK.
 
 See the RFC;
 https://www.notion.so/tecton/RFC-Unified-SDK-for-Notebook-Driven-Development-a377af9d320f46488ea238e51e2ce656
 """
-# nopycln: file
+
 from tecton_core.specs.data_source_spec import *
 from tecton_core.specs.entity_spec import *
 from tecton_core.specs.feature_service_spec import *
 from tecton_core.specs.feature_view_spec import *
 from tecton_core.specs.tecton_object_spec import *
 from tecton_core.specs.transformation_spec import *
```

### Comparing `tecton-0.7.0b9/tecton_core/specs/data_source_spec.py` & `tecton-0.7.0rc0/tecton_core/specs/data_source_spec.py`

 * *Files 4% similar despite different names*

```diff
@@ -18,20 +18,23 @@
 from tecton_proto.args import virtual_data_source_pb2 as virtual_data_source__args_pb2
 from tecton_proto.common import data_source_type_pb2
 from tecton_proto.common import schema_container_pb2
 from tecton_proto.common import spark_schema_pb2
 from tecton_proto.data import batch_data_source_pb2 as batch_data_source__data_pb2
 from tecton_proto.data import stream_data_source_pb2 as stream_data_source__data_pb2
 from tecton_proto.data import virtual_data_source_pb2 as virtual_data_source__data_pb2
+from tecton_proto.validation import validator_pb2
+
 
 __all__ = [
     "DataSourceSpec",
     "DataSourceSpecArgsSupplement",
     "BatchSourceSpec",
     "HiveSourceSpec",
+    "UnitySourceSpec",
     "FileSourceSpec",
     "SparkBatchSourceSpec",
     "RedshiftSourceSpec",
     "SnowflakeSourceSpec",
     "DatetimePartitionColumnSpec",
     "StreamSourceSpec",
     "KinesisSourceSpec",
@@ -43,34 +46,28 @@
 @utils.frozen_strict
 class DataSourceSpec(tecton_object_spec.TectonObjectSpec):
     batch_source: Optional["BatchSourceSpec"]
     stream_source: Optional["StreamSourceSpec"]
     schema: Optional[schema_container_pb2.SchemaContainer]
     type: data_source_type_pb2.DataSourceType.ValueType
 
-    # This is used for calling backend validation
-    # TODO(jake): use validation args here instead
-    data_proto: Optional[virtual_data_source__data_pb2.VirtualDataSource] = attrs.field(
-        metadata={utils.LOCAL_REMOTE_DIVERGENCE_ALLOWED: True}
-    )
-
     @classmethod
     @typechecked
     def from_data_proto(
         cls, proto: virtual_data_source__data_pb2.VirtualDataSource, deserialize_funcs_to_main: bool = False
     ) -> "DataSourceSpec":
         return cls(
             metadata=tecton_object_spec.TectonObjectMetadataSpec.from_data_proto(
                 proto.virtual_data_source_id, proto.fco_metadata
             ),
             batch_source=create_batch_source_from_data_proto(proto.batch_data_source, deserialize_funcs_to_main),
             stream_source=create_stream_source_from_data_proto(proto.stream_data_source, deserialize_funcs_to_main),
             type=utils.get_field_or_none(proto, "data_source_type"),
             schema=utils.get_field_or_none(proto, "schema"),
-            data_proto=proto,
+            validation_args=validator_pb2.FcoValidationArgs(virtual_data_source=proto.validation_args),
         )
 
     @classmethod
     @typechecked
     def from_args_proto(
         cls,
         proto: virtual_data_source__args_pb2.VirtualDataSourceArgs,
@@ -80,15 +77,15 @@
             metadata=tecton_object_spec.TectonObjectMetadataSpec.from_args_proto(
                 proto.virtual_data_source_id, proto.info
             ),
             batch_source=create_batch_source_from_args_proto(proto, supplement),
             stream_source=create_stream_source_from_args_proto(proto, supplement),
             schema=utils.get_field_or_none(proto, "schema"),
             type=utils.get_field_or_none(proto, "type"),
-            data_proto=None,
+            validation_args=None,
         )
 
 
 @attrs.define
 class DataSourceSpecArgsSupplement:
     """A data class used for supplementing args protos during DataSourceSpec construction.
 
@@ -117,18 +114,27 @@
     data_delay: Optional[pendulum.Duration]
 
 
 @typechecked
 def create_batch_source_from_data_proto(
     proto: batch_data_source__data_pb2.BatchDataSource, deserialize_funcs_to_main: bool = False
 ) -> Optional[
-    Union["HiveSourceSpec", "SparkBatchSourceSpec", "FileSourceSpec", "RedshiftSourceSpec", "SnowflakeSourceSpec"]
+    Union[
+        "HiveSourceSpec",
+        "SparkBatchSourceSpec",
+        "FileSourceSpec",
+        "RedshiftSourceSpec",
+        "SnowflakeSourceSpec",
+        "UnitySourceSpec",
+    ]
 ]:
     if proto.HasField("hive_table"):
         return HiveSourceSpec.from_data_proto(proto, deserialize_funcs_to_main)
+    elif proto.HasField("unity_table"):
+        return UnitySourceSpec.from_data_proto(proto, deserialize_funcs_to_main)
     elif proto.HasField("spark_data_source_function"):
         return SparkBatchSourceSpec.from_data_proto(proto, deserialize_funcs_to_main)
     elif proto.HasField("redshift_db"):
         return RedshiftSourceSpec.from_data_proto(proto, deserialize_funcs_to_main)
     elif proto.HasField("snowflake"):
         return SnowflakeSourceSpec.from_data_proto(proto, deserialize_funcs_to_main)
     elif proto.HasField("file"):
@@ -137,30 +143,71 @@
         return None
 
 
 @typechecked
 def create_batch_source_from_args_proto(
     proto: virtual_data_source__args_pb2.VirtualDataSourceArgs, supplement: DataSourceSpecArgsSupplement
 ) -> Optional[
-    Union["HiveSourceSpec", "SparkBatchSourceSpec", "FileSourceSpec", "RedshiftSourceSpec", "SnowflakeSourceSpec"]
+    Union[
+        "HiveSourceSpec",
+        "SparkBatchSourceSpec",
+        "FileSourceSpec",
+        "RedshiftSourceSpec",
+        "SnowflakeSourceSpec",
+        "UnitySourceSpec",
+    ]
 ]:
     if proto.HasField("hive_ds_config"):
         return HiveSourceSpec.from_args_proto(proto.hive_ds_config, supplement)
+    elif proto.HasField("unity_ds_config"):
+        return UnitySourceSpec.from_args_proto(proto.unity_ds_config, supplement)
     elif proto.HasField("spark_batch_config"):
         return SparkBatchSourceSpec.from_args_proto(proto.spark_batch_config, supplement)
     elif proto.HasField("redshift_ds_config"):
         return RedshiftSourceSpec.from_args_proto(proto.redshift_ds_config, supplement)
     elif proto.HasField("snowflake_ds_config"):
         return SnowflakeSourceSpec.from_args_proto(proto.snowflake_ds_config, supplement)
     elif proto.HasField("file_ds_config"):
         return FileSourceSpec.from_args_proto(proto.file_ds_config, supplement)
     else:
         return None
 
 
+def _datepart_to_minimum_seconds(datepart: str) -> int:
+    if datepart == "year":
+        return 365 * 24 * 60 * 60
+    elif datepart == "month":
+        return 28 * 24 * 60 * 60
+    elif datepart == "day":
+        return 24 * 60 * 60
+    elif datepart == "hour":
+        return 60 * 60
+    elif datepart == "date":
+        return 24 * 60 * 60
+    else:
+        msg = f"Unexpected datepart string: {datepart}"
+        raise ValueError(msg)
+
+
+def _datepart_to_default_format_string(datepart: str) -> str:
+    if datepart == "year":
+        return "%Y"
+    elif datepart == "month":
+        return "%m"
+    elif datepart == "day":
+        return "%d"
+    elif datepart == "hour":
+        return "%H"
+    elif datepart == "date":
+        return "%Y-%m-%d"
+    else:
+        msg = f"Unexpected datepart string: {datepart}"
+        raise ValueError(msg)
+
+
 @utils.frozen_strict
 class DatetimePartitionColumnSpec:
     column_name: str
     format_string: str
     minimum_seconds: int
 
     @classmethod
@@ -175,37 +222,32 @@
         )
 
     @classmethod
     @typechecked
     def from_args_proto(cls, proto: data_source__args_pb2.DatetimePartitionColumnArgs) -> "DatetimePartitionColumnSpec":
         # This constructor mirrors backend Kotlin logic: https://github.com/tecton-ai/tecton/blob/6107d76680fa97e6155c6da9ee7d2da47bc3d6a7/java/com/tecton/datamodel/datasource/Utils.kt#L41.
         # In the short-term, divergence between Python and Kotlin will be prevented using integration test coverage. Longer-term, these values should probably be derived client-side only and included in the data source args.
-        if proto.datepart == "year":
-            format_string = "%Y"
-            minimum_seconds = 365 * 24 * 60 * 60
-        elif proto.datepart == "month":
-            format_string = "%m"
-            minimum_seconds = 28 * 24 * 60 * 60
-        elif proto.datepart == "day":
-            format_string = "%d"
-            minimum_seconds = 24 * 60 * 60
-        elif proto.datepart == "hour":
-            format_string = "%H"
-            minimum_seconds = 60 * 60
-        elif proto.datepart == "date":
-            assert proto.zero_padded or not proto.format_string, "Non-zero padded date strings are not supported."
-            format_string = "%Y-%m-%d"
-            minimum_seconds = 24 * 60 * 60
-        else:
-            assert False, f"Unexpected datepart string: {proto.datepart}"
+        minimum_seconds = _datepart_to_minimum_seconds(proto.datepart)
 
         if proto.format_string:
+            # NOTE: we do not use `zero_padded` if the user specified a format_string. However,
+            # we do not directly validate that this is unset. We may want to in the future, but
+            # not making the change now in case it causes a repo failure.
             format_string = proto.format_string
-        elif not proto.zero_padded:
-            format_string = format_string.replace("%", "%-")
+        else:
+            if proto.datepart == "date" and not proto.zero_padded:
+                msg = "Non-zero padded date strings are not supported. Please set `zero_padded = True` in your `DatetimePartitionColumn`."
+                raise ValueError(msg)
+
+            default_format_string = _datepart_to_default_format_string(proto.datepart)
+            if proto.zero_padded:
+                # The default string format should be zero padded.
+                format_string = default_format_string
+            else:
+                format_string = default_format_string.replace("%", "%-")
 
         return DatetimePartitionColumnSpec(
             column_name=proto.column_name, format_string=format_string, minimum_seconds=minimum_seconds
         )
 
 
 @utils.frozen_strict
@@ -272,14 +314,72 @@
             datetime_partition_columns=tuple(
                 DatetimePartitionColumnSpec.from_args_proto(column) for column in proto.datetime_partition_columns
             ),
         )
 
 
 @utils.frozen_strict
+class UnitySourceSpec(BatchSourceSpec):
+    catalog: str
+    schema: str
+    table: str
+    datetime_partition_columns: Tuple[DatetimePartitionColumnSpec, ...]
+
+    @classmethod
+    @typechecked
+    def from_data_proto(
+        cls, proto: batch_data_source__data_pb2.BatchDataSource, deserialize_funcs_to_main: bool = False
+    ) -> "UnitySourceSpec":
+        post_processor = None
+        if proto.HasField("raw_batch_translator"):
+            if deserialize_funcs_to_main:
+                post_processor = function_deserialization.from_proto_to_main(proto.raw_batch_translator)
+            else:
+                post_processor = function_deserialization.from_proto(proto.raw_batch_translator)
+        return cls(
+            timestamp_field=utils.get_field_or_none(proto.timestamp_column_properties, "column_name"),
+            timestamp_format=utils.get_field_or_none(proto.timestamp_column_properties, "format"),
+            post_processor=post_processor,
+            data_delay=utils.get_duration_field_or_none(proto, "data_delay"),
+            spark_schema=utils.get_field_or_none(proto, "spark_schema"),
+            table=utils.get_field_or_none(proto.unity_table, "table"),
+            schema=utils.get_field_or_none(proto.unity_table, "schema"),
+            catalog=utils.get_field_or_none(proto.unity_table, "catalog"),
+            datetime_partition_columns=tuple(
+                DatetimePartitionColumnSpec.from_data_proto(column) for column in proto.datetime_partition_columns
+            ),
+        )
+
+    @classmethod
+    @typechecked
+    def from_args_proto(
+        cls, proto: data_source__args_pb2.UnityDataSourceArgs, supplement: DataSourceSpecArgsSupplement
+    ) -> "UnitySourceSpec":
+        # If a function was serialized for this data source (e.g. because it was defined in a repo), then prefer to
+        # use the serialized function over directly using the Python function.
+        if proto.common_args.HasField("post_processor"):
+            post_processor = function_deserialization.from_proto(proto.common_args.post_processor)
+        else:
+            post_processor = supplement.batch_post_processor
+        return cls(
+            timestamp_field=utils.get_field_or_none(proto.common_args, "timestamp_field"),
+            timestamp_format=utils.get_field_or_none(proto, "timestamp_format"),
+            post_processor=post_processor,
+            data_delay=utils.get_duration_field_or_none(proto.common_args, "data_delay"),
+            spark_schema=supplement.batch_schema,
+            table=utils.get_field_or_none(proto, "table"),
+            schema=utils.get_field_or_none(proto, "schema"),
+            catalog=utils.get_field_or_none(proto, "catalog"),
+            datetime_partition_columns=tuple(
+                DatetimePartitionColumnSpec.from_args_proto(column) for column in proto.datetime_partition_columns
+            ),
+        )
+
+
+@utils.frozen_strict
 class FileSourceSpec(BatchSourceSpec):
     uri: Optional[str]
     file_format: batch_data_source__data_pb2.FileDataSourceFormat.ValueType
     convert_to_glue_format: bool
     schema_uri: Optional[str]
     schema_override: Optional[spark_schema_pb2.SparkSchema]
 
@@ -427,17 +527,18 @@
         )
 
     @classmethod
     @typechecked
     def from_args_proto(
         cls, proto: data_source__args_pb2.RedshiftDataSourceArgs, supplement: DataSourceSpecArgsSupplement
     ) -> "RedshiftSourceSpec":
-        temp_s3 = conf.get_or_none("SPARK_REDSHIFT_TEMP_DIR")
-        if temp_s3 is None:
-            raise errors.REDSHIFT_DS_MISSING_SPARK_TEMP_DIR
+        # The temp_s3 directory should be set, and this should have been verified during validation. The one exception
+        # is for unit tests (i.e. `test_run()`). In unit tests, validation is skipped and the redshift directory should
+        # not need to be set, so fallback to an empty string.
+        temp_s3 = conf.get_or_none("SPARK_REDSHIFT_TEMP_DIR") or ""
 
         # If a function was serialized for this data source (e.g. because it was defined in a repo), then prefer to
         # use the serialized function over directly using the Python function.
         if proto.common_args.HasField("post_processor"):
             post_processor = function_deserialization.from_proto(proto.common_args.post_processor)
         else:
             post_processor = supplement.batch_post_processor
```

### Comparing `tecton-0.7.0b9/tecton_core/specs/entity_spec.py` & `tecton-0.7.0rc0/tecton_core/specs/entity_spec.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,41 +1,37 @@
-from typing import Optional
 from typing import Tuple
 
-import attrs
 from typeguard import typechecked
 
 from tecton_core.specs import tecton_object_spec
 from tecton_core.specs import utils
 from tecton_proto.args import entity_pb2 as entity__args_pb2
 from tecton_proto.data import entity_pb2 as entity__data_pb2
+from tecton_proto.validation import validator_pb2
+
 
 __all__ = [
     "EntitySpec",
 ]
 
 
 @utils.frozen_strict
 class EntitySpec(tecton_object_spec.TectonObjectSpec):
     join_keys: Tuple[str, ...]
 
-    # Temporarily expose the underlying data proto during migration.
-    # TODO(TEC-12443): Remove this attribute.
-    data_proto: Optional[entity__data_pb2.Entity] = attrs.field(metadata={utils.LOCAL_REMOTE_DIVERGENCE_ALLOWED: True})
-
     @classmethod
     @typechecked
     def from_data_proto(cls, proto: entity__data_pb2.Entity) -> "EntitySpec":
         return cls(
             metadata=tecton_object_spec.TectonObjectMetadataSpec.from_data_proto(proto.entity_id, proto.fco_metadata),
             join_keys=utils.get_tuple_from_repeated_field(proto.join_keys),
-            data_proto=proto,
+            validation_args=validator_pb2.FcoValidationArgs(entity=proto.validation_args),
         )
 
     @classmethod
     @typechecked
     def from_args_proto(cls, proto: entity__args_pb2.EntityArgs) -> "EntitySpec":
         return cls(
             metadata=tecton_object_spec.TectonObjectMetadataSpec.from_args_proto(proto.entity_id, proto.info),
             join_keys=utils.get_tuple_from_repeated_field(proto.join_keys),
-            data_proto=None,
+            validation_args=None,
         )
```

### Comparing `tecton-0.7.0b9/tecton_core/specs/feature_service_spec.py` & `tecton-0.7.0rc0/tecton_core/specs/feature_service_spec.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,63 +1,58 @@
 from typing import Dict
-from typing import Optional
 from typing import Tuple
 
 import attrs
 from typeguard import typechecked
 
 from tecton_core import id_helper
 from tecton_core.specs import feature_view_spec
 from tecton_core.specs import tecton_object_spec
 from tecton_core.specs import utils
 from tecton_proto.args import feature_service_pb2 as feature_service__args_pb2
 from tecton_proto.data import feature_service_pb2 as feature_service__data_pb2
+from tecton_proto.validation import validator_pb2
+
 
 __all__ = [
     "FeatureServiceSpec",
     "FeatureSetItemSpec",
     "FeatureServiceSpecArgsSupplement",
 ]
 
 
 @utils.frozen_strict
 class FeatureServiceSpec(tecton_object_spec.TectonObjectSpec):
     feature_set_items: Tuple["FeatureSetItemSpec", ...]
     online_serving_enabled: bool
 
-    # Temporarily expose the underlying data proto during migration.
-    # TODO(TEC-12443): Remove this attribute.
-    data_proto: Optional[feature_service__data_pb2.FeatureService] = attrs.field(
-        metadata={utils.LOCAL_REMOTE_DIVERGENCE_ALLOWED: True}
-    )
-
     @classmethod
     @typechecked
     def from_data_proto(cls, proto: feature_service__data_pb2.FeatureService) -> "FeatureServiceSpec":
         return cls(
             metadata=tecton_object_spec.TectonObjectMetadataSpec.from_data_proto(
                 proto.feature_service_id, proto.fco_metadata
             ),
             feature_set_items=tuple(FeatureSetItemSpec.from_data_proto(item) for item in proto.feature_set_items),
             online_serving_enabled=proto.online_serving_enabled,
-            data_proto=proto,
+            validation_args=validator_pb2.FcoValidationArgs(feature_service=proto.validation_args),
         )
 
     @classmethod
     @typechecked
     def from_args_proto(
         cls, proto: feature_service__args_pb2.FeatureServiceArgs, supplement: "FeatureServiceSpecArgsSupplement"
     ) -> "FeatureServiceSpec":
         return cls(
             metadata=tecton_object_spec.TectonObjectMetadataSpec.from_args_proto(proto.feature_service_id, proto.info),
             feature_set_items=tuple(
                 FeatureSetItemSpec.from_args_proto(fp, supplement) for fp in proto.feature_packages
             ),
             online_serving_enabled=proto.online_serving_enabled,
-            data_proto=None,
+            validation_args=None,
         )
 
 
 @attrs.define
 class FeatureServiceSpecArgsSupplement:
     """A data class used for supplementing args protos during FeatureServiceSpec construction.
 
@@ -70,23 +65,23 @@
 
 @utils.frozen_strict
 class FeatureSetItemSpec:
     feature_view_id: str
     namespace: str
     feature_columns: Tuple[str, ...]
     # Mapping from spine join key to the Feature View join keys.  Not a dict to account for multi-mapping.
-    join_key_mappings: Tuple["JoinKeyMappingSpec", ...]
+    join_key_mappings: Tuple["utils.JoinKeyMappingSpec", ...]
 
     @classmethod
     @typechecked
     def from_data_proto(cls, proto: feature_service__data_pb2.FeatureSetItem) -> "FeatureSetItemSpec":
         join_key_mappings = []
         for join_configuration_item in proto.join_configuration_items:
             join_key_mappings.append(
-                JoinKeyMappingSpec(
+                utils.JoinKeyMappingSpec(
                     spine_column_name=join_configuration_item.spine_column_name,
                     feature_view_column_name=join_configuration_item.package_column_name,
                 )
             )
 
         return cls(
             feature_view_id=id_helper.IdHelper.to_string(proto.feature_view_id),
@@ -106,22 +101,22 @@
         override_mapping = {
             column_pair.feature_column: column_pair.spine_column for column_pair in proto.override_join_keys
         }
         join_key_mappings = []
         for join_key in fv_spec.join_keys:
             if join_key in override_mapping:
                 join_key_mappings.append(
-                    JoinKeyMappingSpec(
+                    utils.JoinKeyMappingSpec(
                         spine_column_name=override_mapping[join_key],
                         feature_view_column_name=join_key,
                     )
                 )
             else:
                 join_key_mappings.append(
-                    JoinKeyMappingSpec(
+                    utils.JoinKeyMappingSpec(
                         spine_column_name=join_key,
                         feature_view_column_name=join_key,
                     )
                 )
 
         if len(proto.features) > 0:
             # Use a subset of the features from the feature view.
@@ -133,16 +128,10 @@
             feature_view_id=feature_view_id,
             namespace=utils.get_field_or_none(proto, "namespace"),
             feature_columns=features,
             join_key_mappings=tuple(join_key_mappings),
         )
 
 
-@utils.frozen_strict
-class JoinKeyMappingSpec:
-    spine_column_name: str
-    feature_view_column_name: str
-
-
 # Resolve forward type declarations.
 attrs.resolve_types(FeatureServiceSpec, locals(), globals())
 attrs.resolve_types(FeatureSetItemSpec, locals(), globals())
```

### Comparing `tecton-0.7.0b9/tecton_core/specs/feature_view_spec.py` & `tecton-0.7.0rc0/tecton_core/specs/feature_view_spec.py`

 * *Files 4% similar despite different names*

```diff
@@ -21,14 +21,16 @@
 from tecton_core.specs import utils
 from tecton_proto.args import feature_view_pb2 as feature_view__args_pb2
 from tecton_proto.args import pipeline_pb2
 from tecton_proto.common import data_source_type_pb2
 from tecton_proto.common import schema_pb2
 from tecton_proto.data import feature_store_pb2
 from tecton_proto.data import feature_view_pb2 as feature_view__data_pb2
+from tecton_proto.validation import validator_pb2
+
 
 __all__ = [
     "FeatureViewSpec",
     "MaterializedFeatureViewSpec",
     "OnDemandFeatureViewSpec",
     "FeatureTableSpec",
     "MaterializedFeatureViewType",
@@ -56,34 +58,42 @@
 
     # materialization_enabled is True if the feature view has online or online set to True, and the feature view is
     # applied to a live workspace.
     materialization_enabled: bool
     online: bool
     offline: bool
 
-    # Temporarily expose the underlying data proto during migration.
-    # TODO(TEC-12443): Remove this attribute.
-    data_proto: Optional[feature_view__data_pb2.FeatureView] = attrs.field(
-        metadata={utils.LOCAL_REMOTE_DIVERGENCE_ALLOWED: True}
-    )
-
     url: Optional[str] = attrs.field(metadata={utils.LOCAL_REMOTE_DIVERGENCE_ALLOWED: True})
 
     @feature_store_format_version.validator
     def check_valid_feature_store_format_version(self, _, value):
         if (
             value < feature_store_pb2.FeatureStoreFormatVersion.FEATURE_STORE_FORMAT_VERSION_DEFAULT
             or value > feature_store_pb2.FeatureStoreFormatVersion.FEATURE_STORE_FORMAT_VERSION_MAX
         ):
-            raise ValueError(f"Unsupported feature_store_format_version: {value}")
+            msg = f"Unsupported feature_store_format_version: {value}"
+            raise ValueError(msg)
 
     @property
     def features(self) -> List[str]:
+        """
+        Returns the output feature names of this feature view
+        """
         raise NotImplementedError
 
+    def join_key_schema(self) -> schema.Schema:
+        """
+        Return the join key schmea which is adjusted by online serving keys. Only ODFV needs to overwrite this.
+        """
+        spine_schema_dict = self.view_schema.to_dict()
+        # If online_serving_keys are specified, spine only needs to contain these keys instead of all join keys.
+        retrieval_keys = self.online_serving_keys if len(self.online_serving_keys) > 0 else self.join_keys
+        spine_schema_dict = {key: spine_schema_dict[key] for key in retrieval_keys}
+        return schema.Schema.from_dict(spine_schema_dict)
+
 
 @attrs.define
 class FeatureViewSpecArgsSupplement:
     """A data class used for supplementing args protos during FeatureViewSpec construction.
 
     This Python data class can be used to include data that is not included in args protos (e.g. schemas) into the
     FeatureViewSpec constructor.
@@ -159,19 +169,20 @@
             slide_interval_string = utils.get_field_or_none(proto.temporal_aggregate, "slide_interval_string")
         elif proto.HasField("temporal"):
             fv_type = MaterializedFeatureViewType.TEMPORAL
             is_continuous = proto.temporal.is_continuous
             data_source_type = utils.get_field_or_none(proto.temporal, "data_source_type")
             incremental_backfills = proto.temporal.incremental_backfills
             slide_interval = None
-            ttl = time_utils.proto_to_duration(proto.temporal.serving_ttl)
+            ttl = utils.get_non_default_duration_field_or_none(proto.temporal, "serving_ttl")
             aggregate_features = tuple()
             slide_interval_string = None
         else:
-            raise TypeError(f"Unexpected feature view type: {proto}")
+            msg = f"Unexpected feature view type: {proto}"
+            raise TypeError(msg)
 
         return cls(
             metadata=tecton_object_spec.TectonObjectMetadataSpec.from_data_proto(
                 proto.feature_view_id, proto.fco_metadata
             ),
             entity_ids=tuple(id_helper.IdHelper.to_string(id) for id in proto.entity_ids),
             join_keys=utils.get_tuple_from_repeated_field(proto.join_keys),
@@ -208,15 +219,15 @@
                 proto.enrichments.fp_materialization.materialized_data_location, "path"
             ),
             materialization_state_transitions=utils.get_tuple_from_repeated_field(
                 proto.materialization_state_transitions
             ),
             time_range_policy=utils.get_field_or_none(proto.materialization_params, "time_range_policy"),
             snowflake_view_name=utils.get_field_or_none(proto.snowflake_data, "snowflake_view_name"),
-            data_proto=proto,
+            validation_args=validator_pb2.FcoValidationArgs(feature_view=proto.validation_args),
             batch_cluster_config=utils.get_field_or_none(proto.materialization_params, "batch_materialization"),
             stream_cluster_config=utils.get_field_or_none(proto.materialization_params, "stream_materialization"),
             batch_trigger=utils.get_field_or_none(proto, "batch_trigger"),
             url=utils.get_field_or_none(proto, "web_url"),
         )
 
     @classmethod
@@ -293,15 +304,15 @@
             max_source_data_delay=_get_max_schedule_offset(proto.pipeline),
             aggregate_features=get_aggregate_features_from_feature_view_args(proto),
             slide_interval_string=slide_interval_string,
             materialized_data_path=None,
             time_range_policy=feature_view__data_pb2.MaterializationTimeRangePolicy.MATERIALIZATION_TIME_RANGE_POLICY_FILTER_TO_RANGE,
             materialization_state_transitions=tuple(),
             snowflake_view_name=None,
-            data_proto=None,
+            validation_args=None,
             batch_cluster_config=utils.get_field_or_none(proto.materialized_feature_view_args, "batch_compute"),
             stream_cluster_config=utils.get_field_or_none(proto.materialized_feature_view_args, "stream_compute"),
             batch_trigger=get_batch_trigger_from_feature_view_args(proto),
             url=None,
         )
 
     @property
@@ -336,15 +347,15 @@
             view_schema=_get_view_schema(proto.schemas),
             materialization_schema=_get_materialization_schema(proto.schemas),
             feature_store_format_version=proto.feature_store_format_version,
             materialization_enabled=False,
             online=False,
             offline=False,
             pipeline=utils.get_field_or_none(proto, "pipeline"),
-            data_proto=proto,
+            validation_args=validator_pb2.FcoValidationArgs(feature_view=proto.validation_args),
             url=utils.get_field_or_none(proto, "web_url"),
         )
 
     @classmethod
     @typechecked
     def from_args_proto(
         cls, proto: feature_view__args_pb2.FeatureViewArgs, supplement: FeatureViewSpecArgsSupplement
@@ -357,27 +368,43 @@
             view_schema=schema.Schema(supplement.view_schema),
             materialization_schema=schema.Schema(supplement.materialization_schema),
             feature_store_format_version=feature_store_pb2.FeatureStoreFormatVersion.FEATURE_STORE_FORMAT_VERSION_TIME_NANOSECONDS,
             materialization_enabled=False,
             online=False,
             offline=False,
             pipeline=utils.get_field_or_none(proto, "pipeline"),
-            data_proto=None,
+            validation_args=None,
             url=None,
         )
 
     @property
     def features(self) -> List[str]:
         return list(self.view_schema.column_names())
 
+    def join_key_schema(
+        self, dependent_fv_specs_and_jk_overrides: List[Tuple[FeatureViewSpec, List[utils.JoinKeyMappingSpec]]]
+    ) -> schema.Schema:
+        """Returns the combined join key schema from all input FVs which are adjusted by join key overridings."""
+        input_fv_schema = schema.Schema(schema_pb2.Schema())
+        for fv_spec, jk_overrides in dependent_fv_specs_and_jk_overrides:
+            fv_schema_dict = fv_spec.join_key_schema().to_dict()
+            for jk_override_spec in jk_overrides:
+                fv_schema_dict[jk_override_spec.spine_column_name] = fv_schema_dict[
+                    jk_override_spec.feature_view_column_name
+                ]
+                # Delete the original feature_view_column_name entry as the data type is assigned to the overriding key.
+                del fv_schema_dict[jk_override_spec.feature_view_column_name]
+            input_fv_schema += schema.Schema.from_dict(fv_schema_dict)
+        return input_fv_schema
+
 
 @utils.frozen_strict
 class FeatureTableSpec(FeatureViewSpec):
     timestamp_field: str
-    ttl: pendulum.Duration
+    ttl: Optional[pendulum.Duration]
 
     offline_store: Optional[feature_view__args_pb2.OfflineFeatureStoreConfig]
     offline_store_params: Optional[feature_view__data_pb2.OfflineStoreParams]
     materialized_data_path: Optional[str]
     time_range_policy: Optional[feature_view__data_pb2.MaterializationTimeRangePolicy.ValueType]
     materialization_state_transitions: Tuple[feature_view__data_pb2.MaterializationStateTransition, ...] = attrs.field(
         metadata={utils.LOCAL_REMOTE_DIVERGENCE_ALLOWED: True}
@@ -401,23 +428,23 @@
             offline_store=utils.get_field_or_none(proto.materialization_params, "offline_store_config"),
             offline_store_params=utils.get_field_or_none(proto.materialization_params, "offline_store_params"),
             timestamp_field=utils.get_field_or_none(proto, "timestamp_key"),
             feature_store_format_version=proto.feature_store_format_version,
             materialization_enabled=proto.materialization_enabled,
             online=proto.feature_table.online_enabled,
             offline=proto.feature_table.offline_enabled,
-            ttl=utils.get_duration_field_or_none(proto.feature_table, "serving_ttl"),
+            ttl=utils.get_non_default_duration_field_or_none(proto.feature_table, "serving_ttl"),
             materialized_data_path=utils.get_field_or_none(
                 proto.enrichments.fp_materialization.materialized_data_location, "path"
             ),
             materialization_state_transitions=utils.get_tuple_from_repeated_field(
                 proto.materialization_state_transitions
             ),
             time_range_policy=utils.get_field_or_none(proto.materialization_params, "time_range_policy"),
-            data_proto=proto,
+            validation_args=validator_pb2.FcoValidationArgs(feature_view=proto.validation_args),
             batch_cluster_config=utils.get_field_or_none(proto.materialization_params, "batch_materialization"),
             url=utils.get_field_or_none(proto, "web_url"),
         )
 
     @classmethod
     @typechecked
     def from_args_proto(
@@ -437,15 +464,15 @@
             materialization_enabled=False,
             online=proto.online_enabled,
             offline=proto.offline_enabled,
             ttl=utils.get_duration_field_or_none(proto.feature_table_args, "serving_ttl"),
             materialized_data_path=None,
             materialization_state_transitions=tuple(),
             time_range_policy=None,
-            data_proto=None,
+            validation_args=None,
             batch_cluster_config=utils.get_field_or_none(proto.feature_table_args, "batch_compute"),
             url=None,
         )
 
     @property
     def features(self) -> List[str]:
         return [
@@ -498,39 +525,40 @@
     if proto.HasField("temporal_aggregate") or proto.HasField("temporal"):
         return MaterializedFeatureViewSpec.from_data_proto(proto)
     elif proto.HasField("on_demand_feature_view"):
         return OnDemandFeatureViewSpec.from_data_proto(proto)
     elif proto.HasField("feature_table"):
         return FeatureTableSpec.from_data_proto(proto)
     else:
-        raise ValueError(f"Unexpect feature view type: {proto}")
+        msg = f"Unexpect feature view type: {proto}"
+        raise ValueError(msg)
 
 
 @typechecked
 def create_feature_view_spec_from_args_proto(
     proto: feature_view__args_pb2.FeatureViewArgs,
     supplement: FeatureViewSpecArgsSupplement,
 ) -> Optional[Union[MaterializedFeatureViewSpec, OnDemandFeatureViewSpec, FeatureTableSpec]]:
     if proto.HasField("materialized_feature_view_args"):
         return MaterializedFeatureViewSpec.from_args_proto(proto, supplement)
     elif proto.HasField("on_demand_args"):
         return OnDemandFeatureViewSpec.from_args_proto(proto, supplement)
     elif proto.HasField("feature_table_args"):
         return FeatureTableSpec.from_args_proto(proto, supplement)
     else:
-        raise ValueError(f"Unexpect feature view type: {proto}")
+        msg = f"Unexpect feature view type: {proto}"
+        raise ValueError(msg)
 
 
 def _get_timestamp_column(schema_proto: schema_pb2.Schema) -> str:
     schema_ = schema.Schema(schema_proto)
     timestamp_columns = [column[0] for column in schema_.column_name_and_data_types() if column[1] == TimestampType()]
     if len(timestamp_columns) != 1:
-        raise errors.TectonValidationError(
-            f"Attempted to infer timestamp. Expected exactly one timestamp column in schema {schema_}"
-        )
+        msg = f"Attempted to infer timestamp. Expected exactly one timestamp column in schema {schema_}"
+        raise errors.TectonValidationError(msg)
     return timestamp_columns[0]
 
 
 def _get_max_schedule_offset(pipeline: pipeline_pb2.Pipeline) -> pendulum.Duration:
     ds_nodes = pipeline_common.get_all_data_source_nodes(pipeline)
     assert len(ds_nodes) > 0
     return max([time_utils.proto_to_duration(ds_node.data_source_node.schedule_offset) for ds_node in ds_nodes])
```

### Comparing `tecton-0.7.0b9/tecton_core/specs/tecton_object_spec.py` & `tecton-0.7.0rc0/tecton_core/specs/tecton_object_spec.py`

 * *Files 14% similar despite different names*

```diff
@@ -7,14 +7,16 @@
 
 from tecton_core import id_helper
 from tecton_core.specs import utils
 from tecton_proto.args import basic_info_pb2
 from tecton_proto.common import framework_version_pb2
 from tecton_proto.common import id_pb2
 from tecton_proto.data import fco_metadata_pb2
+from tecton_proto.validation import validator_pb2
+
 
 __all__ = [
     "TectonObjectSpec",
 ]
 
 
 @utils.frozen_strict
@@ -25,14 +27,15 @@
     id: str
 
     description: Optional[str]
     tags: Dict[str, str]
     owner: Optional[str]
     created_at: Optional[pendulum.DateTime] = attrs.field(metadata={utils.LOCAL_REMOTE_DIVERGENCE_ALLOWED: True})
     workspace: Optional[str] = attrs.field(metadata={utils.LOCAL_REMOTE_DIVERGENCE_ALLOWED: True})
+    defined_in: Optional[str] = attrs.field(metadata={utils.LOCAL_REMOTE_DIVERGENCE_ALLOWED: True})
 
     framework_version: framework_version_pb2.FrameworkVersion.ValueType
 
     # True if this spec represents an object that was defined locally, as opposed to an "applied" object definition
     # retrieved from the backend.
     is_local_object: bool = attrs.field(metadata={utils.LOCAL_REMOTE_DIVERGENCE_ALLOWED: True})
 
@@ -47,14 +50,15 @@
             name=utils.get_field_or_none(fco_metadata, "name"),
             id=id_helper.IdHelper.to_string(id),
             description=utils.get_field_or_none(fco_metadata, "description"),
             tags=dict(fco_metadata.tags),
             owner=utils.get_field_or_none(fco_metadata, "owner"),
             created_at=utils.get_timestamp_field_or_none(fco_metadata, "created_at"),
             workspace=utils.get_field_or_none(fco_metadata, "workspace"),
+            defined_in=utils.get_field_or_none(fco_metadata, "source_filename"),
             framework_version=utils.get_field_or_none(fco_metadata, "framework_version"),
             is_local_object=False,
         )
 
     @classmethod
     @typechecked
     def from_args_proto(cls, id: id_pb2.Id, basic_info: basic_info_pb2.BasicInfo) -> "TectonObjectMetadataSpec":
@@ -62,14 +66,15 @@
             name=utils.get_field_or_none(basic_info, "name"),
             id=id_helper.IdHelper.to_string(id),
             description=utils.get_field_or_none(basic_info, "description"),
             tags=dict(basic_info.tags),
             owner=utils.get_field_or_none(basic_info, "owner"),
             created_at=None,
             workspace=None,
+            defined_in=None,
             framework_version=framework_version_pb2.FrameworkVersion.FWV5,
             is_local_object=True,
         )
 
 
 @utils.frozen_strict
 class TectonObjectSpec:
@@ -80,14 +85,21 @@
 
     See the RFC;
     https://www.notion.so/tecton/RFC-Unified-SDK-for-Notebook-Driven-Development-a377af9d320f46488ea238e51e2ce656
     """
 
     metadata: TectonObjectMetadataSpec
 
+    # validation_args are used during local FCO validation.
+    # They contain the FCO args and some derived data such as derived schemas.
+    # validation_args is only set on the TectonObjectSpec for specs created from data protos, i.e. remote specs.
+    validation_args: Optional[validator_pb2.FcoValidationArgs] = attrs.field(
+        metadata={utils.LOCAL_REMOTE_DIVERGENCE_ALLOWED: True}
+    )
+
     @property
     def name(self) -> str:
         """Convenience accessor."""
         return self.metadata.name
 
     @property
     def id(self) -> str:
```

### Comparing `tecton-0.7.0b9/tecton_core/specs/transformation_spec.py` & `tecton-0.7.0rc0/tecton_core/specs/transformation_spec.py`

 * *Files 10% similar despite different names*

```diff
@@ -5,29 +5,25 @@
 from typeguard import typechecked
 
 from tecton_core import function_deserialization
 from tecton_core.specs import tecton_object_spec
 from tecton_core.specs import utils
 from tecton_proto.args import transformation_pb2 as transformation__args_pb2
 from tecton_proto.data import transformation_pb2 as transformation__data_pb2
+from tecton_proto.validation import validator_pb2
+
 
 __all__ = ["TransformationSpec"]
 
 
 @utils.frozen_strict
 class TransformationSpec(tecton_object_spec.TectonObjectSpec):
     transformation_mode: transformation__args_pb2.TransformationMode.ValueType
     user_function: Callable = attrs.field(metadata={utils.LOCAL_REMOTE_DIVERGENCE_ALLOWED: True})
 
-    # Temporarily expose the underlying data proto during migration.
-    # TODO(TEC-12443): Remove this attribute.
-    data_proto: Optional[transformation__data_pb2.Transformation] = attrs.field(
-        metadata={utils.LOCAL_REMOTE_DIVERGENCE_ALLOWED: True}
-    )
-
     @classmethod
     @typechecked
     def from_data_proto(
         cls, proto: transformation__data_pb2.Transformation, deserialize_funcs_to_main: bool = False
     ) -> "TransformationSpec":
         user_function = None
         if proto.HasField("user_function"):
@@ -38,15 +34,15 @@
 
         return cls(
             metadata=tecton_object_spec.TectonObjectMetadataSpec.from_data_proto(
                 proto.transformation_id, proto.fco_metadata
             ),
             transformation_mode=proto.transformation_mode,
             user_function=user_function,
-            data_proto=proto,
+            validation_args=validator_pb2.FcoValidationArgs(transformation=proto.validation_args),
         )
 
     @classmethod
     @typechecked
     def from_args_proto(
         cls, proto: transformation__args_pb2.TransformationArgs, user_function: Optional[Callable]
     ) -> "TransformationSpec":
@@ -55,9 +51,9 @@
         if proto.HasField("user_function"):
             user_function = function_deserialization.from_proto(proto.user_function)
 
         return cls(
             metadata=tecton_object_spec.TectonObjectMetadataSpec.from_args_proto(proto.transformation_id, proto.info),
             transformation_mode=proto.transformation_mode,
             user_function=user_function,
-            data_proto=None,
+            validation_args=None,
         )
```

### Comparing `tecton-0.7.0b9/tecton_core/specs/utils.py` & `tecton-0.7.0rc0/tecton_core/specs/utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -10,14 +10,15 @@
 import typeguard
 from attrs import validators
 from google import protobuf
 from google.protobuf import duration_pb2
 
 from tecton_core import time_utils
 
+
 # An attrs metadata key use to indicate that a spec field is allowed and expected to diverge for the same object
 # definition between local and remote objects. This metadata used in testing and to codify fields where divergence is
 # permitted.
 LOCAL_REMOTE_DIVERGENCE_ALLOWED = "local_remote_divergence_allowed"
 
 
 @typeguard.typechecked
@@ -82,17 +83,16 @@
         attribute.type, str
     ), f"Found unresolved type annotation `{attribute.type}` for attribute `{attribute.name}`. Do not use forward-declared types with the frozen_strict decorator."
     try:
         typeguard.check_type(attribute.name, value, attribute.type)
     except TypeError as e:
         # Needed because generic types (e.g. `Tuple[str, ...]`) do not have a __name__ attribute.
         attribute_type = attribute.type.__name__ if hasattr(attribute.type, "__name__") else str(attribute.type)
-        raise TypeError(
-            f"{instance.__class__.__name__} initialized with invalid type for attribute `{attribute.name}`. Expected type: `{attribute_type}`. Provided type: `{type(value).__name__}`."
-        ) from e
+        msg = f"{instance.__class__.__name__} initialized with invalid type for attribute `{attribute.name}`. Expected type: `{attribute_type}`. Provided type: `{type(value).__name__}`."
+        raise TypeError(msg) from e
 
 
 def add_strict_type_validation(cls: type, fields: List[attrs.Attribute]) -> List[attrs.Attribute]:
     """An attrs field transformer that add type assertions to all fields."""
     new_fields = []
     for field in fields:
         if field.validator:
@@ -105,7 +105,13 @@
 
 def frozen_strict(cls):
     """A decorator used to define a frozen attrs class where all attribute type annotations are enforced at runtime."""
 
     # Use slots=False because the slots optimization causes issues with serialization (needed for pyspark UDFs run on
     # worker nodes) when combined with attrs inheritance, i.e. attrs classes inheriting from other attrs classes.
     return attrs.frozen(cls, field_transformer=add_strict_type_validation, slots=False)
+
+
+@frozen_strict
+class JoinKeyMappingSpec:
+    spine_column_name: str
+    feature_view_column_name: str
```

### Comparing `tecton-0.7.0b9/tecton_core/time_utils.py` & `tecton-0.7.0rc0/tecton_core/time_utils.py`

 * *Files 10% similar despite different names*

```diff
@@ -10,35 +10,43 @@
 from google.protobuf import timestamp_pb2
 from typeguard import typechecked
 
 from tecton_core.errors import TectonValidationError
 from tecton_proto.data import feature_store_pb2
 
 
+# 10,000 years which is less than but still close to the max duration proto amount, and a digestable number of years to
+# a user
+_MAX_PROTO_DURATION_SECONDS = 10_000 * 365 * 24 * 60 * 60
+
+
 def timedelta_to_duration(td: datetime.timedelta) -> pendulum.Duration:
     return pendulum.duration(days=td.days, seconds=td.seconds, microseconds=td.microseconds)
 
 
 def proto_to_duration(proto_duration: duration_pb2.Duration) -> pendulum.Duration:
     return timedelta_to_duration(proto_duration.ToTimedelta())
 
 
 def timedelta_to_proto(td: Optional[datetime.timedelta]) -> Optional[duration_pb2.Duration]:
     if td is None:
         return None
 
+    if td.total_seconds() > _MAX_PROTO_DURATION_SECONDS:
+        msg = "'Timedelta value must be lower than max value of 10000 years'"
+        raise TectonValidationError(msg)
+
     proto = duration_pb2.Duration()
     proto.FromTimedelta(td)
     return proto
 
 
 def datetime_to_proto(dt: Optional[datetime.datetime]) -> Optional[timestamp_pb2.Timestamp]:
     if dt is None:
         return None
-
     proto = timestamp_pb2.Timestamp()
     proto.FromDatetime(dt)
     return proto
 
 
 @typechecked
 def to_human_readable_str(duration: Union[duration_pb2.Duration, datetime.timedelta]) -> str:
@@ -49,15 +57,16 @@
     """
 
     if isinstance(duration, duration_pb2.Duration):
         duration = proto_to_duration(duration)
     elif isinstance(duration, datetime.timedelta):
         duration = timedelta_to_duration(duration)
     else:
-        raise TypeError(f"Invalid input duration type: {type(duration)}")
+        msg = f"Invalid input duration type: {type(duration)}"
+        raise TypeError(msg)
 
     duration_str_list = []
     if duration.days > 0:
         duration_str_list.append(f"{duration.days}d")
     if duration.hours > 0:
         duration_str_list.append(f"{duration.hours}h")
     if duration.minutes > 0:
@@ -66,22 +75,24 @@
         duration_str_list.append(f"{duration.remaining_seconds}s")
 
     return "0s" if len(duration_str_list) == 0 else "".join(duration_str_list)
 
 
 def assert_is_round_seconds(d: pendulum.Duration) -> pendulum.Duration:
     if d % pendulum.Duration(seconds=1):
-        raise ValueError(f"{d.in_words()} is not a round number of seconds")
+        msg = f"{d.in_words()} is not a round number of seconds"
+        raise ValueError(msg)
     return d
 
 
 def strict_pytimeparse(time_str: str) -> Union[int, float]:
     parsed = pytimeparse.parse(time_str)
     if parsed is None:
-        raise TectonValidationError(f'Could not parse time string "{time_str}"')
+        msg = f'Could not parse time string "{time_str}"'
+        raise TectonValidationError(msg)
     else:
         return parsed
 
 
 def nanos_to_seconds(nanos: int) -> float:
     """
     :param nanos: Nanoseconds
```

### Comparing `tecton-0.7.0b9/tecton_core/vendor/treelib/__init__.py` & `tecton-0.7.0rc0/tecton_core/vendor/treelib/__init__.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_core/vendor/treelib/exceptions.py` & `tecton-0.7.0rc0/tecton_core/vendor/treelib/exceptions.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_core/vendor/treelib/misc.py` & `tecton-0.7.0rc0/tecton_core/vendor/treelib/misc.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_core/vendor/treelib/node.py` & `tecton-0.7.0rc0/tecton_core/vendor/treelib/node.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_core/vendor/treelib/plugins.py` & `tecton-0.7.0rc0/tecton_core/vendor/treelib/plugins.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_core/vendor/treelib/tree.py` & `tecton-0.7.0rc0/tecton_core/vendor/treelib/tree.py`

 * *Files 0% similar despite different names*

```diff
@@ -881,15 +881,15 @@
         """
         if level is None:
             return len(self._nodes)
         else:
             try:
                 level = int(level)
                 return len([node for node in self.all_nodes_itr() if self.level(node.identifier) == level])
-            except:
+            except Exception:
                 raise TypeError(
                     "level should be an integer instead of '%s'" % type(level))
 
     def subtree(self, nid, identifier=None):
         """
         Return a shallow COPY of subtree with nid being the new root.
         If nid is None, return an empty tree.
```

### Comparing `tecton-0.7.0b9/tecton_core/vendor/vendor_treelib.py` & `tecton-0.7.0rc0/tecton_core/vendor/vendor_treelib.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/amplitude/amplitude_pb2.py` & `tecton-0.7.0rc0/tecton_proto/amplitude/amplitude_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/amplitude/client_logging_pb2.py` & `tecton-0.7.0rc0/tecton_proto/amplitude/client_logging_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/api/featureservice/feature_service_request_pb2.py` & `tecton-0.7.0rc0/tecton_proto/api/featureservice/feature_service_request_pb2.py`

 * *Files 12% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 _sym_db = _symbol_database.Default()
 
 
 from google.protobuf import struct_pb2 as google_dot_protobuf_dot_struct__pb2
 from protoc_gen_swagger.options import annotations_pb2 as protoc__gen__swagger_dot_options_dot_annotations__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n=tecton_proto/api/featureservice/feature_service_request.proto\x12 tecton_proto.api.feature_service\x1a\x1cgoogle/protobuf/struct.proto\x1a,protoc-gen-swagger/options/annotations.proto\"\xa5\x02\n\x0fMetadataOptions\x12#\n\rinclude_names\x18\x01 \x01(\x08R\x0cincludeNames\x12\x36\n\x17include_effective_times\x18\x02 \x01(\x08R\x15includeEffectiveTimes\x12\'\n\rinclude_types\x18\x03 \x01(\x08\x42\x02\x18\x01R\x0cincludeTypes\x12,\n\x12include_data_types\x18\x05 \x01(\x08R\x10includeDataTypes\x12(\n\x10include_slo_info\x18\x04 \x01(\x08R\x0eincludeSloInfo\x12\x34\n\x16include_serving_status\x18\x06 \x01(\x08R\x14includeServingStatus\"e\n\x12GetFeaturesRequest\x12O\n\x06params\x18\x01 \x01(\x0b\x32\x37.tecton_proto.api.feature_service.GetFeaturesParametersR\x06params\"o\n\x17GetFeaturesBatchRequest\x12T\n\x06params\x18\x01 \x01(\x0b\x32<.tecton_proto.api.feature_service.GetFeaturesBatchParametersR\x06params\"\xac\t\n\x15GetFeaturesParameters\x12.\n\x12\x66\x65\x61ture_service_id\x18\x01 \x01(\tH\x00R\x10\x66\x65\x61tureServiceId\x12\x32\n\x14\x66\x65\x61ture_service_name\x18\x02 \x01(\tH\x00R\x12\x66\x65\x61tureServiceName\x12.\n\x12\x66\x65\x61ture_package_id\x18\x0c \x01(\tH\x00R\x10\x66\x65\x61turePackageId\x12\x32\n\x14\x66\x65\x61ture_package_name\x18\r \x01(\tH\x00R\x12\x66\x65\x61turePackageName\x12(\n\x0f\x66\x65\x61ture_view_id\x18\x0f \x01(\tH\x00R\rfeatureViewId\x12,\n\x11\x66\x65\x61ture_view_name\x18\x10 \x01(\tH\x00R\x0f\x66\x65\x61tureViewName\x12%\n\x0eworkspace_name\x18\n \x01(\tR\rworkspaceName\x12i\n\x0cjoin_key_map\x18\x08 \x03(\x0b\x32G.tecton_proto.api.feature_service.GetFeaturesParameters.JoinKeyMapEntryR\njoinKeyMap\x12~\n\x13request_context_map\x18\t \x03(\x0b\x32N.tecton_proto.api.feature_service.GetFeaturesParameters.RequestContextMapEntryR\x11requestContextMap\x12\\\n\x10metadata_options\x18\x07 \x01(\x0b\x32\x31.tecton_proto.api.feature_service.MetadataOptionsR\x0fmetadataOptions\x12>\n\x15\x61llow_partial_results\x18\x0b \x01(\x08\x42\n\x92\x41\x07:\x05\x66\x61lseR\x13\x61llowPartialResults\x12$\n\risCallerBatch\x18\x11 \x01(\x08R\risCallerBatch\x1aU\n\x0fJoinKeyMapEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12,\n\x05value\x18\x02 \x01(\x0b\x32\x16.google.protobuf.ValueR\x05value:\x02\x38\x01\x1a\\\n\x16RequestContextMapEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12,\n\x05value\x18\x02 \x01(\x0b\x32\x16.google.protobuf.ValueR\x05value:\x02\x38\x01:\xbe\x01\x92\x41\xba\x01\n\x17*\x15GetFeaturesParameters:\x9e\x01{\"featureServiceName\": \"ctrPredictionService\", \"workspaceName\": \"prod\", \"joinKeyMap\": { \"adId\": \"5417\", \"userUuid\": \"6c423390-9a64-52c8-9bb3-bbb108c74198\" } }B\t\n\x07locatorJ\x04\x08\x03\x10\x04J\x04\x08\x04\x10\x05J\x04\x08\x05\x10\x06J\x04\x08\x06\x10\x07J\x04\x08\x0e\x10\x0f\"\x91\x06\n\x1aGetFeaturesBatchParameters\x12.\n\x12\x66\x65\x61ture_service_id\x18\x01 \x01(\tH\x00R\x10\x66\x65\x61tureServiceId\x12\x32\n\x14\x66\x65\x61ture_service_name\x18\x02 \x01(\tH\x00R\x12\x66\x65\x61tureServiceName\x12%\n\x0eworkspace_name\x18\x03 \x01(\tR\rworkspaceName\x12`\n\x0crequest_data\x18\x04 \x03(\x0b\x32=.tecton_proto.api.feature_service.GetFeaturesBatchRequestDataR\x0brequestData\x12\\\n\x10metadata_options\x18\x05 \x01(\x0b\x32\x31.tecton_proto.api.feature_service.MetadataOptionsR\x0fmetadataOptions:\x9c\x03\x92\x41\x98\x03\n\x1c*\x1aGetFeaturesBatchParameters:\xf7\x02{\"feature_service_name\": \"fraud_detection_feature_service\", \"request_data\": [{\"join_key_map\": { \"user_uuid\": \"6c423390-9a64-52c8-9bb3-bbb108c74198\" },\"request_context_map\": {\"amount\": 2000}},{\"join_key_map\": { \"user_uuid\": \"6c423390-9a64-52c8-9bb3-bbb108c74198\" },\"request_context_map\": {\"amount\": 500}}],\"workspace_name\": \"prod\",\"metadata_options\": {\"include_names\": true} }B\t\n\x07locator\"\xca\x03\n\x1bGetFeaturesBatchRequestData\x12o\n\x0cjoin_key_map\x18\x01 \x03(\x0b\x32M.tecton_proto.api.feature_service.GetFeaturesBatchRequestData.JoinKeyMapEntryR\njoinKeyMap\x12\x84\x01\n\x13request_context_map\x18\x02 \x03(\x0b\x32T.tecton_proto.api.feature_service.GetFeaturesBatchRequestData.RequestContextMapEntryR\x11requestContextMap\x1aU\n\x0fJoinKeyMapEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12,\n\x05value\x18\x02 \x01(\x0b\x32\x16.google.protobuf.ValueR\x05value:\x02\x38\x01\x1a\\\n\x16RequestContextMapEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12,\n\x05value\x18\x02 \x01(\x0b\x32\x16.google.protobuf.ValueR\x05value:\x02\x38\x01\x42\x36Z4github.com/tecton-ai/tecton_proto/api/featureservice')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n=tecton_proto/api/featureservice/feature_service_request.proto\x12 tecton_proto.api.feature_service\x1a\x1cgoogle/protobuf/struct.proto\x1a,protoc-gen-swagger/options/annotations.proto\"\xa5\x02\n\x0fMetadataOptions\x12#\n\rinclude_names\x18\x01 \x01(\x08R\x0cincludeNames\x12\x36\n\x17include_effective_times\x18\x02 \x01(\x08R\x15includeEffectiveTimes\x12\'\n\rinclude_types\x18\x03 \x01(\x08\x42\x02\x18\x01R\x0cincludeTypes\x12,\n\x12include_data_types\x18\x05 \x01(\x08R\x10includeDataTypes\x12(\n\x10include_slo_info\x18\x04 \x01(\x08R\x0eincludeSloInfo\x12\x34\n\x16include_serving_status\x18\x06 \x01(\x08R\x14includeServingStatus\"e\n\x12GetFeaturesRequest\x12O\n\x06params\x18\x01 \x01(\x0b\x32\x37.tecton_proto.api.feature_service.GetFeaturesParametersR\x06params\"o\n\x17GetFeaturesBatchRequest\x12T\n\x06params\x18\x01 \x01(\x0b\x32<.tecton_proto.api.feature_service.GetFeaturesBatchParametersR\x06params\"\xd7\n\n\x15GetFeaturesParameters\x12.\n\x12\x66\x65\x61ture_service_id\x18\x01 \x01(\tH\x00R\x10\x66\x65\x61tureServiceId\x12\x32\n\x14\x66\x65\x61ture_service_name\x18\x02 \x01(\tH\x00R\x12\x66\x65\x61tureServiceName\x12.\n\x12\x66\x65\x61ture_package_id\x18\x0c \x01(\tH\x00R\x10\x66\x65\x61turePackageId\x12\x32\n\x14\x66\x65\x61ture_package_name\x18\r \x01(\tH\x00R\x12\x66\x65\x61turePackageName\x12(\n\x0f\x66\x65\x61ture_view_id\x18\x0f \x01(\tH\x00R\rfeatureViewId\x12,\n\x11\x66\x65\x61ture_view_name\x18\x10 \x01(\tH\x00R\x0f\x66\x65\x61tureViewName\x12%\n\x0eworkspace_name\x18\n \x01(\tR\rworkspaceName\x12i\n\x0cjoin_key_map\x18\x08 \x03(\x0b\x32G.tecton_proto.api.feature_service.GetFeaturesParameters.JoinKeyMapEntryR\njoinKeyMap\x12~\n\x13request_context_map\x18\t \x03(\x0b\x32N.tecton_proto.api.feature_service.GetFeaturesParameters.RequestContextMapEntryR\x11requestContextMap\x12\\\n\x10metadata_options\x18\x07 \x01(\x0b\x32\x31.tecton_proto.api.feature_service.MetadataOptionsR\x0fmetadataOptions\x12>\n\x15\x61llow_partial_results\x18\x0b \x01(\x08\x42\n\x92\x41\x07:\x05\x66\x61lseR\x13\x61llowPartialResults\x12$\n\risCallerBatch\x18\x11 \x01(\x08R\risCallerBatch\x1aU\n\x0fJoinKeyMapEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12,\n\x05value\x18\x02 \x01(\x0b\x32\x16.google.protobuf.ValueR\x05value:\x02\x38\x01\x1a\\\n\x16RequestContextMapEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12,\n\x05value\x18\x02 \x01(\x0b\x32\x16.google.protobuf.ValueR\x05value:\x02\x38\x01:\xe9\x02\x92\x41\xe5\x02\n\x17*\x15GetFeaturesParameters:\xc9\x02{\"featureServiceName\": \"fraud_detection_feature_service\", \"workspaceName\": \"prod\", \"joinKeyMap\":{ \"user_id\": \"A123\", \"ad_id\":\"5417\"}, \"requestContextMap\":{ \"amount\": 500.00 }, \"metadataOptions\": { \"includeSloInfo\": true, \"includeEffectiveTimes\": true, \"includeNames\": true, \"includeDataTypes\": true,\"includeServingStatus\": true}}B\t\n\x07locatorJ\x04\x08\x03\x10\x04J\x04\x08\x04\x10\x05J\x04\x08\x05\x10\x06J\x04\x08\x06\x10\x07J\x04\x08\x0e\x10\x0f\"\xf2\x06\n\x1aGetFeaturesBatchParameters\x12.\n\x12\x66\x65\x61ture_service_id\x18\x01 \x01(\tH\x00R\x10\x66\x65\x61tureServiceId\x12\x32\n\x14\x66\x65\x61ture_service_name\x18\x02 \x01(\tH\x00R\x12\x66\x65\x61tureServiceName\x12%\n\x0eworkspace_name\x18\x03 \x01(\tR\rworkspaceName\x12`\n\x0crequest_data\x18\x04 \x03(\x0b\x32=.tecton_proto.api.feature_service.GetFeaturesBatchRequestDataR\x0brequestData\x12\\\n\x10metadata_options\x18\x05 \x01(\x0b\x32\x31.tecton_proto.api.feature_service.MetadataOptionsR\x0fmetadataOptions:\xfd\x03\x92\x41\xf9\x03\n\x1c*\x1aGetFeaturesBatchParameters:\xd8\x03{\"featureServiceName\": \"fraud_detection_feature_service\", \"requestData\": [{\"joinKeyMap\": { \"user_uuid\": \"6c423390-9a64-52c8-9bb3-bbb108c74198\" },\"requestContextMap\": {\"amount\": 2000}},{\"joinKeyMap\": { \"user_uuid\": \"6c423390-9a64-52c8-9bb3-bbb108c74198\" },\"requestContextMap\": {\"amount\": 500}}],\"workspaceName\": \"prod\", \"metadataOptions\": { \"includeSloInfo\": true, \"includeEffectiveTimes\": true, \"includeNames\": true, \"includeDataTypes\": true,\"includeServingStatus\": true}}B\t\n\x07locator\"\xca\x03\n\x1bGetFeaturesBatchRequestData\x12o\n\x0cjoin_key_map\x18\x01 \x03(\x0b\x32M.tecton_proto.api.feature_service.GetFeaturesBatchRequestData.JoinKeyMapEntryR\njoinKeyMap\x12\x84\x01\n\x13request_context_map\x18\x02 \x03(\x0b\x32T.tecton_proto.api.feature_service.GetFeaturesBatchRequestData.RequestContextMapEntryR\x11requestContextMap\x1aU\n\x0fJoinKeyMapEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12,\n\x05value\x18\x02 \x01(\x0b\x32\x16.google.protobuf.ValueR\x05value:\x02\x38\x01\x1a\\\n\x16RequestContextMapEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12,\n\x05value\x18\x02 \x01(\x0b\x32\x16.google.protobuf.ValueR\x05value:\x02\x38\x01\x42\x36Z4github.com/tecton-ai/tecton_proto/api/featureservice')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.api.featureservice.feature_service_request_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'Z4github.com/tecton-ai/tecton_proto/api/featureservice'
@@ -28,35 +28,35 @@
   _GETFEATURESPARAMETERS_JOINKEYMAPENTRY._options = None
   _GETFEATURESPARAMETERS_JOINKEYMAPENTRY._serialized_options = b'8\001'
   _GETFEATURESPARAMETERS_REQUESTCONTEXTMAPENTRY._options = None
   _GETFEATURESPARAMETERS_REQUESTCONTEXTMAPENTRY._serialized_options = b'8\001'
   _GETFEATURESPARAMETERS.fields_by_name['allow_partial_results']._options = None
   _GETFEATURESPARAMETERS.fields_by_name['allow_partial_results']._serialized_options = b'\222A\007:\005false'
   _GETFEATURESPARAMETERS._options = None
-  _GETFEATURESPARAMETERS._serialized_options = b'\222A\272\001\n\027*\025GetFeaturesParameters:\236\001{\"featureServiceName\": \"ctrPredictionService\", \"workspaceName\": \"prod\", \"joinKeyMap\": { \"adId\": \"5417\", \"userUuid\": \"6c423390-9a64-52c8-9bb3-bbb108c74198\" } }'
+  _GETFEATURESPARAMETERS._serialized_options = b'\222A\345\002\n\027*\025GetFeaturesParameters:\311\002{\"featureServiceName\": \"fraud_detection_feature_service\", \"workspaceName\": \"prod\", \"joinKeyMap\":{ \"user_id\": \"A123\", \"ad_id\":\"5417\"}, \"requestContextMap\":{ \"amount\": 500.00 }, \"metadataOptions\": { \"includeSloInfo\": true, \"includeEffectiveTimes\": true, \"includeNames\": true, \"includeDataTypes\": true,\"includeServingStatus\": true}}'
   _GETFEATURESBATCHPARAMETERS._options = None
-  _GETFEATURESBATCHPARAMETERS._serialized_options = b'\222A\230\003\n\034*\032GetFeaturesBatchParameters:\367\002{\"feature_service_name\": \"fraud_detection_feature_service\", \"request_data\": [{\"join_key_map\": { \"user_uuid\": \"6c423390-9a64-52c8-9bb3-bbb108c74198\" },\"request_context_map\": {\"amount\": 2000}},{\"join_key_map\": { \"user_uuid\": \"6c423390-9a64-52c8-9bb3-bbb108c74198\" },\"request_context_map\": {\"amount\": 500}}],\"workspace_name\": \"prod\",\"metadata_options\": {\"include_names\": true} }'
+  _GETFEATURESBATCHPARAMETERS._serialized_options = b'\222A\371\003\n\034*\032GetFeaturesBatchParameters:\330\003{\"featureServiceName\": \"fraud_detection_feature_service\", \"requestData\": [{\"joinKeyMap\": { \"user_uuid\": \"6c423390-9a64-52c8-9bb3-bbb108c74198\" },\"requestContextMap\": {\"amount\": 2000}},{\"joinKeyMap\": { \"user_uuid\": \"6c423390-9a64-52c8-9bb3-bbb108c74198\" },\"requestContextMap\": {\"amount\": 500}}],\"workspaceName\": \"prod\", \"metadataOptions\": { \"includeSloInfo\": true, \"includeEffectiveTimes\": true, \"includeNames\": true, \"includeDataTypes\": true,\"includeServingStatus\": true}}'
   _GETFEATURESBATCHREQUESTDATA_JOINKEYMAPENTRY._options = None
   _GETFEATURESBATCHREQUESTDATA_JOINKEYMAPENTRY._serialized_options = b'8\001'
   _GETFEATURESBATCHREQUESTDATA_REQUESTCONTEXTMAPENTRY._options = None
   _GETFEATURESBATCHREQUESTDATA_REQUESTCONTEXTMAPENTRY._serialized_options = b'8\001'
   _METADATAOPTIONS._serialized_start=176
   _METADATAOPTIONS._serialized_end=469
   _GETFEATURESREQUEST._serialized_start=471
   _GETFEATURESREQUEST._serialized_end=572
   _GETFEATURESBATCHREQUEST._serialized_start=574
   _GETFEATURESBATCHREQUEST._serialized_end=685
   _GETFEATURESPARAMETERS._serialized_start=688
-  _GETFEATURESPARAMETERS._serialized_end=1884
+  _GETFEATURESPARAMETERS._serialized_end=2055
   _GETFEATURESPARAMETERS_JOINKEYMAPENTRY._serialized_start=1471
   _GETFEATURESPARAMETERS_JOINKEYMAPENTRY._serialized_end=1556
   _GETFEATURESPARAMETERS_REQUESTCONTEXTMAPENTRY._serialized_start=1558
   _GETFEATURESPARAMETERS_REQUESTCONTEXTMAPENTRY._serialized_end=1650
-  _GETFEATURESBATCHPARAMETERS._serialized_start=1887
-  _GETFEATURESBATCHPARAMETERS._serialized_end=2672
-  _GETFEATURESBATCHREQUESTDATA._serialized_start=2675
-  _GETFEATURESBATCHREQUESTDATA._serialized_end=3133
+  _GETFEATURESBATCHPARAMETERS._serialized_start=2058
+  _GETFEATURESBATCHPARAMETERS._serialized_end=2940
+  _GETFEATURESBATCHREQUESTDATA._serialized_start=2943
+  _GETFEATURESBATCHREQUESTDATA._serialized_end=3401
   _GETFEATURESBATCHREQUESTDATA_JOINKEYMAPENTRY._serialized_start=1471
   _GETFEATURESBATCHREQUESTDATA_JOINKEYMAPENTRY._serialized_end=1556
   _GETFEATURESBATCHREQUESTDATA_REQUESTCONTEXTMAPENTRY._serialized_start=1558
   _GETFEATURESBATCHREQUESTDATA_REQUESTCONTEXTMAPENTRY._serialized_end=1650
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/args/basic_info_pb2.py` & `tecton-0.7.0rc0/tecton_proto/args/basic_info_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/args/data_source_config_pb2.py` & `tecton-0.7.0rc0/tecton_proto/args/data_source_config_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/args/data_source_pb2.py` & `tecton-0.7.0rc0/tecton_proto/args/data_source_pb2.py`

 * *Files 2% similar despite different names*

```diff
@@ -15,15 +15,15 @@
 from tecton_proto.args import data_source_config_pb2 as tecton__proto_dot_args_dot_data__source__config__pb2
 from tecton_proto.args import diff_options_pb2 as tecton__proto_dot_args_dot_diff__options__pb2
 from tecton_proto.args import user_defined_function_pb2 as tecton__proto_dot_args_dot_user__defined__function__pb2
 from tecton_proto.common import spark_schema_pb2 as tecton__proto_dot_common_dot_spark__schema__pb2
 from tecton_proto.args import version_constraints_pb2 as tecton__proto_dot_args_dot_version__constraints__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n#tecton_proto/args/data_source.proto\x12\x11tecton_proto.args\x1a\x1egoogle/protobuf/duration.proto\x1a*tecton_proto/args/data_source_config.proto\x1a$tecton_proto/args/diff_options.proto\x1a-tecton_proto/args/user_defined_function.proto\x1a&tecton_proto/common/spark_schema.proto\x1a+tecton_proto/args/version_constraints.proto\"\xa0\x01\n\x1b\x44\x61tetimePartitionColumnArgs\x12\x1f\n\x0b\x63olumn_name\x18\x01 \x01(\tR\ncolumnName\x12\x1a\n\x08\x64\x61tepart\x18\x02 \x01(\tR\x08\x64\x61tepart\x12\x1f\n\x0bzero_padded\x18\x03 \x01(\x08R\nzeroPadded\x12#\n\rformat_string\x18\x04 \x01(\tR\x0c\x66ormatString\"\xcd\x01\n\x19\x42\x61tchDataSourceCommonArgs\x12\'\n\x0ftimestamp_field\x18\x01 \x01(\tR\x0etimestampField\x12M\n\x0epost_processor\x18\x02 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionR\rpostProcessor\x12\x38\n\ndata_delay\x18\x03 \x01(\x0b\x32\x19.google.protobuf.DurationR\tdataDelay\"\xae\x02\n\x1aStreamDataSourceCommonArgs\x12\'\n\x0ftimestamp_field\x18\x01 \x01(\tR\x0etimestampField\x12\\\n\x19watermark_delay_threshold\x18\x02 \x01(\x0b\x32\x19.google.protobuf.DurationB\x05\x92M\x02\x08\x08R\x17watermarkDelayThreshold\x12T\n\x0epost_processor\x18\x03 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionB\x05\x92M\x02\x08\x08R\rpostProcessor\x12\x33\n\x15\x64\x65\x64uplication_columns\x18\x04 \x03(\tR\x14\x64\x65\x64uplicationColumns\"\x98\x04\n\x12HiveDataSourceArgs\x12\x14\n\x05table\x18\x01 \x01(\tR\x05table\x12\x1a\n\x08\x64\x61tabase\x18\x02 \x01(\tR\x08\x64\x61tabase\x12\x39\n\x15\x64\x61te_partition_column\x18\x03 \x01(\tB\x05\x82}\x02\x10\x03R\x13\x64\x61tePartitionColumn\x12\x39\n\x15timestamp_column_name\x18\x04 \x01(\tB\x05\x82}\x02\x10\x03R\x13timestampColumnName\x12)\n\x10timestamp_format\x18\x05 \x01(\tR\x0ftimestampFormat\x12l\n\x1a\x64\x61tetime_partition_columns\x18\x07 \x03(\x0b\x32..tecton_proto.args.DatetimePartitionColumnArgsR\x18\x64\x61tetimePartitionColumns\x12_\n\x14raw_batch_translator\x18\t \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionB\x05\x82}\x02\x10\x03R\x12rawBatchTranslator\x12T\n\x0b\x63ommon_args\x18\n \x01(\x0b\x32,.tecton_proto.args.BatchDataSourceCommonArgsB\x05\x82}\x02\x08\x05R\ncommonArgsJ\x04\x08\x06\x10\x07J\x04\x08\x08\x10\t\"\x95\x04\n\x12\x46ileDataSourceArgs\x12\x10\n\x03uri\x18\x01 \x01(\tR\x03uri\x12\x1f\n\x0b\x66ile_format\x18\x02 \x01(\tR\nfileFormat\x12\x33\n\x16\x63onvert_to_glue_format\x18\x03 \x01(\x08R\x13\x63onvertToGlueFormat\x12_\n\x14raw_batch_translator\x18\x04 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionB\x05\x82}\x02\x10\x03R\x12rawBatchTranslator\x12$\n\nschema_uri\x18\x05 \x01(\tB\x05\x92M\x02\x08\x01R\tschemaUri\x12\x39\n\x15timestamp_column_name\x18\x06 \x01(\tB\x05\x82}\x02\x10\x03R\x13timestampColumnName\x12)\n\x10timestamp_format\x18\x07 \x01(\tR\x0ftimestampFormat\x12I\n\x0fschema_override\x18\t \x01(\x0b\x32 .tecton_proto.common.SparkSchemaR\x0eschemaOverride\x12Y\n\x0b\x63ommon_args\x18\n \x01(\x0b\x32,.tecton_proto.args.BatchDataSourceCommonArgsB\n\x82}\x02\x08\x05\x92M\x02\x10\x01R\ncommonArgsJ\x04\x08\x08\x10\t\"0\n\x06Option\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value\"\x94\x06\n\x15KinesisDataSourceArgs\x12\x1f\n\x0bstream_name\x18\x01 \x01(\tR\nstreamName\x12\x16\n\x06region\x18\x02 \x01(\tR\x06region\x12\x66\n\x15raw_stream_translator\x18\x03 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionB\n\x82}\x02\x10\x03\x92M\x02\x08\x08R\x13rawStreamTranslator\x12*\n\rtimestamp_key\x18\x04 \x01(\tB\x05\x82}\x02\x10\x03R\x0ctimestampKey\x12{\n\x1f\x64\x65\x66\x61ult_initial_stream_position\x18\x05 \x01(\x0e\x32(.tecton_proto.args.InitialStreamPositionB\n\x82}\x02\x10\x03\x92M\x02\x08\x08R\x1c\x64\x65\x66\x61ultInitialStreamPosition\x12l\n\x17initial_stream_position\x18\x0b \x01(\x0e\x32(.tecton_proto.args.InitialStreamPositionB\n\x82}\x02\x08\x05\x92M\x02\x08\x08R\x15initialStreamPosition\x12p\n!default_watermark_delay_threshold\x18\x06 \x01(\x0b\x32\x19.google.protobuf.DurationB\n\x82}\x02\x10\x03\x92M\x02\x08\x08R\x1e\x64\x65\x66\x61ultWatermarkDelayThreshold\x12:\n\x15\x64\x65\x64uplication_columns\x18\x07 \x03(\tB\x05\x82}\x02\x10\x03R\x14\x64\x65\x64uplicationColumns\x12\x33\n\x07options\x18\x08 \x03(\x0b\x32\x19.tecton_proto.args.OptionR\x07options\x12Z\n\x0b\x63ommon_args\x18\n \x01(\x0b\x32-.tecton_proto.args.StreamDataSourceCommonArgsB\n\x82}\x02\x08\x05\x92M\x02\x10\x01R\ncommonArgsJ\x04\x08\t\x10\n\"\xc8\x06\n\x13KafkaDataSourceArgs\x12\x36\n\x17kafka_bootstrap_servers\x18\x01 \x01(\tR\x15kafkaBootstrapServers\x12\x16\n\x06topics\x18\x02 \x01(\tR\x06topics\x12\x66\n\x15raw_stream_translator\x18\x03 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionB\n\x82}\x02\x10\x03\x92M\x02\x08\x08R\x13rawStreamTranslator\x12*\n\rtimestamp_key\x18\x04 \x01(\tB\x05\x82}\x02\x10\x03R\x0ctimestampKey\x12p\n!default_watermark_delay_threshold\x18\x05 \x01(\x0b\x32\x19.google.protobuf.DurationB\n\x82}\x02\x10\x03\x92M\x02\x08\x08R\x1e\x64\x65\x66\x61ultWatermarkDelayThreshold\x12\x33\n\x07options\x18\x06 \x03(\x0b\x32\x19.tecton_proto.args.OptionR\x07options\x12\x39\n\x15ssl_keystore_location\x18\x07 \x01(\tB\x05\x92M\x02\x08\x01R\x13sslKeystoreLocation\x12K\n\x1fssl_keystore_password_secret_id\x18\x08 \x01(\tB\x05\x92M\x02\x08\x01R\x1bsslKeystorePasswordSecretId\x12=\n\x17ssl_truststore_location\x18\t \x01(\tB\x05\x92M\x02\x08\x01R\x15sslTruststoreLocation\x12O\n!ssl_truststore_password_secret_id\x18\n \x01(\tB\x05\x92M\x02\x08\x01R\x1dsslTruststorePasswordSecretId\x12\x32\n\x11security_protocol\x18\x0b \x01(\tB\x05\x92M\x02\x08\x01R\x10securityProtocol\x12Z\n\x0b\x63ommon_args\x18\x0c \x01(\x0b\x32-.tecton_proto.args.StreamDataSourceCommonArgsB\n\x82}\x02\x08\x05\x92M\x02\x10\x01R\ncommonArgs\"\xe3\x02\n\x16RedshiftDataSourceArgs\x12\x1a\n\x08\x65ndpoint\x18\x01 \x01(\tR\x08\x65ndpoint\x12\x16\n\x05table\x18\x02 \x01(\tH\x00R\x05table\x12\x1d\n\x05query\x18\x05 \x01(\tB\x05\x92M\x02\x18\x03H\x00R\x05query\x12_\n\x14raw_batch_translator\x18\x03 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionB\x05\x82}\x02\x10\x03R\x12rawBatchTranslator\x12*\n\rtimestamp_key\x18\x06 \x01(\tB\x05\x82}\x02\x10\x03R\x0ctimestampKey\x12Y\n\x0b\x63ommon_args\x18\x07 \x01(\x0b\x32,.tecton_proto.args.BatchDataSourceCommonArgsB\n\x82}\x02\x08\x05\x92M\x02\x10\x01R\ncommonArgsB\x08\n\x06sourceJ\x04\x08\x04\x10\x05\"\xc8\x03\n\x17SnowflakeDataSourceArgs\x12\x10\n\x03url\x18\x01 \x01(\tR\x03url\x12\x19\n\x04role\x18\x02 \x01(\tB\x05\x92M\x02\x08\x01R\x04role\x12\x1a\n\x08\x64\x61tabase\x18\x03 \x01(\tR\x08\x64\x61tabase\x12\x16\n\x06schema\x18\x04 \x01(\tR\x06schema\x12#\n\twarehouse\x18\x05 \x01(\tB\x05\x92M\x02\x08\x01R\twarehouse\x12\x16\n\x05table\x18\x06 \x01(\tH\x00R\x05table\x12\x1d\n\x05query\x18\x07 \x01(\tB\x05\x92M\x02\x18\x03H\x00R\x05query\x12_\n\x14raw_batch_translator\x18\x08 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionB\x05\x82}\x02\x10\x03R\x12rawBatchTranslator\x12*\n\rtimestamp_key\x18\t \x01(\tB\x05\x82}\x02\x10\x03R\x0ctimestampKey\x12Y\n\x0b\x63ommon_args\x18\x0c \x01(\x0b\x32,.tecton_proto.args.BatchDataSourceCommonArgsB\n\x82}\x02\x08\x05\x92M\x02\x10\x01R\ncommonArgsB\x08\n\x06source\"\xe2\x01\n\x14SparkBatchConfigArgs\x12X\n\x14\x64\x61ta_source_function\x18\x01 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionR\x12\x64\x61taSourceFunction\x12\x38\n\ndata_delay\x18\x02 \x01(\x0b\x32\x19.google.protobuf.DurationR\tdataDelay\x12\x36\n\x17supports_time_filtering\x18\x03 \x01(\x08R\x15supportsTimeFiltering\"q\n\x15SparkStreamConfigArgs\x12X\n\x14\x64\x61ta_source_function\x18\x01 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionR\x12\x64\x61taSourceFunctionB\x13\n\x0f\x63om.tecton.argsP\x01')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n#tecton_proto/args/data_source.proto\x12\x11tecton_proto.args\x1a\x1egoogle/protobuf/duration.proto\x1a*tecton_proto/args/data_source_config.proto\x1a$tecton_proto/args/diff_options.proto\x1a-tecton_proto/args/user_defined_function.proto\x1a&tecton_proto/common/spark_schema.proto\x1a+tecton_proto/args/version_constraints.proto\"\xa0\x01\n\x1b\x44\x61tetimePartitionColumnArgs\x12\x1f\n\x0b\x63olumn_name\x18\x01 \x01(\tR\ncolumnName\x12\x1a\n\x08\x64\x61tepart\x18\x02 \x01(\tR\x08\x64\x61tepart\x12\x1f\n\x0bzero_padded\x18\x03 \x01(\x08R\nzeroPadded\x12#\n\rformat_string\x18\x04 \x01(\tR\x0c\x66ormatString\"\xcd\x01\n\x19\x42\x61tchDataSourceCommonArgs\x12\'\n\x0ftimestamp_field\x18\x01 \x01(\tR\x0etimestampField\x12M\n\x0epost_processor\x18\x02 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionR\rpostProcessor\x12\x38\n\ndata_delay\x18\x03 \x01(\x0b\x32\x19.google.protobuf.DurationR\tdataDelay\"\xae\x02\n\x1aStreamDataSourceCommonArgs\x12\'\n\x0ftimestamp_field\x18\x01 \x01(\tR\x0etimestampField\x12\\\n\x19watermark_delay_threshold\x18\x02 \x01(\x0b\x32\x19.google.protobuf.DurationB\x05\x92M\x02\x08\x08R\x17watermarkDelayThreshold\x12T\n\x0epost_processor\x18\x03 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionB\x05\x92M\x02\x08\x08R\rpostProcessor\x12\x33\n\x15\x64\x65\x64uplication_columns\x18\x04 \x03(\tR\x14\x64\x65\x64uplicationColumns\"\x98\x04\n\x12HiveDataSourceArgs\x12\x14\n\x05table\x18\x01 \x01(\tR\x05table\x12\x1a\n\x08\x64\x61tabase\x18\x02 \x01(\tR\x08\x64\x61tabase\x12\x39\n\x15\x64\x61te_partition_column\x18\x03 \x01(\tB\x05\x82}\x02\x10\x03R\x13\x64\x61tePartitionColumn\x12\x39\n\x15timestamp_column_name\x18\x04 \x01(\tB\x05\x82}\x02\x10\x03R\x13timestampColumnName\x12)\n\x10timestamp_format\x18\x05 \x01(\tR\x0ftimestampFormat\x12l\n\x1a\x64\x61tetime_partition_columns\x18\x07 \x03(\x0b\x32..tecton_proto.args.DatetimePartitionColumnArgsR\x18\x64\x61tetimePartitionColumns\x12_\n\x14raw_batch_translator\x18\t \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionB\x05\x82}\x02\x10\x03R\x12rawBatchTranslator\x12T\n\x0b\x63ommon_args\x18\n \x01(\x0b\x32,.tecton_proto.args.BatchDataSourceCommonArgsB\x05\x82}\x02\x08\x05R\ncommonArgsJ\x04\x08\x06\x10\x07J\x04\x08\x08\x10\t\"\xc5\x02\n\x13UnityDataSourceArgs\x12\x18\n\x07\x63\x61talog\x18\x01 \x01(\tR\x07\x63\x61talog\x12\x16\n\x06schema\x18\x02 \x01(\tR\x06schema\x12\x14\n\x05table\x18\x03 \x01(\tR\x05table\x12M\n\x0b\x63ommon_args\x18\x04 \x01(\x0b\x32,.tecton_proto.args.BatchDataSourceCommonArgsR\ncommonArgs\x12)\n\x10timestamp_format\x18\x05 \x01(\tR\x0ftimestampFormat\x12l\n\x1a\x64\x61tetime_partition_columns\x18\x06 \x03(\x0b\x32..tecton_proto.args.DatetimePartitionColumnArgsR\x18\x64\x61tetimePartitionColumns\"\x95\x04\n\x12\x46ileDataSourceArgs\x12\x10\n\x03uri\x18\x01 \x01(\tR\x03uri\x12\x1f\n\x0b\x66ile_format\x18\x02 \x01(\tR\nfileFormat\x12\x33\n\x16\x63onvert_to_glue_format\x18\x03 \x01(\x08R\x13\x63onvertToGlueFormat\x12_\n\x14raw_batch_translator\x18\x04 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionB\x05\x82}\x02\x10\x03R\x12rawBatchTranslator\x12$\n\nschema_uri\x18\x05 \x01(\tB\x05\x92M\x02\x08\x01R\tschemaUri\x12\x39\n\x15timestamp_column_name\x18\x06 \x01(\tB\x05\x82}\x02\x10\x03R\x13timestampColumnName\x12)\n\x10timestamp_format\x18\x07 \x01(\tR\x0ftimestampFormat\x12I\n\x0fschema_override\x18\t \x01(\x0b\x32 .tecton_proto.common.SparkSchemaR\x0eschemaOverride\x12Y\n\x0b\x63ommon_args\x18\n \x01(\x0b\x32,.tecton_proto.args.BatchDataSourceCommonArgsB\n\x82}\x02\x08\x05\x92M\x02\x10\x01R\ncommonArgsJ\x04\x08\x08\x10\t\"7\n\x06Option\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x1b\n\x05value\x18\x02 \x01(\tB\x05\x92M\x02 \x02R\x05value\"\x94\x06\n\x15KinesisDataSourceArgs\x12\x1f\n\x0bstream_name\x18\x01 \x01(\tR\nstreamName\x12\x16\n\x06region\x18\x02 \x01(\tR\x06region\x12\x66\n\x15raw_stream_translator\x18\x03 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionB\n\x82}\x02\x10\x03\x92M\x02\x08\x08R\x13rawStreamTranslator\x12*\n\rtimestamp_key\x18\x04 \x01(\tB\x05\x82}\x02\x10\x03R\x0ctimestampKey\x12{\n\x1f\x64\x65\x66\x61ult_initial_stream_position\x18\x05 \x01(\x0e\x32(.tecton_proto.args.InitialStreamPositionB\n\x82}\x02\x10\x03\x92M\x02\x08\x08R\x1c\x64\x65\x66\x61ultInitialStreamPosition\x12l\n\x17initial_stream_position\x18\x0b \x01(\x0e\x32(.tecton_proto.args.InitialStreamPositionB\n\x82}\x02\x08\x05\x92M\x02\x08\x08R\x15initialStreamPosition\x12p\n!default_watermark_delay_threshold\x18\x06 \x01(\x0b\x32\x19.google.protobuf.DurationB\n\x82}\x02\x10\x03\x92M\x02\x08\x08R\x1e\x64\x65\x66\x61ultWatermarkDelayThreshold\x12:\n\x15\x64\x65\x64uplication_columns\x18\x07 \x03(\tB\x05\x82}\x02\x10\x03R\x14\x64\x65\x64uplicationColumns\x12\x33\n\x07options\x18\x08 \x03(\x0b\x32\x19.tecton_proto.args.OptionR\x07options\x12Z\n\x0b\x63ommon_args\x18\n \x01(\x0b\x32-.tecton_proto.args.StreamDataSourceCommonArgsB\n\x82}\x02\x08\x05\x92M\x02\x10\x01R\ncommonArgsJ\x04\x08\t\x10\n\"\xc8\x06\n\x13KafkaDataSourceArgs\x12\x36\n\x17kafka_bootstrap_servers\x18\x01 \x01(\tR\x15kafkaBootstrapServers\x12\x16\n\x06topics\x18\x02 \x01(\tR\x06topics\x12\x66\n\x15raw_stream_translator\x18\x03 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionB\n\x82}\x02\x10\x03\x92M\x02\x08\x08R\x13rawStreamTranslator\x12*\n\rtimestamp_key\x18\x04 \x01(\tB\x05\x82}\x02\x10\x03R\x0ctimestampKey\x12p\n!default_watermark_delay_threshold\x18\x05 \x01(\x0b\x32\x19.google.protobuf.DurationB\n\x82}\x02\x10\x03\x92M\x02\x08\x08R\x1e\x64\x65\x66\x61ultWatermarkDelayThreshold\x12\x33\n\x07options\x18\x06 \x03(\x0b\x32\x19.tecton_proto.args.OptionR\x07options\x12\x39\n\x15ssl_keystore_location\x18\x07 \x01(\tB\x05\x92M\x02\x08\x01R\x13sslKeystoreLocation\x12K\n\x1fssl_keystore_password_secret_id\x18\x08 \x01(\tB\x05\x92M\x02\x08\x01R\x1bsslKeystorePasswordSecretId\x12=\n\x17ssl_truststore_location\x18\t \x01(\tB\x05\x92M\x02\x08\x01R\x15sslTruststoreLocation\x12O\n!ssl_truststore_password_secret_id\x18\n \x01(\tB\x05\x92M\x02\x08\x01R\x1dsslTruststorePasswordSecretId\x12\x32\n\x11security_protocol\x18\x0b \x01(\tB\x05\x92M\x02\x08\x01R\x10securityProtocol\x12Z\n\x0b\x63ommon_args\x18\x0c \x01(\x0b\x32-.tecton_proto.args.StreamDataSourceCommonArgsB\n\x82}\x02\x08\x05\x92M\x02\x10\x01R\ncommonArgs\"\xe3\x02\n\x16RedshiftDataSourceArgs\x12\x1a\n\x08\x65ndpoint\x18\x01 \x01(\tR\x08\x65ndpoint\x12\x16\n\x05table\x18\x02 \x01(\tH\x00R\x05table\x12\x1d\n\x05query\x18\x05 \x01(\tB\x05\x92M\x02\x18\x03H\x00R\x05query\x12_\n\x14raw_batch_translator\x18\x03 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionB\x05\x82}\x02\x10\x03R\x12rawBatchTranslator\x12*\n\rtimestamp_key\x18\x06 \x01(\tB\x05\x82}\x02\x10\x03R\x0ctimestampKey\x12Y\n\x0b\x63ommon_args\x18\x07 \x01(\x0b\x32,.tecton_proto.args.BatchDataSourceCommonArgsB\n\x82}\x02\x08\x05\x92M\x02\x10\x01R\ncommonArgsB\x08\n\x06sourceJ\x04\x08\x04\x10\x05\"\xc8\x03\n\x17SnowflakeDataSourceArgs\x12\x10\n\x03url\x18\x01 \x01(\tR\x03url\x12\x19\n\x04role\x18\x02 \x01(\tB\x05\x92M\x02\x08\x01R\x04role\x12\x1a\n\x08\x64\x61tabase\x18\x03 \x01(\tR\x08\x64\x61tabase\x12\x16\n\x06schema\x18\x04 \x01(\tR\x06schema\x12#\n\twarehouse\x18\x05 \x01(\tB\x05\x92M\x02\x08\x01R\twarehouse\x12\x16\n\x05table\x18\x06 \x01(\tH\x00R\x05table\x12\x1d\n\x05query\x18\x07 \x01(\tB\x05\x92M\x02\x18\x03H\x00R\x05query\x12_\n\x14raw_batch_translator\x18\x08 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionB\x05\x82}\x02\x10\x03R\x12rawBatchTranslator\x12*\n\rtimestamp_key\x18\t \x01(\tB\x05\x82}\x02\x10\x03R\x0ctimestampKey\x12Y\n\x0b\x63ommon_args\x18\x0c \x01(\x0b\x32,.tecton_proto.args.BatchDataSourceCommonArgsB\n\x82}\x02\x08\x05\x92M\x02\x10\x01R\ncommonArgsB\x08\n\x06source\"\xe2\x01\n\x14SparkBatchConfigArgs\x12X\n\x14\x64\x61ta_source_function\x18\x01 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionR\x12\x64\x61taSourceFunction\x12\x38\n\ndata_delay\x18\x02 \x01(\x0b\x32\x19.google.protobuf.DurationR\tdataDelay\x12\x36\n\x17supports_time_filtering\x18\x03 \x01(\x08R\x15supportsTimeFiltering\"q\n\x15SparkStreamConfigArgs\x12X\n\x14\x64\x61ta_source_function\x18\x01 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionR\x12\x64\x61taSourceFunctionB\x13\n\x0f\x63om.tecton.argsP\x01')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.args.data_source_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\017com.tecton.argsP\001'
@@ -43,14 +43,16 @@
   _FILEDATASOURCEARGS.fields_by_name['raw_batch_translator']._serialized_options = b'\202}\002\020\003'
   _FILEDATASOURCEARGS.fields_by_name['schema_uri']._options = None
   _FILEDATASOURCEARGS.fields_by_name['schema_uri']._serialized_options = b'\222M\002\010\001'
   _FILEDATASOURCEARGS.fields_by_name['timestamp_column_name']._options = None
   _FILEDATASOURCEARGS.fields_by_name['timestamp_column_name']._serialized_options = b'\202}\002\020\003'
   _FILEDATASOURCEARGS.fields_by_name['common_args']._options = None
   _FILEDATASOURCEARGS.fields_by_name['common_args']._serialized_options = b'\202}\002\010\005\222M\002\020\001'
+  _OPTION.fields_by_name['value']._options = None
+  _OPTION.fields_by_name['value']._serialized_options = b'\222M\002 \002'
   _KINESISDATASOURCEARGS.fields_by_name['raw_stream_translator']._options = None
   _KINESISDATASOURCEARGS.fields_by_name['raw_stream_translator']._serialized_options = b'\202}\002\020\003\222M\002\010\010'
   _KINESISDATASOURCEARGS.fields_by_name['timestamp_key']._options = None
   _KINESISDATASOURCEARGS.fields_by_name['timestamp_key']._serialized_options = b'\202}\002\020\003'
   _KINESISDATASOURCEARGS.fields_by_name['default_initial_stream_position']._options = None
   _KINESISDATASOURCEARGS.fields_by_name['default_initial_stream_position']._serialized_options = b'\202}\002\020\003\222M\002\010\010'
   _KINESISDATASOURCEARGS.fields_by_name['initial_stream_position']._options = None
@@ -103,24 +105,26 @@
   _DATETIMEPARTITIONCOLUMNARGS._serialized_end=465
   _BATCHDATASOURCECOMMONARGS._serialized_start=468
   _BATCHDATASOURCECOMMONARGS._serialized_end=673
   _STREAMDATASOURCECOMMONARGS._serialized_start=676
   _STREAMDATASOURCECOMMONARGS._serialized_end=978
   _HIVEDATASOURCEARGS._serialized_start=981
   _HIVEDATASOURCEARGS._serialized_end=1517
-  _FILEDATASOURCEARGS._serialized_start=1520
-  _FILEDATASOURCEARGS._serialized_end=2053
-  _OPTION._serialized_start=2055
-  _OPTION._serialized_end=2103
-  _KINESISDATASOURCEARGS._serialized_start=2106
-  _KINESISDATASOURCEARGS._serialized_end=2894
-  _KAFKADATASOURCEARGS._serialized_start=2897
-  _KAFKADATASOURCEARGS._serialized_end=3737
-  _REDSHIFTDATASOURCEARGS._serialized_start=3740
-  _REDSHIFTDATASOURCEARGS._serialized_end=4095
-  _SNOWFLAKEDATASOURCEARGS._serialized_start=4098
-  _SNOWFLAKEDATASOURCEARGS._serialized_end=4554
-  _SPARKBATCHCONFIGARGS._serialized_start=4557
-  _SPARKBATCHCONFIGARGS._serialized_end=4783
-  _SPARKSTREAMCONFIGARGS._serialized_start=4785
-  _SPARKSTREAMCONFIGARGS._serialized_end=4898
+  _UNITYDATASOURCEARGS._serialized_start=1520
+  _UNITYDATASOURCEARGS._serialized_end=1845
+  _FILEDATASOURCEARGS._serialized_start=1848
+  _FILEDATASOURCEARGS._serialized_end=2381
+  _OPTION._serialized_start=2383
+  _OPTION._serialized_end=2438
+  _KINESISDATASOURCEARGS._serialized_start=2441
+  _KINESISDATASOURCEARGS._serialized_end=3229
+  _KAFKADATASOURCEARGS._serialized_start=3232
+  _KAFKADATASOURCEARGS._serialized_end=4072
+  _REDSHIFTDATASOURCEARGS._serialized_start=4075
+  _REDSHIFTDATASOURCEARGS._serialized_end=4430
+  _SNOWFLAKEDATASOURCEARGS._serialized_start=4433
+  _SNOWFLAKEDATASOURCEARGS._serialized_end=4889
+  _SPARKBATCHCONFIGARGS._serialized_start=4892
+  _SPARKBATCHCONFIGARGS._serialized_end=5118
+  _SPARKSTREAMCONFIGARGS._serialized_start=5120
+  _SPARKSTREAMCONFIGARGS._serialized_end=5233
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/args/diff_options_pb2.py` & `tecton-0.7.0rc0/tecton_proto/args/diff_options_pb2.py`

 * *Files 19% similar despite different names*

```diff
@@ -10,23 +10,27 @@
 
 _sym_db = _symbol_database.Default()
 
 
 from google.protobuf import descriptor_pb2 as google_dot_protobuf_dot_descriptor__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n$tecton_proto/args/diff_options.proto\x12\x11tecton_proto.args\x1a google/protobuf/descriptor.proto\"\xb9\x01\n\x0b\x44iffOptions\x12\x39\n\x06update\x18\x01 \x01(\x0e\x32!.tecton_proto.args.UpdateStrategyR\x06update\x12\x1b\n\thide_path\x18\x02 \x01(\x08R\x08hidePath\x12R\n\x0erendering_type\x18\x03 \x01(\x0e\x32+.tecton_proto.args.FcoPropertyRenderingTypeR\rrenderingType*\xe9\x01\n\x0eUpdateStrategy\x12\x0c\n\x08RECREATE\x10\x00\x12\x0b\n\x07INPLACE\x10\x01\x12\x12\n\x0eINPLACE_ON_ADD\x10\x03\x12\x15\n\x11INPLACE_ON_REMOVE\x10\x04\x12\x0b\n\x07PASSIVE\x10\x05\x12\x1e\n\x1aRECREATE_UNLESS_SUPPRESSED\x10\x06\x12\x35\n1RECREATE_UNLESS_SUPPRESSED_INVALIDATE_CHECKPOINTS\x10\x07\x12-\n)RECREATE_UNLESS_SUPPRESSED_RESTART_STREAM\x10\x08*\x97\x02\n\x18\x46\x63oPropertyRenderingType\x12+\n\'FCO_PROPERTY_RENDERING_TYPE_UNSPECIFIED\x10\x00\x12*\n&FCO_PROPERTY_RENDERING_TYPE_PLAIN_TEXT\x10\x01\x12&\n\"FCO_PROPERTY_RENDERING_TYPE_PYTHON\x10\x02\x12#\n\x1f\x46\x43O_PROPERTY_RENDERING_TYPE_SQL\x10\x03\x12-\n)FCO_PROPERTY_RENDERING_TYPE_ONLY_DECLARED\x10\x04\x12&\n\"FCO_PROPERTY_RENDERING_TYPE_HIDDEN\x10\x05:a\n\x0c\x64iff_options\x12\x1d.google.protobuf.FieldOptions\x18\xd2\t \x01(\x0b\x32\x1e.tecton_proto.args.DiffOptionsR\x0b\x64iffOptionsB\x13\n\x0f\x63om.tecton.argsP\x01')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n$tecton_proto/args/diff_options.proto\x12\x11tecton_proto.args\x1a google/protobuf/descriptor.proto\"]\n\x11\x46ieldRenameConfig\x12\x1f\n\x0b\x66ormer_name\x18\x01 \x01(\tR\nformerName\x12\'\n\x0f\x63utover_version\x18\x02 \x01(\tR\x0e\x63utoverVersion\"\xc9\x02\n\x0b\x44iffOptions\x12\x39\n\x06update\x18\x01 \x01(\x0e\x32!.tecton_proto.args.UpdateStrategyR\x06update\x12\x1b\n\thide_path\x18\x02 \x01(\x08R\x08hidePath\x12R\n\x0erendering_type\x18\x03 \x01(\x0e\x32+.tecton_proto.args.FcoPropertyRenderingTypeR\rrenderingType\x12P\n\x11\x63ustom_comparator\x18\x04 \x01(\x0e\x32#.tecton_proto.args.CustomComparatorR\x10\x63ustomComparator\x12<\n\x06rename\x18\x05 \x01(\x0b\x32$.tecton_proto.args.FieldRenameConfigR\x06rename*\xe9\x01\n\x0eUpdateStrategy\x12\x0c\n\x08RECREATE\x10\x00\x12\x0b\n\x07INPLACE\x10\x01\x12\x12\n\x0eINPLACE_ON_ADD\x10\x03\x12\x15\n\x11INPLACE_ON_REMOVE\x10\x04\x12\x0b\n\x07PASSIVE\x10\x05\x12\x1e\n\x1aRECREATE_UNLESS_SUPPRESSED\x10\x06\x12\x35\n1RECREATE_UNLESS_SUPPRESSED_INVALIDATE_CHECKPOINTS\x10\x07\x12-\n)RECREATE_UNLESS_SUPPRESSED_RESTART_STREAM\x10\x08*\xc1\x02\n\x18\x46\x63oPropertyRenderingType\x12+\n\'FCO_PROPERTY_RENDERING_TYPE_UNSPECIFIED\x10\x00\x12*\n&FCO_PROPERTY_RENDERING_TYPE_PLAIN_TEXT\x10\x01\x12&\n\"FCO_PROPERTY_RENDERING_TYPE_PYTHON\x10\x02\x12#\n\x1f\x46\x43O_PROPERTY_RENDERING_TYPE_SQL\x10\x03\x12-\n)FCO_PROPERTY_RENDERING_TYPE_ONLY_DECLARED\x10\x04\x12&\n\"FCO_PROPERTY_RENDERING_TYPE_HIDDEN\x10\x05\x12(\n$FCO_PROPERTY_RENDERING_TYPE_REDACTED\x10\x06*\x84\x02\n\x10\x43ustomComparator\x12\x1b\n\x17\x43USTOM_COMPARATOR_UNSET\x10\x00\x12&\n\"CUSTOM_COMPARATOR_AGGREGATION_NAME\x10\x01\x12\x31\n-CUSTOM_COMPARATOR_OPTION_VALUE_WITH_REDACTION\x10\x02\x12$\n CUSTOM_COMPARATOR_DISPLAY_NOTSET\x10\x03\x12$\n CUSTOM_COMPARATOR_REQUEST_SOURCE\x10\x04\x12,\n(CUSTOM_COMPARATOR_STREAM_PROCESSING_MODE\x10\x06:a\n\x0c\x64iff_options\x12\x1d.google.protobuf.FieldOptions\x18\xd2\t \x01(\x0b\x32\x1e.tecton_proto.args.DiffOptionsR\x0b\x64iffOptionsB\x13\n\x0f\x63om.tecton.argsP\x01')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.args.diff_options_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
   google_dot_protobuf_dot_descriptor__pb2.FieldOptions.RegisterExtension(diff_options)
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\017com.tecton.argsP\001'
-  _UPDATESTRATEGY._serialized_start=282
-  _UPDATESTRATEGY._serialized_end=515
-  _FCOPROPERTYRENDERINGTYPE._serialized_start=518
-  _FCOPROPERTYRENDERINGTYPE._serialized_end=797
-  _DIFFOPTIONS._serialized_start=94
-  _DIFFOPTIONS._serialized_end=279
+  _UPDATESTRATEGY._serialized_start=521
+  _UPDATESTRATEGY._serialized_end=754
+  _FCOPROPERTYRENDERINGTYPE._serialized_start=757
+  _FCOPROPERTYRENDERINGTYPE._serialized_end=1078
+  _CUSTOMCOMPARATOR._serialized_start=1081
+  _CUSTOMCOMPARATOR._serialized_end=1341
+  _FIELDRENAMECONFIG._serialized_start=93
+  _FIELDRENAMECONFIG._serialized_end=186
+  _DIFFOPTIONS._serialized_start=189
+  _DIFFOPTIONS._serialized_end=518
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/args/diff_test_pb2.py` & `tecton-0.7.0rc0/tecton_proto/args/diff_test_pb2.py`

 * *Files 7% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 
 from tecton_proto.common import id_pb2 as tecton__proto_dot_common_dot_id__pb2
 from tecton_proto.args import basic_info_pb2 as tecton__proto_dot_args_dot_basic__info__pb2
 from tecton_proto.args import diff_options_pb2 as tecton__proto_dot_args_dot_diff__options__pb2
 from tecton_proto.args import data_source_pb2 as tecton__proto_dot_args_dot_data__source__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n!tecton_proto/args/diff_test.proto\x12\x11tecton_proto.args\x1a\x1ctecton_proto/common/id.proto\x1a\"tecton_proto/args/basic_info.proto\x1a$tecton_proto/args/diff_options.proto\x1a#tecton_proto/args/data_source.proto\"?\n\x0b\x44iffTestFoo\x12\x17\n\x07\x66ield_a\x18\x01 \x01(\tR\x06\x66ieldA\x12\x17\n\x07\x66ield_b\x18\x02 \x01(\tR\x06\x66ieldB\"\xe8\x07\n\x0c\x44iffTestArgs\x12\x39\n\x0ctest_args_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\ntestArgsId\x12\x30\n\x04info\x18\x02 \x01(\x0b\x32\x1c.tecton_proto.args.BasicInfoR\x04info\x12\x42\n\tnew_field\x18\x03 \x01(\x0b\x32\x1e.tecton_proto.args.DiffTestFooB\x05\x92M\x02\x08\x03R\x08newField\x12\x42\n\told_field\x18\x04 \x01(\x0b\x32\x1e.tecton_proto.args.DiffTestFooB\x05\x92M\x02\x08\x04R\x08oldField\x12*\n\rpassive_field\x18\x05 \x01(\tB\x05\x92M\x02\x08\x05R\x0cpassiveField\x12\x45\n\x1brecreate_suppressable_field\x18\x06 \x01(\tB\x05\x92M\x02\x08\x06R\x19recreateSuppressableField\x12z\n\"recreate_suppressable_nested_field\x18\x08 \x01(\x0b\x32&.tecton_proto.args.DiffTestNestedInnerB\x05\x92M\x02\x08\x06R\x1frecreateSuppressableNestedField\x12q\n2recreate_suppressable_invalidate_checkpoints_field\x18\t \x01(\tB\x05\x92M\x02\x08\x07R.recreateSuppressableInvalidateCheckpointsField\x12\xa6\x01\n9recreate_suppressable_invalidate_checkpoints_nested_field\x18\n \x01(\x0b\x32&.tecton_proto.args.DiffTestNestedInnerB\x05\x92M\x02\x08\x07R4recreateSuppressableInvalidateCheckpointsNestedField\x12\x61\n*recreate_suppressable_restart_stream_field\x18\x0b \x01(\tB\x05\x92M\x02\x08\x08R&recreateSuppressableRestartStreamField\x12@\n\x1cunannotated_needing_recreate\x18\x07 \x01(\tR\x1aunannotatedNeedingRecreate\x12\x33\n\x07options\x18\x0c \x03(\x0b\x32\x19.tecton_proto.args.OptionR\x07options\"\x98\x02\n\x13\x44iffTestNestedInner\x12*\n\rinplace_field\x18\x01 \x01(\tB\x05\x92M\x02\x08\x01R\x0cinplaceField\x12\x45\n\x1brecreate_suppressable_field\x18\x02 \x01(\tB\x05\x92M\x02\x08\x06R\x19recreateSuppressableField\x12\x61\n*recreate_suppressable_restart_stream_field\x18\x04 \x01(\tB\x05\x92M\x02\x08\x08R&recreateSuppressableRestartStreamField\x12+\n\x11unannotated_field\x18\x03 \x01(\tR\x10unannotatedField\"Q\n\x13\x44iffTestNestedOuter\x12:\n\x04\x61rgs\x18\x01 \x01(\x0b\x32\x1f.tecton_proto.args.DiffTestArgsB\x05\x92M\x02\x08\x01R\x04\x61rgsB\x13\n\x0f\x63om.tecton.argsP\x01')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n!tecton_proto/args/diff_test.proto\x12\x11tecton_proto.args\x1a\x1ctecton_proto/common/id.proto\x1a\"tecton_proto/args/basic_info.proto\x1a$tecton_proto/args/diff_options.proto\x1a#tecton_proto/args/data_source.proto\"?\n\x0b\x44iffTestFoo\x12\x17\n\x07\x66ield_a\x18\x01 \x01(\tR\x06\x66ieldA\x12\x17\n\x07\x66ield_b\x18\x02 \x01(\tR\x06\x66ieldB\"\xd1\t\n\x0c\x44iffTestArgs\x12\x39\n\x0ctest_args_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\ntestArgsId\x12\x30\n\x04info\x18\x02 \x01(\x0b\x32\x1c.tecton_proto.args.BasicInfoR\x04info\x12\x42\n\tnew_field\x18\x03 \x01(\x0b\x32\x1e.tecton_proto.args.DiffTestFooB\x05\x92M\x02\x08\x03R\x08newField\x12\x42\n\told_field\x18\x04 \x01(\x0b\x32\x1e.tecton_proto.args.DiffTestFooB\x05\x92M\x02\x08\x04R\x08oldField\x12*\n\rpassive_field\x18\x05 \x01(\tB\x05\x92M\x02\x08\x05R\x0cpassiveField\x12\x45\n\x1brecreate_suppressable_field\x18\x06 \x01(\tB\x05\x92M\x02\x08\x06R\x19recreateSuppressableField\x12z\n\"recreate_suppressable_nested_field\x18\x08 \x01(\x0b\x32&.tecton_proto.args.DiffTestNestedInnerB\x05\x92M\x02\x08\x06R\x1frecreateSuppressableNestedField\x12q\n2recreate_suppressable_invalidate_checkpoints_field\x18\t \x01(\tB\x05\x92M\x02\x08\x07R.recreateSuppressableInvalidateCheckpointsField\x12\xa6\x01\n9recreate_suppressable_invalidate_checkpoints_nested_field\x18\n \x01(\x0b\x32&.tecton_proto.args.DiffTestNestedInnerB\x05\x92M\x02\x08\x07R4recreateSuppressableInvalidateCheckpointsNestedField\x12\x61\n*recreate_suppressable_restart_stream_field\x18\x0b \x01(\tB\x05\x92M\x02\x08\x08R&recreateSuppressableRestartStreamField\x12@\n\x1cunannotated_needing_recreate\x18\x07 \x01(\tR\x1aunannotatedNeedingRecreate\x12\x33\n\x07options\x18\x0c \x03(\x0b\x32\x19.tecton_proto.args.OptionR\x07options\x12\x61\n\x17renamed_primitive_field\x18\r \x01(\tB)\x92M\x1a*\x18\n\x16\x66ormer_primitive_field\x92M\t*\x07\x12\x05\x30.6.0R\x15renamedPrimitiveField\x12\x83\x01\n\x15renamed_message_field\x18\x0e \x01(\x0b\x32&.tecton_proto.args.DiffTestNestedInnerB\'\x92M\x18*\x16\n\x14\x66ormer_message_field\x92M\t*\x07\x12\x05\x30.6.0R\x13renamedMessageField\"\x98\x02\n\x13\x44iffTestNestedInner\x12*\n\rinplace_field\x18\x01 \x01(\tB\x05\x92M\x02\x08\x01R\x0cinplaceField\x12\x45\n\x1brecreate_suppressable_field\x18\x02 \x01(\tB\x05\x92M\x02\x08\x06R\x19recreateSuppressableField\x12\x61\n*recreate_suppressable_restart_stream_field\x18\x04 \x01(\tB\x05\x92M\x02\x08\x08R&recreateSuppressableRestartStreamField\x12+\n\x11unannotated_field\x18\x03 \x01(\tR\x10unannotatedField\"Q\n\x13\x44iffTestNestedOuter\x12:\n\x04\x61rgs\x18\x01 \x01(\x0b\x32\x1f.tecton_proto.args.DiffTestArgsB\x05\x92M\x02\x08\x01R\x04\x61rgsB\x13\n\x0f\x63om.tecton.argsP\x01')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.args.diff_test_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\017com.tecton.argsP\001'
@@ -37,24 +37,28 @@
   _DIFFTESTARGS.fields_by_name['recreate_suppressable_nested_field']._serialized_options = b'\222M\002\010\006'
   _DIFFTESTARGS.fields_by_name['recreate_suppressable_invalidate_checkpoints_field']._options = None
   _DIFFTESTARGS.fields_by_name['recreate_suppressable_invalidate_checkpoints_field']._serialized_options = b'\222M\002\010\007'
   _DIFFTESTARGS.fields_by_name['recreate_suppressable_invalidate_checkpoints_nested_field']._options = None
   _DIFFTESTARGS.fields_by_name['recreate_suppressable_invalidate_checkpoints_nested_field']._serialized_options = b'\222M\002\010\007'
   _DIFFTESTARGS.fields_by_name['recreate_suppressable_restart_stream_field']._options = None
   _DIFFTESTARGS.fields_by_name['recreate_suppressable_restart_stream_field']._serialized_options = b'\222M\002\010\010'
+  _DIFFTESTARGS.fields_by_name['renamed_primitive_field']._options = None
+  _DIFFTESTARGS.fields_by_name['renamed_primitive_field']._serialized_options = b'\222M\032*\030\n\026former_primitive_field\222M\t*\007\022\0050.6.0'
+  _DIFFTESTARGS.fields_by_name['renamed_message_field']._options = None
+  _DIFFTESTARGS.fields_by_name['renamed_message_field']._serialized_options = b'\222M\030*\026\n\024former_message_field\222M\t*\007\022\0050.6.0'
   _DIFFTESTNESTEDINNER.fields_by_name['inplace_field']._options = None
   _DIFFTESTNESTEDINNER.fields_by_name['inplace_field']._serialized_options = b'\222M\002\010\001'
   _DIFFTESTNESTEDINNER.fields_by_name['recreate_suppressable_field']._options = None
   _DIFFTESTNESTEDINNER.fields_by_name['recreate_suppressable_field']._serialized_options = b'\222M\002\010\006'
   _DIFFTESTNESTEDINNER.fields_by_name['recreate_suppressable_restart_stream_field']._options = None
   _DIFFTESTNESTEDINNER.fields_by_name['recreate_suppressable_restart_stream_field']._serialized_options = b'\222M\002\010\010'
   _DIFFTESTNESTEDOUTER.fields_by_name['args']._options = None
   _DIFFTESTNESTEDOUTER.fields_by_name['args']._serialized_options = b'\222M\002\010\001'
   _DIFFTESTFOO._serialized_start=197
   _DIFFTESTFOO._serialized_end=260
   _DIFFTESTARGS._serialized_start=263
-  _DIFFTESTARGS._serialized_end=1263
-  _DIFFTESTNESTEDINNER._serialized_start=1266
-  _DIFFTESTNESTEDINNER._serialized_end=1546
-  _DIFFTESTNESTEDOUTER._serialized_start=1548
-  _DIFFTESTNESTEDOUTER._serialized_end=1629
+  _DIFFTESTARGS._serialized_end=1496
+  _DIFFTESTNESTEDINNER._serialized_start=1499
+  _DIFFTESTNESTEDINNER._serialized_end=1779
+  _DIFFTESTNESTEDOUTER._serialized_start=1781
+  _DIFFTESTNESTEDOUTER._serialized_end=1862
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/args/entity_pb2.py` & `tecton-0.7.0rc0/tecton_proto/args/entity_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/args/fco_args_pb2.py` & `tecton-0.7.0rc0/tecton_proto/args/fco_args_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/args/feature_service_pb2.py` & `tecton-0.7.0rc0/tecton_proto/args/feature_service_pb2.py`

 * *Files 3% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 
 from tecton_proto.common import framework_version_pb2 as tecton__proto_dot_common_dot_framework__version__pb2
 from tecton_proto.common import id_pb2 as tecton__proto_dot_common_dot_id__pb2
 from tecton_proto.args import basic_info_pb2 as tecton__proto_dot_args_dot_basic__info__pb2
 from tecton_proto.args import diff_options_pb2 as tecton__proto_dot_args_dot_diff__options__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\'tecton_proto/args/feature_service.proto\x12\x11tecton_proto.args\x1a+tecton_proto/common/framework_version.proto\x1a\x1ctecton_proto/common/id.proto\x1a\"tecton_proto/args/basic_info.proto\x1a$tecton_proto/args/diff_options.proto\"d\n\x11LoggingConfigArgs\x12\x1f\n\x0bsample_rate\x18\x01 \x01(\x02R\nsampleRate\x12.\n\x13log_effective_times\x18\x02 \x01(\x08R\x11logEffectiveTimes\"V\n\nColumnPair\x12!\n\x0cspine_column\x18\x01 \x01(\tR\x0bspineColumn\x12%\n\x0e\x66\x65\x61ture_column\x18\x02 \x01(\tR\rfeatureColumn\"\xec\x01\n\x1c\x46\x65\x61tureServiceFeaturePackage\x12\x45\n\x12\x66\x65\x61ture_package_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x10\x66\x65\x61turePackageId\x12K\n\x12override_join_keys\x18\x02 \x03(\x0b\x32\x1d.tecton_proto.args.ColumnPairR\x10overrideJoinKeys\x12\x1c\n\tnamespace\x18\x03 \x01(\tR\tnamespace\x12\x1a\n\x08\x66\x65\x61tures\x18\x04 \x03(\tR\x08\x66\x65\x61tures\"\xe5\x03\n\x12\x46\x65\x61tureServiceArgs\x12\x45\n\x12\x66\x65\x61ture_service_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x10\x66\x65\x61tureServiceId\x12\x37\n\x04info\x18\x02 \x01(\x0b\x32\x1c.tecton_proto.args.BasicInfoB\x05\x92M\x02\x10\x01R\x04info\x12\x46\n\x07version\x18\x06 \x01(\x0e\x32%.tecton_proto.common.FrameworkVersionB\x05\x92M\x02\x08\x05R\x07version\x12.\n\x0fprevent_destroy\x18\x07 \x01(\x08\x42\x05\x92M\x02\x08\x01R\x0epreventDestroy\x12Z\n\x10\x66\x65\x61ture_packages\x18\x03 \x03(\x0b\x32/.tecton_proto.args.FeatureServiceFeaturePackageR\x0f\x66\x65\x61turePackages\x12;\n\x16online_serving_enabled\x18\x04 \x01(\x08\x42\x05\x92M\x02\x08\x01R\x14onlineServingEnabled\x12>\n\x07logging\x18\x05 \x01(\x0b\x32$.tecton_proto.args.LoggingConfigArgsR\x07loggingB\x13\n\x0f\x63om.tecton.argsP\x01')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\'tecton_proto/args/feature_service.proto\x12\x11tecton_proto.args\x1a+tecton_proto/common/framework_version.proto\x1a\x1ctecton_proto/common/id.proto\x1a\"tecton_proto/args/basic_info.proto\x1a$tecton_proto/args/diff_options.proto\"d\n\x11LoggingConfigArgs\x12\x1f\n\x0bsample_rate\x18\x01 \x01(\x02R\nsampleRate\x12.\n\x13log_effective_times\x18\x02 \x01(\x08R\x11logEffectiveTimes\"V\n\nColumnPair\x12!\n\x0cspine_column\x18\x01 \x01(\tR\x0bspineColumn\x12%\n\x0e\x66\x65\x61ture_column\x18\x02 \x01(\tR\rfeatureColumn\"\xec\x01\n\x1c\x46\x65\x61tureServiceFeaturePackage\x12\x45\n\x12\x66\x65\x61ture_package_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x10\x66\x65\x61turePackageId\x12K\n\x12override_join_keys\x18\x02 \x03(\x0b\x32\x1d.tecton_proto.args.ColumnPairR\x10overrideJoinKeys\x12\x1c\n\tnamespace\x18\x03 \x01(\tR\tnamespace\x12\x1a\n\x08\x66\x65\x61tures\x18\x04 \x03(\tR\x08\x66\x65\x61tures\"\x9f\x04\n\x12\x46\x65\x61tureServiceArgs\x12\x45\n\x12\x66\x65\x61ture_service_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x10\x66\x65\x61tureServiceId\x12\x37\n\x04info\x18\x02 \x01(\x0b\x32\x1c.tecton_proto.args.BasicInfoB\x05\x92M\x02\x10\x01R\x04info\x12\x46\n\x07version\x18\x06 \x01(\x0e\x32%.tecton_proto.common.FrameworkVersionB\x05\x92M\x02\x08\x05R\x07version\x12.\n\x0fprevent_destroy\x18\x07 \x01(\x08\x42\x05\x92M\x02\x08\x01R\x0epreventDestroy\x12Z\n\x10\x66\x65\x61ture_packages\x18\x03 \x03(\x0b\x32/.tecton_proto.args.FeatureServiceFeaturePackageR\x0f\x66\x65\x61turePackages\x12;\n\x16online_serving_enabled\x18\x04 \x01(\x08\x42\x05\x92M\x02\x08\x01R\x14onlineServingEnabled\x12>\n\x07logging\x18\x05 \x01(\x0b\x32$.tecton_proto.args.LoggingConfigArgsR\x07logging\x12\x32\n\x15on_demand_environment\x18\t \x01(\tR\x13onDemandEnvironmentJ\x04\x08\x08\x10\tB\x13\n\x0f\x63om.tecton.argsP\x01')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.args.feature_service_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\017com.tecton.argsP\001'
@@ -36,9 +36,9 @@
   _LOGGINGCONFIGARGS._serialized_start=211
   _LOGGINGCONFIGARGS._serialized_end=311
   _COLUMNPAIR._serialized_start=313
   _COLUMNPAIR._serialized_end=399
   _FEATURESERVICEFEATUREPACKAGE._serialized_start=402
   _FEATURESERVICEFEATUREPACKAGE._serialized_end=638
   _FEATURESERVICEARGS._serialized_start=641
-  _FEATURESERVICEARGS._serialized_end=1126
+  _FEATURESERVICEARGS._serialized_end=1184
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/args/feature_view_pb2.py` & `tecton-0.7.0rc0/tecton_proto/args/feature_view_pb2.py`

 * *Files 3% similar despite different names*

```diff
@@ -23,15 +23,15 @@
 from tecton_proto.common import schema_pb2 as tecton__proto_dot_common_dot_schema__pb2
 from tecton_proto.common import spark_schema_pb2 as tecton__proto_dot_common_dot_spark__schema__pb2
 from tecton_proto.common import data_source_type_pb2 as tecton__proto_dot_common_dot_data__source__type__pb2
 from tecton_proto.args import version_constraints_pb2 as tecton__proto_dot_args_dot_version__constraints__pb2
 from tecton_proto.common import analytics_options_pb2 as tecton__proto_dot_common_dot_analytics__options__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n$tecton_proto/args/feature_view.proto\x12\x11tecton_proto.args\x1a\x1egoogle/protobuf/duration.proto\x1a\x1cgoogle/protobuf/struct.proto\x1a\x1fgoogle/protobuf/timestamp.proto\x1a\"tecton_proto/args/basic_info.proto\x1a tecton_proto/args/pipeline.proto\x1a#tecton_proto/args/data_source.proto\x1a$tecton_proto/args/diff_options.proto\x1a\x1ctecton_proto/common/id.proto\x1a+tecton_proto/common/framework_version.proto\x1a tecton_proto/common/schema.proto\x1a&tecton_proto/common/spark_schema.proto\x1a*tecton_proto/common/data_source_type.proto\x1a+tecton_proto/args/version_constraints.proto\x1a+tecton_proto/common/analytics_options.proto\"\xe7\n\n\x0f\x46\x65\x61tureViewArgs\x12?\n\x0f\x66\x65\x61ture_view_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\rfeatureViewId\x12N\n\x11\x66\x65\x61ture_view_type\x18\x02 \x01(\x0e\x32\".tecton_proto.args.FeatureViewTypeR\x0f\x66\x65\x61tureViewType\x12\x37\n\x04info\x18\x03 \x01(\x0b\x32\x1c.tecton_proto.args.BasicInfoB\x05\x92M\x02\x10\x01R\x04info\x12\x46\n\x07version\x18\x1d \x01(\x0e\x32%.tecton_proto.common.FrameworkVersionB\x05\x92M\x02\x08\x05R\x07version\x12.\n\x0fprevent_destroy\x18  \x01(\x08\x42\x05\x92M\x02\x08\x01R\x0epreventDestroy\x12G\n\x08\x65ntities\x18\x04 \x03(\x0b\x32$.tecton_proto.args.EntityKeyOverrideB\x05\x92M\x02\x08\x06R\x08\x65ntities\x12M\n\rtemporal_args\x18\x17 \x01(\x0b\x32\x1f.tecton_proto.args.TemporalArgsB\x05\x92M\x02\x10\x01H\x00R\x0ctemporalArgs\x12i\n\x17temporal_aggregate_args\x18\x18 \x01(\x0b\x32(.tecton_proto.args.TemporalAggregateArgsB\x05\x92M\x02\x10\x01H\x00R\x15temporalAggregateArgs\x12|\n\x1ematerialized_feature_view_args\x18\x1c \x01(\x0b\x32..tecton_proto.args.MaterializedFeatureViewArgsB\x05\x92M\x02\x10\x01H\x00R\x1bmaterializedFeatureViewArgs\x12N\n\x0eon_demand_args\x18\x19 \x01(\x0b\x32\x1f.tecton_proto.args.OnDemandArgsB\x05\x92M\x02\x10\x01H\x00R\x0conDemandArgs\x12Z\n\x12\x66\x65\x61ture_table_args\x18\x1a \x01(\x0b\x32#.tecton_proto.args.FeatureTableArgsB\x05\x92M\x02\x10\x01H\x00R\x10\x66\x65\x61tureTableArgs\x12\x30\n\x14online_serving_index\x18\x05 \x03(\tR\x12onlineServingIndex\x12,\n\x0eonline_enabled\x18\x0e \x01(\x08\x42\x05\x92M\x02\x08\x01R\ronlineEnabled\x12.\n\x0foffline_enabled\x18\x0f \x01(\x08\x42\x05\x92M\x02\x08\x01R\x0eofflineEnabled\x12>\n\x08pipeline\x18\x06 \x01(\x0b\x32\x1b.tecton_proto.args.PipelineB\x05\x92M\x02\x08\x06R\x08pipeline\x12N\n\x12\x66orced_view_schema\x18\x1e \x01(\x0b\x32 .tecton_proto.common.SparkSchemaR\x10\x66orcedViewSchema\x12^\n\x1a\x66orced_materialized_schema\x18\x1f \x01(\x0b\x32 .tecton_proto.common.SparkSchemaR\x18\x66orcedMaterializedSchemaB\x0b\n\ttype_argsJ\x04\x08\x07\x10\x08J\x04\x08\x08\x10\tJ\x04\x08\t\x10\nJ\x04\x08\n\x10\x0bJ\x04\x08\x0b\x10\x0cJ\x04\x08\x0c\x10\rJ\x04\x08\r\x10\x0eJ\x04\x08\x10\x10\x11J\x04\x08\x11\x10\x12J\x04\x08\x12\x10\x13J\x04\x08\x13\x10\x14J\x04\x08\x14\x10\x15J\x04\x08\x15\x10\x16J\x04\x08\x16\x10\x17J\x04\x08\x1b\x10\x1c\"f\n\x11\x45ntityKeyOverride\x12\x34\n\tentity_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x08\x65ntityId\x12\x1b\n\tjoin_keys\x18\x02 \x03(\tR\x08joinKeys\"K\n\x0e\x42\x61\x63kfillConfig\x12\x39\n\x04mode\x18\x01 \x01(\x0e\x32%.tecton_proto.args.BackfillConfigModeR\x04mode\"\xce\x01\n\x0cOutputStream\x12)\n\x10include_features\x18\x01 \x01(\x08R\x0fincludeFeatures\x12\x44\n\x07kinesis\x18\x02 \x01(\x0b\x32(.tecton_proto.args.KinesisDataSourceArgsH\x00R\x07kinesis\x12>\n\x05kafka\x18\x03 \x01(\x0b\x32&.tecton_proto.args.KafkaDataSourceArgsH\x00R\x05kafkaB\r\n\x0bstream_sink\"\xb9\x08\n\x0cTemporalArgs\x12#\n\rtimestamp_key\x18\x01 \x01(\tR\x0ctimestampKey\x12\x46\n\x11schedule_interval\x18\x02 \x01(\x0b\x32\x19.google.protobuf.DurationR\x10scheduleInterval\x12O\n\x12\x66\x65\x61ture_start_time\x18\x03 \x01(\x0b\x32\x1a.google.protobuf.TimestampB\x05\x92M\x02\x08\x01R\x10\x66\x65\x61tureStartTime\x12^\n\x1emax_batch_aggregation_interval\x18\x04 \x01(\x0b\x32\x19.google.protobuf.DurationR\x1bmaxBatchAggregationInterval\x12\x41\n\x0bserving_ttl\x18\x05 \x01(\x0b\x32\x19.google.protobuf.DurationB\x05\x92M\x02\x08\x06R\nservingTtl\x12S\n\x0eoffline_config\x18\x06 \x01(\x0b\x32,.tecton_proto.args.OfflineFeatureStoreConfigR\rofflineConfig\x12U\n\x15\x62\x61tch_materialization\x18\x07 \x01(\x0b\x32 .tecton_proto.args.ClusterConfigR\x14\x62\x61tchMaterialization\x12]\n\x19streaming_materialization\x18\x08 \x01(\x0b\x32 .tecton_proto.args.ClusterConfigR\x18streamingMaterialization\x12J\n\nmonitoring\x18\t \x01(\x0b\x32#.tecton_proto.args.MonitoringConfigB\x05\x92M\x02\x08\x01R\nmonitoring\x12M\n\x10\x64\x61ta_source_type\x18\n \x01(\x0e\x32#.tecton_proto.common.DataSourceTypeR\x0e\x64\x61taSourceType\x12Q\n\x0f\x62\x61\x63kfill_config\x18\x0b \x01(\x0b\x32!.tecton_proto.args.BackfillConfigB\x05\x92M\x02\x08\x03R\x0e\x62\x61\x63kfillConfig\x12T\n\x13online_store_config\x18\x0c \x01(\x0b\x32$.tecton_proto.args.OnlineStoreConfigR\x11onlineStoreConfig\x12\x33\n\x15incremental_backfills\x18\r \x01(\x08R\x14incrementalBackfills\x12\x44\n\routput_stream\x18\x0e \x01(\x0b\x32\x1f.tecton_proto.args.OutputStreamR\x0coutputStream\"\xe9\x08\n\x15TemporalAggregateArgs\x12k\n!aggregation_slide_period_duration\x18\x01 \x01(\x0b\x32\x19.google.protobuf.DurationB\x05\x92M\x02\x08\x05R\x1e\x61ggregationSlidePeriodDuration\x12\x38\n\x18\x61ggregation_slide_period\x18\x02 \x01(\tR\x16\x61ggregationSlidePeriod\x12I\n\x0c\x61ggregations\x18\x03 \x03(\x0b\x32%.tecton_proto.args.FeatureAggregationR\x0c\x61ggregations\x12#\n\rtimestamp_key\x18\x04 \x01(\tR\x0ctimestampKey\x12\x46\n\x11schedule_interval\x18\x05 \x01(\x0b\x32\x19.google.protobuf.DurationR\x10scheduleInterval\x12O\n\x12\x66\x65\x61ture_start_time\x18\x06 \x01(\x0b\x32\x1a.google.protobuf.TimestampB\x05\x92M\x02\x08\x01R\x10\x66\x65\x61tureStartTime\x12^\n\x1emax_batch_aggregation_interval\x18\x07 \x01(\x0b\x32\x19.google.protobuf.DurationR\x1bmaxBatchAggregationInterval\x12S\n\x0eoffline_config\x18\x08 \x01(\x0b\x32,.tecton_proto.args.OfflineFeatureStoreConfigR\rofflineConfig\x12U\n\x15\x62\x61tch_materialization\x18\t \x01(\x0b\x32 .tecton_proto.args.ClusterConfigR\x14\x62\x61tchMaterialization\x12]\n\x19streaming_materialization\x18\n \x01(\x0b\x32 .tecton_proto.args.ClusterConfigR\x18streamingMaterialization\x12J\n\nmonitoring\x18\x0b \x01(\x0b\x32#.tecton_proto.args.MonitoringConfigB\x05\x92M\x02\x08\x01R\nmonitoring\x12M\n\x10\x64\x61ta_source_type\x18\x0c \x01(\x0e\x32#.tecton_proto.common.DataSourceTypeR\x0e\x64\x61taSourceType\x12T\n\x13online_store_config\x18\r \x01(\x0b\x32$.tecton_proto.args.OnlineStoreConfigR\x11onlineStoreConfig\x12\x44\n\routput_stream\x18\x0e \x01(\x0b\x32\x1f.tecton_proto.args.OutputStreamR\x0coutputStream\"\xc1\n\n\x1bMaterializedFeatureViewArgs\x12\'\n\x0ftimestamp_field\x18\x01 \x01(\tR\x0etimestampField\x12@\n\x0e\x62\x61tch_schedule\x18\x02 \x01(\x0b\x32\x19.google.protobuf.DurationR\rbatchSchedule\x12O\n\x12\x66\x65\x61ture_start_time\x18\x03 \x01(\x0b\x32\x1a.google.protobuf.TimestampB\x05\x92M\x02\x08\x01R\x10\x66\x65\x61tureStartTime\x12^\n\x1emax_batch_aggregation_interval\x18\x04 \x01(\x0b\x32\x19.google.protobuf.DurationR\x1bmaxBatchAggregationInterval\x12\x41\n\x0bserving_ttl\x18\x05 \x01(\x0b\x32\x19.google.protobuf.DurationB\x05\x92M\x02\x08\x06R\nservingTtl\x12Q\n\roffline_store\x18\x06 \x01(\x0b\x32,.tecton_proto.args.OfflineFeatureStoreConfigR\x0cofflineStore\x12\x45\n\rbatch_compute\x18\x07 \x01(\x0b\x32 .tecton_proto.args.ClusterConfigR\x0c\x62\x61tchCompute\x12G\n\x0estream_compute\x18\x08 \x01(\x0b\x32 .tecton_proto.args.ClusterConfigR\rstreamCompute\x12O\n\nmonitoring\x18\t \x01(\x0b\x32#.tecton_proto.args.MonitoringConfigB\n\x92M\x02\x08\x01\x92M\x02\x10\x01R\nmonitoring\x12M\n\x10\x64\x61ta_source_type\x18\n \x01(\x0e\x32#.tecton_proto.common.DataSourceTypeR\x0e\x64\x61taSourceType\x12G\n\x0conline_store\x18\x0b \x01(\x0b\x32$.tecton_proto.args.OnlineStoreConfigR\x0bonlineStore\x12\x33\n\x15incremental_backfills\x18\x0c \x01(\x08R\x14incrementalBackfills\x12L\n\x14\x61ggregation_interval\x18\r \x01(\x0b\x32\x19.google.protobuf.DurationR\x13\x61ggregationInterval\x12]\n\x16stream_processing_mode\x18\x0f \x01(\x0e\x32\'.tecton_proto.args.StreamProcessingModeR\x14streamProcessingMode\x12I\n\x0c\x61ggregations\x18\x0e \x03(\x0b\x32%.tecton_proto.args.FeatureAggregationR\x0c\x61ggregations\x12\x44\n\routput_stream\x18\x10 \x01(\x0b\x32\x1f.tecton_proto.args.OutputStreamR\x0coutputStream\x12O\n\rbatch_trigger\x18\x11 \x01(\x0e\x32#.tecton_proto.args.BatchTriggerTypeB\x05\x92M\x02\x08\x01R\x0c\x62\x61tchTrigger\x12\x33\n\x06schema\x18\x12 \x01(\x0b\x32\x1b.tecton_proto.common.SchemaR\x06schema\"\x9d\x01\n\x0cOnDemandArgs\x12L\n\routput_schema\x18\x01 \x01(\x0b\x32 .tecton_proto.common.SparkSchemaB\x05\x82}\x02\x10\x03R\x0coutputSchema\x12?\n\x06schema\x18\x02 \x01(\x0b\x32 .tecton_proto.common.SparkSchemaB\x05\x82}\x02\x08\x05R\x06schema\"\xf3\x05\n\x10\x46\x65\x61tureTableArgs\x12L\n\routput_schema\x18\x01 \x01(\x0b\x32 .tecton_proto.common.SparkSchemaB\x05\x82}\x02\x10\x03R\x0coutputSchema\x12?\n\x06schema\x18\x06 \x01(\x0b\x32 .tecton_proto.common.SparkSchemaB\x05\x82}\x02\x08\x05R\x06schema\x12\x41\n\x0bserving_ttl\x18\x02 \x01(\x0b\x32\x19.google.protobuf.DurationB\x05\x92M\x02\x08\x06R\nservingTtl\x12Z\n\x0eoffline_config\x18\x03 \x01(\x0b\x32,.tecton_proto.args.OfflineFeatureStoreConfigB\x05\x82}\x02\x10\x03R\rofflineConfig\x12X\n\roffline_store\x18\x07 \x01(\x0b\x32,.tecton_proto.args.OfflineFeatureStoreConfigB\x05\x82}\x02\x08\x05R\x0cofflineStore\x12[\n\x13online_store_config\x18\x04 \x01(\x0b\x32$.tecton_proto.args.OnlineStoreConfigB\x05\x82}\x02\x10\x03R\x11onlineStoreConfig\x12N\n\x0conline_store\x18\x08 \x01(\x0b\x32$.tecton_proto.args.OnlineStoreConfigB\x05\x82}\x02\x08\x05R\x0bonlineStore\x12\\\n\x15\x62\x61tch_materialization\x18\x05 \x01(\x0b\x32 .tecton_proto.args.ClusterConfigB\x05\x82}\x02\x10\x03R\x14\x62\x61tchMaterialization\x12L\n\rbatch_compute\x18\t \x01(\x0b\x32 .tecton_proto.args.ClusterConfigB\x05\x82}\x02\x08\x05R\x0c\x62\x61tchCompute\"\xe7\x03\n\x12\x46\x65\x61tureAggregation\x12\x16\n\x06\x63olumn\x18\x01 \x01(\tR\x06\x63olumn\x12\x1a\n\x08\x66unction\x18\x02 \x01(\tR\x08\x66unction\x12\x62\n\x0f\x66unction_params\x18\x05 \x03(\x0b\x32\x39.tecton_proto.args.FeatureAggregation.FunctionParamsEntryR\x0e\x66unctionParams\x12\x43\n\x0ctime_windows\x18\x03 \x03(\x0b\x32\x19.google.protobuf.DurationB\x05\x82}\x02\x10\x03R\x0btimeWindows\x12\x34\n\x10time_window_strs\x18\x04 \x03(\tB\n\x82}\x02\x10\x03\x92M\x02\x08\x05R\x0etimeWindowStrs\x12\x41\n\x0btime_window\x18\x06 \x01(\x0b\x32\x19.google.protobuf.DurationB\x05\x82}\x02\x08\x05R\ntimeWindow\x12\x19\n\x04name\x18\x07 \x01(\tB\x05\x82}\x02\x08\x05R\x04name\x1a`\n\x13\x46unctionParamsEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x33\n\x05value\x18\x02 \x01(\x0b\x32\x1d.tecton_proto.args.ParamValueR\x05value:\x02\x38\x01\"8\n\nParamValue\x12!\n\x0bint64_value\x18\x01 \x01(\x03H\x00R\nint64ValueB\x07\n\x05value\"\x95\x04\n\rClusterConfig\x12\\\n\x10\x65xisting_cluster\x18\x01 \x01(\x0b\x32(.tecton_proto.args.ExistingClusterConfigB\x05\x92M\x02\x08\x01H\x00R\x0f\x65xistingCluster\x12S\n\x0enew_databricks\x18\x02 \x01(\x0b\x32#.tecton_proto.args.NewClusterConfigB\x05\x92M\x02\x08\x01H\x00R\rnewDatabricks\x12\x45\n\x07new_emr\x18\x03 \x01(\x0b\x32#.tecton_proto.args.NewClusterConfigB\x05\x92M\x02\x08\x01H\x00R\x06newEmr\x12^\n\x0fimplicit_config\x18\x04 \x01(\x0b\x32\'.tecton_proto.args.DefaultClusterConfigB\n\x92M\x02\x08\x01\x92M\x02\x18\x05H\x00R\x0eimplicitConfig\x12V\n\x0fjson_databricks\x18\x05 \x01(\x0b\x32$.tecton_proto.args.JsonClusterConfigB\x05\x92M\x02\x08\x01H\x00R\x0ejsonDatabricks\x12H\n\x08json_emr\x18\x06 \x01(\x0b\x32$.tecton_proto.args.JsonClusterConfigB\x05\x92M\x02\x08\x01H\x00R\x07jsonEmrB\x08\n\x06\x63onfig\"@\n\x11JsonClusterConfig\x12+\n\x04json\x18\x01 \x01(\x0b\x32\x17.google.protobuf.StructR\x04json\"G\n\x15\x45xistingClusterConfig\x12.\n\x13\x65xisting_cluster_id\x18\x01 \x01(\tR\x11\x65xistingClusterId\"\x9f\x03\n\x10NewClusterConfig\x12#\n\rinstance_type\x18\x01 \x01(\tR\x0cinstanceType\x12\x33\n\x15instance_availability\x18\x06 \x01(\tR\x14instanceAvailability\x12*\n\x11number_of_workers\x18\x02 \x01(\x05R\x0fnumberOfWorkers\x12\x32\n\x16root_volume_size_in_gb\x18\x03 \x01(\x05R\x12rootVolumeSizeInGb\x12\x34\n\x16\x65xtra_pip_dependencies\x18\x04 \x03(\tR\x14\x65xtraPipDependencies\x12\x41\n\x0cspark_config\x18\x05 \x01(\x0b\x32\x1e.tecton_proto.args.SparkConfigR\x0bsparkConfig\x12&\n\x0f\x66irst_on_demand\x18\x07 \x01(\x05R\rfirstOnDemand\x12\x30\n\x14pinned_spark_version\x18\x08 \x01(\tR\x12pinnedSparkVersion\"\x8a\x01\n\x14\x44\x65\x66\x61ultClusterConfig\x12?\n\x18\x64\x61tabricks_spark_version\x18\x01 \x01(\tB\x05\x92M\x02\x18\x05R\x16\x64\x61tabricksSparkVersion\x12\x31\n\x11\x65mr_spark_version\x18\x02 \x01(\tB\x05\x92M\x02\x18\x05R\x0f\x65mrSparkVersion\"\x83\x03\n\x0bSparkConfig\x12.\n\x13spark_driver_memory\x18\x01 \x01(\tR\x11sparkDriverMemory\x12\x32\n\x15spark_executor_memory\x18\x02 \x01(\tR\x13sparkExecutorMemory\x12?\n\x1cspark_driver_memory_overhead\x18\x03 \x01(\tR\x19sparkDriverMemoryOverhead\x12\x43\n\x1espark_executor_memory_overhead\x18\x04 \x01(\tR\x1bsparkExecutorMemoryOverhead\x12L\n\nspark_conf\x18\x05 \x03(\x0b\x32-.tecton_proto.args.SparkConfig.SparkConfEntryR\tsparkConf\x1a<\n\x0eSparkConfEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01\"\xa0\x01\n\x11OnlineStoreConfig\x12@\n\x06\x64ynamo\x18\x01 \x01(\x0b\x32&.tecton_proto.args.DynamoDbOnlineStoreH\x00R\x06\x64ynamo\x12;\n\x05redis\x18\x02 \x01(\x0b\x32#.tecton_proto.args.RedisOnlineStoreH\x00R\x05redisB\x0c\n\nstore_type\"\xa1\x02\n\x13\x44ynamoDbOnlineStore\x12\x33\n\x16\x63ross_account_role_arn\x18\x01 \x01(\tR\x13\x63rossAccountRoleArn\x12\x39\n\x19\x63ross_account_external_id\x18\x02 \x01(\tR\x16\x63rossAccountExternalId\x12L\n#cross_account_intermediate_role_arn\x18\x05 \x01(\tR\x1f\x63rossAccountIntermediateRoleArn\x12\x18\n\x07\x65nabled\x18\x03 \x01(\x08R\x07\x65nabled\x12\x32\n\x15\x64\x62\x66s_credentials_path\x18\x04 \x01(\tR\x13\x64\x62\x66sCredentialsPath\"\x91\x02\n\x10RedisOnlineStore\x12)\n\x10primary_endpoint\x18\x02 \x01(\tR\x0fprimaryEndpoint\x12\x31\n\x14\x61uthentication_token\x18\x04 \x01(\tR\x13\x61uthenticationToken\x12\x1f\n\x0btls_enabled\x18\x06 \x01(\x08R\ntlsEnabled\x12\x18\n\x07\x65nabled\x18\x05 \x01(\x08R\x07\x65nabled\x12\'\n\x0f\x63luster_enabled\x18\x07 \x01(\x08R\x0e\x63lusterEnabled\x12/\n\x13operations_endpoint\x18\x08 \x01(\tR\x12operationsEndpointJ\x04\x08\x01\x10\x02J\x04\x08\x03\x10\x04\"\xdb\x01\n\x19OfflineFeatureStoreConfig\x12<\n\x07parquet\x18\x01 \x01(\x0b\x32 .tecton_proto.args.ParquetConfigH\x00R\x07parquet\x12\x36\n\x05\x64\x65lta\x18\x02 \x01(\x0b\x32\x1e.tecton_proto.args.DeltaConfigH\x00R\x05\x64\x65lta\x12:\n\x15subdirectory_override\x18\x03 \x01(\tB\x05\x92M\x02\x08\x01R\x14subdirectoryOverrideB\x0c\n\nstore_type\"\x0f\n\rParquetConfig\"X\n\x0b\x44\x65ltaConfig\x12I\n\x13time_partition_size\x18\x01 \x01(\x0b\x32\x19.google.protobuf.DurationR\x11timePartitionSize\"\x99\x02\n\x10MonitoringConfig\x12+\n\x11monitor_freshness\x18\x01 \x01(\x08R\x10monitorFreshness\x12^\n\x1a\x65xpected_feature_freshness\x18\x02 \x01(\x0b\x32\x19.google.protobuf.DurationB\x05\x82}\x02\x10\x03R\x18\x65xpectedFeatureFreshness\x12O\n\x12\x65xpected_freshness\x18\x04 \x01(\x0b\x32\x19.google.protobuf.DurationB\x05\x82}\x02\x08\x05R\x11\x65xpectedFreshness\x12\'\n\x0b\x61lert_email\x18\x03 \x01(\tB\x06\xf2\xe2\x02\x02\x08\x01R\nalertEmail*\xe9\x01\n\x0f\x46\x65\x61tureViewType\x12\x1d\n\x19\x46\x45\x41TURE_VIEW_TYPE_UNKNOWN\x10\x00\x12\x1e\n\x1a\x46\x45\x41TURE_VIEW_TYPE_TEMPORAL\x10\x01\x12(\n$FEATURE_VIEW_TYPE_TEMPORAL_AGGREGATE\x10\x02\x12\x1f\n\x1b\x46\x45\x41TURE_VIEW_TYPE_ON_DEMAND\x10\x03\x12#\n\x1f\x46\x45\x41TURE_VIEW_TYPE_FEATURE_TABLE\x10\x04\x12\'\n#FEATURE_VIEW_TYPE_FWV5_FEATURE_VIEW\x10\x05*\xbb\x01\n\x12\x42\x61\x63kfillConfigMode\x12 \n\x1c\x42\x41\x43KFILL_CONFIG_MODE_UNKNOWN\x10\x00\x12?\n;BACKFILL_CONFIG_MODE_SINGLE_BATCH_SCHEDULE_INTERVAL_PER_JOB\x10\x01\x12\x42\n>BACKFILL_CONFIG_MODE_MULTIPLE_BATCH_SCHEDULE_INTERVALS_PER_JOB\x10\x02*\x8b\x01\n\x14StreamProcessingMode\x12\"\n\x1eSTREAM_PROCESSING_MODE_UNKNOWN\x10\x00\x12(\n$STREAM_PROCESSING_MODE_TIME_INTERVAL\x10\x01\x12%\n!STREAM_PROCESSING_MODE_CONTINUOUS\x10\x02*\xa4\x01\n\x10\x42\x61tchTriggerType\x12\x1e\n\x1a\x42\x41TCH_TRIGGER_TYPE_UNKNOWN\x10\x00\x12 \n\x1c\x42\x41TCH_TRIGGER_TYPE_SCHEDULED\x10\x01\x12\x1d\n\x19\x42\x41TCH_TRIGGER_TYPE_MANUAL\x10\x02\x12/\n+BATCH_TRIGGER_TYPE_NO_BATCH_MATERIALIZATION\x10\x03\x42\x13\n\x0f\x63om.tecton.argsP\x01')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n$tecton_proto/args/feature_view.proto\x12\x11tecton_proto.args\x1a\x1egoogle/protobuf/duration.proto\x1a\x1cgoogle/protobuf/struct.proto\x1a\x1fgoogle/protobuf/timestamp.proto\x1a\"tecton_proto/args/basic_info.proto\x1a tecton_proto/args/pipeline.proto\x1a#tecton_proto/args/data_source.proto\x1a$tecton_proto/args/diff_options.proto\x1a\x1ctecton_proto/common/id.proto\x1a+tecton_proto/common/framework_version.proto\x1a tecton_proto/common/schema.proto\x1a&tecton_proto/common/spark_schema.proto\x1a*tecton_proto/common/data_source_type.proto\x1a+tecton_proto/args/version_constraints.proto\x1a+tecton_proto/common/analytics_options.proto\"\xc9\x0b\n\x0f\x46\x65\x61tureViewArgs\x12?\n\x0f\x66\x65\x61ture_view_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\rfeatureViewId\x12N\n\x11\x66\x65\x61ture_view_type\x18\x02 \x01(\x0e\x32\".tecton_proto.args.FeatureViewTypeR\x0f\x66\x65\x61tureViewType\x12\x37\n\x04info\x18\x03 \x01(\x0b\x32\x1c.tecton_proto.args.BasicInfoB\x05\x92M\x02\x10\x01R\x04info\x12\x46\n\x07version\x18\x1d \x01(\x0e\x32%.tecton_proto.common.FrameworkVersionB\x05\x92M\x02\x08\x05R\x07version\x12.\n\x0fprevent_destroy\x18  \x01(\x08\x42\x05\x92M\x02\x08\x01R\x0epreventDestroy\x12G\n\x08\x65ntities\x18\x04 \x03(\x0b\x32$.tecton_proto.args.EntityKeyOverrideB\x05\x92M\x02\x08\x06R\x08\x65ntities\x12M\n\rtemporal_args\x18\x17 \x01(\x0b\x32\x1f.tecton_proto.args.TemporalArgsB\x05\x92M\x02\x10\x01H\x00R\x0ctemporalArgs\x12i\n\x17temporal_aggregate_args\x18\x18 \x01(\x0b\x32(.tecton_proto.args.TemporalAggregateArgsB\x05\x92M\x02\x10\x01H\x00R\x15temporalAggregateArgs\x12|\n\x1ematerialized_feature_view_args\x18\x1c \x01(\x0b\x32..tecton_proto.args.MaterializedFeatureViewArgsB\x05\x92M\x02\x10\x01H\x00R\x1bmaterializedFeatureViewArgs\x12N\n\x0eon_demand_args\x18\x19 \x01(\x0b\x32\x1f.tecton_proto.args.OnDemandArgsB\x05\x92M\x02\x10\x01H\x00R\x0conDemandArgs\x12Z\n\x12\x66\x65\x61ture_table_args\x18\x1a \x01(\x0b\x32#.tecton_proto.args.FeatureTableArgsB\x05\x92M\x02\x10\x01H\x00R\x10\x66\x65\x61tureTableArgs\x12\x30\n\x14online_serving_index\x18\x05 \x03(\tR\x12onlineServingIndex\x12,\n\x0eonline_enabled\x18\x0e \x01(\x08\x42\x05\x92M\x02\x08\x01R\ronlineEnabled\x12.\n\x0foffline_enabled\x18\x0f \x01(\x08\x42\x05\x92M\x02\x08\x01R\x0eofflineEnabled\x12>\n\x08pipeline\x18\x06 \x01(\x0b\x32\x1b.tecton_proto.args.PipelineB\x05\x92M\x02\x08\x06R\x08pipeline\x12`\n\x13\x64\x61ta_quality_config\x18! \x01(\x0b\x32$.tecton_proto.args.DataQualityConfigB\n\x92M\x02\x08\x01\x92M\x02\x10\x01R\x11\x64\x61taQualityConfig\x12N\n\x12\x66orced_view_schema\x18\x1e \x01(\x0b\x32 .tecton_proto.common.SparkSchemaR\x10\x66orcedViewSchema\x12^\n\x1a\x66orced_materialized_schema\x18\x1f \x01(\x0b\x32 .tecton_proto.common.SparkSchemaR\x18\x66orcedMaterializedSchemaB\x0b\n\ttype_argsJ\x04\x08\x07\x10\x08J\x04\x08\x08\x10\tJ\x04\x08\t\x10\nJ\x04\x08\n\x10\x0bJ\x04\x08\x0b\x10\x0cJ\x04\x08\x0c\x10\rJ\x04\x08\r\x10\x0eJ\x04\x08\x10\x10\x11J\x04\x08\x11\x10\x12J\x04\x08\x12\x10\x13J\x04\x08\x13\x10\x14J\x04\x08\x14\x10\x15J\x04\x08\x15\x10\x16J\x04\x08\x16\x10\x17J\x04\x08\x1b\x10\x1c\"f\n\x11\x45ntityKeyOverride\x12\x34\n\tentity_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x08\x65ntityId\x12\x1b\n\tjoin_keys\x18\x02 \x03(\tR\x08joinKeys\"K\n\x0e\x42\x61\x63kfillConfig\x12\x39\n\x04mode\x18\x01 \x01(\x0e\x32%.tecton_proto.args.BackfillConfigModeR\x04mode\"\xce\x01\n\x0cOutputStream\x12)\n\x10include_features\x18\x01 \x01(\x08R\x0fincludeFeatures\x12\x44\n\x07kinesis\x18\x02 \x01(\x0b\x32(.tecton_proto.args.KinesisDataSourceArgsH\x00R\x07kinesis\x12>\n\x05kafka\x18\x03 \x01(\x0b\x32&.tecton_proto.args.KafkaDataSourceArgsH\x00R\x05kafkaB\r\n\x0bstream_sink\"\xb9\x08\n\x0cTemporalArgs\x12#\n\rtimestamp_key\x18\x01 \x01(\tR\x0ctimestampKey\x12\x46\n\x11schedule_interval\x18\x02 \x01(\x0b\x32\x19.google.protobuf.DurationR\x10scheduleInterval\x12O\n\x12\x66\x65\x61ture_start_time\x18\x03 \x01(\x0b\x32\x1a.google.protobuf.TimestampB\x05\x92M\x02\x08\x01R\x10\x66\x65\x61tureStartTime\x12^\n\x1emax_batch_aggregation_interval\x18\x04 \x01(\x0b\x32\x19.google.protobuf.DurationR\x1bmaxBatchAggregationInterval\x12\x41\n\x0bserving_ttl\x18\x05 \x01(\x0b\x32\x19.google.protobuf.DurationB\x05\x92M\x02\x08\x06R\nservingTtl\x12S\n\x0eoffline_config\x18\x06 \x01(\x0b\x32,.tecton_proto.args.OfflineFeatureStoreConfigR\rofflineConfig\x12U\n\x15\x62\x61tch_materialization\x18\x07 \x01(\x0b\x32 .tecton_proto.args.ClusterConfigR\x14\x62\x61tchMaterialization\x12]\n\x19streaming_materialization\x18\x08 \x01(\x0b\x32 .tecton_proto.args.ClusterConfigR\x18streamingMaterialization\x12J\n\nmonitoring\x18\t \x01(\x0b\x32#.tecton_proto.args.MonitoringConfigB\x05\x92M\x02\x08\x01R\nmonitoring\x12M\n\x10\x64\x61ta_source_type\x18\n \x01(\x0e\x32#.tecton_proto.common.DataSourceTypeR\x0e\x64\x61taSourceType\x12Q\n\x0f\x62\x61\x63kfill_config\x18\x0b \x01(\x0b\x32!.tecton_proto.args.BackfillConfigB\x05\x92M\x02\x08\x03R\x0e\x62\x61\x63kfillConfig\x12T\n\x13online_store_config\x18\x0c \x01(\x0b\x32$.tecton_proto.args.OnlineStoreConfigR\x11onlineStoreConfig\x12\x33\n\x15incremental_backfills\x18\r \x01(\x08R\x14incrementalBackfills\x12\x44\n\routput_stream\x18\x0e \x01(\x0b\x32\x1f.tecton_proto.args.OutputStreamR\x0coutputStream\"\xe9\x08\n\x15TemporalAggregateArgs\x12k\n!aggregation_slide_period_duration\x18\x01 \x01(\x0b\x32\x19.google.protobuf.DurationB\x05\x92M\x02\x08\x05R\x1e\x61ggregationSlidePeriodDuration\x12\x38\n\x18\x61ggregation_slide_period\x18\x02 \x01(\tR\x16\x61ggregationSlidePeriod\x12I\n\x0c\x61ggregations\x18\x03 \x03(\x0b\x32%.tecton_proto.args.FeatureAggregationR\x0c\x61ggregations\x12#\n\rtimestamp_key\x18\x04 \x01(\tR\x0ctimestampKey\x12\x46\n\x11schedule_interval\x18\x05 \x01(\x0b\x32\x19.google.protobuf.DurationR\x10scheduleInterval\x12O\n\x12\x66\x65\x61ture_start_time\x18\x06 \x01(\x0b\x32\x1a.google.protobuf.TimestampB\x05\x92M\x02\x08\x01R\x10\x66\x65\x61tureStartTime\x12^\n\x1emax_batch_aggregation_interval\x18\x07 \x01(\x0b\x32\x19.google.protobuf.DurationR\x1bmaxBatchAggregationInterval\x12S\n\x0eoffline_config\x18\x08 \x01(\x0b\x32,.tecton_proto.args.OfflineFeatureStoreConfigR\rofflineConfig\x12U\n\x15\x62\x61tch_materialization\x18\t \x01(\x0b\x32 .tecton_proto.args.ClusterConfigR\x14\x62\x61tchMaterialization\x12]\n\x19streaming_materialization\x18\n \x01(\x0b\x32 .tecton_proto.args.ClusterConfigR\x18streamingMaterialization\x12J\n\nmonitoring\x18\x0b \x01(\x0b\x32#.tecton_proto.args.MonitoringConfigB\x05\x92M\x02\x08\x01R\nmonitoring\x12M\n\x10\x64\x61ta_source_type\x18\x0c \x01(\x0e\x32#.tecton_proto.common.DataSourceTypeR\x0e\x64\x61taSourceType\x12T\n\x13online_store_config\x18\r \x01(\x0b\x32$.tecton_proto.args.OnlineStoreConfigR\x11onlineStoreConfig\x12\x44\n\routput_stream\x18\x0e \x01(\x0b\x32\x1f.tecton_proto.args.OutputStreamR\x0coutputStream\"\xd6\x0b\n\x1bMaterializedFeatureViewArgs\x12\'\n\x0ftimestamp_field\x18\x01 \x01(\tR\x0etimestampField\x12@\n\x0e\x62\x61tch_schedule\x18\x02 \x01(\x0b\x32\x19.google.protobuf.DurationR\rbatchSchedule\x12O\n\x12\x66\x65\x61ture_start_time\x18\x03 \x01(\x0b\x32\x1a.google.protobuf.TimestampB\x05\x92M\x02\x08\x01R\x10\x66\x65\x61tureStartTime\x12i\n manual_trigger_backfill_end_time\x18\x13 \x01(\x0b\x32\x1a.google.protobuf.TimestampB\x05\x92M\x02\x08\x01R\x1cmanualTriggerBackfillEndTime\x12\x80\x01\n\x15max_backfill_interval\x18\x04 \x01(\x0b\x32\x19.google.protobuf.DurationB1\x92M\"* \n\x1emax_batch_aggregation_interval\x92M\t*\x07\x12\x05\x30.7.0R\x13maxBackfillInterval\x12\x41\n\x0bserving_ttl\x18\x05 \x01(\x0b\x32\x19.google.protobuf.DurationB\x05\x92M\x02\x08\x06R\nservingTtl\x12Q\n\roffline_store\x18\x06 \x01(\x0b\x32,.tecton_proto.args.OfflineFeatureStoreConfigR\x0cofflineStore\x12\x45\n\rbatch_compute\x18\x07 \x01(\x0b\x32 .tecton_proto.args.ClusterConfigR\x0c\x62\x61tchCompute\x12G\n\x0estream_compute\x18\x08 \x01(\x0b\x32 .tecton_proto.args.ClusterConfigR\rstreamCompute\x12O\n\nmonitoring\x18\t \x01(\x0b\x32#.tecton_proto.args.MonitoringConfigB\n\x92M\x02\x08\x01\x92M\x02\x10\x01R\nmonitoring\x12M\n\x10\x64\x61ta_source_type\x18\n \x01(\x0e\x32#.tecton_proto.common.DataSourceTypeR\x0e\x64\x61taSourceType\x12G\n\x0conline_store\x18\x0b \x01(\x0b\x32$.tecton_proto.args.OnlineStoreConfigR\x0bonlineStore\x12\x33\n\x15incremental_backfills\x18\x0c \x01(\x08R\x14incrementalBackfills\x12L\n\x14\x61ggregation_interval\x18\r \x01(\x0b\x32\x19.google.protobuf.DurationR\x13\x61ggregationInterval\x12\x64\n\x16stream_processing_mode\x18\x0f \x01(\x0e\x32\'.tecton_proto.args.StreamProcessingModeB\x05\x92M\x02 \x06R\x14streamProcessingMode\x12I\n\x0c\x61ggregations\x18\x0e \x03(\x0b\x32%.tecton_proto.args.FeatureAggregationR\x0c\x61ggregations\x12\x44\n\routput_stream\x18\x10 \x01(\x0b\x32\x1f.tecton_proto.args.OutputStreamR\x0coutputStream\x12O\n\rbatch_trigger\x18\x11 \x01(\x0e\x32#.tecton_proto.args.BatchTriggerTypeB\x05\x92M\x02\x08\x01R\x0c\x62\x61tchTrigger\x12\x33\n\x06schema\x18\x12 \x01(\x0b\x32\x1b.tecton_proto.common.SchemaR\x06schema\"\xd3\x01\n\x0cOnDemandArgs\x12L\n\routput_schema\x18\x01 \x01(\x0b\x32 .tecton_proto.common.SparkSchemaB\x05\x82}\x02\x10\x03R\x0coutputSchema\x12?\n\x06schema\x18\x02 \x01(\x0b\x32 .tecton_proto.common.SparkSchemaB\x05\x82}\x02\x08\x05R\x06schema\x12.\n\x0c\x65nvironments\x18\x04 \x03(\tB\n\x92M\x02\x08\x01\x82}\x02\x08\x05R\x0c\x65nvironmentsJ\x04\x08\x03\x10\x04\"\xf3\x05\n\x10\x46\x65\x61tureTableArgs\x12L\n\routput_schema\x18\x01 \x01(\x0b\x32 .tecton_proto.common.SparkSchemaB\x05\x82}\x02\x10\x03R\x0coutputSchema\x12?\n\x06schema\x18\x06 \x01(\x0b\x32 .tecton_proto.common.SparkSchemaB\x05\x82}\x02\x08\x05R\x06schema\x12\x41\n\x0bserving_ttl\x18\x02 \x01(\x0b\x32\x19.google.protobuf.DurationB\x05\x92M\x02\x08\x06R\nservingTtl\x12Z\n\x0eoffline_config\x18\x03 \x01(\x0b\x32,.tecton_proto.args.OfflineFeatureStoreConfigB\x05\x82}\x02\x10\x03R\rofflineConfig\x12X\n\roffline_store\x18\x07 \x01(\x0b\x32,.tecton_proto.args.OfflineFeatureStoreConfigB\x05\x82}\x02\x08\x05R\x0cofflineStore\x12[\n\x13online_store_config\x18\x04 \x01(\x0b\x32$.tecton_proto.args.OnlineStoreConfigB\x05\x82}\x02\x10\x03R\x11onlineStoreConfig\x12N\n\x0conline_store\x18\x08 \x01(\x0b\x32$.tecton_proto.args.OnlineStoreConfigB\x05\x82}\x02\x08\x05R\x0bonlineStore\x12\\\n\x15\x62\x61tch_materialization\x18\x05 \x01(\x0b\x32 .tecton_proto.args.ClusterConfigB\x05\x82}\x02\x10\x03R\x14\x62\x61tchMaterialization\x12L\n\rbatch_compute\x18\t \x01(\x0b\x32 .tecton_proto.args.ClusterConfigB\x05\x82}\x02\x08\x05R\x0c\x62\x61tchCompute\"\xec\x03\n\x12\x46\x65\x61tureAggregation\x12\x16\n\x06\x63olumn\x18\x01 \x01(\tR\x06\x63olumn\x12\x1a\n\x08\x66unction\x18\x02 \x01(\tR\x08\x66unction\x12\x62\n\x0f\x66unction_params\x18\x05 \x03(\x0b\x32\x39.tecton_proto.args.FeatureAggregation.FunctionParamsEntryR\x0e\x66unctionParams\x12\x43\n\x0ctime_windows\x18\x03 \x03(\x0b\x32\x19.google.protobuf.DurationB\x05\x82}\x02\x10\x03R\x0btimeWindows\x12\x34\n\x10time_window_strs\x18\x04 \x03(\tB\n\x82}\x02\x10\x03\x92M\x02\x08\x05R\x0etimeWindowStrs\x12\x41\n\x0btime_window\x18\x06 \x01(\x0b\x32\x19.google.protobuf.DurationB\x05\x82}\x02\x08\x05R\ntimeWindow\x12\x1e\n\x04name\x18\x07 \x01(\tB\n\x82}\x02\x08\x05\x92M\x02 \x01R\x04name\x1a`\n\x13\x46unctionParamsEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x33\n\x05value\x18\x02 \x01(\x0b\x32\x1d.tecton_proto.args.ParamValueR\x05value:\x02\x38\x01\"]\n\nParamValue\x12!\n\x0bint64_value\x18\x01 \x01(\x03H\x00R\nint64Value\x12#\n\x0c\x64ouble_value\x18\x02 \x01(\x01H\x00R\x0b\x64oubleValueB\x07\n\x05value\"\x8f\x01\n\x11\x44\x61taQualityConfig\x12\x37\n\x14\x64\x61ta_quality_enabled\x18\x01 \x01(\x08\x42\x05\x92M\x02 \x03R\x12\x64\x61taQualityEnabled\x12\x41\n\x19skip_default_expectations\x18\x02 \x01(\x08\x42\x05\x92M\x02 \x03R\x17skipDefaultExpectations\"\xe9\x04\n\rClusterConfig\x12\\\n\x10\x65xisting_cluster\x18\x01 \x01(\x0b\x32(.tecton_proto.args.ExistingClusterConfigB\x05\x92M\x02\x08\x01H\x00R\x0f\x65xistingCluster\x12S\n\x0enew_databricks\x18\x02 \x01(\x0b\x32#.tecton_proto.args.NewClusterConfigB\x05\x92M\x02\x08\x01H\x00R\rnewDatabricks\x12\x45\n\x07new_emr\x18\x03 \x01(\x0b\x32#.tecton_proto.args.NewClusterConfigB\x05\x92M\x02\x08\x01H\x00R\x06newEmr\x12^\n\x0fimplicit_config\x18\x04 \x01(\x0b\x32\'.tecton_proto.args.DefaultClusterConfigB\n\x92M\x02\x08\x01\x92M\x02\x18\x05H\x00R\x0eimplicitConfig\x12V\n\x0fjson_databricks\x18\x05 \x01(\x0b\x32$.tecton_proto.args.JsonClusterConfigB\x05\x92M\x02\x08\x01H\x00R\x0ejsonDatabricks\x12H\n\x08json_emr\x18\x06 \x01(\x0b\x32$.tecton_proto.args.JsonClusterConfigB\x05\x92M\x02\x08\x01H\x00R\x07jsonEmr\x12R\n\rjson_dataproc\x18\x07 \x01(\x0b\x32$.tecton_proto.args.JsonClusterConfigB\x05\x92M\x02\x08\x01H\x00R\x0cjsonDataprocB\x08\n\x06\x63onfig\"@\n\x11JsonClusterConfig\x12+\n\x04json\x18\x01 \x01(\x0b\x32\x17.google.protobuf.StructR\x04json\"G\n\x15\x45xistingClusterConfig\x12.\n\x13\x65xisting_cluster_id\x18\x01 \x01(\tR\x11\x65xistingClusterId\"\x9f\x03\n\x10NewClusterConfig\x12#\n\rinstance_type\x18\x01 \x01(\tR\x0cinstanceType\x12\x33\n\x15instance_availability\x18\x06 \x01(\tR\x14instanceAvailability\x12*\n\x11number_of_workers\x18\x02 \x01(\x05R\x0fnumberOfWorkers\x12\x32\n\x16root_volume_size_in_gb\x18\x03 \x01(\x05R\x12rootVolumeSizeInGb\x12\x34\n\x16\x65xtra_pip_dependencies\x18\x04 \x03(\tR\x14\x65xtraPipDependencies\x12\x41\n\x0cspark_config\x18\x05 \x01(\x0b\x32\x1e.tecton_proto.args.SparkConfigR\x0bsparkConfig\x12&\n\x0f\x66irst_on_demand\x18\x07 \x01(\x05R\rfirstOnDemand\x12\x30\n\x14pinned_spark_version\x18\x08 \x01(\tR\x12pinnedSparkVersion\"\x8a\x01\n\x14\x44\x65\x66\x61ultClusterConfig\x12?\n\x18\x64\x61tabricks_spark_version\x18\x01 \x01(\tB\x05\x92M\x02\x18\x05R\x16\x64\x61tabricksSparkVersion\x12\x31\n\x11\x65mr_spark_version\x18\x02 \x01(\tB\x05\x92M\x02\x18\x05R\x0f\x65mrSparkVersion\"\x83\x03\n\x0bSparkConfig\x12.\n\x13spark_driver_memory\x18\x01 \x01(\tR\x11sparkDriverMemory\x12\x32\n\x15spark_executor_memory\x18\x02 \x01(\tR\x13sparkExecutorMemory\x12?\n\x1cspark_driver_memory_overhead\x18\x03 \x01(\tR\x19sparkDriverMemoryOverhead\x12\x43\n\x1espark_executor_memory_overhead\x18\x04 \x01(\tR\x1bsparkExecutorMemoryOverhead\x12L\n\nspark_conf\x18\x05 \x03(\x0b\x32-.tecton_proto.args.SparkConfig.SparkConfEntryR\tsparkConf\x1a<\n\x0eSparkConfEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01\"\xe6\x01\n\x11OnlineStoreConfig\x12@\n\x06\x64ynamo\x18\x01 \x01(\x0b\x32&.tecton_proto.args.DynamoDbOnlineStoreH\x00R\x06\x64ynamo\x12;\n\x05redis\x18\x02 \x01(\x0b\x32#.tecton_proto.args.RedisOnlineStoreH\x00R\x05redis\x12\x44\n\x08\x62igtable\x18\x03 \x01(\x0b\x32&.tecton_proto.args.BigtableOnlineStoreH\x00R\x08\x62igtableB\x0c\n\nstore_type\"\xa1\x02\n\x13\x44ynamoDbOnlineStore\x12\x33\n\x16\x63ross_account_role_arn\x18\x01 \x01(\tR\x13\x63rossAccountRoleArn\x12\x39\n\x19\x63ross_account_external_id\x18\x02 \x01(\tR\x16\x63rossAccountExternalId\x12L\n#cross_account_intermediate_role_arn\x18\x05 \x01(\tR\x1f\x63rossAccountIntermediateRoleArn\x12\x18\n\x07\x65nabled\x18\x03 \x01(\x08R\x07\x65nabled\x12\x32\n\x15\x64\x62\x66s_credentials_path\x18\x04 \x01(\tR\x13\x64\x62\x66sCredentialsPath\"\xe9\x01\n\x10RedisOnlineStore\x12\x30\n\x10primary_endpoint\x18\x02 \x01(\tB\x05\x92M\x02\x08\x06R\x0fprimaryEndpoint\x12=\n\x14\x61uthentication_token\x18\x04 \x01(\tB\n\x92M\x02\x08\x01\x92M\x02\x18\x06R\x13\x61uthenticationToken\x12+\n\x0btls_enabled\x18\x06 \x01(\x08\x42\n\x92M\x02\x08\x01\x92M\x02\x18\x05R\ntlsEnabled\x12\x1f\n\x07\x65nabled\x18\x05 \x01(\x08\x42\x05\x92M\x02\x18\x05R\x07\x65nabledJ\x04\x08\x01\x10\x02J\x04\x08\x03\x10\x04J\x04\x08\x07\x10\x08J\x04\x08\x08\x10\t\"o\n\x13\x42igtableOnlineStore\x12\x18\n\x07\x65nabled\x18\x01 \x01(\x08R\x07\x65nabled\x12\x1d\n\nproject_id\x18\x02 \x01(\tR\tprojectId\x12\x1f\n\x0binstance_id\x18\x03 \x01(\tR\ninstanceId\"\xdb\x01\n\x19OfflineFeatureStoreConfig\x12<\n\x07parquet\x18\x01 \x01(\x0b\x32 .tecton_proto.args.ParquetConfigH\x00R\x07parquet\x12\x36\n\x05\x64\x65lta\x18\x02 \x01(\x0b\x32\x1e.tecton_proto.args.DeltaConfigH\x00R\x05\x64\x65lta\x12:\n\x15subdirectory_override\x18\x03 \x01(\tB\x05\x92M\x02\x08\x01R\x14subdirectoryOverrideB\x0c\n\nstore_type\"\x0f\n\rParquetConfig\"X\n\x0b\x44\x65ltaConfig\x12I\n\x13time_partition_size\x18\x01 \x01(\x0b\x32\x19.google.protobuf.DurationR\x11timePartitionSize\"\x99\x02\n\x10MonitoringConfig\x12+\n\x11monitor_freshness\x18\x01 \x01(\x08R\x10monitorFreshness\x12^\n\x1a\x65xpected_feature_freshness\x18\x02 \x01(\x0b\x32\x19.google.protobuf.DurationB\x05\x82}\x02\x10\x03R\x18\x65xpectedFeatureFreshness\x12O\n\x12\x65xpected_freshness\x18\x04 \x01(\x0b\x32\x19.google.protobuf.DurationB\x05\x82}\x02\x08\x05R\x11\x65xpectedFreshness\x12\'\n\x0b\x61lert_email\x18\x03 \x01(\tB\x06\xf2\xe2\x02\x02\x08\x01R\nalertEmail*\xe9\x01\n\x0f\x46\x65\x61tureViewType\x12\x1d\n\x19\x46\x45\x41TURE_VIEW_TYPE_UNKNOWN\x10\x00\x12\x1e\n\x1a\x46\x45\x41TURE_VIEW_TYPE_TEMPORAL\x10\x01\x12(\n$FEATURE_VIEW_TYPE_TEMPORAL_AGGREGATE\x10\x02\x12\x1f\n\x1b\x46\x45\x41TURE_VIEW_TYPE_ON_DEMAND\x10\x03\x12#\n\x1f\x46\x45\x41TURE_VIEW_TYPE_FEATURE_TABLE\x10\x04\x12\'\n#FEATURE_VIEW_TYPE_FWV5_FEATURE_VIEW\x10\x05*\xbb\x01\n\x12\x42\x61\x63kfillConfigMode\x12 \n\x1c\x42\x41\x43KFILL_CONFIG_MODE_UNKNOWN\x10\x00\x12?\n;BACKFILL_CONFIG_MODE_SINGLE_BATCH_SCHEDULE_INTERVAL_PER_JOB\x10\x01\x12\x42\n>BACKFILL_CONFIG_MODE_MULTIPLE_BATCH_SCHEDULE_INTERVALS_PER_JOB\x10\x02*\x8b\x01\n\x14StreamProcessingMode\x12\"\n\x1eSTREAM_PROCESSING_MODE_UNKNOWN\x10\x00\x12(\n$STREAM_PROCESSING_MODE_TIME_INTERVAL\x10\x01\x12%\n!STREAM_PROCESSING_MODE_CONTINUOUS\x10\x02*\xa4\x01\n\x10\x42\x61tchTriggerType\x12\x1e\n\x1a\x42\x41TCH_TRIGGER_TYPE_UNKNOWN\x10\x00\x12 \n\x1c\x42\x41TCH_TRIGGER_TYPE_SCHEDULED\x10\x01\x12\x1d\n\x19\x42\x41TCH_TRIGGER_TYPE_MANUAL\x10\x02\x12/\n+BATCH_TRIGGER_TYPE_NO_BATCH_MATERIALIZATION\x10\x03\x42\x13\n\x0f\x63om.tecton.argsP\x01')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.args.feature_view_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\017com.tecton.argsP\001'
@@ -55,14 +55,16 @@
   _FEATUREVIEWARGS.fields_by_name['feature_table_args']._serialized_options = b'\222M\002\020\001'
   _FEATUREVIEWARGS.fields_by_name['online_enabled']._options = None
   _FEATUREVIEWARGS.fields_by_name['online_enabled']._serialized_options = b'\222M\002\010\001'
   _FEATUREVIEWARGS.fields_by_name['offline_enabled']._options = None
   _FEATUREVIEWARGS.fields_by_name['offline_enabled']._serialized_options = b'\222M\002\010\001'
   _FEATUREVIEWARGS.fields_by_name['pipeline']._options = None
   _FEATUREVIEWARGS.fields_by_name['pipeline']._serialized_options = b'\222M\002\010\006'
+  _FEATUREVIEWARGS.fields_by_name['data_quality_config']._options = None
+  _FEATUREVIEWARGS.fields_by_name['data_quality_config']._serialized_options = b'\222M\002\010\001\222M\002\020\001'
   _TEMPORALARGS.fields_by_name['feature_start_time']._options = None
   _TEMPORALARGS.fields_by_name['feature_start_time']._serialized_options = b'\222M\002\010\001'
   _TEMPORALARGS.fields_by_name['serving_ttl']._options = None
   _TEMPORALARGS.fields_by_name['serving_ttl']._serialized_options = b'\222M\002\010\006'
   _TEMPORALARGS.fields_by_name['monitoring']._options = None
   _TEMPORALARGS.fields_by_name['monitoring']._serialized_options = b'\222M\002\010\001'
   _TEMPORALARGS.fields_by_name['backfill_config']._options = None
@@ -71,24 +73,32 @@
   _TEMPORALAGGREGATEARGS.fields_by_name['aggregation_slide_period_duration']._serialized_options = b'\222M\002\010\005'
   _TEMPORALAGGREGATEARGS.fields_by_name['feature_start_time']._options = None
   _TEMPORALAGGREGATEARGS.fields_by_name['feature_start_time']._serialized_options = b'\222M\002\010\001'
   _TEMPORALAGGREGATEARGS.fields_by_name['monitoring']._options = None
   _TEMPORALAGGREGATEARGS.fields_by_name['monitoring']._serialized_options = b'\222M\002\010\001'
   _MATERIALIZEDFEATUREVIEWARGS.fields_by_name['feature_start_time']._options = None
   _MATERIALIZEDFEATUREVIEWARGS.fields_by_name['feature_start_time']._serialized_options = b'\222M\002\010\001'
+  _MATERIALIZEDFEATUREVIEWARGS.fields_by_name['manual_trigger_backfill_end_time']._options = None
+  _MATERIALIZEDFEATUREVIEWARGS.fields_by_name['manual_trigger_backfill_end_time']._serialized_options = b'\222M\002\010\001'
+  _MATERIALIZEDFEATUREVIEWARGS.fields_by_name['max_backfill_interval']._options = None
+  _MATERIALIZEDFEATUREVIEWARGS.fields_by_name['max_backfill_interval']._serialized_options = b'\222M\"* \n\036max_batch_aggregation_interval\222M\t*\007\022\0050.7.0'
   _MATERIALIZEDFEATUREVIEWARGS.fields_by_name['serving_ttl']._options = None
   _MATERIALIZEDFEATUREVIEWARGS.fields_by_name['serving_ttl']._serialized_options = b'\222M\002\010\006'
   _MATERIALIZEDFEATUREVIEWARGS.fields_by_name['monitoring']._options = None
   _MATERIALIZEDFEATUREVIEWARGS.fields_by_name['monitoring']._serialized_options = b'\222M\002\010\001\222M\002\020\001'
+  _MATERIALIZEDFEATUREVIEWARGS.fields_by_name['stream_processing_mode']._options = None
+  _MATERIALIZEDFEATUREVIEWARGS.fields_by_name['stream_processing_mode']._serialized_options = b'\222M\002 \006'
   _MATERIALIZEDFEATUREVIEWARGS.fields_by_name['batch_trigger']._options = None
   _MATERIALIZEDFEATUREVIEWARGS.fields_by_name['batch_trigger']._serialized_options = b'\222M\002\010\001'
   _ONDEMANDARGS.fields_by_name['output_schema']._options = None
   _ONDEMANDARGS.fields_by_name['output_schema']._serialized_options = b'\202}\002\020\003'
   _ONDEMANDARGS.fields_by_name['schema']._options = None
   _ONDEMANDARGS.fields_by_name['schema']._serialized_options = b'\202}\002\010\005'
+  _ONDEMANDARGS.fields_by_name['environments']._options = None
+  _ONDEMANDARGS.fields_by_name['environments']._serialized_options = b'\222M\002\010\001\202}\002\010\005'
   _FEATURETABLEARGS.fields_by_name['output_schema']._options = None
   _FEATURETABLEARGS.fields_by_name['output_schema']._serialized_options = b'\202}\002\020\003'
   _FEATURETABLEARGS.fields_by_name['schema']._options = None
   _FEATURETABLEARGS.fields_by_name['schema']._serialized_options = b'\202}\002\010\005'
   _FEATURETABLEARGS.fields_by_name['serving_ttl']._options = None
   _FEATURETABLEARGS.fields_by_name['serving_ttl']._serialized_options = b'\222M\002\010\006'
   _FEATURETABLEARGS.fields_by_name['offline_config']._options = None
@@ -108,95 +118,113 @@
   _FEATUREAGGREGATION.fields_by_name['time_windows']._options = None
   _FEATUREAGGREGATION.fields_by_name['time_windows']._serialized_options = b'\202}\002\020\003'
   _FEATUREAGGREGATION.fields_by_name['time_window_strs']._options = None
   _FEATUREAGGREGATION.fields_by_name['time_window_strs']._serialized_options = b'\202}\002\020\003\222M\002\010\005'
   _FEATUREAGGREGATION.fields_by_name['time_window']._options = None
   _FEATUREAGGREGATION.fields_by_name['time_window']._serialized_options = b'\202}\002\010\005'
   _FEATUREAGGREGATION.fields_by_name['name']._options = None
-  _FEATUREAGGREGATION.fields_by_name['name']._serialized_options = b'\202}\002\010\005'
+  _FEATUREAGGREGATION.fields_by_name['name']._serialized_options = b'\202}\002\010\005\222M\002 \001'
+  _DATAQUALITYCONFIG.fields_by_name['data_quality_enabled']._options = None
+  _DATAQUALITYCONFIG.fields_by_name['data_quality_enabled']._serialized_options = b'\222M\002 \003'
+  _DATAQUALITYCONFIG.fields_by_name['skip_default_expectations']._options = None
+  _DATAQUALITYCONFIG.fields_by_name['skip_default_expectations']._serialized_options = b'\222M\002 \003'
   _CLUSTERCONFIG.fields_by_name['existing_cluster']._options = None
   _CLUSTERCONFIG.fields_by_name['existing_cluster']._serialized_options = b'\222M\002\010\001'
   _CLUSTERCONFIG.fields_by_name['new_databricks']._options = None
   _CLUSTERCONFIG.fields_by_name['new_databricks']._serialized_options = b'\222M\002\010\001'
   _CLUSTERCONFIG.fields_by_name['new_emr']._options = None
   _CLUSTERCONFIG.fields_by_name['new_emr']._serialized_options = b'\222M\002\010\001'
   _CLUSTERCONFIG.fields_by_name['implicit_config']._options = None
   _CLUSTERCONFIG.fields_by_name['implicit_config']._serialized_options = b'\222M\002\010\001\222M\002\030\005'
   _CLUSTERCONFIG.fields_by_name['json_databricks']._options = None
   _CLUSTERCONFIG.fields_by_name['json_databricks']._serialized_options = b'\222M\002\010\001'
   _CLUSTERCONFIG.fields_by_name['json_emr']._options = None
   _CLUSTERCONFIG.fields_by_name['json_emr']._serialized_options = b'\222M\002\010\001'
+  _CLUSTERCONFIG.fields_by_name['json_dataproc']._options = None
+  _CLUSTERCONFIG.fields_by_name['json_dataproc']._serialized_options = b'\222M\002\010\001'
   _DEFAULTCLUSTERCONFIG.fields_by_name['databricks_spark_version']._options = None
   _DEFAULTCLUSTERCONFIG.fields_by_name['databricks_spark_version']._serialized_options = b'\222M\002\030\005'
   _DEFAULTCLUSTERCONFIG.fields_by_name['emr_spark_version']._options = None
   _DEFAULTCLUSTERCONFIG.fields_by_name['emr_spark_version']._serialized_options = b'\222M\002\030\005'
   _SPARKCONFIG_SPARKCONFENTRY._options = None
   _SPARKCONFIG_SPARKCONFENTRY._serialized_options = b'8\001'
+  _REDISONLINESTORE.fields_by_name['primary_endpoint']._options = None
+  _REDISONLINESTORE.fields_by_name['primary_endpoint']._serialized_options = b'\222M\002\010\006'
+  _REDISONLINESTORE.fields_by_name['authentication_token']._options = None
+  _REDISONLINESTORE.fields_by_name['authentication_token']._serialized_options = b'\222M\002\010\001\222M\002\030\006'
+  _REDISONLINESTORE.fields_by_name['tls_enabled']._options = None
+  _REDISONLINESTORE.fields_by_name['tls_enabled']._serialized_options = b'\222M\002\010\001\222M\002\030\005'
+  _REDISONLINESTORE.fields_by_name['enabled']._options = None
+  _REDISONLINESTORE.fields_by_name['enabled']._serialized_options = b'\222M\002\030\005'
   _OFFLINEFEATURESTORECONFIG.fields_by_name['subdirectory_override']._options = None
   _OFFLINEFEATURESTORECONFIG.fields_by_name['subdirectory_override']._serialized_options = b'\222M\002\010\001'
   _MONITORINGCONFIG.fields_by_name['expected_feature_freshness']._options = None
   _MONITORINGCONFIG.fields_by_name['expected_feature_freshness']._serialized_options = b'\202}\002\020\003'
   _MONITORINGCONFIG.fields_by_name['expected_freshness']._options = None
   _MONITORINGCONFIG.fields_by_name['expected_freshness']._serialized_options = b'\202}\002\010\005'
   _MONITORINGCONFIG.fields_by_name['alert_email']._options = None
   _MONITORINGCONFIG.fields_by_name['alert_email']._serialized_options = b'\362\342\002\002\010\001'
-  _FEATUREVIEWTYPE._serialized_start=10357
-  _FEATUREVIEWTYPE._serialized_end=10590
-  _BACKFILLCONFIGMODE._serialized_start=10593
-  _BACKFILLCONFIGMODE._serialized_end=10780
-  _STREAMPROCESSINGMODE._serialized_start=10783
-  _STREAMPROCESSINGMODE._serialized_end=10922
-  _BATCHTRIGGERTYPE._serialized_start=10925
-  _BATCHTRIGGERTYPE._serialized_end=11089
+  _FEATUREVIEWTYPE._serialized_start=11073
+  _FEATUREVIEWTYPE._serialized_end=11306
+  _BACKFILLCONFIGMODE._serialized_start=11309
+  _BACKFILLCONFIGMODE._serialized_end=11496
+  _STREAMPROCESSINGMODE._serialized_start=11499
+  _STREAMPROCESSINGMODE._serialized_end=11638
+  _BATCHTRIGGERTYPE._serialized_start=11641
+  _BATCHTRIGGERTYPE._serialized_end=11805
   _FEATUREVIEWARGS._serialized_start=583
-  _FEATUREVIEWARGS._serialized_end=1966
-  _ENTITYKEYOVERRIDE._serialized_start=1968
-  _ENTITYKEYOVERRIDE._serialized_end=2070
-  _BACKFILLCONFIG._serialized_start=2072
-  _BACKFILLCONFIG._serialized_end=2147
-  _OUTPUTSTREAM._serialized_start=2150
-  _OUTPUTSTREAM._serialized_end=2356
-  _TEMPORALARGS._serialized_start=2359
-  _TEMPORALARGS._serialized_end=3440
-  _TEMPORALAGGREGATEARGS._serialized_start=3443
-  _TEMPORALAGGREGATEARGS._serialized_end=4572
-  _MATERIALIZEDFEATUREVIEWARGS._serialized_start=4575
-  _MATERIALIZEDFEATUREVIEWARGS._serialized_end=5920
-  _ONDEMANDARGS._serialized_start=5923
-  _ONDEMANDARGS._serialized_end=6080
-  _FEATURETABLEARGS._serialized_start=6083
-  _FEATURETABLEARGS._serialized_end=6838
-  _FEATUREAGGREGATION._serialized_start=6841
-  _FEATUREAGGREGATION._serialized_end=7328
-  _FEATUREAGGREGATION_FUNCTIONPARAMSENTRY._serialized_start=7232
-  _FEATUREAGGREGATION_FUNCTIONPARAMSENTRY._serialized_end=7328
-  _PARAMVALUE._serialized_start=7330
-  _PARAMVALUE._serialized_end=7386
-  _CLUSTERCONFIG._serialized_start=7389
-  _CLUSTERCONFIG._serialized_end=7922
-  _JSONCLUSTERCONFIG._serialized_start=7924
-  _JSONCLUSTERCONFIG._serialized_end=7988
-  _EXISTINGCLUSTERCONFIG._serialized_start=7990
-  _EXISTINGCLUSTERCONFIG._serialized_end=8061
-  _NEWCLUSTERCONFIG._serialized_start=8064
-  _NEWCLUSTERCONFIG._serialized_end=8479
-  _DEFAULTCLUSTERCONFIG._serialized_start=8482
-  _DEFAULTCLUSTERCONFIG._serialized_end=8620
-  _SPARKCONFIG._serialized_start=8623
-  _SPARKCONFIG._serialized_end=9010
-  _SPARKCONFIG_SPARKCONFENTRY._serialized_start=8950
-  _SPARKCONFIG_SPARKCONFENTRY._serialized_end=9010
-  _ONLINESTORECONFIG._serialized_start=9013
-  _ONLINESTORECONFIG._serialized_end=9173
-  _DYNAMODBONLINESTORE._serialized_start=9176
-  _DYNAMODBONLINESTORE._serialized_end=9465
-  _REDISONLINESTORE._serialized_start=9468
-  _REDISONLINESTORE._serialized_end=9741
-  _OFFLINEFEATURESTORECONFIG._serialized_start=9744
-  _OFFLINEFEATURESTORECONFIG._serialized_end=9963
-  _PARQUETCONFIG._serialized_start=9965
-  _PARQUETCONFIG._serialized_end=9980
-  _DELTACONFIG._serialized_start=9982
-  _DELTACONFIG._serialized_end=10070
-  _MONITORINGCONFIG._serialized_start=10073
-  _MONITORINGCONFIG._serialized_end=10354
+  _FEATUREVIEWARGS._serialized_end=2064
+  _ENTITYKEYOVERRIDE._serialized_start=2066
+  _ENTITYKEYOVERRIDE._serialized_end=2168
+  _BACKFILLCONFIG._serialized_start=2170
+  _BACKFILLCONFIG._serialized_end=2245
+  _OUTPUTSTREAM._serialized_start=2248
+  _OUTPUTSTREAM._serialized_end=2454
+  _TEMPORALARGS._serialized_start=2457
+  _TEMPORALARGS._serialized_end=3538
+  _TEMPORALAGGREGATEARGS._serialized_start=3541
+  _TEMPORALAGGREGATEARGS._serialized_end=4670
+  _MATERIALIZEDFEATUREVIEWARGS._serialized_start=4673
+  _MATERIALIZEDFEATUREVIEWARGS._serialized_end=6167
+  _ONDEMANDARGS._serialized_start=6170
+  _ONDEMANDARGS._serialized_end=6381
+  _FEATURETABLEARGS._serialized_start=6384
+  _FEATURETABLEARGS._serialized_end=7139
+  _FEATUREAGGREGATION._serialized_start=7142
+  _FEATUREAGGREGATION._serialized_end=7634
+  _FEATUREAGGREGATION_FUNCTIONPARAMSENTRY._serialized_start=7538
+  _FEATUREAGGREGATION_FUNCTIONPARAMSENTRY._serialized_end=7634
+  _PARAMVALUE._serialized_start=7636
+  _PARAMVALUE._serialized_end=7729
+  _DATAQUALITYCONFIG._serialized_start=7732
+  _DATAQUALITYCONFIG._serialized_end=7875
+  _CLUSTERCONFIG._serialized_start=7878
+  _CLUSTERCONFIG._serialized_end=8495
+  _JSONCLUSTERCONFIG._serialized_start=8497
+  _JSONCLUSTERCONFIG._serialized_end=8561
+  _EXISTINGCLUSTERCONFIG._serialized_start=8563
+  _EXISTINGCLUSTERCONFIG._serialized_end=8634
+  _NEWCLUSTERCONFIG._serialized_start=8637
+  _NEWCLUSTERCONFIG._serialized_end=9052
+  _DEFAULTCLUSTERCONFIG._serialized_start=9055
+  _DEFAULTCLUSTERCONFIG._serialized_end=9193
+  _SPARKCONFIG._serialized_start=9196
+  _SPARKCONFIG._serialized_end=9583
+  _SPARKCONFIG_SPARKCONFENTRY._serialized_start=9523
+  _SPARKCONFIG_SPARKCONFENTRY._serialized_end=9583
+  _ONLINESTORECONFIG._serialized_start=9586
+  _ONLINESTORECONFIG._serialized_end=9816
+  _DYNAMODBONLINESTORE._serialized_start=9819
+  _DYNAMODBONLINESTORE._serialized_end=10108
+  _REDISONLINESTORE._serialized_start=10111
+  _REDISONLINESTORE._serialized_end=10344
+  _BIGTABLEONLINESTORE._serialized_start=10346
+  _BIGTABLEONLINESTORE._serialized_end=10457
+  _OFFLINEFEATURESTORECONFIG._serialized_start=10460
+  _OFFLINEFEATURESTORECONFIG._serialized_end=10679
+  _PARQUETCONFIG._serialized_start=10681
+  _PARQUETCONFIG._serialized_end=10696
+  _DELTACONFIG._serialized_start=10698
+  _DELTACONFIG._serialized_end=10786
+  _MONITORINGCONFIG._serialized_start=10789
+  _MONITORINGCONFIG._serialized_end=11070
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/args/pipeline_pb2.py` & `tecton-0.7.0rc0/tecton_proto/args/pipeline_pb2.py`

 * *Files 7% similar despite different names*

```diff
@@ -9,43 +9,47 @@
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
 from google.protobuf import duration_pb2 as google_dot_protobuf_dot_duration__pb2
 from google.protobuf import empty_pb2 as google_dot_protobuf_dot_empty__pb2
+from tecton_proto.args import diff_options_pb2 as tecton__proto_dot_args_dot_diff__options__pb2
 from tecton_proto.common import id_pb2 as tecton__proto_dot_common_dot_id__pb2
 from tecton_proto.common import spark_schema_pb2 as tecton__proto_dot_common_dot_spark__schema__pb2
+from tecton_proto.common import schema_pb2 as tecton__proto_dot_common_dot_schema__pb2
 from tecton_proto.args import feature_service_pb2 as tecton__proto_dot_args_dot_feature__service__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n tecton_proto/args/pipeline.proto\x12\x11tecton_proto.args\x1a\x1egoogle/protobuf/duration.proto\x1a\x1bgoogle/protobuf/empty.proto\x1a\x1ctecton_proto/common/id.proto\x1a&tecton_proto/common/spark_schema.proto\x1a\'tecton_proto/args/feature_service.proto\"z\n\x08Pipeline\x12\x33\n\x04root\x18\x01 \x01(\x0b\x32\x1f.tecton_proto.args.PipelineNodeR\x04root\x12\x33\n\x04mode\x18\x02 \x01(\x0e\x32\x1f.tecton_proto.args.PipelineModeR\x04modeJ\x04\x08\x03\x10\x04\"\xbc\x04\n\x0cPipelineNode\x12X\n\x13transformation_node\x18\x01 \x01(\x0b\x32%.tecton_proto.args.TransformationNodeH\x00R\x12transformationNode\x12M\n\x10\x64\x61ta_source_node\x18\x02 \x01(\x0b\x32!.tecton_proto.args.DataSourceNodeH\x00R\x0e\x64\x61taSourceNode\x12\x46\n\rconstant_node\x18\x03 \x01(\x0b\x32\x1f.tecton_proto.args.ConstantNodeH\x00R\x0c\x63onstantNode\x12\x63\n\x18request_data_source_node\x18\x04 \x01(\x0b\x32(.tecton_proto.args.RequestDataSourceNodeH\x00R\x15requestDataSourceNode\x12P\n\x11\x66\x65\x61ture_view_node\x18\x05 \x01(\x0b\x32\".tecton_proto.args.FeatureViewNodeH\x00R\x0f\x66\x65\x61tureViewNode\x12q\n\x1cmaterialization_context_node\x18\x07 \x01(\x0b\x32-.tecton_proto.args.MaterializationContextNodeH\x00R\x1amaterializationContextNodeB\x0b\n\tnode_typeJ\x04\x08\x06\x10\x07\"\x82\x01\n\x15RequestDataSourceNode\x12J\n\x0frequest_context\x18\x01 \x01(\x0b\x32!.tecton_proto.args.RequestContextR\x0erequestContext\x12\x1d\n\ninput_name\x18\x02 \x01(\tR\tinputName\"J\n\x0eRequestContext\x12\x38\n\x06schema\x18\x01 \x01(\x0b\x32 .tecton_proto.common.SparkSchemaR\x06schema\"\xc5\x01\n\x0f\x46\x65\x61tureViewNode\x12?\n\x0f\x66\x65\x61ture_view_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\rfeatureViewId\x12R\n\x0c\x66\x65\x61ture_view\x18\x02 \x01(\x0b\x32/.tecton_proto.args.FeatureServiceFeaturePackageR\x0b\x66\x65\x61tureView\x12\x1d\n\ninput_name\x18\x03 \x01(\tR\tinputName\"\x8c\x01\n\x12TransformationNode\x12\x44\n\x11transformation_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x10transformationId\x12\x30\n\x06inputs\x18\x02 \x03(\x0b\x32\x18.tecton_proto.args.InputR\x06inputs\"\x84\x01\n\x05Input\x12\x1d\n\targ_index\x18\x01 \x01(\x05H\x00R\x08\x61rgIndex\x12\x1b\n\x08\x61rg_name\x18\x02 \x01(\tH\x00R\x07\x61rgName\x12\x33\n\x04node\x18\x03 \x01(\x0b\x32\x1f.tecton_proto.args.PipelineNodeR\x04nodeB\n\n\x08\x61rg_type\"\xc4\x03\n\x0e\x44\x61taSourceNode\x12L\n\x16virtual_data_source_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x13virtualDataSourceId\x12\x33\n\x06window\x18\x02 \x01(\x0b\x32\x19.google.protobuf.DurationH\x00R\x06window\x12>\n\x1awindow_unbounded_preceding\x18\x04 \x01(\x08H\x00R\x18windowUnboundedPreceding\x12+\n\x10window_unbounded\x18\x06 \x01(\x08H\x00R\x0fwindowUnbounded\x12G\n\x11start_time_offset\x18\x07 \x01(\x0b\x32\x19.google.protobuf.DurationH\x00R\x0fstartTimeOffset\x12\x42\n\x0fschedule_offset\x18\x03 \x01(\x0b\x32\x19.google.protobuf.DurationR\x0escheduleOffset\x12\x1d\n\ninput_name\x18\x05 \x01(\tR\tinputNameB\x16\n\x14\x64\x61ta_window_override\"\xe0\x01\n\x0c\x43onstantNode\x12#\n\x0cstring_const\x18\x01 \x01(\tH\x00R\x0bstringConst\x12\x1d\n\tint_const\x18\x02 \x01(\tH\x00R\x08intConst\x12!\n\x0b\x66loat_const\x18\x03 \x01(\tH\x00R\nfloatConst\x12\x1f\n\nbool_const\x18\x04 \x01(\x08H\x00R\tboolConst\x12\x37\n\nnull_const\x18\x05 \x01(\x0b\x32\x16.google.protobuf.EmptyH\x00R\tnullConstB\x0f\n\rconstant_type\"\x1c\n\x1aMaterializationContextNode*\x9a\x01\n\x0cPipelineMode\x12\x19\n\x15PIPELINE_MODE_UNKNOWN\x10\x00\x12\x17\n\x13PIPELINE_MODE_SPARK\x10\x01\x12\x1b\n\x17PIPELINE_MODE_ON_DEMAND\x10\x02\x12\x1f\n\x1bPIPELINE_MODE_SNOWFLAKE_SQL\x10\x03\x12\x18\n\x14PIPELINE_MODE_ATHENA\x10\x04\x42\x13\n\x0f\x63om.tecton.argsP\x01')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n tecton_proto/args/pipeline.proto\x12\x11tecton_proto.args\x1a\x1egoogle/protobuf/duration.proto\x1a\x1bgoogle/protobuf/empty.proto\x1a$tecton_proto/args/diff_options.proto\x1a\x1ctecton_proto/common/id.proto\x1a&tecton_proto/common/spark_schema.proto\x1a tecton_proto/common/schema.proto\x1a\'tecton_proto/args/feature_service.proto\"z\n\x08Pipeline\x12\x33\n\x04root\x18\x01 \x01(\x0b\x32\x1f.tecton_proto.args.PipelineNodeR\x04root\x12\x33\n\x04mode\x18\x02 \x01(\x0e\x32\x1f.tecton_proto.args.PipelineModeR\x04modeJ\x04\x08\x03\x10\x04\"\xbc\x04\n\x0cPipelineNode\x12X\n\x13transformation_node\x18\x01 \x01(\x0b\x32%.tecton_proto.args.TransformationNodeH\x00R\x12transformationNode\x12M\n\x10\x64\x61ta_source_node\x18\x02 \x01(\x0b\x32!.tecton_proto.args.DataSourceNodeH\x00R\x0e\x64\x61taSourceNode\x12\x46\n\rconstant_node\x18\x03 \x01(\x0b\x32\x1f.tecton_proto.args.ConstantNodeH\x00R\x0c\x63onstantNode\x12\x63\n\x18request_data_source_node\x18\x04 \x01(\x0b\x32(.tecton_proto.args.RequestDataSourceNodeH\x00R\x15requestDataSourceNode\x12P\n\x11\x66\x65\x61ture_view_node\x18\x05 \x01(\x0b\x32\".tecton_proto.args.FeatureViewNodeH\x00R\x0f\x66\x65\x61tureViewNode\x12q\n\x1cmaterialization_context_node\x18\x07 \x01(\x0b\x32-.tecton_proto.args.MaterializationContextNodeH\x00R\x1amaterializationContextNodeB\x0b\n\tnode_typeJ\x04\x08\x06\x10\x07\"\x89\x01\n\x15RequestDataSourceNode\x12Q\n\x0frequest_context\x18\x01 \x01(\x0b\x32!.tecton_proto.args.RequestContextB\x05\x92M\x02 \x04R\x0erequestContext\x12\x1d\n\ninput_name\x18\x02 \x01(\tR\tinputName\"\x8c\x01\n\x0eRequestContext\x12@\n\rtecton_schema\x18\x02 \x01(\x0b\x32\x1b.tecton_proto.common.SchemaR\x0ctectonSchema\x12\x38\n\x06schema\x18\x01 \x01(\x0b\x32 .tecton_proto.common.SparkSchemaR\x06schema\"\xc5\x01\n\x0f\x46\x65\x61tureViewNode\x12?\n\x0f\x66\x65\x61ture_view_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\rfeatureViewId\x12R\n\x0c\x66\x65\x61ture_view\x18\x02 \x01(\x0b\x32/.tecton_proto.args.FeatureServiceFeaturePackageR\x0b\x66\x65\x61tureView\x12\x1d\n\ninput_name\x18\x03 \x01(\tR\tinputName\"\x8c\x01\n\x12TransformationNode\x12\x44\n\x11transformation_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x10transformationId\x12\x30\n\x06inputs\x18\x02 \x03(\x0b\x32\x18.tecton_proto.args.InputR\x06inputs\"\x84\x01\n\x05Input\x12\x1d\n\targ_index\x18\x01 \x01(\x05H\x00R\x08\x61rgIndex\x12\x1b\n\x08\x61rg_name\x18\x02 \x01(\tH\x00R\x07\x61rgName\x12\x33\n\x04node\x18\x03 \x01(\x0b\x32\x1f.tecton_proto.args.PipelineNodeR\x04nodeB\n\n\x08\x61rg_type\"\xc4\x03\n\x0e\x44\x61taSourceNode\x12L\n\x16virtual_data_source_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x13virtualDataSourceId\x12\x33\n\x06window\x18\x02 \x01(\x0b\x32\x19.google.protobuf.DurationH\x00R\x06window\x12>\n\x1awindow_unbounded_preceding\x18\x04 \x01(\x08H\x00R\x18windowUnboundedPreceding\x12+\n\x10window_unbounded\x18\x06 \x01(\x08H\x00R\x0fwindowUnbounded\x12G\n\x11start_time_offset\x18\x07 \x01(\x0b\x32\x19.google.protobuf.DurationH\x00R\x0fstartTimeOffset\x12\x42\n\x0fschedule_offset\x18\x03 \x01(\x0b\x32\x19.google.protobuf.DurationR\x0escheduleOffset\x12\x1d\n\ninput_name\x18\x05 \x01(\tR\tinputNameB\x16\n\x14\x64\x61ta_window_override\"\xe0\x01\n\x0c\x43onstantNode\x12#\n\x0cstring_const\x18\x01 \x01(\tH\x00R\x0bstringConst\x12\x1d\n\tint_const\x18\x02 \x01(\tH\x00R\x08intConst\x12!\n\x0b\x66loat_const\x18\x03 \x01(\tH\x00R\nfloatConst\x12\x1f\n\nbool_const\x18\x04 \x01(\x08H\x00R\tboolConst\x12\x37\n\nnull_const\x18\x05 \x01(\x0b\x32\x16.google.protobuf.EmptyH\x00R\tnullConstB\x0f\n\rconstant_type\"\x1c\n\x1aMaterializationContextNode*\x86\x01\n\x0cPipelineMode\x12\x19\n\x15PIPELINE_MODE_UNKNOWN\x10\x00\x12\x17\n\x13PIPELINE_MODE_SPARK\x10\x01\x12\x1b\n\x17PIPELINE_MODE_ON_DEMAND\x10\x02\x12\x1f\n\x1bPIPELINE_MODE_SNOWFLAKE_SQL\x10\x03\"\x04\x08\x04\x10\x04\x42\x13\n\x0f\x63om.tecton.argsP\x01')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.args.pipeline_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\017com.tecton.argsP\001'
-  _PIPELINEMODE._serialized_start=2326
-  _PIPELINEMODE._serialized_end=2480
-  _PIPELINE._serialized_start=227
-  _PIPELINE._serialized_end=349
-  _PIPELINENODE._serialized_start=352
-  _PIPELINENODE._serialized_end=924
-  _REQUESTDATASOURCENODE._serialized_start=927
-  _REQUESTDATASOURCENODE._serialized_end=1057
-  _REQUESTCONTEXT._serialized_start=1059
-  _REQUESTCONTEXT._serialized_end=1133
-  _FEATUREVIEWNODE._serialized_start=1136
-  _FEATUREVIEWNODE._serialized_end=1333
-  _TRANSFORMATIONNODE._serialized_start=1336
-  _TRANSFORMATIONNODE._serialized_end=1476
-  _INPUT._serialized_start=1479
-  _INPUT._serialized_end=1611
-  _DATASOURCENODE._serialized_start=1614
-  _DATASOURCENODE._serialized_end=2066
-  _CONSTANTNODE._serialized_start=2069
-  _CONSTANTNODE._serialized_end=2293
-  _MATERIALIZATIONCONTEXTNODE._serialized_start=2295
-  _MATERIALIZATIONCONTEXTNODE._serialized_end=2323
+  _REQUESTDATASOURCENODE.fields_by_name['request_context']._options = None
+  _REQUESTDATASOURCENODE.fields_by_name['request_context']._serialized_options = b'\222M\002 \004'
+  _PIPELINEMODE._serialized_start=2472
+  _PIPELINEMODE._serialized_end=2606
+  _PIPELINE._serialized_start=299
+  _PIPELINE._serialized_end=421
+  _PIPELINENODE._serialized_start=424
+  _PIPELINENODE._serialized_end=996
+  _REQUESTDATASOURCENODE._serialized_start=999
+  _REQUESTDATASOURCENODE._serialized_end=1136
+  _REQUESTCONTEXT._serialized_start=1139
+  _REQUESTCONTEXT._serialized_end=1279
+  _FEATUREVIEWNODE._serialized_start=1282
+  _FEATUREVIEWNODE._serialized_end=1479
+  _TRANSFORMATIONNODE._serialized_start=1482
+  _TRANSFORMATIONNODE._serialized_end=1622
+  _INPUT._serialized_start=1625
+  _INPUT._serialized_end=1757
+  _DATASOURCENODE._serialized_start=1760
+  _DATASOURCENODE._serialized_end=2212
+  _CONSTANTNODE._serialized_start=2215
+  _CONSTANTNODE._serialized_end=2439
+  _MATERIALIZATIONCONTEXTNODE._serialized_start=2441
+  _MATERIALIZATIONCONTEXTNODE._serialized_end=2469
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/args/repo_metadata_pb2.py` & `tecton-0.7.0rc0/tecton_proto/args/repo_metadata_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/args/transformation_pb2.py` & `tecton-0.7.0rc0/tecton_proto/args/transformation_pb2.py`

 * *Files 2% similar despite different names*

```diff
@@ -14,15 +14,15 @@
 from tecton_proto.args import user_defined_function_pb2 as tecton__proto_dot_args_dot_user__defined__function__pb2
 from tecton_proto.args import basic_info_pb2 as tecton__proto_dot_args_dot_basic__info__pb2
 from tecton_proto.common import id_pb2 as tecton__proto_dot_common_dot_id__pb2
 from tecton_proto.args import diff_options_pb2 as tecton__proto_dot_args_dot_diff__options__pb2
 from tecton_proto.common import framework_version_pb2 as tecton__proto_dot_common_dot_framework__version__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n&tecton_proto/args/transformation.proto\x12\x11tecton_proto.args\x1a-tecton_proto/args/user_defined_function.proto\x1a\"tecton_proto/args/basic_info.proto\x1a\x1ctecton_proto/common/id.proto\x1a$tecton_proto/args/diff_options.proto\x1a+tecton_proto/common/framework_version.proto\"\x82\x04\n\x12TransformationArgs\x12\x44\n\x11transformation_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x10transformationId\x12\x37\n\x04info\x18\x02 \x01(\x0b\x32\x1c.tecton_proto.args.BasicInfoB\x05\x92M\x02\x10\x01R\x04info\x12\x46\n\x07version\x18\x07 \x01(\x0e\x32%.tecton_proto.common.FrameworkVersionB\x05\x92M\x02\x08\x05R\x07version\x12.\n\x0fprevent_destroy\x18\x08 \x01(\x08\x42\x05\x92M\x02\x08\x01R\x0epreventDestroy\x12]\n\x13transformation_mode\x18\x03 \x01(\x0e\x32%.tecton_proto.args.TransformationModeB\x05\x92M\x02\x08\x06R\x12transformationMode\x12R\n\ruser_function\x18\x04 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionB\x05\x92M\x02\x08\x06R\x0cuserFunction\x12#\n\tdocstring\x18\x06 \x01(\tB\x05\x92M\x02\x08\x01R\tdocstring\x12\x1d\n\nis_builtin\x18\x05 \x01(\x08R\tisBuiltin*\xa2\x02\n\x12TransformationMode\x12\x1f\n\x1bTRANSFORMATION_MODE_UNKNOWN\x10\x00\x12\x1f\n\x1bTRANSFORMATION_MODE_PYSPARK\x10\x01\x12!\n\x1dTRANSFORMATION_MODE_SPARK_SQL\x10\x02\x12\x1e\n\x1aTRANSFORMATION_MODE_PANDAS\x10\x03\x12%\n!TRANSFORMATION_MODE_SNOWFLAKE_SQL\x10\x04\x12\x1e\n\x1aTRANSFORMATION_MODE_PYTHON\x10\x05\x12 \n\x1cTRANSFORMATION_MODE_SNOWPARK\x10\x06\x12\x1e\n\x1aTRANSFORMATION_MODE_ATHENA\x10\x07\x42\x13\n\x0f\x63om.tecton.argsP\x01')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n&tecton_proto/args/transformation.proto\x12\x11tecton_proto.args\x1a-tecton_proto/args/user_defined_function.proto\x1a\"tecton_proto/args/basic_info.proto\x1a\x1ctecton_proto/common/id.proto\x1a$tecton_proto/args/diff_options.proto\x1a+tecton_proto/common/framework_version.proto\"\x82\x04\n\x12TransformationArgs\x12\x44\n\x11transformation_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x10transformationId\x12\x37\n\x04info\x18\x02 \x01(\x0b\x32\x1c.tecton_proto.args.BasicInfoB\x05\x92M\x02\x10\x01R\x04info\x12\x46\n\x07version\x18\x07 \x01(\x0e\x32%.tecton_proto.common.FrameworkVersionB\x05\x92M\x02\x08\x05R\x07version\x12.\n\x0fprevent_destroy\x18\x08 \x01(\x08\x42\x05\x92M\x02\x08\x01R\x0epreventDestroy\x12]\n\x13transformation_mode\x18\x03 \x01(\x0e\x32%.tecton_proto.args.TransformationModeB\x05\x92M\x02\x08\x06R\x12transformationMode\x12R\n\ruser_function\x18\x04 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionB\x05\x92M\x02\x08\x06R\x0cuserFunction\x12#\n\tdocstring\x18\x06 \x01(\tB\x05\x92M\x02\x08\x01R\tdocstring\x12\x1d\n\nis_builtin\x18\x05 \x01(\x08R\tisBuiltin*\x88\x02\n\x12TransformationMode\x12\x1f\n\x1bTRANSFORMATION_MODE_UNKNOWN\x10\x00\x12\x1f\n\x1bTRANSFORMATION_MODE_PYSPARK\x10\x01\x12!\n\x1dTRANSFORMATION_MODE_SPARK_SQL\x10\x02\x12\x1e\n\x1aTRANSFORMATION_MODE_PANDAS\x10\x03\x12%\n!TRANSFORMATION_MODE_SNOWFLAKE_SQL\x10\x04\x12\x1e\n\x1aTRANSFORMATION_MODE_PYTHON\x10\x05\x12 \n\x1cTRANSFORMATION_MODE_SNOWPARK\x10\x06\"\x04\x08\x07\x10\x07\x42\x13\n\x0f\x63om.tecton.argsP\x01')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.args.transformation_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\017com.tecton.argsP\001'
@@ -35,11 +35,11 @@
   _TRANSFORMATIONARGS.fields_by_name['transformation_mode']._options = None
   _TRANSFORMATIONARGS.fields_by_name['transformation_mode']._serialized_options = b'\222M\002\010\006'
   _TRANSFORMATIONARGS.fields_by_name['user_function']._options = None
   _TRANSFORMATIONARGS.fields_by_name['user_function']._serialized_options = b'\222M\002\010\006'
   _TRANSFORMATIONARGS.fields_by_name['docstring']._options = None
   _TRANSFORMATIONARGS.fields_by_name['docstring']._serialized_options = b'\222M\002\010\001'
   _TRANSFORMATIONMODE._serialized_start=775
-  _TRANSFORMATIONMODE._serialized_end=1065
+  _TRANSFORMATIONMODE._serialized_end=1039
   _TRANSFORMATIONARGS._serialized_start=258
   _TRANSFORMATIONARGS._serialized_end=772
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/args/user_defined_function_pb2.py` & `tecton-0.7.0rc0/tecton_proto/args/user_defined_function_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/args/version_constraints_pb2.py` & `tecton-0.7.0rc0/tecton_proto/args/version_constraints_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/args/virtual_data_source_pb2.py` & `tecton-0.7.0rc0/tecton_proto/args/virtual_data_source_pb2.py`

 * *Files 2% similar despite different names*

```diff
@@ -17,15 +17,15 @@
 from tecton_proto.common import schema_container_pb2 as tecton__proto_dot_common_dot_schema__container__pb2
 from tecton_proto.common import framework_version_pb2 as tecton__proto_dot_common_dot_framework__version__pb2
 from tecton_proto.common import data_source_type_pb2 as tecton__proto_dot_common_dot_data__source__type__pb2
 from tecton_proto.args import data_source_pb2 as tecton__proto_dot_args_dot_data__source__pb2
 from tecton_proto.args import diff_options_pb2 as tecton__proto_dot_args_dot_diff__options__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n+tecton_proto/args/virtual_data_source.proto\x12\x11tecton_proto.args\x1a\"tecton_proto/args/basic_info.proto\x1a\x1ctecton_proto/common/id.proto\x1a&tecton_proto/common/spark_schema.proto\x1a*tecton_proto/common/schema_container.proto\x1a+tecton_proto/common/framework_version.proto\x1a*tecton_proto/common/data_source_type.proto\x1a#tecton_proto/args/data_source.proto\x1a$tecton_proto/args/diff_options.proto\"\xdd\n\n\x15VirtualDataSourceArgs\x12L\n\x16virtual_data_source_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x13virtualDataSourceId\x12\x37\n\x04info\x18\x02 \x01(\x0b\x32\x1c.tecton_proto.args.BasicInfoB\x05\x92M\x02\x10\x01R\x04info\x12\x46\n\x07version\x18\n \x01(\x0e\x32%.tecton_proto.common.FrameworkVersionB\x05\x92M\x02\x08\x05R\x07version\x12.\n\x0fprevent_destroy\x18\x10 \x01(\x08\x42\x05\x92M\x02\x08\x01R\x0epreventDestroy\x12T\n\x0ehive_ds_config\x18\x03 \x01(\x0b\x32%.tecton_proto.args.HiveDataSourceArgsB\x05\x92M\x02\x08\x06H\x00R\x0chiveDsConfig\x12T\n\x0e\x66ile_ds_config\x18\x04 \x01(\x0b\x32%.tecton_proto.args.FileDataSourceArgsB\x05\x92M\x02\x08\x06H\x00R\x0c\x66ileDsConfig\x12`\n\x12redshift_ds_config\x18\x07 \x01(\x0b\x32).tecton_proto.args.RedshiftDataSourceArgsB\x05\x92M\x02\x08\x06H\x00R\x10redshiftDsConfig\x12\x63\n\x13snowflake_ds_config\x18\x08 \x01(\x0b\x32*.tecton_proto.args.SnowflakeDataSourceArgsB\x05\x92M\x02\x08\x06H\x00R\x11snowflakeDsConfig\x12^\n\x12spark_batch_config\x18\x0b \x01(\x0b\x32\'.tecton_proto.args.SparkBatchConfigArgsB\x05\x92M\x02\x08\x06H\x00R\x10sparkBatchConfig\x12]\n\x11kinesis_ds_config\x18\x05 \x01(\x0b\x32(.tecton_proto.args.KinesisDataSourceArgsB\x05\x92M\x02\x08\x07H\x01R\x0fkinesisDsConfig\x12W\n\x0fkafka_ds_config\x18\x06 \x01(\x0b\x32&.tecton_proto.args.KafkaDataSourceArgsB\x05\x92M\x02\x08\x07H\x01R\rkafkaDsConfig\x12\x61\n\x13spark_stream_config\x18\x0c \x01(\x0b\x32(.tecton_proto.args.SparkStreamConfigArgsB\x05\x92M\x02\x08\x07H\x01R\x11sparkStreamConfig\x12<\n\x06schema\x18\r \x01(\x0b\x32$.tecton_proto.common.SchemaContainerR\x06schema\x12>\n\x04type\x18\t \x01(\x0e\x32#.tecton_proto.common.DataSourceTypeB\x05\x92M\x02\x08\x03R\x04type\x12W\n\x13\x66orced_batch_schema\x18\x0e \x01(\x0b\x32 .tecton_proto.common.SparkSchemaB\x05\x92M\x02\x08\x01R\x11\x66orcedBatchSchema\x12Y\n\x14\x66orced_stream_schema\x18\x0f \x01(\x0b\x32 .tecton_proto.common.SparkSchemaB\x05\x92M\x02\x08\x01R\x12\x66orcedStreamSchemaB\x11\n\x0f\x62\x61tch_ds_configB\x12\n\x10stream_ds_configB\x13\n\x0f\x63om.tecton.argsP\x01')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n+tecton_proto/args/virtual_data_source.proto\x12\x11tecton_proto.args\x1a\"tecton_proto/args/basic_info.proto\x1a\x1ctecton_proto/common/id.proto\x1a&tecton_proto/common/spark_schema.proto\x1a*tecton_proto/common/schema_container.proto\x1a+tecton_proto/common/framework_version.proto\x1a*tecton_proto/common/data_source_type.proto\x1a#tecton_proto/args/data_source.proto\x1a$tecton_proto/args/diff_options.proto\"\xb6\x0b\n\x15VirtualDataSourceArgs\x12L\n\x16virtual_data_source_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x13virtualDataSourceId\x12\x37\n\x04info\x18\x02 \x01(\x0b\x32\x1c.tecton_proto.args.BasicInfoB\x05\x92M\x02\x10\x01R\x04info\x12\x46\n\x07version\x18\n \x01(\x0e\x32%.tecton_proto.common.FrameworkVersionB\x05\x92M\x02\x08\x05R\x07version\x12.\n\x0fprevent_destroy\x18\x10 \x01(\x08\x42\x05\x92M\x02\x08\x01R\x0epreventDestroy\x12T\n\x0ehive_ds_config\x18\x03 \x01(\x0b\x32%.tecton_proto.args.HiveDataSourceArgsB\x05\x92M\x02\x08\x06H\x00R\x0chiveDsConfig\x12T\n\x0e\x66ile_ds_config\x18\x04 \x01(\x0b\x32%.tecton_proto.args.FileDataSourceArgsB\x05\x92M\x02\x08\x06H\x00R\x0c\x66ileDsConfig\x12`\n\x12redshift_ds_config\x18\x07 \x01(\x0b\x32).tecton_proto.args.RedshiftDataSourceArgsB\x05\x92M\x02\x08\x06H\x00R\x10redshiftDsConfig\x12\x63\n\x13snowflake_ds_config\x18\x08 \x01(\x0b\x32*.tecton_proto.args.SnowflakeDataSourceArgsB\x05\x92M\x02\x08\x06H\x00R\x11snowflakeDsConfig\x12^\n\x12spark_batch_config\x18\x0b \x01(\x0b\x32\'.tecton_proto.args.SparkBatchConfigArgsB\x05\x92M\x02\x08\x06H\x00R\x10sparkBatchConfig\x12W\n\x0funity_ds_config\x18\x11 \x01(\x0b\x32&.tecton_proto.args.UnityDataSourceArgsB\x05\x92M\x02\x08\x06H\x00R\runityDsConfig\x12]\n\x11kinesis_ds_config\x18\x05 \x01(\x0b\x32(.tecton_proto.args.KinesisDataSourceArgsB\x05\x92M\x02\x08\x07H\x01R\x0fkinesisDsConfig\x12W\n\x0fkafka_ds_config\x18\x06 \x01(\x0b\x32&.tecton_proto.args.KafkaDataSourceArgsB\x05\x92M\x02\x08\x07H\x01R\rkafkaDsConfig\x12\x61\n\x13spark_stream_config\x18\x0c \x01(\x0b\x32(.tecton_proto.args.SparkStreamConfigArgsB\x05\x92M\x02\x08\x07H\x01R\x11sparkStreamConfig\x12<\n\x06schema\x18\r \x01(\x0b\x32$.tecton_proto.common.SchemaContainerR\x06schema\x12>\n\x04type\x18\t \x01(\x0e\x32#.tecton_proto.common.DataSourceTypeB\x05\x92M\x02\x08\x03R\x04type\x12W\n\x13\x66orced_batch_schema\x18\x0e \x01(\x0b\x32 .tecton_proto.common.SparkSchemaB\x05\x92M\x02\x08\x01R\x11\x66orcedBatchSchema\x12Y\n\x14\x66orced_stream_schema\x18\x0f \x01(\x0b\x32 .tecton_proto.common.SparkSchemaB\x05\x92M\x02\x08\x01R\x12\x66orcedStreamSchemaB\x11\n\x0f\x62\x61tch_ds_configB\x12\n\x10stream_ds_configB\x13\n\x0f\x63om.tecton.argsP\x01')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.args.virtual_data_source_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\017com.tecton.argsP\001'
@@ -41,22 +41,24 @@
   _VIRTUALDATASOURCEARGS.fields_by_name['file_ds_config']._serialized_options = b'\222M\002\010\006'
   _VIRTUALDATASOURCEARGS.fields_by_name['redshift_ds_config']._options = None
   _VIRTUALDATASOURCEARGS.fields_by_name['redshift_ds_config']._serialized_options = b'\222M\002\010\006'
   _VIRTUALDATASOURCEARGS.fields_by_name['snowflake_ds_config']._options = None
   _VIRTUALDATASOURCEARGS.fields_by_name['snowflake_ds_config']._serialized_options = b'\222M\002\010\006'
   _VIRTUALDATASOURCEARGS.fields_by_name['spark_batch_config']._options = None
   _VIRTUALDATASOURCEARGS.fields_by_name['spark_batch_config']._serialized_options = b'\222M\002\010\006'
+  _VIRTUALDATASOURCEARGS.fields_by_name['unity_ds_config']._options = None
+  _VIRTUALDATASOURCEARGS.fields_by_name['unity_ds_config']._serialized_options = b'\222M\002\010\006'
   _VIRTUALDATASOURCEARGS.fields_by_name['kinesis_ds_config']._options = None
   _VIRTUALDATASOURCEARGS.fields_by_name['kinesis_ds_config']._serialized_options = b'\222M\002\010\007'
   _VIRTUALDATASOURCEARGS.fields_by_name['kafka_ds_config']._options = None
   _VIRTUALDATASOURCEARGS.fields_by_name['kafka_ds_config']._serialized_options = b'\222M\002\010\007'
   _VIRTUALDATASOURCEARGS.fields_by_name['spark_stream_config']._options = None
   _VIRTUALDATASOURCEARGS.fields_by_name['spark_stream_config']._serialized_options = b'\222M\002\010\007'
   _VIRTUALDATASOURCEARGS.fields_by_name['type']._options = None
   _VIRTUALDATASOURCEARGS.fields_by_name['type']._serialized_options = b'\222M\002\010\003'
   _VIRTUALDATASOURCEARGS.fields_by_name['forced_batch_schema']._options = None
   _VIRTUALDATASOURCEARGS.fields_by_name['forced_batch_schema']._serialized_options = b'\222M\002\010\001'
   _VIRTUALDATASOURCEARGS.fields_by_name['forced_stream_schema']._options = None
   _VIRTUALDATASOURCEARGS.fields_by_name['forced_stream_schema']._serialized_options = b'\222M\002\010\001'
   _VIRTUALDATASOURCEARGS._serialized_start=381
-  _VIRTUALDATASOURCEARGS._serialized_end=1754
+  _VIRTUALDATASOURCEARGS._serialized_end=1843
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/auth/acl_pb2.py` & `tecton-0.7.0rc0/tecton_proto/auth/acl_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/auth/authorization_service_pb2.py` & `tecton-0.7.0rc0/tecton_proto/auth/authorization_service_pb2.py`

 * *Files 11% similar despite different names*

```diff
@@ -10,101 +10,114 @@
 
 _sym_db = _symbol_database.Default()
 
 
 from google.api import annotations_pb2 as google_dot_api_dot_annotations__pb2
 from tecton_proto.auth import principal_pb2 as tecton__proto_dot_auth_dot_principal__pb2
 from tecton_proto.auth import resource_pb2 as tecton__proto_dot_auth_dot_resource__pb2
+from tecton_proto.auth import resource_role_assignments_pb2 as tecton__proto_dot_auth_dot_resource__role__assignments__pb2
 from tecton_proto.auth import service_pb2 as tecton__proto_dot_auth_dot_service__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n-tecton_proto/auth/authorization_service.proto\x12\x11tecton_proto.auth\x1a\x1cgoogle/api/annotations.proto\x1a!tecton_proto/auth/principal.proto\x1a tecton_proto/auth/resource.proto\x1a\x1ftecton_proto/auth/service.proto\"\x11\n\x0fGetRolesRequest\"K\n\x10GetRolesResponse\x12\x37\n\x05roles\x18\x01 \x03(\x0b\x32!.tecton_proto.auth.RoleDefinitionR\x05roles\"\x85\x03\n\x0eRoleDefinition\x12\x0e\n\x02id\x18\x01 \x01(\tR\x02id\x12\x12\n\x04name\x18\x02 \x01(\tR\x04name\x12 \n\x0b\x64\x65scription\x18\x03 \x01(\tR\x0b\x64\x65scription\x12`\n\x1c\x61ssignable_on_resource_types\x18\x04 \x03(\x0e\x32\x1f.tecton_proto.auth.ResourceTypeR\x19\x61ssignableOnResourceTypes\x12\x63\n\x1d\x61ssignable_to_principal_types\x18\x05 \x03(\x0e\x32 .tecton_proto.auth.PrincipalTypeR\x1a\x61ssignableToPrincipalTypes\x12I\n\x0bpermissions\x18\x06 \x03(\x0b\x32\'.tecton_proto.auth.PermissionDefinitionR\x0bpermissions\x12\x1b\n\tlegacy_id\x18\x07 \x01(\tR\x08legacyId\"m\n\x14PermissionDefinition\x12\x0e\n\x02id\x18\x01 \x01(\tR\x02id\x12 \n\x0b\x64\x65scription\x18\x02 \x01(\tR\x0b\x64\x65scription\x12#\n\ris_authorized\x18\x03 \x01(\x08R\x0cisAuthorized\"\x84\x02\n\x17GetAssignedRolesRequest\x12G\n\x0eprincipal_type\x18\x01 \x01(\x0e\x32 .tecton_proto.auth.PrincipalTypeR\rprincipalType\x12!\n\x0cprincipal_id\x18\x02 \x01(\tR\x0bprincipalId\x12\x44\n\rresource_type\x18\x03 \x01(\x0e\x32\x1f.tecton_proto.auth.ResourceTypeR\x0cresourceType\x12!\n\x0cresource_ids\x18\x04 \x03(\tR\x0bresourceIds\x12\x14\n\x05roles\x18\x05 \x03(\tR\x05roles\"^\n\x18GetAssignedRolesResponse\x12\x42\n\x0b\x61ssignments\x18\x01 \x03(\x0b\x32 .tecton_proto.auth.ResourceRolesR\x0b\x61ssignments\"\xc5\x01\n\x16GetIsAuthorizedRequest\x12G\n\x0eprincipal_type\x18\x01 \x01(\x0e\x32 .tecton_proto.auth.PrincipalTypeR\rprincipalType\x12!\n\x0cprincipal_id\x18\x02 \x01(\tR\x0bprincipalId\x12?\n\x0bpermissions\x18\x03 \x03(\x0b\x32\x1d.tecton_proto.auth.PermissionR\x0bpermissions\"Z\n\x17GetIsAuthorizedResponse\x12?\n\x0bpermissions\x18\x01 \x03(\x0b\x32\x1d.tecton_proto.auth.PermissionR\x0bpermissions\"F\n\rResourceRoles\x12\x1f\n\x0bresource_id\x18\x01 \x01(\tR\nresourceId\x12\x14\n\x05roles\x18\x02 \x03(\tR\x05roles\"U\n\x12\x41ssignRolesRequest\x12?\n\x0b\x61ssignments\x18\x01 \x03(\x0b\x32\x1d.tecton_proto.auth.AssignmentR\x0b\x61ssignments\"\x15\n\x13\x41ssignRolesResponse\"W\n\x14UnassignRolesRequest\x12?\n\x0b\x61ssignments\x18\x01 \x03(\x0b\x32\x1d.tecton_proto.auth.AssignmentR\x0b\x61ssignments\"\x17\n\x15UnassignRolesResponse\"\xf3\x01\n\nAssignment\x12\x44\n\rresource_type\x18\x01 \x01(\x0e\x32\x1f.tecton_proto.auth.ResourceTypeR\x0cresourceType\x12\x1f\n\x0bresource_id\x18\x02 \x01(\tR\nresourceId\x12\x12\n\x04role\x18\x03 \x01(\tR\x04role\x12G\n\x0eprincipal_type\x18\x04 \x01(\x0e\x32 .tecton_proto.auth.PrincipalTypeR\rprincipalType\x12!\n\x0cprincipal_id\x18\x05 \x01(\tR\x0bprincipalId\"\x80\x02\n\x15\x41ssignRolesPutRequest\x12\x44\n\rresource_type\x18\x01 \x01(\x0e\x32\x1f.tecton_proto.auth.ResourceTypeR\x0cresourceType\x12\x1f\n\x0bresource_id\x18\x02 \x01(\tR\nresourceId\x12G\n\x0eprincipal_type\x18\x03 \x01(\x0e\x32 .tecton_proto.auth.PrincipalTypeR\rprincipalType\x12!\n\x0cprincipal_id\x18\x04 \x01(\tR\x0bprincipalId\x12\x14\n\x05roles\x18\x05 \x03(\tR\x05roles\"\x18\n\x16\x41ssignRolesPutResponse\"\xe9\x01\n\x1dGetAuthorizedResourcesRequest\x12G\n\x0eprincipal_type\x18\x01 \x01(\x0e\x32 .tecton_proto.auth.PrincipalTypeR\rprincipalType\x12!\n\x0cprincipal_id\x18\x02 \x01(\tR\x0bprincipalId\x12\x44\n\rresource_type\x18\x03 \x01(\x0e\x32\x1f.tecton_proto.auth.ResourceTypeR\x0cresourceType\x12\x16\n\x06\x61\x63tion\x18\x04 \x01(\tR\x06\x61\x63tion\"{\n\x1eGetAuthorizedResourcesResponse\x12Y\n\x14\x61uthorized_resources\x18\x01 \x03(\x0b\x32&.tecton_proto.auth.AuthorizedResourcesR\x13\x61uthorizedResources\"P\n\x13\x41uthorizedResources\x12\x1f\n\x0bresource_id\x18\x01 \x01(\tR\nresourceId\x12\x18\n\x07\x61\x63tions\x18\x02 \x03(\tR\x07\x61\x63tions\"\xe6\x01\n\x1cGetAssignedPrincipalsRequest\x12\x44\n\rresource_type\x18\x01 \x01(\x0e\x32\x1f.tecton_proto.auth.ResourceTypeR\x0cresourceType\x12\x1f\n\x0bresource_id\x18\x02 \x01(\tR\nresourceId\x12\x14\n\x05roles\x18\x03 \x03(\tR\x05roles\x12I\n\x0fprincipal_types\x18\x04 \x03(\x0e\x32 .tecton_proto.auth.PrincipalTypeR\x0eprincipalTypes\"e\n\x1dGetAssignedPrincipalsResponse\x12\x44\n\x0b\x61ssignments\x18\x01 \x03(\x0b\x32\".tecton_proto.auth.AssignmentBasicR\x0b\x61ssignments\"h\n\x0f\x41ssignmentBasic\x12?\n\tprincipal\x18\x01 \x01(\x0b\x32!.tecton_proto.auth.PrincipalBasicR\tprincipal\x12\x14\n\x05roles\x18\x02 \x03(\tR\x05roles\"\xee\x01\n\x18GetAppPermissionsRequest\x12G\n\x0eprincipal_type\x18\x01 \x01(\x0e\x32 .tecton_proto.auth.PrincipalTypeR\rprincipalType\x12!\n\x0cprincipal_id\x18\x02 \x01(\tR\x0bprincipalId\x12\x66\n\x19resource_type_permissions\x18\x05 \x03(\x0b\x32*.tecton_proto.auth.ResourceTypePermissionsR\x17resourceTypePermissions\"y\n\x17ResourceTypePermissions\x12\x44\n\rresource_type\x18\x01 \x01(\x0e\x32\x1f.tecton_proto.auth.ResourceTypeR\x0cresourceType\x12\x18\n\x07\x61\x63tions\x18\x02 \x03(\tR\x07\x61\x63tions\"\x93\x01\n\x19GetAppPermissionsResponse\x12v\n\x1fresource_type_permission_values\x18\x01 \x03(\x0b\x32/.tecton_proto.auth.ResourceTypePermissionValuesR\x1cresourceTypePermissionValues\"\xb5\x01\n\x1cResourceTypePermissionValues\x12\x44\n\rresource_type\x18\x01 \x01(\x0e\x32\x1f.tecton_proto.auth.ResourceTypeR\x0cresourceType\x12O\n\x11permission_values\x18\x02 \x03(\x0b\x32\".tecton_proto.auth.PermissionValueR\x10permissionValues\"\x8b\x01\n\nPermission\x12\x44\n\rresource_type\x18\x01 \x01(\x0e\x32\x1f.tecton_proto.auth.ResourceTypeR\x0cresourceType\x12\x1f\n\x0bresource_id\x18\x02 \x01(\tR\nresourceId\x12\x16\n\x06\x61\x63tion\x18\x03 \x01(\tR\x06\x61\x63tion\"N\n\x0fPermissionValue\x12\x16\n\x06\x61\x63tion\x18\x01 \x01(\tR\x06\x61\x63tion\x12#\n\ris_authorized\x18\x02 \x01(\x08R\x0cisAuthorized\"\x97\x02\n\x1eGetWorkspacePermissionsRequest\x12G\n\x0eprincipal_type\x18\x01 \x01(\x0e\x32 .tecton_proto.auth.PrincipalTypeR\rprincipalType\x12!\n\x0cprincipal_id\x18\x02 \x01(\tR\x0bprincipalId\x12!\n\x0cworkspace_id\x18\x03 \x01(\tR\x0bworkspaceId\x12\x66\n\x19resource_type_permissions\x18\x04 \x03(\x0b\x32*.tecton_proto.auth.ResourceTypePermissionsR\x17resourceTypePermissions\"\x99\x01\n\x1fGetWorkspacePermissionsResponse\x12v\n\x1fresource_type_permission_values\x18\x01 \x03(\x0b\x32/.tecton_proto.auth.ResourceTypePermissionValuesR\x1cresourceTypePermissionValues2\xe5\r\n\x14\x41uthorizationService\x12\x8b\x01\n\x08GetRoles\x12\".tecton_proto.auth.GetRolesRequest\x1a#.tecton_proto.auth.GetRolesResponse\"6\x82\xd3\xe4\x93\x02(\"#/v1/authorization-service/get-roles:\x01*\xa2\xbc\xe6\xc0\x05\x02\x10\x01\x12\xac\x01\n\x10GetAssignedRoles\x12*.tecton_proto.auth.GetAssignedRolesRequest\x1a+.tecton_proto.auth.GetAssignedRolesResponse\"?\x82\xd3\xe4\x93\x02\x31\",/v1/authorization-service/get-assigned-roles:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\x97\x01\n\x0b\x41ssignRoles\x12%.tecton_proto.auth.AssignRolesRequest\x1a&.tecton_proto.auth.AssignRolesResponse\"9\x82\xd3\xe4\x93\x02+\"&/v1/authorization-service/assign-roles:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\x9f\x01\n\rUnassignRoles\x12\'.tecton_proto.auth.UnassignRolesRequest\x1a(.tecton_proto.auth.UnassignRolesResponse\";\x82\xd3\xe4\x93\x02-\"(/v1/authorization-service/unassign-roles:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xa0\x01\n\x0e\x41ssignRolesPut\x12(.tecton_proto.auth.AssignRolesPutRequest\x1a).tecton_proto.auth.AssignRolesPutResponse\"9\x82\xd3\xe4\x93\x02+\x1a&/v1/authorization-service/assign-roles:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xc4\x01\n\x16GetAuthorizedResources\x12\x30.tecton_proto.auth.GetAuthorizedResourcesRequest\x1a\x31.tecton_proto.auth.GetAuthorizedResourcesResponse\"E\x82\xd3\xe4\x93\x02\x37\"2/v1/authorization-service/get-authorized-resources:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xc0\x01\n\x15GetAssignedPrincipals\x12/.tecton_proto.auth.GetAssignedPrincipalsRequest\x1a\x30.tecton_proto.auth.GetAssignedPrincipalsResponse\"D\x82\xd3\xe4\x93\x02\x36\"1/v1/authorization-service/get-assigned-principals:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xb0\x01\n\x11GetAppPermissions\x12+.tecton_proto.auth.GetAppPermissionsRequest\x1a,.tecton_proto.auth.GetAppPermissionsResponse\"@\x82\xd3\xe4\x93\x02\x32\"-/v1/authorization-service/get-app-permissions:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xc8\x01\n\x17GetWorkspacePermissions\x12\x31.tecton_proto.auth.GetWorkspacePermissionsRequest\x1a\x32.tecton_proto.auth.GetWorkspacePermissionsResponse\"F\x82\xd3\xe4\x93\x02\x38\"3/v1/authorization-service/get-workspace-permissions:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xa8\x01\n\x0fGetIsAuthorized\x12).tecton_proto.auth.GetIsAuthorizedRequest\x1a*.tecton_proto.auth.GetIsAuthorizedResponse\">\x82\xd3\xe4\x93\x02\x30\"+/v1/authorization-service/get-is-authorized:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x42V\n\x0f\x63om.tecton.authB\x19\x41uthorizationServiceProtoP\x01Z&github.com/tecton-ai/tecton_proto/auth')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n-tecton_proto/auth/authorization_service.proto\x12\x11tecton_proto.auth\x1a\x1cgoogle/api/annotations.proto\x1a!tecton_proto/auth/principal.proto\x1a tecton_proto/auth/resource.proto\x1a\x31tecton_proto/auth/resource_role_assignments.proto\x1a\x1ftecton_proto/auth/service.proto\"\x11\n\x0fGetRolesRequest\"K\n\x10GetRolesResponse\x12\x37\n\x05roles\x18\x01 \x03(\x0b\x32!.tecton_proto.auth.RoleDefinitionR\x05roles\"\x85\x03\n\x0eRoleDefinition\x12\x0e\n\x02id\x18\x01 \x01(\tR\x02id\x12\x12\n\x04name\x18\x02 \x01(\tR\x04name\x12 \n\x0b\x64\x65scription\x18\x03 \x01(\tR\x0b\x64\x65scription\x12`\n\x1c\x61ssignable_on_resource_types\x18\x04 \x03(\x0e\x32\x1f.tecton_proto.auth.ResourceTypeR\x19\x61ssignableOnResourceTypes\x12\x63\n\x1d\x61ssignable_to_principal_types\x18\x05 \x03(\x0e\x32 .tecton_proto.auth.PrincipalTypeR\x1a\x61ssignableToPrincipalTypes\x12I\n\x0bpermissions\x18\x06 \x03(\x0b\x32\'.tecton_proto.auth.PermissionDefinitionR\x0bpermissions\x12\x1b\n\tlegacy_id\x18\x07 \x01(\tR\x08legacyId\"m\n\x14PermissionDefinition\x12\x0e\n\x02id\x18\x01 \x01(\tR\x02id\x12 \n\x0b\x64\x65scription\x18\x02 \x01(\tR\x0b\x64\x65scription\x12#\n\ris_authorized\x18\x03 \x01(\x08R\x0cisAuthorized\"\x84\x02\n\x17GetAssignedRolesRequest\x12G\n\x0eprincipal_type\x18\x01 \x01(\x0e\x32 .tecton_proto.auth.PrincipalTypeR\rprincipalType\x12!\n\x0cprincipal_id\x18\x02 \x01(\tR\x0bprincipalId\x12\x44\n\rresource_type\x18\x03 \x01(\x0e\x32\x1f.tecton_proto.auth.ResourceTypeR\x0cresourceType\x12!\n\x0cresource_ids\x18\x04 \x03(\tR\x0bresourceIds\x12\x14\n\x05roles\x18\x05 \x03(\tR\x05roles\"k\n\x18GetAssignedRolesResponse\x12O\n\x0b\x61ssignments\x18\x01 \x03(\x0b\x32-.tecton_proto.auth.ResourceAndRoleAssignmentsR\x0b\x61ssignments\"\x85\x02\n\x18ListAssignedRolesRequest\x12G\n\x0eprincipal_type\x18\x01 \x01(\x0e\x32 .tecton_proto.auth.PrincipalTypeR\rprincipalType\x12!\n\x0cprincipal_id\x18\x02 \x01(\tR\x0bprincipalId\x12\x44\n\rresource_type\x18\x03 \x01(\x0e\x32\x1f.tecton_proto.auth.ResourceTypeR\x0cresourceType\x12!\n\x0cresource_ids\x18\x04 \x03(\tR\x0bresourceIds\x12\x14\n\x05roles\x18\x05 \x03(\tR\x05roles\"n\n\x19ListAssignedRolesResponse\x12Q\n\x0b\x61ssignments\x18\x01 \x03(\x0b\x32/.tecton_proto.auth.ResourceAndRoleAssignmentsV2R\x0b\x61ssignments\"\xc5\x01\n\x16GetIsAuthorizedRequest\x12G\n\x0eprincipal_type\x18\x01 \x01(\x0e\x32 .tecton_proto.auth.PrincipalTypeR\rprincipalType\x12!\n\x0cprincipal_id\x18\x02 \x01(\tR\x0bprincipalId\x12?\n\x0bpermissions\x18\x03 \x03(\x0b\x32\x1d.tecton_proto.auth.PermissionR\x0bpermissions\"Z\n\x17GetIsAuthorizedResponse\x12?\n\x0bpermissions\x18\x01 \x03(\x0b\x32\x1d.tecton_proto.auth.PermissionR\x0bpermissions\"U\n\x12\x41ssignRolesRequest\x12?\n\x0b\x61ssignments\x18\x01 \x03(\x0b\x32\x1d.tecton_proto.auth.AssignmentR\x0b\x61ssignments\"\x15\n\x13\x41ssignRolesResponse\"W\n\x14UnassignRolesRequest\x12?\n\x0b\x61ssignments\x18\x01 \x03(\x0b\x32\x1d.tecton_proto.auth.AssignmentR\x0b\x61ssignments\"\x17\n\x15UnassignRolesResponse\"\xf3\x01\n\nAssignment\x12\x44\n\rresource_type\x18\x01 \x01(\x0e\x32\x1f.tecton_proto.auth.ResourceTypeR\x0cresourceType\x12\x1f\n\x0bresource_id\x18\x02 \x01(\tR\nresourceId\x12\x12\n\x04role\x18\x03 \x01(\tR\x04role\x12G\n\x0eprincipal_type\x18\x04 \x01(\x0e\x32 .tecton_proto.auth.PrincipalTypeR\rprincipalType\x12!\n\x0cprincipal_id\x18\x05 \x01(\tR\x0bprincipalId\"\x80\x02\n\x15\x41ssignRolesPutRequest\x12\x44\n\rresource_type\x18\x01 \x01(\x0e\x32\x1f.tecton_proto.auth.ResourceTypeR\x0cresourceType\x12\x1f\n\x0bresource_id\x18\x02 \x01(\tR\nresourceId\x12G\n\x0eprincipal_type\x18\x03 \x01(\x0e\x32 .tecton_proto.auth.PrincipalTypeR\rprincipalType\x12!\n\x0cprincipal_id\x18\x04 \x01(\tR\x0bprincipalId\x12\x14\n\x05roles\x18\x05 \x03(\tR\x05roles\"\x18\n\x16\x41ssignRolesPutResponse\"\xe9\x01\n\x1dGetAuthorizedResourcesRequest\x12G\n\x0eprincipal_type\x18\x01 \x01(\x0e\x32 .tecton_proto.auth.PrincipalTypeR\rprincipalType\x12!\n\x0cprincipal_id\x18\x02 \x01(\tR\x0bprincipalId\x12\x44\n\rresource_type\x18\x03 \x01(\x0e\x32\x1f.tecton_proto.auth.ResourceTypeR\x0cresourceType\x12\x16\n\x06\x61\x63tion\x18\x04 \x01(\tR\x06\x61\x63tion\"{\n\x1eGetAuthorizedResourcesResponse\x12Y\n\x14\x61uthorized_resources\x18\x01 \x03(\x0b\x32&.tecton_proto.auth.AuthorizedResourcesR\x13\x61uthorizedResources\"P\n\x13\x41uthorizedResources\x12\x1f\n\x0bresource_id\x18\x01 \x01(\tR\nresourceId\x12\x18\n\x07\x61\x63tions\x18\x02 \x03(\tR\x07\x61\x63tions\"\xe6\x01\n\x1cGetAssignedPrincipalsRequest\x12\x44\n\rresource_type\x18\x01 \x01(\x0e\x32\x1f.tecton_proto.auth.ResourceTypeR\x0cresourceType\x12\x1f\n\x0bresource_id\x18\x02 \x01(\tR\nresourceId\x12\x14\n\x05roles\x18\x03 \x03(\tR\x05roles\x12I\n\x0fprincipal_types\x18\x04 \x03(\x0e\x32 .tecton_proto.auth.PrincipalTypeR\x0eprincipalTypes\"e\n\x1dGetAssignedPrincipalsResponse\x12\x44\n\x0b\x61ssignments\x18\x01 \x03(\x0b\x32\".tecton_proto.auth.AssignmentBasicR\x0b\x61ssignments\"\xe7\x01\n\x1dListAssignedPrincipalsRequest\x12\x44\n\rresource_type\x18\x01 \x01(\x0e\x32\x1f.tecton_proto.auth.ResourceTypeR\x0cresourceType\x12\x1f\n\x0bresource_id\x18\x02 \x01(\tR\nresourceId\x12\x14\n\x05roles\x18\x03 \x03(\tR\x05roles\x12I\n\x0fprincipal_types\x18\x04 \x03(\x0e\x32 .tecton_proto.auth.PrincipalTypeR\x0eprincipalTypes\"h\n\x1eListAssignedPrincipalsResponse\x12\x46\n\x0b\x61ssignments\x18\x01 \x03(\x0b\x32$.tecton_proto.auth.AssignmentBasicV2R\x0b\x61ssignments\"\xc2\x01\n\x0f\x41ssignmentBasic\x12?\n\tprincipal\x18\x01 \x01(\x0b\x32!.tecton_proto.auth.PrincipalBasicR\tprincipal\x12\x14\n\x05roles\x18\x02 \x03(\tR\x05roles\x12X\n\x10role_assignments\x18\x03 \x03(\x0b\x32-.tecton_proto.auth.ResourceAndRoleAssignmentsR\x0froleAssignments\"\xb0\x01\n\x11\x41ssignmentBasicV2\x12?\n\tprincipal\x18\x01 \x01(\x0b\x32!.tecton_proto.auth.PrincipalBasicR\tprincipal\x12Z\n\x10role_assignments\x18\x02 \x03(\x0b\x32/.tecton_proto.auth.ResourceAndRoleAssignmentsV2R\x0froleAssignments\"\xee\x01\n\x18GetAppPermissionsRequest\x12G\n\x0eprincipal_type\x18\x01 \x01(\x0e\x32 .tecton_proto.auth.PrincipalTypeR\rprincipalType\x12!\n\x0cprincipal_id\x18\x02 \x01(\tR\x0bprincipalId\x12\x66\n\x19resource_type_permissions\x18\x05 \x03(\x0b\x32*.tecton_proto.auth.ResourceTypePermissionsR\x17resourceTypePermissions\"y\n\x17ResourceTypePermissions\x12\x44\n\rresource_type\x18\x01 \x01(\x0e\x32\x1f.tecton_proto.auth.ResourceTypeR\x0cresourceType\x12\x18\n\x07\x61\x63tions\x18\x02 \x03(\tR\x07\x61\x63tions\"\x93\x01\n\x19GetAppPermissionsResponse\x12v\n\x1fresource_type_permission_values\x18\x01 \x03(\x0b\x32/.tecton_proto.auth.ResourceTypePermissionValuesR\x1cresourceTypePermissionValues\"\xb5\x01\n\x1cResourceTypePermissionValues\x12\x44\n\rresource_type\x18\x01 \x01(\x0e\x32\x1f.tecton_proto.auth.ResourceTypeR\x0cresourceType\x12O\n\x11permission_values\x18\x02 \x03(\x0b\x32\".tecton_proto.auth.PermissionValueR\x10permissionValues\"\x8b\x01\n\nPermission\x12\x44\n\rresource_type\x18\x01 \x01(\x0e\x32\x1f.tecton_proto.auth.ResourceTypeR\x0cresourceType\x12\x1f\n\x0bresource_id\x18\x02 \x01(\tR\nresourceId\x12\x16\n\x06\x61\x63tion\x18\x03 \x01(\tR\x06\x61\x63tion\"N\n\x0fPermissionValue\x12\x16\n\x06\x61\x63tion\x18\x01 \x01(\tR\x06\x61\x63tion\x12#\n\ris_authorized\x18\x02 \x01(\x08R\x0cisAuthorized\"\x97\x02\n\x1eGetWorkspacePermissionsRequest\x12G\n\x0eprincipal_type\x18\x01 \x01(\x0e\x32 .tecton_proto.auth.PrincipalTypeR\rprincipalType\x12!\n\x0cprincipal_id\x18\x02 \x01(\tR\x0bprincipalId\x12!\n\x0cworkspace_id\x18\x03 \x01(\tR\x0bworkspaceId\x12\x66\n\x19resource_type_permissions\x18\x04 \x03(\x0b\x32*.tecton_proto.auth.ResourceTypePermissionsR\x17resourceTypePermissions\"\x99\x01\n\x1fGetWorkspacePermissionsResponse\x12v\n\x1fresource_type_permission_values\x18\x01 \x03(\x0b\x32/.tecton_proto.auth.ResourceTypePermissionValuesR\x1cresourceTypePermissionValues2\xad\x11\n\x14\x41uthorizationService\x12\x8b\x01\n\x08GetRoles\x12\".tecton_proto.auth.GetRolesRequest\x1a#.tecton_proto.auth.GetRolesResponse\"6\x82\xd3\xe4\x93\x02(\"#/v1/authorization-service/get-roles:\x01*\xa2\xbc\xe6\xc0\x05\x02\x10\x01\x12\xac\x01\n\x10GetAssignedRoles\x12*.tecton_proto.auth.GetAssignedRolesRequest\x1a+.tecton_proto.auth.GetAssignedRolesResponse\"?\x82\xd3\xe4\x93\x02\x31\",/v1/authorization-service/get-assigned-roles:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xb0\x01\n\x11ListAssignedRoles\x12+.tecton_proto.auth.ListAssignedRolesRequest\x1a,.tecton_proto.auth.ListAssignedRolesResponse\"@\x82\xd3\xe4\x93\x02\x32\"-/v1/authorization-service/list-assigned-roles:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xb0\x01\n\x0b\x41ssignRoles\x12%.tecton_proto.auth.AssignRolesRequest\x1a&.tecton_proto.auth.AssignRolesResponse\"R\x82\xd3\xe4\x93\x02+\"&/v1/authorization-service/assign-roles:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\xaa\xbc\xe6\xc0\x05\x13\x08\x01\x12\x0c\x61ssign_roles\x1a\x01\x31\x12\xba\x01\n\rUnassignRoles\x12\'.tecton_proto.auth.UnassignRolesRequest\x1a(.tecton_proto.auth.UnassignRolesResponse\"V\x82\xd3\xe4\x93\x02-\"(/v1/authorization-service/unassign-roles:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\xaa\xbc\xe6\xc0\x05\x15\x08\x01\x12\x0eunassign_roles\x1a\x01\x31\x12\xba\x01\n\x0e\x41ssignRolesPut\x12(.tecton_proto.auth.AssignRolesPutRequest\x1a).tecton_proto.auth.AssignRolesPutResponse\"S\x82\xd3\xe4\x93\x02+\x1a&/v1/authorization-service/assign-roles:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\xaa\xbc\xe6\xc0\x05\x14\x08\x01\x12\x10\x61ssign_roles_put\x12\xc4\x01\n\x16GetAuthorizedResources\x12\x30.tecton_proto.auth.GetAuthorizedResourcesRequest\x1a\x31.tecton_proto.auth.GetAuthorizedResourcesResponse\"E\x82\xd3\xe4\x93\x02\x37\"2/v1/authorization-service/get-authorized-resources:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xc0\x01\n\x15GetAssignedPrincipals\x12/.tecton_proto.auth.GetAssignedPrincipalsRequest\x1a\x30.tecton_proto.auth.GetAssignedPrincipalsResponse\"D\x82\xd3\xe4\x93\x02\x36\"1/v1/authorization-service/get-assigned-principals:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xc4\x01\n\x16ListAssignedPrincipals\x12\x30.tecton_proto.auth.ListAssignedPrincipalsRequest\x1a\x31.tecton_proto.auth.ListAssignedPrincipalsResponse\"E\x82\xd3\xe4\x93\x02\x37\"2/v1/authorization-service/list-assigned-principals:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xb0\x01\n\x11GetAppPermissions\x12+.tecton_proto.auth.GetAppPermissionsRequest\x1a,.tecton_proto.auth.GetAppPermissionsResponse\"@\x82\xd3\xe4\x93\x02\x32\"-/v1/authorization-service/get-app-permissions:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xc8\x01\n\x17GetWorkspacePermissions\x12\x31.tecton_proto.auth.GetWorkspacePermissionsRequest\x1a\x32.tecton_proto.auth.GetWorkspacePermissionsResponse\"F\x82\xd3\xe4\x93\x02\x38\"3/v1/authorization-service/get-workspace-permissions:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xa8\x01\n\x0fGetIsAuthorized\x12).tecton_proto.auth.GetIsAuthorizedRequest\x1a*.tecton_proto.auth.GetIsAuthorizedResponse\">\x82\xd3\xe4\x93\x02\x30\"+/v1/authorization-service/get-is-authorized:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x42V\n\x0f\x63om.tecton.authB\x19\x41uthorizationServiceProtoP\x01Z&github.com/tecton-ai/tecton_proto/auth')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.auth.authorization_service_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\017com.tecton.authB\031AuthorizationServiceProtoP\001Z&github.com/tecton-ai/tecton_proto/auth'
   _AUTHORIZATIONSERVICE.methods_by_name['GetRoles']._options = None
   _AUTHORIZATIONSERVICE.methods_by_name['GetRoles']._serialized_options = b'\202\323\344\223\002(\"#/v1/authorization-service/get-roles:\001*\242\274\346\300\005\002\020\001'
   _AUTHORIZATIONSERVICE.methods_by_name['GetAssignedRoles']._options = None
   _AUTHORIZATIONSERVICE.methods_by_name['GetAssignedRoles']._serialized_options = b'\202\323\344\223\0021\",/v1/authorization-service/get-assigned-roles:\001*\242\274\346\300\005\002@\001'
+  _AUTHORIZATIONSERVICE.methods_by_name['ListAssignedRoles']._options = None
+  _AUTHORIZATIONSERVICE.methods_by_name['ListAssignedRoles']._serialized_options = b'\202\323\344\223\0022\"-/v1/authorization-service/list-assigned-roles:\001*\242\274\346\300\005\002@\001'
   _AUTHORIZATIONSERVICE.methods_by_name['AssignRoles']._options = None
-  _AUTHORIZATIONSERVICE.methods_by_name['AssignRoles']._serialized_options = b'\202\323\344\223\002+\"&/v1/authorization-service/assign-roles:\001*\242\274\346\300\005\002@\001'
+  _AUTHORIZATIONSERVICE.methods_by_name['AssignRoles']._serialized_options = b'\202\323\344\223\002+\"&/v1/authorization-service/assign-roles:\001*\242\274\346\300\005\002@\001\252\274\346\300\005\023\010\001\022\014assign_roles\032\0011'
   _AUTHORIZATIONSERVICE.methods_by_name['UnassignRoles']._options = None
-  _AUTHORIZATIONSERVICE.methods_by_name['UnassignRoles']._serialized_options = b'\202\323\344\223\002-\"(/v1/authorization-service/unassign-roles:\001*\242\274\346\300\005\002@\001'
+  _AUTHORIZATIONSERVICE.methods_by_name['UnassignRoles']._serialized_options = b'\202\323\344\223\002-\"(/v1/authorization-service/unassign-roles:\001*\242\274\346\300\005\002@\001\252\274\346\300\005\025\010\001\022\016unassign_roles\032\0011'
   _AUTHORIZATIONSERVICE.methods_by_name['AssignRolesPut']._options = None
-  _AUTHORIZATIONSERVICE.methods_by_name['AssignRolesPut']._serialized_options = b'\202\323\344\223\002+\032&/v1/authorization-service/assign-roles:\001*\242\274\346\300\005\002@\001'
+  _AUTHORIZATIONSERVICE.methods_by_name['AssignRolesPut']._serialized_options = b'\202\323\344\223\002+\032&/v1/authorization-service/assign-roles:\001*\242\274\346\300\005\002@\001\252\274\346\300\005\024\010\001\022\020assign_roles_put'
   _AUTHORIZATIONSERVICE.methods_by_name['GetAuthorizedResources']._options = None
   _AUTHORIZATIONSERVICE.methods_by_name['GetAuthorizedResources']._serialized_options = b'\202\323\344\223\0027\"2/v1/authorization-service/get-authorized-resources:\001*\242\274\346\300\005\002@\001'
   _AUTHORIZATIONSERVICE.methods_by_name['GetAssignedPrincipals']._options = None
   _AUTHORIZATIONSERVICE.methods_by_name['GetAssignedPrincipals']._serialized_options = b'\202\323\344\223\0026\"1/v1/authorization-service/get-assigned-principals:\001*\242\274\346\300\005\002@\001'
+  _AUTHORIZATIONSERVICE.methods_by_name['ListAssignedPrincipals']._options = None
+  _AUTHORIZATIONSERVICE.methods_by_name['ListAssignedPrincipals']._serialized_options = b'\202\323\344\223\0027\"2/v1/authorization-service/list-assigned-principals:\001*\242\274\346\300\005\002@\001'
   _AUTHORIZATIONSERVICE.methods_by_name['GetAppPermissions']._options = None
   _AUTHORIZATIONSERVICE.methods_by_name['GetAppPermissions']._serialized_options = b'\202\323\344\223\0022\"-/v1/authorization-service/get-app-permissions:\001*\242\274\346\300\005\002@\001'
   _AUTHORIZATIONSERVICE.methods_by_name['GetWorkspacePermissions']._options = None
   _AUTHORIZATIONSERVICE.methods_by_name['GetWorkspacePermissions']._serialized_options = b'\202\323\344\223\0028\"3/v1/authorization-service/get-workspace-permissions:\001*\242\274\346\300\005\002@\001'
   _AUTHORIZATIONSERVICE.methods_by_name['GetIsAuthorized']._options = None
   _AUTHORIZATIONSERVICE.methods_by_name['GetIsAuthorized']._serialized_options = b'\202\323\344\223\0020\"+/v1/authorization-service/get-is-authorized:\001*\242\274\346\300\005\002@\001'
-  _GETROLESREQUEST._serialized_start=200
-  _GETROLESREQUEST._serialized_end=217
-  _GETROLESRESPONSE._serialized_start=219
-  _GETROLESRESPONSE._serialized_end=294
-  _ROLEDEFINITION._serialized_start=297
-  _ROLEDEFINITION._serialized_end=686
-  _PERMISSIONDEFINITION._serialized_start=688
-  _PERMISSIONDEFINITION._serialized_end=797
-  _GETASSIGNEDROLESREQUEST._serialized_start=800
-  _GETASSIGNEDROLESREQUEST._serialized_end=1060
-  _GETASSIGNEDROLESRESPONSE._serialized_start=1062
-  _GETASSIGNEDROLESRESPONSE._serialized_end=1156
-  _GETISAUTHORIZEDREQUEST._serialized_start=1159
-  _GETISAUTHORIZEDREQUEST._serialized_end=1356
-  _GETISAUTHORIZEDRESPONSE._serialized_start=1358
-  _GETISAUTHORIZEDRESPONSE._serialized_end=1448
-  _RESOURCEROLES._serialized_start=1450
-  _RESOURCEROLES._serialized_end=1520
-  _ASSIGNROLESREQUEST._serialized_start=1522
-  _ASSIGNROLESREQUEST._serialized_end=1607
-  _ASSIGNROLESRESPONSE._serialized_start=1609
-  _ASSIGNROLESRESPONSE._serialized_end=1630
-  _UNASSIGNROLESREQUEST._serialized_start=1632
-  _UNASSIGNROLESREQUEST._serialized_end=1719
-  _UNASSIGNROLESRESPONSE._serialized_start=1721
-  _UNASSIGNROLESRESPONSE._serialized_end=1744
-  _ASSIGNMENT._serialized_start=1747
-  _ASSIGNMENT._serialized_end=1990
-  _ASSIGNROLESPUTREQUEST._serialized_start=1993
-  _ASSIGNROLESPUTREQUEST._serialized_end=2249
-  _ASSIGNROLESPUTRESPONSE._serialized_start=2251
-  _ASSIGNROLESPUTRESPONSE._serialized_end=2275
-  _GETAUTHORIZEDRESOURCESREQUEST._serialized_start=2278
-  _GETAUTHORIZEDRESOURCESREQUEST._serialized_end=2511
-  _GETAUTHORIZEDRESOURCESRESPONSE._serialized_start=2513
-  _GETAUTHORIZEDRESOURCESRESPONSE._serialized_end=2636
-  _AUTHORIZEDRESOURCES._serialized_start=2638
-  _AUTHORIZEDRESOURCES._serialized_end=2718
-  _GETASSIGNEDPRINCIPALSREQUEST._serialized_start=2721
-  _GETASSIGNEDPRINCIPALSREQUEST._serialized_end=2951
-  _GETASSIGNEDPRINCIPALSRESPONSE._serialized_start=2953
-  _GETASSIGNEDPRINCIPALSRESPONSE._serialized_end=3054
-  _ASSIGNMENTBASIC._serialized_start=3056
-  _ASSIGNMENTBASIC._serialized_end=3160
-  _GETAPPPERMISSIONSREQUEST._serialized_start=3163
-  _GETAPPPERMISSIONSREQUEST._serialized_end=3401
-  _RESOURCETYPEPERMISSIONS._serialized_start=3403
-  _RESOURCETYPEPERMISSIONS._serialized_end=3524
-  _GETAPPPERMISSIONSRESPONSE._serialized_start=3527
-  _GETAPPPERMISSIONSRESPONSE._serialized_end=3674
-  _RESOURCETYPEPERMISSIONVALUES._serialized_start=3677
-  _RESOURCETYPEPERMISSIONVALUES._serialized_end=3858
-  _PERMISSION._serialized_start=3861
-  _PERMISSION._serialized_end=4000
-  _PERMISSIONVALUE._serialized_start=4002
-  _PERMISSIONVALUE._serialized_end=4080
-  _GETWORKSPACEPERMISSIONSREQUEST._serialized_start=4083
-  _GETWORKSPACEPERMISSIONSREQUEST._serialized_end=4362
-  _GETWORKSPACEPERMISSIONSRESPONSE._serialized_start=4365
-  _GETWORKSPACEPERMISSIONSRESPONSE._serialized_end=4518
-  _AUTHORIZATIONSERVICE._serialized_start=4521
-  _AUTHORIZATIONSERVICE._serialized_end=6286
+  _GETROLESREQUEST._serialized_start=251
+  _GETROLESREQUEST._serialized_end=268
+  _GETROLESRESPONSE._serialized_start=270
+  _GETROLESRESPONSE._serialized_end=345
+  _ROLEDEFINITION._serialized_start=348
+  _ROLEDEFINITION._serialized_end=737
+  _PERMISSIONDEFINITION._serialized_start=739
+  _PERMISSIONDEFINITION._serialized_end=848
+  _GETASSIGNEDROLESREQUEST._serialized_start=851
+  _GETASSIGNEDROLESREQUEST._serialized_end=1111
+  _GETASSIGNEDROLESRESPONSE._serialized_start=1113
+  _GETASSIGNEDROLESRESPONSE._serialized_end=1220
+  _LISTASSIGNEDROLESREQUEST._serialized_start=1223
+  _LISTASSIGNEDROLESREQUEST._serialized_end=1484
+  _LISTASSIGNEDROLESRESPONSE._serialized_start=1486
+  _LISTASSIGNEDROLESRESPONSE._serialized_end=1596
+  _GETISAUTHORIZEDREQUEST._serialized_start=1599
+  _GETISAUTHORIZEDREQUEST._serialized_end=1796
+  _GETISAUTHORIZEDRESPONSE._serialized_start=1798
+  _GETISAUTHORIZEDRESPONSE._serialized_end=1888
+  _ASSIGNROLESREQUEST._serialized_start=1890
+  _ASSIGNROLESREQUEST._serialized_end=1975
+  _ASSIGNROLESRESPONSE._serialized_start=1977
+  _ASSIGNROLESRESPONSE._serialized_end=1998
+  _UNASSIGNROLESREQUEST._serialized_start=2000
+  _UNASSIGNROLESREQUEST._serialized_end=2087
+  _UNASSIGNROLESRESPONSE._serialized_start=2089
+  _UNASSIGNROLESRESPONSE._serialized_end=2112
+  _ASSIGNMENT._serialized_start=2115
+  _ASSIGNMENT._serialized_end=2358
+  _ASSIGNROLESPUTREQUEST._serialized_start=2361
+  _ASSIGNROLESPUTREQUEST._serialized_end=2617
+  _ASSIGNROLESPUTRESPONSE._serialized_start=2619
+  _ASSIGNROLESPUTRESPONSE._serialized_end=2643
+  _GETAUTHORIZEDRESOURCESREQUEST._serialized_start=2646
+  _GETAUTHORIZEDRESOURCESREQUEST._serialized_end=2879
+  _GETAUTHORIZEDRESOURCESRESPONSE._serialized_start=2881
+  _GETAUTHORIZEDRESOURCESRESPONSE._serialized_end=3004
+  _AUTHORIZEDRESOURCES._serialized_start=3006
+  _AUTHORIZEDRESOURCES._serialized_end=3086
+  _GETASSIGNEDPRINCIPALSREQUEST._serialized_start=3089
+  _GETASSIGNEDPRINCIPALSREQUEST._serialized_end=3319
+  _GETASSIGNEDPRINCIPALSRESPONSE._serialized_start=3321
+  _GETASSIGNEDPRINCIPALSRESPONSE._serialized_end=3422
+  _LISTASSIGNEDPRINCIPALSREQUEST._serialized_start=3425
+  _LISTASSIGNEDPRINCIPALSREQUEST._serialized_end=3656
+  _LISTASSIGNEDPRINCIPALSRESPONSE._serialized_start=3658
+  _LISTASSIGNEDPRINCIPALSRESPONSE._serialized_end=3762
+  _ASSIGNMENTBASIC._serialized_start=3765
+  _ASSIGNMENTBASIC._serialized_end=3959
+  _ASSIGNMENTBASICV2._serialized_start=3962
+  _ASSIGNMENTBASICV2._serialized_end=4138
+  _GETAPPPERMISSIONSREQUEST._serialized_start=4141
+  _GETAPPPERMISSIONSREQUEST._serialized_end=4379
+  _RESOURCETYPEPERMISSIONS._serialized_start=4381
+  _RESOURCETYPEPERMISSIONS._serialized_end=4502
+  _GETAPPPERMISSIONSRESPONSE._serialized_start=4505
+  _GETAPPPERMISSIONSRESPONSE._serialized_end=4652
+  _RESOURCETYPEPERMISSIONVALUES._serialized_start=4655
+  _RESOURCETYPEPERMISSIONVALUES._serialized_end=4836
+  _PERMISSION._serialized_start=4839
+  _PERMISSION._serialized_end=4978
+  _PERMISSIONVALUE._serialized_start=4980
+  _PERMISSIONVALUE._serialized_end=5058
+  _GETWORKSPACEPERMISSIONSREQUEST._serialized_start=5061
+  _GETWORKSPACEPERMISSIONSREQUEST._serialized_end=5340
+  _GETWORKSPACEPERMISSIONSRESPONSE._serialized_start=5343
+  _GETWORKSPACEPERMISSIONSRESPONSE._serialized_end=5496
+  _AUTHORIZATIONSERVICE._serialized_start=5499
+  _AUTHORIZATIONSERVICE._serialized_end=7720
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/auth/principal_pb2.py` & `tecton-0.7.0rc0/tecton_proto/auth/principal_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/auth/resource_pb2.py` & `tecton-0.7.0rc0/tecton_proto/auth/resource_pb2.py`

 * *Files 10% similar despite different names*

```diff
@@ -9,18 +9,18 @@
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n tecton_proto/auth/resource.proto\x12\x11tecton_proto.auth*\xc1\x03\n\x0cResourceType\x12\x19\n\x15RESOURCE_TYPE_UNKNOWN\x10\x00\x12\x16\n\x12RESOURCE_TYPE_USER\x10\x01\x12\x1e\n\x1aRESOURCE_TYPE_ORGANIZATION\x10\x02\x12\x1b\n\x17RESOURCE_TYPE_WORKSPACE\x10\x03\x12!\n\x1dRESOURCE_TYPE_SERVICE_ACCOUNT\x10\x04\x12\x17\n\x13RESOURCE_TYPE_GROUP\x10\x05\x12\x18\n\x14RESOURCE_TYPE_SECRET\x10\x06\x12\x18\n\x14RESOURCE_TYPE_ACCESS\x10\x07\x12%\n!RESOURCE_TYPE_DEPLOYMENT_SETTINGS\x10\x08\x12\x19\n\x15RESOURCE_TYPE_DATASET\x10\t\x12\x1a\n\x16RESOURCE_TYPE_FEATURES\x10\n\x12\x18\n\x14RESOURCE_TYPE_CONFIG\x10\x0b\x12\x18\n\x14RESOURCE_TYPE_METRIC\x10\x0c\x12\x16\n\x12RESOURCE_TYPE_DATA\x10\r\x12\'\n#RESOURCE_TYPE_MATERIALIZATION_STATE\x10\x0e\x42;\n\x0f\x63om.tecton.authP\x01Z&github.com/tecton-ai/tecton_proto/auth')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n tecton_proto/auth/resource.proto\x12\x11tecton_proto.auth*\x96\x04\n\x0cResourceType\x12\x19\n\x15RESOURCE_TYPE_UNKNOWN\x10\x00\x12\x16\n\x12RESOURCE_TYPE_USER\x10\x01\x12\x1e\n\x1aRESOURCE_TYPE_ORGANIZATION\x10\x02\x12\x1b\n\x17RESOURCE_TYPE_WORKSPACE\x10\x03\x12!\n\x1dRESOURCE_TYPE_SERVICE_ACCOUNT\x10\x04\x12!\n\x1dRESOURCE_TYPE_PRINCIPAL_GROUP\x10\x05\x12\x18\n\x14RESOURCE_TYPE_SECRET\x10\x06\x12\x18\n\x14RESOURCE_TYPE_ACCESS\x10\x07\x12%\n!RESOURCE_TYPE_DEPLOYMENT_SETTINGS\x10\x08\x12\x19\n\x15RESOURCE_TYPE_DATASET\x10\t\x12\x1a\n\x16RESOURCE_TYPE_FEATURES\x10\n\x12\x18\n\x14RESOURCE_TYPE_CONFIG\x10\x0b\x12\x18\n\x14RESOURCE_TYPE_METRIC\x10\x0c\x12\x16\n\x12RESOURCE_TYPE_DATA\x10\r\x12\'\n#RESOURCE_TYPE_MATERIALIZATION_STATE\x10\x0e\x12\x19\n\x15RESOURCE_TYPE_MEMBERS\x10\x0f\x12.\n*RESOURCE_TYPE_PRINCIPAL_GROUPS_MEMBERSHIPS\x10\x10\x42;\n\x0f\x63om.tecton.authP\x01Z&github.com/tecton-ai/tecton_proto/auth')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.auth.resource_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\017com.tecton.authP\001Z&github.com/tecton-ai/tecton_proto/auth'
   _RESOURCETYPE._serialized_start=56
-  _RESOURCETYPE._serialized_end=505
+  _RESOURCETYPE._serialized_end=590
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/auth/service_pb2.py` & `tecton-0.7.0rc0/tecton_proto/data/tecton_api_key_pb2.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,33 +1,29 @@
 # -*- coding: utf-8 -*-
 # Generated by the protocol buffer compiler.  DO NOT EDIT!
-# source: tecton_proto/auth/service.proto
+# source: tecton_proto/data/tecton_api_key.proto
 """Generated protocol buffer code."""
 from google.protobuf.internal import builder as _builder
 from google.protobuf import descriptor as _descriptor
 from google.protobuf import descriptor_pool as _descriptor_pool
 from google.protobuf import symbol_database as _symbol_database
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
-from google.protobuf import descriptor_pb2 as google_dot_protobuf_dot_descriptor__pb2
+from tecton_proto.auth import principal_pb2 as tecton__proto_dot_auth_dot_principal__pb2
+from tecton_proto.common import id_pb2 as tecton__proto_dot_common_dot_id__pb2
+from google.protobuf import timestamp_pb2 as google_dot_protobuf_dot_timestamp__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x1ftecton_proto/auth/service.proto\x12\x11tecton_proto.auth\x1a google/protobuf/descriptor.proto\"\xff\x03\n\x0c\x41uthMetadata\x12\x36\n\x13skip_authentication\x18\x01 \x01(\x08:\x05\x66\x61lseR\x12skipAuthentication\x12\x34\n\x12skip_authorization\x18\x02 \x01(\x08:\x05\x66\x61lseR\x11skipAuthorization\x12.\n\npermission\x18\x05 \x01(\t:\x0enot_configuredR\npermission\x12i\n\x1d\x61\x64vanced_permission_overrides\x18\x06 \x03(\x0b\x32%.tecton_proto.auth.PermissionOverrideR\x1b\x61\x64vancedPermissionOverrides\x12S\n\x12resource_reference\x18\x03 \x01(\x0b\x32$.tecton_proto.auth.ResourceReferenceR\x11resourceReference\x12\x45\n\x1cLEGACY_require_admin_api_key\x18\x07 \x01(\x08:\x05\x66\x61lseR\x18LEGACYRequireAdminApiKey\x12J\n\x1e\x64\x65\x66\x65r_authorization_to_service\x18\x08 \x01(\x08:\x05\x66\x61lseR\x1b\x64\x65\x66\x65rAuthorizationToService\"\xa0\x01\n\x12PermissionOverride\x12\x30\n\x14\x63ondition_field_path\x18\x01 \x01(\tR\x12\x63onditionFieldPath\x12\'\n\x0f\x63ondition_value\x18\x02 \x01(\tR\x0e\x63onditionValue\x12/\n\x13permission_override\x18\x03 \x01(\tR\x12permissionOverride\"c\n\x11ResourceReference\x12:\n\x04type\x18\x01 \x01(\x0e\x32&.tecton_proto.auth.ResourceRefTypeEnumR\x04type\x12\x12\n\x04path\x18\x02 \x01(\tR\x04path*\x84\x01\n\x13ResourceRefTypeEnum\x12\x1d\n\x19RESOURCE_REF_TYPE_UNKNOWN\x10\x00\x12$\n RESOURCE_REF_TYPE_WORKSPACE_NAME\x10\x01\x12(\n$RESOURCE_REF_TYPE_SERVICE_ACCOUNT_ID\x10\x02:g\n\rauth_metadata\x12\x1e.google.protobuf.MethodOptions\x18\xc4\xe7\x8cX \x01(\x0b\x32\x1f.tecton_proto.auth.AuthMetadataR\x0c\x61uthMetadata')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n&tecton_proto/data/tecton_api_key.proto\x12\x11tecton_proto.data\x1a!tecton_proto/auth/principal.proto\x1a\x1ctecton_proto/common/id.proto\x1a\x1fgoogle/protobuf/timestamp.proto\"\x9b\x03\n\x0cTectonApiKey\x12\'\n\x02id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x02id\x12\x1d\n\nhashed_key\x18\x02 \x01(\tR\thashedKey\x12 \n\x0b\x64\x65scription\x18\x03 \x01(\tR\x0b\x64\x65scription\x12\x1a\n\x08\x61rchived\x18\x04 \x01(\x08R\x08\x61rchived\x12\x1d\n\ncreated_by\x18\x05 \x01(\tR\tcreatedBy\x12!\n\x0cobscured_key\x18\x06 \x01(\tR\x0bobscuredKey\x12\x19\n\x08is_admin\x18\x07 \x01(\x08R\x07isAdmin\x12\x12\n\x04name\x18\x08 \x01(\tR\x04name\x12!\n\tis_active\x18\t \x01(\x08:\x04trueR\x08isActive\x12\x36\n\x07\x63reator\x18\n \x01(\x0b\x32\x1c.tecton_proto.auth.PrincipalR\x07\x63reator\x12\x39\n\ncreated_at\x18\x0b \x01(\x0b\x32\x1a.google.protobuf.TimestampR\tcreatedAtB\x13\n\x0f\x63om.tecton.dataP\x01')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
-_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.auth.service_pb2', globals())
+_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.data.tecton_api_key_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
-  google_dot_protobuf_dot_descriptor__pb2.MethodOptions.RegisterExtension(auth_metadata)
 
   DESCRIPTOR._options = None
-  _RESOURCEREFTYPEENUM._serialized_start=867
-  _RESOURCEREFTYPEENUM._serialized_end=999
-  _AUTHMETADATA._serialized_start=89
-  _AUTHMETADATA._serialized_end=600
-  _PERMISSIONOVERRIDE._serialized_start=603
-  _PERMISSIONOVERRIDE._serialized_end=763
-  _RESOURCEREFERENCE._serialized_start=765
-  _RESOURCEREFERENCE._serialized_end=864
+  DESCRIPTOR._serialized_options = b'\n\017com.tecton.dataP\001'
+  _TECTONAPIKEY._serialized_start=160
+  _TECTONAPIKEY._serialized_end=571
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/cli/repo_diff_pb2.py` & `tecton-0.7.0rc0/tecton_proto/cli/repo_diff_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/common/aggregation_function_pb2.py` & `tecton-0.7.0rc0/tecton_proto/common/aggregation_function_pb2.py`

 * *Files 10% similar despite different names*

```diff
@@ -9,24 +9,28 @@
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n.tecton_proto/common/aggregation_function.proto\x12\x13tecton_proto.common\"\x1b\n\x0bLastNParams\x12\x0c\n\x01n\x18\x01 \x01(\x05R\x01n\"\x1c\n\x0c\x46irstNParams\x12\x0c\n\x01n\x18\x01 \x01(\x05R\x01n\"\x9e\x01\n\x19\x41ggregationFunctionParams\x12\x39\n\x06last_n\x18\x01 \x01(\x0b\x32 .tecton_proto.common.LastNParamsH\x00R\x05lastN\x12<\n\x07\x66irst_n\x18\x02 \x01(\x0b\x32!.tecton_proto.common.FirstNParamsH\x00R\x06\x66irstNB\x08\n\x06params*\xbd\x04\n\x13\x41ggregationFunction\x12 \n\x1c\x41GGREGATION_FUNCTION_UNKNOWN\x10\x00\x12\x1e\n\x1a\x41GGREGATION_FUNCTION_COUNT\x10\x01\x12\x1c\n\x18\x41GGREGATION_FUNCTION_SUM\x10\x02\x12\x1d\n\x19\x41GGREGATION_FUNCTION_MEAN\x10\x03\x12\x1d\n\x19\x41GGREGATION_FUNCTION_LAST\x10\x05\x12\x1c\n\x18\x41GGREGATION_FUNCTION_MIN\x10\x06\x12\x1c\n\x18\x41GGREGATION_FUNCTION_MAX\x10\x07\x12(\n$AGGREGATION_FUNCTION_LAST_DISTINCT_N\x10\t\x12!\n\x1d\x41GGREGATION_FUNCTION_VAR_SAMP\x10\n\x12 \n\x1c\x41GGREGATION_FUNCTION_VAR_POP\x10\x0b\x12$\n AGGREGATION_FUNCTION_STDDEV_SAMP\x10\x0c\x12#\n\x1f\x41GGREGATION_FUNCTION_STDDEV_POP\x10\r\x12,\n(AGGREGATION_FUNCTION_LAST_NON_DISTINCT_N\x10\x0e\x12-\n)AGGREGATION_FUNCTION_FIRST_NON_DISTINCT_N\x10\x0f\x12)\n%AGGREGATION_FUNCTION_FIRST_DISTINCT_N\x10\x10\"\x04\x08\x04\x10\x04\"\x04\x08\x08\x10\x08\x42?\n\x11\x63om.tecton.commonP\x01Z(github.com/tecton-ai/tecton_proto/common')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n.tecton_proto/common/aggregation_function.proto\x12\x13tecton_proto.common\"\x1b\n\x0bLastNParams\x12\x0c\n\x01n\x18\x01 \x01(\x05R\x01n\"\x1c\n\x0c\x46irstNParams\x12\x0c\n\x01n\x18\x01 \x01(\x05R\x01n\"9\n\x19\x41pproxCountDistinctParams\x12\x1c\n\tprecision\x18\x01 \x01(\x05R\tprecision\"V\n\x16\x41pproxPercentileParams\x12\x1e\n\npercentile\x18\x01 \x01(\x01R\npercentile\x12\x1c\n\tprecision\x18\x02 \x01(\x05R\tprecision\"\xe0\x02\n\x19\x41ggregationFunctionParams\x12\x39\n\x06last_n\x18\x01 \x01(\x0b\x32 .tecton_proto.common.LastNParamsH\x00R\x05lastN\x12<\n\x07\x66irst_n\x18\x02 \x01(\x0b\x32!.tecton_proto.common.FirstNParamsH\x00R\x06\x66irstN\x12\x64\n\x15\x61pprox_count_distinct\x18\x03 \x01(\x0b\x32..tecton_proto.common.ApproxCountDistinctParamsH\x00R\x13\x61pproxCountDistinct\x12Z\n\x11\x61pprox_percentile\x18\x04 \x01(\x0b\x32+.tecton_proto.common.ApproxPercentileParamsH\x00R\x10\x61pproxPercentileB\x08\n\x06params*\x99\x05\n\x13\x41ggregationFunction\x12 \n\x1c\x41GGREGATION_FUNCTION_UNKNOWN\x10\x00\x12\x1e\n\x1a\x41GGREGATION_FUNCTION_COUNT\x10\x01\x12\x1c\n\x18\x41GGREGATION_FUNCTION_SUM\x10\x02\x12\x1d\n\x19\x41GGREGATION_FUNCTION_MEAN\x10\x03\x12\x1d\n\x19\x41GGREGATION_FUNCTION_LAST\x10\x05\x12\x1c\n\x18\x41GGREGATION_FUNCTION_MIN\x10\x06\x12\x1c\n\x18\x41GGREGATION_FUNCTION_MAX\x10\x07\x12(\n$AGGREGATION_FUNCTION_LAST_DISTINCT_N\x10\t\x12!\n\x1d\x41GGREGATION_FUNCTION_VAR_SAMP\x10\n\x12 \n\x1c\x41GGREGATION_FUNCTION_VAR_POP\x10\x0b\x12$\n AGGREGATION_FUNCTION_STDDEV_SAMP\x10\x0c\x12#\n\x1f\x41GGREGATION_FUNCTION_STDDEV_POP\x10\r\x12,\n(AGGREGATION_FUNCTION_LAST_NON_DISTINCT_N\x10\x0e\x12-\n)AGGREGATION_FUNCTION_FIRST_NON_DISTINCT_N\x10\x0f\x12)\n%AGGREGATION_FUNCTION_FIRST_DISTINCT_N\x10\x10\x12.\n*AGGREGATION_FUNCTION_APPROX_COUNT_DISTINCT\x10\x11\x12*\n&AGGREGATION_FUNCTION_APPROX_PERCENTILE\x10\x12\"\x04\x08\x04\x10\x04\"\x04\x08\x08\x10\x08\x42?\n\x11\x63om.tecton.commonP\x01Z(github.com/tecton-ai/tecton_proto/common')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.common.aggregation_function_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\021com.tecton.commonP\001Z(github.com/tecton-ai/tecton_proto/common'
-  _AGGREGATIONFUNCTION._serialized_start=292
-  _AGGREGATIONFUNCTION._serialized_end=865
+  _AGGREGATIONFUNCTION._serialized_start=633
+  _AGGREGATIONFUNCTION._serialized_end=1298
   _LASTNPARAMS._serialized_start=71
   _LASTNPARAMS._serialized_end=98
   _FIRSTNPARAMS._serialized_start=100
   _FIRSTNPARAMS._serialized_end=128
-  _AGGREGATIONFUNCTIONPARAMS._serialized_start=131
-  _AGGREGATIONFUNCTIONPARAMS._serialized_end=289
+  _APPROXCOUNTDISTINCTPARAMS._serialized_start=130
+  _APPROXCOUNTDISTINCTPARAMS._serialized_end=187
+  _APPROXPERCENTILEPARAMS._serialized_start=189
+  _APPROXPERCENTILEPARAMS._serialized_end=275
+  _AGGREGATIONFUNCTIONPARAMS._serialized_start=278
+  _AGGREGATIONFUNCTIONPARAMS._serialized_end=630
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/common/analytics_options_pb2.py` & `tecton-0.7.0rc0/tecton_proto/common/analytics_options_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/common/column_type_pb2.py` & `tecton-0.7.0rc0/tecton_proto/common/column_type_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/common/data_source_type_pb2.py` & `tecton-0.7.0rc0/tecton_proto/common/data_source_type_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/common/data_type_pb2.py` & `tecton-0.7.0rc0/tecton_proto/common/data_type_pb2.py`

 * *Files 7% similar despite different names*

```diff
@@ -9,22 +9,22 @@
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n#tecton_proto/common/data_type.proto\x12\x13tecton_proto.common\"\xd5\x01\n\x08\x44\x61taType\x12\x35\n\x04type\x18\x01 \x01(\x0e\x32!.tecton_proto.common.DataTypeEnumR\x04type\x12K\n\x12\x61rray_element_type\x18\x02 \x01(\x0b\x32\x1d.tecton_proto.common.DataTypeR\x10\x61rrayElementType\x12\x45\n\rstruct_fields\x18\x03 \x03(\x0b\x32 .tecton_proto.common.StructFieldR\x0cstructFields\"]\n\x0bStructField\x12\x12\n\x04name\x18\x01 \x01(\tR\x04name\x12:\n\tdata_type\x18\x02 \x01(\x0b\x32\x1d.tecton_proto.common.DataTypeR\x08\x64\x61taType*\x8a\x02\n\x0c\x44\x61taTypeEnum\x12\x15\n\x11\x44\x41TA_TYPE_UNKNOWN\x10\x00\x12\x13\n\x0f\x44\x41TA_TYPE_INT32\x10\x01\x12\x13\n\x0f\x44\x41TA_TYPE_INT64\x10\x02\x12\x15\n\x11\x44\x41TA_TYPE_FLOAT32\x10\x03\x12\x15\n\x11\x44\x41TA_TYPE_FLOAT64\x10\x04\x12\x14\n\x10\x44\x41TA_TYPE_STRING\x10\x05\x12\x12\n\x0e\x44\x41TA_TYPE_BOOL\x10\x06\x12\x17\n\x13\x44\x41TA_TYPE_TIMESTAMP\x10\x07\x12\x13\n\x0f\x44\x41TA_TYPE_ARRAY\x10\x08\x12\x14\n\x10\x44\x41TA_TYPE_STRUCT\x10\t\x12\x1d\n\x19\x44\x41TA_TYPE_TIMESTAMP_INT96\x10\nB\x15\n\x11\x63om.tecton.commonP\x01')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n#tecton_proto/common/data_type.proto\x12\x13tecton_proto.common\"\xdb\x02\n\x08\x44\x61taType\x12\x35\n\x04type\x18\x01 \x01(\x0e\x32!.tecton_proto.common.DataTypeEnumR\x04type\x12K\n\x12\x61rray_element_type\x18\x02 \x01(\x0b\x32\x1d.tecton_proto.common.DataTypeR\x10\x61rrayElementType\x12\x45\n\rstruct_fields\x18\x03 \x03(\x0b\x32 .tecton_proto.common.StructFieldR\x0cstructFields\x12?\n\x0cmap_key_type\x18\x04 \x01(\x0b\x32\x1d.tecton_proto.common.DataTypeR\nmapKeyType\x12\x43\n\x0emap_value_type\x18\x05 \x01(\x0b\x32\x1d.tecton_proto.common.DataTypeR\x0cmapValueType\"]\n\x0bStructField\x12\x12\n\x04name\x18\x01 \x01(\tR\x04name\x12:\n\tdata_type\x18\x02 \x01(\x0b\x32\x1d.tecton_proto.common.DataTypeR\x08\x64\x61taType*\x9d\x02\n\x0c\x44\x61taTypeEnum\x12\x15\n\x11\x44\x41TA_TYPE_UNKNOWN\x10\x00\x12\x13\n\x0f\x44\x41TA_TYPE_INT32\x10\x01\x12\x13\n\x0f\x44\x41TA_TYPE_INT64\x10\x02\x12\x15\n\x11\x44\x41TA_TYPE_FLOAT32\x10\x03\x12\x15\n\x11\x44\x41TA_TYPE_FLOAT64\x10\x04\x12\x14\n\x10\x44\x41TA_TYPE_STRING\x10\x05\x12\x12\n\x0e\x44\x41TA_TYPE_BOOL\x10\x06\x12\x17\n\x13\x44\x41TA_TYPE_TIMESTAMP\x10\x07\x12\x13\n\x0f\x44\x41TA_TYPE_ARRAY\x10\x08\x12\x14\n\x10\x44\x41TA_TYPE_STRUCT\x10\t\x12\x1d\n\x19\x44\x41TA_TYPE_TIMESTAMP_INT96\x10\n\x12\x11\n\rDATA_TYPE_MAP\x10\x0b\x42\x15\n\x11\x63om.tecton.commonP\x01')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.common.data_type_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\021com.tecton.commonP\001'
-  _DATATYPEENUM._serialized_start=372
-  _DATATYPEENUM._serialized_end=638
+  _DATATYPEENUM._serialized_start=506
+  _DATATYPEENUM._serialized_end=791
   _DATATYPE._serialized_start=61
-  _DATATYPE._serialized_end=274
-  _STRUCTFIELD._serialized_start=276
-  _STRUCTFIELD._serialized_end=369
+  _DATATYPE._serialized_end=408
+  _STRUCTFIELD._serialized_start=410
+  _STRUCTFIELD._serialized_end=503
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/common/fco_locator_pb2.py` & `tecton-0.7.0rc0/tecton_proto/common/fco_locator_pb2.py`

 * *Files 12% similar despite different names*

```diff
@@ -10,20 +10,20 @@
 
 _sym_db = _symbol_database.Default()
 
 
 from tecton_proto.common import id_pb2 as tecton__proto_dot_common_dot_id__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n%tecton_proto/common/fco_locator.proto\x12\x13tecton_proto.common\x1a\x1ctecton_proto/common/id.proto\"v\n\nFcoLocator\x12\x14\n\x04name\x18\x01 \x01(\tH\x00R\x04name\x12)\n\x02id\x18\x02 \x01(\x0b\x32\x17.tecton_proto.common.IdH\x00R\x02id\x12\x1c\n\tworkspace\x18\x03 \x01(\tR\tworkspaceB\t\n\x07locator\"[\n\x0cIdFcoLocator\x12\'\n\x02id\x18\x02 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x02id\x12\x1c\n\tworkspace\x18\x03 \x01(\tR\tworkspaceJ\x04\x08\x01\x10\x02\x42\x15\n\x11\x63om.tecton.commonP\x01')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n%tecton_proto/common/fco_locator.proto\x12\x13tecton_proto.common\x1a\x1ctecton_proto/common/id.proto\"v\n\nFcoLocator\x12\x14\n\x04name\x18\x01 \x01(\tH\x00R\x04name\x12)\n\x02id\x18\x02 \x01(\x0b\x32\x17.tecton_proto.common.IdH\x00R\x02id\x12\x1c\n\tworkspace\x18\x03 \x01(\tR\tworkspaceB\t\n\x07locator\"\xa2\x01\n\x0cIdFcoLocator\x12\'\n\x02id\x18\x02 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x02id\x12\x1c\n\tworkspace\x18\x03 \x01(\tR\tworkspace\x12\x45\n\x12workspace_state_id\x18\x04 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x10workspaceStateIdJ\x04\x08\x01\x10\x02\x42\x15\n\x11\x63om.tecton.commonP\x01')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.common.fco_locator_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\021com.tecton.commonP\001'
   _FCOLOCATOR._serialized_start=92
   _FCOLOCATOR._serialized_end=210
-  _IDFCOLOCATOR._serialized_start=212
-  _IDFCOLOCATOR._serialized_end=303
+  _IDFCOLOCATOR._serialized_start=213
+  _IDFCOLOCATOR._serialized_end=375
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/common/framework_version_pb2.py` & `tecton-0.7.0rc0/tecton_proto/common/framework_version_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/common/id_pb2.py` & `tecton-0.7.0rc0/tecton_proto/common/id_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/common/pair_pb2.py` & `tecton-0.7.0rc0/tecton_proto/common/pair_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/common/schema_container_pb2.py` & `tecton-0.7.0rc0/tecton_proto/common/schema_container_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/common/schema_pb2.py` & `tecton-0.7.0rc0/tecton_proto/common/schema_pb2.py`

 * *Files 0% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 _sym_db = _symbol_database.Default()
 
 
 from tecton_proto.common import column_type_pb2 as tecton__proto_dot_common_dot_column__type__pb2
 from tecton_proto.common import data_type_pb2 as tecton__proto_dot_common_dot_data__type__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n tecton_proto/common/schema.proto\x12\x13tecton_proto.common\x1a%tecton_proto/common/column_type.proto\x1a#tecton_proto/common/data_type.proto\"\xe4\x02\n\x06\x43olumn\x12\x12\n\x04name\x18\x01 \x01(\tR\x04name\x12$\n\x0eraw_spark_type\x18\x02 \x01(\tR\x0crawSparkType\x12,\n\x12raw_snowflake_type\x18\x04 \x01(\tR\x10rawSnowflakeType\x12O\n\x13\x66\x65\x61ture_server_type\x18\x03 \x01(\x0e\x32\x1f.tecton_proto.common.ColumnTypeR\x11\x66\x65\x61tureServerType\x12I\n\x11offline_data_type\x18\x05 \x01(\x0b\x32\x1d.tecton_proto.common.DataTypeR\x0fofflineDataType\x12V\n\x18\x66\x65\x61ture_server_data_type\x18\x06 \x01(\x0b\x32\x1d.tecton_proto.common.DataTypeR\x15\x66\x65\x61tureServerDataType\"E\n\x06Schema\x12\x35\n\x07\x63olumns\x18\x02 \x03(\x0b\x32\x1b.tecton_proto.common.ColumnR\x07\x63olumnsJ\x04\x08\x01\x10\x02\x42\x15\n\x11\x63om.tecton.commonP\x01')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n tecton_proto/common/schema.proto\x12\x13tecton_proto.common\x1a%tecton_proto/common/column_type.proto\x1a#tecton_proto/common/data_type.proto\"\xe4\x02\n\x06\x43olumn\x12\x12\n\x04name\x18\x01 \x01(\tR\x04name\x12I\n\x11offline_data_type\x18\x05 \x01(\x0b\x32\x1d.tecton_proto.common.DataTypeR\x0fofflineDataType\x12V\n\x18\x66\x65\x61ture_server_data_type\x18\x06 \x01(\x0b\x32\x1d.tecton_proto.common.DataTypeR\x15\x66\x65\x61tureServerDataType\x12$\n\x0eraw_spark_type\x18\x02 \x01(\tR\x0crawSparkType\x12,\n\x12raw_snowflake_type\x18\x04 \x01(\tR\x10rawSnowflakeType\x12O\n\x13\x66\x65\x61ture_server_type\x18\x03 \x01(\x0e\x32\x1f.tecton_proto.common.ColumnTypeR\x11\x66\x65\x61tureServerType\"E\n\x06Schema\x12\x35\n\x07\x63olumns\x18\x02 \x03(\x0b\x32\x1b.tecton_proto.common.ColumnR\x07\x63olumnsJ\x04\x08\x01\x10\x02\x42\x15\n\x11\x63om.tecton.commonP\x01')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.common.schema_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\021com.tecton.commonP\001'
```

### Comparing `tecton-0.7.0b9/tecton_proto/common/secret_pb2.py` & `tecton-0.7.0rc0/tecton_proto/common/secret_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/common/spark_schema_pb2.py` & `tecton-0.7.0rc0/tecton_proto/common/spark_schema_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/consumption/consumption_pb2.py` & `tecton-0.7.0rc0/tecton_proto/consumption/consumption_pb2.py`

 * *Files 7% similar despite different names*

```diff
@@ -11,24 +11,26 @@
 _sym_db = _symbol_database.Default()
 
 
 from google.protobuf import timestamp_pb2 as google_dot_protobuf_dot_timestamp__pb2
 from tecton_proto.common import id_pb2 as tecton__proto_dot_common_dot_id__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n*tecton_proto/consumption/consumption.proto\x12\x18tecton_proto.consumption\x1a\x1fgoogle/protobuf/timestamp.proto\x1a\x1ctecton_proto/common/id.proto\"\xb9\x03\n\x0f\x43onsumptionInfo\x12\x46\n\x11time_bucket_start\x18\x01 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x0ftimeBucketStart\x12%\n\x0eunits_consumed\x18\x02 \x01(\x03R\runitsConsumed\x12\x16\n\x06metric\x18\x03 \x01(\tR\x06metric\x12P\n\x07\x64\x65tails\x18\x04 \x03(\x0b\x32\x36.tecton_proto.consumption.ConsumptionInfo.DetailsEntryR\x07\x64\x65tails\x12\x1b\n\tsource_id\x18\x05 \x01(\tR\x08sourceId\x12?\n\x0f\x66\x65\x61ture_view_id\x18\x06 \x01(\x0b\x32\x17.tecton_proto.common.IdR\rfeatureViewId\x12\x33\n\x16online_read_aws_region\x18\x07 \x01(\tR\x13onlineReadAwsRegion\x1a:\n\x0c\x44\x65tailsEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01\"\xff\x01\n\x17\x45nrichedConsumptionInfo\x12T\n\x10\x63onsumption_info\x18\x01 \x01(\x0b\x32).tecton_proto.consumption.ConsumptionInfoR\x0f\x63onsumptionInfo\x12\x34\n\x16\x66\x65\x61ture_view_workspace\x18\x03 \x01(\tR\x14\x66\x65\x61tureViewWorkspace\x12*\n\x11\x66\x65\x61ture_view_name\x18\x04 \x01(\tR\x0f\x66\x65\x61tureViewName\x12&\n\x0f\x66\x65\x61ture_view_id\x18\x05 \x01(\tR\rfeatureViewIdJ\x04\x08\x02\x10\x03\x42\x1a\n\x16\x63om.tecton.consumptionP\x01')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n*tecton_proto/consumption/consumption.proto\x12\x18tecton_proto.consumption\x1a\x1fgoogle/protobuf/timestamp.proto\x1a\x1ctecton_proto/common/id.proto\"\x83\x04\n\x0f\x43onsumptionInfo\x12\x46\n\x11time_bucket_start\x18\x01 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x0ftimeBucketStart\x12%\n\x0eunits_consumed\x18\x02 \x01(\x03R\runitsConsumed\x12\x16\n\x06metric\x18\x03 \x01(\tR\x06metric\x12P\n\x07\x64\x65tails\x18\x04 \x03(\x0b\x32\x36.tecton_proto.consumption.ConsumptionInfo.DetailsEntryR\x07\x64\x65tails\x12\x1b\n\tsource_id\x18\x05 \x01(\tR\x08sourceId\x12?\n\x0f\x66\x65\x61ture_view_id\x18\x06 \x01(\x0b\x32\x17.tecton_proto.common.IdR\rfeatureViewId\x12*\n\x11\x66\x65\x61ture_view_name\x18\x08 \x01(\tR\x0f\x66\x65\x61tureViewName\x12\x1c\n\tworkspace\x18\t \x01(\tR\tworkspace\x12\x33\n\x16online_read_aws_region\x18\x07 \x01(\tR\x13onlineReadAwsRegion\x1a:\n\x0c\x44\x65tailsEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01\"\xff\x01\n\x17\x45nrichedConsumptionInfo\x12T\n\x10\x63onsumption_info\x18\x01 \x01(\x0b\x32).tecton_proto.consumption.ConsumptionInfoR\x0f\x63onsumptionInfo\x12\x34\n\x16\x66\x65\x61ture_view_workspace\x18\x03 \x01(\tR\x14\x66\x65\x61tureViewWorkspace\x12*\n\x11\x66\x65\x61ture_view_name\x18\x04 \x01(\tR\x0f\x66\x65\x61tureViewName\x12&\n\x0f\x66\x65\x61ture_view_id\x18\x05 \x01(\tR\rfeatureViewIdJ\x04\x08\x02\x10\x03\"\xac\x01\n\x16\x41ttemptConsumptionInfo\x12.\n\x13online_rows_written\x18\x1b \x01(\x03R\x11onlineRowsWritten\x12\x30\n\x14online_bytes_written\x18\x1c \x01(\x03R\x12onlineBytesWritten\x12\x30\n\x14offline_rows_written\x18\x1d \x01(\x03R\x12offlineRowsWrittenB\x1a\n\x16\x63om.tecton.consumptionP\x01')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.consumption.consumption_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\026com.tecton.consumptionP\001'
   _CONSUMPTIONINFO_DETAILSENTRY._options = None
   _CONSUMPTIONINFO_DETAILSENTRY._serialized_options = b'8\001'
   _CONSUMPTIONINFO._serialized_start=136
-  _CONSUMPTIONINFO._serialized_end=577
-  _CONSUMPTIONINFO_DETAILSENTRY._serialized_start=519
-  _CONSUMPTIONINFO_DETAILSENTRY._serialized_end=577
-  _ENRICHEDCONSUMPTIONINFO._serialized_start=580
-  _ENRICHEDCONSUMPTIONINFO._serialized_end=835
+  _CONSUMPTIONINFO._serialized_end=651
+  _CONSUMPTIONINFO_DETAILSENTRY._serialized_start=593
+  _CONSUMPTIONINFO_DETAILSENTRY._serialized_end=651
+  _ENRICHEDCONSUMPTIONINFO._serialized_start=654
+  _ENRICHEDCONSUMPTIONINFO._serialized_end=909
+  _ATTEMPTCONSUMPTIONINFO._serialized_start=912
+  _ATTEMPTCONSUMPTIONINFO._serialized_end=1084
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/data/batch_data_source_pb2.py` & `tecton-0.7.0rc0/tecton_proto/data/batch_data_source_pb2.py`

 * *Files 7% similar despite different names*

```diff
@@ -16,34 +16,36 @@
 from tecton_proto.args import data_source_pb2 as tecton__proto_dot_args_dot_data__source__pb2
 from tecton_proto.args import data_source_config_pb2 as tecton__proto_dot_args_dot_data__source__config__pb2
 from tecton_proto.args import user_defined_function_pb2 as tecton__proto_dot_args_dot_user__defined__function__pb2
 from tecton_proto.common import spark_schema_pb2 as tecton__proto_dot_common_dot_spark__schema__pb2
 from tecton_proto.data import hive_metastore_pb2 as tecton__proto_dot_data_dot_hive__metastore__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n)tecton_proto/data/batch_data_source.proto\x12\x11tecton_proto.data\x1a\x1egoogle/protobuf/duration.proto\x1a$tecton_proto/args/diff_options.proto\x1a#tecton_proto/args/data_source.proto\x1a*tecton_proto/args/data_source_config.proto\x1a-tecton_proto/args/user_defined_function.proto\x1a&tecton_proto/common/spark_schema.proto\x1a&tecton_proto/data/hive_metastore.proto\"T\n\x19TimestampColumnProperties\x12\x1f\n\x0b\x63olumn_name\x18\x01 \x01(\tR\ncolumnName\x12\x16\n\x06\x66ormat\x18\x02 \x01(\tR\x06\x66ormat\"\x9a\x01\n\x1cSparkBatchDataSourceFunction\x12\x42\n\x08\x66unction\x18\x01 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionR\x08\x66unction\x12\x36\n\x17supports_time_filtering\x18\x02 \x01(\x08R\x15supportsTimeFiltering\"\xd3\x07\n\x0f\x42\x61tchDataSource\x12G\n\nhive_table\x18\x01 \x01(\x0b\x32&.tecton_proto.data.HiveTableDataSourceH\x00R\thiveTable\x12\x37\n\x04\x66ile\x18\x08 \x01(\x0b\x32!.tecton_proto.data.FileDataSourceH\x00R\x04\x66ile\x12H\n\x0bredshift_db\x18\x0b \x01(\x0b\x32%.tecton_proto.data.RedshiftDataSourceH\x00R\nredshiftDb\x12\x46\n\tsnowflake\x18\x0c \x01(\x0b\x32&.tecton_proto.data.SnowflakeDataSourceH\x00R\tsnowflake\x12n\n\x1aspark_data_source_function\x18\r \x01(\x0b\x32/.tecton_proto.data.SparkBatchDataSourceFunctionH\x00R\x17sparkDataSourceFunction\x12\x43\n\x0cspark_schema\x18\t \x01(\x0b\x32 .tecton_proto.common.SparkSchemaR\x0bsparkSchema\x12l\n\x1btimestamp_column_properties\x18\x04 \x01(\x0b\x32,.tecton_proto.data.TimestampColumnPropertiesR\x19timestampColumnProperties\x12\x41\n\x0c\x62\x61tch_config\x18\x05 \x01(\x0b\x32\x1e.tecton_proto.args.BatchConfigR\x0b\x62\x61tchConfig\x12\x32\n\x15\x64\x61te_partition_column\x18\x06 \x01(\tR\x13\x64\x61tePartitionColumn\x12h\n\x1a\x64\x61tetime_partition_columns\x18\x07 \x03(\x0b\x32*.tecton_proto.data.DatetimePartitionColumnR\x18\x64\x61tetimePartitionColumns\x12X\n\x14raw_batch_translator\x18\n \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionR\x12rawBatchTranslator\x12\x38\n\ndata_delay\x18\x0e \x01(\x0b\x32\x19.google.protobuf.DurationR\tdataDelayB\x0e\n\x0c\x62\x61tch_sourceJ\x04\x08\x02\x10\x03\"\x88\x02\n\x0e\x46ileDataSource\x12\x10\n\x03uri\x18\x01 \x02(\tR\x03uri\x12?\n\x06\x66ormat\x18\x02 \x02(\x0e\x32\'.tecton_proto.data.FileDataSourceFormatR\x06\x66ormat\x12\x33\n\x16\x63onvert_to_glue_format\x18\x04 \x01(\x08R\x13\x63onvertToGlueFormat\x12\x1d\n\nschema_uri\x18\x05 \x01(\tR\tschemaUri\x12I\n\x0fschema_override\x18\x06 \x01(\x0b\x32 .tecton_proto.common.SparkSchemaR\x0eschemaOverrideJ\x04\x08\x03\x10\x04\"\x88\x01\n\x17\x44\x61tetimePartitionColumn\x12\x1f\n\x0b\x63olumn_name\x18\x01 \x02(\tR\ncolumnName\x12#\n\rformat_string\x18\x02 \x02(\tR\x0c\x66ormatString\x12\'\n\x0fminimum_seconds\x18\x03 \x02(\x03R\x0eminimumSeconds\"\xc5\x01\n\x12RedshiftDataSource\x12\x1a\n\x08\x65ndpoint\x18\x01 \x01(\tR\x08\x65ndpoint\x12\x1d\n\ncluster_id\x18\x02 \x01(\tR\tclusterId\x12\x1a\n\x08\x64\x61tabase\x18\x03 \x01(\tR\x08\x64\x61tabase\x12\x16\n\x05table\x18\x04 \x01(\tH\x00R\x05table\x12\x1d\n\x05query\x18\x06 \x01(\tB\x05\x92M\x02\x18\x03H\x00R\x05query\x12\x17\n\x07temp_s3\x18\x05 \x01(\tR\x06tempS3B\x08\n\x06source\"g\n\x13SnowflakeDataSource\x12P\n\rsnowflakeArgs\x18\x01 \x01(\x0b\x32*.tecton_proto.args.SnowflakeDataSourceArgsR\rsnowflakeArgs*~\n\x14\x46ileDataSourceFormat\x12 \n\x1c\x46ILE_DATA_SOURCE_FORMAT_JSON\x10\x00\x12#\n\x1f\x46ILE_DATA_SOURCE_FORMAT_PARQUET\x10\x01\x12\x1f\n\x1b\x46ILE_DATA_SOURCE_FORMAT_CSV\x10\x02\x42\x13\n\x0f\x63om.tecton.dataP\x01')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n)tecton_proto/data/batch_data_source.proto\x12\x11tecton_proto.data\x1a\x1egoogle/protobuf/duration.proto\x1a$tecton_proto/args/diff_options.proto\x1a#tecton_proto/args/data_source.proto\x1a*tecton_proto/args/data_source_config.proto\x1a-tecton_proto/args/user_defined_function.proto\x1a&tecton_proto/common/spark_schema.proto\x1a&tecton_proto/data/hive_metastore.proto\"T\n\x19TimestampColumnProperties\x12\x1f\n\x0b\x63olumn_name\x18\x01 \x01(\tR\ncolumnName\x12\x16\n\x06\x66ormat\x18\x02 \x01(\tR\x06\x66ormat\"\x9a\x01\n\x1cSparkBatchDataSourceFunction\x12\x42\n\x08\x66unction\x18\x01 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionR\x08\x66unction\x12\x36\n\x17supports_time_filtering\x18\x02 \x01(\x08R\x15supportsTimeFiltering\"\x9f\x08\n\x0f\x42\x61tchDataSource\x12G\n\nhive_table\x18\x01 \x01(\x0b\x32&.tecton_proto.data.HiveTableDataSourceH\x00R\thiveTable\x12\x37\n\x04\x66ile\x18\x08 \x01(\x0b\x32!.tecton_proto.data.FileDataSourceH\x00R\x04\x66ile\x12H\n\x0bredshift_db\x18\x0b \x01(\x0b\x32%.tecton_proto.data.RedshiftDataSourceH\x00R\nredshiftDb\x12\x46\n\tsnowflake\x18\x0c \x01(\x0b\x32&.tecton_proto.data.SnowflakeDataSourceH\x00R\tsnowflake\x12n\n\x1aspark_data_source_function\x18\r \x01(\x0b\x32/.tecton_proto.data.SparkBatchDataSourceFunctionH\x00R\x17sparkDataSourceFunction\x12J\n\x0bunity_table\x18\x0f \x01(\x0b\x32\'.tecton_proto.data.UnityTableDataSourceH\x00R\nunityTable\x12\x43\n\x0cspark_schema\x18\t \x01(\x0b\x32 .tecton_proto.common.SparkSchemaR\x0bsparkSchema\x12l\n\x1btimestamp_column_properties\x18\x04 \x01(\x0b\x32,.tecton_proto.data.TimestampColumnPropertiesR\x19timestampColumnProperties\x12\x41\n\x0c\x62\x61tch_config\x18\x05 \x01(\x0b\x32\x1e.tecton_proto.args.BatchConfigR\x0b\x62\x61tchConfig\x12\x32\n\x15\x64\x61te_partition_column\x18\x06 \x01(\tR\x13\x64\x61tePartitionColumn\x12h\n\x1a\x64\x61tetime_partition_columns\x18\x07 \x03(\x0b\x32*.tecton_proto.data.DatetimePartitionColumnR\x18\x64\x61tetimePartitionColumns\x12X\n\x14raw_batch_translator\x18\n \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionR\x12rawBatchTranslator\x12\x38\n\ndata_delay\x18\x0e \x01(\x0b\x32\x19.google.protobuf.DurationR\tdataDelayB\x0e\n\x0c\x62\x61tch_sourceJ\x04\x08\x02\x10\x03\"^\n\x14UnityTableDataSource\x12\x18\n\x07\x63\x61talog\x18\x01 \x01(\tR\x07\x63\x61talog\x12\x16\n\x06schema\x18\x02 \x01(\tR\x06schema\x12\x14\n\x05table\x18\x03 \x01(\tR\x05table\"\x88\x02\n\x0e\x46ileDataSource\x12\x10\n\x03uri\x18\x01 \x02(\tR\x03uri\x12?\n\x06\x66ormat\x18\x02 \x02(\x0e\x32\'.tecton_proto.data.FileDataSourceFormatR\x06\x66ormat\x12\x33\n\x16\x63onvert_to_glue_format\x18\x04 \x01(\x08R\x13\x63onvertToGlueFormat\x12\x1d\n\nschema_uri\x18\x05 \x01(\tR\tschemaUri\x12I\n\x0fschema_override\x18\x06 \x01(\x0b\x32 .tecton_proto.common.SparkSchemaR\x0eschemaOverrideJ\x04\x08\x03\x10\x04\"\x88\x01\n\x17\x44\x61tetimePartitionColumn\x12\x1f\n\x0b\x63olumn_name\x18\x01 \x02(\tR\ncolumnName\x12#\n\rformat_string\x18\x02 \x02(\tR\x0c\x66ormatString\x12\'\n\x0fminimum_seconds\x18\x03 \x02(\x03R\x0eminimumSeconds\"\xc5\x01\n\x12RedshiftDataSource\x12\x1a\n\x08\x65ndpoint\x18\x01 \x01(\tR\x08\x65ndpoint\x12\x1d\n\ncluster_id\x18\x02 \x01(\tR\tclusterId\x12\x1a\n\x08\x64\x61tabase\x18\x03 \x01(\tR\x08\x64\x61tabase\x12\x16\n\x05table\x18\x04 \x01(\tH\x00R\x05table\x12\x1d\n\x05query\x18\x06 \x01(\tB\x05\x92M\x02\x18\x03H\x00R\x05query\x12\x17\n\x07temp_s3\x18\x05 \x01(\tR\x06tempS3B\x08\n\x06source\"g\n\x13SnowflakeDataSource\x12P\n\rsnowflakeArgs\x18\x01 \x01(\x0b\x32*.tecton_proto.args.SnowflakeDataSourceArgsR\rsnowflakeArgs*~\n\x14\x46ileDataSourceFormat\x12 \n\x1c\x46ILE_DATA_SOURCE_FORMAT_JSON\x10\x00\x12#\n\x1f\x46ILE_DATA_SOURCE_FORMAT_PARQUET\x10\x01\x12\x1f\n\x1b\x46ILE_DATA_SOURCE_FORMAT_CSV\x10\x02\x42\x13\n\x0f\x63om.tecton.dataP\x01')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.data.batch_data_source_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\017com.tecton.dataP\001'
   _REDSHIFTDATASOURCE.fields_by_name['query']._options = None
   _REDSHIFTDATASOURCE.fields_by_name['query']._serialized_options = b'\222M\002\030\003'
-  _FILEDATASOURCEFORMAT._serialized_start=2278
-  _FILEDATASOURCEFORMAT._serialized_end=2404
+  _FILEDATASOURCEFORMAT._serialized_start=2450
+  _FILEDATASOURCEFORMAT._serialized_end=2576
   _TIMESTAMPCOLUMNPROPERTIES._serialized_start=342
   _TIMESTAMPCOLUMNPROPERTIES._serialized_end=426
   _SPARKBATCHDATASOURCEFUNCTION._serialized_start=429
   _SPARKBATCHDATASOURCEFUNCTION._serialized_end=583
   _BATCHDATASOURCE._serialized_start=586
-  _BATCHDATASOURCE._serialized_end=1565
-  _FILEDATASOURCE._serialized_start=1568
-  _FILEDATASOURCE._serialized_end=1832
-  _DATETIMEPARTITIONCOLUMN._serialized_start=1835
-  _DATETIMEPARTITIONCOLUMN._serialized_end=1971
-  _REDSHIFTDATASOURCE._serialized_start=1974
-  _REDSHIFTDATASOURCE._serialized_end=2171
-  _SNOWFLAKEDATASOURCE._serialized_start=2173
-  _SNOWFLAKEDATASOURCE._serialized_end=2276
+  _BATCHDATASOURCE._serialized_end=1641
+  _UNITYTABLEDATASOURCE._serialized_start=1643
+  _UNITYTABLEDATASOURCE._serialized_end=1737
+  _FILEDATASOURCE._serialized_start=1740
+  _FILEDATASOURCE._serialized_end=2004
+  _DATETIMEPARTITIONCOLUMN._serialized_start=2007
+  _DATETIMEPARTITIONCOLUMN._serialized_end=2143
+  _REDSHIFTDATASOURCE._serialized_start=2146
+  _REDSHIFTDATASOURCE._serialized_end=2343
+  _SNOWFLAKEDATASOURCE._serialized_start=2345
+  _SNOWFLAKEDATASOURCE._serialized_end=2448
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/data/entity_pb2.py` & `tecton-0.7.0rc0/tecton_proto/data/entity_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/data/fco_metadata_pb2.py` & `tecton-0.7.0rc0/tecton_proto/data/fco_metadata_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/data/fco_pb2.py` & `tecton-0.7.0rc0/tecton_proto/data/fco_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/data/feature_service_pb2.py` & `tecton-0.7.0rc0/tecton_proto/data/feature_service_pb2.py`

 * *Files 6% similar despite different names*

```diff
@@ -12,30 +12,31 @@
 
 
 from tecton_proto.validation import validator_pb2 as tecton__proto_dot_validation_dot_validator__pb2
 from tecton_proto.common import data_type_pb2 as tecton__proto_dot_common_dot_data__type__pb2
 from tecton_proto.args import feature_service_pb2 as tecton__proto_dot_args_dot_feature__service__pb2
 from tecton_proto.data import fco_metadata_pb2 as tecton__proto_dot_data_dot_fco__metadata__pb2
 from tecton_proto.common import id_pb2 as tecton__proto_dot_common_dot_id__pb2
+from tecton_proto.data import odfv_compute_pb2 as tecton__proto_dot_data_dot_odfv__compute__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\'tecton_proto/data/feature_service.proto\x12\x11tecton_proto.data\x1a\'tecton_proto/validation/validator.proto\x1a#tecton_proto/common/data_type.proto\x1a\'tecton_proto/args/feature_service.proto\x1a$tecton_proto/data/fco_metadata.proto\x1a\x1ctecton_proto/common/id.proto\"\xe9\x03\n\x0e\x46\x65\x61tureService\x12\x45\n\x12\x66\x65\x61ture_service_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x10\x66\x65\x61tureServiceId\x12M\n\x11\x66\x65\x61ture_set_items\x18\x02 \x03(\x0b\x32!.tecton_proto.data.FeatureSetItemR\x0f\x66\x65\x61tureSetItems\x12\x41\n\x0c\x66\x63o_metadata\x18\t \x01(\x0b\x32\x1e.tecton_proto.data.FcoMetadataR\x0b\x66\x63oMetadata\x12\x34\n\x16online_serving_enabled\x18\x0b \x01(\x08R\x14onlineServingEnabled\x12>\n\x07logging\x18\x0c \x01(\x0b\x32$.tecton_proto.args.LoggingConfigArgsR\x07logging\x12^\n\x0fvalidation_args\x18\r \x01(\x0b\x32\x35.tecton_proto.validation.FeatureServiceValidationArgsR\x0evalidationArgsJ\x04\x08\x03\x10\x04J\x04\x08\x04\x10\x05J\x04\x08\x05\x10\x06J\x04\x08\x06\x10\x07J\x04\x08\x07\x10\x08J\x04\x08\x08\x10\tJ\x04\x08\n\x10\x0b\"\xca\x01\n\x10JoinKeyComponent\x12*\n\x11spine_column_name\x18\x01 \x01(\tR\x0fspineColumnName\x12H\n\x0c\x62inding_type\x18\x02 \x01(\x0e\x32%.tecton_proto.data.JoinKeyBindingTypeR\x0b\x62indingType\x12:\n\tdata_type\x18\x04 \x01(\x0b\x32\x1d.tecton_proto.common.DataTypeR\x08\x64\x61taTypeJ\x04\x08\x03\x10\x04\"V\n\x0fJoinKeyTemplate\x12\x43\n\ncomponents\x18\x01 \x03(\x0b\x32#.tecton_proto.data.JoinKeyComponentR\ncomponents\"\x8a\x02\n\x0e\x46\x65\x61tureSetItem\x12?\n\x0f\x66\x65\x61ture_view_id\x18\x06 \x01(\x0b\x32\x17.tecton_proto.common.IdR\rfeatureViewId\x12\x62\n\x18join_configuration_items\x18\x03 \x03(\x0b\x32(.tecton_proto.data.JoinConfigurationItemR\x16joinConfigurationItems\x12\x1c\n\tnamespace\x18\x04 \x01(\tR\tnamespace\x12\'\n\x0f\x66\x65\x61ture_columns\x18\x05 \x03(\tR\x0e\x66\x65\x61tureColumnsJ\x04\x08\x01\x10\x02J\x06\x08\xe8\x07\x10\xe9\x07\"s\n\x15JoinConfigurationItem\x12*\n\x11spine_column_name\x18\x01 \x01(\tR\x0fspineColumnName\x12.\n\x13package_column_name\x18\x02 \x01(\tR\x11packageColumnName*|\n\x12JoinKeyBindingType\x12!\n\x1dJOIN_KEY_BINDING_TYPE_UNKNOWN\x10\x00\x12\x1f\n\x1bJOIN_KEY_BINDING_TYPE_BOUND\x10\x01\x12\"\n\x1eJOIN_KEY_BINDING_TYPE_WILDCARD\x10\x02\x42\x13\n\x0f\x63om.tecton.dataP\x01')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\'tecton_proto/data/feature_service.proto\x12\x11tecton_proto.data\x1a\'tecton_proto/validation/validator.proto\x1a#tecton_proto/common/data_type.proto\x1a\'tecton_proto/args/feature_service.proto\x1a$tecton_proto/data/fco_metadata.proto\x1a\x1ctecton_proto/common/id.proto\x1a$tecton_proto/data/odfv_compute.proto\"\xcb\x04\n\x0e\x46\x65\x61tureService\x12\x45\n\x12\x66\x65\x61ture_service_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x10\x66\x65\x61tureServiceId\x12M\n\x11\x66\x65\x61ture_set_items\x18\x02 \x03(\x0b\x32!.tecton_proto.data.FeatureSetItemR\x0f\x66\x65\x61tureSetItems\x12\x41\n\x0c\x66\x63o_metadata\x18\t \x01(\x0b\x32\x1e.tecton_proto.data.FcoMetadataR\x0b\x66\x63oMetadata\x12\x34\n\x16online_serving_enabled\x18\x0b \x01(\x08R\x14onlineServingEnabled\x12>\n\x07logging\x18\x0c \x01(\x0b\x32$.tecton_proto.args.LoggingConfigArgsR\x07logging\x12^\n\x0fvalidation_args\x18\r \x01(\x0b\x32\x35.tecton_proto.validation.FeatureServiceValidationArgsR\x0evalidationArgs\x12Z\n\x15on_demand_environment\x18\x0f \x01(\x0b\x32&.tecton_proto.data.OnlineComputeConfigR\x13onDemandEnvironmentJ\x04\x08\x03\x10\x04J\x04\x08\x04\x10\x05J\x04\x08\x05\x10\x06J\x04\x08\x06\x10\x07J\x04\x08\x07\x10\x08J\x04\x08\x08\x10\tJ\x04\x08\n\x10\x0bJ\x04\x08\x0e\x10\x0f\"\xca\x01\n\x10JoinKeyComponent\x12*\n\x11spine_column_name\x18\x01 \x01(\tR\x0fspineColumnName\x12H\n\x0c\x62inding_type\x18\x02 \x01(\x0e\x32%.tecton_proto.data.JoinKeyBindingTypeR\x0b\x62indingType\x12:\n\tdata_type\x18\x04 \x01(\x0b\x32\x1d.tecton_proto.common.DataTypeR\x08\x64\x61taTypeJ\x04\x08\x03\x10\x04\"V\n\x0fJoinKeyTemplate\x12\x43\n\ncomponents\x18\x01 \x03(\x0b\x32#.tecton_proto.data.JoinKeyComponentR\ncomponents\"\x8a\x02\n\x0e\x46\x65\x61tureSetItem\x12?\n\x0f\x66\x65\x61ture_view_id\x18\x06 \x01(\x0b\x32\x17.tecton_proto.common.IdR\rfeatureViewId\x12\x62\n\x18join_configuration_items\x18\x03 \x03(\x0b\x32(.tecton_proto.data.JoinConfigurationItemR\x16joinConfigurationItems\x12\x1c\n\tnamespace\x18\x04 \x01(\tR\tnamespace\x12\'\n\x0f\x66\x65\x61ture_columns\x18\x05 \x03(\tR\x0e\x66\x65\x61tureColumnsJ\x04\x08\x01\x10\x02J\x06\x08\xe8\x07\x10\xe9\x07\"s\n\x15JoinConfigurationItem\x12*\n\x11spine_column_name\x18\x01 \x01(\tR\x0fspineColumnName\x12.\n\x13package_column_name\x18\x02 \x01(\tR\x11packageColumnName*|\n\x12JoinKeyBindingType\x12!\n\x1dJOIN_KEY_BINDING_TYPE_UNKNOWN\x10\x00\x12\x1f\n\x1bJOIN_KEY_BINDING_TYPE_BOUND\x10\x01\x12\"\n\x1eJOIN_KEY_BINDING_TYPE_WILDCARD\x10\x02\x42\x13\n\x0f\x63om.tecton.dataP\x01')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.data.feature_service_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\017com.tecton.dataP\001'
-  _JOINKEYBINDINGTYPE._serialized_start=1420
-  _JOINKEYBINDINGTYPE._serialized_end=1544
-  _FEATURESERVICE._serialized_start=250
-  _FEATURESERVICE._serialized_end=739
-  _JOINKEYCOMPONENT._serialized_start=742
-  _JOINKEYCOMPONENT._serialized_end=944
-  _JOINKEYTEMPLATE._serialized_start=946
-  _JOINKEYTEMPLATE._serialized_end=1032
-  _FEATURESETITEM._serialized_start=1035
-  _FEATURESETITEM._serialized_end=1301
-  _JOINCONFIGURATIONITEM._serialized_start=1303
-  _JOINCONFIGURATIONITEM._serialized_end=1418
+  _JOINKEYBINDINGTYPE._serialized_start=1556
+  _JOINKEYBINDINGTYPE._serialized_end=1680
+  _FEATURESERVICE._serialized_start=288
+  _FEATURESERVICE._serialized_end=875
+  _JOINKEYCOMPONENT._serialized_start=878
+  _JOINKEYCOMPONENT._serialized_end=1080
+  _JOINKEYTEMPLATE._serialized_start=1082
+  _JOINKEYTEMPLATE._serialized_end=1168
+  _FEATURESETITEM._serialized_start=1171
+  _FEATURESETITEM._serialized_end=1437
+  _JOINCONFIGURATIONITEM._serialized_start=1439
+  _JOINCONFIGURATIONITEM._serialized_end=1554
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/data/feature_store_pb2.py` & `tecton-0.7.0rc0/tecton_proto/data/feature_store_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/data/feature_view_pb2.py` & `tecton-0.7.0rc0/tecton_proto/data/feature_view_pb2.py`

 * *Files 2% similar despite different names*

```diff
@@ -18,67 +18,70 @@
 from tecton_proto.common import aggregation_function_pb2 as tecton__proto_dot_common_dot_aggregation__function__pb2
 from tecton_proto.data import fv_materialization_pb2 as tecton__proto_dot_data_dot_fv__materialization__pb2
 from tecton_proto.common import schema_pb2 as tecton__proto_dot_common_dot_schema__pb2
 from tecton_proto.common import data_source_type_pb2 as tecton__proto_dot_common_dot_data__source__type__pb2
 from tecton_proto.common import framework_version_pb2 as tecton__proto_dot_common_dot_framework__version__pb2
 from tecton_proto.data import fco_metadata_pb2 as tecton__proto_dot_data_dot_fco__metadata__pb2
 from tecton_proto.common import id_pb2 as tecton__proto_dot_common_dot_id__pb2
+from tecton_proto.data import odfv_compute_pb2 as tecton__proto_dot_data_dot_odfv__compute__pb2
 from tecton_proto.validation import validator_pb2 as tecton__proto_dot_validation_dot_validator__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n$tecton_proto/data/feature_view.proto\x12\x11tecton_proto.data\x1a\x1egoogle/protobuf/duration.proto\x1a\x1fgoogle/protobuf/timestamp.proto\x1a tecton_proto/args/pipeline.proto\x1a$tecton_proto/args/feature_view.proto\x1a.tecton_proto/common/aggregation_function.proto\x1a*tecton_proto/data/fv_materialization.proto\x1a tecton_proto/common/schema.proto\x1a*tecton_proto/common/data_source_type.proto\x1a+tecton_proto/common/framework_version.proto\x1a$tecton_proto/data/fco_metadata.proto\x1a\x1ctecton_proto/common/id.proto\x1a\'tecton_proto/validation/validator.proto\"\xb1\r\n\x0b\x46\x65\x61tureView\x12?\n\x0f\x66\x65\x61ture_view_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\rfeatureViewId\x12\x41\n\x0c\x66\x63o_metadata\x18\x02 \x01(\x0b\x32\x1e.tecton_proto.data.FcoMetadataR\x0b\x66\x63oMetadata\x12\x36\n\nentity_ids\x18\x03 \x03(\x0b\x32\x17.tecton_proto.common.IdR\tentityIds\x12\x1b\n\tjoin_keys\x18\x04 \x03(\tR\x08joinKeys\x12?\n\x07schemas\x18\x0b \x01(\x0b\x32%.tecton_proto.data.FeatureViewSchemasR\x07schemas\x12P\n\x0b\x65nrichments\x18\xe8\x07 \x01(\x0b\x32).tecton_proto.data.FeatureViewEnrichmentsB\x02\x18\x01R\x0b\x65nrichments\x12U\n\x12temporal_aggregate\x18\x07 \x01(\x0b\x32$.tecton_proto.data.TemporalAggregateH\x00R\x11temporalAggregate\x12\x39\n\x08temporal\x18\x08 \x01(\x0b\x32\x1b.tecton_proto.data.TemporalH\x00R\x08temporal\x12]\n\x16on_demand_feature_view\x18\x11 \x01(\x0b\x32&.tecton_proto.data.OnDemandFeatureViewH\x00R\x13onDemandFeatureView\x12\x46\n\rfeature_table\x18\x10 \x01(\x0b\x32\x1f.tecton_proto.data.FeatureTableH\x00R\x0c\x66\x65\x61tureTable\x12#\n\rtimestamp_key\x18\n \x01(\tR\x0ctimestampKey\x12W\n\x14online_serving_index\x18\x05 \x01(\x0b\x32%.tecton_proto.data.OnlineServingIndexR\x12onlineServingIndex\x12\x37\n\x08pipeline\x18\x06 \x01(\x0b\x32\x1b.tecton_proto.args.PipelineR\x08pipeline\x12\x62\n\x16materialization_params\x18\t \x01(\x0b\x32+.tecton_proto.data.NewMaterializationParamsR\x15materializationParams\x12\x37\n\x17materialization_enabled\x18\x0c \x01(\x08R\x16materializationEnabled\x12}\n!materialization_state_transitions\x18\r \x03(\x0b\x32\x31.tecton_proto.data.MaterializationStateTransitionR\x1fmaterializationStateTransitions\x12P\n\x11monitoring_params\x18\x0e \x01(\x0b\x32#.tecton_proto.data.MonitoringParamsR\x10monitoringParams\x12?\n\x1c\x66\x65\x61ture_store_format_version\x18\x17 \x01(\x05R\x19\x66\x65\x61tureStoreFormatVersion\x12G\n\x0esnowflake_data\x18\x18 \x01(\x0b\x32 .tecton_proto.data.SnowflakeDataR\rsnowflakeData\x12/\n\x11\x66ramework_version\x18\x13 \x01(\x05\x42\x02\x18\x01R\x10\x66rameworkVersion\x12H\n\nfw_version\x18\x1a \x01(\x0e\x32%.tecton_proto.common.FrameworkVersionB\x02\x18\x01R\tfwVersion\x12\x17\n\x07web_url\x18\x19 \x01(\tR\x06webUrl\x12H\n\rbatch_trigger\x18\x1b \x01(\x0e\x32#.tecton_proto.args.BatchTriggerTypeR\x0c\x62\x61tchTrigger\x12[\n\x0fvalidation_args\x18\x1c \x01(\x0b\x32\x32.tecton_proto.validation.FeatureViewValidationArgsR\x0evalidationArgsB\x13\n\x11\x66\x65\x61ture_view_type\"\xbe\x02\n\x11TemporalAggregate\x12@\n\x0eslide_interval\x18\x01 \x01(\x0b\x32\x19.google.protobuf.DurationR\rslideInterval\x12\x32\n\x15slide_interval_string\x18\x05 \x01(\tR\x13slideIntervalString\x12?\n\x08\x66\x65\x61tures\x18\x02 \x03(\x0b\x32#.tecton_proto.data.AggregateFeatureR\x08\x66\x65\x61tures\x12#\n\ris_continuous\x18\x03 \x01(\x08R\x0cisContinuous\x12M\n\x10\x64\x61ta_source_type\x18\x04 \x01(\x0e\x32#.tecton_proto.common.DataSourceTypeR\x0e\x64\x61taSourceType\"\xc2\x02\n\x10\x41ggregateFeature\x12,\n\x12input_feature_name\x18\x01 \x01(\tR\x10inputFeatureName\x12.\n\x13output_feature_name\x18\x02 \x01(\tR\x11outputFeatureName\x12\x44\n\x08\x66unction\x18\x03 \x01(\x0e\x32(.tecton_proto.common.AggregationFunctionR\x08\x66unction\x12W\n\x0f\x66unction_params\x18\x05 \x01(\x0b\x32..tecton_proto.common.AggregationFunctionParamsR\x0e\x66unctionParams\x12\x31\n\x06window\x18\x04 \x01(\x0b\x32\x19.google.protobuf.DurationR\x06window\"\xf5\x01\n\x1dTrailingTimeWindowAggregation\x12\x19\n\x08time_key\x18\x01 \x01(\tR\x07timeKey\x12S\n\x18\x61ggregation_slide_period\x18\x02 \x01(\x0b\x32\x19.google.protobuf.DurationR\x16\x61ggregationSlidePeriod\x12?\n\x08\x66\x65\x61tures\x18\x03 \x03(\x0b\x32#.tecton_proto.data.AggregateFeatureR\x08\x66\x65\x61tures\x12#\n\ris_continuous\x18\x04 \x01(\x08R\x0cisContinuous\"\xbb\x02\n\x08Temporal\x12:\n\x0bserving_ttl\x18\x10 \x01(\x0b\x32\x19.google.protobuf.DurationR\nservingTtl\x12M\n\x10\x64\x61ta_source_type\x18\x11 \x01(\x0e\x32#.tecton_proto.common.DataSourceTypeR\x0e\x64\x61taSourceType\x12J\n\x0f\x62\x61\x63kfill_config\x18\x12 \x01(\x0b\x32!.tecton_proto.args.BackfillConfigR\x0e\x62\x61\x63kfillConfig\x12\x33\n\x15incremental_backfills\x18\x13 \x01(\x08R\x14incrementalBackfills\x12#\n\ris_continuous\x18\x14 \x01(\x08R\x0cisContinuous\"\x9a\x01\n\x0c\x46\x65\x61tureTable\x12%\n\x0eonline_enabled\x18\x01 \x01(\x08R\ronlineEnabled\x12\'\n\x0foffline_enabled\x18\x02 \x01(\x08R\x0eofflineEnabled\x12:\n\x0bserving_ttl\x18\x03 \x01(\x0b\x32\x19.google.protobuf.DurationR\nservingTtl\"*\n\x13OnDemandFeatureView\x12\x13\n\x05no_op\x18\x01 \x01(\x08R\x04noOp\"\xa6\x01\n\x12\x46\x65\x61tureViewSchemas\x12<\n\x0bview_schema\x18\x01 \x01(\x0b\x32\x1b.tecton_proto.common.SchemaR\nviewSchema\x12R\n\x16materialization_schema\x18\x02 \x01(\x0b\x32\x1b.tecton_proto.common.SchemaR\x15materializationSchema\"1\n\x12OnlineServingIndex\x12\x1b\n\tjoin_keys\x18\x01 \x03(\tR\x08joinKeys\"\xc4\x02\n\x1eMaterializationStateTransition\x12\x38\n\ttimestamp\x18\x01 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\ttimestamp\x12%\n\x0eonline_enabled\x18\x02 \x01(\x08R\ronlineEnabled\x12\'\n\x0foffline_enabled\x18\x03 \x01(\x08R\x0eofflineEnabled\x12R\n\x17\x66\x65\x61ture_start_timestamp\x18\x04 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x15\x66\x65\x61tureStartTimestamp\x12\x44\n\x1ematerialization_serial_version\x18\x05 \x01(\x05R\x1cmaterializationSerialVersion\"\xaf\x01\n\x19ParquetOfflineStoreParams\x12I\n\x13time_partition_size\x18\x01 \x01(\x0b\x32\x19.google.protobuf.DurationR\x11timePartitionSize\x12G\n\x07version\x18\x02 \x01(\x0e\x32-.tecton_proto.data.ParquetOfflineStoreVersionR\x07version\"\xab\x01\n\x17\x44\x65ltaOfflineStoreParams\x12I\n\x13time_partition_size\x18\x01 \x01(\x0b\x32\x19.google.protobuf.DurationR\x11timePartitionSize\x12\x45\n\x07version\x18\x02 \x01(\x0e\x32+.tecton_proto.data.DeltaOfflineStoreVersionR\x07version\"\xac\x01\n\x12OfflineStoreParams\x12H\n\x07parquet\x18\x01 \x01(\x0b\x32,.tecton_proto.data.ParquetOfflineStoreParamsH\x00R\x07parquet\x12\x42\n\x05\x64\x65lta\x18\x02 \x01(\x0b\x32*.tecton_proto.data.DeltaOfflineStoreParamsH\x00R\x05\x64\x65ltaB\x08\n\x06params\"\x98\t\n\x18NewMaterializationParams\x12\x46\n\x11schedule_interval\x18\x01 \x01(\x0b\x32\x19.google.protobuf.DurationR\x10scheduleInterval\x12\x62\n\x1fmaterialization_start_timestamp\x18\x02 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x1dmaterializationStartTimestamp\x12R\n\x17\x66\x65\x61ture_start_timestamp\x18\r \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x15\x66\x65\x61tureStartTimestamp\x12^\n\x1emax_batch_aggregation_interval\x18\x03 \x01(\x0b\x32\x19.google.protobuf.DurationR\x1bmaxBatchAggregationInterval\x12\x33\n\x16writes_to_online_store\x18\x04 \x01(\x08R\x13writesToOnlineStore\x12\x35\n\x17writes_to_offline_store\x18\x05 \x01(\x08R\x14writesToOfflineStore\x12^\n\x14offline_store_config\x18\x06 \x01(\x0b\x32,.tecton_proto.args.OfflineFeatureStoreConfigR\x12offlineStoreConfig\x12W\n\x14offline_store_params\x18\x0e \x01(\x0b\x32%.tecton_proto.data.OfflineStoreParamsR\x12offlineStoreParams\x12U\n\x15\x62\x61tch_materialization\x18\x07 \x01(\x0b\x32 .tecton_proto.args.ClusterConfigR\x14\x62\x61tchMaterialization\x12W\n\x16stream_materialization\x18\x08 \x01(\x0b\x32 .tecton_proto.args.ClusterConfigR\x15streamMaterialization\x12L\n\x15max_source_data_delay\x18\t \x01(\x0b\x32\x19.google.protobuf.DurationR\x12maxSourceDataDelay\x12T\n\x13online_store_params\x18\n \x01(\x0b\x32$.tecton_proto.data.OnlineStoreParamsR\x11onlineStoreParams\x12\x44\n\routput_stream\x18\x0b \x01(\x0b\x32\x1f.tecton_proto.args.OutputStreamR\x0coutputStream\x12]\n\x11time_range_policy\x18\x0c \x01(\x0e\x32\x31.tecton_proto.data.MaterializationTimeRangePolicyR\x0ftimeRangePolicy\"\xc7\x01\n\x11OnlineStoreParams\x12%\n\x0euser_specified\x18\x01 \x01(\x08R\ruserSpecified\x12@\n\x06\x64ynamo\x18\x02 \x01(\x0b\x32&.tecton_proto.args.DynamoDbOnlineStoreH\x00R\x06\x64ynamo\x12;\n\x05redis\x18\x03 \x01(\x0b\x32#.tecton_proto.args.RedisOnlineStoreH\x00R\x05redisB\x0c\n\nstore_type\"\x92\x02\n\x10MonitoringParams\x12%\n\x0euser_specified\x18\x01 \x01(\x08R\ruserSpecified\x12+\n\x11monitor_freshness\x18\x02 \x01(\x08R\x10monitorFreshness\x12W\n\x1a\x65xpected_feature_freshness\x18\x03 \x01(\x0b\x32\x19.google.protobuf.DurationR\x18\x65xpectedFeatureFreshness\x12\x1f\n\x0b\x61lert_email\x18\x04 \x01(\tR\nalertEmail\x12\x30\n\x14grace_period_seconds\x18\x05 \x01(\x05R\x12gracePeriodSeconds\"\x85\x01\n\x16\x46\x65\x61tureViewEnrichments\x12S\n\x12\x66p_materialization\x18\x04 \x01(\x0b\x32$.tecton_proto.data.FvMaterializationR\x11\x66pMaterializationJ\x04\x08\x01\x10\x02J\x04\x08\x02\x10\x03J\x04\x08\x03\x10\x04J\x04\x08\x05\x10\x06\"?\n\rSnowflakeData\x12.\n\x13snowflake_view_name\x18\x01 \x01(\tR\x11snowflakeViewName*\x91\x01\n\x1aParquetOfflineStoreVersion\x12)\n%PARQUET_OFFLINE_STORE_VERSION_UNKNOWN\x10\x00\x12#\n\x1fPARQUET_OFFLINE_STORE_VERSION_1\x10\x01\x12#\n\x1fPARQUET_OFFLINE_STORE_VERSION_2\x10\x02*f\n\x18\x44\x65ltaOfflineStoreVersion\x12\'\n#DELTA_OFFLINE_STORE_VERSION_UNKNOWN\x10\x00\x12!\n\x1d\x44\x45LTA_OFFLINE_STORE_VERSION_1\x10\x01*\xc6\x01\n\x1eMaterializationTimeRangePolicy\x12\x31\n-MATERIALIZATION_TIME_RANGE_POLICY_UNSPECIFIED\x10\x00\x12:\n6MATERIALIZATION_TIME_RANGE_POLICY_FAIL_IF_OUT_OF_RANGE\x10\x01\x12\x35\n1MATERIALIZATION_TIME_RANGE_POLICY_FILTER_TO_RANGE\x10\x02\x42\x13\n\x0f\x63om.tecton.dataP\x01')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n$tecton_proto/data/feature_view.proto\x12\x11tecton_proto.data\x1a\x1egoogle/protobuf/duration.proto\x1a\x1fgoogle/protobuf/timestamp.proto\x1a tecton_proto/args/pipeline.proto\x1a$tecton_proto/args/feature_view.proto\x1a.tecton_proto/common/aggregation_function.proto\x1a*tecton_proto/data/fv_materialization.proto\x1a tecton_proto/common/schema.proto\x1a*tecton_proto/common/data_source_type.proto\x1a+tecton_proto/common/framework_version.proto\x1a$tecton_proto/data/fco_metadata.proto\x1a\x1ctecton_proto/common/id.proto\x1a$tecton_proto/data/odfv_compute.proto\x1a\'tecton_proto/validation/validator.proto\"\x87\x0e\n\x0b\x46\x65\x61tureView\x12?\n\x0f\x66\x65\x61ture_view_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\rfeatureViewId\x12\x41\n\x0c\x66\x63o_metadata\x18\x02 \x01(\x0b\x32\x1e.tecton_proto.data.FcoMetadataR\x0b\x66\x63oMetadata\x12\x36\n\nentity_ids\x18\x03 \x03(\x0b\x32\x17.tecton_proto.common.IdR\tentityIds\x12\x1b\n\tjoin_keys\x18\x04 \x03(\tR\x08joinKeys\x12?\n\x07schemas\x18\x0b \x01(\x0b\x32%.tecton_proto.data.FeatureViewSchemasR\x07schemas\x12P\n\x0b\x65nrichments\x18\xe8\x07 \x01(\x0b\x32).tecton_proto.data.FeatureViewEnrichmentsB\x02\x18\x01R\x0b\x65nrichments\x12U\n\x12temporal_aggregate\x18\x07 \x01(\x0b\x32$.tecton_proto.data.TemporalAggregateH\x00R\x11temporalAggregate\x12\x39\n\x08temporal\x18\x08 \x01(\x0b\x32\x1b.tecton_proto.data.TemporalH\x00R\x08temporal\x12]\n\x16on_demand_feature_view\x18\x11 \x01(\x0b\x32&.tecton_proto.data.OnDemandFeatureViewH\x00R\x13onDemandFeatureView\x12\x46\n\rfeature_table\x18\x10 \x01(\x0b\x32\x1f.tecton_proto.data.FeatureTableH\x00R\x0c\x66\x65\x61tureTable\x12#\n\rtimestamp_key\x18\n \x01(\tR\x0ctimestampKey\x12W\n\x14online_serving_index\x18\x05 \x01(\x0b\x32%.tecton_proto.data.OnlineServingIndexR\x12onlineServingIndex\x12\x37\n\x08pipeline\x18\x06 \x01(\x0b\x32\x1b.tecton_proto.args.PipelineR\x08pipeline\x12\x62\n\x16materialization_params\x18\t \x01(\x0b\x32+.tecton_proto.data.NewMaterializationParamsR\x15materializationParams\x12\x37\n\x17materialization_enabled\x18\x0c \x01(\x08R\x16materializationEnabled\x12}\n!materialization_state_transitions\x18\r \x03(\x0b\x32\x31.tecton_proto.data.MaterializationStateTransitionR\x1fmaterializationStateTransitions\x12P\n\x11monitoring_params\x18\x0e \x01(\x0b\x32#.tecton_proto.data.MonitoringParamsR\x10monitoringParams\x12?\n\x1c\x66\x65\x61ture_store_format_version\x18\x17 \x01(\x05R\x19\x66\x65\x61tureStoreFormatVersion\x12G\n\x0esnowflake_data\x18\x18 \x01(\x0b\x32 .tecton_proto.data.SnowflakeDataR\rsnowflakeData\x12/\n\x11\x66ramework_version\x18\x13 \x01(\x05\x42\x02\x18\x01R\x10\x66rameworkVersion\x12H\n\nfw_version\x18\x1a \x01(\x0e\x32%.tecton_proto.common.FrameworkVersionB\x02\x18\x01R\tfwVersion\x12\x17\n\x07web_url\x18\x19 \x01(\tR\x06webUrl\x12H\n\rbatch_trigger\x18\x1b \x01(\x0e\x32#.tecton_proto.args.BatchTriggerTypeR\x0c\x62\x61tchTrigger\x12[\n\x0fvalidation_args\x18\x1c \x01(\x0b\x32\x32.tecton_proto.validation.FeatureViewValidationArgsR\x0evalidationArgs\x12T\n\x13\x64\x61ta_quality_config\x18\x1d \x01(\x0b\x32$.tecton_proto.data.DataQualityConfigR\x11\x64\x61taQualityConfigB\x13\n\x11\x66\x65\x61ture_view_type\"\xbe\x02\n\x11TemporalAggregate\x12@\n\x0eslide_interval\x18\x01 \x01(\x0b\x32\x19.google.protobuf.DurationR\rslideInterval\x12\x32\n\x15slide_interval_string\x18\x05 \x01(\tR\x13slideIntervalString\x12?\n\x08\x66\x65\x61tures\x18\x02 \x03(\x0b\x32#.tecton_proto.data.AggregateFeatureR\x08\x66\x65\x61tures\x12#\n\ris_continuous\x18\x03 \x01(\x08R\x0cisContinuous\x12M\n\x10\x64\x61ta_source_type\x18\x04 \x01(\x0e\x32#.tecton_proto.common.DataSourceTypeR\x0e\x64\x61taSourceType\"\xc2\x02\n\x10\x41ggregateFeature\x12,\n\x12input_feature_name\x18\x01 \x01(\tR\x10inputFeatureName\x12.\n\x13output_feature_name\x18\x02 \x01(\tR\x11outputFeatureName\x12\x44\n\x08\x66unction\x18\x03 \x01(\x0e\x32(.tecton_proto.common.AggregationFunctionR\x08\x66unction\x12W\n\x0f\x66unction_params\x18\x05 \x01(\x0b\x32..tecton_proto.common.AggregationFunctionParamsR\x0e\x66unctionParams\x12\x31\n\x06window\x18\x04 \x01(\x0b\x32\x19.google.protobuf.DurationR\x06window\"\xf5\x01\n\x1dTrailingTimeWindowAggregation\x12\x19\n\x08time_key\x18\x01 \x01(\tR\x07timeKey\x12S\n\x18\x61ggregation_slide_period\x18\x02 \x01(\x0b\x32\x19.google.protobuf.DurationR\x16\x61ggregationSlidePeriod\x12?\n\x08\x66\x65\x61tures\x18\x03 \x03(\x0b\x32#.tecton_proto.data.AggregateFeatureR\x08\x66\x65\x61tures\x12#\n\ris_continuous\x18\x04 \x01(\x08R\x0cisContinuous\"\xbb\x02\n\x08Temporal\x12:\n\x0bserving_ttl\x18\x10 \x01(\x0b\x32\x19.google.protobuf.DurationR\nservingTtl\x12M\n\x10\x64\x61ta_source_type\x18\x11 \x01(\x0e\x32#.tecton_proto.common.DataSourceTypeR\x0e\x64\x61taSourceType\x12J\n\x0f\x62\x61\x63kfill_config\x18\x12 \x01(\x0b\x32!.tecton_proto.args.BackfillConfigR\x0e\x62\x61\x63kfillConfig\x12\x33\n\x15incremental_backfills\x18\x13 \x01(\x08R\x14incrementalBackfills\x12#\n\ris_continuous\x18\x14 \x01(\x08R\x0cisContinuous\"\x9a\x01\n\x0c\x46\x65\x61tureTable\x12%\n\x0eonline_enabled\x18\x01 \x01(\x08R\ronlineEnabled\x12\'\n\x0foffline_enabled\x18\x02 \x01(\x08R\x0eofflineEnabled\x12:\n\x0bserving_ttl\x18\x03 \x01(\x0b\x32\x19.google.protobuf.DurationR\nservingTtl\"\x8f\x01\n\x13OnDemandFeatureView\x12\x13\n\x05no_op\x18\x01 \x01(\x08R\x04noOp\x12]\n\x16supported_environments\x18\x03 \x03(\x0b\x32&.tecton_proto.data.RemoteComputeConfigR\x15supportedEnvironmentsJ\x04\x08\x02\x10\x03\"\xa6\x01\n\x12\x46\x65\x61tureViewSchemas\x12<\n\x0bview_schema\x18\x01 \x01(\x0b\x32\x1b.tecton_proto.common.SchemaR\nviewSchema\x12R\n\x16materialization_schema\x18\x02 \x01(\x0b\x32\x1b.tecton_proto.common.SchemaR\x15materializationSchema\"1\n\x12OnlineServingIndex\x12\x1b\n\tjoin_keys\x18\x01 \x03(\tR\x08joinKeys\"\xfd\x02\n\x1eMaterializationStateTransition\x12\x38\n\ttimestamp\x18\x01 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\ttimestamp\x12%\n\x0eonline_enabled\x18\x02 \x01(\x08R\ronlineEnabled\x12\'\n\x0foffline_enabled\x18\x03 \x01(\x08R\x0eofflineEnabled\x12R\n\x17\x66\x65\x61ture_start_timestamp\x18\x04 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x15\x66\x65\x61tureStartTimestamp\x12\x44\n\x1ematerialization_serial_version\x18\x05 \x01(\x05R\x1cmaterializationSerialVersion\x12\x37\n\x18\x66orce_stream_job_restart\x18\x06 \x01(\x08R\x15\x66orceStreamJobRestart\"\xaf\x01\n\x19ParquetOfflineStoreParams\x12I\n\x13time_partition_size\x18\x01 \x01(\x0b\x32\x19.google.protobuf.DurationR\x11timePartitionSize\x12G\n\x07version\x18\x02 \x01(\x0e\x32-.tecton_proto.data.ParquetOfflineStoreVersionR\x07version\"\xab\x01\n\x17\x44\x65ltaOfflineStoreParams\x12I\n\x13time_partition_size\x18\x01 \x01(\x0b\x32\x19.google.protobuf.DurationR\x11timePartitionSize\x12\x45\n\x07version\x18\x02 \x01(\x0e\x32+.tecton_proto.data.DeltaOfflineStoreVersionR\x07version\"\xac\x01\n\x12OfflineStoreParams\x12H\n\x07parquet\x18\x01 \x01(\x0b\x32,.tecton_proto.data.ParquetOfflineStoreParamsH\x00R\x07parquet\x12\x42\n\x05\x64\x65lta\x18\x02 \x01(\x0b\x32*.tecton_proto.data.DeltaOfflineStoreParamsH\x00R\x05\x64\x65ltaB\x08\n\x06params\"\xf5\t\n\x18NewMaterializationParams\x12\x46\n\x11schedule_interval\x18\x01 \x01(\x0b\x32\x19.google.protobuf.DurationR\x10scheduleInterval\x12\x62\n\x1fmaterialization_start_timestamp\x18\x02 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x1dmaterializationStartTimestamp\x12R\n\x17\x66\x65\x61ture_start_timestamp\x18\r \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x15\x66\x65\x61tureStartTimestamp\x12l\n%manual_trigger_backfill_end_timestamp\x18\x0f \x01(\x0b\x32\x1a.google.protobuf.TimestampR!manualTriggerBackfillEndTimestamp\x12M\n\x15max_backfill_interval\x18\x03 \x01(\x0b\x32\x19.google.protobuf.DurationR\x13maxBackfillInterval\x12\x33\n\x16writes_to_online_store\x18\x04 \x01(\x08R\x13writesToOnlineStore\x12\x35\n\x17writes_to_offline_store\x18\x05 \x01(\x08R\x14writesToOfflineStore\x12^\n\x14offline_store_config\x18\x06 \x01(\x0b\x32,.tecton_proto.args.OfflineFeatureStoreConfigR\x12offlineStoreConfig\x12W\n\x14offline_store_params\x18\x0e \x01(\x0b\x32%.tecton_proto.data.OfflineStoreParamsR\x12offlineStoreParams\x12U\n\x15\x62\x61tch_materialization\x18\x07 \x01(\x0b\x32 .tecton_proto.args.ClusterConfigR\x14\x62\x61tchMaterialization\x12W\n\x16stream_materialization\x18\x08 \x01(\x0b\x32 .tecton_proto.args.ClusterConfigR\x15streamMaterialization\x12L\n\x15max_source_data_delay\x18\t \x01(\x0b\x32\x19.google.protobuf.DurationR\x12maxSourceDataDelay\x12T\n\x13online_store_params\x18\n \x01(\x0b\x32$.tecton_proto.data.OnlineStoreParamsR\x11onlineStoreParams\x12\x44\n\routput_stream\x18\x0b \x01(\x0b\x32\x1f.tecton_proto.args.OutputStreamR\x0coutputStream\x12]\n\x11time_range_policy\x18\x0c \x01(\x0e\x32\x31.tecton_proto.data.MaterializationTimeRangePolicyR\x0ftimeRangePolicy\"\x8d\x02\n\x11OnlineStoreParams\x12%\n\x0euser_specified\x18\x01 \x01(\x08R\ruserSpecified\x12@\n\x06\x64ynamo\x18\x02 \x01(\x0b\x32&.tecton_proto.args.DynamoDbOnlineStoreH\x00R\x06\x64ynamo\x12;\n\x05redis\x18\x03 \x01(\x0b\x32#.tecton_proto.args.RedisOnlineStoreH\x00R\x05redis\x12\x44\n\x08\x62igtable\x18\x04 \x01(\x0b\x32&.tecton_proto.args.BigtableOnlineStoreH\x00R\x08\x62igtableB\x0c\n\nstore_type\"\x92\x02\n\x10MonitoringParams\x12%\n\x0euser_specified\x18\x01 \x01(\x08R\ruserSpecified\x12+\n\x11monitor_freshness\x18\x02 \x01(\x08R\x10monitorFreshness\x12W\n\x1a\x65xpected_feature_freshness\x18\x03 \x01(\x0b\x32\x19.google.protobuf.DurationR\x18\x65xpectedFeatureFreshness\x12\x1f\n\x0b\x61lert_email\x18\x04 \x01(\tR\nalertEmail\x12\x30\n\x14grace_period_seconds\x18\x05 \x01(\x05R\x12gracePeriodSeconds\"\x85\x01\n\x16\x46\x65\x61tureViewEnrichments\x12S\n\x12\x66p_materialization\x18\x04 \x01(\x0b\x32$.tecton_proto.data.FvMaterializationR\x11\x66pMaterializationJ\x04\x08\x01\x10\x02J\x04\x08\x02\x10\x03J\x04\x08\x03\x10\x04J\x04\x08\x05\x10\x06\"?\n\rSnowflakeData\x12.\n\x13snowflake_view_name\x18\x01 \x01(\tR\x11snowflakeViewName\"\x81\x01\n\x11\x44\x61taQualityConfig\x12\x30\n\x14\x64\x61ta_quality_enabled\x18\x01 \x01(\x08R\x12\x64\x61taQualityEnabled\x12:\n\x19skip_default_expectations\x18\x02 \x01(\x08R\x17skipDefaultExpectations*\x91\x01\n\x1aParquetOfflineStoreVersion\x12)\n%PARQUET_OFFLINE_STORE_VERSION_UNKNOWN\x10\x00\x12#\n\x1fPARQUET_OFFLINE_STORE_VERSION_1\x10\x01\x12#\n\x1fPARQUET_OFFLINE_STORE_VERSION_2\x10\x02*f\n\x18\x44\x65ltaOfflineStoreVersion\x12\'\n#DELTA_OFFLINE_STORE_VERSION_UNKNOWN\x10\x00\x12!\n\x1d\x44\x45LTA_OFFLINE_STORE_VERSION_1\x10\x01*\xc6\x01\n\x1eMaterializationTimeRangePolicy\x12\x31\n-MATERIALIZATION_TIME_RANGE_POLICY_UNSPECIFIED\x10\x00\x12:\n6MATERIALIZATION_TIME_RANGE_POLICY_FAIL_IF_OUT_OF_RANGE\x10\x01\x12\x35\n1MATERIALIZATION_TIME_RANGE_POLICY_FILTER_TO_RANGE\x10\x02\x42\x13\n\x0f\x63om.tecton.dataP\x01')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.data.feature_view_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\017com.tecton.dataP\001'
   _FEATUREVIEW.fields_by_name['enrichments']._options = None
   _FEATUREVIEW.fields_by_name['enrichments']._serialized_options = b'\030\001'
   _FEATUREVIEW.fields_by_name['framework_version']._options = None
   _FEATUREVIEW.fields_by_name['framework_version']._serialized_options = b'\030\001'
   _FEATUREVIEW.fields_by_name['fw_version']._options = None
   _FEATUREVIEW.fields_by_name['fw_version']._serialized_options = b'\030\001'
-  _PARQUETOFFLINESTOREVERSION._serialized_start=6583
-  _PARQUETOFFLINESTOREVERSION._serialized_end=6728
-  _DELTAOFFLINESTOREVERSION._serialized_start=6730
-  _DELTAOFFLINESTOREVERSION._serialized_end=6832
-  _MATERIALIZATIONTIMERANGEPOLICY._serialized_start=6835
-  _MATERIALIZATIONTIMERANGEPOLICY._serialized_end=7033
-  _FEATUREVIEW._serialized_start=521
-  _FEATUREVIEW._serialized_end=2234
-  _TEMPORALAGGREGATE._serialized_start=2237
-  _TEMPORALAGGREGATE._serialized_end=2555
-  _AGGREGATEFEATURE._serialized_start=2558
-  _AGGREGATEFEATURE._serialized_end=2880
-  _TRAILINGTIMEWINDOWAGGREGATION._serialized_start=2883
-  _TRAILINGTIMEWINDOWAGGREGATION._serialized_end=3128
-  _TEMPORAL._serialized_start=3131
-  _TEMPORAL._serialized_end=3446
-  _FEATURETABLE._serialized_start=3449
-  _FEATURETABLE._serialized_end=3603
-  _ONDEMANDFEATUREVIEW._serialized_start=3605
-  _ONDEMANDFEATUREVIEW._serialized_end=3647
-  _FEATUREVIEWSCHEMAS._serialized_start=3650
-  _FEATUREVIEWSCHEMAS._serialized_end=3816
-  _ONLINESERVINGINDEX._serialized_start=3818
-  _ONLINESERVINGINDEX._serialized_end=3867
-  _MATERIALIZATIONSTATETRANSITION._serialized_start=3870
-  _MATERIALIZATIONSTATETRANSITION._serialized_end=4194
-  _PARQUETOFFLINESTOREPARAMS._serialized_start=4197
-  _PARQUETOFFLINESTOREPARAMS._serialized_end=4372
-  _DELTAOFFLINESTOREPARAMS._serialized_start=4375
-  _DELTAOFFLINESTOREPARAMS._serialized_end=4546
-  _OFFLINESTOREPARAMS._serialized_start=4549
-  _OFFLINESTOREPARAMS._serialized_end=4721
-  _NEWMATERIALIZATIONPARAMS._serialized_start=4724
-  _NEWMATERIALIZATIONPARAMS._serialized_end=5900
-  _ONLINESTOREPARAMS._serialized_start=5903
-  _ONLINESTOREPARAMS._serialized_end=6102
-  _MONITORINGPARAMS._serialized_start=6105
-  _MONITORINGPARAMS._serialized_end=6379
-  _FEATUREVIEWENRICHMENTS._serialized_start=6382
-  _FEATUREVIEWENRICHMENTS._serialized_end=6515
-  _SNOWFLAKEDATA._serialized_start=6517
-  _SNOWFLAKEDATA._serialized_end=6580
+  _PARQUETOFFLINESTOREVERSION._serialized_start=7161
+  _PARQUETOFFLINESTOREVERSION._serialized_end=7306
+  _DELTAOFFLINESTOREVERSION._serialized_start=7308
+  _DELTAOFFLINESTOREVERSION._serialized_end=7410
+  _MATERIALIZATIONTIMERANGEPOLICY._serialized_start=7413
+  _MATERIALIZATIONTIMERANGEPOLICY._serialized_end=7611
+  _FEATUREVIEW._serialized_start=559
+  _FEATUREVIEW._serialized_end=2358
+  _TEMPORALAGGREGATE._serialized_start=2361
+  _TEMPORALAGGREGATE._serialized_end=2679
+  _AGGREGATEFEATURE._serialized_start=2682
+  _AGGREGATEFEATURE._serialized_end=3004
+  _TRAILINGTIMEWINDOWAGGREGATION._serialized_start=3007
+  _TRAILINGTIMEWINDOWAGGREGATION._serialized_end=3252
+  _TEMPORAL._serialized_start=3255
+  _TEMPORAL._serialized_end=3570
+  _FEATURETABLE._serialized_start=3573
+  _FEATURETABLE._serialized_end=3727
+  _ONDEMANDFEATUREVIEW._serialized_start=3730
+  _ONDEMANDFEATUREVIEW._serialized_end=3873
+  _FEATUREVIEWSCHEMAS._serialized_start=3876
+  _FEATUREVIEWSCHEMAS._serialized_end=4042
+  _ONLINESERVINGINDEX._serialized_start=4044
+  _ONLINESERVINGINDEX._serialized_end=4093
+  _MATERIALIZATIONSTATETRANSITION._serialized_start=4096
+  _MATERIALIZATIONSTATETRANSITION._serialized_end=4477
+  _PARQUETOFFLINESTOREPARAMS._serialized_start=4480
+  _PARQUETOFFLINESTOREPARAMS._serialized_end=4655
+  _DELTAOFFLINESTOREPARAMS._serialized_start=4658
+  _DELTAOFFLINESTOREPARAMS._serialized_end=4829
+  _OFFLINESTOREPARAMS._serialized_start=4832
+  _OFFLINESTOREPARAMS._serialized_end=5004
+  _NEWMATERIALIZATIONPARAMS._serialized_start=5007
+  _NEWMATERIALIZATIONPARAMS._serialized_end=6276
+  _ONLINESTOREPARAMS._serialized_start=6279
+  _ONLINESTOREPARAMS._serialized_end=6548
+  _MONITORINGPARAMS._serialized_start=6551
+  _MONITORINGPARAMS._serialized_end=6825
+  _FEATUREVIEWENRICHMENTS._serialized_start=6828
+  _FEATUREVIEWENRICHMENTS._serialized_end=6961
+  _SNOWFLAKEDATA._serialized_start=6963
+  _SNOWFLAKEDATA._serialized_end=7026
+  _DATAQUALITYCONFIG._serialized_start=7029
+  _DATAQUALITYCONFIG._serialized_end=7158
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/data/freshness_status_pb2.py` & `tecton-0.7.0rc0/tecton_proto/data/freshness_status_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/data/fv_materialization_pb2.py` & `tecton-0.7.0rc0/tecton_proto/data/fv_materialization_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/data/hive_metastore_pb2.py` & `tecton-0.7.0rc0/tecton_proto/data/hive_metastore_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/data/internal_spark_cluster_status_pb2.py` & `tecton-0.7.0rc0/tecton_proto/data/internal_spark_cluster_status_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/data/materialization_status_pb2.py` & `tecton-0.7.0rc0/tecton_proto/data/materialization_status_pb2.py`

 * *Files 3% similar despite different names*

```diff
@@ -13,24 +13,24 @@
 
 from google.protobuf import duration_pb2 as google_dot_protobuf_dot_duration__pb2
 from google.protobuf import timestamp_pb2 as google_dot_protobuf_dot_timestamp__pb2
 from tecton_proto.consumption import consumption_pb2 as tecton__proto_dot_consumption_dot_consumption__pb2
 from tecton_proto.common import id_pb2 as tecton__proto_dot_common_dot_id__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n.tecton_proto/data/materialization_status.proto\x12\x11tecton_proto.data\x1a\x1egoogle/protobuf/duration.proto\x1a\x1fgoogle/protobuf/timestamp.proto\x1a*tecton_proto/consumption/consumption.proto\x1a\x1ctecton_proto/common/id.proto\"\xac\x0c\n\x1cMaterializationAttemptStatus\x12K\n\x10\x64\x61ta_source_type\x18\x01 \x01(\x0e\x32!.tecton_proto.data.DataSourceTypeR\x0e\x64\x61taSourceType\x12\x46\n\x11window_start_time\x18\x02 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x0fwindowStartTime\x12\x42\n\x0fwindow_end_time\x18\x03 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\rwindowEndTime\x12H\n\x12\x66\x65\x61ture_start_time\x18\x0e \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x10\x66\x65\x61tureStartTime\x12\x44\n\x10\x66\x65\x61ture_end_time\x18\x0f \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x0e\x66\x65\x61tureEndTime\x12\x62\n\x15materialization_state\x18\x04 \x01(\x0e\x32-.tecton_proto.data.MaterializationStatusStateR\x14materializationState\x12#\n\rstate_message\x18\n \x01(\tR\x0cstateMessage\x12-\n\x12termination_reason\x18\x0b \x01(\tR\x11terminationReason\x12\x32\n\x15spot_instance_failure\x18\x0c \x01(\x08R\x13spotInstanceFailure\x12O\n\x17materialization_task_id\x18\x05 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x15materializationTaskId\x12\x61\n\x1fmaterialization_task_created_at\x18\r \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x1cmaterializationTaskCreatedAt\x12%\n\x0e\x61ttempt_number\x18\x06 \x01(\x05R\rattemptNumber\x12I\n!spark_cluster_environment_version\x18\x07 \x01(\x05R\x1esparkClusterEnvironmentVersion\x12 \n\x0crun_page_url\x18\x08 \x01(\tR\nrunPageUrl\x12T\n\x10\x63onsumption_info\x18\x1a \x03(\x0b\x32).tecton_proto.consumption.ConsumptionInfoR\x0f\x63onsumptionInfo\x12H\n\x12\x61ttempt_created_at\x18\t \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x10\x61ttemptCreatedAt\x12\x30\n\x14is_permanent_failure\x18\x10 \x01(\x08R\x12isPermanentFailure\x12\x39\n\nretry_time\x18\x11 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\tretryTime\x12\x1a\n\x08progress\x18\x12 \x01(\x02R\x08progress\x12\x35\n\x08\x64uration\x18\x13 \x01(\x0b\x32\x19.google.protobuf.DurationR\x08\x64uration\x12,\n\x12\x61llow_forced_retry\x18\x14 \x01(\x08R\x10\x61llowForcedRetry\x12\x32\n\x15\x61llow_overwrite_retry\x18\x15 \x01(\x08R\x13\x61llowOverwriteRetry\x12!\n\x0c\x61llow_cancel\x18\x16 \x01(\x08R\x0b\x61llowCancel\x12@\n\x1dwrite_to_online_feature_store\x18\x17 \x01(\x08R\x19writeToOnlineFeatureStore\x12\x42\n\x1ewrite_to_offline_feature_store\x18\x18 \x01(\x08R\x1awriteToOfflineFeatureStoreJ\x04\x08\x19\x10\x1a\"\xca\x01\n\x15MaterializationStatus\x12\x45\n\x12\x66\x65\x61ture_package_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x10\x66\x65\x61turePackageId\x12j\n\x18materialization_attempts\x18\x02 \x03(\x0b\x32/.tecton_proto.data.MaterializationAttemptStatusR\x17materializationAttempts*\xa3\x01\n\x0e\x44\x61taSourceType\x12\x1c\n\x18\x44\x41TA_SOURCE_TYPE_UNKNOWN\x10\x00\x12\x1a\n\x16\x44\x41TA_SOURCE_TYPE_BATCH\x10\x01\x12\x1b\n\x17\x44\x41TA_SOURCE_TYPE_STREAM\x10\x02\x12\x1b\n\x17\x44\x41TA_SOURCE_TYPE_INGEST\x10\x03\x12\x1d\n\x19\x44\x41TA_SOURCE_TYPE_DELETION\x10\x04*\xb7\x03\n\x1aMaterializationStatusState\x12(\n$MATERIALIZATION_STATUS_STATE_UNKNOWN\x10\x00\x12*\n&MATERIALIZATION_STATUS_STATE_SCHEDULED\x10\x01\x12(\n$MATERIALIZATION_STATUS_STATE_PENDING\x10\x02\x12(\n$MATERIALIZATION_STATUS_STATE_RUNNING\x10\x03\x12(\n$MATERIALIZATION_STATUS_STATE_SUCCESS\x10\x04\x12&\n\"MATERIALIZATION_STATUS_STATE_ERROR\x10\x05\x12(\n$MATERIALIZATION_STATUS_STATE_DRAINED\x10\x06\x12>\n:MATERIALIZATION_STATUS_STATE_MANUAL_CANCELLATION_REQUESTED\x10\x07\x12\x33\n/MATERIALIZATION_STATUS_STATE_MANUALLY_CANCELLED\x10\x08\x42\x13\n\x0f\x63om.tecton.dataP\x01')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n.tecton_proto/data/materialization_status.proto\x12\x11tecton_proto.data\x1a\x1egoogle/protobuf/duration.proto\x1a\x1fgoogle/protobuf/timestamp.proto\x1a*tecton_proto/consumption/consumption.proto\x1a\x1ctecton_proto/common/id.proto\"\xb4\r\n\x1cMaterializationAttemptStatus\x12K\n\x10\x64\x61ta_source_type\x18\x01 \x01(\x0e\x32!.tecton_proto.data.DataSourceTypeR\x0e\x64\x61taSourceType\x12\x46\n\x11window_start_time\x18\x02 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x0fwindowStartTime\x12\x42\n\x0fwindow_end_time\x18\x03 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\rwindowEndTime\x12H\n\x12\x66\x65\x61ture_start_time\x18\x0e \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x10\x66\x65\x61tureStartTime\x12\x44\n\x10\x66\x65\x61ture_end_time\x18\x0f \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x0e\x66\x65\x61tureEndTime\x12\x62\n\x15materialization_state\x18\x04 \x01(\x0e\x32-.tecton_proto.data.MaterializationStatusStateR\x14materializationState\x12#\n\rstate_message\x18\n \x01(\tR\x0cstateMessage\x12-\n\x12termination_reason\x18\x0b \x01(\tR\x11terminationReason\x12\x32\n\x15spot_instance_failure\x18\x0c \x01(\x08R\x13spotInstanceFailure\x12O\n\x17materialization_task_id\x18\x05 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x15materializationTaskId\x12\x61\n\x1fmaterialization_task_created_at\x18\r \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x1cmaterializationTaskCreatedAt\x12%\n\x0e\x61ttempt_number\x18\x06 \x01(\x05R\rattemptNumber\x12I\n!spark_cluster_environment_version\x18\x07 \x01(\x05R\x1esparkClusterEnvironmentVersion\x12 \n\x0crun_page_url\x18\x08 \x01(\tR\nrunPageUrl\x12T\n\x10\x63onsumption_info\x18\x1a \x03(\x0b\x32).tecton_proto.consumption.ConsumptionInfoR\x0f\x63onsumptionInfo\x12\x61\n\x13\x61ttempt_consumption\x18\x1b \x01(\x0b\x32\x30.tecton_proto.consumption.AttemptConsumptionInfoR\x12\x61ttemptConsumption\x12H\n\x12\x61ttempt_created_at\x18\t \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x10\x61ttemptCreatedAt\x12\x30\n\x14is_permanent_failure\x18\x10 \x01(\x08R\x12isPermanentFailure\x12\x39\n\nretry_time\x18\x11 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\tretryTime\x12\x1a\n\x08progress\x18\x12 \x01(\x02R\x08progress\x12\x35\n\x08\x64uration\x18\x13 \x01(\x0b\x32\x19.google.protobuf.DurationR\x08\x64uration\x12,\n\x12\x61llow_forced_retry\x18\x14 \x01(\x08R\x10\x61llowForcedRetry\x12\x32\n\x15\x61llow_overwrite_retry\x18\x15 \x01(\x08R\x13\x61llowOverwriteRetry\x12!\n\x0c\x61llow_cancel\x18\x16 \x01(\x08R\x0b\x61llowCancel\x12#\n\rallow_restart\x18\x1c \x01(\x08R\x0c\x61llowRestart\x12@\n\x1dwrite_to_online_feature_store\x18\x17 \x01(\x08R\x19writeToOnlineFeatureStore\x12\x42\n\x1ewrite_to_offline_feature_store\x18\x18 \x01(\x08R\x1awriteToOfflineFeatureStoreJ\x04\x08\x19\x10\x1a\"\x80\x02\n\x15MaterializationStatus\x12\x45\n\x12\x66\x65\x61ture_package_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x10\x66\x65\x61turePackageId\x12j\n\x18materialization_attempts\x18\x02 \x03(\x0b\x32/.tecton_proto.data.MaterializationAttemptStatusR\x17materializationAttempts\x12\x34\n\x16schedule_error_message\x18\x03 \x01(\tR\x14scheduleErrorMessage*\xa3\x01\n\x0e\x44\x61taSourceType\x12\x1c\n\x18\x44\x41TA_SOURCE_TYPE_UNKNOWN\x10\x00\x12\x1a\n\x16\x44\x41TA_SOURCE_TYPE_BATCH\x10\x01\x12\x1b\n\x17\x44\x41TA_SOURCE_TYPE_STREAM\x10\x02\x12\x1b\n\x17\x44\x41TA_SOURCE_TYPE_INGEST\x10\x03\x12\x1d\n\x19\x44\x41TA_SOURCE_TYPE_DELETION\x10\x04*\xb7\x03\n\x1aMaterializationStatusState\x12(\n$MATERIALIZATION_STATUS_STATE_UNKNOWN\x10\x00\x12*\n&MATERIALIZATION_STATUS_STATE_SCHEDULED\x10\x01\x12(\n$MATERIALIZATION_STATUS_STATE_PENDING\x10\x02\x12(\n$MATERIALIZATION_STATUS_STATE_RUNNING\x10\x03\x12(\n$MATERIALIZATION_STATUS_STATE_SUCCESS\x10\x04\x12&\n\"MATERIALIZATION_STATUS_STATE_ERROR\x10\x05\x12(\n$MATERIALIZATION_STATUS_STATE_DRAINED\x10\x06\x12>\n:MATERIALIZATION_STATUS_STATE_MANUAL_CANCELLATION_REQUESTED\x10\x07\x12\x33\n/MATERIALIZATION_STATUS_STATE_MANUALLY_CANCELLED\x10\x08\x42\x13\n\x0f\x63om.tecton.dataP\x01')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.data.materialization_status_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\017com.tecton.dataP\001'
-  _DATASOURCETYPE._serialized_start=1997
-  _DATASOURCETYPE._serialized_end=2160
-  _MATERIALIZATIONSTATUSSTATE._serialized_start=2163
-  _MATERIALIZATIONSTATUSSTATE._serialized_end=2602
+  _DATASOURCETYPE._serialized_start=2187
+  _DATASOURCETYPE._serialized_end=2350
+  _MATERIALIZATIONSTATUSSTATE._serialized_start=2353
+  _MATERIALIZATIONSTATUSSTATE._serialized_end=2792
   _MATERIALIZATIONATTEMPTSTATUS._serialized_start=209
-  _MATERIALIZATIONATTEMPTSTATUS._serialized_end=1789
-  _MATERIALIZATIONSTATUS._serialized_start=1792
-  _MATERIALIZATIONSTATUS._serialized_end=1994
+  _MATERIALIZATIONATTEMPTSTATUS._serialized_end=1925
+  _MATERIALIZATIONSTATUS._serialized_start=1928
+  _MATERIALIZATIONSTATUS._serialized_end=2184
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/data/onboarding_pb2.py` & `tecton-0.7.0rc0/tecton_proto/data/onboarding_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/data/remote_spark_pb2.py` & `tecton-0.7.0rc0/tecton_proto/data/remote_spark_pb2.py`

 * *Files 8% similar despite different names*

```diff
@@ -18,54 +18,56 @@
 from tecton_proto.common import spark_schema_pb2 as tecton__proto_dot_common_dot_spark__schema__pb2
 from tecton_proto.data import feature_view_pb2 as tecton__proto_dot_data_dot_feature__view__pb2
 from tecton_proto.data import hive_metastore_pb2 as tecton__proto_dot_data_dot_hive__metastore__pb2
 from tecton_proto.data import virtual_data_source_pb2 as tecton__proto_dot_data_dot_virtual__data__source__pb2
 from tecton_proto.data import transformation_pb2 as tecton__proto_dot_data_dot_transformation__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n$tecton_proto/data/remote_spark.proto\x12\x11tecton_proto.data\x1a*tecton_proto/args/data_source_config.proto\x1a-tecton_proto/args/user_defined_function.proto\x1a$tecton_proto/args/feature_view.proto\x1a tecton_proto/common/schema.proto\x1a&tecton_proto/common/spark_schema.proto\x1a$tecton_proto/data/feature_view.proto\x1a&tecton_proto/data/hive_metastore.proto\x1a+tecton_proto/data/virtual_data_source.proto\x1a&tecton_proto/data/transformation.proto\"\xf2\x01\n\x12GetHiveTableSchema\x12\x1a\n\x08\x64\x61tabase\x18\x01 \x01(\tR\x08\x64\x61tabase\x12\x14\n\x05table\x18\x02 \x01(\tR\x05table\x12(\n\x0ftimestampColumn\x18\x03 \x01(\tR\x0ftimestampColumn\x12(\n\x0ftimestampFormat\x18\x04 \x01(\tR\x0ftimestampFormat\x12V\n\x12rawBatchTranslator\x18\x05 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionR\x12rawBatchTranslator\"\x9e\x01\n GetBatchDataSourceFunctionSchema\x12\x42\n\x08\x66unction\x18\x01 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionR\x08\x66unction\x12\x36\n\x17supports_time_filtering\x18\x02 \x01(\x08R\x15supportsTimeFiltering\"g\n!GetStreamDataSourceFunctionSchema\x12\x42\n\x08\x66unction\x18\x01 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionR\x08\x66unction\"\xd8\x01\n\x16GetRedshiftTableSchema\x12\x1a\n\x08\x65ndpoint\x18\x01 \x01(\tR\x08\x65ndpoint\x12\x14\n\x05table\x18\x02 \x01(\tR\x05table\x12\x14\n\x05query\x18\x03 \x01(\tR\x05query\x12V\n\x12rawBatchTranslator\x18\x04 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionR\x12rawBatchTranslator\x12\x1e\n\x0btemp_s3_dir\x18\x05 \x01(\tR\ttempS3Dir\"\x95\x02\n\x12GetSnowflakeSchema\x12\x10\n\x03url\x18\x01 \x01(\tR\x03url\x12\x12\n\x04role\x18\x02 \x01(\tR\x04role\x12\x1a\n\x08\x64\x61tabase\x18\x03 \x01(\tR\x08\x64\x61tabase\x12\x16\n\x06schema\x18\x04 \x01(\tR\x06schema\x12\x1c\n\twarehouse\x18\x05 \x01(\tR\twarehouse\x12\x16\n\x05table\x18\x06 \x01(\tH\x00R\x05table\x12\x16\n\x05query\x18\x07 \x01(\tH\x00R\x05query\x12M\n\x0epost_processor\x18\x08 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionR\rpostProcessorB\x08\n\x06source\"\x8d\x03\n\x13GetFileSourceSchema\x12\x10\n\x03uri\x18\x01 \x01(\tR\x03uri\x12\x1e\n\nfileFormat\x18\x02 \x01(\tR\nfileFormat\x12\x30\n\x13\x63onvertToGlueFormat\x18\x03 \x01(\x08R\x13\x63onvertToGlueFormat\x12V\n\x12rawBatchTranslator\x18\x04 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionR\x12rawBatchTranslator\x12\x1c\n\tschemaUri\x18\x05 \x01(\tR\tschemaUri\x12(\n\x0ftimestampColumn\x18\x06 \x01(\tR\x0ftimestampColumn\x12(\n\x0ftimestampFormat\x18\x07 \x01(\tR\x0ftimestampFormat\x12H\n\x0eschemaOverride\x18\x08 \x01(\x0b\x32 .tecton_proto.common.SparkSchemaR\x0eschemaOverride\"\x92\x01\n\x16GetKinesisSourceSchema\x12\x1e\n\nstreamName\x18\x01 \x01(\tR\nstreamName\x12X\n\x13rawStreamTranslator\x18\x02 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionR\x13rawStreamTranslator\"\xea\x01\n\x14GetKafkaSourceSchema\x12X\n\x13rawStreamTranslator\x18\x01 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionR\x13rawStreamTranslator\x12\x32\n\x15ssl_keystore_location\x18\x02 \x01(\tR\x13sslKeystoreLocation\x12\x44\n\x1fssl_keystore_password_secret_id\x18\x03 \x01(\tR\x1bsslKeystorePasswordSecretId\"\xae\x03\n#GetFeatureViewMaterializationSchema\x12V\n\x14virtual_data_sources\x18\x02 \x03(\x0b\x32$.tecton_proto.data.VirtualDataSourceR\x12virtualDataSources\x12K\n\x0ftransformations\x18\x03 \x03(\x0b\x32!.tecton_proto.data.TransformationR\x0ftransformations\x12#\n\rtimestamp_key\x18\x04 \x01(\tR\x0ctimestampKey\x12\x1b\n\tjoin_keys\x18\x05 \x03(\tR\x08joinKeys\x12S\n\x12temporal_aggregate\x18\x06 \x01(\x0b\x32$.tecton_proto.data.TemporalAggregateR\x11temporalAggregate\x12\x45\n\x0c\x66\x65\x61ture_view\x18\x07 \x01(\x0b\x32\".tecton_proto.args.FeatureViewArgsR\x0b\x66\x65\x61tureViewJ\x04\x08\x01\x10\x02\"\x8e\x02\n\x14GetFeatureViewSchema\x12V\n\x14virtual_data_sources\x18\x02 \x03(\x0b\x32$.tecton_proto.data.VirtualDataSourceR\x12virtualDataSources\x12K\n\x0ftransformations\x18\x03 \x03(\x0b\x32!.tecton_proto.data.TransformationR\x0ftransformations\x12\x45\n\x0c\x66\x65\x61ture_view\x18\x05 \x01(\x0b\x32\".tecton_proto.args.FeatureViewArgsR\x0b\x66\x65\x61tureViewJ\x04\x08\x01\x10\x02J\x04\x08\x04\x10\x05\"\x9a\x02\n&GetQueryPlanInfoForFeatureViewPipeline\x12V\n\x14virtual_data_sources\x18\x02 \x03(\x0b\x32$.tecton_proto.data.VirtualDataSourceR\x12virtualDataSources\x12K\n\x0ftransformations\x18\x03 \x03(\x0b\x32!.tecton_proto.data.TransformationR\x0ftransformations\x12\x45\n\x0c\x66\x65\x61ture_view\x18\x04 \x01(\x0b\x32\".tecton_proto.args.FeatureViewArgsR\x0b\x66\x65\x61tureViewJ\x04\x08\x01\x10\x02\"\x13\n\x11ListHiveDatabases\",\n\x0eListHiveTables\x12\x1a\n\x08\x64\x61tabase\x18\x01 \x01(\tR\x08\x64\x61tabase\"H\n\x14ListHiveTableColumns\x12\x1a\n\x08\x64\x61tabase\x18\x01 \x01(\tR\x08\x64\x61tabase\x12\x14\n\x05table\x18\x02 \x01(\tR\x05table\"\x91\r\n\x0e\x45xecuteRequest\x12W\n\x12getHiveTableSchema\x18\x01 \x01(\x0b\x32%.tecton_proto.data.GetHiveTableSchemaH\x00R\x12getHiveTableSchema\x12\x63\n\x16getRedshiftTableSchema\x18\x02 \x01(\x0b\x32).tecton_proto.data.GetRedshiftTableSchemaH\x00R\x16getRedshiftTableSchema\x12Z\n\x13getFileSourceSchema\x18\x03 \x01(\x0b\x32&.tecton_proto.data.GetFileSourceSchemaH\x00R\x13getFileSourceSchema\x12\x63\n\x16getKinesisSourceSchema\x18\x04 \x01(\x0b\x32).tecton_proto.data.GetKinesisSourceSchemaH\x00R\x16getKinesisSourceSchema\x12]\n\x14getKafkaSourceSchema\x18\x05 \x01(\x0b\x32\'.tecton_proto.data.GetKafkaSourceSchemaH\x00R\x14getKafkaSourceSchema\x12]\n\x14getFeatureViewSchema\x18\x0e \x01(\x0b\x32\'.tecton_proto.data.GetFeatureViewSchemaH\x00R\x14getFeatureViewSchema\x12\x8a\x01\n#getFeatureViewMaterializationSchema\x18\x0f \x01(\x0b\x32\x36.tecton_proto.data.GetFeatureViewMaterializationSchemaH\x00R#getFeatureViewMaterializationSchema\x12W\n\x12getSnowflakeSchema\x18\x0c \x01(\x0b\x32%.tecton_proto.data.GetSnowflakeSchemaH\x00R\x12getSnowflakeSchema\x12\x81\x01\n getBatchDataSourceFunctionSchema\x18\x15 \x01(\x0b\x32\x33.tecton_proto.data.GetBatchDataSourceFunctionSchemaH\x00R getBatchDataSourceFunctionSchema\x12\x84\x01\n!getStreamDataSourceFunctionSchema\x18\x16 \x01(\x0b\x32\x34.tecton_proto.data.GetStreamDataSourceFunctionSchemaH\x00R!getStreamDataSourceFunctionSchema\x12T\n\x11listHiveDatabases\x18\x12 \x01(\x0b\x32$.tecton_proto.data.ListHiveDatabasesH\x00R\x11listHiveDatabases\x12K\n\x0elistHiveTables\x18\x13 \x01(\x0b\x32!.tecton_proto.data.ListHiveTablesH\x00R\x0elistHiveTables\x12]\n\x14listHiveTableColumns\x18\x14 \x01(\x0b\x32\'.tecton_proto.data.ListHiveTableColumnsH\x00R\x14listHiveTableColumns\x12\x93\x01\n&getQueryPlanInfoForFeatureViewPipeline\x18\r \x01(\x0b\x32\x39.tecton_proto.data.GetQueryPlanInfoForFeatureViewPipelineH\x00R&getQueryPlanInfoForFeatureViewPipeline\x12H\n\x07\x65nvVars\x18\x10 \x03(\x0b\x32..tecton_proto.data.ExecuteRequest.EnvVarsEntryR\x07\x65nvVars\x1a:\n\x0c\x45nvVarsEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01\x42\t\n\x07requestJ\x04\x08\x06\x10\x07J\x04\x08\x07\x10\x08J\x04\x08\x08\x10\tJ\x04\x08\t\x10\nJ\x04\x08\n\x10\x0bJ\x04\x08\x0b\x10\x0cJ\x04\x08\x11\x10\x12\"]\n\rQueryPlanInfo\x12\x1b\n\thas_joins\x18\x01 \x01(\x08R\x08hasJoins\x12)\n\x10has_aggregations\x18\x02 \x01(\x08R\x0fhasAggregationsJ\x04\x08\x03\x10\x04\"\x81\x03\n\rExecuteResult\x12&\n\runcaughtError\x18\x01 \x01(\tH\x00R\runcaughtError\x12*\n\x0fvalidationError\x18\x05 \x01(\tH\x00R\x0fvalidationError\x12\x44\n\x0bsparkSchema\x18\x02 \x01(\x0b\x32 .tecton_proto.common.SparkSchemaH\x00R\x0bsparkSchema\x12H\n\rqueryPlanInfo\x18\x03 \x01(\x0b\x32 .tecton_proto.data.QueryPlanInfoH\x00R\rqueryPlanInfo\x12\x35\n\x06schema\x18\x04 \x01(\x0b\x32\x1b.tecton_proto.common.SchemaH\x00R\x06schema\x12K\n\x0elistHiveResult\x18\x06 \x01(\x0b\x32!.tecton_proto.data.ListHiveResultH\x00R\x0elistHiveResultB\x08\n\x06resultB\x13\n\x0f\x63om.tecton.dataP\x01')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n$tecton_proto/data/remote_spark.proto\x12\x11tecton_proto.data\x1a*tecton_proto/args/data_source_config.proto\x1a-tecton_proto/args/user_defined_function.proto\x1a$tecton_proto/args/feature_view.proto\x1a tecton_proto/common/schema.proto\x1a&tecton_proto/common/spark_schema.proto\x1a$tecton_proto/data/feature_view.proto\x1a&tecton_proto/data/hive_metastore.proto\x1a+tecton_proto/data/virtual_data_source.proto\x1a&tecton_proto/data/transformation.proto\"\xf2\x01\n\x12GetHiveTableSchema\x12\x1a\n\x08\x64\x61tabase\x18\x01 \x01(\tR\x08\x64\x61tabase\x12\x14\n\x05table\x18\x02 \x01(\tR\x05table\x12(\n\x0ftimestampColumn\x18\x03 \x01(\tR\x0ftimestampColumn\x12(\n\x0ftimestampFormat\x18\x04 \x01(\tR\x0ftimestampFormat\x12V\n\x12rawBatchTranslator\x18\x05 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionR\x12rawBatchTranslator\"\x89\x02\n\x13GetUnityTableSchema\x12\x18\n\x07\x63\x61talog\x18\x01 \x01(\tR\x07\x63\x61talog\x12\x16\n\x06schema\x18\x02 \x01(\tR\x06schema\x12\x14\n\x05table\x18\x03 \x01(\tR\x05table\x12(\n\x0ftimestampColumn\x18\x04 \x01(\tR\x0ftimestampColumn\x12(\n\x0ftimestampFormat\x18\x05 \x01(\tR\x0ftimestampFormat\x12V\n\x12rawBatchTranslator\x18\x06 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionR\x12rawBatchTranslator\"\x9e\x01\n GetBatchDataSourceFunctionSchema\x12\x42\n\x08\x66unction\x18\x01 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionR\x08\x66unction\x12\x36\n\x17supports_time_filtering\x18\x02 \x01(\x08R\x15supportsTimeFiltering\"g\n!GetStreamDataSourceFunctionSchema\x12\x42\n\x08\x66unction\x18\x01 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionR\x08\x66unction\"\xd8\x01\n\x16GetRedshiftTableSchema\x12\x1a\n\x08\x65ndpoint\x18\x01 \x01(\tR\x08\x65ndpoint\x12\x14\n\x05table\x18\x02 \x01(\tR\x05table\x12\x14\n\x05query\x18\x03 \x01(\tR\x05query\x12V\n\x12rawBatchTranslator\x18\x04 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionR\x12rawBatchTranslator\x12\x1e\n\x0btemp_s3_dir\x18\x05 \x01(\tR\ttempS3Dir\"\x95\x02\n\x12GetSnowflakeSchema\x12\x10\n\x03url\x18\x01 \x01(\tR\x03url\x12\x12\n\x04role\x18\x02 \x01(\tR\x04role\x12\x1a\n\x08\x64\x61tabase\x18\x03 \x01(\tR\x08\x64\x61tabase\x12\x16\n\x06schema\x18\x04 \x01(\tR\x06schema\x12\x1c\n\twarehouse\x18\x05 \x01(\tR\twarehouse\x12\x16\n\x05table\x18\x06 \x01(\tH\x00R\x05table\x12\x16\n\x05query\x18\x07 \x01(\tH\x00R\x05query\x12M\n\x0epost_processor\x18\x08 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionR\rpostProcessorB\x08\n\x06source\"\x8d\x03\n\x13GetFileSourceSchema\x12\x10\n\x03uri\x18\x01 \x01(\tR\x03uri\x12\x1e\n\nfileFormat\x18\x02 \x01(\tR\nfileFormat\x12\x30\n\x13\x63onvertToGlueFormat\x18\x03 \x01(\x08R\x13\x63onvertToGlueFormat\x12V\n\x12rawBatchTranslator\x18\x04 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionR\x12rawBatchTranslator\x12\x1c\n\tschemaUri\x18\x05 \x01(\tR\tschemaUri\x12(\n\x0ftimestampColumn\x18\x06 \x01(\tR\x0ftimestampColumn\x12(\n\x0ftimestampFormat\x18\x07 \x01(\tR\x0ftimestampFormat\x12H\n\x0eschemaOverride\x18\x08 \x01(\x0b\x32 .tecton_proto.common.SparkSchemaR\x0eschemaOverride\"\x92\x01\n\x16GetKinesisSourceSchema\x12\x1e\n\nstreamName\x18\x01 \x01(\tR\nstreamName\x12X\n\x13rawStreamTranslator\x18\x02 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionR\x13rawStreamTranslator\"\xea\x01\n\x14GetKafkaSourceSchema\x12X\n\x13rawStreamTranslator\x18\x01 \x01(\x0b\x32&.tecton_proto.args.UserDefinedFunctionR\x13rawStreamTranslator\x12\x32\n\x15ssl_keystore_location\x18\x02 \x01(\tR\x13sslKeystoreLocation\x12\x44\n\x1fssl_keystore_password_secret_id\x18\x03 \x01(\tR\x1bsslKeystorePasswordSecretId\"\xae\x03\n#GetFeatureViewMaterializationSchema\x12V\n\x14virtual_data_sources\x18\x02 \x03(\x0b\x32$.tecton_proto.data.VirtualDataSourceR\x12virtualDataSources\x12K\n\x0ftransformations\x18\x03 \x03(\x0b\x32!.tecton_proto.data.TransformationR\x0ftransformations\x12#\n\rtimestamp_key\x18\x04 \x01(\tR\x0ctimestampKey\x12\x1b\n\tjoin_keys\x18\x05 \x03(\tR\x08joinKeys\x12S\n\x12temporal_aggregate\x18\x06 \x01(\x0b\x32$.tecton_proto.data.TemporalAggregateR\x11temporalAggregate\x12\x45\n\x0c\x66\x65\x61ture_view\x18\x07 \x01(\x0b\x32\".tecton_proto.args.FeatureViewArgsR\x0b\x66\x65\x61tureViewJ\x04\x08\x01\x10\x02\"\x8e\x02\n\x14GetFeatureViewSchema\x12V\n\x14virtual_data_sources\x18\x02 \x03(\x0b\x32$.tecton_proto.data.VirtualDataSourceR\x12virtualDataSources\x12K\n\x0ftransformations\x18\x03 \x03(\x0b\x32!.tecton_proto.data.TransformationR\x0ftransformations\x12\x45\n\x0c\x66\x65\x61ture_view\x18\x05 \x01(\x0b\x32\".tecton_proto.args.FeatureViewArgsR\x0b\x66\x65\x61tureViewJ\x04\x08\x01\x10\x02J\x04\x08\x04\x10\x05\"\x9a\x02\n&GetQueryPlanInfoForFeatureViewPipeline\x12V\n\x14virtual_data_sources\x18\x02 \x03(\x0b\x32$.tecton_proto.data.VirtualDataSourceR\x12virtualDataSources\x12K\n\x0ftransformations\x18\x03 \x03(\x0b\x32!.tecton_proto.data.TransformationR\x0ftransformations\x12\x45\n\x0c\x66\x65\x61ture_view\x18\x04 \x01(\x0b\x32\".tecton_proto.args.FeatureViewArgsR\x0b\x66\x65\x61tureViewJ\x04\x08\x01\x10\x02\"\x13\n\x11ListHiveDatabases\",\n\x0eListHiveTables\x12\x1a\n\x08\x64\x61tabase\x18\x01 \x01(\tR\x08\x64\x61tabase\"H\n\x14ListHiveTableColumns\x12\x1a\n\x08\x64\x61tabase\x18\x01 \x01(\tR\x08\x64\x61tabase\x12\x14\n\x05table\x18\x02 \x01(\tR\x05table\"\xed\r\n\x0e\x45xecuteRequest\x12W\n\x12getHiveTableSchema\x18\x01 \x01(\x0b\x32%.tecton_proto.data.GetHiveTableSchemaH\x00R\x12getHiveTableSchema\x12\x63\n\x16getRedshiftTableSchema\x18\x02 \x01(\x0b\x32).tecton_proto.data.GetRedshiftTableSchemaH\x00R\x16getRedshiftTableSchema\x12Z\n\x13getFileSourceSchema\x18\x03 \x01(\x0b\x32&.tecton_proto.data.GetFileSourceSchemaH\x00R\x13getFileSourceSchema\x12\x63\n\x16getKinesisSourceSchema\x18\x04 \x01(\x0b\x32).tecton_proto.data.GetKinesisSourceSchemaH\x00R\x16getKinesisSourceSchema\x12]\n\x14getKafkaSourceSchema\x18\x05 \x01(\x0b\x32\'.tecton_proto.data.GetKafkaSourceSchemaH\x00R\x14getKafkaSourceSchema\x12]\n\x14getFeatureViewSchema\x18\x0e \x01(\x0b\x32\'.tecton_proto.data.GetFeatureViewSchemaH\x00R\x14getFeatureViewSchema\x12\x8a\x01\n#getFeatureViewMaterializationSchema\x18\x0f \x01(\x0b\x32\x36.tecton_proto.data.GetFeatureViewMaterializationSchemaH\x00R#getFeatureViewMaterializationSchema\x12W\n\x12getSnowflakeSchema\x18\x0c \x01(\x0b\x32%.tecton_proto.data.GetSnowflakeSchemaH\x00R\x12getSnowflakeSchema\x12\x81\x01\n getBatchDataSourceFunctionSchema\x18\x15 \x01(\x0b\x32\x33.tecton_proto.data.GetBatchDataSourceFunctionSchemaH\x00R getBatchDataSourceFunctionSchema\x12\x84\x01\n!getStreamDataSourceFunctionSchema\x18\x16 \x01(\x0b\x32\x34.tecton_proto.data.GetStreamDataSourceFunctionSchemaH\x00R!getStreamDataSourceFunctionSchema\x12Z\n\x13getUnityTableSchema\x18\x17 \x01(\x0b\x32&.tecton_proto.data.GetUnityTableSchemaH\x00R\x13getUnityTableSchema\x12T\n\x11listHiveDatabases\x18\x12 \x01(\x0b\x32$.tecton_proto.data.ListHiveDatabasesH\x00R\x11listHiveDatabases\x12K\n\x0elistHiveTables\x18\x13 \x01(\x0b\x32!.tecton_proto.data.ListHiveTablesH\x00R\x0elistHiveTables\x12]\n\x14listHiveTableColumns\x18\x14 \x01(\x0b\x32\'.tecton_proto.data.ListHiveTableColumnsH\x00R\x14listHiveTableColumns\x12\x93\x01\n&getQueryPlanInfoForFeatureViewPipeline\x18\r \x01(\x0b\x32\x39.tecton_proto.data.GetQueryPlanInfoForFeatureViewPipelineH\x00R&getQueryPlanInfoForFeatureViewPipeline\x12H\n\x07\x65nvVars\x18\x10 \x03(\x0b\x32..tecton_proto.data.ExecuteRequest.EnvVarsEntryR\x07\x65nvVars\x1a:\n\x0c\x45nvVarsEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01\x42\t\n\x07requestJ\x04\x08\x06\x10\x07J\x04\x08\x07\x10\x08J\x04\x08\x08\x10\tJ\x04\x08\t\x10\nJ\x04\x08\n\x10\x0bJ\x04\x08\x0b\x10\x0cJ\x04\x08\x11\x10\x12\"]\n\rQueryPlanInfo\x12\x1b\n\thas_joins\x18\x01 \x01(\x08R\x08hasJoins\x12)\n\x10has_aggregations\x18\x02 \x01(\x08R\x0fhasAggregationsJ\x04\x08\x03\x10\x04\"\x81\x03\n\rExecuteResult\x12&\n\runcaughtError\x18\x01 \x01(\tH\x00R\runcaughtError\x12*\n\x0fvalidationError\x18\x05 \x01(\tH\x00R\x0fvalidationError\x12\x44\n\x0bsparkSchema\x18\x02 \x01(\x0b\x32 .tecton_proto.common.SparkSchemaH\x00R\x0bsparkSchema\x12H\n\rqueryPlanInfo\x18\x03 \x01(\x0b\x32 .tecton_proto.data.QueryPlanInfoH\x00R\rqueryPlanInfo\x12\x35\n\x06schema\x18\x04 \x01(\x0b\x32\x1b.tecton_proto.common.SchemaH\x00R\x06schema\x12K\n\x0elistHiveResult\x18\x06 \x01(\x0b\x32!.tecton_proto.data.ListHiveResultH\x00R\x0elistHiveResultB\x08\n\x06resultB\x13\n\x0f\x63om.tecton.dataP\x01')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.data.remote_spark_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\017com.tecton.dataP\001'
   _EXECUTEREQUEST_ENVVARSENTRY._options = None
   _EXECUTEREQUEST_ENVVARSENTRY._serialized_options = b'8\001'
   _GETHIVETABLESCHEMA._serialized_start=426
   _GETHIVETABLESCHEMA._serialized_end=668
-  _GETBATCHDATASOURCEFUNCTIONSCHEMA._serialized_start=671
-  _GETBATCHDATASOURCEFUNCTIONSCHEMA._serialized_end=829
-  _GETSTREAMDATASOURCEFUNCTIONSCHEMA._serialized_start=831
-  _GETSTREAMDATASOURCEFUNCTIONSCHEMA._serialized_end=934
-  _GETREDSHIFTTABLESCHEMA._serialized_start=937
-  _GETREDSHIFTTABLESCHEMA._serialized_end=1153
-  _GETSNOWFLAKESCHEMA._serialized_start=1156
-  _GETSNOWFLAKESCHEMA._serialized_end=1433
-  _GETFILESOURCESCHEMA._serialized_start=1436
-  _GETFILESOURCESCHEMA._serialized_end=1833
-  _GETKINESISSOURCESCHEMA._serialized_start=1836
-  _GETKINESISSOURCESCHEMA._serialized_end=1982
-  _GETKAFKASOURCESCHEMA._serialized_start=1985
-  _GETKAFKASOURCESCHEMA._serialized_end=2219
-  _GETFEATUREVIEWMATERIALIZATIONSCHEMA._serialized_start=2222
-  _GETFEATUREVIEWMATERIALIZATIONSCHEMA._serialized_end=2652
-  _GETFEATUREVIEWSCHEMA._serialized_start=2655
-  _GETFEATUREVIEWSCHEMA._serialized_end=2925
-  _GETQUERYPLANINFOFORFEATUREVIEWPIPELINE._serialized_start=2928
-  _GETQUERYPLANINFOFORFEATUREVIEWPIPELINE._serialized_end=3210
-  _LISTHIVEDATABASES._serialized_start=3212
-  _LISTHIVEDATABASES._serialized_end=3231
-  _LISTHIVETABLES._serialized_start=3233
-  _LISTHIVETABLES._serialized_end=3277
-  _LISTHIVETABLECOLUMNS._serialized_start=3279
-  _LISTHIVETABLECOLUMNS._serialized_end=3351
-  _EXECUTEREQUEST._serialized_start=3354
-  _EXECUTEREQUEST._serialized_end=5035
-  _EXECUTEREQUEST_ENVVARSENTRY._serialized_start=4924
-  _EXECUTEREQUEST_ENVVARSENTRY._serialized_end=4982
-  _QUERYPLANINFO._serialized_start=5037
-  _QUERYPLANINFO._serialized_end=5130
-  _EXECUTERESULT._serialized_start=5133
-  _EXECUTERESULT._serialized_end=5518
+  _GETUNITYTABLESCHEMA._serialized_start=671
+  _GETUNITYTABLESCHEMA._serialized_end=936
+  _GETBATCHDATASOURCEFUNCTIONSCHEMA._serialized_start=939
+  _GETBATCHDATASOURCEFUNCTIONSCHEMA._serialized_end=1097
+  _GETSTREAMDATASOURCEFUNCTIONSCHEMA._serialized_start=1099
+  _GETSTREAMDATASOURCEFUNCTIONSCHEMA._serialized_end=1202
+  _GETREDSHIFTTABLESCHEMA._serialized_start=1205
+  _GETREDSHIFTTABLESCHEMA._serialized_end=1421
+  _GETSNOWFLAKESCHEMA._serialized_start=1424
+  _GETSNOWFLAKESCHEMA._serialized_end=1701
+  _GETFILESOURCESCHEMA._serialized_start=1704
+  _GETFILESOURCESCHEMA._serialized_end=2101
+  _GETKINESISSOURCESCHEMA._serialized_start=2104
+  _GETKINESISSOURCESCHEMA._serialized_end=2250
+  _GETKAFKASOURCESCHEMA._serialized_start=2253
+  _GETKAFKASOURCESCHEMA._serialized_end=2487
+  _GETFEATUREVIEWMATERIALIZATIONSCHEMA._serialized_start=2490
+  _GETFEATUREVIEWMATERIALIZATIONSCHEMA._serialized_end=2920
+  _GETFEATUREVIEWSCHEMA._serialized_start=2923
+  _GETFEATUREVIEWSCHEMA._serialized_end=3193
+  _GETQUERYPLANINFOFORFEATUREVIEWPIPELINE._serialized_start=3196
+  _GETQUERYPLANINFOFORFEATUREVIEWPIPELINE._serialized_end=3478
+  _LISTHIVEDATABASES._serialized_start=3480
+  _LISTHIVEDATABASES._serialized_end=3499
+  _LISTHIVETABLES._serialized_start=3501
+  _LISTHIVETABLES._serialized_end=3545
+  _LISTHIVETABLECOLUMNS._serialized_start=3547
+  _LISTHIVETABLECOLUMNS._serialized_end=3619
+  _EXECUTEREQUEST._serialized_start=3622
+  _EXECUTEREQUEST._serialized_end=5395
+  _EXECUTEREQUEST_ENVVARSENTRY._serialized_start=5284
+  _EXECUTEREQUEST_ENVVARSENTRY._serialized_end=5342
+  _QUERYPLANINFO._serialized_start=5397
+  _QUERYPLANINFO._serialized_end=5490
+  _EXECUTERESULT._serialized_start=5493
+  _EXECUTERESULT._serialized_end=5878
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/data/saved_feature_data_frame_pb2.py` & `tecton-0.7.0rc0/tecton_proto/data/saved_feature_data_frame_pb2.py`

 * *Files 6% similar despite different names*

```diff
@@ -12,22 +12,22 @@
 
 
 from google.protobuf import timestamp_pb2 as google_dot_protobuf_dot_timestamp__pb2
 from tecton_proto.common import id_pb2 as tecton__proto_dot_common_dot_id__pb2
 from tecton_proto.common import spark_schema_pb2 as tecton__proto_dot_common_dot_spark__schema__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n0tecton_proto/data/saved_feature_data_frame.proto\x12\x11tecton_proto.data\x1a\x1fgoogle/protobuf/timestamp.proto\x1a\x1ctecton_proto/common/id.proto\x1a&tecton_proto/common/spark_schema.proto\"\xd1\x01\n\x19SavedFeatureDataFrameInfo\x12\x12\n\x04name\x18\x01 \x01(\tR\x04name\x12\x39\n\ncreated_at\x18\x02 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\tcreatedAt\x12\x1f\n\x0bis_archived\x18\x03 \x01(\x08R\nisArchived\x12\x1c\n\tworkspace\x18\x04 \x01(\tR\tworkspace\x12&\n\x0fis_data_deleted\x18\x05 \x01(\x08R\risDataDeleted\"\x9f\x06\n\x15SavedFeatureDataFrame\x12T\n\x1asaved_feature_dataframe_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x17savedFeatureDataframeId\x12@\n\x04info\x18\x02 \x01(\x0b\x32,.tecton_proto.data.SavedFeatureDataFrameInfoR\x04info\x12-\n\x12\x64\x61taframe_location\x18\x03 \x01(\tR\x11\x64\x61taframeLocation\x12G\n\x12\x66\x65\x61ture_package_id\x18\x04 \x01(\x0b\x32\x17.tecton_proto.common.IdH\x00R\x10\x66\x65\x61turePackageId\x12G\n\x12\x66\x65\x61ture_service_id\x18\x05 \x01(\x0b\x32\x17.tecton_proto.common.IdH\x00R\x10\x66\x65\x61tureServiceId\x12\x32\n\x14\x66\x65\x61ture_package_name\x18\x06 \x01(\tH\x01R\x12\x66\x65\x61turePackageName\x12\x32\n\x14\x66\x65\x61ture_service_name\x18\x07 \x01(\tH\x01R\x12\x66\x65\x61tureServiceName\x12>\n\x1cstate_update_entry_commit_id\x18\x08 \x01(\tR\x18stateUpdateEntryCommitId\x12\x31\n\x15join_key_column_names\x18\t \x03(\tR\x12joinKeyColumnNames\x12\x32\n\x15timestamp_column_name\x18\n \x01(\tR\x13timestampColumnName\x12\x38\n\x06schema\x18\x0b \x01(\x0b\x32 .tecton_proto.common.SparkSchemaR\x06schema\x12@\n\x04type\x18\x0c \x01(\x0e\x32,.tecton_proto.data.SavedFeatureDataFrameTypeR\x04typeB\x0f\n\rsource_fco_idB\x11\n\x0fsource_fco_name*?\n\x19SavedFeatureDataFrameType\x12\x0b\n\x07NOT_SET\x10\x00\x12\t\n\x05SAVED\x10\x01\x12\n\n\x06LOGGED\x10\x02\x42\x13\n\x0f\x63om.tecton.dataP\x01')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n0tecton_proto/data/saved_feature_data_frame.proto\x12\x11tecton_proto.data\x1a\x1fgoogle/protobuf/timestamp.proto\x1a\x1ctecton_proto/common/id.proto\x1a&tecton_proto/common/spark_schema.proto\"\x95\x02\n\x19SavedFeatureDataFrameInfo\x12\x12\n\x04name\x18\x01 \x01(\tR\x04name\x12\x39\n\ncreated_at\x18\x02 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\tcreatedAt\x12\x1f\n\x0bis_archived\x18\x03 \x01(\x08R\nisArchived\x12\x1c\n\tworkspace\x18\x04 \x01(\tR\tworkspace\x12&\n\x0fis_data_deleted\x18\x05 \x01(\x08R\risDataDeleted\x12\x42\n\x0f\x64\x61ta_deleted_at\x18\x06 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\rdataDeletedAt\"\x9f\x06\n\x15SavedFeatureDataFrame\x12T\n\x1asaved_feature_dataframe_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x17savedFeatureDataframeId\x12@\n\x04info\x18\x02 \x01(\x0b\x32,.tecton_proto.data.SavedFeatureDataFrameInfoR\x04info\x12-\n\x12\x64\x61taframe_location\x18\x03 \x01(\tR\x11\x64\x61taframeLocation\x12G\n\x12\x66\x65\x61ture_package_id\x18\x04 \x01(\x0b\x32\x17.tecton_proto.common.IdH\x00R\x10\x66\x65\x61turePackageId\x12G\n\x12\x66\x65\x61ture_service_id\x18\x05 \x01(\x0b\x32\x17.tecton_proto.common.IdH\x00R\x10\x66\x65\x61tureServiceId\x12\x32\n\x14\x66\x65\x61ture_package_name\x18\x06 \x01(\tH\x01R\x12\x66\x65\x61turePackageName\x12\x32\n\x14\x66\x65\x61ture_service_name\x18\x07 \x01(\tH\x01R\x12\x66\x65\x61tureServiceName\x12>\n\x1cstate_update_entry_commit_id\x18\x08 \x01(\tR\x18stateUpdateEntryCommitId\x12\x31\n\x15join_key_column_names\x18\t \x03(\tR\x12joinKeyColumnNames\x12\x32\n\x15timestamp_column_name\x18\n \x01(\tR\x13timestampColumnName\x12\x38\n\x06schema\x18\x0b \x01(\x0b\x32 .tecton_proto.common.SparkSchemaR\x06schema\x12@\n\x04type\x18\x0c \x01(\x0e\x32,.tecton_proto.data.SavedFeatureDataFrameTypeR\x04typeB\x0f\n\rsource_fco_idB\x11\n\x0fsource_fco_name*?\n\x19SavedFeatureDataFrameType\x12\x0b\n\x07NOT_SET\x10\x00\x12\t\n\x05SAVED\x10\x01\x12\n\n\x06LOGGED\x10\x02\x42\x13\n\x0f\x63om.tecton.dataP\x01')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.data.saved_feature_data_frame_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\017com.tecton.dataP\001'
-  _SAVEDFEATUREDATAFRAMETYPE._serialized_start=1188
-  _SAVEDFEATUREDATAFRAMETYPE._serialized_end=1251
+  _SAVEDFEATUREDATAFRAMETYPE._serialized_start=1256
+  _SAVEDFEATUREDATAFRAMETYPE._serialized_end=1319
   _SAVEDFEATUREDATAFRAMEINFO._serialized_start=175
-  _SAVEDFEATUREDATAFRAMEINFO._serialized_end=384
-  _SAVEDFEATUREDATAFRAME._serialized_start=387
-  _SAVEDFEATUREDATAFRAME._serialized_end=1186
+  _SAVEDFEATUREDATAFRAMEINFO._serialized_end=452
+  _SAVEDFEATUREDATAFRAME._serialized_start=455
+  _SAVEDFEATUREDATAFRAME._serialized_end=1254
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/data/serving_status_pb2.py` & `tecton-0.7.0rc0/tecton_proto/data/serving_status_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/data/state_update_pb2.py` & `tecton-0.7.0rc0/tecton_proto/data/state_update_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/data/stream_data_source_pb2.py` & `tecton-0.7.0rc0/tecton_proto/data/stream_data_source_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/data/summary_pb2.py` & `tecton-0.7.0rc0/tecton_proto/data/summary_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/data/tecton_api_key_pb2.py` & `tecton-0.7.0rc0/tecton_proto/materialization/spark_cluster_pb2.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,28 +1,33 @@
 # -*- coding: utf-8 -*-
 # Generated by the protocol buffer compiler.  DO NOT EDIT!
-# source: tecton_proto/data/tecton_api_key.proto
+# source: tecton_proto/materialization/spark_cluster.proto
 """Generated protocol buffer code."""
 from google.protobuf.internal import builder as _builder
 from google.protobuf import descriptor as _descriptor
 from google.protobuf import descriptor_pool as _descriptor_pool
 from google.protobuf import symbol_database as _symbol_database
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
-from tecton_proto.auth import principal_pb2 as tecton__proto_dot_auth_dot_principal__pb2
-from tecton_proto.common import id_pb2 as tecton__proto_dot_common_dot_id__pb2
+from tecton_proto.spark_api import jobs_pb2 as tecton__proto_dot_spark__api_dot_jobs__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n&tecton_proto/data/tecton_api_key.proto\x12\x11tecton_proto.data\x1a!tecton_proto/auth/principal.proto\x1a\x1ctecton_proto/common/id.proto\"\xe0\x02\n\x0cTectonApiKey\x12\'\n\x02id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x02id\x12\x1d\n\nhashed_key\x18\x02 \x01(\tR\thashedKey\x12 \n\x0b\x64\x65scription\x18\x03 \x01(\tR\x0b\x64\x65scription\x12\x1a\n\x08\x61rchived\x18\x04 \x01(\x08R\x08\x61rchived\x12\x1d\n\ncreated_by\x18\x05 \x01(\tR\tcreatedBy\x12!\n\x0cobscured_key\x18\x06 \x01(\tR\x0bobscuredKey\x12\x19\n\x08is_admin\x18\x07 \x01(\x08R\x07isAdmin\x12\x12\n\x04name\x18\x08 \x01(\tR\x04name\x12!\n\tis_active\x18\t \x01(\x08:\x04trueR\x08isActive\x12\x36\n\x07\x63reator\x18\n \x01(\x0b\x32\x1c.tecton_proto.auth.PrincipalR\x07\x63reatorB\x13\n\x0f\x63om.tecton.dataP\x01')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n0tecton_proto/materialization/spark_cluster.proto\x12\x1ctecton_proto.materialization\x1a!tecton_proto/spark_api/jobs.proto\"\xad\x02\n\x17SparkClusterEnvironment\x12I\n!spark_cluster_environment_version\x18\x01 \x01(\x05R\x1esparkClusterEnvironmentVersion\x12\x65\n\x15job_request_templates\x18\n \x01(\x0b\x32\x31.tecton_proto.materialization.JobRequestTemplatesR\x13jobRequestTemplates\x12T\n\'merged_user_deployment_settings_version\x18\x0c \x01(\x05R#mergedUserDeploymentSettingsVersionJ\x04\x08\x02\x10\nJ\x04\x08\x0b\x10\x0c\"\xc1\x01\n\x13JobRequestTemplates\x12X\n\x13\x64\x61tabricks_template\x18\x02 \x01(\x0b\x32\'.tecton_proto.spark_api.StartJobRequestR\x12\x64\x61tabricksTemplate\x12J\n\x0c\x65mr_template\x18\x03 \x01(\x0b\x32\'.tecton_proto.spark_api.StartJobRequestR\x0b\x65mrTemplateJ\x04\x08\x01\x10\x02*b\n\x08TaskType\x12\x0b\n\x07UNKNOWN\x10\x00\x12\t\n\x05\x42\x41TCH\x10\x01\x12\r\n\tSTREAMING\x10\x02\x12\n\n\x06INGEST\x10\x03\x12\x0c\n\x08\x44\x45LETION\x10\x04\x12\x15\n\x11\x44\x45LTA_MAINTENANCE\x10\x05*\x80\x01\n\x14\x45xecutionEnvironment\x12\x13\n\x0f\x45NV_UNSPECIFIED\x10\x00\x12\x1b\n\x17\x45NV_DATABRICKS_NOTEBOOK\x10\x01\x12\x0b\n\x07\x45NV_EMR\x10\x02\x12\x11\n\rENV_SNOWFLAKE\x10\x03\x12\x10\n\x0c\x45NV_DATAPROC\x10\x05\"\x04\x08\x04\x10\x04\x42\x1e\n\x1a\x63om.tecton.materializationP\x01')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
-_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.data.tecton_api_key_pb2', globals())
+_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.materialization.spark_cluster_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
-  DESCRIPTOR._serialized_options = b'\n\017com.tecton.dataP\001'
-  _TECTONAPIKEY._serialized_start=127
-  _TECTONAPIKEY._serialized_end=479
+  DESCRIPTOR._serialized_options = b'\n\032com.tecton.materializationP\001'
+  _TASKTYPE._serialized_start=617
+  _TASKTYPE._serialized_end=715
+  _EXECUTIONENVIRONMENT._serialized_start=718
+  _EXECUTIONENVIRONMENT._serialized_end=846
+  _SPARKCLUSTERENVIRONMENT._serialized_start=118
+  _SPARKCLUSTERENVIRONMENT._serialized_end=419
+  _JOBREQUESTTEMPLATES._serialized_start=422
+  _JOBREQUESTTEMPLATES._serialized_end=615
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/data/transformation_pb2.py` & `tecton-0.7.0rc0/tecton_proto/data/transformation_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/data/user_deployment_settings_pb2.py` & `tecton-0.7.0rc0/tecton_proto/data/user_deployment_settings_pb2.py`

 * *Files 2% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 
 _sym_db = _symbol_database.Default()
 
 
 from tecton_proto.common import secret_pb2 as tecton__proto_dot_common_dot_secret__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n0tecton_proto/data/user_deployment_settings.proto\x12\x11tecton_proto.data\x1a tecton_proto/common/secret.proto\"\xfa\x02\n\x16UserDeploymentSettings\x12G\n user_deployment_settings_version\x18\x01 \x01(\x05R\x1duserDeploymentSettingsVersion\x12R\n\x11\x64\x61tabricks_config\x18\x02 \x01(\x0b\x32#.tecton_proto.data.DatabricksConfigH\x00R\x10\x64\x61tabricksConfig\x12T\n\x13user_spark_settings\x18\x03 \x01(\x0b\x32$.tecton_proto.data.UserSparkSettingsR\x11userSparkSettings\x12O\n\x0ftenant_settings\x18\x05 \x01(\x0b\x32&.tecton_proto.data.TenantSettingsProtoR\x0etenantSettingsB\x16\n\x14\x64\x61ta_platform_configJ\x04\x08\x04\x10\x05\"\xdf\x01\n\x10\x44\x61tabricksConfig\x12#\n\rworkspace_url\x18\x01 \x01(\tR\x0cworkspaceUrl\x12\x38\n\tapi_token\x18\x02 \x01(\x0b\x32\x1b.tecton_proto.common.SecretR\x08\x61piToken\x12\x1b\n\tuser_name\x18\x03 \x01(\tR\x08userName\x12*\n\x11user_display_name\x18\x04 \x01(\tR\x0fuserDisplayName\x12#\n\rspark_version\x18\x05 \x01(\tR\x0csparkVersion\"\xd7\x01\n\x11UserSparkSettings\x12\x30\n\x14instance_profile_arn\x18\x01 \x01(\tR\x12instanceProfileArn\x12R\n\nspark_conf\x18\x02 \x03(\x0b\x32\x33.tecton_proto.data.UserSparkSettings.SparkConfEntryR\tsparkConf\x1a<\n\x0eSparkConfEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01\"\x94\x06\n\x13TenantSettingsProto\x12M\n\x14\x63hronosphere_api_key\x18\x01 \x01(\x0b\x32\x1b.tecton_proto.common.SecretR\x12\x63hronosphereApiKey\x12I\n!chronosphere_restrict_label_value\x18\x04 \x01(\tR\x1e\x63hronosphereRestrictLabelValue\x12M\n pseudonymize_amplitude_user_name\x18\x02 \x01(\x08:\x04trueR\x1dpseudonymizeAmplitudeUserName\x12T\n\'enable_user_editing_deployment_settings\x18\x03 \x01(\x08R#enableUserEditingDeploymentSettings\x12-\n\x13okta_admin_group_id\x18\x05 \x01(\tR\x10oktaAdminGroupId\x12+\n\x12okta_user_group_id\x18\x06 \x01(\tR\x0foktaUserGroupId\x12\x39\n\x19\x62\x61se_metadata_service_url\x18\x07 \x01(\tR\x16\x62\x61seMetadataServiceUrl\x12\x37\n\x18\x62\x61se_feature_service_url\x18\x08 \x01(\tR\x15\x62\x61seFeatureServiceUrl\x12:\n\x19spicedb_organization_name\x18\t \x01(\tR\x17spicedbOrganizationName\x12=\n\x1b\x63ustomer_facing_tenant_name\x18\n \x01(\tR\x18\x63ustomerFacingTenantName\x12\x41\n\x0c\x61ws_settings\x18\x0b \x01(\x0b\x32\x1e.tecton_proto.data.AwsSettingsR\x0b\x61wsSettings\x12\x30\n\x14internal_tenant_name\x18\x0c \x01(\tR\x12internalTenantName\"\xa8\x05\n\x0b\x41wsSettings\x12>\n\x0b\x64ynamo_role\x18\x06 \x01(\x0b\x32\x1d.tecton_proto.data.AwsIamRoleR\ndynamoRole\x12_\n\x11\x64ynamo_extra_tags\x18\x08 \x03(\x0b\x32\x33.tecton_proto.data.AwsSettings.DynamoExtraTagsEntryR\x0f\x64ynamoExtraTags\x12\x62\n\x12\x63ompute_extra_tags\x18\t \x03(\x0b\x32\x34.tecton_proto.data.AwsSettings.ComputeExtraTagsEntryR\x10\x63omputeExtraTags\x12\x41\n\x0c\x65mr_settings\x18\x0c \x01(\x0b\x32\x1e.tecton_proto.data.EmrSettingsR\x0b\x65mrSettings\x12Q\n\x12\x64ynamo_table_names\x18\r \x01(\x0b\x32#.tecton_proto.data.DynamoTableNamesR\x10\x64ynamoTableNames\x12]\n\x16object_store_locations\x18\x0e \x01(\x0b\x32\'.tecton_proto.data.ObjectStoreLocationsR\x14objectStoreLocations\x1a\x42\n\x14\x44ynamoExtraTagsEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01\x1a\x43\n\x15\x43omputeExtraTagsEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01J\x04\x08\x01\x10\x06J\x04\x08\x07\x10\x08J\x04\x08\n\x10\x0bJ\x04\x08\x0b\x10\x0c\"\x94\x01\n\nAwsIamRole\x12\x19\n\x08role_arn\x18\x01 \x01(\tR\x07roleArn\x12J\n\x11intermediate_role\x18\x02 \x01(\x0b\x32\x1d.tecton_proto.data.AwsIamRoleR\x10intermediateRole\x12\x1f\n\x0b\x65xternal_id\x18\x03 \x01(\tR\nexternalId\"S\n\nS3Location\x12\x12\n\x04path\x18\x01 \x01(\tR\x04path\x12\x31\n\x04role\x18\x02 \x01(\x0b\x32\x1d.tecton_proto.data.AwsIamRoleR\x04role\"\"\n\x0c\x44\x42\x46SLocation\x12\x12\n\x04path\x18\x01 \x01(\tR\x04path\"!\n\x0bGCSLocation\x12\x12\n\x04path\x18\x01 \x01(\tR\x04path\"\xf2\x01\n\x13ObjectStoreLocation\x12@\n\x0bs3_location\x18\x01 \x01(\x0b\x32\x1d.tecton_proto.data.S3LocationH\x00R\ns3Location\x12\x46\n\rdbfs_location\x18\x02 \x01(\x0b\x32\x1f.tecton_proto.data.DBFSLocationH\x00R\x0c\x64\x62\x66sLocation\x12\x43\n\x0cgcs_location\x18\x03 \x01(\x0b\x32\x1e.tecton_proto.data.GCSLocationH\x00R\x0bgcsLocationB\x0c\n\nstore_type\"V\n\x0b\x45mrSettings\x12G\n\x10\x65mr_control_role\x18\x01 \x01(\x0b\x32\x1d.tecton_proto.data.AwsIamRoleR\x0e\x65mrControlRole\"\xa8\x03\n\x10\x44ynamoTableNames\x12*\n\x11\x64\x61ta_table_prefix\x18\x01 \x01(\tR\x0f\x64\x61taTablePrefix\x12*\n\x11status_table_name\x18\x02 \x01(\tR\x0fstatusTableName\x12\x42\n\x1ejob_idempotence_key_table_name\x18\x03 \x01(\tR\x1ajobIdempotenceKeyTableName\x12*\n\x11\x63\x61nary_table_name\x18\x04 \x01(\tR\x0f\x63\x61naryTableName\x12/\n\x14\x64\x65lta_log_table_name\x18\x05 \x01(\tR\x11\x64\x65ltaLogTableName\x12.\n\x13metric_table_prefix\x18\x06 \x01(\tR\x11metricTablePrefix\x12\x34\n\x17\x64\x65lta_log_table_name_v2\x18\x07 \x01(\tR\x13\x64\x65ltaLogTableNameV2\x12\x35\n\x17job_metadata_table_name\x18\x08 \x01(\tR\x14jobMetadataTableName\"\xe0\x07\n\x14ObjectStoreLocations\x12P\n\x0fmaterialization\x18\x01 \x01(\x0b\x32&.tecton_proto.data.ObjectStoreLocationR\x0fmaterialization\x12Y\n\x14streaming_checkpoint\x18\x02 \x01(\x0b\x32&.tecton_proto.data.ObjectStoreLocationR\x13streamingCheckpoint\x12h\n\x1c\x66\x65\x61ture_server_configuration\x18\x03 \x01(\x0b\x32&.tecton_proto.data.ObjectStoreLocationR\x1a\x66\x65\x61tureServerConfiguration\x12I\n\x0c\x66\x65\x61ture_repo\x18\x04 \x01(\x0b\x32&.tecton_proto.data.ObjectStoreLocationR\x0b\x66\x65\x61tureRepo\x12G\n\x0b\x65mr_scripts\x18\x05 \x01(\x0b\x32&.tecton_proto.data.ObjectStoreLocationR\nemrScripts\x12]\n\x16materialization_params\x18\x06 \x01(\x0b\x32&.tecton_proto.data.ObjectStoreLocationR\x15materializationParams\x12S\n\x11intermediate_data\x18\x07 \x01(\x0b\x32&.tecton_proto.data.ObjectStoreLocationR\x10intermediateData\x12\\\n\x16\x66\x65\x61ture_server_logging\x18\x08 \x01(\x0b\x32&.tecton_proto.data.ObjectStoreLocationR\x14\x66\x65\x61tureServerLogging\x12\\\n\x16kafka_credentials_base\x18\t \x01(\x0b\x32&.tecton_proto.data.ObjectStoreLocationR\x14kafkaCredentialsBase\x12\\\n\x16push_api_configuration\x18\n \x01(\x0b\x32&.tecton_proto.data.ObjectStoreLocationR\x14pushApiConfiguration\x12O\n\x0f\x64\x61ta_validation\x18\x0b \x01(\x0b\x32&.tecton_proto.data.ObjectStoreLocationR\x0e\x64\x61taValidationB\x13\n\x0f\x63om.tecton.dataP\x01')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n0tecton_proto/data/user_deployment_settings.proto\x12\x11tecton_proto.data\x1a tecton_proto/common/secret.proto\"\xfa\x02\n\x16UserDeploymentSettings\x12G\n user_deployment_settings_version\x18\x01 \x01(\x05R\x1duserDeploymentSettingsVersion\x12R\n\x11\x64\x61tabricks_config\x18\x02 \x01(\x0b\x32#.tecton_proto.data.DatabricksConfigH\x00R\x10\x64\x61tabricksConfig\x12T\n\x13user_spark_settings\x18\x03 \x01(\x0b\x32$.tecton_proto.data.UserSparkSettingsR\x11userSparkSettings\x12O\n\x0ftenant_settings\x18\x05 \x01(\x0b\x32&.tecton_proto.data.TenantSettingsProtoR\x0etenantSettingsB\x16\n\x14\x64\x61ta_platform_configJ\x04\x08\x04\x10\x05\"\xdf\x01\n\x10\x44\x61tabricksConfig\x12#\n\rworkspace_url\x18\x01 \x01(\tR\x0cworkspaceUrl\x12\x38\n\tapi_token\x18\x02 \x01(\x0b\x32\x1b.tecton_proto.common.SecretR\x08\x61piToken\x12\x1b\n\tuser_name\x18\x03 \x01(\tR\x08userName\x12*\n\x11user_display_name\x18\x04 \x01(\tR\x0fuserDisplayName\x12#\n\rspark_version\x18\x05 \x01(\tR\x0csparkVersion\"\xd7\x01\n\x11UserSparkSettings\x12\x30\n\x14instance_profile_arn\x18\x01 \x01(\tR\x12instanceProfileArn\x12R\n\nspark_conf\x18\x02 \x03(\x0b\x32\x33.tecton_proto.data.UserSparkSettings.SparkConfEntryR\tsparkConf\x1a<\n\x0eSparkConfEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01\"\x94\x06\n\x13TenantSettingsProto\x12M\n\x14\x63hronosphere_api_key\x18\x01 \x01(\x0b\x32\x1b.tecton_proto.common.SecretR\x12\x63hronosphereApiKey\x12I\n!chronosphere_restrict_label_value\x18\x04 \x01(\tR\x1e\x63hronosphereRestrictLabelValue\x12M\n pseudonymize_amplitude_user_name\x18\x02 \x01(\x08:\x04trueR\x1dpseudonymizeAmplitudeUserName\x12T\n\'enable_user_editing_deployment_settings\x18\x03 \x01(\x08R#enableUserEditingDeploymentSettings\x12-\n\x13okta_admin_group_id\x18\x05 \x01(\tR\x10oktaAdminGroupId\x12+\n\x12okta_user_group_id\x18\x06 \x01(\tR\x0foktaUserGroupId\x12\x39\n\x19\x62\x61se_metadata_service_url\x18\x07 \x01(\tR\x16\x62\x61seMetadataServiceUrl\x12\x37\n\x18\x62\x61se_feature_service_url\x18\x08 \x01(\tR\x15\x62\x61seFeatureServiceUrl\x12:\n\x19spicedb_organization_name\x18\t \x01(\tR\x17spicedbOrganizationName\x12=\n\x1b\x63ustomer_facing_tenant_name\x18\n \x01(\tR\x18\x63ustomerFacingTenantName\x12\x41\n\x0c\x61ws_settings\x18\x0b \x01(\x0b\x32\x1e.tecton_proto.data.AwsSettingsR\x0b\x61wsSettings\x12\x30\n\x14internal_tenant_name\x18\x0c \x01(\tR\x12internalTenantName\"\xa8\x05\n\x0b\x41wsSettings\x12>\n\x0b\x64ynamo_role\x18\x06 \x01(\x0b\x32\x1d.tecton_proto.data.AwsIamRoleR\ndynamoRole\x12_\n\x11\x64ynamo_extra_tags\x18\x08 \x03(\x0b\x32\x33.tecton_proto.data.AwsSettings.DynamoExtraTagsEntryR\x0f\x64ynamoExtraTags\x12\x62\n\x12\x63ompute_extra_tags\x18\t \x03(\x0b\x32\x34.tecton_proto.data.AwsSettings.ComputeExtraTagsEntryR\x10\x63omputeExtraTags\x12\x41\n\x0c\x65mr_settings\x18\x0c \x01(\x0b\x32\x1e.tecton_proto.data.EmrSettingsR\x0b\x65mrSettings\x12Q\n\x12\x64ynamo_table_names\x18\r \x01(\x0b\x32#.tecton_proto.data.DynamoTableNamesR\x10\x64ynamoTableNames\x12]\n\x16object_store_locations\x18\x0e \x01(\x0b\x32\'.tecton_proto.data.ObjectStoreLocationsR\x14objectStoreLocations\x1a\x42\n\x14\x44ynamoExtraTagsEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01\x1a\x43\n\x15\x43omputeExtraTagsEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01J\x04\x08\x01\x10\x06J\x04\x08\x07\x10\x08J\x04\x08\n\x10\x0bJ\x04\x08\x0b\x10\x0c\"\x94\x01\n\nAwsIamRole\x12\x19\n\x08role_arn\x18\x01 \x01(\tR\x07roleArn\x12J\n\x11intermediate_role\x18\x02 \x01(\x0b\x32\x1d.tecton_proto.data.AwsIamRoleR\x10intermediateRole\x12\x1f\n\x0b\x65xternal_id\x18\x03 \x01(\tR\nexternalId\"S\n\nS3Location\x12\x12\n\x04path\x18\x01 \x01(\tR\x04path\x12\x31\n\x04role\x18\x02 \x01(\x0b\x32\x1d.tecton_proto.data.AwsIamRoleR\x04role\"\"\n\x0c\x44\x42\x46SLocation\x12\x12\n\x04path\x18\x01 \x01(\tR\x04path\"!\n\x0bGCSLocation\x12\x12\n\x04path\x18\x01 \x01(\tR\x04path\"\xf2\x01\n\x13ObjectStoreLocation\x12@\n\x0bs3_location\x18\x01 \x01(\x0b\x32\x1d.tecton_proto.data.S3LocationH\x00R\ns3Location\x12\x46\n\rdbfs_location\x18\x02 \x01(\x0b\x32\x1f.tecton_proto.data.DBFSLocationH\x00R\x0c\x64\x62\x66sLocation\x12\x43\n\x0cgcs_location\x18\x03 \x01(\x0b\x32\x1e.tecton_proto.data.GCSLocationH\x00R\x0bgcsLocationB\x0c\n\nstore_type\"V\n\x0b\x45mrSettings\x12G\n\x10\x65mr_control_role\x18\x01 \x01(\x0b\x32\x1d.tecton_proto.data.AwsIamRoleR\x0e\x65mrControlRole\"\xa8\x03\n\x10\x44ynamoTableNames\x12*\n\x11\x64\x61ta_table_prefix\x18\x01 \x01(\tR\x0f\x64\x61taTablePrefix\x12*\n\x11status_table_name\x18\x02 \x01(\tR\x0fstatusTableName\x12\x42\n\x1ejob_idempotence_key_table_name\x18\x03 \x01(\tR\x1ajobIdempotenceKeyTableName\x12*\n\x11\x63\x61nary_table_name\x18\x04 \x01(\tR\x0f\x63\x61naryTableName\x12/\n\x14\x64\x65lta_log_table_name\x18\x05 \x01(\tR\x11\x64\x65ltaLogTableName\x12.\n\x13metric_table_prefix\x18\x06 \x01(\tR\x11metricTablePrefix\x12\x34\n\x17\x64\x65lta_log_table_name_v2\x18\x07 \x01(\tR\x13\x64\x65ltaLogTableNameV2\x12\x35\n\x17job_metadata_table_name\x18\x08 \x01(\tR\x14jobMetadataTableName\"\x88\n\n\x14ObjectStoreLocations\x12P\n\x0fmaterialization\x18\x01 \x01(\x0b\x32&.tecton_proto.data.ObjectStoreLocationR\x0fmaterialization\x12Y\n\x14streaming_checkpoint\x18\x02 \x01(\x0b\x32&.tecton_proto.data.ObjectStoreLocationR\x13streamingCheckpoint\x12h\n\x1c\x66\x65\x61ture_server_configuration\x18\x03 \x01(\x0b\x32&.tecton_proto.data.ObjectStoreLocationR\x1a\x66\x65\x61tureServerConfiguration\x12I\n\x0c\x66\x65\x61ture_repo\x18\x04 \x01(\x0b\x32&.tecton_proto.data.ObjectStoreLocationR\x0b\x66\x65\x61tureRepo\x12G\n\x0b\x65mr_scripts\x18\x05 \x01(\x0b\x32&.tecton_proto.data.ObjectStoreLocationR\nemrScripts\x12]\n\x16materialization_params\x18\x06 \x01(\x0b\x32&.tecton_proto.data.ObjectStoreLocationR\x15materializationParams\x12S\n\x11intermediate_data\x18\x07 \x01(\x0b\x32&.tecton_proto.data.ObjectStoreLocationR\x10intermediateData\x12\\\n\x16\x66\x65\x61ture_server_logging\x18\x08 \x01(\x0b\x32&.tecton_proto.data.ObjectStoreLocationR\x14\x66\x65\x61tureServerLogging\x12\\\n\x16kafka_credentials_base\x18\t \x01(\x0b\x32&.tecton_proto.data.ObjectStoreLocationR\x14kafkaCredentialsBase\x12T\n\x12job_metadata_table\x18\r \x01(\x0b\x32&.tecton_proto.data.ObjectStoreLocationR\x10jobMetadataTable\x12\\\n\x16push_api_configuration\x18\n \x01(\x0b\x32&.tecton_proto.data.ObjectStoreLocationR\x14pushApiConfiguration\x12O\n\x0f\x64\x61ta_validation\x18\x0b \x01(\x0b\x32&.tecton_proto.data.ObjectStoreLocationR\x0e\x64\x61taValidation\x12v\n#observability_service_configuration\x18\x0c \x01(\x0b\x32&.tecton_proto.data.ObjectStoreLocationR!observabilityServiceConfiguration\x12X\n\x14system_audit_logging\x18\x0e \x01(\x0b\x32&.tecton_proto.data.ObjectStoreLocationR\x12systemAuditLoggingB\x13\n\x0f\x63om.tecton.dataP\x01')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.data.user_deployment_settings_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\017com.tecton.dataP\001'
@@ -55,9 +55,9 @@
   _OBJECTSTORELOCATION._serialized_start=2712
   _OBJECTSTORELOCATION._serialized_end=2954
   _EMRSETTINGS._serialized_start=2956
   _EMRSETTINGS._serialized_end=3042
   _DYNAMOTABLENAMES._serialized_start=3045
   _DYNAMOTABLENAMES._serialized_end=3469
   _OBJECTSTORELOCATIONS._serialized_start=3472
-  _OBJECTSTORELOCATIONS._serialized_end=4464
+  _OBJECTSTORELOCATIONS._serialized_end=4760
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/data/user_pb2.py` & `tecton-0.7.0rc0/tecton_proto/data/user_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/data/virtual_data_source_pb2.py` & `tecton-0.7.0rc0/tecton_proto/data/virtual_data_source_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/data/workspace_pb2.py` & `tecton-0.7.0rc0/tecton_proto/data/workspace_pb2.py`

 * *Files 8% similar despite different names*

```diff
@@ -7,22 +7,23 @@
 from google.protobuf import descriptor_pool as _descriptor_pool
 from google.protobuf import symbol_database as _symbol_database
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
+from google.protobuf import timestamp_pb2 as google_dot_protobuf_dot_timestamp__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n!tecton_proto/data/workspace.proto\x12\x11tecton_proto.data\"\x93\x01\n\x15WorkspaceCapabilities\x12&\n\x0ematerializable\x18\x01 \x01(\x08R\x0ematerializable\x12R\n\"offline_store_subdirectory_enabled\x18\x02 \x01(\x08:\x05\x66\x61lseR\x1fofflineStoreSubdirectoryEnabled\"m\n\tWorkspace\x12\x12\n\x04name\x18\x01 \x01(\tR\x04name\x12L\n\x0c\x63\x61pabilities\x18\x02 \x01(\x0b\x32(.tecton_proto.data.WorkspaceCapabilitiesR\x0c\x63\x61pabilitiesB\x13\n\x0f\x63om.tecton.dataP\x01')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n!tecton_proto/data/workspace.proto\x12\x11tecton_proto.data\x1a\x1fgoogle/protobuf/timestamp.proto\"\x93\x01\n\x15WorkspaceCapabilities\x12&\n\x0ematerializable\x18\x01 \x01(\x08R\x0ematerializable\x12R\n\"offline_store_subdirectory_enabled\x18\x02 \x01(\x08:\x05\x66\x61lseR\x1fofflineStoreSubdirectoryEnabled\"\xa8\x01\n\tWorkspace\x12\x12\n\x04name\x18\x01 \x01(\tR\x04name\x12L\n\x0c\x63\x61pabilities\x18\x02 \x01(\x0b\x32(.tecton_proto.data.WorkspaceCapabilitiesR\x0c\x63\x61pabilities\x12\x39\n\ncreated_at\x18\x03 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\tcreatedAtB\x13\n\x0f\x63om.tecton.dataP\x01')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.data.workspace_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\017com.tecton.dataP\001'
-  _WORKSPACECAPABILITIES._serialized_start=57
-  _WORKSPACECAPABILITIES._serialized_end=204
-  _WORKSPACE._serialized_start=206
-  _WORKSPACE._serialized_end=315
+  _WORKSPACECAPABILITIES._serialized_start=90
+  _WORKSPACECAPABILITIES._serialized_end=237
+  _WORKSPACE._serialized_start=240
+  _WORKSPACE._serialized_end=408
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/databricks_api/clusters_pb2.py` & `tecton-0.7.0rc0/tecton_proto/databricks_api/clusters_pb2.py`

 * *Files 3% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 
 _sym_db = _symbol_database.Default()
 
 
 from tecton_proto.spark_common import clusters_pb2 as tecton__proto_dot_spark__common_dot_clusters__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n*tecton_proto/databricks_api/clusters.proto\x12\x1btecton_proto.databricks_api\x1a(tecton_proto/spark_common/clusters.proto\"3\n\x12\x43lustersGetRequest\x12\x1d\n\ncluster_id\x18\x01 \x01(\tR\tclusterId\"\xdf\x08\n\x14\x43lusterCreateRequest\x12_\n\nspark_conf\x18\x01 \x03(\x0b\x32@.tecton_proto.databricks_api.ClusterCreateRequest.SparkConfEntryR\tsparkConf\x12-\n\x13\x64river_node_type_id\x18\x02 \x01(\tR\x10\x64riverNodeTypeId\x12 \n\x0cnode_type_id\x18\x03 \x01(\tR\nnodeTypeId\x12\x1f\n\x0bnum_workers\x18\x04 \x01(\x05R\nnumWorkers\x12!\n\x0c\x63luster_name\x18\x05 \x01(\tR\x0b\x63lusterName\x12#\n\rspark_version\x18\x06 \x01(\tR\x0csparkVersion\x12O\n\x0e\x61ws_attributes\x18\x07 \x01(\x0b\x32(.tecton_proto.spark_common.AwsAttributesR\rawsAttributes\x12+\n\x11idempotency_token\x18\x08 \x01(\tR\x10idempotencyToken\x12i\n\x0espark_env_vars\x18\t \x03(\x0b\x32\x43.tecton_proto.databricks_api.ClusterCreateRequest.SparkEnvVarsEntryR\x0csparkEnvVars\x12\x62\n\x0b\x63ustom_tags\x18\n \x03(\x0b\x32\x41.tecton_proto.databricks_api.ClusterCreateRequest.CustomTagsEntryR\ncustomTags\x12\x37\n\x17\x61utotermination_minutes\x18\x0b \x01(\x05R\x16\x61utoterminationMinutes\x12.\n\x13\x65nable_elastic_disk\x18\x0c \x01(\x08R\x11\x65nableElasticDisk\x12K\n\tautoscale\x18\r \x01(\x0b\x32-.tecton_proto.databricks_api.ClusterAutoScaleR\tautoscale\x12N\n\x0cinit_scripts\x18\x0e \x03(\x0b\x32+.tecton_proto.spark_common.ResourceLocationR\x0binitScripts\x12\x1b\n\tpolicy_id\x18\x0f \x01(\tR\x08policyId\x1a<\n\x0eSparkConfEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01\x1a?\n\x11SparkEnvVarsEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01\x1a=\n\x0f\x43ustomTagsEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01\"T\n\x10\x43lusterAutoScale\x12\x1f\n\x0bmin_workers\x18\x01 \x01(\x05R\nminWorkers\x12\x1f\n\x0bmax_workers\x18\x02 \x01(\x05R\nmaxWorkers\"6\n\x15\x43lusterCreateResponse\x12\x1d\n\ncluster_id\x18\x01 \x01(\tR\tclusterId\"8\n\x17\x43lusterTerminateRequest\x12\x1d\n\ncluster_id\x18\x01 \x01(\tR\tclusterId\"\xf9\x01\n\x13\x43lustersGetResponse\x12\x1d\n\ncluster_id\x18\x01 \x01(\tR\tclusterId\x12#\n\rstate_message\x18\x02 \x01(\tR\x0cstateMessage\x12]\n\x12termination_reason\x18\x03 \x01(\x0b\x32..tecton_proto.databricks_api.TerminationReasonR\x11terminationReason\x12?\n\x05state\x18\x04 \x01(\x0e\x32).tecton_proto.databricks_api.ClusterStateR\x05state\"\x97\x01\n\x11TerminationReason\x12@\n\x04\x63ode\x18\x01 \x01(\x0e\x32,.tecton_proto.databricks_api.TerminationCodeR\x04\x63ode\x12@\n\x04type\x18\x02 \x01(\x0e\x32,.tecton_proto.databricks_api.TerminationTypeR\x04type\"W\n\x13\x43lusterListResponse\x12@\n\x08\x63lusters\x18\x01 \x03(\x0b\x32$.tecton_proto.databricks_api.ClusterR\x08\x63lusters\"\xc7\x02\n\x07\x43luster\x12\x1d\n\ncluster_id\x18\x01 \x01(\tR\tclusterId\x12!\n\x0c\x63luster_name\x18\x02 \x01(\tR\x0b\x63lusterName\x12#\n\rspark_version\x18\x03 \x01(\tR\x0csparkVersion\x12?\n\x05state\x18\x04 \x01(\x0e\x32).tecton_proto.databricks_api.ClusterStateR\x05state\x12U\n\x0b\x63ustom_tags\x18\x05 \x03(\x0b\x32\x34.tecton_proto.databricks_api.Cluster.CustomTagsEntryR\ncustomTags\x1a=\n\x0f\x43ustomTagsEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01*\x7f\n\x0c\x43lusterState\x12\x0b\n\x07PENDING\x10\x00\x12\x0b\n\x07RUNNING\x10\x01\x12\x0e\n\nRESTARTING\x10\x02\x12\x0c\n\x08RESIZING\x10\x03\x12\x0f\n\x0bTERMINATING\x10\x04\x12\x0e\n\nTERMINATED\x10\x05\x12\t\n\x05\x45RROR\x10\x06\x12\x0b\n\x07UNKNOWN\x10\x07*\xf0\x04\n\x0fTerminationCode\x12\x1d\n\x19UNKNOWN_TERMINATION_STATE\x10\x00\x12\x10\n\x0cUSER_REQUEST\x10\x01\x12\x10\n\x0cJOB_FINISHED\x10\x02\x12\x0e\n\nINACTIVITY\x10\x03\x12\x1b\n\x17\x43LOUD_PROVIDER_SHUTDOWN\x10\x04\x12\x16\n\x12\x43OMMUNICATION_LOST\x10\x05\x12!\n\x1d\x43LOUD_PROVIDER_LAUNCH_FAILURE\x10\x06\x12\x19\n\x15SPARK_STARTUP_FAILURE\x10\x07\x12\x14\n\x10INVALID_ARGUMENT\x10\x08\x12\x1d\n\x19UNEXPECTED_LAUNCH_FAILURE\x10\t\x12\x12\n\x0eINTERNAL_ERROR\x10\n\x12\x0f\n\x0bSPARK_ERROR\x10\x0b\x12!\n\x1dMETASTORE_COMPONENT_UNHEALTHY\x10\x0c\x12\x1c\n\x18\x44\x42\x46S_COMPONENT_UNHEALTHY\x10\r\x12\x16\n\x12\x44RIVER_UNREACHABLE\x10\x0e\x12\x17\n\x13\x44RIVER_UNRESPONSIVE\x10\x0f\x12\x18\n\x14INSTANCE_UNREACHABLE\x10\x10\x12\x1c\n\x18\x43ONTAINER_LAUNCH_FAILURE\x10\x11\x12!\n\x1dINSTANCE_POOL_CLUSTER_FAILURE\x10\x12\x12\x14\n\x10REQUEST_REJECTED\x10\x13\x12\x17\n\x13INIT_SCRIPT_FAILURE\x10\x14\x12\x11\n\rTRIAL_EXPIRED\x10\x15\x12.\n*AWS_INSUFFICIENT_INSTANCE_CAPACITY_FAILURE\x10\x16*t\n\x0fTerminationType\x12\x1c\n\x18UNKNOWN_TERMINATION_TYPE\x10\x00\x12\x0b\n\x07SUCCESS\x10\x01\x12\x10\n\x0c\x43LIENT_ERROR\x10\x02\x12\x11\n\rSERVICE_FAULT\x10\x03\x12\x11\n\rCLOUD_FAILURE\x10\x04\x42\x1d\n\x19\x63om.tecton.databricks_apiP\x01')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n*tecton_proto/databricks_api/clusters.proto\x12\x1btecton_proto.databricks_api\x1a(tecton_proto/spark_common/clusters.proto\"3\n\x12\x43lustersGetRequest\x12\x1d\n\ncluster_id\x18\x01 \x01(\tR\tclusterId\"\x88\n\n\x14\x43lusterCreateRequest\x12_\n\nspark_conf\x18\x01 \x03(\x0b\x32@.tecton_proto.databricks_api.ClusterCreateRequest.SparkConfEntryR\tsparkConf\x12-\n\x13\x64river_node_type_id\x18\x02 \x01(\tR\x10\x64riverNodeTypeId\x12 \n\x0cnode_type_id\x18\x03 \x01(\tR\nnodeTypeId\x12\x1f\n\x0bnum_workers\x18\x04 \x01(\x05R\nnumWorkers\x12!\n\x0c\x63luster_name\x18\x05 \x01(\tR\x0b\x63lusterName\x12#\n\rspark_version\x18\x06 \x01(\tR\x0csparkVersion\x12O\n\x0e\x61ws_attributes\x18\x07 \x01(\x0b\x32(.tecton_proto.spark_common.AwsAttributesR\rawsAttributes\x12+\n\x11idempotency_token\x18\x08 \x01(\tR\x10idempotencyToken\x12i\n\x0espark_env_vars\x18\t \x03(\x0b\x32\x43.tecton_proto.databricks_api.ClusterCreateRequest.SparkEnvVarsEntryR\x0csparkEnvVars\x12\x62\n\x0b\x63ustom_tags\x18\n \x03(\x0b\x32\x41.tecton_proto.databricks_api.ClusterCreateRequest.CustomTagsEntryR\ncustomTags\x12\x37\n\x17\x61utotermination_minutes\x18\x0b \x01(\x05R\x16\x61utoterminationMinutes\x12.\n\x13\x65nable_elastic_disk\x18\x0c \x01(\x08R\x11\x65nableElasticDisk\x12K\n\tautoscale\x18\r \x01(\x0b\x32-.tecton_proto.databricks_api.ClusterAutoScaleR\tautoscale\x12N\n\x0cinit_scripts\x18\x0e \x03(\x0b\x32+.tecton_proto.spark_common.ResourceLocationR\x0binitScripts\x12\x1b\n\tpolicy_id\x18\x0f \x01(\tR\x08policyId\x12O\n\x0egcp_attributes\x18\x10 \x01(\x0b\x32(.tecton_proto.spark_common.GCPAttributesR\rgcpAttributes\x12,\n\x12\x64\x61ta_security_mode\x18\x11 \x01(\tR\x10\x64\x61taSecurityMode\x12(\n\x10single_user_name\x18\x12 \x01(\tR\x0esingleUserName\x1a<\n\x0eSparkConfEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01\x1a?\n\x11SparkEnvVarsEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01\x1a=\n\x0f\x43ustomTagsEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01\"T\n\x10\x43lusterAutoScale\x12\x1f\n\x0bmin_workers\x18\x01 \x01(\x05R\nminWorkers\x12\x1f\n\x0bmax_workers\x18\x02 \x01(\x05R\nmaxWorkers\"6\n\x15\x43lusterCreateResponse\x12\x1d\n\ncluster_id\x18\x01 \x01(\tR\tclusterId\"8\n\x17\x43lusterTerminateRequest\x12\x1d\n\ncluster_id\x18\x01 \x01(\tR\tclusterId\"\xf9\x01\n\x13\x43lustersGetResponse\x12\x1d\n\ncluster_id\x18\x01 \x01(\tR\tclusterId\x12#\n\rstate_message\x18\x02 \x01(\tR\x0cstateMessage\x12]\n\x12termination_reason\x18\x03 \x01(\x0b\x32..tecton_proto.databricks_api.TerminationReasonR\x11terminationReason\x12?\n\x05state\x18\x04 \x01(\x0e\x32).tecton_proto.databricks_api.ClusterStateR\x05state\"\x97\x01\n\x11TerminationReason\x12@\n\x04\x63ode\x18\x01 \x01(\x0e\x32,.tecton_proto.databricks_api.TerminationCodeR\x04\x63ode\x12@\n\x04type\x18\x02 \x01(\x0e\x32,.tecton_proto.databricks_api.TerminationTypeR\x04type\"W\n\x13\x43lusterListResponse\x12@\n\x08\x63lusters\x18\x01 \x03(\x0b\x32$.tecton_proto.databricks_api.ClusterR\x08\x63lusters\"\xc7\x02\n\x07\x43luster\x12\x1d\n\ncluster_id\x18\x01 \x01(\tR\tclusterId\x12!\n\x0c\x63luster_name\x18\x02 \x01(\tR\x0b\x63lusterName\x12#\n\rspark_version\x18\x03 \x01(\tR\x0csparkVersion\x12?\n\x05state\x18\x04 \x01(\x0e\x32).tecton_proto.databricks_api.ClusterStateR\x05state\x12U\n\x0b\x63ustom_tags\x18\x05 \x03(\x0b\x32\x34.tecton_proto.databricks_api.Cluster.CustomTagsEntryR\ncustomTags\x1a=\n\x0f\x43ustomTagsEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01*\x7f\n\x0c\x43lusterState\x12\x0b\n\x07PENDING\x10\x00\x12\x0b\n\x07RUNNING\x10\x01\x12\x0e\n\nRESTARTING\x10\x02\x12\x0c\n\x08RESIZING\x10\x03\x12\x0f\n\x0bTERMINATING\x10\x04\x12\x0e\n\nTERMINATED\x10\x05\x12\t\n\x05\x45RROR\x10\x06\x12\x0b\n\x07UNKNOWN\x10\x07*\xf0\x04\n\x0fTerminationCode\x12\x1d\n\x19UNKNOWN_TERMINATION_STATE\x10\x00\x12\x10\n\x0cUSER_REQUEST\x10\x01\x12\x10\n\x0cJOB_FINISHED\x10\x02\x12\x0e\n\nINACTIVITY\x10\x03\x12\x1b\n\x17\x43LOUD_PROVIDER_SHUTDOWN\x10\x04\x12\x16\n\x12\x43OMMUNICATION_LOST\x10\x05\x12!\n\x1d\x43LOUD_PROVIDER_LAUNCH_FAILURE\x10\x06\x12\x19\n\x15SPARK_STARTUP_FAILURE\x10\x07\x12\x14\n\x10INVALID_ARGUMENT\x10\x08\x12\x1d\n\x19UNEXPECTED_LAUNCH_FAILURE\x10\t\x12\x12\n\x0eINTERNAL_ERROR\x10\n\x12\x0f\n\x0bSPARK_ERROR\x10\x0b\x12!\n\x1dMETASTORE_COMPONENT_UNHEALTHY\x10\x0c\x12\x1c\n\x18\x44\x42\x46S_COMPONENT_UNHEALTHY\x10\r\x12\x16\n\x12\x44RIVER_UNREACHABLE\x10\x0e\x12\x17\n\x13\x44RIVER_UNRESPONSIVE\x10\x0f\x12\x18\n\x14INSTANCE_UNREACHABLE\x10\x10\x12\x1c\n\x18\x43ONTAINER_LAUNCH_FAILURE\x10\x11\x12!\n\x1dINSTANCE_POOL_CLUSTER_FAILURE\x10\x12\x12\x14\n\x10REQUEST_REJECTED\x10\x13\x12\x17\n\x13INIT_SCRIPT_FAILURE\x10\x14\x12\x11\n\rTRIAL_EXPIRED\x10\x15\x12.\n*AWS_INSUFFICIENT_INSTANCE_CAPACITY_FAILURE\x10\x16*t\n\x0fTerminationType\x12\x1c\n\x18UNKNOWN_TERMINATION_TYPE\x10\x00\x12\x0b\n\x07SUCCESS\x10\x01\x12\x10\n\x0c\x43LIENT_ERROR\x10\x02\x12\x11\n\rSERVICE_FAULT\x10\x03\x12\x11\n\rCLOUD_FAILURE\x10\x04\x42\x1d\n\x19\x63om.tecton.databricks_apiP\x01')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.databricks_api.clusters_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\031com.tecton.databricks_apiP\001'
@@ -26,40 +26,40 @@
   _CLUSTERCREATEREQUEST_SPARKCONFENTRY._serialized_options = b'8\001'
   _CLUSTERCREATEREQUEST_SPARKENVVARSENTRY._options = None
   _CLUSTERCREATEREQUEST_SPARKENVVARSENTRY._serialized_options = b'8\001'
   _CLUSTERCREATEREQUEST_CUSTOMTAGSENTRY._options = None
   _CLUSTERCREATEREQUEST_CUSTOMTAGSENTRY._serialized_options = b'8\001'
   _CLUSTER_CUSTOMTAGSENTRY._options = None
   _CLUSTER_CUSTOMTAGSENTRY._serialized_options = b'8\001'
-  _CLUSTERSTATE._serialized_start=2317
-  _CLUSTERSTATE._serialized_end=2444
-  _TERMINATIONCODE._serialized_start=2447
-  _TERMINATIONCODE._serialized_end=3071
-  _TERMINATIONTYPE._serialized_start=3073
-  _TERMINATIONTYPE._serialized_end=3189
+  _CLUSTERSTATE._serialized_start=2486
+  _CLUSTERSTATE._serialized_end=2613
+  _TERMINATIONCODE._serialized_start=2616
+  _TERMINATIONCODE._serialized_end=3240
+  _TERMINATIONTYPE._serialized_start=3242
+  _TERMINATIONTYPE._serialized_end=3358
   _CLUSTERSGETREQUEST._serialized_start=117
   _CLUSTERSGETREQUEST._serialized_end=168
   _CLUSTERCREATEREQUEST._serialized_start=171
-  _CLUSTERCREATEREQUEST._serialized_end=1290
-  _CLUSTERCREATEREQUEST_SPARKCONFENTRY._serialized_start=1102
-  _CLUSTERCREATEREQUEST_SPARKCONFENTRY._serialized_end=1162
-  _CLUSTERCREATEREQUEST_SPARKENVVARSENTRY._serialized_start=1164
-  _CLUSTERCREATEREQUEST_SPARKENVVARSENTRY._serialized_end=1227
-  _CLUSTERCREATEREQUEST_CUSTOMTAGSENTRY._serialized_start=1229
-  _CLUSTERCREATEREQUEST_CUSTOMTAGSENTRY._serialized_end=1290
-  _CLUSTERAUTOSCALE._serialized_start=1292
-  _CLUSTERAUTOSCALE._serialized_end=1376
-  _CLUSTERCREATERESPONSE._serialized_start=1378
-  _CLUSTERCREATERESPONSE._serialized_end=1432
-  _CLUSTERTERMINATEREQUEST._serialized_start=1434
-  _CLUSTERTERMINATEREQUEST._serialized_end=1490
-  _CLUSTERSGETRESPONSE._serialized_start=1493
-  _CLUSTERSGETRESPONSE._serialized_end=1742
-  _TERMINATIONREASON._serialized_start=1745
-  _TERMINATIONREASON._serialized_end=1896
-  _CLUSTERLISTRESPONSE._serialized_start=1898
-  _CLUSTERLISTRESPONSE._serialized_end=1985
-  _CLUSTER._serialized_start=1988
-  _CLUSTER._serialized_end=2315
-  _CLUSTER_CUSTOMTAGSENTRY._serialized_start=1229
-  _CLUSTER_CUSTOMTAGSENTRY._serialized_end=1290
+  _CLUSTERCREATEREQUEST._serialized_end=1459
+  _CLUSTERCREATEREQUEST_SPARKCONFENTRY._serialized_start=1271
+  _CLUSTERCREATEREQUEST_SPARKCONFENTRY._serialized_end=1331
+  _CLUSTERCREATEREQUEST_SPARKENVVARSENTRY._serialized_start=1333
+  _CLUSTERCREATEREQUEST_SPARKENVVARSENTRY._serialized_end=1396
+  _CLUSTERCREATEREQUEST_CUSTOMTAGSENTRY._serialized_start=1398
+  _CLUSTERCREATEREQUEST_CUSTOMTAGSENTRY._serialized_end=1459
+  _CLUSTERAUTOSCALE._serialized_start=1461
+  _CLUSTERAUTOSCALE._serialized_end=1545
+  _CLUSTERCREATERESPONSE._serialized_start=1547
+  _CLUSTERCREATERESPONSE._serialized_end=1601
+  _CLUSTERTERMINATEREQUEST._serialized_start=1603
+  _CLUSTERTERMINATEREQUEST._serialized_end=1659
+  _CLUSTERSGETRESPONSE._serialized_start=1662
+  _CLUSTERSGETRESPONSE._serialized_end=1911
+  _TERMINATIONREASON._serialized_start=1914
+  _TERMINATIONREASON._serialized_end=2065
+  _CLUSTERLISTRESPONSE._serialized_start=2067
+  _CLUSTERLISTRESPONSE._serialized_end=2154
+  _CLUSTER._serialized_start=2157
+  _CLUSTER._serialized_end=2484
+  _CLUSTER_CUSTOMTAGSENTRY._serialized_start=1398
+  _CLUSTER_CUSTOMTAGSENTRY._serialized_end=1459
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/databricks_api/dbfs_pb2.py` & `tecton-0.7.0rc0/tecton_proto/databricks_api/dbfs_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/databricks_api/error_pb2.py` & `tecton-0.7.0rc0/tecton_proto/databricks_api/error_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/databricks_api/execution_pb2.py` & `tecton-0.7.0rc0/tecton_proto/databricks_api/execution_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/databricks_api/instance_profiles_pb2.py` & `tecton-0.7.0rc0/tecton_proto/databricks_api/instance_profiles_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/databricks_api/jobs_pb2.py` & `tecton-0.7.0rc0/tecton_proto/databricks_api/jobs_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/databricks_api/libraries_pb2.py` & `tecton-0.7.0rc0/tecton_proto/databricks_api/libraries_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/databricks_api/permissions_pb2.py` & `tecton-0.7.0rc0/tecton_proto/databricks_api/permissions_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/databricks_api/scim_pb2.py` & `tecton-0.7.0rc0/tecton_proto/databricks_api/scim_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/databricks_api/secrets_pb2.py` & `tecton-0.7.0rc0/tecton_proto/databricks_api/secrets_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/databricks_api/workspace_pb2.py` & `tecton-0.7.0rc0/tecton_proto/databricks_api/workspace_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/dataobs/config_pb2.py` & `tecton-0.7.0rc0/tecton_proto/dataobs/config_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/dataobs/expectation_pb2.py` & `tecton-0.7.0rc0/tecton_proto/dataobs/expectation_pb2.py`

 * *Files 8% similar despite different names*

```diff
@@ -11,20 +11,20 @@
 _sym_db = _symbol_database.Default()
 
 
 from google.protobuf import timestamp_pb2 as google_dot_protobuf_dot_timestamp__pb2
 from tecton_proto.dataobs import metric_pb2 as tecton__proto_dot_dataobs_dot_metric__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n&tecton_proto/dataobs/expectation.proto\x12\x14tecton_proto.dataobs\x1a\x1fgoogle/protobuf/timestamp.proto\x1a!tecton_proto/dataobs/metric.proto\"\xb3\x02\n\x12\x46\x65\x61tureExpectation\x12\x12\n\x04name\x18\x01 \x01(\tR\x04name\x12\x1e\n\nexpression\x18\x02 \x01(\tR\nexpression\x12\x34\n\x16\x61lert_message_template\x18\x03 \x01(\tR\x14\x61lertMessageTemplate\x12,\n\x12input_column_names\x18\x04 \x03(\tR\x10inputColumnNames\x12?\n\rcreation_time\x18\x06 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x0c\x63reationTime\x12\x44\n\x10last_update_time\x18\x07 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x0elastUpdateTime\"\xce\x02\n\x11MetricExpectation\x12\x12\n\x04name\x18\x01 \x01(\tR\x04name\x12\x1e\n\nexpression\x18\x02 \x01(\tR\nexpression\x12\x34\n\x16\x61lert_message_template\x18\x03 \x01(\tR\x14\x61lertMessageTemplate\x12H\n\rinput_metrics\x18\x04 \x03(\x0b\x32#.tecton_proto.dataobs.FeatureMetricR\x0cinputMetrics\x12?\n\rcreation_time\x18\x06 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x0c\x63reationTime\x12\x44\n\x10last_update_time\x18\x07 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x0elastUpdateTimeB\x16\n\x12\x63om.tecton.dataobsP\x01')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n&tecton_proto/dataobs/expectation.proto\x12\x14tecton_proto.dataobs\x1a\x1fgoogle/protobuf/timestamp.proto\x1a!tecton_proto/dataobs/metric.proto\"\xb3\x02\n\x12\x46\x65\x61tureExpectation\x12\x12\n\x04name\x18\x01 \x01(\tR\x04name\x12\x1e\n\nexpression\x18\x02 \x01(\tR\nexpression\x12\x34\n\x16\x61lert_message_template\x18\x03 \x01(\tR\x14\x61lertMessageTemplate\x12,\n\x12input_column_names\x18\x04 \x03(\tR\x10inputColumnNames\x12?\n\rcreation_time\x18\x06 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x0c\x63reationTime\x12\x44\n\x10last_update_time\x18\x07 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x0elastUpdateTime\"\xf1\x02\n\x11MetricExpectation\x12\x12\n\x04name\x18\x01 \x01(\tR\x04name\x12!\n\x0c\x64isplay_name\x18\x08 \x01(\tR\x0b\x64isplayName\x12\x1e\n\nexpression\x18\x02 \x01(\tR\nexpression\x12\x34\n\x16\x61lert_message_template\x18\x03 \x01(\tR\x14\x61lertMessageTemplate\x12H\n\rinput_metrics\x18\x04 \x03(\x0b\x32#.tecton_proto.dataobs.FeatureMetricR\x0cinputMetrics\x12?\n\rcreation_time\x18\x06 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x0c\x63reationTime\x12\x44\n\x10last_update_time\x18\x07 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x0elastUpdateTimeBA\n\x12\x63om.tecton.dataobsP\x01Z)github.com/tecton-ai/tecton_proto/dataobs')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.dataobs.expectation_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
-  DESCRIPTOR._serialized_options = b'\n\022com.tecton.dataobsP\001'
+  DESCRIPTOR._serialized_options = b'\n\022com.tecton.dataobsP\001Z)github.com/tecton-ai/tecton_proto/dataobs'
   _FEATUREEXPECTATION._serialized_start=133
   _FEATUREEXPECTATION._serialized_end=440
   _METRICEXPECTATION._serialized_start=443
-  _METRICEXPECTATION._serialized_end=777
+  _METRICEXPECTATION._serialized_end=812
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/dataobs/metric_pb2.py` & `tecton-0.7.0rc0/tecton_proto/dataobs/metric_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/dataobs/validation_pb2.py` & `tecton-0.7.0rc0/tecton_proto/dataobs/validation_pb2.py`

 * *Files 3% similar despite different names*

```diff
@@ -12,34 +12,34 @@
 
 
 from google.protobuf import timestamp_pb2 as google_dot_protobuf_dot_timestamp__pb2
 from tecton_proto.common import id_pb2 as tecton__proto_dot_common_dot_id__pb2
 from tecton_proto.dataobs import expectation_pb2 as tecton__proto_dot_dataobs_dot_expectation__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n%tecton_proto/dataobs/validation.proto\x12\x14tecton_proto.dataobs\x1a\x1fgoogle/protobuf/timestamp.proto\x1a\x1ctecton_proto/common/id.proto\x1a&tecton_proto/dataobs/expectation.proto\"\xc3\x05\n\x11\x45xpectationResult\x12\x43\n\x11validation_job_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x0fvalidationJobId\x12\x1c\n\tworkspace\x18\x02 \x01(\tR\tworkspace\x12*\n\x11\x66\x65\x61ture_view_name\x18\t \x01(\tR\x0f\x66\x65\x61tureViewName\x12\x45\n\x12\x66\x65\x61ture_package_id\x18\x03 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x10\x66\x65\x61turePackageId\x12\x43\n\x0fvalidation_time\x18\x04 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x0evalidationTime\x12Y\n\x1b\x66\x65\x61ture_interval_start_time\x18\x05 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x18\x66\x65\x61tureIntervalStartTime\x12t\n\x1c\x66\x65\x61ture_expectation_metadata\x18\x06 \x01(\x0b\x32\x30.tecton_proto.dataobs.FeatureExpectationMetadataH\x00R\x1a\x66\x65\x61tureExpectationMetadata\x12q\n\x1bmetric_expectation_metadata\x18\x07 \x01(\x0b\x32/.tecton_proto.dataobs.MetricExpectationMetadataH\x00R\x19metricExpectationMetadata\x12\x43\n\x06result\x18\x08 \x01(\x0e\x32+.tecton_proto.dataobs.ExpectationResultEnumR\x06resultB\n\n\x08metadata\"\xeb\x01\n\x1a\x46\x65\x61tureExpectationMetadata\x12J\n\x0b\x65xpectation\x18\x01 \x01(\x0b\x32(.tecton_proto.dataobs.FeatureExpectationR\x0b\x65xpectation\x12\x1b\n\talert_msg\x18\x03 \x01(\tR\x08\x61lertMsg\x12-\n\x12\x66\x61ilure_percentage\x18\x04 \x01(\x01R\x11\x66\x61ilurePercentage\x12\x35\n\x17\x66\x61iled_join_key_samples\x18\x05 \x03(\tR\x14\x66\x61iledJoinKeySamples\"\x81\x03\n\x19MetricExpectationMetadata\x12I\n\x0b\x65xpectation\x18\x01 \x01(\x0b\x32\'.tecton_proto.dataobs.MetricExpectationR\x0b\x65xpectation\x12\x1b\n\talert_msg\x18\x03 \x01(\tR\x08\x61lertMsg\x12]\n\x0cparam_values\x18\x04 \x03(\x0b\x32:.tecton_proto.dataobs.MetricExpectationMetadata.ParamValueR\x0bparamValues\x1a\x9c\x01\n\nParamValue\x12\x1f\n\x0bmetric_name\x18\x01 \x01(\tR\nmetricName\x12!\n\x0c\x61\x63tual_value\x18\x02 \x01(\tR\x0b\x61\x63tualValue\x12J\n\x13interval_start_time\x18\x03 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x11intervalStartTime\"o\n\rResultSummary\x12\x16\n\x06passed\x18\x01 \x01(\x05R\x06passed\x12\x16\n\x06\x66\x61iled\x18\x02 \x01(\x05R\x06\x66\x61iled\x12\x14\n\x05\x65rror\x18\x03 \x01(\x05R\x05\x65rror\x12\x18\n\x07unknown\x18\x04 \x01(\x05R\x07unknown\"\xd7\x01\n\x16WorkspaceResultSummary\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12=\n\x07summary\x18\x02 \x01(\x0b\x32#.tecton_proto.dataobs.ResultSummaryR\x07summary\x12`\n\x14\x66\x65\x61ture_view_summary\x18\x03 \x03(\x0b\x32..tecton_proto.dataobs.FeatureViewResultSummaryR\x12\x66\x65\x61tureViewSummary\"\xe6\x01\n\x18\x46\x65\x61tureViewResultSummary\x12*\n\x11\x66\x65\x61ture_view_name\x18\x01 \x01(\tR\x0f\x66\x65\x61tureViewName\x12=\n\x07summary\x18\x02 \x01(\x0b\x32#.tecton_proto.dataobs.ResultSummaryR\x07summary\x12_\n\x13\x65xpectation_summary\x18\x03 \x03(\x0b\x32..tecton_proto.dataobs.ExpectationResultSummaryR\x12\x65xpectationSummary\"\x84\x01\n\x18\x45xpectationResultSummary\x12)\n\x10\x65xpectation_name\x18\x01 \x01(\tR\x0f\x65xpectationName\x12=\n\x07summary\x18\x02 \x01(\x0b\x32#.tecton_proto.dataobs.ResultSummaryR\x07summary*c\n\x15\x45xpectationResultEnum\x12\x12\n\x0eRESULT_UNKNOWN\x10\x00\x12\x11\n\rRESULT_PASSED\x10\x01\x12\x11\n\rRESULT_FAILED\x10\x02\x12\x10\n\x0cRESULT_ERROR\x10\x03\x42\x16\n\x12\x63om.tecton.dataobsP\x01')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n%tecton_proto/dataobs/validation.proto\x12\x14tecton_proto.dataobs\x1a\x1fgoogle/protobuf/timestamp.proto\x1a\x1ctecton_proto/common/id.proto\x1a&tecton_proto/dataobs/expectation.proto\"\xd0\x06\n\x11\x45xpectationResult\x12\x43\n\x11validation_job_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x0fvalidationJobId\x12\x1c\n\tworkspace\x18\x02 \x01(\tR\tworkspace\x12*\n\x11\x66\x65\x61ture_view_name\x18\t \x01(\tR\x0f\x66\x65\x61tureViewName\x12\x45\n\x12\x66\x65\x61ture_package_id\x18\x03 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x10\x66\x65\x61turePackageId\x12\x43\n\x0fvalidation_time\x18\x04 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x0evalidationTime\x12Y\n\x1b\x66\x65\x61ture_interval_start_time\x18\x05 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x18\x66\x65\x61tureIntervalStartTime\x12U\n\x19\x66\x65\x61ture_interval_end_time\x18\x0b \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x16\x66\x65\x61tureIntervalEndTime\x12t\n\x1c\x66\x65\x61ture_expectation_metadata\x18\x06 \x01(\x0b\x32\x30.tecton_proto.dataobs.FeatureExpectationMetadataH\x00R\x1a\x66\x65\x61tureExpectationMetadata\x12q\n\x1bmetric_expectation_metadata\x18\x07 \x01(\x0b\x32/.tecton_proto.dataobs.MetricExpectationMetadataH\x00R\x19metricExpectationMetadata\x12\x43\n\x06result\x18\x08 \x01(\x0e\x32+.tecton_proto.dataobs.ExpectationResultEnumR\x06result\x12\x34\n\tresult_id\x18\n \x01(\x0b\x32\x17.tecton_proto.common.IdR\x08resultIdB\n\n\x08metadata\"\xeb\x01\n\x1a\x46\x65\x61tureExpectationMetadata\x12J\n\x0b\x65xpectation\x18\x01 \x01(\x0b\x32(.tecton_proto.dataobs.FeatureExpectationR\x0b\x65xpectation\x12\x1b\n\talert_msg\x18\x03 \x01(\tR\x08\x61lertMsg\x12-\n\x12\x66\x61ilure_percentage\x18\x04 \x01(\x01R\x11\x66\x61ilurePercentage\x12\x35\n\x17\x66\x61iled_join_key_samples\x18\x05 \x03(\tR\x14\x66\x61iledJoinKeySamples\"\x81\x03\n\x19MetricExpectationMetadata\x12I\n\x0b\x65xpectation\x18\x01 \x01(\x0b\x32\'.tecton_proto.dataobs.MetricExpectationR\x0b\x65xpectation\x12\x1b\n\talert_msg\x18\x03 \x01(\tR\x08\x61lertMsg\x12]\n\x0cparam_values\x18\x04 \x03(\x0b\x32:.tecton_proto.dataobs.MetricExpectationMetadata.ParamValueR\x0bparamValues\x1a\x9c\x01\n\nParamValue\x12\x1f\n\x0bmetric_name\x18\x01 \x01(\tR\nmetricName\x12!\n\x0c\x61\x63tual_value\x18\x02 \x01(\tR\x0b\x61\x63tualValue\x12J\n\x13interval_start_time\x18\x03 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x11intervalStartTime\"o\n\rResultSummary\x12\x16\n\x06passed\x18\x01 \x01(\x05R\x06passed\x12\x16\n\x06\x66\x61iled\x18\x02 \x01(\x05R\x06\x66\x61iled\x12\x14\n\x05\x65rror\x18\x03 \x01(\x05R\x05\x65rror\x12\x18\n\x07unknown\x18\x04 \x01(\x05R\x07unknown\"\xd7\x01\n\x16WorkspaceResultSummary\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12=\n\x07summary\x18\x02 \x01(\x0b\x32#.tecton_proto.dataobs.ResultSummaryR\x07summary\x12`\n\x14\x66\x65\x61ture_view_summary\x18\x03 \x03(\x0b\x32..tecton_proto.dataobs.FeatureViewResultSummaryR\x12\x66\x65\x61tureViewSummary\"\xe6\x01\n\x18\x46\x65\x61tureViewResultSummary\x12*\n\x11\x66\x65\x61ture_view_name\x18\x01 \x01(\tR\x0f\x66\x65\x61tureViewName\x12=\n\x07summary\x18\x02 \x01(\x0b\x32#.tecton_proto.dataobs.ResultSummaryR\x07summary\x12_\n\x13\x65xpectation_summary\x18\x03 \x03(\x0b\x32..tecton_proto.dataobs.ExpectationResultSummaryR\x12\x65xpectationSummary\"\x84\x01\n\x18\x45xpectationResultSummary\x12)\n\x10\x65xpectation_name\x18\x01 \x01(\tR\x0f\x65xpectationName\x12=\n\x07summary\x18\x02 \x01(\x0b\x32#.tecton_proto.dataobs.ResultSummaryR\x07summary*c\n\x15\x45xpectationResultEnum\x12\x12\n\x0eRESULT_UNKNOWN\x10\x00\x12\x11\n\rRESULT_PASSED\x10\x01\x12\x11\n\rRESULT_FAILED\x10\x02\x12\x10\n\x0cRESULT_ERROR\x10\x03\x42\x16\n\x12\x63om.tecton.dataobsP\x01')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.dataobs.validation_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\022com.tecton.dataobsP\001'
-  _EXPECTATIONRESULTENUM._serialized_start=2201
-  _EXPECTATIONRESULTENUM._serialized_end=2300
+  _EXPECTATIONRESULTENUM._serialized_start=2342
+  _EXPECTATIONRESULTENUM._serialized_end=2441
   _EXPECTATIONRESULT._serialized_start=167
-  _EXPECTATIONRESULT._serialized_end=874
-  _FEATUREEXPECTATIONMETADATA._serialized_start=877
-  _FEATUREEXPECTATIONMETADATA._serialized_end=1112
-  _METRICEXPECTATIONMETADATA._serialized_start=1115
-  _METRICEXPECTATIONMETADATA._serialized_end=1500
-  _METRICEXPECTATIONMETADATA_PARAMVALUE._serialized_start=1344
-  _METRICEXPECTATIONMETADATA_PARAMVALUE._serialized_end=1500
-  _RESULTSUMMARY._serialized_start=1502
-  _RESULTSUMMARY._serialized_end=1613
-  _WORKSPACERESULTSUMMARY._serialized_start=1616
-  _WORKSPACERESULTSUMMARY._serialized_end=1831
-  _FEATUREVIEWRESULTSUMMARY._serialized_start=1834
-  _FEATUREVIEWRESULTSUMMARY._serialized_end=2064
-  _EXPECTATIONRESULTSUMMARY._serialized_start=2067
-  _EXPECTATIONRESULTSUMMARY._serialized_end=2199
+  _EXPECTATIONRESULT._serialized_end=1015
+  _FEATUREEXPECTATIONMETADATA._serialized_start=1018
+  _FEATUREEXPECTATIONMETADATA._serialized_end=1253
+  _METRICEXPECTATIONMETADATA._serialized_start=1256
+  _METRICEXPECTATIONMETADATA._serialized_end=1641
+  _METRICEXPECTATIONMETADATA_PARAMVALUE._serialized_start=1485
+  _METRICEXPECTATIONMETADATA_PARAMVALUE._serialized_end=1641
+  _RESULTSUMMARY._serialized_start=1643
+  _RESULTSUMMARY._serialized_end=1754
+  _WORKSPACERESULTSUMMARY._serialized_start=1757
+  _WORKSPACERESULTSUMMARY._serialized_end=1972
+  _FEATUREVIEWRESULTSUMMARY._serialized_start=1975
+  _FEATUREVIEWRESULTSUMMARY._serialized_end=2205
+  _EXPECTATIONRESULTSUMMARY._serialized_start=2208
+  _EXPECTATIONRESULTSUMMARY._serialized_end=2340
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/dataobs/validation_task_params_pb2.py` & `tecton-0.7.0rc0/tecton_proto/dataobs/validation_task_params_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/dataobs/validation_task_pb2.py` & `tecton-0.7.0rc0/tecton_proto/dataobs/validation_task_pb2.py`

 * *Files 18% similar despite different names*

```diff
@@ -16,22 +16,24 @@
 from tecton_proto.common import id_pb2 as tecton__proto_dot_common_dot_id__pb2
 from tecton_proto.common import fco_locator_pb2 as tecton__proto_dot_common_dot_fco__locator__pb2
 from tecton_proto.dataobs import expectation_pb2 as tecton__proto_dot_dataobs_dot_expectation__pb2
 from tecton_proto.dataobs import validation_pb2 as tecton__proto_dot_dataobs_dot_validation__pb2
 from tecton_proto.dataobs import validation_task_params_pb2 as tecton__proto_dot_dataobs_dot_validation__task__params__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n*tecton_proto/dataobs/validation_task.proto\x12\x14tecton_proto.dataobs\x1a\x1fgoogle/protobuf/timestamp.proto\x1a\x1egoogle/protobuf/duration.proto\x1a\x1ctecton_proto/common/id.proto\x1a%tecton_proto/common/fco_locator.proto\x1a&tecton_proto/dataobs/expectation.proto\x1a%tecton_proto/dataobs/validation.proto\x1a\x31tecton_proto/dataobs/validation_task_params.proto\"\xc3\x05\n\x0eValidationTask\x12\x43\n\x11validation_job_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x0fvalidationJobId\x12S\n\x14\x66\x65\x61ture_view_locator\x18\x02 \x01(\x0b\x32!.tecton_proto.common.IdFcoLocatorR\x12\x66\x65\x61tureViewLocator\x12X\n\x13metric_expectations\x18\x03 \x03(\x0b\x32\'.tecton_proto.dataobs.MetricExpectationR\x12metricExpectations\x12H\n\x12\x66\x65\x61ture_start_time\x18\x04 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x10\x66\x65\x61tureStartTime\x12\x44\n\x10\x66\x65\x61ture_end_time\x18\x05 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x0e\x66\x65\x61tureEndTime\x12\x45\n\ttask_type\x18\x08 \x01(\x0e\x32(.tecton_proto.dataobs.ValidationTaskTypeR\x08taskType\x12\x33\n\x07timeout\x18\t \x01(\x0b\x32\x19.google.protobuf.DurationR\x07timeout\x12V\n\x12\x64ynamo_data_source\x18\x06 \x01(\x0b\x32&.tecton_proto.dataobs.DynamoDataSourceH\x00R\x10\x64ynamoDataSource\x12J\n\x0es3_data_source\x18\x07 \x01(\x0b\x32\".tecton_proto.dataobs.S3DataSourceH\x00R\x0cs3DataSourceB\r\n\x0b\x64\x61ta_source\"\x93\x03\n\x14ValidationTaskResult\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12\x45\n\x12\x66\x65\x61ture_package_id\x18\x02 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x10\x66\x65\x61turePackageId\x12\x43\n\x11validation_job_id\x18\x03 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x0fvalidationJobId\x12H\n\x12\x66\x65\x61ture_start_time\x18\x04 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x10\x66\x65\x61tureStartTime\x12\x44\n\x10\x66\x65\x61ture_end_time\x18\x05 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x0e\x66\x65\x61tureEndTime\x12\x41\n\x07results\x18\x06 \x03(\x0b\x32\'.tecton_proto.dataobs.ExpectationResultR\x07results*^\n\x12ValidationTaskType\x12 \n\x1cVALIDATION_TASK_TYPE_UNKNOWN\x10\x00\x12&\n\"VALIDATION_TASK_TYPE_BATCH_METRICS\x10\x01\x42\x16\n\x12\x63om.tecton.dataobsP\x01')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n*tecton_proto/dataobs/validation_task.proto\x12\x14tecton_proto.dataobs\x1a\x1fgoogle/protobuf/timestamp.proto\x1a\x1egoogle/protobuf/duration.proto\x1a\x1ctecton_proto/common/id.proto\x1a%tecton_proto/common/fco_locator.proto\x1a&tecton_proto/dataobs/expectation.proto\x1a%tecton_proto/dataobs/validation.proto\x1a\x31tecton_proto/dataobs/validation_task_params.proto\"\xc3\x05\n\x0eValidationTask\x12\x43\n\x11validation_job_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x0fvalidationJobId\x12S\n\x14\x66\x65\x61ture_view_locator\x18\x02 \x01(\x0b\x32!.tecton_proto.common.IdFcoLocatorR\x12\x66\x65\x61tureViewLocator\x12X\n\x13metric_expectations\x18\x03 \x03(\x0b\x32\'.tecton_proto.dataobs.MetricExpectationR\x12metricExpectations\x12H\n\x12\x66\x65\x61ture_start_time\x18\x04 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x10\x66\x65\x61tureStartTime\x12\x44\n\x10\x66\x65\x61ture_end_time\x18\x05 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x0e\x66\x65\x61tureEndTime\x12\x45\n\ttask_type\x18\x08 \x01(\x0e\x32(.tecton_proto.dataobs.ValidationTaskTypeR\x08taskType\x12\x33\n\x07timeout\x18\t \x01(\x0b\x32\x19.google.protobuf.DurationR\x07timeout\x12V\n\x12\x64ynamo_data_source\x18\x06 \x01(\x0b\x32&.tecton_proto.dataobs.DynamoDataSourceH\x00R\x10\x64ynamoDataSource\x12J\n\x0es3_data_source\x18\x07 \x01(\x0b\x32\".tecton_proto.dataobs.S3DataSourceH\x00R\x0cs3DataSourceB\r\n\x0b\x64\x61ta_source\"\x9f\x04\n\x14ValidationTaskResult\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12\x45\n\x12\x66\x65\x61ture_package_id\x18\x02 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x10\x66\x65\x61turePackageId\x12\x43\n\x11validation_job_id\x18\x03 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x0fvalidationJobId\x12H\n\x12\x66\x65\x61ture_start_time\x18\x04 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x10\x66\x65\x61tureStartTime\x12\x44\n\x10\x66\x65\x61ture_end_time\x18\x05 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x0e\x66\x65\x61tureEndTime\x12\x41\n\x07results\x18\x06 \x03(\x0b\x32\'.tecton_proto.dataobs.ExpectationResultR\x07results\x12\x45\n\x07metrics\x18\x07 \x01(\x0b\x32+.tecton_proto.dataobs.ValidationTaskMetricsR\x07metrics\x12\x43\n\x0fvalidation_time\x18\x08 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x0evalidationTime\"\xbc\x01\n\x15ValidationTaskMetrics\x12(\n\x10metric_rows_read\x18\x01 \x01(\rR\x0emetricRowsRead\x12*\n\x11\x66\x65\x61ture_rows_read\x18\x02 \x01(\rR\x0f\x66\x65\x61tureRowsRead\x12M\n\x15query_execution_times\x18\x03 \x03(\x0b\x32\x19.google.protobuf.DurationR\x13queryExecutionTimes*^\n\x12ValidationTaskType\x12 \n\x1cVALIDATION_TASK_TYPE_UNKNOWN\x10\x00\x12&\n\"VALIDATION_TASK_TYPE_BATCH_METRICS\x10\x01\x42\x16\n\x12\x63om.tecton.dataobsP\x01')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.dataobs.validation_task_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\022com.tecton.dataobsP\001'
-  _VALIDATIONTASKTYPE._serialized_start=1448
-  _VALIDATIONTASKTYPE._serialized_end=1542
+  _VALIDATIONTASKTYPE._serialized_start=1779
+  _VALIDATIONTASKTYPE._serialized_end=1873
   _VALIDATIONTASK._serialized_start=333
   _VALIDATIONTASK._serialized_end=1040
   _VALIDATIONTASKRESULT._serialized_start=1043
-  _VALIDATIONTASKRESULT._serialized_end=1446
+  _VALIDATIONTASKRESULT._serialized_end=1586
+  _VALIDATIONTASKMETRICS._serialized_start=1589
+  _VALIDATIONTASKMETRICS._serialized_end=1777
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/feature_server/configuration/feature_server_configuration_pb2.py` & `tecton-0.7.0rc0/tecton_proto/feature_server/configuration/feature_server_configuration_pb2.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,59 +12,61 @@
 
 
 from google.protobuf import duration_pb2 as google_dot_protobuf_dot_duration__pb2
 from google.protobuf import timestamp_pb2 as google_dot_protobuf_dot_timestamp__pb2
 from tecton_proto.args import pipeline_pb2 as tecton__proto_dot_args_dot_pipeline__pb2
 from tecton_proto.auth import acl_pb2 as tecton__proto_dot_auth_dot_acl__pb2
 from tecton_proto.common import aggregation_function_pb2 as tecton__proto_dot_common_dot_aggregation__function__pb2
-from tecton_proto.common import column_type_pb2 as tecton__proto_dot_common_dot_column__type__pb2
 from tecton_proto.common import id_pb2 as tecton__proto_dot_common_dot_id__pb2
 from tecton_proto.data import feature_service_pb2 as tecton__proto_dot_data_dot_feature__service__pb2
 from tecton_proto.data import feature_view_pb2 as tecton__proto_dot_data_dot_feature__view__pb2
 from tecton_proto.data import tecton_api_key_pb2 as tecton__proto_dot_data_dot_tecton__api__key__pb2
 from tecton_proto.data import transformation_pb2 as tecton__proto_dot_data_dot_transformation__pb2
 from tecton_proto.common import data_type_pb2 as tecton__proto_dot_common_dot_data__type__pb2
+from tecton_proto.data import odfv_compute_pb2 as tecton__proto_dot_data_dot_odfv__compute__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\nLtecton_proto/feature_server/configuration/feature_server_configuration.proto\x12)tecton_proto.feature_server.configuration\x1a\x1egoogle/protobuf/duration.proto\x1a\x1fgoogle/protobuf/timestamp.proto\x1a tecton_proto/args/pipeline.proto\x1a\x1btecton_proto/auth/acl.proto\x1a.tecton_proto/common/aggregation_function.proto\x1a%tecton_proto/common/column_type.proto\x1a\x1ctecton_proto/common/id.proto\x1a\'tecton_proto/data/feature_service.proto\x1a$tecton_proto/data/feature_view.proto\x1a&tecton_proto/data/tecton_api_key.proto\x1a&tecton_proto/data/transformation.proto\x1a#tecton_proto/common/data_type.proto\"\xeb\x01\n\x0c\x46\x65\x61turesPlan\x12[\n\x0c\x66\x65\x61ture_plan\x18\x01 \x01(\x0b\x32\x36.tecton_proto.feature_server.configuration.FeaturePlanH\x00R\x0b\x66\x65\x61turePlan\x12x\n\x17on_demand_features_plan\x18\x02 \x01(\x0b\x32?.tecton_proto.feature_server.configuration.OnDemandFeaturesPlanH\x00R\x14onDemandFeaturesPlanB\x04\n\x02\x66p\"\xec\x01\n\x06\x43olumn\x12:\n\tdata_type\x18\x05 \x01(\x0b\x32\x1d.tecton_proto.common.DataTypeR\x08\x64\x61taType\x12\x35\n\x17\x66\x65\x61ture_view_space_name\x18\x02 \x01(\tR\x14\x66\x65\x61tureViewSpaceName\x12;\n\x1a\x66\x65\x61ture_service_space_name\x18\x03 \x01(\tR\x17\x66\x65\x61tureServiceSpaceName\x12,\n\x12\x66\x65\x61ture_view_index\x18\x04 \x01(\x05R\x10\x66\x65\x61tureViewIndexJ\x04\x08\x01\x10\x02\"\xab\x0b\n\x0b\x46\x65\x61turePlan\x12V\n\routput_column\x18\x04 \x01(\x0b\x32\x31.tecton_proto.feature_server.configuration.ColumnR\x0coutputColumn\x12V\n\rinput_columns\x18\x01 \x03(\x0b\x32\x31.tecton_proto.feature_server.configuration.ColumnR\x0cinputColumns\x12[\n\x14\x61ggregation_function\x18\x03 \x01(\x0e\x32(.tecton_proto.common.AggregationFunctionR\x13\x61ggregationFunction\x12n\n\x1b\x61ggregation_function_params\x18\x06 \x01(\x0b\x32..tecton_proto.common.AggregationFunctionParamsR\x19\x61ggregationFunctionParams\x12H\n\x12\x61ggregation_window\x18\x05 \x01(\x0b\x32\x19.google.protobuf.DurationR\x11\x61ggregationWindow\x12N\n\tjoin_keys\x18\x07 \x03(\x0b\x32\x31.tecton_proto.feature_server.configuration.ColumnR\x08joinKeys\x12_\n\x12wildcard_join_keys\x18\x08 \x03(\x0b\x32\x31.tecton_proto.feature_server.configuration.ColumnR\x10wildcardJoinKeys\x12\x1d\n\ntable_name\x18\t \x01(\tR\ttableName\x12|\n\x19\x64\x61ta_table_timestamp_type\x18\x0f \x01(\x0e\x32\x41.tecton_proto.feature_server.configuration.DataTableTimestampTypeR\x16\x64\x61taTableTimestampType\x12\x82\x01\n\x1bstatus_table_timestamp_type\x18\x10 \x01(\x0e\x32\x43.tecton_proto.feature_server.configuration.StatusTableTimestampTypeR\x18statusTableTimestampType\x12#\n\rtimestamp_key\x18\x0c \x01(\tR\x0ctimestampKey\x12<\n\x0cslide_period\x18\r \x01(\x0b\x32\x19.google.protobuf.DurationR\x0bslidePeriod\x12:\n\x0bserving_ttl\x18\x0e \x01(\x0b\x32\x19.google.protobuf.DurationR\nservingTtl\x12\x30\n\x14refresh_status_table\x18\x11 \x01(\x08R\x12refreshStatusTable\x12*\n\x11\x66\x65\x61ture_view_name\x18\x15 \x01(\tR\x0f\x66\x65\x61tureViewName\x12&\n\x0f\x66\x65\x61ture_view_id\x18\x17 \x01(\tR\rfeatureViewId\x12?\n\x1c\x66\x65\x61ture_store_format_version\x18\x12 \x01(\x05R\x19\x66\x65\x61tureStoreFormatVersion\x12T\n\x13online_store_params\x18\x13 \x01(\x0b\x32$.tecton_proto.data.OnlineStoreParamsR\x11onlineStoreParams\x12.\n\x12\x64\x65letionTimeWindow\x18\x16 \x01(\x03R\x12\x64\x65letionTimeWindowJ\x04\x08\x02\x10\x03J\x04\x08\n\x10\x0bJ\x04\x08\x0b\x10\x0cJ\x04\x08\x14\x10\x15\"g\n\x11\x46\x65\x61tureVectorPlan\x12R\n\x08\x66\x65\x61tures\x18\x01 \x03(\x0b\x32\x36.tecton_proto.feature_server.configuration.FeaturePlanR\x08\x66\x65\x61tures\"\xc1\x05\n\x14OnDemandFeaturesPlan\x12l\n\x19\x61rgs_from_request_context\x18\x02 \x03(\x0b\x32\x31.tecton_proto.feature_server.configuration.ColumnR\x16\x61rgsFromRequestContext\x12K\n\x07outputs\x18\x03 \x03(\x0b\x32\x31.tecton_proto.feature_server.configuration.ColumnR\x07outputs\x12\x83\x01\n\x12\x66\x65\x61ture_set_inputs\x18\x05 \x03(\x0b\x32U.tecton_proto.feature_server.configuration.OnDemandFeaturesPlan.FeatureSetInputsEntryR\x10\x66\x65\x61tureSetInputs\x12\x37\n\x08pipeline\x18\x06 \x01(\x0b\x32\x1b.tecton_proto.args.PipelineR\x08pipeline\x12K\n\x0ftransformations\x18\x07 \x03(\x0b\x32!.tecton_proto.data.TransformationR\x0ftransformations\x12*\n\x11\x66\x65\x61ture_view_name\x18\x08 \x01(\tR\x0f\x66\x65\x61tureViewName\x12&\n\x0f\x66\x65\x61ture_view_id\x18\t \x01(\tR\rfeatureViewId\x1a\x81\x01\n\x15\x46\x65\x61tureSetInputsEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12R\n\x05value\x18\x02 \x01(\x0b\x32<.tecton_proto.feature_server.configuration.FeatureVectorPlanR\x05value:\x02\x38\x01J\x04\x08\x01\x10\x02J\x04\x08\x04\x10\x05\"\x81\x01\n\rLoggingConfig\x12\x1f\n\x0bsample_rate\x18\x01 \x01(\x02R\nsampleRate\x12.\n\x13log_effective_times\x18\x02 \x01(\x08R\x11logEffectiveTimes\x12\x1f\n\x0b\x61vro_schema\x18\x03 \x01(\tR\navroSchema\"\x8d\x05\n\x12\x46\x65\x61tureServicePlan\x12\x45\n\x12\x66\x65\x61ture_service_id\x18\x04 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x10\x66\x65\x61tureServiceId\x12?\n\x0f\x66\x65\x61ture_view_id\x18\n \x01(\x0b\x32\x17.tecton_proto.common.IdR\rfeatureViewId\x12\x30\n\x14\x66\x65\x61ture_service_name\x18\x02 \x01(\tR\x12\x66\x65\x61tureServiceName\x12*\n\x11\x66\x65\x61ture_view_name\x18\t \x01(\tR\x0f\x66\x65\x61tureViewName\x12%\n\x0eworkspace_name\x18\x08 \x01(\tR\rworkspaceName\x12^\n\x0e\x66\x65\x61tures_plans\x18\r \x03(\x0b\x32\x37.tecton_proto.feature_server.configuration.FeaturesPlanR\rfeaturesPlans\x12N\n\x11join_key_template\x18\x05 \x01(\x0b\x32\".tecton_proto.data.JoinKeyTemplateR\x0fjoinKeyTemplate\x12\x41\n\x10\x66\x65\x61ture_view_ids\x18\x06 \x03(\x0b\x32\x17.tecton_proto.common.IdR\x0e\x66\x65\x61tureViewIds\x12_\n\x0elogging_config\x18\x0c \x01(\x0b\x32\x38.tecton_proto.feature_server.configuration.LoggingConfigR\rloggingConfigJ\x04\x08\x01\x10\x02J\x04\x08\x03\x10\x04J\x04\x08\x07\x10\x08J\x04\x08\x0b\x10\x0c\"\x80\x04\n\x11GlobalTableConfig\x12?\n\x0f\x66\x65\x61ture_view_id\x18\x04 \x01(\x0b\x32\x17.tecton_proto.common.IdR\rfeatureViewId\x12<\n\x0cslide_period\x18\x02 \x01(\x0b\x32\x19.google.protobuf.DurationR\x0bslidePeriod\x12\x12\n\x04size\x18\x03 \x01(\x05R\x04size\x12\x82\x01\n\x1bstatus_table_timestamp_type\x18\x06 \x01(\x0e\x32\x43.tecton_proto.feature_server.configuration.StatusTableTimestampTypeR\x18statusTableTimestampType\x12\x30\n\x14refresh_status_table\x18\x07 \x01(\x08R\x12refreshStatusTable\x12?\n\x1c\x66\x65\x61ture_store_format_version\x18\x08 \x01(\x05R\x19\x66\x65\x61tureStoreFormatVersion\x12T\n\x13online_store_params\x18\t \x01(\x0b\x32$.tecton_proto.data.OnlineStoreParamsR\x11onlineStoreParamsJ\x04\x08\x01\x10\x02J\x04\x08\x05\x10\x06\"\x87\x01\n\x12\x46\x65\x61tureServiceAcls\x12\x45\n\x12\x66\x65\x61ture_service_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x10\x66\x65\x61tureServiceId\x12*\n\x04\x61\x63ls\x18\x02 \x03(\x0b\x32\x16.tecton_proto.auth.AclR\x04\x61\x63ls\"b\n\rWorkspaceAcls\x12%\n\x0eworkspace_name\x18\x01 \x01(\tR\rworkspaceName\x12*\n\x04\x61\x63ls\x18\x02 \x03(\x0b\x32\x16.tecton_proto.auth.AclR\x04\x61\x63ls\"\x98\x06\n\x1a\x46\x65\x61tureServerConfiguration\x12?\n\rcomputed_time\x18\x02 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x0c\x63omputedTime\x12h\n\x10\x66\x65\x61ture_services\x18\x01 \x03(\x0b\x32=.tecton_proto.feature_server.configuration.FeatureServicePlanR\x0f\x66\x65\x61tureServices\x12\xa0\x01\n\x1bglobal_table_config_by_name\x18\x03 \x03(\x0b\x32\x62.tecton_proto.feature_server.configuration.FeatureServerConfiguration.GlobalTableConfigByNameEntryR\x17globalTableConfigByName\x12O\n\x13\x61uthorized_api_keys\x18\x04 \x03(\x0b\x32\x1f.tecton_proto.data.TectonApiKeyR\x11\x61uthorizedApiKeys\x12o\n\x14\x66\x65\x61ture_service_acls\x18\x05 \x03(\x0b\x32=.tecton_proto.feature_server.configuration.FeatureServiceAclsR\x12\x66\x65\x61tureServiceAcls\x12_\n\x0eworkspace_acls\x18\x06 \x03(\x0b\x32\x38.tecton_proto.feature_server.configuration.WorkspaceAclsR\rworkspaceAcls\x1a\x88\x01\n\x1cGlobalTableConfigByNameEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12R\n\x05value\x18\x02 \x01(\x0b\x32<.tecton_proto.feature_server.configuration.GlobalTableConfigR\x05value:\x02\x38\x01*\x90\x01\n\x16\x44\x61taTableTimestampType\x12%\n!DATA_TABLE_TIMESTAMP_TYPE_UNKNOWN\x10\x00\x12&\n\"DATA_TABLE_TIMESTAMP_TYPE_SORT_KEY\x10\x01\x12\'\n#DATA_TABLE_TIMESTAMP_TYPE_ATTRIBUTE\x10\x02*\x98\x01\n\x18StatusTableTimestampType\x12\'\n#STATUS_TABLE_TIMESTAMP_TYPE_UNKNOWN\x10\x00\x12(\n$STATUS_TABLE_TIMESTAMP_TYPE_SORT_KEY\x10\x01\x12)\n%STATUS_TABLE_TIMESTAMP_TYPE_ATTRIBUTE\x10\x02\x42l\n(com.tecton.feature_service.configurationP\x01Z>github.com/tecton-ai/tecton_proto/feature_server/configuration')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\nLtecton_proto/feature_server/configuration/feature_server_configuration.proto\x12)tecton_proto.feature_server.configuration\x1a\x1egoogle/protobuf/duration.proto\x1a\x1fgoogle/protobuf/timestamp.proto\x1a tecton_proto/args/pipeline.proto\x1a\x1btecton_proto/auth/acl.proto\x1a.tecton_proto/common/aggregation_function.proto\x1a\x1ctecton_proto/common/id.proto\x1a\'tecton_proto/data/feature_service.proto\x1a$tecton_proto/data/feature_view.proto\x1a&tecton_proto/data/tecton_api_key.proto\x1a&tecton_proto/data/transformation.proto\x1a#tecton_proto/common/data_type.proto\x1a$tecton_proto/data/odfv_compute.proto\"\xeb\x01\n\x0c\x46\x65\x61turesPlan\x12[\n\x0c\x66\x65\x61ture_plan\x18\x01 \x01(\x0b\x32\x36.tecton_proto.feature_server.configuration.FeaturePlanH\x00R\x0b\x66\x65\x61turePlan\x12x\n\x17on_demand_features_plan\x18\x02 \x01(\x0b\x32?.tecton_proto.feature_server.configuration.OnDemandFeaturesPlanH\x00R\x14onDemandFeaturesPlanB\x04\n\x02\x66p\"\xec\x01\n\x06\x43olumn\x12:\n\tdata_type\x18\x05 \x01(\x0b\x32\x1d.tecton_proto.common.DataTypeR\x08\x64\x61taType\x12\x35\n\x17\x66\x65\x61ture_view_space_name\x18\x02 \x01(\tR\x14\x66\x65\x61tureViewSpaceName\x12;\n\x1a\x66\x65\x61ture_service_space_name\x18\x03 \x01(\tR\x17\x66\x65\x61tureServiceSpaceName\x12,\n\x12\x66\x65\x61ture_view_index\x18\x04 \x01(\x05R\x10\x66\x65\x61tureViewIndexJ\x04\x08\x01\x10\x02\"\xab\x0b\n\x0b\x46\x65\x61turePlan\x12V\n\routput_column\x18\x04 \x01(\x0b\x32\x31.tecton_proto.feature_server.configuration.ColumnR\x0coutputColumn\x12V\n\rinput_columns\x18\x01 \x03(\x0b\x32\x31.tecton_proto.feature_server.configuration.ColumnR\x0cinputColumns\x12[\n\x14\x61ggregation_function\x18\x03 \x01(\x0e\x32(.tecton_proto.common.AggregationFunctionR\x13\x61ggregationFunction\x12n\n\x1b\x61ggregation_function_params\x18\x06 \x01(\x0b\x32..tecton_proto.common.AggregationFunctionParamsR\x19\x61ggregationFunctionParams\x12H\n\x12\x61ggregation_window\x18\x05 \x01(\x0b\x32\x19.google.protobuf.DurationR\x11\x61ggregationWindow\x12N\n\tjoin_keys\x18\x07 \x03(\x0b\x32\x31.tecton_proto.feature_server.configuration.ColumnR\x08joinKeys\x12_\n\x12wildcard_join_keys\x18\x08 \x03(\x0b\x32\x31.tecton_proto.feature_server.configuration.ColumnR\x10wildcardJoinKeys\x12\x1d\n\ntable_name\x18\t \x01(\tR\ttableName\x12|\n\x19\x64\x61ta_table_timestamp_type\x18\x0f \x01(\x0e\x32\x41.tecton_proto.feature_server.configuration.DataTableTimestampTypeR\x16\x64\x61taTableTimestampType\x12\x82\x01\n\x1bstatus_table_timestamp_type\x18\x10 \x01(\x0e\x32\x43.tecton_proto.feature_server.configuration.StatusTableTimestampTypeR\x18statusTableTimestampType\x12#\n\rtimestamp_key\x18\x0c \x01(\tR\x0ctimestampKey\x12<\n\x0cslide_period\x18\r \x01(\x0b\x32\x19.google.protobuf.DurationR\x0bslidePeriod\x12:\n\x0bserving_ttl\x18\x0e \x01(\x0b\x32\x19.google.protobuf.DurationR\nservingTtl\x12\x30\n\x14refresh_status_table\x18\x11 \x01(\x08R\x12refreshStatusTable\x12*\n\x11\x66\x65\x61ture_view_name\x18\x15 \x01(\tR\x0f\x66\x65\x61tureViewName\x12&\n\x0f\x66\x65\x61ture_view_id\x18\x17 \x01(\tR\rfeatureViewId\x12?\n\x1c\x66\x65\x61ture_store_format_version\x18\x12 \x01(\x05R\x19\x66\x65\x61tureStoreFormatVersion\x12T\n\x13online_store_params\x18\x13 \x01(\x0b\x32$.tecton_proto.data.OnlineStoreParamsR\x11onlineStoreParams\x12.\n\x12\x64\x65letionTimeWindow\x18\x16 \x01(\x03R\x12\x64\x65letionTimeWindowJ\x04\x08\x02\x10\x03J\x04\x08\n\x10\x0bJ\x04\x08\x0b\x10\x0cJ\x04\x08\x14\x10\x15\"g\n\x11\x46\x65\x61tureVectorPlan\x12R\n\x08\x66\x65\x61tures\x18\x01 \x03(\x0b\x32\x36.tecton_proto.feature_server.configuration.FeaturePlanR\x08\x66\x65\x61tures\"\xc1\x05\n\x14OnDemandFeaturesPlan\x12l\n\x19\x61rgs_from_request_context\x18\x02 \x03(\x0b\x32\x31.tecton_proto.feature_server.configuration.ColumnR\x16\x61rgsFromRequestContext\x12K\n\x07outputs\x18\x03 \x03(\x0b\x32\x31.tecton_proto.feature_server.configuration.ColumnR\x07outputs\x12\x83\x01\n\x12\x66\x65\x61ture_set_inputs\x18\x05 \x03(\x0b\x32U.tecton_proto.feature_server.configuration.OnDemandFeaturesPlan.FeatureSetInputsEntryR\x10\x66\x65\x61tureSetInputs\x12\x37\n\x08pipeline\x18\x06 \x01(\x0b\x32\x1b.tecton_proto.args.PipelineR\x08pipeline\x12K\n\x0ftransformations\x18\x07 \x03(\x0b\x32!.tecton_proto.data.TransformationR\x0ftransformations\x12*\n\x11\x66\x65\x61ture_view_name\x18\x08 \x01(\tR\x0f\x66\x65\x61tureViewName\x12&\n\x0f\x66\x65\x61ture_view_id\x18\t \x01(\tR\rfeatureViewId\x1a\x81\x01\n\x15\x46\x65\x61tureSetInputsEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12R\n\x05value\x18\x02 \x01(\x0b\x32<.tecton_proto.feature_server.configuration.FeatureVectorPlanR\x05value:\x02\x38\x01J\x04\x08\x01\x10\x02J\x04\x08\x04\x10\x05\"\x81\x01\n\rLoggingConfig\x12\x1f\n\x0bsample_rate\x18\x01 \x01(\x02R\nsampleRate\x12.\n\x13log_effective_times\x18\x02 \x01(\x08R\x11logEffectiveTimes\x12\x1f\n\x0b\x61vro_schema\x18\x03 \x01(\tR\navroSchema\"\xe9\x05\n\x12\x46\x65\x61tureServicePlan\x12\x45\n\x12\x66\x65\x61ture_service_id\x18\x04 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x10\x66\x65\x61tureServiceId\x12?\n\x0f\x66\x65\x61ture_view_id\x18\n \x01(\x0b\x32\x17.tecton_proto.common.IdR\rfeatureViewId\x12\x30\n\x14\x66\x65\x61ture_service_name\x18\x02 \x01(\tR\x12\x66\x65\x61tureServiceName\x12*\n\x11\x66\x65\x61ture_view_name\x18\t \x01(\tR\x0f\x66\x65\x61tureViewName\x12%\n\x0eworkspace_name\x18\x08 \x01(\tR\rworkspaceName\x12^\n\x0e\x66\x65\x61tures_plans\x18\r \x03(\x0b\x32\x37.tecton_proto.feature_server.configuration.FeaturesPlanR\rfeaturesPlans\x12N\n\x11join_key_template\x18\x05 \x01(\x0b\x32\".tecton_proto.data.JoinKeyTemplateR\x0fjoinKeyTemplate\x12\x41\n\x10\x66\x65\x61ture_view_ids\x18\x06 \x03(\x0b\x32\x17.tecton_proto.common.IdR\x0e\x66\x65\x61tureViewIds\x12_\n\x0elogging_config\x18\x0c \x01(\x0b\x32\x38.tecton_proto.feature_server.configuration.LoggingConfigR\rloggingConfig\x12Z\n\x15on_demand_environment\x18\x0e \x01(\x0b\x32&.tecton_proto.data.OnlineComputeConfigR\x13onDemandEnvironmentJ\x04\x08\x01\x10\x02J\x04\x08\x03\x10\x04J\x04\x08\x07\x10\x08J\x04\x08\x0b\x10\x0c\"\x80\x04\n\x11GlobalTableConfig\x12?\n\x0f\x66\x65\x61ture_view_id\x18\x04 \x01(\x0b\x32\x17.tecton_proto.common.IdR\rfeatureViewId\x12<\n\x0cslide_period\x18\x02 \x01(\x0b\x32\x19.google.protobuf.DurationR\x0bslidePeriod\x12\x12\n\x04size\x18\x03 \x01(\x05R\x04size\x12\x82\x01\n\x1bstatus_table_timestamp_type\x18\x06 \x01(\x0e\x32\x43.tecton_proto.feature_server.configuration.StatusTableTimestampTypeR\x18statusTableTimestampType\x12\x30\n\x14refresh_status_table\x18\x07 \x01(\x08R\x12refreshStatusTable\x12?\n\x1c\x66\x65\x61ture_store_format_version\x18\x08 \x01(\x05R\x19\x66\x65\x61tureStoreFormatVersion\x12T\n\x13online_store_params\x18\t \x01(\x0b\x32$.tecton_proto.data.OnlineStoreParamsR\x11onlineStoreParamsJ\x04\x08\x01\x10\x02J\x04\x08\x05\x10\x06\"\x87\x01\n\x12\x46\x65\x61tureServiceAcls\x12\x45\n\x12\x66\x65\x61ture_service_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x10\x66\x65\x61tureServiceId\x12*\n\x04\x61\x63ls\x18\x02 \x03(\x0b\x32\x16.tecton_proto.auth.AclR\x04\x61\x63ls\"b\n\rWorkspaceAcls\x12%\n\x0eworkspace_name\x18\x01 \x01(\tR\rworkspaceName\x12*\n\x04\x61\x63ls\x18\x02 \x03(\x0b\x32\x16.tecton_proto.auth.AclR\x04\x61\x63ls\"\xe1\x01\n\x0c\x43\x61naryConfig\x12\x37\n\x18\x66\x65\x61ture_server_canary_id\x18\x01 \x01(\tR\x15\x66\x65\x61tureServerCanaryId\x12\x42\n\x1e\x66\x65\x61ture_server_canary_pod_name\x18\x02 \x01(\tR\x1a\x66\x65\x61tureServerCanaryPodName\x12T\n\'feature_server_canary_follower_endpoint\x18\x03 \x01(\tR#featureServerCanaryFollowerEndpoint\"\xef\x07\n\x1a\x46\x65\x61tureServerConfiguration\x12?\n\rcomputed_time\x18\x02 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x0c\x63omputedTime\x12h\n\x10\x66\x65\x61ture_services\x18\x01 \x03(\x0b\x32=.tecton_proto.feature_server.configuration.FeatureServicePlanR\x0f\x66\x65\x61tureServices\x12\xa0\x01\n\x1bglobal_table_config_by_name\x18\x03 \x03(\x0b\x32\x62.tecton_proto.feature_server.configuration.FeatureServerConfiguration.GlobalTableConfigByNameEntryR\x17globalTableConfigByName\x12O\n\x13\x61uthorized_api_keys\x18\x04 \x03(\x0b\x32\x1f.tecton_proto.data.TectonApiKeyR\x11\x61uthorizedApiKeys\x12o\n\x14\x66\x65\x61ture_service_acls\x18\x05 \x03(\x0b\x32=.tecton_proto.feature_server.configuration.FeatureServiceAclsR\x12\x66\x65\x61tureServiceAcls\x12_\n\x0eworkspace_acls\x18\x06 \x03(\x0b\x32\x38.tecton_proto.feature_server.configuration.WorkspaceAclsR\rworkspaceAcls\x12[\n\x17\x61ll_online_store_params\x18\x07 \x03(\x0b\x32$.tecton_proto.data.OnlineStoreParamsR\x14\x61llOnlineStoreParams\x12x\n\x1c\x66\x65\x61ture_server_canary_config\x18\x08 \x01(\x0b\x32\x37.tecton_proto.feature_server.configuration.CanaryConfigR\x19\x66\x65\x61tureServerCanaryConfig\x1a\x88\x01\n\x1cGlobalTableConfigByNameEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12R\n\x05value\x18\x02 \x01(\x0b\x32<.tecton_proto.feature_server.configuration.GlobalTableConfigR\x05value:\x02\x38\x01*\x90\x01\n\x16\x44\x61taTableTimestampType\x12%\n!DATA_TABLE_TIMESTAMP_TYPE_UNKNOWN\x10\x00\x12&\n\"DATA_TABLE_TIMESTAMP_TYPE_SORT_KEY\x10\x01\x12\'\n#DATA_TABLE_TIMESTAMP_TYPE_ATTRIBUTE\x10\x02*\x98\x01\n\x18StatusTableTimestampType\x12\'\n#STATUS_TABLE_TIMESTAMP_TYPE_UNKNOWN\x10\x00\x12(\n$STATUS_TABLE_TIMESTAMP_TYPE_SORT_KEY\x10\x01\x12)\n%STATUS_TABLE_TIMESTAMP_TYPE_ATTRIBUTE\x10\x02\x42l\n(com.tecton.feature_service.configurationP\x01Z>github.com/tecton-ai/tecton_proto/feature_server/configuration')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.feature_server.configuration.feature_server_configuration_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n(com.tecton.feature_service.configurationP\001Z>github.com/tecton-ai/tecton_proto/feature_server/configuration'
   _ONDEMANDFEATURESPLAN_FEATURESETINPUTSENTRY._options = None
   _ONDEMANDFEATURESPLAN_FEATURESETINPUTSENTRY._serialized_options = b'8\001'
   _FEATURESERVERCONFIGURATION_GLOBALTABLECONFIGBYNAMEENTRY._options = None
   _FEATURESERVERCONFIGURATION_GLOBALTABLECONFIGBYNAMEENTRY._serialized_options = b'8\001'
-  _DATATABLETIMESTAMPTYPE._serialized_start=5645
-  _DATATABLETIMESTAMPTYPE._serialized_end=5789
-  _STATUSTABLETIMESTAMPTYPE._serialized_start=5792
-  _STATUSTABLETIMESTAMPTYPE._serialized_end=5944
-  _FEATURESPLAN._serialized_start=565
-  _FEATURESPLAN._serialized_end=800
-  _COLUMN._serialized_start=803
-  _COLUMN._serialized_end=1039
-  _FEATUREPLAN._serialized_start=1042
-  _FEATUREPLAN._serialized_end=2493
-  _FEATUREVECTORPLAN._serialized_start=2495
-  _FEATUREVECTORPLAN._serialized_end=2598
-  _ONDEMANDFEATURESPLAN._serialized_start=2601
-  _ONDEMANDFEATURESPLAN._serialized_end=3306
-  _ONDEMANDFEATURESPLAN_FEATURESETINPUTSENTRY._serialized_start=3165
-  _ONDEMANDFEATURESPLAN_FEATURESETINPUTSENTRY._serialized_end=3294
-  _LOGGINGCONFIG._serialized_start=3309
-  _LOGGINGCONFIG._serialized_end=3438
-  _FEATURESERVICEPLAN._serialized_start=3441
-  _FEATURESERVICEPLAN._serialized_end=4094
-  _GLOBALTABLECONFIG._serialized_start=4097
-  _GLOBALTABLECONFIG._serialized_end=4609
-  _FEATURESERVICEACLS._serialized_start=4612
-  _FEATURESERVICEACLS._serialized_end=4747
-  _WORKSPACEACLS._serialized_start=4749
-  _WORKSPACEACLS._serialized_end=4847
-  _FEATURESERVERCONFIGURATION._serialized_start=4850
-  _FEATURESERVERCONFIGURATION._serialized_end=5642
-  _FEATURESERVERCONFIGURATION_GLOBALTABLECONFIGBYNAMEENTRY._serialized_start=5506
-  _FEATURESERVERCONFIGURATION_GLOBALTABLECONFIGBYNAMEENTRY._serialized_end=5642
+  _DATATABLETIMESTAMPTYPE._serialized_start=6179
+  _DATATABLETIMESTAMPTYPE._serialized_end=6323
+  _STATUSTABLETIMESTAMPTYPE._serialized_start=6326
+  _STATUSTABLETIMESTAMPTYPE._serialized_end=6478
+  _FEATURESPLAN._serialized_start=564
+  _FEATURESPLAN._serialized_end=799
+  _COLUMN._serialized_start=802
+  _COLUMN._serialized_end=1038
+  _FEATUREPLAN._serialized_start=1041
+  _FEATUREPLAN._serialized_end=2492
+  _FEATUREVECTORPLAN._serialized_start=2494
+  _FEATUREVECTORPLAN._serialized_end=2597
+  _ONDEMANDFEATURESPLAN._serialized_start=2600
+  _ONDEMANDFEATURESPLAN._serialized_end=3305
+  _ONDEMANDFEATURESPLAN_FEATURESETINPUTSENTRY._serialized_start=3164
+  _ONDEMANDFEATURESPLAN_FEATURESETINPUTSENTRY._serialized_end=3293
+  _LOGGINGCONFIG._serialized_start=3308
+  _LOGGINGCONFIG._serialized_end=3437
+  _FEATURESERVICEPLAN._serialized_start=3440
+  _FEATURESERVICEPLAN._serialized_end=4185
+  _GLOBALTABLECONFIG._serialized_start=4188
+  _GLOBALTABLECONFIG._serialized_end=4700
+  _FEATURESERVICEACLS._serialized_start=4703
+  _FEATURESERVICEACLS._serialized_end=4838
+  _WORKSPACEACLS._serialized_start=4840
+  _WORKSPACEACLS._serialized_end=4938
+  _CANARYCONFIG._serialized_start=4941
+  _CANARYCONFIG._serialized_end=5166
+  _FEATURESERVERCONFIGURATION._serialized_start=5169
+  _FEATURESERVERCONFIGURATION._serialized_end=6176
+  _FEATURESERVERCONFIGURATION_GLOBALTABLECONFIGBYNAMEENTRY._serialized_start=6040
+  _FEATURESERVERCONFIGURATION_GLOBALTABLECONFIGBYNAMEENTRY._serialized_end=6176
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/materialization/job_metadata_pb2.py` & `tecton-0.7.0rc0/tecton_proto/materialization/job_metadata_pb2.py`

 * *Files 12% similar despite different names*

```diff
@@ -7,51 +7,56 @@
 from google.protobuf import descriptor_pool as _descriptor_pool
 from google.protobuf import symbol_database as _symbol_database
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
-from google.protobuf import timestamp_pb2 as google_dot_protobuf_dot_timestamp__pb2
 from google.protobuf import duration_pb2 as google_dot_protobuf_dot_duration__pb2
 from tecton_proto.spark_common import clusters_pb2 as tecton__proto_dot_spark__common_dot_clusters__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n/tecton_proto/materialization/job_metadata.proto\x12\x1ctecton_proto.materialization\x1a\x1fgoogle/protobuf/timestamp.proto\x1a\x1egoogle/protobuf/duration.proto\x1a(tecton_proto/spark_common/clusters.proto\"\xaf\x02\n\x0bJobMetadata\x12\x8a\x01\n\"online_store_copier_execution_info\x18\x01 \x01(\x0b\x32<.tecton_proto.materialization.OnlineStoreCopierExecutionInfoH\x00R\x1eonlineStoreCopierExecutionInfo\x12\x86\x01\n materialization_consumption_info\x18\x02 \x01(\x0b\x32<.tecton_proto.materialization.MaterializationConsumptionInfoR\x1ematerializationConsumptionInfoB\n\n\x08job_info\"?\n\x1eOnlineStoreCopierExecutionInfo\x12\x1d\n\nis_revoked\x18\x01 \x01(\x08R\tisRevoked\"\xfc\x02\n\x1eMaterializationConsumptionInfo\x12z\n\x19offline_store_consumption\x18\x01 \x01(\x0b\x32>.tecton_proto.materialization.OfflineStoreWriteConsumptionInfoR\x17offlineStoreConsumption\x12w\n\x18online_store_consumption\x18\x02 \x01(\x0b\x32=.tecton_proto.materialization.OnlineStoreWriteConsumptionInfoR\x16onlineStoreConsumption\x12\x65\n\x13\x63ompute_consumption\x18\x03 \x01(\x0b\x32\x34.tecton_proto.materialization.ComputeConsumptionInfoR\x12\x63omputeConsumption\"\xfc\x02\n OfflineStoreWriteConsumptionInfo\x12~\n\x10\x63onsumption_info\x18\x01 \x03(\x0b\x32S.tecton_proto.materialization.OfflineStoreWriteConsumptionInfo.ConsumptionInfoEntryR\x0f\x63onsumptionInfo\x12\\\n\x12offline_store_type\x18\x02 \x01(\x0e\x32..tecton_proto.materialization.OfflineStoreTypeR\x10offlineStoreType\x1az\n\x14\x43onsumptionInfoEntry\x12\x10\n\x03key\x18\x01 \x01(\x03R\x03key\x12L\n\x05value\x18\x02 \x01(\x0b\x32\x36.tecton_proto.materialization.OfflineConsumptionBucketR\x05value:\x02\x38\x01\"h\n\x18OfflineConsumptionBucket\x12!\n\x0crows_written\x18\x01 \x01(\x03R\x0browsWritten\x12)\n\x10\x66\x65\x61tures_written\x18\x02 \x01(\x03R\x0f\x66\x65\x61turesWritten\"\xf6\x02\n\x1fOnlineStoreWriteConsumptionInfo\x12}\n\x10\x63onsumption_info\x18\x01 \x03(\x0b\x32R.tecton_proto.materialization.OnlineStoreWriteConsumptionInfo.ConsumptionInfoEntryR\x0f\x63onsumptionInfo\x12Y\n\x11online_store_type\x18\x02 \x01(\x0e\x32-.tecton_proto.materialization.OnlineStoreTypeR\x0fonlineStoreType\x1ay\n\x14\x43onsumptionInfoEntry\x12\x10\n\x03key\x18\x01 \x01(\x03R\x03key\x12K\n\x05value\x18\x02 \x01(\x0b\x32\x35.tecton_proto.materialization.OnlineConsumptionBucketR\x05value:\x02\x38\x01\"\x8c\x01\n\x17OnlineConsumptionBucket\x12!\n\x0crows_written\x18\x01 \x01(\x03R\x0browsWritten\x12)\n\x10\x66\x65\x61tures_written\x18\x02 \x01(\x03R\x0f\x66\x65\x61turesWritten\x12#\n\rbytes_written\x18\x03 \x01(\x03R\x0c\x62ytesWritten\"\xa0\x01\n\x16\x43omputeConsumptionInfo\x12\x35\n\x08\x64uration\x18\x01 \x01(\x0b\x32\x19.google.protobuf.DurationR\x08\x64uration\x12O\n\rcompute_usage\x18\x02 \x03(\x0b\x32*.tecton_proto.materialization.ComputeUsageR\x0c\x63omputeUsage\"\xbb\x01\n\x0c\x43omputeUsage\x12_\n\x15instance_availability\x18\x01 \x01(\x0e\x32*.tecton_proto.spark_common.AwsAvailabilityR\x14instanceAvailability\x12#\n\rinstance_type\x18\x02 \x01(\tR\x0cinstanceType\x12%\n\x0einstance_count\x18\x03 \x01(\x03R\rinstanceCount*k\n\x0fOnlineStoreType\x12\x1d\n\x19ONLINE_STORE_TYPE_UNKNOWN\x10\x00\x12\x1c\n\x18ONLINE_STORE_TYPE_DYNAMO\x10\x01\x12\x1b\n\x17ONLINE_STORE_TYPE_REDIS\x10\x02*j\n\x10OfflineStoreType\x12\x1e\n\x1aOFFLINE_STORE_TYPE_UNKNOWN\x10\x00\x12\x19\n\x15OFFLINE_STORE_TYPE_S3\x10\x01\x12\x1b\n\x17OFFLINE_STORE_TYPE_DBFS\x10\x02\x42\x1e\n\x1a\x63om.tecton.materializationP\x01')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n/tecton_proto/materialization/job_metadata.proto\x12\x1ctecton_proto.materialization\x1a\x1egoogle/protobuf/duration.proto\x1a(tecton_proto/spark_common/clusters.proto\"\x98\x03\n\x0bJobMetadata\x12\x8a\x01\n\"online_store_copier_execution_info\x18\x01 \x01(\x0b\x32<.tecton_proto.materialization.OnlineStoreCopierExecutionInfoH\x00R\x1eonlineStoreCopierExecutionInfo\x12g\n\x14spark_execution_info\x18\x03 \x01(\x0b\x32\x33.tecton_proto.materialization.SparkJobExecutionInfoH\x00R\x12sparkExecutionInfo\x12\x86\x01\n materialization_consumption_info\x18\x02 \x01(\x0b\x32<.tecton_proto.materialization.MaterializationConsumptionInfoR\x1ematerializationConsumptionInfoB\n\n\x08job_info\"?\n\x1eOnlineStoreCopierExecutionInfo\x12\x1d\n\nis_revoked\x18\x01 \x01(\x08R\tisRevoked\"\xdd\x01\n\x15SparkJobExecutionInfo\x12\x15\n\x06run_id\x18\x01 \x01(\tR\x05runId\x12\x1d\n\nis_revoked\x18\x02 \x01(\x08R\tisRevoked\x12\x8d\x01\n#stream_handoff_synchronization_info\x18\x03 \x01(\x0b\x32>.tecton_proto.materialization.StreamHandoffSynchronizationInfoR streamHandoffSynchronizationInfo\"\x91\x02\n StreamHandoffSynchronizationInfo\x12.\n\x13new_cluster_started\x18\x01 \x01(\x08R\x11newClusterStarted\x12;\n\x1astream_query_start_allowed\x18\x02 \x01(\x08R\x17streamQueryStartAllowed\x12@\n\x1cquery_cancellation_requested\x18\x03 \x01(\x08R\x1aqueryCancellationRequested\x12>\n\x1bquery_cancellation_complete\x18\x04 \x01(\x08R\x19queryCancellationComplete\"\xfc\x02\n\x1eMaterializationConsumptionInfo\x12z\n\x19offline_store_consumption\x18\x01 \x01(\x0b\x32>.tecton_proto.materialization.OfflineStoreWriteConsumptionInfoR\x17offlineStoreConsumption\x12w\n\x18online_store_consumption\x18\x02 \x01(\x0b\x32=.tecton_proto.materialization.OnlineStoreWriteConsumptionInfoR\x16onlineStoreConsumption\x12\x65\n\x13\x63ompute_consumption\x18\x03 \x01(\x0b\x32\x34.tecton_proto.materialization.ComputeConsumptionInfoR\x12\x63omputeConsumption\"\xfc\x02\n OfflineStoreWriteConsumptionInfo\x12~\n\x10\x63onsumption_info\x18\x01 \x03(\x0b\x32S.tecton_proto.materialization.OfflineStoreWriteConsumptionInfo.ConsumptionInfoEntryR\x0f\x63onsumptionInfo\x12\\\n\x12offline_store_type\x18\x02 \x01(\x0e\x32..tecton_proto.materialization.OfflineStoreTypeR\x10offlineStoreType\x1az\n\x14\x43onsumptionInfoEntry\x12\x10\n\x03key\x18\x01 \x01(\x03R\x03key\x12L\n\x05value\x18\x02 \x01(\x0b\x32\x36.tecton_proto.materialization.OfflineConsumptionBucketR\x05value:\x02\x38\x01\"h\n\x18OfflineConsumptionBucket\x12!\n\x0crows_written\x18\x01 \x01(\x03R\x0browsWritten\x12)\n\x10\x66\x65\x61tures_written\x18\x02 \x01(\x03R\x0f\x66\x65\x61turesWritten\"\xf6\x02\n\x1fOnlineStoreWriteConsumptionInfo\x12}\n\x10\x63onsumption_info\x18\x01 \x03(\x0b\x32R.tecton_proto.materialization.OnlineStoreWriteConsumptionInfo.ConsumptionInfoEntryR\x0f\x63onsumptionInfo\x12Y\n\x11online_store_type\x18\x02 \x01(\x0e\x32-.tecton_proto.materialization.OnlineStoreTypeR\x0fonlineStoreType\x1ay\n\x14\x43onsumptionInfoEntry\x12\x10\n\x03key\x18\x01 \x01(\x03R\x03key\x12K\n\x05value\x18\x02 \x01(\x0b\x32\x35.tecton_proto.materialization.OnlineConsumptionBucketR\x05value:\x02\x38\x01\"\x8c\x01\n\x17OnlineConsumptionBucket\x12!\n\x0crows_written\x18\x01 \x01(\x03R\x0browsWritten\x12)\n\x10\x66\x65\x61tures_written\x18\x02 \x01(\x03R\x0f\x66\x65\x61turesWritten\x12#\n\rbytes_written\x18\x03 \x01(\x03R\x0c\x62ytesWritten\"\xa0\x01\n\x16\x43omputeConsumptionInfo\x12\x35\n\x08\x64uration\x18\x01 \x01(\x0b\x32\x19.google.protobuf.DurationR\x08\x64uration\x12O\n\rcompute_usage\x18\x02 \x03(\x0b\x32*.tecton_proto.materialization.ComputeUsageR\x0c\x63omputeUsage\"\xbb\x01\n\x0c\x43omputeUsage\x12_\n\x15instance_availability\x18\x01 \x01(\x0e\x32*.tecton_proto.spark_common.AwsAvailabilityR\x14instanceAvailability\x12#\n\rinstance_type\x18\x02 \x01(\tR\x0cinstanceType\x12%\n\x0einstance_count\x18\x03 \x01(\x03R\rinstanceCount*\x8b\x01\n\x0fOnlineStoreType\x12\x1d\n\x19ONLINE_STORE_TYPE_UNKNOWN\x10\x00\x12\x1c\n\x18ONLINE_STORE_TYPE_DYNAMO\x10\x01\x12\x1b\n\x17ONLINE_STORE_TYPE_REDIS\x10\x02\x12\x1e\n\x1aONLINE_STORE_TYPE_BIGTABLE\x10\x03*\x86\x01\n\x10OfflineStoreType\x12\x1e\n\x1aOFFLINE_STORE_TYPE_UNKNOWN\x10\x00\x12\x19\n\x15OFFLINE_STORE_TYPE_S3\x10\x01\x12\x1b\n\x17OFFLINE_STORE_TYPE_DBFS\x10\x02\x12\x1a\n\x16OFFLINE_STORE_TYPE_GCS\x10\x03*\x80\x01\n\x14JobMetadataTableType\x12#\n\x1fJOB_METADATA_TABLE_TYPE_UNKNOWN\x10\x00\x12\"\n\x1eJOB_METADATA_TABLE_TYPE_DYNAMO\x10\x01\x12\x1f\n\x1bJOB_METADATA_TABLE_TYPE_GCS\x10\x02\x42\x1e\n\x1a\x63om.tecton.materializationP\x01')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.materialization.job_metadata_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\032com.tecton.materializationP\001'
   _OFFLINESTOREWRITECONSUMPTIONINFO_CONSUMPTIONINFOENTRY._options = None
   _OFFLINESTOREWRITECONSUMPTIONINFO_CONSUMPTIONINFOENTRY._serialized_options = b'8\001'
   _ONLINESTOREWRITECONSUMPTIONINFO_CONSUMPTIONINFOENTRY._options = None
   _ONLINESTOREWRITECONSUMPTIONINFO_CONSUMPTIONINFOENTRY._serialized_options = b'8\001'
-  _ONLINESTORETYPE._serialized_start=2304
-  _ONLINESTORETYPE._serialized_end=2411
-  _OFFLINESTORETYPE._serialized_start=2413
-  _OFFLINESTORETYPE._serialized_end=2519
-  _JOBMETADATA._serialized_start=189
-  _JOBMETADATA._serialized_end=492
-  _ONLINESTORECOPIEREXECUTIONINFO._serialized_start=494
-  _ONLINESTORECOPIEREXECUTIONINFO._serialized_end=557
-  _MATERIALIZATIONCONSUMPTIONINFO._serialized_start=560
-  _MATERIALIZATIONCONSUMPTIONINFO._serialized_end=940
-  _OFFLINESTOREWRITECONSUMPTIONINFO._serialized_start=943
-  _OFFLINESTOREWRITECONSUMPTIONINFO._serialized_end=1323
-  _OFFLINESTOREWRITECONSUMPTIONINFO_CONSUMPTIONINFOENTRY._serialized_start=1201
-  _OFFLINESTOREWRITECONSUMPTIONINFO_CONSUMPTIONINFOENTRY._serialized_end=1323
-  _OFFLINECONSUMPTIONBUCKET._serialized_start=1325
-  _OFFLINECONSUMPTIONBUCKET._serialized_end=1429
-  _ONLINESTOREWRITECONSUMPTIONINFO._serialized_start=1432
-  _ONLINESTOREWRITECONSUMPTIONINFO._serialized_end=1806
-  _ONLINESTOREWRITECONSUMPTIONINFO_CONSUMPTIONINFOENTRY._serialized_start=1685
-  _ONLINESTOREWRITECONSUMPTIONINFO_CONSUMPTIONINFOENTRY._serialized_end=1806
-  _ONLINECONSUMPTIONBUCKET._serialized_start=1809
-  _ONLINECONSUMPTIONBUCKET._serialized_end=1949
-  _COMPUTECONSUMPTIONINFO._serialized_start=1952
-  _COMPUTECONSUMPTIONINFO._serialized_end=2112
-  _COMPUTEUSAGE._serialized_start=2115
-  _COMPUTEUSAGE._serialized_end=2302
+  _ONLINESTORETYPE._serialized_start=2877
+  _ONLINESTORETYPE._serialized_end=3016
+  _OFFLINESTORETYPE._serialized_start=3019
+  _OFFLINESTORETYPE._serialized_end=3153
+  _JOBMETADATATABLETYPE._serialized_start=3156
+  _JOBMETADATATABLETYPE._serialized_end=3284
+  _JOBMETADATA._serialized_start=156
+  _JOBMETADATA._serialized_end=564
+  _ONLINESTORECOPIEREXECUTIONINFO._serialized_start=566
+  _ONLINESTORECOPIEREXECUTIONINFO._serialized_end=629
+  _SPARKJOBEXECUTIONINFO._serialized_start=632
+  _SPARKJOBEXECUTIONINFO._serialized_end=853
+  _STREAMHANDOFFSYNCHRONIZATIONINFO._serialized_start=856
+  _STREAMHANDOFFSYNCHRONIZATIONINFO._serialized_end=1129
+  _MATERIALIZATIONCONSUMPTIONINFO._serialized_start=1132
+  _MATERIALIZATIONCONSUMPTIONINFO._serialized_end=1512
+  _OFFLINESTOREWRITECONSUMPTIONINFO._serialized_start=1515
+  _OFFLINESTOREWRITECONSUMPTIONINFO._serialized_end=1895
+  _OFFLINESTOREWRITECONSUMPTIONINFO_CONSUMPTIONINFOENTRY._serialized_start=1773
+  _OFFLINESTOREWRITECONSUMPTIONINFO_CONSUMPTIONINFOENTRY._serialized_end=1895
+  _OFFLINECONSUMPTIONBUCKET._serialized_start=1897
+  _OFFLINECONSUMPTIONBUCKET._serialized_end=2001
+  _ONLINESTOREWRITECONSUMPTIONINFO._serialized_start=2004
+  _ONLINESTOREWRITECONSUMPTIONINFO._serialized_end=2378
+  _ONLINESTOREWRITECONSUMPTIONINFO_CONSUMPTIONINFOENTRY._serialized_start=2257
+  _ONLINESTOREWRITECONSUMPTIONINFO_CONSUMPTIONINFOENTRY._serialized_end=2378
+  _ONLINECONSUMPTIONBUCKET._serialized_start=2381
+  _ONLINECONSUMPTIONBUCKET._serialized_end=2521
+  _COMPUTECONSUMPTIONINFO._serialized_start=2524
+  _COMPUTECONSUMPTIONINFO._serialized_end=2684
+  _COMPUTEUSAGE._serialized_start=2687
+  _COMPUTEUSAGE._serialized_end=2874
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/materialization/materialization_states_pb2.py` & `tecton-0.7.0rc0/tecton_proto/materialization/materialization_states_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/materialization/spark_cluster_pb2.py` & `tecton-0.7.0rc0/tecton_proto/data/remote_compute_environment_pb2.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,33 +1,32 @@
 # -*- coding: utf-8 -*-
 # Generated by the protocol buffer compiler.  DO NOT EDIT!
-# source: tecton_proto/materialization/spark_cluster.proto
+# source: tecton_proto/data/remote_compute_environment.proto
 """Generated protocol buffer code."""
 from google.protobuf.internal import builder as _builder
 from google.protobuf import descriptor as _descriptor
 from google.protobuf import descriptor_pool as _descriptor_pool
 from google.protobuf import symbol_database as _symbol_database
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
-from tecton_proto.spark_api import jobs_pb2 as tecton__proto_dot_spark__api_dot_jobs__pb2
+from google.protobuf import timestamp_pb2 as google_dot_protobuf_dot_timestamp__pb2
+from tecton_proto.common import container_image_pb2 as tecton__proto_dot_common_dot_container__image__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n0tecton_proto/materialization/spark_cluster.proto\x12\x1ctecton_proto.materialization\x1a!tecton_proto/spark_api/jobs.proto\"\xad\x02\n\x17SparkClusterEnvironment\x12I\n!spark_cluster_environment_version\x18\x01 \x01(\x05R\x1esparkClusterEnvironmentVersion\x12\x65\n\x15job_request_templates\x18\n \x01(\x0b\x32\x31.tecton_proto.materialization.JobRequestTemplatesR\x13jobRequestTemplates\x12T\n\'merged_user_deployment_settings_version\x18\x0c \x01(\x05R#mergedUserDeploymentSettingsVersionJ\x04\x08\x02\x10\nJ\x04\x08\x0b\x10\x0c\"\xc1\x01\n\x13JobRequestTemplates\x12X\n\x13\x64\x61tabricks_template\x18\x02 \x01(\x0b\x32\'.tecton_proto.spark_api.StartJobRequestR\x12\x64\x61tabricksTemplate\x12J\n\x0c\x65mr_template\x18\x03 \x01(\x0b\x32\'.tecton_proto.spark_api.StartJobRequestR\x0b\x65mrTemplateJ\x04\x08\x01\x10\x02*b\n\x08TaskType\x12\x0b\n\x07UNKNOWN\x10\x00\x12\t\n\x05\x42\x41TCH\x10\x01\x12\r\n\tSTREAMING\x10\x02\x12\n\n\x06INGEST\x10\x03\x12\x0c\n\x08\x44\x45LETION\x10\x04\x12\x15\n\x11\x44\x45LTA_MAINTENANCE\x10\x05*x\n\x14\x45xecutionEnvironment\x12\x13\n\x0f\x45NV_UNSPECIFIED\x10\x00\x12\x1b\n\x17\x45NV_DATABRICKS_NOTEBOOK\x10\x01\x12\x0b\n\x07\x45NV_EMR\x10\x02\x12\x11\n\rENV_SNOWFLAKE\x10\x03\x12\x0e\n\nENV_ATHENA\x10\x04\x42\x1e\n\x1a\x63om.tecton.materializationP\x01')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n2tecton_proto/data/remote_compute_environment.proto\x12\x11tecton_proto.data\x1a\x1fgoogle/protobuf/timestamp.proto\x1a)tecton_proto/common/container_image.proto\"\xc8\x03\n\x18RemoteComputeEnvironment\x12\x0e\n\x02id\x18\x01 \x01(\tR\x02id\x12\x12\n\x04name\x18\x02 \x01(\tR\x04name\x12\x38\n\x04type\x18\x03 \x01(\x0e\x32$.tecton_proto.data.RemoteComputeTypeR\x04type\x12\x42\n\x06status\x18\x04 \x01(\x0e\x32*.tecton_proto.data.RemoteEnvironmentStatusR\x06status\x12\x42\n\nimage_info\x18\x05 \x01(\x0b\x32#.tecton_proto.common.ContainerImageR\timageInfo\x12.\n\x13remote_function_uri\x18\x06 \x01(\tR\x11remoteFunctionUri\x12\x39\n\ncreated_at\x18\x07 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\tcreatedAt\x12\x39\n\nupdated_at\x18\x08 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\tupdatedAt\x12 \n\x0b\x64\x65scription\x18\t \x01(\tR\x0b\x64\x65scription*\x8a\x01\n\x17RemoteEnvironmentStatus\x12%\n!REMOTE_ENVIRONMENT_STATUS_PENDING\x10\x00\x12#\n\x1fREMOTE_ENVIRONMENT_STATUS_READY\x10\x01\x12#\n\x1fREMOTE_ENVIRONMENT_STATUS_ERROR\x10\x02*\x95\x01\n\x11RemoteComputeType\x12\x1c\n\x18REMOTE_COMPUTE_TYPE_CORE\x10\x00\x12 \n\x1cREMOTE_COMPUTE_TYPE_EXTENDED\x10\x01\x12 \n\x1cREMOTE_COMPUTE_TYPE_SNOWPARK\x10\x02\x12\x1e\n\x1aREMOTE_COMPUTE_TYPE_CUSTOM\x10\x03\x42\x13\n\x0f\x63om.tecton.dataP\x01')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
-_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.materialization.spark_cluster_pb2', globals())
+_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.data.remote_compute_environment_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
-  DESCRIPTOR._serialized_options = b'\n\032com.tecton.materializationP\001'
-  _TASKTYPE._serialized_start=617
-  _TASKTYPE._serialized_end=715
-  _EXECUTIONENVIRONMENT._serialized_start=717
-  _EXECUTIONENVIRONMENT._serialized_end=837
-  _SPARKCLUSTERENVIRONMENT._serialized_start=118
-  _SPARKCLUSTERENVIRONMENT._serialized_end=419
-  _JOBREQUESTTEMPLATES._serialized_start=422
-  _JOBREQUESTTEMPLATES._serialized_end=615
+  DESCRIPTOR._serialized_options = b'\n\017com.tecton.dataP\001'
+  _REMOTEENVIRONMENTSTATUS._serialized_start=609
+  _REMOTEENVIRONMENTSTATUS._serialized_end=747
+  _REMOTECOMPUTETYPE._serialized_start=750
+  _REMOTECOMPUTETYPE._serialized_end=899
+  _REMOTECOMPUTEENVIRONMENT._serialized_start=150
+  _REMOTECOMPUTEENVIRONMENT._serialized_end=606
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/materializationjobservice/materialization_job_service_pb2.py` & `tecton-0.7.0rc0/tecton_proto/materializationjobservice/materialization_job_service_pb2.py`

 * *Files 22% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 
 from google.api import annotations_pb2 as google_dot_api_dot_annotations__pb2
 from google.protobuf import descriptor_pb2 as google_dot_protobuf_dot_descriptor__pb2
 from google.protobuf import timestamp_pb2 as google_dot_protobuf_dot_timestamp__pb2
 from tecton_proto.auth import service_pb2 as tecton__proto_dot_auth_dot_service__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\nHtecton_proto/materializationjobservice/materialization_job_service.proto\x12&tecton_proto.materializationjobservice\x1a\x1cgoogle/api/annotations.proto\x1a google/protobuf/descriptor.proto\x1a\x1fgoogle/protobuf/timestamp.proto\x1a\x1ftecton_proto/auth/service.proto\"\xdb\x02\n\x19MaterializationJobRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12!\n\x0c\x66\x65\x61ture_view\x18\x02 \x01(\tR\x0b\x66\x65\x61tureView\x12\x39\n\nstart_time\x18\x03 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\tstartTime\x12\x35\n\x08\x65nd_time\x18\x04 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x07\x65ndTime\x12\x16\n\x06online\x18\x05 \x01(\x08R\x06online\x12\x18\n\x07offline\x18\x06 \x01(\x08R\x07offline\x12;\n\x1ause_tecton_managed_retries\x18\x07 \x01(\x08R\x17useTectonManagedRetries\x12\x1c\n\toverwrite\x18\x08 \x01(\x08R\toverwrite\"j\n\x1aMaterializationJobResponse\x12L\n\x03job\x18\x01 \x01(\x0b\x32:.tecton_proto.materializationjobservice.MaterializationJobR\x03job\"\xc4\x04\n\x12MaterializationJob\x12\x0e\n\x02id\x18\x01 \x01(\tR\x02id\x12\x1c\n\tworkspace\x18\x02 \x01(\tR\tworkspace\x12!\n\x0c\x66\x65\x61ture_view\x18\x03 \x01(\tR\x0b\x66\x65\x61tureView\x12\x39\n\nstart_time\x18\x04 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\tstartTime\x12\x35\n\x08\x65nd_time\x18\x05 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x07\x65ndTime\x12\x39\n\ncreated_at\x18\x06 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\tcreatedAt\x12\x39\n\nupdated_at\x18\x07 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\tupdatedAt\x12\x14\n\x05state\x18\t \x01(\tR\x05state\x12N\n\x08\x61ttempts\x18\n \x03(\x0b\x32\x32.tecton_proto.materializationjobservice.JobAttemptR\x08\x61ttempts\x12\x42\n\x0fnext_attempt_at\x18\x0b \x01(\x0b\x32\x1a.google.protobuf.TimestampR\rnextAttemptAt\x12\x16\n\x06online\x18\x0c \x01(\x08R\x06online\x12\x18\n\x07offline\x18\r \x01(\x08R\x07offline\x12\x19\n\x08job_type\x18\x0e \x01(\tR\x07jobType\"\xc1\x01\n\nJobAttempt\x12\x0e\n\x02id\x18\x01 \x01(\tR\x02id\x12\x39\n\ncreated_at\x18\x02 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\tcreatedAt\x12\x39\n\nupdated_at\x18\x03 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\tupdatedAt\x12\x14\n\x05state\x18\x04 \x01(\tR\x05state\x12\x17\n\x07run_url\x18\x05 \x01(\tR\x06runUrl\"R\n\x0fListJobsRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12!\n\x0c\x66\x65\x61ture_view\x18\x02 \x01(\tR\x0b\x66\x65\x61tureView\"b\n\x10ListJobsResponse\x12N\n\x04jobs\x18\x01 \x03(\x0b\x32:.tecton_proto.materializationjobservice.MaterializationJobR\x04jobs\"g\n\rGetJobRequest\x12\x15\n\x06job_id\x18\x01 \x01(\tR\x05jobId\x12\x1c\n\tworkspace\x18\x02 \x01(\tR\tworkspace\x12!\n\x0c\x66\x65\x61ture_view\x18\x03 \x01(\tR\x0b\x66\x65\x61tureView\"^\n\x0eGetJobResponse\x12L\n\x03job\x18\x01 \x01(\x0b\x32:.tecton_proto.materializationjobservice.MaterializationJobR\x03job\"j\n\x10\x43\x61ncelJobRequest\x12\x15\n\x06job_id\x18\x01 \x01(\tR\x05jobId\x12\x1c\n\tworkspace\x18\x02 \x01(\tR\tworkspace\x12!\n\x0c\x66\x65\x61ture_view\x18\x03 \x01(\tR\x0b\x66\x65\x61tureView\"a\n\x11\x43\x61ncelJobResponse\x12L\n\x03job\x18\x01 \x01(\x0b\x32:.tecton_proto.materializationjobservice.MaterializationJobR\x03job\"\x85\x01\n\x19GetLatestReadyTimeRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12!\n\x0c\x66\x65\x61ture_view\x18\x02 \x01(\tR\x0b\x66\x65\x61tureView\x12\'\n\x0f\x66\x65\x61ture_service\x18\x03 \x01(\tR\x0e\x66\x65\x61tureService\"\xc8\x01\n\x1aGetLatestReadyTimeResponse\x12S\n\x18online_latest_ready_time\x18\x01 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x15onlineLatestReadyTime\x12U\n\x19offline_latest_ready_time\x18\x02 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x16offlineLatestReadyTime2\xc6\t\n\x19MaterializationJobService\x12\x95\x02\n\x18SubmitMaterializationJob\x12\x41.tecton_proto.materializationjobservice.MaterializationJobRequest\x1a\x42.tecton_proto.materializationjobservice.MaterializationJobResponse\"r\x82\xd3\xe4\x93\x02(\"#/v1/jobs/submit-materialization-job:\x01*\xa2\xbc\xe6\xc0\x05>\x1a\r\x08\x01\x12\tworkspace*-tecton/workspace#update_materialization_state\x12\xfd\x01\n\x17ListMaterializationJobs\x12\x37.tecton_proto.materializationjobservice.ListJobsRequest\x1a\x38.tecton_proto.materializationjobservice.ListJobsResponse\"o\x82\xd3\xe4\x93\x02\'\"\"/v1/jobs/list-materialization-jobs:\x01*\xa2\xbc\xe6\xc0\x05<\x1a\r\x08\x01\x12\tworkspace*+tecton/workspace#read_materialization_state\x12\xbb\x01\n\x15GetMaterializationJob\x12\x35.tecton_proto.materializationjobservice.GetJobRequest\x1a\x36.tecton_proto.materializationjobservice.GetJobResponse\"3\x82\xd3\xe4\x93\x02%\" /v1/jobs/get-materialization-job:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xc7\x01\n\x18\x43\x61ncelMaterializationJob\x12\x38.tecton_proto.materializationjobservice.CancelJobRequest\x1a\x39.tecton_proto.materializationjobservice.CancelJobResponse\"6\x82\xd3\xe4\x93\x02(\"#/v1/jobs/cancel-materialization-job:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\x88\x02\n\x12GetLatestReadyTime\x12\x41.tecton_proto.materializationjobservice.GetLatestReadyTimeRequest\x1a\x42.tecton_proto.materializationjobservice.GetLatestReadyTimeResponse\"k\x82\xd3\xe4\x93\x02#\"\x1e/v1/jobs/get-latest-ready-time:\x01*\xa2\xbc\xe6\xc0\x05<\x1a\r\x08\x01\x12\tworkspace*+tecton/workspace#read_materialization_stateB\x85\x01\n$com.tecton.materializationjobserviceB\x1eMaterializationJobServiceProtoP\x01Z;github.com/tecton-ai/tecton_proto/materializationjobservice')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\nHtecton_proto/materializationjobservice/materialization_job_service.proto\x12&tecton_proto.materializationjobservice\x1a\x1cgoogle/api/annotations.proto\x1a google/protobuf/descriptor.proto\x1a\x1fgoogle/protobuf/timestamp.proto\x1a\x1ftecton_proto/auth/service.proto\"\xdb\x02\n\x19MaterializationJobRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12!\n\x0c\x66\x65\x61ture_view\x18\x02 \x01(\tR\x0b\x66\x65\x61tureView\x12\x39\n\nstart_time\x18\x03 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\tstartTime\x12\x35\n\x08\x65nd_time\x18\x04 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x07\x65ndTime\x12\x16\n\x06online\x18\x05 \x01(\x08R\x06online\x12\x18\n\x07offline\x18\x06 \x01(\x08R\x07offline\x12;\n\x1ause_tecton_managed_retries\x18\x07 \x01(\x08R\x17useTectonManagedRetries\x12\x1c\n\toverwrite\x18\x08 \x01(\x08R\toverwrite\"j\n\x1aMaterializationJobResponse\x12L\n\x03job\x18\x01 \x01(\x0b\x32:.tecton_proto.materializationjobservice.MaterializationJobR\x03job\"\xe5\x04\n\x12MaterializationJob\x12\x0e\n\x02id\x18\x01 \x01(\tR\x02id\x12\x1c\n\tworkspace\x18\x02 \x01(\tR\tworkspace\x12!\n\x0c\x66\x65\x61ture_view\x18\x03 \x01(\tR\x0b\x66\x65\x61tureView\x12\x39\n\nstart_time\x18\x04 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\tstartTime\x12\x35\n\x08\x65nd_time\x18\x05 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x07\x65ndTime\x12\x39\n\ncreated_at\x18\x06 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\tcreatedAt\x12\x39\n\nupdated_at\x18\x07 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\tupdatedAt\x12\x14\n\x05state\x18\t \x01(\tR\x05state\x12N\n\x08\x61ttempts\x18\n \x03(\x0b\x32\x32.tecton_proto.materializationjobservice.JobAttemptR\x08\x61ttempts\x12\x42\n\x0fnext_attempt_at\x18\x0b \x01(\x0b\x32\x1a.google.protobuf.TimestampR\rnextAttemptAt\x12\x16\n\x06online\x18\x0c \x01(\x08R\x06online\x12\x18\n\x07offline\x18\r \x01(\x08R\x07offline\x12\x19\n\x08job_type\x18\x0e \x01(\tR\x07jobType\x12\x1f\n\x0bingest_path\x18\x0f \x01(\tR\ningestPath\"\xc1\x01\n\nJobAttempt\x12\x0e\n\x02id\x18\x01 \x01(\tR\x02id\x12\x39\n\ncreated_at\x18\x02 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\tcreatedAt\x12\x39\n\nupdated_at\x18\x03 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\tupdatedAt\x12\x14\n\x05state\x18\x04 \x01(\tR\x05state\x12\x17\n\x07run_url\x18\x05 \x01(\tR\x06runUrl\"R\n\x0fListJobsRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12!\n\x0c\x66\x65\x61ture_view\x18\x02 \x01(\tR\x0b\x66\x65\x61tureView\"b\n\x10ListJobsResponse\x12N\n\x04jobs\x18\x01 \x03(\x0b\x32:.tecton_proto.materializationjobservice.MaterializationJobR\x04jobs\"g\n\rGetJobRequest\x12\x15\n\x06job_id\x18\x01 \x01(\tR\x05jobId\x12\x1c\n\tworkspace\x18\x02 \x01(\tR\tworkspace\x12!\n\x0c\x66\x65\x61ture_view\x18\x03 \x01(\tR\x0b\x66\x65\x61tureView\"^\n\x0eGetJobResponse\x12L\n\x03job\x18\x01 \x01(\x0b\x32:.tecton_proto.materializationjobservice.MaterializationJobR\x03job\"j\n\x10\x43\x61ncelJobRequest\x12\x15\n\x06job_id\x18\x01 \x01(\tR\x05jobId\x12\x1c\n\tworkspace\x18\x02 \x01(\tR\tworkspace\x12!\n\x0c\x66\x65\x61ture_view\x18\x03 \x01(\tR\x0b\x66\x65\x61tureView\"a\n\x11\x43\x61ncelJobResponse\x12L\n\x03job\x18\x01 \x01(\x0b\x32:.tecton_proto.materializationjobservice.MaterializationJobR\x03job\"\x85\x01\n\x19GetLatestReadyTimeRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12!\n\x0c\x66\x65\x61ture_view\x18\x02 \x01(\tR\x0b\x66\x65\x61tureView\x12\'\n\x0f\x66\x65\x61ture_service\x18\x03 \x01(\tR\x0e\x66\x65\x61tureService\"\xc8\x01\n\x1aGetLatestReadyTimeResponse\x12S\n\x18online_latest_ready_time\x18\x01 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x15onlineLatestReadyTime\x12U\n\x19offline_latest_ready_time\x18\x02 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x16offlineLatestReadyTime\"w\n+TestOnlyGetMaterializationTaskParamsRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12*\n\x11\x66\x65\x61ture_view_name\x18\x02 \x01(\tR\x0f\x66\x65\x61tureViewName\"t\n,TestOnlyGetMaterializationTaskParamsResponse\x12\x44\n\x1e\x65ncoded_materialization_params\x18\x01 \x01(\tR\x1c\x65ncodedMaterializationParams\"Z\n\x17GetDataframeInfoRequest\x12!\n\x0c\x66\x65\x61ture_view\x18\x01 \x01(\tR\x0b\x66\x65\x61tureView\x12\x1c\n\tworkspace\x18\x02 \x01(\tR\tworkspace\"k\n\x18GetDataframeInfoResponse\x12\x17\n\x07\x64\x66_path\x18\x01 \x01(\tR\x06\x64\x66Path\x12\x36\n\x18signed_url_for_df_upload\x18\x02 \x01(\tR\x14signedUrlForDfUpload\"\xb5\x01\n\x1cIngestDataframeFromS3Request\x12!\n\x0c\x66\x65\x61ture_view\x18\x01 \x01(\tR\x0b\x66\x65\x61tureView\x12\x17\n\x07\x64\x66_path\x18\x02 \x01(\tR\x06\x64\x66Path\x12\x1c\n\tworkspace\x18\x03 \x01(\tR\tworkspace\x12;\n\x1ause_tecton_managed_retries\x18\x04 \x01(\x08R\x17useTectonManagedRetries\"m\n\x1dIngestDataframeFromS3Response\x12L\n\x03job\x18\x01 \x01(\x0b\x32:.tecton_proto.materializationjobservice.MaterializationJobR\x03job\"V\n\'TestOnlyWriteFeatureServerConfigRequest\x12+\n\x11\x61\x62solute_filepath\x18\x01 \x01(\tR\x10\x61\x62soluteFilepath\"*\n(TestOnlyWriteFeatureServerConfigResponse2\xbd\x11\n\x19MaterializationJobService\x12\x95\x02\n\x18SubmitMaterializationJob\x12\x41.tecton_proto.materializationjobservice.MaterializationJobRequest\x1a\x42.tecton_proto.materializationjobservice.MaterializationJobResponse\"r\x82\xd3\xe4\x93\x02(\"#/v1/jobs/submit-materialization-job:\x01*\xa2\xbc\xe6\xc0\x05>\x1a\r\x08\x01\x12\tworkspace*-tecton/workspace#update_materialization_state\x12\xfd\x01\n\x17ListMaterializationJobs\x12\x37.tecton_proto.materializationjobservice.ListJobsRequest\x1a\x38.tecton_proto.materializationjobservice.ListJobsResponse\"o\x82\xd3\xe4\x93\x02\'\"\"/v1/jobs/list-materialization-jobs:\x01*\xa2\xbc\xe6\xc0\x05<\x1a\r\x08\x01\x12\tworkspace*+tecton/workspace#read_materialization_state\x12\xbb\x01\n\x15GetMaterializationJob\x12\x35.tecton_proto.materializationjobservice.GetJobRequest\x1a\x36.tecton_proto.materializationjobservice.GetJobResponse\"3\x82\xd3\xe4\x93\x02%\" /v1/jobs/get-materialization-job:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xc7\x01\n\x18\x43\x61ncelMaterializationJob\x12\x38.tecton_proto.materializationjobservice.CancelJobRequest\x1a\x39.tecton_proto.materializationjobservice.CancelJobResponse\"6\x82\xd3\xe4\x93\x02(\"#/v1/jobs/cancel-materialization-job:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\x88\x02\n\x12GetLatestReadyTime\x12\x41.tecton_proto.materializationjobservice.GetLatestReadyTimeRequest\x1a\x42.tecton_proto.materializationjobservice.GetLatestReadyTimeResponse\"k\x82\xd3\xe4\x93\x02#\"\x1e/v1/jobs/get-latest-ready-time:\x01*\xa2\xbc\xe6\xc0\x05<\x1a\r\x08\x01\x12\tworkspace*+tecton/workspace#read_materialization_state\x12\x81\x02\n\x10GetDataframeInfo\x12?.tecton_proto.materializationjobservice.GetDataframeInfoRequest\x1a@.tecton_proto.materializationjobservice.GetDataframeInfoResponse\"j\x82\xd3\xe4\x93\x02 \"\x1b/v1/jobs/get-dataframe-info:\x01*\xa2\xbc\xe6\xc0\x05>\x1a\r\x08\x01\x12\tworkspace*-tecton/workspace#update_materialization_state\x12\x8e\x02\n\x15IngestDataframeFromS3\x12\x44.tecton_proto.materializationjobservice.IngestDataframeFromS3Request\x1a\x45.tecton_proto.materializationjobservice.IngestDataframeFromS3Response\"h\x82\xd3\xe4\x93\x02\x1e\"\x19/v1/jobs/ingest-dataframe:\x01*\xa2\xbc\xe6\xc0\x05>\x1a\r\x08\x01\x12\tworkspace*-tecton/workspace#update_materialization_state\x12\xf4\x01\n$TestOnlyGetMaterializationTaskParams\x12S.tecton_proto.materializationjobservice.TestOnlyGetMaterializationTaskParamsRequest\x1aT.tecton_proto.materializationjobservice.TestOnlyGetMaterializationTaskParamsResponse\"!\xa2\xbc\xe6\xc0\x05\x1b*\x19tecton/organization#admin\x12\xe8\x01\n TestOnlyWriteFeatureServerConfig\x12O.tecton_proto.materializationjobservice.TestOnlyWriteFeatureServerConfigRequest\x1aP.tecton_proto.materializationjobservice.TestOnlyWriteFeatureServerConfigResponse\"!\xa2\xbc\xe6\xc0\x05\x1b*\x19tecton/organization#adminB\x85\x01\n$com.tecton.materializationjobserviceB\x1eMaterializationJobServiceProtoP\x01Z;github.com/tecton-ai/tecton_proto/materializationjobservice')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.materializationjobservice.materialization_job_service_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n$com.tecton.materializationjobserviceB\036MaterializationJobServiceProtoP\001Z;github.com/tecton-ai/tecton_proto/materializationjobservice'
@@ -31,34 +31,58 @@
   _MATERIALIZATIONJOBSERVICE.methods_by_name['ListMaterializationJobs']._serialized_options = b'\202\323\344\223\002\'\"\"/v1/jobs/list-materialization-jobs:\001*\242\274\346\300\005<\032\r\010\001\022\tworkspace*+tecton/workspace#read_materialization_state'
   _MATERIALIZATIONJOBSERVICE.methods_by_name['GetMaterializationJob']._options = None
   _MATERIALIZATIONJOBSERVICE.methods_by_name['GetMaterializationJob']._serialized_options = b'\202\323\344\223\002%\" /v1/jobs/get-materialization-job:\001*\242\274\346\300\005\002@\001'
   _MATERIALIZATIONJOBSERVICE.methods_by_name['CancelMaterializationJob']._options = None
   _MATERIALIZATIONJOBSERVICE.methods_by_name['CancelMaterializationJob']._serialized_options = b'\202\323\344\223\002(\"#/v1/jobs/cancel-materialization-job:\001*\242\274\346\300\005\002@\001'
   _MATERIALIZATIONJOBSERVICE.methods_by_name['GetLatestReadyTime']._options = None
   _MATERIALIZATIONJOBSERVICE.methods_by_name['GetLatestReadyTime']._serialized_options = b'\202\323\344\223\002#\"\036/v1/jobs/get-latest-ready-time:\001*\242\274\346\300\005<\032\r\010\001\022\tworkspace*+tecton/workspace#read_materialization_state'
+  _MATERIALIZATIONJOBSERVICE.methods_by_name['GetDataframeInfo']._options = None
+  _MATERIALIZATIONJOBSERVICE.methods_by_name['GetDataframeInfo']._serialized_options = b'\202\323\344\223\002 \"\033/v1/jobs/get-dataframe-info:\001*\242\274\346\300\005>\032\r\010\001\022\tworkspace*-tecton/workspace#update_materialization_state'
+  _MATERIALIZATIONJOBSERVICE.methods_by_name['IngestDataframeFromS3']._options = None
+  _MATERIALIZATIONJOBSERVICE.methods_by_name['IngestDataframeFromS3']._serialized_options = b'\202\323\344\223\002\036\"\031/v1/jobs/ingest-dataframe:\001*\242\274\346\300\005>\032\r\010\001\022\tworkspace*-tecton/workspace#update_materialization_state'
+  _MATERIALIZATIONJOBSERVICE.methods_by_name['TestOnlyGetMaterializationTaskParams']._options = None
+  _MATERIALIZATIONJOBSERVICE.methods_by_name['TestOnlyGetMaterializationTaskParams']._serialized_options = b'\242\274\346\300\005\033*\031tecton/organization#admin'
+  _MATERIALIZATIONJOBSERVICE.methods_by_name['TestOnlyWriteFeatureServerConfig']._options = None
+  _MATERIALIZATIONJOBSERVICE.methods_by_name['TestOnlyWriteFeatureServerConfig']._serialized_options = b'\242\274\346\300\005\033*\031tecton/organization#admin'
   _MATERIALIZATIONJOBREQUEST._serialized_start=247
   _MATERIALIZATIONJOBREQUEST._serialized_end=594
   _MATERIALIZATIONJOBRESPONSE._serialized_start=596
   _MATERIALIZATIONJOBRESPONSE._serialized_end=702
   _MATERIALIZATIONJOB._serialized_start=705
-  _MATERIALIZATIONJOB._serialized_end=1285
-  _JOBATTEMPT._serialized_start=1288
-  _JOBATTEMPT._serialized_end=1481
-  _LISTJOBSREQUEST._serialized_start=1483
-  _LISTJOBSREQUEST._serialized_end=1565
-  _LISTJOBSRESPONSE._serialized_start=1567
-  _LISTJOBSRESPONSE._serialized_end=1665
-  _GETJOBREQUEST._serialized_start=1667
-  _GETJOBREQUEST._serialized_end=1770
-  _GETJOBRESPONSE._serialized_start=1772
-  _GETJOBRESPONSE._serialized_end=1866
-  _CANCELJOBREQUEST._serialized_start=1868
-  _CANCELJOBREQUEST._serialized_end=1974
-  _CANCELJOBRESPONSE._serialized_start=1976
-  _CANCELJOBRESPONSE._serialized_end=2073
-  _GETLATESTREADYTIMEREQUEST._serialized_start=2076
-  _GETLATESTREADYTIMEREQUEST._serialized_end=2209
-  _GETLATESTREADYTIMERESPONSE._serialized_start=2212
-  _GETLATESTREADYTIMERESPONSE._serialized_end=2412
-  _MATERIALIZATIONJOBSERVICE._serialized_start=2415
-  _MATERIALIZATIONJOBSERVICE._serialized_end=3637
+  _MATERIALIZATIONJOB._serialized_end=1318
+  _JOBATTEMPT._serialized_start=1321
+  _JOBATTEMPT._serialized_end=1514
+  _LISTJOBSREQUEST._serialized_start=1516
+  _LISTJOBSREQUEST._serialized_end=1598
+  _LISTJOBSRESPONSE._serialized_start=1600
+  _LISTJOBSRESPONSE._serialized_end=1698
+  _GETJOBREQUEST._serialized_start=1700
+  _GETJOBREQUEST._serialized_end=1803
+  _GETJOBRESPONSE._serialized_start=1805
+  _GETJOBRESPONSE._serialized_end=1899
+  _CANCELJOBREQUEST._serialized_start=1901
+  _CANCELJOBREQUEST._serialized_end=2007
+  _CANCELJOBRESPONSE._serialized_start=2009
+  _CANCELJOBRESPONSE._serialized_end=2106
+  _GETLATESTREADYTIMEREQUEST._serialized_start=2109
+  _GETLATESTREADYTIMEREQUEST._serialized_end=2242
+  _GETLATESTREADYTIMERESPONSE._serialized_start=2245
+  _GETLATESTREADYTIMERESPONSE._serialized_end=2445
+  _TESTONLYGETMATERIALIZATIONTASKPARAMSREQUEST._serialized_start=2447
+  _TESTONLYGETMATERIALIZATIONTASKPARAMSREQUEST._serialized_end=2566
+  _TESTONLYGETMATERIALIZATIONTASKPARAMSRESPONSE._serialized_start=2568
+  _TESTONLYGETMATERIALIZATIONTASKPARAMSRESPONSE._serialized_end=2684
+  _GETDATAFRAMEINFOREQUEST._serialized_start=2686
+  _GETDATAFRAMEINFOREQUEST._serialized_end=2776
+  _GETDATAFRAMEINFORESPONSE._serialized_start=2778
+  _GETDATAFRAMEINFORESPONSE._serialized_end=2885
+  _INGESTDATAFRAMEFROMS3REQUEST._serialized_start=2888
+  _INGESTDATAFRAMEFROMS3REQUEST._serialized_end=3069
+  _INGESTDATAFRAMEFROMS3RESPONSE._serialized_start=3071
+  _INGESTDATAFRAMEFROMS3RESPONSE._serialized_end=3180
+  _TESTONLYWRITEFEATURESERVERCONFIGREQUEST._serialized_start=3182
+  _TESTONLYWRITEFEATURESERVERCONFIGREQUEST._serialized_end=3268
+  _TESTONLYWRITEFEATURESERVERCONFIGRESPONSE._serialized_start=3270
+  _TESTONLYWRITEFEATURESERVERCONFIGRESPONSE._serialized_end=3312
+  _MATERIALIZATIONJOBSERVICE._serialized_start=3315
+  _MATERIALIZATIONJOBSERVICE._serialized_end=5552
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/metadataservice/http_over_grpc_pb2.py` & `tecton-0.7.0rc0/tecton_proto/metadataservice/http_over_grpc_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/metadataservice/metadata_service_pb2.py` & `tecton-0.7.0rc0/tecton_proto/metadataservice/metadata_service_pb2.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,31 +9,29 @@
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
 from google.api import annotations_pb2 as google_dot_api_dot_annotations__pb2
 from google.protobuf import descriptor_pb2 as google_dot_protobuf_dot_descriptor__pb2
+from tecton_proto.data import hive_metastore_pb2 as tecton__proto_dot_data_dot_hive__metastore__pb2
 from google.protobuf import duration_pb2 as google_dot_protobuf_dot_duration__pb2
 from google.protobuf import empty_pb2 as google_dot_protobuf_dot_empty__pb2
 from google.protobuf import timestamp_pb2 as google_dot_protobuf_dot_timestamp__pb2
 from google.protobuf import field_mask_pb2 as google_dot_protobuf_dot_field__mask__pb2
 from tecton_proto.auth import principal_pb2 as tecton__proto_dot_auth_dot_principal__pb2
 from tecton_proto.data import entity_pb2 as tecton__proto_dot_data_dot_entity__pb2
-from tecton_proto.args import fco_args_pb2 as tecton__proto_dot_args_dot_fco__args__pb2
 from tecton_proto.auth import service_pb2 as tecton__proto_dot_auth_dot_service__pb2
 from tecton_proto.data import fco_pb2 as tecton__proto_dot_data_dot_fco__pb2
 from tecton_proto.data import feature_view_pb2 as tecton__proto_dot_data_dot_feature__view__pb2
 from tecton_proto.data import feature_service_pb2 as tecton__proto_dot_data_dot_feature__service__pb2
-from tecton_proto.data import hive_metastore_pb2 as tecton__proto_dot_data_dot_hive__metastore__pb2
 from tecton_proto.data import state_update_pb2 as tecton__proto_dot_data_dot_state__update__pb2
 from tecton_proto.data import tecton_api_key_pb2 as tecton__proto_dot_data_dot_tecton__api__key__pb2
 from tecton_proto.data import user_deployment_settings_pb2 as tecton__proto_dot_data_dot_user__deployment__settings__pb2
 from tecton_proto.data import user_pb2 as tecton__proto_dot_data_dot_user__pb2
-from tecton_proto.spark_common import clusters_pb2 as tecton__proto_dot_spark__common_dot_clusters__pb2
 from tecton_proto.dataobs import expectation_pb2 as tecton__proto_dot_dataobs_dot_expectation__pb2
 from tecton_proto.consumption import consumption_pb2 as tecton__proto_dot_consumption_dot_consumption__pb2
 from tecton_proto.dataobs import metric_pb2 as tecton__proto_dot_dataobs_dot_metric__pb2
 from tecton_proto.dataobs import validation_pb2 as tecton__proto_dot_dataobs_dot_validation__pb2
 from tecton_proto.common import id_pb2 as tecton__proto_dot_common_dot_id__pb2
 from tecton_proto.common import fco_locator_pb2 as tecton__proto_dot_common_dot_fco__locator__pb2
 from tecton_proto.common import spark_schema_pb2 as tecton__proto_dot_common_dot_spark__schema__pb2
@@ -50,15 +48,15 @@
 from tecton_proto.amplitude import client_logging_pb2 as tecton__proto_dot_amplitude_dot_client__logging__pb2
 from tecton_proto.amplitude import amplitude_pb2 as tecton__proto_dot_amplitude_dot_amplitude__pb2
 from tecton_proto.validation import validator_pb2 as tecton__proto_dot_validation_dot_validator__pb2
 from tecton_proto.materialization import materialization_states_pb2 as tecton__proto_dot_materialization_dot_materialization__states__pb2
 from tecton_proto.materialization import spark_cluster_pb2 as tecton__proto_dot_materialization_dot_spark__cluster__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n3tecton_proto/metadataservice/metadata_service.proto\x12\x1ctecton_proto.metadataservice\x1a\x1cgoogle/api/annotations.proto\x1a google/protobuf/descriptor.proto\x1a\x1egoogle/protobuf/duration.proto\x1a\x1bgoogle/protobuf/empty.proto\x1a\x1fgoogle/protobuf/timestamp.proto\x1a google/protobuf/field_mask.proto\x1a!tecton_proto/auth/principal.proto\x1a\x1etecton_proto/data/entity.proto\x1a tecton_proto/args/fco_args.proto\x1a\x1ftecton_proto/auth/service.proto\x1a\x1btecton_proto/data/fco.proto\x1a$tecton_proto/data/feature_view.proto\x1a\'tecton_proto/data/feature_service.proto\x1a&tecton_proto/data/hive_metastore.proto\x1a$tecton_proto/data/state_update.proto\x1a&tecton_proto/data/tecton_api_key.proto\x1a\x30tecton_proto/data/user_deployment_settings.proto\x1a\x1ctecton_proto/data/user.proto\x1a(tecton_proto/spark_common/clusters.proto\x1a&tecton_proto/dataobs/expectation.proto\x1a*tecton_proto/consumption/consumption.proto\x1a!tecton_proto/dataobs/metric.proto\x1a%tecton_proto/dataobs/validation.proto\x1a\x1ctecton_proto/common/id.proto\x1a%tecton_proto/common/fco_locator.proto\x1a&tecton_proto/common/spark_schema.proto\x1a.tecton_proto/data/materialization_status.proto\x1a(tecton_proto/data/freshness_status.proto\x1a&tecton_proto/data/serving_status.proto\x1a\x35tecton_proto/data/internal_spark_cluster_status.proto\x1a\x1ftecton_proto/data/summary.proto\x1a\x30tecton_proto/data/saved_feature_data_frame.proto\x1a\"tecton_proto/data/onboarding.proto\x1a&tecton_proto/data/transformation.proto\x1a+tecton_proto/data/virtual_data_source.proto\x1a!tecton_proto/data/workspace.proto\x1a+tecton_proto/amplitude/client_logging.proto\x1a&tecton_proto/amplitude/amplitude.proto\x1a\'tecton_proto/validation/validator.proto\x1a\x39tecton_proto/materialization/materialization_states.proto\x1a\x30tecton_proto/materialization/spark_cluster.proto\"p\n\nJobsKeySet\x12\x39\n\nupdated_at\x18\x01 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\tupdatedAt\x12\'\n\x02id\x18\x02 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x02id\"\xd0\x01\n\x11PaginationRequest\x12\x12\n\x04page\x18\x01 \x01(\rR\x04page\x12\x19\n\x08per_page\x18\x02 \x01(\rR\x07perPage\x12\x19\n\x08sort_key\x18\x03 \x01(\tR\x07sortKey\x12R\n\x0esort_direction\x18\x04 \x01(\x0e\x32+.tecton_proto.metadataservice.SortDirectionR\rsortDirection\x12\x1d\n\npage_token\x18\x05 \x01(\tR\tpageToken\"\xf0\x01\n\x12PaginationResponse\x12\x12\n\x04page\x18\x01 \x01(\rR\x04page\x12\x19\n\x08per_page\x18\x02 \x01(\rR\x07perPage\x12\x14\n\x05total\x18\x03 \x01(\rR\x05total\x12&\n\x0fnext_page_token\x18\x04 \x01(\tR\rnextPageToken\x12\x19\n\x08sort_key\x18\x05 \x01(\tR\x07sortKey\x12R\n\x0esort_direction\x18\x06 \x01(\x0e\x32+.tecton_proto.metadataservice.SortDirectionR\rsortDirection\"\x9a\x01\n\x18GetFeatureServiceRequest\x12+\n\x11service_reference\x18\x02 \x01(\tR\x10serviceReference\x12\x1c\n\tworkspace\x18\x03 \x01(\tR\tworkspace\x12\'\n\x02id\x18\x04 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x02idJ\x04\x08\x01\x10\x02J\x04\x08\x05\x10\x06\"a\n\x19GetFeatureServiceResponse\x12\x44\n\rfco_container\x18\x02 \x01(\x0b\x32\x1f.tecton_proto.data.FcoContainerR\x0c\x66\x63oContainer\"<\n\x1cGetAllFeatureServicesRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\"m\n\x1dGetAllFeatureServicesResponse\x12L\n\x10\x66\x65\x61ture_services\x18\x01 \x03(\x0b\x32!.tecton_proto.data.FeatureServiceR\x0f\x66\x65\x61tureServices\"\xc7\x01\n\x1fGetFeatureServiceSummaryRequest\x12G\n\x12\x66\x65\x61ture_service_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdH\x00R\x10\x66\x65\x61tureServiceId\x12\x32\n\x14\x66\x65\x61ture_service_name\x18\x02 \x01(\tH\x00R\x12\x66\x65\x61tureServiceName\x12\x1c\n\tworkspace\x18\x03 \x01(\tR\tworkspaceB\t\n\x07locator\"\x8c\x01\n GetFeatureServiceSummaryResponse\x12\x43\n\rgeneral_items\x18\x01 \x03(\x0b\x32\x1e.tecton_proto.data.SummaryItemR\x0cgeneralItems\x12#\n\rvariant_names\x18\x03 \x03(\tR\x0cvariantNames\"f\n\"GetVirtualDataSourceSummaryRequest\x12@\n\x0b\x66\x63o_locator\x18\x01 \x01(\x0b\x32\x1f.tecton_proto.common.FcoLocatorR\nfcoLocator\"e\n#GetVirtualDataSourceSummaryResponse\x12>\n\x0b\x66\x63o_summary\x18\x01 \x01(\x0b\x32\x1d.tecton_proto.data.FcoSummaryR\nfcoSummary\"c\n\x1fGetTransformationSummaryRequest\x12@\n\x0b\x66\x63o_locator\x18\x01 \x01(\x0b\x32\x1f.tecton_proto.common.FcoLocatorR\nfcoLocator\"b\n GetTransformationSummaryResponse\x12>\n\x0b\x66\x63o_summary\x18\x01 \x01(\x0b\x32\x1d.tecton_proto.data.FcoSummaryR\nfcoSummary\"[\n\x17GetEntitySummaryRequest\x12@\n\x0b\x66\x63o_locator\x18\x01 \x01(\x0b\x32\x1f.tecton_proto.common.FcoLocatorR\nfcoLocator\"Z\n\x18GetEntitySummaryResponse\x12>\n\x0b\x66\x63o_summary\x18\x01 \x01(\x0b\x32\x1d.tecton_proto.data.FcoSummaryR\nfcoSummary\"\xd9\x01\n\x17GetServingStatusRequest\x12G\n\x12\x66\x65\x61ture_package_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdH\x00R\x10\x66\x65\x61turePackageId\x12G\n\x12\x66\x65\x61ture_service_id\x18\x02 \x01(\x0b\x32\x17.tecton_proto.common.IdH\x00R\x10\x66\x65\x61tureServiceId\x12\x1c\n\tworkspace\x18\x03 \x01(\tR\tworkspaceB\x0e\n\x0crequest_type\"\xd6\x01\n\x1eGetFVServingStatusForFSRequest\x12\x45\n\x12\x66\x65\x61ture_service_id\x18\x02 \x02(\x0b\x32\x17.tecton_proto.common.IdR\x10\x66\x65\x61tureServiceId\x12\x1c\n\tworkspace\x18\x03 \x01(\tR\tworkspace\x12O\n\npagination\x18\x04 \x01(\x0b\x32/.tecton_proto.metadataservice.PaginationRequestR\npagination\"\xe7\x01\n\x1fGetFVServingStatusForFSResponse\x12r\n\x1b\x66ull_serving_status_summary\x18\x01 \x01(\x0b\x32\x33.tecton_proto.data.FullFeatureServiceServingSummaryR\x18\x66ullServingStatusSummary\x12P\n\npagination\x18\x02 \x01(\x0b\x32\x30.tecton_proto.metadataservice.PaginationResponseR\npagination\"y\n\x18GetServingStatusResponse\x12]\n\x16serving_status_summary\x18\x05 \x01(\x0b\x32\'.tecton_proto.data.ServingStatusSummaryR\x14servingStatusSummary\"=\n\x1dGetAllFeatureFreshnessRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\"s\n\x1eGetAllFeatureFreshnessResponse\x12Q\n\x12\x66reshness_statuses\x18\x01 \x03(\x0b\x32\".tecton_proto.data.FreshnessStatusR\x11\x66reshnessStatuses\"^\n\x1aGetFeatureFreshnessRequest\x12@\n\x0b\x66\x63o_locator\x18\x01 \x01(\x0b\x32\x1f.tecton_proto.common.FcoLocatorR\nfcoLocator\"l\n\x1bGetFeatureFreshnessResponse\x12M\n\x10\x66reshness_status\x18\x01 \x01(\x0b\x32\".tecton_proto.data.FreshnessStatusR\x0f\x66reshnessStatus\"`\n\x1cGetFeatureConsumptionRequest\x12@\n\x0b\x66\x63o_locator\x18\x01 \x01(\x0b\x32\x1f.tecton_proto.common.FcoLocatorR\nfcoLocator\"u\n\x1dGetFeatureConsumptionResponse\x12T\n\x10\x63onsumption_info\x18\x01 \x03(\x0b\x32).tecton_proto.consumption.ConsumptionInfoR\x0f\x63onsumptionInfo\"h\n\x1fGetMaterializationStatusRequest\x12\x45\n\x12\x66\x65\x61ture_package_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x10\x66\x65\x61turePackageId\"\x83\x01\n GetMaterializationStatusResponse\x12_\n\x16materialization_status\x18\x01 \x01(\x0b\x32(.tecton_proto.data.MaterializationStatusR\x15materializationStatus\"V\n2GetAllMaterializationStatusInLiveWorkspacesRequest\x12 \n\x0c\x63ut_off_days\x18\x01 \x01(\x05R\ncutOffDays\"4\n\nCountRange\x12\x14\n\x05start\x18\x01 \x01(\rR\x05start\x12\x10\n\x03\x65nd\x18\x02 \x01(\rR\x03\x65nd\"m\n\rDurationRange\x12/\n\x05start\x18\x01 \x01(\x0b\x32\x19.google.protobuf.DurationR\x05start\x12+\n\x03\x65nd\x18\x02 \x01(\x0b\x32\x19.google.protobuf.DurationR\x03\x65nd\"o\n\rDateTimeRange\x12\x30\n\x05start\x18\x06 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x05start\x12,\n\x03\x65nd\x18\x07 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x03\x65nd\"\x82\x06\n\x0eGetJobsRequest\x12\x1e\n\nworkspaces\x18\x01 \x03(\tR\nworkspaces\x12#\n\rfeature_views\x18\x02 \x03(\tR\x0c\x66\x65\x61tureViews\x12R\n\x08statuses\x18\x03 \x03(\x0e\x32\x36.tecton_proto.materialization.MaterializationTaskStateR\x08statuses\x12J\n\ncreated_at\x18\x06 \x01(\x0b\x32+.tecton_proto.metadataservice.DateTimeRangeR\tcreatedAt\x12\x43\n\ttask_type\x18\x04 \x03(\x0e\x32&.tecton_proto.materialization.TaskTypeR\x08taskType\x12K\n\x0cnum_attempts\x18\x05 \x01(\x0b\x32(.tecton_proto.metadataservice.CountRangeR\x0bnumAttempts\x12-\n\x12manually_triggered\x18\x07 \x01(\x08R\x11manuallyTriggered\x12G\n\x08\x64uration\x18\x08 \x01(\x0b\x32+.tecton_proto.metadataservice.DurationRangeR\x08\x64uration\x12Y\n\x12\x66\x65\x61ture_start_time\x18\n \x01(\x0b\x32+.tecton_proto.metadataservice.DateTimeRangeR\x10\x66\x65\x61tureStartTime\x12U\n\x10\x66\x65\x61ture_end_time\x18\x0b \x01(\x0b\x32+.tecton_proto.metadataservice.DateTimeRangeR\x0e\x66\x65\x61tureEndTime\x12O\n\npagination\x18\t \x01(\x0b\x32/.tecton_proto.metadataservice.PaginationRequestR\npagination\"\xc5\x01\n FeatureViewMaterializationStatus\x12@\n\x0b\x66\x63o_locator\x18\x01 \x01(\x0b\x32\x1f.tecton_proto.common.FcoLocatorR\nfcoLocator\x12_\n\x16materialization_status\x18\x03 \x01(\x0b\x32(.tecton_proto.data.MaterializationStatusR\x15materializationStatus\"\xbe\x04\n\x10TaskWithAttempts\x12@\n\x0b\x66\x63o_locator\x18\x01 \x01(\x0b\x32\x1f.tecton_proto.common.FcoLocatorR\nfcoLocator\x12T\n\ttaskState\x18\x02 \x01(\x0e\x32\x36.tecton_proto.materialization.MaterializationTaskStateR\ttaskState\x12_\n\x16materialization_status\x18\x03 \x01(\x0b\x32(.tecton_proto.data.MaterializationStatusR\x15materializationStatus\x12\x30\n\x07task_id\x18\x04 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x06taskId\x12\x43\n\ttask_type\x18\x08 \x01(\x0e\x32&.tecton_proto.materialization.TaskTypeR\x08taskType\x12*\n\x11\x66\x65\x61ture_view_name\x18\x05 \x01(\tR\x0f\x66\x65\x61tureViewName\x12H\n\x12\x66\x65\x61ture_start_time\x18\x06 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x10\x66\x65\x61tureStartTime\x12\x44\n\x10\x66\x65\x61ture_end_time\x18\x07 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x0e\x66\x65\x61tureEndTime\"\xc5\x01\n3GetAllMaterializationStatusInLiveWorkspacesResponse\x12\x8d\x01\n#feature_view_materialization_status\x18\x01 \x03(\x0b\x32>.tecton_proto.metadataservice.FeatureViewMaterializationStatusR featureViewMaterializationStatus\"\xc1\x01\n\x0fGetJobsResponse\x12\\\n\x11tasksWithAttempts\x18\x01 \x03(\x0b\x32..tecton_proto.metadataservice.TaskWithAttemptsR\x11tasksWithAttempts\x12P\n\npagination\x18\x02 \x01(\x0b\x32\x30.tecton_proto.metadataservice.PaginationResponseR\npagination\"\xa0\x01\n$ForceRetryMaterializationTaskRequest\x12O\n\x17materialization_task_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x15materializationTaskId\x12\'\n\x0f\x61llow_overwrite\x18\x02 \x01(\x08R\x0e\x61llowOverwrite\"L\n%ForceRetryMaterializationTaskResponse\x12#\n\rerror_message\x18\x01 \x01(\tR\x0c\x65rrorMessage\".\n\x0eGetFcosRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\"W\n\x0fGetFcosResponse\x12\x44\n\rfco_container\x18\x01 \x01(\x0b\x32\x1f.tecton_proto.data.FcoContainerR\x0c\x66\x63oContainer\"a\n\x15GetSparkConfigRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12*\n\x11\x66\x65\x61ture_view_name\x18\x02 \x01(\tR\x0f\x66\x65\x61tureViewName\"\xb0\x01\n\x16GetSparkConfigResponse\x12I\n\x0c\x62\x61tch_config\x18\x01 \x01(\x0b\x32&.tecton_proto.spark_common.ClusterInfoR\x0b\x62\x61tchConfig\x12K\n\rstream_config\x18\x02 \x01(\x0b\x32&.tecton_proto.spark_common.ClusterInfoR\x0cstreamConfig\"t\n(GetMetricAndExpectationDefinitionRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12*\n\x11\x66\x65\x61ture_view_name\x18\x02 \x01(\tR\x0f\x66\x65\x61tureViewName\"\xe4\x02\n)GetMetricAndExpectationDefinitionResponse\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12*\n\x11\x66\x65\x61ture_view_name\x18\x02 \x01(\tR\x0f\x66\x65\x61tureViewName\x12\x36\n\x07metrics\x18\x03 \x03(\x0b\x32\x1c.tecton_proto.dataobs.MetricR\x07metrics\x12[\n\x14\x66\x65\x61ture_expectations\x18\x04 \x03(\x0b\x32(.tecton_proto.dataobs.FeatureExpectationR\x13\x66\x65\x61tureExpectations\x12X\n\x13metric_expectations\x18\x05 \x03(\x0b\x32\'.tecton_proto.dataobs.MetricExpectationR\x12metricExpectations\"\x91\x01\n\x15GetFeatureViewRequest\x12+\n\x11version_specifier\x18\x01 \x01(\tR\x10versionSpecifier\x12\x1c\n\tworkspace\x18\x03 \x01(\tR\tworkspace\x12\'\n\x02id\x18\x04 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x02idJ\x04\x08\x05\x10\x06\"^\n\x16GetFeatureViewResponse\x12\x44\n\rfco_container\x18\x02 \x01(\x0b\x32\x1f.tecton_proto.data.FcoContainerR\x0c\x66\x63oContainer\"`\n\x1cGetFeatureViewSummaryRequest\x12@\n\x0b\x66\x63o_locator\x18\x01 \x01(\x0b\x32\x1f.tecton_proto.common.FcoLocatorR\nfcoLocator\"_\n\x1dGetFeatureViewSummaryResponse\x12>\n\x0b\x66\x63o_summary\x18\x01 \x01(\x0b\x32\x1d.tecton_proto.data.FcoSummaryR\nfcoSummary\"R\n\x18QueryFeatureViewsRequest\x12\x12\n\x04name\x18\x01 \x01(\tR\x04name\x12\x1c\n\tworkspace\x18\x03 \x01(\tR\tworkspaceJ\x04\x08\x05\x10\x06\"a\n\x19QueryFeatureViewsResponse\x12\x44\n\rfco_container\x18\x02 \x01(\x0b\x32\x1f.tecton_proto.data.FcoContainerR\x0c\x66\x63oContainer\"{\n4GetMaterializingFeatureViewsInLiveWorkspacesResponse\x12\x43\n\rfeature_views\x18\x01 \x03(\x0b\x32\x1e.tecton_proto.data.FeatureViewR\x0c\x66\x65\x61tureViews\"\xba\x01\n\x1bGetVirtualDataSourceRequest\x12N\n\x16virtual_data_source_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdH\x00R\x13virtualDataSourceId\x12\x14\n\x04name\x18\x02 \x01(\tH\x00R\x04name\x12\x1c\n\tworkspace\x18\x03 \x01(\tR\tworkspaceB\x11\n\x0fidentifier_typeJ\x04\x08\x05\x10\x06\"d\n\x1cGetVirtualDataSourceResponse\x12\x44\n\rfco_container\x18\x02 \x01(\x0b\x32\x1f.tecton_proto.data.FcoContainerR\x0c\x66\x63oContainer\"?\n\x1fGetAllVirtualDataSourcesRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\"z\n GetAllVirtualDataSourcesResponse\x12V\n\x14virtual_data_sources\x18\x01 \x03(\x0b\x32$.tecton_proto.data.VirtualDataSourceR\x12virtualDataSources\"\x97\x01\n\x10GetEntityRequest\x12\x36\n\tentity_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdH\x00R\x08\x65ntityId\x12\x14\n\x04name\x18\x02 \x01(\tH\x00R\x04name\x12\x1c\n\tworkspace\x18\x03 \x01(\tR\tworkspaceB\x11\n\x0fidentifier_typeJ\x04\x08\x05\x10\x06\"Y\n\x11GetEntityResponse\x12\x44\n\rfco_container\x18\x02 \x01(\x0b\x32\x1f.tecton_proto.data.FcoContainerR\x0c\x66\x63oContainer\"5\n\x15GetAllEntitiesRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\"O\n\x16GetAllEntitiesResponse\x12\x35\n\x08\x65ntities\x18\x01 \x03(\x0b\x32\x19.tecton_proto.data.EntityR\x08\x65ntities\"\xaf\x01\n\x18GetTransformationRequest\x12\x46\n\x11transformation_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdH\x00R\x10transformationId\x12\x14\n\x04name\x18\x02 \x01(\tH\x00R\x04name\x12\x1c\n\tworkspace\x18\x04 \x01(\tR\tworkspaceB\x11\n\x0fidentifier_typeJ\x04\x08\x05\x10\x06\"g\n\x19GetTransformationResponse\x12\x44\n\rfco_container\x18\x03 \x01(\x0b\x32\x1f.tecton_proto.data.FcoContainerR\x0c\x66\x63oContainerJ\x04\x08\x01\x10\x02\"<\n\x1cGetAllTransformationsRequest\x12\x1c\n\tworkspace\x18\x02 \x01(\tR\tworkspace\"r\n\x1dGetAllTransformationsResponse\x12K\n\x0ftransformations\x18\x02 \x03(\x0b\x32!.tecton_proto.data.TransformationR\x0ftransformationsJ\x04\x08\x01\x10\x02\"B\n\x17\x46indFcoWorkspaceRequest\x12\'\n\x02id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x02id\"8\n\x18\x46indFcoWorkspaceResponse\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\"z\n\x17IngestClientLogsRequest\x12_\n\x15sdk_method_invocation\x18\x01 \x01(\x0b\x32+.tecton_proto.amplitude.SDKMethodInvocationR\x13sdkMethodInvocation\"v\n\x16IngestAnalyticsRequest\x12>\n\x06\x65vents\x18\x01 \x03(\x0b\x32&.tecton_proto.amplitude.AmplitudeEventR\x06\x65vents\x12\x1c\n\tworkspace\x18\x02 \x01(\tR\tworkspace\"\xbf\x01\n\x15NewStateUpdateRequest\x12?\n\x07request\x18\x01 \x01(\x0b\x32%.tecton_proto.data.StateUpdateRequestR\x07request\x12\x31\n\x15\x62locking_dry_run_mode\x18\x02 \x01(\x08R\x12\x62lockingDryRunMode\x12\x32\n\x15\x65nable_eager_response\x18\x03 \x01(\x08R\x13\x65nableEagerResponse\"\xfd\x01\n\x17NewStateUpdateRequestV2\x12?\n\x07request\x18\x01 \x01(\x0b\x32%.tecton_proto.data.StateUpdateRequestR\x07request\x12\x31\n\x15\x62locking_dry_run_mode\x18\x02 \x01(\x08R\x12\x62lockingDryRunMode\x12\x32\n\x15\x65nable_eager_response\x18\x03 \x01(\x08R\x13\x65nableEagerResponse\x12\x19\n\x08no_color\x18\x04 \x01(\x08R\x07noColor\x12\x1f\n\x0bjson_output\x18\x05 \x01(\x08R\njsonOutput\"\xe7\x01\n\x16NewStateUpdateResponse\x12\x32\n\x08state_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x07stateId\x12:\n\x1asigned_url_for_repo_upload\x18\x02 \x01(\tR\x16signedUrlForRepoUpload\x12]\n\x0e\x65\x61ger_response\x18\x03 \x01(\x0b\x32\x36.tecton_proto.metadataservice.QueryStateUpdateResponseR\reagerResponse\"\xeb\x01\n\x18NewStateUpdateResponseV2\x12\x32\n\x08state_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x07stateId\x12:\n\x1asigned_url_for_repo_upload\x18\x02 \x01(\tR\x16signedUrlForRepoUpload\x12_\n\x0e\x65\x61ger_response\x18\x03 \x01(\x0b\x32\x38.tecton_proto.metadataservice.QueryStateUpdateResponseV2R\reagerResponse\"k\n\x17QueryStateUpdateRequest\x12\x32\n\x08state_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x07stateId\x12\x1c\n\tworkspace\x18\x02 \x01(\tR\tworkspace\"\xa9\x01\n\x19QueryStateUpdateRequestV2\x12\x32\n\x08state_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x07stateId\x12\x1c\n\tworkspace\x18\x02 \x01(\tR\tworkspace\x12\x19\n\x08no_color\x18\x03 \x01(\x08R\x07noColor\x12\x1f\n\x0bjson_output\x18\x04 \x01(\x08R\njsonOutput\"\xd4\x02\n\x18QueryStateUpdateResponse\x12\x14\n\x05ready\x18\x01 \x01(\x08R\x05ready\x12\x18\n\x07success\x18\x02 \x01(\x08R\x07success\x12\x14\n\x05\x65rror\x18\x03 \x01(\tR\x05\x65rror\x12\x31\n\x14recreates_suppressed\x18\x07 \x01(\x08R\x13recreatesSuppressed\x12P\n\x11validation_result\x18\x04 \x01(\x0b\x32#.tecton_proto.data.ValidationResultR\x10validationResult\x12\x39\n\ndiff_items\x18\x05 \x03(\x0b\x32\x1a.tecton_proto.data.FcoDiffR\tdiffItems\x12\x32\n\x15latest_status_message\x18\x06 \x01(\tR\x13latestStatusMessage\"\xe9\x02\n\x1aQueryStateUpdateResponseV2\x12\x14\n\x05ready\x18\x01 \x01(\x08R\x05ready\x12\x18\n\x07success\x18\x02 \x01(\x08R\x07success\x12\x14\n\x05\x65rror\x18\x03 \x01(\tR\x05\x65rror\x12\x32\n\x15latest_status_message\x18\x06 \x01(\tR\x13latestStatusMessage\x12R\n\x11validation_errors\x18\x08 \x01(\x0b\x32#.tecton_proto.data.ValidationResultH\x00R\x10validationErrors\x12_\n\x16successful_plan_output\x18\t \x01(\x0b\x32\'.tecton_proto.data.SuccessfulPlanOutputH\x00R\x14successfulPlanOutputB\n\n\x08responseJ\x04\x08\x04\x10\x05J\x04\x08\x05\x10\x06J\x04\x08\x07\x10\x08\"T\n GetStateUpdatePlanSummaryRequest\x12\x30\n\x07plan_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x06planId\"b\n!GetStateUpdatePlanSummaryResponse\x12=\n\x04plan\x18\x01 \x01(\x0b\x32).tecton_proto.data.StateUpdatePlanSummaryR\x04plan\"p\n\x17\x41pplyStateUpdateRequest\x12\x32\n\x08state_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x07stateId\x12!\n\napplied_by\x18\x02 \x01(\tB\x02\x18\x01R\tappliedBy\"\x1a\n\x18\x41pplyStateUpdateResponse\"\xb2\x01\n\x12GetConfigsResponse\x12^\n\nkey_values\x18\x01 \x03(\x0b\x32?.tecton_proto.metadataservice.GetConfigsResponse.KeyValuesEntryR\tkeyValues\x1a<\n\x0eKeyValuesEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01\"\xc2\x01\n\x1aGetGlobalsForWebUIResponse\x12\x66\n\nkey_values\x18\x01 \x03(\x0b\x32G.tecton_proto.metadataservice.GetGlobalsForWebUIResponse.KeyValuesEntryR\tkeyValues\x1a<\n\x0eKeyValuesEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01\"N\n\x18GetStateUpdateLogRequest\x12\x14\n\x05limit\x18\x01 \x01(\x05R\x05limit\x12\x1c\n\tworkspace\x18\x02 \x01(\tR\tworkspace\"Z\n\x19GetStateUpdateLogResponse\x12=\n\x07\x65ntries\x18\x01 \x03(\x0b\x32#.tecton_proto.data.StateUpdateEntryR\x07\x65ntries\"R\n\x15GetRestoreInfoRequest\x12\x1b\n\tcommit_id\x18\x01 \x01(\tR\x08\x63ommitId\x12\x1c\n\tworkspace\x18\x02 \x01(\tR\tworkspace\"\x96\x01\n\x16GetRestoreInfoResponse\x12>\n\x1csigned_url_for_repo_download\x18\x01 \x01(\tR\x18signedUrlForRepoDownload\x12\x1b\n\tcommit_id\x18\x02 \x01(\tR\x08\x63ommitId\x12\x1f\n\x0bsdk_version\x18\x03 \x01(\tR\nsdkVersion\"\x93\x01\n\x16\x43reateWorkspaceRequest\x12%\n\x0eworkspace_name\x18\x02 \x01(\tR\rworkspaceName\x12L\n\x0c\x63\x61pabilities\x18\x03 \x01(\x0b\x32(.tecton_proto.data.WorkspaceCapabilitiesR\x0c\x63\x61pabilitiesJ\x04\x08\x01\x10\x02\"6\n\x16\x44\x65leteWorkspaceRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\"2\n\x17IntrospectApiKeyRequest\x12\x17\n\x07\x61pi_key\x18\x01 \x01(\tR\x06\x61piKey\"\xcb\x01\n\x18IntrospectApiKeyResponse\x12\'\n\x02id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x02id\x12 \n\x0b\x64\x65scription\x18\x02 \x01(\tR\x0b\x64\x65scription\x12\x1d\n\ncreated_by\x18\x03 \x01(\tR\tcreatedBy\x12\x16\n\x06\x61\x63tive\x18\x04 \x01(\x08R\x06\x61\x63tive\x12\x19\n\x08is_admin\x18\x05 \x01(\x08R\x07isAdmin\x12\x12\n\x04name\x18\x06 \x01(\tR\x04name\"R\n\x13\x43reateApiKeyRequest\x12 \n\x0b\x64\x65scription\x18\x01 \x01(\tR\x0b\x64\x65scription\x12\x19\n\x08is_admin\x18\x02 \x01(\x08R\x07isAdmin\"Q\n\x14\x43reateApiKeyResponse\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\'\n\x02id\x18\x02 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x02id\">\n\x13\x44\x65leteApiKeyRequest\x12\'\n\x02id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x02id\"?\n\x12ListApiKeysRequest\x12)\n\x10include_archived\x18\x01 \x01(\x08R\x0fincludeArchived\"Q\n\x13ListApiKeysResponse\x12:\n\x08\x61pi_keys\x18\x01 \x03(\x0b\x32\x1f.tecton_proto.data.TectonApiKeyR\x07\x61piKeys\"\xe4\x01\n\x0eServiceAccount\x12\x0e\n\x02id\x18\x01 \x01(\tR\x02id\x12\x12\n\x04name\x18\x02 \x01(\tR\x04name\x12 \n\x0b\x64\x65scription\x18\x03 \x01(\tR\x0b\x64\x65scription\x12\x1b\n\tis_active\x18\x04 \x01(\x08R\x08isActive\x12\x36\n\x07\x63reator\x18\x05 \x01(\x0b\x32\x1c.tecton_proto.auth.PrincipalR\x07\x63reator\x12\x37\n\x05owner\x18\x06 \x01(\x0b\x32!.tecton_proto.auth.PrincipalBasicR\x05owner\"S\n\x1b\x43reateServiceAccountRequest\x12\x12\n\x04name\x18\x01 \x01(\tR\x04name\x12 \n\x0b\x64\x65scription\x18\x02 \x01(\tR\x0b\x64\x65scription\"\x9a\x01\n\x1c\x43reateServiceAccountResponse\x12\x0e\n\x02id\x18\x01 \x01(\tR\x02id\x12\x12\n\x04name\x18\x02 \x01(\tR\x04name\x12 \n\x0b\x64\x65scription\x18\x03 \x01(\tR\x0b\x64\x65scription\x12\x1b\n\tis_active\x18\x04 \x01(\x08R\x08isActive\x12\x17\n\x07\x61pi_key\x18\x05 \x01(\tR\x06\x61piKey\"E\n\x19GetServiceAccountsRequest\x12\x16\n\x06search\x18\x01 \x01(\tR\x06search\x12\x10\n\x03ids\x18\x02 \x03(\tR\x03ids\"u\n\x1aGetServiceAccountsResponse\x12W\n\x10service_accounts\x18\x01 \x03(\x0b\x32,.tecton_proto.metadataservice.ServiceAccountR\x0fserviceAccounts\"\x80\x01\n\x1bUpdateServiceAccountRequest\x12\x0e\n\x02id\x18\x01 \x01(\tR\x02id\x12\x12\n\x04name\x18\x02 \x01(\tR\x04name\x12 \n\x0b\x64\x65scription\x18\x03 \x01(\tR\x0b\x64\x65scription\x12\x1b\n\tis_active\x18\x04 \x01(\x08R\x08isActive\"\x81\x01\n\x1cUpdateServiceAccountResponse\x12\x0e\n\x02id\x18\x01 \x01(\tR\x02id\x12\x12\n\x04name\x18\x02 \x01(\tR\x04name\x12 \n\x0b\x64\x65scription\x18\x03 \x01(\tR\x0b\x64\x65scription\x12\x1b\n\tis_active\x18\x04 \x01(\x08R\x08isActive\"-\n\x1b\x44\x65leteServiceAccountRequest\x12\x0e\n\x02id\x18\x01 \x01(\tR\x02id\"\x1e\n\x1c\x44\x65leteServiceAccountResponse\"\x17\n\x15ListWorkspacesRequest\"V\n\x16ListWorkspacesResponse\x12<\n\nworkspaces\x18\x01 \x03(\x0b\x32\x1c.tecton_proto.data.WorkspaceR\nworkspaces\"<\n\x13GetWorkspaceRequest\x12%\n\x0eworkspace_name\x18\x01 \x01(\tR\rworkspaceName\"R\n\x14GetWorkspaceResponse\x12:\n\tworkspace\x18\x01 \x01(\x0b\x32\x1c.tecton_proto.data.WorkspaceR\tworkspace\"\x93\x03\n\"CreateSavedFeatureDataFrameRequest\x12\x12\n\x04name\x18\x01 \x01(\tR\x04name\x12\x1c\n\tworkspace\x18\x07 \x01(\tR\tworkspace\x12G\n\x12\x66\x65\x61ture_package_id\x18\x02 \x01(\x0b\x32\x17.tecton_proto.common.IdH\x00R\x10\x66\x65\x61turePackageId\x12G\n\x12\x66\x65\x61ture_service_id\x18\x03 \x01(\x0b\x32\x17.tecton_proto.common.IdH\x00R\x10\x66\x65\x61tureServiceId\x12\x31\n\x15join_key_column_names\x18\x04 \x03(\tR\x12joinKeyColumnNames\x12\x32\n\x15timestamp_column_name\x18\x05 \x01(\tR\x13timestampColumnName\x12\x38\n\x06schema\x18\x06 \x01(\x0b\x32 .tecton_proto.common.SparkSchemaR\x06schemaB\x08\n\x06source\"\x87\x01\n#CreateSavedFeatureDataFrameResponse\x12`\n\x17saved_feature_dataframe\x18\x01 \x01(\x0b\x32(.tecton_proto.data.SavedFeatureDataFrameR\x15savedFeatureDataframe\"{\n#ArchiveSavedFeatureDataFrameRequest\x12T\n\x1asaved_feature_dataframe_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x17savedFeatureDataframeId\"&\n$ArchiveSavedFeatureDataFrameResponse\"\xe7\x01\n\x1fGetSavedFeatureDataFrameRequest\x12\x41\n\x1csaved_feature_dataframe_name\x18\x01 \x01(\tH\x00R\x19savedFeatureDataframeName\x12V\n\x1asaved_feature_dataframe_id\x18\x02 \x01(\x0b\x32\x17.tecton_proto.common.IdH\x00R\x17savedFeatureDataframeId\x12\x1c\n\tworkspace\x18\x03 \x01(\tR\tworkspaceB\x0b\n\treference\"\xa5\x01\n\x1bGetClusterAdminInfoResponse\x12&\n\x0f\x63\x61ller_is_admin\x18\x01 \x01(\x08R\rcallerIsAdmin\x12-\n\x05users\x18\x02 \x03(\x0b\x32\x17.tecton_proto.data.UserR\x05users\x12/\n\x06\x61\x64mins\x18\x03 \x03(\x0b\x32\x17.tecton_proto.data.UserR\x06\x61\x64mins\";\n\x18\x43reateClusterUserRequest\x12\x1f\n\x0blogin_email\x18\x01 \x01(\tR\nloginEmail\"@\n\x19\x43reateClusterUserResponse\x12#\n\rerror_message\x18\x01 \x01(\tR\x0c\x65rrorMessage\"3\n\x18\x44\x65leteClusterUserRequest\x12\x17\n\x07okta_id\x18\x01 \x01(\tR\x06oktaId\"@\n\x19\x44\x65leteClusterUserResponse\x12#\n\rerror_message\x18\x01 \x01(\tR\x0c\x65rrorMessage\"\xe2\x01\n\x18\x43lusterUserActionRequest\x12\x17\n\x07okta_id\x18\x01 \x01(\tR\x06oktaId\x12\x38\n\x17resend_activation_email\x18\x02 \x01(\x08H\x00R\x15resendActivationEmail\x12!\n\x0bunlock_user\x18\x03 \x01(\x08H\x00R\nunlockUser\x12!\n\x0bgrant_admin\x18\x04 \x01(\x08H\x00R\ngrantAdmin\x12#\n\x0crevoke_admin\x18\x05 \x01(\x08H\x00R\x0brevokeAdminB\x08\n\x06\x61\x63tion\"@\n\x19\x43lusterUserActionResponse\x12#\n\rerror_message\x18\x01 \x01(\tR\x0c\x65rrorMessage\"\x84\x01\n GetSavedFeatureDataFrameResponse\x12`\n\x17saved_feature_dataframe\x18\x01 \x01(\x0b\x32(.tecton_proto.data.SavedFeatureDataFrameR\x15savedFeatureDataframe\"C\n#GetAllSavedFeatureDataFramesRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\"\x8a\x01\n$GetAllSavedFeatureDataFramesResponse\x12\x62\n\x18saved_feature_dataframes\x18\x01 \x03(\x0b\x32(.tecton_proto.data.SavedFeatureDataFrameR\x16savedFeatureDataframes\"\x9c\x01\n\x16IngestDataframeRequest\x12K\n\x15\x66\x65\x61ture_definition_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x13\x66\x65\x61tureDefinitionId\x12\x17\n\x07\x64\x66_path\x18\x02 \x01(\tR\x06\x64\x66Path\x12\x1c\n\tworkspace\x18\x03 \x01(\tR\tworkspace\"\x19\n\x17IngestDataframeResponse\"o\n GetNewIngestDataframeInfoRequest\x12K\n\x15\x66\x65\x61ture_definition_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x13\x66\x65\x61tureDefinitionId\"t\n!GetNewIngestDataframeInfoResponse\x12\x17\n\x07\x64\x66_path\x18\x01 \x01(\tR\x06\x64\x66Path\x12\x36\n\x18signed_url_for_df_upload\x18\x02 \x01(\tR\x14signedUrlForDfUpload\"\x88\x01\n!GetUserDeploymentSettingsResponse\x12\x63\n\x18user_deployment_settings\x18\x01 \x01(\x0b\x32).tecton_proto.data.UserDeploymentSettingsR\x16userDeploymentSettings\"\xc5\x01\n#UpdateUserDeploymentSettingsRequest\x12\x63\n\x18user_deployment_settings\x18\x01 \x01(\x0b\x32).tecton_proto.data.UserDeploymentSettingsR\x16userDeploymentSettings\x12\x39\n\nfield_mask\x18\x02 \x01(\x0b\x32\x1a.google.protobuf.FieldMaskR\tfieldMask\"e\n$UpdateUserDeploymentSettingsResponse\x12\x18\n\x07success\x18\x01 \x01(\x08R\x07success\x12#\n\rerror_message\x18\x02 \x01(\tR\x0c\x65rrorMessage\"n\n%GetInternalSparkClusterStatusResponse\x12\x45\n\x06status\x18\x01 \x01(\x0b\x32-.tecton_proto.data.InternalSparkClusterStatusR\x06status\"k\n\x1cGetDeleteEntitiesInfoRequest\x12K\n\x15\x66\x65\x61ture_definition_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x13\x66\x65\x61tureDefinitionId\"\xca\x01\n\x1dGetDeleteEntitiesInfoResponse\x12\x17\n\x07\x64\x66_path\x18\x01 \x01(\tR\x06\x64\x66Path\x12\x43\n\x1fsigned_url_for_df_upload_online\x18\x03 \x01(\tR\x1asignedUrlForDfUploadOnline\x12\x45\n signed_url_for_df_upload_offline\x18\x04 \x01(\tR\x1bsignedUrlForDfUploadOfflineJ\x04\x08\x02\x10\x03\"\xf3\x01\n\x15\x44\x65leteEntitiesRequest\x12@\n\x0b\x66\x63o_locator\x18\x01 \x01(\x0b\x32\x1f.tecton_proto.common.FcoLocatorR\nfcoLocator\x12\x31\n\x15online_join_keys_path\x18\x02 \x01(\tR\x12onlineJoinKeysPath\x12\x33\n\x16offline_join_keys_path\x18\x03 \x01(\tR\x13offlineJoinKeysPath\x12\x16\n\x06online\x18\x04 \x01(\x08R\x06online\x12\x18\n\x07offline\x18\x05 \x01(\x08R\x07offline\"\x18\n\x16\x44\x65leteEntitiesResponse\"\xf9\x01\n\x16GetHiveMetadataRequest\x12S\n\x06\x61\x63tion\x18\x01 \x01(\x0e\x32;.tecton_proto.metadataservice.GetHiveMetadataRequest.ActionR\x06\x61\x63tion\x12\x1a\n\x08\x64\x61tabase\x18\x02 \x01(\tR\x08\x64\x61tabase\x12\x14\n\x05table\x18\x03 \x01(\tR\x05table\"X\n\x06\x41\x63tion\x12\x19\n\x15\x41\x43TION_LIST_DATABASES\x10\x00\x12\x16\n\x12\x41\x43TION_LIST_TABLES\x10\x01\x12\x1b\n\x17\x41\x43TION_GET_TABLE_SCHEMA\x10\x02\"\xd1\x02\n\x17GetHiveMetadataResponse\x12\x18\n\x07success\x18\x01 \x01(\x08R\x07success\x12#\n\rerror_message\x18\x02 \x01(\tR\x0c\x65rrorMessage\x12\x41\n\tdatabases\x18\x03 \x01(\x0b\x32!.tecton_proto.data.ListHiveResultH\x00R\tdatabases\x12;\n\x06tables\x18\x04 \x01(\x0b\x32!.tecton_proto.data.ListHiveResultH\x00R\x06tables\x12=\n\x07\x63olumns\x18\x05 \x01(\x0b\x32!.tecton_proto.data.ListHiveResultH\x00R\x07\x63olumns\x12.\n\x13\x64\x65\x62ug_error_message\x18\x06 \x01(\tR\x11\x64\x65\x62ugErrorMessageB\x08\n\x06result\"K\n\x16GetFileMetadataRequest\x12\x10\n\x03uri\x18\x01 \x01(\tR\x03uri\x12\x1f\n\x0b\x66ile_format\x18\x02 \x01(\tR\nfileFormat\"\xab\x01\n\x17GetFileMetadataResponse\x12\x18\n\x07success\x18\x01 \x01(\x08R\x07success\x12#\n\rerror_message\x18\x02 \x01(\tR\x0c\x65rrorMessage\x12!\n\x0c\x63olumn_names\x18\x03 \x03(\tR\x0b\x63olumnNames\x12.\n\x13\x64\x65\x62ug_error_message\x18\x04 \x01(\tR\x11\x64\x65\x62ugErrorMessage\"R\n\x13ValidateFcosRequest\x12\x35\n\x08\x66\x63o_args\x18\x02 \x03(\x0b\x32\x1a.tecton_proto.args.FcoArgsR\x07\x66\x63oArgsJ\x04\x08\x01\x10\x02\"\xe6\x01\n\x14ValidateFcosResponse\x12\x18\n\x07success\x18\x01 \x01(\x08R\x07success\x12P\n\x11validation_result\x18\x02 \x01(\x0b\x32#.tecton_proto.data.ValidationResultR\x10validationResult\x12\x32\n\x15\x64isplay_error_message\x18\x04 \x01(\tR\x13\x64isplayErrorMessage\x12.\n\x13\x64\x65\x62ug_error_message\x18\x03 \x01(\tR\x11\x64\x65\x62ugErrorMessage\"\x95\x01\n\x17ValidateLocalFcoRequest\x12Y\n\x12validation_request\x18\x01 \x01(\x0b\x32*.tecton_proto.validation.ValidationRequestR\x11validationRequest\x12\x1f\n\x0bsdk_version\x18\x02 \x01(\tR\nsdkVersion\"\x9c\x01\n\x18ValidateLocalFcoResponse\x12\x18\n\x07success\x18\x01 \x01(\x08R\x07success\x12P\n\x11validation_result\x18\x02 \x01(\x0b\x32#.tecton_proto.data.ValidationResultR\x10validationResult\x12\x14\n\x05\x65rror\x18\x03 \x01(\tR\x05\x65rror\":\n\x1aGetOnboardingStatusRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\"\x9c\x02\n\x1bGetOnboardingStatusResponse\x12N\n\x0esetup_platform\x18\x03 \x01(\x0e\x32\'.tecton_proto.data.OnboardingStatusEnumR\rsetupPlatform\x12W\n\x13\x63reate_feature_view\x18\x01 \x01(\x0e\x32\'.tecton_proto.data.OnboardingStatusEnumR\x11\x63reateFeatureView\x12T\n\x11\x66inish_onboarding\x18\x02 \x01(\x0e\x32\'.tecton_proto.data.OnboardingStatusEnumR\x10\x66inishOnboarding\"]\n\x1aSetOnboardingStatusRequest\x12?\n\x06status\x18\x01 \x01(\x0e\x32\'.tecton_proto.data.OnboardingStatusEnumR\x06status\"\x92\x01\n\"GetDataPlatformSetupStatusResponse\x12&\n\x0esetupCompleted\x18\x01 \x01(\x08R\x0esetupCompleted\x12\x44\n\x05tasks\x18\x02 \x03(\x0b\x32..tecton_proto.data.DataPlatformSetupTaskStatusR\x05tasks\"7\n#GetRetrieveFeaturesNotebookResponse\x12\x10\n\x03url\x18\x01 \x01(\tR\x03url\"i\n\x1dGetObservabilityConfigRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12*\n\x11\x66\x65\x61ture_view_name\x18\x02 \x01(\tR\x0f\x66\x65\x61tureViewName\"\xa5\x01\n\x1eGetObservabilityConfigResponse\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12*\n\x11\x66\x65\x61ture_view_name\x18\x02 \x01(\tR\x0f\x66\x65\x61tureViewName\x12\x39\n\x19is_dataobs_metric_enabled\x18\x03 \x01(\x08R\x16isDataobsMetricEnabled\"\x93\x02\n\x12QueryMetricRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12*\n\x11\x66\x65\x61ture_view_name\x18\x02 \x01(\tR\x0f\x66\x65\x61tureViewName\x12\x41\n\x0bmetric_type\x18\x03 \x01(\x0e\x32 .tecton_proto.dataobs.MetricTypeR\nmetricType\x12\x39\n\nstart_time\x18\x04 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\tstartTime\x12\x35\n\x08\x65nd_time\x18\x05 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x07\x65ndTime\"\xf5\x03\n\x13QueryMetricResponse\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12*\n\x11\x66\x65\x61ture_view_name\x18\x02 \x01(\tR\x0f\x66\x65\x61tureViewName\x12\x41\n\x0bmetric_type\x18\x03 \x01(\x0e\x32 .tecton_proto.dataobs.MetricTypeR\nmetricType\x12V\n\x1ametric_data_point_interval\x18\x05 \x01(\x0b\x32\x19.google.protobuf.DurationR\x17metricDataPointInterval\x12H\n\x12\x61ligned_start_time\x18\x07 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x10\x61lignedStartTime\x12\x44\n\x10\x61ligned_end_time\x18\x08 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x0e\x61lignedEndTime\x12\x46\n\x0bmetric_data\x18\x04 \x03(\x0b\x32%.tecton_proto.dataobs.MetricDataPointR\nmetricData\x12!\n\x0c\x63olumn_names\x18\x06 \x03(\tR\x0b\x63olumnNames\"\x89\x01\n\"GetFeatureValidationHistoryRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12*\n\x11\x66\x65\x61ture_view_name\x18\x02 \x01(\tR\x0f\x66\x65\x61tureViewName\x12\x19\n\x05limit\x18\x03 \x01(\x05:\x03\x31\x30\x30R\x05limit\"\xb2\x01\n#GetFeatureValidationHistoryResponse\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12*\n\x11\x66\x65\x61ture_view_name\x18\x02 \x01(\tR\x0f\x66\x65\x61tureViewName\x12\x41\n\x07results\x18\x03 \x03(\x0b\x32\'.tecton_proto.dataobs.ExpectationResultR\x07results\"\x80\x04\n!GetFeatureValidationResultRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12N\n\x15validation_start_time\x18\x02 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x13validationStartTime\x12J\n\x13validation_end_time\x18\x03 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x11validationEndTime\x12\x39\n\x19\x66ilter_feature_view_names\x18\x04 \x03(\tR\x16\x66ilterFeatureViewNames\x12\x38\n\x18\x66ilter_expectation_names\x18\x05 \x03(\tR\x16\x66ilterExpectationNames\x12[\n\x13\x66ilter_result_types\x18\x06 \x03(\x0e\x32+.tecton_proto.dataobs.ExpectationResultEnumR\x11\x66ilterResultTypes\x12O\n\npagination\x18\x07 \x01(\x0b\x32/.tecton_proto.metadataservice.PaginationRequestR\npagination\"\x8c\x01\n\"GetFeatureValidationResultResponse\x12#\n\rtotal_results\x18\x01 \x01(\x05R\x0ctotalResults\x12\x41\n\x07results\x18\x02 \x03(\x0b\x32\'.tecton_proto.dataobs.ExpectationResultR\x07results\"\xde\x01\n\"GetFeatureValidationSummaryRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12N\n\x15validation_start_time\x18\x02 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x13validationStartTime\x12J\n\x13validation_end_time\x18\x03 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x11validationEndTime\"\xba\x02\n#GetFeatureValidationSummaryResponse\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12N\n\x15validation_start_time\x18\x02 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x13validationStartTime\x12J\n\x13validation_end_time\x18\x03 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x11validationEndTime\x12Y\n\x11workspace_summary\x18\x04 \x01(\x0b\x32,.tecton_proto.dataobs.WorkspaceResultSummaryR\x10workspaceSummary\"6\n\x0eGetUserRequest\x12\x0e\n\x02id\x18\x01 \x01(\tR\x02id\x12\x14\n\x05\x65mail\x18\x02 \x01(\tR\x05\x65mail\"C\n\x0fGetUserResponse\x12\x30\n\x04user\x18\x01 \x01(\x0b\x32\x1c.tecton_proto.auth.UserBasicR\x04user\"\x1f\n\x1dGetFeatureServerConfigRequest\"\x90\x01\n\x1eGetFeatureServerConfigResponse\x12\"\n\x0c\x63urrentCount\x18\x01 \x01(\rR\x0c\x63urrentCount\x12&\n\x0e\x61vailableCount\x18\x02 \x01(\rR\x0e\x61vailableCount\x12\"\n\x0c\x64\x65siredCount\x18\x03 \x01(\rR\x0c\x64\x65siredCount\"5\n\x1dSetFeatureServerConfigRequest\x12\x14\n\x05\x63ount\x18\x01 \x01(\rR\x05\x63ount*>\n\rSortDirection\x12\x10\n\x0cSORT_UNKNOWN\x10\x00\x12\x0c\n\x08SORT_ASC\x10\x01\x12\r\n\tSORT_DESC\x10\x02\x32\xac\x97\x01\n\x0fMetadataService\x12\xec\x01\n\x11GetFeatureService\x12\x36.tecton_proto.metadataservice.GetFeatureServiceRequest\x1a\x37.tecton_proto.metadataservice.GetFeatureServiceResponse\"f\x82\xd3\xe4\x93\x02-\"(/v1/metadata-service/get-feature-service:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\xfd\x01\n\x15GetAllFeatureServices\x12:.tecton_proto.metadataservice.GetAllFeatureServicesRequest\x1a;.tecton_proto.metadataservice.GetAllFeatureServicesResponse\"k\x82\xd3\xe4\x93\x02\x32\"-/v1/metadata-service/get-all-feature-services:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\x98\x02\n\x18GetFeatureServiceSummary\x12=.tecton_proto.metadataservice.GetFeatureServiceSummaryRequest\x1a>.tecton_proto.metadataservice.GetFeatureServiceSummaryResponse\"}\x82\xd3\xe4\x93\x02\x35\"0/v1/metadata-service/get-feature-service-summary:\x01*\xa2\xbc\xe6\xc0\x05<\x1a\r\x08\x01\x12\tworkspace*+tecton/workspace#read_materialization_state\x12\xf7\x01\n\x10GetServingStatus\x12\x35.tecton_proto.metadataservice.GetServingStatusRequest\x1a\x36.tecton_proto.metadataservice.GetServingStatusResponse\"t\x82\xd3\xe4\x93\x02,\"\'/v1/metadata-service/get-serving-status:\x01*\xa2\xbc\xe6\xc0\x05<\x1a\r\x08\x01\x12\tworkspace*+tecton/workspace#read_materialization_state\x12\x96\x02\n\x17GetFVServingStatusForFS\x12<.tecton_proto.metadataservice.GetFVServingStatusForFSRequest\x1a=.tecton_proto.metadataservice.GetFVServingStatusForFSResponse\"~\x82\xd3\xe4\x93\x02\x36\"1/v1/metadata-service/get-fv-serving-status-for-fs:\x01*\xa2\xbc\xe6\xc0\x05<\x1a\r\x08\x01\x12\tworkspace*+tecton/workspace#read_materialization_state\x12\x90\x02\n\x16GetAllFeatureFreshness\x12;.tecton_proto.metadataservice.GetAllFeatureFreshnessRequest\x1a<.tecton_proto.metadataservice.GetAllFeatureFreshnessResponse\"{\x82\xd3\xe4\x93\x02\x33\"./v1/metadata-service/get-all-feature-freshness:\x01*\xa2\xbc\xe6\xc0\x05<\x1a\r\x08\x01\x12\tworkspace*+tecton/workspace#read_materialization_state\x12\x90\x02\n\x13GetFeatureFreshness\x12\x38.tecton_proto.metadataservice.GetFeatureFreshnessRequest\x1a\x39.tecton_proto.metadataservice.GetFeatureFreshnessResponse\"\x83\x01\x82\xd3\xe4\x93\x02/\"*/v1/metadata-service/get-feature-freshness:\x01*\xa2\xbc\xe6\xc0\x05H\x1a\x19\x08\x01\x12\x15\x66\x63o_locator.workspace*+tecton/workspace#read_materialization_state\x12\xdd\x01\n\x18GetMaterializationStatus\x12=.tecton_proto.metadataservice.GetMaterializationStatusRequest\x1a>.tecton_proto.metadataservice.GetMaterializationStatusResponse\"B\x82\xd3\xe4\x93\x02\x34\"//v1/metadata-service/get-materialization-status:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xac\x02\n+GetAllMaterializationStatusInLiveWorkspaces\x12P.tecton_proto.metadataservice.GetAllMaterializationStatusInLiveWorkspacesRequest\x1aQ.tecton_proto.metadataservice.GetAllMaterializationStatusInLiveWorkspacesResponse\"X\x82\xd3\xe4\x93\x02J\"E/v1/metadata-service/get-all-materialization-status-in-live-workspace:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\x94\x01\n\x07GetJobs\x12,.tecton_proto.metadataservice.GetJobsRequest\x1a-.tecton_proto.metadataservice.GetJobsResponse\",\x82\xd3\xe4\x93\x02\x1e\"\x19/v1/metadata-service/jobs:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xf2\x01\n\x1d\x46orceRetryMaterializationTask\x12\x42.tecton_proto.metadataservice.ForceRetryMaterializationTaskRequest\x1a\x43.tecton_proto.metadataservice.ForceRetryMaterializationTaskResponse\"H\x82\xd3\xe4\x93\x02:\"5/v1/metadata-service/force-retry-materialization-task:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xe0\x01\n\x0eGetSparkConfig\x12\x33.tecton_proto.metadataservice.GetSparkConfigRequest\x1a\x34.tecton_proto.metadataservice.GetSparkConfigResponse\"c\x82\xd3\xe4\x93\x02*\"%/v1/metadata-service/get-spark-config:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\xae\x02\n!GetMetricAndExpectationDefinition\x12\x46.tecton_proto.metadataservice.GetMetricAndExpectationDefinitionRequest\x1aG.tecton_proto.metadataservice.GetMetricAndExpectationDefinitionResponse\"x\x82\xd3\xe4\x93\x02?\":/v1/metadata-service/get-metric-and-expectation-definition:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\xe0\x01\n\x0eGetFeatureView\x12\x33.tecton_proto.metadataservice.GetFeatureViewRequest\x1a\x34.tecton_proto.metadataservice.GetFeatureViewResponse\"c\x82\xd3\xe4\x93\x02*\"%/v1/metadata-service/get-feature-view:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\xf0\x01\n,GetMaterializingFeatureViewsInLiveWorkspaces\x12\x16.google.protobuf.Empty\x1aR.tecton_proto.metadataservice.GetMaterializingFeatureViewsInLiveWorkspacesResponse\"T\x82\xd3\xe4\x93\x02\x46\"A/v1/metadata-service/get-materializing-features-in-live-workspace:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xec\x01\n\x11QueryFeatureViews\x12\x36.tecton_proto.metadataservice.QueryFeatureViewsRequest\x1a\x37.tecton_proto.metadataservice.QueryFeatureViewsResponse\"f\x82\xd3\xe4\x93\x02-\"(/v1/metadata-service/query-feature-views:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\x99\x02\n\x15GetFeatureViewSummary\x12:.tecton_proto.metadataservice.GetFeatureViewSummaryRequest\x1a;.tecton_proto.metadataservice.GetFeatureViewSummaryResponse\"\x86\x01\x82\xd3\xe4\x93\x02\x32\"-/v1/metadata-package/get-feature-view-summary:\x01*\xa2\xbc\xe6\xc0\x05H\x1a\x19\x08\x01\x12\x15\x66\x63o_locator.workspace*+tecton/workspace#read_materialization_state\x12\x8d\x02\n\x15GetFeatureConsumption\x12:.tecton_proto.metadataservice.GetFeatureConsumptionRequest\x1a;.tecton_proto.metadataservice.GetFeatureConsumptionResponse\"{\x82\xd3\xe4\x93\x02\x36\"1/v1/metadata-service/get-feature-view-consumption:\x01*\xa2\xbc\xe6\xc0\x05\x39\x1a\x19\x08\x01\x12\x15\x66\x63o_locator.workspace*\x1ctecton/workspace#read_config\x12\xf9\x01\n\x14GetVirtualDataSource\x12\x39.tecton_proto.metadataservice.GetVirtualDataSourceRequest\x1a:.tecton_proto.metadataservice.GetVirtualDataSourceResponse\"j\x82\xd3\xe4\x93\x02\x31\",/v1/metadata-service/get-virtual-data-source:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\x8a\x02\n\x18GetAllVirtualDataSources\x12=.tecton_proto.metadataservice.GetAllVirtualDataSourcesRequest\x1a>.tecton_proto.metadataservice.GetAllVirtualDataSourcesResponse\"o\x82\xd3\xe4\x93\x02\x36\"1/v1/metadata-service/get-all-virtual-data-sources:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\x90\x01\n\x10IngestClientLogs\x12\x35.tecton_proto.metadataservice.IngestClientLogsRequest\x1a\x16.google.protobuf.Empty\"-\xa2\xbc\xe6\xc0\x05\'*%tecton/organization#read_organization\x12\xbe\x01\n\x0fIngestAnalytics\x12\x34.tecton_proto.metadataservice.IngestAnalyticsRequest\x1a\x16.google.protobuf.Empty\"]\x82\xd3\xe4\x93\x02*\"%/v1/metadata-service/ingest-analytics:\x01*\xa2\xbc\xe6\xc0\x05\'*%tecton/organization#read_organization\x12\xa2\x02\n\x1bGetVirtualDataSourceSummary\x12@.tecton_proto.metadataservice.GetVirtualDataSourceSummaryRequest\x1a\x41.tecton_proto.metadataservice.GetVirtualDataSourceSummaryResponse\"~\x82\xd3\xe4\x93\x02\x39\"4/v1/metadata-service/get-virtual-data-source-summary:\x01*\xa2\xbc\xe6\xc0\x05\x39\x1a\x19\x08\x01\x12\x15\x66\x63o_locator.workspace*\x1ctecton/workspace#read_config\x12\xc3\x01\n\x07GetFcos\x12,.tecton_proto.metadataservice.GetFcosRequest\x1a-.tecton_proto.metadataservice.GetFcosResponse\"[\x82\xd3\xe4\x93\x02\"\"\x1d/v1/metadata-service/get-fcos:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\xa1\x01\n\tGetEntity\x12..tecton_proto.metadataservice.GetEntityRequest\x1a/.tecton_proto.metadataservice.GetEntityResponse\"3\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\xe0\x01\n\x0eGetAllEntities\x12\x33.tecton_proto.metadataservice.GetAllEntitiesRequest\x1a\x34.tecton_proto.metadataservice.GetAllEntitiesResponse\"c\x82\xd3\xe4\x93\x02*\"%/v1/metadata-service/get-all-entities:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\xf4\x01\n\x10GetEntitySummary\x12\x35.tecton_proto.metadataservice.GetEntitySummaryRequest\x1a\x36.tecton_proto.metadataservice.GetEntitySummaryResponse\"q\x82\xd3\xe4\x93\x02,\"\'/v1/metadata-package/get-entity-summary:\x01*\xa2\xbc\xe6\xc0\x05\x39\x1a\x19\x08\x01\x12\x15\x66\x63o_locator.workspace*\x1ctecton/workspace#read_config\x12\xeb\x01\n\x11GetTransformation\x12\x36.tecton_proto.metadataservice.GetTransformationRequest\x1a\x37.tecton_proto.metadataservice.GetTransformationResponse\"e\x82\xd3\xe4\x93\x02,\"\'/v1/metadata-service/get-transformation:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\xfc\x01\n\x15GetAllTransformations\x12:.tecton_proto.metadataservice.GetAllTransformationsRequest\x1a;.tecton_proto.metadataservice.GetAllTransformationsResponse\"j\x82\xd3\xe4\x93\x02\x31\",/v1/metadata-service/get-all-transformations:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\x94\x02\n\x18GetTransformationSummary\x12=.tecton_proto.metadataservice.GetTransformationSummaryRequest\x1a>.tecton_proto.metadataservice.GetTransformationSummaryResponse\"y\x82\xd3\xe4\x93\x02\x34\"//v1/metadata-package/get-transformation-summary:\x01*\xa2\xbc\xe6\xc0\x05\x39\x1a\x19\x08\x01\x12\x15\x66\x63o_locator.workspace*\x1ctecton/workspace#read_config\x12\xbd\x01\n\x10\x46indFcoWorkspace\x12\x35.tecton_proto.metadataservice.FindFcoWorkspaceRequest\x1a\x36.tecton_proto.metadataservice.FindFcoWorkspaceResponse\":\x82\xd3\xe4\x93\x02,\"\'/v1/metadata-service/find-fco-workspace:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xb8\x01\n\x0eNewStateUpdate\x12\x33.tecton_proto.metadataservice.NewStateUpdateRequest\x1a\x34.tecton_proto.metadataservice.NewStateUpdateResponse\";\xa2\xbc\xe6\xc0\x05\x35\x1a\x15\x08\x01\x12\x11request.workspace*\x1ctecton/workspace#read_config\x12\xbe\x01\n\x10NewStateUpdateV2\x12\x35.tecton_proto.metadataservice.NewStateUpdateRequestV2\x1a\x36.tecton_proto.metadataservice.NewStateUpdateResponseV2\";\xa2\xbc\xe6\xc0\x05\x35\x1a\x15\x08\x01\x12\x11request.workspace*\x1ctecton/workspace#read_config\x12\xb8\x01\n\x10ValidateLocalFco\x12\x35.tecton_proto.metadataservice.ValidateLocalFcoRequest\x1a\x36.tecton_proto.metadataservice.ValidateLocalFcoResponse\"5\xa2\xbc\xe6\xc0\x05/*-tecton/organization#validate_notebook_objects\x12\xb6\x01\n\x10QueryStateUpdate\x12\x35.tecton_proto.metadataservice.QueryStateUpdateRequest\x1a\x36.tecton_proto.metadataservice.QueryStateUpdateResponse\"3\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\xbc\x01\n\x12QueryStateUpdateV2\x12\x37.tecton_proto.metadataservice.QueryStateUpdateRequestV2\x1a\x38.tecton_proto.metadataservice.QueryStateUpdateResponseV2\"3\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\x8b\x01\n\x10\x41pplyStateUpdate\x12\x35.tecton_proto.metadataservice.ApplyStateUpdateRequest\x1a\x36.tecton_proto.metadataservice.ApplyStateUpdateResponse\"\x08\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xb0\x01\n\nGetConfigs\x12\x16.google.protobuf.Empty\x1a\x30.tecton_proto.metadataservice.GetConfigsResponse\"X\x82\xd3\xe4\x93\x02%\" /v1/metadata-service/get-configs:\x01*\xa2\xbc\xe6\xc0\x05\'*%tecton/organization#read_organization\x12\xcb\x01\n\x12GetGlobalsForWebUI\x12\x16.google.protobuf.Empty\x1a\x38.tecton_proto.metadataservice.GetGlobalsForWebUIResponse\"c\x82\xd3\xe4\x93\x02\x30\"+/v1/metadata-service/get-globals-for-web-ui:\x01*\xa2\xbc\xe6\xc0\x05\'*%tecton/organization#read_organization\x12\x98\x01\n\x0fGetLoginConfigs\x12\x16.google.protobuf.Empty\x1a\x30.tecton_proto.metadataservice.GetConfigsResponse\";\x82\xd3\xe4\x93\x02+\"&/v1/metadata-service/get-login-configs:\x01*\xa2\xbc\xe6\xc0\x05\x04\x08\x01\x10\x01\x12?\n\x03Nop\x12\x16.google.protobuf.Empty\x1a\x16.google.protobuf.Empty\"\x08\xa2\xbc\xe6\xc0\x05\x02\x10\x01\x12\xe3\x01\n\x19GetStateUpdatePlanSummary\x12>.tecton_proto.metadataservice.GetStateUpdatePlanSummaryRequest\x1a?.tecton_proto.metadataservice.GetStateUpdatePlanSummaryResponse\"E\x82\xd3\xe4\x93\x02\x37\"2/v1/metadata-service/get-state-update-plan-summary:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xed\x01\n\x11GetStateUpdateLog\x12\x36.tecton_proto.metadataservice.GetStateUpdateLogRequest\x1a\x37.tecton_proto.metadataservice.GetStateUpdateLogResponse\"g\x82\xd3\xe4\x93\x02.\")/v1/metadata-service/get-state-update-log:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\xb0\x01\n\x0eGetRestoreInfo\x12\x33.tecton_proto.metadataservice.GetRestoreInfoRequest\x1a\x34.tecton_proto.metadataservice.GetRestoreInfoResponse\"3\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\x8e\x02\n\x0f\x43reateWorkspace\x12\x34.tecton_proto.metadataservice.CreateWorkspaceRequest\x1a\x16.google.protobuf.Empty\"\xac\x01\xa2\xbc\xe6\xc0\x05v*$tecton/organization#create_workspace2N\n\x1b\x63\x61pabilities.materializable\x12\x04true\x1a)tecton/organization#create_workspace_live\x82\xd3\xe4\x93\x02*\"%/v1/metadata-service/create-workspace:\x01*\x12\x99\x01\n\x0f\x44\x65leteWorkspace\x12\x34.tecton_proto.metadataservice.DeleteWorkspaceRequest\x1a\x16.google.protobuf.Empty\"8\xa2\xbc\xe6\xc0\x05\x32\x1a\r\x08\x01\x12\tworkspace*!tecton/workspace#delete_workspace\x12\xb4\x01\n\x0eListWorkspaces\x12\x33.tecton_proto.metadataservice.ListWorkspacesRequest\x1a\x34.tecton_proto.metadataservice.ListWorkspacesResponse\"7\x82\xd3\xe4\x93\x02)\"$/v1/metadata-service/list-workspaces:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xdf\x01\n\x0cGetWorkspace\x12\x31.tecton_proto.metadataservice.GetWorkspaceRequest\x1a\x32.tecton_proto.metadataservice.GetWorkspaceResponse\"h\x82\xd3\xe4\x93\x02\'\"\"/v1/metadata-service/get-workspace:\x01*\xa2\xbc\xe6\xc0\x05\x35\x1a\x12\x08\x01\x12\x0eworkspace_name*\x1ftecton/workspace#read_workspace\x12\xe5\x01\n\x10IntrospectApiKey\x12\x35.tecton_proto.metadataservice.IntrospectApiKeyRequest\x1a\x36.tecton_proto.metadataservice.IntrospectApiKeyResponse\"b\x82\xd3\xe4\x93\x02,\"\'/v1/metadata-service/introspect-api-key:\x01*\xa2\xbc\xe6\xc0\x05**(tecton/organization#list_service_account\x12\xf7\x01\n\x14\x43reateServiceAccount\x12\x39.tecton_proto.metadataservice.CreateServiceAccountRequest\x1a:.tecton_proto.metadataservice.CreateServiceAccountResponse\"h\x82\xd3\xe4\x93\x02\x30\"+/v1/metadata-service/create-service-account:\x01*\xa2\xbc\xe6\xc0\x05,**tecton/organization#create_service_account\x12\xec\x01\n\x12GetServiceAccounts\x12\x37.tecton_proto.metadataservice.GetServiceAccountsRequest\x1a\x38.tecton_proto.metadataservice.GetServiceAccountsResponse\"c\x82\xd3\xe4\x93\x02-\"(/v1/metadata-service/get-service-account:\x01*\xa2\xbc\xe6\xc0\x05**(tecton/organization#list_service_account\x12\x82\x02\n\x14UpdateServiceAccount\x12\x39.tecton_proto.metadataservice.UpdateServiceAccountRequest\x1a:.tecton_proto.metadataservice.UpdateServiceAccountResponse\"s\x82\xd3\xe4\x93\x02\x30\"+/v1/metadata-service/update-service-account:\x01*\xa2\xbc\xe6\xc0\x05\x37\x1a\x06\x08\x02\x12\x02id*-tecton/service_account#manage_service_account\x12\x82\x02\n\x14\x44\x65leteServiceAccount\x12\x39.tecton_proto.metadataservice.DeleteServiceAccountRequest\x1a:.tecton_proto.metadataservice.DeleteServiceAccountResponse\"s\x82\xd3\xe4\x93\x02\x30\"+/v1/metadata-service/delete-service-account:\x01*\xa2\xbc\xe6\xc0\x05\x37\x1a\x06\x08\x02\x12\x02id*-tecton/service_account#manage_service_account\x12\xef\x01\n\x0c\x43reateApiKey\x12\x31.tecton_proto.metadataservice.CreateApiKeyRequest\x1a\x32.tecton_proto.metadataservice.CreateApiKeyResponse\"x\xa2\xbc\xe6\xc0\x05r**tecton/organization#create_service_account2B\n\x08is_admin\x12\x04true\x1a\x30tecton/organization#create_service_account_admin8\x01\x12\x9a\x01\n\x0c\x44\x65leteApiKey\x12\x31.tecton_proto.metadataservice.DeleteApiKeyRequest\x1a\x16.google.protobuf.Empty\"?\xa2\xbc\xe6\xc0\x05\x39\x1a\x06\x08\x02\x12\x02id*-tecton/service_account#manage_service_account8\x01\x12\xa4\x01\n\x0bListApiKeys\x12\x30.tecton_proto.metadataservice.ListApiKeysRequest\x1a\x31.tecton_proto.metadataservice.ListApiKeysResponse\"0\xa2\xbc\xe6\xc0\x05**(tecton/organization#list_service_account\x12\xda\x01\n\x1b\x43reateSavedFeatureDataFrame\x12@.tecton_proto.metadataservice.CreateSavedFeatureDataFrameRequest\x1a\x41.tecton_proto.metadataservice.CreateSavedFeatureDataFrameResponse\"6\xa2\xbc\xe6\xc0\x05\x30\x1a\r\x08\x01\x12\tworkspace*\x1ftecton/workspace#update_dataset\x12\xaf\x01\n\x1c\x41rchiveSavedFeatureDataFrame\x12\x41.tecton_proto.metadataservice.ArchiveSavedFeatureDataFrameRequest\x1a\x42.tecton_proto.metadataservice.ArchiveSavedFeatureDataFrameResponse\"\x08\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xcf\x01\n\x18GetSavedFeatureDataFrame\x12=.tecton_proto.metadataservice.GetSavedFeatureDataFrameRequest\x1a>.tecton_proto.metadataservice.GetSavedFeatureDataFrameResponse\"4\xa2\xbc\xe6\xc0\x05.\x1a\r\x08\x01\x12\tworkspace*\x1dtecton/workspace#read_dataset\x12\x9b\x02\n\x1cGetAllSavedFeatureDataFrames\x12\x41.tecton_proto.metadataservice.GetAllSavedFeatureDataFramesRequest\x1a\x42.tecton_proto.metadataservice.GetAllSavedFeatureDataFramesResponse\"t\x82\xd3\xe4\x93\x02:\"5/v1/metadata-service/get-all-saved-feature-dataframes:\x01*\xa2\xbc\xe6\xc0\x05.\x1a\r\x08\x01\x12\tworkspace*\x1dtecton/workspace#read_dataset\x12\xa6\x01\n\x19GetNewIngestDataframeInfo\x12>.tecton_proto.metadataservice.GetNewIngestDataframeInfoRequest\x1a?.tecton_proto.metadataservice.GetNewIngestDataframeInfoResponse\"\x08\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xb7\x01\n\x0fIngestDataframe\x12\x34.tecton_proto.metadataservice.IngestDataframeRequest\x1a\x35.tecton_proto.metadataservice.IngestDataframeResponse\"7\xa2\xbc\xe6\xc0\x05\x31\x1a\r\x08\x01\x12\tworkspace* tecton/workspace#ingest_features\x12\xcd\x01\n\x13GetClusterAdminInfo\x12\x16.google.protobuf.Empty\x1a\x39.tecton_proto.metadataservice.GetClusterAdminInfoResponse\"c\x82\xd3\xe4\x93\x02\x30\"+/v1/metadata-service/get-cluster-admin-info:\x01*\xa2\xbc\xe6\xc0\x05\'*%tecton/organization#read_organization\x12\xe0\x01\n\x11\x43reateClusterUser\x12\x36.tecton_proto.metadataservice.CreateClusterUserRequest\x1a\x37.tecton_proto.metadataservice.CreateClusterUserResponse\"Z\x82\xd3\xe4\x93\x02-\"(/v1/metadata-service/create-cluster-user:\x01*\xa2\xbc\xe6\xc0\x05!*\x1ftecton/organization#create_user\x12\xe0\x01\n\x11\x44\x65leteClusterUser\x12\x36.tecton_proto.metadataservice.DeleteClusterUserRequest\x1a\x37.tecton_proto.metadataservice.DeleteClusterUserResponse\"Z\x82\xd3\xe4\x93\x02-\"(/v1/metadata-service/delete-cluster-user:\x01*\xa2\xbc\xe6\xc0\x05!*\x1ftecton/organization#manage_user\x12\xe0\x01\n\x11\x43lusterUserAction\x12\x36.tecton_proto.metadataservice.ClusterUserActionRequest\x1a\x37.tecton_proto.metadataservice.ClusterUserActionResponse\"Z\x82\xd3\xe4\x93\x02-\"(/v1/metadata-service/cluster-user-action:\x01*\xa2\xbc\xe6\xc0\x05!*\x1ftecton/organization#manage_user\x12\xdf\x01\n\x19GetUserDeploymentSettings\x12\x16.google.protobuf.Empty\x1a?.tecton_proto.metadataservice.GetUserDeploymentSettingsResponse\"i\x82\xd3\xe4\x93\x02\x36\"1/v1/metadata-service/get-user-deployment-settings:\x01*\xa2\xbc\xe6\xc0\x05\'*%tecton/organization#read_organization\x12\x9c\x02\n\x1cUpdateUserDeploymentSettings\x12\x41.tecton_proto.metadataservice.UpdateUserDeploymentSettingsRequest\x1a\x42.tecton_proto.metadataservice.UpdateUserDeploymentSettingsResponse\"u\x82\xd3\xe4\x93\x02\x39\"4/v1/metadata-service/update-user-deployment-settings:\x01*\xa2\xbc\xe6\xc0\x05\x30*.tecton/organization#manage_deployment_settings\x12\xec\x01\n\x1dGetInternalSparkClusterStatus\x12\x16.google.protobuf.Empty\x1a\x43.tecton_proto.metadataservice.GetInternalSparkClusterStatusResponse\"n\x82\xd3\xe4\x93\x02;\"6/v1/metadata-service/get-internal-spark-cluster-status:\x01*\xa2\xbc\xe6\xc0\x05\'*%tecton/organization#read_organization\x12\x9a\x01\n\x15GetDeleteEntitiesInfo\x12:.tecton_proto.metadataservice.GetDeleteEntitiesInfoRequest\x1a;.tecton_proto.metadataservice.GetDeleteEntitiesInfoResponse\"\x08\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xc0\x01\n\x0e\x44\x65leteEntities\x12\x33.tecton_proto.metadataservice.DeleteEntitiesRequest\x1a\x34.tecton_proto.metadataservice.DeleteEntitiesResponse\"C\xa2\xbc\xe6\xc0\x05=\x1a\x19\x08\x01\x12\x15\x66\x63o_locator.workspace* tecton/workspace#delete_features\x12\xde\x01\n\x0fGetHiveMetadata\x12\x34.tecton_proto.metadataservice.GetHiveMetadataRequest\x1a\x35.tecton_proto.metadataservice.GetHiveMetadataResponse\"^\x82\xd3\xe4\x93\x02+\"&/v1/metadata-service/get-hive-metadata:\x01*\xa2\xbc\xe6\xc0\x05\'*%tecton/organization#read_organization\x12\xde\x01\n\x0fGetFileMetadata\x12\x34.tecton_proto.metadataservice.GetFileMetadataRequest\x1a\x35.tecton_proto.metadataservice.GetFileMetadataResponse\"^\x82\xd3\xe4\x93\x02+\"&/v1/metadata-service/get-file-metadata:\x01*\xa2\xbc\xe6\xc0\x05\'*%tecton/organization#read_organization\x12\xda\x01\n\x0cValidateFcos\x12\x31.tecton_proto.metadataservice.ValidateFcosRequest\x1a\x32.tecton_proto.metadataservice.ValidateFcosResponse\"c\x82\xd3\xe4\x93\x02\'\"\"/v1/metadata-service/validate-fcos:\x01*\xa2\xbc\xe6\xc0\x05\x30*.tecton/organization#manage_deployment_settings\x12\xf7\x01\n\x13GetOnboardingStatus\x12\x38.tecton_proto.metadataservice.GetOnboardingStatusRequest\x1a\x39.tecton_proto.metadataservice.GetOnboardingStatusResponse\"k\x82\xd3\xe4\x93\x02/\"*/v1/metadata-service/get-onboarding-status:\x01*\xa2\xbc\xe6\xc0\x05\x30\x1a\r\x08\x01\x12\tworkspace*\x1ftecton/workspace#read_workspace\x12\xcb\x01\n\x13SetOnboardingStatus\x12\x38.tecton_proto.metadataservice.SetOnboardingStatusRequest\x1a\x16.google.protobuf.Empty\"b\x82\xd3\xe4\x93\x02/\"*/v1/metadata-service/set-onboarding-status:\x01*\xa2\xbc\xe6\xc0\x05\'*%tecton/organization#read_organization\x12\xe3\x01\n\x1aGetDataPlatformSetupStatus\x12\x16.google.protobuf.Empty\x1a@.tecton_proto.metadataservice.GetDataPlatformSetupStatusResponse\"k\x82\xd3\xe4\x93\x02\x38\"3/v1/metadata-service/get-data-platform-setup-status:\x01*\xa2\xbc\xe6\xc0\x05\'*%tecton/organization#read_organization\x12\xe5\x01\n\x1bGetRetrieveFeaturesNotebook\x12\x16.google.protobuf.Empty\x1a\x41.tecton_proto.metadataservice.GetRetrieveFeaturesNotebookResponse\"k\x82\xd3\xe4\x93\x02\x38\"3/v1/metadata-service/get-retrieve-features-notebook:\x01*\xa2\xbc\xe6\xc0\x05\'*%tecton/organization#read_organization\x12\x80\x02\n\x16GetObservabilityConfig\x12;.tecton_proto.metadataservice.GetObservabilityConfigRequest\x1a<.tecton_proto.metadataservice.GetObservabilityConfigResponse\"k\x82\xd3\xe4\x93\x02\x32\"-/v1/metadata-service/get-observability-config:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\xd3\x01\n\x0bQueryMetric\x12\x30.tecton_proto.metadataservice.QueryMetricRequest\x1a\x31.tecton_proto.metadataservice.QueryMetricResponse\"_\x82\xd3\xe4\x93\x02&\"!/v1/metadata-service/query-metric:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\x95\x02\n\x1bGetFeatureValidationHistory\x12@.tecton_proto.metadataservice.GetFeatureValidationHistoryRequest\x1a\x41.tecton_proto.metadataservice.GetFeatureValidationHistoryResponse\"q\x82\xd3\xe4\x93\x02\x38\"3/v1/metadata-service/get-feature-validation-history:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\x95\x02\n\x1bGetFeatureValidationSummary\x12@.tecton_proto.metadataservice.GetFeatureValidationSummaryRequest\x1a\x41.tecton_proto.metadataservice.GetFeatureValidationSummaryResponse\"q\x82\xd3\xe4\x93\x02\x38\"3/v1/metadata-service/get-feature-validation-summary:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_metric\x12\x91\x02\n\x1aGetFeatureValidationResult\x12?.tecton_proto.metadataservice.GetFeatureValidationResultRequest\x1a@.tecton_proto.metadataservice.GetFeatureValidationResultResponse\"p\x82\xd3\xe4\x93\x02\x37\"2/v1/metadata-service/get-feature-validation-result:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_metric\x12\xfb\x01\n\x16GetFeatureServerConfig\x12;.tecton_proto.metadataservice.GetFeatureServerConfigRequest\x1a<.tecton_proto.metadataservice.GetFeatureServerConfigResponse\"f\x82\xd3\xe4\x93\x02\x33\"./v1/metadata-service/get-feature-server-config:\x01*\xa2\xbc\xe6\xc0\x05\'*%tecton/organization#read_organization\x12\xfe\x01\n\x16SetFeatureServerConfig\x12;.tecton_proto.metadataservice.SetFeatureServerConfigRequest\x1a<.tecton_proto.metadataservice.GetFeatureServerConfigResponse\"i\x82\xd3\xe4\x93\x02\x33\"./v1/metadata-service/set-feature-server-config:\x01*\xa2\xbc\xe6\xc0\x05**(tecton/organization#scale_feature_server\x12\xb5\x01\n\x07GetUser\x12,.tecton_proto.metadataservice.GetUserRequest\x1a-.tecton_proto.metadataservice.GetUserResponse\"M\x82\xd3\xe4\x93\x02\"\"\x1d/v1/metadata-service/get-user:\x01*\xa2\xbc\xe6\xc0\x05\x1f*\x1dtecton/organization#list_userBg\n\x1a\x63om.tecton.metadataserviceB\x14MetadataServiceProtoP\x01Z1github.com/tecton-ai/tecton_proto/metadataservice')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n3tecton_proto/metadataservice/metadata_service.proto\x12\x1ctecton_proto.metadataservice\x1a\x1cgoogle/api/annotations.proto\x1a google/protobuf/descriptor.proto\x1a&tecton_proto/data/hive_metastore.proto\x1a\x1egoogle/protobuf/duration.proto\x1a\x1bgoogle/protobuf/empty.proto\x1a\x1fgoogle/protobuf/timestamp.proto\x1a google/protobuf/field_mask.proto\x1a!tecton_proto/auth/principal.proto\x1a\x1etecton_proto/data/entity.proto\x1a\x1ftecton_proto/auth/service.proto\x1a\x1btecton_proto/data/fco.proto\x1a$tecton_proto/data/feature_view.proto\x1a\'tecton_proto/data/feature_service.proto\x1a$tecton_proto/data/state_update.proto\x1a&tecton_proto/data/tecton_api_key.proto\x1a\x30tecton_proto/data/user_deployment_settings.proto\x1a\x1ctecton_proto/data/user.proto\x1a&tecton_proto/dataobs/expectation.proto\x1a*tecton_proto/consumption/consumption.proto\x1a!tecton_proto/dataobs/metric.proto\x1a%tecton_proto/dataobs/validation.proto\x1a\x1ctecton_proto/common/id.proto\x1a%tecton_proto/common/fco_locator.proto\x1a&tecton_proto/common/spark_schema.proto\x1a.tecton_proto/data/materialization_status.proto\x1a(tecton_proto/data/freshness_status.proto\x1a&tecton_proto/data/serving_status.proto\x1a\x35tecton_proto/data/internal_spark_cluster_status.proto\x1a\x1ftecton_proto/data/summary.proto\x1a\x30tecton_proto/data/saved_feature_data_frame.proto\x1a\"tecton_proto/data/onboarding.proto\x1a&tecton_proto/data/transformation.proto\x1a+tecton_proto/data/virtual_data_source.proto\x1a!tecton_proto/data/workspace.proto\x1a+tecton_proto/amplitude/client_logging.proto\x1a&tecton_proto/amplitude/amplitude.proto\x1a\'tecton_proto/validation/validator.proto\x1a\x39tecton_proto/materialization/materialization_states.proto\x1a\x30tecton_proto/materialization/spark_cluster.proto\"\x90\x01\n\nJobsKeySet\x12\x39\n\nupdated_at\x18\x01 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\tupdatedAt\x12\x1e\n\ncomparison\x18\x03 \x01(\x03R\ncomparison\x12\'\n\x02id\x18\x02 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x02id\"\xd0\x01\n\x11PaginationRequest\x12\x12\n\x04page\x18\x01 \x01(\rR\x04page\x12\x19\n\x08per_page\x18\x02 \x01(\rR\x07perPage\x12\x19\n\x08sort_key\x18\x03 \x01(\tR\x07sortKey\x12R\n\x0esort_direction\x18\x04 \x01(\x0e\x32+.tecton_proto.metadataservice.SortDirectionR\rsortDirection\x12\x1d\n\npage_token\x18\x05 \x01(\tR\tpageToken\"\xf0\x01\n\x12PaginationResponse\x12\x12\n\x04page\x18\x01 \x01(\rR\x04page\x12\x19\n\x08per_page\x18\x02 \x01(\rR\x07perPage\x12\x14\n\x05total\x18\x03 \x01(\rR\x05total\x12&\n\x0fnext_page_token\x18\x04 \x01(\tR\rnextPageToken\x12\x19\n\x08sort_key\x18\x05 \x01(\tR\x07sortKey\x12R\n\x0esort_direction\x18\x06 \x01(\x0e\x32+.tecton_proto.metadataservice.SortDirectionR\rsortDirection\"\xbd\x01\n\x15ValidationResultToken\x12\x43\n\x0fvalidation_time\x18\x01 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x0evalidationTime\x12\x34\n\tresult_id\x18\x02 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x08resultId\x12)\n\x10\x65xpectation_name\x18\x03 \x01(\tR\x0f\x65xpectationName\"\xd3\x01\n\x18GetFeatureServiceRequest\x12+\n\x11service_reference\x18\x02 \x01(\tR\x10serviceReference\x12\x1c\n\tworkspace\x18\x03 \x01(\tR\tworkspace\x12\'\n\x02id\x18\x04 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x02id\x12\x37\n\x18run_object_version_check\x18\x06 \x01(\x08R\x15runObjectVersionCheckJ\x04\x08\x01\x10\x02J\x04\x08\x05\x10\x06\"a\n\x19GetFeatureServiceResponse\x12\x44\n\rfco_container\x18\x02 \x01(\x0b\x32\x1f.tecton_proto.data.FcoContainerR\x0c\x66\x63oContainer\"<\n\x1cGetAllFeatureServicesRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\"m\n\x1dGetAllFeatureServicesResponse\x12L\n\x10\x66\x65\x61ture_services\x18\x01 \x03(\x0b\x32!.tecton_proto.data.FeatureServiceR\x0f\x66\x65\x61tureServices\"\xc7\x01\n\x1fGetFeatureServiceSummaryRequest\x12G\n\x12\x66\x65\x61ture_service_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdH\x00R\x10\x66\x65\x61tureServiceId\x12\x32\n\x14\x66\x65\x61ture_service_name\x18\x02 \x01(\tH\x00R\x12\x66\x65\x61tureServiceName\x12\x1c\n\tworkspace\x18\x03 \x01(\tR\tworkspaceB\t\n\x07locator\"\x8c\x01\n GetFeatureServiceSummaryResponse\x12\x43\n\rgeneral_items\x18\x01 \x03(\x0b\x32\x1e.tecton_proto.data.SummaryItemR\x0cgeneralItems\x12#\n\rvariant_names\x18\x03 \x03(\tR\x0cvariantNames\"f\n\"GetVirtualDataSourceSummaryRequest\x12@\n\x0b\x66\x63o_locator\x18\x01 \x01(\x0b\x32\x1f.tecton_proto.common.FcoLocatorR\nfcoLocator\"e\n#GetVirtualDataSourceSummaryResponse\x12>\n\x0b\x66\x63o_summary\x18\x01 \x01(\x0b\x32\x1d.tecton_proto.data.FcoSummaryR\nfcoSummary\"c\n\x1fGetTransformationSummaryRequest\x12@\n\x0b\x66\x63o_locator\x18\x01 \x01(\x0b\x32\x1f.tecton_proto.common.FcoLocatorR\nfcoLocator\"b\n GetTransformationSummaryResponse\x12>\n\x0b\x66\x63o_summary\x18\x01 \x01(\x0b\x32\x1d.tecton_proto.data.FcoSummaryR\nfcoSummary\"[\n\x17GetEntitySummaryRequest\x12@\n\x0b\x66\x63o_locator\x18\x01 \x01(\x0b\x32\x1f.tecton_proto.common.FcoLocatorR\nfcoLocator\"Z\n\x18GetEntitySummaryResponse\x12>\n\x0b\x66\x63o_summary\x18\x01 \x01(\x0b\x32\x1d.tecton_proto.data.FcoSummaryR\nfcoSummary\"\xd9\x01\n\x17GetServingStatusRequest\x12G\n\x12\x66\x65\x61ture_package_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdH\x00R\x10\x66\x65\x61turePackageId\x12G\n\x12\x66\x65\x61ture_service_id\x18\x02 \x01(\x0b\x32\x17.tecton_proto.common.IdH\x00R\x10\x66\x65\x61tureServiceId\x12\x1c\n\tworkspace\x18\x03 \x01(\tR\tworkspaceB\x0e\n\x0crequest_type\"\xd6\x01\n\x1eGetFVServingStatusForFSRequest\x12\x45\n\x12\x66\x65\x61ture_service_id\x18\x02 \x02(\x0b\x32\x17.tecton_proto.common.IdR\x10\x66\x65\x61tureServiceId\x12\x1c\n\tworkspace\x18\x03 \x01(\tR\tworkspace\x12O\n\npagination\x18\x04 \x01(\x0b\x32/.tecton_proto.metadataservice.PaginationRequestR\npagination\"\xe7\x01\n\x1fGetFVServingStatusForFSResponse\x12r\n\x1b\x66ull_serving_status_summary\x18\x01 \x01(\x0b\x32\x33.tecton_proto.data.FullFeatureServiceServingSummaryR\x18\x66ullServingStatusSummary\x12P\n\npagination\x18\x02 \x01(\x0b\x32\x30.tecton_proto.metadataservice.PaginationResponseR\npagination\"y\n\x18GetServingStatusResponse\x12]\n\x16serving_status_summary\x18\x05 \x01(\x0b\x32\'.tecton_proto.data.ServingStatusSummaryR\x14servingStatusSummary\"=\n\x1dGetAllFeatureFreshnessRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\"s\n\x1eGetAllFeatureFreshnessResponse\x12Q\n\x12\x66reshness_statuses\x18\x01 \x03(\x0b\x32\".tecton_proto.data.FreshnessStatusR\x11\x66reshnessStatuses\"^\n\x1aGetFeatureFreshnessRequest\x12@\n\x0b\x66\x63o_locator\x18\x01 \x01(\x0b\x32\x1f.tecton_proto.common.FcoLocatorR\nfcoLocator\"l\n\x1bGetFeatureFreshnessResponse\x12M\n\x10\x66reshness_status\x18\x01 \x01(\x0b\x32\".tecton_proto.data.FreshnessStatusR\x0f\x66reshnessStatus\"`\n\x1cGetFeatureConsumptionRequest\x12@\n\x0b\x66\x63o_locator\x18\x01 \x01(\x0b\x32\x1f.tecton_proto.common.FcoLocatorR\nfcoLocator\"u\n\x1dGetFeatureConsumptionResponse\x12T\n\x10\x63onsumption_info\x18\x01 \x03(\x0b\x32).tecton_proto.consumption.ConsumptionInfoR\x0f\x63onsumptionInfo\"\xaf\x01\n\x1fGetMaterializationStatusRequest\x12\x45\n\x12\x66\x65\x61ture_package_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x10\x66\x65\x61turePackageId\x12\x1c\n\tworkspace\x18\x03 \x01(\tR\tworkspace\x12\'\n\x0finclude_deleted\x18\x02 \x01(\x08R\x0eincludeDeleted\"\x83\x01\n GetMaterializationStatusResponse\x12_\n\x16materialization_status\x18\x01 \x01(\x0b\x32(.tecton_proto.data.MaterializationStatusR\x15materializationStatus\"V\n2GetAllMaterializationStatusInLiveWorkspacesRequest\x12 \n\x0c\x63ut_off_days\x18\x01 \x01(\x05R\ncutOffDays\"4\n\nCountRange\x12\x14\n\x05start\x18\x01 \x01(\rR\x05start\x12\x10\n\x03\x65nd\x18\x02 \x01(\rR\x03\x65nd\"m\n\rDurationRange\x12/\n\x05start\x18\x01 \x01(\x0b\x32\x19.google.protobuf.DurationR\x05start\x12+\n\x03\x65nd\x18\x02 \x01(\x0b\x32\x19.google.protobuf.DurationR\x03\x65nd\"o\n\rDateTimeRange\x12\x30\n\x05start\x18\x06 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x05start\x12,\n\x03\x65nd\x18\x07 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x03\x65nd\"\xe0\x06\n\x0eGetJobsRequest\x12\x1e\n\nworkspaces\x18\x01 \x03(\tR\nworkspaces\x12#\n\rfeature_views\x18\x02 \x03(\tR\x0c\x66\x65\x61tureViews\x12I\n\x08statuses\x18\x03 \x03(\x0e\x32-.tecton_proto.data.MaterializationStatusStateR\x08statuses\x12`\n\x16last_task_state_change\x18\x06 \x01(\x0b\x32+.tecton_proto.metadataservice.DateTimeRangeR\x13lastTaskStateChange\x12\x43\n\ttask_type\x18\x04 \x03(\x0e\x32&.tecton_proto.materialization.TaskTypeR\x08taskType\x12K\n\x0cnum_attempts\x18\x05 \x01(\x0b\x32(.tecton_proto.metadataservice.CountRangeR\x0bnumAttempts\x12-\n\x12manually_triggered\x18\x07 \x01(\x08R\x11manuallyTriggered\x12G\n\x08\x64uration\x18\x08 \x01(\x0b\x32+.tecton_proto.metadataservice.DurationRangeR\x08\x64uration\x12Y\n\x12\x66\x65\x61ture_start_time\x18\n \x01(\x0b\x32+.tecton_proto.metadataservice.DateTimeRangeR\x10\x66\x65\x61tureStartTime\x12U\n\x10\x66\x65\x61ture_end_time\x18\x0b \x01(\x0b\x32+.tecton_proto.metadataservice.DateTimeRangeR\x0e\x66\x65\x61tureEndTime\x12O\n$include_update_materialization_flags\x18\x0c \x01(\x08R!includeUpdateMaterializationFlags\x12O\n\npagination\x18\t \x01(\x0b\x32/.tecton_proto.metadataservice.PaginationRequestR\npagination\"\xc5\x01\n FeatureViewMaterializationStatus\x12@\n\x0b\x66\x63o_locator\x18\x01 \x01(\x0b\x32\x1f.tecton_proto.common.FcoLocatorR\nfcoLocator\x12_\n\x16materialization_status\x18\x03 \x01(\x0b\x32(.tecton_proto.data.MaterializationStatusR\x15materializationStatus\"\xb5\x05\n\x10TaskWithAttempts\x12@\n\x0b\x66\x63o_locator\x18\x01 \x01(\x0b\x32\x1f.tecton_proto.common.FcoLocatorR\nfcoLocator\x12K\n\ttaskState\x18\x02 \x01(\x0e\x32-.tecton_proto.data.MaterializationStatusStateR\ttaskState\x12_\n\x16materialization_status\x18\x03 \x01(\x0b\x32(.tecton_proto.data.MaterializationStatusR\x15materializationStatus\x12\x30\n\x07task_id\x18\x04 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x06taskId\x12\x43\n\ttask_type\x18\x08 \x01(\x0e\x32&.tecton_proto.materialization.TaskTypeR\x08taskType\x12-\n\x12manually_triggered\x18\t \x01(\x08R\x11manuallyTriggered\x12O\n\x16last_task_state_change\x18\n \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x13lastTaskStateChange\x12*\n\x11\x66\x65\x61ture_view_name\x18\x05 \x01(\tR\x0f\x66\x65\x61tureViewName\x12H\n\x12\x66\x65\x61ture_start_time\x18\x06 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x10\x66\x65\x61tureStartTime\x12\x44\n\x10\x66\x65\x61ture_end_time\x18\x07 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x0e\x66\x65\x61tureEndTime\"\xc5\x01\n3GetAllMaterializationStatusInLiveWorkspacesResponse\x12\x8d\x01\n#feature_view_materialization_status\x18\x01 \x03(\x0b\x32>.tecton_proto.metadataservice.FeatureViewMaterializationStatusR featureViewMaterializationStatus\"\xc1\x01\n\x0fGetJobsResponse\x12\\\n\x11tasksWithAttempts\x18\x01 \x03(\x0b\x32..tecton_proto.metadataservice.TaskWithAttemptsR\x11tasksWithAttempts\x12P\n\npagination\x18\x02 \x01(\x0b\x32\x30.tecton_proto.metadataservice.PaginationResponseR\npagination\"\xa0\x01\n$ForceRetryMaterializationTaskRequest\x12O\n\x17materialization_task_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x15materializationTaskId\x12\'\n\x0f\x61llow_overwrite\x18\x02 \x01(\x08R\x0e\x61llowOverwrite\"L\n%ForceRetryMaterializationTaskResponse\x12#\n\rerror_message\x18\x01 \x01(\tR\x0c\x65rrorMessage\"t\n!RestartMaterializationTaskRequest\x12O\n\x17materialization_task_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x15materializationTaskId\"\xa1\x01\n\"RestartMaterializationTaskResponse\x12#\n\rerror_message\x18\x01 \x01(\tR\x0c\x65rrorMessage\x12V\n\x1bnew_materialization_task_id\x18\x02 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x18newMaterializationTaskId\".\n\x0eGetFcosRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\"W\n\x0fGetFcosResponse\x12\x44\n\rfco_container\x18\x01 \x01(\x0b\x32\x1f.tecton_proto.data.FcoContainerR\x0c\x66\x63oContainer\"a\n\x15GetSparkConfigRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12*\n\x11\x66\x65\x61ture_view_name\x18\x02 \x01(\tR\x0f\x66\x65\x61tureViewName\"F\n\x12SparkClusterConfig\x12\x1a\n\x08original\x18\x01 \x01(\tR\x08original\x12\x14\n\x05\x66inal\x18\x02 \x01(\tR\x05\x66inal\"\xc4\x01\n\x16GetSparkConfigResponse\x12S\n\x0c\x62\x61tch_config\x18\x01 \x01(\x0b\x32\x30.tecton_proto.metadataservice.SparkClusterConfigR\x0b\x62\x61tchConfig\x12U\n\rstream_config\x18\x02 \x01(\x0b\x32\x30.tecton_proto.metadataservice.SparkClusterConfigR\x0cstreamConfig\"t\n(GetMetricAndExpectationDefinitionRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12*\n\x11\x66\x65\x61ture_view_name\x18\x02 \x01(\tR\x0f\x66\x65\x61tureViewName\"\xe4\x02\n)GetMetricAndExpectationDefinitionResponse\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12*\n\x11\x66\x65\x61ture_view_name\x18\x02 \x01(\tR\x0f\x66\x65\x61tureViewName\x12\x36\n\x07metrics\x18\x03 \x03(\x0b\x32\x1c.tecton_proto.dataobs.MetricR\x07metrics\x12[\n\x14\x66\x65\x61ture_expectations\x18\x04 \x03(\x0b\x32(.tecton_proto.dataobs.FeatureExpectationR\x13\x66\x65\x61tureExpectations\x12X\n\x13metric_expectations\x18\x05 \x03(\x0b\x32\'.tecton_proto.dataobs.MetricExpectationR\x12metricExpectations\"\xca\x01\n\x15GetFeatureViewRequest\x12+\n\x11version_specifier\x18\x01 \x01(\tR\x10versionSpecifier\x12\x1c\n\tworkspace\x18\x03 \x01(\tR\tworkspace\x12\'\n\x02id\x18\x04 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x02id\x12\x37\n\x18run_object_version_check\x18\x06 \x01(\x08R\x15runObjectVersionCheckJ\x04\x08\x05\x10\x06\"^\n\x16GetFeatureViewResponse\x12\x44\n\rfco_container\x18\x02 \x01(\x0b\x32\x1f.tecton_proto.data.FcoContainerR\x0c\x66\x63oContainer\"`\n\x1cGetFeatureViewSummaryRequest\x12@\n\x0b\x66\x63o_locator\x18\x01 \x01(\x0b\x32\x1f.tecton_proto.common.FcoLocatorR\nfcoLocator\"_\n\x1dGetFeatureViewSummaryResponse\x12>\n\x0b\x66\x63o_summary\x18\x01 \x01(\x0b\x32\x1d.tecton_proto.data.FcoSummaryR\nfcoSummary\"R\n\x18QueryFeatureViewsRequest\x12\x12\n\x04name\x18\x01 \x01(\tR\x04name\x12\x1c\n\tworkspace\x18\x03 \x01(\tR\tworkspaceJ\x04\x08\x05\x10\x06\"a\n\x19QueryFeatureViewsResponse\x12\x44\n\rfco_container\x18\x02 \x01(\x0b\x32\x1f.tecton_proto.data.FcoContainerR\x0c\x66\x63oContainer\"{\n4GetMaterializingFeatureViewsInLiveWorkspacesResponse\x12\x43\n\rfeature_views\x18\x01 \x03(\x0b\x32\x1e.tecton_proto.data.FeatureViewR\x0c\x66\x65\x61tureViews\"\xf3\x01\n\x1bGetVirtualDataSourceRequest\x12N\n\x16virtual_data_source_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdH\x00R\x13virtualDataSourceId\x12\x14\n\x04name\x18\x02 \x01(\tH\x00R\x04name\x12\x1c\n\tworkspace\x18\x03 \x01(\tR\tworkspace\x12\x37\n\x18run_object_version_check\x18\x04 \x01(\x08R\x15runObjectVersionCheckB\x11\n\x0fidentifier_typeJ\x04\x08\x05\x10\x06\"d\n\x1cGetVirtualDataSourceResponse\x12\x44\n\rfco_container\x18\x02 \x01(\x0b\x32\x1f.tecton_proto.data.FcoContainerR\x0c\x66\x63oContainer\"?\n\x1fGetAllVirtualDataSourcesRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\"z\n GetAllVirtualDataSourcesResponse\x12V\n\x14virtual_data_sources\x18\x01 \x03(\x0b\x32$.tecton_proto.data.VirtualDataSourceR\x12virtualDataSources\"\xd0\x01\n\x10GetEntityRequest\x12\x36\n\tentity_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdH\x00R\x08\x65ntityId\x12\x14\n\x04name\x18\x02 \x01(\tH\x00R\x04name\x12\x1c\n\tworkspace\x18\x03 \x01(\tR\tworkspace\x12\x37\n\x18run_object_version_check\x18\x06 \x01(\x08R\x15runObjectVersionCheckB\x11\n\x0fidentifier_typeJ\x04\x08\x05\x10\x06\"Y\n\x11GetEntityResponse\x12\x44\n\rfco_container\x18\x02 \x01(\x0b\x32\x1f.tecton_proto.data.FcoContainerR\x0c\x66\x63oContainer\"5\n\x15GetAllEntitiesRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\"O\n\x16GetAllEntitiesResponse\x12\x35\n\x08\x65ntities\x18\x01 \x03(\x0b\x32\x19.tecton_proto.data.EntityR\x08\x65ntities\"\xe8\x01\n\x18GetTransformationRequest\x12\x46\n\x11transformation_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdH\x00R\x10transformationId\x12\x14\n\x04name\x18\x02 \x01(\tH\x00R\x04name\x12\x1c\n\tworkspace\x18\x04 \x01(\tR\tworkspace\x12\x37\n\x18run_object_version_check\x18\x06 \x01(\x08R\x15runObjectVersionCheckB\x11\n\x0fidentifier_typeJ\x04\x08\x05\x10\x06\"g\n\x19GetTransformationResponse\x12\x44\n\rfco_container\x18\x03 \x01(\x0b\x32\x1f.tecton_proto.data.FcoContainerR\x0c\x66\x63oContainerJ\x04\x08\x01\x10\x02\"<\n\x1cGetAllTransformationsRequest\x12\x1c\n\tworkspace\x18\x02 \x01(\tR\tworkspace\"r\n\x1dGetAllTransformationsResponse\x12K\n\x0ftransformations\x18\x02 \x03(\x0b\x32!.tecton_proto.data.TransformationR\x0ftransformationsJ\x04\x08\x01\x10\x02\"Z\n\x17\x46indFcoWorkspaceRequest\x12?\n\x0f\x66\x65\x61ture_view_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\rfeatureViewId\"8\n\x18\x46indFcoWorkspaceResponse\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\"z\n\x17IngestClientLogsRequest\x12_\n\x15sdk_method_invocation\x18\x01 \x01(\x0b\x32+.tecton_proto.amplitude.SDKMethodInvocationR\x13sdkMethodInvocation\"v\n\x16IngestAnalyticsRequest\x12>\n\x06\x65vents\x18\x01 \x03(\x0b\x32&.tecton_proto.amplitude.AmplitudeEventR\x06\x65vents\x12\x1c\n\tworkspace\x18\x02 \x01(\tR\tworkspace\"\xbf\x01\n\x15NewStateUpdateRequest\x12?\n\x07request\x18\x01 \x01(\x0b\x32%.tecton_proto.data.StateUpdateRequestR\x07request\x12\x31\n\x15\x62locking_dry_run_mode\x18\x02 \x01(\x08R\x12\x62lockingDryRunMode\x12\x32\n\x15\x65nable_eager_response\x18\x03 \x01(\x08R\x13\x65nableEagerResponse\"\xfd\x01\n\x17NewStateUpdateRequestV2\x12?\n\x07request\x18\x01 \x01(\x0b\x32%.tecton_proto.data.StateUpdateRequestR\x07request\x12\x31\n\x15\x62locking_dry_run_mode\x18\x02 \x01(\x08R\x12\x62lockingDryRunMode\x12\x32\n\x15\x65nable_eager_response\x18\x03 \x01(\x08R\x13\x65nableEagerResponse\x12\x19\n\x08no_color\x18\x04 \x01(\x08R\x07noColor\x12\x1f\n\x0bjson_output\x18\x05 \x01(\x08R\njsonOutput\"\xe7\x01\n\x16NewStateUpdateResponse\x12\x32\n\x08state_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x07stateId\x12:\n\x1asigned_url_for_repo_upload\x18\x02 \x01(\tR\x16signedUrlForRepoUpload\x12]\n\x0e\x65\x61ger_response\x18\x03 \x01(\x0b\x32\x36.tecton_proto.metadataservice.QueryStateUpdateResponseR\reagerResponse\"\xeb\x01\n\x18NewStateUpdateResponseV2\x12\x32\n\x08state_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x07stateId\x12:\n\x1asigned_url_for_repo_upload\x18\x02 \x01(\tR\x16signedUrlForRepoUpload\x12_\n\x0e\x65\x61ger_response\x18\x03 \x01(\x0b\x32\x38.tecton_proto.metadataservice.QueryStateUpdateResponseV2R\reagerResponse\"k\n\x17QueryStateUpdateRequest\x12\x32\n\x08state_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x07stateId\x12\x1c\n\tworkspace\x18\x02 \x01(\tR\tworkspace\"\xa9\x01\n\x19QueryStateUpdateRequestV2\x12\x32\n\x08state_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x07stateId\x12\x1c\n\tworkspace\x18\x02 \x01(\tR\tworkspace\x12\x19\n\x08no_color\x18\x03 \x01(\x08R\x07noColor\x12\x1f\n\x0bjson_output\x18\x04 \x01(\x08R\njsonOutput\"\xd4\x02\n\x18QueryStateUpdateResponse\x12\x14\n\x05ready\x18\x01 \x01(\x08R\x05ready\x12\x18\n\x07success\x18\x02 \x01(\x08R\x07success\x12\x14\n\x05\x65rror\x18\x03 \x01(\tR\x05\x65rror\x12\x31\n\x14recreates_suppressed\x18\x07 \x01(\x08R\x13recreatesSuppressed\x12P\n\x11validation_result\x18\x04 \x01(\x0b\x32#.tecton_proto.data.ValidationResultR\x10validationResult\x12\x39\n\ndiff_items\x18\x05 \x03(\x0b\x32\x1a.tecton_proto.data.FcoDiffR\tdiffItems\x12\x32\n\x15latest_status_message\x18\x06 \x01(\tR\x13latestStatusMessage\"\xe9\x02\n\x1aQueryStateUpdateResponseV2\x12\x14\n\x05ready\x18\x01 \x01(\x08R\x05ready\x12\x18\n\x07success\x18\x02 \x01(\x08R\x07success\x12\x14\n\x05\x65rror\x18\x03 \x01(\tR\x05\x65rror\x12\x32\n\x15latest_status_message\x18\x06 \x01(\tR\x13latestStatusMessage\x12R\n\x11validation_errors\x18\x08 \x01(\x0b\x32#.tecton_proto.data.ValidationResultH\x00R\x10validationErrors\x12_\n\x16successful_plan_output\x18\t \x01(\x0b\x32\'.tecton_proto.data.SuccessfulPlanOutputH\x00R\x14successfulPlanOutputB\n\n\x08responseJ\x04\x08\x04\x10\x05J\x04\x08\x05\x10\x06J\x04\x08\x07\x10\x08\"T\n GetStateUpdatePlanSummaryRequest\x12\x30\n\x07plan_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x06planId\"b\n!GetStateUpdatePlanSummaryResponse\x12=\n\x04plan\x18\x01 \x01(\x0b\x32).tecton_proto.data.StateUpdatePlanSummaryR\x04plan\"p\n\x17\x41pplyStateUpdateRequest\x12\x32\n\x08state_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x07stateId\x12!\n\napplied_by\x18\x02 \x01(\tB\x02\x18\x01R\tappliedBy\"\x1a\n\x18\x41pplyStateUpdateResponse\"\xb2\x01\n\x12GetConfigsResponse\x12^\n\nkey_values\x18\x01 \x03(\x0b\x32?.tecton_proto.metadataservice.GetConfigsResponse.KeyValuesEntryR\tkeyValues\x1a<\n\x0eKeyValuesEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01\"\xc2\x01\n\x1aGetGlobalsForWebUIResponse\x12\x66\n\nkey_values\x18\x01 \x03(\x0b\x32G.tecton_proto.metadataservice.GetGlobalsForWebUIResponse.KeyValuesEntryR\tkeyValues\x1a<\n\x0eKeyValuesEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01\"N\n\x18GetStateUpdateLogRequest\x12\x14\n\x05limit\x18\x01 \x01(\x05R\x05limit\x12\x1c\n\tworkspace\x18\x02 \x01(\tR\tworkspace\"Z\n\x19GetStateUpdateLogResponse\x12=\n\x07\x65ntries\x18\x01 \x03(\x0b\x32#.tecton_proto.data.StateUpdateEntryR\x07\x65ntries\"R\n\x15GetRestoreInfoRequest\x12\x1b\n\tcommit_id\x18\x01 \x01(\tR\x08\x63ommitId\x12\x1c\n\tworkspace\x18\x02 \x01(\tR\tworkspace\"\x96\x01\n\x16GetRestoreInfoResponse\x12>\n\x1csigned_url_for_repo_download\x18\x01 \x01(\tR\x18signedUrlForRepoDownload\x12\x1b\n\tcommit_id\x18\x02 \x01(\tR\x08\x63ommitId\x12\x1f\n\x0bsdk_version\x18\x03 \x01(\tR\nsdkVersion\"\x93\x01\n\x16\x43reateWorkspaceRequest\x12%\n\x0eworkspace_name\x18\x02 \x01(\tR\rworkspaceName\x12L\n\x0c\x63\x61pabilities\x18\x03 \x01(\x0b\x32(.tecton_proto.data.WorkspaceCapabilitiesR\x0c\x63\x61pabilitiesJ\x04\x08\x01\x10\x02\"6\n\x16\x44\x65leteWorkspaceRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\"2\n\x17IntrospectApiKeyRequest\x12\x17\n\x07\x61pi_key\x18\x01 \x01(\tR\x06\x61piKey\"\xcb\x01\n\x18IntrospectApiKeyResponse\x12\'\n\x02id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x02id\x12 \n\x0b\x64\x65scription\x18\x02 \x01(\tR\x0b\x64\x65scription\x12\x1d\n\ncreated_by\x18\x03 \x01(\tR\tcreatedBy\x12\x16\n\x06\x61\x63tive\x18\x04 \x01(\x08R\x06\x61\x63tive\x12\x19\n\x08is_admin\x18\x05 \x01(\x08R\x07isAdmin\x12\x12\n\x04name\x18\x06 \x01(\tR\x04name\"R\n\x13\x43reateApiKeyRequest\x12 \n\x0b\x64\x65scription\x18\x01 \x01(\tR\x0b\x64\x65scription\x12\x19\n\x08is_admin\x18\x02 \x01(\x08R\x07isAdmin\"Q\n\x14\x43reateApiKeyResponse\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\'\n\x02id\x18\x02 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x02id\">\n\x13\x44\x65leteApiKeyRequest\x12\'\n\x02id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x02id\"?\n\x12ListApiKeysRequest\x12)\n\x10include_archived\x18\x01 \x01(\x08R\x0fincludeArchived\"Q\n\x13ListApiKeysResponse\x12:\n\x08\x61pi_keys\x18\x01 \x03(\x0b\x32\x1f.tecton_proto.data.TectonApiKeyR\x07\x61piKeys\"\x9f\x02\n\x0eServiceAccount\x12\x0e\n\x02id\x18\x01 \x01(\tR\x02id\x12\x12\n\x04name\x18\x02 \x01(\tR\x04name\x12 \n\x0b\x64\x65scription\x18\x03 \x01(\tR\x0b\x64\x65scription\x12\x1b\n\tis_active\x18\x04 \x01(\x08R\x08isActive\x12\x36\n\x07\x63reator\x18\x05 \x01(\x0b\x32\x1c.tecton_proto.auth.PrincipalR\x07\x63reator\x12\x37\n\x05owner\x18\x06 \x01(\x0b\x32!.tecton_proto.auth.PrincipalBasicR\x05owner\x12\x39\n\ncreated_at\x18\x07 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\tcreatedAt\"S\n\x1b\x43reateServiceAccountRequest\x12\x12\n\x04name\x18\x01 \x01(\tR\x04name\x12 \n\x0b\x64\x65scription\x18\x02 \x01(\tR\x0b\x64\x65scription\"\xd5\x01\n\x1c\x43reateServiceAccountResponse\x12\x0e\n\x02id\x18\x01 \x01(\tR\x02id\x12\x12\n\x04name\x18\x02 \x01(\tR\x04name\x12 \n\x0b\x64\x65scription\x18\x03 \x01(\tR\x0b\x64\x65scription\x12\x1b\n\tis_active\x18\x04 \x01(\x08R\x08isActive\x12\x17\n\x07\x61pi_key\x18\x05 \x01(\tR\x06\x61piKey\x12\x39\n\ncreated_at\x18\x06 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\tcreatedAt\"E\n\x19GetServiceAccountsRequest\x12\x16\n\x06search\x18\x01 \x01(\tR\x06search\x12\x10\n\x03ids\x18\x02 \x03(\tR\x03ids\"u\n\x1aGetServiceAccountsResponse\x12W\n\x10service_accounts\x18\x01 \x03(\x0b\x32,.tecton_proto.metadataservice.ServiceAccountR\x0fserviceAccounts\"\x80\x01\n\x1bUpdateServiceAccountRequest\x12\x0e\n\x02id\x18\x01 \x01(\tR\x02id\x12\x12\n\x04name\x18\x02 \x01(\tR\x04name\x12 \n\x0b\x64\x65scription\x18\x03 \x01(\tR\x0b\x64\x65scription\x12\x1b\n\tis_active\x18\x04 \x01(\x08R\x08isActive\"\xbc\x01\n\x1cUpdateServiceAccountResponse\x12\x0e\n\x02id\x18\x01 \x01(\tR\x02id\x12\x12\n\x04name\x18\x02 \x01(\tR\x04name\x12 \n\x0b\x64\x65scription\x18\x03 \x01(\tR\x0b\x64\x65scription\x12\x1b\n\tis_active\x18\x04 \x01(\x08R\x08isActive\x12\x39\n\ncreated_at\x18\x05 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\tcreatedAt\"-\n\x1b\x44\x65leteServiceAccountRequest\x12\x0e\n\x02id\x18\x01 \x01(\tR\x02id\"\x1e\n\x1c\x44\x65leteServiceAccountResponse\"\x17\n\x15ListWorkspacesRequest\"V\n\x16ListWorkspacesResponse\x12<\n\nworkspaces\x18\x01 \x03(\x0b\x32\x1c.tecton_proto.data.WorkspaceR\nworkspaces\"<\n\x13GetWorkspaceRequest\x12%\n\x0eworkspace_name\x18\x01 \x01(\tR\rworkspaceName\"R\n\x14GetWorkspaceResponse\x12:\n\tworkspace\x18\x01 \x01(\x0b\x32\x1c.tecton_proto.data.WorkspaceR\tworkspace\"\x93\x03\n\"CreateSavedFeatureDataFrameRequest\x12\x12\n\x04name\x18\x01 \x01(\tR\x04name\x12\x1c\n\tworkspace\x18\x07 \x01(\tR\tworkspace\x12G\n\x12\x66\x65\x61ture_package_id\x18\x02 \x01(\x0b\x32\x17.tecton_proto.common.IdH\x00R\x10\x66\x65\x61turePackageId\x12G\n\x12\x66\x65\x61ture_service_id\x18\x03 \x01(\x0b\x32\x17.tecton_proto.common.IdH\x00R\x10\x66\x65\x61tureServiceId\x12\x31\n\x15join_key_column_names\x18\x04 \x03(\tR\x12joinKeyColumnNames\x12\x32\n\x15timestamp_column_name\x18\x05 \x01(\tR\x13timestampColumnName\x12\x38\n\x06schema\x18\x06 \x01(\x0b\x32 .tecton_proto.common.SparkSchemaR\x06schemaB\x08\n\x06source\"\x87\x01\n#CreateSavedFeatureDataFrameResponse\x12`\n\x17saved_feature_dataframe\x18\x01 \x01(\x0b\x32(.tecton_proto.data.SavedFeatureDataFrameR\x15savedFeatureDataframe\"{\n#ArchiveSavedFeatureDataFrameRequest\x12T\n\x1asaved_feature_dataframe_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x17savedFeatureDataframeId\"&\n$ArchiveSavedFeatureDataFrameResponse\"\xe7\x01\n\x1fGetSavedFeatureDataFrameRequest\x12\x41\n\x1csaved_feature_dataframe_name\x18\x01 \x01(\tH\x00R\x19savedFeatureDataframeName\x12V\n\x1asaved_feature_dataframe_id\x18\x02 \x01(\x0b\x32\x17.tecton_proto.common.IdH\x00R\x17savedFeatureDataframeId\x12\x1c\n\tworkspace\x18\x03 \x01(\tR\tworkspaceB\x0b\n\treference\"\xa5\x01\n\x1bGetClusterAdminInfoResponse\x12&\n\x0f\x63\x61ller_is_admin\x18\x01 \x01(\x08R\rcallerIsAdmin\x12-\n\x05users\x18\x02 \x03(\x0b\x32\x17.tecton_proto.data.UserR\x05users\x12/\n\x06\x61\x64mins\x18\x03 \x03(\x0b\x32\x17.tecton_proto.data.UserR\x06\x61\x64mins\";\n\x18\x43reateClusterUserRequest\x12\x1f\n\x0blogin_email\x18\x01 \x01(\tR\nloginEmail\"!\n\x19\x43reateClusterUserResponseJ\x04\x08\x01\x10\x02\"3\n\x18\x44\x65leteClusterUserRequest\x12\x17\n\x07okta_id\x18\x01 \x01(\tR\x06oktaId\"!\n\x19\x44\x65leteClusterUserResponseJ\x04\x08\x01\x10\x02\"\xe2\x01\n\x18\x43lusterUserActionRequest\x12\x17\n\x07okta_id\x18\x01 \x01(\tR\x06oktaId\x12\x38\n\x17resend_activation_email\x18\x02 \x01(\x08H\x00R\x15resendActivationEmail\x12!\n\x0bunlock_user\x18\x03 \x01(\x08H\x00R\nunlockUser\x12!\n\x0bgrant_admin\x18\x04 \x01(\x08H\x00R\ngrantAdmin\x12#\n\x0crevoke_admin\x18\x05 \x01(\x08H\x00R\x0brevokeAdminB\x08\n\x06\x61\x63tion\"!\n\x19\x43lusterUserActionResponseJ\x04\x08\x01\x10\x02\"\x84\x01\n GetSavedFeatureDataFrameResponse\x12`\n\x17saved_feature_dataframe\x18\x01 \x01(\x0b\x32(.tecton_proto.data.SavedFeatureDataFrameR\x15savedFeatureDataframe\"C\n#GetAllSavedFeatureDataFramesRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\"\x8a\x01\n$GetAllSavedFeatureDataFramesResponse\x12\x62\n\x18saved_feature_dataframes\x18\x01 \x03(\x0b\x32(.tecton_proto.data.SavedFeatureDataFrameR\x16savedFeatureDataframes\"\x9c\x01\n\x16IngestDataframeRequest\x12K\n\x15\x66\x65\x61ture_definition_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x13\x66\x65\x61tureDefinitionId\x12\x17\n\x07\x64\x66_path\x18\x02 \x01(\tR\x06\x64\x66Path\x12\x1c\n\tworkspace\x18\x03 \x01(\tR\tworkspace\"\x19\n\x17IngestDataframeResponse\"o\n GetNewIngestDataframeInfoRequest\x12K\n\x15\x66\x65\x61ture_definition_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x13\x66\x65\x61tureDefinitionId\"t\n!GetNewIngestDataframeInfoResponse\x12\x17\n\x07\x64\x66_path\x18\x01 \x01(\tR\x06\x64\x66Path\x12\x36\n\x18signed_url_for_df_upload\x18\x02 \x01(\tR\x14signedUrlForDfUpload\"\x88\x01\n!GetUserDeploymentSettingsResponse\x12\x63\n\x18user_deployment_settings\x18\x01 \x01(\x0b\x32).tecton_proto.data.UserDeploymentSettingsR\x16userDeploymentSettings\"\xc5\x01\n#UpdateUserDeploymentSettingsRequest\x12\x63\n\x18user_deployment_settings\x18\x01 \x01(\x0b\x32).tecton_proto.data.UserDeploymentSettingsR\x16userDeploymentSettings\x12\x39\n\nfield_mask\x18\x02 \x01(\x0b\x32\x1a.google.protobuf.FieldMaskR\tfieldMask\"e\n$UpdateUserDeploymentSettingsResponse\x12\x18\n\x07success\x18\x01 \x01(\x08R\x07success\x12#\n\rerror_message\x18\x02 \x01(\tR\x0c\x65rrorMessage\"n\n%GetInternalSparkClusterStatusResponse\x12\x45\n\x06status\x18\x01 \x01(\x0b\x32-.tecton_proto.data.InternalSparkClusterStatusR\x06status\"k\n\x1cGetDeleteEntitiesInfoRequest\x12K\n\x15\x66\x65\x61ture_definition_id\x18\x01 \x01(\x0b\x32\x17.tecton_proto.common.IdR\x13\x66\x65\x61tureDefinitionId\"\xca\x01\n\x1dGetDeleteEntitiesInfoResponse\x12\x17\n\x07\x64\x66_path\x18\x01 \x01(\tR\x06\x64\x66Path\x12\x43\n\x1fsigned_url_for_df_upload_online\x18\x03 \x01(\tR\x1asignedUrlForDfUploadOnline\x12\x45\n signed_url_for_df_upload_offline\x18\x04 \x01(\tR\x1bsignedUrlForDfUploadOfflineJ\x04\x08\x02\x10\x03\"\xf3\x01\n\x15\x44\x65leteEntitiesRequest\x12@\n\x0b\x66\x63o_locator\x18\x01 \x01(\x0b\x32\x1f.tecton_proto.common.FcoLocatorR\nfcoLocator\x12\x31\n\x15online_join_keys_path\x18\x02 \x01(\tR\x12onlineJoinKeysPath\x12\x33\n\x16offline_join_keys_path\x18\x03 \x01(\tR\x13offlineJoinKeysPath\x12\x16\n\x06online\x18\x04 \x01(\x08R\x06online\x12\x18\n\x07offline\x18\x05 \x01(\x08R\x07offline\"\x18\n\x16\x44\x65leteEntitiesResponse\"\xfd\x01\n\x16GetHiveMetadataRequest\x12S\n\x06\x61\x63tion\x18\x01 \x01(\x0e\x32;.tecton_proto.metadataservice.GetHiveMetadataRequest.ActionR\x06\x61\x63tion\x12\x1a\n\x08\x64\x61tabase\x18\x02 \x01(\tR\x08\x64\x61tabase\x12\x14\n\x05table\x18\x03 \x01(\tR\x05table\"\\\n\x06\x41\x63tion\x12\x19\n\x15\x41\x43TION_LIST_DATABASES\x10\x00\"\x04\x08\x01\x10\x01\"\x04\x08\x02\x10\x02*\x12\x41\x43TION_LIST_TABLES*\x17\x41\x43TION_GET_TABLE_SCHEMA\"\xe1\x01\n\x17GetHiveMetadataResponse\x12\x18\n\x07success\x18\x01 \x01(\x08R\x07success\x12#\n\rerror_message\x18\x02 \x01(\tR\x0c\x65rrorMessage\x12\x41\n\tdatabases\x18\x03 \x01(\x0b\x32!.tecton_proto.data.ListHiveResultH\x00R\tdatabases\x12.\n\x13\x64\x65\x62ug_error_message\x18\x06 \x01(\tR\x11\x64\x65\x62ugErrorMessageB\x08\n\x06resultJ\x04\x08\x04\x10\x05J\x04\x08\x05\x10\x06\"\x95\x01\n\x17ValidateLocalFcoRequest\x12Y\n\x12validation_request\x18\x01 \x01(\x0b\x32*.tecton_proto.validation.ValidationRequestR\x11validationRequest\x12\x1f\n\x0bsdk_version\x18\x02 \x01(\tR\nsdkVersion\"\x9c\x01\n\x18ValidateLocalFcoResponse\x12\x18\n\x07success\x18\x01 \x01(\x08R\x07success\x12P\n\x11validation_result\x18\x02 \x01(\x0b\x32#.tecton_proto.data.ValidationResultR\x10validationResult\x12\x14\n\x05\x65rror\x18\x03 \x01(\tR\x05\x65rror\":\n\x1aGetOnboardingStatusRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\"\xc9\x01\n\x1bGetOnboardingStatusResponse\x12N\n\x0esetup_platform\x18\x03 \x01(\x0e\x32\'.tecton_proto.data.OnboardingStatusEnumR\rsetupPlatform\x12T\n\x11\x66inish_onboarding\x18\x02 \x01(\x0e\x32\'.tecton_proto.data.OnboardingStatusEnumR\x10\x66inishOnboardingJ\x04\x08\x01\x10\x02\"\x92\x01\n\"GetDataPlatformSetupStatusResponse\x12&\n\x0esetupCompleted\x18\x01 \x01(\x08R\x0esetupCompleted\x12\x44\n\x05tasks\x18\x02 \x03(\x0b\x32..tecton_proto.data.DataPlatformSetupTaskStatusR\x05tasks\"i\n\x1dGetObservabilityConfigRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12*\n\x11\x66\x65\x61ture_view_name\x18\x02 \x01(\tR\x0f\x66\x65\x61tureViewName\"\xa5\x01\n\x1eGetObservabilityConfigResponse\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12*\n\x11\x66\x65\x61ture_view_name\x18\x02 \x01(\tR\x0f\x66\x65\x61tureViewName\x12\x39\n\x19is_dataobs_metric_enabled\x18\x03 \x01(\x08R\x16isDataobsMetricEnabled\"\xaf\x02\n\x12QueryMetricRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12*\n\x11\x66\x65\x61ture_view_name\x18\x02 \x01(\tR\x0f\x66\x65\x61tureViewName\x12\x41\n\x0bmetric_type\x18\x03 \x01(\x0e\x32 .tecton_proto.dataobs.MetricTypeR\nmetricType\x12\x39\n\nstart_time\x18\x04 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\tstartTime\x12\x35\n\x08\x65nd_time\x18\x05 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x07\x65ndTime\x12\x1a\n\x05limit\x18\x06 \x01(\x05:\x04\x35\x30\x30\x30R\x05limit\"\xf5\x03\n\x13QueryMetricResponse\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12*\n\x11\x66\x65\x61ture_view_name\x18\x02 \x01(\tR\x0f\x66\x65\x61tureViewName\x12\x41\n\x0bmetric_type\x18\x03 \x01(\x0e\x32 .tecton_proto.dataobs.MetricTypeR\nmetricType\x12V\n\x1ametric_data_point_interval\x18\x05 \x01(\x0b\x32\x19.google.protobuf.DurationR\x17metricDataPointInterval\x12H\n\x12\x61ligned_start_time\x18\x07 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x10\x61lignedStartTime\x12\x44\n\x10\x61ligned_end_time\x18\x08 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x0e\x61lignedEndTime\x12\x46\n\x0bmetric_data\x18\x04 \x03(\x0b\x32%.tecton_proto.dataobs.MetricDataPointR\nmetricData\x12!\n\x0c\x63olumn_names\x18\x06 \x03(\tR\x0b\x63olumnNames\"\x80\x04\n!GetFeatureValidationResultRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12N\n\x15validation_start_time\x18\x02 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x13validationStartTime\x12J\n\x13validation_end_time\x18\x03 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x11validationEndTime\x12\x39\n\x19\x66ilter_feature_view_names\x18\x04 \x03(\tR\x16\x66ilterFeatureViewNames\x12\x38\n\x18\x66ilter_expectation_names\x18\x05 \x03(\tR\x16\x66ilterExpectationNames\x12[\n\x13\x66ilter_result_types\x18\x06 \x03(\x0e\x32+.tecton_proto.dataobs.ExpectationResultEnumR\x11\x66ilterResultTypes\x12O\n\npagination\x18\x07 \x01(\x0b\x32/.tecton_proto.metadataservice.PaginationRequestR\npagination\"\xb9\x01\n\"GetFeatureValidationResultResponse\x12\x41\n\x07results\x18\x02 \x03(\x0b\x32\'.tecton_proto.dataobs.ExpectationResultR\x07results\x12P\n\npagination\x18\x03 \x01(\x0b\x32\x30.tecton_proto.metadataservice.PaginationResponseR\npagination\"\xde\x01\n\"GetFeatureValidationSummaryRequest\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12N\n\x15validation_start_time\x18\x02 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x13validationStartTime\x12J\n\x13validation_end_time\x18\x03 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x11validationEndTime\"\xba\x02\n#GetFeatureValidationSummaryResponse\x12\x1c\n\tworkspace\x18\x01 \x01(\tR\tworkspace\x12N\n\x15validation_start_time\x18\x02 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x13validationStartTime\x12J\n\x13validation_end_time\x18\x03 \x01(\x0b\x32\x1a.google.protobuf.TimestampR\x11validationEndTime\x12Y\n\x11workspace_summary\x18\x04 \x01(\x0b\x32,.tecton_proto.dataobs.WorkspaceResultSummaryR\x10workspaceSummary\"6\n\x0eGetUserRequest\x12\x0e\n\x02id\x18\x01 \x01(\tR\x02id\x12\x14\n\x05\x65mail\x18\x02 \x01(\tR\x05\x65mail\"C\n\x0fGetUserResponse\x12\x30\n\x04user\x18\x01 \x01(\x0b\x32\x1c.tecton_proto.auth.UserBasicR\x04user\"\x1f\n\x1dGetFeatureServerConfigRequest\"\x90\x01\n\x1eGetFeatureServerConfigResponse\x12\"\n\x0c\x63urrentCount\x18\x01 \x01(\rR\x0c\x63urrentCount\x12&\n\x0e\x61vailableCount\x18\x02 \x01(\rR\x0e\x61vailableCount\x12\"\n\x0c\x64\x65siredCount\x18\x03 \x01(\rR\x0c\x64\x65siredCount\"5\n\x1dSetFeatureServerConfigRequest\x12\x14\n\x05\x63ount\x18\x01 \x01(\rR\x05\x63ount*>\n\rSortDirection\x12\x10\n\x0cSORT_UNKNOWN\x10\x00\x12\x0c\n\x08SORT_ASC\x10\x01\x12\r\n\tSORT_DESC\x10\x02\x32\xcd\x92\x01\n\x0fMetadataService\x12\xec\x01\n\x11GetFeatureService\x12\x36.tecton_proto.metadataservice.GetFeatureServiceRequest\x1a\x37.tecton_proto.metadataservice.GetFeatureServiceResponse\"f\x82\xd3\xe4\x93\x02-\"(/v1/metadata-service/get-feature-service:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\xfd\x01\n\x15GetAllFeatureServices\x12:.tecton_proto.metadataservice.GetAllFeatureServicesRequest\x1a;.tecton_proto.metadataservice.GetAllFeatureServicesResponse\"k\x82\xd3\xe4\x93\x02\x32\"-/v1/metadata-service/get-all-feature-services:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\x98\x02\n\x18GetFeatureServiceSummary\x12=.tecton_proto.metadataservice.GetFeatureServiceSummaryRequest\x1a>.tecton_proto.metadataservice.GetFeatureServiceSummaryResponse\"}\x82\xd3\xe4\x93\x02\x35\"0/v1/metadata-service/get-feature-service-summary:\x01*\xa2\xbc\xe6\xc0\x05<\x1a\r\x08\x01\x12\tworkspace*+tecton/workspace#read_materialization_state\x12\xf7\x01\n\x10GetServingStatus\x12\x35.tecton_proto.metadataservice.GetServingStatusRequest\x1a\x36.tecton_proto.metadataservice.GetServingStatusResponse\"t\x82\xd3\xe4\x93\x02,\"\'/v1/metadata-service/get-serving-status:\x01*\xa2\xbc\xe6\xc0\x05<\x1a\r\x08\x01\x12\tworkspace*+tecton/workspace#read_materialization_state\x12\x96\x02\n\x17GetFVServingStatusForFS\x12<.tecton_proto.metadataservice.GetFVServingStatusForFSRequest\x1a=.tecton_proto.metadataservice.GetFVServingStatusForFSResponse\"~\x82\xd3\xe4\x93\x02\x36\"1/v1/metadata-service/get-fv-serving-status-for-fs:\x01*\xa2\xbc\xe6\xc0\x05<\x1a\r\x08\x01\x12\tworkspace*+tecton/workspace#read_materialization_state\x12\x90\x02\n\x16GetAllFeatureFreshness\x12;.tecton_proto.metadataservice.GetAllFeatureFreshnessRequest\x1a<.tecton_proto.metadataservice.GetAllFeatureFreshnessResponse\"{\x82\xd3\xe4\x93\x02\x33\"./v1/metadata-service/get-all-feature-freshness:\x01*\xa2\xbc\xe6\xc0\x05<\x1a\r\x08\x01\x12\tworkspace*+tecton/workspace#read_materialization_state\x12\x90\x02\n\x13GetFeatureFreshness\x12\x38.tecton_proto.metadataservice.GetFeatureFreshnessRequest\x1a\x39.tecton_proto.metadataservice.GetFeatureFreshnessResponse\"\x83\x01\x82\xd3\xe4\x93\x02/\"*/v1/metadata-service/get-feature-freshness:\x01*\xa2\xbc\xe6\xc0\x05H\x1a\x19\x08\x01\x12\x15\x66\x63o_locator.workspace*+tecton/workspace#read_materialization_state\x12\xdd\x01\n\x18GetMaterializationStatus\x12=.tecton_proto.metadataservice.GetMaterializationStatusRequest\x1a>.tecton_proto.metadataservice.GetMaterializationStatusResponse\"B\x82\xd3\xe4\x93\x02\x34\"//v1/metadata-service/get-materialization-status:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xac\x02\n+GetAllMaterializationStatusInLiveWorkspaces\x12P.tecton_proto.metadataservice.GetAllMaterializationStatusInLiveWorkspacesRequest\x1aQ.tecton_proto.metadataservice.GetAllMaterializationStatusInLiveWorkspacesResponse\"X\x82\xd3\xe4\x93\x02J\"E/v1/metadata-service/get-all-materialization-status-in-live-workspace:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\x94\x01\n\x07GetJobs\x12,.tecton_proto.metadataservice.GetJobsRequest\x1a-.tecton_proto.metadataservice.GetJobsResponse\",\x82\xd3\xe4\x93\x02\x1e\"\x19/v1/metadata-service/jobs:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xf2\x01\n\x1d\x46orceRetryMaterializationTask\x12\x42.tecton_proto.metadataservice.ForceRetryMaterializationTaskRequest\x1a\x43.tecton_proto.metadataservice.ForceRetryMaterializationTaskResponse\"H\x82\xd3\xe4\x93\x02:\"5/v1/metadata-service/force-retry-materialization-task:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xe5\x01\n\x1aRestartMaterializationTask\x12?.tecton_proto.metadataservice.RestartMaterializationTaskRequest\x1a@.tecton_proto.metadataservice.RestartMaterializationTaskResponse\"D\x82\xd3\xe4\x93\x02\x36\"1/v1/metadata-service/restart-materialization-task:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xe0\x01\n\x0eGetSparkConfig\x12\x33.tecton_proto.metadataservice.GetSparkConfigRequest\x1a\x34.tecton_proto.metadataservice.GetSparkConfigResponse\"c\x82\xd3\xe4\x93\x02*\"%/v1/metadata-service/get-spark-config:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\xae\x02\n!GetMetricAndExpectationDefinition\x12\x46.tecton_proto.metadataservice.GetMetricAndExpectationDefinitionRequest\x1aG.tecton_proto.metadataservice.GetMetricAndExpectationDefinitionResponse\"x\x82\xd3\xe4\x93\x02?\":/v1/metadata-service/get-metric-and-expectation-definition:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\xe0\x01\n\x0eGetFeatureView\x12\x33.tecton_proto.metadataservice.GetFeatureViewRequest\x1a\x34.tecton_proto.metadataservice.GetFeatureViewResponse\"c\x82\xd3\xe4\x93\x02*\"%/v1/metadata-service/get-feature-view:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\xf0\x01\n,GetMaterializingFeatureViewsInLiveWorkspaces\x12\x16.google.protobuf.Empty\x1aR.tecton_proto.metadataservice.GetMaterializingFeatureViewsInLiveWorkspacesResponse\"T\x82\xd3\xe4\x93\x02\x46\"A/v1/metadata-service/get-materializing-features-in-live-workspace:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xec\x01\n\x11QueryFeatureViews\x12\x36.tecton_proto.metadataservice.QueryFeatureViewsRequest\x1a\x37.tecton_proto.metadataservice.QueryFeatureViewsResponse\"f\x82\xd3\xe4\x93\x02-\"(/v1/metadata-service/query-feature-views:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\x99\x02\n\x15GetFeatureViewSummary\x12:.tecton_proto.metadataservice.GetFeatureViewSummaryRequest\x1a;.tecton_proto.metadataservice.GetFeatureViewSummaryResponse\"\x86\x01\x82\xd3\xe4\x93\x02\x32\"-/v1/metadata-package/get-feature-view-summary:\x01*\xa2\xbc\xe6\xc0\x05H\x1a\x19\x08\x01\x12\x15\x66\x63o_locator.workspace*+tecton/workspace#read_materialization_state\x12\x93\x02\n\x15GetFeatureConsumption\x12:.tecton_proto.metadataservice.GetFeatureConsumptionRequest\x1a;.tecton_proto.metadataservice.GetFeatureConsumptionResponse\"\x80\x01\x82\xd3\xe4\x93\x02\x36\"1/v1/metadata-service/get-feature-view-consumption:\x01*\xa2\xbc\xe6\xc0\x05>\x1a\x19\x08\x01\x12\x15\x66\x63o_locator.workspace*!tecton/workspace#read_consumption\x12\xf9\x01\n\x14GetVirtualDataSource\x12\x39.tecton_proto.metadataservice.GetVirtualDataSourceRequest\x1a:.tecton_proto.metadataservice.GetVirtualDataSourceResponse\"j\x82\xd3\xe4\x93\x02\x31\",/v1/metadata-service/get-virtual-data-source:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\x8a\x02\n\x18GetAllVirtualDataSources\x12=.tecton_proto.metadataservice.GetAllVirtualDataSourcesRequest\x1a>.tecton_proto.metadataservice.GetAllVirtualDataSourcesResponse\"o\x82\xd3\xe4\x93\x02\x36\"1/v1/metadata-service/get-all-virtual-data-sources:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\x90\x01\n\x10IngestClientLogs\x12\x35.tecton_proto.metadataservice.IngestClientLogsRequest\x1a\x16.google.protobuf.Empty\"-\xa2\xbc\xe6\xc0\x05\'*%tecton/organization#read_organization\x12\xbe\x01\n\x0fIngestAnalytics\x12\x34.tecton_proto.metadataservice.IngestAnalyticsRequest\x1a\x16.google.protobuf.Empty\"]\x82\xd3\xe4\x93\x02*\"%/v1/metadata-service/ingest-analytics:\x01*\xa2\xbc\xe6\xc0\x05\'*%tecton/organization#read_organization\x12\xa2\x02\n\x1bGetVirtualDataSourceSummary\x12@.tecton_proto.metadataservice.GetVirtualDataSourceSummaryRequest\x1a\x41.tecton_proto.metadataservice.GetVirtualDataSourceSummaryResponse\"~\x82\xd3\xe4\x93\x02\x39\"4/v1/metadata-service/get-virtual-data-source-summary:\x01*\xa2\xbc\xe6\xc0\x05\x39\x1a\x19\x08\x01\x12\x15\x66\x63o_locator.workspace*\x1ctecton/workspace#read_config\x12\xc3\x01\n\x07GetFcos\x12,.tecton_proto.metadataservice.GetFcosRequest\x1a-.tecton_proto.metadataservice.GetFcosResponse\"[\x82\xd3\xe4\x93\x02\"\"\x1d/v1/metadata-service/get-fcos:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\xa1\x01\n\tGetEntity\x12..tecton_proto.metadataservice.GetEntityRequest\x1a/.tecton_proto.metadataservice.GetEntityResponse\"3\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\xe0\x01\n\x0eGetAllEntities\x12\x33.tecton_proto.metadataservice.GetAllEntitiesRequest\x1a\x34.tecton_proto.metadataservice.GetAllEntitiesResponse\"c\x82\xd3\xe4\x93\x02*\"%/v1/metadata-service/get-all-entities:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\xf4\x01\n\x10GetEntitySummary\x12\x35.tecton_proto.metadataservice.GetEntitySummaryRequest\x1a\x36.tecton_proto.metadataservice.GetEntitySummaryResponse\"q\x82\xd3\xe4\x93\x02,\"\'/v1/metadata-package/get-entity-summary:\x01*\xa2\xbc\xe6\xc0\x05\x39\x1a\x19\x08\x01\x12\x15\x66\x63o_locator.workspace*\x1ctecton/workspace#read_config\x12\xeb\x01\n\x11GetTransformation\x12\x36.tecton_proto.metadataservice.GetTransformationRequest\x1a\x37.tecton_proto.metadataservice.GetTransformationResponse\"e\x82\xd3\xe4\x93\x02,\"\'/v1/metadata-service/get-transformation:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\xfc\x01\n\x15GetAllTransformations\x12:.tecton_proto.metadataservice.GetAllTransformationsRequest\x1a;.tecton_proto.metadataservice.GetAllTransformationsResponse\"j\x82\xd3\xe4\x93\x02\x31\",/v1/metadata-service/get-all-transformations:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\x94\x02\n\x18GetTransformationSummary\x12=.tecton_proto.metadataservice.GetTransformationSummaryRequest\x1a>.tecton_proto.metadataservice.GetTransformationSummaryResponse\"y\x82\xd3\xe4\x93\x02\x34\"//v1/metadata-package/get-transformation-summary:\x01*\xa2\xbc\xe6\xc0\x05\x39\x1a\x19\x08\x01\x12\x15\x66\x63o_locator.workspace*\x1ctecton/workspace#read_config\x12\xbd\x01\n\x10\x46indFcoWorkspace\x12\x35.tecton_proto.metadataservice.FindFcoWorkspaceRequest\x1a\x36.tecton_proto.metadataservice.FindFcoWorkspaceResponse\":\x82\xd3\xe4\x93\x02,\"\'/v1/metadata-service/find-fco-workspace:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xb8\x01\n\x0eNewStateUpdate\x12\x33.tecton_proto.metadataservice.NewStateUpdateRequest\x1a\x34.tecton_proto.metadataservice.NewStateUpdateResponse\";\xa2\xbc\xe6\xc0\x05\x35\x1a\x15\x08\x01\x12\x11request.workspace*\x1ctecton/workspace#create_plan\x12\xbe\x01\n\x10NewStateUpdateV2\x12\x35.tecton_proto.metadataservice.NewStateUpdateRequestV2\x1a\x36.tecton_proto.metadataservice.NewStateUpdateResponseV2\";\xa2\xbc\xe6\xc0\x05\x35\x1a\x15\x08\x01\x12\x11request.workspace*\x1ctecton/workspace#create_plan\x12\xb8\x01\n\x10ValidateLocalFco\x12\x35.tecton_proto.metadataservice.ValidateLocalFcoRequest\x1a\x36.tecton_proto.metadataservice.ValidateLocalFcoResponse\"5\xa2\xbc\xe6\xc0\x05/*-tecton/organization#validate_notebook_objects\x12\xb6\x01\n\x10QueryStateUpdate\x12\x35.tecton_proto.metadataservice.QueryStateUpdateRequest\x1a\x36.tecton_proto.metadataservice.QueryStateUpdateResponse\"3\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#create_plan\x12\xbc\x01\n\x12QueryStateUpdateV2\x12\x37.tecton_proto.metadataservice.QueryStateUpdateRequestV2\x1a\x38.tecton_proto.metadataservice.QueryStateUpdateResponseV2\"3\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#create_plan\x12\x8b\x01\n\x10\x41pplyStateUpdate\x12\x35.tecton_proto.metadataservice.ApplyStateUpdateRequest\x1a\x36.tecton_proto.metadataservice.ApplyStateUpdateResponse\"\x08\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xb0\x01\n\nGetConfigs\x12\x16.google.protobuf.Empty\x1a\x30.tecton_proto.metadataservice.GetConfigsResponse\"X\x82\xd3\xe4\x93\x02%\" /v1/metadata-service/get-configs:\x01*\xa2\xbc\xe6\xc0\x05\'*%tecton/organization#read_organization\x12\xcb\x01\n\x12GetGlobalsForWebUI\x12\x16.google.protobuf.Empty\x1a\x38.tecton_proto.metadataservice.GetGlobalsForWebUIResponse\"c\x82\xd3\xe4\x93\x02\x30\"+/v1/metadata-service/get-globals-for-web-ui:\x01*\xa2\xbc\xe6\xc0\x05\'*%tecton/organization#read_organization\x12\x98\x01\n\x0fGetLoginConfigs\x12\x16.google.protobuf.Empty\x1a\x30.tecton_proto.metadataservice.GetConfigsResponse\";\x82\xd3\xe4\x93\x02+\"&/v1/metadata-service/get-login-configs:\x01*\xa2\xbc\xe6\xc0\x05\x04\x08\x01\x10\x01\x12?\n\x03Nop\x12\x16.google.protobuf.Empty\x1a\x16.google.protobuf.Empty\"\x08\xa2\xbc\xe6\xc0\x05\x02\x10\x01\x12\xe3\x01\n\x19GetStateUpdatePlanSummary\x12>.tecton_proto.metadataservice.GetStateUpdatePlanSummaryRequest\x1a?.tecton_proto.metadataservice.GetStateUpdatePlanSummaryResponse\"E\x82\xd3\xe4\x93\x02\x37\"2/v1/metadata-service/get-state-update-plan-summary:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xed\x01\n\x11GetStateUpdateLog\x12\x36.tecton_proto.metadataservice.GetStateUpdateLogRequest\x1a\x37.tecton_proto.metadataservice.GetStateUpdateLogResponse\"g\x82\xd3\xe4\x93\x02.\")/v1/metadata-service/get-state-update-log:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\xb0\x01\n\x0eGetRestoreInfo\x12\x33.tecton_proto.metadataservice.GetRestoreInfoRequest\x1a\x34.tecton_proto.metadataservice.GetRestoreInfoResponse\"3\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\xab\x02\n\x0f\x43reateWorkspace\x12\x34.tecton_proto.metadataservice.CreateWorkspaceRequest\x1a\x16.google.protobuf.Empty\"\xc9\x01\xa2\xbc\xe6\xc0\x05v*$tecton/organization#create_workspace2N\n\x1b\x63\x61pabilities.materializable\x12\x04true\x1a)tecton/organization#create_workspace_live\x82\xd3\xe4\x93\x02*\"%/v1/metadata-service/create-workspace:\x01*\xaa\xbc\xe6\xc0\x05\x17\x08\x01\x12\x10\x63reate_workspace\x1a\x01\x31\x12\xb6\x01\n\x0f\x44\x65leteWorkspace\x12\x34.tecton_proto.metadataservice.DeleteWorkspaceRequest\x1a\x16.google.protobuf.Empty\"U\xa2\xbc\xe6\xc0\x05\x32\x1a\r\x08\x01\x12\tworkspace*!tecton/workspace#delete_workspace\xaa\xbc\xe6\xc0\x05\x17\x08\x01\x12\x10\x64\x65lete_workspace\x1a\x01\x31\x12\xb4\x01\n\x0eListWorkspaces\x12\x33.tecton_proto.metadataservice.ListWorkspacesRequest\x1a\x34.tecton_proto.metadataservice.ListWorkspacesResponse\"7\x82\xd3\xe4\x93\x02)\"$/v1/metadata-service/list-workspaces:\x01*\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xdf\x01\n\x0cGetWorkspace\x12\x31.tecton_proto.metadataservice.GetWorkspaceRequest\x1a\x32.tecton_proto.metadataservice.GetWorkspaceResponse\"h\x82\xd3\xe4\x93\x02\'\"\"/v1/metadata-service/get-workspace:\x01*\xa2\xbc\xe6\xc0\x05\x35\x1a\x12\x08\x01\x12\x0eworkspace_name*\x1ftecton/workspace#read_workspace\x12\xe5\x01\n\x10IntrospectApiKey\x12\x35.tecton_proto.metadataservice.IntrospectApiKeyRequest\x1a\x36.tecton_proto.metadataservice.IntrospectApiKeyResponse\"b\x82\xd3\xe4\x93\x02,\"\'/v1/metadata-service/introspect-api-key:\x01*\xa2\xbc\xe6\xc0\x05**(tecton/organization#list_service_account\x12\x9b\x02\n\x14\x43reateServiceAccount\x12\x39.tecton_proto.metadataservice.CreateServiceAccountRequest\x1a:.tecton_proto.metadataservice.CreateServiceAccountResponse\"\x8b\x01\x82\xd3\xe4\x93\x02\x30\"+/v1/metadata-service/create-service-account:\x01*\xa2\xbc\xe6\xc0\x05,**tecton/organization#create_service_account\xaa\xbc\xe6\xc0\x05\x1d\x08\x01\x12\x16\x63reate_service_account\x1a\x01\x31\x12\xec\x01\n\x12GetServiceAccounts\x12\x37.tecton_proto.metadataservice.GetServiceAccountsRequest\x1a\x38.tecton_proto.metadataservice.GetServiceAccountsResponse\"c\x82\xd3\xe4\x93\x02-\"(/v1/metadata-service/get-service-account:\x01*\xa2\xbc\xe6\xc0\x05**(tecton/organization#list_service_account\x12\xa6\x02\n\x14UpdateServiceAccount\x12\x39.tecton_proto.metadataservice.UpdateServiceAccountRequest\x1a:.tecton_proto.metadataservice.UpdateServiceAccountResponse\"\x96\x01\x82\xd3\xe4\x93\x02\x30\"+/v1/metadata-service/update-service-account:\x01*\xa2\xbc\xe6\xc0\x05\x37\x1a\x06\x08\x02\x12\x02id*-tecton/service_account#manage_service_account\xaa\xbc\xe6\xc0\x05\x1d\x08\x01\x12\x16update_service_account\x1a\x01\x31\x12\xa6\x02\n\x14\x44\x65leteServiceAccount\x12\x39.tecton_proto.metadataservice.DeleteServiceAccountRequest\x1a:.tecton_proto.metadataservice.DeleteServiceAccountResponse\"\x96\x01\x82\xd3\xe4\x93\x02\x30\"+/v1/metadata-service/delete-service-account:\x01*\xa2\xbc\xe6\xc0\x05\x37\x1a\x06\x08\x02\x12\x02id*-tecton/service_account#manage_service_account\xaa\xbc\xe6\xc0\x05\x1d\x08\x01\x12\x16\x64\x65lete_service_account\x1a\x01\x31\x12\x8b\x02\n\x0c\x43reateApiKey\x12\x31.tecton_proto.metadataservice.CreateApiKeyRequest\x1a\x32.tecton_proto.metadataservice.CreateApiKeyResponse\"\x93\x01\xa2\xbc\xe6\xc0\x05r**tecton/organization#create_service_account2B\n\x08is_admin\x12\x04true\x1a\x30tecton/organization#create_service_account_admin8\x01\xaa\xbc\xe6\xc0\x05\x15\x08\x01\x12\x0e\x63reate_api_key\x1a\x01\x31\x12\xb7\x01\n\x0c\x44\x65leteApiKey\x12\x31.tecton_proto.metadataservice.DeleteApiKeyRequest\x1a\x16.google.protobuf.Empty\"\\\xa2\xbc\xe6\xc0\x05\x39\x1a\x06\x08\x02\x12\x02id*-tecton/service_account#manage_service_account8\x01\xaa\xbc\xe6\xc0\x05\x17\x08\x01\x12\x10\x63reate_workspace\x1a\x01\x31\x12\xa4\x01\n\x0bListApiKeys\x12\x30.tecton_proto.metadataservice.ListApiKeysRequest\x1a\x31.tecton_proto.metadataservice.ListApiKeysResponse\"0\xa2\xbc\xe6\xc0\x05**(tecton/organization#list_service_account\x12\xda\x01\n\x1b\x43reateSavedFeatureDataFrame\x12@.tecton_proto.metadataservice.CreateSavedFeatureDataFrameRequest\x1a\x41.tecton_proto.metadataservice.CreateSavedFeatureDataFrameResponse\"6\xa2\xbc\xe6\xc0\x05\x30\x1a\r\x08\x01\x12\tworkspace*\x1ftecton/workspace#update_dataset\x12\xaf\x01\n\x1c\x41rchiveSavedFeatureDataFrame\x12\x41.tecton_proto.metadataservice.ArchiveSavedFeatureDataFrameRequest\x1a\x42.tecton_proto.metadataservice.ArchiveSavedFeatureDataFrameResponse\"\x08\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xcf\x01\n\x18GetSavedFeatureDataFrame\x12=.tecton_proto.metadataservice.GetSavedFeatureDataFrameRequest\x1a>.tecton_proto.metadataservice.GetSavedFeatureDataFrameResponse\"4\xa2\xbc\xe6\xc0\x05.\x1a\r\x08\x01\x12\tworkspace*\x1dtecton/workspace#read_dataset\x12\x9b\x02\n\x1cGetAllSavedFeatureDataFrames\x12\x41.tecton_proto.metadataservice.GetAllSavedFeatureDataFramesRequest\x1a\x42.tecton_proto.metadataservice.GetAllSavedFeatureDataFramesResponse\"t\x82\xd3\xe4\x93\x02:\"5/v1/metadata-service/get-all-saved-feature-dataframes:\x01*\xa2\xbc\xe6\xc0\x05.\x1a\r\x08\x01\x12\tworkspace*\x1dtecton/workspace#read_dataset\x12\xa6\x01\n\x19GetNewIngestDataframeInfo\x12>.tecton_proto.metadataservice.GetNewIngestDataframeInfoRequest\x1a?.tecton_proto.metadataservice.GetNewIngestDataframeInfoResponse\"\x08\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xb7\x01\n\x0fIngestDataframe\x12\x34.tecton_proto.metadataservice.IngestDataframeRequest\x1a\x35.tecton_proto.metadataservice.IngestDataframeResponse\"7\xa2\xbc\xe6\xc0\x05\x31\x1a\r\x08\x01\x12\tworkspace* tecton/workspace#ingest_features\x12\xcd\x01\n\x13GetClusterAdminInfo\x12\x16.google.protobuf.Empty\x1a\x39.tecton_proto.metadataservice.GetClusterAdminInfoResponse\"c\x82\xd3\xe4\x93\x02\x30\"+/v1/metadata-service/get-cluster-admin-info:\x01*\xa2\xbc\xe6\xc0\x05\'*%tecton/organization#read_organization\x12\x80\x02\n\x11\x43reateClusterUser\x12\x36.tecton_proto.metadataservice.CreateClusterUserRequest\x1a\x37.tecton_proto.metadataservice.CreateClusterUserResponse\"z\x82\xd3\xe4\x93\x02-\"(/v1/metadata-service/create-cluster-user:\x01*\xa2\xbc\xe6\xc0\x05!*\x1ftecton/organization#create_user\xaa\xbc\xe6\xc0\x05\x1a\x08\x01\x12\x13\x63reate_cluster_user\x1a\x01\x31\x12\x80\x02\n\x11\x44\x65leteClusterUser\x12\x36.tecton_proto.metadataservice.DeleteClusterUserRequest\x1a\x37.tecton_proto.metadataservice.DeleteClusterUserResponse\"z\x82\xd3\xe4\x93\x02-\"(/v1/metadata-service/delete-cluster-user:\x01*\xa2\xbc\xe6\xc0\x05!*\x1ftecton/organization#manage_user\xaa\xbc\xe6\xc0\x05\x1a\x08\x01\x12\x13\x64\x65lete_cluster_user\x1a\x01\x31\x12\x80\x02\n\x11\x43lusterUserAction\x12\x36.tecton_proto.metadataservice.ClusterUserActionRequest\x1a\x37.tecton_proto.metadataservice.ClusterUserActionResponse\"z\x82\xd3\xe4\x93\x02-\"(/v1/metadata-service/cluster-user-action:\x01*\xa2\xbc\xe6\xc0\x05!*\x1ftecton/organization#manage_user\xaa\xbc\xe6\xc0\x05\x1a\x08\x01\x12\x13\x63luster_user_action\x1a\x01\x31\x12\xdf\x01\n\x19GetUserDeploymentSettings\x12\x16.google.protobuf.Empty\x1a?.tecton_proto.metadataservice.GetUserDeploymentSettingsResponse\"i\x82\xd3\xe4\x93\x02\x36\"1/v1/metadata-service/get-user-deployment-settings:\x01*\xa2\xbc\xe6\xc0\x05\'*%tecton/organization#read_organization\x12\x9c\x02\n\x1cUpdateUserDeploymentSettings\x12\x41.tecton_proto.metadataservice.UpdateUserDeploymentSettingsRequest\x1a\x42.tecton_proto.metadataservice.UpdateUserDeploymentSettingsResponse\"u\x82\xd3\xe4\x93\x02\x39\"4/v1/metadata-service/update-user-deployment-settings:\x01*\xa2\xbc\xe6\xc0\x05\x30*.tecton/organization#manage_deployment_settings\x12\xec\x01\n\x1dGetInternalSparkClusterStatus\x12\x16.google.protobuf.Empty\x1a\x43.tecton_proto.metadataservice.GetInternalSparkClusterStatusResponse\"n\x82\xd3\xe4\x93\x02;\"6/v1/metadata-service/get-internal-spark-cluster-status:\x01*\xa2\xbc\xe6\xc0\x05\'*%tecton/organization#read_organization\x12\x9a\x01\n\x15GetDeleteEntitiesInfo\x12:.tecton_proto.metadataservice.GetDeleteEntitiesInfoRequest\x1a;.tecton_proto.metadataservice.GetDeleteEntitiesInfoResponse\"\x08\xa2\xbc\xe6\xc0\x05\x02@\x01\x12\xc0\x01\n\x0e\x44\x65leteEntities\x12\x33.tecton_proto.metadataservice.DeleteEntitiesRequest\x1a\x34.tecton_proto.metadataservice.DeleteEntitiesResponse\"C\xa2\xbc\xe6\xc0\x05=\x1a\x19\x08\x01\x12\x15\x66\x63o_locator.workspace* tecton/workspace#delete_features\x12\xde\x01\n\x0fGetHiveMetadata\x12\x34.tecton_proto.metadataservice.GetHiveMetadataRequest\x1a\x35.tecton_proto.metadataservice.GetHiveMetadataResponse\"^\x82\xd3\xe4\x93\x02+\"&/v1/metadata-service/get-hive-metadata:\x01*\xa2\xbc\xe6\xc0\x05\'*%tecton/organization#read_organization\x12\xf7\x01\n\x13GetOnboardingStatus\x12\x38.tecton_proto.metadataservice.GetOnboardingStatusRequest\x1a\x39.tecton_proto.metadataservice.GetOnboardingStatusResponse\"k\x82\xd3\xe4\x93\x02/\"*/v1/metadata-service/get-onboarding-status:\x01*\xa2\xbc\xe6\xc0\x05\x30\x1a\r\x08\x01\x12\tworkspace*\x1ftecton/workspace#read_workspace\x12\xe3\x01\n\x1aGetDataPlatformSetupStatus\x12\x16.google.protobuf.Empty\x1a@.tecton_proto.metadataservice.GetDataPlatformSetupStatusResponse\"k\x82\xd3\xe4\x93\x02\x38\"3/v1/metadata-service/get-data-platform-setup-status:\x01*\xa2\xbc\xe6\xc0\x05\'*%tecton/organization#read_organization\x12\x80\x02\n\x16GetObservabilityConfig\x12;.tecton_proto.metadataservice.GetObservabilityConfigRequest\x1a<.tecton_proto.metadataservice.GetObservabilityConfigResponse\"k\x82\xd3\xe4\x93\x02\x32\"-/v1/metadata-service/get-observability-config:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\xd3\x01\n\x0bQueryMetric\x12\x30.tecton_proto.metadataservice.QueryMetricRequest\x1a\x31.tecton_proto.metadataservice.QueryMetricResponse\"_\x82\xd3\xe4\x93\x02&\"!/v1/metadata-service/query-metric:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_config\x12\x95\x02\n\x1bGetFeatureValidationSummary\x12@.tecton_proto.metadataservice.GetFeatureValidationSummaryRequest\x1a\x41.tecton_proto.metadataservice.GetFeatureValidationSummaryResponse\"q\x82\xd3\xe4\x93\x02\x38\"3/v1/metadata-service/get-feature-validation-summary:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_metric\x12\x91\x02\n\x1aGetFeatureValidationResult\x12?.tecton_proto.metadataservice.GetFeatureValidationResultRequest\x1a@.tecton_proto.metadataservice.GetFeatureValidationResultResponse\"p\x82\xd3\xe4\x93\x02\x37\"2/v1/metadata-service/get-feature-validation-result:\x01*\xa2\xbc\xe6\xc0\x05-\x1a\r\x08\x01\x12\tworkspace*\x1ctecton/workspace#read_metric\x12\xfb\x01\n\x16GetFeatureServerConfig\x12;.tecton_proto.metadataservice.GetFeatureServerConfigRequest\x1a<.tecton_proto.metadataservice.GetFeatureServerConfigResponse\"f\x82\xd3\xe4\x93\x02\x33\"./v1/metadata-service/get-feature-server-config:\x01*\xa2\xbc\xe6\xc0\x05\'*%tecton/organization#read_organization\x12\xfe\x01\n\x16SetFeatureServerConfig\x12;.tecton_proto.metadataservice.SetFeatureServerConfigRequest\x1a<.tecton_proto.metadataservice.GetFeatureServerConfigResponse\"i\x82\xd3\xe4\x93\x02\x33\"./v1/metadata-service/set-feature-server-config:\x01*\xa2\xbc\xe6\xc0\x05**(tecton/organization#scale_feature_server\x12\xb5\x01\n\x07GetUser\x12,.tecton_proto.metadataservice.GetUserRequest\x1a-.tecton_proto.metadataservice.GetUserResponse\"M\x82\xd3\xe4\x93\x02\"\"\x1d/v1/metadata-service/get-user:\x01*\xa2\xbc\xe6\xc0\x05\x1f*\x1dtecton/organization#list_userBg\n\x1a\x63om.tecton.metadataserviceB\x14MetadataServiceProtoP\x01Z1github.com/tecton-ai/tecton_proto/metadataservice')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.metadataservice.metadata_service_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\032com.tecton.metadataserviceB\024MetadataServiceProtoP\001Z1github.com/tecton-ai/tecton_proto/metadataservice'
@@ -86,28 +84,30 @@
   _METADATASERVICE.methods_by_name['GetMaterializationStatus']._serialized_options = b'\202\323\344\223\0024\"//v1/metadata-service/get-materialization-status:\001*\242\274\346\300\005\002@\001'
   _METADATASERVICE.methods_by_name['GetAllMaterializationStatusInLiveWorkspaces']._options = None
   _METADATASERVICE.methods_by_name['GetAllMaterializationStatusInLiveWorkspaces']._serialized_options = b'\202\323\344\223\002J\"E/v1/metadata-service/get-all-materialization-status-in-live-workspace:\001*\242\274\346\300\005\002@\001'
   _METADATASERVICE.methods_by_name['GetJobs']._options = None
   _METADATASERVICE.methods_by_name['GetJobs']._serialized_options = b'\202\323\344\223\002\036\"\031/v1/metadata-service/jobs:\001*\242\274\346\300\005\002@\001'
   _METADATASERVICE.methods_by_name['ForceRetryMaterializationTask']._options = None
   _METADATASERVICE.methods_by_name['ForceRetryMaterializationTask']._serialized_options = b'\202\323\344\223\002:\"5/v1/metadata-service/force-retry-materialization-task:\001*\242\274\346\300\005\002@\001'
+  _METADATASERVICE.methods_by_name['RestartMaterializationTask']._options = None
+  _METADATASERVICE.methods_by_name['RestartMaterializationTask']._serialized_options = b'\202\323\344\223\0026\"1/v1/metadata-service/restart-materialization-task:\001*\242\274\346\300\005\002@\001'
   _METADATASERVICE.methods_by_name['GetSparkConfig']._options = None
   _METADATASERVICE.methods_by_name['GetSparkConfig']._serialized_options = b'\202\323\344\223\002*\"%/v1/metadata-service/get-spark-config:\001*\242\274\346\300\005-\032\r\010\001\022\tworkspace*\034tecton/workspace#read_config'
   _METADATASERVICE.methods_by_name['GetMetricAndExpectationDefinition']._options = None
   _METADATASERVICE.methods_by_name['GetMetricAndExpectationDefinition']._serialized_options = b'\202\323\344\223\002?\":/v1/metadata-service/get-metric-and-expectation-definition:\001*\242\274\346\300\005-\032\r\010\001\022\tworkspace*\034tecton/workspace#read_config'
   _METADATASERVICE.methods_by_name['GetFeatureView']._options = None
   _METADATASERVICE.methods_by_name['GetFeatureView']._serialized_options = b'\202\323\344\223\002*\"%/v1/metadata-service/get-feature-view:\001*\242\274\346\300\005-\032\r\010\001\022\tworkspace*\034tecton/workspace#read_config'
   _METADATASERVICE.methods_by_name['GetMaterializingFeatureViewsInLiveWorkspaces']._options = None
   _METADATASERVICE.methods_by_name['GetMaterializingFeatureViewsInLiveWorkspaces']._serialized_options = b'\202\323\344\223\002F\"A/v1/metadata-service/get-materializing-features-in-live-workspace:\001*\242\274\346\300\005\002@\001'
   _METADATASERVICE.methods_by_name['QueryFeatureViews']._options = None
   _METADATASERVICE.methods_by_name['QueryFeatureViews']._serialized_options = b'\202\323\344\223\002-\"(/v1/metadata-service/query-feature-views:\001*\242\274\346\300\005-\032\r\010\001\022\tworkspace*\034tecton/workspace#read_config'
   _METADATASERVICE.methods_by_name['GetFeatureViewSummary']._options = None
   _METADATASERVICE.methods_by_name['GetFeatureViewSummary']._serialized_options = b'\202\323\344\223\0022\"-/v1/metadata-package/get-feature-view-summary:\001*\242\274\346\300\005H\032\031\010\001\022\025fco_locator.workspace*+tecton/workspace#read_materialization_state'
   _METADATASERVICE.methods_by_name['GetFeatureConsumption']._options = None
-  _METADATASERVICE.methods_by_name['GetFeatureConsumption']._serialized_options = b'\202\323\344\223\0026\"1/v1/metadata-service/get-feature-view-consumption:\001*\242\274\346\300\0059\032\031\010\001\022\025fco_locator.workspace*\034tecton/workspace#read_config'
+  _METADATASERVICE.methods_by_name['GetFeatureConsumption']._serialized_options = b'\202\323\344\223\0026\"1/v1/metadata-service/get-feature-view-consumption:\001*\242\274\346\300\005>\032\031\010\001\022\025fco_locator.workspace*!tecton/workspace#read_consumption'
   _METADATASERVICE.methods_by_name['GetVirtualDataSource']._options = None
   _METADATASERVICE.methods_by_name['GetVirtualDataSource']._serialized_options = b'\202\323\344\223\0021\",/v1/metadata-service/get-virtual-data-source:\001*\242\274\346\300\005-\032\r\010\001\022\tworkspace*\034tecton/workspace#read_config'
   _METADATASERVICE.methods_by_name['GetAllVirtualDataSources']._options = None
   _METADATASERVICE.methods_by_name['GetAllVirtualDataSources']._serialized_options = b'\202\323\344\223\0026\"1/v1/metadata-service/get-all-virtual-data-sources:\001*\242\274\346\300\005-\032\r\010\001\022\tworkspace*\034tecton/workspace#read_config'
   _METADATASERVICE.methods_by_name['IngestClientLogs']._options = None
   _METADATASERVICE.methods_by_name['IngestClientLogs']._serialized_options = b'\242\274\346\300\005\'*%tecton/organization#read_organization'
   _METADATASERVICE.methods_by_name['IngestAnalytics']._options = None
@@ -127,23 +127,23 @@
   _METADATASERVICE.methods_by_name['GetAllTransformations']._options = None
   _METADATASERVICE.methods_by_name['GetAllTransformations']._serialized_options = b'\202\323\344\223\0021\",/v1/metadata-service/get-all-transformations:\001*\242\274\346\300\005-\032\r\010\001\022\tworkspace*\034tecton/workspace#read_config'
   _METADATASERVICE.methods_by_name['GetTransformationSummary']._options = None
   _METADATASERVICE.methods_by_name['GetTransformationSummary']._serialized_options = b'\202\323\344\223\0024\"//v1/metadata-package/get-transformation-summary:\001*\242\274\346\300\0059\032\031\010\001\022\025fco_locator.workspace*\034tecton/workspace#read_config'
   _METADATASERVICE.methods_by_name['FindFcoWorkspace']._options = None
   _METADATASERVICE.methods_by_name['FindFcoWorkspace']._serialized_options = b'\202\323\344\223\002,\"\'/v1/metadata-service/find-fco-workspace:\001*\242\274\346\300\005\002@\001'
   _METADATASERVICE.methods_by_name['NewStateUpdate']._options = None
-  _METADATASERVICE.methods_by_name['NewStateUpdate']._serialized_options = b'\242\274\346\300\0055\032\025\010\001\022\021request.workspace*\034tecton/workspace#read_config'
+  _METADATASERVICE.methods_by_name['NewStateUpdate']._serialized_options = b'\242\274\346\300\0055\032\025\010\001\022\021request.workspace*\034tecton/workspace#create_plan'
   _METADATASERVICE.methods_by_name['NewStateUpdateV2']._options = None
-  _METADATASERVICE.methods_by_name['NewStateUpdateV2']._serialized_options = b'\242\274\346\300\0055\032\025\010\001\022\021request.workspace*\034tecton/workspace#read_config'
+  _METADATASERVICE.methods_by_name['NewStateUpdateV2']._serialized_options = b'\242\274\346\300\0055\032\025\010\001\022\021request.workspace*\034tecton/workspace#create_plan'
   _METADATASERVICE.methods_by_name['ValidateLocalFco']._options = None
   _METADATASERVICE.methods_by_name['ValidateLocalFco']._serialized_options = b'\242\274\346\300\005/*-tecton/organization#validate_notebook_objects'
   _METADATASERVICE.methods_by_name['QueryStateUpdate']._options = None
-  _METADATASERVICE.methods_by_name['QueryStateUpdate']._serialized_options = b'\242\274\346\300\005-\032\r\010\001\022\tworkspace*\034tecton/workspace#read_config'
+  _METADATASERVICE.methods_by_name['QueryStateUpdate']._serialized_options = b'\242\274\346\300\005-\032\r\010\001\022\tworkspace*\034tecton/workspace#create_plan'
   _METADATASERVICE.methods_by_name['QueryStateUpdateV2']._options = None
-  _METADATASERVICE.methods_by_name['QueryStateUpdateV2']._serialized_options = b'\242\274\346\300\005-\032\r\010\001\022\tworkspace*\034tecton/workspace#read_config'
+  _METADATASERVICE.methods_by_name['QueryStateUpdateV2']._serialized_options = b'\242\274\346\300\005-\032\r\010\001\022\tworkspace*\034tecton/workspace#create_plan'
   _METADATASERVICE.methods_by_name['ApplyStateUpdate']._options = None
   _METADATASERVICE.methods_by_name['ApplyStateUpdate']._serialized_options = b'\242\274\346\300\005\002@\001'
   _METADATASERVICE.methods_by_name['GetConfigs']._options = None
   _METADATASERVICE.methods_by_name['GetConfigs']._serialized_options = b'\202\323\344\223\002%\" /v1/metadata-service/get-configs:\001*\242\274\346\300\005\'*%tecton/organization#read_organization'
   _METADATASERVICE.methods_by_name['GetGlobalsForWebUI']._options = None
   _METADATASERVICE.methods_by_name['GetGlobalsForWebUI']._serialized_options = b'\202\323\344\223\0020\"+/v1/metadata-service/get-globals-for-web-ui:\001*\242\274\346\300\005\'*%tecton/organization#read_organization'
   _METADATASERVICE.methods_by_name['GetLoginConfigs']._options = None
@@ -153,35 +153,35 @@
   _METADATASERVICE.methods_by_name['GetStateUpdatePlanSummary']._options = None
   _METADATASERVICE.methods_by_name['GetStateUpdatePlanSummary']._serialized_options = b'\202\323\344\223\0027\"2/v1/metadata-service/get-state-update-plan-summary:\001*\242\274\346\300\005\002@\001'
   _METADATASERVICE.methods_by_name['GetStateUpdateLog']._options = None
   _METADATASERVICE.methods_by_name['GetStateUpdateLog']._serialized_options = b'\202\323\344\223\002.\")/v1/metadata-service/get-state-update-log:\001*\242\274\346\300\005-\032\r\010\001\022\tworkspace*\034tecton/workspace#read_config'
   _METADATASERVICE.methods_by_name['GetRestoreInfo']._options = None
   _METADATASERVICE.methods_by_name['GetRestoreInfo']._serialized_options = b'\242\274\346\300\005-\032\r\010\001\022\tworkspace*\034tecton/workspace#read_config'
   _METADATASERVICE.methods_by_name['CreateWorkspace']._options = None
-  _METADATASERVICE.methods_by_name['CreateWorkspace']._serialized_options = b'\242\274\346\300\005v*$tecton/organization#create_workspace2N\n\033capabilities.materializable\022\004true\032)tecton/organization#create_workspace_live\202\323\344\223\002*\"%/v1/metadata-service/create-workspace:\001*'
+  _METADATASERVICE.methods_by_name['CreateWorkspace']._serialized_options = b'\242\274\346\300\005v*$tecton/organization#create_workspace2N\n\033capabilities.materializable\022\004true\032)tecton/organization#create_workspace_live\202\323\344\223\002*\"%/v1/metadata-service/create-workspace:\001*\252\274\346\300\005\027\010\001\022\020create_workspace\032\0011'
   _METADATASERVICE.methods_by_name['DeleteWorkspace']._options = None
-  _METADATASERVICE.methods_by_name['DeleteWorkspace']._serialized_options = b'\242\274\346\300\0052\032\r\010\001\022\tworkspace*!tecton/workspace#delete_workspace'
+  _METADATASERVICE.methods_by_name['DeleteWorkspace']._serialized_options = b'\242\274\346\300\0052\032\r\010\001\022\tworkspace*!tecton/workspace#delete_workspace\252\274\346\300\005\027\010\001\022\020delete_workspace\032\0011'
   _METADATASERVICE.methods_by_name['ListWorkspaces']._options = None
   _METADATASERVICE.methods_by_name['ListWorkspaces']._serialized_options = b'\202\323\344\223\002)\"$/v1/metadata-service/list-workspaces:\001*\242\274\346\300\005\002@\001'
   _METADATASERVICE.methods_by_name['GetWorkspace']._options = None
   _METADATASERVICE.methods_by_name['GetWorkspace']._serialized_options = b'\202\323\344\223\002\'\"\"/v1/metadata-service/get-workspace:\001*\242\274\346\300\0055\032\022\010\001\022\016workspace_name*\037tecton/workspace#read_workspace'
   _METADATASERVICE.methods_by_name['IntrospectApiKey']._options = None
   _METADATASERVICE.methods_by_name['IntrospectApiKey']._serialized_options = b'\202\323\344\223\002,\"\'/v1/metadata-service/introspect-api-key:\001*\242\274\346\300\005**(tecton/organization#list_service_account'
   _METADATASERVICE.methods_by_name['CreateServiceAccount']._options = None
-  _METADATASERVICE.methods_by_name['CreateServiceAccount']._serialized_options = b'\202\323\344\223\0020\"+/v1/metadata-service/create-service-account:\001*\242\274\346\300\005,**tecton/organization#create_service_account'
+  _METADATASERVICE.methods_by_name['CreateServiceAccount']._serialized_options = b'\202\323\344\223\0020\"+/v1/metadata-service/create-service-account:\001*\242\274\346\300\005,**tecton/organization#create_service_account\252\274\346\300\005\035\010\001\022\026create_service_account\032\0011'
   _METADATASERVICE.methods_by_name['GetServiceAccounts']._options = None
   _METADATASERVICE.methods_by_name['GetServiceAccounts']._serialized_options = b'\202\323\344\223\002-\"(/v1/metadata-service/get-service-account:\001*\242\274\346\300\005**(tecton/organization#list_service_account'
   _METADATASERVICE.methods_by_name['UpdateServiceAccount']._options = None
-  _METADATASERVICE.methods_by_name['UpdateServiceAccount']._serialized_options = b'\202\323\344\223\0020\"+/v1/metadata-service/update-service-account:\001*\242\274\346\300\0057\032\006\010\002\022\002id*-tecton/service_account#manage_service_account'
+  _METADATASERVICE.methods_by_name['UpdateServiceAccount']._serialized_options = b'\202\323\344\223\0020\"+/v1/metadata-service/update-service-account:\001*\242\274\346\300\0057\032\006\010\002\022\002id*-tecton/service_account#manage_service_account\252\274\346\300\005\035\010\001\022\026update_service_account\032\0011'
   _METADATASERVICE.methods_by_name['DeleteServiceAccount']._options = None
-  _METADATASERVICE.methods_by_name['DeleteServiceAccount']._serialized_options = b'\202\323\344\223\0020\"+/v1/metadata-service/delete-service-account:\001*\242\274\346\300\0057\032\006\010\002\022\002id*-tecton/service_account#manage_service_account'
+  _METADATASERVICE.methods_by_name['DeleteServiceAccount']._serialized_options = b'\202\323\344\223\0020\"+/v1/metadata-service/delete-service-account:\001*\242\274\346\300\0057\032\006\010\002\022\002id*-tecton/service_account#manage_service_account\252\274\346\300\005\035\010\001\022\026delete_service_account\032\0011'
   _METADATASERVICE.methods_by_name['CreateApiKey']._options = None
-  _METADATASERVICE.methods_by_name['CreateApiKey']._serialized_options = b'\242\274\346\300\005r**tecton/organization#create_service_account2B\n\010is_admin\022\004true\0320tecton/organization#create_service_account_admin8\001'
+  _METADATASERVICE.methods_by_name['CreateApiKey']._serialized_options = b'\242\274\346\300\005r**tecton/organization#create_service_account2B\n\010is_admin\022\004true\0320tecton/organization#create_service_account_admin8\001\252\274\346\300\005\025\010\001\022\016create_api_key\032\0011'
   _METADATASERVICE.methods_by_name['DeleteApiKey']._options = None
-  _METADATASERVICE.methods_by_name['DeleteApiKey']._serialized_options = b'\242\274\346\300\0059\032\006\010\002\022\002id*-tecton/service_account#manage_service_account8\001'
+  _METADATASERVICE.methods_by_name['DeleteApiKey']._serialized_options = b'\242\274\346\300\0059\032\006\010\002\022\002id*-tecton/service_account#manage_service_account8\001\252\274\346\300\005\027\010\001\022\020create_workspace\032\0011'
   _METADATASERVICE.methods_by_name['ListApiKeys']._options = None
   _METADATASERVICE.methods_by_name['ListApiKeys']._serialized_options = b'\242\274\346\300\005**(tecton/organization#list_service_account'
   _METADATASERVICE.methods_by_name['CreateSavedFeatureDataFrame']._options = None
   _METADATASERVICE.methods_by_name['CreateSavedFeatureDataFrame']._serialized_options = b'\242\274\346\300\0050\032\r\010\001\022\tworkspace*\037tecton/workspace#update_dataset'
   _METADATASERVICE.methods_by_name['ArchiveSavedFeatureDataFrame']._options = None
   _METADATASERVICE.methods_by_name['ArchiveSavedFeatureDataFrame']._serialized_options = b'\242\274\346\300\005\002@\001'
   _METADATASERVICE.methods_by_name['GetSavedFeatureDataFrame']._options = None
@@ -191,387 +191,369 @@
   _METADATASERVICE.methods_by_name['GetNewIngestDataframeInfo']._options = None
   _METADATASERVICE.methods_by_name['GetNewIngestDataframeInfo']._serialized_options = b'\242\274\346\300\005\002@\001'
   _METADATASERVICE.methods_by_name['IngestDataframe']._options = None
   _METADATASERVICE.methods_by_name['IngestDataframe']._serialized_options = b'\242\274\346\300\0051\032\r\010\001\022\tworkspace* tecton/workspace#ingest_features'
   _METADATASERVICE.methods_by_name['GetClusterAdminInfo']._options = None
   _METADATASERVICE.methods_by_name['GetClusterAdminInfo']._serialized_options = b'\202\323\344\223\0020\"+/v1/metadata-service/get-cluster-admin-info:\001*\242\274\346\300\005\'*%tecton/organization#read_organization'
   _METADATASERVICE.methods_by_name['CreateClusterUser']._options = None
-  _METADATASERVICE.methods_by_name['CreateClusterUser']._serialized_options = b'\202\323\344\223\002-\"(/v1/metadata-service/create-cluster-user:\001*\242\274\346\300\005!*\037tecton/organization#create_user'
+  _METADATASERVICE.methods_by_name['CreateClusterUser']._serialized_options = b'\202\323\344\223\002-\"(/v1/metadata-service/create-cluster-user:\001*\242\274\346\300\005!*\037tecton/organization#create_user\252\274\346\300\005\032\010\001\022\023create_cluster_user\032\0011'
   _METADATASERVICE.methods_by_name['DeleteClusterUser']._options = None
-  _METADATASERVICE.methods_by_name['DeleteClusterUser']._serialized_options = b'\202\323\344\223\002-\"(/v1/metadata-service/delete-cluster-user:\001*\242\274\346\300\005!*\037tecton/organization#manage_user'
+  _METADATASERVICE.methods_by_name['DeleteClusterUser']._serialized_options = b'\202\323\344\223\002-\"(/v1/metadata-service/delete-cluster-user:\001*\242\274\346\300\005!*\037tecton/organization#manage_user\252\274\346\300\005\032\010\001\022\023delete_cluster_user\032\0011'
   _METADATASERVICE.methods_by_name['ClusterUserAction']._options = None
-  _METADATASERVICE.methods_by_name['ClusterUserAction']._serialized_options = b'\202\323\344\223\002-\"(/v1/metadata-service/cluster-user-action:\001*\242\274\346\300\005!*\037tecton/organization#manage_user'
+  _METADATASERVICE.methods_by_name['ClusterUserAction']._serialized_options = b'\202\323\344\223\002-\"(/v1/metadata-service/cluster-user-action:\001*\242\274\346\300\005!*\037tecton/organization#manage_user\252\274\346\300\005\032\010\001\022\023cluster_user_action\032\0011'
   _METADATASERVICE.methods_by_name['GetUserDeploymentSettings']._options = None
   _METADATASERVICE.methods_by_name['GetUserDeploymentSettings']._serialized_options = b'\202\323\344\223\0026\"1/v1/metadata-service/get-user-deployment-settings:\001*\242\274\346\300\005\'*%tecton/organization#read_organization'
   _METADATASERVICE.methods_by_name['UpdateUserDeploymentSettings']._options = None
   _METADATASERVICE.methods_by_name['UpdateUserDeploymentSettings']._serialized_options = b'\202\323\344\223\0029\"4/v1/metadata-service/update-user-deployment-settings:\001*\242\274\346\300\0050*.tecton/organization#manage_deployment_settings'
   _METADATASERVICE.methods_by_name['GetInternalSparkClusterStatus']._options = None
   _METADATASERVICE.methods_by_name['GetInternalSparkClusterStatus']._serialized_options = b'\202\323\344\223\002;\"6/v1/metadata-service/get-internal-spark-cluster-status:\001*\242\274\346\300\005\'*%tecton/organization#read_organization'
   _METADATASERVICE.methods_by_name['GetDeleteEntitiesInfo']._options = None
   _METADATASERVICE.methods_by_name['GetDeleteEntitiesInfo']._serialized_options = b'\242\274\346\300\005\002@\001'
   _METADATASERVICE.methods_by_name['DeleteEntities']._options = None
   _METADATASERVICE.methods_by_name['DeleteEntities']._serialized_options = b'\242\274\346\300\005=\032\031\010\001\022\025fco_locator.workspace* tecton/workspace#delete_features'
   _METADATASERVICE.methods_by_name['GetHiveMetadata']._options = None
   _METADATASERVICE.methods_by_name['GetHiveMetadata']._serialized_options = b'\202\323\344\223\002+\"&/v1/metadata-service/get-hive-metadata:\001*\242\274\346\300\005\'*%tecton/organization#read_organization'
-  _METADATASERVICE.methods_by_name['GetFileMetadata']._options = None
-  _METADATASERVICE.methods_by_name['GetFileMetadata']._serialized_options = b'\202\323\344\223\002+\"&/v1/metadata-service/get-file-metadata:\001*\242\274\346\300\005\'*%tecton/organization#read_organization'
-  _METADATASERVICE.methods_by_name['ValidateFcos']._options = None
-  _METADATASERVICE.methods_by_name['ValidateFcos']._serialized_options = b'\202\323\344\223\002\'\"\"/v1/metadata-service/validate-fcos:\001*\242\274\346\300\0050*.tecton/organization#manage_deployment_settings'
   _METADATASERVICE.methods_by_name['GetOnboardingStatus']._options = None
   _METADATASERVICE.methods_by_name['GetOnboardingStatus']._serialized_options = b'\202\323\344\223\002/\"*/v1/metadata-service/get-onboarding-status:\001*\242\274\346\300\0050\032\r\010\001\022\tworkspace*\037tecton/workspace#read_workspace'
-  _METADATASERVICE.methods_by_name['SetOnboardingStatus']._options = None
-  _METADATASERVICE.methods_by_name['SetOnboardingStatus']._serialized_options = b'\202\323\344\223\002/\"*/v1/metadata-service/set-onboarding-status:\001*\242\274\346\300\005\'*%tecton/organization#read_organization'
   _METADATASERVICE.methods_by_name['GetDataPlatformSetupStatus']._options = None
   _METADATASERVICE.methods_by_name['GetDataPlatformSetupStatus']._serialized_options = b'\202\323\344\223\0028\"3/v1/metadata-service/get-data-platform-setup-status:\001*\242\274\346\300\005\'*%tecton/organization#read_organization'
-  _METADATASERVICE.methods_by_name['GetRetrieveFeaturesNotebook']._options = None
-  _METADATASERVICE.methods_by_name['GetRetrieveFeaturesNotebook']._serialized_options = b'\202\323\344\223\0028\"3/v1/metadata-service/get-retrieve-features-notebook:\001*\242\274\346\300\005\'*%tecton/organization#read_organization'
   _METADATASERVICE.methods_by_name['GetObservabilityConfig']._options = None
   _METADATASERVICE.methods_by_name['GetObservabilityConfig']._serialized_options = b'\202\323\344\223\0022\"-/v1/metadata-service/get-observability-config:\001*\242\274\346\300\005-\032\r\010\001\022\tworkspace*\034tecton/workspace#read_config'
   _METADATASERVICE.methods_by_name['QueryMetric']._options = None
   _METADATASERVICE.methods_by_name['QueryMetric']._serialized_options = b'\202\323\344\223\002&\"!/v1/metadata-service/query-metric:\001*\242\274\346\300\005-\032\r\010\001\022\tworkspace*\034tecton/workspace#read_config'
-  _METADATASERVICE.methods_by_name['GetFeatureValidationHistory']._options = None
-  _METADATASERVICE.methods_by_name['GetFeatureValidationHistory']._serialized_options = b'\202\323\344\223\0028\"3/v1/metadata-service/get-feature-validation-history:\001*\242\274\346\300\005-\032\r\010\001\022\tworkspace*\034tecton/workspace#read_config'
   _METADATASERVICE.methods_by_name['GetFeatureValidationSummary']._options = None
   _METADATASERVICE.methods_by_name['GetFeatureValidationSummary']._serialized_options = b'\202\323\344\223\0028\"3/v1/metadata-service/get-feature-validation-summary:\001*\242\274\346\300\005-\032\r\010\001\022\tworkspace*\034tecton/workspace#read_metric'
   _METADATASERVICE.methods_by_name['GetFeatureValidationResult']._options = None
   _METADATASERVICE.methods_by_name['GetFeatureValidationResult']._serialized_options = b'\202\323\344\223\0027\"2/v1/metadata-service/get-feature-validation-result:\001*\242\274\346\300\005-\032\r\010\001\022\tworkspace*\034tecton/workspace#read_metric'
   _METADATASERVICE.methods_by_name['GetFeatureServerConfig']._options = None
   _METADATASERVICE.methods_by_name['GetFeatureServerConfig']._serialized_options = b'\202\323\344\223\0023\"./v1/metadata-service/get-feature-server-config:\001*\242\274\346\300\005\'*%tecton/organization#read_organization'
   _METADATASERVICE.methods_by_name['SetFeatureServerConfig']._options = None
   _METADATASERVICE.methods_by_name['SetFeatureServerConfig']._serialized_options = b'\202\323\344\223\0023\"./v1/metadata-service/set-feature-server-config:\001*\242\274\346\300\005**(tecton/organization#scale_feature_server'
   _METADATASERVICE.methods_by_name['GetUser']._options = None
   _METADATASERVICE.methods_by_name['GetUser']._serialized_options = b'\202\323\344\223\002\"\"\035/v1/metadata-service/get-user:\001*\242\274\346\300\005\037*\035tecton/organization#list_user'
-  _SORTDIRECTION._serialized_start=24884
-  _SORTDIRECTION._serialized_end=24946
-  _JOBSKEYSET._serialized_start=1685
-  _JOBSKEYSET._serialized_end=1797
-  _PAGINATIONREQUEST._serialized_start=1800
-  _PAGINATIONREQUEST._serialized_end=2008
-  _PAGINATIONRESPONSE._serialized_start=2011
-  _PAGINATIONRESPONSE._serialized_end=2251
-  _GETFEATURESERVICEREQUEST._serialized_start=2254
-  _GETFEATURESERVICEREQUEST._serialized_end=2408
-  _GETFEATURESERVICERESPONSE._serialized_start=2410
-  _GETFEATURESERVICERESPONSE._serialized_end=2507
-  _GETALLFEATURESERVICESREQUEST._serialized_start=2509
-  _GETALLFEATURESERVICESREQUEST._serialized_end=2569
-  _GETALLFEATURESERVICESRESPONSE._serialized_start=2571
-  _GETALLFEATURESERVICESRESPONSE._serialized_end=2680
-  _GETFEATURESERVICESUMMARYREQUEST._serialized_start=2683
-  _GETFEATURESERVICESUMMARYREQUEST._serialized_end=2882
-  _GETFEATURESERVICESUMMARYRESPONSE._serialized_start=2885
-  _GETFEATURESERVICESUMMARYRESPONSE._serialized_end=3025
-  _GETVIRTUALDATASOURCESUMMARYREQUEST._serialized_start=3027
-  _GETVIRTUALDATASOURCESUMMARYREQUEST._serialized_end=3129
-  _GETVIRTUALDATASOURCESUMMARYRESPONSE._serialized_start=3131
-  _GETVIRTUALDATASOURCESUMMARYRESPONSE._serialized_end=3232
-  _GETTRANSFORMATIONSUMMARYREQUEST._serialized_start=3234
-  _GETTRANSFORMATIONSUMMARYREQUEST._serialized_end=3333
-  _GETTRANSFORMATIONSUMMARYRESPONSE._serialized_start=3335
-  _GETTRANSFORMATIONSUMMARYRESPONSE._serialized_end=3433
-  _GETENTITYSUMMARYREQUEST._serialized_start=3435
-  _GETENTITYSUMMARYREQUEST._serialized_end=3526
-  _GETENTITYSUMMARYRESPONSE._serialized_start=3528
-  _GETENTITYSUMMARYRESPONSE._serialized_end=3618
-  _GETSERVINGSTATUSREQUEST._serialized_start=3621
-  _GETSERVINGSTATUSREQUEST._serialized_end=3838
-  _GETFVSERVINGSTATUSFORFSREQUEST._serialized_start=3841
-  _GETFVSERVINGSTATUSFORFSREQUEST._serialized_end=4055
-  _GETFVSERVINGSTATUSFORFSRESPONSE._serialized_start=4058
-  _GETFVSERVINGSTATUSFORFSRESPONSE._serialized_end=4289
-  _GETSERVINGSTATUSRESPONSE._serialized_start=4291
-  _GETSERVINGSTATUSRESPONSE._serialized_end=4412
-  _GETALLFEATUREFRESHNESSREQUEST._serialized_start=4414
-  _GETALLFEATUREFRESHNESSREQUEST._serialized_end=4475
-  _GETALLFEATUREFRESHNESSRESPONSE._serialized_start=4477
-  _GETALLFEATUREFRESHNESSRESPONSE._serialized_end=4592
-  _GETFEATUREFRESHNESSREQUEST._serialized_start=4594
-  _GETFEATUREFRESHNESSREQUEST._serialized_end=4688
-  _GETFEATUREFRESHNESSRESPONSE._serialized_start=4690
-  _GETFEATUREFRESHNESSRESPONSE._serialized_end=4798
-  _GETFEATURECONSUMPTIONREQUEST._serialized_start=4800
-  _GETFEATURECONSUMPTIONREQUEST._serialized_end=4896
-  _GETFEATURECONSUMPTIONRESPONSE._serialized_start=4898
-  _GETFEATURECONSUMPTIONRESPONSE._serialized_end=5015
-  _GETMATERIALIZATIONSTATUSREQUEST._serialized_start=5017
-  _GETMATERIALIZATIONSTATUSREQUEST._serialized_end=5121
-  _GETMATERIALIZATIONSTATUSRESPONSE._serialized_start=5124
-  _GETMATERIALIZATIONSTATUSRESPONSE._serialized_end=5255
-  _GETALLMATERIALIZATIONSTATUSINLIVEWORKSPACESREQUEST._serialized_start=5257
-  _GETALLMATERIALIZATIONSTATUSINLIVEWORKSPACESREQUEST._serialized_end=5343
-  _COUNTRANGE._serialized_start=5345
-  _COUNTRANGE._serialized_end=5397
-  _DURATIONRANGE._serialized_start=5399
-  _DURATIONRANGE._serialized_end=5508
-  _DATETIMERANGE._serialized_start=5510
-  _DATETIMERANGE._serialized_end=5621
-  _GETJOBSREQUEST._serialized_start=5624
-  _GETJOBSREQUEST._serialized_end=6394
-  _FEATUREVIEWMATERIALIZATIONSTATUS._serialized_start=6397
-  _FEATUREVIEWMATERIALIZATIONSTATUS._serialized_end=6594
-  _TASKWITHATTEMPTS._serialized_start=6597
-  _TASKWITHATTEMPTS._serialized_end=7171
-  _GETALLMATERIALIZATIONSTATUSINLIVEWORKSPACESRESPONSE._serialized_start=7174
-  _GETALLMATERIALIZATIONSTATUSINLIVEWORKSPACESRESPONSE._serialized_end=7371
-  _GETJOBSRESPONSE._serialized_start=7374
-  _GETJOBSRESPONSE._serialized_end=7567
-  _FORCERETRYMATERIALIZATIONTASKREQUEST._serialized_start=7570
-  _FORCERETRYMATERIALIZATIONTASKREQUEST._serialized_end=7730
-  _FORCERETRYMATERIALIZATIONTASKRESPONSE._serialized_start=7732
-  _FORCERETRYMATERIALIZATIONTASKRESPONSE._serialized_end=7808
-  _GETFCOSREQUEST._serialized_start=7810
-  _GETFCOSREQUEST._serialized_end=7856
-  _GETFCOSRESPONSE._serialized_start=7858
-  _GETFCOSRESPONSE._serialized_end=7945
-  _GETSPARKCONFIGREQUEST._serialized_start=7947
-  _GETSPARKCONFIGREQUEST._serialized_end=8044
-  _GETSPARKCONFIGRESPONSE._serialized_start=8047
-  _GETSPARKCONFIGRESPONSE._serialized_end=8223
-  _GETMETRICANDEXPECTATIONDEFINITIONREQUEST._serialized_start=8225
-  _GETMETRICANDEXPECTATIONDEFINITIONREQUEST._serialized_end=8341
-  _GETMETRICANDEXPECTATIONDEFINITIONRESPONSE._serialized_start=8344
-  _GETMETRICANDEXPECTATIONDEFINITIONRESPONSE._serialized_end=8700
-  _GETFEATUREVIEWREQUEST._serialized_start=8703
-  _GETFEATUREVIEWREQUEST._serialized_end=8848
-  _GETFEATUREVIEWRESPONSE._serialized_start=8850
-  _GETFEATUREVIEWRESPONSE._serialized_end=8944
-  _GETFEATUREVIEWSUMMARYREQUEST._serialized_start=8946
-  _GETFEATUREVIEWSUMMARYREQUEST._serialized_end=9042
-  _GETFEATUREVIEWSUMMARYRESPONSE._serialized_start=9044
-  _GETFEATUREVIEWSUMMARYRESPONSE._serialized_end=9139
-  _QUERYFEATUREVIEWSREQUEST._serialized_start=9141
-  _QUERYFEATUREVIEWSREQUEST._serialized_end=9223
-  _QUERYFEATUREVIEWSRESPONSE._serialized_start=9225
-  _QUERYFEATUREVIEWSRESPONSE._serialized_end=9322
-  _GETMATERIALIZINGFEATUREVIEWSINLIVEWORKSPACESRESPONSE._serialized_start=9324
-  _GETMATERIALIZINGFEATUREVIEWSINLIVEWORKSPACESRESPONSE._serialized_end=9447
-  _GETVIRTUALDATASOURCEREQUEST._serialized_start=9450
-  _GETVIRTUALDATASOURCEREQUEST._serialized_end=9636
-  _GETVIRTUALDATASOURCERESPONSE._serialized_start=9638
-  _GETVIRTUALDATASOURCERESPONSE._serialized_end=9738
-  _GETALLVIRTUALDATASOURCESREQUEST._serialized_start=9740
-  _GETALLVIRTUALDATASOURCESREQUEST._serialized_end=9803
-  _GETALLVIRTUALDATASOURCESRESPONSE._serialized_start=9805
-  _GETALLVIRTUALDATASOURCESRESPONSE._serialized_end=9927
-  _GETENTITYREQUEST._serialized_start=9930
-  _GETENTITYREQUEST._serialized_end=10081
-  _GETENTITYRESPONSE._serialized_start=10083
-  _GETENTITYRESPONSE._serialized_end=10172
-  _GETALLENTITIESREQUEST._serialized_start=10174
-  _GETALLENTITIESREQUEST._serialized_end=10227
-  _GETALLENTITIESRESPONSE._serialized_start=10229
-  _GETALLENTITIESRESPONSE._serialized_end=10308
-  _GETTRANSFORMATIONREQUEST._serialized_start=10311
-  _GETTRANSFORMATIONREQUEST._serialized_end=10486
-  _GETTRANSFORMATIONRESPONSE._serialized_start=10488
-  _GETTRANSFORMATIONRESPONSE._serialized_end=10591
-  _GETALLTRANSFORMATIONSREQUEST._serialized_start=10593
-  _GETALLTRANSFORMATIONSREQUEST._serialized_end=10653
-  _GETALLTRANSFORMATIONSRESPONSE._serialized_start=10655
-  _GETALLTRANSFORMATIONSRESPONSE._serialized_end=10769
-  _FINDFCOWORKSPACEREQUEST._serialized_start=10771
-  _FINDFCOWORKSPACEREQUEST._serialized_end=10837
-  _FINDFCOWORKSPACERESPONSE._serialized_start=10839
-  _FINDFCOWORKSPACERESPONSE._serialized_end=10895
-  _INGESTCLIENTLOGSREQUEST._serialized_start=10897
-  _INGESTCLIENTLOGSREQUEST._serialized_end=11019
-  _INGESTANALYTICSREQUEST._serialized_start=11021
-  _INGESTANALYTICSREQUEST._serialized_end=11139
-  _NEWSTATEUPDATEREQUEST._serialized_start=11142
-  _NEWSTATEUPDATEREQUEST._serialized_end=11333
-  _NEWSTATEUPDATEREQUESTV2._serialized_start=11336
-  _NEWSTATEUPDATEREQUESTV2._serialized_end=11589
-  _NEWSTATEUPDATERESPONSE._serialized_start=11592
-  _NEWSTATEUPDATERESPONSE._serialized_end=11823
-  _NEWSTATEUPDATERESPONSEV2._serialized_start=11826
-  _NEWSTATEUPDATERESPONSEV2._serialized_end=12061
-  _QUERYSTATEUPDATEREQUEST._serialized_start=12063
-  _QUERYSTATEUPDATEREQUEST._serialized_end=12170
-  _QUERYSTATEUPDATEREQUESTV2._serialized_start=12173
-  _QUERYSTATEUPDATEREQUESTV2._serialized_end=12342
-  _QUERYSTATEUPDATERESPONSE._serialized_start=12345
-  _QUERYSTATEUPDATERESPONSE._serialized_end=12685
-  _QUERYSTATEUPDATERESPONSEV2._serialized_start=12688
-  _QUERYSTATEUPDATERESPONSEV2._serialized_end=13049
-  _GETSTATEUPDATEPLANSUMMARYREQUEST._serialized_start=13051
-  _GETSTATEUPDATEPLANSUMMARYREQUEST._serialized_end=13135
-  _GETSTATEUPDATEPLANSUMMARYRESPONSE._serialized_start=13137
-  _GETSTATEUPDATEPLANSUMMARYRESPONSE._serialized_end=13235
-  _APPLYSTATEUPDATEREQUEST._serialized_start=13237
-  _APPLYSTATEUPDATEREQUEST._serialized_end=13349
-  _APPLYSTATEUPDATERESPONSE._serialized_start=13351
-  _APPLYSTATEUPDATERESPONSE._serialized_end=13377
-  _GETCONFIGSRESPONSE._serialized_start=13380
-  _GETCONFIGSRESPONSE._serialized_end=13558
-  _GETCONFIGSRESPONSE_KEYVALUESENTRY._serialized_start=13498
-  _GETCONFIGSRESPONSE_KEYVALUESENTRY._serialized_end=13558
-  _GETGLOBALSFORWEBUIRESPONSE._serialized_start=13561
-  _GETGLOBALSFORWEBUIRESPONSE._serialized_end=13755
-  _GETGLOBALSFORWEBUIRESPONSE_KEYVALUESENTRY._serialized_start=13498
-  _GETGLOBALSFORWEBUIRESPONSE_KEYVALUESENTRY._serialized_end=13558
-  _GETSTATEUPDATELOGREQUEST._serialized_start=13757
-  _GETSTATEUPDATELOGREQUEST._serialized_end=13835
-  _GETSTATEUPDATELOGRESPONSE._serialized_start=13837
-  _GETSTATEUPDATELOGRESPONSE._serialized_end=13927
-  _GETRESTOREINFOREQUEST._serialized_start=13929
-  _GETRESTOREINFOREQUEST._serialized_end=14011
-  _GETRESTOREINFORESPONSE._serialized_start=14014
-  _GETRESTOREINFORESPONSE._serialized_end=14164
-  _CREATEWORKSPACEREQUEST._serialized_start=14167
-  _CREATEWORKSPACEREQUEST._serialized_end=14314
-  _DELETEWORKSPACEREQUEST._serialized_start=14316
-  _DELETEWORKSPACEREQUEST._serialized_end=14370
-  _INTROSPECTAPIKEYREQUEST._serialized_start=14372
-  _INTROSPECTAPIKEYREQUEST._serialized_end=14422
-  _INTROSPECTAPIKEYRESPONSE._serialized_start=14425
-  _INTROSPECTAPIKEYRESPONSE._serialized_end=14628
-  _CREATEAPIKEYREQUEST._serialized_start=14630
-  _CREATEAPIKEYREQUEST._serialized_end=14712
-  _CREATEAPIKEYRESPONSE._serialized_start=14714
-  _CREATEAPIKEYRESPONSE._serialized_end=14795
-  _DELETEAPIKEYREQUEST._serialized_start=14797
-  _DELETEAPIKEYREQUEST._serialized_end=14859
-  _LISTAPIKEYSREQUEST._serialized_start=14861
-  _LISTAPIKEYSREQUEST._serialized_end=14924
-  _LISTAPIKEYSRESPONSE._serialized_start=14926
-  _LISTAPIKEYSRESPONSE._serialized_end=15007
-  _SERVICEACCOUNT._serialized_start=15010
-  _SERVICEACCOUNT._serialized_end=15238
-  _CREATESERVICEACCOUNTREQUEST._serialized_start=15240
-  _CREATESERVICEACCOUNTREQUEST._serialized_end=15323
-  _CREATESERVICEACCOUNTRESPONSE._serialized_start=15326
-  _CREATESERVICEACCOUNTRESPONSE._serialized_end=15480
-  _GETSERVICEACCOUNTSREQUEST._serialized_start=15482
-  _GETSERVICEACCOUNTSREQUEST._serialized_end=15551
-  _GETSERVICEACCOUNTSRESPONSE._serialized_start=15553
-  _GETSERVICEACCOUNTSRESPONSE._serialized_end=15670
-  _UPDATESERVICEACCOUNTREQUEST._serialized_start=15673
-  _UPDATESERVICEACCOUNTREQUEST._serialized_end=15801
-  _UPDATESERVICEACCOUNTRESPONSE._serialized_start=15804
-  _UPDATESERVICEACCOUNTRESPONSE._serialized_end=15933
-  _DELETESERVICEACCOUNTREQUEST._serialized_start=15935
-  _DELETESERVICEACCOUNTREQUEST._serialized_end=15980
-  _DELETESERVICEACCOUNTRESPONSE._serialized_start=15982
-  _DELETESERVICEACCOUNTRESPONSE._serialized_end=16012
-  _LISTWORKSPACESREQUEST._serialized_start=16014
-  _LISTWORKSPACESREQUEST._serialized_end=16037
-  _LISTWORKSPACESRESPONSE._serialized_start=16039
-  _LISTWORKSPACESRESPONSE._serialized_end=16125
-  _GETWORKSPACEREQUEST._serialized_start=16127
-  _GETWORKSPACEREQUEST._serialized_end=16187
-  _GETWORKSPACERESPONSE._serialized_start=16189
-  _GETWORKSPACERESPONSE._serialized_end=16271
-  _CREATESAVEDFEATUREDATAFRAMEREQUEST._serialized_start=16274
-  _CREATESAVEDFEATUREDATAFRAMEREQUEST._serialized_end=16677
-  _CREATESAVEDFEATUREDATAFRAMERESPONSE._serialized_start=16680
-  _CREATESAVEDFEATUREDATAFRAMERESPONSE._serialized_end=16815
-  _ARCHIVESAVEDFEATUREDATAFRAMEREQUEST._serialized_start=16817
-  _ARCHIVESAVEDFEATUREDATAFRAMEREQUEST._serialized_end=16940
-  _ARCHIVESAVEDFEATUREDATAFRAMERESPONSE._serialized_start=16942
-  _ARCHIVESAVEDFEATUREDATAFRAMERESPONSE._serialized_end=16980
-  _GETSAVEDFEATUREDATAFRAMEREQUEST._serialized_start=16983
-  _GETSAVEDFEATUREDATAFRAMEREQUEST._serialized_end=17214
-  _GETCLUSTERADMININFORESPONSE._serialized_start=17217
-  _GETCLUSTERADMININFORESPONSE._serialized_end=17382
-  _CREATECLUSTERUSERREQUEST._serialized_start=17384
-  _CREATECLUSTERUSERREQUEST._serialized_end=17443
-  _CREATECLUSTERUSERRESPONSE._serialized_start=17445
-  _CREATECLUSTERUSERRESPONSE._serialized_end=17509
-  _DELETECLUSTERUSERREQUEST._serialized_start=17511
-  _DELETECLUSTERUSERREQUEST._serialized_end=17562
-  _DELETECLUSTERUSERRESPONSE._serialized_start=17564
-  _DELETECLUSTERUSERRESPONSE._serialized_end=17628
-  _CLUSTERUSERACTIONREQUEST._serialized_start=17631
-  _CLUSTERUSERACTIONREQUEST._serialized_end=17857
-  _CLUSTERUSERACTIONRESPONSE._serialized_start=17859
-  _CLUSTERUSERACTIONRESPONSE._serialized_end=17923
-  _GETSAVEDFEATUREDATAFRAMERESPONSE._serialized_start=17926
-  _GETSAVEDFEATUREDATAFRAMERESPONSE._serialized_end=18058
-  _GETALLSAVEDFEATUREDATAFRAMESREQUEST._serialized_start=18060
-  _GETALLSAVEDFEATUREDATAFRAMESREQUEST._serialized_end=18127
-  _GETALLSAVEDFEATUREDATAFRAMESRESPONSE._serialized_start=18130
-  _GETALLSAVEDFEATUREDATAFRAMESRESPONSE._serialized_end=18268
-  _INGESTDATAFRAMEREQUEST._serialized_start=18271
-  _INGESTDATAFRAMEREQUEST._serialized_end=18427
-  _INGESTDATAFRAMERESPONSE._serialized_start=18429
-  _INGESTDATAFRAMERESPONSE._serialized_end=18454
-  _GETNEWINGESTDATAFRAMEINFOREQUEST._serialized_start=18456
-  _GETNEWINGESTDATAFRAMEINFOREQUEST._serialized_end=18567
-  _GETNEWINGESTDATAFRAMEINFORESPONSE._serialized_start=18569
-  _GETNEWINGESTDATAFRAMEINFORESPONSE._serialized_end=18685
-  _GETUSERDEPLOYMENTSETTINGSRESPONSE._serialized_start=18688
-  _GETUSERDEPLOYMENTSETTINGSRESPONSE._serialized_end=18824
-  _UPDATEUSERDEPLOYMENTSETTINGSREQUEST._serialized_start=18827
-  _UPDATEUSERDEPLOYMENTSETTINGSREQUEST._serialized_end=19024
-  _UPDATEUSERDEPLOYMENTSETTINGSRESPONSE._serialized_start=19026
-  _UPDATEUSERDEPLOYMENTSETTINGSRESPONSE._serialized_end=19127
-  _GETINTERNALSPARKCLUSTERSTATUSRESPONSE._serialized_start=19129
-  _GETINTERNALSPARKCLUSTERSTATUSRESPONSE._serialized_end=19239
-  _GETDELETEENTITIESINFOREQUEST._serialized_start=19241
-  _GETDELETEENTITIESINFOREQUEST._serialized_end=19348
-  _GETDELETEENTITIESINFORESPONSE._serialized_start=19351
-  _GETDELETEENTITIESINFORESPONSE._serialized_end=19553
-  _DELETEENTITIESREQUEST._serialized_start=19556
-  _DELETEENTITIESREQUEST._serialized_end=19799
-  _DELETEENTITIESRESPONSE._serialized_start=19801
-  _DELETEENTITIESRESPONSE._serialized_end=19825
-  _GETHIVEMETADATAREQUEST._serialized_start=19828
-  _GETHIVEMETADATAREQUEST._serialized_end=20077
-  _GETHIVEMETADATAREQUEST_ACTION._serialized_start=19989
-  _GETHIVEMETADATAREQUEST_ACTION._serialized_end=20077
-  _GETHIVEMETADATARESPONSE._serialized_start=20080
-  _GETHIVEMETADATARESPONSE._serialized_end=20417
-  _GETFILEMETADATAREQUEST._serialized_start=20419
-  _GETFILEMETADATAREQUEST._serialized_end=20494
-  _GETFILEMETADATARESPONSE._serialized_start=20497
-  _GETFILEMETADATARESPONSE._serialized_end=20668
-  _VALIDATEFCOSREQUEST._serialized_start=20670
-  _VALIDATEFCOSREQUEST._serialized_end=20752
-  _VALIDATEFCOSRESPONSE._serialized_start=20755
-  _VALIDATEFCOSRESPONSE._serialized_end=20985
-  _VALIDATELOCALFCOREQUEST._serialized_start=20988
-  _VALIDATELOCALFCOREQUEST._serialized_end=21137
-  _VALIDATELOCALFCORESPONSE._serialized_start=21140
-  _VALIDATELOCALFCORESPONSE._serialized_end=21296
-  _GETONBOARDINGSTATUSREQUEST._serialized_start=21298
-  _GETONBOARDINGSTATUSREQUEST._serialized_end=21356
-  _GETONBOARDINGSTATUSRESPONSE._serialized_start=21359
-  _GETONBOARDINGSTATUSRESPONSE._serialized_end=21643
-  _SETONBOARDINGSTATUSREQUEST._serialized_start=21645
-  _SETONBOARDINGSTATUSREQUEST._serialized_end=21738
-  _GETDATAPLATFORMSETUPSTATUSRESPONSE._serialized_start=21741
-  _GETDATAPLATFORMSETUPSTATUSRESPONSE._serialized_end=21887
-  _GETRETRIEVEFEATURESNOTEBOOKRESPONSE._serialized_start=21889
-  _GETRETRIEVEFEATURESNOTEBOOKRESPONSE._serialized_end=21944
-  _GETOBSERVABILITYCONFIGREQUEST._serialized_start=21946
-  _GETOBSERVABILITYCONFIGREQUEST._serialized_end=22051
-  _GETOBSERVABILITYCONFIGRESPONSE._serialized_start=22054
-  _GETOBSERVABILITYCONFIGRESPONSE._serialized_end=22219
-  _QUERYMETRICREQUEST._serialized_start=22222
-  _QUERYMETRICREQUEST._serialized_end=22497
-  _QUERYMETRICRESPONSE._serialized_start=22500
-  _QUERYMETRICRESPONSE._serialized_end=23001
-  _GETFEATUREVALIDATIONHISTORYREQUEST._serialized_start=23004
-  _GETFEATUREVALIDATIONHISTORYREQUEST._serialized_end=23141
-  _GETFEATUREVALIDATIONHISTORYRESPONSE._serialized_start=23144
-  _GETFEATUREVALIDATIONHISTORYRESPONSE._serialized_end=23322
-  _GETFEATUREVALIDATIONRESULTREQUEST._serialized_start=23325
-  _GETFEATUREVALIDATIONRESULTREQUEST._serialized_end=23837
-  _GETFEATUREVALIDATIONRESULTRESPONSE._serialized_start=23840
-  _GETFEATUREVALIDATIONRESULTRESPONSE._serialized_end=23980
-  _GETFEATUREVALIDATIONSUMMARYREQUEST._serialized_start=23983
-  _GETFEATUREVALIDATIONSUMMARYREQUEST._serialized_end=24205
-  _GETFEATUREVALIDATIONSUMMARYRESPONSE._serialized_start=24208
-  _GETFEATUREVALIDATIONSUMMARYRESPONSE._serialized_end=24522
-  _GETUSERREQUEST._serialized_start=24524
-  _GETUSERREQUEST._serialized_end=24578
-  _GETUSERRESPONSE._serialized_start=24580
-  _GETUSERRESPONSE._serialized_end=24647
-  _GETFEATURESERVERCONFIGREQUEST._serialized_start=24649
-  _GETFEATURESERVERCONFIGREQUEST._serialized_end=24680
-  _GETFEATURESERVERCONFIGRESPONSE._serialized_start=24683
-  _GETFEATURESERVERCONFIGRESPONSE._serialized_end=24827
-  _SETFEATURESERVERCONFIGREQUEST._serialized_start=24829
-  _SETFEATURESERVERCONFIGREQUEST._serialized_end=24882
-  _METADATASERVICE._serialized_start=24950
-  _METADATASERVICE._serialized_end=44322
+  _SORTDIRECTION._serialized_start=24926
+  _SORTDIRECTION._serialized_end=24988
+  _JOBSKEYSET._serialized_start=1610
+  _JOBSKEYSET._serialized_end=1754
+  _PAGINATIONREQUEST._serialized_start=1757
+  _PAGINATIONREQUEST._serialized_end=1965
+  _PAGINATIONRESPONSE._serialized_start=1968
+  _PAGINATIONRESPONSE._serialized_end=2208
+  _VALIDATIONRESULTTOKEN._serialized_start=2211
+  _VALIDATIONRESULTTOKEN._serialized_end=2400
+  _GETFEATURESERVICEREQUEST._serialized_start=2403
+  _GETFEATURESERVICEREQUEST._serialized_end=2614
+  _GETFEATURESERVICERESPONSE._serialized_start=2616
+  _GETFEATURESERVICERESPONSE._serialized_end=2713
+  _GETALLFEATURESERVICESREQUEST._serialized_start=2715
+  _GETALLFEATURESERVICESREQUEST._serialized_end=2775
+  _GETALLFEATURESERVICESRESPONSE._serialized_start=2777
+  _GETALLFEATURESERVICESRESPONSE._serialized_end=2886
+  _GETFEATURESERVICESUMMARYREQUEST._serialized_start=2889
+  _GETFEATURESERVICESUMMARYREQUEST._serialized_end=3088
+  _GETFEATURESERVICESUMMARYRESPONSE._serialized_start=3091
+  _GETFEATURESERVICESUMMARYRESPONSE._serialized_end=3231
+  _GETVIRTUALDATASOURCESUMMARYREQUEST._serialized_start=3233
+  _GETVIRTUALDATASOURCESUMMARYREQUEST._serialized_end=3335
+  _GETVIRTUALDATASOURCESUMMARYRESPONSE._serialized_start=3337
+  _GETVIRTUALDATASOURCESUMMARYRESPONSE._serialized_end=3438
+  _GETTRANSFORMATIONSUMMARYREQUEST._serialized_start=3440
+  _GETTRANSFORMATIONSUMMARYREQUEST._serialized_end=3539
+  _GETTRANSFORMATIONSUMMARYRESPONSE._serialized_start=3541
+  _GETTRANSFORMATIONSUMMARYRESPONSE._serialized_end=3639
+  _GETENTITYSUMMARYREQUEST._serialized_start=3641
+  _GETENTITYSUMMARYREQUEST._serialized_end=3732
+  _GETENTITYSUMMARYRESPONSE._serialized_start=3734
+  _GETENTITYSUMMARYRESPONSE._serialized_end=3824
+  _GETSERVINGSTATUSREQUEST._serialized_start=3827
+  _GETSERVINGSTATUSREQUEST._serialized_end=4044
+  _GETFVSERVINGSTATUSFORFSREQUEST._serialized_start=4047
+  _GETFVSERVINGSTATUSFORFSREQUEST._serialized_end=4261
+  _GETFVSERVINGSTATUSFORFSRESPONSE._serialized_start=4264
+  _GETFVSERVINGSTATUSFORFSRESPONSE._serialized_end=4495
+  _GETSERVINGSTATUSRESPONSE._serialized_start=4497
+  _GETSERVINGSTATUSRESPONSE._serialized_end=4618
+  _GETALLFEATUREFRESHNESSREQUEST._serialized_start=4620
+  _GETALLFEATUREFRESHNESSREQUEST._serialized_end=4681
+  _GETALLFEATUREFRESHNESSRESPONSE._serialized_start=4683
+  _GETALLFEATUREFRESHNESSRESPONSE._serialized_end=4798
+  _GETFEATUREFRESHNESSREQUEST._serialized_start=4800
+  _GETFEATUREFRESHNESSREQUEST._serialized_end=4894
+  _GETFEATUREFRESHNESSRESPONSE._serialized_start=4896
+  _GETFEATUREFRESHNESSRESPONSE._serialized_end=5004
+  _GETFEATURECONSUMPTIONREQUEST._serialized_start=5006
+  _GETFEATURECONSUMPTIONREQUEST._serialized_end=5102
+  _GETFEATURECONSUMPTIONRESPONSE._serialized_start=5104
+  _GETFEATURECONSUMPTIONRESPONSE._serialized_end=5221
+  _GETMATERIALIZATIONSTATUSREQUEST._serialized_start=5224
+  _GETMATERIALIZATIONSTATUSREQUEST._serialized_end=5399
+  _GETMATERIALIZATIONSTATUSRESPONSE._serialized_start=5402
+  _GETMATERIALIZATIONSTATUSRESPONSE._serialized_end=5533
+  _GETALLMATERIALIZATIONSTATUSINLIVEWORKSPACESREQUEST._serialized_start=5535
+  _GETALLMATERIALIZATIONSTATUSINLIVEWORKSPACESREQUEST._serialized_end=5621
+  _COUNTRANGE._serialized_start=5623
+  _COUNTRANGE._serialized_end=5675
+  _DURATIONRANGE._serialized_start=5677
+  _DURATIONRANGE._serialized_end=5786
+  _DATETIMERANGE._serialized_start=5788
+  _DATETIMERANGE._serialized_end=5899
+  _GETJOBSREQUEST._serialized_start=5902
+  _GETJOBSREQUEST._serialized_end=6766
+  _FEATUREVIEWMATERIALIZATIONSTATUS._serialized_start=6769
+  _FEATUREVIEWMATERIALIZATIONSTATUS._serialized_end=6966
+  _TASKWITHATTEMPTS._serialized_start=6969
+  _TASKWITHATTEMPTS._serialized_end=7662
+  _GETALLMATERIALIZATIONSTATUSINLIVEWORKSPACESRESPONSE._serialized_start=7665
+  _GETALLMATERIALIZATIONSTATUSINLIVEWORKSPACESRESPONSE._serialized_end=7862
+  _GETJOBSRESPONSE._serialized_start=7865
+  _GETJOBSRESPONSE._serialized_end=8058
+  _FORCERETRYMATERIALIZATIONTASKREQUEST._serialized_start=8061
+  _FORCERETRYMATERIALIZATIONTASKREQUEST._serialized_end=8221
+  _FORCERETRYMATERIALIZATIONTASKRESPONSE._serialized_start=8223
+  _FORCERETRYMATERIALIZATIONTASKRESPONSE._serialized_end=8299
+  _RESTARTMATERIALIZATIONTASKREQUEST._serialized_start=8301
+  _RESTARTMATERIALIZATIONTASKREQUEST._serialized_end=8417
+  _RESTARTMATERIALIZATIONTASKRESPONSE._serialized_start=8420
+  _RESTARTMATERIALIZATIONTASKRESPONSE._serialized_end=8581
+  _GETFCOSREQUEST._serialized_start=8583
+  _GETFCOSREQUEST._serialized_end=8629
+  _GETFCOSRESPONSE._serialized_start=8631
+  _GETFCOSRESPONSE._serialized_end=8718
+  _GETSPARKCONFIGREQUEST._serialized_start=8720
+  _GETSPARKCONFIGREQUEST._serialized_end=8817
+  _SPARKCLUSTERCONFIG._serialized_start=8819
+  _SPARKCLUSTERCONFIG._serialized_end=8889
+  _GETSPARKCONFIGRESPONSE._serialized_start=8892
+  _GETSPARKCONFIGRESPONSE._serialized_end=9088
+  _GETMETRICANDEXPECTATIONDEFINITIONREQUEST._serialized_start=9090
+  _GETMETRICANDEXPECTATIONDEFINITIONREQUEST._serialized_end=9206
+  _GETMETRICANDEXPECTATIONDEFINITIONRESPONSE._serialized_start=9209
+  _GETMETRICANDEXPECTATIONDEFINITIONRESPONSE._serialized_end=9565
+  _GETFEATUREVIEWREQUEST._serialized_start=9568
+  _GETFEATUREVIEWREQUEST._serialized_end=9770
+  _GETFEATUREVIEWRESPONSE._serialized_start=9772
+  _GETFEATUREVIEWRESPONSE._serialized_end=9866
+  _GETFEATUREVIEWSUMMARYREQUEST._serialized_start=9868
+  _GETFEATUREVIEWSUMMARYREQUEST._serialized_end=9964
+  _GETFEATUREVIEWSUMMARYRESPONSE._serialized_start=9966
+  _GETFEATUREVIEWSUMMARYRESPONSE._serialized_end=10061
+  _QUERYFEATUREVIEWSREQUEST._serialized_start=10063
+  _QUERYFEATUREVIEWSREQUEST._serialized_end=10145
+  _QUERYFEATUREVIEWSRESPONSE._serialized_start=10147
+  _QUERYFEATUREVIEWSRESPONSE._serialized_end=10244
+  _GETMATERIALIZINGFEATUREVIEWSINLIVEWORKSPACESRESPONSE._serialized_start=10246
+  _GETMATERIALIZINGFEATUREVIEWSINLIVEWORKSPACESRESPONSE._serialized_end=10369
+  _GETVIRTUALDATASOURCEREQUEST._serialized_start=10372
+  _GETVIRTUALDATASOURCEREQUEST._serialized_end=10615
+  _GETVIRTUALDATASOURCERESPONSE._serialized_start=10617
+  _GETVIRTUALDATASOURCERESPONSE._serialized_end=10717
+  _GETALLVIRTUALDATASOURCESREQUEST._serialized_start=10719
+  _GETALLVIRTUALDATASOURCESREQUEST._serialized_end=10782
+  _GETALLVIRTUALDATASOURCESRESPONSE._serialized_start=10784
+  _GETALLVIRTUALDATASOURCESRESPONSE._serialized_end=10906
+  _GETENTITYREQUEST._serialized_start=10909
+  _GETENTITYREQUEST._serialized_end=11117
+  _GETENTITYRESPONSE._serialized_start=11119
+  _GETENTITYRESPONSE._serialized_end=11208
+  _GETALLENTITIESREQUEST._serialized_start=11210
+  _GETALLENTITIESREQUEST._serialized_end=11263
+  _GETALLENTITIESRESPONSE._serialized_start=11265
+  _GETALLENTITIESRESPONSE._serialized_end=11344
+  _GETTRANSFORMATIONREQUEST._serialized_start=11347
+  _GETTRANSFORMATIONREQUEST._serialized_end=11579
+  _GETTRANSFORMATIONRESPONSE._serialized_start=11581
+  _GETTRANSFORMATIONRESPONSE._serialized_end=11684
+  _GETALLTRANSFORMATIONSREQUEST._serialized_start=11686
+  _GETALLTRANSFORMATIONSREQUEST._serialized_end=11746
+  _GETALLTRANSFORMATIONSRESPONSE._serialized_start=11748
+  _GETALLTRANSFORMATIONSRESPONSE._serialized_end=11862
+  _FINDFCOWORKSPACEREQUEST._serialized_start=11864
+  _FINDFCOWORKSPACEREQUEST._serialized_end=11954
+  _FINDFCOWORKSPACERESPONSE._serialized_start=11956
+  _FINDFCOWORKSPACERESPONSE._serialized_end=12012
+  _INGESTCLIENTLOGSREQUEST._serialized_start=12014
+  _INGESTCLIENTLOGSREQUEST._serialized_end=12136
+  _INGESTANALYTICSREQUEST._serialized_start=12138
+  _INGESTANALYTICSREQUEST._serialized_end=12256
+  _NEWSTATEUPDATEREQUEST._serialized_start=12259
+  _NEWSTATEUPDATEREQUEST._serialized_end=12450
+  _NEWSTATEUPDATEREQUESTV2._serialized_start=12453
+  _NEWSTATEUPDATEREQUESTV2._serialized_end=12706
+  _NEWSTATEUPDATERESPONSE._serialized_start=12709
+  _NEWSTATEUPDATERESPONSE._serialized_end=12940
+  _NEWSTATEUPDATERESPONSEV2._serialized_start=12943
+  _NEWSTATEUPDATERESPONSEV2._serialized_end=13178
+  _QUERYSTATEUPDATEREQUEST._serialized_start=13180
+  _QUERYSTATEUPDATEREQUEST._serialized_end=13287
+  _QUERYSTATEUPDATEREQUESTV2._serialized_start=13290
+  _QUERYSTATEUPDATEREQUESTV2._serialized_end=13459
+  _QUERYSTATEUPDATERESPONSE._serialized_start=13462
+  _QUERYSTATEUPDATERESPONSE._serialized_end=13802
+  _QUERYSTATEUPDATERESPONSEV2._serialized_start=13805
+  _QUERYSTATEUPDATERESPONSEV2._serialized_end=14166
+  _GETSTATEUPDATEPLANSUMMARYREQUEST._serialized_start=14168
+  _GETSTATEUPDATEPLANSUMMARYREQUEST._serialized_end=14252
+  _GETSTATEUPDATEPLANSUMMARYRESPONSE._serialized_start=14254
+  _GETSTATEUPDATEPLANSUMMARYRESPONSE._serialized_end=14352
+  _APPLYSTATEUPDATEREQUEST._serialized_start=14354
+  _APPLYSTATEUPDATEREQUEST._serialized_end=14466
+  _APPLYSTATEUPDATERESPONSE._serialized_start=14468
+  _APPLYSTATEUPDATERESPONSE._serialized_end=14494
+  _GETCONFIGSRESPONSE._serialized_start=14497
+  _GETCONFIGSRESPONSE._serialized_end=14675
+  _GETCONFIGSRESPONSE_KEYVALUESENTRY._serialized_start=14615
+  _GETCONFIGSRESPONSE_KEYVALUESENTRY._serialized_end=14675
+  _GETGLOBALSFORWEBUIRESPONSE._serialized_start=14678
+  _GETGLOBALSFORWEBUIRESPONSE._serialized_end=14872
+  _GETGLOBALSFORWEBUIRESPONSE_KEYVALUESENTRY._serialized_start=14615
+  _GETGLOBALSFORWEBUIRESPONSE_KEYVALUESENTRY._serialized_end=14675
+  _GETSTATEUPDATELOGREQUEST._serialized_start=14874
+  _GETSTATEUPDATELOGREQUEST._serialized_end=14952
+  _GETSTATEUPDATELOGRESPONSE._serialized_start=14954
+  _GETSTATEUPDATELOGRESPONSE._serialized_end=15044
+  _GETRESTOREINFOREQUEST._serialized_start=15046
+  _GETRESTOREINFOREQUEST._serialized_end=15128
+  _GETRESTOREINFORESPONSE._serialized_start=15131
+  _GETRESTOREINFORESPONSE._serialized_end=15281
+  _CREATEWORKSPACEREQUEST._serialized_start=15284
+  _CREATEWORKSPACEREQUEST._serialized_end=15431
+  _DELETEWORKSPACEREQUEST._serialized_start=15433
+  _DELETEWORKSPACEREQUEST._serialized_end=15487
+  _INTROSPECTAPIKEYREQUEST._serialized_start=15489
+  _INTROSPECTAPIKEYREQUEST._serialized_end=15539
+  _INTROSPECTAPIKEYRESPONSE._serialized_start=15542
+  _INTROSPECTAPIKEYRESPONSE._serialized_end=15745
+  _CREATEAPIKEYREQUEST._serialized_start=15747
+  _CREATEAPIKEYREQUEST._serialized_end=15829
+  _CREATEAPIKEYRESPONSE._serialized_start=15831
+  _CREATEAPIKEYRESPONSE._serialized_end=15912
+  _DELETEAPIKEYREQUEST._serialized_start=15914
+  _DELETEAPIKEYREQUEST._serialized_end=15976
+  _LISTAPIKEYSREQUEST._serialized_start=15978
+  _LISTAPIKEYSREQUEST._serialized_end=16041
+  _LISTAPIKEYSRESPONSE._serialized_start=16043
+  _LISTAPIKEYSRESPONSE._serialized_end=16124
+  _SERVICEACCOUNT._serialized_start=16127
+  _SERVICEACCOUNT._serialized_end=16414
+  _CREATESERVICEACCOUNTREQUEST._serialized_start=16416
+  _CREATESERVICEACCOUNTREQUEST._serialized_end=16499
+  _CREATESERVICEACCOUNTRESPONSE._serialized_start=16502
+  _CREATESERVICEACCOUNTRESPONSE._serialized_end=16715
+  _GETSERVICEACCOUNTSREQUEST._serialized_start=16717
+  _GETSERVICEACCOUNTSREQUEST._serialized_end=16786
+  _GETSERVICEACCOUNTSRESPONSE._serialized_start=16788
+  _GETSERVICEACCOUNTSRESPONSE._serialized_end=16905
+  _UPDATESERVICEACCOUNTREQUEST._serialized_start=16908
+  _UPDATESERVICEACCOUNTREQUEST._serialized_end=17036
+  _UPDATESERVICEACCOUNTRESPONSE._serialized_start=17039
+  _UPDATESERVICEACCOUNTRESPONSE._serialized_end=17227
+  _DELETESERVICEACCOUNTREQUEST._serialized_start=17229
+  _DELETESERVICEACCOUNTREQUEST._serialized_end=17274
+  _DELETESERVICEACCOUNTRESPONSE._serialized_start=17276
+  _DELETESERVICEACCOUNTRESPONSE._serialized_end=17306
+  _LISTWORKSPACESREQUEST._serialized_start=17308
+  _LISTWORKSPACESREQUEST._serialized_end=17331
+  _LISTWORKSPACESRESPONSE._serialized_start=17333
+  _LISTWORKSPACESRESPONSE._serialized_end=17419
+  _GETWORKSPACEREQUEST._serialized_start=17421
+  _GETWORKSPACEREQUEST._serialized_end=17481
+  _GETWORKSPACERESPONSE._serialized_start=17483
+  _GETWORKSPACERESPONSE._serialized_end=17565
+  _CREATESAVEDFEATUREDATAFRAMEREQUEST._serialized_start=17568
+  _CREATESAVEDFEATUREDATAFRAMEREQUEST._serialized_end=17971
+  _CREATESAVEDFEATUREDATAFRAMERESPONSE._serialized_start=17974
+  _CREATESAVEDFEATUREDATAFRAMERESPONSE._serialized_end=18109
+  _ARCHIVESAVEDFEATUREDATAFRAMEREQUEST._serialized_start=18111
+  _ARCHIVESAVEDFEATUREDATAFRAMEREQUEST._serialized_end=18234
+  _ARCHIVESAVEDFEATUREDATAFRAMERESPONSE._serialized_start=18236
+  _ARCHIVESAVEDFEATUREDATAFRAMERESPONSE._serialized_end=18274
+  _GETSAVEDFEATUREDATAFRAMEREQUEST._serialized_start=18277
+  _GETSAVEDFEATUREDATAFRAMEREQUEST._serialized_end=18508
+  _GETCLUSTERADMININFORESPONSE._serialized_start=18511
+  _GETCLUSTERADMININFORESPONSE._serialized_end=18676
+  _CREATECLUSTERUSERREQUEST._serialized_start=18678
+  _CREATECLUSTERUSERREQUEST._serialized_end=18737
+  _CREATECLUSTERUSERRESPONSE._serialized_start=18739
+  _CREATECLUSTERUSERRESPONSE._serialized_end=18772
+  _DELETECLUSTERUSERREQUEST._serialized_start=18774
+  _DELETECLUSTERUSERREQUEST._serialized_end=18825
+  _DELETECLUSTERUSERRESPONSE._serialized_start=18827
+  _DELETECLUSTERUSERRESPONSE._serialized_end=18860
+  _CLUSTERUSERACTIONREQUEST._serialized_start=18863
+  _CLUSTERUSERACTIONREQUEST._serialized_end=19089
+  _CLUSTERUSERACTIONRESPONSE._serialized_start=19091
+  _CLUSTERUSERACTIONRESPONSE._serialized_end=19124
+  _GETSAVEDFEATUREDATAFRAMERESPONSE._serialized_start=19127
+  _GETSAVEDFEATUREDATAFRAMERESPONSE._serialized_end=19259
+  _GETALLSAVEDFEATUREDATAFRAMESREQUEST._serialized_start=19261
+  _GETALLSAVEDFEATUREDATAFRAMESREQUEST._serialized_end=19328
+  _GETALLSAVEDFEATUREDATAFRAMESRESPONSE._serialized_start=19331
+  _GETALLSAVEDFEATUREDATAFRAMESRESPONSE._serialized_end=19469
+  _INGESTDATAFRAMEREQUEST._serialized_start=19472
+  _INGESTDATAFRAMEREQUEST._serialized_end=19628
+  _INGESTDATAFRAMERESPONSE._serialized_start=19630
+  _INGESTDATAFRAMERESPONSE._serialized_end=19655
+  _GETNEWINGESTDATAFRAMEINFOREQUEST._serialized_start=19657
+  _GETNEWINGESTDATAFRAMEINFOREQUEST._serialized_end=19768
+  _GETNEWINGESTDATAFRAMEINFORESPONSE._serialized_start=19770
+  _GETNEWINGESTDATAFRAMEINFORESPONSE._serialized_end=19886
+  _GETUSERDEPLOYMENTSETTINGSRESPONSE._serialized_start=19889
+  _GETUSERDEPLOYMENTSETTINGSRESPONSE._serialized_end=20025
+  _UPDATEUSERDEPLOYMENTSETTINGSREQUEST._serialized_start=20028
+  _UPDATEUSERDEPLOYMENTSETTINGSREQUEST._serialized_end=20225
+  _UPDATEUSERDEPLOYMENTSETTINGSRESPONSE._serialized_start=20227
+  _UPDATEUSERDEPLOYMENTSETTINGSRESPONSE._serialized_end=20328
+  _GETINTERNALSPARKCLUSTERSTATUSRESPONSE._serialized_start=20330
+  _GETINTERNALSPARKCLUSTERSTATUSRESPONSE._serialized_end=20440
+  _GETDELETEENTITIESINFOREQUEST._serialized_start=20442
+  _GETDELETEENTITIESINFOREQUEST._serialized_end=20549
+  _GETDELETEENTITIESINFORESPONSE._serialized_start=20552
+  _GETDELETEENTITIESINFORESPONSE._serialized_end=20754
+  _DELETEENTITIESREQUEST._serialized_start=20757
+  _DELETEENTITIESREQUEST._serialized_end=21000
+  _DELETEENTITIESRESPONSE._serialized_start=21002
+  _DELETEENTITIESRESPONSE._serialized_end=21026
+  _GETHIVEMETADATAREQUEST._serialized_start=21029
+  _GETHIVEMETADATAREQUEST._serialized_end=21282
+  _GETHIVEMETADATAREQUEST_ACTION._serialized_start=21190
+  _GETHIVEMETADATAREQUEST_ACTION._serialized_end=21282
+  _GETHIVEMETADATARESPONSE._serialized_start=21285
+  _GETHIVEMETADATARESPONSE._serialized_end=21510
+  _VALIDATELOCALFCOREQUEST._serialized_start=21513
+  _VALIDATELOCALFCOREQUEST._serialized_end=21662
+  _VALIDATELOCALFCORESPONSE._serialized_start=21665
+  _VALIDATELOCALFCORESPONSE._serialized_end=21821
+  _GETONBOARDINGSTATUSREQUEST._serialized_start=21823
+  _GETONBOARDINGSTATUSREQUEST._serialized_end=21881
+  _GETONBOARDINGSTATUSRESPONSE._serialized_start=21884
+  _GETONBOARDINGSTATUSRESPONSE._serialized_end=22085
+  _GETDATAPLATFORMSETUPSTATUSRESPONSE._serialized_start=22088
+  _GETDATAPLATFORMSETUPSTATUSRESPONSE._serialized_end=22234
+  _GETOBSERVABILITYCONFIGREQUEST._serialized_start=22236
+  _GETOBSERVABILITYCONFIGREQUEST._serialized_end=22341
+  _GETOBSERVABILITYCONFIGRESPONSE._serialized_start=22344
+  _GETOBSERVABILITYCONFIGRESPONSE._serialized_end=22509
+  _QUERYMETRICREQUEST._serialized_start=22512
+  _QUERYMETRICREQUEST._serialized_end=22815
+  _QUERYMETRICRESPONSE._serialized_start=22818
+  _QUERYMETRICRESPONSE._serialized_end=23319
+  _GETFEATUREVALIDATIONRESULTREQUEST._serialized_start=23322
+  _GETFEATUREVALIDATIONRESULTREQUEST._serialized_end=23834
+  _GETFEATUREVALIDATIONRESULTRESPONSE._serialized_start=23837
+  _GETFEATUREVALIDATIONRESULTRESPONSE._serialized_end=24022
+  _GETFEATUREVALIDATIONSUMMARYREQUEST._serialized_start=24025
+  _GETFEATUREVALIDATIONSUMMARYREQUEST._serialized_end=24247
+  _GETFEATUREVALIDATIONSUMMARYRESPONSE._serialized_start=24250
+  _GETFEATUREVALIDATIONSUMMARYRESPONSE._serialized_end=24564
+  _GETUSERREQUEST._serialized_start=24566
+  _GETUSERREQUEST._serialized_end=24620
+  _GETUSERRESPONSE._serialized_start=24622
+  _GETUSERRESPONSE._serialized_end=24689
+  _GETFEATURESERVERCONFIGREQUEST._serialized_start=24691
+  _GETFEATURESERVERCONFIGREQUEST._serialized_end=24722
+  _GETFEATURESERVERCONFIGRESPONSE._serialized_start=24725
+  _GETFEATURESERVERCONFIGRESPONSE._serialized_end=24869
+  _SETFEATURESERVERCONFIGREQUEST._serialized_start=24871
+  _SETFEATURESERVERCONFIGREQUEST._serialized_end=24924
+  _METADATASERVICE._serialized_start=24992
+  _METADATASERVICE._serialized_end=43757
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/online_store/feature_value_pb2.py` & `tecton-0.7.0rc0/tecton_proto/online_store/feature_value_pb2.py`

 * *Files 24% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n-tecton_proto/online_store/feature_value.proto\x12\x19tecton_proto.online_store\"\xdf\x02\n\x0c\x46\x65\x61tureValue\x12H\n\x0b\x61rray_value\x18\x01 \x01(\x0b\x32%.tecton_proto.online_store.ArrayValueH\x00R\narrayValue\x12%\n\rfloat64_value\x18\x02 \x01(\x01H\x00R\x0c\x66loat64Value\x12%\n\rfloat32_value\x18\x03 \x01(\x02H\x00R\x0c\x66loat32Value\x12!\n\x0bint64_value\x18\x04 \x01(\x03H\x00R\nint64Value\x12\x1f\n\nbool_value\x18\x05 \x01(\x08H\x00R\tboolValue\x12#\n\x0cstring_value\x18\x06 \x01(\tH\x00R\x0bstringValue\x12\x45\n\nnull_value\x18\x07 \x01(\x0b\x32$.tecton_proto.online_store.NullValueH\x00R\tnullValueB\x07\n\x05value\"b\n\x10\x46\x65\x61tureValueList\x12N\n\x0e\x66\x65\x61ture_values\x18\x01 \x03(\x0b\x32\'.tecton_proto.online_store.FeatureValueR\rfeatureValues\"\xaa\x01\n\rRedisTAFVData\x12\x1f\n\x0b\x61nchor_time\x18\x01 \x01(\x03R\nanchorTime\x12(\n\x10written_by_batch\x18\x02 \x01(\x08R\x0ewrittenByBatch\x12N\n\x0e\x66\x65\x61ture_values\x18\x03 \x03(\x0b\x32\'.tecton_proto.online_store.FeatureValueR\rfeatureValues\"\x0b\n\tNullValue\"\x9f\x02\n\nArrayValue\x12#\n\rstring_values\x18\x01 \x03(\tR\x0cstringValues\x12H\n\x0c\x61rray_values\x18\x02 \x03(\x0b\x32%.tecton_proto.online_store.ArrayValueR\x0b\x61rrayValues\x12%\n\x0cint64_values\x18\x06 \x03(\x03\x42\x02\x10\x01R\x0bint64Values\x12)\n\x0e\x66loat32_values\x18\x07 \x03(\x02\x42\x02\x10\x01R\rfloat32Values\x12)\n\x0e\x66loat64_values\x18\x08 \x03(\x01\x42\x02\x10\x01R\rfloat64Values\x12%\n\x0cnull_indices\x18\t \x03(\x05\x42\x02\x10\x01R\x0bnullIndicesB\x1a\n\x16\x63om.tecton.onlinestoreP\x01')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n-tecton_proto/online_store/feature_value.proto\x12\x19tecton_proto.online_store\"\xf0\x03\n\x0c\x46\x65\x61tureValue\x12H\n\x0b\x61rray_value\x18\x01 \x01(\x0b\x32%.tecton_proto.online_store.ArrayValueH\x00R\narrayValue\x12%\n\rfloat64_value\x18\x02 \x01(\x01H\x00R\x0c\x66loat64Value\x12%\n\rfloat32_value\x18\x03 \x01(\x02H\x00R\x0c\x66loat32Value\x12!\n\x0bint64_value\x18\x04 \x01(\x03H\x00R\nint64Value\x12\x1f\n\nbool_value\x18\x05 \x01(\x08H\x00R\tboolValue\x12#\n\x0cstring_value\x18\x06 \x01(\tH\x00R\x0bstringValue\x12\x45\n\nnull_value\x18\x07 \x01(\x0b\x32$.tecton_proto.online_store.NullValueH\x00R\tnullValue\x12K\n\x0cstruct_value\x18\x08 \x01(\x0b\x32&.tecton_proto.online_store.StructValueH\x00R\x0bstructValue\x12\x42\n\tmap_value\x18\t \x01(\x0b\x32#.tecton_proto.online_store.MapValueH\x00R\x08mapValueB\x07\n\x05value\"b\n\x10\x46\x65\x61tureValueList\x12N\n\x0e\x66\x65\x61ture_values\x18\x01 \x03(\x0b\x32\'.tecton_proto.online_store.FeatureValueR\rfeatureValues\"\xaa\x01\n\rRedisTAFVData\x12\x1f\n\x0b\x61nchor_time\x18\x01 \x01(\x03R\nanchorTime\x12(\n\x10written_by_batch\x18\x02 \x01(\x08R\x0ewrittenByBatch\x12N\n\x0e\x66\x65\x61ture_values\x18\x03 \x03(\x0b\x32\'.tecton_proto.online_store.FeatureValueR\rfeatureValues\"\x0b\n\tNullValue\"\xd1\x03\n\nArrayValue\x12#\n\rstring_values\x18\x01 \x03(\tR\x0cstringValues\x12H\n\x0c\x61rray_values\x18\x02 \x03(\x0b\x32%.tecton_proto.online_store.ArrayValueR\x0b\x61rrayValues\x12K\n\rstruct_values\x18\x03 \x03(\x0b\x32&.tecton_proto.online_store.StructValueR\x0cstructValues\x12\x42\n\nmap_values\x18\x04 \x03(\x0b\x32#.tecton_proto.online_store.MapValueR\tmapValues\x12\x1f\n\x0b\x62ool_values\x18\x05 \x03(\x08R\nboolValues\x12%\n\x0cint64_values\x18\x06 \x03(\x03\x42\x02\x10\x01R\x0bint64Values\x12)\n\x0e\x66loat32_values\x18\x07 \x03(\x02\x42\x02\x10\x01R\rfloat32Values\x12)\n\x0e\x66loat64_values\x18\x08 \x03(\x01\x42\x02\x10\x01R\rfloat64Values\x12%\n\x0cnull_indices\x18\t \x03(\x05\x42\x02\x10\x01R\x0bnullIndices\"N\n\x0bStructValue\x12?\n\x06values\x18\x01 \x03(\x0b\x32\'.tecton_proto.online_store.FeatureValueR\x06values\"\x84\x01\n\x08MapValue\x12\x39\n\x04keys\x18\x01 \x01(\x0b\x32%.tecton_proto.online_store.ArrayValueR\x04keys\x12=\n\x06values\x18\x02 \x01(\x0b\x32%.tecton_proto.online_store.ArrayValueR\x06valuesB\x1a\n\x16\x63om.tecton.onlinestoreP\x01')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.online_store.feature_value_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\026com.tecton.onlinestoreP\001'
@@ -26,17 +26,21 @@
   _ARRAYVALUE.fields_by_name['float32_values']._options = None
   _ARRAYVALUE.fields_by_name['float32_values']._serialized_options = b'\020\001'
   _ARRAYVALUE.fields_by_name['float64_values']._options = None
   _ARRAYVALUE.fields_by_name['float64_values']._serialized_options = b'\020\001'
   _ARRAYVALUE.fields_by_name['null_indices']._options = None
   _ARRAYVALUE.fields_by_name['null_indices']._serialized_options = b'\020\001'
   _FEATUREVALUE._serialized_start=77
-  _FEATUREVALUE._serialized_end=428
-  _FEATUREVALUELIST._serialized_start=430
-  _FEATUREVALUELIST._serialized_end=528
-  _REDISTAFVDATA._serialized_start=531
-  _REDISTAFVDATA._serialized_end=701
-  _NULLVALUE._serialized_start=703
-  _NULLVALUE._serialized_end=714
-  _ARRAYVALUE._serialized_start=717
-  _ARRAYVALUE._serialized_end=1004
+  _FEATUREVALUE._serialized_end=573
+  _FEATUREVALUELIST._serialized_start=575
+  _FEATUREVALUELIST._serialized_end=673
+  _REDISTAFVDATA._serialized_start=676
+  _REDISTAFVDATA._serialized_end=846
+  _NULLVALUE._serialized_start=848
+  _NULLVALUE._serialized_end=859
+  _ARRAYVALUE._serialized_start=862
+  _ARRAYVALUE._serialized_end=1327
+  _STRUCTVALUE._serialized_start=1329
+  _STRUCTVALUE._serialized_end=1407
+  _MAPVALUE._serialized_start=1410
+  _MAPVALUE._serialized_end=1542
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/online_store/status_entry_pb2.py` & `tecton-0.7.0rc0/tecton_proto/online_store/status_entry_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/spark_api/error_pb2.py` & `tecton-0.7.0rc0/tecton_proto/spark_api/error_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/spark_api/jobs_pb2.py` & `tecton-0.7.0rc0/tecton_proto/spark_api/jobs_pb2.py`

 * *Files 2% similar despite different names*

```diff
@@ -11,32 +11,32 @@
 _sym_db = _symbol_database.Default()
 
 
 from tecton_proto.spark_common import clusters_pb2 as tecton__proto_dot_spark__common_dot_clusters__pb2
 from tecton_proto.spark_common import libraries_pb2 as tecton__proto_dot_spark__common_dot_libraries__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n!tecton_proto/spark_api/jobs.proto\x12\x16tecton_proto.spark_api\x1a(tecton_proto/spark_common/clusters.proto\x1a)tecton_proto/spark_common/libraries.proto\"\xb7\x03\n\x19PythonMaterializationTask\x12\x38\n\x18materialization_path_uri\x18\x01 \x01(\tR\x16materializationPathUri\x12n\n\x0f\x62\x61se_parameters\x18\x02 \x03(\x0b\x32\x45.tecton_proto.spark_api.PythonMaterializationTask.BaseParametersEntryR\x0e\x62\x61seParameters\x12V\n\x08taskType\x18\x03 \x01(\x0e\x32:.tecton_proto.spark_api.PythonMaterializationTask.TaskTypeR\x08taskType\x1a\x41\n\x13\x42\x61seParametersEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01\"U\n\x08TaskType\x12\r\n\tSTREAMING\x10\x00\x12\t\n\x05\x42\x41TCH\x10\x01\x12\n\n\x06INGEST\x10\x02\x12\x0c\n\x08\x44\x45LETION\x10\x03\x12\x15\n\x11\x44\x45LTA_MAINTENANCE\x10\x04\"\x98\x04\n\x0fStartJobRequest\x12H\n\x0bnew_cluster\x18\x01 \x01(\x0b\x32%.tecton_proto.spark_common.NewClusterH\x00R\nnewCluster\x12W\n\x10\x65xisting_cluster\x18\x07 \x01(\x0b\x32*.tecton_proto.spark_common.ExistingClusterH\x00R\x0f\x65xistingCluster\x12\x64\n\x14materialization_task\x18\x02 \x01(\x0b\x32\x31.tecton_proto.spark_api.PythonMaterializationTaskR\x13materializationTask\x12\x19\n\x08run_name\x18\x03 \x01(\tR\x07runName\x12@\n\tlibraries\x18\x04 \x03(\x0b\x32\".tecton_proto.spark_common.LibraryR\tlibraries\x12\'\n\x0ftimeout_seconds\x18\x05 \x01(\x05R\x0etimeoutSeconds\x12\x1f\n\x0bis_notebook\x18\x06 \x01(\x08R\nisNotebook\x12>\n\x1buse_stepped_materialization\x18\x0b \x01(\x08R\x19useSteppedMaterializationB\t\n\x07\x63lusterJ\x04\x08\t\x10\nJ\x04\x08\n\x10\x0b\")\n\x10StartJobResponse\x12\x15\n\x06run_id\x18\x01 \x01(\tR\x05runId\"&\n\rGetJobRequest\x12\x15\n\x06run_id\x18\x01 \x01(\tR\x05runId\"\x86\x03\n\x0eGetJobResponse\x12\x15\n\x06run_id\x18\x01 \x01(\tR\x05runId\x12\x15\n\x06job_id\x18\x07 \x01(\tR\x05jobId\x12 \n\x0crun_page_url\x18\x03 \x01(\tR\nrunPageUrl\x12(\n\x10spark_cluster_id\x18\x06 \x01(\tR\x0esparkClusterId\x12<\n\x07\x64\x65tails\x18\x04 \x01(\x0b\x32\".tecton_proto.spark_api.RunDetailsR\x07\x64\x65tails\x12o\n\x13\x61\x64\x64itional_metadata\x18\x05 \x03(\x0b\x32>.tecton_proto.spark_api.GetJobResponse.AdditionalMetadataEntryR\x12\x61\x64\x64itionalMetadata\x1a\x45\n\x17\x41\x64\x64itionalMetadataEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01J\x04\x08\x02\x10\x03\"\xd0\x01\n\nRunDetails\x12@\n\nrun_status\x18\x01 \x01(\x0e\x32!.tecton_proto.spark_api.RunStatusR\trunStatus\x12[\n\x12termination_reason\x18\x03 \x01(\x0e\x32,.tecton_proto.spark_api.RunTerminationReasonR\x11terminationReason\x12#\n\rstate_message\x18\x02 \x01(\tR\x0cstateMessage\"\'\n\x0eStopJobRequest\x12\x15\n\x06run_id\x18\x01 \x01(\tR\x05runId\"@\n\x0eListJobRequest\x12\x16\n\x06offset\x18\x01 \x01(\x05R\x06offset\x12\x16\n\x06marker\x18\x02 \x01(\tR\x06marker\"\x9f\x02\n\nRunSummary\x12\x15\n\x06run_id\x18\x01 \x01(\tR\x05runId\x12\x1b\n\trun_state\x18\x02 \x01(\tR\x08runState\x12)\n\x10resource_locator\x18\x03 \x01(\tR\x0fresourceLocator\x12k\n\x13\x61\x64\x64itional_metadata\x18\x04 \x03(\x0b\x32:.tecton_proto.spark_api.RunSummary.AdditionalMetadataEntryR\x12\x61\x64\x64itionalMetadata\x1a\x45\n\x17\x41\x64\x64itionalMetadataEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01\"|\n\x0fListJobResponse\x12\x36\n\x04runs\x18\x01 \x03(\x0b\x32\".tecton_proto.spark_api.RunSummaryR\x04runs\x12\x19\n\x08has_more\x18\x02 \x01(\x08R\x07hasMore\x12\x16\n\x06marker\x18\x03 \x01(\tR\x06marker*\xb6\x01\n\tRunStatus\x12\x16\n\x12RUN_STATUS_UNKNOWN\x10\x00\x12\x16\n\x12RUN_STATUS_PENDING\x10\x01\x12\x16\n\x12RUN_STATUS_RUNNING\x10\x02\x12\x16\n\x12RUN_STATUS_SUCCESS\x10\x03\x12\x14\n\x10RUN_STATUS_ERROR\x10\x04\x12\x1a\n\x16RUN_STATUS_TERMINATING\x10\x05\x12\x17\n\x13RUN_STATUS_CANCELED\x10\x06*\x98\x01\n\x14RunTerminationReason\x12\x1e\n\x1aUNKNOWN_TERMINATION_REASON\x10\x00\x12\x10\n\x0cJOB_FINISHED\x10\x01\x12\x16\n\x12MANUAL_CANCELATION\x10\x02\x12\x1f\n\x1bINSTANCE_ALLOCATION_FAILURE\x10\x03\x12\x15\n\x11NON_CLOUD_FAILURE\x10\x04\x42\x18\n\x14\x63om.tecton.spark_apiP\x01')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n!tecton_proto/spark_api/jobs.proto\x12\x16tecton_proto.spark_api\x1a(tecton_proto/spark_common/clusters.proto\x1a)tecton_proto/spark_common/libraries.proto\"\xb7\x03\n\x19PythonMaterializationTask\x12\x38\n\x18materialization_path_uri\x18\x01 \x01(\tR\x16materializationPathUri\x12n\n\x0f\x62\x61se_parameters\x18\x02 \x03(\x0b\x32\x45.tecton_proto.spark_api.PythonMaterializationTask.BaseParametersEntryR\x0e\x62\x61seParameters\x12V\n\x08taskType\x18\x03 \x01(\x0e\x32:.tecton_proto.spark_api.PythonMaterializationTask.TaskTypeR\x08taskType\x1a\x41\n\x13\x42\x61seParametersEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01\"U\n\x08TaskType\x12\r\n\tSTREAMING\x10\x00\x12\t\n\x05\x42\x41TCH\x10\x01\x12\n\n\x06INGEST\x10\x02\x12\x0c\n\x08\x44\x45LETION\x10\x03\x12\x15\n\x11\x44\x45LTA_MAINTENANCE\x10\x04\"\x98\x04\n\x0fStartJobRequest\x12H\n\x0bnew_cluster\x18\x01 \x01(\x0b\x32%.tecton_proto.spark_common.NewClusterH\x00R\nnewCluster\x12W\n\x10\x65xisting_cluster\x18\x07 \x01(\x0b\x32*.tecton_proto.spark_common.ExistingClusterH\x00R\x0f\x65xistingCluster\x12\x64\n\x14materialization_task\x18\x02 \x01(\x0b\x32\x31.tecton_proto.spark_api.PythonMaterializationTaskR\x13materializationTask\x12\x19\n\x08run_name\x18\x03 \x01(\tR\x07runName\x12@\n\tlibraries\x18\x04 \x03(\x0b\x32\".tecton_proto.spark_common.LibraryR\tlibraries\x12\'\n\x0ftimeout_seconds\x18\x05 \x01(\x05R\x0etimeoutSeconds\x12\x1f\n\x0bis_notebook\x18\x06 \x01(\x08R\nisNotebook\x12>\n\x1buse_stepped_materialization\x18\x0b \x01(\x08R\x19useSteppedMaterializationB\t\n\x07\x63lusterJ\x04\x08\t\x10\nJ\x04\x08\n\x10\x0b\")\n\x10StartJobResponse\x12\x15\n\x06run_id\x18\x01 \x01(\tR\x05runId\"&\n\rGetJobRequest\x12\x15\n\x06run_id\x18\x01 \x01(\tR\x05runId\"\x86\x03\n\x0eGetJobResponse\x12\x15\n\x06run_id\x18\x01 \x01(\tR\x05runId\x12\x15\n\x06job_id\x18\x07 \x01(\tR\x05jobId\x12 \n\x0crun_page_url\x18\x03 \x01(\tR\nrunPageUrl\x12(\n\x10spark_cluster_id\x18\x06 \x01(\tR\x0esparkClusterId\x12<\n\x07\x64\x65tails\x18\x04 \x01(\x0b\x32\".tecton_proto.spark_api.RunDetailsR\x07\x64\x65tails\x12o\n\x13\x61\x64\x64itional_metadata\x18\x05 \x03(\x0b\x32>.tecton_proto.spark_api.GetJobResponse.AdditionalMetadataEntryR\x12\x61\x64\x64itionalMetadata\x1a\x45\n\x17\x41\x64\x64itionalMetadataEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01J\x04\x08\x02\x10\x03\"\xd0\x01\n\nRunDetails\x12@\n\nrun_status\x18\x01 \x01(\x0e\x32!.tecton_proto.spark_api.RunStatusR\trunStatus\x12[\n\x12termination_reason\x18\x03 \x01(\x0e\x32,.tecton_proto.spark_api.RunTerminationReasonR\x11terminationReason\x12#\n\rstate_message\x18\x02 \x01(\tR\x0cstateMessage\"\'\n\x0eStopJobRequest\x12\x15\n\x06run_id\x18\x01 \x01(\tR\x05runId\"@\n\x0eListJobRequest\x12\x16\n\x06offset\x18\x01 \x01(\x05R\x06offset\x12\x16\n\x06marker\x18\x02 \x01(\tR\x06marker\"\x9f\x02\n\nRunSummary\x12\x15\n\x06run_id\x18\x01 \x01(\tR\x05runId\x12\x1b\n\trun_state\x18\x02 \x01(\tR\x08runState\x12)\n\x10resource_locator\x18\x03 \x01(\tR\x0fresourceLocator\x12k\n\x13\x61\x64\x64itional_metadata\x18\x04 \x03(\x0b\x32:.tecton_proto.spark_api.RunSummary.AdditionalMetadataEntryR\x12\x61\x64\x64itionalMetadata\x1a\x45\n\x17\x41\x64\x64itionalMetadataEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01\"|\n\x0fListJobResponse\x12\x36\n\x04runs\x18\x01 \x03(\x0b\x32\".tecton_proto.spark_api.RunSummaryR\x04runs\x12\x19\n\x08has_more\x18\x02 \x01(\x08R\x07hasMore\x12\x16\n\x06marker\x18\x03 \x01(\tR\x06marker*\xd7\x01\n\tRunStatus\x12\x16\n\x12RUN_STATUS_UNKNOWN\x10\x00\x12\x16\n\x12RUN_STATUS_PENDING\x10\x01\x12\x16\n\x12RUN_STATUS_RUNNING\x10\x02\x12\x16\n\x12RUN_STATUS_SUCCESS\x10\x03\x12\x14\n\x10RUN_STATUS_ERROR\x10\x04\x12\x1a\n\x16RUN_STATUS_TERMINATING\x10\x05\x12\x17\n\x13RUN_STATUS_CANCELED\x10\x06\x12\x1f\n\x1bRUN_STATUS_SUBMISSION_ERROR\x10\x07*\xae\x01\n\x14RunTerminationReason\x12\x1e\n\x1aUNKNOWN_TERMINATION_REASON\x10\x00\x12\x10\n\x0cJOB_FINISHED\x10\x01\x12\x16\n\x12MANUAL_CANCELATION\x10\x02\x12\x1f\n\x1bINSTANCE_ALLOCATION_FAILURE\x10\x03\x12\x15\n\x11NON_CLOUD_FAILURE\x10\x04\x12\x14\n\x10SUBMISSION_ERROR\x10\x05\x42\x18\n\x14\x63om.tecton.spark_apiP\x01')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.spark_api.jobs_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\024com.tecton.spark_apiP\001'
   _PYTHONMATERIALIZATIONTASK_BASEPARAMETERSENTRY._options = None
   _PYTHONMATERIALIZATIONTASK_BASEPARAMETERSENTRY._serialized_options = b'8\001'
   _GETJOBRESPONSE_ADDITIONALMETADATAENTRY._options = None
   _GETJOBRESPONSE_ADDITIONALMETADATAENTRY._serialized_options = b'8\001'
   _RUNSUMMARY_ADDITIONALMETADATAENTRY._options = None
   _RUNSUMMARY_ADDITIONALMETADATAENTRY._serialized_options = b'8\001'
   _RUNSTATUS._serialized_start=2338
-  _RUNSTATUS._serialized_end=2520
-  _RUNTERMINATIONREASON._serialized_start=2523
-  _RUNTERMINATIONREASON._serialized_end=2675
+  _RUNSTATUS._serialized_end=2553
+  _RUNTERMINATIONREASON._serialized_start=2556
+  _RUNTERMINATIONREASON._serialized_end=2730
   _PYTHONMATERIALIZATIONTASK._serialized_start=147
   _PYTHONMATERIALIZATIONTASK._serialized_end=586
   _PYTHONMATERIALIZATIONTASK_BASEPARAMETERSENTRY._serialized_start=434
   _PYTHONMATERIALIZATIONTASK_BASEPARAMETERSENTRY._serialized_end=499
   _PYTHONMATERIALIZATIONTASK_TASKTYPE._serialized_start=501
   _PYTHONMATERIALIZATIONTASK_TASKTYPE._serialized_end=586
   _STARTJOBREQUEST._serialized_start=589
```

### Comparing `tecton-0.7.0b9/tecton_proto/spark_common/clusters_pb2.py` & `tecton-0.7.0rc0/tecton_proto/spark_common/clusters_pb2.py`

 * *Files 10% similar despite different names*

```diff
@@ -10,50 +10,52 @@
 
 _sym_db = _symbol_database.Default()
 
 
 from google.protobuf import struct_pb2 as google_dot_protobuf_dot_struct__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n(tecton_proto/spark_common/clusters.proto\x12\x19tecton_proto.spark_common\x1a\x1cgoogle/protobuf/struct.proto\"0\n\x0f\x45xistingCluster\x12\x1d\n\ncluster_id\x18\x01 \x01(\tR\tclusterId\"\xbc\x08\n\nNewCluster\x12!\n\x0bnum_workers\x18\x01 \x01(\x05H\x00R\nnumWorkers\x12\x44\n\tautoscale\x18\x02 \x01(\x0b\x32$.tecton_proto.spark_common.AutoScaleH\x00R\tautoscale\x12!\n\x0c\x63luster_name\x18\x03 \x01(\tR\x0b\x63lusterName\x12#\n\rspark_version\x18\x04 \x01(\tR\x0csparkVersion\x12S\n\nspark_conf\x18\x0f \x03(\x0b\x32\x34.tecton_proto.spark_common.NewCluster.SparkConfEntryR\tsparkConf\x12O\n\x0e\x61ws_attributes\x18\x07 \x01(\x0b\x32(.tecton_proto.spark_common.AwsAttributesR\rawsAttributes\x12 \n\x0cnode_type_id\x18\x06 \x01(\tR\nnodeTypeId\x12.\n\x13\x65nable_elastic_disk\x18\x0c \x01(\x08R\x11\x65nableElasticDisk\x12N\n\x0cinit_scripts\x18\t \x03(\x0b\x32+.tecton_proto.spark_common.ResourceLocationR\x0binitScripts\x12U\n\x10\x63luster_log_conf\x18\n \x01(\x0b\x32+.tecton_proto.spark_common.ResourceLocationR\x0e\x63lusterLogConf\x12\x46\n\x0b\x63ustom_tags\x18\x0b \x03(\x0b\x32%.tecton_proto.spark_common.ClusterTagR\ncustomTags\x12\x30\n\x13terminateOnComplete\x18\r \x01(\x08R\x13terminateOnComplete\x12]\n\x0espark_env_vars\x18\x10 \x03(\x0b\x32\x37.tecton_proto.spark_common.NewCluster.SparkEnvVarsEntryR\x0csparkEnvVars\x12G\n\x13json_cluster_config\x18\x11 \x01(\x0b\x32\x17.google.protobuf.StructR\x11jsonClusterConfig\x12\x1b\n\tpolicy_id\x18\x12 \x01(\tR\x08policyId\x1a<\n\x0eSparkConfEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01\x1a?\n\x11SparkEnvVarsEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01\x42\x0e\n\x0cworkers_typeJ\x04\x08\x05\x10\x06J\x04\x08\x08\x10\tJ\x04\x08\x0e\x10\x0f\"\x90\x01\n\x0b\x43lusterInfo\x12\x46\n\x0bnew_cluster\x18\x01 \x01(\x0b\x32%.tecton_proto.spark_common.NewClusterR\nnewCluster\x12\x1d\n\nfinal_json\x18\x02 \x01(\tR\tfinalJson\x12\x1a\n\x08warnings\x18\x03 \x03(\tR\x08warnings\"M\n\tAutoScale\x12\x1f\n\x0bmin_workers\x18\x01 \x01(\x05R\nminWorkers\x12\x1f\n\x0bmax_workers\x18\x02 \x01(\x05R\nmaxWorkers\"\xab\x03\n\rAwsAttributes\x12&\n\x0f\x66irst_on_demand\x18\x05 \x01(\x05R\rfirstOnDemand\x12N\n\x0c\x61vailability\x18\x06 \x01(\x0e\x32*.tecton_proto.spark_common.AwsAvailabilityR\x0c\x61vailability\x12\x17\n\x07zone_id\x18\x07 \x01(\tR\x06zoneId\x12\x33\n\x16spot_bid_price_percent\x18\x08 \x01(\x05R\x13spotBidPricePercent\x12\x30\n\x14instance_profile_arn\x18\x04 \x01(\tR\x12instanceProfileArn\x12P\n\x0f\x65\x62s_volume_type\x18\x01 \x01(\x0e\x32(.tecton_proto.spark_common.EbsVolumeTypeR\rebsVolumeType\x12(\n\x10\x65\x62s_volume_count\x18\x02 \x01(\x05R\x0e\x65\x62sVolumeCount\x12&\n\x0f\x65\x62s_volume_size\x18\x03 \x01(\x05R\rebsVolumeSize\"\xe1\x01\n\x10ResourceLocation\x12:\n\x02s3\x18\x01 \x01(\x0b\x32(.tecton_proto.spark_common.S3StorageInfoH\x00R\x02s3\x12@\n\x04\x64\x62\x66s\x18\x03 \x01(\x0b\x32*.tecton_proto.spark_common.DbfsStorageInfoH\x00R\x04\x64\x62\x66s\x12\x43\n\x05local\x18\x02 \x01(\x0b\x32+.tecton_proto.spark_common.LocalStorageInfoH\x00R\x05localB\n\n\x08location\"I\n\rS3StorageInfo\x12 \n\x0b\x64\x65stination\x18\x01 \x01(\tR\x0b\x64\x65stination\x12\x16\n\x06region\x18\x02 \x01(\tR\x06region\"3\n\x0f\x44\x62\x66sStorageInfo\x12 \n\x0b\x64\x65stination\x18\x01 \x01(\tR\x0b\x64\x65stination\"&\n\x10LocalStorageInfo\x12\x12\n\x04path\x18\x01 \x01(\tR\x04path\"J\n\x0e\x43lusterLogConf\x12\x38\n\x02s3\x18\x01 \x01(\x0b\x32(.tecton_proto.spark_common.S3StorageInfoR\x02s3\"4\n\nClusterTag\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value*\x8f\x01\n\x0f\x41wsAvailability\x12\x1c\n\x18UNKNOWN_AWS_AVAILABILITY\x10\x00\x12\x08\n\x04SPOT\x10\x01\x12\r\n\tON_DEMAND\x10\x02\x12\x16\n\x12SPOT_WITH_FALLBACK\x10\x03\x12-\n)INSTANCE_FLEET_FOR_INTEGRATION_TESTS_ONLY\x10\x04*c\n\rEbsVolumeType\x12\x1b\n\x17UNKNOWN_EBS_VOLUME_TYPE\x10\x00\x12\x17\n\x13GENERAL_PURPOSE_SSD\x10\x01\x12\x1c\n\x18THROUGHPUT_OPTIMIZED_HDD\x10\x02\x42\x1b\n\x17\x63om.tecton.spark_commonP\x01')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n(tecton_proto/spark_common/clusters.proto\x12\x19tecton_proto.spark_common\x1a\x1cgoogle/protobuf/struct.proto\"0\n\x0f\x45xistingCluster\x12\x1d\n\ncluster_id\x18\x01 \x01(\tR\tclusterId\"\x9f\n\n\nNewCluster\x12!\n\x0bnum_workers\x18\x01 \x01(\x05H\x00R\nnumWorkers\x12\x44\n\tautoscale\x18\x02 \x01(\x0b\x32$.tecton_proto.spark_common.AutoScaleH\x00R\tautoscale\x12!\n\x0c\x63luster_name\x18\x03 \x01(\tR\x0b\x63lusterName\x12#\n\rspark_version\x18\x04 \x01(\tR\x0csparkVersion\x12S\n\nspark_conf\x18\x0f \x03(\x0b\x32\x34.tecton_proto.spark_common.NewCluster.SparkConfEntryR\tsparkConf\x12O\n\x0e\x61ws_attributes\x18\x07 \x01(\x0b\x32(.tecton_proto.spark_common.AwsAttributesR\rawsAttributes\x12 \n\x0cnode_type_id\x18\x06 \x01(\tR\nnodeTypeId\x12.\n\x13\x65nable_elastic_disk\x18\x0c \x01(\x08R\x11\x65nableElasticDisk\x12N\n\x0cinit_scripts\x18\t \x03(\x0b\x32+.tecton_proto.spark_common.ResourceLocationR\x0binitScripts\x12U\n\x10\x63luster_log_conf\x18\n \x01(\x0b\x32+.tecton_proto.spark_common.ResourceLocationR\x0e\x63lusterLogConf\x12\x46\n\x0b\x63ustom_tags\x18\x0b \x03(\x0b\x32%.tecton_proto.spark_common.ClusterTagR\ncustomTags\x12\x30\n\x13terminateOnComplete\x18\r \x01(\x08R\x13terminateOnComplete\x12]\n\x0espark_env_vars\x18\x10 \x03(\x0b\x32\x37.tecton_proto.spark_common.NewCluster.SparkEnvVarsEntryR\x0csparkEnvVars\x12O\n\x0egcp_attributes\x18\x13 \x01(\x0b\x32(.tecton_proto.spark_common.GCPAttributesR\rgcpAttributes\x12G\n\x13json_cluster_config\x18\x11 \x01(\x0b\x32\x17.google.protobuf.StructR\x11jsonClusterConfig\x12\x1b\n\tpolicy_id\x18\x12 \x01(\tR\x08policyId\x12\x32\n\x16root_volume_size_in_gb\x18\x14 \x01(\x03R\x12rootVolumeSizeInGb\x12,\n\x12\x64\x61ta_security_mode\x18\x16 \x01(\tR\x10\x64\x61taSecurityMode\x12(\n\x10single_user_name\x18\x17 \x01(\tR\x0esingleUserName\x1a<\n\x0eSparkConfEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01\x1a?\n\x11SparkEnvVarsEntry\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value:\x02\x38\x01\x42\x0e\n\x0cworkers_typeJ\x04\x08\x05\x10\x06J\x04\x08\x08\x10\tJ\x04\x08\x0e\x10\x0fJ\x04\x08\x15\x10\x16\"\x90\x01\n\x0b\x43lusterInfo\x12\x46\n\x0bnew_cluster\x18\x01 \x01(\x0b\x32%.tecton_proto.spark_common.NewClusterR\nnewCluster\x12\x1d\n\nfinal_json\x18\x02 \x01(\tR\tfinalJson\x12\x1a\n\x08warnings\x18\x03 \x03(\tR\x08warnings\"M\n\tAutoScale\x12\x1f\n\x0bmin_workers\x18\x01 \x01(\x05R\nminWorkers\x12\x1f\n\x0bmax_workers\x18\x02 \x01(\x05R\nmaxWorkers\"\xab\x03\n\rAwsAttributes\x12&\n\x0f\x66irst_on_demand\x18\x05 \x01(\x05R\rfirstOnDemand\x12N\n\x0c\x61vailability\x18\x06 \x01(\x0e\x32*.tecton_proto.spark_common.AwsAvailabilityR\x0c\x61vailability\x12\x17\n\x07zone_id\x18\x07 \x01(\tR\x06zoneId\x12\x33\n\x16spot_bid_price_percent\x18\x08 \x01(\x05R\x13spotBidPricePercent\x12\x30\n\x14instance_profile_arn\x18\x04 \x01(\tR\x12instanceProfileArn\x12P\n\x0f\x65\x62s_volume_type\x18\x01 \x01(\x0e\x32(.tecton_proto.spark_common.EbsVolumeTypeR\rebsVolumeType\x12(\n\x10\x65\x62s_volume_count\x18\x02 \x01(\x05R\x0e\x65\x62sVolumeCount\x12&\n\x0f\x65\x62s_volume_size\x18\x03 \x01(\x05R\rebsVolumeSize\"\xe1\x01\n\x10ResourceLocation\x12:\n\x02s3\x18\x01 \x01(\x0b\x32(.tecton_proto.spark_common.S3StorageInfoH\x00R\x02s3\x12@\n\x04\x64\x62\x66s\x18\x03 \x01(\x0b\x32*.tecton_proto.spark_common.DbfsStorageInfoH\x00R\x04\x64\x62\x66s\x12\x43\n\x05local\x18\x02 \x01(\x0b\x32+.tecton_proto.spark_common.LocalStorageInfoH\x00R\x05localB\n\n\x08location\"I\n\rS3StorageInfo\x12 \n\x0b\x64\x65stination\x18\x01 \x01(\tR\x0b\x64\x65stination\x12\x16\n\x06region\x18\x02 \x01(\tR\x06region\"3\n\x0f\x44\x62\x66sStorageInfo\x12 \n\x0b\x64\x65stination\x18\x01 \x01(\tR\x0b\x64\x65stination\"&\n\x10LocalStorageInfo\x12\x12\n\x04path\x18\x01 \x01(\tR\x04path\"J\n\x0e\x43lusterLogConf\x12\x38\n\x02s3\x18\x01 \x01(\x0b\x32(.tecton_proto.spark_common.S3StorageInfoR\x02s3\"4\n\nClusterTag\x12\x10\n\x03key\x18\x01 \x01(\tR\x03key\x12\x14\n\x05value\x18\x02 \x01(\tR\x05value\"\xe4\x01\n\rGCPAttributes\x12:\n\x19use_preemptible_executors\x18\x01 \x01(\x08R\x17usePreemptibleExecutors\x12\x34\n\x16google_service_account\x18\x02 \x01(\tR\x14googleServiceAccount\x12$\n\x0e\x62oot_disk_size\x18\x03 \x01(\x05R\x0c\x62ootDiskSize\x12\"\n\x0c\x61vailability\x18\x04 \x01(\tR\x0c\x61vailability\x12\x17\n\x07zone_id\x18\x05 \x01(\tR\x06zoneId*\x8f\x01\n\x0f\x41wsAvailability\x12\x1c\n\x18UNKNOWN_AWS_AVAILABILITY\x10\x00\x12\x08\n\x04SPOT\x10\x01\x12\r\n\tON_DEMAND\x10\x02\x12\x16\n\x12SPOT_WITH_FALLBACK\x10\x03\x12-\n)INSTANCE_FLEET_FOR_INTEGRATION_TESTS_ONLY\x10\x04*c\n\rEbsVolumeType\x12\x1b\n\x17UNKNOWN_EBS_VOLUME_TYPE\x10\x00\x12\x17\n\x13GENERAL_PURPOSE_SSD\x10\x01\x12\x1c\n\x18THROUGHPUT_OPTIMIZED_HDD\x10\x02\x42\x1b\n\x17\x63om.tecton.spark_commonP\x01')
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
 _builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'tecton_proto.spark_common.clusters_pb2', globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
 
   DESCRIPTOR._options = None
   DESCRIPTOR._serialized_options = b'\n\027com.tecton.spark_commonP\001'
   _NEWCLUSTER_SPARKCONFENTRY._options = None
   _NEWCLUSTER_SPARKCONFENTRY._serialized_options = b'8\001'
   _NEWCLUSTER_SPARKENVVARSENTRY._options = None
   _NEWCLUSTER_SPARKENVVARSENTRY._serialized_options = b'8\001'
-  _AWSAVAILABILITY._serialized_start=2421
-  _AWSAVAILABILITY._serialized_end=2564
-  _EBSVOLUMETYPE._serialized_start=2566
-  _EBSVOLUMETYPE._serialized_end=2665
+  _AWSAVAILABILITY._serialized_start=2879
+  _AWSAVAILABILITY._serialized_end=3022
+  _EBSVOLUMETYPE._serialized_start=3024
+  _EBSVOLUMETYPE._serialized_end=3123
   _EXISTINGCLUSTER._serialized_start=101
   _EXISTINGCLUSTER._serialized_end=149
   _NEWCLUSTER._serialized_start=152
-  _NEWCLUSTER._serialized_end=1236
-  _NEWCLUSTER_SPARKCONFENTRY._serialized_start=1077
-  _NEWCLUSTER_SPARKCONFENTRY._serialized_end=1137
-  _NEWCLUSTER_SPARKENVVARSENTRY._serialized_start=1139
-  _NEWCLUSTER_SPARKENVVARSENTRY._serialized_end=1202
-  _CLUSTERINFO._serialized_start=1239
-  _CLUSTERINFO._serialized_end=1383
-  _AUTOSCALE._serialized_start=1385
-  _AUTOSCALE._serialized_end=1462
-  _AWSATTRIBUTES._serialized_start=1465
-  _AWSATTRIBUTES._serialized_end=1892
-  _RESOURCELOCATION._serialized_start=1895
-  _RESOURCELOCATION._serialized_end=2120
-  _S3STORAGEINFO._serialized_start=2122
-  _S3STORAGEINFO._serialized_end=2195
-  _DBFSSTORAGEINFO._serialized_start=2197
-  _DBFSSTORAGEINFO._serialized_end=2248
-  _LOCALSTORAGEINFO._serialized_start=2250
-  _LOCALSTORAGEINFO._serialized_end=2288
-  _CLUSTERLOGCONF._serialized_start=2290
-  _CLUSTERLOGCONF._serialized_end=2364
-  _CLUSTERTAG._serialized_start=2366
-  _CLUSTERTAG._serialized_end=2418
+  _NEWCLUSTER._serialized_end=1463
+  _NEWCLUSTER_SPARKCONFENTRY._serialized_start=1298
+  _NEWCLUSTER_SPARKCONFENTRY._serialized_end=1358
+  _NEWCLUSTER_SPARKENVVARSENTRY._serialized_start=1360
+  _NEWCLUSTER_SPARKENVVARSENTRY._serialized_end=1423
+  _CLUSTERINFO._serialized_start=1466
+  _CLUSTERINFO._serialized_end=1610
+  _AUTOSCALE._serialized_start=1612
+  _AUTOSCALE._serialized_end=1689
+  _AWSATTRIBUTES._serialized_start=1692
+  _AWSATTRIBUTES._serialized_end=2119
+  _RESOURCELOCATION._serialized_start=2122
+  _RESOURCELOCATION._serialized_end=2347
+  _S3STORAGEINFO._serialized_start=2349
+  _S3STORAGEINFO._serialized_end=2422
+  _DBFSSTORAGEINFO._serialized_start=2424
+  _DBFSSTORAGEINFO._serialized_end=2475
+  _LOCALSTORAGEINFO._serialized_start=2477
+  _LOCALSTORAGEINFO._serialized_end=2515
+  _CLUSTERLOGCONF._serialized_start=2517
+  _CLUSTERLOGCONF._serialized_end=2591
+  _CLUSTERTAG._serialized_start=2593
+  _CLUSTERTAG._serialized_end=2645
+  _GCPATTRIBUTES._serialized_start=2648
+  _GCPATTRIBUTES._serialized_end=2876
 # @@protoc_insertion_point(module_scope)
```

### Comparing `tecton-0.7.0b9/tecton_proto/spark_common/libraries_pb2.py` & `tecton-0.7.0rc0/tecton_proto/spark_common/libraries_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_proto/validation/validator_pb2.py` & `tecton-0.7.0rc0/tecton_proto/validation/validator_pb2.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_snowflake/pipeline_helper.py` & `tecton-0.7.0rc0/tecton_snowflake/pipeline_helper.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 import re
 import sys
+import typing
 from dataclasses import dataclass
 from textwrap import dedent
 from typing import Callable
 from typing import Dict
 from typing import List
 from typing import Optional
 from typing import Tuple
@@ -16,22 +17,22 @@
 from tecton_core.data_types import ArrayType
 from tecton_core.data_types import BoolType
 from tecton_core.data_types import DataType
 from tecton_core.data_types import Float32Type
 from tecton_core.data_types import Float64Type
 from tecton_core.data_types import Int64Type
 from tecton_core.data_types import StringType
+from tecton_core.errors import UDF_ERROR
 from tecton_core.errors import TectonInternalError
 from tecton_core.errors import TectonSnowflakeNotImplementedError
-from tecton_core.errors import UDF_ERROR
 from tecton_core.id_helper import IdHelper
 from tecton_core.materialization_context import BaseMaterializationContext
-from tecton_core.pipeline_common import constant_node_to_value
 from tecton_core.pipeline_common import CONSTANT_TYPE
 from tecton_core.pipeline_common import CONSTANT_TYPE_OBJECTS
+from tecton_core.pipeline_common import constant_node_to_value
 from tecton_core.pipeline_common import get_keyword_inputs
 from tecton_core.pipeline_common import get_time_window_from_data_source_node
 from tecton_core.pipeline_common import positional_inputs
 from tecton_core.pipeline_common import transformation_type_checker
 from tecton_proto.args.pipeline_pb2 import DataSourceNode
 from tecton_proto.args.pipeline_pb2 import Pipeline
 from tecton_proto.args.pipeline_pb2 import PipelineNode
@@ -39,14 +40,19 @@
 from tecton_proto.args.transformation_pb2 import TransformationArgs
 from tecton_proto.args.transformation_pb2 import TransformationMode
 from tecton_proto.data.transformation_pb2 import Transformation
 from tecton_snowflake.templates_utils import load_template
 from tecton_snowflake.utils import format_sql
 from tecton_snowflake.utils import generate_random_name
 
+
+if typing.TYPE_CHECKING:
+    import snowflake.snowpark
+
+
 PIPELINE_TEMPLATE = load_template("transformation_pipeline.sql")
 TIME_LIMIT_TEMPLATE = load_template("time_limit.sql")
 DATA_SOURCE_TEMPLATE = load_template("data_source.sql")
 TEMP_CTE_PREFIX = "_TT_CTE_"
 TEMP_DS_PREFIX = "_TT_DS_"
 SPINE_TABLE_NAME = "_TT_SPINE_TABLE"
 UDF_OUTPUT_COLUMN_NAME = "_TT_UDF_OUTPUT"
@@ -174,15 +180,16 @@
             #   xxx
             #   xxx);
             # Remove the "create or replace view XXX as" part
             m = re.search(f"create or replace view {temp_pipeline_view_name}\([^()]*\) as ((?s:.)*);", generated_sql)
             if m:
                 view_sql = m.group(1)
             else:
-                raise TectonInternalError(f"Couldn't extract from generated sql query: {generated_sql}")
+                msg = f"Couldn't extract from generated sql query: {generated_sql}"
+                raise TectonInternalError(msg)
             self._session.sql(f"DROP VIEW IF EXISTS {temp_pipeline_view_name}")
             return view_sql
         else:
             sql_str = self._node_to_value(self._pipeline.root)
             assert isinstance(sql_str, str)
             sql_str = DATA_SOURCE_TEMPLATE.render(data_sources=self._ds_to_sql_str, source=sql_str)
             return sql_str
@@ -199,87 +206,85 @@
         elif pipeline_node.HasField("data_source_node"):
             return self._data_source_node_to_value(pipeline_node.data_source_node)
         elif pipeline_node.HasField("constant_node"):
             return constant_node_to_value(pipeline_node.constant_node)
         elif pipeline_node.HasField("materialization_context_node"):
             return self._materialization_context
         elif pipeline_node.HasField("request_data_source_node"):
-            raise TectonSnowflakeNotImplementedError("RequestDataSource is not supported in Snowflake SQL pipelines")
+            msg = "RequestDataSource is not supported in Snowflake SQL pipelines"
+            raise TectonSnowflakeNotImplementedError(msg)
         elif pipeline_node.HasField("feature_view_node"):
-            raise TectonSnowflakeNotImplementedError(
-                "Dependent FeatureViews are not supported in Snowflake SQL pipelines"
-            )
+            msg = "Dependent FeatureViews are not supported in Snowflake SQL pipelines"
+            raise TectonSnowflakeNotImplementedError(msg)
         else:
-            raise KeyError(f"Unknown PipelineNode type: {pipeline_node}")
+            msg = f"Unknown PipelineNode type: {pipeline_node}"
+            raise KeyError(msg)
 
     def _data_source_node_to_value(
         self, data_source_node: DataSourceNode
     ) -> Union[str, "snowflake.snowpark.DataFrame"]:
         """Creates a sql string from a ds and time parameters."""
+        sql_str = ""
         if self._mock_sql_inputs is not None and data_source_node.input_name in self._mock_sql_inputs:
-            return f"({self._mock_sql_inputs[data_source_node.input_name]})"
+            sql_str = self._mock_sql_inputs[data_source_node.input_name]
         else:
             time_window = get_time_window_from_data_source_node(
                 feature_time_limits=pendulum.Period(
                     self._materialization_context.start_time, self._materialization_context.end_time
                 ),
                 schedule_interval=self._materialization_context.batch_schedule,
                 data_source_node=data_source_node,
             )
             ds = self._id_to_ds[IdHelper.to_string(data_source_node.virtual_data_source_id)]
-            ds = self._get_ds_sql_str(ds, time_window)
-            if self._has_snowpark:
-                return self._session.sql(ds)
-            else:
-                return ds
+            sql_str = self._get_ds_sql_str(ds, time_window)
+
+        cte_name = f"{TEMP_DS_PREFIX}_{generate_random_name()}"
+        self._ds_to_sql_str[cte_name] = sql_str
+        if self._has_snowpark:
+            return self._session.sql(sql_str)
+        else:
+            return cte_name
 
     def _get_ds_sql_str(self, ds: specs.DataSourceSpec, time_window: Optional[pendulum.Period]) -> str:
         if not ds.batch_source:
-            raise TectonSnowflakeNotImplementedError("Snowflake SQL pipeline only supports batch data source")
+            msg = "Snowflake SQL pipeline only supports batch data source"
+            raise TectonSnowflakeNotImplementedError(msg)
         if not isinstance(ds.batch_source, specs.SnowflakeSourceSpec):
-            raise TectonSnowflakeNotImplementedError(
-                f"Snowflake SQL pipeline does not support batch data source: {ds.batch_source}"
-            )
+            msg = f"Snowflake SQL pipeline does not support batch data source: {ds.batch_source}"
+            raise TectonSnowflakeNotImplementedError(msg)
 
         snowflake_source = ds.batch_source
 
         if snowflake_source.table:
             if snowflake_source.query:
-                raise TectonInternalError("Only one of table and query can be specified")
+                msg = "Only one of table and query can be specified"
+                raise TectonInternalError(msg)
             # Makes sure we have all the info for the table
             assert snowflake_source.database
             assert snowflake_source.schema
             sql_str = f"{snowflake_source.database}.{snowflake_source.schema}.{snowflake_source.table}"
         elif snowflake_source.query:
             sql_str = snowflake_source.query
         else:
-            raise TectonInternalError(f"Either table or query must be specified for Snowflake data source")
+            msg = "Either table or query must be specified for Snowflake data source"
+            raise TectonInternalError(msg)
 
-        # If we have a time window, we need to filter the source based on it
-        if time_window is not None:
-            if not snowflake_source.timestamp_field:
-                raise TectonInternalError(
-                    f"timestamp_field must be set within Snowflake data source '{ds.name}' to use time filtering in this feature view."
-                )
-
-            sql_str = TIME_LIMIT_TEMPLATE.render(
-                source=sql_str,
-                timestamp_key=snowflake_source.timestamp_field,
-                start_time=time_window.start,
-                end_time=time_window.end,
-            )
-        else:
-            sql_str = f"SELECT * FROM ({sql_str})"
+        if time_window is None:
+            return f"SELECT * FROM ({sql_str})"
 
-        cte_name = f"{TEMP_DS_PREFIX}_{generate_random_name()}"
-        self._ds_to_sql_str[cte_name] = sql_str
-        if self._has_snowpark:
-            return sql_str
-        else:
-            return cte_name
+        # If we have a time window, we need to filter the source based on it
+        if not snowflake_source.timestamp_field:
+            msg = f"timestamp_field must be set within Snowflake data source '{ds.name}' to use time filtering in this feature view."
+            raise TectonInternalError(msg)
+        return TIME_LIMIT_TEMPLATE.render(
+            source=sql_str,
+            timestamp_key=snowflake_source.timestamp_field,
+            start_time=time_window.start,
+            end_time=time_window.end,
+        )
 
     def _transformation_node_to_value(
         self, transformation_node: TransformationNode
     ) -> Union[str, "snowflake.snowpark.DataFrame"]:
         """Recursively translates inputs to values and then passes them to the transformation."""
         args = []
         kwargs = {}
@@ -287,15 +292,16 @@
             node_value = self._node_to_value(transformation_input.node)
             if transformation_input.HasField("arg_index"):
                 assert len(args) == transformation_input.arg_index
                 args.append(node_value)
             elif transformation_input.HasField("arg_name"):
                 kwargs[transformation_input.arg_name] = node_value
             else:
-                raise KeyError(f"Unknown argument type for Input node: {transformation_input}")
+                msg = f"Unknown argument type for Input node: {transformation_input}"
+                raise KeyError(msg)
 
         return self._apply_transformation_function(transformation_node, args, kwargs)
 
     def _apply_transformation_function(
         self, transformation_node, args, kwargs
     ) -> Union[str, "snowflake.snowpark.DataFrame"]:
         """For the given transformation node, returns the corresponding sql string or dataframe."""
@@ -305,17 +311,16 @@
         if transformation.transformation_mode == TransformationMode.TRANSFORMATION_MODE_SNOWFLAKE_SQL:
             return self._wrap_sql_function(transformation_node, user_function)(*args, **kwargs)
         elif transformation.transformation_mode == TransformationMode.TRANSFORMATION_MODE_SNOWPARK:
             res = user_function(*args, **kwargs)
             transformation_type_checker(transformation.name, res, "snowpark", self._possible_modes())
             return res
         else:
-            raise KeyError(
-                f"Invalid transformation mode: {TransformationMode.Name(transformation.transformation_mode)} for a Snowflake SQL pipeline"
-            )
+            msg = f"Invalid transformation mode: {TransformationMode.Name(transformation.transformation_mode)} for a Snowflake SQL pipeline"
+            raise KeyError(msg)
 
     def _wrap_sql_function(
         self, transformation_node: TransformationNode, user_function: Callable[..., str]
     ) -> Callable[..., str]:
         def wrapped(*args, **kwargs):
             transformationInputs = []
             wrapped_args = []
@@ -392,15 +397,16 @@
         self._fv_id = fv_id
         self._id_to_transformation = {t.id: t for t in transformations}
         self._output_schema = output_schema
         self._append_prefix = append_prefix
 
     def get_df(self) -> "snowflake.snowpark.DataFrame":
         if not self._pipeline.root.HasField("transformation_node"):
-            raise ValueError("Root pipeline has to be a transformation for pandas mode")
+            msg = "Root pipeline has to be a transformation for pandas mode"
+            raise ValueError(msg)
         output_df, _ = self._transformation_node_to_df(
             self._pipeline.root.transformation_node, is_top_level_transformation=True
         )
         return output_df
 
     def _transformation_node_to_df(
         self, transformation_node: TransformationNode, is_top_level_transformation: bool = False
@@ -458,17 +464,16 @@
                         continue
                     input_columns.append(feature)
                     features.append(feature)
                 # We don't know the param name for nested transformations so we set a placeholder name.
                 input_map[TRANSFORMATION_INPUT_PARAMETER] = features
                 prefix_map[TRANSFORMATION_INPUT_PARAMETER] = prefix
             else:
-                raise TectonSnowflakeNotImplementedError(
-                    "Snowflake only supports feature view and request data source as input."
-                )
+                msg = "Snowflake only supports feature view and request data source as input."
+                raise TectonSnowflakeNotImplementedError(msg)
         # Get back the name of the UDF
         ondemand_udf = self._generate_on_demand_udf(transformation_node, input_map, prefix_map, input_columns)
         # Call the udf and return the output dataframe and columns for this dataframe.
         # We return the columns explicitly since snowflake will auto capitalize them which causes some issues.
         return self._call_udf(ondemand_udf, input_columns, input_df, is_top_level_transformation)
 
     def _get_dict_keys_from_udf_output(self, output_df: "snowflake.snowpark.DataFrame"):
@@ -489,15 +494,16 @@
     def _call_udf(
         self,
         ondemand_udf: "snowflake.snowpark.udf.UserDefinedFunction",
         input_columns: List[str],
         input_df: "snowflake.snowpark.DataFrame",
         is_top_level_transformation: bool,
     ) -> ("snowflake.snowpark.DataFrame", list):
-        from snowflake.snowpark.functions import array_construct, col
+        from snowflake.snowpark.functions import array_construct
+        from snowflake.snowpark.functions import col
 
         # Call the udf and append the result to the input dataframe.
         try:
             output_df = input_df.withColumn(UDF_OUTPUT_COLUMN_NAME, ondemand_udf(array_construct(*input_columns)))
         except Exception as e:
             raise UDF_ERROR(e)
 
@@ -505,24 +511,18 @@
         # Only do this for the top level transformation in the pipeline.
         if is_top_level_transformation:
             for column in self._output_schema.keys():
                 output_df = output_df.withColumn(
                     self._namespace + SEPERATOR + column if self._append_prefix else column,
                     col(UDF_OUTPUT_COLUMN_NAME)[column].cast(SPARK_TO_SNOWFLAKE_TYPES[self._output_schema[column]]),
                 )
-            columns_to_select = list(
-                filter(
-                    lambda x: "_UDF_INTERNAL" not in x,
-                    list(input_df.columns)
-                    + [
-                        self._namespace + SEPERATOR + column if self._append_prefix else column
-                        for column in self._output_schema.keys()
-                    ],
-                )
-            )
+            columns_to_select = list(input_df.columns) + [
+                self._namespace + SEPERATOR + column if self._append_prefix else column
+                for column in self._output_schema.keys()
+            ]
             return output_df.select(*columns_to_select), columns_to_select
 
         udf_column_names_list = self._get_dict_keys_from_udf_output(output_df)
         # Rename the output columns with the transformation prefix since this df will be the input to another transformation.
         feature_columns = []
         for feature in udf_column_names_list:
             feature_name = TRANSFORMATION_COLUMN_PREFIX + feature
@@ -556,15 +556,15 @@
                 # For udfs that have a data source or feature view as an input, use kwargs.
                 # For udfs that have a transformation result as an input, use args.
                 kwargs = {}
                 args = None
                 all_inputs_df = pandas.DataFrame([params], columns=input_columns)
                 for input_name, columns in input_map.items():
                     if input_name == TRANSFORMATION_INPUT_PARAMETER:
-                        assert args == None, "Only one transformation can be the input to another transformation"
+                        assert args is None, "Only one transformation can be the input to another transformation"
                         df = all_inputs_df[columns]
                         df.columns = df.columns.str[len(prefix_map[input_name]) :]
                         args = df
                     else:
                         df = all_inputs_df[columns]
                         df.columns = df.columns.str[len(prefix_map[input_name]) :]
                         kwargs[input_name] = df
@@ -585,23 +585,23 @@
                 # For udfs that have a data source or feature view as an input, use kwargs.
                 # For udfs that have a transformation result as an input, use args.
                 kwargs = {}
                 args = None
                 all_inputs = dict(zip(input_columns, params))
                 for input_name, columns in input_map.items():
                     if input_name == TRANSFORMATION_INPUT_PARAMETER:
-                        assert args == None, "Only one transformation can be the input to another transformation"
+                        assert args is None, "Only one transformation can be the input to another transformation"
                         args = {
                             _consume_prefix(column, prefix_map[input_name]): all_inputs[column] for column in columns
                         }
                     else:
                         kwargs[input_name] = {
                             _consume_prefix(column, prefix_map[input_name]): all_inputs[column] for column in columns
                         }
-                if args != None:
+                if args is not None:
                     return user_function(args)
 
                 return user_function(**kwargs)
 
         if use_pandas:
             # Make sure to update the pandas version in python/requirements.in as well to keep it in sync.
             self._session.add_packages("pandas==1.3.5")
```

### Comparing `tecton-0.7.0b9/tecton_snowflake/schema_derivation_utils.py` & `tecton-0.7.0rc0/tecton_snowflake/schema_derivation_utils.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,12 @@
 import json
+import typing
 from typing import Dict
 from typing import List
+from typing import Optional
 
 import pendulum
 
 from tecton_core import errors
 from tecton_core import feature_definition_wrapper
 from tecton_core import id_helper
 from tecton_core import materialization_context
@@ -17,27 +19,55 @@
 from tecton_proto.common import schema_pb2
 from tecton_proto.common import spark_schema_pb2
 from tecton_snowflake import pipeline_helper
 from tecton_snowflake import snowflake_type_utils
 from tecton_snowflake.utils import format_sql
 
 
+if typing.TYPE_CHECKING:
+    import snowflake.connector
+    import snowflake.snowpark
+
+
+def _get_mock_sql_for_data_source(data_source_spec: specs.DataSourceSpec) -> str:
+    """Adds LIMIT 0 to data sources during fv schema derivation to improve performance.
+
+    Similar to get_data_source_schema_sql expect this method runs on specs."""
+    snowflake_source = data_source_spec.batch_source
+    if type(snowflake_source) != specs.SnowflakeSourceSpec:
+        msg = f"The batch source for Data Source {data_source_spec.name} must be a Snowflake source."
+        raise ValueError(msg)
+
+    if snowflake_source.table:
+        sql_str = (
+            f"SELECT * FROM ({snowflake_source.database}.{snowflake_source.schema}.{snowflake_source.table}) LIMIT 0"
+        )
+    elif snowflake_source.query:
+        sql_str = f"SELECT * FROM ({snowflake_source.query}) LIMIT 0"
+    else:
+        msg = "A Snowflake source must have one of 'query' or 'table' set"
+        raise ValueError(msg)
+    return format_sql(sql_str)
+
+
 def get_data_source_schema_sql(ds_args: data_source_pb2.SnowflakeDataSourceArgs) -> str:
-    """Return the SQL used to query the data source provided."""
+    """Return the SQL used to query the data source provided during schema derivation.
+
+    Similar to _get_mock_sql_for_data_source expect this method runs on data source args."""
     if ds_args.HasField("table"):
         if not ds_args.database and not ds_args.schema:
-            raise ValueError(
-                "A Snowflake source must set 'database', 'schema', and 'table' to read from a Snowflake table."
-            )
+            msg = "A Snowflake source must set 'database', 'schema', and 'table' to read from a Snowflake table."
+            raise ValueError(msg)
         full_table_name = f"{ds_args.database}.{ds_args.schema}.{ds_args.table}"
         sql_str = f"SELECT * FROM {full_table_name} LIMIT 0"
     elif ds_args.HasField("query"):
         sql_str = f"SELECT * FROM ({ds_args.query}) LIMIT 0"
     else:
-        raise ValueError("A Snowflake source must have one of 'query' or 'table' set")
+        msg = "A Snowflake source must have one of 'query' or 'table' set"
+        raise ValueError(msg)
     return format_sql(sql_str)
 
 
 def get_snowflake_schema(
     ds_args: virtual_data_source_pb2.VirtualDataSourceArgs, connection: "snowflake.connector.Connection"
 ) -> spark_schema_pb2.SparkSchema:
     """Derive schema for snowflake data source on snowflake compute.
@@ -47,15 +77,16 @@
     """
     cur = connection.cursor()
 
     sql_str = get_data_source_schema_sql(ds_args.snowflake_ds_config)
     try:
         cur.execute(sql_str)
     except Exception:
-        raise errors.TectonInternalError(f"Running the following SQL failed: {sql_str}")
+        msg = f"Running the following SQL failed: {sql_str}"
+        raise errors.TectonInternalError(msg)
 
     # Get the schema from the previously ran query
     query_id = cur.sfqid
     cur.execute(f"DESCRIBE RESULT '{query_id}';")
     schema_list = cur.fetchall()  # TODO: use fetch_pandas_all() once it supports describe statements
 
     proto = spark_schema_pb2.SparkSchema()
@@ -64,47 +95,55 @@
         name = row[0]
         proto_field = proto.fields.add()
         proto_field.name = name
         proto_field.structfield_json = json.dumps({"name": name, "type": row[1], "nullable": True, "metadata": {}})
     return proto
 
 
-def _get_mock_sql_for_data_source(data_source_spec: specs.DataSourceSpec) -> str:
-    """Adds LIMIT 0 to data sources during fv schema derivation to improve performance."""
-    snowflake_source = data_source_spec.batch_source
-    if type(snowflake_source) != specs.SnowflakeSourceSpec:
-        raise ValueError(f"The batch source for Data Source {data_source_spec.name} must be a Snowflake source.")
-
-    if snowflake_source.table:
-        return f"SELECT * FROM ({snowflake_source.database}.{snowflake_source.schema}.{snowflake_source.table}) LIMIT 0"
-    elif snowflake_source.query:
-        return f"SELECT * FROM ({snowflake_source.query}) LIMIT 0"
-    else:
-        raise ValueError("A Snowflake source must have one of 'query' or 'table' set")
-
-
 def _get_mock_sql_inputs_for_schema_derivation(
     pipeline: pipeline_pb2.Pipeline, data_source_specs: List[specs.DataSourceSpec]
 ) -> Dict[str, str]:
     data_source_nodes = feature_definition_wrapper.pipeline_to_ds_inputs(pipeline).values()
     id_to_spec = {spec.id: spec for spec in data_source_specs}
     mock_sql_inputs = {}
     for node in data_source_nodes:
         data_source_id = id_helper.IdHelper.to_string(node.virtual_data_source_id)
         spec = id_to_spec[data_source_id]
         # Map the data source input name to the SQL to run. The input name and data source name are not always the same.
         mock_sql_inputs[node.input_name] = _get_mock_sql_for_data_source(spec)
     return mock_sql_inputs
 
 
+def get_feature_view_schema_sql(
+    pipeline: pipeline_pb2.Pipeline,
+    transformation_specs: List[specs.TransformationSpec],
+    data_source_specs: List[specs.DataSourceSpec],
+    materialization_context: materialization_context.BaseMaterializationContext,
+    session: Optional["snowflake.snowpark.Session"] = None,
+) -> str:
+    """Return the SQL used to run fv schema derivation."""
+    # Create mock sql inputs for data sources to improve performance.
+    mock_sql_inputs = _get_mock_sql_inputs_for_schema_derivation(pipeline, data_source_specs)
+
+    return pipeline_helper.pipeline_to_sql_string(
+        pipeline,
+        data_source_specs,
+        transformation_specs,
+        mock_sql_inputs=mock_sql_inputs,
+        materialization_context=materialization_context,
+        session=session,
+    )
+
+
 def get_feature_view_view_schema(
     feature_view_args: feature_view_pb2.FeatureViewArgs,
     transformation_specs: List[specs.TransformationSpec],
     data_source_specs: List[specs.DataSourceSpec],
     connection: "snowflake.connector.Connection",
+    session: Optional["snowflake.snowpark.Session"] = None,
 ) -> schema_pb2.Schema:
     """Compute the Feature View view schema.
 
     This method is used for notebook driven development.
     The logic should mirror logic in resolve() in SnowflakeDDL.kt.
     """
     cur = connection.cursor()
@@ -112,41 +151,32 @@
     # Create a default materialization context for the feature view.
     _tecton_materialization_context = materialization_context.BoundMaterializationContext._create_internal(
         pendulum.from_timestamp(0, pendulum.tz.UTC),
         pendulum.datetime(2100, 1, 1),
         pendulum.Duration(),
     )
 
-    # Create mock sql inputs for data sources to improve performance.
-    mock_sql_inputs = _get_mock_sql_inputs_for_schema_derivation(feature_view_args.pipeline, data_source_specs)
-
-    sql_str = pipeline_helper.pipeline_to_sql_string(
-        feature_view_args.pipeline,
-        data_source_specs,
-        transformation_specs,
-        mock_sql_inputs=mock_sql_inputs,
-        materialization_context=_tecton_materialization_context,
+    sql_str = get_feature_view_schema_sql(
+        feature_view_args.pipeline, transformation_specs, data_source_specs, _tecton_materialization_context, session
     )
     try:
         cur.execute(sql_str)
     except Exception:
-        raise errors.TectonInternalError(f"Running the following SQL failed: {sql_str}")
+        msg = f"Running the following SQL failed: {sql_str}"
+        raise errors.TectonInternalError(msg)
 
     # Get the schema from the previously ran query
     query_id = cur.sfqid
     cur.execute(f"DESCRIBE RESULT '{query_id}';")
     schema_list = cur.fetchall()  # TODO: use fetch_pandas_all() once it supports describe statements
 
     columns = []
     for row in schema_list:
         # schema returned is in the form (name, type,...)
         name = row[0]
-        raw_snowflake_type = row[1].split("(")[
-            0
-        ]  # ex. a string type is returned as VARCHAR(16777216) from snowflake. The raw_snowflake_type field expects VARCHAR.
-        tecton_type = snowflake_type_utils.snowflake_type_to_tecton_type(raw_snowflake_type, name)
+        tecton_type = snowflake_type_utils.snowflake_type_to_tecton_type(row[1], name)
         column_proto = schema_pb2.Column()
         column_proto.CopyFrom(core_schema_derivation_utils.column_from_tecton_data_type(tecton_type))
         column_proto.name = name
         columns.append(column_proto)
 
     return schema_pb2.Schema(columns=columns)
```

### Comparing `tecton-0.7.0b9/tecton_snowflake/sql_helper.py` & `tecton-0.7.0rc0/tecton_snowflake/sql_helper.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,19 +1,21 @@
 import re
+import typing
 from collections import defaultdict
 from dataclasses import dataclass
 from datetime import datetime
 from typing import Dict
 from typing import List
 from typing import Optional
 from typing import Set
 from typing import Union
 
 import pandas
 import pendulum
+from google.protobuf import duration_pb2
 
 from tecton_core import conf
 from tecton_core import data_types
 from tecton_core import errors
 from tecton_core import specs
 from tecton_core.errors import INVALID_SPINE_SQL
 from tecton_core.errors import START_TIME_NOT_BEFORE_END_TIME
@@ -27,27 +29,34 @@
 from tecton_proto.data import feature_view_pb2
 from tecton_snowflake.pipeline_helper import pipeline_to_df_with_input
 from tecton_snowflake.pipeline_helper import pipeline_to_sql_string
 from tecton_snowflake.templates_utils import load_template
 from tecton_snowflake.utils import format_sql
 from tecton_snowflake.utils import generate_random_name
 
+
+if typing.TYPE_CHECKING:
+    import snowflake.connector
+    import snowflake.snowpark
+
+
 TEMP_SPINE_TABLE_NAME_FROM_DF = "_TEMP_SPINE_TABLE_FROM_DF"
 TEMP_SPINE_TABLE_NAME_PREFIX = "_TT_SPINE_TABLE"
 
 FULL_AGGREGATION_TEMPLATE = load_template("run_full_aggregation.sql")
 HISTORICAL_FEATURES_TEMPLATE = load_template("historical_features.sql")
 MATERIALIZATION_TILE_TEMPLATE = load_template("materialization_tile.sql")
 MATERIALIZED_FEATURE_VIEW_TEMPLATE = load_template("materialized_feature_view.sql")
 ONLINE_STORE_COPIER_TEMPLATE = load_template("online_store_copier.sql")
 PARTIAL_AGGREGATION_TEMPLATE = load_template("run_partial_aggregation.sql")
 TIME_LIMIT_TEMPLATE = load_template("time_limit.sql")
 DELETE_STAGED_FILES_TEMPLATE = load_template("delete_staged_files.sql")
 CREATE_TEMP_TABLE_BFV_TEMPLATE = load_template("create_temp_table_for_bfv.sql")
 CREATE_TEMP_TABLE_BWAFV_TEMPLATE = load_template("create_temp_table_for_bwafv.sql")
+DELETE_ORPHANED_SCHEMAS_TEMPLATE = load_template("delete_orphaned_schemas.sql")
 
 # TODO(TEC-6204): Last and LastN are not currently supported
 #
 # Map of proto function type -> set of (output column prefix, snowflake function name)
 AGGREGATION_PLANS = {
     afpb.AGGREGATION_FUNCTION_SUM: lambda: {("SUM", "SUM")},
     afpb.AGGREGATION_FUNCTION_MIN: lambda: {("MIN", "MIN")},
@@ -96,14 +105,16 @@
     timestamp_key: str
     join_keys: Dict[str, str]
     features: List[str]
     sql: str
     aggregation: Optional[feature_view_pb2.TrailingTimeWindowAggregation]
     ttl_seconds: Optional[int]
     append_prefix: bool
+    batch_schedule: int
+    data_delay: int
 
 
 def get_historical_features_sql(
     spine_sql: Optional[str],
     feature_set_config: FeatureSetConfig,
     timestamp_key: Optional[str],
     from_source: Optional[bool],
@@ -111,16 +122,17 @@
     start_time: Optional[datetime] = None,
     end_time: Optional[datetime] = None,
     session: "snowflake.snowpark.Session" = None,
     append_prefix: bool = True,  # Whether to append the prefix to the feature column name
 ) -> List[str]:
     suffix = generate_random_name()
     spine_table_name = f"{TEMP_SPINE_TABLE_NAME_PREFIX}_{suffix}"
-    # Whether to register temp tables with the session, or use a gaint query
+    # Whether to register temp views with the session, or use a giant query
     use_short_sql = conf.get_bool("SNOWFLAKE_SHORT_SQL_ENABLED")
+    use_temp_table = conf.get_bool("SNOWFLAKE_TEMP_TABLE_ENABLED")
     feature_set_items = feature_set_config.definitions_and_configs
     input_items = []
     if spine_sql is None:
         # Only feature view is supported when the spine is not provided.
         # Feature service should always provide the spine.
         # SDK methods should never fail this check
         assert len(feature_set_items) == 1
@@ -133,15 +145,16 @@
         if not fd.is_on_demand:
             join_keys = {key: value for key, value in item.join_keys}
             spine_keys.update(join_keys)
 
     for item in feature_set_items:
         fd = item.feature_definition
         if fd.is_on_demand and not conf.get_bool("ALPHA_SNOWFLAKE_SNOWPARK_ENABLED"):
-            raise TectonSnowflakeNotImplementedError("On-demand features are only supported with Snowpark enabled")
+            msg = "On-demand features are only supported with Snowpark enabled"
+            raise TectonSnowflakeNotImplementedError(msg)
         # Change the feature view name if it's for internal udf use.
         is_internal_udf_feature = item.namespace.startswith(UDF_INTERNAL)
         if is_internal_udf_feature:
             name = item.namespace.upper()
         else:
             name = fd.name
 
@@ -149,15 +162,16 @@
             join_keys = {key: value for key, value in item.join_keys}
             features = [
                 col_name
                 for col_name in fd.view_schema.column_names()
                 if col_name not in (list(join_keys.keys()) + [fd.timestamp_key])
             ]
             if len(fd.online_serving_index.join_keys) != len(fd.join_keys):
-                raise TectonSnowflakeNotImplementedError("Wildcard is not supported for Snowflake")
+                msg = "Wildcard is not supported for Snowflake"
+                raise TectonSnowflakeNotImplementedError(msg)
             if start_time is None or (
                 fd.feature_start_timestamp is not None and start_time < fd.feature_start_timestamp
             ):
                 raw_data_start_time = fd.feature_start_timestamp
             else:
                 raw_data_start_time = start_time
 
@@ -173,66 +187,74 @@
                 feature_end_time=raw_data_end_time,
                 # If the spine sql has undetermined results(LIMIT X etc.), we don't want
                 # to run it twice. So we use the subquery we defined here directly.
                 spine=spine_table_name if spine_sql is not None else None,
                 spine_timestamp_key=timestamp_key,
                 session=session,
                 from_source=from_source,
+                data_delay=fd.max_source_data_delay,
             )
             input_items.append(
                 _FeatureSetItemInput(
                     name=name,
                     namespace=item.namespace or name,
                     timestamp_key=fd.timestamp_key,
                     join_keys=join_keys,
                     features=features,
                     sql=sql_str,
                     aggregation=(fd.trailing_time_window_aggregation if fd.is_temporal_aggregate else None),
                     ttl_seconds=(int(fd.serving_ttl.total_seconds()) if fd.is_temporal else None),
                     append_prefix=append_prefix or is_internal_udf_feature,
+                    batch_schedule=0
+                    if fd.min_scheduling_interval is None
+                    else int(fd.min_scheduling_interval.total_seconds()),
+                    data_delay=fd.max_source_data_delay.total_seconds(),
                 )
             )
     queries = []
 
     if spine_sql is not None:
-        if use_short_sql:
-            queries.append(f"CREATE TEMPORARY VIEW {spine_table_name} AS ({spine_sql})")
+        if use_short_sql or use_temp_table:
+            view_or_table = "TABLE" if use_temp_table else "VIEW"
+            queries.append(f"CREATE TEMPORARY {view_or_table} {spine_table_name} AS ({spine_sql})")
             for item in input_items:
                 if item.aggregation is not None:
                     queries.append(
                         format_sql(
                             CREATE_TEMP_TABLE_BWAFV_TEMPLATE.render(
                                 item=item,
                                 spine_table_name=spine_table_name,
                                 spine_keys=list(spine_keys),
                                 spine_timestamp_key=timestamp_key,
                                 suffix=suffix,
+                                view_or_table=view_or_table,
                             )
                         )
                     )
                 else:
                     queries.append(
                         format_sql(
                             CREATE_TEMP_TABLE_BFV_TEMPLATE.render(
                                 item=item,
                                 spine_table_name=spine_table_name,
                                 spine_keys=list(spine_keys),
                                 spine_timestamp_key=timestamp_key,
                                 include_feature_view_timestamp_columns=include_feature_view_timestamp_columns,
                                 suffix=suffix,
+                                view_or_table=view_or_table,
                             )
                         )
                     )
         sql_str = HISTORICAL_FEATURES_TEMPLATE.render(
             feature_set_items=input_items,
             spine_timestamp_key=timestamp_key,
             spine_sql=spine_sql,
             include_feature_view_timestamp_columns=include_feature_view_timestamp_columns,
             spine_keys=list(spine_keys),
-            use_temp_tables=use_short_sql,
+            use_temp_tables=use_short_sql or use_temp_table,
             spine_table_name=spine_table_name,
             suffix=suffix,
         )
     if start_time is not None or end_time is not None:
         timestamp = timestamp_key if spine_sql is not None else fd.timestamp_key
         sql_str = TIME_LIMIT_TEMPLATE.render(
             source=sql_str, timestamp_key=timestamp, start_time=start_time, end_time=end_time
@@ -284,15 +306,16 @@
     df: pandas.DataFrame,
     table_name: str,
     session: "snowflake.snowpark.Session" = None,
     connection: "snowflake.connector.Connection" = None,
 ) -> str:
     """Generate a TABLE from pandas.DataFrame. Returns the sql query to select * from the table"""
     if session is None and connection is None:
-        raise ValueError("Either session or connection must be provided")
+        msg = "Either session or connection must be provided"
+        raise ValueError(msg)
 
     if session is not None:
         session.sql(f"DROP TABLE IF EXISTS {table_name}").collect(statement_params={"SF_PARTNER": "tecton-ai"})
         session.write_pandas(df, table_name, auto_create_table=True, table_type="temporary")
         return f"SELECT * FROM {table_name}"
 
     if connection is not None:
@@ -314,27 +337,24 @@
     timestamp_key: str,
     join_keys: List[str],
     request_context_keys: List[str] = [],
 ):
     from snowflake.snowpark import types
 
     if timestamp_key not in spine_df.columns:
-        raise errors.TectonValidationError(
-            f"Expected to find '{timestamp_key}' among available spine columns: '{', '.join(spine_df.columns)}'."
-        )
+        msg = f"Expected to find '{timestamp_key}' among available spine columns: '{', '.join(spine_df.columns)}'."
+        raise errors.TectonValidationError(msg)
     for field in spine_df.schema.fields:
         if field.name == timestamp_key and not isinstance(field.datatype, types.TimestampType):
-            raise errors.TectonValidationError(
-                f"Invalid type of timestamp_key column in the given spine. Expected Timestamp, got {field.datatype}"
-            )
+            msg = f"Invalid type of timestamp_key column in the given spine. Expected Timestamp, got {field.datatype}"
+            raise errors.TectonValidationError(msg)
     for key in join_keys + request_context_keys:
         if key not in spine_df.columns:
-            raise errors.TectonValidationError(
-                f"Expected to find '{key}' among available spine columns: '{', '.join(spine_df.columns)}'."
-            )
+            msg = f"Expected to find '{key}' among available spine columns: '{', '.join(spine_df.columns)}'."
+            raise errors.TectonValidationError(msg)
 
 
 def get_historical_features_with_snowpark(
     spine: Union[pandas.DataFrame, str, "snowflake.snowpark.DataFrame"],
     session: "snowflake.snowpark.Session",
     feature_set_config: FeatureSetConfig,
     timestamp_key: Optional[str],
@@ -367,17 +387,16 @@
         except Exception as e:
             raise INVALID_SPINE_SQL(e)
         if timestamp_key is None:
             schema = spine_schema
             timestamp_cols = [field.name for field in schema.fields if isinstance(field.datatype, types.TimestampType)]
 
             if len(timestamp_cols) > 1 or len(timestamp_cols) == 0:
-                raise errors.TectonValidationError(
-                    f"Could not infer timestamp keys from {schema}; please specify explicitly"
-                )
+                msg = f"Could not infer timestamp keys from {schema}; please specify explicitly"
+                raise errors.TectonValidationError(msg)
             timestamp_key = timestamp_cols[0]
 
         join_keys = [join_key for fd in feature_set_config.feature_definitions for join_key in fd.join_keys]
         request_context_keys = [key for fd in feature_set_config.feature_definitions for key in fd.request_context_keys]
         validate_spine_dataframe(
             spine_df, timestamp_key, join_keys=join_keys, request_context_keys=request_context_keys
         )
@@ -434,14 +453,15 @@
     # Currently only work with full aggregation.
     spine: Optional[str] = None,
     spine_timestamp_key: Optional[str] = None,
     spine_keys: Optional[List[str]] = None,
     mock_sql_inputs: Optional[Dict[str, str]] = None,
     materialization_context: Optional[BaseMaterializationContext] = None,
     session: "snowflake.snowpark.Session" = None,
+    data_delay: Optional[pendulum.Duration] = None,
 ) -> str:
     # Set a default materilization_context if not provided.
     # This is following the same logic as spark.
     if materialization_context is None:
         materialization_feature_start_time = feature_start_time or pendulum.from_timestamp(0, pendulum.tz.UTC)
         materialization_feature_end_time = feature_end_time or pendulum.datetime(2100, 1, 1)
         if not materialization_feature_start_time < materialization_feature_end_time:
@@ -473,14 +493,15 @@
             session=session,
         )
         materialized_sql = get_materialization_query(
             feature_definition=feature_definition,
             feature_start_time=feature_start_time,
             feature_end_time=feature_end_time,
             source=pipeline_sql,
+            offline_materialization=True,
         )
     else:
         materialized_sql = TIME_LIMIT_TEMPLATE.render(
             source=f"SELECT * FROM {feature_definition.fv_spec.snowflake_view_name}",
             timestamp_key=feature_definition.time_key,
             start_time=feature_start_time,
             end_time=feature_end_time,
@@ -493,14 +514,17 @@
                 aggregation=feature_definition.trailing_time_window_aggregation,
                 timestamp_key=feature_definition.time_key,
                 name=feature_definition.name,
                 spine=spine,
                 spine_timestamp_key=spine_timestamp_key,
                 spine_keys=spine_keys,
                 batch_schedule=int(materialization_context.batch_schedule.total_seconds()),
+                data_delay=int(data_delay.total_seconds()) if data_delay else 0,
+                continuous=feature_definition.is_continuous,
+                offline_materialization=True,
             )
             return format_sql(aggregated_sql_str)
         elif aggregation_level == "partial":
             # Rename the output columns, and add tile start/end time columns
             partial_aggregated_sql_str = PARTIAL_AGGREGATION_TEMPLATE.render(
                 source=materialized_sql,
                 join_keys=feature_definition.join_keys,
@@ -515,15 +539,16 @@
                 source=pipeline_sql,
                 timestamp_key=feature_definition.time_key,
                 start_time=feature_start_time,
                 end_time=feature_end_time,
             )
             return format_sql(sql_str)
         else:
-            raise ValueError(f"Unsupported aggregation level: {aggregation_level}")
+            msg = f"Unsupported aggregation level: {aggregation_level}"
+            raise ValueError(msg)
 
     else:
         return format_sql(materialized_sql)
 
 
 # By default Snowflake unloads numerical types as byte arrays because they have higher precision than available Parquet
 # types. We could decode these in the OnlineStoreCopier, but since we will be downcasting them to the precision of
@@ -539,25 +564,31 @@
 
 def get_materialization_query(
     feature_definition: FeatureDefinition,
     source: str,
     # start is inclusive and end is exclusive
     feature_start_time: Optional[datetime] = None,
     feature_end_time: Optional[datetime] = None,
+    # Whether we are building offline materialization query. If we are building an materialization query, in continuous
+    # mode, we will have to hardcode the schedule interval to 1 day
+    offline_materialization: bool = False,
 ):
     """Returns a SQL query for time-limited materialization.
 
     Does not include a terminating `;` or any COPY or INSERT statements."""
+
     if feature_definition.is_temporal_aggregate:
         source = MATERIALIZATION_TILE_TEMPLATE.render(
             source=source,
             join_keys=feature_definition.join_keys,
             aggregations=_get_feature_view_aggregations(feature_definition),
             slide_interval=feature_definition.aggregate_slide_interval,
             timestamp_key=feature_definition.time_key,
+            continuous=feature_definition.is_continuous,
+            offline_materialization=offline_materialization,
         )
     return format_sql(
         TIME_LIMIT_TEMPLATE.render(
             source=source,
             timestamp_key=feature_definition.time_key,
             start_time=feature_start_time,
             end_time=feature_end_time,
@@ -571,14 +602,22 @@
         destination_stage=f"'{destination_stage}'",
         days=days,
     )
     sql = "\n".join(("EXECUTE IMMEDIATE", "$$", format_sql(script_sql), "$$;"))
     return format_sql(sql)
 
 
+def get_delete_orphaned_schemas_sql(snowflake_database: str):
+    script_sql = DELETE_ORPHANED_SCHEMAS_TEMPLATE.render(
+        snowflake_database=snowflake_database,
+    )
+    sql = "\n".join(("EXECUTE IMMEDIATE", "$$", format_sql(script_sql), "$$;"))
+    return format_sql(sql)
+
+
 def get_materialization_copy_sql(
     feature_definition: FeatureDefinition,
     # start is inclusive and end is exclusive
     time_limits: pendulum.Period,
     destination_stage: Optional[str],
     destination_table: Optional[str],
     # this is materialization task id; it's probably being used incorrectly because
@@ -597,22 +636,24 @@
     source = pipeline_to_sql_string(
         pipeline=feature_definition.pipeline,
         data_sources=feature_definition.data_sources,
         transformations=feature_definition.transformations,
         materialization_context=materialization_context,
         session=session,
     )
+
+    materialize_online = destination_stage is not None
+    materialize_offline = destination_table is not None
     query = get_materialization_query(
         source=source,
         feature_definition=feature_definition,
         feature_start_time=time_limits.start,
         feature_end_time=time_limits.end,
+        offline_materialization=materialize_offline,
     )
-    materialize_online = destination_stage is not None
-    materialize_offline = destination_table is not None
     common_context = dict(
         source=query,
         materialize_online=materialize_online,
         materialize_offline=materialize_offline,
         destination_stage=destination_stage,
         materialization_schema=feature_definition.materialization_schema.to_proto(),
         materialization_id=materialization_id,
@@ -621,14 +662,22 @@
     if materialize_offline:
         view_name = feature_definition.fv_spec.snowflake_view_name
         database, schema, view = view_name.split(".")
         script_sql = MATERIALIZED_FEATURE_VIEW_TEMPLATE.render(
             destination_table=destination_table,
             workspace=f"{database}.{schema}",
             destination_view=view_name,
+            start_time=time_limits.start,
+            end_time=time_limits.end,
+            continuous=feature_definition.is_continuous,
+            timestamp_field=feature_definition.time_key,
+            is_aggregate=feature_definition.is_temporal_aggregate,
+            slide_interval=feature_definition.aggregate_slide_interval
+            if feature_definition.is_temporal_aggregate
+            else duration_pb2.Duration(),
             **common_context,
         )
         sql = "\n".join(("EXECUTE IMMEDIATE", "$$", format_sql(script_sql), "$$;"))
     else:
         sql = ONLINE_STORE_COPIER_TEMPLATE.render(**common_context)
     return format_sql(sql)
 
@@ -645,34 +694,32 @@
 
     if data_source.query:
         source = data_source.query
     else:
         source = f"{data_source.database}.{data_source.schema}.{data_source.table}"
 
     if (start_time is not None or end_time is not None) and data_source.timestamp_field is None:
-        raise ValueError(
-            "Filtering by start_time or end_time requires the timestamp_field parameter to be set on this data source"
-        )
+        msg = "Filtering by start_time or end_time requires the timestamp_field parameter to be set on this data source"
+        raise ValueError(msg)
     sql_str = TIME_LIMIT_TEMPLATE.render(
         source=source,
         timestamp_key=data_source.timestamp_field,
         start_time=start_time,
         end_time=end_time,
     )
     return session.sql(sql_str)
 
 
 def _get_feature_view_aggregations(feature_defintion: FeatureDefinition) -> Dict[str, Set[str]]:
     aggregations = defaultdict(set)
     for feature in feature_defintion.fv_spec.aggregate_features:
         aggregate_function_lambda = AGGREGATION_PLANS[feature.function]
         if not aggregate_function_lambda:
-            raise TectonSnowflakeNotImplementedError(
-                f"Unsupported aggregation function {feature.function} in snowflake pipeline"
-            )
+            msg = f"Unsupported aggregation function {feature.function} in snowflake pipeline"
+            raise TectonSnowflakeNotImplementedError(msg)
         if feature.function == afpb.AGGREGATION_FUNCTION_LAST_NON_DISTINCT_N:
             aggregate_function = aggregate_function_lambda(feature.function_params.last_n.n)
         elif feature.function == afpb.AGGREGATION_FUNCTION_FIRST_NON_DISTINCT_N:
             aggregate_function = aggregate_function_lambda(feature.function_params.first_n.n)
         else:
             aggregate_function = aggregate_function_lambda()
         aggregations[feature.input_feature_name].update(aggregate_function)
```

### Comparing `tecton-0.7.0b9/tecton_snowflake/templates/copier_macro.sql` & `tecton-0.7.0rc0/tecton_snowflake/templates/copier_macro.sql`

 * *Files 14% similar despite different names*

```diff
@@ -13,8 +13,10 @@
             {%- if not loop.last %}, {%- endif %}
         {%- endfor %}
     FROM ({{ source }})
 )
 header = true
 detailed_output = true
 file_format = (type=parquet)
+{#-  Default size is 16777216(16MB), we use 16777216 / 8 here to reduce the file size being copied by lambda. #}
+MAX_FILE_SIZE = 2097152
 {% endmacro %}
```

### Comparing `tecton-0.7.0b9/tecton_snowflake/templates/delete_staged_files.sql` & `tecton-0.7.0rc0/tecton_snowflake/templates/delete_staged_files.sql`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_snowflake/templates/historical_features.sql` & `tecton-0.7.0rc0/tecton_snowflake/templates/historical_features.sql`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_snowflake/templates/historical_features_macros.sql` & `tecton-0.7.0rc0/tecton_snowflake/templates/historical_features_macros.sql`

 * *Files 5% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 {%- set separator = "__"  %}
+{%- set effective_timestamp = "_EFFECTIVE_TIMESTAMP" %}
 
 {%- macro rename_item_columns(join_keys, feature_timestamp_key, spine_timestamp_key, feature_columns, prefix) %}
     {{ feature_timestamp_key }} AS {{ spine_timestamp_key }},
 
     {%- for k, v in join_keys.items() %}
         {{ k }} AS {{ v }},
     {%- endfor %}
@@ -10,14 +11,16 @@
     {%- for col in feature_columns %}
         {{ col }} AS {{ prefix|upper }}{{ separator }}{{ col|upper }}{%- if not loop.last %}, {%- endif %}
     {%- endfor %}
 {%- endmacro %}
 
 {%- macro join(item, spine, spine_keys, spine_timestamp_key, include_feature_view_timestamp_columns) %}
     {%- set name = item.name %}
+    {%- set data_delay = item.data_delay %}
+    {%- set batch_schedule = item.batch_schedule %}
     {%- set feature_timestamp_key = item.timestamp_key %}
     {%- set feature_columns = item.features %}
     {%- set join_keys = item.join_keys %}
     {%- set ttl_seconds = item.ttl_seconds %}
     {%- set prefix = item.namespace %}
     {%- set all_join_keys = (join_keys.values()|list + [spine_timestamp_key])|join(", ") %}
     {%- set join_keys_list = (join_keys.values()|list)|join(", ") %}
@@ -25,19 +28,25 @@
 
     WITH
         {#-  Spine may contain duplicate join keys, we need to filter out the duplications. #}
         FILTERED_SPINE AS (
             SELECT DISTINCT {{ all_join_keys }}
             FROM {{ spine }}
         ),
-        RENAMED AS (
+        WITH_EFFECTIVE_TIMESTAMP AS (
             SELECT
-                {{- rename_item_columns(join_keys, feature_timestamp_key, spine_timestamp_key, feature_columns, prefix) | indent(12)}}
+                *,
+                DATEADD('NANOSECOND', -MOD(DATE_PART(EPOCH_NANOSECOND, {{ feature_timestamp_key }}) - 1, {{ batch_schedule * 1_000_000_000 }}) + {{ batch_schedule * 1_000_000_000 }} + {{ data_delay * 1_000_000_000 }}, DATE_TRUNC('NANOSECOND', {{ feature_timestamp_key }})) AS {{ effective_timestamp }}
             FROM {{ name|upper }}_TABLE
         ),
+        RENAMED AS (
+            SELECT
+                {{- rename_item_columns(join_keys, effective_timestamp, spine_timestamp_key, feature_columns, prefix) | indent(12)}}
+            FROM WITH_EFFECTIVE_TIMESTAMP
+        ),
         {#-  Filter the rows within the given ttl #}
         FILTERED AS (
             {#-  DISTINCT is needed if spine has duplicate join keys #}
             SELECT DISTINCT RENAMED.*
             FROM RENAMED
             INNER JOIN FILTERED_SPINE USING({{ join_keys_list }})
             WHERE RENAMED.{{ spine_timestamp_key }} >= DATEADD('SECOND', -{{ ttl_seconds }}, FILTERED_SPINE.{{ spine_timestamp_key }})
```

### Comparing `tecton-0.7.0b9/tecton_snowflake/templates/materialization_tile.sql` & `tecton-0.7.0rc0/tecton_snowflake/templates/materialization_tile.sql`

 * *Files 11% similar despite different names*

```diff
@@ -1,14 +1,24 @@
 {%- set join_key_list = join_keys|join(", ")  %}
+
+{%- if continuous and not offline_materialization %}
+WITH _SOURCE_WITH_TILE_TIME AS (
+SELECT
+    {{ timestamp_key }} AS _TILE_TIMESTAMP_KEY,
+    *
+FROM ({{ source }})
+)
+{% else %}
 WITH _SOURCE_WITH_TILE_TIME AS (
     SELECT
-        DATEADD('SECOND', -MOD(DATE_PART(EPOCH_SECOND, {{ timestamp_key }}), {{ slide_interval.ToSeconds() }}), DATE_TRUNC('SECOND', {{ timestamp_key }})) AS _TILE_TIMESTAMP_KEY,
+        DATEADD('SECOND', -MOD(DATE_PART(EPOCH_SECOND, {{ timestamp_key }}), {{ 86400 if offline_materialization and continuous else slide_interval.ToSeconds() }}), DATE_TRUNC('SECOND', {{ timestamp_key }})) AS _TILE_TIMESTAMP_KEY,
         *
 	FROM ({{ source }})
 )
+{% endif %}
 SELECT
     {{ join_key_list }},
     {%- for column, functions in aggregations.items() -%}
         {%- for prefix, snowflake_function in functions %}
     	    {%- if prefix == "SUM_OF_SQUARES" %}
                 SUM(SQUARE(CAST({{ column }} AS float))) AS {{ prefix }}_{{ column }},
             {%- elif prefix.startswith("LAST_NON_DISTINCT_N") %}
```

### Comparing `tecton-0.7.0b9/tecton_snowflake/templates/materialized_feature_view.sql` & `tecton-0.7.0rc0/tecton_snowflake/templates/materialized_feature_view.sql`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 {% from 'copier_macro.sql' import copy_into %}
+{% from 'offline_materialization_macros.sql' import delete_materialized_data_within_range %}
 
 {#
   This template uses Snowflake scripting features and needs to be run inside e.g. EXECUTE IMMEDIATE. The EXECUTE
   IMMEDIATE itself is not in the template so we can format the SQL before putting the whole thing into a string.
 
   This script materializes offline. It also optionally copies stuff online (if enabled). It does so by fetching offline rows matching some materialization_id.
 #}
@@ -20,43 +21,45 @@
 );
 CREATE VIEW IF NOT EXISTS {{ destination_view }} AS (SELECT
     {%- for column in materialization_schema.columns %}
     {{ column.name }}{%- if not loop.last %}, {%- endif %}
     {%- endfor %}
 FROM {{ destination_table }});
 
+{# drop "old" data from the offline table within the materialization time window. This makes it so retries overwrite previous jobs. #}
+{{ delete_materialized_data_within_range(destination_table, start_time, end_time, continuous, slide_interval, is_aggregate, timestamp_field) }}
+
 BEGIN TRANSACTION;
 
 {# Insert into the snowflake table #}
 {# Insert into is purely based on the order of the columns, not the name #}
 {# So we have to do a select with the exact same order here. #}
-INSERT INTO {{ destination_table }}(
+LET RES RESULTSET := (INSERT INTO {{ destination_table }}(
     {%- for column in materialization_schema.columns %}
     {{ column.name }},
     {%- endfor %}
     __tecton_internal_materialization_id
 )
 WITH SOURCE AS ( {{  source }} )
 SELECT
     {%- for column in materialization_schema.columns %}
     {{ column.name }},
     {%- endfor %}
     '{{ materialization_id }}' AS __tecton_internal_materialization_id
 FROM SOURCE
-;
+);
 
 {# copy inserted rows into the stage #}
 {%- if materialize_online %}
     {% set copy_source %}
     SELECT *
     FROM {{ destination_table }}
     WHERE __tecton_internal_materialization_id = '{{ materialization_id }}'
     {% endset %}
-    LET RES RESULTSET := (
+    RES := (
         {{  copy_into(destination_stage, copy_source, materialization_schema, cast_types) }}
     );
 {% else %}
-LET RES RESULTSET := (select 0 "total_rows" ) ;
 {% endif -%}
     COMMIT;
     RETURN TABLE(RES);
     END;
```

### Comparing `tecton-0.7.0b9/tecton_snowflake/templates/run_full_aggregation.sql` & `tecton-0.7.0rc0/tecton_snowflake/templates/run_full_aggregation.sql`

 * *Files 2% similar despite different names*

```diff
@@ -54,23 +54,23 @@
     WHERE _SPINE._ANCHOR_TIME >= _PARTIAL_AGGREGATION_TABLE._ANCHOR_TIME
     AND   _SPINE._ANCHOR_TIME <  _PARTIAL_AGGREGATION_TABLE._ANCHOR_TIME + {{ feature.window.ToSeconds() }}
     GROUP BY {{ all_join_keys_list }}
 {%- endmacro %}
 
 WITH _PARTIAL_AGGREGATION_TABLE AS (
     SELECT *,
-    DATE_PART(EPOCH_SECOND, {{ timestamp_key }}) AS _ANCHOR_TIME
+    DATE_PART(EPOCH_SECOND, DATEADD(SECOND, {{ data_delay }}, {{ timestamp_key }})) AS _ANCHOR_TIME
     FROM ({{ source }})
 ),
 _SPINE AS (
     SELECT DISTINCT
         {{ join_keys_list }},
         {{ spine_timestamp_key }},
         {%- if spine is not none %}
-            (DATE_PART(EPOCH_SECOND, {{ spine_timestamp_key }}) - {{ batch_schedule }} - MOD(DATE_PART(EPOCH_SECOND, {{ spine_timestamp_key }}), {{ slide_interval }})) AS _ANCHOR_TIME
+            (DATE_PART(EPOCH_SECOND, {{ spine_timestamp_key }}) - {{ batch_schedule }}) AS _ANCHOR_TIME
             FROM ({{ spine }})
         {%- else %}
             _ANCHOR_TIME
             {# TODO(TEC-8312): Full aggregation won't output all the possible time windows. It will use rows in the data source as the spine. #}
             FROM _PARTIAL_AGGREGATION_TABLE
         {%- endif %}
 ),
```

### Comparing `tecton-0.7.0b9/tecton_snowflake/templates/run_partial_aggregation.sql` & `tecton-0.7.0rc0/tecton_snowflake/templates/run_partial_aggregation.sql`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_snowflake/templates/time_limit.sql` & `tecton-0.7.0rc0/tecton_snowflake/templates/time_limit.sql`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_snowflake/templates_utils.py` & `tecton-0.7.0rc0/tecton_snowflake/templates_utils.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_spark/aggregation_plans.py` & `tecton-0.7.0rc0/tecton_spark/aggregation_plans.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,114 +1,284 @@
 from typing import Callable
 from typing import List
+from typing import Optional
 
 import attr
+import pyspark.sql.functions as F
 from pyspark import SparkContext
 from pyspark.sql import Column
-from pyspark.sql import functions
 from pyspark.sql import WindowSpec
+from pyspark.sql import functions
 from pyspark.sql.functions import expr
 
-from tecton_core.aggregation_utils import get_aggregation_function_name
-from tecton_core.aggregation_utils import sum_of_squares_column_prefix
+from tecton_core.aggregation_utils import get_materialization_aggregation_column_prefixes
 from tecton_proto.common import aggregation_function_pb2 as afpb
 
 
 # WARNING: If you're changing this class there's a good chance you need to change
 # AggregationPlans.kt. Please look over that file carefully.
 
 
 @attr.s(auto_attribs=True)
 class AggregationPlan(object):
     """
     An AggregationPlan contains all the methods required to compute feature values for a specific Tecton aggregation.
 
-    Partial aggregates are returned as a list of pyspark columns. Full aggregates use the partial aggregate columns as inputs and are returned as a single pyspark column.
+    Partial aggregates are returned as a list of pyspark columns. A single column is insufficient since certain
+    aggregations require multiple intermediate columns. For example, the mean aggregation needs sum and count columns.
+
+    Full aggregates use the partial aggregate columns as inputs and are returned as a single pyspark column.
 
     The order of the columns must be the same in:
     * the return list in partial_aggregation_transform
+    * the return list in continuous_partial_aggregation_transform
     * the arguments list in full_aggregation_transform
     * materialized_column_prefixes
 
+    Similarly, if there exists a partial_aggregation_preprocessor, the order of the columns must be the same in:
+    * the output columns of partial_aggregation_preprocessor
+    * the input columns to partial_aggregation_transform
+    * intermediate_column_prefixes
+
     Attributes:
-        partial_aggregation_transform: A method that maps an input column name to a list of output pyspark columns containing the partial aggregates.
+        partial_aggregation_transform: A method that maps a list of input column names to a list of output pyspark columns containing the partial aggregates.
+        continuous_partial_aggregation_transform: Maps a list of input column names to a list of output pyspark columns.
         full_aggregation_transform: A method that maps a list of input partial aggregate columns and a WindowSpec to an output pyspark column containing the full aggregates.
         materialized_column_prefixes: The list of prefixes that should be applied to the pyspark columns produced by `partial_aggregation_transform`.
+            containing the partial aggregates for continuous mode.
+        partial_aggregation_preprocessor: Creates intermediate columns to be used by the partial_aggregation_transform.
+            This should only be necessary when the partial aggregation computation needs to be split across pyspark and
+            a UDAF, as is the case for the approx count distinct aggregation. If it exists, it is applied in both
+            continuous and non-continuous mode.
+        intermediate_column_prefixes: The list of prefixes that should be applied to the pyspark columns produced by `partial_aggregation_preprocessor`.
     """
 
-    partial_aggregation_transform: Callable[[str], List[Column]]
+    partial_aggregation_transform: Callable[[List[str]], List[Column]]
+    continuous_partial_aggregation_transform: Callable[[List[str]], List[Column]]
     full_aggregation_transform: Callable[[List[str], WindowSpec], Column]
     materialized_column_prefixes: List[str]
+    partial_aggregation_preprocessor: Optional[Callable[[str], List[Column]]] = None
+    intermediate_column_prefixes: Optional[List[str]] = None
 
     def materialized_column_names(self, input_column_name: str) -> List[str]:
         return [f"{prefix}_{input_column_name}" for prefix in self.materialized_column_prefixes]
 
+    def intermediate_column_names(self, input_column_name: str) -> List[str]:
+        return [f"{prefix}_{input_column_name}" for prefix in self.intermediate_column_prefixes]
+
 
 def get_aggregation_plan(
     aggregation_function: afpb.AggregationFunction,
     function_params: afpb.AggregationFunctionParams,
     is_continuous: bool,
     time_key: str,
 ) -> AggregationPlan:
     plan = AGGREGATION_PLANS.get(aggregation_function, None)
     if plan is None:
-        raise ValueError(f"Unsupported aggregation function {aggregation_function}")
+        msg = f"Unsupported aggregation function {aggregation_function}"
+        raise ValueError(msg)
 
     if callable(plan):
         return plan(time_key, function_params, is_continuous)
     else:
         return plan
 
 
 def _simple_partial_aggregation_transform(spark_transform):
-    return lambda col: [spark_transform(col)]
+    return lambda cols: [spark_transform(cols[0])]
+
+
+def _simple_continuous_partial_aggregation_transform():
+    return lambda cols: [F.col(cols[0])]
 
 
 def _simple_full_aggregation_transform(spark_transform):
     return lambda cols, window: spark_transform(cols[0]).over(window)
 
 
 def _simple_aggregation_plan(aggregation_function: afpb.AggregationFunction, spark_transform):
     return AggregationPlan(
         partial_aggregation_transform=_simple_partial_aggregation_transform(spark_transform),
+        continuous_partial_aggregation_transform=_simple_continuous_partial_aggregation_transform(),
         full_aggregation_transform=_simple_full_aggregation_transform(spark_transform),
-        materialized_column_prefixes=[get_aggregation_function_name(aggregation_function)],
+        materialized_column_prefixes=get_materialization_aggregation_column_prefixes(aggregation_function),
     )
 
 
+def ApproxPercentilePartialAgg(col: str, precision: int) -> List[Column]:
+    sc = SparkContext._active_spark_context
+    udf_name = f"tecton_approx_percentile_partial_{precision}"
+    sc._jvm.com.tecton.udfs.spark3.ApproxPercentilePartialRegister().register(udf_name, precision)
+    columns = expr(f"{udf_name}({col})")
+    return [columns.processedMeans, columns.processedWeights]
+
+
+def _make_approx_percentile_partial_aggregation(precision: int) -> Callable:
+    def _approx_percentile_partial_aggregation(cols: List[str]) -> List[Column]:
+        return ApproxPercentilePartialAgg(cols[0], precision)
+
+    return _approx_percentile_partial_aggregation
+
+
+def ApproxPercentileFullAgg(
+    processed_means: str,
+    processed_weights: str,
+    percentile: float,
+    precision: int,
+) -> Column:
+    sc = SparkContext._active_spark_context
+    percentile_str = str(percentile).replace(".", "_")
+    udf_name = f"tecton_approx_percentile_full_{percentile_str}"
+    sc._jvm.com.tecton.udfs.spark3.ApproxPercentileFullRegister().register(udf_name, precision, percentile)
+    return expr(f"{udf_name}({processed_means}, {processed_weights})")
+
+
+def _make_approx_percentile_full_aggregation(percentile: float, precision: int) -> Callable:
+    def _approx_percentile_full_aggregation(cols: List[str], window: WindowSpec) -> Column:
+        processed_means, processed_weights = cols
+        return (
+            ApproxPercentileFullAgg(processed_means, processed_weights, percentile, precision).over(window).percentile
+        )
+
+    return _approx_percentile_full_aggregation
+
+
+def _make_approx_percentile_continuous_partial() -> Callable[[List[str]], List[Column]]:
+    def _approx_percentile_continuous_partial(cols: List[str]) -> List[Column]:
+        msg = "approx_percentile does not support continuous mode"
+        raise NotImplementedError(msg)
+
+    return _approx_percentile_continuous_partial
+
+
+def ApproxCountDistinctFullAgg(indices: str, registers: str, precision: int) -> Column:
+    sc = SparkContext._active_spark_context
+    udf_name = "tecton_approx_count_distinct_full"
+    sc._jvm.com.tecton.udfs.spark3.ApproxCountDistinctFullRegister().register(udf_name, precision)
+    return expr(f"{udf_name}({indices}, {registers})")
+
+
+def _make_approx_count_distinct_full_aggregation(precision: int) -> Callable:
+    def _approx_count_distinct_full_aggregation(cols: List[str], window: WindowSpec) -> Column:
+        indices, registers = cols[0], cols[1]
+        return ApproxCountDistinctFullAgg(indices, registers, precision).over(window).count
+
+    return _approx_count_distinct_full_aggregation
+
+
+def ApproxCountDistinctPartialAgg(index: str, register: str) -> List[Column]:
+    sc = SparkContext._active_spark_context
+    udf_name = "tecton_approx_count_distinct_partial"
+    sc._jvm.com.tecton.udfs.spark3.ApproxCountDistinctPartialRegister().register(udf_name)
+    columns = expr(f"{udf_name}({index}, {register})")
+    return [columns.indices, columns.registers]
+
+
+def _make_approx_count_distinct_partial_aggregation() -> Callable:
+    def _approx_count_distinct_partial_aggregation(cols: List[str]) -> List[Column]:
+        index, register = cols[0], cols[1]
+        return ApproxCountDistinctPartialAgg(index, register)
+
+    return _approx_count_distinct_partial_aggregation
+
+
+def _make_approx_count_distinct_partial_aggregation_helper(precision: int) -> Callable:
+    def _add_index_and_register_column(input_column: str) -> List[Column]:
+        """Computes the index and register values and returns them as bigint columns.
+
+        The input column is first hashed with the SHA256 hash function and truncated to 64 bits. Then:
+        * The index is the first 'precision' bits of the hash.
+        * The register value is 1 + (number of leading zeros in the final 64 - 'precision' bits of the hash).
+
+        For example, if the the input column has an entry "foo" and the precision is 8:
+        * The SHA256 hash is '2c26b46b68ffc68ff99b453c1d30413413422d706483bfa0f98a5e886266e7ae'.
+        * The first 64 bits are '2c26b46b68ffc68f', or '0010110000100110101101000110101101101000111111111100011010001111' in binary.
+        * The first 8 bits are '00101100', or 44 in decimal. This is the index.
+        * The last 56 bits are '00100110101101000110101101101000111111111100011010001111'. The register value is 3.
+        """
+        # The input column must be cast since the pyspark sha256 function only operates on binary columns.
+        c = F.col(input_column).cast("binary")
+        c = F.sha2(c, 256)
+
+        # Take first 16 hex characters. The pyspark substring method is 1-indexed.
+        c = F.substring(c, 1, 16)
+
+        # Convert the hex string to binary. This operation is done as if the input column represented an integer, so
+        # the resulting column might not contain 64 bits.
+        c = F.conv(c, 16, 2)
+
+        # Left pad the binary string with 0s to 64 characters, since the column might not have 64 bits.
+        c = F.lpad(c, 64, "0")
+
+        # Take the first 'precision' bits of the hash, convert to decimal, and cast to bigint.
+        index = F.substring(c, 1, precision)
+        index = F.conv(index, 2, 10).cast("bigint")
+
+        # Take the final 64 - 'precision' bits of the hash. Then find the index of the first instance of '1' in the
+        # column. The index starts at 1, so we do not need to modify it.
+        register = F.substring(c, precision + 1, 64)
+        register = F.instr(register, "1").cast("bigint")
+
+        return [index, register]
+
+    def _approx_count_distinct_partial_aggregation_helper(col: str) -> List[Column]:
+        return _add_index_and_register_column(col)
+
+    return _approx_count_distinct_partial_aggregation_helper
+
+
+def _mean_full_aggregation(cols: List[str], window: WindowSpec):
+    # Window aggregation doesn't work with more than one built-in function like this
+    #   sum(mean_clicked * count_clicked) / sum(count_clicked)
+    # And it does not support UDFs on bounded windows (the kind we use)
+    #   https://issues.apache.org/jira/browse/SPARK-22239
+    # We work around this limitations by calculating ratio over two window aggregations
+    mean_col, count_col = cols
+    return functions.sum(functions.col(mean_col) * functions.col(count_col)).over(window) / functions.sum(
+        count_col
+    ).over(window)
+
+
 # Partial aggregator used by first distinct N.
 def FirstNAgg(timestamp: str, col: str, n: int) -> Column:
     sc = SparkContext._active_spark_context
     udf_name = f"tecton_first_distinct_{n}_partial_aggregation"
     sc._jvm.com.tecton.udfs.spark3.FirstNRegister().register(n, udf_name, True)
     return expr(f"{udf_name}({timestamp},{col}).values")
 
 
 def _make_first_distinct_n_partial_aggregation(time_key: str, n: int) -> Callable:
-    def _first_distinct_n_partial_aggregation(col: str) -> List[Column]:
-        return [FirstNAgg(time_key, col, n)]
+    def _first_distinct_n_partial_aggregation(cols: List[str]) -> List[Column]:
+        return [FirstNAgg(time_key, cols[0], n)]
 
     return _first_distinct_n_partial_aggregation
 
 
 # Partial aggregator used by last distinct N.
 def LastDistinctNAgg(col1: str, col2: str, n: int) -> Column:
     sc = SparkContext._active_spark_context
     udf_name = f"tecton_last_distinct_{n}_partial_aggregation"
     sc._jvm.com.tecton.udfs.spark3.LastNRegister().register(n, udf_name, True)
     return expr(f"{udf_name}({col1}, {col2}).values")
 
 
 def _make_last_distinct_n_partial_aggregation(time_key: str, n: int) -> Callable:
-    def _last_distinct_n_partial_aggregation(col: str) -> List[Column]:
-        return [LastDistinctNAgg(time_key, col, n)]
+    def _last_distinct_n_partial_aggregation(cols: List[str]) -> List[Column]:
+        return [LastDistinctNAgg(time_key, cols[0], n)]
 
     return _last_distinct_n_partial_aggregation
 
 
+def _make_first_and_last_n_continuous_partial() -> Callable[[List[str]], List[Column]]:
+    def _first_and_last_n_continuous_partial(cols: List[str]) -> List[Column]:
+        return [F.array(F.col(cols[0]))]
+
+    return _first_and_last_n_continuous_partial
+
+
 # Full aggregator used by both last and first distinct N.
 def LimitedListConcatAgg(col1: str, n: int, keep_last_items: bool) -> Column:
     sc = SparkContext._active_spark_context
     udf_name = (
         f"tecton_last_distinct_{n}_full_aggregation"
         if keep_last_items
         else f"tecton_first_distinct_{n}_full_aggregation"
@@ -141,16 +311,17 @@
         return functions.slice(functions.flatten(functions.collect_list(cols[0]).over(window)), 1, n)
 
     return _first_non_distinct_n_full_aggregation
 
 
 # Partial aggregator used by last non-distinct N.
 def _make_last_non_distinct_n_partial_aggregation(time_key: str, n: int) -> Callable:
-    def last_non_distinct_n_partial_aggregation(col: str) -> List[Column]:
+    def last_non_distinct_n_partial_aggregation(cols: List[str]) -> List[Column]:
         # Sort items in descending order based on timestamp.
+        col = cols[0]
         sort_function = f"(left, right) -> case when left.{time_key} < right.{time_key} then 1 when left.{time_key} > right.{time_key} then -1 else 0 end)"
         return [
             functions.reverse(
                 functions.slice(
                     functions.expr(f"array_sort(collect_list(struct({col}, {time_key})), {sort_function}"),
                     1,
                     n,
@@ -159,16 +330,17 @@
         ]
 
     return last_non_distinct_n_partial_aggregation
 
 
 # Partial aggregator used by first non-distinct N.
 def _make_first_non_distinct_n_partial_aggregation(time_key: str, n: int) -> Callable:
-    def first_non_distinct_n_partial_aggregation(col: str) -> List[Column]:
+    def first_non_distinct_n_partial_aggregation(cols: List[str]) -> List[Column]:
         # Sort items in ascending order based on timestamp.
+        col = cols[0]
         sort_function = f"(left,right) -> case when left.{time_key} < right.{time_key} then -1 when left.{time_key} > right.{time_key} then 1 else 0 end)"
         return [
             functions.slice(
                 functions.expr(f"array_sort(collect_list(struct({col}, {time_key})), {sort_function}"),
                 1,
                 n,
             ).getItem(col)
@@ -220,103 +392,135 @@
     return [
         functions.sum(functions.pow(col, 2)),
         functions.count(col),
         functions.sum(functions.col(col)),
     ]
 
 
-stddev_var_materialized_column_prefixes = [
-    sum_of_squares_column_prefix,
-    get_aggregation_function_name(afpb.AGGREGATION_FUNCTION_COUNT),
-    get_aggregation_function_name(afpb.AGGREGATION_FUNCTION_SUM),
-]
+def _make_stddev_var_continuous_partial() -> Callable[[List[str]], List[Column]]:
+    def _stddev_var_continuous_partial(cols: List[str]) -> List[Column]:
+        col = cols[0]
+        return [
+            F.pow(col, 2),
+            F.lit(1).cast("long"),
+            F.col(col),
+        ]
+
+    return _stddev_var_continuous_partial
+
+
+def _make_approx_count_distinct_continuous_partial() -> Callable[[List[str]], List[Column]]:
+    def _approx_count_distinct_continuous_partial(cols: List[str]) -> List[Column]:
+        index, register = cols[0], cols[1]
+        return [F.array(index), F.array(register)]
+
+    return _approx_count_distinct_continuous_partial
+
+
+_approx_count_distinct_intermediate_column_prefixes = ["approx_count_distinct_index", "approx_count_distinct_register"]
 
 AGGREGATION_PLANS = {
     afpb.AGGREGATION_FUNCTION_SUM: _simple_aggregation_plan(afpb.AGGREGATION_FUNCTION_SUM, functions.sum),
     afpb.AGGREGATION_FUNCTION_MIN: _simple_aggregation_plan(afpb.AGGREGATION_FUNCTION_MIN, functions.min),
     afpb.AGGREGATION_FUNCTION_MAX: _simple_aggregation_plan(afpb.AGGREGATION_FUNCTION_MAX, functions.max),
     afpb.AGGREGATION_FUNCTION_LAST: _simple_aggregation_plan(
         afpb.AGGREGATION_FUNCTION_LAST, lambda col: functions.last(col, ignorenulls=True)
     ),
     # Needs to use COUNT for partial and SUM for full aggregation
     afpb.AGGREGATION_FUNCTION_COUNT: AggregationPlan(
         partial_aggregation_transform=_simple_partial_aggregation_transform(functions.count),
+        continuous_partial_aggregation_transform=lambda _: [F.lit(1).cast("long")],
         full_aggregation_transform=_sum_with_default,
-        materialized_column_prefixes=[get_aggregation_function_name(afpb.AGGREGATION_FUNCTION_COUNT)],
+        materialized_column_prefixes=get_materialization_aggregation_column_prefixes(afpb.AGGREGATION_FUNCTION_COUNT),
+    ),
+    afpb.AGGREGATION_FUNCTION_MEAN: AggregationPlan(
+        partial_aggregation_transform=lambda cols: [functions.mean(cols[0]), functions.count(cols[0])],
+        continuous_partial_aggregation_transform=lambda cols: [F.col(cols[0]).cast("double"), F.lit(1).cast("long")],
+        full_aggregation_transform=_mean_full_aggregation,
+        materialized_column_prefixes=get_materialization_aggregation_column_prefixes(afpb.AGGREGATION_FUNCTION_MEAN),
     ),
     afpb.AGGREGATION_FUNCTION_LAST_DISTINCT_N: lambda time_key, params, is_continuous: AggregationPlan(
         partial_aggregation_transform=_make_last_distinct_n_partial_aggregation(time_key, params.last_n.n),
+        continuous_partial_aggregation_transform=_make_first_and_last_n_continuous_partial(),
         full_aggregation_transform=_make_fixed_size_n_full_aggregation(params.last_n.n, True),
-        materialized_column_prefixes=[
-            get_aggregation_function_name(afpb.AGGREGATION_FUNCTION_LAST_DISTINCT_N)
-            + (str(params.last_n.n) if not is_continuous else "")
-        ],
+        materialized_column_prefixes=get_materialization_aggregation_column_prefixes(
+            afpb.AGGREGATION_FUNCTION_LAST_DISTINCT_N, function_params=params, is_continuous=is_continuous
+        ),
     ),
     afpb.AGGREGATION_FUNCTION_LAST_NON_DISTINCT_N: lambda time_key, params, is_continuous: AggregationPlan(
         partial_aggregation_transform=_make_last_non_distinct_n_partial_aggregation(time_key, params.last_n.n),
+        continuous_partial_aggregation_transform=_make_first_and_last_n_continuous_partial(),
         full_aggregation_transform=_make_last_non_distinct_n_full_aggregation(params.last_n.n),
-        materialized_column_prefixes=[
-            get_aggregation_function_name(afpb.AGGREGATION_FUNCTION_LAST_NON_DISTINCT_N)
-            + (str(params.last_n.n) if not is_continuous else "")
-        ],
+        materialized_column_prefixes=get_materialization_aggregation_column_prefixes(
+            afpb.AGGREGATION_FUNCTION_LAST_NON_DISTINCT_N, function_params=params, is_continuous=is_continuous
+        ),
     ),
     afpb.AGGREGATION_FUNCTION_VAR_POP: AggregationPlan(
-        partial_aggregation_transform=lambda col: _stddev_var_partial(col),
+        partial_aggregation_transform=lambda cols: _stddev_var_partial(cols[0]),
+        continuous_partial_aggregation_transform=_make_stddev_var_continuous_partial(),
         full_aggregation_transform=_var_pop_full_aggregation,
-        materialized_column_prefixes=stddev_var_materialized_column_prefixes,
+        materialized_column_prefixes=get_materialization_aggregation_column_prefixes(afpb.AGGREGATION_FUNCTION_VAR_POP),
     ),
     afpb.AGGREGATION_FUNCTION_STDDEV_POP: AggregationPlan(
-        partial_aggregation_transform=lambda col: _stddev_var_partial(col),
+        partial_aggregation_transform=lambda cols: _stddev_var_partial(cols[0]),
+        continuous_partial_aggregation_transform=_make_stddev_var_continuous_partial(),
         full_aggregation_transform=_stddev_pop_full_aggregation,
-        materialized_column_prefixes=stddev_var_materialized_column_prefixes,
+        materialized_column_prefixes=get_materialization_aggregation_column_prefixes(
+            afpb.AGGREGATION_FUNCTION_STDDEV_POP
+        ),
     ),
     afpb.AGGREGATION_FUNCTION_VAR_SAMP: AggregationPlan(
-        partial_aggregation_transform=lambda col: _stddev_var_partial(col),
+        partial_aggregation_transform=lambda cols: _stddev_var_partial(cols[0]),
+        continuous_partial_aggregation_transform=_make_stddev_var_continuous_partial(),
         full_aggregation_transform=_var_samp_full_aggregation,
-        materialized_column_prefixes=stddev_var_materialized_column_prefixes,
+        materialized_column_prefixes=get_materialization_aggregation_column_prefixes(
+            afpb.AGGREGATION_FUNCTION_VAR_SAMP
+        ),
     ),
     afpb.AGGREGATION_FUNCTION_STDDEV_SAMP: AggregationPlan(
-        partial_aggregation_transform=lambda col: _stddev_var_partial(col),
+        partial_aggregation_transform=lambda cols: _stddev_var_partial(cols[0]),
+        continuous_partial_aggregation_transform=_make_stddev_var_continuous_partial(),
         full_aggregation_transform=_stddev_samp_full_aggregation,
-        materialized_column_prefixes=stddev_var_materialized_column_prefixes,
+        materialized_column_prefixes=get_materialization_aggregation_column_prefixes(
+            afpb.AGGREGATION_FUNCTION_STDDEV_SAMP
+        ),
+    ),
+    # TODO(TEC-14292): Support continuous mode for approx_count_distinct.
+    afpb.AGGREGATION_FUNCTION_APPROX_COUNT_DISTINCT: lambda time_key, params, _: AggregationPlan(
+        partial_aggregation_transform=_make_approx_count_distinct_partial_aggregation(),
+        continuous_partial_aggregation_transform=_make_approx_count_distinct_continuous_partial(),
+        full_aggregation_transform=_make_approx_count_distinct_full_aggregation(params.approx_count_distinct.precision),
+        materialized_column_prefixes=get_materialization_aggregation_column_prefixes(
+            afpb.AGGREGATION_FUNCTION_APPROX_COUNT_DISTINCT
+        ),
+        partial_aggregation_preprocessor=_make_approx_count_distinct_partial_aggregation_helper(
+            params.approx_count_distinct.precision
+        ),
+        intermediate_column_prefixes=_approx_count_distinct_intermediate_column_prefixes,
+    ),
+    afpb.AGGREGATION_FUNCTION_APPROX_PERCENTILE: lambda time_key, params, _: AggregationPlan(
+        partial_aggregation_transform=_make_approx_percentile_partial_aggregation(params.approx_percentile.precision),
+        materialized_column_prefixes=get_materialization_aggregation_column_prefixes(
+            afpb.AGGREGATION_FUNCTION_APPROX_PERCENTILE
+        ),
+        continuous_partial_aggregation_transform=_make_approx_percentile_continuous_partial(),
+        full_aggregation_transform=_make_approx_percentile_full_aggregation(
+            params.approx_percentile.percentile, params.approx_percentile.precision
+        ),
     ),
     afpb.AGGREGATION_FUNCTION_FIRST_NON_DISTINCT_N: lambda time_key, params, is_continuous: AggregationPlan(
         partial_aggregation_transform=_make_first_non_distinct_n_partial_aggregation(time_key, params.first_n.n),
+        continuous_partial_aggregation_transform=_make_first_and_last_n_continuous_partial(),
         full_aggregation_transform=_make_first_non_distinct_n_full_aggregation(params.first_n.n),
-        materialized_column_prefixes=[
-            get_aggregation_function_name(afpb.AGGREGATION_FUNCTION_FIRST_NON_DISTINCT_N)
-            + (str(params.first_n.n) if not is_continuous else "")
-        ],
+        materialized_column_prefixes=get_materialization_aggregation_column_prefixes(
+            afpb.AGGREGATION_FUNCTION_FIRST_NON_DISTINCT_N, function_params=params, is_continuous=is_continuous
+        ),
     ),
     afpb.AGGREGATION_FUNCTION_FIRST_DISTINCT_N: lambda time_key, params, is_continuous: AggregationPlan(
         partial_aggregation_transform=_make_first_distinct_n_partial_aggregation(time_key, params.first_n.n),
+        continuous_partial_aggregation_transform=_make_first_and_last_n_continuous_partial(),
         full_aggregation_transform=_make_fixed_size_n_full_aggregation(params.first_n.n, False),
-        materialized_column_prefixes=[
-            get_aggregation_function_name(afpb.AGGREGATION_FUNCTION_FIRST_DISTINCT_N)
-            + (str(params.first_n.n) if not is_continuous else "")
-        ],
+        materialized_column_prefixes=get_materialization_aggregation_column_prefixes(
+            afpb.AGGREGATION_FUNCTION_FIRST_DISTINCT_N, function_params=params, is_continuous=is_continuous
+        ),
     ),
 }
-
-
-def _mean_full_aggregation(cols: List[str], window: WindowSpec):
-    # Window aggregation doesn't work with more than one built-in function like this
-    #   sum(mean_clicked * count_clicked) / sum(count_clicked)
-    # And it does not support UDFs on bounded windows (the kind we use)
-    #   https://issues.apache.org/jira/browse/SPARK-22239
-    # We work around this limitations by calculating ratio over two window aggregations
-    mean_col, count_col = cols
-    return functions.sum(functions.col(mean_col) * functions.col(count_col)).over(window) / functions.sum(
-        count_col
-    ).over(window)
-
-
-# It is important that `partial_aggregation_transform` or `materialized_column_prefixes`
-# contain aggregation data in the same ordering.
-AGGREGATION_PLANS[afpb.AGGREGATION_FUNCTION_MEAN] = AggregationPlan(
-    partial_aggregation_transform=lambda col: [functions.mean(col), functions.count(col)],
-    full_aggregation_transform=_mean_full_aggregation,
-    materialized_column_prefixes=[
-        get_aggregation_function_name(afpb.AGGREGATION_FUNCTION_MEAN),
-        get_aggregation_function_name(afpb.AGGREGATION_FUNCTION_COUNT),
-    ],
-)
```

### Comparing `tecton-0.7.0b9/tecton_spark/data_observability.py` & `tecton-0.7.0rc0/tecton_spark/data_observability.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,12 +1,14 @@
+from datetime import datetime
 from typing import TYPE_CHECKING
 
 from pyspark.sql import DataFrame
 from pyspark.sql import SparkSession
 
+
 if TYPE_CHECKING:
     from tecton_proto.materialization.params_pb2 import MaterializationTaskParams
 
 
 _active_metrics_collector = None
 
 
@@ -41,22 +43,33 @@
         new_jdf = self._jvm_collector.observe(df._jdf)
         return DataFrame(new_jdf, df.sql_ctx)
 
     def publish(self):
         self._jvm_collector.publish()
 
 
-def create_feature_metrics_collector(spark: SparkSession, params: "MaterializationTaskParams") -> MetricsCollector:
+def create_feature_metrics_collector(
+    spark: SparkSession,
+    params: "MaterializationTaskParams",
+    feature_start_time: datetime = None,
+    feature_end_time: datetime = None,
+) -> MetricsCollector:
     global _active_metrics_collector
 
     if not params.HasField("data_observability_config"):
         return NoopMetricsCollector()
 
     config = params.data_observability_config
     if not config.enabled:
         return NoopMetricsCollector()
 
+    feature_start_time = feature_start_time or params.feature_start_time.ToDatetime()
+    feature_end_time = feature_end_time or params.feature_end_time.ToDatetime()
+
     jvm_collector = spark._jvm.com.tecton.dataobs.spark.MetricsCollector.fromMaterializationTaskParams(
-        params.SerializeToString(), spark._jsparkSession
+        params.SerializeToString(),
+        int(feature_start_time.timestamp()),
+        int(feature_end_time.timestamp()),
+        spark._jsparkSession,
     )
     _active_metrics_collector = SparkMetricsCollector(jvm_collector)
     return _active_metrics_collector
```

### Comparing `tecton-0.7.0b9/tecton_spark/data_source_credentials.py` & `tecton-0.7.0rc0/tecton_spark/data_source_credentials.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_spark/data_source_helper.py` & `tecton-0.7.0rc0/tecton_spark/data_source_helper.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,38 +9,40 @@
 from typing import Any
 from typing import Dict
 from typing import Optional
 from typing import Union
 
 import pendulum
 from pyspark.sql import DataFrame
-from pyspark.sql import functions
 from pyspark.sql import SparkSession
+from pyspark.sql import functions
 from pyspark.sql.functions import coalesce
 from pyspark.sql.functions import to_timestamp
 from pyspark.sql.types import ArrayType
 from pyspark.sql.types import DateType
 from pyspark.sql.types import MapType
 from pyspark.sql.types import StringType
 from pyspark.sql.types import StructField
 from pyspark.sql.types import StructType
 from typeguard import typechecked
 
 import tecton_spark.errors_spark
 from tecton_core import conf
 from tecton_core import specs
+from tecton_core.errors import TectonValidationError
 from tecton_core.filter_context import FilterContext
 from tecton_proto.args.data_source_config_pb2 import INITIAL_STREAM_POSITION_LATEST
 from tecton_proto.args.data_source_config_pb2 import INITIAL_STREAM_POSITION_TRIM_HORIZON
 from tecton_proto.args.data_source_config_pb2 import INITIAL_STREAM_POSITION_UNSPECIFIED
 from tecton_proto.common.data_source_type_pb2 import DataSourceType
 from tecton_proto.data.batch_data_source_pb2 import FileDataSourceFormat
 from tecton_spark.data_source_credentials import get_kafka_secrets
 from tecton_spark.spark_schema_wrapper import SparkSchemaWrapper
 
+
 INITIAL_STREAM_POSITION_STR_TO_ENUM = {
     "latest": INITIAL_STREAM_POSITION_LATEST,
     "trim_horizon": INITIAL_STREAM_POSITION_TRIM_HORIZON,
 }
 
 INITIAL_STREAM_POSITION_ENUM_TO_STR: Dict[str, Optional[str]] = {
     v: k for k, v in INITIAL_STREAM_POSITION_STR_TO_ENUM.items()
@@ -58,26 +60,42 @@
 KAFKA_MAX_OFFSETS_PER_TRIGGER_ENV = "KAFKA_MAX_OFFSETS_PER_TRIGGER"
 # Option to set starting timestamps for each Kafka partitions so that all of the Kafka's retention
 # data is not processed. It's only relevant for the fist streaming job before the checkpoint exists on S3.
 KAFKA_STARTING_OFFSETS_NUM_HOURS_IN_THE_PAST_ENV = "KAFKA_STARTING_OFFSETS_NUM_HOURS_IN_THE_PAST"
 # Only necessary when KAFKA_STARTING_OFFSETS_NUM_HOURS_IN_THE_PAST_ENV is set.
 KAFKA_NUM_PARTITIONS_ENV = "KAFKA_NUM_PARTITIONS"
 
+TEST_ONLY_UNITY_CATALOG_NAME = "TECTON_LOCAL_INTEGRATION_TEST_ONLY_UNITY_CATALOG"
 
-def _is_running_on_emr() -> bool:
-    import os
 
-    return "EMR_RELEASE_LABEL" in os.environ
+def _is_running_on_emr() -> bool:
+    return (
+        "EMR_RELEASE_LABEL" in os.environ
+        or os.environ.get("TECTON_RUNTIME_ENV") == "EMR"
+        or conf.get_or_none("TECTON_RUNTIME_ENV") == "EMR"
+    )
 
 
 def _get_raw_hive_table_dataframe(spark: SparkSession, database: str, table: str) -> DataFrame:
     spark.sql("USE {}".format(database))
     return spark.table(table)
 
 
+def _get_raw_unity_table_dataframe(spark: SparkSession, catalog: str, schema: str, table: str) -> DataFrame:
+    # USE CATALOG is only supported in databricks sql but not local pyspark
+    # so we'd have to tweak for local integration tests to use "USE" sql statement
+    # this means customers won't be able to use a unity catalog named "TECTON_LOCAL_INTEGRATION_TEST_ONLY_UNITY_CATALOG"
+    if catalog == TEST_ONLY_UNITY_CATALOG_NAME:
+        spark.sql(f"USE {catalog}")
+    else:
+        spark.sql(f"USE CATALOG {catalog}")
+    spark.sql(f"USE {schema}")
+    return spark.table(table)
+
+
 @typechecked
 def get_non_dsf_raw_dataframe(
     spark: SparkSession, data_source: specs.BatchSourceSpec, called_for_schema_computation=False
 ) -> DataFrame:
     """Returns a DataFrame of the raw, untranslated data defined by the given BatchDataSource proto.
 
     :param spark: Spark session.
@@ -90,14 +108,16 @@
 
     assert not isinstance(
         data_source, specs.SparkBatchSourceSpec
     ), "get_raw_dataframe can not be used with data source function (spark_batch_config)."
 
     if isinstance(data_source, specs.HiveSourceSpec):
         df = _get_raw_hive_table_dataframe(spark, data_source.database, data_source.table)
+    elif isinstance(data_source, specs.UnitySourceSpec):
+        df = _get_raw_unity_table_dataframe(spark, data_source.catalog, data_source.schema, data_source.table)
     elif isinstance(data_source, specs.RedshiftSourceSpec):
         df = get_redshift_dataframe(
             spark,
             data_source.endpoint,
             data_source.temp_s3,
             data_source.table,
             data_source.query,
@@ -124,27 +144,38 @@
             uri = data_source.schema_uri
 
         if data_source.schema_override:
             schema = SparkSchemaWrapper.from_proto(data_source.schema_override)
             reader = reader.schema(schema.unwrap())
 
         if data_source.file_format == FileDataSourceFormat.FILE_DATA_SOURCE_FORMAT_JSON:
-            action = lambda: reader.json(uri)
+
+            def action():
+                return reader.json(uri)
+
         elif data_source.file_format == FileDataSourceFormat.FILE_DATA_SOURCE_FORMAT_PARQUET:
-            action = lambda: reader.parquet(uri)
+
+            def action():
+                return reader.parquet(uri)
+
         elif data_source.file_format == FileDataSourceFormat.FILE_DATA_SOURCE_FORMAT_CSV:
-            action = lambda: reader.csv(uri, header=True)
+
+            def action():
+                return reader.csv(uri, header=True)
+
         else:
-            raise AssertionError(f"Unsupported file format '{data_source.file_format}'")
+            msg = f"Unsupported file format '{data_source.file_format}'"
+            raise AssertionError(msg)
 
         df = tecton_spark.errors_spark.handleDataAccessErrors(action, data_source.uri)
         if data_source.convert_to_glue_format:
             df = convert_json_like_schema_to_glue_format(spark, df)
     else:
-        raise ValueError(f"Unexpected data source type for source {data_source}")
+        msg = f"Unexpected data source type for source {data_source}"
+        raise ValueError(msg)
 
     return df
 
 
 @typechecked
 def get_table_dataframe(
     spark: SparkSession, data_source: specs.BatchSourceSpec, called_for_schema_computation=False
@@ -185,28 +216,30 @@
     """
 
     if _is_running_on_emr():
         spark_format = "io.github.spark_redshift_community.spark.redshift"
     else:
         spark_format = "com.databricks.spark.redshift"
 
-    params = {"user": conf.get_or_none("REDSHIFT_USER"), "password": conf.get_or_none("REDSHIFT_PASSWORD")}
+    params = {"user": conf.get_or_raise("REDSHIFT_USER"), "password": conf.get_or_raise("REDSHIFT_PASSWORD")}
     full_connection_string = f"jdbc:redshift://{endpoint};user={params['user']};password={params['password']}"
 
     df_reader = (
         spark.read.format(spark_format)
         .option("url", full_connection_string)
         .option("tempdir", temp_s3)
         .option("forward_spark_s3_credentials", "true")
     )
 
     if table and query:
-        raise AssertionError(f"Should only specify one of table and query sources for redshift")
+        msg = "Should only specify one of table and query sources for redshift"
+        raise AssertionError(msg)
     if not table and not query:
-        raise AssertionError(f"Missing both table and query sources for redshift, exactly one must be present")
+        msg = "Missing both table and query sources for redshift, exactly one must be present"
+        raise AssertionError(msg)
 
     if table:
         df_reader = df_reader.option("dbtable", table)
     else:
         df_reader = df_reader.option("query", query)
 
     df = df_reader.load()
@@ -228,24 +261,25 @@
     :param spark: Spark session.
     :param url: The table name in Snowflake
 
     :return: The DataFrame created from the data source.
     """
 
     if table and query:
-        raise AssertionError(f"Should only specify one of table and query sources for Snowflake")
+        msg = "Should only specify one of table and query sources for Snowflake"
+        raise AssertionError(msg)
     if not table and not query:
-        raise AssertionError(f"Missing both table and query sources for Snowflake, exactly one must be present")
+        msg = "Missing both table and query sources for Snowflake, exactly one must be present"
+        raise AssertionError(msg)
 
     user = conf.get_or_none("SNOWFLAKE_USER")
     password = conf.get_or_none("SNOWFLAKE_PASSWORD")
     if not user or not password:
-        raise AssertionError(
-            "Snowflake user and password not configured. Instructions at https://docs.tecton.ai/v2/setting-up-tecton/03g-connecting-snowflake.html"
-        )
+        msg = "Snowflake user and password not configured. Instructions at https://docs.tecton.ai/v2/setting-up-tecton/03g-connecting-snowflake.html"
+        raise AssertionError(msg)
 
     options = {
         "sfUrl": url,
         "sfUser": user,
         "sfPassword": password,
         "sfDatabase": database,
         "sfSchema": schema,
@@ -267,15 +301,16 @@
     return df
 
 
 def apply_timestamp_column(df: DataFrame, ts_column: str, ts_format: Optional[str]) -> DataFrame:
     # Verify the raw source's timestamp column is of type "string"
     column_names = df.schema.names
     if ts_column not in column_names:
-        raise AssertionError(f"Timestamp Column '{ts_column}' not found in schema. Found: {column_names}")
+        msg = f"Timestamp Column '{ts_column}' not found in schema. Found: {column_names}"
+        raise AssertionError(msg)
 
     ts_type = df.schema[ts_column].dataType.jsonValue()
     if ts_type != "timestamp":
         assert (
             ts_type == "string"
         ), f"Timestamp Column '{ts_column}' has type '{ts_type}', expected 'string' or 'timestamp'"
         # Apply timestamp transform
@@ -580,15 +615,16 @@
     elif isinstance(stream_data_source, specs.KafkaSourceSpec):
         df = create_kafka_stream_reader(
             spark,
             stream_data_source,
             option_overrides,
         )
     else:
-        raise ValueError(f"Unknown stream data source type: {stream_data_source}")
+        msg = f"Unknown stream data source type: {stream_data_source}"
+        raise ValueError(msg)
     return df
 
 
 def get_stream_dataframe(
     spark: SparkSession, stream_data_source: specs.StreamSourceSpec, option_overrides: Optional[Dict[str, str]] = None
 ) -> DataFrame:
     """Returns a DataFrame representing a stream data source *without* any options specified.
@@ -656,15 +692,16 @@
     consume_streaming_data_source: bool,
     start_time: Optional[Union[pendulum.DateTime, datetime]] = None,
     end_time: Optional[Union[pendulum.DateTime, datetime]] = None,
     called_for_schema_computation=False,
     stream_option_overrides: Optional[Dict[str, str]] = None,
 ):
     if consume_streaming_data_source and (start_time or end_time):
-        raise AssertionError("Can't specify start or end time when consuming streaming data source")
+        msg = "Can't specify start or end time when consuming streaming data source"
+        raise AssertionError(msg)
 
     if consume_streaming_data_source:
         assert (
             data_source.stream_source
         ), f"Can't consume streaming data source from the data source: {data_source.name}."
 
         if isinstance(data_source.stream_source, specs.SparkStreamSourceSpec):
@@ -707,15 +744,16 @@
 
     def _get_lowercase_schema(datatype):
         if type(datatype) == ArrayType:
             return _get_lowercase_array_schema(datatype)
         elif type(datatype) == StructType:
             return _get_lowercase_structtype_schema(datatype)
         elif type(col.dataType) == MapType:
-            raise TypeError("MapType not supported in JSON schema")
+            msg = "MapType not supported in JSON schema"
+            raise TypeError(msg)
         return datatype
 
     def _get_lowercase_structtype_schema(s) -> StructType:
         assert type(s) == StructType, f"Invalid argument type {type(s)}, expected StructType"
         struct_fields = []
         for col in s:
             datatype = _get_lowercase_schema(col.dataType)
@@ -743,30 +781,31 @@
         elif type(col.dataType) is DateType:
             new_fields.append(functions.col(col.name).cast(StringType()).alias(col.name.lower()))
         else:
             new_fields.append(functions.col(col.name).alias(col.name.lower()))
     return df.select(new_fields)
 
 
-def validate_data_source_supports_time_filtering(batch_source: specs.BatchSourceSpec):
+def validate_data_source_supports_time_filtering(data_source: specs.DataSourceSpec):
     """
     Verifies the batch data_source has the timestamp_column_properties field or supports_time_filtering is True if it is a Data Source Function.
 
-    :param data_source: a specs.BatchSourceSpec containing a batch_data_source
+    :param data_source: a specs.DataSourceSpec containing a batch source.
     """
+    batch_source = data_source.batch_source
+    assert batch_source is not None
+
     if isinstance(batch_source, specs.SparkBatchSourceSpec):
         if not batch_source.supports_time_filtering:
-            raise Exception(
-                f"DataSource {batch_source.name} does not support time filtering. Must set supports_time_filtering to True and implement time filtering within the data source function."
-            )
+            msg = f"DataSource {data_source.name} does not support time filtering. Must set supports_time_filtering to True and implement time filtering within the data source function."
+            raise TectonValidationError(msg)
     else:
         if not batch_source.timestamp_field:
-            raise Exception(
-                f"DataSource {batch_source.name} does not support time filtering. Must specify the data source timestamp field."
-            )
+            msg = f"DataSource {data_source.name} does not support time filtering. Must specify the data source timestamp field."
+            raise TectonValidationError(msg)
 
 
 def get_data_source_function_batch_dataframe(
     spark: SparkSession,
     data_source: specs.SparkBatchSourceSpec,
     start_time: Optional[datetime],
     end_time: Optional[datetime],
```

### Comparing `tecton-0.7.0b9/tecton_spark/feature_view_spark_utils.py` & `tecton-0.7.0rc0/tecton_spark/feature_view_spark_utils.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_spark/ingest_utils.py` & `tecton-0.7.0rc0/tecton/_internals/ingest_utils.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,27 +1,28 @@
 """ This files contains utilities for running feature_table.ingest()."""
 import io
+import logging
 
 import pandas as pd
 import requests
 from pyspark.sql import dataframe as pyspark_dataframe
 from pyspark.sql.types import ArrayType
 from pyspark.sql.types import DoubleType
 from pyspark.sql.types import FloatType
 from pyspark.sql.types import IntegerType
 from pyspark.sql.types import LongType
 from pyspark.sql.types import StructType
 
 from tecton import tecton_context
 from tecton_core import errors
 from tecton_core import schema
-from tecton_core.logger import get_logger
 from tecton_spark import schema_spark_utils
 
-logger = get_logger("FeatureTable")
+
+logger = logging.getLogger(__name__)
 
 
 def upload_df_pandas(upload_url: str, df: pd.DataFrame):
     out_buffer = io.BytesIO()
     df.to_parquet(out_buffer, index=False)
 
     # Maximum 1GB per ingestion
```

### Comparing `tecton-0.7.0b9/tecton_spark/jars/tecton-udfs-spark-3.jar` & `tecton-0.7.0rc0/tecton_spark/jars/tecton-udfs-spark-3.jar`

 * *Files 1% similar despite different names*

#### zipinfo {}

```diff
@@ -1,30 +1,48 @@
-Zip file size: 1567067 bytes, number of entries: 954
+Zip file size: 1604241 bytes, number of entries: 972
 -rw----     2.0 fat        0 bx stor 10-Jan-01 00:00 META-INF/
 -rw----     2.0 fat      111 b- defN 10-Jan-01 00:00 META-INF/MANIFEST.MF
 -rw----     2.0 fat      281 b- defN 10-Jan-01 00:00 build-data.properties
--rw----     2.0 fat       62 bl defN 10-Jan-01 00:00 META-INF/java_com_tecton_udfs_spark3-tecton-udfs-lib.kotlin_module
+-rw----     2.0 fat       92 bl defN 10-Jan-01 00:00 META-INF/java_com_tecton_udfs_spark3-tecton-udfs-lib.kotlin_module
 -rw----     1.0 fat        0 b- stor 10-Jan-01 00:00 com/
 -rw----     1.0 fat        0 b- stor 10-Jan-01 00:00 com/tecton/
 -rw----     1.0 fat        0 b- stor 10-Jan-01 00:00 com/tecton/udfs/
 -rw----     1.0 fat        0 b- stor 10-Jan-01 00:00 com/tecton/udfs/spark3/
 -rw----     2.0 fat     1234 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/AggUtilsKt$concatDistinct$1.class
 -rw----     2.0 fat     1233 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/AggUtilsKt$concatDistinct$2.class
 -rw----     2.0 fat     1973 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/AggUtilsKt$fixedSizeMergeList$$inlined$sortBy$1.class
--rw----     2.0 fat     8383 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/AggUtilsKt.class
+-rw----     2.0 fat     8651 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/AggUtilsKt.class
+-rw----     2.0 fat     5736 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/ApproxCountDistinctFullAgg.class
+-rw----     2.0 fat     2244 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/ApproxCountDistinctFullRegister.class
+-rw----     2.0 fat     4516 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/ApproxCountDistinctPartialAgg.class
+-rw----     2.0 fat     2170 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/ApproxCountDistinctPartialRegister.class
+-rw----     2.0 fat     4274 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/ApproxPercentileFullAgg.class
+-rw----     2.0 fat     2295 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/ApproxPercentileFullRegister.class
+-rw----     2.0 fat     4775 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/ApproxPercentilePartialAgg.class
+-rw----     2.0 fat     2223 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/ApproxPercentilePartialAggKt$process$$inlined$sortedBy$1.class
+-rw----     2.0 fat     6492 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/ApproxPercentilePartialAggKt.class
+-rw----     2.0 fat     2290 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/ApproxPercentilePartialRegister.class
+-rw----     2.0 fat     2393 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/Count.class
 -rw----     2.0 fat     2938 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/FeatureTimestampValidator.class
 -rw----     2.0 fat     5372 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/FirstNAgg.class
 -rw----     2.0 fat     2212 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/FirstNRegister.class
 -rw----     2.0 fat     5394 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/LastNAgg.class
 -rw----     2.0 fat     2207 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/LastNRegister.class
 -rw----     2.0 fat     3994 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/LimitedListConcatAgg.class
 -rw----     2.0 fat     2254 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/LimitedListConcatRegister.class
 -rw----     2.0 fat     3014 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/OrderedOutput.class
 -rw----     2.0 fat     2974 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/Output.class
+-rw----     2.0 fat     2472 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/Percentile.class
+-rw----     2.0 fat     2650 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/PercentileValue.class
 -rw----     2.0 fat     1782 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/RegisterFeatureTimestampValidator.class
+-rw----     2.0 fat     4826 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/SparseArray.class
+-rw----     2.0 fat     3296 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/SparseArrayItem.class
+-rw----     2.0 fat     9773 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/TDigest.class
+-rw----     2.0 fat    10430 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/TDigestFinal.class
+-rw----     2.0 fat     3941 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/TDigestProcessed.class
 -rw----     2.0 fat      968 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/TectonFeatureValidationException.class
 -rw----     2.0 fat     2960 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/Tile.class
 -rw----     2.0 fat     3962 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/TimedTile.class
 -rw----     2.0 fat     3455 bl defN 10-Jan-01 00:00 com/tecton/udfs/spark3/TimedTileValue.class
 drwxr-xr-x  2.0 unx        0 b- defN 10-Jan-01 00:00 kotlin/
 drwxr-xr-x  2.0 unx        0 b- defN 10-Jan-01 00:00 kotlin/collections/
 -rw-r--r--  2.0 unx      596 b- defN 10-Jan-01 00:00 kotlin/collections/ArraysUtilJVM.class
@@ -949,8 +967,8 @@
 -rw-r--r--  2.0 unx      646 b- defN 10-Jan-01 00:00 kotlin/internal/internal.kotlin_builtins
 -rw-r--r--  2.0 unx    16434 b- defN 10-Jan-01 00:00 kotlin/kotlin.kotlin_builtins
 -rw-r--r--  2.0 unx     3305 b- defN 10-Jan-01 00:00 kotlin/ranges/ranges.kotlin_builtins
 -rw-r--r--  2.0 unx     2395 b- defN 10-Jan-01 00:00 kotlin/reflect/reflect.kotlin_builtins
 drwxr-xr-x  2.0 unx        0 b- defN 10-Jan-01 00:00 META-INF/versions/
 drwxr-xr-x  2.0 unx        0 b- defN 10-Jan-01 00:00 META-INF/versions/9/
 -rw-r--r--  2.0 unx     1024 b- defN 10-Jan-01 00:00 META-INF/versions/9/module-info.class
-954 files, 3876356 bytes uncompressed, 1412275 bytes compressed:  63.6%
+972 files, 3953450 bytes uncompressed, 1445943 bytes compressed:  63.4%
```

#### zipnote «TEMP»/diffoscope_0tdyephx_/tmpb98cuiy9_.zip

```diff
@@ -30,14 +30,47 @@
 
 Filename: com/tecton/udfs/spark3/AggUtilsKt$fixedSizeMergeList$$inlined$sortBy$1.class
 Comment: 
 
 Filename: com/tecton/udfs/spark3/AggUtilsKt.class
 Comment: 
 
+Filename: com/tecton/udfs/spark3/ApproxCountDistinctFullAgg.class
+Comment: 
+
+Filename: com/tecton/udfs/spark3/ApproxCountDistinctFullRegister.class
+Comment: 
+
+Filename: com/tecton/udfs/spark3/ApproxCountDistinctPartialAgg.class
+Comment: 
+
+Filename: com/tecton/udfs/spark3/ApproxCountDistinctPartialRegister.class
+Comment: 
+
+Filename: com/tecton/udfs/spark3/ApproxPercentileFullAgg.class
+Comment: 
+
+Filename: com/tecton/udfs/spark3/ApproxPercentileFullRegister.class
+Comment: 
+
+Filename: com/tecton/udfs/spark3/ApproxPercentilePartialAgg.class
+Comment: 
+
+Filename: com/tecton/udfs/spark3/ApproxPercentilePartialAggKt$process$$inlined$sortedBy$1.class
+Comment: 
+
+Filename: com/tecton/udfs/spark3/ApproxPercentilePartialAggKt.class
+Comment: 
+
+Filename: com/tecton/udfs/spark3/ApproxPercentilePartialRegister.class
+Comment: 
+
+Filename: com/tecton/udfs/spark3/Count.class
+Comment: 
+
 Filename: com/tecton/udfs/spark3/FeatureTimestampValidator.class
 Comment: 
 
 Filename: com/tecton/udfs/spark3/FirstNAgg.class
 Comment: 
 
 Filename: com/tecton/udfs/spark3/FirstNRegister.class
@@ -57,17 +90,38 @@
 
 Filename: com/tecton/udfs/spark3/OrderedOutput.class
 Comment: 
 
 Filename: com/tecton/udfs/spark3/Output.class
 Comment: 
 
+Filename: com/tecton/udfs/spark3/Percentile.class
+Comment: 
+
+Filename: com/tecton/udfs/spark3/PercentileValue.class
+Comment: 
+
 Filename: com/tecton/udfs/spark3/RegisterFeatureTimestampValidator.class
 Comment: 
 
+Filename: com/tecton/udfs/spark3/SparseArray.class
+Comment: 
+
+Filename: com/tecton/udfs/spark3/SparseArrayItem.class
+Comment: 
+
+Filename: com/tecton/udfs/spark3/TDigest.class
+Comment: 
+
+Filename: com/tecton/udfs/spark3/TDigestFinal.class
+Comment: 
+
+Filename: com/tecton/udfs/spark3/TDigestProcessed.class
+Comment: 
+
 Filename: com/tecton/udfs/spark3/TectonFeatureValidationException.class
 Comment: 
 
 Filename: com/tecton/udfs/spark3/Tile.class
 Comment: 
 
 Filename: com/tecton/udfs/spark3/TimedTile.class
```

#### META-INF/java_com_tecton_udfs_spark3-tecton-udfs-lib.kotlin_module

```diff
@@ -1,4 +1,6 @@
 00000000: 0000 0003 0000 0001 0000 0007 0000 0001  ................
-00000010: 0000 0000 0a24 0a16 636f 6d2e 7465 6374  .....$..com.tect
+00000010: 0000 0000 0a42 0a16 636f 6d2e 7465 6374  .....B..com.tect
 00000020: 6f6e 2e75 6466 732e 7370 6172 6b33 120a  on.udfs.spark3..
-00000030: 4167 6755 7469 6c73 4b74 2200 2a00       AggUtilsKt".*.
+00000030: 4167 6755 7469 6c73 4b74 121c 4170 7072  AggUtilsKt..Appr
+00000040: 6f78 5065 7263 656e 7469 6c65 5061 7274  oxPercentilePart
+00000050: 6961 6c41 6767 4b74 2200 2a00            ialAggKt".*.
```

#### com/tecton/udfs/spark3/AggUtilsKt.class

##### procyon -ec {}

```diff
@@ -12,15 +12,15 @@
 import java.util.function.Predicate;
 import kotlin.collections.CollectionsKt;
 import kotlin.jvm.internal.Intrinsics;
 import org.jetbrains.annotations.NotNull;
 import java.util.List;
 import kotlin.Metadata;
 
-@Metadata(mv = { 1, 7, 1 }, k = 2, xi = 48, d1 = { "\u0000(\n\u0000\n\u0002\u0010!\n\u0002\u0018\u0002\n\u0002\u0018\u0002\n\u0002\b\u0006\n\u0002\u0010\b\n\u0000\n\u0002\u0010\u000b\n\u0002\b\u000b\n\u0002\u0010\u0002\n\u0002\b\t\u001ap\u0010\u0000\u001a\u0014\u0012\u0010\u0012\u000e\u0012\u0004\u0012\u00020\u0003\u0012\u0004\u0012\u0002H\u00040\u00020\u0001\"\u0004\b\u0000\u0010\u00042\f\u0010\u0005\u001a\b\u0012\u0004\u0012\u00020\u00030\u00012\f\u0010\u0006\u001a\b\u0012\u0004\u0012\u0002H\u00040\u00012\f\u0010\u0007\u001a\b\u0012\u0004\u0012\u00020\u00030\u00012\f\u0010\b\u001a\b\u0012\u0004\u0012\u0002H\u00040\u00012\u0006\u0010\t\u001a\u00020\n2\u0006\u0010\u000b\u001a\u00020\f2\b\b\u0002\u0010\r\u001a\u00020\f\u001ao\u0010\u000e\u001a\u001c\u0012\n\u0012\b\u0012\u0004\u0012\u00020\u00030\u0001\u0012\n\u0012\b\u0012\u0004\u0012\u0002H\u00040\u0001\u0018\u00010\u0002\"\u0004\b\u0000\u0010\u00042\f\u0010\u000f\u001a\b\u0012\u0004\u0012\u00020\u00030\u00012\f\u0010\u0010\u001a\b\u0012\u0004\u0012\u0002H\u00040\u00012\u0006\u0010\u0011\u001a\u00020\u00032\u0006\u0010\u0012\u001a\u0002H\u00042\u0006\u0010\t\u001a\u00020\n2\u0006\u0010\u000b\u001a\u00020\f2\u0006\u0010\u0013\u001a\u00020\f¢\u0006\u0002\u0010\u0014\u001a:\u0010\u0015\u001a\b\u0012\u0004\u0012\u0002H\u00040\u0001\"\u0004\b\u0000\u0010\u0004*\b\u0012\u0004\u0012\u0002H\u00040\u00012\f\u0010\u0016\u001a\b\u0012\u0004\u0012\u0002H\u00040\u00012\u0006\u0010\t\u001a\u00020\n2\u0006\u0010\u0013\u001a\u00020\f\u001a3\u0010\u0017\u001a\u00020\u0018\"\u0004\b\u0000\u0010\u0004*\b\u0012\u0004\u0012\u0002H\u00040\u00012\u0006\u0010\u0019\u001a\u00020\n2\u0006\u0010\u001a\u001a\u00020\n2\u0006\u0010\u001b\u001a\u0002H\u0004¢\u0006\u0002\u0010\u001c\u001a=\u0010\u001d\u001a\u00020\u0018\"\u0004\b\u0000\u0010\u0004*\b\u0012\u0004\u0012\u0002H\u00040\u00012\u0006\u0010\u001e\u001a\u00020\n2\u0006\u0010\u001b\u001a\u0002H\u00042\u0006\u0010\u001f\u001a\u00020\n2\b\b\u0002\u0010\u0013\u001a\u00020\f¢\u0006\u0002\u0010 ¨\u0006!" }, d2 = { "fixedSizeMergeList", "", "Lkotlin/Pair;", "Ljava/sql/Timestamp;", "T", "leftTs", "left", "rightTs", "right", "n", "", "distinct", "", "keepLastItems", "fixedSizeUpdateList", "tsList", "valList", "newTs", "newVal", "keep_last_items", "(Ljava/util/List;Ljava/util/List;Ljava/sql/Timestamp;Ljava/lang/Object;IZZ)Lkotlin/Pair;", "concatDistinct", "other", "fixedSizeBumpItem", "", "oldPos", "newPos", "value", "(Ljava/util/List;IILjava/lang/Object;)V", "fixedSizeInsertItem", "insertionPoint", "maxSize", "(Ljava/util/List;ILjava/lang/Object;IZ)V", "java_com_tecton_udfs_spark3-tecton-udfs-lib" })
+@Metadata(mv = { 1, 7, 1 }, k = 2, xi = 48, d1 = { "\u00000\n\u0000\n\u0002\u0010!\n\u0002\u0018\u0002\n\u0002\u0018\u0002\n\u0002\b\u0006\n\u0002\u0010\b\n\u0000\n\u0002\u0010\u000b\n\u0002\b\t\n\u0002\u0010\u0006\n\u0002\b\u0004\n\u0002\u0010\u0002\n\u0002\b\t\u001ap\u0010\u0000\u001a\u0014\u0012\u0010\u0012\u000e\u0012\u0004\u0012\u00020\u0003\u0012\u0004\u0012\u0002H\u00040\u00020\u0001\"\u0004\b\u0000\u0010\u00042\f\u0010\u0005\u001a\b\u0012\u0004\u0012\u00020\u00030\u00012\f\u0010\u0006\u001a\b\u0012\u0004\u0012\u0002H\u00040\u00012\f\u0010\u0007\u001a\b\u0012\u0004\u0012\u00020\u00030\u00012\f\u0010\b\u001a\b\u0012\u0004\u0012\u0002H\u00040\u00012\u0006\u0010\t\u001a\u00020\n2\u0006\u0010\u000b\u001a\u00020\f2\b\b\u0002\u0010\r\u001a\u00020\f\u001ao\u0010\u000e\u001a\u001c\u0012\n\u0012\b\u0012\u0004\u0012\u00020\u00030\u0001\u0012\n\u0012\b\u0012\u0004\u0012\u0002H\u00040\u0001\u0018\u00010\u0002\"\u0004\b\u0000\u0010\u00042\f\u0010\u000f\u001a\b\u0012\u0004\u0012\u00020\u00030\u00012\f\u0010\u0010\u001a\b\u0012\u0004\u0012\u0002H\u00040\u00012\u0006\u0010\u0011\u001a\u00020\u00032\u0006\u0010\u0012\u001a\u0002H\u00042\u0006\u0010\t\u001a\u00020\n2\u0006\u0010\u000b\u001a\u00020\f2\u0006\u0010\u0013\u001a\u00020\f¢\u0006\u0002\u0010\u0014\u001a\u000e\u0010\u0015\u001a\u00020\u00162\u0006\u0010\u0017\u001a\u00020\n\u001a:\u0010\u0018\u001a\b\u0012\u0004\u0012\u0002H\u00040\u0001\"\u0004\b\u0000\u0010\u0004*\b\u0012\u0004\u0012\u0002H\u00040\u00012\f\u0010\u0019\u001a\b\u0012\u0004\u0012\u0002H\u00040\u00012\u0006\u0010\t\u001a\u00020\n2\u0006\u0010\u0013\u001a\u00020\f\u001a3\u0010\u001a\u001a\u00020\u001b\"\u0004\b\u0000\u0010\u0004*\b\u0012\u0004\u0012\u0002H\u00040\u00012\u0006\u0010\u001c\u001a\u00020\n2\u0006\u0010\u001d\u001a\u00020\n2\u0006\u0010\u001e\u001a\u0002H\u0004¢\u0006\u0002\u0010\u001f\u001a=\u0010 \u001a\u00020\u001b\"\u0004\b\u0000\u0010\u0004*\b\u0012\u0004\u0012\u0002H\u00040\u00012\u0006\u0010!\u001a\u00020\n2\u0006\u0010\u001e\u001a\u0002H\u00042\u0006\u0010\"\u001a\u00020\n2\b\b\u0002\u0010\u0013\u001a\u00020\f¢\u0006\u0002\u0010#¨\u0006$" }, d2 = { "fixedSizeMergeList", "", "Lkotlin/Pair;", "Ljava/sql/Timestamp;", "T", "leftTs", "left", "rightTs", "right", "n", "", "distinct", "", "keepLastItems", "fixedSizeUpdateList", "tsList", "valList", "newTs", "newVal", "keep_last_items", "(Ljava/util/List;Ljava/util/List;Ljava/sql/Timestamp;Ljava/lang/Object;IZZ)Lkotlin/Pair;", "hllAlpha", "", "numRegisters", "concatDistinct", "other", "fixedSizeBumpItem", "", "oldPos", "newPos", "value", "(Ljava/util/List;IILjava/lang/Object;)V", "fixedSizeInsertItem", "insertionPoint", "maxSize", "(Ljava/util/List;ILjava/lang/Object;IZ)V", "java_com_tecton_udfs_spark3-tecton-udfs-lib" })
 public final class AggUtilsKt
 {
     public static final <T> void fixedSizeInsertItem(@NotNull final List<T> $this$fixedSizeInsertItem, final int insertionPoint, final T value, final int maxSize, final boolean keep_last_items) {
         Intrinsics.checkNotNullParameter((Object)$this$fixedSizeInsertItem, "<this>");
         if (keep_last_items && $this$fixedSizeInsertItem.size() == maxSize) {
             CollectionsKt.removeFirst((List)$this$fixedSizeInsertItem);
             $this$fixedSizeInsertItem.add(insertionPoint - 1, value);
@@ -165,8 +165,25 @@
             }
             return (Pair<List<Timestamp>, List<T>>)pair;
         }
         fixedSizeInsertItem(tsList, insertionPoint, newTs, n, keep_last_items);
         fixedSizeInsertItem(valList, insertionPoint, newVal, n, keep_last_items);
         return (Pair<List<Timestamp>, List<T>>)new Pair((Object)tsList, (Object)valList);
     }
+    
+    public static final double hllAlpha(final int numRegisters) {
+        switch (numRegisters) {
+            case 16: {
+                return 0.673;
+            }
+            case 32: {
+                return 0.697;
+            }
+            case 64: {
+                return 0.709;
+            }
+            default: {
+                return 0.7213 / (1 + 1.079 / numRegisters);
+            }
+        }
+    }
 }
```

### Comparing `tecton-0.7.0b9/tecton_spark/materialization_plan.py` & `tecton-0.7.0rc0/tecton_spark/materialization_plan.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,27 +1,27 @@
+import logging
 from typing import Optional
 
 import attr
 import pendulum
 from pyspark.sql import DataFrame
-from pyspark.sql import functions
 from pyspark.sql import SparkSession
+from pyspark.sql import functions
 
 from tecton_core.feature_definition_wrapper import FeatureDefinitionWrapper as FeatureDefinition
-from tecton_core.logger import get_logger
 from tecton_core.query.builder import build_materialization_querytree
 from tecton_core.query.node_interface import NodeRef
-from tecton_core.query.rewrite import rewrite_tree
 from tecton_core.query_consts import ANCHOR_TIME
 from tecton_core.time_utils import convert_timedelta_for_version
 from tecton_spark.data_observability import MetricsCollector
 from tecton_spark.query import translate
 
+
 MATERIALIZED_RAW_DATA_END_TIME = "_materialized_raw_data_end_time"
-logger = get_logger("MaterializationPlan")
+logger = logging.getLogger(__name__)
 
 
 @attr.s(auto_attribs=True)
 class MaterializationPlan:
     """Computes dataframes required for materialization to offline and online stores.
 
     This class contains `offline_store_data_frame` and `online_store_data_frame`, but both might not always be necessary.
@@ -78,22 +78,20 @@
     """
     query_tree = build_materialization_querytree(
         feature_definition,
         for_stream=False,
         feature_data_time_limits=feature_data_time_limits,
         enable_feature_metrics=(metrics_collector is not None),
     )
-    rewrite_tree(query_tree)
     return MaterializationPlan.from_querytree(fd=feature_definition, query_tree=query_tree, spark=spark)
 
 
 def get_stream_materialization_plan(
     *,
     spark: SparkSession,
     feature_definition: FeatureDefinition,
     metrics_collector: Optional[MetricsCollector] = None,
 ) -> MaterializationPlan:
     query_tree = build_materialization_querytree(
         feature_definition, for_stream=True, enable_feature_metrics=(metrics_collector is not None)
     )
-    rewrite_tree(query_tree)
     return MaterializationPlan.from_querytree(fd=feature_definition, query_tree=query_tree, spark=spark)
```

### Comparing `tecton-0.7.0b9/tecton_spark/offline_store.py` & `tecton-0.7.0rc0/tecton_spark/offline_store.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 import functools
 import itertools
+import logging
 import os
 import random
 import time
 from abc import ABC
 from abc import abstractmethod
 from dataclasses import dataclass
 from datetime import datetime
@@ -11,44 +12,46 @@
 from typing import List
 from typing import Optional
 
 import pendulum
 from py4j.protocol import Py4JJavaError
 from pyspark.sql import Column
 from pyspark.sql import DataFrame
-from pyspark.sql import functions
 from pyspark.sql import SparkSession
+from pyspark.sql import functions
 from pyspark.sql.types import IntegerType
 from pyspark.sql.types import LongType
 from pyspark.sql.types import StructType
 from pyspark.sql.types import TimestampType
 
 from tecton_core import time_utils as core_time_utils
 from tecton_core.feature_definition_wrapper import FeatureDefinitionWrapper as FeatureDefinition
-from tecton_core.logger import get_logger
+from tecton_core.offline_store import TIME_PARTITION
 from tecton_core.offline_store import _check_supported_offline_store_version
 from tecton_core.offline_store import _timestamp_formats
 from tecton_core.offline_store import partition_col_for_parquet
 from tecton_core.offline_store import partition_size_for_delta
 from tecton_core.offline_store import partition_size_for_parquet
-from tecton_core.offline_store import TIME_PARTITION
 from tecton_core.offline_store import window_size_seconds
 from tecton_core.query_consts import ANCHOR_TIME
 from tecton_spark import time_utils as spark_time_utils
 
+
 DBRICKS_MULTI_CLUSTER_WRITES_ENABLED = "spark.databricks.delta.multiClusterWrites.enabled"
 DBRICKS_RUNTIME_VERSION = "DATABRICKS_RUNTIME_VERSION"
 
+SPARK_GCS_DELTA_LOGSTORE_CLASS = "io.delta.storage.GCSLogStore"
+
 SPARK31_DELTA_LOGSTORE_CLASS = "spark.delta.logStore.class"
 SPARK31_DYNAMODB_LOGSTORE_CLASS = "io.delta.storage.DynamoDBLogStore"
 
 SPARK32_OR_HIGHER_DELTA_LOGSTORE_CLASS = "spark.delta.logStore.s3.impl"
 SPARK32_OR_HIGHER_DYNAMODB_LOGSTORE_CLASS = "io.delta.storage.S3DynamoDBLogStore"
 
-logger = get_logger("offline_store")
+logger = logging.getLogger(__name__)
 
 
 @dataclass
 class OfflineStoreWriterParams:
     s3_path: str
 
     always_store_anchor_column: bool
@@ -78,23 +81,25 @@
     elif case == "parquet":
         return ParquetWriter(fd, params, spark, version)
     # Remove default after database migration is complete.
     # raise KeyError(case)
     return ParquetWriter(fd, params, spark, version)
 
 
-def get_offline_store_reader(spark: SparkSession, fd: FeatureDefinition) -> "OfflineStoreReader":
+def get_offline_store_reader(
+    spark: SparkSession, fd: FeatureDefinition, path: Optional[str] = None
+) -> "OfflineStoreReader":
     case = fd.offline_store_config.WhichOneof("store_type")
     if case == "delta":
-        return DeltaReader(spark, fd)
+        return DeltaReader(spark, fd, path=path)
     elif case == "parquet":
-        return ParquetReader(spark, fd)
+        return ParquetReader(spark, fd, path=path)
     # Remove default after database migration is complete.
     # raise KeyError(case)
-    return ParquetReader(spark, fd)
+    return ParquetReader(spark, fd, path=path)
 
 
 class OfflineStoreWriter(ABC):
     """Interface for Offline Feature Store writers."""
 
     @abstractmethod
     def append_dataframe(self, data_frame):
@@ -115,15 +120,16 @@
 
         Return number of successfully deleted keys."""
         raise NotImplementedError
 
 
 class OfflineStoreReader(ABC):
     @abstractmethod
-    def read(self, time_limits: pendulum.Period) -> DataFrame:
+    def read(self, partition_time_limits: pendulum.Period) -> DataFrame:
+        """Note that partition_time_limits only applies partition filtering, so you can have records outside it"""
         raise NotImplementedError
 
 
 class ParquetWriter(OfflineStoreWriter):
     """Parquet implementation of OfflineStoreWriter"""
 
     def __init__(self, fd: FeatureDefinition, params: OfflineStoreWriterParams, spark: SparkSession, version: int):
@@ -132,15 +138,15 @@
         self._params = params
         self._spark = spark
         self._version = version
         self._partition_size = partition_size_for_parquet(fd).as_timedelta()
         self._partition_col = partition_col_for_parquet(fd)
 
     def append_dataframe(self, data_frame):
-        if self._params.is_continuous or self._fd.offline_store_config is not None:
+        if self._partition_col == TIME_PARTITION:
             align_duration = core_time_utils.convert_timedelta_for_version(self._partition_size, self._version)
             aligned_time = _align_timestamp(functions.col(ANCHOR_TIME), functions.lit(align_duration))
             data_frame = data_frame.withColumn(TIME_PARTITION, aligned_time)
 
         data_frame.write.option("partitionOverwriteMode", "dynamic").partitionBy(self._partition_col).parquet(
             self._params.s3_path, mode="overwrite"
         )
@@ -149,35 +155,35 @@
         raise NotImplementedError()
 
     def delete_keys(self, data_frame):
         raise NotImplementedError()
 
 
 class ParquetReader(OfflineStoreReader):
-    def __init__(self, spark: SparkSession, fd: FeatureDefinition):
+    def __init__(self, spark: SparkSession, fd: FeatureDefinition, path: Optional[str]):
         _check_supported_offline_store_version(fd)
         self._spark = spark
         assert fd.materialization_enabled and fd.writes_to_offline_store
-        self._path = fd.materialized_data_path
+        self._path = path or fd.materialized_data_path
         self._partition_col = partition_col_for_parquet(fd)
         self._partition_size = partition_size_for_parquet(fd).as_timedelta()
         self.version = fd.get_feature_store_format_version
 
-    def read(self, time_limits: Optional[pendulum.Period]):
+    def read(self, partition_time_limits: Optional[pendulum.Period]):
         spark_df = self._spark.read.parquet(self._path)
 
         # Parquet is partitioned by TIME_PARTITION when is_continuous and ANCHOR_TIME when not.
         # We want to explicitly cast the partition type in case:
         #   `spark.sql.sources.partitionColumnTypeInference.enabled` = "false"
 
         spark_df = spark_df.withColumn(self._partition_col, functions.col(self._partition_col).cast("long"))
 
-        if time_limits and self._partition_size:
-            aligned_start_time = core_time_utils.align_time_downwards(time_limits.start, self._partition_size)
-            aligned_end_time = core_time_utils.align_time_downwards(time_limits.end, self._partition_size)
+        if partition_time_limits and self._partition_size:
+            aligned_start_time = core_time_utils.align_time_downwards(partition_time_limits.start, self._partition_size)
+            aligned_end_time = core_time_utils.align_time_downwards(partition_time_limits.end, self._partition_size)
             start_time_epoch = core_time_utils.convert_timestamp_for_version(aligned_start_time, self.version)
             end_time_epoch = core_time_utils.convert_timestamp_for_version(aligned_end_time, self.version)
             partition_col = functions.col(self._partition_col)
             spark_df = spark_df.where((start_time_epoch <= partition_col) & (partition_col <= end_time_epoch))
 
         return spark_df.drop(TIME_PARTITION)
 
@@ -200,23 +206,21 @@
 
 
 def _with_delta_retries(f, max_retries=5):
     """Retries the wrapped function upon Deltalake conflict errors."""
 
     @functools.wraps(f)
     def wrapper(*args, **kwargs):
-        from delta.exceptions import (
-            ConcurrentAppendException,
-            ConcurrentDeleteDeleteException,
-            ConcurrentDeleteReadException,
-            ConcurrentTransactionException,
-            DeltaConcurrentModificationException,
-            MetadataChangedException,
-            ProtocolChangedException,
-        )
+        from delta.exceptions import ConcurrentAppendException
+        from delta.exceptions import ConcurrentDeleteDeleteException
+        from delta.exceptions import ConcurrentDeleteReadException
+        from delta.exceptions import ConcurrentTransactionException
+        from delta.exceptions import DeltaConcurrentModificationException
+        from delta.exceptions import MetadataChangedException
+        from delta.exceptions import ProtocolChangedException
 
         final_exception = None
         for i in range(max_retries):
             try:
                 if i > 0:
                     # Add a random delay (with exponential backoff) before the retries to decrease
                     # the chance of recurrent conflicts between the parallel offline store writers.
@@ -248,56 +252,63 @@
                 logger.info(
                     f"Delta transaction failed (attempt {i + 1}/5); retrying",
                     exc_info=True,  # Include information about the exception currently being handled
                 )
             except Exception:
                 logger.warning("Uncaught exception raised during Delta write", exc_info=True)
                 raise
-        raise Exception(f"Exceeded maximum Delta transaction retries ({max_retries})") from final_exception
+        msg = f"Exceeded maximum Delta transaction retries ({max_retries})"
+        raise Exception(msg) from final_exception
 
     return wrapper
 
 
 def _assert_safe_delta_write_configuration(spark: SparkSession):
     """Asserts that the Spark configuration is such that it is safe to write to Delta concurrently.
 
     With the Open Source Delta JAR installed (as it is on EMR), writing to a Delta table concurrently with another
     Spark cluster could corrupt the table unless the Delta Logstore class is overridden.
 
     On Databricks everything is fine as multi-cluster writes are enabled (the default).
     """
+
     configs = {
         DBRICKS_RUNTIME_VERSION: os.environ.get(DBRICKS_RUNTIME_VERSION, None),
         DBRICKS_MULTI_CLUSTER_WRITES_ENABLED: spark.conf.get(DBRICKS_MULTI_CLUSTER_WRITES_ENABLED, None),
         SPARK31_DELTA_LOGSTORE_CLASS: spark.conf.get(SPARK31_DELTA_LOGSTORE_CLASS, None),
         SPARK32_OR_HIGHER_DELTA_LOGSTORE_CLASS: spark.conf.get(SPARK32_OR_HIGHER_DELTA_LOGSTORE_CLASS, None),
     }
     if configs[DBRICKS_RUNTIME_VERSION] and configs[DBRICKS_MULTI_CLUSTER_WRITES_ENABLED] == "true":
         return True
 
     # either the spark 3.1 or spark 3.2+ DELTA_LOGSTORE_CLASS name can be set
-    if configs[SPARK31_DELTA_LOGSTORE_CLASS] == SPARK31_DYNAMODB_LOGSTORE_CLASS:
+    if configs[SPARK31_DELTA_LOGSTORE_CLASS] in [SPARK31_DYNAMODB_LOGSTORE_CLASS, SPARK_GCS_DELTA_LOGSTORE_CLASS]:
         return True
-    if configs[SPARK32_OR_HIGHER_DELTA_LOGSTORE_CLASS] == SPARK32_OR_HIGHER_DYNAMODB_LOGSTORE_CLASS:
+    if configs[SPARK32_OR_HIGHER_DELTA_LOGSTORE_CLASS] in [
+        SPARK32_OR_HIGHER_DYNAMODB_LOGSTORE_CLASS,
+        SPARK_GCS_DELTA_LOGSTORE_CLASS,
+    ]:
         return True
-    raise AssertionError(f"Configuration is not safe for concurrent writes: {configs}")
+    msg = f"Configuration is not safe for concurrent writes: {configs}"
+    raise AssertionError(msg)
 
 
 class DeltaWriter(OfflineStoreWriter):
     """DeltaLake implementation of OfflineStoreWriter"""
 
     def __init__(self, fd: FeatureDefinition, params: OfflineStoreWriterParams, spark: SparkSession, version: int):
         _check_supported_offline_store_version(fd)
         self._params = params
         self._version = version
         self._spark = spark
         self._partition_size = partition_size_for_delta(fd).as_timedelta()
         self._metadata_writer = DeltaMetadataWriter(spark)
         if not spark.conf.get("spark.databricks.delta.commitInfo.userMetadata"):
-            raise AssertionError(f"Expected spark.databricks.delta.commitInfo.userMetadata to be set for delta writes")
+            msg = "Expected spark.databricks.delta.commitInfo.userMetadata to be set for delta writes"
+            raise AssertionError(msg)
 
     def append_dataframe(self, data_frame: DataFrame):
         data_frame = self._add_partition(data_frame)
         self._ensure_table_exists(self._spark, data_frame.schema)
         self._append_dataframe(data_frame)
 
     def upsert_dataframe(self, data_frame):
@@ -398,18 +409,21 @@
         """Ensures that the table exists with the given schema.
 
         Some operations (including merge) fail when the table doesn't already exist. Others (append) can have conflicts
         where they wouldn't normally when they also create a new table. This function ensures neither will happen.
         """
         df = spark.createDataFrame([], schema)  # DF with 0 rows
         self._append_dataframe(df)
-        # we set auto manifest so each job generates its own manifest (necessary for athena retrieval)
-        self._metadata_writer.set_table_property(
-            self._params.s3_path, "delta.compatibility.symlinkFormatManifest.enabled", "true"
-        )
+
+        # Manifest files are not supported on GCS
+        if not self._params.s3_path.startswith("gs://"):
+            # we set auto manifest so each job generates its own manifest (necessary for athena retrieval)
+            self._metadata_writer.set_table_property(
+                self._params.s3_path, "delta.compatibility.symlinkFormatManifest.enabled", "true"
+            )
 
     @_with_delta_retries
     def _append_dataframe(self, df: DataFrame):
         _assert_safe_delta_write_configuration(self._spark)
         df.write.partitionBy(TIME_PARTITION).format("delta").mode("append").save(self._params.s3_path)
 
 
@@ -432,53 +446,55 @@
         has_tbl_property = any(key == r.key and val == r.value for r in existing_tbl_properties)
         # we only set it if not already set to avoid delta conflicts
         if not has_tbl_property:
             self._spark.sql(f"ALTER TABLE spark_catalog.delta.`{path}` SET TBLPROPERTIES({key}={val})")
 
 
 class DeltaReader(OfflineStoreReader):
-    def __init__(self, spark: SparkSession, fd: FeatureDefinition):
+    def __init__(self, spark: SparkSession, fd: FeatureDefinition, path: Optional[str]):
         _check_supported_offline_store_version(fd)
         self._spark = spark
         assert fd.materialization_enabled and fd.writes_to_offline_store
-        self._path = fd.materialized_data_path
+        self._path = path or fd.materialized_data_path
         self._partition_size = partition_size_for_delta(fd).as_timedelta()
 
-    def read(self, time_limits: Optional[pendulum.Period]):
+    def read(self, partition_time_limits: Optional[pendulum.Period]):
         spark_df = self._spark.read.format("delta").load(self._path)
 
         # Whenever the partition filtering logic is changed, also make sure the changes are applied to the sql based
         # version in query/nodes.py
 
         # Delta is always partitioned by TIME_PARTITION. We want to explicitly cast to a timestamp in case:
         #   `spark.sql.sources.partitionColumnTypeInference.enabled` = "false"
         spark_df = spark_df.withColumn(TIME_PARTITION, functions.col(TIME_PARTITION).cast("timestamp"))
 
-        if time_limits is not None and self._partition_size:
-            aligned_start_time = core_time_utils.align_time_downwards(time_limits.start, self._partition_size)
-            aligned_end_time = core_time_utils.align_time_downwards(time_limits.end, self._partition_size)
+        if partition_time_limits is not None and self._partition_size:
+            aligned_start_time = core_time_utils.align_time_downwards(partition_time_limits.start, self._partition_size)
+            aligned_end_time = core_time_utils.align_time_downwards(partition_time_limits.end, self._partition_size)
             start_partition = _datetime_to_partition_str(aligned_start_time, self._partition_size)
             end_partition = _datetime_to_partition_str(aligned_end_time, self._partition_size)
             partition_col = functions.col(TIME_PARTITION)
             spark_df = spark_df.where((start_partition <= partition_col) & (partition_col <= end_partition))
 
         return spark_df.drop(TIME_PARTITION)
 
 
 def _timestamp_to_partition_column(df: DataFrame, time_col: str, partition_size: timedelta, version: int) -> Column:
     # For some insane reason from_unixtime returns a timestamp in the session timezone, so it's pretty annoying to
     # convert a unix time to a formatted UTC timestamp unless the session is set to UTC. This only runs in
     # materialization so we can just assert that that's the case.
     tz = df.sql_ctx.sparkSession.conf.get("spark.sql.session.timeZone")
     if tz not in {"UTC", "Etc/UTC", "GMT"}:
-        raise AssertionError(f"spark.sql.session.timeZone must be UTC, not {tz}")
+        msg = f"spark.sql.session.timeZone must be UTC, not {tz}"
+        raise AssertionError(msg)
     time_column_type = df.schema[time_col].dataType
     allowed_types = {IntegerType(), TimestampType(), LongType()}
     if time_column_type not in allowed_types:
-        raise AssertionError(f"timestamp column must be one of {allowed_types}, not {time_column_type}")
+        msg = f"timestamp column must be one of {allowed_types}, not {time_column_type}"
+        raise AssertionError(msg)
     time_val = functions.col(time_col).cast(LongType())
     if time_col == ANCHOR_TIME:
         time_val = spark_time_utils.convert_epoch_to_datetime(functions.col(time_col), version).cast(LongType())
     aligned = functions.from_unixtime(_align_timestamp(time_val, window_size_seconds(partition_size)))
     partition_format = _timestamp_formats(partition_size).spark_format
     return functions.date_format(aligned.cast(TimestampType()), partition_format)
```

### Comparing `tecton-0.7.0b9/tecton_spark/partial_aggregations.py` & `tecton-0.7.0rc0/tecton_spark/partial_aggregations.py`

 * *Files 20% similar despite different names*

```diff
@@ -2,63 +2,46 @@
 from typing import Dict
 from typing import List
 from typing import Optional
 
 import pyspark
 import pyspark.sql.functions as F
 
-from tecton_core.aggregation_utils import get_aggregation_column_prefix_from_column_name
-from tecton_core.aggregation_utils import get_aggregation_column_prefixes
-from tecton_core.aggregation_utils import get_aggregation_function_name
-from tecton_core.aggregation_utils import sum_of_squares_column_prefix
-from tecton_core.feature_view_utils import resolve_function_name
+from tecton_core.aggregation_utils import get_materialization_aggregation_column_prefixes
+from tecton_core.aggregation_utils import get_pretty_column_prefix
 from tecton_core.query_consts import ANCHOR_TIME
-from tecton_proto.common import aggregation_function_pb2 as afpb
 from tecton_proto.data.feature_view_pb2 import TrailingTimeWindowAggregation
 from tecton_spark.aggregation_plans import AggregationPlan
 from tecton_spark.aggregation_plans import get_aggregation_plan
 from tecton_spark.time_utils import convert_timestamp_to_epoch
 from tecton_spark.time_utils import get_timestamp_in_seconds
 
+
 WINDOW_COLUMN_NAME = "window"
 
 
 def _get_feature_partial_aggregations(aggregation_plan: AggregationPlan, feature_name: str):
     column_names = set()
+    input_columns = (
+        aggregation_plan.intermediate_column_names(feature_name)
+        if aggregation_plan.intermediate_column_prefixes is not None
+        else [feature_name]
+    )
+
     for column_name, aggregated_column in zip(
         aggregation_plan.materialized_column_names(feature_name),
-        aggregation_plan.partial_aggregation_transform(feature_name),
+        aggregation_plan.partial_aggregation_transform(input_columns),
     ):
         if column_name in column_names:
             continue
         column_names.add(column_name)
 
         yield column_name, aggregated_column.alias(column_name)
 
 
-def _get_continuous_aggregation_value(column_prefix, feature_input_name):
-    if column_prefix == get_aggregation_function_name(afpb.AGGREGATION_FUNCTION_COUNT):
-        return F.lit(1).cast("long")
-    elif column_prefix in set(
-        [
-            get_aggregation_function_name(afpb.AGGREGATION_FUNCTION_LAST_DISTINCT_N),
-            get_aggregation_function_name(afpb.AGGREGATION_FUNCTION_LAST_NON_DISTINCT_N),
-            get_aggregation_function_name(afpb.AGGREGATION_FUNCTION_FIRST_NON_DISTINCT_N),
-            get_aggregation_function_name(afpb.AGGREGATION_FUNCTION_FIRST_DISTINCT_N),
-        ]
-    ):
-        return F.array(F.col(feature_input_name))
-    elif column_prefix == get_aggregation_function_name(afpb.AGGREGATION_FUNCTION_MEAN):
-        return F.col(feature_input_name).cast("double")
-    elif column_prefix == sum_of_squares_column_prefix:
-        return F.pow(feature_input_name, 2)
-    else:
-        return F.col(feature_input_name)
-
-
 def _convert_window_to_anchor_time(
     output_df: pyspark.sql.DataFrame,
     is_continuous: bool,
     time_key: str,
     version: int,
     window_start_column_name: Optional[str],
     window_end_column_name: Optional[str],
@@ -142,37 +125,71 @@
             slide_period_seconds = time_aggregation.aggregation_slide_period.seconds
             anchor_time_offset_seconds = anchor_time_epoch % slide_period_seconds
             anchor_time_offset_string = f"{anchor_time_offset_seconds} seconds"
 
         window_spec = F.window(time_aggregation.time_key, slide_str, slide_str, anchor_time_offset_string)
         group_by_cols = [window_spec] + group_by_cols
         aggregations = []
+        intermediate_columns_added = set([])
         for feature in time_aggregation.features:
             aggregation_plan = get_aggregation_plan(
                 feature.function, feature.function_params, time_aggregation.is_continuous, time_aggregation.time_key
             )
+
+            if aggregation_plan.partial_aggregation_preprocessor is not None:
+                intermediate_columns = aggregation_plan.partial_aggregation_preprocessor(feature.input_feature_name)
+                intermediate_column_names = aggregation_plan.intermediate_column_names(feature.input_feature_name)
+                for column, column_name in zip(intermediate_columns, intermediate_column_names):
+                    if column_name not in intermediate_columns_added:
+                        intermediate_columns_added.add(column_name)
+                        df = df.withColumn(column_name, column)
+
             for name, aggregation in _get_feature_partial_aggregations(aggregation_plan, feature.input_feature_name):
                 if name in output_columns:
                     continue
                 output_columns.add(name)
                 aggregations.append(aggregation)
         output_df = df.groupBy(*group_by_cols).agg(*aggregations)
     else:
         columns_to_drop = set()
+        intermediate_columns_added = set([])
         for feature in time_aggregation.features:
-            column_prefixes = get_aggregation_column_prefixes(feature.function)
-            for column_prefix in column_prefixes:
-                full_name = f"{column_prefix}_{feature.input_feature_name}"
-                if full_name in output_columns:
+            aggregation_plan = get_aggregation_plan(
+                feature.function, feature.function_params, time_aggregation.is_continuous, time_aggregation.time_key
+            )
+
+            if aggregation_plan.partial_aggregation_preprocessor is not None:
+                intermediate_columns = aggregation_plan.partial_aggregation_preprocessor(feature.input_feature_name)
+                intermediate_column_names = aggregation_plan.intermediate_column_names(feature.input_feature_name)
+                for column, column_name in zip(intermediate_columns, intermediate_column_names):
+                    if column_name not in intermediate_columns_added:
+                        intermediate_columns_added.add(column_name)
+                        df = df.withColumn(column_name, column)
+
+            input_columns = (
+                aggregation_plan.intermediate_column_names(feature.input_feature_name)
+                if aggregation_plan.intermediate_column_prefixes is not None
+                else [feature.input_feature_name]
+            )
+            continuous_columns = aggregation_plan.continuous_partial_aggregation_transform(input_columns)
+
+            continuous_column_prefixes = get_materialization_aggregation_column_prefixes(
+                feature.function, is_continuous=True
+            )
+            continuous_column_names = [
+                f"{column_prefix}_{feature.input_feature_name}" for column_prefix in continuous_column_prefixes
+            ]
+
+            for column_name, column in zip(continuous_column_names, continuous_columns):
+                if column_name in output_columns:
                     continue
-                output_columns.add(full_name)
-                df = df.withColumn(
-                    full_name, _get_continuous_aggregation_value(column_prefix, feature.input_feature_name)
-                )
+                output_columns.add(column_name)
+                df = df.withColumn(column_name, column)
             columns_to_drop.add(feature.input_feature_name)
+            columns_to_drop.update(input_columns)
         # Drop the original feature columns.
         for column in columns_to_drop:
             df = df.drop(column)
         output_df = df
 
     output_df = _convert_window_to_anchor_time(
         output_df,
@@ -196,26 +213,17 @@
     for feature in trailing_time_window_aggregation.features:
         aggregation_plan = get_aggregation_plan(
             feature.function,
             feature.function_params,
             trailing_time_window_aggregation.is_continuous,
             trailing_time_window_aggregation.time_key,
         )
-        for old_name in aggregation_plan.materialized_column_names(feature.input_feature_name):
-            aggregation_function_name = resolve_function_name(
-                get_aggregation_column_prefix_from_column_name(feature.function, old_name), feature.function_params
-            )
-            renaming_map[old_name] = f"{feature.input_feature_name}_{aggregation_function_name}_{slide_interval_string}"
-    return renaming_map
 
-
-def rename_partial_aggregate_columns(
-    df: pyspark.sql.DataFrame,
-    slide_interval_string: str,
-    trailing_time_window_aggregation: TrailingTimeWindowAggregation,
-) -> pyspark.sql.DataFrame:
-    """Rename partial aggregate columns to human readable format."""
-    # Create a map from intermediate rollup column name to preferred column names.
-    renaming_map = partial_aggregate_column_renames(slide_interval_string, trailing_time_window_aggregation)
-    for old_name, new_name in renaming_map.items():
-        df = df.withColumnRenamed(old_name, new_name)
-    return df
+        for materialized_column_prefix, materialized_column_name in zip(
+            aggregation_plan.materialized_column_prefixes,
+            aggregation_plan.materialized_column_names(feature.input_feature_name),
+        ):
+            new_prefix = get_pretty_column_prefix(materialized_column_prefix)
+            renaming_map[
+                materialized_column_name
+            ] = f"{feature.input_feature_name}_{new_prefix}_{slide_interval_string}"
+    return renaming_map
```

### Comparing `tecton-0.7.0b9/tecton_spark/pipeline_helper.py` & `tecton-0.7.0rc0/tecton_spark/pipeline_helper.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,44 +1,59 @@
 import random
 import re
 import string
-from typing import *
+from typing import Any
+from typing import Callable
+from typing import Dict
+from typing import List
+from typing import Optional
+from typing import Set
+from typing import Tuple
+from typing import Union
 
+import numpy as np
 import pandas
 import pandas as pd
 import pendulum
 from pyspark.sql import DataFrame
 from pyspark.sql import SparkSession
+from pyspark.sql.column import Column
 from pyspark.sql.types import ArrayType
+from pyspark.sql.types import MapType
 from pyspark.sql.types import StringType
 from pyspark.sql.types import StructType
 
 from tecton_core import specs
 from tecton_core.errors import UDF_ERROR
+from tecton_core.errors import UDF_TYPE_ERROR
+from tecton_core.feature_definition_wrapper import FeatureDefinitionWrapper
 from tecton_core.id_helper import IdHelper
 from tecton_core.materialization_context import BaseMaterializationContext
 from tecton_core.materialization_context import BoundMaterializationContext
-from tecton_core.pipeline_common import constant_node_to_value
 from tecton_core.pipeline_common import CONSTANT_TYPE
 from tecton_core.pipeline_common import CONSTANT_TYPE_OBJECTS
+from tecton_core.pipeline_common import constant_node_to_value
 from tecton_core.pipeline_common import get_keyword_inputs
 from tecton_core.pipeline_common import get_time_window_from_data_source_node
 from tecton_core.pipeline_common import positional_inputs
 from tecton_core.pipeline_common import transformation_type_checker
+from tecton_core.query.sql_compat import dialect
 from tecton_core.query_consts import UDF_INTERNAL
 from tecton_proto.args.pipeline_pb2 import DataSourceNode
 from tecton_proto.args.pipeline_pb2 import Input as InputProto
 from tecton_proto.args.pipeline_pb2 import Pipeline
 from tecton_proto.args.pipeline_pb2 import PipelineNode
 from tecton_proto.args.pipeline_pb2 import TransformationNode
 from tecton_proto.args.transformation_pb2 import TransformationMode
 from tecton_proto.common.data_source_type_pb2 import DataSourceType
 from tecton_spark import data_source_helper
+from tecton_spark.schema_spark_utils import schema_to_spark
 from tecton_spark.spark_schema_wrapper import SparkSchemaWrapper
 
+
 MAX_INT64 = (2**63) - 1
 
 
 # TODO(TEC-8978): remove \. from namespace regex when FWv3 FVs are no longer supported.
 _NAMESPACE_SEPARATOR_REGEX = re.compile(r"__|\.")
 
 
@@ -53,68 +68,88 @@
     spl = _NAMESPACE_SEPARATOR_REGEX.split(namespaced_feature_name)
     if len(spl) == 2:
         return spl[1]
 
     return namespaced_feature_name.split(".")[1]
 
 
-# Pandas Pipeline (ODFV)
-# input_df (spark df) is the spine passed in by the user (including request context),
-# and it has been augmented with dependent fv fields in of the form "_udf_internal_{input_name}_{odfv_id}".
-# The spark dataframe we return will be everything from the spine, with the on-demand features added
-#
-# NB: If the user defines their transformation to produce extra columns (besides what's specified in output_schema) they will be ignored
-# And if they are missing columns they will fail in this function during runtime.
-def dataframe_with_input(
-    spark: SparkSession,
-    pipeline: Pipeline,
-    # This should have data from all inputs
-    input_df: DataFrame,
-    output_schema: StructType,
-    transformations: List[specs.TransformationSpec],
-    name: str,
-    fv_id: str,
-    namespace_separator: str,
-    namespace: Optional[str],
-) -> DataFrame:
-    # pass in only the non-internal fields and udf-internal fields corresponding to this particular odfv
-    udf_args = [f"{c.name}" for c in input_df.schema if (UDF_INTERNAL not in c.name or fv_id in c.name)]
+def build_odfv_udf_col(
+    input_df: DataFrame, fdw: FeatureDefinitionWrapper, namespace: str
+) -> Tuple[Column, List[Column]]:
+    """
+    Builds a Spark udf for executing a specific ODFV. This runs an ODFV,
+    which outputs a single temporary object (dict/map for python mode, json for
+    pandas mode). We then deserialize this to get the feature columns.
+
+    We use this function in two phases in parallel across multiple ODFVs:
+    1. Run an ODFV to get the tmp object
+    2. Select columns from the tmp object
+
+    To support running these two phases in parallel, we use this method
+    to output the column for (1) and the columns for (2), and concat them
+    all together.
+
+    :return: select_column (the tmp odfv output col), output_columns (the
+    columns of the tmp odfv output, which map to the output features of an
+    odfv)
+    """
+    fv_name = fdw.name
+    fv_id = fdw.id
+    pipeline = fdw.pipeline
+    output_schema = schema_to_spark(fdw.view_schema)
+    transformations = fdw.transformations
+    namespace_separator = fdw.namespace_separator
+    if namespace is None:
+        namespace = fv_name
+
+    odfv_tmp_output_name = f"_{namespace}_odfv_output"
+
+    # Pass in only the non-internal fields and udf-internal fields
+    # corresponding to this particular odfv
+    udf_args = []
+    for input_col in input_df.schema:
+        if UDF_INTERNAL not in input_col.name or fv_id in input_col.name:
+            udf_args.append(input_col.name)
     udf_arg_idx_map = {}
-    for idx in range(len(udf_args)):
-        udf_arg_idx_map[udf_args[idx]] = idx
+    for arg_idx in range(len(udf_args)):
+        udf_arg_idx_map[udf_args[arg_idx]] = arg_idx
     builder = _ODFVPipelineBuilder(
-        name=name,
+        name=fv_name,
         fv_id=fv_id,
         pipeline=pipeline,
         transformations=transformations,
         udf_arg_idx_map=udf_arg_idx_map,
         output_schema=output_schema,
     )
 
-    from pyspark.sql.functions import col, udf, pandas_udf, from_json
+    from pyspark.sql.functions import col
 
-    input_columns = [f"`{c.name}`" for c in input_df.schema]
-    odfv_output_object = "_odfv_output"
-    if namespace is None:
-        namespace = name
     output_columns = [
-        col(f"{odfv_output_object}.{c.name}").alias(f"{namespace}{namespace_separator}{c.name}") for c in output_schema
+        col(f"{odfv_tmp_output_name}.{c.name}").alias(f"{namespace}{namespace_separator}{c.name}")
+        for c in output_schema
     ]
+    from pyspark.sql.functions import from_json
+    from pyspark.sql.functions import pandas_udf
+    from pyspark.sql.functions import udf
+
     if builder.mode == "python":
+        # Python features are output as a single dict / map column, so we
+        # map that into individual columns
         _odfv_udf = udf(builder.py_wrapper, output_schema)
-        return input_df.select(
-            *input_columns, _odfv_udf(*[f"`{c}`" for c in udf_args]).alias(odfv_output_object)
-        ).select(*input_columns, *output_columns)
+        udf_col = _odfv_udf(*[f"`{c}`" for c in udf_args]).alias(odfv_tmp_output_name)
+        return udf_col, output_columns
     else:
         assert builder.mode == "pandas"
-        # Note: from_json will return null in the case of an unparseable string.
+        # Pandas features are output into a single struct, so we deserialize
+        # here + cast into multiple columns.
+        # Note: from_json will return null in the case of an unparseable
+        # string.
         _odfv_udf = pandas_udf(builder.pandas_udf_wrapper, StringType())
-        return input_df.select(
-            *input_columns, from_json(_odfv_udf(*[f"`{c}`" for c in udf_args]), output_schema).alias(odfv_output_object)
-        ).select(*input_columns, *output_columns)
+        deserialized_udf_col = from_json(_odfv_udf(*[f"`{c}`" for c in udf_args]), output_schema)
+        return deserialized_udf_col.alias(odfv_tmp_output_name), output_columns
 
 
 # TODO: if the run api should support some type of mock inputs other than dicts, then we'd need to modify this
 # For now, the same pipeline evaluation works for both.
 def run_mock_odfv_pipeline(
     pipeline: Pipeline,
     transformations: List[specs.TransformationSpec],
@@ -169,26 +204,33 @@
         names_set.add(node.feature_view_node.input_name)
     elif node.HasField("transformation_node"):
         for child in node.transformation_node.inputs:
             _get_all_input_keys_helper(child.node, names_set)
     return names_set
 
 
-def get_all_input_ds_id_map(node: PipelineNode) -> Dict[str, str]:
-    names_dict = dict()
-    _get_all_input_ds_id_map_helper(node, names_dict)
+def get_fco_ids_to_input_keys(node: PipelineNode) -> Dict[str, str]:
+    names_dict = {}
+    _get_fco_ids_to_input_keys_helper(node, names_dict)
     return names_dict
 
 
-def _get_all_input_ds_id_map_helper(node: PipelineNode, names_dict: Dict[str, str]):
-    if node.HasField("data_source_node"):
-        names_dict[node.data_source_node.input_name] = IdHelper.to_string(node.data_source_node.virtual_data_source_id)
+def _get_fco_ids_to_input_keys_helper(node: PipelineNode, names_dict: Dict[str, str]):
+    if node.HasField("request_data_source_node"):
+        # request data sources don't have fco ids
+        pass
+    elif node.HasField("data_source_node"):
+        ds_node = node.data_source_node
+        names_dict[IdHelper.to_string(ds_node.virtual_data_source_id)] = ds_node.input_name
+    elif node.HasField("feature_view_node"):
+        fv_node = node.feature_view_node
+        names_dict[IdHelper.to_string(fv_node.feature_view_id)] = fv_node.input_name
     elif node.HasField("transformation_node"):
         for child in node.transformation_node.inputs:
-            _get_all_input_ds_id_map_helper(child.node, names_dict)
+            _get_fco_ids_to_input_keys_helper(child.node, names_dict)
     return names_dict
 
 
 # Constructs empty data frames matching schema of DS inputs for the purpose of
 # schema-validating the transformation pipeline.
 def populate_empty_passed_in_inputs(
     node: PipelineNode,
@@ -235,17 +277,22 @@
         # we only use mode and name from these
         transformations: List[specs.TransformationSpec],
         feature_time_limits: Optional[pendulum.Period],
         schedule_interval: Optional[pendulum.Duration] = None,
         # If None, we will compute inputs from raw data sources and apply time filtering.
         # Otherwise we will prefer these inputs instead
         passed_in_inputs: Optional[Dict[str, DataFrame]] = None,
+        # output_schema is only used by python/pandas transformations during backfills.
+        # Specifically, it applies for Stream feature views with push sources and batch config, during
+        # the batch materialization jobs.
+        output_schema: Optional[StructType] = None,
     ):
         self._spark = spark
         self._pipeline = pipeline
+        self._output_schema = output_schema
         self._consume_streaming_data_sources = consume_streaming_data_sources
         self._feature_time_limits = feature_time_limits
         self._id_to_ds = {ds.id: ds for ds in data_sources}
         self._id_to_transformation = {t.id: t for t in transformations}
 
         self._registered_temp_view_names: List[str] = []
         self._schedule_interval = schedule_interval
@@ -271,21 +318,22 @@
             ):
                 ds_id = IdHelper.to_string(data_source_node.virtual_data_source_id)
                 raw_data_time_limits = get_time_window_from_data_source_node(
                     self._feature_time_limits, self._schedule_interval, data_source_node
                 )
                 if ds_id not in self._id_to_ds or raw_data_time_limits is None:
                     return self._passed_in_inputs[pipeline_node.data_source_node.input_name]
-                data_source_helper.validate_data_source_supports_time_filtering(self._id_to_ds[ds_id].batch_source)
+                data_source_helper.validate_data_source_supports_time_filtering(self._id_to_ds[ds_id])
                 return data_source_helper.apply_partition_and_timestamp_filter(
                     df=self._passed_in_inputs[pipeline_node.data_source_node.input_name],
                     batch_source=self._id_to_ds[ds_id].batch_source,
                     start_time=raw_data_time_limits.start if raw_data_time_limits else None,
                     end_time=raw_data_time_limits.end if raw_data_time_limits else None,
                 )
+
             else:
                 return self._data_source_node_to_dataframe(pipeline_node.data_source_node)
         elif pipeline_node.HasField("constant_node"):
             return constant_node_to_value(pipeline_node.constant_node)
         elif pipeline_node.HasField("materialization_context_node"):
             if self._feature_time_limits is not None:
                 feature_start_time = self._feature_time_limits.start
@@ -293,33 +341,37 @@
                 batch_schedule = self._schedule_interval
             else:
                 feature_start_time = pendulum.from_timestamp(0, pendulum.tz.UTC)
                 feature_end_time = pendulum.datetime(2100, 1, 1)
                 batch_schedule = self._schedule_interval or pendulum.duration()
             return BoundMaterializationContext._create_internal(feature_start_time, feature_end_time, batch_schedule)
         elif pipeline_node.HasField("request_data_source_node"):
-            raise ValueError("RequestDataSource is not supported in Spark pipelines")
+            msg = "RequestDataSource is not supported in Spark pipelines"
+            raise ValueError(msg)
         elif pipeline_node.HasField("feature_view_node"):
-            raise ValueError("Dependent FeatureViews are not supported in Spark pipelines")
+            msg = "Dependent FeatureViews are not supported in Spark pipelines"
+            raise ValueError(msg)
         else:
-            raise KeyError(f"Unknown PipelineNode type: {pipeline_node}")
+            msg = f"Unknown PipelineNode type: {pipeline_node}"
+            raise KeyError(msg)
 
     def _transformation_node_to_dataframe(self, transformation_node: TransformationNode) -> DataFrame:
         """Recursively translates inputs to values and then passes them to the transformation."""
         args: List[Union[DataFrame, str, int, float, bool]] = []
         kwargs = {}
         for transformation_input in transformation_node.inputs:
             node_value = self._node_to_value(transformation_input.node)
             if transformation_input.HasField("arg_index"):
                 assert len(args) == transformation_input.arg_index
                 args.append(node_value)
             elif transformation_input.HasField("arg_name"):
                 kwargs[transformation_input.arg_name] = node_value
             else:
-                raise KeyError(f"Unknown argument type for Input node: {transformation_input}")
+                msg = f"Unknown argument type for Input node: {transformation_input}"
+                raise KeyError(msg)
 
         return self._apply_transformation_function(transformation_node, args, kwargs)
 
     def _apply_transformation_function(
         self, transformation_node, args, kwargs
     ) -> Union[Dict[str, Any], pd.DataFrame, DataFrame]:
         """For the given transformation node, returns the corresponding DataFrame transformation.
@@ -336,31 +388,72 @@
                 raise UDF_ERROR(e)
             transformation_type_checker(transformation.name, res, "pyspark", self._possible_modes())
             return res
         elif transformation.transformation_mode == TransformationMode.TRANSFORMATION_MODE_SPARK_SQL:
             # type checking happens inside this function
             return self._wrap_sql_function(transformation_node, user_function)(*args, **kwargs)
         elif transformation.transformation_mode == TransformationMode.TRANSFORMATION_MODE_PANDAS:
+            # TODO(achal): This code block should be cleaned up by breaking out some udf wrapping logic in a separate
+            # top level function.
             try:
-                res = user_function(*args, **kwargs)
+                if isinstance(self, _ODFVPipelineBuilder):
+                    res = user_function(*args, **kwargs)
+                else:
+                    import pyspark.pandas as ps
+
+                    # Assumes that we only have one argument that is a pyspark Dataframe built from the batch source
+                    # because this code path is for ingest api which should only read from one batch source.
+                    assert len(args) + len(kwargs) == 1, "Pandas transformations only support a single input"
+                    df = args[0] if len(args) == 1 else list(kwargs.values())[0]
+
+                    psdf = ps.DataFrame(df)
+                    psdf = psdf.pandas_on_spark.apply_batch(user_function)
+
+                    res = psdf.to_spark()
+
+                return res
             except Exception as e:
                 raise UDF_ERROR(e)
-            transformation_type_checker(transformation.name, res, "pandas", self._possible_modes())
-            return res
         elif transformation.transformation_mode == TransformationMode.TRANSFORMATION_MODE_PYTHON:
+            # TODO(achal): This code block should be cleaned up by breaking out some udf wrapping logic in a separate
+            # top level function.
             try:
-                res = user_function(*args, **kwargs)
+                if isinstance(self, _ODFVPipelineBuilder):
+                    res = user_function(*args, **kwargs)
+                else:
+                    from pyspark.sql.functions import struct
+                    from pyspark.sql.functions import udf
+
+                    # Assumes that we only have one argument that is a pyspark Dataframe built from the batch source
+                    # because this code path is for ingest api which should only read from one batch source.
+                    assert len(args) + len(kwargs) == 1, "Pandas transformations only support a single input"
+                    df = args[0] if len(args) == 1 else list(kwargs.values())[0]
+
+                    @udf(self._output_schema)
+                    def transform_rows(df_group):
+                        # Apply your desired transformation on the input group of Rows(this applies row by row)
+                        transformed_df_group = user_function(df_group.asDict())
+
+                        # Return the transformed group of rows as a Pandas DataFrame
+                        return transformed_df_group
+
+                    df = df.select(struct("*").alias("data"))
+                    df = df.select(transform_rows("data").alias("result"))
+                    res = df.select(*[f"result.{field.name}" for field in self._output_schema])
+                return res
+            except TypeError as e:
+                raise UDF_TYPE_ERROR(e)
             except Exception as e:
                 raise UDF_ERROR(e)
-            # Only restrict types on the root node of python-mode transforms
-            if transformation_node == self._pipeline.root:
-                transformation_type_checker(transformation.name, res, "python", self._possible_modes())
-            return res
         else:
-            raise KeyError(f"Unknown transformation mode: {transformation.transformation_mode}")
+            msg = (
+                f"unknown transformation mode({transformation.transformation_mode}) "
+                f"on compute mode ({dialect().value})"
+            )
+            raise KeyError(msg)
 
     def _wrap_sql_function(
         self, transformation_node: TransformationNode, user_function: Callable[..., str]
     ) -> Callable[..., DataFrame]:
         def wrapped(*args, **kwargs):
             wrapped_args = []
             for arg, node_input in zip(args, positional_inputs(transformation_node)):
@@ -400,15 +493,16 @@
         elif node.HasField("data_source_node"):
             if node.data_source_node.HasField("input_name"):
                 return node.data_source_node.input_name
             # TODO(TEC-5076): remove this legacy code, since input_name will always be set
             name = self._id_to_ds[IdHelper.to_string(node.data_source_node.virtual_data_source_id)].fco_metadata.name
             return f"data_source_{name}_output"
         else:
-            raise Exception(f"Expected transformation or data source node: {node}")
+            msg = f"Expected transformation or data source node: {node}"
+            raise Exception(msg)
 
     def _register_temp_table(self, name: str, df: DataFrame) -> str:
         """Registers a Dataframe as a temp table and returns its name."""
         unique_name = name + self._random_suffix()
         self._registered_temp_view_names.append(unique_name)
         df.createOrReplaceTempView(unique_name)
         return unique_name
@@ -419,27 +513,27 @@
     def _data_source_node_to_dataframe(self, data_source_node: DataSourceNode) -> DataFrame:
         """Creates a DataFrame from a DS and time parameters."""
         ds = self._id_to_ds[IdHelper.to_string(data_source_node.virtual_data_source_id)]
         time_window = get_time_window_from_data_source_node(
             self._feature_time_limits, self._schedule_interval, data_source_node
         )
         if not self._consume_streaming_data_sources and time_window:
-            data_source_helper.validate_data_source_supports_time_filtering(ds.batch_source)
+            data_source_helper.validate_data_source_supports_time_filtering(ds)
         return data_source_helper.get_ds_dataframe(
             self._spark,
             ds,
             consume_streaming_data_source=self._consume_streaming_data_sources,
             start_time=time_window.start if time_window else None,
             end_time=time_window.end if time_window else None,
         )
 
     def _possible_modes(self):
         # note that pipeline is included since this is meant to be a user hint, and it's
         # theoretically possible a pipeline wound up deeper than expected
-        return ["pyspark", "spark_sql", "pipeline"]
+        return ["pyspark", "spark_sql", "pipeline", "python", "pandas"]
 
 
 # For Pandas-mode:
 # We need to take the call a udf constructed from the pipeline that will generate the on-demand features.
 # A pandas udf takes as inputs (pd.Series...) and outputs pd.Series.
 # However, the user-defined transforms take as input pd.DataFrame and output pd.DataFrame.
 # We use _ODFVPipelineBuilder to construct a udf wrapper function that translates the inputs and outputs and
@@ -490,42 +584,39 @@
     # FOR PANDAS
     def pandas_udf_wrapper(self, *args):
         assert self.mode == "pandas"
 
         # self.udf_arg_idx_map tells us which of these pd.Series correspond to a given RequestDataSource or FeatureView input
         self._udf_args: List[pd.Series] = args
 
-        import pandas as pd
         import json
-        import numpy as np
+
+        import pandas as pd
 
         output_df = self._udf_node_to_value(self._pipeline.root)
 
         assert isinstance(
             output_df, pd.DataFrame
         ), f"Transformer returns {str(output_df)}, but must return a pandas.DataFrame instead."
 
         for field in self._output_schema:
             assert field.name in output_df.columns, (
                 f"Expected output schema field '{field.name}' not found in columns of DataFrame returned by "
                 f"'{self._name}': [" + ", ".join(output_df.columns) + "]"
             )
             # Convert np.arrays to python lists which are JSON serializable by the default serializer.
-            if isinstance(field.dataType, ArrayType):
-                output_df[field.name] = output_df[field.name].apply(
-                    lambda arr: arr.tolist() if isinstance(arr, np.ndarray) else arr
-                )
+            if isinstance(field.dataType, (ArrayType, MapType, StructType)):
+                output_df[field.name] = output_df[field.name].apply(_convert_ndarray_to_list)
 
         output_strs = []
 
         # itertuples() is used instead of iterrows() to preserve type safety.
         # See notes in https://pandas.pydata.org/pandas-docs/version/0.17.1/generated/pandas.DataFrame.iterrows.html.
         for row in output_df.itertuples(index=False):
             output_strs.append(json.dumps(row._asdict()))
-
         return pd.Series(output_strs)
 
     def _transformation_node_to_online_dataframe(
         self, transformation_node: TransformationNode
     ) -> Union[Dict[str, Any], pd.DataFrame]:
         """Recursively translates inputs to values and then passes them to the transformation."""
         args: List[Union[DataFrame, str, int, float, bool]] = []
@@ -534,15 +625,16 @@
             node_value = self._udf_node_to_value(transformation_input.node)
             if transformation_input.HasField("arg_index"):
                 assert len(args) == transformation_input.arg_index
                 args.append(node_value)
             elif transformation_input.HasField("arg_name"):
                 kwargs[transformation_input.arg_name] = node_value
             else:
-                raise KeyError(f"Unknown argument type for Input node: {transformation_input}")
+                msg = f"Unknown argument type for Input node: {transformation_input}"
+                raise KeyError(msg)
 
         return self._apply_transformation_function(transformation_node, args, kwargs)
 
     # evaluate a node in the Pipeline
     def _udf_node_to_value(
         self, pipeline_node: PipelineNode
     ) -> Union[str, int, float, bool, None, Dict[str, Any], pd.DataFrame, DataFrame, pd.Series]:
@@ -575,41 +667,56 @@
                         continue
                     idx = self.udf_arg_idx_map[feature]
                     all_series.append(self._udf_args[idx])
                     features.append(feature_name(feature))
                 df = pd.concat(all_series, keys=features, axis=1)
                 return df
             else:
-                raise NotImplementedError("Transform mode {self.mode} is not yet implemented")
+                msg = "Transform mode {self.mode} is not yet implemented"
+                raise NotImplementedError(msg)
         elif pipeline_node.HasField("request_data_source_node"):
             if self._passed_in_inputs is not None:
                 return self._passed_in_inputs[pipeline_node.request_data_source_node.input_name]
             elif self.mode == "python":
                 request_context = pipeline_node.request_data_source_node.request_context
-                field_names = [field.name for field in request_context.schema.fields]
+                field_names = [c.name for c in request_context.tecton_schema.columns]
                 fields_dict = {}
                 for input_col in field_names:
                     idx = self.udf_arg_idx_map[input_col]
                     fields_dict[input_col] = self._udf_args[idx]
                 return fields_dict
             elif self.mode == "pandas":
                 all_series = []
                 request_context = pipeline_node.request_data_source_node.request_context
-                field_names = [field.name for field in request_context.schema.fields]
+                field_names = [c.name for c in request_context.tecton_schema.columns]
                 for input_col in field_names:
                     idx = self.udf_arg_idx_map[input_col]
                     all_series.append(self._udf_args[idx])
                 df = pd.concat(all_series, keys=field_names, axis=1)
                 return df
             else:
-                raise NotImplementedError("Transform mode {self.mode} is not yet implemented")
+                msg = "Transform mode {self.mode} is not yet implemented"
+                raise NotImplementedError(msg)
         elif pipeline_node.HasField("transformation_node"):
             return self._transformation_node_to_online_dataframe(pipeline_node.transformation_node)
         elif pipeline_node.HasField("materialization_context_node"):
-            raise ValueError("MaterializationContext is unsupported for pandas pipelines")
+            msg = "MaterializationContext is unsupported for pandas pipelines"
+            raise ValueError(msg)
         else:
-            raise NotImplementedError("This is not yet implemented")
+            msg = "This is not yet implemented"
+            raise NotImplementedError(msg)
 
     def _possible_modes(self):
         # note that pipeline is included since this is meant to be a user hint, and it's
         # theoretically possible a pipeline wound up deeper than expected
         return ["pandas", "pipeline", "python"]
+
+
+def _convert_ndarray_to_list(item):
+    if isinstance(item, np.ndarray):
+        return item.tolist()
+    elif isinstance(item, dict):
+        return {key: _convert_ndarray_to_list(value) for key, value in item.items()}
+    elif isinstance(item, list):
+        return [_convert_ndarray_to_list(value) for value in item]
+    else:
+        return item
```

### Comparing `tecton-0.7.0b9/tecton_spark/query/data_source.py` & `tecton-0.7.0rc0/tecton_spark/query/data_source.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,41 +1,35 @@
+import logging
 from datetime import datetime
 from typing import Dict
 from typing import Optional
 
 import attrs
-import pandas as pd
 import pendulum
 import pyspark
 
-from tecton_core import logger as logger_lib
 from tecton_core import specs
 from tecton_core.feature_definition_wrapper import FeatureDefinitionWrapper
 from tecton_core.query.node_interface import DataframeWrapper
 from tecton_proto.args.pipeline_pb2 import DataSourceNode
 from tecton_spark import data_source_helper
 from tecton_spark import offline_store
 from tecton_spark.query.node import SparkExecNode
 
-logger = logger_lib.get_logger("DataSource")
+
+logger = logging.getLogger(__name__)
 
 
 @attrs.frozen
 class UserSpecifiedDataSparkNode(SparkExecNode):
     data: DataframeWrapper
     metadata: Optional[Dict[str, any]]
 
     def _to_dataframe(self, spark: pyspark.sql.SparkSession) -> pyspark.sql.DataFrame:
-        if isinstance(self.data._dataframe, pyspark.sql.DataFrame):
-            df = self.data._dataframe
-        elif isinstance(self.data._dataframe, pd.DataFrame):
-            df = spark.createDataFrame(self.data._dataframe)
-        else:
-            raise Exception(f"Unimplemented data type: {self.data._dataframe}")
-        return df
+        return self.data.to_spark()
 
 
 @attrs.frozen
 class MockDataSourceScanSparkNode(SparkExecNode):
     data: SparkExecNode
     ds: specs.DataSourceSpec
     start_time: Optional[datetime]
@@ -77,20 +71,20 @@
         df = data_source_helper.get_non_dsf_raw_dataframe(spark, self.ds.batch_source)
         return df
 
 
 @attrs.frozen
 class OfflineStoreScanSparkNode(SparkExecNode):
     feature_definition_wrapper: FeatureDefinitionWrapper
-    time_filter: Optional[pendulum.Period]
+    partition_time_filter: Optional[pendulum.Period]
 
     def _to_dataframe(self, spark: pyspark.sql.SparkSession) -> pyspark.sql.DataFrame:
         offline_reader = offline_store.get_offline_store_reader(spark, self.feature_definition_wrapper)
         try:
             # None implies no timestamp filtering. When we implement time filter pushdown, it will go here
-            df = offline_reader.read(self.time_filter)
+            df = offline_reader.read(self.partition_time_filter)
             return df
         except pyspark.sql.utils.AnalysisException as e:
             logger.warn(
                 f"Failed to read from the Offline Store. Please ensure that materialization backfills have completed for Feature View '{self.feature_definition_wrapper.name}' or set `from_source=True`."
             )
             raise e
```

### Comparing `tecton-0.7.0b9/tecton_spark/query/filter.py` & `tecton-0.7.0rc0/tecton_spark/query/filter.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,48 +1,49 @@
+import logging
 from datetime import datetime
 from typing import List
 from typing import Optional
 from typing import Union
 
 import attrs
 import pendulum
 import pyspark
 from pyspark.sql import DataFrame
-from pyspark.sql import functions
 from pyspark.sql import SparkSession
+from pyspark.sql import functions
 from pyspark.sql.types import LongType
 from pyspark.sql.types import TimestampType
 
-import tecton_core
 from tecton_core import conf
 from tecton_proto.data.feature_view_pb2 import MaterializationTimeRangePolicy
 from tecton_spark.query.node import SparkExecNode
 
 
 TECTON_FEATURE_TIMESTAMP_VALIDATOR = "_tecton_feature_timestamp_validator"
 SKIP_FEATURE_TIMESTAMP_VALIDATION_ENV = "SKIP_FEATURE_TIMESTAMP_VALIDATION"
 TIMESTAMP_VALIDATOR_UDF_REGISTERED = False
 
 
-logger = tecton_core.logger.get_logger("QueryTree")
+logger = logging.getLogger(__name__)
 
 
 def _apply_or_check_feature_data_time_limits(
     spark: SparkSession,
     feature_df: DataFrame,
     time_range_policy: MaterializationTimeRangePolicy,
     timestamp_key: str,
     feature_data_time_limits: Optional[pendulum.Period],
 ) -> DataFrame:
     if time_range_policy == MaterializationTimeRangePolicy.MATERIALIZATION_TIME_RANGE_POLICY_FAIL_IF_OUT_OF_RANGE:
         return _validate_feature_timestamps(spark, feature_df, feature_data_time_limits, timestamp_key)
     elif time_range_policy == MaterializationTimeRangePolicy.MATERIALIZATION_TIME_RANGE_POLICY_FILTER_TO_RANGE:
         return _filter_to_feature_data_time_limits(feature_df, feature_data_time_limits, timestamp_key)
     else:
-        raise ValueError(f"Unhandled time range policy: {time_range_policy}")
+        msg = f"Unhandled time range policy: {time_range_policy}"
+        raise ValueError(msg)
 
 
 def _filter_to_feature_data_time_limits(
     feature_df: DataFrame,
     feature_data_time_limits: Optional[pendulum.Period],
     timestamp_key: Optional[str],
 ) -> DataFrame:
@@ -79,15 +80,15 @@
     spark: SparkSession,
     feature_df: DataFrame,
     feature_data_time_limits: Optional[pendulum.Period],
     timestamp_key: Optional[str],
 ) -> DataFrame:
     if conf.get_or_none(SKIP_FEATURE_TIMESTAMP_VALIDATION_ENV) is True:
         logger.info(
-            f"Note: skipping the feature timestamp validation step because `SKIP_FEATURE_TIMESTAMP_VALIDATION` is set to true."
+            "Note: skipping the feature timestamp validation step because `SKIP_FEATURE_TIMESTAMP_VALIDATION` is set to true."
         )
         return feature_df
 
     if feature_data_time_limits:
         _ensure_timestamp_validation_udf_registered(spark)
 
         start_time_expr = f"to_timestamp('{feature_data_time_limits.start}')"
@@ -148,24 +149,23 @@
 
     def _to_dataframe(self, spark: pyspark.sql.SparkSession) -> pyspark.sql.DataFrame:
         ret = self.input_node.to_dataframe(spark)
 
         ts_col_datatype = ret.schema[self.retrieval_time_col].dataType
         if isinstance(self.feature_start_time, int):
             if not isinstance(ts_col_datatype, LongType):
-                raise RuntimeError(
-                    f"Invalid feature_start_time column type, expected LongType but got: {ts_col_datatype}"
-                )
+                msg = f"Invalid feature_start_time column type, expected LongType but got: {ts_col_datatype}"
+                raise RuntimeError(msg)
         elif isinstance(self.feature_start_time, datetime):
             if not isinstance(ts_col_datatype, TimestampType):
-                raise RuntimeError(
-                    f"Invalid feature_start_time column type, expected TimestampType but got: {ts_col_datatype}"
-                )
+                msg = f"Invalid feature_start_time column type, expected TimestampType but got: {ts_col_datatype}"
+                raise RuntimeError(msg)
         else:
-            raise RuntimeError(f"Invalid feature_start_time type: {type(self.feature_start_time)}")
+            msg = f"Invalid feature_start_time type: {type(self.feature_start_time)}"
+            raise RuntimeError(msg)
 
         cond = functions.col(self.retrieval_time_col) >= functions.lit(self.feature_start_time)
         # select all non-feature cols, and null out any features outside of feature start time
         project_list = [col for col in ret.columns if col not in self.features]
         for c in self.features:
             newcol = functions.when(cond, functions.col(f"`{c}`")).otherwise(functions.lit(None)).alias(c)
             project_list.append(newcol)
```

### Comparing `tecton-0.7.0b9/tecton_spark/query/join.py` & `tecton-0.7.0rc0/tecton_spark/query/join.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,29 +1,30 @@
+import logging
 from typing import ClassVar
 from typing import List
 from typing import Optional
 from typing import Sequence
 from typing import Tuple
 
 import attrs
 import pendulum
 import pyspark
 import pyspark.sql.functions as F
 import pyspark.sql.window as spark_window
 
 from tecton_core.feature_definition_wrapper import FeatureDefinitionWrapper
-from tecton_core.logger import get_logger
 from tecton_core.query_consts import ANCHOR_TIME
 from tecton_core.schema import Schema
 from tecton_core.time_utils import convert_proto_duration_for_version
 from tecton_core.time_utils import convert_timedelta_for_version
 from tecton_spark.aggregation_plans import get_aggregation_plan
 from tecton_spark.query.node import SparkExecNode
 
-logger = get_logger("query_tree")
+
+logger = logging.getLogger(__name__)
 
 
 @attrs.frozen
 class JoinSparkNode(SparkExecNode):
     """
     A basic left join on 2 inputs
     """
@@ -104,15 +105,16 @@
         right_df: pyspark.sql.DataFrame,
         left_ts_cols: Sequence[str],
         right_ts_cols: Sequence[str],
         partition_cols: Sequence[str],
         use_window_range_between_value: bool,
     ):
         if len(left_ts_cols) != len(right_ts_cols):
-            raise RuntimeError(f"Timestamp columns are not equal length: left({left_ts_cols}), right({right_ts_cols})")
+            msg = f"Timestamp columns are not equal length: left({left_ts_cols}), right({right_ts_cols})"
+            raise RuntimeError(msg)
 
         timestamp_cols = [f"{cls._timestamp_prefix}_{i}" for i in range(len(left_ts_cols))]
 
         for new_col, old_col in zip(timestamp_cols, left_ts_cols):
             left_df = left_df.withColumn(new_col, F.col(old_col))
 
         for new_col, old_col in zip(timestamp_cols, right_ts_cols):
```

### Comparing `tecton-0.7.0b9/tecton_spark/query/node.py` & `tecton-0.7.0rc0/tecton_spark/query/node.py`

 * *Files identical despite different names*

### Comparing `tecton-0.7.0b9/tecton_spark/query/pipeline.py` & `tecton-0.7.0rc0/tecton_spark/query/pipeline.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,42 +1,52 @@
 from datetime import datetime
 from typing import Dict
+from typing import List
 from typing import Optional
+from typing import Tuple
 
 import attrs
 import pendulum
 import pyspark
 
 from tecton_core.feature_definition_wrapper import FeatureDefinitionWrapper
 from tecton_spark import data_observability
 from tecton_spark.partial_aggregations import construct_partial_time_aggregation_df
 from tecton_spark.pipeline_helper import _PipelineBuilder
-from tecton_spark.pipeline_helper import dataframe_with_input
+from tecton_spark.pipeline_helper import build_odfv_udf_col
 from tecton_spark.query.node import SparkExecNode
 from tecton_spark.schema_spark_utils import schema_to_spark
 
 
 @attrs.frozen
-class OdfvPipelineSparkNode(SparkExecNode):
+class MultiOdfvPipelineSparkNode(SparkExecNode):
     input_node: SparkExecNode
-    feature_definition_wrapper: FeatureDefinitionWrapper
-    namespace: str
+    feature_definition_wrappers_namespaces: List[Tuple[FeatureDefinitionWrapper, str]]
 
     def _to_dataframe(self, spark: pyspark.sql.SparkSession) -> pyspark.sql.DataFrame:
-        return dataframe_with_input(
-            spark,
-            self.feature_definition_wrapper.pipeline,
-            self.input_node.to_dataframe(spark),
-            schema_to_spark(self.feature_definition_wrapper.view_schema),
-            transformations=self.feature_definition_wrapper.transformations,
-            name=self.feature_definition_wrapper.name,
-            fv_id=self.feature_definition_wrapper.id,
-            namespace_separator=self.feature_definition_wrapper.namespace_separator,
-            namespace=self.namespace,
-        )
+        """
+        Executes multiple ODFV transformations on the same input dataframe.
+
+        Note: If the user defines their transformation to produce extra columns
+        (besides what's specified in output_schema), they will be ignored. If
+        there are missing columns they will fail in this function during
+        runtime.
+        """
+        udf_select_columns = []
+        odfv_output_columns = []
+        input_df = self.input_node.to_dataframe(spark)
+        for fdw, namespace in self.feature_definition_wrappers_namespaces:
+            select_col, output_cols = build_odfv_udf_col(input_df, fdw, namespace)
+            udf_select_columns.append(select_col)
+            odfv_output_columns.extend(output_cols)
+
+        # Execute odfvs in parallel, then deserialize outputs into columns
+        input_columns = [f"`{c.name}`" for c in input_df.schema]
+        odfv_tmp_outputs = input_df.select(*input_columns, *udf_select_columns)
+        return odfv_tmp_outputs.select(*input_columns, *odfv_output_columns)
 
 
 @attrs.frozen
 class PipelineEvalSparkNode(SparkExecNode):
     inputs_map: Dict[str, SparkExecNode]
     feature_definition_wrapper: FeatureDefinitionWrapper
 
@@ -49,14 +59,15 @@
             self.feature_definition_wrapper.pipeline,
             consume_streaming_data_sources=False,
             data_sources=self.feature_definition_wrapper.data_sources,
             transformations=self.feature_definition_wrapper.transformations,
             feature_time_limits=self.feature_time_limits,
             schedule_interval=self.feature_definition_wrapper.batch_materialization_schedule,
             passed_in_inputs={k: self.inputs_map[k].to_dataframe(spark) for k in self.inputs_map},
+            output_schema=schema_to_spark(self.feature_definition_wrapper.view_schema),
         ).get_dataframe()
         return df
 
 
 @attrs.frozen
 class PartialAggSparkNode(SparkExecNode):
     input_node: SparkExecNode
```

### Comparing `tecton-0.7.0b9/tecton_spark/query/projection.py` & `tecton-0.7.0rc0/tecton_spark/query/projection.py`

 * *Files 0% similar despite different names*

```diff
@@ -121,15 +121,15 @@
             effective_timestamp = functions.col(self.timestamp_field)
         else:
             slide_str = f"{self.batch_schedule_seconds} seconds"
             timestamp_col = functions.col(self.timestamp_field)
             # Timestamp of temporal aggregate is end of the anchor time window. Subtract 1 micro
             # to get the correct bucket for batch schedule.
             if self.is_temporal_aggregate:
-                timestamp_col -= expr(f"interval 1 microseconds")
+                timestamp_col -= expr("interval 1 microseconds")
             window_spec = functions.window(timestamp_col, slide_str, slide_str)
             effective_timestamp = window_spec.end + expr(f"interval {self.data_delay_seconds} seconds")
 
         df = input_df.withColumn(self.effective_timestamp_name, effective_timestamp)
         return df
```

### Comparing `tecton-0.7.0b9/tecton_spark/query/translate.py` & `tecton-0.7.0rc0/tecton_spark/query/translate.py`

 * *Files 7% similar despite different names*

```diff
@@ -17,15 +17,15 @@
 from tecton_core.query.nodes import DataSourceScanNode
 from tecton_core.query.nodes import EntityFilterNode
 from tecton_core.query.nodes import FeatureTimeFilterNode
 from tecton_core.query.nodes import FeatureViewPipelineNode
 from tecton_core.query.nodes import JoinNode
 from tecton_core.query.nodes import MetricsCollectorNode
 from tecton_core.query.nodes import MockDataSourceScanNode
-from tecton_core.query.nodes import OdfvPipelineNode
+from tecton_core.query.nodes import MultiOdfvPipelineNode
 from tecton_core.query.nodes import OfflineStoreScanNode
 from tecton_core.query.nodes import PartialAggNode
 from tecton_core.query.nodes import RawDataSourceScanNode
 from tecton_core.query.nodes import RenameColsNode
 from tecton_core.query.nodes import RespectFeatureStartTimeNode
 from tecton_core.query.nodes import RespectTTLNode
 from tecton_core.query.nodes import SelectDistinctNode
@@ -35,31 +35,34 @@
 from tecton_spark.query import data_source
 from tecton_spark.query import filter
 from tecton_spark.query import join
 from tecton_spark.query import pipeline
 from tecton_spark.query import projection
 from tecton_spark.query.node import SparkExecNode
 
+
 if sys.version_info >= (3, 9):
-    from typing import get_args, get_origin
+    from typing import get_args
+    from typing import get_origin
 else:
-    from typing_extensions import get_args, get_origin
+    from typing_extensions import get_args
+    from typing_extensions import get_origin
 
 
 # convert from logical tree to physical tree
 def spark_convert(node_ref: NodeRef) -> SparkExecNode:
     logical_tree_node = node_ref.node
     node_mapping = {
         CustomFilterNode: filter.CustomFilterSparkNode,
         DataSourceScanNode: data_source.DataSourceScanSparkNode,
         RawDataSourceScanNode: data_source.RawDataSourceScanSparkNode,
         MockDataSourceScanNode: data_source.MockDataSourceScanSparkNode,
         OfflineStoreScanNode: data_source.OfflineStoreScanSparkNode,
         FeatureViewPipelineNode: pipeline.PipelineEvalSparkNode,
-        OdfvPipelineNode: pipeline.OdfvPipelineSparkNode,
+        MultiOdfvPipelineNode: pipeline.MultiOdfvPipelineSparkNode,
         FeatureTimeFilterNode: filter.FeatureTimeFilterSparkNode,
         EntityFilterNode: filter.EntityFilterSparkNode,
         RespectTTLNode: filter.RespectTTLSparkNode,
         RespectFeatureStartTimeNode: filter.RespectFeatureStartTimeSparkNode,
         AddAnchorTimeNode: projection.AddAnchorTimeSparkNode,
         AddRetrievalAnchorTimeNode: projection.AddRetrievalAnchorTimeSparkNode,
         StreamWatermarkNode: filter.StreamWatermarkSparkNode,
@@ -71,22 +74,22 @@
         AsofJoinFullAggNode: join.AsofJoinFullAggSparkNode,
         AsofWildcardExplodeNode: join.AsofWildcardExplodeSparkNode,
         RenameColsNode: projection.RenameColsSparkNode,
         SelectDistinctNode: projection.SelectDistinctSparkNode,
         ConvertEpochToTimestampNode: projection.ConvertEpochToTimestampSparkNode,
         AddEffectiveTimestampNode: projection.AddEffectiveTimestampSparkNode,
         MetricsCollectorNode: pipeline.MetricsCollectorSparkNode,
-        SelectDistinctNode: projection.SelectDistinctSparkNode,
         AddDurationNode: projection.AddDurationSparkNode,
     }
 
     if logical_tree_node.__class__ in node_mapping:
         return node_mapping[logical_tree_node.__class__].from_query_node(logical_tree_node)
 
-    raise Exception(f"TODO: mapping for {logical_tree_node.__class__}")
+    msg = f"TODO: mapping for {logical_tree_node.__class__}"
+    raise Exception(msg)
 
 
 def attrs_spark_converter(attrs_inst: typing.Any, attr: attrs.Attribute, item: typing.Any) -> typing.Any:
     """
     This converts a NodeRef into a SparkNode if it's a NodeRef.
     """
     # Check if type is a typing wrapper.
```

### Comparing `tecton-0.7.0b9/tecton_spark/schema_derivation_utils.py` & `tecton-0.7.0rc0/tecton_spark/schema_derivation_utils.py`

 * *Files 5% similar despite different names*

```diff
@@ -37,14 +37,34 @@
         ts_format = None
         if timestamp_format:
             ts_format = timestamp_format
         df = data_source_helper.apply_timestamp_column(df, timestamp_field, ts_format)
     return spark_schema_wrapper.SparkSchemaWrapper.from_spark_schema(df.schema)
 
 
+def get_unity_table_schema(
+    spark: pyspark.sql.SparkSession,
+    catalog: str,
+    schema: str,
+    table: str,
+    post_processor: Optional[Callable],
+    timestamp_field: str,
+    timestamp_format: str,
+) -> spark_schema_pb2.SparkSchema:
+    df = data_source_helper._get_raw_unity_table_dataframe(spark, catalog, schema, table)
+    if post_processor is not None:
+        df = post_processor(df)
+    if timestamp_field:
+        ts_format = None
+        if timestamp_format:
+            ts_format = timestamp_format
+        df = data_source_helper.apply_timestamp_column(df, timestamp_field, ts_format)
+    return spark_schema_wrapper.SparkSchemaWrapper.from_spark_schema(df.schema)
+
+
 @typechecked
 def get_redshift_table_schema(
     spark: pyspark.sql.SparkSession,
     endpoint: str,
     table: str,
     query: str,
     temp_s3: str,
@@ -138,21 +158,31 @@
     else:
         uri = file_uri
 
     if schema_override is not None:
         reader = reader.schema(schema_override.unwrap())
 
     if file_format == "json":
-        action = lambda: reader.json(uri)
+
+        def action():
+            return reader.json(uri)
+
     elif file_format == "parquet":
-        action = lambda: reader.parquet(uri)
+
+        def action():
+            return reader.parquet(uri)
+
     elif file_format == "csv":
-        action = lambda: reader.csv(uri, header=True)
+
+        def action():
+            return reader.csv(uri, header=True)
+
     else:
-        raise AssertionError(f"Unsupported file format '{file_format}'")
+        msg = f"Unsupported file format '{file_format}'"
+        raise AssertionError(msg)
 
     df = errors_spark.handleDataAccessErrors(action, file_uri)
 
     if convert_to_glue:
         df = data_source_helper.convert_json_like_schema_to_glue_format(spark, df)
     if post_processor is not None:
         df = post_processor(df)
@@ -242,23 +272,14 @@
 def get_feature_view_empty_view_df(
     spark: pyspark.sql.SparkSession,
     feature_view: feature_view_pb2.FeatureViewArgs,
     transformations: Sequence[specs.TransformationSpec],
     data_sources: Sequence[specs.DataSourceSpec],
 ) -> pyspark.sql.DataFrame:
     """Return a pyspark dataframe for the feature view "view" (i.e. before agggregations) using mock/empty data."""
-    # TODO: rm temporary check here and put in DataSource validation
-    for ds in data_sources:
-        # Data sources are not required to have the timestamp column set, but if it is set, the column should be in
-        # the data source schema.
-        if ds.batch_source and ds.batch_source.timestamp_field:
-            assert any(
-                [ds.batch_source.timestamp_field == column.name for column in ds.batch_source.spark_schema.fields]
-            ), "Timestamp column not found in data source schema"
-
     # Create empty data frames for each DS input matching the DS schema.
     id_to_ds = {ds.id: ds for ds in data_sources}
     empty_mock_inputs = pipeline_helper.populate_empty_passed_in_inputs(feature_view.pipeline.root, id_to_ds, spark)
 
     return pipeline_helper.pipeline_to_dataframe(
         spark,
         pipeline=feature_view.pipeline,
```

### Comparing `tecton-0.7.0b9/tecton_spark/schema_spark_utils.py` & `tecton-0.7.0rc0/tecton_spark/schema_spark_utils.py`

 * *Files 19% similar despite different names*

```diff
@@ -4,21 +4,23 @@
 from tecton_core.data_types import ArrayType
 from tecton_core.data_types import BoolType
 from tecton_core.data_types import DataType
 from tecton_core.data_types import Float32Type
 from tecton_core.data_types import Float64Type
 from tecton_core.data_types import Int32Type
 from tecton_core.data_types import Int64Type
+from tecton_core.data_types import MapType
 from tecton_core.data_types import StringType
 from tecton_core.data_types import StructField
 from tecton_core.data_types import StructType
 from tecton_core.data_types import TimestampType
 from tecton_core.schema import Schema
 from tecton_proto.common.schema_pb2 import Schema as SchemaProto
 
+
 # Keep in sync with DataTypeUtils.kt and tecton_core/schema_derivation_utils. . Use "simple strings" as the keys so that fields like "nullable" are ignored.
 PRIMITIVE_SPARK_TYPE_SIMPLE_STRING_TO_TECTON_TYPE = {
     spark_types.StringType().simpleString(): StringType(),
     spark_types.LongType().simpleString(): Int64Type(),
     spark_types.FloatType().simpleString(): Float32Type(),
     spark_types.DoubleType().simpleString(): Float64Type(),
     spark_types.BooleanType().simpleString(): BoolType(),
@@ -64,31 +66,42 @@
         struct_fields = []
         for field in spark_datatype:
             tecton_type = tecton_data_type_from_spark_data_type(field.dataType)
             struct_fields.append(StructField(field.name, tecton_type))
         return StructType(struct_fields)
     elif isinstance(spark_datatype, spark_types.ArrayType):
         return ArrayType(tecton_data_type_from_spark_data_type(spark_datatype.elementType))
+    elif isinstance(spark_datatype, spark_types.MapType):
+        return MapType(
+            tecton_data_type_from_spark_data_type(spark_datatype.keyType),
+            tecton_data_type_from_spark_data_type(spark_datatype.valueType),
+        )
     elif spark_datatype.simpleString() in PRIMITIVE_SPARK_TYPE_SIMPLE_STRING_TO_TECTON_TYPE:
         return PRIMITIVE_SPARK_TYPE_SIMPLE_STRING_TO_TECTON_TYPE[spark_datatype.simpleString()]
     else:
-        raise ValueError(f"{spark_datatype.simpleString()}, is not a supported type for features.")
+        msg = f"{spark_datatype.simpleString()}, is not a supported type for features."
+        raise ValueError(msg)
 
 
 def spark_data_type_from_tecton_data_type(tecton_data_type: DataType) -> spark_types.DataType:
     if tecton_data_type in PRIMITIVE_TECTON_DATA_TYPE_TO_SPARK_DATA_TYPE:
         return PRIMITIVE_TECTON_DATA_TYPE_TO_SPARK_DATA_TYPE[tecton_data_type]
     elif isinstance(tecton_data_type, ArrayType):
         element_type = spark_data_type_from_tecton_data_type(tecton_data_type.element_type)
         return spark_types.ArrayType(element_type)
     elif isinstance(tecton_data_type, StructType):
         spark_struct = spark_types.StructType()
         for field in tecton_data_type.fields:
             spark_struct.add(field.name, spark_data_type_from_tecton_data_type(field.data_type))
         return spark_struct
+    elif isinstance(tecton_data_type, MapType):
+        return spark_types.MapType(
+            spark_data_type_from_tecton_data_type(tecton_data_type.key_type),
+            spark_data_type_from_tecton_data_type(tecton_data_type.value_type),
+        )
     else:
         assert False, f"Unsupported type: {tecton_data_type}"
 
 
 def schema_to_spark(schema: Schema) -> spark_types.StructType:
     ret = spark_types.StructType()
     for col_name, col_spark_data_type in column_name_spark_data_types(schema):
```

### Comparing `tecton-0.7.0b9/tecton_spark/spark_helper.py` & `tecton-0.7.0rc0/tecton_spark/spark_helper.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 import json
+import logging
 import os
 from dataclasses import dataclass
 from typing import Optional
 
 from pyspark.sql import DataFrame
 
-from tecton_core.logger import get_logger
 from tecton_proto.args.feature_view_pb2 import ClusterConfig
 
 
-logger = get_logger("SparkHelper")
+logger = logging.getLogger(__name__)
 
 
 @dataclass
 class QueryPlanInfo:
     has_joins: bool
     has_aggregations: bool
 
@@ -45,15 +45,15 @@
         actual_spark_version = os.environ.get("DATABRICKS_RUNTIME_VERSION")
     elif config.HasField("new_emr") and config.new_emr.HasField("pinned_spark_version"):
         expected_spark_version = config.new_emr.pinned_spark_version
         try:
             with open("/mnt/var/lib/info/extraInstanceData.json", "r") as f:
                 emr_cluster_info = json.load(f)
                 actual_spark_version = emr_cluster_info["releaseLabel"]
-        except:
+        except Exception:
             actual_spark_version = None
     else:
         return
 
     # We do startswith rather than == because databricks lies about its scala version, so we just compare the first part of the dbr.
     if actual_spark_version is not None and expected_spark_version.startswith(actual_spark_version):
         logger.info(
```

### Comparing `tecton-0.7.0b9/tecton_spark/spark_schema_wrapper.py` & `tecton-0.7.0rc0/tecton_spark/spark_schema_wrapper.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,18 +1,19 @@
 import json
+import logging
 from typing import List
 
 from pyspark.sql.types import DataType
 from pyspark.sql.types import StructField
 from pyspark.sql.types import StructType
 
-from tecton_core.logger import get_logger
 from tecton_proto.common.spark_schema_pb2 import SparkSchema
 
-logger = get_logger("SparkSchemaWrapper")
+
+logger = logging.getLogger(__name__)
 
 
 class SparkSchemaWrapper:
     """
     Wrapper around Spark schema (StructType).
     """
```

### Comparing `tecton-0.7.0b9/tecton_spark/time_utils.py` & `tecton-0.7.0rc0/tecton_spark/time_utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -15,17 +15,16 @@
 
 WINDOW_UNBOUNDED_PRECEDING = "unbounded_preceding"
 
 
 def assert_valid_time_string(time_str: str, allow_unbounded=False):
     if allow_unbounded:
         if time_str.lower() != WINDOW_UNBOUNDED_PRECEDING and pytimeparse.parse(time_str) is None:
-            raise TectonValidationError(
-                f"Time string {time_str} must either be `tecton.WINDOW_UNBOUNDED_PRECEDING` or a valid time string."
-            )
+            msg = f"Time string {time_str} must either be `tecton.WINDOW_UNBOUNDED_PRECEDING` or a valid time string."
+            raise TectonValidationError(msg)
     else:
         strict_pytimeparse(time_str)
 
 
 def subtract_seconds_from_timestamp(
     timestamp: Union[Column, datetime.datetime], delta_seconds: int
 ) -> Union[Column, datetime.datetime]:
```

### Comparing `tecton-0.7.0b9/tecton_spark/udfs.py` & `tecton-0.7.0rc0/tecton_spark/udfs.py`

 * *Files 2% similar despite different names*

```diff
@@ -5,16 +5,16 @@
 from pyspark.sql.functions import udf
 from pyspark.sql.types import ArrayType
 from pyspark.sql.types import TimestampType
 
 from tecton_core.errors import TectonValidationError
 from tecton_core.time_utils import align_time_downwards
 from tecton_core.time_utils import strict_pytimeparse
-from tecton_spark.time_utils import assert_valid_time_string
 from tecton_spark.time_utils import WINDOW_UNBOUNDED_PRECEDING
+from tecton_spark.time_utils import assert_valid_time_string
 
 
 def _parse_time(duration: str, allow_unbounded: bool) -> Optional[datetime.timedelta]:
     if allow_unbounded and duration.lower() == WINDOW_UNBOUNDED_PRECEDING:
         return None
 
     return datetime.timedelta(seconds=strict_pytimeparse(duration))
@@ -48,26 +48,26 @@
 
 def _validate_and_parse_time(duration: str, field_name: str, allow_unbounded: bool) -> Optional[datetime.timedelta]:
     duration_td = _parse_time(duration, allow_unbounded)
     if duration_td is not None:
         # called for nice error message
         assert_valid_time_string(duration, allow_unbounded=allow_unbounded)
         if duration_td.total_seconds() <= 0:
-            raise TectonValidationError(f"Duration {duration} provided for field {field_name} must be positive.")
+            msg = f"Duration {duration} provided for field {field_name} must be positive."
+            raise TectonValidationError(msg)
     return duration_td
 
 
 def _validate_sliding_window_duration(
     window_size: str, slide_interval: str
 ) -> (Optional[datetime.timedelta], datetime.timedelta):
     slide_interval_td = _validate_and_parse_time(slide_interval, "slide_interval", allow_unbounded=False)
     window_size_td = _validate_and_parse_time(window_size, "window_size", allow_unbounded=True)
     if window_size_td is not None:
         # note this also confirms window >= slide since a>0, b>0, a % b = 0 implies a >= b
         if window_size_td.total_seconds() % slide_interval_td.total_seconds() != 0:
-            raise TectonValidationError(
-                f"Window size {window_size} must be a multiple of slide interval {slide_interval}"
-            )
+            msg = f"Window size {window_size} must be a multiple of slide interval {slide_interval}"
+            raise TectonValidationError(msg)
     return window_size_td, slide_interval_td
 
 
 tecton_sliding_window_udf = udf(_tecton_sliding_window_impl, ArrayType(TimestampType()))
```

