# Comparing `tmp/functime-0.1.9-py3-none-any.whl.zip` & `tmp/functime-0.2.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,42 +1,44 @@
-Zip file size: 47656 bytes, number of entries: 40
--rw-r--r--  2.0 unx       29 b- defN 23-Jun-18 21:54 functime/__init__.py
--rw-r--r--  2.0 unx      117 b- defN 23-Jun-18 21:54 functime/__main__.py
--rw-r--r--  2.0 unx     1141 b- defN 23-Jun-18 21:54 functime/config.py
--rw-r--r--  2.0 unx     5324 b- defN 23-Jun-18 21:54 functime/cross_validation.py
--rw-r--r--  2.0 unx     1339 b- defN 23-Jun-18 21:54 functime/embeddings.py
--rw-r--r--  2.0 unx     2241 b- defN 23-Jun-18 21:54 functime/offsets.py
--rw-r--r--  2.0 unx    19281 b- defN 23-Jun-18 21:54 functime/preprocessing.py
--rw-r--r--  2.0 unx     1976 b- defN 23-Jun-18 21:54 functime/ranges.py
--rw-r--r--  2.0 unx     2556 b- defN 23-Jun-18 21:54 functime/stats.py
--rw-r--r--  2.0 unx      155 b- defN 23-Jun-18 21:54 functime/base/__init__.py
--rw-r--r--  2.0 unx     2793 b- defN 23-Jun-18 21:54 functime/base/metric.py
--rw-r--r--  2.0 unx     3111 b- defN 23-Jun-18 21:54 functime/base/model.py
--rw-r--r--  2.0 unx     2237 b- defN 23-Jun-18 21:54 functime/base/transformer.py
--rw-r--r--  2.0 unx       81 b- defN 23-Jun-18 21:54 functime/cli/__init__.py
--rw-r--r--  2.0 unx      139 b- defN 23-Jun-18 21:54 functime/cli/_styling.py
--rw-r--r--  2.0 unx     1463 b- defN 23-Jun-18 21:54 functime/cli/deploy.py
--rw-r--r--  2.0 unx     1438 b- defN 23-Jun-18 21:54 functime/cli/entrypoint.py
--rw-r--r--  2.0 unx     2474 b- defN 23-Jun-18 21:54 functime/cli/list.py
--rw-r--r--  2.0 unx     3093 b- defN 23-Jun-18 21:54 functime/cli/login.py
--rw-r--r--  2.0 unx     1112 b- defN 23-Jun-18 21:54 functime/cli/token.py
--rw-r--r--  2.0 unx     2172 b- defN 23-Jun-18 21:54 functime/cli/usage.py
--rw-r--r--  2.0 unx      285 b- defN 23-Jun-18 21:54 functime/feature_extraction/__init__.py
--rw-r--r--  2.0 unx     4221 b- defN 23-Jun-18 21:54 functime/feature_extraction/calendar.py
--rw-r--r--  2.0 unx      421 b- defN 23-Jun-18 21:54 functime/forecasting/__init__.py
--rw-r--r--  2.0 unx     4989 b- defN 23-Jun-18 21:54 functime/forecasting/_base.py
--rw-r--r--  2.0 unx     9921 b- defN 23-Jun-18 21:54 functime/forecasting/auto.py
--rw-r--r--  2.0 unx     3908 b- defN 23-Jun-18 21:54 functime/forecasting/forecasters.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-18 21:54 functime/io/__init__.py
--rw-r--r--  2.0 unx      494 b- defN 23-Jun-18 21:54 functime/io/_serialize.py
--rw-r--r--  2.0 unx     5664 b- defN 23-Jun-18 21:54 functime/io/client.py
--rw-r--r--  2.0 unx      229 b- defN 23-Jun-18 21:54 functime/metrics/__init__.py
--rw-r--r--  2.0 unx     3826 b- defN 23-Jun-18 21:54 functime/metrics/multi_objective.py
--rw-r--r--  2.0 unx     6810 b- defN 23-Jun-18 21:54 functime/metrics/point.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-18 21:54 functime/metrics/probabilistic.py
--rw-r--r--  2.0 unx    34523 b- defN 23-Jun-18 21:54 functime-0.1.9.dist-info/LICENSE
--rw-r--r--  2.0 unx     8290 b- defN 23-Jun-18 21:54 functime-0.1.9.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jun-18 21:54 functime-0.1.9.dist-info/WHEEL
--rw-r--r--  2.0 unx       52 b- defN 23-Jun-18 21:54 functime-0.1.9.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        9 b- defN 23-Jun-18 21:54 functime-0.1.9.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     3265 b- defN 23-Jun-18 21:54 functime-0.1.9.dist-info/RECORD
-40 files, 141271 bytes uncompressed, 42472 bytes compressed:  69.9%
+Zip file size: 58240 bytes, number of entries: 42
+-rw-r--r--  2.0 unx       66 b- defN 23-Jul-17 14:27 functime/__init__.py
+-rw-r--r--  2.0 unx      117 b- defN 23-Jul-17 14:27 functime/__main__.py
+-rw-r--r--  2.0 unx     5893 b- defN 23-Jul-17 14:27 functime/backtesting.py
+-rw-r--r--  2.0 unx     1838 b- defN 23-Jul-17 14:27 functime/conformal.py
+-rw-r--r--  2.0 unx     1379 b- defN 23-Jul-17 14:27 functime/conversion.py
+-rw-r--r--  2.0 unx     5549 b- defN 23-Jul-17 14:27 functime/cross_validation.py
+-rw-r--r--  2.0 unx     2241 b- defN 23-Jul-17 14:27 functime/offsets.py
+-rw-r--r--  2.0 unx      393 b- defN 23-Jul-17 14:27 functime/plotting.py
+-rw-r--r--  2.0 unx    18818 b- defN 23-Jul-17 14:27 functime/preprocessing.py
+-rw-r--r--  2.0 unx     1976 b- defN 23-Jul-17 14:27 functime/ranges.py
+-rw-r--r--  2.0 unx     1481 b- defN 23-Jul-17 14:27 functime/stats.py
+-rw-r--r--  2.0 unx      506 b- defN 23-Jul-17 14:27 functime/umap.py
+-rw-r--r--  2.0 unx      217 b- defN 23-Jul-17 14:27 functime/base/__init__.py
+-rw-r--r--  2.0 unx     7424 b- defN 23-Jul-17 14:27 functime/base/forecaster.py
+-rw-r--r--  2.0 unx     1630 b- defN 23-Jul-17 14:27 functime/base/metric.py
+-rw-r--r--  2.0 unx     2771 b- defN 23-Jul-17 14:27 functime/base/model.py
+-rw-r--r--  2.0 unx     2237 b- defN 23-Jul-17 14:27 functime/base/transformer.py
+-rw-r--r--  2.0 unx      285 b- defN 23-Jul-17 14:27 functime/feature_extraction/__init__.py
+-rw-r--r--  2.0 unx     4064 b- defN 23-Jul-17 14:27 functime/feature_extraction/calendar.py
+-rw-r--r--  2.0 unx      745 b- defN 23-Jul-17 14:27 functime/forecasting/__init__.py
+-rw-r--r--  2.0 unx    12183 b- defN 23-Jul-17 14:27 functime/forecasting/_ar.py
+-rw-r--r--  2.0 unx     4798 b- defN 23-Jul-17 14:27 functime/forecasting/_evaluate.py
+-rw-r--r--  2.0 unx     2231 b- defN 23-Jul-17 14:27 functime/forecasting/_reduction.py
+-rw-r--r--  2.0 unx     8481 b- defN 23-Jul-17 14:27 functime/forecasting/_regressors.py
+-rw-r--r--  2.0 unx     8215 b- defN 23-Jul-17 14:27 functime/forecasting/automl.py
+-rw-r--r--  2.0 unx     2175 b- defN 23-Jul-17 14:27 functime/forecasting/catboost.py
+-rw-r--r--  2.0 unx     3714 b- defN 23-Jul-17 14:27 functime/forecasting/censored.py
+-rw-r--r--  2.0 unx     1034 b- defN 23-Jul-17 14:27 functime/forecasting/knn.py
+-rw-r--r--  2.0 unx     3703 b- defN 23-Jul-17 14:27 functime/forecasting/lance.py
+-rw-r--r--  2.0 unx     4215 b- defN 23-Jul-17 14:27 functime/forecasting/lightgbm.py
+-rw-r--r--  2.0 unx     4018 b- defN 23-Jul-17 14:27 functime/forecasting/linear.py
+-rw-r--r--  2.0 unx     2412 b- defN 23-Jul-17 14:27 functime/forecasting/xgboost.py
+-rw-r--r--  2.0 unx      229 b- defN 23-Jul-17 14:27 functime/metrics/__init__.py
+-rw-r--r--  2.0 unx     3826 b- defN 23-Jul-17 14:27 functime/metrics/multi_objective.py
+-rw-r--r--  2.0 unx     6810 b- defN 23-Jul-17 14:27 functime/metrics/point.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-17 14:27 functime/metrics/probabilistic.py
+-rw-r--r--  2.0 unx    34523 b- defN 23-Jul-17 14:28 functime-0.2.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx     8269 b- defN 23-Jul-17 14:28 functime-0.2.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jul-17 14:28 functime-0.2.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       52 b- defN 23-Jul-17 14:28 functime-0.2.0.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        9 b- defN 23-Jul-17 14:28 functime-0.2.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     3506 b- defN 23-Jul-17 14:28 functime-0.2.0.dist-info/RECORD
+42 files, 174125 bytes uncompressed, 52662 bytes compressed:  69.8%
```

## zipnote {}

```diff
@@ -1,121 +1,127 @@
 Filename: functime/__init__.py
 Comment: 
 
 Filename: functime/__main__.py
 Comment: 
 
-Filename: functime/config.py
+Filename: functime/backtesting.py
 Comment: 
 
-Filename: functime/cross_validation.py
+Filename: functime/conformal.py
 Comment: 
 
-Filename: functime/embeddings.py
+Filename: functime/conversion.py
+Comment: 
+
+Filename: functime/cross_validation.py
 Comment: 
 
 Filename: functime/offsets.py
 Comment: 
 
+Filename: functime/plotting.py
+Comment: 
+
 Filename: functime/preprocessing.py
 Comment: 
 
 Filename: functime/ranges.py
 Comment: 
 
 Filename: functime/stats.py
 Comment: 
 
-Filename: functime/base/__init__.py
+Filename: functime/umap.py
 Comment: 
 
-Filename: functime/base/metric.py
+Filename: functime/base/__init__.py
 Comment: 
 
-Filename: functime/base/model.py
+Filename: functime/base/forecaster.py
 Comment: 
 
-Filename: functime/base/transformer.py
+Filename: functime/base/metric.py
 Comment: 
 
-Filename: functime/cli/__init__.py
+Filename: functime/base/model.py
 Comment: 
 
-Filename: functime/cli/_styling.py
+Filename: functime/base/transformer.py
 Comment: 
 
-Filename: functime/cli/deploy.py
+Filename: functime/feature_extraction/__init__.py
 Comment: 
 
-Filename: functime/cli/entrypoint.py
+Filename: functime/feature_extraction/calendar.py
 Comment: 
 
-Filename: functime/cli/list.py
+Filename: functime/forecasting/__init__.py
 Comment: 
 
-Filename: functime/cli/login.py
+Filename: functime/forecasting/_ar.py
 Comment: 
 
-Filename: functime/cli/token.py
+Filename: functime/forecasting/_evaluate.py
 Comment: 
 
-Filename: functime/cli/usage.py
+Filename: functime/forecasting/_reduction.py
 Comment: 
 
-Filename: functime/feature_extraction/__init__.py
+Filename: functime/forecasting/_regressors.py
 Comment: 
 
-Filename: functime/feature_extraction/calendar.py
+Filename: functime/forecasting/automl.py
 Comment: 
 
-Filename: functime/forecasting/__init__.py
+Filename: functime/forecasting/catboost.py
 Comment: 
 
-Filename: functime/forecasting/_base.py
+Filename: functime/forecasting/censored.py
 Comment: 
 
-Filename: functime/forecasting/auto.py
+Filename: functime/forecasting/knn.py
 Comment: 
 
-Filename: functime/forecasting/forecasters.py
+Filename: functime/forecasting/lance.py
 Comment: 
 
-Filename: functime/io/__init__.py
+Filename: functime/forecasting/lightgbm.py
 Comment: 
 
-Filename: functime/io/_serialize.py
+Filename: functime/forecasting/linear.py
 Comment: 
 
-Filename: functime/io/client.py
+Filename: functime/forecasting/xgboost.py
 Comment: 
 
 Filename: functime/metrics/__init__.py
 Comment: 
 
 Filename: functime/metrics/multi_objective.py
 Comment: 
 
 Filename: functime/metrics/point.py
 Comment: 
 
 Filename: functime/metrics/probabilistic.py
 Comment: 
 
-Filename: functime-0.1.9.dist-info/LICENSE
+Filename: functime-0.2.0.dist-info/LICENSE
 Comment: 
 
-Filename: functime-0.1.9.dist-info/METADATA
+Filename: functime-0.2.0.dist-info/METADATA
 Comment: 
 
-Filename: functime-0.1.9.dist-info/WHEEL
+Filename: functime-0.2.0.dist-info/WHEEL
 Comment: 
 
-Filename: functime-0.1.9.dist-info/entry_points.txt
+Filename: functime-0.2.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: functime-0.1.9.dist-info/top_level.txt
+Filename: functime-0.2.0.dist-info/top_level.txt
 Comment: 
 
-Filename: functime-0.1.9.dist-info/RECORD
+Filename: functime-0.2.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## functime/__init__.py

```diff
@@ -1 +1 @@
-"Python client for functime"
+"functime: Time-series machine learning and embeddings at scale."
```

## functime/cross_validation.py

```diff
@@ -1,20 +1,24 @@
 from typing import Mapping, Optional, Tuple
 
 import numpy as np
 import polars as pl
 
 
-def train_test_split(test_size: int) -> Tuple[pl.LazyFrame, pl.LazyFrame]:
+def train_test_split(
+    test_size: int, eager: bool = False
+) -> Tuple[pl.LazyFrame, pl.LazyFrame]:
     """Return a time-ordered train set and test set given `test_size`.
 
     Parameters
     ----------
     test_size : int
         Number of test samples.
+    eager : bool
+        If True, evaluate immediately and returns tuple of train-test `DataFrame`.
 
     Returns
     -------
     splitter : Callable[pl.LazyFrame, Tuple[pl.LazyFrame, pl.LazyFrame]]
         Function that takes a panel LazyFrame and returns tuple of train / test LazyFrames.
     """
 
@@ -27,14 +31,16 @@
             .explode(pl.all().exclude(entity_col))
         )
         test_split = (
             X.groupby(entity_col)
             .agg(pl.all().slice(-1 * test_size, test_size))
             .explode(pl.all().exclude(entity_col))
         )
+        if eager:
+            train_split, test_split = pl.collect_all([train_split, test_split])
         return train_split, test_split
 
     return split
 
 
 def _window_split(
     X: pl.LazyFrame,
```

## functime/preprocessing.py

```diff
@@ -1,7 +1,8 @@
+from itertools import product
 from typing import List, Mapping, Union
 
 import polars as pl
 from scipy.stats import boxcox_normmax
 from typing_extensions import Literal
 
 from functime.base import transformer
@@ -15,14 +16,26 @@
 PL_INT_COLS = pl.col(PL_INT_DTYPES)
 
 
 def PL_NUMERIC_COLS(*exclude):
     return pl.col(PL_NUMERIC_DTYPES).exclude(exclude)
 
 
+def reindex(X: pl.DataFrame) -> pl.DataFrame:
+    entity_col, time_col = X.columns[:2]
+    dtypes = X.dtypes[:2]
+    entities = sorted(set(X.get_column(entity_col)))
+    timestamps = sorted(set(X.get_column(time_col)))
+    X = pl.DataFrame(
+        product(entities, timestamps),
+        schema={entity_col: dtypes[0], time_col: dtypes[1]},
+    ).join(X, how="left", on=[entity_col, time_col])
+    return X
+
+
 @transformer
 def coerce_dtypes(schema: Mapping[str, pl.DataType]):
     """Coerces the column datatypes of a DataFrame using the provided schema.
 
     Parameters
     ----------
     schema : Mapping[str, pl.DataType]
@@ -34,14 +47,45 @@
         artifacts = {"X_new": X_new}
         return artifacts
 
     return transform
 
 
 @transformer
+def time_to_arange(eager: bool = False):
+    """Coerces time column into arange per entity.
+
+    Assumes even-spaced time-series and homogenous start dates.
+    """
+
+    def transform(X: pl.LazyFrame) -> pl.LazyFrame:
+        entity_col, time_col = X.columns[:2]
+        range_expr = pl.arange(0, pl.col(time_col).count()).alias(time_col)
+        other_cols = pl.all().exclude(time_col)
+        X_new = (
+            X.groupby(entity_col)
+            .agg([range_expr, other_cols])
+            .explode(pl.all().exclude(entity_col))
+            .select(
+                [
+                    entity_col,
+                    pl.col(time_col).cast(pl.Int32),
+                    pl.all().exclude([entity_col, time_col]),
+                ]
+            )
+        )
+        if eager:
+            X_new = X_new.collect(streaming=True)
+        artifacts = {"X_new": X_new}
+        return artifacts
+
+    return transform
+
+
+@transformer
 def resample(freq: str, agg_method: str, impute_method: Union[str, int, float]):
     """
     Resamples and transforms a DataFrame using the specified frequency, aggregation method, and imputation method.
 
     Parameters
     ----------
     freq : str
@@ -64,16 +108,14 @@
             # Defensive resampling
             X.lazy()
             .groupby_dynamic(time_col, every=freq, by=entity_col)
             .agg(agg_exprs[agg_method])
             # Must defensive sort columns otherwise time_col and target_col
             # positions are incorrectly swapped in lazy
             .select([entity_col, time_col, target_col])
-            # Reindex full (entity, time) index
-            .pipe(reindex_panel(freq=freq, sort=True))
             # Impute gaps after reindex
             .pipe(impute(impute_method))
             # Defensive fill null with 0 for impute method `ffill`
             .fill_null(0)
         )
         artifacts = {"X_new": X_new}
         return artifacts
@@ -474,85 +516,33 @@
         )
         return X_new
 
     return transform, invert
 
 
 @transformer
-def reindex_panel(freq: str, sort: bool = False):
-    """Reindexes a panel DataFrame to a specified frequency.
-
-    Parameters
-    ----------
-    freq : str
-        Offset alias supported by Polars.
-    sort : bool
-        If True, sort DataFrame by the entity and time columns.
-    """
-
-    def transform(X: pl.LazyFrame) -> pl.LazyFrame:
-        # Create new index
-        entity_col = X.columns[0]
-        time_col = X.columns[1]
-        dtypes = X.dtypes[:2]
-
-        with pl.StringCache():
-            entities = X.collect().get_column(entity_col).unique().to_frame()
-            dates = X.collect().get_column(time_col)
-            timestamps = pl.date_range(
-                dates.min(), dates.max(), interval=freq
-            ).to_frame(name=time_col)
-
-            full_idx = entities.join(timestamps, how="cross")
-            # Defensive cast dtypes to be consistent with df
-            full_idx = full_idx.select(
-                [pl.col(col).cast(dtypes[i]) for i, col in enumerate(full_idx.columns)]
-            )
-
-            # Outer join
-            X_new = (
-                # Must collect before join otherwise will hit error:
-                # Joins/or comparisons on categorical dtypes can only happen if they are created under the same global string cache.
-                X.collect().join(full_idx, on=[entity_col, time_col], how="outer")
-            )
-
-        if sort:
-            X_new = X_new.sort([entity_col, time_col]).with_columns(
-                [pl.col(entity_col).set_sorted(), pl.col(time_col).set_sorted()]
-            )
-
-        artifacts = {"X_new": X_new.lazy()}
-        return artifacts
-
-    return transform
-
-
-@transformer
-def zero_pad(freq: str, include_null: bool = True, include_nan: bool = True):
-    """Reindexes a panel DataFrame to a specified frequency and fills nulls/nans with zeros.
-
-    Parameters
-    ----------
-    freq : str
-        Offset alias supported by Polars.
-    include_null : bool
-        If True, fill null values with zeros.
-    include_nan : bool
-        If True, fill NaN values with zeros.
-    """
-
+def trim(direction: Literal["both", "left", "right"] = "both"):
     def transform(X: pl.LazyFrame) -> pl.LazyFrame:
-        target_cols = X.columns[2:]
-        transform = reindex_panel(freq=freq)
-
-        if include_null and include_nan:
-            expr = [pl.col(col).fill_null(0).fill_nan(0) for col in target_cols]
-        elif include_null:
-            expr = [pl.col(col).fill_null(0) for col in target_cols]
+        entity_col, time_col = X.columns[:2]
+        maxmin = (
+            X.groupby(entity_col)
+            .agg(pl.col(time_col).min())
+            .select(pl.col(time_col).max())
+        )
+        minmax = (
+            X.groupby(entity_col)
+            .agg(pl.col(time_col).max())
+            .select(pl.col(time_col).min())
+        )
+        start, end = pl.collect_all([minmax, maxmin])
+        start, end = start.item(), end.item()
+        if direction == "both":
+            expr = (pl.col(time_col) >= start) & (pl.col(time_col) <= end)
+        elif direction == "left":
+            expr = pl.col(time_col) >= start
         else:
-            expr = [pl.col(col).fill_nan(0) for col in target_cols]
-
-        X_new = transform(X=X).with_columns(expr)
-        artifacts = {"X_new": X_new.lazy()}
+            expr = pl.col(time_col) <= start
+        X_new = X.filter(expr)
+        artifacts = {"X_new": X_new}
         return artifacts
 
     return transform
```

## functime/stats.py

```diff
@@ -1,8 +1,7 @@
-import numpy as np
 import polars as pl
 from statsmodels.tsa.stattools import adfuller, kpss
 
 
 def check_stationarity(y: pl.DataFrame, alpha: float = 0.05):
     y_arr = y.get_column(y.columns[-1])
     _, adf_pval, _ = adfuller(y_arr, regression="n")
@@ -23,39 +22,7 @@
         # KPSS: Under null, reject null that series is trend stationary (non-stationary)
         res = False, "trend", adf_pval, kpss_pval
     else:
         # ADF: Under null, cannot reject null that series has unit root (non-stationary)
         # KPSS: Under null, cannot reject null that series is trend stationary (stationary)
         res = False, "diff", adf_pval, kpss_pval
     return res
-
-
-def check_excess_risks(risks: pl.DataFrame, scores: pl.DataFrame) -> pl.DataFrame:
-    entity_col = scores.columns[0]
-    stat_col = risks.columns[-1]
-    metric_col = scores.columns[-1]
-    risks_arr = (
-        risks.sort(entity_col).get_column(stat_col).to_numpy(zero_copy_only=True)
-    )
-    scores_arr = (
-        scores.sort(entity_col).get_column(metric_col).to_numpy(zero_copy_only=True)
-    )
-    slope, intercept = np.polyfit(risks_arr, scores_arr, 1)
-    res = (
-        risks.with_columns((slope * pl.col(stat_col) + intercept).alias("trend"))
-        .join(scores, on=entity_col)
-        .with_columns((pl.col(metric_col) - pl.col("trend")).alias("distance"))
-        .with_columns((pl.col("distance") > 0).alias("is_excess"))
-        # Sort by highest "risk"
-        .sort("distance", reverse=True)
-        .select(
-            [
-                entity_col,
-                metric_col,
-                stat_col,
-                "trend",
-                "distance",
-                "is_excess",
-            ]
-        )
-    )
-    return res
```

## functime/base/__init__.py

```diff
@@ -1,4 +1,5 @@
+from functime.base.forecaster import Forecaster
 from functime.base.metric import metric
 from functime.base.transformer import Transformer, transformer
 
-__all__ = ["Transformer", "transformer", "metric"]
+__all__ = ["Forecaster", "Transformer", "transformer", "metric"]
```

## functime/base/metric.py

```diff
@@ -1,78 +1,50 @@
 from functools import wraps
-from typing import Callable, Mapping, Union
+from typing import Callable, Union
 
 import polars as pl
 
-
-def _set_string_cache(df: pl.DataFrame) -> pl.DataFrame:
-    entity_col = df.columns[0]
-    entities = df.get_column(entity_col).unique()
-    string_cache = {entity: i for i, entity in enumerate(entities)}
-    entity_col_dtype = df.schema[entity_col]
-    if entity_col_dtype == pl.Categorical:
-        # Reset categorical to string type
-        df = df.with_columns(pl.col(entity_col).cast(pl.Utf8))
-    df_new = df.with_columns(
-        pl.col(entity_col).map_dict(string_cache, return_dtype=pl.Int32)
-    )
-    inv_string_cache = {i: entity for entity, i in string_cache.items()}
-    return df_new, string_cache, inv_string_cache
-
-
-def _enforce_string_cache(
-    df: pl.DataFrame, string_cache: Mapping[Union[int, str], int]
-) -> pl.DataFrame:
-    entity_col = df.columns[0]
-    if df.schema[entity_col] == pl.Categorical:
-        # Reset categorical to string type
-        df = df.with_columns(pl.col(entity_col).cast(pl.Utf8))
-    return df.with_columns(
-        pl.col(entity_col).map_dict(string_cache, return_dtype=pl.Int32)
-    )
-
-
-def _reset_string_cache(
-    df: pl.DataFrame, inv_string_cache: Mapping[int, Union[int, str]], return_dtype
-) -> pl.DataFrame:
-    return df.with_columns(
-        pl.col(df.columns[0]).map_dict(inv_string_cache, return_dtype=return_dtype)
-    )
+from functime.base.model import (
+    _enforce_string_cache,
+    _reset_string_cache,
+    _set_string_cache,
+)
 
 
 # Simple wrapper to collect y_true, y_pred if lazy
 def metric(score: Callable):
     @wraps(score)
     def _score(
         y_true: Union[pl.LazyFrame, pl.DataFrame],
         y_pred: Union[pl.LazyFrame, pl.DataFrame],
         *args,
         **kwargs,
     ) -> pl.DataFrame:
 
-        entity_col_dtype = y_true.schema[y_true.columns[0]]
         if isinstance(y_true, pl.LazyFrame):
-            y_true = y_true.collect()
+            y_true = y_true.collect(streaming=True)
 
         if isinstance(y_pred, pl.LazyFrame):
-            y_pred = y_pred.collect()
+            y_pred = y_pred.collect(streaming=True)
 
-        y_true, string_cache, inv_string_cache = y_true.pipe(_set_string_cache)
+        y_true, entity_col_dtype, string_cache, inv_string_cache = y_true.pipe(
+            _set_string_cache
+        )
         y_pred = y_pred.pipe(_enforce_string_cache, string_cache=string_cache)
         # Coerce columnn names and dtypes
         cols = y_true.columns
         y_pred = y_pred.rename({x: y for x, y in zip(y_pred.columns, cols)}).select(
             [pl.col(col).cast(dtype) for col, dtype in y_true.schema.items()]
         )
 
         if "y_train" in kwargs:
             y_train = (
                 kwargs["y_train"]
                 .lazy()
-                .collect()
+                .collect(streaming=True)
                 .pipe(_enforce_string_cache, string_cache=string_cache)
             )
             kwargs["y_train"] = y_train
 
         scores = score(y_true, y_pred, *args, **kwargs).pipe(
             _reset_string_cache,
             inv_string_cache=inv_string_cache,
```

## functime/base/model.py

```diff
@@ -1,14 +1,48 @@
-import inspect
 from dataclasses import dataclass
-from typing import Any, Callable, Mapping, Protocol
+from typing import Any, Mapping, Protocol, Union
 
 import polars as pl
 
 
+def _set_string_cache(df: pl.DataFrame):
+    entity_col = df.columns[0]
+    entities = df.get_column(entity_col).unique()
+    string_cache = {entity: i for i, entity in enumerate(entities)}
+    entity_col_dtype = df.schema[entity_col]
+    if entity_col_dtype == pl.Categorical:
+        # Reset categorical to string type
+        df = df.with_columns(pl.col(entity_col).cast(pl.Utf8))
+    df_new = df.with_columns(
+        pl.col(entity_col).map_dict(string_cache, return_dtype=pl.Int32)
+    )
+    inv_string_cache = {i: entity for entity, i in string_cache.items()}
+    return df_new, entity_col_dtype, string_cache, inv_string_cache
+
+
+def _enforce_string_cache(
+    df: pl.DataFrame, string_cache: Mapping[Union[int, str], int]
+) -> pl.DataFrame:
+    entity_col = df.columns[0]
+    if df.schema[entity_col] == pl.Categorical:
+        # Reset categorical to string type
+        df = df.with_columns(pl.col(entity_col).cast(pl.Utf8))
+    return df.with_columns(
+        pl.col(entity_col).map_dict(string_cache, return_dtype=pl.Int32)
+    )
+
+
+def _reset_string_cache(
+    df: pl.DataFrame, inv_string_cache: Mapping[int, Union[int, str]], return_dtype
+) -> pl.DataFrame:
+    return df.with_columns(
+        pl.col(df.columns[0]).map_dict(inv_string_cache, return_dtype=return_dtype)
+    )
+
+
 class Regressor(Protocol):
     def fit(self, X, y, sample_weight=None):
         ...
 
     def predict(self, X):
         ...
 
@@ -30,78 +64,27 @@
 class ModelState:
     entity: str
     time: str
     artifacts: Mapping[str, Any]
 
 
 class Model:
-    """A functime Model definition.
+    """A functime Model definition."""
 
-    Inherited by `functime` model types:
-    - check
-    - classifier
-    - clusterer
-    - forecaster
-    - transformer
-    """
-
-    def __init__(self, model: Callable, *args, **kwargs):
-        self.model = model
-        self.args = args
-        self.kwargs = kwargs
+    def __init__(self):
         self.state = None
         self.entity_col_dtype = None
         self.string_cache = {}
         self.inv_string_cache = {}
 
     def _set_string_cache(self, df: pl.DataFrame) -> pl.DataFrame:
-        entity_col = df.columns[0]
-        entity_col_dtype = df.schema[entity_col]
-        entities = df.get_column(entity_col).unique()
-        string_cache = {entity: i for i, entity in enumerate(entities)}
-        if entity_col_dtype == pl.Categorical:
-            # Reset categorical to string type
-            df = df.with_columns(pl.col(entity_col).cast(pl.Utf8))
-        df_new = df.with_columns(
-            pl.col(entity_col).map_dict(string_cache, return_dtype=pl.Int32)
-        )
+        df_new, entity_col_dtype, string_cache, inv_string_cache = _set_string_cache(df)
         self.entity_col_dtype = entity_col_dtype
         self.string_cache = string_cache
-        self.inv_string_cache = {i: entity for entity, i in string_cache.items()}
+        self.inv_string_cache = inv_string_cache
         return df_new
 
     def _enforce_string_cache(self, df: pl.DataFrame) -> pl.DataFrame:
-        entity_col = df.columns[0]
-        if df.schema[entity_col] == pl.Categorical:
-            # Reset categorical to string type
-            df = df.with_columns(pl.col(entity_col).cast(pl.Utf8))
-        return df.with_columns(
-            pl.col(entity_col).map_dict(self.string_cache, return_dtype=pl.Int32)
-        )
+        return _enforce_string_cache(df, self.string_cache)
 
     def _reset_string_cache(self, df: pl.DataFrame) -> pl.DataFrame:
-        return df.with_columns(
-            pl.col(df.columns[0]).map_dict(
-                self.inv_string_cache, return_dtype=self.entity_col_dtype
-            )
-        )
-
-    @property
-    def func(self):
-        return self.model(*self.args, **self.kwargs)
-
-    @property
-    def params(self):
-        model = self.model
-        kwargs = self.kwargs
-        sig = inspect.signature(model)
-        model_params = sig.parameters
-        model_args = list(model_params.keys())
-        params = {
-            **{
-                k: kwargs.get(k, v.default)
-                for k, v in model_params.items()
-                if k != "kwargs"
-            },
-            **{model_args[i]: p for i, p in enumerate(self.args)},
-        }
-        return params
+        return _reset_string_cache(df, self.inv_string_cache, self.entity_col_dtype)
```

## functime/feature_extraction/calendar.py

```diff
@@ -1,8 +1,7 @@
-from functools import reduce
 from typing import List, Optional
 
 import polars as pl
 from holidays import country_holidays
 from typing_extensions import Literal
 
 from functime.base import transformer
@@ -58,48 +57,51 @@
         A list of ISO-2 country codes.
     freq : str
         Sampling frequency at which to group data.
         Must be specified as an offset alias supported by Polars.
     """
 
     def transform(X: pl.LazyFrame) -> pl.LazyFrame:
+
+        # Get min and max timestamps
         time_col = X.columns[1]
-        dt_min_max = X.select(
-            [pl.col(time_col).min().alias("min"), pl.col(time_col).max().alias("max")]
-        ).collect(streaming=True)
-        dt_min, dt_max = dt_min_max[0, "min"], dt_min_max[0, "max"]
-        years = range(dt_min.year, dt_max.year + 1)
+        timestamps = (
+            X.select(time_col)
+            .collect(streaming=True)
+            .get_column(time_col)
+            .unique()
+            .to_list()
+        )
+        min_ts, max_ts = min(timestamps), max(timestamps)
         # Instantiate countries mapping
+        years = range(min_ts.year, max_ts.year + 1)
         countries = [country_holidays(code, years=years) for code in country_codes]
         # Add holiday effects and cast as categorical
-        dt_range = (
-            pl.date_range(dt_min, dt_max, interval="1d", eager=True)
-            .alias(time_col)
-            .to_frame()
-            .lazy()
-        )
-        holidays = [
-            dt_range.with_columns(
-                pl.col(time_col)
-                .apply(country.get)
-                .cast(pl.Utf8)
+        holidays = []
+        for i, country in enumerate(countries):
+            labels = pl.Series(
+                values=[country.get(t) for t in timestamps],
+                name=f"holiday__{country_codes[i]}",
+            ).to_frame()
+            holidays.append(labels)
+        # Concat
+        holidays = (
+            pl.concat(holidays, how="horizontal")
+            .select(
+                pl.all()
                 .str.to_lowercase()
                 .str.replace_all("'", "")
                 .str.replace_all("-", "")
                 .str.replace_all(" ", "_")
                 .cast(pl.Categorical)
-                .alias(f"holiday__{country_codes[i]}")
             )
-            for i, country in enumerate(countries)
-        ]
-        holidays = (
-            reduce(lambda df1, df2: (df1.join(df2, how="inner", on=time_col)), holidays)
-            .groupby_dynamic(time_col, every=freq)
-            .agg(pl.all().exclude(time_col).drop_nulls().first())
-            .with_columns(pl.col(time_col).cast(X.schema[time_col]))
+            .with_columns(
+                pl.Series(values=timestamps, name=time_col).cast(X.schema[time_col])
+            )
+            .lazy()
         )
         X_new = X.join(holidays, how="left", on=time_col)
         artifacts = {"X_new": X_new}
         return artifacts
 
     return transform
```

## functime/forecasting/__init__.py

```diff
@@ -1,24 +1,36 @@
-from .auto import (
-    AutoElasticNet,
-    AutoKNN,
-    AutoLasso,
-    AutoLightGBM,
-    AutoLinearModel,
-    AutoRidge,
+from .automl import (
+    auto_elastic_net,
+    auto_knn,
+    auto_lasso,
+    auto_lightgbm,
+    auto_linear_model,
+    auto_ridge,
 )
-from .forecasters import KNN, ElasticNet, Lasso, LightGBM, LinearModel, Ridge
+from .catboost import catboost
+from .censored import censored_model, zero_inflated_model
+from .knn import knn
+from .lance import ann
+from .lightgbm import flaml_lightgbm, lightgbm
+from .linear import elastic_net, lasso, linear_model, ridge
+from .xgboost import xgboost
 
 __all__ = [
-    "AutoElasticNet",
-    "AutoKNN",
-    "AutoLasso",
-    "AutoLightGBM",
-    "AutoLinearModel",
-    "AutoRidge",
-    "ElasticNet",
-    "KNN",
-    "Lasso",
-    "LinearModel",
-    "LightGBM",
-    "Ridge",
+    "ann",
+    "auto_elastic_net",
+    "auto_knn",
+    "auto_lasso",
+    "auto_lightgbm",
+    "auto_linear_model",
+    "auto_ridge",
+    "catboost",
+    "censored_model",
+    "elastic_net",
+    "flaml_lightgbm",
+    "knn",
+    "lasso",
+    "lightgbm",
+    "linear_model",
+    "ridge",
+    "xgboost",
+    "zero_inflated_model",
 ]
```

## Comparing `functime-0.1.9.dist-info/LICENSE` & `functime-0.2.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `functime-0.1.9.dist-info/METADATA` & `functime-0.2.0.dist-info/METADATA`

 * *Files 14% similar despite different names*

```diff
@@ -1,194 +1,207 @@
 Metadata-Version: 2.1
 Name: functime
-Version: 0.1.9
+Version: 0.2.0
 Summary: The easiest way to run and scale time-series machine learning in the Cloud.
 Author-email: functime Team <team@functime.ai>, Chris Lo <chris@functime.ai>, Daryl Lim <daryl@functime.ai>
-Project-URL: Homepage, https://github.com/indexhub-ai/functime
+Project-URL: Homepage, https://github.com/descendant-ai/functime
 Classifier: Development Status :: 4 - Beta
 Classifier: Intended Audience :: Science/Research
 Classifier: Intended Audience :: Developers
 Classifier: Topic :: Software Development :: Libraries :: Python Modules
 Classifier: Topic :: Scientific/Engineering
 Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
 Classifier: Programming Language :: Python :: 3.11
 Requires-Python: >=3.8
 Description-Content-Type: text/markdown
 License-File: LICENSE
+Requires-Dist: catboost
+Requires-Dist: dask
+Requires-Dist: flaml[automl] (==1.2.4)
 Requires-Dist: holidays
-Requires-Dist: httpx[http2]
+Requires-Dist: joblib
+Requires-Dist: kaleido
+Requires-Dist: lightgbm
 Requires-Dist: numpy
 Requires-Dist: pandas
+Requires-Dist: plotly
 Requires-Dist: polars
 Requires-Dist: pyarrow
+Requires-Dist: pylance
+Requires-Dist: pynndescent (==0.5.8)
 Requires-Dist: rich (>=12.0.0)
+Requires-Dist: scikit-learn (==1.2.2)
 Requires-Dist: scipy
-Requires-Dist: toml
-Requires-Dist: typer (>=0.6.1)
+Requires-Dist: typing-extensions
+Requires-Dist: umap-learn
+Requires-Dist: xgboost
+Requires-Dist: zarr
 Provides-Extra: doc
 Requires-Dist: mkdocs ; extra == 'doc'
 Requires-Dist: mkdocs-material ; extra == 'doc'
 Requires-Dist: mkdocstrings-python ; extra == 'doc'
+Provides-Extra: performance
+Requires-Dist: scikit-learn-intelex ; extra == 'performance'
 Provides-Extra: test
 Requires-Dist: coverage[toml] ; extra == 'test'
+Requires-Dist: fastapi ; extra == 'test'
+Requires-Dist: mlforecast ; extra == 'test'
 Requires-Dist: pytest-benchmark ; extra == 'test'
 Requires-Dist: pytest-memray ; extra == 'test'
 Requires-Dist: pytest-timeout ; extra == 'test'
 Requires-Dist: pytest ; extra == 'test'
-Requires-Dist: scikit-learn ; extra == 'test'
-Requires-Dist: fastapi ; extra == 'test'
+Requires-Dist: skforecast ; extra == 'test'
+Requires-Dist: u8darts (==0.24.0) ; extra == 'test'
 
 <div align="center">
-    <h1>Run and scale time-series machine learning</h1>
+    <h1>Time-series machine learning and embeddings at scale</h1>
 <br />
 
-![functime](https://github.com/indexhub-ai/functime/raw/main/static/images/functime_banner.png)
+![functime](https://github.com/descendant-ai/functime/raw/main/static/images/functime_banner.png)
 [![Python](https://img.shields.io/pypi/pyversions/functime)](https://pypi.org/project/functime/)
 [![PyPi](https://img.shields.io/pypi/v/functime?color=blue)](https://pypi.org/project/functime/)
 [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
-[![GitHub Publish to PyPI](https://github.com/indexhub-ai/functime/actions/workflows/publish.yml/badge.svg)](https://github.com/indexhub-ai/functime/actions/workflows/publish.yml)
-[![GitHub Build Docs](https://github.com/indexhub-ai/functime/actions/workflows/docs.yml/badge.svg)](https://docs.functime.ai/)
-[![GitHub Run Quickstart](https://github.com/indexhub-ai/functime/actions/workflows/quickstart.yml/badge.svg)](https://github.com/indexhub-ai/functime/actions/workflows/quickstart.yml)
+[![GitHub Publish to PyPI](https://github.com/descendant-ai/functime/actions/workflows/publish.yml/badge.svg)](https://github.com/descendant-ai/functime/actions/workflows/publish.yml)
+[![GitHub Build Docs](https://github.com/descendant-ai/functime/actions/workflows/docs.yml/badge.svg)](https://docs.functime.ai/)
+[![GitHub Run Quickstart](https://github.com/descendant-ai/functime/actions/workflows/quickstart.yml/badge.svg)](https://github.com/descendant-ai/functime/actions/workflows/quickstart.yml)
 
 </div>
 
 ---
-**functime** is a powerful and easy-to-use [Cloud service](https://functime.ai) for AutoML forecasting and time-series embeddings.
-The `functime` [Python package](https://pypi.org/project/functime/) provides a scikit-learn API and command-line interface to interact with **functime Cloud**.
+**functime** is a powerful [Python library]((https://pypi.org/project/functime/)) for production-ready AutoML forecasting and temporal embeddings.
 
-**functime** also comes with open-sourced [Apache 2.0](https://github.com/indexhub-ai/functime/blob/HEAD/LICENSING.md) time-series [preprocessing](https://docs.functime.ai/ref/preprocessing/) (box-cox, differencing etc), cross-validation [splitters](https://docs.functime.ai/ref/cross-validation/) (expanding and sliding window), and forecast [metrics](https://docs.functime.ai/ref/metrics/) (MASE, SMAPE etc). All optimized as [lazy Polars](https://pola-rs.github.io/polars-book/user-guide/lazy/using/) transforms.
+**functime** also comes with time-series [preprocessing](https://docs.functime.ai/ref/preprocessing/) (box-cox, differencing etc), cross-validation [splitters](https://docs.functime.ai/ref/cross-validation/) (expanding and sliding window), and forecast [metrics](https://docs.functime.ai/ref/metrics/) (MASE, SMAPE etc). All optimized as [lazy Polars](https://pola-rs.github.io/polars-book/user-guide/lazy/using/) transforms.
 
-Want to use **functime** for seamless time-series analytics across your data team
-Looking for fully-managed production-grade AI/ML forecasting and time-series embeddings?
-Book a [15 minute discovery call](https://calendly.com/functime-indexhub) to learn more about functime's Team / Enterprise plans.
+Want to use **functime** for seamless time-series predictive analytics across your data team?
+Looking for production-grade time-series AutoML in a [serverless](#serverless-deployment) Cloud deployment?
+Shoot Chris a message on [LinkedIn](https://www.linkedin.com/in/chrislohy/) to learn more about `functime` Enterprise.
 
 ## Highlights
 - **Fast:** Forecast 100,000 time series in seconds *on your laptop*
 - **Efficient:** Embarrassingly parallel [feature engineering](https://docs.functime.ai/ref/preprocessing/) for time-series using [`Polars`](https://www.pola.rs/)
 - **Battle-tested:** Machine learning algorithms that deliver real business impact and win competitions
 - **Exogenous features:** supported by every forecaster
 - **Backtesting** with expanding window and sliding window splitters
 - **AutoML**: Automated lags and hyperparameter tuning using [`FLAML`](https://github.com/microsoft/FLAML)
-- Utilities to add calendar effects, special events (e.g. holidays), weather patterns, and economic trends
-- Supports recursive, direct, and ensemble forecast strategies
-
-**Note:** All preprocessors, time-series splitters, and forecasting metrics are implemented with [`Polars`](https://www.pola.rs/) and open-sourced under the Apache-2.0 license. Contributions are always welcome.
+- **Censored model:** for zero-inflated and thresholding forecasts
 
 ## Getting Started
-1. First, install `functime` via the [pip](https://pypi.org/project/functime) package manager.
+Install `functime` via the [pip](https://pypi.org/project/functime) package manager.
 ```bash
 pip install functime
 ```
-2. Then sign-up for a free `functime` Cloud account via the command-line interface (CLI).
-```bash
-functime login
-```
-3. That's it! You can execute time series predictions at scale using functime's `scikit-learn` fit-predict API.
 
 ### Forecasting
 
 ```python
 import polars as pl
 from functime.cross_validation import train_test_split
-from functime.forecasting import LightGBM
+from functime.forecasting import lightgbm
 from functime.metrics import mase
 
 # Load example data
-y = pl.read_parquet("https://bit.ly/commodities-data")
+y = pl.read_parquet("https://github.com/descendant-ai/functime/raw/main/data/commodities.parquet")
 entity_col, time_col = y.columns[:2]
 
 # Time series split
 y_train, y_test = y.pipe(train_test_split(test_size=3))
 
 # Fit-predict
-model = LightGBM(freq="1mo", lags=24, max_horizons=3, strategy="ensemble")
+model = lightgbm(freq="1mo", lags=24, max_horizons=3, strategy="ensemble")
 model.fit(y=y_train)
 y_pred = model.predict(fh=3)
 
 # functime ❤️ functional design
 # fit-predict in a single line
-y_pred = LightGBM(freq="1mo", lags=24)(y=y_train, fh=3)
+y_pred = lightgbm(freq="1mo", lags=24)(y=y_train, fh=3)
 
 # Score forecasts in parallel
 scores = mase(y_true=y_test, y_pred=y_pred, y_train=y_train)
 ```
 
 ### Classification
 
+Only available on `functime` Enterprise.
+
 ```python
 import polars as pl
 import functime
 from sklearn.linear_model import RidgeClassifierCV
 from sklearn.preprocessing import StandardScaler
 from sklearn.metrics import accuracy_score
 from sklearn.pipeline import make_pipeline
 
 # Load GunPoint dataset (150 observations, 150 timestamps)
-X_y_train = pl.read_parquet("https://bit.ly/gunpoint-train")
-X_y_test = pl.read_parquet("https://bit.ly/gunpoint-test")
+X_y_train = pl.read_parquet("https://github.com/descendant-ai/functime/raw/main/data/gunpoint_train.parquet")
+X_y_test = pl.read_parquet("https://github.com/descendant-ai/functime/raw/main/data/gunpoint_test.parquet")
 
 # Train-test split
 X_train, y_train = X_y_train.select(pl.all().exclude("label")), X_y_train.select("label")
 X_test, y_test = X_y_test.select(pl.all().exclude("label")), X_y_test.select("label")
 
-X_train_embs = functime.embeddings.embed(X_train, model="minirocket")
+X_train_embs = functime.embeddings.embed(X_train)
 
 # Fit classifier on the embeddings
 classifier = make_pipeline(
     StandardScaler(with_mean=False),
     RidgeClassifierCV(alphas=np.logspace(-3, 3, 10)),
 )
 classifier.fit(X_train_embs, y_train)
 
 # Predict and
-X_test_embs = embed(X_test, model="minirocket")
+X_test_embs = embed(X_test)
 labels = classifier.predict(X_test_embs)
 accuracy = accuracy_score(predictions, y_test)
 ```
 
 ### Clustering
 
+Only available on `functime` Enterprise.
+
 ```python
 import functime
 import polars as pl
 from hdbscan import HDBSCAN
 from umap import UMAP
 from functime.preprocessing import roll
 
 # Load S&P500 panel data from 2022-06-01 to 2023-06-01
 # Columns: ticker, time, price
-y = pl.read_parquet("https://bit.ly/sp500-data")
+y = pl.read_parquet("https://github.com/descendant-ai/functime/raw/main/data/sp500.parquet")
 
 # Create embeddings
-embeddings = functime.embeddings.embed(y_ma_60, model="minirocket")
+embeddings = functime.embeddings.embed(y_ma_60)
 
 # Reduce dimensionality with UMAP
 reducer = UMAP(n_components=500, n_neighbors=10, metric="manhattan")
 umap_embeddings = reducer.fit_transform(embeddings)
 
 # Cluster with HDBSCAN
 clusterer = HDBSCAN(metric="minkowski", p=1)
 estimator.fit(X)
 
 # Get predicted cluster labels
 labels = estimator.predict(X)
 ```
 
-## Deployment
-`functime` deploys and trains your forecasting models the moment you call any `.fit` method.
+## Serverless Deployment
+
+Only available on `functime` Enterprise.
+
+Deploy and train forecasters the moment you call any `.fit` method.
 Run the `functime list` CLI command to list all deployed models.
-To view data and forecasts usage, run the `functime usage` CLI command.
+Finally, track data and forecasts usage using `functime usage` CLI command.
 
 ![Example CLI usage](static/gifs/functime_cli_usage.gif)
 
 You can reuse a deployed model for predictions anywhere using the `stub_id` variable.
 Note: the `.from_deployed` model class must be the same as during `.fit`.
 ```python
 forecaster = LightGBM.from_deployed(stub_id)
 y_pred = forecaster.predict(fh=3)
 ```
 
 ## License
-`functime` is distributed under [AGPL-3.0-only](LICENSE). For Apache-2.0 exceptions, see [LICENSING.md](https://github.com/indexhub-ai/functime/blob/HEAD/LICENSING.md).
+`functime` is distributed under [AGPL-3.0-only](LICENSE). For Apache-2.0 exceptions, see [LICENSING.md](https://github.com/descendant-ai/functime/blob/HEAD/LICENSING.md).
```

## Comparing `functime-0.1.9.dist-info/RECORD` & `functime-0.2.0.dist-info/RECORD`

 * *Files 14% similar despite different names*

```diff
@@ -1,40 +1,42 @@
-functime/__init__.py,sha256=NM7kykIVEhdbgnsILzqU4tR0tYjnm-Z5vRxMJNSD78Q,29
+functime/__init__.py,sha256=IpTQKBIY-7DJ1VnnWAQcziICkAKKoLMM-rJkzlO_x2o,66
 functime/__main__.py,sha256=HpYJUAIeN5HXQhaM5_kDwo441jDPe9GJ8dLjF9HE9VQ,117
-functime/config.py,sha256=ybZS5SKL6jaqeOQytE8CQFUfKlBcQ8AXoxWVYVIOngA,1141
-functime/cross_validation.py,sha256=WJyA7kOdZHeQal2eX9MIgLg4DDtOHvzvuhcYmBbZfY0,5324
-functime/embeddings.py,sha256=lfAdm_xx9TrO_yPshN-_FNWEuNW4nqHnHaUS4PP0bSg,1339
+functime/backtesting.py,sha256=zuFuaqM_qWyf-3-xpnlGgo7s24ngV9akp1UNPpmgo9s,5893
+functime/conformal.py,sha256=SLLPOEEzOdzVdFzCpNfmETmr4fHKqIhtJC-Xce6mHyo,1838
+functime/conversion.py,sha256=ulS0QWDUWgA3KgJ_2Gn0R-eMRWADN9sVfSOghtzjUIE,1379
+functime/cross_validation.py,sha256=uTkcotiLATM_qA0zfgWAIPaD4fJW9LSqiBABfAbeqvY,5549
 functime/offsets.py,sha256=MO-St46MdhutFtkPJyr5XskYr_0yj_l1nnxh-1fC7GQ,2241
-functime/preprocessing.py,sha256=qTgbKPNx80J4U5K1RCgQybxjLDOc3fH1RsyBi1N_rQ4,19281
+functime/plotting.py,sha256=r-CaXqOhRzWKTdRDbp5l4fTyerGq5qj2Re91mQ9W15U,393
+functime/preprocessing.py,sha256=6Mn3A4UzplSWdoUB_glulvRZuhSD3LlYJ4GKjFfW5q0,18818
 functime/ranges.py,sha256=b3biGP7s2OpoYopEGExELes79D_LdloDW4QH9j1slxI,1976
-functime/stats.py,sha256=hadK1-5bbLhtEOKEdre4thjV6Mv-RqUeJns1YvOvz00,2556
-functime/base/__init__.py,sha256=QhTMdB5mW3GgG7feOGltOa89cwEptd5ndcc4e4kUUUM,155
-functime/base/metric.py,sha256=f_lx6CV0tMbMB54I3pC2osP-6lzpu2692Xcqb_jWcy8,2793
-functime/base/model.py,sha256=RTTsmkRj0U1XpDdzEvKn1VRrnjwirQfulZW--TZ5LI8,3111
+functime/stats.py,sha256=fDy8OpsQl0TCYyD98r9D9Y5IVo8m2u9DoY8aOsYA7CA,1481
+functime/umap.py,sha256=3fL33DtRzqLH4EFJb9EP5MW4ARnUxQXdch6auFh-dzQ,506
+functime/base/__init__.py,sha256=YKzO_7RyxYbn3-jTa4gipQxrwnzUQN2_d2edjDehLmw,217
+functime/base/forecaster.py,sha256=wryuI4OGMGwWVcizx0lKlKHB-ROMw35YSpE1gb3ggeQ,7424
+functime/base/metric.py,sha256=Ous83EZ4ADpcyvHnTGNaxekqKEkwQB7qbYWuzKiGAAI,1630
+functime/base/model.py,sha256=mL8GFL7qNNb4rufm3ZbLzxyDPoDuoHgQhpZ0t20AHv4,2771
 functime/base/transformer.py,sha256=mL0ljSUtdJ6ZX-nEck553pbHk6odZdkqxnGkfYDqWJs,2237
-functime/cli/__init__.py,sha256=VZ9aBV8IvuZx_C375Oet2ixLHXlY7yPKYSFNepsr06E,81
-functime/cli/_styling.py,sha256=I0vvmyiNdrf_sugxJ10PkeON6hdHVTUMk0OOXEDJGUI,139
-functime/cli/deploy.py,sha256=2I9yoD3DMzgavI7g7epo63m44VN0IA25DQ0YWe5lUqU,1463
-functime/cli/entrypoint.py,sha256=pBfkoY0z2dXUFgEWAsYmYqxSqQb_3lSA9KnUobg61R0,1438
-functime/cli/list.py,sha256=kxlarIBbE7uyuP-l0oJXWFhqDVRG2HJ7J5DH2GygnRo,2474
-functime/cli/login.py,sha256=are8-NBrkN6TVwBDxvnvAlQr7IBx_1nm-KA-hOpj9fI,3093
-functime/cli/token.py,sha256=yzNiXiqveOFGrUQCp1zCr-bvzzgITLhCZ7-HVMKXkRc,1112
-functime/cli/usage.py,sha256=T2Vlsk41s_fKXjkGK2Ki-qfZ7fanZCh3AUbAfuixeYo,2172
 functime/feature_extraction/__init__.py,sha256=GS4FtQUUuzJZAJJ4g7fgamtbZhAecz8iwaQsejPTVhs,285
-functime/feature_extraction/calendar.py,sha256=iJhwWnh-kdF97ljC59ysJQWFw-a9namQp_DQVUBSK-M,4221
-functime/forecasting/__init__.py,sha256=eQ2NVNZ9-FXG0vC7U9Fx7drScTmnyLToORdXfgpN5wk,421
-functime/forecasting/_base.py,sha256=7dHPy2AWtKPeG3T_dn33p932mL7VmsKnQusCDPtvjFY,4989
-functime/forecasting/auto.py,sha256=wG4aIwLxLck31LLrKDwNLWevznJhh--aN4fo_-C-Ivs,9921
-functime/forecasting/forecasters.py,sha256=_POugsD_BTMNCiui17uN2uV9KtZH1llgwae-7bDcPf4,3908
-functime/io/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-functime/io/_serialize.py,sha256=C1gLl-KZMMKLICWepIjiYIv8-gnjYlfF_eTFuB0YW-Q,494
-functime/io/client.py,sha256=L8OiNUUCFgHddgfSty4sWzqvpI_2JAl8JdJeIghhOaA,5664
+functime/feature_extraction/calendar.py,sha256=yQwbo_27M2lyCqr9FxUKwEHC5_Kp75XB003HhEsnAmE,4064
+functime/forecasting/__init__.py,sha256=XWBiKZpNADvDUe-aaMtdppZt_8LvEYth7xzELJuy6Do,745
+functime/forecasting/_ar.py,sha256=dcqb7JxcNEdnB6VpoKNV8s_9Q1PR-yCMdLo1Clhbhnw,12183
+functime/forecasting/_evaluate.py,sha256=vwAntAvolOza_-U9e1UZlpHnUwldl8dFQTHgC1Vn1CQ,4798
+functime/forecasting/_reduction.py,sha256=FLYt6FXJCIcg5j60BiJNX8HvE8LHnkBdoc0BvYyDEtA,2231
+functime/forecasting/_regressors.py,sha256=sH7T8Ieafoj1Q4TVPKp6lIlbvo4mfc_lm9mezkB_Dn8,8481
+functime/forecasting/automl.py,sha256=4I3qNEjS35f7XlymJYZOjCS6rcjjIpTbNYBMrxSgDxE,8215
+functime/forecasting/catboost.py,sha256=nF8I4AxJH7qDIu2BUGE_l7w-guNt8iYuNKH9oy-N3tU,2175
+functime/forecasting/censored.py,sha256=rZjxId4AZlz3bkf0YnKp5fqR5S3kI9Q42Swl0gGFtaU,3714
+functime/forecasting/knn.py,sha256=Uai51RhAOPfluG8US5KmmyRAPMqU2HV2hcSW3smGS-Q,1034
+functime/forecasting/lance.py,sha256=LiEhbLrd6DpMoFhYjbjyPXWhHxNEdM_v_XdpPcgpwiU,3703
+functime/forecasting/lightgbm.py,sha256=b0nDwFQLFtyAB-0vbeL8JRehO_sszjJFm_PAOT5MFJY,4215
+functime/forecasting/linear.py,sha256=42vzUSPukIfXiyyW3krq8QQorvw--cHexjUaAmw7WP0,4018
+functime/forecasting/xgboost.py,sha256=kguM3IvqtJKSD3tysxgCiOe-NwApBew5tOO9_zxt4-I,2412
 functime/metrics/__init__.py,sha256=Bx-vU-jxkAg-zy2mVWYDtNOkluvgKJnL9uf0IhKIapw,229
 functime/metrics/multi_objective.py,sha256=EP9sGQ4HSCRN1SQazoJfntcovMP6RAVf7hZMXgcA2gE,3826
 functime/metrics/point.py,sha256=HlkcYvGAtFPr8kiTHQnoXZujysg2DK-BuUJVmLwiNKM,6810
 functime/metrics/probabilistic.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-functime-0.1.9.dist-info/LICENSE,sha256=hIahDEOTzuHCU5J2nd07LWwkLW7Hko4UFO__ffsvB-8,34523
-functime-0.1.9.dist-info/METADATA,sha256=78-Oi9fdOx2BcwI7ofHaCy-lvZ5GfyQWtpa7C33cszs,8290
-functime-0.1.9.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-functime-0.1.9.dist-info/entry_points.txt,sha256=y-9Na7pOh73f05jw87NkCt13P24wV_2qPEDF6ylwSXI,52
-functime-0.1.9.dist-info/top_level.txt,sha256=RUXkPSxtl5ViacwkIXqV7W8uyhA9yNRqrLMFS-jhFP4,9
-functime-0.1.9.dist-info/RECORD,,
+functime-0.2.0.dist-info/LICENSE,sha256=hIahDEOTzuHCU5J2nd07LWwkLW7Hko4UFO__ffsvB-8,34523
+functime-0.2.0.dist-info/METADATA,sha256=hdMMy1m4FDIKRxb-Pq-FGjXR_injPqr0-Rr2q-TyODs,8269
+functime-0.2.0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+functime-0.2.0.dist-info/entry_points.txt,sha256=y-9Na7pOh73f05jw87NkCt13P24wV_2qPEDF6ylwSXI,52
+functime-0.2.0.dist-info/top_level.txt,sha256=RUXkPSxtl5ViacwkIXqV7W8uyhA9yNRqrLMFS-jhFP4,9
+functime-0.2.0.dist-info/RECORD,,
```

