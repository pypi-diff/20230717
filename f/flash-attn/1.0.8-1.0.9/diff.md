# Comparing `tmp/flash_attn-1.0.8.tar.gz` & `tmp/flash_attn-1.0.9.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "flash_attn-1.0.8.tar", last modified: Mon Jul  3 00:07:02 2023, max compression
+gzip compressed data, was "flash_attn-1.0.9.tar", last modified: Mon Jul 17 10:19:08 2023, max compression
```

## Comparing `flash_attn-1.0.8.tar` & `flash_attn-1.0.9.tar`

### file list

```diff
@@ -1,1779 +1,1553 @@
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:07:02.822880 flash_attn-1.0.8/
--rw-r--r--   0 root         (0) root         (0)       56 2022-11-17 23:40:55.000000 flash_attn-1.0.8/AUTHORS
--rw-r--r--   0 root         (0) root         (0)     1558 2022-09-09 19:08:03.000000 flash_attn-1.0.8/LICENSE
--rw-r--r--   0 root         (0) root         (0)      251 2023-04-16 00:48:36.000000 flash_attn-1.0.8/MANIFEST.in
--rw-rw-r--   0 root         (0) root         (0)    10197 2023-07-03 00:07:02.813728 flash_attn-1.0.8/PKG-INFO
--rw-r--r--   0 root         (0) root         (0)     9689 2023-05-25 23:52:21.000000 flash_attn-1.0.8/README.md
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:47.186890 flash_attn-1.0.8/csrc/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:47.364979 flash_attn-1.0.8/csrc/flash_attn/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:46.934574 flash_attn-1.0.8/csrc/flash_attn/cutlass/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:47.391943 flash_attn-1.0.8/csrc/flash_attn/cutlass/cmake/
--rw-r--r--   0 root         (0) root         (0)     2023 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/cmake/nop.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:46.440221 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:47.422070 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/00_basic_gemm/
--rw-r--r--   0 root         (0) root         (0)    14698 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/00_basic_gemm/basic_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:47.446613 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/01_cutlass_utilities/
--rw-r--r--   0 root         (0) root         (0)    13255 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/01_cutlass_utilities/cutlass_utilities.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:47.472551 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/02_dump_reg_shmem/
--rw-r--r--   0 root         (0) root         (0)     7157 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/02_dump_reg_shmem/dump_reg_shmem.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:47.676587 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/03_visualize_layout/
--rw-r--r--   0 root         (0) root         (0)     4478 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/03_visualize_layout/options.h
--rw-r--r--   0 root         (0) root         (0)     7081 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/03_visualize_layout/register_layout.cu
--rw-r--r--   0 root         (0) root         (0)     2691 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/03_visualize_layout/register_layout.h
--rw-r--r--   0 root         (0) root         (0)     5819 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/03_visualize_layout/visualize_layout.cpp
--rw-r--r--   0 root         (0) root         (0)    11415 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/03_visualize_layout/visualize_layout.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:47.701931 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/04_tile_iterator/
--rw-r--r--   0 root         (0) root         (0)     8226 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/04_tile_iterator/tile_iterator.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:47.727393 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/05_batched_gemm/
--rw-r--r--   0 root         (0) root         (0)    15161 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/05_batched_gemm/batched_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:47.752138 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/06_splitK_gemm/
--rw-r--r--   0 root         (0) root         (0)    17570 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/06_splitK_gemm/splitk_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:47.777871 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/07_volta_tensorop_gemm/
--rw-r--r--   0 root         (0) root         (0)    18280 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/07_volta_tensorop_gemm/volta_tensorop_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:47.803394 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/08_turing_tensorop_gemm/
--rw-r--r--   0 root         (0) root         (0)    18226 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/08_turing_tensorop_gemm/turing_tensorop_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:47.828903 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/09_turing_tensorop_conv2dfprop/
--rw-r--r--   0 root         (0) root         (0)    28124 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/09_turing_tensorop_conv2dfprop/turing_tensorop_conv2dfprop.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:47.853744 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/10_planar_complex/
--rw-r--r--   0 root         (0) root         (0)    21947 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/10_planar_complex/planar_complex.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:47.880088 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/11_planar_complex_array/
--rw-r--r--   0 root         (0) root         (0)    23244 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/11_planar_complex_array/planar_complex_array.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:47.905698 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/12_gemm_bias_relu/
--rw-r--r--   0 root         (0) root         (0)    13151 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/12_gemm_bias_relu/gemm_bias_relu.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:49.839045 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/
--rw-r--r--   0 root         (0) root         (0)    26102 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_conv2d_run.h
--rw-r--r--   0 root         (0) root         (0)    22877 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_gemm_run.h
--rw-r--r--   0 root         (0) root         (0)    28268 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_conv2d_run.h
--rw-r--r--   0 root         (0) root         (0)    24493 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_gemm_run.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:49.881591 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/
--rw-r--r--   0 root         (0) root         (0)    15552 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/b2b_gemm.h
--rw-r--r--   0 root         (0) root         (0)    11520 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/b2b_implicit_gemm_convolution.h
--rw-r--r--   0 root         (0) root         (0)     8756 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_rf.cu
--rw-r--r--   0 root         (0) root         (0)     8759 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_shmem.cu
--rw-r--r--   0 root         (0) root         (0)     8712 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_rf.cu
--rw-r--r--   0 root         (0) root         (0)     8762 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_shmem.cu
--rw-r--r--   0 root         (0) root         (0)     8787 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_rf.cu
--rw-r--r--   0 root         (0) root         (0)     8793 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_shmem.cu
--rw-r--r--   0 root         (0) root         (0)     8711 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_rf.cu
--rw-r--r--   0 root         (0) root         (0)     8775 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_shmem.cu
--rw-r--r--   0 root         (0) root         (0)     7269 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_rf.cu
--rw-r--r--   0 root         (0) root         (0)     7338 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_shmem.cu
--rw-r--r--   0 root         (0) root         (0)     7294 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_rf.cu
--rw-r--r--   0 root         (0) root         (0)     7359 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_shmem.cu
--rw-r--r--   0 root         (0) root         (0)     7362 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_rf.cu
--rw-r--r--   0 root         (0) root         (0)     7430 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_shmem.cu
--rw-r--r--   0 root         (0) root         (0)     7627 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_rf.cu
--rw-r--r--   0 root         (0) root         (0)     7634 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_shmem.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:50.158186 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/
--rw-r--r--   0 root         (0) root         (0)    16152 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_gemm.h
--rw-r--r--   0 root         (0) root         (0)    18151 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_implicit_gemm_convolution.h
--rw-r--r--   0 root         (0) root         (0)     3973 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop.h
--rw-r--r--   0 root         (0) root         (0)    26762 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm75.h
--rw-r--r--   0 root         (0) root         (0)    26775 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm80.h
--rw-r--r--   0 root         (0) root         (0)    28422 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm75.h
--rw-r--r--   0 root         (0) root         (0)    28073 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm80.h
--rw-r--r--   0 root         (0) root         (0)    17111 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm.h
--rw-r--r--   0 root         (0) root         (0)    15658 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm_smem_accumulator.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:46.127038 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/reference/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:50.187960 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/reference/device/
--rw-r--r--   0 root         (0) root         (0)    10368 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/reference/device/tensor_scale_bias.h
--rw-r--r--   0 root         (0) root         (0)     3577 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/test_run.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:50.426605 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/
--rw-r--r--   0 root         (0) root         (0)    31616 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage.h
--rw-r--r--   0 root         (0) root         (0)    31443 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage_smem_accumulator.h
--rw-r--r--   0 root         (0) root         (0)    21010 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined.h
--rw-r--r--   0 root         (0) root         (0)    20493 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined_smem_accumulator.h
--rw-r--r--   0 root         (0) root         (0)     7983 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base.h
--rw-r--r--   0 root         (0) root         (0)     6047 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base_smem_accumulator.h
--rw-r--r--   0 root         (0) root         (0)    33788 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage.h
--rw-r--r--   0 root         (0) root         (0)    33506 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage_smem_accumulator.h
--rw-r--r--   0 root         (0) root         (0)    21451 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined.h
--rw-r--r--   0 root         (0) root         (0)    21065 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined_smem_accumulator.h
--rw-r--r--   0 root         (0) root         (0)    27144 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma.h
--rw-r--r--   0 root         (0) root         (0)    27400 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma_smem_accumulator.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:50.451232 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/14_ampere_tf32_tensorop_gemm/
--rw-r--r--   0 root         (0) root         (0)    18020 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/14_ampere_tf32_tensorop_gemm/ampere_tf32_tensorop_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:50.478334 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/15_ampere_sparse_tensorop_gemm/
--rw-r--r--   0 root         (0) root         (0)    15042 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/15_ampere_sparse_tensorop_gemm/ampere_sparse_tensorop_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:50.503340 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/16_ampere_tensorop_conv2dfprop/
--rw-r--r--   0 root         (0) root         (0)    27755 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/16_ampere_tensorop_conv2dfprop/ampere_tensorop_conv2dfprop.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:50.530337 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/17_fprop_per_channel_bias/
--rw-r--r--   0 root         (0) root         (0)    12580 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/17_fprop_per_channel_bias/fprop_per_channel_bias.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:50.556228 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/18_ampere_fp64_tensorop_affine2_gemm/
--rw-r--r--   0 root         (0) root         (0)    14007 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/18_ampere_fp64_tensorop_affine2_gemm/ampere_fp64_tensorop_affine2_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:50.582754 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/19_tensorop_canonical/
--rw-r--r--   0 root         (0) root         (0)    13401 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/19_tensorop_canonical/tensorop_canonical.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:50.609309 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/20_simt_canonical/
--rw-r--r--   0 root         (0) root         (0)    12556 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/20_simt_canonical/simt_canonical.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:50.633947 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/21_quaternion_gemm/
--rw-r--r--   0 root         (0) root         (0)    17319 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/21_quaternion_gemm/quaternion_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:50.699038 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/22_quaternion_conv/
--rw-r--r--   0 root         (0) root         (0)    21495 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/22_quaternion_conv/quaternion_conv.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:50.838837 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/23_ampere_gemm_operand_reduction_fusion/
--rw-r--r--   0 root         (0) root         (0)    27530 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/23_ampere_gemm_operand_reduction_fusion/ampere_gemm_operand_reduction_fusion.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:50.865143 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/24_gemm_grouped/
--rw-r--r--   0 root         (0) root         (0)    50996 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/24_gemm_grouped/gemm_grouped.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:50.910192 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/
--rw-r--r--   0 root         (0) root         (0)    26547 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_3d_fprop_mainloop_fusion.cu
--rw-r--r--   0 root         (0) root         (0)    25628 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_fprop_mainloop_fusion.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:50.934886 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/26_ampere_wgrad_mainloop_fusion/
--rw-r--r--   0 root         (0) root         (0)    25538 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/26_ampere_wgrad_mainloop_fusion/ampere_wgrad_mainloop_fusion.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:50.960638 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/
--rw-r--r--   0 root         (0) root         (0)    30446 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:50.987047 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/
--rw-r--r--   0 root         (0) root         (0)    28159 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/ampere_3xtf32_fast_accurate_tensorop_fprop.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:51.030661 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/
--rw-r--r--   0 root         (0) root         (0)    28382 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_3xtf32_complex_gemm.cu
--rw-r--r--   0 root         (0) root         (0)    28403 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:51.056375 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/30_wgrad_split_k/
--rw-r--r--   0 root         (0) root         (0)    27329 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/30_wgrad_split_k/30_wgrad_split_k.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:51.081009 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/31_basic_syrk/
--rw-r--r--   0 root         (0) root         (0)    15206 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/31_basic_syrk/basic_syrk.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:51.106835 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/32_basic_trmm/
--rw-r--r--   0 root         (0) root         (0)    15907 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/32_basic_trmm/basic_trmm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:51.133017 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/33_ampere_3xtf32_tensorop_symm/
--rw-r--r--   0 root         (0) root         (0)    31803 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/33_ampere_3xtf32_tensorop_symm/ampere_3xtf32_tensorop_symm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:51.158933 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/34_transposed_conv2d/
--rw-r--r--   0 root         (0) root         (0)    22378 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/34_transposed_conv2d/34_transposed_conv2d.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:52.305663 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/35_gemm_softmax/
--rw-r--r--   0 root         (0) root         (0)    23114 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_softmax.cu
--rw-r--r--   0 root         (0) root         (0)    16723 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_with_epilogue_visitor.h
--rw-r--r--   0 root         (0) root         (0)    18713 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_with_softmax.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:52.332304 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/36_gather_scatter_fusion/
--rw-r--r--   0 root         (0) root         (0)    20795 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/36_gather_scatter_fusion/gather_scatter_fusion.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:52.396052 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/
--rw-r--r--   0 root         (0) root         (0)    31111 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu
--rw-r--r--   0 root         (0) root         (0)    13982 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h
--rw-r--r--   0 root         (0) root         (0)    33916 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:52.423236 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/38_syr2k_grouped/
--rw-r--r--   0 root         (0) root         (0)    47455 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/38_syr2k_grouped/syr2k_grouped.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:52.489975 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/39_gemm_permute/
--rw-r--r--   0 root         (0) root         (0)    37896 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/39_gemm_permute/gemm_permute.cu
--rw-r--r--   0 root         (0) root         (0)    15309 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/39_gemm_permute/layouts.h
--rw-r--r--   0 root         (0) root         (0)    11985 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/39_gemm_permute/permute_info.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:54.380143 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/
--rw-r--r--   0 root         (0) root         (0)    18389 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/attention_scaling_coefs_updater.h
--rw-r--r--   0 root         (0) root         (0)     8286 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/debug_utils.h
--rw-r--r--   0 root         (0) root         (0)     9888 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/default_fmha_grouped.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:54.452729 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/
--rw-r--r--   0 root         (0) root         (0)    22349 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_pipelined.h
--rw-r--r--   0 root         (0) root         (0)     9162 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_rescale_output.h
--rw-r--r--   0 root         (0) root         (0)     6111 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_thread_apply_logsumexp.h
--rw-r--r--   0 root         (0) root         (0)    22349 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_pipelined.h
--rw-r--r--   0 root         (0) root         (0)     9162 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_rescale_output.h
--rw-r--r--   0 root         (0) root         (0)     6111 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_thread_apply_logsumexp.h
--rw-r--r--   0 root         (0) root         (0)     6768 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/find_default_mma.h
--rw-r--r--   0 root         (0) root         (0)    29972 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h
--rw-r--r--   0 root         (0) root         (0)     6666 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped_problem_visitor.h
--rw-r--r--   0 root         (0) root         (0)    11078 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multi_head_attention_backward.cu
--rw-r--r--   0 root         (0) root         (0)    37104 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu
--rw-r--r--   0 root         (0) root         (0)    39975 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:54.595244 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/
--rw-r--r--   0 root         (0) root         (0)     3994 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma.h
--rw-r--r--   0 root         (0) root         (0)     6241 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_base.h
--rw-r--r--   0 root         (0) root         (0)    27198 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_multistage.h
--rw-r--r--   0 root         (0) root         (0)    14090 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_pipelined.h
--rw-r--r--   0 root         (0) root         (0)     6782 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/find_default_mma.h
--rw-r--r--   0 root         (0) root         (0)    13959 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/mma_accum_lambda_iterator.h
--rw-r--r--   0 root         (0) root         (0)    72983 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/mma_from_smem.h
--rw-r--r--   0 root         (0) root         (0)    12089 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm_kernel_utils.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:54.745631 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/
--rw-r--r--   0 root         (0) root         (0)    23805 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/epilogue_predicated_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)     3142 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/make_residual_last.h
--rw-r--r--   0 root         (0) root         (0)    64480 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_access_iterator_residual_last.h
--rw-r--r--   0 root         (0) root         (0)    64500 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_iterator_residual_last.h
--rw-r--r--   0 root         (0) root         (0)     2435 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/transpose_warp_iterator.h
--rw-r--r--   0 root         (0) root         (0)     9497 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/warp_iterator_from_smem.h
--rw-r--r--   0 root         (0) root         (0)    86943 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/kernel_backward.h
--rw-r--r--   0 root         (0) root         (0)    37905 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h
--rw-r--r--   0 root         (0) root         (0)    61195 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:54.771181 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/transform/
--rw-r--r--   0 root         (0) root         (0)     3747 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/transform/tile_smem_loader.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:54.837363 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_multi_head_attention/
--rw-r--r--   0 root         (0) root         (0)    37396 2023-04-12 16:23:45.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_multi_head_attention/fused_multihead_attention.cu
--rw-r--r--   0 root         (0) root         (0)    17839 2023-04-12 16:23:45.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_multi_head_attention/gemm_attention.h
--rw-r--r--   0 root         (0) root         (0)    16152 2023-04-12 16:23:45.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_multi_head_attention/gemm_grouped_with_softmax_visitor.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:55.014012 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/42_ampere_tensorop_group_conv/
--rw-r--r--   0 root         (0) root         (0)    23901 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/42_ampere_tensorop_group_conv/ampere_tensorop_group_conv.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:55.039720 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/43_ell_block_sparse_gemm/
--rw-r--r--   0 root         (0) root         (0)    23867 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/43_ell_block_sparse_gemm/ell_block_sparse_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:55.080922 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:46.351920 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:46.340565 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:55.170653 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/
--rw-r--r--   0 root         (0) root         (0)     6370 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_bias_act_epilogue_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     4099 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_thread_map_tensor_op_for_fused_bias.h
--rw-r--r--   0 root         (0) root         (0)     8285 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/fused_bias_act_epilogue.h
--rw-r--r--   0 root         (0) root         (0)    10439 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/output_tile_thread_map_for_fused_bias.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:55.196679 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/warp/
--rw-r--r--   0 root         (0) root         (0)     6848 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/warp/fused_bias_act_fragment_iterator_tensor_op.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:46.358012 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:55.225835 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/warp/
--rw-r--r--   0 root         (0) root         (0)    14747 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/warp/mma_tensor_op_fragment_iterator_without_output_op.h
--rw-r--r--   0 root         (0) root         (0)    10231 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/leaky_bias.h
--rw-r--r--   0 root         (0) root         (0)     3745 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/utils.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:55.306198 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/45_dual_gemm/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:55.330605 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/45_dual_gemm/device/
--rw-r--r--   0 root         (0) root         (0)    15362 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/45_dual_gemm/device/dual_gemm.h
--rw-r--r--   0 root         (0) root         (0)     8109 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm.cu
--rw-r--r--   0 root         (0) root         (0)     2366 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm_common.h
--rw-r--r--   0 root         (0) root         (0)    26478 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm_run.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:55.355150 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/45_dual_gemm/kernel/
--rw-r--r--   0 root         (0) root         (0)    16424 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/45_dual_gemm/kernel/dual_gemm.h
--rw-r--r--   0 root         (0) root         (0)     3577 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/45_dual_gemm/test_run.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:55.382165 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/45_dual_gemm/thread/
--rw-r--r--   0 root         (0) root         (0)     5818 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/45_dual_gemm/thread/left_silu_and_mul.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:55.448697 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/
--rw-r--r--   0 root         (0) root         (0)    15613 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_epilogue.h
--rw-r--r--   0 root         (0) root         (0)     7264 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_base.h
--rw-r--r--   0 root         (0) root         (0)    28984 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_multistage.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:55.573164 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/46_depthwise_simt_conv2dfprop/
--rw-r--r--   0 root         (0) root         (0)    24464 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/46_depthwise_simt_conv2dfprop/depthwise_simt_conv2dfprop.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:55.598020 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/47_ampere_gemm_universal_streamk/
--rw-r--r--   0 root         (0) root         (0)    22677 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/47_ampere_gemm_universal_streamk/ampere_gemm_universal_streamk.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:55.622859 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/48_hopper_warp_specialized_gemm/
--rw-r--r--   0 root         (0) root         (0)    16736 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/48_hopper_warp_specialized_gemm/48_hopper_warp_specialized_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:55.663643 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/49_hopper_gemm_schedules_with_collective_builder/
--rw-r--r--   0 root         (0) root         (0)    22631 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/49_hopper_gemm_schedules_with_collective_builder/49_hopper_gemm_schedules_with_collective_builder.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:55.689641 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/49_hopper_gemm_with_collective_builder/
--rw-r--r--   0 root         (0) root         (0)    24957 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/49_hopper_gemm_with_collective_builder/49_collective_builder.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:55.717989 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/50_hopper_gemm_with_epilogue_swizzle/
--rw-r--r--   0 root         (0) root         (0)    18635 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/50_hopper_gemm_with_epilogue_swizzle/50_hopper_gemm_with_epilogue_swizzle.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:55.859320 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/51_hopper_gett/
--rw-r--r--   0 root         (0) root         (0)    17256 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/51_hopper_gett/51_hopper_gett.cu
--rw-r--r--   0 root         (0) root         (0)     5572 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/51_hopper_gett/gett_kernel.cuh
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:55.885904 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/60_cutlass_import/
--rw-r--r--   0 root         (0) root         (0)     2849 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/60_cutlass_import/main.cpp
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:56.008381 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/common/
--rw-r--r--   0 root         (0) root         (0)     2621 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/common/helper.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:46.446286 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/cute/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:56.211620 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/cute/tutorial/
--rw-r--r--   0 root         (0) root         (0)    14342 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/cute/tutorial/sgemm_nt_1.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:46.458689 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:58.653511 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/
--rw-r--r--   0 root         (0) root         (0)     3793 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/aligned_buffer.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:00.424946 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/
--rw-r--r--   0 root         (0) root         (0)     3538 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/arch.h
--rw-r--r--   0 root         (0) root         (0)    12127 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/barrier.h
--rw-r--r--   0 root         (0) root         (0)     2691 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/cache_operation.h
--rw-r--r--   0 root         (0) root         (0)    14313 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/memory.h
--rw-r--r--   0 root         (0) root         (0)    10490 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm75.h
--rw-r--r--   0 root         (0) root         (0)    15166 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm80.h
--rw-r--r--   0 root         (0) root         (0)     8073 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/mma.h
--rw-r--r--   0 root         (0) root         (0)    11096 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm50.h
--rw-r--r--   0 root         (0) root         (0)     7040 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm60.h
--rw-r--r--   0 root         (0) root         (0)     4193 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm61.h
--rw-r--r--   0 root         (0) root         (0)    16554 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm70.h
--rw-r--r--   0 root         (0) root         (0)    31682 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm75.h
--rw-r--r--   0 root         (0) root         (0)    55573 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm80.h
--rw-r--r--   0 root         (0) root         (0)     4430 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm90.h
--rw-r--r--   0 root         (0) root         (0)    43978 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sparse_sm80.h
--rw-r--r--   0 root         (0) root         (0)     2622 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/reg_reconfig.h
--rw-r--r--   0 root         (0) root         (0)     3998 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/simd.h
--rw-r--r--   0 root         (0) root         (0)     3656 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/simd_sm60.h
--rw-r--r--   0 root         (0) root         (0)     5102 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/simd_sm61.h
--rw-r--r--   0 root         (0) root         (0)     8473 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/wmma.h
--rw-r--r--   0 root         (0) root         (0)     5286 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm70.h
--rw-r--r--   0 root         (0) root         (0)     7746 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm72.h
--rw-r--r--   0 root         (0) root         (0)     7616 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm75.h
--rw-r--r--   0 root         (0) root         (0)    62709 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/array.h
--rw-r--r--   0 root         (0) root         (0)     3662 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/array_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)    13154 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/array_subbyte.h
--rw-r--r--   0 root         (0) root         (0)     6371 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/barrier.h
--rw-r--r--   0 root         (0) root         (0)    13371 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/bfloat16.h
--rw-r--r--   0 root         (0) root         (0)     6338 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/blas3.h
--rw-r--r--   0 root         (0) root         (0)     9372 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/block_striped.h
--rw-r--r--   0 root         (0) root         (0)    19422 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/complex.h
--rw-r--r--   0 root         (0) root         (0)    47943 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/constants.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:00.584087 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/
--rw-r--r--   0 root         (0) root         (0)    22725 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/conv2d_problem_size.h
--rw-r--r--   0 root         (0) root         (0)    16292 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/conv3d_problem_size.h
--rw-r--r--   0 root         (0) root         (0)     6664 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/convolution.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:00.939403 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/device/
--rw-r--r--   0 root         (0) root         (0)     9744 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/device/direct_convolution.h
--rw-r--r--   0 root         (0) root         (0)    12078 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/device/implicit_gemm_convolution.h
--rw-r--r--   0 root         (0) root         (0)    10044 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/device/implicit_gemm_convolution_fusion.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:02.345662 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/
--rw-r--r--   0 root         (0) root         (0)     7671 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d.h
--rw-r--r--   0 root         (0) root         (0)    53546 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_dgrad.h
--rw-r--r--   0 root         (0) root         (0)    56838 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop.h
--rw-r--r--   0 root         (0) root         (0)    11953 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_fusion.h
--rw-r--r--   0 root         (0) root         (0)     4658 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_broadcast.h
--rw-r--r--   0 root         (0) root         (0)     4660 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_reduction.h
--rw-r--r--   0 root         (0) root         (0)    15891 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_group_fprop.h
--rw-r--r--   0 root         (0) root         (0)    28745 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad.h
--rw-r--r--   0 root         (0) root         (0)    10459 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad_fusion.h
--rw-r--r--   0 root         (0) root         (0)     9324 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_dgrad.h
--rw-r--r--   0 root         (0) root         (0)    14864 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop.h
--rw-r--r--   0 root         (0) root         (0)    11980 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop_fusion.h
--rw-r--r--   0 root         (0) root         (0)    14883 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_wgrad.h
--rw-r--r--   0 root         (0) root         (0)    19294 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_depthwise_fprop.h
--rw-r--r--   0 root         (0) root         (0)    18048 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/direct_convolution.h
--rw-r--r--   0 root         (0) root         (0)    15454 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution.h
--rw-r--r--   0 root         (0) root         (0)    15709 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_fusion.h
--rw-r--r--   0 root         (0) root         (0)    17131 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_strided_dgrad.h
--rw-r--r--   0 root         (0) root         (0)    16749 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_with_fused_epilogue.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:02.373276 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/thread/
--rw-r--r--   0 root         (0) root         (0)     9689 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/thread/depthwise_mma.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:05.108392 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/
--rw-r--r--   0 root         (0) root         (0)    15306 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    19735 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)    18940 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    26137 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)    10953 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    11529 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_few_channels.h
--rw-r--r--   0 root         (0) root         (0)    11333 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_fixed_channels.h
--rw-r--r--   0 root         (0) root         (0)    13664 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)    10627 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)     9314 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_few_channels.h
--rw-r--r--   0 root         (0) root         (0)     9018 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_fixed_channels.h
--rw-r--r--   0 root         (0) root         (0)    10387 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)    30197 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_params.h
--rw-r--r--   0 root         (0) root         (0)    11202 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)    10350 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    11520 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)     9043 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    10832 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)     8450 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)     9569 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)    11020 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    15014 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)     9634 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    15132 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)     7945 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)     8891 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)    18249 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_params.h
--rw-r--r--   0 root         (0) root         (0)     9971 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    12024 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)     8821 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    10744 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)     8871 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_direct_conv_params.h
--rw-r--r--   0 root         (0) root         (0)    10747 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_fixed_stride_dilation.h
--rw-r--r--   0 root         (0) root         (0)     9899 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_optimized.h
--rw-r--r--   0 root         (0) root         (0)    20899 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_direct_conv_multistage.h
--rw-r--r--   0 root         (0) root         (0)     8921 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_filter_tile_access_iterator_direct_conv_optimized.h
--rw-r--r--   0 root         (0) root         (0)    12744 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_pipelined.h
--rw-r--r--   0 root         (0) root         (0)     8097 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_mma_base.h
--rw-r--r--   0 root         (0) root         (0)    36697 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_mma_core_with_lane_access_size.h
--rw-r--r--   0 root         (0) root         (0)    30106 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_fprop_fusion_multistage.h
--rw-r--r--   0 root         (0) root         (0)    20086 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_multistage.h
--rw-r--r--   0 root         (0) root         (0)    12174 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_pipelined.h
--rw-r--r--   0 root         (0) root         (0)    26320 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_wgrad_fusion_multistage.h
--rw-r--r--   0 root         (0) root         (0)    16915 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)    12476 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_iterator.h
--rw-r--r--   0 root         (0) root         (0)     8050 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/threadblock_swizzle.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:05.171113 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/warp/
--rw-r--r--   0 root         (0) root         (0)    12419 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/warp/mma_depthwise_simt.h
--rw-r--r--   0 root         (0) root         (0)    30655 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/warp/mma_depthwise_simt_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)     8772 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/warp/scale_bias_relu_transform.h
--rw-r--r--   0 root         (0) root         (0)    11827 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/coord.h
--rw-r--r--   0 root         (0) root         (0)    11077 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/core_io.h
--rw-r--r--   0 root         (0) root         (0)     7838 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/cutlass.h
--rw-r--r--   0 root         (0) root         (0)     3108 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/device_kernel.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:46.521520 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:06.685027 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/
--rw-r--r--   0 root         (0) root         (0)    18909 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/activation.h
--rw-r--r--   0 root         (0) root         (0)     4691 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/conversion_op.h
--rw-r--r--   0 root         (0) root         (0)     9349 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination.h
--rw-r--r--   0 root         (0) root         (0)     8344 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_elementwise.h
--rw-r--r--   0 root         (0) root         (0)    13490 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_relu.h
--rw-r--r--   0 root         (0) root         (0)    23649 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_clamp.h
--rw-r--r--   0 root         (0) root         (0)     9067 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_dgelu.h
--rw-r--r--   0 root         (0) root         (0)    15195 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_drelu.h
--rw-r--r--   0 root         (0) root         (0)     3669 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_gelu.h
--rw-r--r--   0 root         (0) root         (0)     8065 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_generic.h
--rw-r--r--   0 root         (0) root         (0)     3693 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_hardswish.h
--rw-r--r--   0 root         (0) root         (0)     8344 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_leaky_relu.h
--rw-r--r--   0 root         (0) root         (0)     3058 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_params.h
--rw-r--r--   0 root         (0) root         (0)     9351 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)    20486 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_relu.h
--rw-r--r--   0 root         (0) root         (0)    19348 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_relu0.h
--rw-r--r--   0 root         (0) root         (0)    11855 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_residual_block.h
--rw-r--r--   0 root         (0) root         (0)     3688 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_sigmoid.h
--rw-r--r--   0 root         (0) root         (0)     3669 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_silu.h
--rw-r--r--   0 root         (0) root         (0)     8662 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_with_elementwise.h
--rw-r--r--   0 root         (0) root         (0)     3416 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/reduction_op.h
--rw-r--r--   0 root         (0) root         (0)     2656 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/scale_type.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:11.628049 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/
--rw-r--r--   0 root         (0) root         (0)     9142 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     9441 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op_blas3.h
--rw-r--r--   0 root         (0) root         (0)     3234 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_direct_store.h
--rw-r--r--   0 root         (0) root         (0)     7209 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)    13385 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_simt.h
--rw-r--r--   0 root         (0) root         (0)    27150 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     7129 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op_blas3.h
--rw-r--r--   0 root         (0) root         (0)    10846 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     5817 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_broadcast.h
--rw-r--r--   0 root         (0) root         (0)     5763 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_reduction.h
--rw-r--r--   0 root         (0) root         (0)     5947 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     4409 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_simt.h
--rw-r--r--   0 root         (0) root         (0)     7398 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     7303 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_volta_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     4098 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_wmma_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     4678 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/direct_store_epilogue_iterator.h
--rw-r--r--   0 root         (0) root         (0)    19214 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue.h
--rw-r--r--   0 root         (0) root         (0)     8279 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_base.h
--rw-r--r--   0 root         (0) root         (0)     7455 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_base_streamk.h
--rw-r--r--   0 root         (0) root         (0)    13424 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_depthwise.h
--rw-r--r--   0 root         (0) root         (0)    13933 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_direct_store.h
--rw-r--r--   0 root         (0) root         (0)     7401 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_gemm_k_reduction.h
--rw-r--r--   0 root         (0) root         (0)    14610 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)     9073 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_smem_accumulator.h
--rw-r--r--   0 root         (0) root         (0)    16804 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_visitor_with_softmax.h
--rw-r--r--   0 root         (0) root         (0)    52430 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_broadcast.h
--rw-r--r--   0 root         (0) root         (0)    29199 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_reduction.h
--rw-r--r--   0 root         (0) root         (0)    13454 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h
--rw-r--r--   0 root         (0) root         (0)     7308 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_workspace.h
--rw-r--r--   0 root         (0) root         (0)    14359 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/interleaved_epilogue.h
--rw-rw-r--   0 root         (0) root         (0)     2912 2022-06-02 16:47:41.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/output_iterator_parameter.h
--rw-r--r--   0 root         (0) root         (0)    19750 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/output_tile_thread_map.h
--rw-r--r--   0 root         (0) root         (0)    40870 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)    18821 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine.h
--rw-r--r--   0 root         (0) root         (0)     5636 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine_layout_params.h
--rw-r--r--   0 root         (0) root         (0)    21249 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_blas3.h
--rw-r--r--   0 root         (0) root         (0)    13872 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_direct_conv.h
--rw-r--r--   0 root         (0) root         (0)    14496 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_params.h
--rw-r--r--   0 root         (0) root         (0)     9146 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_predicates.h
--rw-r--r--   0 root         (0) root         (0)    15536 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_strided_dgrad.h
--rw-r--r--   0 root         (0) root         (0)     7487 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator.h
--rw-r--r--   0 root         (0) root         (0)    17683 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_mixed.h
--rw-r--r--   0 root         (0) root         (0)     7394 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_pitch_liner.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:12.017845 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/
--rw-r--r--   0 root         (0) root         (0)     7055 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_complex_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     7736 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_gaussian_complex_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     5880 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_simt.h
--rw-r--r--   0 root         (0) root         (0)     9883 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     8924 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_volta_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     6045 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_wmma_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     4864 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/simt_policy.h
--rw-r--r--   0 root         (0) root         (0)     5979 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tensor_op_policy.h
--rw-r--r--   0 root         (0) root         (0)    25658 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_simt.h
--rw-r--r--   0 root         (0) root         (0)    20290 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    22857 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op_mixed.h
--rw-r--r--   0 root         (0) root         (0)    14258 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_volta_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     7704 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_wmma_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     7485 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/volta_tensor_op_policy.h
--rw-r--r--   0 root         (0) root         (0)     3916 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/wmma_tensor_op_policy.h
--rw-r--r--   0 root         (0) root         (0)    26026 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/fast_math.h
--rw-r--r--   0 root         (0) root         (0)    35325 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/float8.h
--rw-r--r--   0 root         (0) root         (0)     2645 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/floating_point_nvrtc.h
--rw-r--r--   0 root         (0) root         (0)    11242 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/functional.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:12.040682 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:13.314921 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/
--rw-r--r--   0 root         (0) root         (0)    17023 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/base_grouped.h
--rw-r--r--   0 root         (0) root         (0)    24413 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/default_gemm_configuration.h
--rw-r--r--   0 root         (0) root         (0)    27616 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/ell_gemm.h
--rw-r--r--   0 root         (0) root         (0)    25202 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm.h
--rw-r--r--   0 root         (0) root         (0)    22367 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_array.h
--rw-r--r--   0 root         (0) root         (0)    22375 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_batched.h
--rw-r--r--   0 root         (0) root         (0)    22725 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_complex.h
--rw-r--r--   0 root         (0) root         (0)     2591 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_grouped.h
--rw-r--r--   0 root         (0) root         (0)    13736 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_layernorm_mainloop_fusion.h
--rw-r--r--   0 root         (0) root         (0)    17329 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_sparse.h
--rw-r--r--   0 root         (0) root         (0)    20450 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_splitk_parallel.h
--rw-r--r--   0 root         (0) root         (0)    14902 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal.h
--rw-r--r--   0 root         (0) root         (0)     7444 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_adapter.h
--rw-r--r--   0 root         (0) root         (0)    13352 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_base.h
--rw-r--r--   0 root         (0) root         (0)    13968 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_with_broadcast.h
--rw-r--r--   0 root         (0) root         (0)    14853 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_with_k_reduction.h
--rw-r--r--   0 root         (0) root         (0)     5690 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemv.h
--rw-r--r--   0 root         (0) root         (0)    18127 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_2k.h
--rw-r--r--   0 root         (0) root         (0)     2747 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_2k_grouped.h
--rw-r--r--   0 root         (0) root         (0)    16719 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_k.h
--rwxr-xr-x   0 root         (0) root         (0)    21050 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/symm.h
--rw-r--r--   0 root         (0) root         (0)    26464 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/trmm.h
--rw-r--r--   0 root         (0) root         (0)    11570 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/gemm.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:14.712713 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/
--rw-r--r--   0 root         (0) root         (0)    29360 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_ell_gemm.h
--rw-r--r--   0 root         (0) root         (0)    37752 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm.h
--rw-r--r--   0 root         (0) root         (0)    16130 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_complex.h
--rw-r--r--   0 root         (0) root         (0)    12385 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped.h
--rw-r--r--   0 root         (0) root         (0)     6592 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped_softmax_mainloop_fusion.h
--rw-r--r--   0 root         (0) root         (0)     5848 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_layernorm_mainloop_fusion.h
--rw-r--r--   0 root         (0) root         (0)    11104 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_planar_complex_universal.h
--rw-r--r--   0 root         (0) root         (0)     7983 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_sparse.h
--rw-r--r--   0 root         (0) root         (0)     4932 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_splitk_parallel.h
--rw-r--r--   0 root         (0) root         (0)    11951 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_universal.h
--rw-r--r--   0 root         (0) root         (0)     8063 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_broadcast.h
--rw-r--r--   0 root         (0) root         (0)     6457 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_k_reduction.h
--rw-r--r--   0 root         (0) root         (0)     8086 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_reduction.h
--rwxr-xr-x   0 root         (0) root         (0)     5349 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemv.h
--rw-r--r--   0 root         (0) root         (0)    11560 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k.h
--rw-r--r--   0 root         (0) root         (0)    20509 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_complex.h
--rw-r--r--   0 root         (0) root         (0)    12470 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_grouped.h
--rw-r--r--   0 root         (0) root         (0)    10620 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_universal.h
--rw-r--r--   0 root         (0) root         (0)     9872 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k.h
--rw-r--r--   0 root         (0) root         (0)    16990 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k_complex.h
--rw-r--r--   0 root         (0) root         (0)     9444 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k_universal.h
--rwxr-xr-x   0 root         (0) root         (0)    13375 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm.h
--rwxr-xr-x   0 root         (0) root         (0)    21830 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm_complex.h
--rwxr-xr-x   0 root         (0) root         (0)    10315 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm_universal.h
--rw-r--r--   0 root         (0) root         (0)    10873 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm.h
--rw-r--r--   0 root         (0) root         (0)    10730 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm_complex.h
--rw-r--r--   0 root         (0) root         (0)    10850 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm_universal.h
--rw-r--r--   0 root         (0) root         (0)    28916 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/ell_gemm.h
--rw-r--r--   0 root         (0) root         (0)    13381 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm.h
--rw-r--r--   0 root         (0) root         (0)     8717 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_array.h
--rw-r--r--   0 root         (0) root         (0)     8785 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_batched.h
--rw-r--r--   0 root         (0) root         (0)    14711 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped.h
--rw-r--r--   0 root         (0) root         (0)     4691 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped_problem_visitor.h
--rw-r--r--   0 root         (0) root         (0)    15623 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped_softmax_mainloop_fusion.h
--rw-r--r--   0 root         (0) root         (0)    27281 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_layernorm_mainloop_fusion.h
--rwxr-xr-x   0 root         (0) root         (0)     6144 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_params.h
--rw-r--r--   0 root         (0) root         (0)     5165 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_pipelined.h
--rw-r--r--   0 root         (0) root         (0)    22973 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)    18961 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex_array.h
--rw-r--r--   0 root         (0) root         (0)     8142 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_splitk_parallel.h
--rw-r--r--   0 root         (0) root         (0)     4291 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_transpose_operands.h
--rw-r--r--   0 root         (0) root         (0)    22913 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_universal.h
--rw-r--r--   0 root         (0) root         (0)    41469 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_universal_streamk.h
--rw-r--r--   0 root         (0) root         (0)    47222 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_with_fused_epilogue.h
--rw-r--r--   0 root         (0) root         (0)    23629 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_with_k_reduction.h
--rw-r--r--   0 root         (0) root         (0)     8090 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemv.h
--rwxr-xr-x   0 root         (0) root         (0)     8979 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemv_batched_strided.h
--rw-r--r--   0 root         (0) root         (0)    16849 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/grouped_problem_visitor.h
--rw-r--r--   0 root         (0) root         (0)     7148 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/params_universal_base.h
--rw-r--r--   0 root         (0) root         (0)    22962 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped.h
--rw-r--r--   0 root         (0) root         (0)    16100 2023-04-15 15:41:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped_problem_visitor.h
--rw-r--r--   0 root         (0) root         (0)     4334 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_transpose_operands.h
--rw-r--r--   0 root         (0) root         (0)    24162 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_universal.h
--rw-r--r--   0 root         (0) root         (0)    17567 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_k_universal.h
--rw-r--r--   0 root         (0) root         (0)    13610 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/sparse_gemm.h
--rwxr-xr-x   0 root         (0) root         (0)    23900 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/symm_universal.h
--rw-r--r--   0 root         (0) root         (0)    19537 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/trmm_universal.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:14.794118 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/
--rw-r--r--   0 root         (0) root         (0)     3567 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma.h
--rw-r--r--   0 root         (0) root         (0)    15373 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm50.h
--rw-r--r--   0 root         (0) root         (0)    29987 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm60.h
--rw-r--r--   0 root         (0) root         (0)     8142 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm61.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:16.269248 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/
--rw-r--r--   0 root         (0) root         (0)    31930 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_ell_mma.h
--rwxr-xr-x   0 root         (0) root         (0)     6979 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_gemv_core.h
--rw-r--r--   0 root         (0) root         (0)    34241 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma.h
--rw-r--r--   0 root         (0) root         (0)     5123 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core.h
--rw-r--r--   0 root         (0) root         (0)    57426 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_simt.h
--rw-r--r--   0 root         (0) root         (0)    19257 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm70.h
--rw-r--r--   0 root         (0) root         (0)    42310 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm75.h
--rw-r--r--   0 root         (0) root         (0)   103000 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm80.h
--rw-r--r--   0 root         (0) root         (0)    32106 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h
--rw-r--r--   0 root         (0) root         (0)    12645 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_access_size.h
--rw-r--r--   0 root         (0) root         (0)     7387 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_reduction.h
--rw-r--r--   0 root         (0) root         (0)    20975 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_wmma.h
--rw-r--r--   0 root         (0) root         (0)     7998 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_layernorm_mainloop_fusion.h
--rw-r--r--   0 root         (0) root         (0)     5110 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_multistage.h
--rw-r--r--   0 root         (0) root         (0)     4627 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_pipelined.h
--rw-r--r--   0 root         (0) root         (0)     7113 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_softmax_mainloop_fusion.h
--rw-r--r--   0 root         (0) root         (0)     6323 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_with_reduction.h
--rw-r--r--   0 root         (0) root         (0)     7121 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex.h
--rw-r--r--   0 root         (0) root         (0)     4959 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core.h
--rw-r--r--   0 root         (0) root         (0)    65201 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core_sm80.h
--rw-r--r--   0 root         (0) root         (0)    25495 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_trmm_complex.h
--rw-r--r--   0 root         (0) root         (0)     8509 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_sparse_mma.h
--rw-r--r--   0 root         (0) root         (0)    19515 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_trmm.h
--rw-r--r--   0 root         (0) root         (0)    24047 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/ell_mma_multistage.h
--rw-r--r--   0 root         (0) root         (0)    13836 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/ell_mma_pipelined.h
--rwxr-xr-x   0 root         (0) root         (0)     4726 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/gemv.h
--rw-r--r--   0 root         (0) root         (0)     3652 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/index_remat.h
--rw-r--r--   0 root         (0) root         (0)     7823 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_base.h
--rw-r--r--   0 root         (0) root         (0)    27415 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_blas3_multistage.h
--rw-r--r--   0 root         (0) root         (0)    32894 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_layernorm_mainloop_fusion_multistage.h
--rw-r--r--   0 root         (0) root         (0)    28015 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_multistage.h
--rw-r--r--   0 root         (0) root         (0)    15995 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_pipelined.h
--rw-r--r--   0 root         (0) root         (0)     6901 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_base.h
--rw-r--r--   0 root         (0) root         (0)    22653 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_multistage.h
--rw-r--r--   0 root         (0) root         (0)    14746 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_pipelined.h
--rw-r--r--   0 root         (0) root         (0)     9864 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_singlestage.h
--rw-r--r--   0 root         (0) root         (0)    27061 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_softmax_mainloop_fusion_multistage.h
--rw-r--r--   0 root         (0) root         (0)     9210 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_sparse_base.h
--rw-r--r--   0 root         (0) root         (0)    25333 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_sparse_multistage.h
--rw-r--r--   0 root         (0) root         (0)    20473 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_with_reduction_multistage.h
--rw-r--r--   0 root         (0) root         (0)    15007 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle.h
--rw-r--r--   0 root         (0) root         (0)    26621 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle_streamk.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:18.272981 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/
--rw-r--r--   0 root         (0) root         (0)    20553 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_complex_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     6684 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_sparse_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     5160 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     9026 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op_sm80.h
--rw-r--r--   0 root         (0) root         (0)     4053 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_with_reduction_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     4685 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_wmma_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     5725 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/layernorm_scale_bias_transform.h
--rw-r--r--   0 root         (0) root         (0)     2619 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma.h
--rw-r--r--   0 root         (0) root         (0)    37705 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    23132 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_fast_f32.h
--rw-r--r--   0 root         (0) root         (0)    78615 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_tile_iterator_sm80.h
--rw-r--r--   0 root         (0) root         (0)    21205 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    14589 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op_tile_iterator_sm80.h
--rw-r--r--   0 root         (0) root         (0)     6144 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)     8446 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt.h
--rw-r--r--   0 root         (0) root         (0)     3079 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt_policy.h
--rw-r--r--   0 root         (0) root         (0)    59793 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)    11758 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_sparse_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    14407 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    15721 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_fast_f32.h
--rw-rw-r--   0 root         (0) root         (0)    18643 2022-06-02 16:47:41.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_fragment_iterator.h
--rw-r--r--   0 root         (0) root         (0)     2939 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_policy.h
--rw-r--r--   0 root         (0) root         (0)     8966 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_sm70.h
--rw-r--r--   0 root         (0) root         (0)    11017 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)   136033 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)    99649 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm70.h
--rw-r--r--   0 root         (0) root         (0)    75179 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm80.h
--rw-r--r--   0 root         (0) root         (0)    13151 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sparse.h
--rw-r--r--   0 root         (0) root         (0)    27101 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_wmma.h
--rw-r--r--   0 root         (0) root         (0)     7241 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_wmma.h
--rw-r--r--   0 root         (0) root         (0)    17271 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_with_reduction_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    19125 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/scale_bias_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)     4610 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/softmax_scale_bias_transform.h
--rw-r--r--   0 root         (0) root         (0)     8728 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/tile_iterator_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)    23615 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/half.h
--rw-r--r--   0 root         (0) root         (0)     6893 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/integer_subbyte.h
--rw-r--r--   0 root         (0) root         (0)     2801 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/kernel_launch.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:18.717763 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/layout/
--rw-r--r--   0 root         (0) root         (0)     3020 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/layout/layout.h
--rw-r--r--   0 root         (0) root         (0)    34712 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/layout/matrix.h
--rw-r--r--   0 root         (0) root         (0)     9133 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/layout/permute.h
--rw-r--r--   0 root         (0) root         (0)     4696 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/layout/pitch_linear.h
--rw-r--r--   0 root         (0) root         (0)    18295 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/layout/tensor.h
--rw-r--r--   0 root         (0) root         (0)    29599 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm70.h
--rw-r--r--   0 root         (0) root         (0)    33137 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm75.h
--rw-r--r--   0 root         (0) root         (0)    29336 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm80.h
--rw-r--r--   0 root         (0) root         (0)     3328 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/layout/vector.h
--rw-r--r--   0 root         (0) root         (0)   364115 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/matrix.h
--rw-r--r--   0 root         (0) root         (0)     4991 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/matrix_coord.h
--rw-r--r--   0 root         (0) root         (0)     2726 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/matrix_shape.h
--rw-r--r--   0 root         (0) root         (0)    71278 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/numeric_conversion.h
--rw-r--r--   0 root         (0) root         (0)     3505 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/numeric_types.h
--rw-r--r--   0 root         (0) root         (0)     5492 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/pitch_linear_coord.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:18.743859 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/platform/
--rw-r--r--   0 root         (0) root         (0)    26097 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/platform/platform.h
--rw-r--r--   0 root         (0) root         (0)    15565 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/predicate_vector.h
--rw-r--r--   0 root         (0) root         (0)    20901 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/quaternion.h
--rw-r--r--   0 root         (0) root         (0)     2369 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/real.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:18.767143 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/reduction/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:18.850768 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/reduction/device/
--rw-r--r--   0 root         (0) root         (0)     6823 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/reduction/device/reduce_split_k.h
--rw-r--r--   0 root         (0) root         (0)     8152 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce.h
--rw-r--r--   0 root         (0) root         (0)    11579 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_contiguous.h
--rw-r--r--   0 root         (0) root         (0)    11448 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_strided.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:19.147623 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/
--rw-r--r--   0 root         (0) root         (0)     8762 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/reduce_softmax_final.h
--rw-r--r--   0 root         (0) root         (0)     7897 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/reduce_split_k.h
--rw-r--r--   0 root         (0) root         (0)    20685 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_contiguous.h
--rw-r--r--   0 root         (0) root         (0)    21662 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_strided.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:19.193496 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/
--rw-r--r--   0 root         (0) root         (0)     7208 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/reduce.h
--rw-r--r--   0 root         (0) root         (0)     6790 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/reduction_operators.h
--rw-r--r--   0 root         (0) root         (0)     2936 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/reduction/threadblock_swizzle.h
--rw-r--r--   0 root         (0) root         (0)     5929 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/relatively_equal.h
--rw-r--r--   0 root         (0) root         (0)     4186 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/semaphore.h
--rw-r--r--   0 root         (0) root         (0)    17243 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/subbyte_reference.h
--rw-r--r--   0 root         (0) root         (0)     8964 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/tensor_coord.h
--rw-r--r--   0 root         (0) root         (0)    12207 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/tensor_ref.h
--rw-r--r--   0 root         (0) root         (0)    11201 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/tensor_ref_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)     9509 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/tensor_view.h
--rw-r--r--   0 root         (0) root         (0)    10250 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/tensor_view_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)    13017 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/tfloat32.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:19.218919 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/thread/
--rw-r--r--   0 root         (0) root         (0)     5931 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/thread/matrix.h
--rw-r--r--   0 root         (0) root         (0)     2581 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/trace.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:19.241321 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/
--rw-r--r--   0 root         (0) root         (0)    33392 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/pitch_linear_thread_map.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:19.285308 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/thread/
--rw-r--r--   0 root         (0) root         (0)     3835 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/thread/transpose.h
--rw-r--r--   0 root         (0) root         (0)     4309 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/thread/unary_op.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:20.044494 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/
--rw-r--r--   0 root         (0) root         (0)     6181 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_iterator.h
--rw-r--r--   0 root         (0) root         (0)    44443 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)    44309 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)    12890 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)    11097 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_iterator.h
--rw-r--r--   0 root         (0) root         (0)    70684 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)    28232 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_2dthreadtile.h
--rwxr-xr-x   0 root         (0) root         (0)    10243 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_params.h
--rw-r--r--   0 root         (0) root         (0)    31412 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_triangular_matrix.h
--rw-r--r--   0 root         (0) root         (0)    62672 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)    27175 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h
--rw-r--r--   0 root         (0) root         (0)    28064 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_triangular_matrix.h
--rw-r--r--   0 root         (0) root         (0)    13088 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_vector_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)     8232 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_scale_bias_vector_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)     2638 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)    13283 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear.h
--rw-r--r--   0 root         (0) root         (0)    18623 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear_direct_conv.h
--rw-r--r--   0 root         (0) root         (0)    27922 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    47789 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op_sm80.h
--rw-r--r--   0 root         (0) root         (0)     2616 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)    16510 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear.h
--rw-r--r--   0 root         (0) root         (0)    15486 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear_2dthreadtile.h
--rw-r--r--   0 root         (0) root         (0)    36050 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    43663 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op_sm70.h
--rw-r--r--   0 root         (0) root         (0)     5226 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/vector_iterator.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:20.069904 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/warp/
--rw-r--r--   0 root         (0) root         (0)     8828 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/warp/vector_fragment_iterator.h
--rw-r--r--   0 root         (0) root         (0)     8166 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/uint128.h
--rw-r--r--   0 root         (0) root         (0)     3359 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/wmma_array.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:46.630577 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:46.636600 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:20.158595 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/
--rw-r--r--   0 root         (0) root         (0)     2788 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/compiler.h
--rw-r--r--   0 root         (0) root         (0)     6233 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/cutlass_bindings.cpp
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:21.353659 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/
--rw-r--r--   0 root         (0) root         (0)     2854 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/arch.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:21.414413 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/conv/
--rw-r--r--   0 root         (0) root         (0)     5897 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/conv/conv_problem_size.h
--rw-r--r--   0 root         (0) root         (0)     4763 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/conv/convolution.h
--rw-r--r--   0 root         (0) root         (0)     2650 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/conv/host.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:21.457426 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/
--rw-r--r--   0 root         (0) root         (0)     6845 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_generic.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:21.690920 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/
--rw-r--r--   0 root         (0) root         (0)     3003 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/binary_ops.h
--rw-r--r--   0 root         (0) root         (0)     6103 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/unary_ops.h
--rw-r--r--   0 root         (0) root         (0)     4698 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_accumulator.h
--rw-r--r--   0 root         (0) root         (0)     8397 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_binary.h
--rw-r--r--   0 root         (0) root         (0)     8830 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_broadcast.h
--rw-r--r--   0 root         (0) root         (0)    12998 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_reduction.h
--rw-r--r--   0 root         (0) root         (0)     9454 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_linear_combination.h
--rw-r--r--   0 root         (0) root         (0)     9085 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_broadcast.h
--rw-r--r--   0 root         (0) root         (0)    11975 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_reduction.h
--rw-r--r--   0 root         (0) root         (0)     6177 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_input.h
--rw-r--r--   0 root         (0) root         (0)     8017 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_output.h
--rw-r--r--   0 root         (0) root         (0)     7201 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_unary.h
--rw-r--r--   0 root         (0) root         (0)    16970 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_with_layernorm.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:21.755028 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/gemm/
--rw-r--r--   0 root         (0) root         (0)     3674 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/gemm/gemm.h
--rw-r--r--   0 root         (0) root         (0)    21710 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/gemm/gemm_universal_with_visitor.h
--rw-r--r--   0 root         (0) root         (0)     2328 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/gemm/host.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:21.817172 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/layout/
--rw-r--r--   0 root         (0) root         (0)     2115 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/layout/layout.h
--rw-r--r--   0 root         (0) root         (0)     4337 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/layout/matrix.h
--rw-r--r--   0 root         (0) root         (0)     3694 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/layout/tensor.h
--rw-r--r--   0 root         (0) root         (0)     9039 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/swizzling.h
--rw-r--r--   0 root         (0) root         (0)     3902 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/tensor_coord.h
--rw-r--r--   0 root         (0) root         (0)     5543 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/tensor_ref_view.h
--rw-r--r--   0 root         (0) root         (0)     4855 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/types.h
--rw-r--r--   0 root         (0) root         (0)      811 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/library.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:46.688528 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/test/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:21.944663 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/test/conv/
--rw-r--r--   0 root         (0) root         (0)     2651 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/test/conv/conv_problems.h
--rw-r--r--   0 root         (0) root         (0)     2253 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/test/conv/convolution.h
--rw-r--r--   0 root         (0) root         (0)     8786 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/test/conv/host.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:21.987438 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/test/gemm/
--rw-r--r--   0 root         (0) root         (0)     2139 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/test/gemm/gemm.h
--rw-r--r--   0 root         (0) root         (0)    18930 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/test/gemm/host.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:46.705844 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:22.013867 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:22.047573 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cluster_launch/
--rw-r--r--   0 root         (0) root         (0)    12088 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cluster_launch/cluster_launch.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:22.225931 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/common/
--rw-r--r--   0 root         (0) root         (0)     4273 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/common/cutlass_unit_test.h
--rw-r--r--   0 root         (0) root         (0)     4341 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/common/filter_architecture.cpp
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:46.730329 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:23.965834 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/
--rw-r--r--   0 root         (0) root         (0)    21797 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/cache_testbed_output.h
--rw-r--r--   0 root         (0) root         (0)     5344 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     5443 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    11470 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5239 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     9110 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     8485 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5243 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5378 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12054 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_few_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9603 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_fixed_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5267 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     5357 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5089 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
--rw-r--r--   0 root         (0) root         (0)    13690 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5390 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5191 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    11136 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     5291 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3551 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     5157 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu
--rwxr-xr-x   0 root         (0) root         (0)     8278 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_qf32nhwc_qf32nhwc_qf32nhwc_simt_f32_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    20555 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    20647 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5155 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     5239 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    26114 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    26210 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5111 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     5194 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5738 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5439 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7363 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     3984 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_reduction_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    39452 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_problems.h
--rw-r--r--   0 root         (0) root         (0)    14471 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4662 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    26224 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_testbed.h
--rw-r--r--   0 root         (0) root         (0)    21174 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_testbed_interleaved.h
--rw-r--r--   0 root         (0) root         (0)     5179 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     5358 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5264 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3615 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7591 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    10514 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5157 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5772 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    23460 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_with_broadcast_testbed.h
--rw-r--r--   0 root         (0) root         (0)    21512 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_with_reduction_testbed.h
--rw-r--r--   0 root         (0) root         (0)     5135 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5347 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3736 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     6560 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5257 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12276 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_problems.h
--rw-r--r--   0 root         (0) root         (0)    21643 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_testbed.h
--rw-r--r--   0 root         (0) root         (0)     3622 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     6560 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5256 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    17700 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_direct_conv_testbed.h
--rw-r--r--   0 root         (0) root         (0)    18437 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
--rw-r--r--   0 root         (0) root         (0)    22194 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_fixed_stride_dilation_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
--rw-r--r--   0 root         (0) root         (0)     9383 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
--rw-r--r--   0 root         (0) root         (0)     9387 2023-04-12 16:23:45.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
--rw-r--r--   0 root         (0) root         (0)    16100 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/group_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:24.262030 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/
--rw-r--r--   0 root         (0) root         (0)     7365 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/array.cu
--rw-r--r--   0 root         (0) root         (0)     7353 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/bfloat16.cu
--rw-r--r--   0 root         (0) root         (0)     6981 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/complex.cu
--rw-r--r--   0 root         (0) root         (0)     4009 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/float8.cu
--rw-r--r--   0 root         (0) root         (0)    13001 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/functional.cu
--rw-r--r--   0 root         (0) root         (0)     3553 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/half.cu
--rw-r--r--   0 root         (0) root         (0)     5295 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/matrix.cu
--rw-r--r--   0 root         (0) root         (0)     8592 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/matrix_coord.cu
--rw-r--r--   0 root         (0) root         (0)    11508 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/numeric_conversion.cu
--rw-r--r--   0 root         (0) root         (0)     8148 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/predicate_vector.cu
--rw-r--r--   0 root         (0) root         (0)     5777 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/quaternion.cu
--rw-r--r--   0 root         (0) root         (0)     6746 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/tensor_ref.cu
--rw-r--r--   0 root         (0) root         (0)     8885 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/tensor_view.cu
--rw-r--r--   0 root         (0) root         (0)     2050 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/test_unit_core.cpp
--rw-r--r--   0 root         (0) root         (0)     7088 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/tfloat32.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:46.768020 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:24.545967 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/ampere/
--rw-r--r--   0 root         (0) root         (0)     3527 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/ampere/cp_async.cu
--rw-r--r--   0 root         (0) root         (0)    14320 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/ampere/ldsm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:25.049394 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/core/
--rw-r--r--   0 root         (0) root         (0)     3838 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/core/array_subbyte.cpp
--rw-r--r--   0 root         (0) root         (0)     3332 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/core/bitfield.cpp
--rw-r--r--   0 root         (0) root         (0)     4861 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/core/coalesce.cpp
--rw-r--r--   0 root         (0) root         (0)     8195 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/core/compact_xmajor.cpp
--rw-r--r--   0 root         (0) root         (0)     5620 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/core/compare.cpp
--rw-r--r--   0 root         (0) root         (0)     7178 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/core/complement.cpp
--rw-r--r--   0 root         (0) root         (0)    12569 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/core/composition.cpp
--rw-r--r--   0 root         (0) root         (0)     4856 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/core/inverse_left.cpp
--rw-r--r--   0 root         (0) root         (0)     6702 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/core/inverse_right.cpp
--rw-r--r--   0 root         (0) root         (0)     6734 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/core/logical_divide.cpp
--rw-r--r--   0 root         (0) root         (0)     5914 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/core/logical_product.cpp
--rw-r--r--   0 root         (0) root         (0)     3488 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/core/mixedbits.cpp
--rw-r--r--   0 root         (0) root         (0)     2342 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/core/transform.cpp
--rw-r--r--   0 root         (0) root         (0)    13304 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/core/tuple.cpp
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:25.154434 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/hopper/
--rw-r--r--   0 root         (0) root         (0)     7030 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/hopper/bulk_load.cu
--rw-r--r--   0 root         (0) root         (0)     6407 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/hopper/bulk_store.cu
--rw-r--r--   0 root         (0) root         (0)    14365 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/hopper/stsm.cu
--rw-r--r--   0 root         (0) root         (0)    18990 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/hopper/tma_load.cu
--rw-r--r--   0 root         (0) root         (0)    13875 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/hopper/tma_store.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:25.180618 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/layout/
--rw-r--r--   0 root         (0) root         (0)     4544 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/layout/layout_operator.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:25.207514 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/msvc_compilation/
--rw-r--r--   0 root         (0) root         (0)     6533 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/msvc_compilation/tuple.cpp
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:46.790156 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:25.273577 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/thread/
--rw-r--r--   0 root         (0) root         (0)    15818 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/thread/activation.cu
--rw-r--r--   0 root         (0) root         (0)     6534 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination.cu
--rw-r--r--   0 root         (0) root         (0)     9964 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination_planar_complex.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:25.804313 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/
--rw-r--r--   0 root         (0) root         (0)    13824 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_planar_complex.cu
--rw-r--r--   0 root         (0) root         (0)    27176 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt.cu
--rw-r--r--   0 root         (0) root         (0)    12061 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm60.cu
--rw-r--r--   0 root         (0) root         (0)    25275 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm61.cu
--rw-r--r--   0 root         (0) root         (0)    84612 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_tensor_op.cu
--rw-r--r--   0 root         (0) root         (0)    70486 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_volta_tensor_op.cu
--rw-r--r--   0 root         (0) root         (0)    25293 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_tensor_op.cu
--rw-r--r--   0 root         (0) root         (0)    13012 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_testbed.h
--rw-r--r--   0 root         (0) root         (0)     7743 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_wmma_tensor_op_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    19178 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/output_tile_threadmap.cu
--rw-r--r--   0 root         (0) root         (0)    28433 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/predicated_tile_iterator.cu
--rw-r--r--   0 root         (0) root         (0)    11038 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/testbed.h
--rw-r--r--   0 root         (0) root         (0)    11734 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/testbed_planar_complex.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:25.868080 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/warp/
--rw-r--r--   0 root         (0) root         (0)     6783 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_tensor_op.cu
--rw-r--r--   0 root         (0) root         (0)     7275 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_volta_tensor_op.cu
--rw-r--r--   0 root         (0) root         (0)     6616 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_wmma_tensor_op.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:46.825839 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:42.479728 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/
--rw-r--r--   0 root         (0) root         (0)    10189 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    17899 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8933 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_wmma_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    10164 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    17984 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8915 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_wmma_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16447 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_bf16n_bf16n_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    16575 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_bf16t_bf16t_bf16t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8318 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf32n_cf32t_cf32t_tensor_op_tf32_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8317 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf32t_cf32n_cf32t_tensor_op_tf32_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6714 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6735 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     7895 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7918 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     6516 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6537 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     9016 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9041 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     4628 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_direct_store_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6165 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6124 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     9634 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16357 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13189 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8845 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_volta_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    13583 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    13464 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     9571 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16239 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6140 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     9544 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16417 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13075 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8775 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_volta_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    11470 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6156 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6116 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     3528 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     3539 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7965 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16470 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13273 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3648 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8608 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_volta_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    13518 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     3645 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6096 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7845 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    18135 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13008 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8505 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_volta_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    11497 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    11090 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_singlestage_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6156 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6116 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    11066 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_singlestage_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    17114 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_broadcast_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3528 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     3540 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7964 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16457 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13266 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8933 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_volta_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    13551 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    13540 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6130 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     8160 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_singlestage_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7847 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16131 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13014 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8754 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_volta_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    11497 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6147 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6107 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    13518 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    13398 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7845 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16149 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6119 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7827 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16101 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9526 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7898 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_volta_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    11470 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     3584 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_bf16_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3473 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12967 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12931 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32t_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12930 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32t_f32n_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12895 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32t_f32t_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8349 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7288 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     8348 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7279 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)    10240 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_grouped_scheduler_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    26146 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    11339 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7346 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    12397 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6859 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     7239 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8121 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16882 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8407 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_wmma_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     8103 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    17111 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12637 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8388 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_wmma_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    10044 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    17544 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    10020 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    17544 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9588 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    11288 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3514 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_f16t_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7977 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16531 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5693 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_wmma_tensor_op_s32_sm72.cu
--rw-r--r--   0 root         (0) root         (0)     7959 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16691 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12408 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6864 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_wmma_tensor_op_s32_sm72.cu
--rw-r--r--   0 root         (0) root         (0)     7744 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16531 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6675 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_wmma_tensor_op_s32_sm72.cu
--rw-r--r--   0 root         (0) root         (0)     7752 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16484 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6663 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_wmma_tensor_op_s32_sm72.cu
--rw-r--r--   0 root         (0) root         (0)     4663 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_serial_tensor_op_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     4945 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_simt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     6616 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    10581 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16950 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32n_tf32n_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    16902 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32n_tf32t_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    15131 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32t_tf32n_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    16855 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32t_tf32t_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6854 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_u8t_u8n_s32t_wmma_tensor_op_s32_sm72.cu
--rw-r--r--   0 root         (0) root         (0)     6686 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf32n_cf32n_cf32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6755 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6687 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4726 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32n_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     4718 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32t_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16715 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_broadcast_f16n_f16n_f16n_tensorop_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    12841 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_reduction_f16n_f16n_f16n_tensorop_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     4544 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_reduction_f16t_f16n_f16n_tensorop_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13157 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemv.cu
--rw-r--r--   0 root         (0) root         (0)     6028 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6031 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6064 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6067 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4897 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64_cf64_cf64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     6088 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6037 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6040 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_rs_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5382 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5406 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5390 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64_cf64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)    13055 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64h_cf64n_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13027 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5388 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6939 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7677 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7725 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3842 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf64_cf64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     6396 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf64h_cf64n_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    10023 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/multistage_testbed.h
--rw-r--r--   0 root         (0) root         (0)     9189 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/multistage_testbed_interleaved.h
--rw-r--r--   0 root         (0) root         (0)    11186 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/rank_2k_grouped_scheduler_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    46795 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    54085 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     8318 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    46687 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     8411 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    46578 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    40533 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_nn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    47656 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_nt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    40441 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    40354 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_tt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     3513 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_f8gemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    89517 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_nn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    89304 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_nt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    89304 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    89091 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_tt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    69175 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_nn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    71438 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_nt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    67796 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    70056 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_tt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     7156 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61.cu
--rw-r--r--   0 root         (0) root         (0)     6067 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_perf.cu
--rw-r--r--   0 root         (0) root         (0)     9063 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_sliced_k.cu
--rw-r--r--   0 root         (0) root         (0)    35894 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_nn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    35813 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_nt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    35813 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    35732 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_tt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    70872 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    73136 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     8870 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    69488 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     8865 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    71755 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    33231 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_nn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    33156 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_nt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    33156 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    33081 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_tt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     5238 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm50_gemm_f32_f32_f32_simt.cu
--rw-r--r--   0 root         (0) root         (0)     5253 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm50_gemm_f64_f64_f64_simt.cu
--rw-r--r--   0 root         (0) root         (0)     5357 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm61_gemm_s8_s8_s32_simt.cu
--rw-r--r--   0 root         (0) root         (0)     5479 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f16_f16_f32_tensor_op_f32.cu
--rw-r--r--   0 root         (0) root         (0)     5238 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f32_f32_f32_simt.cu
--rw-r--r--   0 root         (0) root         (0)     5253 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f64_f64_f64_simt.cu
--rw-r--r--   0 root         (0) root         (0)     3875 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f64_f64_f64_tensor_op_f64.cu
--rw-r--r--   0 root         (0) root         (0)     3734 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_s8_s8_s32_tensor_op.cu
--rw-r--r--   0 root         (0) root         (0)     5387 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_tf32_tf32_f32_tensor_op_f32.cu
--rw-r--r--   0 root         (0) root         (0)     7436 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_bf16_bf16_bf16_alignx_tensor_op_f32.cu
--rw-r--r--   0 root         (0) root         (0)     7409 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_bf16_bf16_bf16_tensor_op_f32.cu
--rw-r--r--   0 root         (0) root         (0)    17391 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_alignx_tensor_op.cu
--rw-r--r--   0 root         (0) root         (0)    42504 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op.cu
--rw-r--r--   0 root         (0) root         (0)    22602 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_unspecialized.cu
--rw-r--r--   0 root         (0) root         (0)    22874 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized.cu
--rw-r--r--   0 root         (0) root         (0)    32884 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_cooperative.cu
--rw-r--r--   0 root         (0) root         (0)    14716 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_cooperative_bias_elementwise.cu
--rw-r--r--   0 root         (0) root         (0)    42526 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_persistent.cu
--rw-r--r--   0 root         (0) root         (0)    49311 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_pingpong.cu
--rw-r--r--   0 root         (0) root         (0)    14621 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_pingpong_bias_elementwise.cu
--rw-r--r--   0 root         (0) root         (0)    12049 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_tensor_broadcast.cu
--rw-r--r--   0 root         (0) root         (0)     3810 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f32_f32_f32_tensor_op_f32.cu
--rw-r--r--   0 root         (0) root         (0)     4507 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f32_f32_f32_tensor_op_f32_tensor_broadcast.cu
--rw-r--r--   0 root         (0) root         (0)     5976 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_s8_s8_s8_alignx_tensor_op_s32.cu
--rw-r--r--   0 root         (0) root         (0)     9313 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_s8_s8_s8_tensor_op_s32.cu
--rw-r--r--   0 root         (0) root         (0)     4515 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_s8_s8_s8_tensor_op_s32_tensor_broadcast.cu
--rw-r--r--   0 root         (0) root         (0)     5986 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_alignx_tensor_op_f32.cu
--rw-r--r--   0 root         (0) root         (0)     7268 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_tensor_op_f32.cu
--rw-r--r--   0 root         (0) root         (0)    21505 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_tensor_op_f32_gmma_rs_cluster_warpspecialized.cu
--rw-r--r--   0 root         (0) root         (0)     5923 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5926 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5959 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5962 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4827 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64_cf64_cf64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     5983 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5932 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5935 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_rs_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    15203 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8623 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    15104 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4765 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64_f64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     8103 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8108 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8088 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8093 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8073 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8078 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8058 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8063 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    15071 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8551 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    14972 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32t_f32t_tensor_op_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5362 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5386 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5356 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5380 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5367 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64_cf64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)    12952 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5368 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7208 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5362 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7199 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64n_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7190 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64t_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4794 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f32n_f32n_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4783 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f32t_f32n_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4728 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64_f64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)    19145 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7991 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    11015 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7976 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12342 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7961 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12321 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64t_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4786 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_tf32n_f32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4775 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_tf32t_f32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4993 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5017 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4987 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5011 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5012 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64_cf64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     4996 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64n_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3793 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4990 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    16083 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f32n_f32t_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    16041 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f32t_f32t_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4518 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64_f64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     7451 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64n_f64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9401 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64t_f64n_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    16027 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_tf32n_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    15985 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_tf32t_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    20333 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed.h
--rw-r--r--   0 root         (0) root         (0)     8136 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_complex.h
--rw-r--r--   0 root         (0) root         (0)    20590 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_gemm_with_broadcast.h
--rw-r--r--   0 root         (0) root         (0)    19346 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_gemm_with_reduction.h
--rw-r--r--   0 root         (0) root         (0)    16502 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped.h
--rw-r--r--   0 root         (0) root         (0)    16562 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k.h
--rw-r--r--   0 root         (0) root         (0)    17002 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k_scheduler.h
--rw-r--r--   0 root         (0) root         (0)    14698 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_scheduler.h
--rw-r--r--   0 root         (0) root         (0)    10130 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_interleaved.h
--rw-r--r--   0 root         (0) root         (0)     9481 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)    20761 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_rank2k_universal.h
--rw-r--r--   0 root         (0) root         (0)    15562 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_rank_k_universal.h
--rw-r--r--   0 root         (0) root         (0)     8639 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_sanity.h
--rw-r--r--   0 root         (0) root         (0)    15769 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_sparse.h
--rw-r--r--   0 root         (0) root         (0)     6124 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_splitk.h
--rw-r--r--   0 root         (0) root         (0)    19861 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_symm_universal.h
--rw-r--r--   0 root         (0) root         (0)    20200 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_trmm_universal.h
--rw-r--r--   0 root         (0) root         (0)    17311 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_universal.h
--rw-r--r--   0 root         (0) root         (0)     2626 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_utils.h
--rw-r--r--   0 root         (0) root         (0)     9916 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9988 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4977 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64_cf64_cf64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     4992 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9762 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    15614 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8733 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    14089 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    14444 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32t_tensor_op_fast_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4596 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64_f64_f64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)    12798 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12809 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12764 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64t_f64t_tensor_op_f64_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12768 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12779 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    15504 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8673 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13989 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32n_tensor_op_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    14344 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32t_tensor_op_f32_ls_sm80.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:42.524201 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/kernel/
--rwxr-xr-x   0 root         (0) root         (0)    46470 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/kernel/batched_gemv.cu
--rwxr-xr-x   0 root         (0) root         (0)    14362 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/kernel/testbed_gemv.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:42.687916 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/thread/
--rw-r--r--   0 root         (0) root         (0)     4847 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    12503 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm60.cu
--rw-r--r--   0 root         (0) root         (0)     3109 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm61.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:42.732642 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/
--rw-r--r--   0 root         (0) root         (0)     5198 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/gemm_sm60_host.cu
--rw-r--r--   0 root         (0) root         (0)     7161 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/testbed_host.h
--rw-r--r--   0 root         (0) root         (0)     7124 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/thread/testbed.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:43.572929 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/
--rw-r--r--   0 root         (0) root         (0)    25036 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/batched_gemv.cu
--rw-r--r--   0 root         (0) root         (0)     4345 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/epilogue_workspace.cu
--rw-r--r--   0 root         (0) root         (0)   135045 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage.cu
--rw-r--r--   0 root         (0) root         (0)     4644 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_slicedk.cu
--rw-r--r--   0 root         (0) root         (0)    94442 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse.cu
--rw-r--r--   0 root         (0) root         (0)    17109 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse_testbed.h
--rw-r--r--   0 root         (0) root         (0)    13131 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed.h
--rw-r--r--   0 root         (0) root         (0)    14539 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed_slicedk.h
--rw-r--r--   0 root         (0) root         (0)    49052 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_simt.cu
--rw-r--r--   0 root         (0) root         (0)     8407 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_slicedk.cu
--rw-r--r--   0 root         (0) root         (0)    18705 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    78122 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    21051 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13413 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_testbed.h
--rw-r--r--   0 root         (0) root         (0)    14239 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_testbed_slicedk.h
--rw-r--r--   0 root         (0) root         (0)    29772 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    12395 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     3502 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_planar_complex_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12138 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_planar_complex_testbed.h
--rw-r--r--   0 root         (0) root         (0)    16308 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_singlestage_wmma_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    12502 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_singlestage_wmma_sm75.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:43.873393 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/warp/
--rw-r--r--   0 root         (0) root         (0)    22128 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_complex_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    10904 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_complex_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     9873 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_gaussian_complex_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    18220 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     4920 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm60.cu
--rw-r--r--   0 root         (0) root         (0)     6291 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm61.cu
--rw-r--r--   0 root         (0) root         (0)     9297 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    37942 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    81659 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9077 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm90.cu
--rw-r--r--   0 root         (0) root         (0)    48928 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    45327 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/warp/testbed.h
--rw-r--r--   0 root         (0) root         (0)    25780 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7544 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm72.cu
--rw-r--r--   0 root         (0) root         (0)     6487 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm75.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:43.977092 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/layout/
--rw-r--r--   0 root         (0) root         (0)     5788 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/layout/matrix.cu
--rw-r--r--   0 root         (0) root         (0)     5984 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/layout/tensor.cu
--rw-r--r--   0 root         (0) root         (0)     7081 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/layout/tensor_nhwc.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:46.871948 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/nvrtc/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:46.854801 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/nvrtc/cutlass/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:44.009597 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/nvrtc/cutlass/nvrtc/
--rw-r--r--   0 root         (0) root         (0)     2096 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/nvrtc/cutlass/nvrtc/environment.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:46.866728 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/nvrtc/kernel/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:44.059237 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/nvrtc/kernel/thread/
--rw-r--r--   0 root         (0) root         (0)     2915 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/nvrtc/kernel/thread/testbed_kernel.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:44.200915 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/nvrtc/stdlib/
--rw-rw-r--   0 root         (0) root         (0)        0 2022-06-02 16:47:41.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/nvrtc/stdlib/assert.h
--rw-r--r--   0 root         (0) root         (0)     4250 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/nvrtc/stdlib/stdint.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:44.246092 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/
--rw-r--r--   0 root         (0) root         (0)     5727 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/gemm_nvrtc.cu
--rw-r--r--   0 root         (0) root         (0)    10328 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/testbed.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:44.369582 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/pipeline/
--rw-r--r--   0 root         (0) root         (0)    15532 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_async.cu
--rw-r--r--   0 root         (0) root         (0)    15490 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_tma_async.cu
--rw-r--r--   0 root         (0) root         (0)    17098 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_tma_async_warp_specialized.cu
--rw-r--r--   0 root         (0) root         (0)    20252 2023-04-15 23:28:02.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_tma_async_warp_specialized_persistent.cu
--rw-r--r--   0 root         (0) root         (0)     7623 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/pipeline/sequence_barrier.cu
--rw-r--r--   0 root         (0) root         (0)     4327 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/pipeline/testbed.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:46.904160 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/reduction/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:44.515542 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/reduction/device/
--rw-r--r--   0 root         (0) root         (0)    14684 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_contiguous.cu
--rw-r--r--   0 root         (0) root         (0)    15609 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_strided.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:44.562459 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/reduction/kernel/
--rw-r--r--   0 root         (0) root         (0)    11350 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk.cu
--rw-r--r--   0 root         (0) root         (0)     2228 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk_testbed.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:44.606110 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/reduction/thread/
--rw-r--r--   0 root         (0) root         (0)     3110 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/reduction/thread/reduction_thread.cu
--rw-r--r--   0 root         (0) root         (0)     6657 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/reduction/thread/testbed.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:44.767445 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/substrate/
--rw-r--r--   0 root         (0) root         (0)     3312 2023-04-15 18:47:44.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/substrate/dependent_false.cpp
--rw-r--r--   0 root         (0) root         (0)     2047 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/test_unit.cpp
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:46.920710 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/transform/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:44.815426 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/transform/threadblock/
--rw-r--r--   0 root         (0) root         (0)    25527 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/transform/threadblock/predicated_tile_iterator.cu
--rw-r--r--   0 root         (0) root         (0)     9501 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/transform/threadblock/regular_tile_iterator_tensor_op.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:44.859713 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/util/
--rw-r--r--   0 root         (0) root         (0)     2663 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/util/cutlass_test_levels.cu
--rw-r--r--   0 root         (0) root         (0)     7474 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/util/tensor_reduce.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:47.083871 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:47.051815 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:46.953690 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/include/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:46.959876 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/include/cutlass/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:45.123770 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/
--rw-r--r--   0 root         (0) root         (0)     4118 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/arch_mappings.h
--rw-r--r--   0 root         (0) root         (0)    16013 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/handle.h
--rw-r--r--   0 root         (0) root         (0)    38340 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/library.h
--rw-r--r--   0 root         (0) root         (0)     4070 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/manifest.h
--rw-r--r--   0 root         (0) root         (0)    17934 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/operation_table.h
--rw-r--r--   0 root         (0) root         (0)     2724 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/singleton.h
--rw-r--r--   0 root         (0) root         (0)     7904 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/util.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:46.974221 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:46.980002 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:46.985824 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:45.216883 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/
--rw-r--r--   0 root         (0) root         (0)     2788 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/compiler.h
--rw-r--r--   0 root         (0) root         (0)     2460 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/cute.cpp
--rw-r--r--   0 root         (0) root         (0)     6223 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/cutlass.cpp
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:45.314760 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/
--rw-r--r--   0 root         (0) root         (0)     2851 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/arch.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:45.483037 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/
--rw-r--r--   0 root         (0) root         (0)     5897 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/conv_problem_size.h
--rw-r--r--   0 root         (0) root         (0)     4763 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/convolution.h
--rw-r--r--   0 root         (0) root         (0)     2650 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/host.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:45.524646 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/
--rw-r--r--   0 root         (0) root         (0)     6844 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_generic.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:45.853418 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/
--rw-r--r--   0 root         (0) root         (0)     3047 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/binary_ops.h
--rw-r--r--   0 root         (0) root         (0)     6134 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/unary_ops.h
--rw-r--r--   0 root         (0) root         (0)     4702 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_accumulator.h
--rw-r--r--   0 root         (0) root         (0)     8467 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_binary.h
--rw-r--r--   0 root         (0) root         (0)     8815 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_broadcast.h
--rw-r--r--   0 root         (0) root         (0)    13049 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_reduction.h
--rw-r--r--   0 root         (0) root         (0)     9493 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_linear_combination.h
--rw-r--r--   0 root         (0) root         (0)     9070 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_broadcast.h
--rw-r--r--   0 root         (0) root         (0)    12029 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_reduction.h
--rw-r--r--   0 root         (0) root         (0)     6177 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_input.h
--rw-r--r--   0 root         (0) root         (0)     8017 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_output.h
--rw-r--r--   0 root         (0) root         (0)     7235 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_unary.h
--rw-r--r--   0 root         (0) root         (0)    16970 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_with_layernorm.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:45.922347 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/
--rw-r--r--   0 root         (0) root         (0)     3673 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/gemm.h
--rw-r--r--   0 root         (0) root         (0)    22378 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/gemm_universal_with_visitor.h
--rw-r--r--   0 root         (0) root         (0)     2328 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/host.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:46.240643 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/
--rw-r--r--   0 root         (0) root         (0)     2115 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/layout.h
--rw-r--r--   0 root         (0) root         (0)     4337 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/matrix.h
--rw-r--r--   0 root         (0) root         (0)     3694 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/tensor.h
--rw-r--r--   0 root         (0) root         (0)     8624 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/swizzling.h
--rw-r--r--   0 root         (0) root         (0)     3902 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_coord.h
--rw-r--r--   0 root         (0) root         (0)     5563 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_ref_view.h
--rw-r--r--   0 root         (0) root         (0)     4855 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/types.h
--rw-r--r--   0 root         (0) root         (0)      811 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/library.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:47.041643 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:46.306260 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/
--rw-r--r--   0 root         (0) root         (0)     2651 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/conv_problems.h
--rw-r--r--   0 root         (0) root         (0)     2253 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/convolution.h
--rw-r--r--   0 root         (0) root         (0)     8826 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/host.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:46.351238 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/
--rw-r--r--   0 root         (0) root         (0)     2139 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/gemm.h
--rw-r--r--   0 root         (0) root         (0)    18930 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/host.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:46.801097 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/
--rw-r--r--   0 root         (0) root         (0)    22377 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/conv2d_operation.h
--rw-r--r--   0 root         (0) root         (0)    13851 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/conv3d_operation.h
--rw-r--r--   0 root         (0) root         (0)    42129 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/gemm_operation.h
--rw-r--r--   0 root         (0) root         (0)    35709 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/handle.cu
--rw-r--r--   0 root         (0) root         (0)    12616 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/library_internal.h
--rw-r--r--   0 root         (0) root         (0)     3782 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/manifest.cpp
--rw-r--r--   0 root         (0) root         (0)     5468 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/operation_table.cu
--rw-r--r--   0 root         (0) root         (0)    12873 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/rank_2k_operation.h
--rw-r--r--   0 root         (0) root         (0)    11367 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/rank_k_operation.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:46.865915 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/reduction/
--rw-r--r--   0 root         (0) root         (0)     3190 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/reduction/init_reduction_operations.cu
--rw-r--r--   0 root         (0) root         (0)     6367 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/reduction/reduction_device.cu
--rw-r--r--   0 root         (0) root         (0)    10270 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/reduction/reduction_operation.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:47.368202 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/reference/
--rw-r--r--   0 root         (0) root         (0)     6746 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/reference/conv2d.cu
--rw-r--r--   0 root         (0) root         (0)     6286 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/reference/conv3d.cu
--rw-r--r--   0 root         (0) root         (0)    17191 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/reference/conv_reference_operation.h
--rw-r--r--   0 root         (0) root         (0)     7199 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/reference/gemm.cu
--rw-r--r--   0 root         (0) root         (0)    14732 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/reference/gemm_reference_operation.h
--rw-r--r--   0 root         (0) root         (0)     2857 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/reference/initialize_reference_operations.cu
--rw-r--r--   0 root         (0) root         (0)     2669 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/singleton.cu
--rw-r--r--   0 root         (0) root         (0)    13134 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/symm_operation.h
--rw-r--r--   0 root         (0) root         (0)    11698 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/trmm_operation.h
--rw-r--r--   0 root         (0) root         (0)    43704 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/util.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:47.076048 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:48.651530 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/
--rw-r--r--   0 root         (0) root         (0)    54128 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/conv2d_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)    18170 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/conv2d_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)    48659 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/conv3d_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)    16043 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/conv3d_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)    36462 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/cublas_helpers.cu
--rw-r--r--   0 root         (0) root         (0)    10627 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/cublas_helpers.h
--rw-r--r--   0 root         (0) root         (0)    17049 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/cudnn_helpers.cpp
--rw-r--r--   0 root         (0) root         (0)    20433 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/cudnn_helpers.h
--rw-r--r--   0 root         (0) root         (0)     7233 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/cutlass_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     3233 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/cutlass_profiler.h
--rw-r--r--   0 root         (0) root         (0)     2453 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/debug.h
--rw-r--r--   0 root         (0) root         (0)    53643 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/device_allocation.cu
--rw-r--r--   0 root         (0) root         (0)     7217 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/device_allocation.h
--rw-r--r--   0 root         (0) root         (0)     6841 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/device_context.cu
--rw-r--r--   0 root         (0) root         (0)     4300 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/device_context.h
--rw-r--r--   0 root         (0) root         (0)     8296 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/enumerated_types.cpp
--rw-r--r--   0 root         (0) root         (0)     6421 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/enumerated_types.h
--rw-r--r--   0 root         (0) root         (0)    41919 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/gemm_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     8544 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/gemm_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)     3874 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/gpu_timer.cpp
--rw-r--r--   0 root         (0) root         (0)     2724 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/gpu_timer.h
--rw-r--r--   0 root         (0) root         (0)     2340 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/main.cpp
--rw-r--r--   0 root         (0) root         (0)    20944 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     7876 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)    27172 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/options.cu
--rw-r--r--   0 root         (0) root         (0)     8773 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/options.h
--rw-r--r--   0 root         (0) root         (0)    14192 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/performance_report.cpp
--rw-r--r--   0 root         (0) root         (0)     4337 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/performance_report.h
--rw-r--r--   0 root         (0) root         (0)     2494 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/performance_result.cu
--rw-r--r--   0 root         (0) root         (0)     3941 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/performance_result.h
--rw-r--r--   0 root         (0) root         (0)    37487 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/problem_space.cpp
--rw-r--r--   0 root         (0) root         (0)    27747 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/problem_space.h
--rw-r--r--   0 root         (0) root         (0)    25014 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/rank_2k_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     6891 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/rank_2k_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)    24253 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/rank_k_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     6830 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/rank_k_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)     5452 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/reduction_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)    20688 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     6471 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)    26610 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/symm_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     6933 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/symm_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)    24431 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/trmm_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     6599 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/trmm_operation_profiler.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:47.089934 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:47.096399 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:47.103023 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:49.467710 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/
--rw-r--r--   0 root         (0) root         (0)     9774 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/command_line.h
--rw-r--r--   0 root         (0) root         (0)     5104 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/debug.h
--rw-r--r--   0 root         (0) root         (0)     5953 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_dump.h
--rw-r--r--   0 root         (0) root         (0)    17695 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_groupnorm.h
--rw-r--r--   0 root         (0) root         (0)    20880 2023-04-16 04:29:29.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_layernorm.h
--rw-r--r--   0 root         (0) root         (0)    10561 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_memory.h
--rw-r--r--   0 root         (0) root         (0)     5219 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nchw_to_nhwc.h
--rw-r--r--   0 root         (0) root         (0)    11067 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_padding.h
--rw-r--r--   0 root         (0) root         (0)    18653 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_pooling.h
--rw-r--r--   0 root         (0) root         (0)     5214 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_to_nchw.h
--rw-r--r--   0 root         (0) root         (0)     4007 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_utils.h
--rw-r--r--   0 root         (0) root         (0)     4597 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/distribution.h
--rw-r--r--   0 root         (0) root         (0)     2674 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/exceptions.h
--rw-r--r--   0 root         (0) root         (0)     4821 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_reorder.h
--rw-r--r--   0 root         (0) root         (0)    16745 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_tensor.h
--rw-r--r--   0 root         (0) root         (0)    20354 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_tensor_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)     5890 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_uncompress.h
--rw-r--r--   0 root         (0) root         (0)     1962 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/index_sequence.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:05:47.139644 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:49.516330 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/
--rw-r--r--   0 root         (0) root         (0)     4606 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/inner_product.h
--rw-r--r--   0 root         (0) root         (0)     3527 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/linear_to_coordinate.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:50.000285 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/
--rw-r--r--   0 root         (0) root         (0)    48350 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/convolution.h
--rw-r--r--   0 root         (0) root         (0)    14296 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm.h
--rw-r--r--   0 root         (0) root         (0)    10524 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm_complex.h
--rw-r--r--   0 root         (0) root         (0)     9652 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm_planar_complex.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:50.062466 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/
--rw-r--r--   0 root         (0) root         (0)     5381 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/gemm.h
--rw-r--r--   0 root         (0) root         (0)     6198 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_elementwise.h
--rw-r--r--   0 root         (0) root         (0)     5126 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_foreach.h
--rw-r--r--   0 root         (0) root         (0)    11615 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/rank_2k_complex.h
--rw-r--r--   0 root         (0) root         (0)     7278 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_compare.h
--rw-r--r--   0 root         (0) root         (0)    46444 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_fill.h
--rw-r--r--   0 root         (0) root         (0)     5293 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_foreach.h
--rw-r--r--   0 root         (0) root         (0)    15964 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_reduce.h
--rw-r--r--   0 root         (0) root         (0)     4589 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_relu.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:50.090067 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/thread/
--rw-r--r--   0 root         (0) root         (0)     5872 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/thread/gemm.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:50.803713 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/
--rw-r--r--   0 root         (0) root         (0)    28439 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/convolution.h
--rw-r--r--   0 root         (0) root         (0)     2766 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/error_metrics.h
--rw-r--r--   0 root         (0) root         (0)    17163 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm.h
--rw-r--r--   0 root         (0) root         (0)     7097 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm_complex.h
--rw-r--r--   0 root         (0) root         (0)     7708 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)     9441 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k.h
--rw-r--r--   0 root         (0) root         (0)    11444 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k_complex.h
--rw-r--r--   0 root         (0) root         (0)     8148 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_k_complex.h
--rw-r--r--   0 root         (0) root         (0)    10509 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/symm.h
--rw-r--r--   0 root         (0) root         (0)    12296 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/symm_complex.h
--rw-r--r--   0 root         (0) root         (0)     8440 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_compare.h
--rw-r--r--   0 root         (0) root         (0)     8317 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_copy.h
--rw-r--r--   0 root         (0) root         (0)     9027 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_elementwise.h
--rw-r--r--   0 root         (0) root         (0)    43961 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_fill.h
--rw-r--r--   0 root         (0) root         (0)     4756 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_foreach.h
--rw-r--r--   0 root         (0) root         (0)     2133 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_norm.h
--rw-r--r--   0 root         (0) root         (0)     6111 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_reduce.h
--rw-r--r--   0 root         (0) root         (0)     7670 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/trmm.h
--rw-r--r--   0 root         (0) root         (0)     9874 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/trmm_complex.h
--rw-r--r--   0 root         (0) root         (0)     8285 2023-04-15 23:28:03.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/tensor_view_io.h
--rw-r--r--   0 root         (0) root         (0)     8809 2023-04-12 22:58:14.000000 flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/type_traits.h
--rw-r--r--   0 root         (0) root         (0)    32526 2023-04-16 00:20:27.000000 flash_attn-1.0.8/csrc/flash_attn/flash_api.cpp
--rw-r--r--   0 root         (0) root         (0)    33267 2023-04-26 16:18:56.000000 flash_attn-1.0.8/csrc/flash_attn/fmha_api.cpp
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:54.260575 flash_attn-1.0.8/csrc/flash_attn/src/
--rw-r--r--   0 root         (0) root         (0)     1664 2023-04-16 00:20:27.000000 flash_attn-1.0.8/csrc/flash_attn/src/block_info.h
--rw-r--r--   0 root         (0) root         (0)     3931 2023-04-16 00:20:27.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash.h
--rw-r--r--   0 root         (0) root         (0)      983 2023-04-07 15:25:56.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_bwd_hdim128.cu
--rw-r--r--   0 root         (0) root         (0)      574 2023-04-16 00:20:27.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_bwd_hdim128_bf16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     1124 2023-04-16 00:20:27.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_bwd_hdim128_fp16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      983 2023-04-07 15:25:56.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_bwd_hdim128_sm80_fp16.cu
--rw-r--r--   0 root         (0) root         (0)      425 2023-04-07 15:25:56.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_bwd_hdim32.cu
--rw-r--r--   0 root         (0) root         (0)      450 2023-04-16 00:20:27.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_bwd_hdim32_bf16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      442 2023-04-16 00:20:27.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_bwd_hdim32_fp16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      425 2023-04-07 15:25:56.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_bwd_hdim32_sm80_fp16.cu
--rw-r--r--   0 root         (0) root         (0)     2074 2023-04-07 15:25:56.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_bwd_hdim64.cu
--rw-r--r--   0 root         (0) root         (0)      450 2023-04-16 00:20:27.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_bwd_hdim64_bf16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     2458 2023-04-16 00:20:27.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_bwd_hdim64_fp16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     2074 2023-04-07 15:25:56.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_bwd_hdim64_sm80_fp16.cu
--rw-r--r--   0 root         (0) root         (0)      581 2023-04-07 15:25:56.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_bwd_hdim96.cu
--rw-r--r--   0 root         (0) root         (0)      449 2023-04-16 00:20:27.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_bwd_hdim96_bf16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      598 2023-04-16 00:20:27.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_bwd_hdim96_fp16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      581 2023-04-07 15:25:56.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_bwd_hdim96_sm80_fp16.cu
--rw-r--r--   0 root         (0) root         (0)    83190 2023-04-16 00:20:27.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_bwd_kernel.h
--rw-r--r--   0 root         (0) root         (0)    41051 2023-03-25 22:30:12.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_bwd_kernel_bak.h
--rw-r--r--   0 root         (0) root         (0)    39628 2023-03-25 21:25:30.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_bwd_kernel_new.h
--rw-r--r--   0 root         (0) root         (0)    39403 2023-04-08 17:10:55.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_bwd_kernel_reverse.h
--rw-r--r--   0 root         (0) root         (0)     8911 2023-04-16 00:20:27.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_bwd_launch_template.h
--rw-r--r--   0 root         (0) root         (0)      645 2023-04-07 21:42:50.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_fwd_hdim128.cu
--rw-r--r--   0 root         (0) root         (0)      592 2023-04-16 00:20:27.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      662 2023-04-16 00:20:27.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      662 2023-04-08 06:50:03.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_fwd_hdim128_sm80_fp16.cu
--rw-r--r--   0 root         (0) root         (0)      990 2023-04-07 18:39:34.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_fwd_hdim160.cu
--rw-r--r--   0 root         (0) root         (0)      495 2023-04-16 00:20:27.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     1007 2023-04-16 00:20:27.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     1007 2023-04-08 06:50:03.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_fwd_hdim160_sm80_fp16.cu
--rw-r--r--   0 root         (0) root         (0)     1041 2023-04-07 18:40:27.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_fwd_hdim192.cu
--rw-r--r--   0 root         (0) root         (0)      494 2023-04-16 00:20:27.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     1058 2023-04-16 00:20:27.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     1058 2023-04-08 06:50:03.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_fwd_hdim192_sm80_fp16.cu
--rw-r--r--   0 root         (0) root         (0)      945 2023-04-07 18:37:45.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_fwd_hdim32.cu
--rw-r--r--   0 root         (0) root         (0)      494 2023-04-16 00:20:27.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      962 2023-04-16 00:20:27.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      962 2023-04-08 06:50:03.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_fwd_hdim32_sm80_fp16.cu
--rw-r--r--   0 root         (0) root         (0)     1149 2023-04-07 22:02:41.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_fwd_hdim64.cu
--rw-r--r--   0 root         (0) root         (0)      588 2023-04-16 00:20:27.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     1166 2023-04-16 00:20:27.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     1166 2023-04-08 06:50:03.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_fwd_hdim64_sm80_fp16.cu
--rw-r--r--   0 root         (0) root         (0)      711 2023-04-07 22:17:50.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_fwd_hdim96.cu
--rw-r--r--   0 root         (0) root         (0)      492 2023-04-16 00:20:27.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      728 2023-04-16 00:20:27.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      728 2023-04-08 06:50:03.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_fwd_hdim96_sm80_fp16.cu
--rw-r--r--   0 root         (0) root         (0)    30072 2023-04-16 00:20:27.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_fwd_kernel.h
--rw-r--r--   0 root         (0) root         (0)    27253 2023-04-08 17:10:55.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_fwd_kernel_old.h
--rw-r--r--   0 root         (0) root         (0)     2564 2023-04-16 00:20:27.000000 flash_attn-1.0.8/csrc/flash_attn/src/flash_fwd_launch_template.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:54.411633 flash_attn-1.0.8/csrc/flash_attn/src/fmha/
--rw-r--r--   0 root         (0) root         (0)    17999 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/flash_attn/src/fmha/gemm.h
--rw-r--r--   0 root         (0) root         (0)    22872 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/flash_attn/src/fmha/gmem_tile.h
--rw-r--r--   0 root         (0) root         (0)     5997 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/flash_attn/src/fmha/kernel_traits.h
--rw-r--r--   0 root         (0) root         (0)     4362 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/flash_attn/src/fmha/mask.h
--rw-r--r--   0 root         (0) root         (0)    74010 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/flash_attn/src/fmha/smem_tile.h
--rw-r--r--   0 root         (0) root         (0)    25514 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/flash_attn/src/fmha/softmax.h
--rw-r--r--   0 root         (0) root         (0)    41059 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/flash_attn/src/fmha/utils.h
--rw-r--r--   0 root         (0) root         (0)     7237 2023-04-16 04:28:21.000000 flash_attn-1.0.8/csrc/flash_attn/src/fmha.h
--rw-r--r--   0 root         (0) root         (0)     4118 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/flash_attn/src/fmha_block_dgrad_fp16_kernel_loop.sm80.cu
--rw-r--r--   0 root         (0) root         (0)    33506 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/flash_attn/src/fmha_block_dgrad_kernel_1xN_loop.h
--rw-r--r--   0 root         (0) root         (0)     5292 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/flash_attn/src/fmha_block_fprop_fp16_kernel.sm80.cu
--rw-r--r--   0 root         (0) root         (0)    23207 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/flash_attn/src/fmha_block_fprop_kernel_1xN.h
--rw-r--r--   0 root         (0) root         (0)     2502 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/flash_attn/src/fmha_blockmask.h
--rw-r--r--   0 root         (0) root         (0)      465 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/flash_attn/src/fmha_bwd_hdim128.cu
--rw-r--r--   0 root         (0) root         (0)      727 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/flash_attn/src/fmha_bwd_hdim32.cu
--rw-r--r--   0 root         (0) root         (0)     1713 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/flash_attn/src/fmha_bwd_hdim64.cu
--rw-r--r--   0 root         (0) root         (0)     6453 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/flash_attn/src/fmha_bwd_launch_template.h
--rw-r--r--   0 root         (0) root         (0)    37168 2023-04-16 04:28:21.000000 flash_attn-1.0.8/csrc/flash_attn/src/fmha_dgrad_kernel_1xN_loop.h
--rw-r--r--   0 root         (0) root         (0)    31042 2023-04-16 04:28:21.000000 flash_attn-1.0.8/csrc/flash_attn/src/fmha_fprop_kernel_1xN.h
--rw-r--r--   0 root         (0) root         (0)      445 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/flash_attn/src/fmha_fwd_hdim128.cu
--rw-r--r--   0 root         (0) root         (0)      724 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/flash_attn/src/fmha_fwd_hdim32.cu
--rw-r--r--   0 root         (0) root         (0)      725 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/flash_attn/src/fmha_fwd_hdim64.cu
--rw-r--r--   0 root         (0) root         (0)     4393 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/flash_attn/src/fmha_fwd_launch_template.h
--rw-r--r--   0 root         (0) root         (0)     3104 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/flash_attn/src/fmha_kernel.h
--rw-r--r--   0 root         (0) root         (0)     4892 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/flash_attn/src/fmha_utils.h
--rw-r--r--   0 root         (0) root         (0)    17820 2023-04-16 00:20:27.000000 flash_attn-1.0.8/csrc/flash_attn/src/kernel_traits.h
--rw-r--r--   0 root         (0) root         (0)     2927 2023-03-31 21:49:35.000000 flash_attn-1.0.8/csrc/flash_attn/src/mask.h
--rw-r--r--   0 root         (0) root         (0)     5462 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/flash_attn/src/philox.cuh
--rw-r--r--   0 root         (0) root         (0)    14205 2023-04-16 00:20:27.000000 flash_attn-1.0.8/csrc/flash_attn/src/softmax.h
--rw-r--r--   0 root         (0) root         (0)     1686 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/flash_attn/src/static_switch.h
--rw-r--r--   0 root         (0) root         (0)    16378 2023-04-16 00:20:27.000000 flash_attn-1.0.8/csrc/flash_attn/src/utils.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:54.438816 flash_attn-1.0.8/csrc/flash_gen/
--rw-r--r--   0 root         (0) root         (0)     7018 2022-11-21 06:35:03.000000 flash_attn-1.0.8/csrc/flash_gen/decoder_masked_multihead_attention.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:54.560374 flash_attn-1.0.8/csrc/ft_attention/
--rw-r--r--   0 root         (0) root         (0)     8253 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/ft_attention/cuda_bf16_fallbacks.cuh
--rw-r--r--   0 root         (0) root         (0)      867 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/ft_attention/cuda_bf16_wrapper.h
--rw-r--r--   0 root         (0) root         (0)     7069 2023-06-06 06:13:59.000000 flash_attn-1.0.8/csrc/ft_attention/decoder_masked_multihead_attention.cu
--rw-r--r--   0 root         (0) root         (0)     7627 2023-07-02 20:11:51.000000 flash_attn-1.0.8/csrc/ft_attention/decoder_masked_multihead_attention.h
--rw-r--r--   0 root         (0) root         (0)    66918 2023-07-02 22:05:10.000000 flash_attn-1.0.8/csrc/ft_attention/decoder_masked_multihead_attention_utils.h
--rw-r--r--   0 root         (0) root         (0)    10172 2023-07-02 20:48:23.000000 flash_attn-1.0.8/csrc/ft_attention/ft_attention.cpp
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:54.696664 flash_attn-1.0.8/csrc/fused_dense_lib/
--rw-r--r--   0 root         (0) root         (0)    10179 2023-05-30 21:13:46.000000 flash_attn-1.0.8/csrc/fused_dense_lib/fused_dense.cpp
--rw-r--r--   0 root         (0) root         (0)    24690 2023-05-30 21:14:57.000000 flash_attn-1.0.8/csrc/fused_dense_lib/fused_dense_cuda.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:54.912333 flash_attn-1.0.8/csrc/fused_softmax/
--rw-r--r--   0 root         (0) root         (0)     5037 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/fused_softmax/fused_softmax.cpp
--rw-r--r--   0 root         (0) root         (0)    23616 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/fused_softmax/scaled_masked_softmax.h
--rw-r--r--   0 root         (0) root         (0)     4209 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/fused_softmax/scaled_masked_softmax_cuda.cu
--rw-r--r--   0 root         (0) root         (0)    24659 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/fused_softmax/scaled_upper_triang_masked_softmax.h
--rw-r--r--   0 root         (0) root         (0)     3154 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/fused_softmax/scaled_upper_triang_masked_softmax_cuda.cu
--rw-r--r--   0 root         (0) root         (0)     1216 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/fused_softmax/type_shim.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:58.442640 flash_attn-1.0.8/csrc/layer_norm/
--rw-r--r--   0 root         (0) root         (0)     7248 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/layer_norm/ln.h
--rw-r--r--   0 root         (0) root         (0)    36418 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/layer_norm/ln_api.cpp
--rw-r--r--   0 root         (0) root         (0)      987 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/layer_norm/ln_bwd_1024.cu
--rw-r--r--   0 root         (0) root         (0)      987 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/layer_norm/ln_bwd_1280.cu
--rw-r--r--   0 root         (0) root         (0)      977 2023-04-16 00:48:36.000000 flash_attn-1.0.8/csrc/layer_norm/ln_bwd_1536.cu
--rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_bwd_2048.cu
--rw-r--r--   0 root         (0) root         (0)      977 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_bwd_256.cu
--rw-r--r--   0 root         (0) root         (0)      977 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_bwd_2560.cu
--rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_bwd_3072.cu
--rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_bwd_4096.cu
--rw-r--r--   0 root         (0) root         (0)      977 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_bwd_512.cu
--rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_bwd_5120.cu
--rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_bwd_6144.cu
--rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_bwd_7168.cu
--rw-r--r--   0 root         (0) root         (0)      977 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_bwd_768.cu
--rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_bwd_8192.cu
--rw-r--r--   0 root         (0) root         (0)    25647 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_bwd_kernels.cuh
--rw-r--r--   0 root         (0) root         (0)    19944 2023-01-19 07:34:02.000000 flash_attn-1.0.8/csrc/layer_norm/ln_bwd_semi_cuda_kernel_old.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_fwd_1024.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-01-22 09:05:55.000000 flash_attn-1.0.8/csrc/layer_norm/ln_fwd_10240.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-01-22 09:07:15.000000 flash_attn-1.0.8/csrc/layer_norm/ln_fwd_12288.cu
--rw-r--r--   0 root         (0) root         (0)      925 2022-12-05 08:45:58.000000 flash_attn-1.0.8/csrc/layer_norm/ln_fwd_128.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_fwd_1280.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_fwd_1536.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_fwd_2048.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_fwd_256.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_fwd_2560.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_fwd_3072.cu
--rw-r--r--   0 root         (0) root         (0)      925 2022-12-05 08:50:57.000000 flash_attn-1.0.8/csrc/layer_norm/ln_fwd_384.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_fwd_4096.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_fwd_512.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_fwd_5120.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_fwd_6144.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_fwd_7168.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_fwd_768.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_fwd_8192.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-01-22 08:41:06.000000 flash_attn-1.0.8/csrc/layer_norm/ln_fwd_9216.cu
--rw-r--r--   0 root         (0) root         (0)    18000 2022-12-06 21:18:58.000000 flash_attn-1.0.8/csrc/layer_norm/ln_fwd_cuda_kernel_old.cu
--rw-r--r--   0 root         (0) root         (0)    12721 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_fwd_kernels.cuh
--rw-r--r--   0 root         (0) root         (0)     6655 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_kernel_traits.h
--rw-r--r--   0 root         (0) root         (0)     1095 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_bwd_1024.cu
--rw-r--r--   0 root         (0) root         (0)     1095 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_bwd_1280.cu
--rw-r--r--   0 root         (0) root         (0)     1085 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_bwd_1536.cu
--rw-r--r--   0 root         (0) root         (0)     1084 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_bwd_2048.cu
--rw-r--r--   0 root         (0) root         (0)     1085 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_bwd_256.cu
--rw-r--r--   0 root         (0) root         (0)     1085 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_bwd_2560.cu
--rw-r--r--   0 root         (0) root         (0)     1084 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_bwd_3072.cu
--rw-r--r--   0 root         (0) root         (0)     1145 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_bwd_4096.cu
--rw-r--r--   0 root         (0) root         (0)     1085 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_bwd_512.cu
--rw-r--r--   0 root         (0) root         (0)     1145 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_bwd_5120.cu
--rw-r--r--   0 root         (0) root         (0)     1084 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_bwd_6144.cu
--rw-r--r--   0 root         (0) root         (0)     1084 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_bwd_7168.cu
--rw-r--r--   0 root         (0) root         (0)     1085 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_bwd_768.cu
--rw-r--r--   0 root         (0) root         (0)     1084 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_bwd_8192.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_fwd_1024.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-03-29 20:52:04.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_fwd_128.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_fwd_1280.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_fwd_1536.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_fwd_2048.cu
--rw-r--r--   0 root         (0) root         (0)     1032 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_fwd_256.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_fwd_2560.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_fwd_3072.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_fwd_4096.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_fwd_512.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_fwd_5120.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_fwd_6144.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_fwd_7168.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_fwd_768.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_fwd_8192.cu
--rw-r--r--   0 root         (0) root         (0)    11720 2023-03-29 19:53:46.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_res_fwd_kernel.cuh
--rw-r--r--   0 root         (0) root         (0)      977 2023-03-27 04:43:57.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_residual_bwd_512.cu
--rw-r--r--   0 root         (0) root         (0)    24916 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_residual_bwd_kernels.cuh
--rw-r--r--   0 root         (0) root         (0)    11515 2023-03-29 20:50:46.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_residual_fwd_kernel.cuh
--rw-r--r--   0 root         (0) root         (0)    12530 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_parallel_residual_fwd_kernels.cuh
--rw-r--r--   0 root         (0) root         (0)    29989 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/ln_utils.cuh
--rw-r--r--   0 root         (0) root         (0)     1278 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/layer_norm/static_switch.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:58.486266 flash_attn-1.0.8/csrc/rotary/
--rw-r--r--   0 root         (0) root         (0)     1806 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/rotary/rotary.cpp
--rw-r--r--   0 root         (0) root         (0)     1984 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/rotary/rotary_cuda.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:06:58.530083 flash_attn-1.0.8/csrc/xentropy/
--rw-r--r--   0 root         (0) root         (0)     2290 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/xentropy/interface.cpp
--rw-r--r--   0 root         (0) root         (0)    25783 2023-04-16 00:48:37.000000 flash_attn-1.0.8/csrc/xentropy/xentropy_kernel.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:07:00.660642 flash_attn-1.0.8/flash_attn/
--rw-rw-r--   0 root         (0) root         (0)       22 2023-07-03 00:04:35.000000 flash_attn-1.0.8/flash_attn/__init__.py
--rw-rw-r--   0 root         (0) root         (0)    20845 2022-10-31 02:25:05.000000 flash_attn-1.0.8/flash_attn/attention_kernl.py
--rw-r--r--   0 root         (0) root         (0)     5898 2023-04-16 00:48:37.000000 flash_attn-1.0.8/flash_attn/bert_padding.py
--rw-r--r--   0 root         (0) root         (0)     4722 2023-04-16 00:48:37.000000 flash_attn-1.0.8/flash_attn/flash_attention.py
--rw-r--r--   0 root         (0) root         (0)    20506 2023-07-03 00:05:13.000000 flash_attn-1.0.8/flash_attn/flash_attn_interface.py
--rw-r--r--   0 root         (0) root         (0)    38148 2023-04-16 00:48:37.000000 flash_attn-1.0.8/flash_attn/flash_attn_triton.py
--rw-r--r--   0 root         (0) root         (0)    10593 2023-04-16 00:48:37.000000 flash_attn-1.0.8/flash_attn/flash_attn_triton_og.py
--rw-r--r--   0 root         (0) root         (0)     8255 2022-11-18 03:30:00.000000 flash_attn-1.0.8/flash_attn/flash_attn_triton_single_query.py
--rw-r--r--   0 root         (0) root         (0)    37797 2023-03-17 09:16:10.000000 flash_attn-1.0.8/flash_attn/flash_attn_triton_tmp.py
--rw-r--r--   0 root         (0) root         (0)    10640 2023-03-12 08:48:14.000000 flash_attn-1.0.8/flash_attn/flash_attn_triton_tmp_og.py
--rw-rw-r--   0 root         (0) root         (0)    22919 2022-10-31 00:28:55.000000 flash_attn-1.0.8/flash_attn/flash_attn_triton_varlen.py
--rw-r--r--   0 root         (0) root         (0)     6819 2022-06-26 00:59:43.000000 flash_attn-1.0.8/flash_attn/flash_blocksparse_attention.py
--rw-r--r--   0 root         (0) root         (0)     7036 2022-06-26 00:59:43.000000 flash_attn-1.0.8/flash_attn/flash_blocksparse_attn_interface.py
--rw-r--r--   0 root         (0) root         (0)     7902 2023-04-16 00:48:37.000000 flash_attn-1.0.8/flash_attn/fused_softmax.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:07:00.805745 flash_attn-1.0.8/flash_attn/layers/
--rw-r--r--   0 root         (0) root         (0)        0 2023-04-16 00:48:37.000000 flash_attn-1.0.8/flash_attn/layers/__init__.py
--rw-r--r--   0 root         (0) root         (0)     2039 2023-04-16 00:48:37.000000 flash_attn-1.0.8/flash_attn/layers/patch_embed.py
--rw-r--r--   0 root         (0) root         (0)    12609 2023-07-02 23:36:17.000000 flash_attn-1.0.8/flash_attn/layers/rotary.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:07:00.962658 flash_attn-1.0.8/flash_attn/losses/
--rw-r--r--   0 root         (0) root         (0)        0 2023-04-16 00:48:37.000000 flash_attn-1.0.8/flash_attn/losses/__init__.py
--rw-r--r--   0 root         (0) root         (0)     6697 2023-04-16 00:48:37.000000 flash_attn-1.0.8/flash_attn/losses/cross_entropy.py
--rw-r--r--   0 root         (0) root         (0)     2122 2022-12-18 05:19:38.000000 flash_attn-1.0.8/flash_attn/losses/cross_entropy_apex.py
--rw-r--r--   0 root         (0) root         (0)     6649 2022-12-23 22:38:19.000000 flash_attn-1.0.8/flash_attn/losses/cross_entropy_parallel.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:07:02.255524 flash_attn-1.0.8/flash_attn/models/
--rw-r--r--   0 root         (0) root         (0)        0 2023-04-16 00:48:37.000000 flash_attn-1.0.8/flash_attn/models/__init__.py
--rw-r--r--   0 root         (0) root         (0)    26570 2023-04-18 20:33:07.000000 flash_attn-1.0.8/flash_attn/models/bert.py
--rw-r--r--   0 root         (0) root         (0)    38025 2023-06-02 19:10:34.000000 flash_attn-1.0.8/flash_attn/models/gpt.py
--rw-r--r--   0 root         (0) root         (0)     4863 2023-03-22 21:08:53.000000 flash_attn-1.0.8/flash_attn/models/gpt_j.py
--rw-r--r--   0 root         (0) root         (0)     5025 2023-04-16 00:48:37.000000 flash_attn-1.0.8/flash_attn/models/gpt_neox.py
--rw-r--r--   0 root         (0) root         (0)     4365 2023-04-18 20:33:24.000000 flash_attn-1.0.8/flash_attn/models/gptj.py
--rw-r--r--   0 root         (0) root         (0)     5761 2023-04-19 04:11:30.000000 flash_attn-1.0.8/flash_attn/models/llama.py
--rw-r--r--   0 root         (0) root         (0)     5130 2023-04-18 20:33:17.000000 flash_attn-1.0.8/flash_attn/models/opt.py
--rw-r--r--   0 root         (0) root         (0)    13621 2023-04-16 00:48:37.000000 flash_attn-1.0.8/flash_attn/models/vit.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:07:02.355763 flash_attn-1.0.8/flash_attn/modules/
--rw-r--r--   0 root         (0) root         (0)        0 2023-04-16 00:48:37.000000 flash_attn-1.0.8/flash_attn/modules/__init__.py
--rw-r--r--   0 root         (0) root         (0)    16313 2023-06-02 18:01:52.000000 flash_attn-1.0.8/flash_attn/modules/block.py
--rw-r--r--   0 root         (0) root         (0)     8620 2023-04-16 00:48:37.000000 flash_attn-1.0.8/flash_attn/modules/embedding.py
--rw-r--r--   0 root         (0) root         (0)    34955 2023-07-03 00:05:13.000000 flash_attn-1.0.8/flash_attn/modules/mha.py
--rw-r--r--   0 root         (0) root         (0)     2221 2023-07-02 06:31:30.000000 flash_attn-1.0.8/flash_attn/modules/mlp.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:07:02.468316 flash_attn-1.0.8/flash_attn/ops/
--rw-r--r--   0 root         (0) root         (0)        0 2023-04-16 00:48:37.000000 flash_attn-1.0.8/flash_attn/ops/__init__.py
--rw-r--r--   0 root         (0) root         (0)     3002 2023-04-16 00:48:37.000000 flash_attn-1.0.8/flash_attn/ops/activations.py
--rw-r--r--   0 root         (0) root         (0)    26087 2023-04-18 10:32:08.000000 flash_attn-1.0.8/flash_attn/ops/fused_dense.py
--rw-r--r--   0 root         (0) root         (0)     2685 2023-04-13 20:57:51.000000 flash_attn-1.0.8/flash_attn/ops/gelu_activation.py
--rw-r--r--   0 root         (0) root         (0)    18366 2023-04-19 04:25:05.000000 flash_attn-1.0.8/flash_attn/ops/layer_norm.py
--rw-r--r--   0 root         (0) root         (0)     3672 2023-04-18 22:26:53.000000 flash_attn-1.0.8/flash_attn/ops/rms_norm.py
--rw-r--r--   0 root         (0) root         (0)     5855 2023-04-16 00:20:27.000000 flash_attn-1.0.8/flash_attn/rotary.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:07:02.509500 flash_attn-1.0.8/flash_attn/triton/
--rw-rw-r--   0 root         (0) root         (0)        0 2022-11-18 00:51:48.000000 flash_attn-1.0.8/flash_attn/triton/__init__.py
--rw-rw-r--   0 root         (0) root         (0)    14332 2022-10-23 23:52:09.000000 flash_attn-1.0.8/flash_attn/triton/fused_attention.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:07:02.793624 flash_attn-1.0.8/flash_attn/utils/
--rw-r--r--   0 root         (0) root         (0)        0 2023-04-16 00:48:37.000000 flash_attn-1.0.8/flash_attn/utils/__init__.py
--rw-r--r--   0 root         (0) root         (0)     5909 2023-04-16 00:48:37.000000 flash_attn-1.0.8/flash_attn/utils/benchmark.py
--rw-r--r--   0 root         (0) root         (0)     5545 2023-04-16 00:48:37.000000 flash_attn-1.0.8/flash_attn/utils/distributed.py
--rw-r--r--   0 root         (0) root         (0)    14105 2023-04-21 18:28:20.000000 flash_attn-1.0.8/flash_attn/utils/generation.py
--rw-r--r--   0 root         (0) root         (0)     1824 2023-04-16 00:48:37.000000 flash_attn-1.0.8/flash_attn/utils/pretrained.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-03 00:07:00.749731 flash_attn-1.0.8/flash_attn.egg-info/
--rw-rw-r--   0 root         (0) root         (0)    10197 2023-07-03 00:05:23.000000 flash_attn-1.0.8/flash_attn.egg-info/PKG-INFO
--rw-rw-r--   0 root         (0) root         (0)   118178 2023-07-03 00:05:45.000000 flash_attn-1.0.8/flash_attn.egg-info/SOURCES.txt
--rw-rw-r--   0 root         (0) root         (0)        1 2023-07-03 00:05:23.000000 flash_attn-1.0.8/flash_attn.egg-info/dependency_links.txt
--rw-rw-r--   0 root         (0) root         (0)       29 2023-07-03 00:05:24.000000 flash_attn-1.0.8/flash_attn.egg-info/requires.txt
--rw-rw-r--   0 root         (0) root         (0)       27 2023-07-03 00:05:24.000000 flash_attn-1.0.8/flash_attn.egg-info/top_level.txt
--rw-rw-r--   0 root         (0) root         (0)      112 2023-05-25 23:52:09.000000 flash_attn-1.0.8/pyproject.toml
--rw-rw-r--   0 root         (0) root         (0)       38 2023-07-03 00:07:02.818727 flash_attn-1.0.8/setup.cfg
--rw-r--r--   0 root         (0) root         (0)     8044 2023-05-19 16:41:14.000000 flash_attn-1.0.8/setup.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:08.030051 flash_attn-1.0.9/
+-rw-r--r--   0 root         (0) root         (0)       56 2022-11-17 23:40:55.000000 flash_attn-1.0.9/AUTHORS
+-rw-r--r--   0 root         (0) root         (0)     1558 2022-09-09 19:08:03.000000 flash_attn-1.0.9/LICENSE
+-rw-r--r--   0 root         (0) root         (0)      251 2023-04-16 00:48:36.000000 flash_attn-1.0.9/MANIFEST.in
+-rw-rw-r--   0 root         (0) root         (0)    10197 2023-07-17 10:19:08.024764 flash_attn-1.0.9/PKG-INFO
+-rw-r--r--   0 root         (0) root         (0)     9689 2023-05-25 23:52:21.000000 flash_attn-1.0.9/README.md
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.760420 flash_attn-1.0.9/csrc/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.869229 flash_attn-1.0.9/csrc/flash_attn/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.560240 flash_attn-1.0.9/csrc/flash_attn/cutlass/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.890489 flash_attn-1.0.9/csrc/flash_attn/cutlass/cmake/
+-rw-r--r--   0 root         (0) root         (0)     2023 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/cmake/nop.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.254948 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.912239 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/00_basic_gemm/
+-rw-r--r--   0 root         (0) root         (0)    14698 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/00_basic_gemm/basic_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.932576 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/01_cutlass_utilities/
+-rw-r--r--   0 root         (0) root         (0)    13255 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/01_cutlass_utilities/cutlass_utilities.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.954599 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/02_dump_reg_shmem/
+-rw-r--r--   0 root         (0) root         (0)     7157 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/02_dump_reg_shmem/dump_reg_shmem.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.034545 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/03_visualize_layout/
+-rw-r--r--   0 root         (0) root         (0)     4478 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/03_visualize_layout/options.h
+-rw-r--r--   0 root         (0) root         (0)     7081 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/03_visualize_layout/register_layout.cu
+-rw-r--r--   0 root         (0) root         (0)     2691 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/03_visualize_layout/register_layout.h
+-rw-r--r--   0 root         (0) root         (0)     5819 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/03_visualize_layout/visualize_layout.cpp
+-rw-r--r--   0 root         (0) root         (0)    11415 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/03_visualize_layout/visualize_layout.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.055167 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/04_tile_iterator/
+-rw-r--r--   0 root         (0) root         (0)     8226 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/04_tile_iterator/tile_iterator.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.075244 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/05_batched_gemm/
+-rw-r--r--   0 root         (0) root         (0)    15161 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/05_batched_gemm/batched_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.095573 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/06_splitK_gemm/
+-rw-r--r--   0 root         (0) root         (0)    17570 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/06_splitK_gemm/splitk_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.115767 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/07_volta_tensorop_gemm/
+-rw-r--r--   0 root         (0) root         (0)    18280 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/07_volta_tensorop_gemm/volta_tensorop_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.136182 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/08_turing_tensorop_gemm/
+-rw-r--r--   0 root         (0) root         (0)    18226 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/08_turing_tensorop_gemm/turing_tensorop_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.156347 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/09_turing_tensorop_conv2dfprop/
+-rw-r--r--   0 root         (0) root         (0)    28124 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/09_turing_tensorop_conv2dfprop/turing_tensorop_conv2dfprop.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.177762 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/10_planar_complex/
+-rw-r--r--   0 root         (0) root         (0)    21947 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/10_planar_complex/planar_complex.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.197916 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/11_planar_complex_array/
+-rw-r--r--   0 root         (0) root         (0)    23244 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/11_planar_complex_array/planar_complex_array.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.218608 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/12_gemm_bias_relu/
+-rw-r--r--   0 root         (0) root         (0)    13151 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/12_gemm_bias_relu/gemm_bias_relu.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.546240 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/
+-rw-r--r--   0 root         (0) root         (0)    26102 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_conv2d_run.h
+-rw-r--r--   0 root         (0) root         (0)    22877 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_gemm_run.h
+-rw-r--r--   0 root         (0) root         (0)    28268 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_conv2d_run.h
+-rw-r--r--   0 root         (0) root         (0)    24493 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_gemm_run.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.581635 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/
+-rw-r--r--   0 root         (0) root         (0)    15552 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/b2b_gemm.h
+-rw-r--r--   0 root         (0) root         (0)    11520 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/b2b_implicit_gemm_convolution.h
+-rw-r--r--   0 root         (0) root         (0)     8756 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_rf.cu
+-rw-r--r--   0 root         (0) root         (0)     8759 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_shmem.cu
+-rw-r--r--   0 root         (0) root         (0)     8712 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_rf.cu
+-rw-r--r--   0 root         (0) root         (0)     8762 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_shmem.cu
+-rw-r--r--   0 root         (0) root         (0)     8787 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_rf.cu
+-rw-r--r--   0 root         (0) root         (0)     8793 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_shmem.cu
+-rw-r--r--   0 root         (0) root         (0)     8711 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_rf.cu
+-rw-r--r--   0 root         (0) root         (0)     8775 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_shmem.cu
+-rw-r--r--   0 root         (0) root         (0)     7269 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_rf.cu
+-rw-r--r--   0 root         (0) root         (0)     7338 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_shmem.cu
+-rw-r--r--   0 root         (0) root         (0)     7294 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_rf.cu
+-rw-r--r--   0 root         (0) root         (0)     7359 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_shmem.cu
+-rw-r--r--   0 root         (0) root         (0)     7362 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_rf.cu
+-rw-r--r--   0 root         (0) root         (0)     7430 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_shmem.cu
+-rw-r--r--   0 root         (0) root         (0)     7627 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_rf.cu
+-rw-r--r--   0 root         (0) root         (0)     7634 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_shmem.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.722921 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/
+-rw-r--r--   0 root         (0) root         (0)    16152 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_gemm.h
+-rw-r--r--   0 root         (0) root         (0)    18151 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_implicit_gemm_convolution.h
+-rw-r--r--   0 root         (0) root         (0)     3973 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop.h
+-rw-r--r--   0 root         (0) root         (0)    26762 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm75.h
+-rw-r--r--   0 root         (0) root         (0)    26775 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm80.h
+-rw-r--r--   0 root         (0) root         (0)    28422 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm75.h
+-rw-r--r--   0 root         (0) root         (0)    28073 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm80.h
+-rw-r--r--   0 root         (0) root         (0)    17111 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm.h
+-rw-r--r--   0 root         (0) root         (0)    15658 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm_smem_accumulator.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.035203 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/reference/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.746531 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/reference/device/
+-rw-r--r--   0 root         (0) root         (0)    10368 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/reference/device/tensor_scale_bias.h
+-rw-r--r--   0 root         (0) root         (0)     3577 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/test_run.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.949405 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/
+-rw-r--r--   0 root         (0) root         (0)    31616 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    31443 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage_smem_accumulator.h
+-rw-r--r--   0 root         (0) root         (0)    21010 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)    20493 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined_smem_accumulator.h
+-rw-r--r--   0 root         (0) root         (0)     7983 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base.h
+-rw-r--r--   0 root         (0) root         (0)     6047 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base_smem_accumulator.h
+-rw-r--r--   0 root         (0) root         (0)    33788 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    33506 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage_smem_accumulator.h
+-rw-r--r--   0 root         (0) root         (0)    21451 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)    21065 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined_smem_accumulator.h
+-rw-r--r--   0 root         (0) root         (0)    27144 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma.h
+-rw-r--r--   0 root         (0) root         (0)    27400 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma_smem_accumulator.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.972803 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/14_ampere_tf32_tensorop_gemm/
+-rw-r--r--   0 root         (0) root         (0)    18020 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/14_ampere_tf32_tensorop_gemm/ampere_tf32_tensorop_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.995189 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/15_ampere_sparse_tensorop_gemm/
+-rw-r--r--   0 root         (0) root         (0)    15042 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/15_ampere_sparse_tensorop_gemm/ampere_sparse_tensorop_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.015483 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/16_ampere_tensorop_conv2dfprop/
+-rw-r--r--   0 root         (0) root         (0)    27755 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/16_ampere_tensorop_conv2dfprop/ampere_tensorop_conv2dfprop.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.037174 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/17_fprop_per_channel_bias/
+-rw-r--r--   0 root         (0) root         (0)    12580 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/17_fprop_per_channel_bias/fprop_per_channel_bias.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.057270 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/18_ampere_fp64_tensorop_affine2_gemm/
+-rw-r--r--   0 root         (0) root         (0)    14007 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/18_ampere_fp64_tensorop_affine2_gemm/ampere_fp64_tensorop_affine2_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.078023 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/19_tensorop_canonical/
+-rw-r--r--   0 root         (0) root         (0)    13401 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/19_tensorop_canonical/tensorop_canonical.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.098693 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/20_simt_canonical/
+-rw-r--r--   0 root         (0) root         (0)    12556 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/20_simt_canonical/simt_canonical.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.119272 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/21_quaternion_gemm/
+-rw-r--r--   0 root         (0) root         (0)    17319 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/21_quaternion_gemm/quaternion_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.139926 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/22_quaternion_conv/
+-rw-r--r--   0 root         (0) root         (0)    21495 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/22_quaternion_conv/quaternion_conv.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.160615 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/23_ampere_gemm_operand_reduction_fusion/
+-rw-r--r--   0 root         (0) root         (0)    27530 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/23_ampere_gemm_operand_reduction_fusion/ampere_gemm_operand_reduction_fusion.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.180930 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/24_gemm_grouped/
+-rw-r--r--   0 root         (0) root         (0)    50996 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/24_gemm_grouped/gemm_grouped.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.217414 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/
+-rw-r--r--   0 root         (0) root         (0)    26547 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_3d_fprop_mainloop_fusion.cu
+-rw-r--r--   0 root         (0) root         (0)    25628 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_fprop_mainloop_fusion.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.237774 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/26_ampere_wgrad_mainloop_fusion/
+-rw-r--r--   0 root         (0) root         (0)    25538 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/26_ampere_wgrad_mainloop_fusion/ampere_wgrad_mainloop_fusion.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.258229 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/
+-rw-r--r--   0 root         (0) root         (0)    30446 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.279356 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/
+-rw-r--r--   0 root         (0) root         (0)    28159 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/ampere_3xtf32_fast_accurate_tensorop_fprop.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.299724 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/
+-rw-r--r--   0 root         (0) root         (0)    28403 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.320169 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/30_wgrad_split_k/
+-rw-r--r--   0 root         (0) root         (0)    27329 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/30_wgrad_split_k/30_wgrad_split_k.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.340319 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/31_basic_syrk/
+-rw-r--r--   0 root         (0) root         (0)    15206 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/31_basic_syrk/basic_syrk.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.360436 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/32_basic_trmm/
+-rw-r--r--   0 root         (0) root         (0)    15907 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/32_basic_trmm/basic_trmm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.380552 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/33_ampere_3xtf32_tensorop_symm/
+-rw-r--r--   0 root         (0) root         (0)    31803 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/33_ampere_3xtf32_tensorop_symm/ampere_3xtf32_tensorop_symm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.401820 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/34_transposed_conv2d/
+-rw-r--r--   0 root         (0) root         (0)    22378 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/34_transposed_conv2d/34_transposed_conv2d.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.454304 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/35_gemm_softmax/
+-rw-r--r--   0 root         (0) root         (0)    23114 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_softmax.cu
+-rw-r--r--   0 root         (0) root         (0)    16723 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_with_epilogue_visitor.h
+-rw-r--r--   0 root         (0) root         (0)    18713 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_with_softmax.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.474489 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/36_gather_scatter_fusion/
+-rw-r--r--   0 root         (0) root         (0)    20795 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/36_gather_scatter_fusion/gather_scatter_fusion.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.526117 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/
+-rw-r--r--   0 root         (0) root         (0)    31111 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu
+-rw-r--r--   0 root         (0) root         (0)    13982 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h
+-rw-r--r--   0 root         (0) root         (0)    33916 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.546987 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/38_syr2k_grouped/
+-rw-r--r--   0 root         (0) root         (0)    47455 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/38_syr2k_grouped/syr2k_grouped.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.568570 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/39_gemm_permute/
+-rw-r--r--   0 root         (0) root         (0)    37896 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/39_gemm_permute/gemm_permute.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.791070 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/
+-rw-r--r--   0 root         (0) root         (0)    18389 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/attention_scaling_coefs_updater.h
+-rw-r--r--   0 root         (0) root         (0)     8286 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/debug_utils.h
+-rw-r--r--   0 root         (0) root         (0)     9888 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/default_fmha_grouped.h
+-rw-r--r--   0 root         (0) root         (0)    22349 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)     9162 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_rescale_output.h
+-rw-r--r--   0 root         (0) root         (0)     6111 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_thread_apply_logsumexp.h
+-rw-r--r--   0 root         (0) root         (0)     6768 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/find_default_mma.h
+-rw-r--r--   0 root         (0) root         (0)    29972 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h
+-rw-r--r--   0 root         (0) root         (0)     6666 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped_problem_visitor.h
+-rw-r--r--   0 root         (0) root         (0)    37104 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu
+-rw-r--r--   0 root         (0) root         (0)    39975 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.857765 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/
+-rw-r--r--   0 root         (0) root         (0)     3994 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma.h
+-rw-r--r--   0 root         (0) root         (0)     6241 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_base.h
+-rw-r--r--   0 root         (0) root         (0)    27198 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    14090 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)    12089 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm_kernel_utils.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.932241 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/
+-rw-r--r--   0 root         (0) root         (0)    23805 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/epilogue_predicated_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)     3142 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/make_residual_last.h
+-rw-r--r--   0 root         (0) root         (0)    64480 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_access_iterator_residual_last.h
+-rw-r--r--   0 root         (0) root         (0)    64500 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_iterator_residual_last.h
+-rw-r--r--   0 root         (0) root         (0)    37905 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h
+-rw-r--r--   0 root         (0) root         (0)    61195 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.957981 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/42_ampere_tensorop_group_conv/
+-rw-r--r--   0 root         (0) root         (0)    23901 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/42_ampere_tensorop_group_conv/ampere_tensorop_group_conv.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.979523 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/43_ell_block_sparse_gemm/
+-rw-r--r--   0 root         (0) root         (0)    23867 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/43_ell_block_sparse_gemm/ell_block_sparse_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:47.013797 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.209645 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.199160 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:47.087340 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/
+-rw-r--r--   0 root         (0) root         (0)     6370 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_bias_act_epilogue_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     4099 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_thread_map_tensor_op_for_fused_bias.h
+-rw-r--r--   0 root         (0) root         (0)     8285 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/fused_bias_act_epilogue.h
+-rw-r--r--   0 root         (0) root         (0)    10439 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/output_tile_thread_map_for_fused_bias.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:47.107500 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/warp/
+-rw-r--r--   0 root         (0) root         (0)     6848 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/warp/fused_bias_act_fragment_iterator_tensor_op.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.214256 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:47.130835 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/warp/
+-rw-r--r--   0 root         (0) root         (0)    14747 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/warp/mma_tensor_op_fragment_iterator_without_output_op.h
+-rw-r--r--   0 root         (0) root         (0)    10231 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/leaky_bias.h
+-rw-r--r--   0 root         (0) root         (0)     3745 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/utils.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:47.181167 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:47.201437 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/device/
+-rw-r--r--   0 root         (0) root         (0)    15362 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/device/dual_gemm.h
+-rw-r--r--   0 root         (0) root         (0)     8109 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm.cu
+-rw-r--r--   0 root         (0) root         (0)    26478 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm_run.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:47.222190 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/kernel/
+-rw-r--r--   0 root         (0) root         (0)    16424 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/kernel/dual_gemm.h
+-rw-r--r--   0 root         (0) root         (0)     3577 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/test_run.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:47.242488 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/thread/
+-rw-r--r--   0 root         (0) root         (0)     5818 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/thread/left_silu_and_mul.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:47.292738 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/
+-rw-r--r--   0 root         (0) root         (0)    15613 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_epilogue.h
+-rw-r--r--   0 root         (0) root         (0)     7264 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_base.h
+-rw-r--r--   0 root         (0) root         (0)    28984 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_multistage.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:47.314089 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/46_depthwise_simt_conv2dfprop/
+-rw-r--r--   0 root         (0) root         (0)    24464 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/46_depthwise_simt_conv2dfprop/depthwise_simt_conv2dfprop.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:47.334599 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/47_ampere_gemm_universal_streamk/
+-rw-r--r--   0 root         (0) root         (0)    22677 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/47_ampere_gemm_universal_streamk/ampere_gemm_universal_streamk.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:47.355425 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/60_cutlass_import/
+-rw-r--r--   0 root         (0) root         (0)     2849 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/60_cutlass_import/main.cpp
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:47.376262 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/common/
+-rw-r--r--   0 root         (0) root         (0)     2621 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/common/helper.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.266734 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:48.037540 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/
+-rw-r--r--   0 root         (0) root         (0)     3793 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/aligned_buffer.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:48.364668 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/
+-rw-r--r--   0 root         (0) root         (0)     3538 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/arch.h
+-rw-r--r--   0 root         (0) root         (0)     2691 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/cache_operation.h
+-rw-r--r--   0 root         (0) root         (0)    14313 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/memory.h
+-rw-r--r--   0 root         (0) root         (0)    10490 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm75.h
+-rw-r--r--   0 root         (0) root         (0)    15166 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm80.h
+-rw-r--r--   0 root         (0) root         (0)     8073 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma.h
+-rw-r--r--   0 root         (0) root         (0)    11096 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm50.h
+-rw-r--r--   0 root         (0) root         (0)     7040 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm60.h
+-rw-r--r--   0 root         (0) root         (0)     4193 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm61.h
+-rw-r--r--   0 root         (0) root         (0)    16554 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm70.h
+-rw-r--r--   0 root         (0) root         (0)    31682 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm75.h
+-rw-r--r--   0 root         (0) root         (0)    55573 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm80.h
+-rw-r--r--   0 root         (0) root         (0)     4430 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm90.h
+-rw-r--r--   0 root         (0) root         (0)    43978 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sparse_sm80.h
+-rw-r--r--   0 root         (0) root         (0)     3998 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/simd.h
+-rw-r--r--   0 root         (0) root         (0)     3656 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/simd_sm60.h
+-rw-r--r--   0 root         (0) root         (0)     5102 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/simd_sm61.h
+-rw-r--r--   0 root         (0) root         (0)     8473 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/wmma.h
+-rw-r--r--   0 root         (0) root         (0)     5286 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm70.h
+-rw-r--r--   0 root         (0) root         (0)     7746 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm72.h
+-rw-r--r--   0 root         (0) root         (0)     7616 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm75.h
+-rw-r--r--   0 root         (0) root         (0)    62709 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/array.h
+-rw-r--r--   0 root         (0) root         (0)     3662 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/array_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)    13154 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/array_subbyte.h
+-rw-r--r--   0 root         (0) root         (0)     6371 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/barrier.h
+-rw-r--r--   0 root         (0) root         (0)    13371 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/bfloat16.h
+-rw-r--r--   0 root         (0) root         (0)     6338 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/blas3.h
+-rw-r--r--   0 root         (0) root         (0)     9372 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/block_striped.h
+-rw-r--r--   0 root         (0) root         (0)    19422 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/complex.h
+-rw-r--r--   0 root         (0) root         (0)    47943 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/constants.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:48.418141 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/
+-rw-r--r--   0 root         (0) root         (0)    22725 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/conv2d_problem_size.h
+-rw-r--r--   0 root         (0) root         (0)    16292 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/conv3d_problem_size.h
+-rw-r--r--   0 root         (0) root         (0)     6664 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/convolution.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:48.468593 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/device/
+-rw-r--r--   0 root         (0) root         (0)     9744 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/device/direct_convolution.h
+-rw-r--r--   0 root         (0) root         (0)    12078 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/device/implicit_gemm_convolution.h
+-rw-r--r--   0 root         (0) root         (0)    10044 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/device/implicit_gemm_convolution_fusion.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:48.769540 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/
+-rw-r--r--   0 root         (0) root         (0)     7671 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d.h
+-rw-r--r--   0 root         (0) root         (0)    53546 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_dgrad.h
+-rw-r--r--   0 root         (0) root         (0)    56838 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop.h
+-rw-r--r--   0 root         (0) root         (0)    11953 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_fusion.h
+-rw-r--r--   0 root         (0) root         (0)     4658 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_broadcast.h
+-rw-r--r--   0 root         (0) root         (0)     4660 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_reduction.h
+-rw-r--r--   0 root         (0) root         (0)    15891 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_group_fprop.h
+-rw-r--r--   0 root         (0) root         (0)    28745 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad.h
+-rw-r--r--   0 root         (0) root         (0)    10459 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad_fusion.h
+-rw-r--r--   0 root         (0) root         (0)     9324 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_dgrad.h
+-rw-r--r--   0 root         (0) root         (0)    14864 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop.h
+-rw-r--r--   0 root         (0) root         (0)    11980 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop_fusion.h
+-rw-r--r--   0 root         (0) root         (0)    14883 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_wgrad.h
+-rw-r--r--   0 root         (0) root         (0)    19294 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_depthwise_fprop.h
+-rw-r--r--   0 root         (0) root         (0)    18048 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/direct_convolution.h
+-rw-r--r--   0 root         (0) root         (0)    15454 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution.h
+-rw-r--r--   0 root         (0) root         (0)    15709 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_fusion.h
+-rw-r--r--   0 root         (0) root         (0)    17131 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_strided_dgrad.h
+-rw-r--r--   0 root         (0) root         (0)    16749 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_with_fused_epilogue.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:48.789924 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/thread/
+-rw-r--r--   0 root         (0) root         (0)     9689 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/thread/depthwise_mma.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:49.515409 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/
+-rw-r--r--   0 root         (0) root         (0)    15306 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    19735 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)    18940 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    26137 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)    10953 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    11529 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_few_channels.h
+-rw-r--r--   0 root         (0) root         (0)    11333 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_fixed_channels.h
+-rw-r--r--   0 root         (0) root         (0)    13664 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)    10627 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)     9314 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_few_channels.h
+-rw-r--r--   0 root         (0) root         (0)     9018 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_fixed_channels.h
+-rw-r--r--   0 root         (0) root         (0)    10387 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)    30197 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_params.h
+-rw-r--r--   0 root         (0) root         (0)    11202 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    10350 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    11520 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)     9043 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    10832 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)     8450 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)     9569 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)    11020 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    15014 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)     9634 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    15132 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)     7945 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)     8891 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)    18249 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_params.h
+-rw-r--r--   0 root         (0) root         (0)     9971 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    12024 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)     8821 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    10744 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)     8871 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_direct_conv_params.h
+-rw-r--r--   0 root         (0) root         (0)    10747 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_fixed_stride_dilation.h
+-rw-r--r--   0 root         (0) root         (0)     9899 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_optimized.h
+-rw-r--r--   0 root         (0) root         (0)    20899 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_direct_conv_multistage.h
+-rw-r--r--   0 root         (0) root         (0)     8921 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_filter_tile_access_iterator_direct_conv_optimized.h
+-rw-r--r--   0 root         (0) root         (0)    12744 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)     8097 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_mma_base.h
+-rw-r--r--   0 root         (0) root         (0)    36697 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_mma_core_with_lane_access_size.h
+-rw-r--r--   0 root         (0) root         (0)    30106 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_fprop_fusion_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    20086 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    12174 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)    26320 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_wgrad_fusion_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    16915 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_access_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    12476 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_iterator.h
+-rw-r--r--   0 root         (0) root         (0)     8050 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/threadblock_swizzle.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:49.567108 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/warp/
+-rw-r--r--   0 root         (0) root         (0)    12419 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/warp/mma_depthwise_simt.h
+-rw-r--r--   0 root         (0) root         (0)    30655 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/warp/mma_depthwise_simt_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)     8772 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/warp/scale_bias_relu_transform.h
+-rw-r--r--   0 root         (0) root         (0)    11827 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/coord.h
+-rw-r--r--   0 root         (0) root         (0)    11077 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/core_io.h
+-rw-r--r--   0 root         (0) root         (0)     7838 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/cutlass.h
+-rw-r--r--   0 root         (0) root         (0)     3108 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/device_kernel.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.318316 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:49.922597 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/
+-rw-r--r--   0 root         (0) root         (0)    18909 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/activation.h
+-rw-r--r--   0 root         (0) root         (0)     4691 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/conversion_op.h
+-rw-r--r--   0 root         (0) root         (0)     9349 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination.h
+-rw-r--r--   0 root         (0) root         (0)     8344 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_elementwise.h
+-rw-r--r--   0 root         (0) root         (0)    13490 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_relu.h
+-rw-r--r--   0 root         (0) root         (0)    23649 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_clamp.h
+-rw-r--r--   0 root         (0) root         (0)     9067 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_dgelu.h
+-rw-r--r--   0 root         (0) root         (0)    15195 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_drelu.h
+-rw-r--r--   0 root         (0) root         (0)     3669 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_gelu.h
+-rw-r--r--   0 root         (0) root         (0)     8065 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_generic.h
+-rw-r--r--   0 root         (0) root         (0)     3693 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_hardswish.h
+-rw-r--r--   0 root         (0) root         (0)     8344 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_leaky_relu.h
+-rw-r--r--   0 root         (0) root         (0)     3058 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_params.h
+-rw-r--r--   0 root         (0) root         (0)     9351 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)    20486 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_relu.h
+-rw-r--r--   0 root         (0) root         (0)    19348 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_relu0.h
+-rw-r--r--   0 root         (0) root         (0)    11855 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_residual_block.h
+-rw-r--r--   0 root         (0) root         (0)     3688 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_sigmoid.h
+-rw-r--r--   0 root         (0) root         (0)     3669 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_silu.h
+-rw-r--r--   0 root         (0) root         (0)     8662 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_with_elementwise.h
+-rw-r--r--   0 root         (0) root         (0)     3416 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/reduction_op.h
+-rw-r--r--   0 root         (0) root         (0)     2656 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/scale_type.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:50.606997 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/
+-rw-r--r--   0 root         (0) root         (0)     9142 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     9441 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op_blas3.h
+-rw-r--r--   0 root         (0) root         (0)     3234 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_direct_store.h
+-rw-r--r--   0 root         (0) root         (0)     7209 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)    13385 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_simt.h
+-rw-r--r--   0 root         (0) root         (0)    27150 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     7129 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op_blas3.h
+-rw-r--r--   0 root         (0) root         (0)    10846 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     5817 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_broadcast.h
+-rw-r--r--   0 root         (0) root         (0)     5763 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_reduction.h
+-rw-r--r--   0 root         (0) root         (0)     5947 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     4409 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_simt.h
+-rw-r--r--   0 root         (0) root         (0)     7398 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     7303 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_volta_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     4098 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_wmma_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     4678 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/direct_store_epilogue_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    19214 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue.h
+-rw-r--r--   0 root         (0) root         (0)     8279 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_base.h
+-rw-r--r--   0 root         (0) root         (0)     7455 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_base_streamk.h
+-rw-r--r--   0 root         (0) root         (0)    13424 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_depthwise.h
+-rw-r--r--   0 root         (0) root         (0)    13933 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_direct_store.h
+-rw-r--r--   0 root         (0) root         (0)     7401 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_gemm_k_reduction.h
+-rw-r--r--   0 root         (0) root         (0)    14610 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)     9073 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_smem_accumulator.h
+-rw-r--r--   0 root         (0) root         (0)    16804 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_visitor_with_softmax.h
+-rw-r--r--   0 root         (0) root         (0)    52430 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_broadcast.h
+-rw-r--r--   0 root         (0) root         (0)    29199 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_reduction.h
+-rw-r--r--   0 root         (0) root         (0)    13454 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h
+-rw-r--r--   0 root         (0) root         (0)     7308 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_workspace.h
+-rw-r--r--   0 root         (0) root         (0)    14359 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/interleaved_epilogue.h
+-rw-rw-r--   0 root         (0) root         (0)     2912 2022-06-02 16:47:41.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/output_iterator_parameter.h
+-rw-r--r--   0 root         (0) root         (0)    19750 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/output_tile_thread_map.h
+-rw-r--r--   0 root         (0) root         (0)    40870 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    18821 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine.h
+-rw-r--r--   0 root         (0) root         (0)     5636 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine_layout_params.h
+-rw-r--r--   0 root         (0) root         (0)    21249 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_blas3.h
+-rw-r--r--   0 root         (0) root         (0)    13872 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_direct_conv.h
+-rw-r--r--   0 root         (0) root         (0)    14496 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_params.h
+-rw-r--r--   0 root         (0) root         (0)     9146 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_predicates.h
+-rw-r--r--   0 root         (0) root         (0)    15536 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_strided_dgrad.h
+-rw-r--r--   0 root         (0) root         (0)     7487 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    17683 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_mixed.h
+-rw-r--r--   0 root         (0) root         (0)     7394 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_pitch_liner.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:50.855581 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/
+-rw-r--r--   0 root         (0) root         (0)     7055 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_complex_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     7736 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_gaussian_complex_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     5880 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_simt.h
+-rw-r--r--   0 root         (0) root         (0)     9883 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     8924 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_volta_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     6045 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_wmma_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     4864 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/simt_policy.h
+-rw-r--r--   0 root         (0) root         (0)     5979 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tensor_op_policy.h
+-rw-r--r--   0 root         (0) root         (0)    25658 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_simt.h
+-rw-r--r--   0 root         (0) root         (0)    20290 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)    22857 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op_mixed.h
+-rw-r--r--   0 root         (0) root         (0)    14258 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_volta_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     7704 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_wmma_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     7485 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/volta_tensor_op_policy.h
+-rw-r--r--   0 root         (0) root         (0)     3916 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/wmma_tensor_op_policy.h
+-rw-r--r--   0 root         (0) root         (0)    26026 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/fast_math.h
+-rw-r--r--   0 root         (0) root         (0)    35325 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/float8.h
+-rw-r--r--   0 root         (0) root         (0)     2645 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/floating_point_nvrtc.h
+-rw-r--r--   0 root         (0) root         (0)    11242 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/functional.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:50.875664 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:51.220565 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/
+-rw-r--r--   0 root         (0) root         (0)    17023 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/base_grouped.h
+-rw-r--r--   0 root         (0) root         (0)    24413 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/default_gemm_configuration.h
+-rw-r--r--   0 root         (0) root         (0)    27616 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/ell_gemm.h
+-rw-r--r--   0 root         (0) root         (0)    25202 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm.h
+-rw-r--r--   0 root         (0) root         (0)    22367 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_array.h
+-rw-r--r--   0 root         (0) root         (0)    22375 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_batched.h
+-rw-r--r--   0 root         (0) root         (0)    22725 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_complex.h
+-rw-r--r--   0 root         (0) root         (0)     2591 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_grouped.h
+-rw-r--r--   0 root         (0) root         (0)    13736 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_layernorm_mainloop_fusion.h
+-rw-r--r--   0 root         (0) root         (0)    17329 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_sparse.h
+-rw-r--r--   0 root         (0) root         (0)    20450 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_splitk_parallel.h
+-rw-r--r--   0 root         (0) root         (0)    14902 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal.h
+-rw-r--r--   0 root         (0) root         (0)     7444 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_adapter.h
+-rw-r--r--   0 root         (0) root         (0)    13352 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_base.h
+-rw-r--r--   0 root         (0) root         (0)    13968 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_with_broadcast.h
+-rw-r--r--   0 root         (0) root         (0)    14853 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_with_k_reduction.h
+-rw-r--r--   0 root         (0) root         (0)     5690 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemv.h
+-rw-r--r--   0 root         (0) root         (0)    18127 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_2k.h
+-rw-r--r--   0 root         (0) root         (0)     2747 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_2k_grouped.h
+-rw-r--r--   0 root         (0) root         (0)    16719 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_k.h
+-rwxr-xr-x   0 root         (0) root         (0)    21050 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/symm.h
+-rw-r--r--   0 root         (0) root         (0)    26464 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/trmm.h
+-rw-r--r--   0 root         (0) root         (0)    11570 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/gemm.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:52.115850 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/
+-rw-r--r--   0 root         (0) root         (0)    29360 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_ell_gemm.h
+-rw-r--r--   0 root         (0) root         (0)    37752 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm.h
+-rw-r--r--   0 root         (0) root         (0)    16130 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_complex.h
+-rw-r--r--   0 root         (0) root         (0)    12385 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped.h
+-rw-r--r--   0 root         (0) root         (0)     6592 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped_softmax_mainloop_fusion.h
+-rw-r--r--   0 root         (0) root         (0)     5848 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_layernorm_mainloop_fusion.h
+-rw-r--r--   0 root         (0) root         (0)    11104 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_planar_complex_universal.h
+-rw-r--r--   0 root         (0) root         (0)     7983 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_sparse.h
+-rw-r--r--   0 root         (0) root         (0)     4932 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_splitk_parallel.h
+-rw-r--r--   0 root         (0) root         (0)    11951 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_universal.h
+-rw-r--r--   0 root         (0) root         (0)     8063 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_broadcast.h
+-rw-r--r--   0 root         (0) root         (0)     6457 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_k_reduction.h
+-rw-r--r--   0 root         (0) root         (0)     8086 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_reduction.h
+-rwxr-xr-x   0 root         (0) root         (0)     5349 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemv.h
+-rw-r--r--   0 root         (0) root         (0)    11560 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k.h
+-rw-r--r--   0 root         (0) root         (0)    20509 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_complex.h
+-rw-r--r--   0 root         (0) root         (0)    12470 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_grouped.h
+-rw-r--r--   0 root         (0) root         (0)    10620 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_universal.h
+-rw-r--r--   0 root         (0) root         (0)     9872 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k.h
+-rw-r--r--   0 root         (0) root         (0)    16990 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k_complex.h
+-rw-r--r--   0 root         (0) root         (0)     9444 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k_universal.h
+-rwxr-xr-x   0 root         (0) root         (0)    13375 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm.h
+-rwxr-xr-x   0 root         (0) root         (0)    21830 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm_complex.h
+-rwxr-xr-x   0 root         (0) root         (0)    10315 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm_universal.h
+-rw-r--r--   0 root         (0) root         (0)    10873 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm.h
+-rw-r--r--   0 root         (0) root         (0)    10730 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm_complex.h
+-rw-r--r--   0 root         (0) root         (0)    10850 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm_universal.h
+-rw-r--r--   0 root         (0) root         (0)    28916 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/ell_gemm.h
+-rw-r--r--   0 root         (0) root         (0)    13381 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm.h
+-rw-r--r--   0 root         (0) root         (0)     8717 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_array.h
+-rw-r--r--   0 root         (0) root         (0)     8785 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_batched.h
+-rw-r--r--   0 root         (0) root         (0)    14711 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped.h
+-rw-r--r--   0 root         (0) root         (0)     4691 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped_problem_visitor.h
+-rw-r--r--   0 root         (0) root         (0)    15623 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped_softmax_mainloop_fusion.h
+-rw-r--r--   0 root         (0) root         (0)    27281 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_layernorm_mainloop_fusion.h
+-rwxr-xr-x   0 root         (0) root         (0)     6144 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_params.h
+-rw-r--r--   0 root         (0) root         (0)     5165 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)    22973 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)    18961 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex_array.h
+-rw-r--r--   0 root         (0) root         (0)     8142 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_splitk_parallel.h
+-rw-r--r--   0 root         (0) root         (0)     4291 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_transpose_operands.h
+-rw-r--r--   0 root         (0) root         (0)    22913 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_universal.h
+-rw-r--r--   0 root         (0) root         (0)    41469 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_universal_streamk.h
+-rw-r--r--   0 root         (0) root         (0)    47222 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_with_fused_epilogue.h
+-rw-r--r--   0 root         (0) root         (0)    23629 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_with_k_reduction.h
+-rw-r--r--   0 root         (0) root         (0)     8090 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemv.h
+-rwxr-xr-x   0 root         (0) root         (0)     8979 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemv_batched_strided.h
+-rw-r--r--   0 root         (0) root         (0)    16849 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/grouped_problem_visitor.h
+-rw-r--r--   0 root         (0) root         (0)     7148 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/params_universal_base.h
+-rw-r--r--   0 root         (0) root         (0)    22962 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped.h
+-rw-r--r--   0 root         (0) root         (0)    16100 2023-04-15 15:41:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped_problem_visitor.h
+-rw-r--r--   0 root         (0) root         (0)     4334 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_transpose_operands.h
+-rw-r--r--   0 root         (0) root         (0)    24162 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_universal.h
+-rw-r--r--   0 root         (0) root         (0)    17567 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_k_universal.h
+-rw-r--r--   0 root         (0) root         (0)    13610 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/sparse_gemm.h
+-rwxr-xr-x   0 root         (0) root         (0)    23900 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/symm_universal.h
+-rw-r--r--   0 root         (0) root         (0)    19537 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/trmm_universal.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:52.182437 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/
+-rw-r--r--   0 root         (0) root         (0)     3567 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma.h
+-rw-r--r--   0 root         (0) root         (0)    15373 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm50.h
+-rw-r--r--   0 root         (0) root         (0)    29987 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm60.h
+-rw-r--r--   0 root         (0) root         (0)     8142 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm61.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:52.842889 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/
+-rw-r--r--   0 root         (0) root         (0)    31930 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_ell_mma.h
+-rwxr-xr-x   0 root         (0) root         (0)     6979 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_gemv_core.h
+-rw-r--r--   0 root         (0) root         (0)    34241 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma.h
+-rw-r--r--   0 root         (0) root         (0)     5123 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core.h
+-rw-r--r--   0 root         (0) root         (0)    57426 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_simt.h
+-rw-r--r--   0 root         (0) root         (0)    19257 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm70.h
+-rw-r--r--   0 root         (0) root         (0)    42310 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm75.h
+-rw-r--r--   0 root         (0) root         (0)   103000 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm80.h
+-rw-r--r--   0 root         (0) root         (0)    32106 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h
+-rw-r--r--   0 root         (0) root         (0)    12645 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_access_size.h
+-rw-r--r--   0 root         (0) root         (0)     7387 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_reduction.h
+-rw-r--r--   0 root         (0) root         (0)    20975 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_wmma.h
+-rw-r--r--   0 root         (0) root         (0)     7998 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_layernorm_mainloop_fusion.h
+-rw-r--r--   0 root         (0) root         (0)     5110 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_multistage.h
+-rw-r--r--   0 root         (0) root         (0)     4627 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)     7113 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_softmax_mainloop_fusion.h
+-rw-r--r--   0 root         (0) root         (0)     6323 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_with_reduction.h
+-rw-r--r--   0 root         (0) root         (0)     7121 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex.h
+-rw-r--r--   0 root         (0) root         (0)     4959 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core.h
+-rw-r--r--   0 root         (0) root         (0)    65201 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core_sm80.h
+-rw-r--r--   0 root         (0) root         (0)    25495 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_trmm_complex.h
+-rw-r--r--   0 root         (0) root         (0)     8509 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_sparse_mma.h
+-rw-r--r--   0 root         (0) root         (0)    19515 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_trmm.h
+-rw-r--r--   0 root         (0) root         (0)    24047 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/ell_mma_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    13836 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/ell_mma_pipelined.h
+-rwxr-xr-x   0 root         (0) root         (0)     4726 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/gemv.h
+-rw-r--r--   0 root         (0) root         (0)     3652 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/index_remat.h
+-rw-r--r--   0 root         (0) root         (0)     7823 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_base.h
+-rw-r--r--   0 root         (0) root         (0)    27415 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_blas3_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    32894 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_layernorm_mainloop_fusion_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    28015 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    15995 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)     6901 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_base.h
+-rw-r--r--   0 root         (0) root         (0)    22653 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    14746 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)     9864 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_singlestage.h
+-rw-r--r--   0 root         (0) root         (0)    27061 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_softmax_mainloop_fusion_multistage.h
+-rw-r--r--   0 root         (0) root         (0)     9210 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_sparse_base.h
+-rw-r--r--   0 root         (0) root         (0)    25333 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_sparse_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    20473 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_with_reduction_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    15007 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle.h
+-rw-r--r--   0 root         (0) root         (0)    26621 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle_streamk.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:53.397215 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/
+-rw-r--r--   0 root         (0) root         (0)    20553 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_complex_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     6684 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_sparse_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     5160 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     9026 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op_sm80.h
+-rw-r--r--   0 root         (0) root         (0)     4053 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_with_reduction_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     4685 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_wmma_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     5725 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/layernorm_scale_bias_transform.h
+-rw-r--r--   0 root         (0) root         (0)     2619 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma.h
+-rw-r--r--   0 root         (0) root         (0)    37705 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)    23132 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_fast_f32.h
+-rw-r--r--   0 root         (0) root         (0)    78615 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_tile_iterator_sm80.h
+-rw-r--r--   0 root         (0) root         (0)    21205 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)    14589 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op_tile_iterator_sm80.h
+-rw-r--r--   0 root         (0) root         (0)     6144 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)     8446 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt.h
+-rw-r--r--   0 root         (0) root         (0)     3079 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt_policy.h
+-rw-r--r--   0 root         (0) root         (0)    59793 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    11758 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_sparse_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)    14407 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)    15721 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_fast_f32.h
+-rw-rw-r--   0 root         (0) root         (0)    18643 2022-06-02 16:47:41.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_fragment_iterator.h
+-rw-r--r--   0 root         (0) root         (0)     2939 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_policy.h
+-rw-r--r--   0 root         (0) root         (0)     8966 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_sm70.h
+-rw-r--r--   0 root         (0) root         (0)    11017 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_access_iterator.h
+-rw-r--r--   0 root         (0) root         (0)   136033 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    99649 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm70.h
+-rw-r--r--   0 root         (0) root         (0)    75179 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm80.h
+-rw-r--r--   0 root         (0) root         (0)    13151 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sparse.h
+-rw-r--r--   0 root         (0) root         (0)    27101 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_wmma.h
+-rw-r--r--   0 root         (0) root         (0)     7241 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_wmma.h
+-rw-r--r--   0 root         (0) root         (0)    17271 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_with_reduction_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)    19125 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/scale_bias_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)     4610 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/softmax_scale_bias_transform.h
+-rw-r--r--   0 root         (0) root         (0)     8728 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/tile_iterator_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)    23615 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/half.h
+-rw-r--r--   0 root         (0) root         (0)     6893 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/integer_subbyte.h
+-rw-r--r--   0 root         (0) root         (0)     2801 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/kernel_launch.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:53.549120 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/
+-rw-r--r--   0 root         (0) root         (0)     3020 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/layout.h
+-rw-r--r--   0 root         (0) root         (0)    34712 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/matrix.h
+-rw-r--r--   0 root         (0) root         (0)     9133 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/permute.h
+-rw-r--r--   0 root         (0) root         (0)     4696 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/pitch_linear.h
+-rw-r--r--   0 root         (0) root         (0)    18295 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/tensor.h
+-rw-r--r--   0 root         (0) root         (0)    29599 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm70.h
+-rw-r--r--   0 root         (0) root         (0)    33137 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm75.h
+-rw-r--r--   0 root         (0) root         (0)    29336 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm80.h
+-rw-r--r--   0 root         (0) root         (0)     3328 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/vector.h
+-rw-r--r--   0 root         (0) root         (0)   364115 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/matrix.h
+-rw-r--r--   0 root         (0) root         (0)     4991 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/matrix_coord.h
+-rw-r--r--   0 root         (0) root         (0)     2726 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/matrix_shape.h
+-rw-r--r--   0 root         (0) root         (0)    71278 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/numeric_conversion.h
+-rw-r--r--   0 root         (0) root         (0)     3505 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/numeric_types.h
+-rw-r--r--   0 root         (0) root         (0)     5492 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/pitch_linear_coord.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:53.569616 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/platform/
+-rw-r--r--   0 root         (0) root         (0)    26097 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/platform/platform.h
+-rw-r--r--   0 root         (0) root         (0)    15565 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/predicate_vector.h
+-rw-r--r--   0 root         (0) root         (0)    20901 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/quaternion.h
+-rw-r--r--   0 root         (0) root         (0)     2369 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/real.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:53.589365 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:53.659793 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/device/
+-rw-r--r--   0 root         (0) root         (0)     6823 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/device/reduce_split_k.h
+-rw-r--r--   0 root         (0) root         (0)     8152 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce.h
+-rw-r--r--   0 root         (0) root         (0)    11579 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_contiguous.h
+-rw-r--r--   0 root         (0) root         (0)    11448 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_strided.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:53.729681 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/
+-rw-r--r--   0 root         (0) root         (0)     8762 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/reduce_softmax_final.h
+-rw-r--r--   0 root         (0) root         (0)     7897 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/reduce_split_k.h
+-rw-r--r--   0 root         (0) root         (0)    20685 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_contiguous.h
+-rw-r--r--   0 root         (0) root         (0)    21662 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_strided.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:53.765345 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/
+-rw-r--r--   0 root         (0) root         (0)     7208 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/reduce.h
+-rw-r--r--   0 root         (0) root         (0)     6790 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/reduction_operators.h
+-rw-r--r--   0 root         (0) root         (0)     2936 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/threadblock_swizzle.h
+-rw-r--r--   0 root         (0) root         (0)     5929 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/relatively_equal.h
+-rw-r--r--   0 root         (0) root         (0)     4186 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/semaphore.h
+-rw-r--r--   0 root         (0) root         (0)    17243 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/subbyte_reference.h
+-rw-r--r--   0 root         (0) root         (0)     8964 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/tensor_coord.h
+-rw-r--r--   0 root         (0) root         (0)    12207 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/tensor_ref.h
+-rw-r--r--   0 root         (0) root         (0)    11201 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/tensor_ref_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)     9509 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/tensor_view.h
+-rw-r--r--   0 root         (0) root         (0)    10250 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/tensor_view_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)    13017 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/tfloat32.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:53.786094 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/thread/
+-rw-r--r--   0 root         (0) root         (0)     5931 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/thread/matrix.h
+-rw-r--r--   0 root         (0) root         (0)     2581 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/trace.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:53.804532 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/
+-rw-r--r--   0 root         (0) root         (0)    33392 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/pitch_linear_thread_map.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:53.840282 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/thread/
+-rw-r--r--   0 root         (0) root         (0)     3835 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/thread/transpose.h
+-rw-r--r--   0 root         (0) root         (0)     4309 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/thread/unary_op.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:54.240776 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/
+-rw-r--r--   0 root         (0) root         (0)     6181 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    44443 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_access_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    44309 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    12890 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_access_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    11097 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    70684 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    28232 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_2dthreadtile.h
+-rwxr-xr-x   0 root         (0) root         (0)    10243 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_params.h
+-rw-r--r--   0 root         (0) root         (0)    31412 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_triangular_matrix.h
+-rw-r--r--   0 root         (0) root         (0)    62672 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    27175 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h
+-rw-r--r--   0 root         (0) root         (0)    28064 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_triangular_matrix.h
+-rw-r--r--   0 root         (0) root         (0)    13088 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_vector_access_iterator.h
+-rw-r--r--   0 root         (0) root         (0)     8232 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_scale_bias_vector_access_iterator.h
+-rw-r--r--   0 root         (0) root         (0)     2638 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    13283 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear.h
+-rw-r--r--   0 root         (0) root         (0)    18623 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear_direct_conv.h
+-rw-r--r--   0 root         (0) root         (0)    27922 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)    47789 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op_sm80.h
+-rw-r--r--   0 root         (0) root         (0)     2616 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    16510 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear.h
+-rw-r--r--   0 root         (0) root         (0)    15486 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear_2dthreadtile.h
+-rw-r--r--   0 root         (0) root         (0)    36050 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)    43663 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op_sm70.h
+-rw-r--r--   0 root         (0) root         (0)     5226 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/vector_iterator.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:54.260987 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/warp/
+-rw-r--r--   0 root         (0) root         (0)     8828 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/warp/vector_fragment_iterator.h
+-rw-r--r--   0 root         (0) root         (0)     8166 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/uint128.h
+-rw-r--r--   0 root         (0) root         (0)     3359 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/wmma_array.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.407931 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:54.282603 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:54.317731 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/common/
+-rw-r--r--   0 root         (0) root         (0)     4273 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/common/cutlass_unit_test.h
+-rw-r--r--   0 root         (0) root         (0)     4341 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/common/filter_architecture.cpp
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.423378 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:55.312395 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/
+-rw-r--r--   0 root         (0) root         (0)    21797 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/cache_testbed_output.h
+-rw-r--r--   0 root         (0) root         (0)     5344 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     5443 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    11470 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5239 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     9110 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     8485 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5243 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5378 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12054 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_few_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     9603 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_fixed_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5267 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     5357 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5089 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
+-rw-r--r--   0 root         (0) root         (0)    13690 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5390 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5191 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    11136 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     5291 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     3551 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     5157 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu
+-rwxr-xr-x   0 root         (0) root         (0)     8278 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_qf32nhwc_qf32nhwc_qf32nhwc_simt_f32_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    20555 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    20647 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5155 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     5239 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    26114 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    26210 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5111 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     5194 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5738 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5439 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     7363 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     3984 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_reduction_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    39452 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_problems.h
+-rw-r--r--   0 root         (0) root         (0)    14471 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4662 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    26224 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_testbed.h
+-rw-r--r--   0 root         (0) root         (0)    21174 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_testbed_interleaved.h
+-rw-r--r--   0 root         (0) root         (0)     5179 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     5358 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5264 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     3615 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     7591 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    10514 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5157 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5772 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    23460 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_with_broadcast_testbed.h
+-rw-r--r--   0 root         (0) root         (0)    21512 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_with_reduction_testbed.h
+-rw-r--r--   0 root         (0) root         (0)     5135 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5347 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     3736 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     6560 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5257 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12276 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_problems.h
+-rw-r--r--   0 root         (0) root         (0)    21643 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_testbed.h
+-rw-r--r--   0 root         (0) root         (0)     3622 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     6560 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5256 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    17700 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_direct_conv_testbed.h
+-rw-r--r--   0 root         (0) root         (0)    18437 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
+-rw-r--r--   0 root         (0) root         (0)    22194 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_fixed_stride_dilation_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
+-rw-r--r--   0 root         (0) root         (0)     9383 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
+-rw-r--r--   0 root         (0) root         (0)    16100 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/group_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:55.548003 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/
+-rw-r--r--   0 root         (0) root         (0)     7365 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/array.cu
+-rw-r--r--   0 root         (0) root         (0)     7353 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/bfloat16.cu
+-rw-r--r--   0 root         (0) root         (0)     6981 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/complex.cu
+-rw-r--r--   0 root         (0) root         (0)     4009 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/float8.cu
+-rw-r--r--   0 root         (0) root         (0)    13001 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/functional.cu
+-rw-r--r--   0 root         (0) root         (0)     3553 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/half.cu
+-rw-r--r--   0 root         (0) root         (0)     5295 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/matrix.cu
+-rw-r--r--   0 root         (0) root         (0)     8592 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/matrix_coord.cu
+-rw-r--r--   0 root         (0) root         (0)    11508 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/numeric_conversion.cu
+-rw-r--r--   0 root         (0) root         (0)     8148 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/predicate_vector.cu
+-rw-r--r--   0 root         (0) root         (0)     5777 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/quaternion.cu
+-rw-r--r--   0 root         (0) root         (0)     6746 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/tensor_ref.cu
+-rw-r--r--   0 root         (0) root         (0)     8885 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/tensor_view.cu
+-rw-r--r--   0 root         (0) root         (0)     2050 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/test_unit_core.cpp
+-rw-r--r--   0 root         (0) root         (0)     7088 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/tfloat32.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.446234 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:55.601470 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/thread/
+-rw-r--r--   0 root         (0) root         (0)    15818 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/thread/activation.cu
+-rw-r--r--   0 root         (0) root         (0)     6534 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination.cu
+-rw-r--r--   0 root         (0) root         (0)     9964 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination_planar_complex.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:55.811010 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/
+-rw-r--r--   0 root         (0) root         (0)    13824 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_planar_complex.cu
+-rw-r--r--   0 root         (0) root         (0)    27176 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt.cu
+-rw-r--r--   0 root         (0) root         (0)    12061 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm60.cu
+-rw-r--r--   0 root         (0) root         (0)    25275 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm61.cu
+-rw-r--r--   0 root         (0) root         (0)    84612 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_tensor_op.cu
+-rw-r--r--   0 root         (0) root         (0)    70486 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_volta_tensor_op.cu
+-rw-r--r--   0 root         (0) root         (0)    25293 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_tensor_op.cu
+-rw-r--r--   0 root         (0) root         (0)    13012 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_testbed.h
+-rw-r--r--   0 root         (0) root         (0)     7743 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_wmma_tensor_op_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    19178 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/output_tile_threadmap.cu
+-rw-r--r--   0 root         (0) root         (0)    28433 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/predicated_tile_iterator.cu
+-rw-r--r--   0 root         (0) root         (0)    11038 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/testbed.h
+-rw-r--r--   0 root         (0) root         (0)    11734 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/testbed_planar_complex.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:55.860833 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/warp/
+-rw-r--r--   0 root         (0) root         (0)     6783 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_tensor_op.cu
+-rw-r--r--   0 root         (0) root         (0)     7275 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_volta_tensor_op.cu
+-rw-r--r--   0 root         (0) root         (0)     6616 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_wmma_tensor_op.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.481410 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:00.632117 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/
+-rw-r--r--   0 root         (0) root         (0)    10189 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    17899 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8933 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_wmma_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    10164 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    17984 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8915 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_wmma_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16447 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_bf16n_bf16n_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    16575 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_bf16t_bf16t_bf16t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8318 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf32n_cf32t_cf32t_tensor_op_tf32_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8317 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf32t_cf32n_cf32t_tensor_op_tf32_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6714 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6735 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     7895 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7918 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     6516 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6537 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     9016 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     9041 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     4628 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_direct_store_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6165 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     6124 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     9634 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16357 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13189 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8845 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_volta_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    13583 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    13464 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     9571 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16239 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6140 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     9544 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16417 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13075 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8775 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_volta_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    11470 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     6156 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     6116 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     3528 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     3539 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7965 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16470 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13273 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     3648 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8608 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_volta_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    13518 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     3645 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     6096 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32n_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     7845 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    18135 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13008 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8505 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_volta_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    11497 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    11090 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_singlestage_wmma_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     6156 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     6116 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    11066 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_singlestage_wmma_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    17114 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_broadcast_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     3528 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     3540 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7964 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16457 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13266 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8933 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_volta_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    13551 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    13540 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     6130 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32n_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     8160 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_singlestage_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     7847 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16131 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13014 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8754 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_volta_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    11497 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     6147 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     6107 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    13518 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    13398 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     7845 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16149 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6119 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     7827 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16101 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     9526 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7898 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_volta_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    11470 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     3584 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_bf16_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     3473 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12967 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12931 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32t_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12930 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32t_f32n_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12895 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32t_f32t_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8349 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7288 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     8348 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7279 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)    10240 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_grouped_scheduler_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    26146 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    11339 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     7346 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    12397 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6859 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     7239 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8121 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16882 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8407 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_wmma_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     8103 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    17111 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12637 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8388 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_wmma_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    10044 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    17544 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    10020 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    17544 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     9588 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    11288 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7977 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16531 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5693 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_wmma_tensor_op_s32_sm72.cu
+-rw-r--r--   0 root         (0) root         (0)     7959 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16691 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12408 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6864 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_wmma_tensor_op_s32_sm72.cu
+-rw-r--r--   0 root         (0) root         (0)     7744 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16531 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6675 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_wmma_tensor_op_s32_sm72.cu
+-rw-r--r--   0 root         (0) root         (0)     7752 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16484 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6663 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_wmma_tensor_op_s32_sm72.cu
+-rw-r--r--   0 root         (0) root         (0)     4663 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_serial_tensor_op_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     4945 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_simt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     6616 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    10581 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16950 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32n_tf32n_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    16902 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32n_tf32t_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    15131 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32t_tf32n_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    16855 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32t_tf32t_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6854 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_u8t_u8n_s32t_wmma_tensor_op_s32_sm72.cu
+-rw-r--r--   0 root         (0) root         (0)     6686 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf32n_cf32n_cf32n_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6755 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6687 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4726 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32n_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     4718 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32t_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16715 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_broadcast_f16n_f16n_f16n_tensorop_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    12841 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_reduction_f16n_f16n_f16n_tensorop_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     4544 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_reduction_f16t_f16n_f16n_tensorop_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13157 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemv.cu
+-rw-r--r--   0 root         (0) root         (0)     6028 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6031 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6064 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6067 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4897 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64_cf64_cf64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     6088 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6037 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6040 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_rs_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5382 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5406 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5390 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64_cf64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)    13055 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64h_cf64n_tensor_op_f64_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13027 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5388 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6939 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7677 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7725 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     3842 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf64_cf64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     6396 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf64h_cf64n_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    10023 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/multistage_testbed.h
+-rw-r--r--   0 root         (0) root         (0)     9189 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/multistage_testbed_interleaved.h
+-rw-r--r--   0 root         (0) root         (0)    11186 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/rank_2k_grouped_scheduler_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    46795 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    54085 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     8318 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    46687 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     8411 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    46578 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    40533 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_nn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    47656 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_nt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    40441 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_tn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    40354 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_tt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     3513 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_f8gemm_tn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    89517 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_nn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    89304 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_nt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    89304 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_tn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    89091 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_tt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    69175 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_nn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    71438 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_nt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    67796 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_tn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    70056 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_tt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     7156 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61.cu
+-rw-r--r--   0 root         (0) root         (0)     6067 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_perf.cu
+-rw-r--r--   0 root         (0) root         (0)     9063 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_sliced_k.cu
+-rw-r--r--   0 root         (0) root         (0)    35894 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_nn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    35813 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_nt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    35813 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_tn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    35732 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_tt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    70872 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    73136 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     8870 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    69488 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     8865 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    71755 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    33231 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_nn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    33156 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_nt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    33156 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_tn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    33081 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_tt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     5923 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5926 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5959 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5962 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4827 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64_cf64_cf64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     5983 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5932 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5935 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_rs_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    15203 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8623 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    15104 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4765 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64_f64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     8103 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8108 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8088 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8093 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8073 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8078 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8058 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8063 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    15071 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8551 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    14972 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32t_f32t_tensor_op_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5362 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5386 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5356 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5380 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5367 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64_cf64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)    12952 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5368 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7208 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5362 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7199 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64n_tensor_op_f64_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7190 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64t_tensor_op_f64_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4794 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f32n_f32n_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4783 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f32t_f32n_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4728 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64_f64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)    19145 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7991 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    11015 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7976 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12342 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7961 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12321 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64t_tensor_op_f64_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4786 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_tf32n_f32n_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4775 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_tf32t_f32n_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4993 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5017 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4987 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5011 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5012 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64_cf64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     4996 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64n_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     3793 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4990 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    16083 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f32n_f32t_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    16041 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f32t_f32t_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4518 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64_f64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     7451 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64n_f64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     9401 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64t_f64n_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    16027 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_tf32n_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    15985 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_tf32t_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    20333 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed.h
+-rw-r--r--   0 root         (0) root         (0)     8136 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_complex.h
+-rw-r--r--   0 root         (0) root         (0)    20590 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_gemm_with_broadcast.h
+-rw-r--r--   0 root         (0) root         (0)    19346 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_gemm_with_reduction.h
+-rw-r--r--   0 root         (0) root         (0)    16502 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped.h
+-rw-r--r--   0 root         (0) root         (0)    16562 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k.h
+-rw-r--r--   0 root         (0) root         (0)    17002 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k_scheduler.h
+-rw-r--r--   0 root         (0) root         (0)    14698 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_scheduler.h
+-rw-r--r--   0 root         (0) root         (0)    10130 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_interleaved.h
+-rw-r--r--   0 root         (0) root         (0)     9481 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)    20761 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_rank2k_universal.h
+-rw-r--r--   0 root         (0) root         (0)    15562 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_rank_k_universal.h
+-rw-r--r--   0 root         (0) root         (0)     8639 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_sanity.h
+-rw-r--r--   0 root         (0) root         (0)    15769 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_sparse.h
+-rw-r--r--   0 root         (0) root         (0)     6124 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_splitk.h
+-rw-r--r--   0 root         (0) root         (0)    19861 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_symm_universal.h
+-rw-r--r--   0 root         (0) root         (0)    20200 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_trmm_universal.h
+-rw-r--r--   0 root         (0) root         (0)    17311 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_universal.h
+-rw-r--r--   0 root         (0) root         (0)     2626 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_utils.h
+-rw-r--r--   0 root         (0) root         (0)     9916 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     9988 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4977 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64_cf64_cf64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     4992 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     9762 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    15614 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8733 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    14089 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    14444 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32t_tensor_op_fast_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4596 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64_f64_f64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)    12798 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12809 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12764 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64t_f64t_tensor_op_f64_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12768 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12779 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    15504 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8673 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13989 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32n_tensor_op_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    14344 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32t_tensor_op_f32_ls_sm80.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:00.668100 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/kernel/
+-rwxr-xr-x   0 root         (0) root         (0)    46470 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/kernel/batched_gemv.cu
+-rwxr-xr-x   0 root         (0) root         (0)    14362 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/kernel/testbed_gemv.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:00.731975 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/thread/
+-rw-r--r--   0 root         (0) root         (0)     4847 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    12503 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm60.cu
+-rw-r--r--   0 root         (0) root         (0)     3109 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm61.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:00.767570 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/
+-rw-r--r--   0 root         (0) root         (0)     5198 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/gemm_sm60_host.cu
+-rw-r--r--   0 root         (0) root         (0)     7161 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/testbed_host.h
+-rw-r--r--   0 root         (0) root         (0)     7124 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/thread/testbed.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:01.115895 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/
+-rw-r--r--   0 root         (0) root         (0)    25036 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/batched_gemv.cu
+-rw-r--r--   0 root         (0) root         (0)     4345 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/epilogue_workspace.cu
+-rw-r--r--   0 root         (0) root         (0)   135045 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage.cu
+-rw-r--r--   0 root         (0) root         (0)     4644 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_slicedk.cu
+-rw-r--r--   0 root         (0) root         (0)    94442 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse.cu
+-rw-r--r--   0 root         (0) root         (0)    17109 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse_testbed.h
+-rw-r--r--   0 root         (0) root         (0)    13131 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed.h
+-rw-r--r--   0 root         (0) root         (0)    14539 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed_slicedk.h
+-rw-r--r--   0 root         (0) root         (0)    49052 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_simt.cu
+-rw-r--r--   0 root         (0) root         (0)     8407 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_slicedk.cu
+-rw-r--r--   0 root         (0) root         (0)    18705 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    78122 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    21051 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13413 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_testbed.h
+-rw-r--r--   0 root         (0) root         (0)    14239 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_testbed_slicedk.h
+-rw-r--r--   0 root         (0) root         (0)    29772 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    12395 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     3502 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_planar_complex_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12138 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_planar_complex_testbed.h
+-rw-r--r--   0 root         (0) root         (0)    16308 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_singlestage_wmma_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    12502 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_singlestage_wmma_sm75.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:01.362003 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/
+-rw-r--r--   0 root         (0) root         (0)    22128 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_complex_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    10904 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_complex_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     9873 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_gaussian_complex_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    18220 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     4920 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm60.cu
+-rw-r--r--   0 root         (0) root         (0)     6291 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm61.cu
+-rw-r--r--   0 root         (0) root         (0)     9297 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    37942 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    81659 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     9077 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)    48928 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    45327 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/testbed.h
+-rw-r--r--   0 root         (0) root         (0)    25780 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     7544 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm72.cu
+-rw-r--r--   0 root         (0) root         (0)     6487 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm75.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:01.412411 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/layout/
+-rw-r--r--   0 root         (0) root         (0)     5788 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/layout/matrix.cu
+-rw-r--r--   0 root         (0) root         (0)     5984 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/layout/tensor.cu
+-rw-r--r--   0 root         (0) root         (0)     7081 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/layout/tensor_nhwc.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.515245 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.500991 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/cutlass/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:01.439025 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/cutlass/nvrtc/
+-rw-r--r--   0 root         (0) root         (0)     2096 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/cutlass/nvrtc/environment.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.510849 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/kernel/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:01.462366 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/kernel/thread/
+-rw-r--r--   0 root         (0) root         (0)     2915 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/kernel/thread/testbed_kernel.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:01.496581 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/stdlib/
+-rw-rw-r--   0 root         (0) root         (0)        0 2022-06-02 16:47:41.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/stdlib/assert.h
+-rw-r--r--   0 root         (0) root         (0)     4250 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/stdlib/stdint.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:01.531978 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/
+-rw-r--r--   0 root         (0) root         (0)     5727 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/gemm_nvrtc.cu
+-rw-r--r--   0 root         (0) root         (0)    10328 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/testbed.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.538331 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:01.571043 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/device/
+-rw-r--r--   0 root         (0) root         (0)    14684 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_contiguous.cu
+-rw-r--r--   0 root         (0) root         (0)    15609 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_strided.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:01.606550 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/kernel/
+-rw-r--r--   0 root         (0) root         (0)    11350 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk.cu
+-rw-r--r--   0 root         (0) root         (0)     2228 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk_testbed.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:01.642085 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/thread/
+-rw-r--r--   0 root         (0) root         (0)     3110 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/thread/reduction_thread.cu
+-rw-r--r--   0 root         (0) root         (0)     6657 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/thread/testbed.h
+-rw-r--r--   0 root         (0) root         (0)     2047 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/test_unit.cpp
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.548167 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/transform/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:01.680652 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/transform/threadblock/
+-rw-r--r--   0 root         (0) root         (0)    25527 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/transform/threadblock/predicated_tile_iterator.cu
+-rw-r--r--   0 root         (0) root         (0)     9501 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/transform/threadblock/regular_tile_iterator_tensor_op.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:01.716336 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/util/
+-rw-r--r--   0 root         (0) root         (0)     2663 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/util/cutlass_test_levels.cu
+-rw-r--r--   0 root         (0) root         (0)     7474 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/util/tensor_reduce.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.675409 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.649152 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.575322 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.580060 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:01.844772 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/
+-rw-r--r--   0 root         (0) root         (0)     4118 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/arch_mappings.h
+-rw-r--r--   0 root         (0) root         (0)    16013 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/handle.h
+-rw-r--r--   0 root         (0) root         (0)    38340 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/library.h
+-rw-r--r--   0 root         (0) root         (0)     4070 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/manifest.h
+-rw-r--r--   0 root         (0) root         (0)    17934 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/operation_table.h
+-rw-r--r--   0 root         (0) root         (0)     2724 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/singleton.h
+-rw-r--r--   0 root         (0) root         (0)     7904 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/util.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.591539 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.596265 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.600930 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:01.906624 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/
+-rw-r--r--   0 root         (0) root         (0)     2788 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/compiler.h
+-rw-r--r--   0 root         (0) root         (0)     6223 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/cutlass.cpp
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:01.993954 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/
+-rw-r--r--   0 root         (0) root         (0)     2851 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/arch.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:02.043948 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/
+-rw-r--r--   0 root         (0) root         (0)     5897 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/conv_problem_size.h
+-rw-r--r--   0 root         (0) root         (0)     4763 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/convolution.h
+-rw-r--r--   0 root         (0) root         (0)     2650 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/host.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:02.076977 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/
+-rw-r--r--   0 root         (0) root         (0)     6844 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_generic.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:02.280399 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/
+-rw-r--r--   0 root         (0) root         (0)     3047 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/binary_ops.h
+-rw-r--r--   0 root         (0) root         (0)     6134 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/unary_ops.h
+-rw-r--r--   0 root         (0) root         (0)     4702 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_accumulator.h
+-rw-r--r--   0 root         (0) root         (0)     8467 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_binary.h
+-rw-r--r--   0 root         (0) root         (0)     8815 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_broadcast.h
+-rw-r--r--   0 root         (0) root         (0)    13049 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_reduction.h
+-rw-r--r--   0 root         (0) root         (0)     9493 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_linear_combination.h
+-rw-r--r--   0 root         (0) root         (0)     9070 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_broadcast.h
+-rw-r--r--   0 root         (0) root         (0)    12029 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_reduction.h
+-rw-r--r--   0 root         (0) root         (0)     6177 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_input.h
+-rw-r--r--   0 root         (0) root         (0)     8017 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_output.h
+-rw-r--r--   0 root         (0) root         (0)     7235 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_unary.h
+-rw-r--r--   0 root         (0) root         (0)    16970 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_with_layernorm.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:02.330936 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/
+-rw-r--r--   0 root         (0) root         (0)     3673 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/gemm.h
+-rw-r--r--   0 root         (0) root         (0)    22378 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/gemm_universal_with_visitor.h
+-rw-r--r--   0 root         (0) root         (0)     2328 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/host.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:02.380843 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/
+-rw-r--r--   0 root         (0) root         (0)     2115 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/layout.h
+-rw-r--r--   0 root         (0) root         (0)     4337 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/matrix.h
+-rw-r--r--   0 root         (0) root         (0)     3694 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/tensor.h
+-rw-r--r--   0 root         (0) root         (0)     8624 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/swizzling.h
+-rw-r--r--   0 root         (0) root         (0)     3902 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_coord.h
+-rw-r--r--   0 root         (0) root         (0)     5563 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_ref_view.h
+-rw-r--r--   0 root         (0) root         (0)     4855 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/types.h
+-rw-r--r--   0 root         (0) root         (0)      811 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/library.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.640414 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:02.436543 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/
+-rw-r--r--   0 root         (0) root         (0)     2651 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/conv_problems.h
+-rw-r--r--   0 root         (0) root         (0)     2253 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/convolution.h
+-rw-r--r--   0 root         (0) root         (0)     8826 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/host.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:02.472908 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/
+-rw-r--r--   0 root         (0) root         (0)     2139 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/gemm.h
+-rw-r--r--   0 root         (0) root         (0)    18930 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/host.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:02.673188 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/
+-rw-r--r--   0 root         (0) root         (0)    22377 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/conv2d_operation.h
+-rw-r--r--   0 root         (0) root         (0)    13851 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/conv3d_operation.h
+-rw-r--r--   0 root         (0) root         (0)    42129 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/gemm_operation.h
+-rw-r--r--   0 root         (0) root         (0)    35709 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/handle.cu
+-rw-r--r--   0 root         (0) root         (0)    12616 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/library_internal.h
+-rw-r--r--   0 root         (0) root         (0)     3782 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/manifest.cpp
+-rw-r--r--   0 root         (0) root         (0)     5468 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/operation_table.cu
+-rw-r--r--   0 root         (0) root         (0)    12873 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/rank_2k_operation.h
+-rw-r--r--   0 root         (0) root         (0)    11367 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/rank_k_operation.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:02.723985 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reduction/
+-rw-r--r--   0 root         (0) root         (0)     3190 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reduction/init_reduction_operations.cu
+-rw-r--r--   0 root         (0) root         (0)     6367 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reduction/reduction_device.cu
+-rw-r--r--   0 root         (0) root         (0)    10270 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reduction/reduction_operation.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:02.828175 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reference/
+-rw-r--r--   0 root         (0) root         (0)     6746 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reference/conv2d.cu
+-rw-r--r--   0 root         (0) root         (0)     6286 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reference/conv3d.cu
+-rw-r--r--   0 root         (0) root         (0)    17191 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reference/conv_reference_operation.h
+-rw-r--r--   0 root         (0) root         (0)     7199 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reference/gemm.cu
+-rw-r--r--   0 root         (0) root         (0)    14732 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reference/gemm_reference_operation.h
+-rw-r--r--   0 root         (0) root         (0)     2857 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reference/initialize_reference_operations.cu
+-rw-r--r--   0 root         (0) root         (0)     2669 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/singleton.cu
+-rw-r--r--   0 root         (0) root         (0)    13134 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/symm_operation.h
+-rw-r--r--   0 root         (0) root         (0)    11698 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/trmm_operation.h
+-rw-r--r--   0 root         (0) root         (0)    43704 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/util.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.668611 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:03.504174 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/
+-rw-r--r--   0 root         (0) root         (0)    54128 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/conv2d_operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)    18170 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/conv2d_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)    48659 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/conv3d_operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)    16043 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/conv3d_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)    36462 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/cublas_helpers.cu
+-rw-r--r--   0 root         (0) root         (0)    10627 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/cublas_helpers.h
+-rw-r--r--   0 root         (0) root         (0)    17049 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/cudnn_helpers.cpp
+-rw-r--r--   0 root         (0) root         (0)    20433 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/cudnn_helpers.h
+-rw-r--r--   0 root         (0) root         (0)     7233 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/cutlass_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)     3233 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/cutlass_profiler.h
+-rw-r--r--   0 root         (0) root         (0)     2453 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/debug.h
+-rw-r--r--   0 root         (0) root         (0)    53643 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/device_allocation.cu
+-rw-r--r--   0 root         (0) root         (0)     7217 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/device_allocation.h
+-rw-r--r--   0 root         (0) root         (0)     6841 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/device_context.cu
+-rw-r--r--   0 root         (0) root         (0)     4300 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/device_context.h
+-rw-r--r--   0 root         (0) root         (0)     8296 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/enumerated_types.cpp
+-rw-r--r--   0 root         (0) root         (0)     6421 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/enumerated_types.h
+-rw-r--r--   0 root         (0) root         (0)    41919 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/gemm_operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)     8544 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/gemm_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)     3874 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/gpu_timer.cpp
+-rw-r--r--   0 root         (0) root         (0)     2724 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/gpu_timer.h
+-rw-r--r--   0 root         (0) root         (0)     2340 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/main.cpp
+-rw-r--r--   0 root         (0) root         (0)    20944 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)     7876 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)    27172 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/options.cu
+-rw-r--r--   0 root         (0) root         (0)     8773 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/options.h
+-rw-r--r--   0 root         (0) root         (0)    14192 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/performance_report.cpp
+-rw-r--r--   0 root         (0) root         (0)     4337 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/performance_report.h
+-rw-r--r--   0 root         (0) root         (0)     2494 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/performance_result.cu
+-rw-r--r--   0 root         (0) root         (0)     3941 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/performance_result.h
+-rw-r--r--   0 root         (0) root         (0)    37487 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/problem_space.cpp
+-rw-r--r--   0 root         (0) root         (0)    27747 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/problem_space.h
+-rw-r--r--   0 root         (0) root         (0)    25014 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/rank_2k_operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)     6891 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/rank_2k_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)    24253 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/rank_k_operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)     6830 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/rank_k_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)     5452 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/reduction_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)    20688 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)     6471 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)    26610 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/symm_operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)     6933 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/symm_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)    24431 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/trmm_operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)     6599 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/trmm_operation_profiler.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.680166 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.684853 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.689468 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:03.828015 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/
+-rw-r--r--   0 root         (0) root         (0)     9774 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/command_line.h
+-rw-r--r--   0 root         (0) root         (0)     5104 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/debug.h
+-rw-r--r--   0 root         (0) root         (0)     5953 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_dump.h
+-rw-r--r--   0 root         (0) root         (0)    17695 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_groupnorm.h
+-rw-r--r--   0 root         (0) root         (0)    20880 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_layernorm.h
+-rw-r--r--   0 root         (0) root         (0)    10561 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_memory.h
+-rw-r--r--   0 root         (0) root         (0)     5219 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nchw_to_nhwc.h
+-rw-r--r--   0 root         (0) root         (0)    11067 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_padding.h
+-rw-r--r--   0 root         (0) root         (0)    18653 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_pooling.h
+-rw-r--r--   0 root         (0) root         (0)     5214 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_to_nchw.h
+-rw-r--r--   0 root         (0) root         (0)     4007 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_utils.h
+-rw-r--r--   0 root         (0) root         (0)     4597 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/distribution.h
+-rw-r--r--   0 root         (0) root         (0)     2674 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/exceptions.h
+-rw-r--r--   0 root         (0) root         (0)     4821 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_reorder.h
+-rw-r--r--   0 root         (0) root         (0)    16745 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_tensor.h
+-rw-r--r--   0 root         (0) root         (0)    20354 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_tensor_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)     5890 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_uncompress.h
+-rw-r--r--   0 root         (0) root         (0)     1962 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/index_sequence.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.720217 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:03.866755 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/
+-rw-r--r--   0 root         (0) root         (0)     4606 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/inner_product.h
+-rw-r--r--   0 root         (0) root         (0)     3527 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/linear_to_coordinate.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:04.041005 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/
+-rw-r--r--   0 root         (0) root         (0)    48350 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/convolution.h
+-rw-r--r--   0 root         (0) root         (0)    14296 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm.h
+-rw-r--r--   0 root         (0) root         (0)    10524 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm_complex.h
+-rw-r--r--   0 root         (0) root         (0)     9652 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm_planar_complex.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:04.091718 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/
+-rw-r--r--   0 root         (0) root         (0)     5381 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/gemm.h
+-rw-r--r--   0 root         (0) root         (0)     6198 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_elementwise.h
+-rw-r--r--   0 root         (0) root         (0)     5126 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_foreach.h
+-rw-r--r--   0 root         (0) root         (0)    11615 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/rank_2k_complex.h
+-rw-r--r--   0 root         (0) root         (0)     7278 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_compare.h
+-rw-r--r--   0 root         (0) root         (0)    46444 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_fill.h
+-rw-r--r--   0 root         (0) root         (0)     5293 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_foreach.h
+-rw-r--r--   0 root         (0) root         (0)    15964 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_reduce.h
+-rw-r--r--   0 root         (0) root         (0)     4589 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_relu.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:04.111833 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/thread/
+-rw-r--r--   0 root         (0) root         (0)     5872 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/thread/gemm.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:04.411078 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/
+-rw-r--r--   0 root         (0) root         (0)    28439 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/convolution.h
+-rw-r--r--   0 root         (0) root         (0)     2766 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/error_metrics.h
+-rw-r--r--   0 root         (0) root         (0)    17163 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm.h
+-rw-r--r--   0 root         (0) root         (0)     7097 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm_complex.h
+-rw-r--r--   0 root         (0) root         (0)     7708 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)     9441 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k.h
+-rw-r--r--   0 root         (0) root         (0)    11444 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k_complex.h
+-rw-r--r--   0 root         (0) root         (0)     8148 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_k_complex.h
+-rw-r--r--   0 root         (0) root         (0)    10509 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/symm.h
+-rw-r--r--   0 root         (0) root         (0)    12296 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/symm_complex.h
+-rw-r--r--   0 root         (0) root         (0)     8440 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_compare.h
+-rw-r--r--   0 root         (0) root         (0)     8317 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_copy.h
+-rw-r--r--   0 root         (0) root         (0)     9027 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_elementwise.h
+-rw-r--r--   0 root         (0) root         (0)    43961 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_fill.h
+-rw-r--r--   0 root         (0) root         (0)     4756 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_foreach.h
+-rw-r--r--   0 root         (0) root         (0)     2133 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_norm.h
+-rw-r--r--   0 root         (0) root         (0)     6111 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_reduce.h
+-rw-r--r--   0 root         (0) root         (0)     7670 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/trmm.h
+-rw-r--r--   0 root         (0) root         (0)     9874 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/trmm_complex.h
+-rw-r--r--   0 root         (0) root         (0)     8285 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/tensor_view_io.h
+-rw-r--r--   0 root         (0) root         (0)     8809 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/type_traits.h
+-rw-r--r--   0 root         (0) root         (0)    33267 2023-04-26 16:18:56.000000 flash_attn-1.0.9/csrc/flash_attn/fmha_api.cpp
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:05.745953 flash_attn-1.0.9/csrc/flash_attn/src/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:05.858058 flash_attn-1.0.9/csrc/flash_attn/src/fmha/
+-rw-r--r--   0 root         (0) root         (0)    17999 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha/gemm.h
+-rw-r--r--   0 root         (0) root         (0)    22872 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha/gmem_tile.h
+-rw-r--r--   0 root         (0) root         (0)     5997 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha/kernel_traits.h
+-rw-r--r--   0 root         (0) root         (0)     4362 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha/mask.h
+-rw-r--r--   0 root         (0) root         (0)    74010 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha/smem_tile.h
+-rw-r--r--   0 root         (0) root         (0)    25514 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha/softmax.h
+-rw-r--r--   0 root         (0) root         (0)    41059 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha/utils.h
+-rw-r--r--   0 root         (0) root         (0)     7237 2023-04-16 04:28:21.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha.h
+-rw-r--r--   0 root         (0) root         (0)     4118 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_block_dgrad_fp16_kernel_loop.sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    33506 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_block_dgrad_kernel_1xN_loop.h
+-rw-r--r--   0 root         (0) root         (0)     5292 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_block_fprop_fp16_kernel.sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    23207 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_block_fprop_kernel_1xN.h
+-rw-r--r--   0 root         (0) root         (0)     2502 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_blockmask.h
+-rw-r--r--   0 root         (0) root         (0)      465 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_bwd_hdim128.cu
+-rw-r--r--   0 root         (0) root         (0)      727 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_bwd_hdim32.cu
+-rw-r--r--   0 root         (0) root         (0)     1713 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_bwd_hdim64.cu
+-rw-r--r--   0 root         (0) root         (0)     6453 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_bwd_launch_template.h
+-rw-r--r--   0 root         (0) root         (0)    37168 2023-04-16 04:28:21.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_dgrad_kernel_1xN_loop.h
+-rw-r--r--   0 root         (0) root         (0)    31042 2023-04-16 04:28:21.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_fprop_kernel_1xN.h
+-rw-r--r--   0 root         (0) root         (0)      445 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_fwd_hdim128.cu
+-rw-r--r--   0 root         (0) root         (0)      724 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_fwd_hdim32.cu
+-rw-r--r--   0 root         (0) root         (0)      725 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_fwd_hdim64.cu
+-rw-r--r--   0 root         (0) root         (0)     4393 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_fwd_launch_template.h
+-rw-r--r--   0 root         (0) root         (0)     3104 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_kernel.h
+-rw-r--r--   0 root         (0) root         (0)     4892 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_utils.h
+-rw-r--r--   0 root         (0) root         (0)     5462 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/philox.cuh
+-rw-r--r--   0 root         (0) root         (0)     1686 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/static_switch.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:05.881065 flash_attn-1.0.9/csrc/flash_gen/
+-rw-r--r--   0 root         (0) root         (0)     7018 2022-11-21 06:35:03.000000 flash_attn-1.0.9/csrc/flash_gen/decoder_masked_multihead_attention.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:05.977571 flash_attn-1.0.9/csrc/ft_attention/
+-rw-r--r--   0 root         (0) root         (0)     8253 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/ft_attention/cuda_bf16_fallbacks.cuh
+-rw-r--r--   0 root         (0) root         (0)      867 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/ft_attention/cuda_bf16_wrapper.h
+-rw-r--r--   0 root         (0) root         (0)     7069 2023-06-06 06:13:59.000000 flash_attn-1.0.9/csrc/ft_attention/decoder_masked_multihead_attention.cu
+-rw-r--r--   0 root         (0) root         (0)     7627 2023-07-02 20:11:51.000000 flash_attn-1.0.9/csrc/ft_attention/decoder_masked_multihead_attention.h
+-rw-r--r--   0 root         (0) root         (0)    64946 2023-07-03 16:25:44.000000 flash_attn-1.0.9/csrc/ft_attention/decoder_masked_multihead_attention_utils.h
+-rw-r--r--   0 root         (0) root         (0)    10119 2023-07-06 22:24:25.000000 flash_attn-1.0.9/csrc/ft_attention/ft_attention.cpp
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:06.013082 flash_attn-1.0.9/csrc/fused_dense_lib/
+-rw-r--r--   0 root         (0) root         (0)    10179 2023-05-30 21:13:46.000000 flash_attn-1.0.9/csrc/fused_dense_lib/fused_dense.cpp
+-rw-r--r--   0 root         (0) root         (0)    24690 2023-05-30 21:14:57.000000 flash_attn-1.0.9/csrc/fused_dense_lib/fused_dense_cuda.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:06.110020 flash_attn-1.0.9/csrc/fused_softmax/
+-rw-r--r--   0 root         (0) root         (0)     5037 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/fused_softmax/fused_softmax.cpp
+-rw-r--r--   0 root         (0) root         (0)    23616 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/fused_softmax/scaled_masked_softmax.h
+-rw-r--r--   0 root         (0) root         (0)     4209 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/fused_softmax/scaled_masked_softmax_cuda.cu
+-rw-r--r--   0 root         (0) root         (0)    24659 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/fused_softmax/scaled_upper_triang_masked_softmax.h
+-rw-r--r--   0 root         (0) root         (0)     3154 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/fused_softmax/scaled_upper_triang_masked_softmax_cuda.cu
+-rw-r--r--   0 root         (0) root         (0)     1216 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/fused_softmax/type_shim.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:07.226441 flash_attn-1.0.9/csrc/layer_norm/
+-rw-r--r--   0 root         (0) root         (0)     7248 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/layer_norm/ln.h
+-rw-r--r--   0 root         (0) root         (0)    36418 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/layer_norm/ln_api.cpp
+-rw-r--r--   0 root         (0) root         (0)      987 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_1024.cu
+-rw-r--r--   0 root         (0) root         (0)      987 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_1280.cu
+-rw-r--r--   0 root         (0) root         (0)      977 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_1536.cu
+-rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_2048.cu
+-rw-r--r--   0 root         (0) root         (0)      977 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_256.cu
+-rw-r--r--   0 root         (0) root         (0)      977 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_2560.cu
+-rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_3072.cu
+-rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_4096.cu
+-rw-r--r--   0 root         (0) root         (0)      977 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_512.cu
+-rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_5120.cu
+-rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_6144.cu
+-rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_7168.cu
+-rw-r--r--   0 root         (0) root         (0)      977 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_768.cu
+-rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_8192.cu
+-rw-r--r--   0 root         (0) root         (0)    25647 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_kernels.cuh
+-rw-r--r--   0 root         (0) root         (0)    19944 2023-01-19 07:34:02.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_semi_cuda_kernel_old.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_1024.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-01-22 09:05:55.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_10240.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-01-22 09:07:15.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_12288.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2022-12-05 08:45:58.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_128.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_1280.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_1536.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_2048.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_256.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_2560.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_3072.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2022-12-05 08:50:57.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_384.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_4096.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_512.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_5120.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_6144.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_7168.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_768.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_8192.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-01-22 08:41:06.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_9216.cu
+-rw-r--r--   0 root         (0) root         (0)    18000 2022-12-06 21:18:58.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_cuda_kernel_old.cu
+-rw-r--r--   0 root         (0) root         (0)    12721 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_kernels.cuh
+-rw-r--r--   0 root         (0) root         (0)     6655 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_kernel_traits.h
+-rw-r--r--   0 root         (0) root         (0)     1095 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_1024.cu
+-rw-r--r--   0 root         (0) root         (0)     1095 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_1280.cu
+-rw-r--r--   0 root         (0) root         (0)     1085 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_1536.cu
+-rw-r--r--   0 root         (0) root         (0)     1084 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_2048.cu
+-rw-r--r--   0 root         (0) root         (0)     1085 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_256.cu
+-rw-r--r--   0 root         (0) root         (0)     1085 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_2560.cu
+-rw-r--r--   0 root         (0) root         (0)     1084 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_3072.cu
+-rw-r--r--   0 root         (0) root         (0)     1145 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_4096.cu
+-rw-r--r--   0 root         (0) root         (0)     1085 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_512.cu
+-rw-r--r--   0 root         (0) root         (0)     1145 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_5120.cu
+-rw-r--r--   0 root         (0) root         (0)     1084 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_6144.cu
+-rw-r--r--   0 root         (0) root         (0)     1084 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_7168.cu
+-rw-r--r--   0 root         (0) root         (0)     1085 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_768.cu
+-rw-r--r--   0 root         (0) root         (0)     1084 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_8192.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_1024.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_1280.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_1536.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_2048.cu
+-rw-r--r--   0 root         (0) root         (0)     1032 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_256.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_2560.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_3072.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_4096.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_512.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_5120.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_6144.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_7168.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_768.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_8192.cu
+-rw-r--r--   0 root         (0) root         (0)    24916 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_residual_bwd_kernels.cuh
+-rw-r--r--   0 root         (0) root         (0)    12530 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_residual_fwd_kernels.cuh
+-rw-r--r--   0 root         (0) root         (0)    29989 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_utils.cuh
+-rw-r--r--   0 root         (0) root         (0)     1278 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/static_switch.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:07.261442 flash_attn-1.0.9/csrc/rotary/
+-rw-r--r--   0 root         (0) root         (0)     1806 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/rotary/rotary.cpp
+-rw-r--r--   0 root         (0) root         (0)     1984 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/rotary/rotary_cuda.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:07.297073 flash_attn-1.0.9/csrc/xentropy/
+-rw-r--r--   0 root         (0) root         (0)     2290 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/xentropy/interface.cpp
+-rw-r--r--   0 root         (0) root         (0)    25783 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/xentropy/xentropy_kernel.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:07.497263 flash_attn-1.0.9/flash_attn/
+-rw-rw-r--   0 root         (0) root         (0)       22 2023-07-17 10:16:17.000000 flash_attn-1.0.9/flash_attn/__init__.py
+-rw-rw-r--   0 root         (0) root         (0)    20845 2022-10-31 02:25:05.000000 flash_attn-1.0.9/flash_attn/attention_kernl.py
+-rw-r--r--   0 root         (0) root         (0)     5898 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/bert_padding.py
+-rw-r--r--   0 root         (0) root         (0)     4722 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/flash_attention.py
+-rw-r--r--   0 root         (0) root         (0)    20510 2023-07-17 10:13:51.000000 flash_attn-1.0.9/flash_attn/flash_attn_interface.py
+-rw-r--r--   0 root         (0) root         (0)    38148 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/flash_attn_triton.py
+-rw-r--r--   0 root         (0) root         (0)    10593 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/flash_attn_triton_og.py
+-rw-r--r--   0 root         (0) root         (0)     8255 2022-11-18 03:30:00.000000 flash_attn-1.0.9/flash_attn/flash_attn_triton_single_query.py
+-rw-r--r--   0 root         (0) root         (0)    37797 2023-03-17 09:16:10.000000 flash_attn-1.0.9/flash_attn/flash_attn_triton_tmp.py
+-rw-r--r--   0 root         (0) root         (0)    10640 2023-03-12 08:48:14.000000 flash_attn-1.0.9/flash_attn/flash_attn_triton_tmp_og.py
+-rw-r--r--   0 root         (0) root         (0)     6819 2022-06-26 00:59:43.000000 flash_attn-1.0.9/flash_attn/flash_blocksparse_attention.py
+-rw-r--r--   0 root         (0) root         (0)     7036 2022-06-26 00:59:43.000000 flash_attn-1.0.9/flash_attn/flash_blocksparse_attn_interface.py
+-rw-r--r--   0 root         (0) root         (0)     7902 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/fused_softmax.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:07.616086 flash_attn-1.0.9/flash_attn/layers/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/layers/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     2039 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/layers/patch_embed.py
+-rw-r--r--   0 root         (0) root         (0)    12738 2023-07-17 10:14:25.000000 flash_attn-1.0.9/flash_attn/layers/rotary.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:07.650704 flash_attn-1.0.9/flash_attn/losses/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/losses/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     6697 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/losses/cross_entropy.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:07.776771 flash_attn-1.0.9/flash_attn/models/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/models/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    26570 2023-04-18 20:33:07.000000 flash_attn-1.0.9/flash_attn/models/bert.py
+-rw-r--r--   0 root         (0) root         (0)    38025 2023-06-02 19:10:34.000000 flash_attn-1.0.9/flash_attn/models/gpt.py
+-rw-r--r--   0 root         (0) root         (0)     5025 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/models/gpt_neox.py
+-rw-r--r--   0 root         (0) root         (0)     4365 2023-04-18 20:33:24.000000 flash_attn-1.0.9/flash_attn/models/gptj.py
+-rw-r--r--   0 root         (0) root         (0)     5761 2023-04-19 04:11:30.000000 flash_attn-1.0.9/flash_attn/models/llama.py
+-rw-r--r--   0 root         (0) root         (0)     5130 2023-04-18 20:33:17.000000 flash_attn-1.0.9/flash_attn/models/opt.py
+-rw-r--r--   0 root         (0) root         (0)    13621 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/models/vit.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:07.857983 flash_attn-1.0.9/flash_attn/modules/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/modules/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    16313 2023-07-17 10:18:12.000000 flash_attn-1.0.9/flash_attn/modules/block.py
+-rw-r--r--   0 root         (0) root         (0)     8620 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/modules/embedding.py
+-rw-r--r--   0 root         (0) root         (0)    34955 2023-07-17 10:18:12.000000 flash_attn-1.0.9/flash_attn/modules/mha.py
+-rw-r--r--   0 root         (0) root         (0)     2221 2023-07-02 06:31:30.000000 flash_attn-1.0.9/flash_attn/modules/mlp.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:07.939536 flash_attn-1.0.9/flash_attn/ops/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/ops/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     3002 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/ops/activations.py
+-rw-r--r--   0 root         (0) root         (0)    26087 2023-04-18 10:32:08.000000 flash_attn-1.0.9/flash_attn/ops/fused_dense.py
+-rw-r--r--   0 root         (0) root         (0)    19306 2023-07-04 21:52:07.000000 flash_attn-1.0.9/flash_attn/ops/layer_norm.py
+-rw-r--r--   0 root         (0) root         (0)     3672 2023-04-18 22:26:53.000000 flash_attn-1.0.9/flash_attn/ops/rms_norm.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:08.020019 flash_attn-1.0.9/flash_attn/utils/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/utils/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     5909 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/utils/benchmark.py
+-rw-r--r--   0 root         (0) root         (0)     5545 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/utils/distributed.py
+-rw-r--r--   0 root         (0) root         (0)    14105 2023-04-21 18:28:20.000000 flash_attn-1.0.9/flash_attn/utils/generation.py
+-rw-r--r--   0 root         (0) root         (0)     1824 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/utils/pretrained.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:07.567049 flash_attn-1.0.9/flash_attn.egg-info/
+-rw-rw-r--   0 root         (0) root         (0)    10197 2023-07-17 10:18:39.000000 flash_attn-1.0.9/flash_attn.egg-info/PKG-INFO
+-rw-rw-r--   0 root         (0) root         (0)   105073 2023-07-17 10:18:43.000000 flash_attn-1.0.9/flash_attn.egg-info/SOURCES.txt
+-rw-rw-r--   0 root         (0) root         (0)        1 2023-07-17 10:18:39.000000 flash_attn-1.0.9/flash_attn.egg-info/dependency_links.txt
+-rw-rw-r--   0 root         (0) root         (0)       29 2023-07-17 10:18:39.000000 flash_attn-1.0.9/flash_attn.egg-info/requires.txt
+-rw-rw-r--   0 root         (0) root         (0)       27 2023-07-17 10:18:39.000000 flash_attn-1.0.9/flash_attn.egg-info/top_level.txt
+-rw-rw-r--   0 root         (0) root         (0)       38 2023-07-17 10:19:08.028764 flash_attn-1.0.9/setup.cfg
+-rw-r--r--   0 root         (0) root         (0)     8044 2023-07-06 03:55:40.000000 flash_attn-1.0.9/setup.py
```

### Comparing `flash_attn-1.0.8/LICENSE` & `flash_attn-1.0.9/LICENSE`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/PKG-INFO` & `flash_attn-1.0.9/PKG-INFO`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: flash_attn
-Version: 1.0.8
+Version: 1.0.9
 Summary: Flash Attention: Fast and Memory-Efficient Exact Attention
 Home-page: https://github.com/HazyResearch/flash-attention
 Author: Tri Dao
 Author-email: trid@stanford.edu
 License: UNKNOWN
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
```

### Comparing `flash_attn-1.0.8/README.md` & `flash_attn-1.0.9/README.md`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/cmake/nop.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/cmake/nop.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/00_basic_gemm/basic_gemm.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/00_basic_gemm/basic_gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/01_cutlass_utilities/cutlass_utilities.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/01_cutlass_utilities/cutlass_utilities.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/02_dump_reg_shmem/dump_reg_shmem.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/02_dump_reg_shmem/dump_reg_shmem.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/03_visualize_layout/options.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/03_visualize_layout/options.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/03_visualize_layout/register_layout.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/03_visualize_layout/register_layout.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/03_visualize_layout/register_layout.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/03_visualize_layout/register_layout.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/03_visualize_layout/visualize_layout.cpp` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/03_visualize_layout/visualize_layout.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/03_visualize_layout/visualize_layout.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/03_visualize_layout/visualize_layout.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/04_tile_iterator/tile_iterator.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/04_tile_iterator/tile_iterator.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/05_batched_gemm/batched_gemm.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/05_batched_gemm/batched_gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/06_splitK_gemm/splitk_gemm.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/06_splitK_gemm/splitk_gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/07_volta_tensorop_gemm/volta_tensorop_gemm.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/07_volta_tensorop_gemm/volta_tensorop_gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/08_turing_tensorop_gemm/turing_tensorop_gemm.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/08_turing_tensorop_gemm/turing_tensorop_gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/09_turing_tensorop_conv2dfprop/turing_tensorop_conv2dfprop.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/09_turing_tensorop_conv2dfprop/turing_tensorop_conv2dfprop.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/10_planar_complex/planar_complex.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/10_planar_complex/planar_complex.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/11_planar_complex_array/planar_complex_array.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/11_planar_complex_array/planar_complex_array.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/12_gemm_bias_relu/gemm_bias_relu.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/12_gemm_bias_relu/gemm_bias_relu.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_conv2d_run.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_conv2d_run.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_gemm_run.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_gemm_run.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_conv2d_run.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_conv2d_run.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_gemm_run.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_gemm_run.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/b2b_gemm.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/b2b_gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/b2b_implicit_gemm_convolution.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/b2b_implicit_gemm_convolution.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_rf.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_rf.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_shmem.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_shmem.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_rf.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_rf.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_shmem.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_shmem.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_rf.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_rf.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_shmem.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_shmem.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_rf.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_rf.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_shmem.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_shmem.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_rf.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_rf.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_shmem.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_shmem.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_rf.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_rf.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_shmem.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_shmem.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_rf.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_rf.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_shmem.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_shmem.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_rf.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_rf.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_shmem.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_shmem.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_gemm.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_implicit_gemm_convolution.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_implicit_gemm_convolution.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm75.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm75.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm80.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm80.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm75.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm75.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm80.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm80.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm_smem_accumulator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm_smem_accumulator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/reference/device/tensor_scale_bias.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/reference/device/tensor_scale_bias.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/test_run.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/test_run.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage_smem_accumulator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage_smem_accumulator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined_smem_accumulator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined_smem_accumulator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base_smem_accumulator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base_smem_accumulator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage_smem_accumulator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage_smem_accumulator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined_smem_accumulator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined_smem_accumulator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma_smem_accumulator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma_smem_accumulator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/14_ampere_tf32_tensorop_gemm/ampere_tf32_tensorop_gemm.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/14_ampere_tf32_tensorop_gemm/ampere_tf32_tensorop_gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/15_ampere_sparse_tensorop_gemm/ampere_sparse_tensorop_gemm.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/15_ampere_sparse_tensorop_gemm/ampere_sparse_tensorop_gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/16_ampere_tensorop_conv2dfprop/ampere_tensorop_conv2dfprop.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/16_ampere_tensorop_conv2dfprop/ampere_tensorop_conv2dfprop.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/17_fprop_per_channel_bias/fprop_per_channel_bias.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/17_fprop_per_channel_bias/fprop_per_channel_bias.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/18_ampere_fp64_tensorop_affine2_gemm/ampere_fp64_tensorop_affine2_gemm.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/18_ampere_fp64_tensorop_affine2_gemm/ampere_fp64_tensorop_affine2_gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/19_tensorop_canonical/tensorop_canonical.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/19_tensorop_canonical/tensorop_canonical.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/20_simt_canonical/simt_canonical.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/20_simt_canonical/simt_canonical.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/21_quaternion_gemm/quaternion_gemm.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/21_quaternion_gemm/quaternion_gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/22_quaternion_conv/quaternion_conv.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/22_quaternion_conv/quaternion_conv.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/23_ampere_gemm_operand_reduction_fusion/ampere_gemm_operand_reduction_fusion.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/23_ampere_gemm_operand_reduction_fusion/ampere_gemm_operand_reduction_fusion.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/24_gemm_grouped/gemm_grouped.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/24_gemm_grouped/gemm_grouped.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_3d_fprop_mainloop_fusion.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_3d_fprop_mainloop_fusion.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_fprop_mainloop_fusion.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_fprop_mainloop_fusion.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/26_ampere_wgrad_mainloop_fusion/ampere_wgrad_mainloop_fusion.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/26_ampere_wgrad_mainloop_fusion/ampere_wgrad_mainloop_fusion.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/ampere_3xtf32_fast_accurate_tensorop_fprop.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/ampere_3xtf32_fast_accurate_tensorop_fprop.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_3xtf32_complex_gemm.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm.cu`

 * *Files 1% similar despite different names*

```diff
@@ -177,15 +177,15 @@
       << "  --beta=<f32>                Epilogue scalar beta\n\n"
       << "  --rand_mode=<string>        gauss / uniform*\n\n"
       << "  --seed=<int>                Random number seed (1*)\n\n"
       << "  --iterations=<int>          Number of profiling iterations to perform.\n\n"
       << "  --benchmark                 If set (true), performance benchmarking on several layers and batch-size.\n\n";
 
     out << "\n\nExamples:\n\n"
-      << "$ ./examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_3xtf32_complex_gemm --m=1024 --n=512 \\\n"
+      << "$ ./examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_ampere_3xtf32_fast_accurate_complex_gemm --m=1024 --n=512 \\\n"
       << "     --alpha=2 --beta=0.707 \n\n";
 
     return out;
   }
 
   /// Compute performance in GFLOP/s
   double gflops(double runtime_s) const {
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/33_ampere_3xtf32_tensorop_symm/ampere_3xtf32_tensorop_symm.cu`

 * *Files 12% similar despite different names*

```diff
@@ -26,281 +26,283 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /**
-  This example is almost the same as example 27 which uses 3xTF32 to run GEMM.  The only
-  difference is that this example uses 3xtf32 on complex gemm.
+NVIDIA Ampere architecture starts supporting tfloat32 (see include/cutlass/tfloat32.h)
+data types in tensor cores.  One big advantage is that we can load in F32 data and convert them
+implicitly to tf32 inside the SYMM kernel which means no change is needed to accelerate traditional
+F32 data by using NVIDIA Ampere architecture.
 
-  To enable this feature, the only change needs to make is to change OpMultiplyAddComplex
-  to OpMultiplyAddComplexFastF32. 
+We can use the tf32 mode of tensor core to emulate a fast accurate SYMM kernel which is accelerated
+using Ampere Tensor Cores (see include/cutlass/gemm/warp/mma_tensor_op_fast_f32.h). 
+
+The trick is very simple
+  a x b = (a_big + a_small) x (b_big + b_small) = a_big x b_big + a_big x b_small + a_small x b_big
+  big = convert_to_tf32(F32)
+  small = convert_to_tf32(F32 - big)
+
+a_small x b_small is discarded because they are too small.
+
+This example demonstrates usage of this kernel, along with accuracy measurements w.r.t. actual F32 
+results (SSYMM from cuBLAS) and against F64 results (DSYMM from CUTLASS)
+
+To enable this feature, the only change needs to make is to change the default OpMultiplyAdd to 
+OpMultiplyAddFastF32. 
+
+Now, we have two different flavors of SSYMM in the profiler for Ampere:
+
+  s1688symm       // Use 3xTF32 to emulate F32.  F32 in, converted in TF32-big and TF32-small internally,
+                  // accumulated in F32, F32 out.
+  s1688tf32symm   // Use 1xTF32.  F32 in, converted to one TF32 internally, accumulated in F32, F32 out.
 */
 
 #include <iostream>
 #include <vector>
 #include <limits>
 
-#include "cutlass/cutlass.h"
-#include "cutlass/gemm/device/gemm_complex.h"
+#include "cutlass/blas3.h"
+#include "cutlass/gemm/device/symm.h"
 
 #include "cutlass/util/command_line.h"
 #include "cutlass/util/host_tensor.h"
 
-#include "cutlass/util/reference/device/gemm_complex.h"
+#include "cutlass/util/reference/host/symm.h"
 #include "cutlass/util/reference/host/tensor_reduce.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_norm.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/reference/host/error_metrics.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "helper.h"
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Result structure
-struct Result {
-
-  double runtime_ms;
-  double gflops;
-  cutlass::Status status;
-  cudaError_t error;
-
-  int m, n, k;
-  double l2_norm_3xtf32_vs_fp64;
-  double l2_norm_1xtf32_vs_fp64;
-  double l2_norm_fp32_vs_fp64;
-
-  // ctor
-  Result(  
-    int m, int n, int k,
-    double runtime_ms, double gflops,
-    double l2_norm_3xtf32_vs_fp64,
-    double l2_norm_1xtf32_vs_fp64,
-    double l2_norm_fp32_vs_fp64) : 
-    m(m), n(n), k(k),
-    runtime_ms(runtime_ms), gflops(gflops), 
-    l2_norm_3xtf32_vs_fp64(l2_norm_3xtf32_vs_fp64),
-    l2_norm_1xtf32_vs_fp64(l2_norm_1xtf32_vs_fp64),
-    l2_norm_fp32_vs_fp64(l2_norm_fp32_vs_fp64)   {}
-
-  Result() {}
-
-  //
-  // Methods
-  //
-  static void print_csv_header() {
-    std::cout << "M,N,K,Runtime(ms),GFLOPS,3xTF32_vs_FP64,1xTF32_vs_FP64,FP32_vs_FP64" << std::endl;
-  }
-
-  void print_csv_row() {
-    std::cout << m << ","
-              << n << ","
-              << k << ","
-              << runtime_ms << ","
-              << gflops << ","
-              << l2_norm_3xtf32_vs_fp64 << ","
-              << l2_norm_1xtf32_vs_fp64 << ","
-              << l2_norm_fp32_vs_fp64 << std::endl;
-  }
-};
-
-std::vector<Result> results;
+#if CUTLASS_ENABLE_CUBLAS
+#include <cublas_v2.h>
+#endif
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 // Command line options parsing
 struct Options {
 
   bool help;
 
   cutlass::gemm::GemmCoord problem_size;
   float alpha;
   float beta;
   std::string rand_mode;
-
-  int iterations;
   int seed;
-  bool benchmark;
   
   Options():
     help(false),
-    problem_size({3456, 4096, 4096}),
-    iterations(20),
+    problem_size({4096, 4096, 4096}),
     seed(1),
     alpha(1),
     beta(),
-    rand_mode("uniform"),
-    benchmark(false) { }
+    rand_mode("uniform") { }
 
   bool valid() {
+    //
+    // CUTLASS attempts to load 128b vectors of F32 elements. Consequently,
+    // all pointers, strides, and tensor extents must be divisible by 4 elements.
+    //
+    int const kAlignment = 4;
+
+    if ((problem_size.m() % kAlignment) ||
+      (problem_size.n() % kAlignment) ||
+      (problem_size.k() % kAlignment)) {
+
+      // misaligned tensors
+      return false;
+    }
+
     return true;
   }
 
   // Parses the command line
   void parse(int argc, char const **args) {
     cutlass::CommandLine cmd(argc, args);
 
     if (cmd.check_cmd_line_flag("help")) {
       help = true;
     }
 
     cmd.get_cmd_line_argument("m", problem_size.m());
     cmd.get_cmd_line_argument("n", problem_size.n());
-    cmd.get_cmd_line_argument("k", problem_size.k());
+    // Since the kernels in this example are in Left Side Mode
+    cmd.get_cmd_line_argument("m", problem_size.k());
 
     cmd.get_cmd_line_argument("alpha", alpha);
     cmd.get_cmd_line_argument("beta", beta);
     
-    cmd.get_cmd_line_argument("iterations", iterations);
     cmd.get_cmd_line_argument("seed", seed);
     cmd.get_cmd_line_argument("rand_mode", rand_mode);
 
-    if (cmd.check_cmd_line_flag("benchmark")) {
-      benchmark = true;
-    }
   }
 
   /// Prints the usage statement.
   std::ostream & print_usage(std::ostream &out) const {
 
-    out << "29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm example\n\n"
-      << "  This example uses the CUTLASS Library to emulate FP32 complex GEMM computations with TF32 tensor cores.\n\n"
+    out << "33_ampere_3xtf32_tensorop_symm example\n\n"
+      << "  This example uses the CUTLASS Library to execute 3xTF32 tensorop SYMM computations.\n\n"
       << "Options:\n\n"
       << "  --help                      If specified, displays this usage statement.\n\n"
-      << "  --m=<int>                   GEMM M dimension\n"
-      << "  --n=<int>                   GEMM N dimension\n"
-      << "  --k=<int>                   GEMM K dimension\n"
+      << "  --m=<int>                   SYMM M dimension\n"
+      << "  --n=<int>                   SYMM N dimension\n"
       << "  --alpha=<f32>               Epilogue scalar alpha\n"
       << "  --beta=<f32>                Epilogue scalar beta\n\n"
       << "  --rand_mode=<string>        gauss / uniform*\n\n"
-      << "  --seed=<int>                Random number seed (1*)\n\n"
-      << "  --iterations=<int>          Number of profiling iterations to perform.\n\n"
-      << "  --benchmark                 If set (true), performance benchmarking on several layers and batch-size.\n\n";
+      << "  --seed=<int>                Random number seed (1*)\n\n";
 
     out << "\n\nExamples:\n\n"
-      << "$ ./examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_ampere_3xtf32_fast_accurate_complex_gemm --m=1024 --n=512 \\\n"
-      << "     --alpha=2 --beta=0.707 \n\n";
+      << "$ ./examples/33_ampere_3xtf32_tensorop_symm/33_ampere_3xtf32_tensorop_symm --m=1024 --n=512 \\\n"
+      << "     --alpha=2 --beta=1 \n\n";
 
     return out;
   }
-
-  /// Compute performance in GFLOP/s
-  double gflops(double runtime_s) const {
-
-    // Number of real-valued multiply-adds 
-    int64_t fmas = problem_size.product();
-    
-    // Two flops per multiply-add
-    return 2.0 * double(fmas) / double(1.0e9) / runtime_s;
-  }
 };
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 // The code section below describes matrix layout of input and output matrices. Column Major for
-// Matrix A, Row Major for Matrix B and Row Major for Matrix C
+// Matrix A, Matrix B and Matrix C (since that's what cuBLAS supports, CUTLASS supports Row Major too)
 using LayoutInputA = cutlass::layout::ColumnMajor;
-using LayoutInputB = cutlass::layout::RowMajor;
-using LayoutOutput = cutlass::layout::RowMajor;
+using LayoutInputB = cutlass::layout::ColumnMajor;
+using LayoutOutput = cutlass::layout::ColumnMajor;
+
+// Symmetric Matrix A is in Left Side mode
+constexpr cutlass::SideMode SideModeA = cutlass::SideMode::kLeft;
+// Symmetric Matrix A is in Lower Filled mode
+constexpr cutlass::FillMode FillModeA = cutlass::FillMode::kLower;
 
 // This code section describes whether you want to use tensor cores or regular SIMT cores on GPU SM
 using MMAOp = cutlass::arch::OpClassTensorOp;
 
 // This code section describes CUDA SM architecture number
 using SmArch = cutlass::arch::Sm80;
 
 // This code section describes the tile size a thread block will compute
 using ShapeMMAThreadBlock =
-    cutlass::gemm::GemmShape<64, 64, 16>;  // <- threadblock tile M = 128, N = 128, K = 16
+    cutlass::gemm::GemmShape<128, 64, 16>;  // <- threadblock tile M = 128, N = 128, K = 16
 // This code section describes tile size a warp will compute
-using ShapeMMAWarp = cutlass::gemm::GemmShape<32, 32, 16>;  // <- warp tile M = 64, N = 64, K = 16
+using ShapeMMAWarp = cutlass::gemm::GemmShape<64, 32, 16>;  // <- warp tile M = 64, N = 64, K = 16
 // This code section describes the size of MMA op
 using ShapeMMAOp = cutlass::gemm::GemmShape<16, 8, 8>;  // <- MMA Op tile M = 16, N = 8, K = 8
 
 // This code section describes how threadblocks are scheduled on GPU
 using SwizzleThreadBlock = cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>;  // <- ??
 
 // This code section describes the epilogue part of the kernel
 using EpilogueOp = cutlass::epilogue::thread::LinearCombination<
-    cutlass::complex<float>,                 // <- data type of output matrix
-    1,                                       // <- the number of elements per vectorized
-                                             //    memory access. For a byte, it's 16
-                                             //    elements. This becomes the vector width of
-                                             //    math instructions in the epilogue too
-    cutlass::complex<float>,                 // <- data type of accumulator
-    cutlass::complex<float>>;                 // <- data type for alpha/beta in linear combination function
+    float,                                     // <- data type of output matrix
+    128 / cutlass::sizeof_bits<float>::value,  // <- the number of elements per vectorized
+                                                       // memory access. For a byte, it's 16
+                                                       // elements. This becomes the vector width of
+                                                       // math instructions in the epilogue too
+    float,                                   // <- data type of accumulator
+    float>;                                  // <- data type for alpha/beta in linear combination function
 
 // Number of pipelines you want to use
 constexpr int NumStages = 3;
-// Transform
-constexpr cutlass::ComplexTransform TransformA = cutlass::ComplexTransform::kNone;
-constexpr cutlass::ComplexTransform TransformB = cutlass::ComplexTransform::kNone;
+// Alignment 
+constexpr int Alignment = 4;
 
 // 
-// Gemm Operators (Gemm_3xTF32, Gemm_1xTF32, GEMM_F32, GEMM_F64)
+// CUTLASS Symm Operators (SSYM: Symm_3xTF32, Symm_1xTF32, DSYMM: Symm_F64)
 //
 
-// Gemm_3xTF32
-using Gemm_3xTF32 = cutlass::gemm::device::GemmComplex<
-                                              cutlass::complex<float>,
+// Symm_3xTF32
+using Symm_3xTF32 = cutlass::gemm::device::Symm<
+                                              float,
                                               LayoutInputA,
-                                              cutlass::complex<float>,
+                                              SideModeA,
+                                              FillModeA,
+                                              float,
                                               LayoutInputB,
-                                              cutlass::complex<float>,
+                                              float,
                                               LayoutOutput,
-                                              cutlass::complex<float>,
+                                              float,
                                               MMAOp,
                                               SmArch,
                                               ShapeMMAThreadBlock,
                                               ShapeMMAWarp,
                                               ShapeMMAOp,
                                               EpilogueOp,
                                               SwizzleThreadBlock,
                                               NumStages,
-                                              TransformA, 
-                                              TransformB,
-                                              cutlass::arch::OpMultiplyAddComplexFastF32>;
-
-// Gemm_1xTF32
-using Gemm_1xTF32 = cutlass::gemm::device::GemmComplex<
-                                              cutlass::complex<float>,
+                                              1, // Symmetric matrix is always align 1 
+                                              Alignment,
+                                              false,
+                                              cutlass::arch::OpMultiplyAddFastF32>;
+
+// Symm_1xTF32
+using Symm_1xTF32 = cutlass::gemm::device::Symm<
+                                              float,
                                               LayoutInputA,
-                                              cutlass::complex<float>,
+                                              SideModeA,
+                                              FillModeA,
+                                              float,
                                               LayoutInputB,
-                                              cutlass::complex<float>,
+                                              float,
                                               LayoutOutput,
-                                              cutlass::complex<float>,
+                                              float,
                                               MMAOp,
                                               SmArch,
                                               ShapeMMAThreadBlock,
                                               ShapeMMAWarp,
                                               ShapeMMAOp,
                                               EpilogueOp,
                                               SwizzleThreadBlock,
                                               NumStages,
-                                              TransformA, 
-                                              TransformB,
-                                              cutlass::arch::OpMultiplyAddComplex>;
+                                              1, // Symmetric matrix is always align 1 
+                                              Alignment,
+                                              false,
+                                              cutlass::arch::OpMultiplyAdd>;
+
+// Symm_F64
+using Symm_F64 = cutlass::gemm::device::Symm<
+                                              double,
+                                              LayoutInputA,
+                                              SideModeA,
+                                              FillModeA,
+                                              double,
+                                              LayoutInputB,
+                                              double,
+                                              LayoutOutput,
+                                              double,
+                                              cutlass::arch::OpClassTensorOp,
+                                              cutlass::arch::Sm80,
+                                              cutlass::gemm::GemmShape<32, 32, 16>,
+                                              cutlass::gemm::GemmShape<16, 16, 16>,
+                                              cutlass::gemm::GemmShape<8, 8, 4>,
+                                              cutlass::epilogue::thread::LinearCombination<
+                                                double,
+                                                1,
+                                                double,
+                                                double
+                                              >,
+                                              cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+                                              4>;
 
 bool run(Options &options) {
 
   // Create a tuple of problem size for matrix multiplication
   cutlass::gemm::GemmCoord problem_size = options.problem_size;
 
   ////////////////////////////////////////////////////////////////////////////////
   /// 1. Initialize F32 Precision input tensors using CUTLASS helper functions
   ////////////////////////////////////////////////////////////////////////////////
-  cutlass::HostTensor<cutlass::complex<float>, LayoutInputA> tensor_a_F32(problem_size.mk());  // <- Create matrix A with dimensions M x K
-  cutlass::HostTensor<cutlass::complex<float>, LayoutInputB> tensor_b_F32(problem_size.kn());  // <- Create matrix B with dimensions K x N
-  cutlass::HostTensor<cutlass::complex<float>, LayoutOutput> tensor_c_F32(problem_size.mn());  // <- Create matrix C with dimensions M x N
-  cutlass::HostTensor<cutlass::complex<float>, LayoutOutput> tensor_d_F32(problem_size.mn());  // <- Create matrix D with dimensions M x N 
+  cutlass::HostTensor<float, LayoutInputA> tensor_a_F32(problem_size.mk());  // <- Create matrix A with dimensions M x K
+  cutlass::HostTensor<float, LayoutInputB> tensor_b_F32(problem_size.kn());  // <- Create matrix B with dimensions K x N
+  cutlass::HostTensor<float, LayoutOutput> tensor_c_F32(problem_size.mn());  // <- Create matrix C with dimensions M x N
+  cutlass::HostTensor<float, LayoutOutput> tensor_d_F32(problem_size.mn());  // <- Create matrix D with dimensions M x N 
 
   if (options.rand_mode == "uniform") {
     const float min = -1;
     const float max =  1;
     // Fill input and output matrices on host using CUTLASS helper functions
     cutlass::reference::host::TensorFillRandomUniform(
         tensor_a_F32.host_view(),
@@ -341,279 +343,295 @@
   // Copy data from host to GPU
   tensor_a_F32.sync_device();
   tensor_b_F32.sync_device();
   tensor_c_F32.sync_device();
   tensor_d_F32.sync_device();
 
   ////////////////////////////////////////////////////////////////////////////////
-  /// 2. Initialize F64 tensors using the same values used for F32
+  /// 2. Initialize F64 tensors, Output tensors and setup arguments
   ////////////////////////////////////////////////////////////////////////////////
-  // Gemm input operands (A, B, C)
-  cutlass::HostTensor<cutlass::complex<double>, LayoutInputA> tensor_a_F64(problem_size.mk());  // <- Create matrix A with dimensions M x K
-  cutlass::HostTensor<cutlass::complex<double>, LayoutInputB> tensor_b_F64(problem_size.kn());  // <- Create matrix B with dimensions K x N
-  cutlass::HostTensor<cutlass::complex<double>, LayoutOutput> tensor_c_F64(problem_size.mn());  // <- Create matrix C with dimensions M x N
+  // Symm F64 input operands (A, B, C)
+  cutlass::HostTensor<double, LayoutInputA> tensor_a_F64(problem_size.mk());  // <- Create matrix A with dimensions M x K
+  cutlass::HostTensor<double, LayoutInputB> tensor_b_F64(problem_size.kn());  // <- Create matrix B with dimensions K x N
+  cutlass::HostTensor<double, LayoutOutput> tensor_c_F64(problem_size.mn());  // <- Create matrix C with dimensions M x N
   
-  // Gemm output (D) for GEMM_F64
-  cutlass::HostTensor<cutlass::complex<double>, LayoutOutput> tensor_d_F64(problem_size.mn());  // <- Create matrix D with dimensions M x N
-  // Gemm output (D) for GEMM_3xTF32
-  cutlass::HostTensor<cutlass::complex<float>, LayoutOutput> tensor_d_3xTF32(problem_size.mn());  // <- Create matrix D with dimensions M x N
-  // Gemm output (D) for GEMM_1xTF32
-  cutlass::HostTensor<cutlass::complex<float>, LayoutOutput> tensor_d_1xTF32(problem_size.mn());  // <- Create matrix D with dimensions M x N
+  // Symm output (D) for SYMM_3xTF32
+  cutlass::HostTensor<float, LayoutOutput> tensor_d_3xTF32(problem_size.mn());  // <- Create matrix D with dimensions M x N
+  // Symm output (D) for SYMM_1xTF32
+  cutlass::HostTensor<float, LayoutOutput> tensor_d_1xTF32(problem_size.mn());  // <- Create matrix D with dimensions M x N
+  // Symm output (D) for SYMM_F64
+  cutlass::HostTensor<double, LayoutOutput> tensor_d_F64(problem_size.mn());  // <- Create matrix D with dimensions M x N
+#if CUTLASS_ENABLE_CUBLAS
+  // Symm output (D) for SYMM_cublasF32
+  cutlass::HostTensor<float, LayoutOutput> tensor_d_cublasF32(problem_size.mn());  // <- Create matrix D with dimensions M x N
+#endif
 
   // Copy values from the DP tensors
   cutlass::reference::host::TensorCopy(tensor_a_F64.host_view(), tensor_a_F32.host_view());
   cutlass::reference::host::TensorCopy(tensor_b_F64.host_view(), tensor_b_F32.host_view());
   cutlass::reference::host::TensorCopy(tensor_c_F64.host_view(), tensor_c_F32.host_view());
   cutlass::reference::host::TensorCopy(tensor_d_F64.host_view(), tensor_d_F32.host_view());
   cutlass::reference::host::TensorCopy(tensor_d_3xTF32.host_view(), tensor_d_F32.host_view());
   cutlass::reference::host::TensorCopy(tensor_d_1xTF32.host_view(), tensor_d_F32.host_view());
+#if CUTLASS_ENABLE_CUBLAS
+  cutlass::reference::host::TensorCopy(tensor_d_cublasF32.host_view(), tensor_d_F32.host_view());
+#endif
   
   // Copy data from host to GPU
   tensor_a_F64.sync_device();
   tensor_b_F64.sync_device();
   tensor_c_F64.sync_device();
   tensor_d_F64.sync_device();
   tensor_d_3xTF32.sync_device();
   tensor_d_1xTF32.sync_device();
+#if CUTLASS_ENABLE_CUBLAS
+  tensor_d_cublasF32.sync_device();
+#endif
 
   // Initialize alpha and beta for dot product computation
-  cutlass::complex<float> alpha = cutlass::complex<float>(options.alpha);
-  cutlass::complex<float> beta =  cutlass::complex<float>(options.beta);
+  float alpha = float(options.alpha);
+  float beta =  float(options.beta);
 
-  // Split K dimension into 1 partitions
-  int split_k_slices = 1;
+  // Batch count as 1
+  int batch_count = 1;
+
+  // Batch stride for A, when matrix A is in Left Side mode
+  int batch_stride_A = problem_size.m()*problem_size.m();
 
   ////////////////////////////////////////////////////////////////////////////////
-  /// 3. Run  3xTF32 kernel within a profiling loop
+  /// 3. Run 3xTF32 kernel
   ////////////////////////////////////////////////////////////////////////////////
   // Create a tuple of gemm kernel arguments. This is later passed as arguments to launch
   // instantiated CUTLASS kernel
-  typename Gemm_3xTF32::Arguments arguments_3xtf32{problem_size,  // <- problem size of matrix multiplication
-                                     tensor_a_F32.device_ref(),  // <- reference to matrix A on device
-                                     tensor_b_F32.device_ref(),  // <- reference to matrix B on device
-                                     tensor_c_F32.device_ref(),  // <- reference to matrix C on device
-                                     tensor_d_3xTF32.device_ref(),  // <- reference to matrix D on device
-                                     {alpha, beta},          // <- tuple of alpha and beta
-                                     split_k_slices};        // <- k-dimension split factor
+  typename Symm_3xTF32::Arguments arguments_3xtf32{
+                                     cutlass::gemm::GemmUniversalMode::kGemm,
+                                     problem_size,                  // <- problem size of matrix multiplication
+                                     batch_count,                   // <- batch count
+                                     {alpha, beta},                 // <- tuple of alpha and beta
+                                     tensor_a_F32.device_data(),    // <- reference to matrix A on device
+                                     tensor_b_F32.device_data(),    // <- reference to matrix B on device
+                                     tensor_c_F32.device_data(),    // <- reference to matrix C on device
+                                     tensor_d_3xTF32.device_data(), // <- reference to matrix D on device
+                                     batch_stride_A,                // <- batch stride and ld for matrices
+                                     problem_size.m() * problem_size.n(),
+                                     problem_size.m() * problem_size.n(),
+                                     problem_size.m() * problem_size.n(),
+                                     tensor_a_F32.layout().stride(0),
+                                     tensor_b_F32.layout().stride(0),
+                                     tensor_c_F32.layout().stride(0),
+                                     tensor_d_3xTF32.layout().stride(0)
+                                     };
 
   // Using the arguments, query for extra workspace required for matrix multiplication computation
-  size_t workspace_size_3xtf32 = Gemm_3xTF32::get_workspace_size(arguments_3xtf32);
+  size_t workspace_size_3xtf32 = Symm_3xTF32::get_workspace_size(arguments_3xtf32);
 
   // Allocate workspace memory
   cutlass::device_memory::allocation<uint8_t> workspace_3xtf32(workspace_size_3xtf32);
 
   // Instantiate CUTLASS kernel depending on templates
-  Gemm_3xTF32 gemm_op;
+  Symm_3xTF32 symm_op_3xtf32;
 
   // Check the problem size is supported or not 
-  cutlass::Status status_3xtf32 = gemm_op.can_implement(arguments_3xtf32);
+  cutlass::Status status_3xtf32 = symm_op_3xtf32.can_implement(arguments_3xtf32);
   CUTLASS_CHECK(status_3xtf32);
 
   // Initialize CUTLASS kernel with arguments and workspace pointer
-  status_3xtf32 = gemm_op.initialize(arguments_3xtf32, workspace_3xtf32.get());
+  status_3xtf32 = symm_op_3xtf32.initialize(arguments_3xtf32, workspace_3xtf32.get());
   CUTLASS_CHECK(status_3xtf32);
 
-  // Result structure
-  Result result;
-
-  //
-  // Construct events
-  //
-
-  cudaEvent_t events[2];
-
-  for (auto & event : events) {
-    result.error = cudaEventCreate(&event);
-    if (result.error != cudaSuccess) {
-      std::cerr << "cudaEventCreate() failed: " << cudaGetErrorString(result.error) << std::endl;
-      return false;
-    }
-  }
-
-  // Record an event at the start of a series of GEMMs
-  result.error = cudaEventRecord(events[0]);
-  if (result.error != cudaSuccess) {
-    std::cerr << "cudaEventRecord() failed: " << cudaGetErrorString(result.error) << std::endl;
-    return false;
-  }
-
-  //
-  // Run profiling loop
-  //
-
-  for (int iter = 0; iter < options.iterations; ++iter) {
-    // Launch initialized CUTLASS kernel
-    status_3xtf32 = gemm_op();
-    CUTLASS_CHECK(status_3xtf32);
-  }
-
-  //
-  // Stop profiling loop
-  //
-
-  // Record an event when the GEMMs are complete
-  result.error = cudaEventRecord(events[1]);
-  if (result.error != cudaSuccess) {
-    std::cerr << "cudaEventRecord() failed: " << cudaGetErrorString(result.error) << std::endl;
-    return false;
-  }
-
-  // Wait for work on the device to complete.
-  result.error = cudaEventSynchronize(events[1]);
-  if (result.error != cudaSuccess) {
-    std::cerr << "cudaEventSynchronize() failed: " << cudaGetErrorString(result.error) << std::endl;
-    return false;
-  }
-
-  // Measure elapsed runtime
-  float runtime_ms = 0;
-  result.error = cudaEventElapsedTime(&runtime_ms, events[0], events[1]);
-  if (result.error != cudaSuccess) {
-    std::cerr << "cudaEventElapsed() failed: " << cudaGetErrorString(result.error) << std::endl;
-    return false;
-  }
-
-  // Compute average runtime and GFLOPs.
-  result.m = problem_size.m();
-  result.n = problem_size.n();
-  result.k = problem_size.k();
-  result.runtime_ms = double(runtime_ms) / double(options.iterations);
-  result.gflops = options.gflops(result.runtime_ms / 1000.0);
-
-  // Cleanup
-  for (auto event : events) {
-    (void)cudaEventDestroy(event);
-  }
+  // Launch initialized CUTLASS kernel
+  status_3xtf32 = symm_op_3xtf32();
+  CUTLASS_CHECK(status_3xtf32);
 
   tensor_d_3xTF32.sync_host();
 
   ////////////////////////////////////////////////////////////////////////////////
-  /// 4. Run TF32 kernel without profiling loop
+  /// 4. Run 1xTF32 kernel
   ////////////////////////////////////////////////////////////////////////////////
   // Create a tuple of gemm kernel arguments. This is later passed as arguments to launch
   // instantiated CUTLASS kernel
-  typename Gemm_1xTF32::Arguments arguments_1xtf32{problem_size,  // <- problem size of matrix multiplication
-                                          tensor_a_F32.device_ref(),  // <- reference to matrix A on device
-                                          tensor_b_F32.device_ref(),  // <- reference to matrix B on device
-                                          tensor_c_F32.device_ref(),  // <- reference to matrix C on device
-                                          tensor_d_1xTF32.device_ref(),  // <- reference to matrix D on device
-                                          {alpha, beta},          // <- tuple of alpha and beta
-                                          split_k_slices};        // <- k-dimension split factor
+  typename Symm_1xTF32::Arguments arguments_1xtf32{
+                                     cutlass::gemm::GemmUniversalMode::kGemm,
+                                     problem_size,                  // <- problem size of matrix multiplication
+                                     batch_count,                   // <- batch count
+                                     {alpha, beta},                 // <- tuple of alpha and beta
+                                     tensor_a_F32.device_data(),    // <- reference to matrix A on device
+                                     tensor_b_F32.device_data(),    // <- reference to matrix B on device
+                                     tensor_c_F32.device_data(),    // <- reference to matrix C on device
+                                     tensor_d_1xTF32.device_data(), // <- reference to matrix D on device
+                                     batch_stride_A,                // <- batch stride and ld for matrices
+                                     problem_size.m() * problem_size.n(),
+                                     problem_size.m() * problem_size.n(),
+                                     problem_size.m() * problem_size.n(),
+                                     tensor_a_F32.layout().stride(0),
+                                     tensor_b_F32.layout().stride(0),
+                                     tensor_c_F32.layout().stride(0),
+                                     tensor_d_1xTF32.layout().stride(0)
+                                     };
 
   // Using the arguments, query for extra workspace required for matrix multiplication computation
-  size_t workspace_size_1xtf32 = Gemm_1xTF32::get_workspace_size(arguments_1xtf32);
+  size_t workspace_size_1xtf32 = Symm_1xTF32::get_workspace_size(arguments_1xtf32);
 
   // Allocate workspace memory
   cutlass::device_memory::allocation<uint8_t> workspace_1xtf32(workspace_size_1xtf32);
 
   // Instantiate CUTLASS kernel depending on templates
-  Gemm_1xTF32 gemm_op_1xtf32;
+  Symm_1xTF32 symm_op_1xtf32;
 
   // Check the problem size is supported or not 
-  cutlass::Status status_1xtf32 = gemm_op_1xtf32.can_implement(arguments_1xtf32);
+  cutlass::Status status_1xtf32 = symm_op_1xtf32.can_implement(arguments_1xtf32);
   CUTLASS_CHECK(status_1xtf32);
 
   // Initialize CUTLASS kernel with arguments and workspace pointer
-  status_1xtf32 = gemm_op_1xtf32.initialize(arguments_1xtf32, workspace_1xtf32.get());
+  status_1xtf32 = symm_op_1xtf32.initialize(arguments_1xtf32, workspace_1xtf32.get());
   CUTLASS_CHECK(status_1xtf32);
 
   // Launch initialized CUTLASS kernel
-  status_1xtf32 = gemm_op_1xtf32();
+  status_1xtf32 = symm_op_1xtf32();
   CUTLASS_CHECK(status_1xtf32);
 
   tensor_d_1xTF32.sync_host();
 
   ////////////////////////////////////////////////////////////////////////////////
-  // Run reference kernel (F64)
+  /// 5. Run F64 kernel
   ////////////////////////////////////////////////////////////////////////////////
+  // Create a tuple of gemm kernel arguments. This is later passed as arguments to launch
+  // instantiated CUTLASS kernel
+  typename Symm_F64::Arguments arguments_f64{
+                                     cutlass::gemm::GemmUniversalMode::kGemm,
+                                     problem_size,                  // <- problem size of matrix multiplication
+                                     batch_count,                   // <- batch count
+                                     {double(options.alpha), double(options.alpha)},                 // <- tuple of alpha and beta
+                                     tensor_a_F64.device_data(),    // <- reference to matrix A on device
+                                     tensor_b_F64.device_data(),    // <- reference to matrix B on device
+                                     tensor_c_F64.device_data(),    // <- reference to matrix C on device
+                                     tensor_d_F64.device_data(),    // <- reference to matrix D on device
+                                     batch_stride_A,                // <- batch stride and ld for matrices
+                                     problem_size.m() * problem_size.n(),
+                                     problem_size.m() * problem_size.n(),
+                                     problem_size.m() * problem_size.n(),
+                                     tensor_a_F64.layout().stride(0),
+                                     tensor_b_F64.layout().stride(0),
+                                     tensor_c_F64.layout().stride(0),
+                                     tensor_d_F64.layout().stride(0)
+                                     };
+
+  // Using the arguments, query for extra workspace required for matrix multiplication computation
+  size_t workspace_size_f64 = Symm_F64::get_workspace_size(arguments_f64);
 
-  // Launch device reference gemm kernel
-  cutlass::reference::device::GemmComplex(
-                   problem_size,
-                   alpha,
-                   tensor_a_F64.device_ref(),
-                   TransformA,
-                   tensor_b_F64.device_ref(),
-                   TransformB,
-                   beta,
-                   tensor_c_F64.device_ref(),
-                   tensor_d_F64.device_ref(),
-                   cutlass::complex<double>(0.f));
+  // Allocate workspace memory
+  cutlass::device_memory::allocation<uint8_t> workspace_f64(workspace_size_f64);
+
+  // Instantiate CUTLASS kernel depending on templates
+  Symm_F64 symm_op_f64;
+
+  // Check the problem size is supported or not 
+  cutlass::Status status_f64 = symm_op_f64.can_implement(arguments_f64);
+  CUTLASS_CHECK(status_f64);
+
+  // Initialize CUTLASS kernel with arguments and workspace pointer
+  status_f64 = symm_op_f64.initialize(arguments_f64, workspace_f64.get());
+  CUTLASS_CHECK(status_f64);
+
+  // Launch initialized CUTLASS kernel
+  status_f64 = symm_op_f64();
+  CUTLASS_CHECK(status_f64);
 
-  // Wait for kernels to finish
   cudaDeviceSynchronize();
 
-  // Copy output data from CUTLASS and reference kernel to host for comparison
   tensor_d_F64.sync_host();
 
   ////////////////////////////////////////////////////////////////////////////////
-  // Run reference kernel (F32)
+  /// 6. Run cuBLAS SSYMM kernel
   ////////////////////////////////////////////////////////////////////////////////
 
-  // Launch device reference gemm kernel
-  cutlass::reference::device::GemmComplex(
-                   problem_size,
-                   alpha,
-                   tensor_a_F32.device_ref(),
-                   TransformA,
-                   tensor_b_F32.device_ref(),
-                   TransformB,
-                   beta,
-                   tensor_c_F32.device_ref(),
-                   tensor_d_F32.device_ref(),
-                   cutlass::complex<float>(0.f));
+#if CUTLASS_ENABLE_CUBLAS
+  cublasStatus_t cublas_status;
+  cublasHandle_t handle;
+
+  cublas_status = cublasCreate(&handle);
+  if (cublas_status != CUBLAS_STATUS_SUCCESS) {
+  std::cerr << "Failed to create cuBLAS handle." << std::endl;
+    return false;
+  }
+
+  cublas_status = cublasSsymm(
+      handle,
+      CUBLAS_SIDE_LEFT,
+      CUBLAS_FILL_MODE_LOWER,
+      problem_size.m(),
+      problem_size.n(),
+      static_cast<const float*>(&alpha),
+      static_cast<const float*>(tensor_a_F32.device_data()),
+      int(tensor_a_F32.layout().stride(0)),
+      static_cast<const float*>(tensor_b_F32.device_data()),
+      int(tensor_b_F32.layout().stride(0)),
+      static_cast<const float*>(&beta),
+      static_cast<float*>(tensor_d_cublasF32.device_data()),
+      int(tensor_d_cublasF32.layout().stride(0))
+    );   
 
-  // Wait for kernels to finish
   cudaDeviceSynchronize();
 
-  // Copy output data from CUTLASS and reference kernel to host for comparison
-  tensor_d_F32.sync_host();
+  tensor_d_cublasF32.sync_host();
+#endif
 
   ////////////////////////////////////////////////////////////////////////////////
-  ///////               Compute l2 norms 
+  /// 7. Compute l2 norms 
   ////////////////////////////////////////////////////////////////////////////////
 
+#if CUTLASS_ENABLE_CUBLAS
+  // l2 norm cuBLAS F32 vs F64
+  cutlass::HostTensor<double, LayoutOutput> tensor_d_cublasF32_in_F64(problem_size.mn());
+  cutlass::reference::host::TensorCopy(tensor_d_cublasF32_in_F64.host_view(), tensor_d_cublasF32.host_view());
+
+  double l2_norm_cublasf32_vs_f64 = cutlass::reference::host::TensorRelativeErrorMetric(
+    tensor_d_cublasF32_in_F64.host_view(), tensor_d_F64.host_view());
+#endif
+
   // l2 norm 3xTF32 vs F64
-  cutlass::HostTensor<cutlass::complex<double>, LayoutOutput> tensor_d_3xTF32_in_F64(problem_size.mn());
+  cutlass::HostTensor<double, LayoutOutput> tensor_d_3xTF32_in_F64(problem_size.mn());
   cutlass::reference::host::TensorCopy(tensor_d_3xTF32_in_F64.host_view(), tensor_d_3xTF32.host_view());
-
-  result.l2_norm_3xtf32_vs_fp64 = cutlass::reference::host::TensorRelativeErrorMetric(
+  double l2_norm_3xtf32_vs_f64 = cutlass::reference::host::TensorRelativeErrorMetric(
     tensor_d_3xTF32_in_F64.host_view(), tensor_d_F64.host_view());
 
   // l2 norm 1xTF32 vs F64
-  cutlass::HostTensor<cutlass::complex<double>, LayoutOutput> tensor_d_1xTF32_in_F64(problem_size.mn());
+  cutlass::HostTensor<double, LayoutOutput> tensor_d_1xTF32_in_F64(problem_size.mn());
   cutlass::reference::host::TensorCopy(tensor_d_1xTF32_in_F64.host_view(), tensor_d_1xTF32.host_view());
-
-  result.l2_norm_1xtf32_vs_fp64 = cutlass::reference::host::TensorRelativeErrorMetric(
+  double l2_norm_1xtf32_vs_f64 = cutlass::reference::host::TensorRelativeErrorMetric(
     tensor_d_1xTF32_in_F64.host_view(), tensor_d_F64.host_view());
 
-  // l2 norm F32 vs F64
-  cutlass::HostTensor<cutlass::complex<double>, LayoutOutput> tensor_d_F32_in_F64(problem_size.mn());
-  cutlass::reference::host::TensorCopy(tensor_d_F32_in_F64.host_view(), tensor_d_F32.host_view());
-
-  result.l2_norm_fp32_vs_fp64 = cutlass::reference::host::TensorRelativeErrorMetric(
-    tensor_d_F32_in_F64.host_view(), tensor_d_F64.host_view());
-
-  results.push_back(result);
+#if CUTLASS_ENABLE_CUBLAS
+  // l2 norm 3xTF32 vs cuBLAS F32
+  double l2_norm_3xtf32_vs_cublasf32 = cutlass::reference::host::TensorRelativeErrorMetric(
+    tensor_d_3xTF32.host_view(), tensor_d_cublasF32.host_view());
+#endif
+  
+  // l2 norm 3xTF32 vs 1xTF32
+  double l2_norm_3xtf32_vs_1xtf32 = cutlass::reference::host::TensorRelativeErrorMetric(
+    tensor_d_3xTF32.host_view(), tensor_d_1xTF32.host_view());
 
   ///////////////////////////////////////////////////////////////////////////////
 
-  // Check if output from CUTLASS kernel and reference kernel are equal or not
-
+  // Print kernel info and L2 norms 
+  std::cout << "Problem Size: (" << problem_size.m() << "," << problem_size.n() << "," << problem_size.k() << ") "
+            << "Alpha: "  << alpha << "," << " Beta: "  << beta << std::endl;
   std::cout << std::fixed;
-  std::cout.precision(4);
-  std::cout << "Runtime: " << result.runtime_ms << " ms" << std::endl;
-  std::cout.precision(2);
-  std::cout << "GFLOPs: " << result.gflops << std::endl;
   std::cout << "Normalized L2 norm of" << std::endl;
   std::cout.precision(8);
   std::cout << std::scientific 
-            << " - 3xTF32 error with FP64 reference : " << result.l2_norm_3xtf32_vs_fp64 << std::endl
-            << " - 1xTF32 error with FP64 reference : " << result.l2_norm_1xtf32_vs_fp64 << std::endl
-            << " - FP32 error with FP64 reference   : " << result.l2_norm_fp32_vs_fp64 << std::endl;
+#if CUTLASS_ENABLE_CUBLAS
+            << " - cuBLAS F32 error with F64 reference    : " << l2_norm_cublasf32_vs_f64 << std::endl
+#endif
+            << " - 3xTF32 error with F64 reference        : " << l2_norm_3xtf32_vs_f64 << std::endl
+            << " - 1xTF32 error with F64 reference        : " << l2_norm_1xtf32_vs_f64 << std::endl
+#if CUTLASS_ENABLE_CUBLAS
+            << " - 3xTF32 error with cuBLAS F32 reference : " << l2_norm_3xtf32_vs_cublasf32 << std::endl
+#endif
+            << " - 3xTF32 error with 1xTF32 reference     : " << l2_norm_3xtf32_vs_1xtf32 << std::endl;
 
   return true;
 }
 
 int main(int argc, const char **argv) {
   
   bool notSupported = false;
@@ -652,41 +670,18 @@
   if (options.help) {
     options.print_usage(std::cout) << std::endl;
     return 0;
   }
 
   bool result = true;
 
-  if (options.benchmark) {
-    for (int k = 4; k <= 65536; k *= 2) {
-  
-      options.problem_size[2] = k;
-  
-      printf("Gemm problem size: %d x %d x %d\n", \
-        options.problem_size.m(), options.problem_size.n(), options.problem_size.k());
-  
-      if (!options.valid()) {
-        std::cerr << "Invalid problem." << std::endl;
-        return -1;
-      }
-  
-      result &= run(options);
-    }
-  } else {
-    // Execute one problem size
-    if (!options.valid()) {
-      std::cerr << "Invalid problem." << std::endl;
-      return -1;
-    }
-
-    result = run(options);
+  if (!options.valid()) {
+    std::cerr << "Invalid problem." << std::endl;
+    return -1;
   }
 
-  if (!result) return -1;
+  result = run(options);
 
-  std::cout << std::endl << "CSV results" << std::endl;
-  Result::print_csv_header();
-  for(auto &r : results)
-    r.print_csv_row();
+  if (!result) return -1;
 
   return 0;
 }
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/30_wgrad_split_k/30_wgrad_split_k.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/30_wgrad_split_k/30_wgrad_split_k.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/31_basic_syrk/basic_syrk.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/31_basic_syrk/basic_syrk.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/32_basic_trmm/basic_trmm.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/32_basic_trmm/basic_trmm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/34_transposed_conv2d/34_transposed_conv2d.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/34_transposed_conv2d/34_transposed_conv2d.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_softmax.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_softmax.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_with_epilogue_visitor.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_with_epilogue_visitor.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_with_softmax.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_with_softmax.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/36_gather_scatter_fusion/gather_scatter_fusion.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/36_gather_scatter_fusion/gather_scatter_fusion.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/38_syr2k_grouped/syr2k_grouped.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/38_syr2k_grouped/syr2k_grouped.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/39_gemm_permute/gemm_permute.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/39_gemm_permute/gemm_permute.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/39_gemm_permute/layouts.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/tensor.h`

 * *Files 13% similar despite different names*

```diff
@@ -25,36 +25,47 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Defines additional layout functions used in Permute GEMM example to simplify
-    computing reference permutations of 4/5D tensors when source data is column-major.
+    \brief Defines layout functions used by TensorRef and derived classes for common 4-D and 5-D
+      tensor formats.
+
+    Layout functions map logical coordinates to linear memory. They often require additional
+    data to describe strides between elements.
+
+    Layout functions must implement all members in the public interface of IdentityTensorLayout<>
+    defined in cutlass/tensor_ref.h.
 */
 #pragma once
 #if defined(__CUDACC_RTC__)
 #include <cuda/std/cassert>
 #else
 #include "assert.h"
 #endif
 #include "cutlass/cutlass.h"
+#include "cutlass/fast_math.h"
 #include "cutlass/layout/pitch_linear.h"
 #include "cutlass/layout/matrix.h"
 #include "cutlass/coord.h"
 #include "cutlass/tensor_coord.h"
 
 namespace cutlass {
 namespace layout {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
+//
+// Defines data layouts of various tensor formats usable by TensorRef and other classes.
+//
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Mapping function for 4-D CWHN tensors.
-class TensorCWHN {
+/// Mapping function for 4-D NHWC tensors.
+class TensorNHWC {
 public:
   /// Logical rank of tensor
   static int const kRank = 4;
 
   /// Rank of stride vector
   static int const kStrideRank = 3;
 
@@ -71,72 +82,106 @@
   using Stride = Coord<kStrideRank>;
 
 private:
   //
   // Data members
   //
 
-  /// Stride data member - [n, hn, whn]
+  /// Stride data member - [stride_w, stride_h, stride_n]
   Stride stride_;
 
 public:
   //
   // Methods
   //
 
   /// Constructor
   CUTLASS_HOST_DEVICE
-  TensorCWHN(Stride const &stride = Stride(0)): stride_(stride) { }
+  TensorNHWC(Stride const &stride = Stride(0)): stride_(stride) { }
 
   /// Constructor
   CUTLASS_HOST_DEVICE
-  TensorCWHN(
-    typename Stride::Index stride_h,    ///< number of elements between adjacent N coordinates
-    typename Stride::Index stride_w,    ///< number of elements between adjacent C coordinates
-    typename Stride::Index stride_c     ///< number of elements between adjacent W coordinates
+  TensorNHWC(
+    typename Stride::Index stride_w,    ///< number of elements between adjacent W coordinates
+    typename Stride::Index stride_h,    ///< number of elements between adjacent H coordinates
+    typename Stride::Index stride_n     ///< number of elements between adjacent N coordinates
   ): 
-    stride_(make_Coord(stride_h, stride_w, stride_c)) { }
+    stride_(make_Coord(stride_w, stride_h, stride_n)) { }
 
   /// Constructor
   // Once convolutions implement 64b stride this ctor can be deleted
   CUTLASS_HOST_DEVICE
-  TensorCWHN(Coord<kStrideRank, LongIndex> const &stride): 
+  TensorNHWC(Coord<kStrideRank, LongIndex> const &stride): 
     stride_(make_Coord(
       static_cast<typename Stride::Index>(stride[0]), 
       static_cast<typename Stride::Index>(stride[1]), 
       static_cast<typename Stride::Index>(stride[2]))
     ) { }
 
-  /// Helper returns a layout to a tightly packed WCNH tensor.
+  /// Helper returns a layout to a tightly packed NHWC tensor.
   CUTLASS_HOST_DEVICE
-  static TensorCWHN packed(TensorCoord const &extent) {
-    return TensorCWHN(
+  static TensorNHWC packed(TensorCoord const &extent) {
+    return TensorNHWC(
       make_Coord(
-        extent.n(), 
-        extent.h() * extent.n(),
-        extent.w() * extent.h() * extent.n()
+        extent.c(), 
+        extent.w() * extent.c(),
+        extent.h() * extent.w() * extent.c()
       )
     );
   }
   
   /// Returns the offset of a coordinate (n, h, w, c) in linear memory. 
   CUTLASS_HOST_DEVICE
   LongIndex operator()(TensorCoord const &coord) const {
-    return coord.n() + 
-      LongIndex(stride_[0] * coord.h()) + 
-      LongIndex(stride_[1] * coord.w()) +
-      LongIndex(stride_[2] * coord.c());
+    return coord.c() + 
+      LongIndex(stride_[0] * coord.w()) + 
+      LongIndex(stride_[1] * coord.h()) +
+      LongIndex(stride_[2] * coord.n());
   }
   
   /// Returns the offset of a pitchlinear coordinate in linear memory. 
   CUTLASS_HOST_DEVICE
   LongIndex operator()(PitchLinearCoord coord) const {
     return coord.contiguous() + LongIndex(coord.strided() * stride_[2]);
   }
 
+  /// Returns the logical coordinate (n, h, w, c) from a given offset in linear memory.
+  CUTLASS_HOST_DEVICE
+  TensorCoord inverse(LongIndex index) const {
+
+    int n = 0, h = 0, w = 0, c = 0;
+
+    #if defined(__CUDA_ARCH__)
+    int tmp = 0;
+    c = int(index % static_cast<int>(stride_[0]));
+
+    unsigned int hw_mul, hw_shr, w_mul, w_shr, c_mul, c_shr;
+
+    find_divisor(hw_mul, hw_shr, stride_[2]);
+    find_divisor(w_mul, w_shr, stride_[1]);
+    find_divisor(c_mul, c_shr, stride_[0]);
+
+    fast_divmod(n, tmp, index, int(stride_[2]), hw_mul, hw_shr);
+    fast_divmod(h, w, tmp, int(stride_[1]), w_mul, w_shr);
+    fast_divmod(w, tmp, w, int(stride_[0]), c_mul, c_shr);
+    #else
+
+    n = int(index / stride_[2]);
+    LongIndex residual = index % stride_[2];
+
+    h = int(residual / stride_[1]);
+    residual = (residual % stride_[1]);
+
+    w = int(residual / stride_[0]);
+    c = int(residual % stride_[0]);
+
+    #endif
+    return TensorCoord(n, h, w, c);
+  }
+
   /// Returns the stride of the layout
   CUTLASS_HOST_DEVICE
   Stride stride() const {
     return stride_;
   }
 
   /// Returns the stride of the layout
@@ -147,108 +192,192 @@
 
   /// Compute the number of contiguous elements needed to store a tensor with the given size
   CUTLASS_HOST_DEVICE
   LongIndex capacity(TensorCoord const &extent) const {
     // it does not make sense if the extent is larger than stride
     // and we could not rely on the capacity calculation in such cases
     // we could move this checkers to debug code only
-    if ((extent.n() > stride_[0])
-        || (extent.h() * stride_[0] > stride_[1]) 
-        || (extent.w() * stride_[1] > stride_[2])) {
+    if ((extent.c() > stride_[0])
+        || (extent.w() * stride_[0] > stride_[1]) 
+        || (extent.h() * stride_[1] > stride_[2])) {
       assert(0);
     }
-    return extent.c() * stride_[2];
+    return extent.n() * stride_[2];
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Mapping function for 4-D NHCW tensors.
-class TensorNHCW {
+/// Mapping function for 4-D NCHW tensors.
+class TensorNCHW {
 public:
   /// Logical rank of tensor
   static int const kRank = 4;
 
   /// Rank of stride vector
   static int const kStrideRank = 3;
 
   /// Index type used for coordinates
   using Index = int32_t;
 
   /// Long index type used for offsets
   using LongIndex = int64_t;
 
-  /// Logical coordinate (n, h, w, c)
+  /// Logical coordinate
+  using TensorCoord = Tensor4DCoord;
+
+  /// Stride vector
+  using Stride = Coord<kStrideRank>;
+
+private:
+  //
+  // Data members
+  //
+
+  /// Stride data member - [w, hw, chw]
+  Stride stride_;
+
+public:
+  //
+  // Methods
+  //
+
+  /// Constructor
+  CUTLASS_HOST_DEVICE
+  TensorNCHW(Stride const &stride = Stride(0)): stride_(stride) { }
+
+  /// Helper returns a layout to a tightly packed tensor
+  CUTLASS_HOST_DEVICE
+  static TensorNCHW packed(TensorCoord const &extent) {
+    return TensorNCHW(
+      make_Coord(
+        extent.w(),
+        extent.w() * extent.h(),
+        extent.h() * extent.w() * extent.c()
+      )
+    );
+  }
+
+  /// Returns the offset of a coordinate in linear memory. 
+  CUTLASS_HOST_DEVICE
+  LongIndex operator()(TensorCoord const &coord) const {
+    return coord.w() + 
+      LongIndex(stride_[0] * coord.h()) + 
+      LongIndex(stride_[1] * coord.c()) + 
+      LongIndex(stride_[2] * coord.n());
+  }
+
+  /// Returns the stride of the layout
+  CUTLASS_HOST_DEVICE
+  Stride stride() const {
+    return stride_;
+  }
+
+  /// Returns the stride of the layout
+  CUTLASS_HOST_DEVICE
+  Stride & stride() {
+    return stride_;
+  }
+
+  /// Compute the number of contiguous elements needed to store a tensor with the given size
+  CUTLASS_HOST_DEVICE
+  LongIndex capacity(TensorCoord const &extent) const {
+    return extent.n() * stride_[2];
+  }
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Mapping function for 4-D NC/xHWx tensors.
+template <int Interleave>
+class TensorNCxHWx {
+public:
+
+  /// Interleaving quantity
+  static int const kInterleave = Interleave;
+
+  /// Logical rank of tensor
+  static int const kRank = 4;
+
+  /// Rank of stride vector
+  static int const kStrideRank = 3;
+
+  /// Index type used for coordinates
+  using Index = int32_t;
+
+  /// Long index type used for offsets
+  using LongIndex = int64_t;
+
+  /// Logical coordinate
   using TensorCoord = Tensor4DCoord;
 
   /// Stride vector
   using Stride = Coord<kStrideRank>;
 
 private:
   //
   // Data members
   //
 
-  /// Stride data member - [w, cw, hcw]
+  /// Stride data member - [Interleave x w, Interleave x wh, hwc]
   Stride stride_;
 
 public:
   //
   // Methods
   //
 
   /// Constructor
   CUTLASS_HOST_DEVICE
-  TensorNHCW(Stride const &stride = Stride(0)): stride_(stride) { }
+  TensorNCxHWx(Stride const &stride = Stride(0)): stride_(stride) { }
 
   /// Constructor
   CUTLASS_HOST_DEVICE
-  TensorNHCW(
-    typename Stride::Index stride_c,    ///< number of elements between adjacent C coordinates
+  TensorNCxHWx(
+    typename Stride::Index stride_w,    ///< number of elements between adjacent W coordinates
     typename Stride::Index stride_h,    ///< number of elements between adjacent H coordinates
     typename Stride::Index stride_n     ///< number of elements between adjacent N coordinates
-  ): 
-    stride_(make_Coord(stride_c, stride_h, stride_n)) { }
+  ):
+    stride_(make_Coord(stride_w, stride_h, stride_n)) { }
 
   /// Constructor
   // Once convolutions implement 64b stride this ctor can be deleted
   CUTLASS_HOST_DEVICE
-  TensorNHCW(Coord<kStrideRank, LongIndex> const &stride): 
+  TensorNCxHWx(Coord<kStrideRank, LongIndex> const &stride): 
     stride_(make_Coord(
       static_cast<typename Stride::Index>(stride[0]), 
       static_cast<typename Stride::Index>(stride[1]), 
       static_cast<typename Stride::Index>(stride[2]))
     ) { }
 
-  /// Helper returns a layout to a tightly packed WCNH tensor.
+  /// Helper returns a layout to a tightly packed tensor
   CUTLASS_HOST_DEVICE
-  static TensorNHCW packed(TensorCoord const &extent) {
-    return TensorNHCW(
+  static TensorNCxHWx packed(TensorCoord const &extent) {
+    return TensorNCxHWx(
       make_Coord(
-        extent.w(), 
-        extent.c() * extent.w(),
-        extent.h() * extent.c() * extent.w()
+        kInterleave * extent.w(),
+        kInterleave * extent.w() * extent.h(),
+        extent.h() * extent.w() * extent.c()
       )
     );
   }
-  
-  /// Returns the offset of a coordinate (n, h, w, c) in linear memory. 
+
+  /// Returns the offset of a coordinate in linear memory. 
   CUTLASS_HOST_DEVICE
   LongIndex operator()(TensorCoord const &coord) const {
-    return coord.w() + 
-      LongIndex(stride_[0] * coord.c()) + 
-      LongIndex(stride_[1] * coord.h()) +
+
+    Index c_minor = (coord.c() % kInterleave);
+    Index c_major = (coord.c() / kInterleave);
+
+    return c_minor + 
+      LongIndex(kInterleave * coord.w()) + 
+      LongIndex(stride_[0] * coord.h()) + 
+      LongIndex(stride_[1] * c_major) + 
       LongIndex(stride_[2] * coord.n());
   }
-  
-  /// Returns the offset of a pitchlinear coordinate in linear memory. 
-  CUTLASS_HOST_DEVICE
-  LongIndex operator()(PitchLinearCoord coord) const {
-    return coord.contiguous() + LongIndex(coord.strided() * stride_[2]);
-  }
 
   /// Returns the stride of the layout
   CUTLASS_HOST_DEVICE
   Stride stride() const {
     return stride_;
   }
 
@@ -257,110 +386,115 @@
   Stride & stride() {
     return stride_;
   }
 
   /// Compute the number of contiguous elements needed to store a tensor with the given size
   CUTLASS_HOST_DEVICE
   LongIndex capacity(TensorCoord const &extent) const {
-    // it does not make sense if the extent is larger than stride
-    // and we could not rely on the capacity calculation in such cases
-    // we could move this checkers to debug code only
-    if ((extent.w() > stride_[0])
-        || (extent.c() * stride_[0] > stride_[1]) 
-        || (extent.h() * stride_[1] > stride_[2])) {
-      assert(0);
-    }
     return extent.n() * stride_[2];
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Mapping function for 4-D NHCW tensors.
-class TensorNCWH {
+/// Mapping function for 4-D CxRSKx tensors.
+template <int Interleave>
+class TensorCxRSKx {
 public:
+
+  /// Interleaving quantity
+  static int const kInterleave = Interleave;
+
   /// Logical rank of tensor
   static int const kRank = 4;
 
   /// Rank of stride vector
   static int const kStrideRank = 3;
 
   /// Index type used for coordinates
   using Index = int32_t;
 
   /// Long index type used for offsets
   using LongIndex = int64_t;
 
-  /// Logical coordinate (n, h, w, c)
+  /// Logical coordinate
   using TensorCoord = Tensor4DCoord;
 
   /// Stride vector
   using Stride = Coord<kStrideRank>;
 
 private:
   //
   // Data members
   //
 
-  /// Stride data member - [h, wh, cwh]
+  /// Stride data member - [Interleave x n, Interleave x nw, Interleave x nwh]
   Stride stride_;
 
 public:
   //
   // Methods
   //
 
   /// Constructor
   CUTLASS_HOST_DEVICE
-  TensorNCWH(Stride const &stride = Stride(0)): stride_(stride) { }
+  TensorCxRSKx(Stride const &stride = Stride(0)): stride_(stride) { }
 
   /// Constructor
   CUTLASS_HOST_DEVICE
-  TensorNCWH(
-    typename Stride::Index stride_w,    ///< number of elements between adjacent C coordinates
-    typename Stride::Index stride_c,    ///< number of elements between adjacent H coordinates
+  TensorCxRSKx(
+    typename Stride::Index stride_w,    ///< number of elements between adjacent W coordinates
+    typename Stride::Index stride_h,    ///< number of elements between adjacent H coordinates
     typename Stride::Index stride_n     ///< number of elements between adjacent N coordinates
-  ): 
-    stride_(make_Coord(stride_w, stride_c, stride_n)) { }
+  ):
+    stride_(make_Coord(stride_w, stride_h, stride_n)) { }
 
   /// Constructor
   // Once convolutions implement 64b stride this ctor can be deleted
   CUTLASS_HOST_DEVICE
-  TensorNCWH(Coord<kStrideRank, LongIndex> const &stride): 
+  TensorCxRSKx(Coord<kStrideRank, LongIndex> const &stride): 
     stride_(make_Coord(
       static_cast<typename Stride::Index>(stride[0]), 
       static_cast<typename Stride::Index>(stride[1]), 
       static_cast<typename Stride::Index>(stride[2]))
     ) { }
 
-  /// Helper returns a layout to a tightly packed WCNH tensor.
+
+  /// Helper returns a layout to a tightly packed tensor
   CUTLASS_HOST_DEVICE
-  static TensorNCWH packed(TensorCoord const &extent) {
-    return TensorNCWH(
+  static TensorCxRSKx packed(TensorCoord const &extent) {
+    return TensorCxRSKx(
       make_Coord(
-        extent.h(), 
-        extent.w() * extent.h(),
-        extent.c() * extent.w() * extent.h()
+        kInterleave * extent.n(),
+        kInterleave * extent.n() * extent.w(),
+        kInterleave * extent.n() * extent.w() * extent.h()
       )
     );
   }
-  
-  /// Returns the offset of a coordinate (n, h, w, c) in linear memory. 
+
+  /// Returns the offset of a coordinate in linear memory. 
   CUTLASS_HOST_DEVICE
   LongIndex operator()(TensorCoord const &coord) const {
-    return coord.h() + 
+
+    Index c_minor = (coord.c() % kInterleave);
+    Index c_major = (coord.c() / kInterleave);
+
+    return c_minor + 
+      LongIndex(kInterleave * coord.n()) + 
       LongIndex(stride_[0] * coord.w()) + 
-      LongIndex(stride_[1] * coord.c()) +
-      LongIndex(stride_[2] * coord.n());
+      LongIndex(stride_[1] * coord.h()) + 
+      LongIndex(stride_[2] * c_major);
   }
-  
+
   /// Returns the offset of a pitchlinear coordinate in linear memory. 
   CUTLASS_HOST_DEVICE
-  LongIndex operator()(PitchLinearCoord coord) const {
-    return coord.contiguous() + LongIndex(coord.strided() * stride_[2]);
+  LongIndex operator()(PitchLinearCoord const &coord) const {
+    return (coord.contiguous() % kInterleave) +
+      LongIndex((coord.contiguous() / kInterleave) * stride_[2]) +
+      LongIndex(coord.strided() * kInterleave);
   }
 
   /// Returns the stride of the layout
   CUTLASS_HOST_DEVICE
   Stride stride() const {
     return stride_;
   }
@@ -370,30 +504,22 @@
   Stride & stride() {
     return stride_;
   }
 
   /// Compute the number of contiguous elements needed to store a tensor with the given size
   CUTLASS_HOST_DEVICE
   LongIndex capacity(TensorCoord const &extent) const {
-    // it does not make sense if the extent is larger than stride
-    // and we could not rely on the capacity calculation in such cases
-    // we could move this checkers to debug code only
-    if ((extent.h() > stride_[0])
-        || (extent.w() * stride_[0] > stride_[1]) 
-        || (extent.c() * stride_[1] > stride_[2])) {
-      assert(0);
-    }
-    return extent.n() * stride_[2];
+    return (extent.c() / kInterleave * stride_[2]);
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Mapping function for 5-D CWHDN tensors.
-class TensorCWHDN {
+/// Mapping function for 5-D NDHWC tensors.
+class TensorNDHWC {
 public:
   /// Logical rank of tensor
   static int const kRank = 5;
 
   /// Rank of stride vector
   static int const kStrideRank = 4;
 
@@ -410,67 +536,67 @@
   using Stride = Coord<kStrideRank>;
 
 private:
   //
   // Data members
   //
 
-  /// Stride data member - [n, dn, hdn, whdn]
+  /// Stride data member - [c, wc, hwc, dhwc]
   Stride stride_;
 
 public:
   //
   // Methods
   //
 
   /// Constructor
   CUTLASS_HOST_DEVICE
-  TensorCWHDN(Stride const &stride = Stride(0)): stride_(stride) { }
+  TensorNDHWC(Stride const &stride = Stride(0)): stride_(stride) { }
 
   /// Constructor
   CUTLASS_HOST_DEVICE
-  TensorCWHDN(
-    typename Stride::Index n, 
-    typename Stride::Index dn, 
-    typename Stride::Index hdn, 
-    typename Stride::Index whdn): 
-  stride_(make_Coord(n, dn, hdn, whdn)) { }
+  TensorNDHWC(
+    typename Stride::Index c, 
+    typename Stride::Index wc, 
+    typename Stride::Index hwc, 
+    typename Stride::Index dhwc): 
+  stride_(make_Coord(c, wc, hwc, dhwc)) { }
 
   /// Constructor
   // Once convolutions implement 64b stride this ctor can be deleted
   CUTLASS_HOST_DEVICE
-  TensorCWHDN(Coord<kStrideRank, LongIndex> const &stride): 
+  TensorNDHWC(Coord<kStrideRank, LongIndex> const &stride): 
     stride_(make_Coord(
       static_cast<typename Stride::Index>(stride[0]), 
       static_cast<typename Stride::Index>(stride[1]), 
       static_cast<typename Stride::Index>(stride[2]),
       static_cast<typename Stride::Index>(stride[3]))
     ) { }
 
-  /// Helper returns a layout to a tightly packed CWHDN tensor.
+  /// Helper returns a layout to a tightly packed NHWC tensor.
   CUTLASS_HOST_DEVICE
-  static TensorCWHDN packed(TensorCoord const &extent) {
-    return TensorCWHDN(
+  static TensorNDHWC packed(TensorCoord const &extent) {
+    return TensorNDHWC(
       make_Coord(
-        extent.n(), 
-        extent.d() * extent.n(),
-        extent.h() * extent.d() * extent.n(),
-        extent.w() * extent.h() * extent.d() * extent.n()
+        extent.c(), 
+        extent.w() * extent.c(),
+        extent.h() * extent.w() * extent.c(),
+        extent.d() * extent.h() * extent.w() * extent.c()
       )
     );
   }
   
   /// Returns the offset of a coordinate (n, d, h, w, c) in linear memory. 
   CUTLASS_HOST_DEVICE
   LongIndex operator()(TensorCoord const &coord) const {
-    return coord.n() + 
-      LongIndex(stride_[0] * coord.d()) + 
+    return coord.c() + 
+      LongIndex(stride_[0] * coord.w()) + 
       LongIndex(stride_[1] * coord.h()) +
-      LongIndex(stride_[2] * coord.w()) +
-      LongIndex(stride_[3] * coord.c());
+      LongIndex(stride_[2] * coord.d()) +
+      LongIndex(stride_[3] * coord.n());
   }
 
   /// Returns the offset of a pitchlinear coordinate in linear memory. 
   CUTLASS_HOST_DEVICE
   LongIndex operator()(PitchLinearCoord coord) const {
     return coord.contiguous() + LongIndex(coord.strided() * stride_[3]);
   }
@@ -489,21 +615,21 @@
 
   /// Compute the number of contiguous elements needed to store a tensor with the given size
   CUTLASS_HOST_DEVICE
   LongIndex capacity(TensorCoord const &extent) const {
     // it does not make sense if the extent is larger than stride
     // and we could not rely on the capacity calculation in such cases
     // we could move this checkers to debug code only
-    if ((extent.n() > stride_[0])
-        || (extent.d() * stride_[0] > stride_[1]) 
+    if ((extent.c() > stride_[0])
+        || (extent.w() * stride_[0] > stride_[1]) 
         || (extent.h() * stride_[1] > stride_[2])
-        || (extent.w() * stride_[2] > stride_[3])) {
+        || (extent.d() * stride_[2] > stride_[3])) {
       assert(0);
     }
-    return extent.c() * stride_[3];
+    return extent.n() * stride_[3];
   }
 };
 
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace layout
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/attention_scaling_coefs_updater.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/attention_scaling_coefs_updater.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/debug_utils.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/debug_utils.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/default_fmha_grouped.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/default_fmha_grouped.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_pipelined.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_pipelined.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_rescale_output.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_rescale_output.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_thread_apply_logsumexp.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_thread_apply_logsumexp.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_pipelined.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue.h`

 * *Files 20% similar despite different names*

```diff
@@ -1,630 +1,533 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights
- *reserved. SPDX-License-Identifier: BSD-3-Clause
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
- * 1. Redistributions of source code must retain the above copyright notice,
- *this list of conditions and the following disclaimer.
+ * 1. Redistributions of source code must retain the above copyright notice, this
+ * list of conditions and the following disclaimer.
  *
  * 2. Redistributions in binary form must reproduce the above copyright notice,
  * this list of conditions and the following disclaimer in the documentation
  * and/or other materials provided with the distribution.
  *
  * 3. Neither the name of the copyright holder nor the names of its
  * contributors may be used to endorse or promote products derived from
  * this software without specific prior written permission.
  *
  * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
  * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- *ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
- *LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- *CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- *SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- *INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- *CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- *ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- *POSSIBILITY OF SUCH DAMAGE.
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+ * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
   \brief Epilogue for threadblock scoped GEMMs using Tensor Ops.
 
-  File copied from "cutlass/epilogue/threadblock/epilogue.h"
-  then modified to:
-  (1) load 2 source fragments at the same time (pipelining)
-  (2) support reading from a different dtype
-  (3) pass the row id to the OutputOp if it takes it
-    (see MemoryEfficientAttentionNormalize)
-  Note that in general the fragment passed to the OutputOp could
-  span multiple rows but it does not happen with the configurations we have
+  The epilogue rearranges the result of a matrix product through shared memory to match canonical
+  tensor layouts in global memory. Epilogues support conversion and reduction operations.
+
+  The shared memory resource is time-sliced across warps.
 */
 
 #pragma once
 
 #if defined(__CUDACC_RTC__)
 #include <cuda/std/cassert>
 #else
 #include <assert.h>
 #endif
 
-#include "cutlass/aligned_buffer.h"
-#include "cutlass/array.h"
 #include "cutlass/cutlass.h"
-#include "cutlass/functional.h"
-#include "cutlass/layout/tensor.h"
-#include "cutlass/layout/vector.h"
 #include "cutlass/numeric_types.h"
+#include "cutlass/array.h"
+#include "cutlass/layout/vector.h"
+#include "cutlass/layout/tensor.h"
 #include "cutlass/tensor_coord.h"
+#include "cutlass/aligned_buffer.h"
+#include "cutlass/functional.h"
 
 #include "cutlass/gemm/gemm.h"
 
 #include "cutlass/transform/pitch_linear_thread_map.h"
 #include "cutlass/transform/threadblock/regular_tile_iterator.h"
 
 #include "cutlass/epilogue/threadblock/epilogue_base.h"
+#include "cutlass/epilogue/threadblock/epilogue_base_streamk.h"
 #include "cutlass/epilogue/threadblock/predicated_tile_iterator.h"
-#include "cutlass/numeric_types.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace threadblock {
 
-template <typename Op>
-struct ApplyEpilogueOp {
-  static CUTLASS_DEVICE typename Op::FragmentOutput apply(
-      Op const& output_op,
-      int row_id,
-      typename Op::FragmentAccumulator const& accum,
-      typename Op::FragmentOutput const& source) {
-    return output_op(accum, source);
-  }
-  static CUTLASS_DEVICE typename Op::FragmentOutput apply(
-      Op const& output_op,
-      int row_id,
-      typename Op::FragmentAccumulator const& accum) {
-    return output_op(accum);
-  }
-};
 
 ////////////////////////////////////////////////////////////////////////////////
 
 /// Epilogue operator
 template <
-    typename Shape_, ///< Shape of threadblock tile (concept: GemmShape)
-    typename WarpMmaOperator_, ///< Warp-level MMA operator (concept:
-                               ///< gemm::warp::MmaTensorOp)
-    int PartitionsK, ///< Number of partitions of the K dimension
-    typename OutputTileIterator_, ///< Tile iterator writing output tensors
-    typename AccumulatorFragmentIterator_, ///< Fragment iterator selecting
-                                           ///< accumulators
-    typename WarpTileIterator_, ///< Warp-scoped tile iterator writing
-                                ///< accumulators to SMEM
-    typename SharedLoadIterator_, ///< Threadblock-scoped tile iterator loading
-                                  ///< from SMEM
-    typename OutputOp_, ///< Output operator
-    typename Padding_, ///< Padding added to SMEM allocation to avoid bank
-                       ///< conflicts (concept: MatrixShape)
-    int FragmentsPerPartition =
-        1, ///< Used to coarsten the epilogue granularity
-    int IterationsUnroll = ///< Used to reduce binary size when epilogue op is
-                           ///< large
-    (!IsEpilogueFunctorHeavy<OutputOp_>::value),
-    typename OutputTileSourceIterator_ =
-        OutputTileIterator_ ///< Tile iterator reading tensors
-    >
-class EpiloguePipelined : public EpilogueBase<
-                              Shape_,
-                              typename WarpMmaOperator_::Shape,
-                              PartitionsK,
-                              AccumulatorFragmentIterator_,
-                              WarpTileIterator_,
-                              Padding_,
-                              FragmentsPerPartition> {
- public:
+  typename Shape_,                          ///< Shape of threadblock tile (concept: GemmShape)
+  typename WarpMmaOperator_,                ///< Warp-level MMA operator (concept: gemm::warp::MmaTensorOp)
+  int PartitionsK,                          ///< Number of partitions of the K dimension
+  typename OutputTileIterator_,             ///< Tile iterator reading and writing output tensors
+  typename AccumulatorFragmentIterator_,    ///< Fragment iterator selecting accumulators
+  typename WarpTileIterator_,               ///< Warp-scoped tile iterator writing accumulators to SMEM
+  typename SharedLoadIterator_,             ///< Threadblock-scoped tile iterator loading from SMEM
+  typename OutputOp_,                       ///< Output operator
+  typename Padding_,                        ///< Padding added to SMEM allocation to avoid bank conflicts (concept: MatrixShape)
+  int FragmentsPerPartition = 1,            ///< Used to coarsten the epilogue granularity
+  int IterationsUnroll =                    ///< Used to reduce binary size when epilogue op is large
+    (!IsEpilogueFunctorHeavy<OutputOp_>::value)
+>
+class Epilogue :
+  public EpilogueBase<
+    Shape_,
+    typename WarpMmaOperator_::Shape,
+    PartitionsK,
+    AccumulatorFragmentIterator_,
+    WarpTileIterator_,
+    Padding_,
+    FragmentsPerPartition>,
+  public EpilogueBaseStreamK<
+    Shape_,
+    PartitionsK,
+    WarpMmaOperator_,
+    AccumulatorFragmentIterator_>
+{
+
+public:
+
   using Base = EpilogueBase<
-      Shape_,
-      typename WarpMmaOperator_::Shape,
-      PartitionsK,
-      AccumulatorFragmentIterator_,
-      WarpTileIterator_,
-      Padding_,
-      FragmentsPerPartition>;
+    Shape_,
+    typename WarpMmaOperator_::Shape,
+    PartitionsK,
+    AccumulatorFragmentIterator_,
+    WarpTileIterator_,
+    Padding_,
+    FragmentsPerPartition>;
+
+  using BaseStreamK = EpilogueBaseStreamK<
+    Shape_,
+    PartitionsK,
+    WarpMmaOperator_,
+    AccumulatorFragmentIterator_>;
 
   using Shape = Shape_;
   using WarpMmaOperator = WarpMmaOperator_;
   static int const kPartitionsK = PartitionsK;
   using OutputTileIterator = OutputTileIterator_;
-  using OutputTileSourceIterator = OutputTileSourceIterator_;
   using AccumulatorFragmentIterator = AccumulatorFragmentIterator_;
   using WarpTileIterator = WarpTileIterator_;
   using SharedLoadIterator = SharedLoadIterator_;
   using OutputOp = OutputOp_;
   using Padding = Padding_;
-
   using Layout = layout::RowMajor;
   using LongIndex = typename Layout::LongIndex;
 
-  /// The complete warp-level accumulator tile
+  /// Number of warps per block
+  using WarpCount = typename Base::WarpCount;
+
+  /// Number of threads per block
+  static int const kBlockThreads = 32 * WarpCount::kCount;
+
+  /// Per-thread accumulator tile type
   using AccumulatorTile = typename Base::AccumulatorTile;
 
-  /// Accumulator element
-  using ElementAccumulator = typename WarpTileIterator::Element;
+  /// Numerical accumulation element type
+  using ElementAccumulator = typename WarpMmaOperator::ElementC;
+
+  /// Fragment type used by the accumulator tile's fragment iterator
+  using AccumulatorFragment = typename AccumulatorFragmentIterator::Fragment;
 
   /// Output element
   using ElementOutput = typename OutputTileIterator::Element;
-  using ElementSource = typename OutputTileSourceIterator::Element;
 
   /// Output access size
   static int const kElementsPerAccess = OutputTileIterator::kElementsPerAccess;
 
   /// Tensor reference to destination tensor
   using TensorRef = typename OutputTileIterator::TensorRef;
 
   /// Tensor reference to sync tensor
-  using SyncTensorRef =
-      typename cutlass::TensorRef<int, cutlass::layout::PackedVectorLayout>;
+  using SyncTensorRef = typename cutlass::TensorRef<int, cutlass::layout::PackedVectorLayout>;
 
   /// Const tensor reference to source tensor
   using ConstTensorRef = typename OutputTileIterator::ConstTensorRef;
 
-  /// Array type used to output
+  /// Vector type used by the global output iterator
   using OutputAccessType = Array<
-      typename OutputTileIterator::Element,
-      OutputTileIterator::kElementsPerAccess>;
-  using SourceAccessType = Array<
-      typename OutputTileSourceIterator::Element,
-      OutputTileSourceIterator::kElementsPerAccess>;
-
-  /// Array type used by output functor
-  using AccumulatorAccessType = Array<
-      typename WarpTileIterator::Element,
-      OutputTileIterator::kElementsPerAccess>;
+    typename OutputTileIterator::Element, OutputTileIterator::kElementsPerAccess>;
 
-  /// Number of warps
-  using WarpCount = typename Base::WarpCount;
+  /// Vector type used by the shared output iterator
+  using AccumulatorAccessType = Array<typename WarpTileIterator::Element, OutputTileIterator::kElementsPerAccess>;
 
-  static int constexpr kSmemTiles = Base::kFragmentsPerIteration > 1
-      ? Base::kFragmentsPerIteration
-      : kPartitionsK;
-  static int constexpr kSmemPointerOffset =
-      Base::SharedStorage::StorageShape::kCount / kSmemTiles;
-
- public:
-  static_assert(
-      OutputTileSourceIterator::Fragment::kElements ==
-          OutputTileIterator::Fragment::kElements,
-      "Mismatch between input tile and output tile iterator (kElements)");
-  static_assert(
-      OutputTileSourceIterator::kIterations == OutputTileIterator::kIterations,
-      "Mismatch between input tile and output tile iterator (kIterations)");
-  static_assert(
-      SharedLoadIterator::Fragment::kElements ==
-          OutputTileIterator::Fragment::kElements,
-      "Mismatch between shared load iterator and output tile iterator.");
-
-  static_assert(
-      OutputTileIterator::kElementsPerAccess,
-      "OutputTileIterator::kElementsPerAccess must not be zero.");
-
-  static_assert(
-      !(OutputTileIterator::Fragment::kElements %
-        OutputTileIterator::kElementsPerAccess),
-      "Divisibility");
+  static int constexpr kSmemTiles = Base::kFragmentsPerIteration > 1 ? Base::kFragmentsPerIteration : kPartitionsK;
 
- private:
-  /// Loads fragment from shared memory aligned with output tensor
-  SharedLoadIterator shared_load_iterator_;
+  static int constexpr kSmemPointerOffset = Base::SharedStorage::StorageShape::kCount / kSmemTiles;
 
- public:
-  /// Constructor
-  CUTLASS_DEVICE
-  EpiloguePipelined(
-      typename Base::SharedStorage& shared_storage, ///< Shared storage object
-      int thread_idx, ///< ID of a thread within the threadblock
-      int warp_idx, ///< ID of warp within threadblock
-      int lane_idx ///< Id of thread within warp
-      )
-      : Base(shared_storage, thread_idx, warp_idx, lane_idx),
-        shared_load_iterator_(shared_storage.reference(), thread_idx) {}
 
-  /// Streams the result to global memory
-  CUTLASS_DEVICE
-  void operator()(
-      OutputOp const& output_op, ///< Output operator
-      OutputTileIterator
-          destination_iterator, ///< Tile iterator for destination
-      AccumulatorTile const&
-          accumulators, ///< Complete warp-level accumulator tile
-      OutputTileSourceIterator
-          source_iterator) { ///< Threadblock tile coordinate in GEMM (in units
-                             ///< of threadblock tiles)
-
-    if (!output_op.is_source_needed()) {
-      compute_source_not_needed_(output_op, destination_iterator, accumulators);
-    } else {
-      compute_source_needed_(
-          output_op, destination_iterator, accumulators, source_iterator);
-    }
-  }
-  CUTLASS_DEVICE
-  void operator()(
-      OutputOp const& output_op, ///< Output operator
-      OutputTileIterator
-          destination_iterator, ///< Tile iterator for destination
-      AccumulatorTile const&
-          accumulators) { ///< Complete warp-level accumulator tile
-    compute_source_not_needed_(output_op, destination_iterator, accumulators);
-  }
+public:
 
- private:
-  template <class Seq>
-  struct acc2smem_source_not_needed;
-
-  template <size_t... Seq>
-  struct acc2smem_source_not_needed<cutlass::index_sequence<Seq...>> {
-    template <int Advance>
-    CUTLASS_DEVICE static void helper(
-        AccumulatorFragmentIterator accum_fragment_iterator,
-        WarpTileIterator& warp_tile_iterator) {
-      CUTLASS_PRAGMA_UNROLL
-      for (int i = 0; i < Advance; i++) {
-        ++accum_fragment_iterator;
-      }
+  static_assert(SharedLoadIterator::Fragment::kElements == OutputTileIterator::Fragment::kElements,
+    "Mismatch between shared load iterator and output tile iterator.");
 
-      CUTLASS_PRAGMA_UNROLL
-      for (int p = 0; p < Base::kFragmentsPerIteration; ++p) {
-        typename AccumulatorFragmentIterator::Fragment accum_fragment;
+  static_assert(OutputTileIterator::kElementsPerAccess, "OutputTileIterator::kElementsPerAccess must not be zero.");
 
-        accum_fragment_iterator.load(accum_fragment);
-        ++accum_fragment_iterator;
+  static_assert(!(OutputTileIterator::Fragment::kElements % OutputTileIterator::kElementsPerAccess), 
+    "Divisibility");
 
-        warp_tile_iterator.store(accum_fragment);
-        if (p < Base::kFragmentsPerIteration - 1) {
-          warp_tile_iterator.add_pointer_offset(kSmemPointerOffset);
-        }
-      }
+  static_assert(kPartitionsK == 1 || Base::kFragmentsPerIteration == 1, "One of these must be exactly 1.");
 
-      if (Base::kFragmentsPerIteration > 1) {
-        warp_tile_iterator.add_pointer_offset(
-            kSmemPointerOffset * (1 - Base::kFragmentsPerIteration));
-      }
-    }
 
+public:
+
+  /// Aspect for when epilogue source is not needed
+  struct SourceAspectNotNeeded
+  {
+    /// Constructor
+    CUTLASS_DEVICE
+    SourceAspectNotNeeded()
+    {}
+
+    /// Invoke the output functor over each vector of output
     CUTLASS_DEVICE
-    static void push(
-        size_t pos,
-        AccumulatorFragmentIterator const& iterator_begin,
-        WarpTileIterator& warp_tile_iterator) {
-      int dummy[] = {
-          (pos == (Seq * Base::kFragmentsPerIteration)) &&
-          (helper<Seq * Base::kFragmentsPerIteration>(
-               iterator_begin, warp_tile_iterator),
-           0)...};
+    void apply_output_operator(
+      typename OutputTileIterator::Fragment &output_fragment,
+      OutputOp const &output_op,
+      typename SharedLoadIterator::Fragment const &aligned_accum_fragment)
+    {
+      OutputAccessType *output_frag_ptr =
+        reinterpret_cast<OutputAccessType *>(&output_fragment);
 
-      CUTLASS_UNUSED(dummy[0]);
+      AccumulatorAccessType const *compute_frag_ptr =
+        reinterpret_cast<AccumulatorAccessType const *>(&aligned_accum_fragment);
+
+      int const kOutputOpIterations =
+        OutputTileIterator::Fragment::kElements / OutputTileIterator::kElementsPerAccess;
+
+      CUTLASS_PRAGMA_UNROLL
+      for (int i = 0; i < kOutputOpIterations; ++i)
+      {
+        // Call the output operator
+        output_frag_ptr[i] = output_op(compute_frag_ptr[i]);
+      }
     }
   };
 
-  static_assert(
-      kPartitionsK == 1 || Base::kFragmentsPerIteration == 1,
-      "One of these must be exactly 1.");
 
-  /// Streams the result to global memory
-  CUTLASS_DEVICE
-  void compute_source_not_needed_(
-      OutputOp const& output_op, ///< Output operator
-      OutputTileIterator
-          destination_iterator, ///< Tile iterator for destination
-      AccumulatorTile const&
-          accumulators ///< Complete warp-level accumulator tile
-  ) {
-    //
-    // Iterator over warp-level accumulator fragment
-    //
+  /// Aspect for when epilogue source is needed
+  struct SourceAspectNeeded
+  {
+    OutputTileIterator source_iterator;
 
-    AccumulatorFragmentIterator accum_fragment_iterator(accumulators);
+    typename OutputTileIterator::Fragment source_fragment;
 
-    //
-    // Iterate over accumulator tile
-    //
+    /// Invoke the output functor over each vector of output
+    CUTLASS_DEVICE
+    static void apply_output_operator(
+      typename OutputTileIterator::Fragment &output_fragment,
+      OutputOp const &output_op,
+      typename SharedLoadIterator::Fragment const &aligned_accum_fragment,
+      typename OutputTileIterator::Fragment const &source_fragment)
+    {
+      OutputAccessType *output_frag_ptr =
+        reinterpret_cast<OutputAccessType *>(&output_fragment);
 
-#pragma unroll(                                                          \
-    IterationsUnroll                                                     \
-        ? OutputTileIterator::kIterations / Base::kFragmentsPerIteration \
-        : 1)
-    for (int iter = 0; iter < OutputTileIterator::kIterations;
-         iter += Base::kFragmentsPerIteration) {
-      //
-      // Convert and store fragment
-      //
+      AccumulatorAccessType const *compute_frag_ptr =
+        reinterpret_cast<AccumulatorAccessType const *>(&aligned_accum_fragment);
 
-      __syncthreads();
+      OutputAccessType const *source_frag_ptr =
+        reinterpret_cast<OutputAccessType const *>(&source_fragment);
 
-      acc2smem_source_not_needed<cutlass::make_index_sequence<
-          OutputTileIterator::kIterations / Base::kFragmentsPerIteration>>::
-          push(iter, accum_fragment_iterator, this->warp_tile_iterator_);
+      int const kOutputOpIterations =
+        OutputTileIterator::Fragment::kElements / OutputTileIterator::kElementsPerAccess;
 
-      __syncthreads();
+      CUTLASS_PRAGMA_UNROLL
+      for (int i = 0; i < kOutputOpIterations; ++i)
+      {
+        // Call the output operator
+        output_frag_ptr[i] = output_op(compute_frag_ptr[i], source_frag_ptr[i]);
+      }
+    }
 
-      //
-      // Load fragments from shared memory
-      //
+    /// Constructor
+    CUTLASS_DEVICE
+    SourceAspectNeeded(OutputTileIterator source_iterator) :
+      source_iterator(source_iterator)
+    {
+      source_fragment.clear();
+    }
 
-      CUTLASS_PRAGMA_UNROLL
-      for (int p = 0; p < Base::kFragmentsPerIteration; ++p) {
-        typename SharedLoadIterator::Fragment
-            aligned_accum_fragment[kPartitionsK];
+    /// Invoke the output functor over each vector of output
+    CUTLASS_DEVICE
+    void apply_output_operator(
+      typename OutputTileIterator::Fragment &output_fragment,
+      OutputOp const &output_op,
+      typename SharedLoadIterator::Fragment const &aligned_accum_fragment)
+    {
+      // Load addend source fragment from global memory
+      source_iterator.load(source_fragment);
+      ++source_iterator;
 
-        shared_load_iterator_.load(aligned_accum_fragment[0]);
+      apply_output_operator(output_fragment, output_op, aligned_accum_fragment, source_fragment);
+    }
+  };
 
-        if (p < Base::kFragmentsPerIteration - 1) {
-          shared_load_iterator_.add_pointer_offset(kSmemPointerOffset);
-        } else if (kPartitionsK > 1) {
-          plus<typename SharedLoadIterator::Fragment> add_fragments;
 
-          CUTLASS_PRAGMA_UNROLL
-          for (int i = 1; i < kPartitionsK; ++i) {
-            shared_load_iterator_.add_pointer_offset(kSmemPointerOffset);
-            shared_load_iterator_.load(aligned_accum_fragment[i]);
-            aligned_accum_fragment[0] = add_fragments(
-                aligned_accum_fragment[0], aligned_accum_fragment[i]);
-          }
+private:
 
-          shared_load_iterator_.add_pointer_offset(
-              (1 - kPartitionsK) * kSmemPointerOffset);
-        }
+  /// Loads fragment from shared memory aligned with output tensor
+  SharedLoadIterator shared_load_iterator_;
 
-        //
-        // Compute the output result
-        //
+  /// Thread index in the threadblock
+  int thread_idx;
 
-        typename OutputTileIterator::Fragment output_fragment;
+  /// Warp index in the threadblock
+  int warp_idx;
 
-        apply_output_operator_source_not_needed_(
-            destination_iterator.thread_start_row(),
-            output_fragment,
-            output_op,
-            aligned_accum_fragment[0]);
+public:
 
-        //
-        // Store the final result
-        //
+  /// Constructor
+  CUTLASS_DEVICE
+  Epilogue(
+      typename Base::SharedStorage &shared_storage,   ///< Shared storage object
+      int thread_idx,                                 ///< ID of a thread within the threadblock
+      int warp_idx,                                   ///< ID of warp within threadblock
+      int lane_idx)                                   ///< Id of thread within warp
+  :
+      Base(shared_storage, thread_idx, warp_idx, lane_idx),
+      BaseStreamK(thread_idx),
+      shared_load_iterator_(shared_storage.reference(), thread_idx),
+      thread_idx(thread_idx),
+      warp_idx(warp_idx)
+  {}
 
-        destination_iterator.store(output_fragment);
-        ++destination_iterator;
-      }
 
-      if (Base::kFragmentsPerIteration > 1) {
-        shared_load_iterator_.add_pointer_offset(
-            kSmemPointerOffset * (1 - Base::kFragmentsPerIteration));
-      }
+  /// Aggregates the accumulator sets shared by peer blocks in the global workspace,
+  /// performing epilogue computations, writing to output
+  CUTLASS_DEVICE
+  void reduce(
+      int peer_idx_begin,
+      int peer_idx_end,
+      int reduce_fragment_idx,
+      void *element_workspace,
+      OutputOp const &output_op,                      ///< Output operator
+      OutputTileIterator destination_iterator,        ///< Tile iterator for destination
+      OutputTileIterator source_iterator)             ///< Threadblock tile coordinate in GEMM (in units of threadblock tiles)
+  {
+    // Redcuce peer accumulator fragments into one fragment
+    AccumulatorFragment accum_fragment;
+    BaseStreamK::reduce(accum_fragment, peer_idx_begin, peer_idx_end, reduce_fragment_idx, element_workspace);
+
+    // Store fragment to shared memory
+    this->warp_tile_iterator_.store(accum_fragment);
+
+    __syncthreads();
+
+    // Initialize/load source-fragment data
+    typename OutputTileIterator::Fragment source_fragment;
+    source_fragment.clear();
+
+    if (output_op.is_source_needed())
+    {
+      source_iterator += reduce_fragment_idx;
+      source_iterator.load(source_fragment);
     }
-  }
 
-  template <class Seq>
-  struct acc2smem_source_needed;
+    // Load fragment from shared memory
+    typename SharedLoadIterator::Fragment aligned_accum_fragment;
+    shared_load_iterator_.load(aligned_accum_fragment);
+
+    // Add fragments shared by other k partitions
+    if (kPartitionsK > 1)
+    {
+      plus <typename SharedLoadIterator::Fragment> add_fragments;
 
-  template <size_t... Seq>
-  struct acc2smem_source_needed<cutlass::index_sequence<Seq...>> {
-    template <int Advance>
-    CUTLASS_DEVICE static void helper(
-        AccumulatorFragmentIterator accum_fragment_iterator,
-        WarpTileIterator& warp_tile_iterator) {
       CUTLASS_PRAGMA_UNROLL
-      for (int i = 0; i < Advance; i++) {
-        ++accum_fragment_iterator;
+      for ( int i = 1; i < kPartitionsK; ++i) {
+        typename SharedLoadIterator::Fragment aligned_addend_fragment;
+        shared_load_iterator_.add_pointer_offset(kSmemPointerOffset);
+        shared_load_iterator_.load(aligned_addend_fragment);
+        aligned_accum_fragment = add_fragments(aligned_accum_fragment, aligned_addend_fragment);
       }
+    }
+
+    // Compute the output result
+    typename OutputTileIterator::Fragment output_fragment;
+
+    // Apply the output operator
+    SourceAspectNeeded::apply_output_operator(
+        output_fragment,
+        output_op,
+        aligned_accum_fragment,
+        source_fragment);
+
+    // Store the final result
+    destination_iterator += reduce_fragment_idx;
+    destination_iterator.store(output_fragment);
+  }
+
 
-      typename AccumulatorFragmentIterator::Fragment accum_fragment;
-      accum_fragment_iterator.load(accum_fragment);
-      warp_tile_iterator.store(accum_fragment);
+  /// Perform the epilogue computations and stream the result to global memory.
+  CUTLASS_DEVICE
+  void operator()(
+    OutputOp const &output_op,                      ///< Output operator
+    OutputTileIterator destination_iterator,        ///< Tile iterator for destination
+    AccumulatorTile const &accumulators)            ///< Complete warp-level accumulator tile
+  {
+    operator()(output_op, destination_iterator, accumulators, SourceAspectNotNeeded());
+  }
+
+
+  /// Perform the epilogue computations and stream the result to global memory.  Implements
+  /// two alternative codepaths, depending on whether the output op requires addend data to be loaded.
+  CUTLASS_DEVICE
+  void operator()(
+    OutputOp const &output_op,                      ///< Output operator
+    OutputTileIterator destination_iterator,        ///< Tile iterator for destination
+    AccumulatorTile const &accumulators,            ///< Complete warp-level accumulator tile
+    OutputTileIterator source_iterator )            ///< Tile iterator for addend source
+  {
+    if (output_op.is_source_needed())
+    {
+      operator()(output_op, destination_iterator, accumulators, SourceAspectNeeded(source_iterator));
+    }
+    else
+    {
+      operator()(output_op, destination_iterator, accumulators, SourceAspectNotNeeded());
     }
+  }
 
-    CUTLASS_DEVICE
-    static void push(
-        size_t pos,
-        AccumulatorFragmentIterator const& iterator_begin,
-        WarpTileIterator& warp_tile_iterator) {
-      int dummy[] = {
-          (pos == Seq) &&
-          (helper<Seq>(iterator_begin, warp_tile_iterator), 0)...};
+
+  /// Perform the epilogue computations and stream the result to global memory.  Implements a
+  /// single codepath, regardless of whether the output op requires addend data to be loaded
+  CUTLASS_DEVICE
+  void unified(
+    OutputOp const &output_op,                      ///< Output operator
+    OutputTileIterator destination_iterator,        ///< Tile iterator for destination
+    AccumulatorTile const &accumulators,            ///< Complete warp-level accumulator tile
+    OutputTileIterator source_iterator )            ///< Tile iterator for addend source
+  {
+    if (!output_op.is_source_needed())
+    {
+      source_iterator.clear_mask();
+      __syncthreads();  // Dummy (CUDA 11.0)
     }
-  };
+
+    operator()(output_op, destination_iterator, accumulators, SourceAspectNeeded(source_iterator));
+  }
+
 
   /// Streams the result to global memory
+  template <typename SourceAspect>
   CUTLASS_DEVICE
-  void compute_source_needed_(
-      OutputOp const& output_op, ///< Output operator
-      OutputTileIterator
-          destination_iterator, ///< Tile iterator for destination
-      AccumulatorTile const&
-          accumulators, ///< Complete warp-level accumulator tile
-      OutputTileSourceIterator
-          source_iterator ///< Threadblock tile coordinate in GEMM (in units of
-                          ///< threadblock tiles)
-  ) {
-    typename OutputTileSourceIterator::Fragment source_fragment[2];
-
-    source_fragment[0].clear();
-    source_iterator.load(source_fragment[0]);
-    ++source_iterator;
-    source_fragment[1].clear();
-
-    //
+  void operator()(
+    OutputOp const &output_op,                      ///< Output operator
+    OutputTileIterator destination_iterator,        ///< Tile iterator for destination
+    AccumulatorTile const &accumulators,            ///< Complete warp-level accumulator tile
+    SourceAspect source)
+  {
     // Iterator over warp-level accumulator fragment
-    //
-
     AccumulatorFragmentIterator accum_fragment_iterator(accumulators);
 
     //
     // Iterate over accumulator tile
     //
 
-#pragma unroll(IterationsUnroll ? OutputTileIterator::kIterations : 1)
-    for (int iter = 0; iter < OutputTileIterator::kIterations; ++iter) {
-      if (iter > 0) {
-        __syncthreads();
-      }
+    #pragma unroll(IterationsUnroll ? OutputTileIterator::kIterations / Base::kFragmentsPerIteration : 1)
+    for (int iter = 0; iter < OutputTileIterator::kIterations; iter += Base::kFragmentsPerIteration)
+    {
+
       //
-      // Load the source for next iteration (pipelining)
+      // Convert and store fragment
       //
 
-      if (iter + 1 < OutputTileIterator::kIterations) {
-        source_iterator.load(source_fragment[(iter + 1) % 2]);
-      }
-      ++source_iterator;
-      acc2smem_source_needed<
-          cutlass::make_index_sequence<OutputTileIterator::kIterations>>::
-          push(iter, accum_fragment_iterator, this->warp_tile_iterator_);
-
       __syncthreads();
 
-      //
-      // Load fragments from shared memory
-      //
-
-      typename SharedLoadIterator::Fragment
-          aligned_accum_fragment[kPartitionsK];
+      CUTLASS_PRAGMA_UNROLL
+      for (int p = 0; p < Base::kFragmentsPerIteration; ++p)
+      {
+        typename AccumulatorFragmentIterator::Fragment accum_fragment;
 
-      shared_load_iterator_.load(aligned_accum_fragment[0]);
+        accum_fragment_iterator.load(accum_fragment);
+        ++accum_fragment_iterator;
 
-      // If the number of k-slices is > 1 - perform a reduction amongst the
-      // k-slices
-      if (kPartitionsK > 1) {
-        plus<typename SharedLoadIterator::Fragment> add_fragments;
+        this->warp_tile_iterator_.store(accum_fragment);
 
-        CUTLASS_PRAGMA_UNROLL
-        for (int i = 1; i < kPartitionsK; ++i) {
-          shared_load_iterator_.add_pointer_offset(kSmemPointerOffset);
-          shared_load_iterator_.load(aligned_accum_fragment[i]);
-          aligned_accum_fragment[0] = add_fragments(
-              aligned_accum_fragment[0], aligned_accum_fragment[i]);
+        if (p < Base::kFragmentsPerIteration - 1) {
+          this->warp_tile_iterator_.add_pointer_offset(kSmemPointerOffset);
         }
-
-        shared_load_iterator_.add_pointer_offset(
-            (1 - kPartitionsK) * kSmemPointerOffset);
       }
 
-      //
-      // Compute the output result
-      //
-
-      typename OutputTileIterator::Fragment output_fragment;
+      if (Base::kFragmentsPerIteration > 1) {
+        this->warp_tile_iterator_.add_pointer_offset(kSmemPointerOffset * (1 - Base::kFragmentsPerIteration));
+      }
 
-      apply_output_operator_(
-          destination_iterator.thread_start_row(),
-          output_fragment,
-          output_op,
-          aligned_accum_fragment[0],
-          source_fragment[iter % 2]);
 
       //
-      // Store the final result
+      // Load fragments from shared memory
       //
 
-      destination_iterator.store(output_fragment);
-      ++destination_iterator;
-    }
-  }
+      __syncthreads();
 
-  /// Helper to invoke the output functor over each vector of output
-  CUTLASS_DEVICE
-  void apply_output_operator_(
-      int begin_row,
-      typename OutputTileIterator::Fragment& output_fragment,
-      OutputOp const& output_op, ///< Output operator
-      typename SharedLoadIterator::Fragment const& aligned_accum_fragment,
-      typename OutputTileSourceIterator::Fragment const& source_fragment) {
-    OutputAccessType* output_frag_ptr =
-        reinterpret_cast<OutputAccessType*>(&output_fragment);
-
-    AccumulatorAccessType const* compute_frag_ptr =
-        reinterpret_cast<AccumulatorAccessType const*>(&aligned_accum_fragment);
-
-    SourceAccessType const* source_frag_ptr =
-        reinterpret_cast<SourceAccessType const*>(&source_fragment);
-
-    int const kOutputOpIterations = OutputTileIterator::Fragment::kElements /
-        OutputTileIterator::kElementsPerAccess;
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < kOutputOpIterations; ++i) {
-      // Call the output operator
-      output_frag_ptr[i] = ApplyEpilogueOp<OutputOp>::apply(
-          output_op,
-          begin_row + getRowOffset(i * OutputTileIterator::kElementsPerAccess),
-          compute_frag_ptr[i],
-          source_frag_ptr[i]);
-    }
-  }
+      CUTLASS_PRAGMA_UNROLL
+      for (int p = 0; p < Base::kFragmentsPerIteration; ++p)
+      {
+        typename SharedLoadIterator::Fragment aligned_accum_fragment;
+        shared_load_iterator_.load(aligned_accum_fragment);
 
-  /// Helper to invoke the output functor over each vector of output
-  CUTLASS_DEVICE
-  void apply_output_operator_source_not_needed_(
-      int begin_row,
-      typename OutputTileIterator::Fragment& output_fragment,
-      OutputOp const& output_op, ///< Output operator
-      typename SharedLoadIterator::Fragment const& aligned_accum_fragment) {
-    OutputAccessType* output_frag_ptr =
-        reinterpret_cast<OutputAccessType*>(&output_fragment);
-
-    AccumulatorAccessType const* compute_frag_ptr =
-        reinterpret_cast<AccumulatorAccessType const*>(&aligned_accum_fragment);
-
-    int const kOutputOpIterations = OutputTileIterator::Fragment::kElements /
-        OutputTileIterator::kElementsPerAccess;
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < kOutputOpIterations; ++i) {
-      // Call the output operator
-      output_frag_ptr[i] = ApplyEpilogueOp<OutputOp>::apply(
-          output_op,
-          begin_row + getRowOffset(i * OutputTileIterator::kElementsPerAccess),
-          compute_frag_ptr[i]);
-    }
-  }
+        if (p < Base::kFragmentsPerIteration - 1)
+        {
+          shared_load_iterator_.add_pointer_offset(kSmemPointerOffset);
+        }
+        else if (kPartitionsK > 1)
+        {
+          plus <typename SharedLoadIterator::Fragment> add_fragments;
 
-  // This should be constexpr, but it's only supported on c++14
-  static int CUTLASS_HOST_DEVICE getRowOffset(int i) {
-    using ThreadMap = typename OutputTileIterator::ThreadMap;
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int cluster = 0; cluster < ThreadMap::Iterations::kCluster;
-         ++cluster) {
-      CUTLASS_PRAGMA_UNROLL
-      for (int group = 0; group < ThreadMap::Iterations::kGroup; ++group) {
-        CUTLASS_PRAGMA_UNROLL
-        for (int row = 0; row < ThreadMap::Iterations::kRow; ++row) {
-          int row_offset = row * ThreadMap::Delta::kRow +
-              group * ThreadMap::Delta::kGroup +
-              cluster * ThreadMap::Delta::kCluster;
-          int frag_row_idx =
-              (row +
-               ThreadMap::Iterations::kRow *
-                   (group + ThreadMap::Iterations::kGroup * cluster));
           CUTLASS_PRAGMA_UNROLL
-          for (int column = 0; column < ThreadMap::Iterations::kColumn;
-               ++column) {
-            int frag_idx = ThreadMap::kElementsPerAccess *
-                (frag_row_idx * ThreadMap::Iterations::kColumn + column);
-            if (i < frag_idx + ThreadMap::kElementsPerAccess) {
-              return row_offset;
-            }
+          for ( int i = 1; i < kPartitionsK; ++i) {
+            typename SharedLoadIterator::Fragment aligned_accum_fragment_addend;
+            shared_load_iterator_.add_pointer_offset(kSmemPointerOffset);
+            shared_load_iterator_.load(aligned_accum_fragment_addend);
+            aligned_accum_fragment = add_fragments(aligned_accum_fragment, aligned_accum_fragment_addend);
           }
+
+          shared_load_iterator_.add_pointer_offset((1 - kPartitionsK) * kSmemPointerOffset);
         }
+
+        //
+        // Compute the output result
+        //
+
+        typename OutputTileIterator::Fragment output_fragment;
+        source.apply_output_operator(output_fragment, output_op, aligned_accum_fragment);
+
+        //
+        // Store the final result
+        //
+
+        destination_iterator.store(output_fragment);
+        ++destination_iterator;
+      }
+
+      if (Base::kFragmentsPerIteration > 1) {
+        shared_load_iterator_.add_pointer_offset(kSmemPointerOffset * (1 - Base::kFragmentsPerIteration));
       }
     }
-    return -1;
   }
+
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
 } // namespace threadblock
 } // namespace epilogue
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_rescale_output.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_planar_complex.h`

 * *Files 17% similar despite different names*

```diff
@@ -8,15 +8,15 @@
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
  *
  * 2. Redistributions in binary form must reproduce the above copyright notice,
  * this list of conditions and the following disclaimer in the documentation
  * and/or other materials provided with the distribution.
  *
- * 3. Neither the name of the copyright holdvr nor the names of its
+ * 3. Neither the name of the copyright holder nor the names of its
  * contributors may be used to endorse or promote products derived from
  * this software without specific prior written permission.
  *
  * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
  * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
  * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
@@ -24,239 +24,214 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
 /*! \file
-  \brief Epilogue for threadblock scoped GEMMs using Tensor Ops.
-
-  The epilogue rearranges the result of a matrix product through shared memory
-  to match canonical tensor layouts in global memory. Epilogues support
-  conversion and reduction operations.
-
-  This is a copy of cutlass/epilogue/threadblock/epilogue.h that can
-  handle "row_id" as a first argument, as uses it to get the corresponding
-  `m_prime` / `s_prime` to rescale the output.
+  \brief Functor performing linear combination operations on planar-complex arrays
 */
 
 #pragma once
 
-#if defined(__CUDACC_RTC__)
-#include <cuda/std/cassert>
-#else
-#include <assert.h>
-#endif
-
-#include "cutlass/aligned_buffer.h"
-#include "cutlass/array.h"
 #include "cutlass/cutlass.h"
-#include "cutlass/functional.h"
-#include "cutlass/layout/tensor.h"
-#include "cutlass/layout/vector.h"
 #include "cutlass/numeric_types.h"
-#include "cutlass/tensor_coord.h"
-
-#include "cutlass/gemm/gemm.h"
-
-#include "cutlass/transform/pitch_linear_thread_map.h"
-#include "cutlass/transform/threadblock/regular_tile_iterator.h"
-
-#include "cutlass/epilogue/threadblock/epilogue_base.h"
-#include "cutlass/epilogue/threadblock/predicated_tile_iterator.h"
-#include "cutlass/numeric_types.h"
-
-#include "cutlass/array.h"
-#include "cutlass/cutlass.h"
-#include "cutlass/epilogue/thread/scale_type.h"
+#include "cutlass/complex.h"
+#include "cutlass/array_planar_complex.h"
 #include "cutlass/functional.h"
 #include "cutlass/numeric_conversion.h"
-#include "cutlass/numeric_types.h"
-#include "epilogue_pipelined.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace thread {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Applies a linear combination operator to an array of elements.
-// output <- alpha * accumulator + beta * source
-//   with:
-//     alpha = 1 / s_prime (to normalize when isLast=True, 1 otherwise)
-//     beta = alpha / m_prime (renormalize the output when the max changes)
-//     source is the current output
+/// Applies a linear combination operator to arrays of planar-complex elements.
+///
+/// D = alpha * accumulator + beta * source + uniform
+///
+/// Note, as with most CUTLASS components for planar complex, the template arguments describe
+/// the underlying real data type.
 template <
-    typename ElementOutput_, ///< Data type used to store tensors
-    typename ElementSource_, //< Data type for source (usually matches
-                             //`ElementOutput`)
-    int Count, ///< Number of elements computed per operation.
-               ///< Usually it is 128/sizeof_bits<ElementOutput_>,
-               ///< but we use 64 or 32 sometimes when there are not enough data
-               ///< to store
-    typename ElementAccumulator_, ///< Accumulator data type
-    typename ElementCompute_, ///< Data type used to compute linear combination
-    bool isFirst,
-    bool isLast,
-    typename FragmentAlphaBeta_,
-    FloatRoundStyle Round = FloatRoundStyle::round_to_nearest>
-class MemoryEfficientAttentionNormalize {
- public:
+  typename ElementOutput_,                             ///< Data type used to load and store tensors
+  int Count,                                           ///< Number of elements computed per operation
+                                                       ///< Usually it is 128/sizeof_bits<ElementOutput_>,
+                                                       ///< but we use 64 or 32 sometimes when there are not enough data to store
+  typename ElementAccumulator_ = ElementOutput_,       ///< Accumulator data type
+  typename ElementCompute_ = ElementOutput_,           ///< Data type used to compute linear combination
+  FloatRoundStyle Round = FloatRoundStyle::round_to_nearest
+>
+class LinearCombinationPlanarComplex {
+public:
+
   using ElementOutput = ElementOutput_;
-  using ElementSource = ElementSource_;
   using ElementAccumulator = ElementAccumulator_;
   using ElementCompute = ElementCompute_;
 
   static int const kCount = Count;
 
-  using FragmentOutput = Array<ElementOutput, kCount>;
-  using FragmentSource = Array<ElementSource, kCount>;
-  using FragmentAccumulator = Array<ElementAccumulator, kCount>;
-  using ComputeFragment = Array<ElementCompute, kCount>;
-  using FragmentAlphaBeta = FragmentAlphaBeta_;
+  using FragmentOutput = ArrayPlanarComplex<ElementOutput, kCount>;
+  using FragmentAccumulator = ArrayPlanarComplex<ElementAccumulator, kCount>;
+  using ComputeFragment = ArrayPlanarComplex<ElementCompute, kCount>;
 
   static FloatRoundStyle const kRound = Round;
 
- private:
+  /// Host-constructable parameters structure
+  struct Params {
+
+    complex<ElementCompute> alpha;                  ///< scales accumulators
+    complex<ElementCompute> beta;                   ///< scales source tensor
+    complex<ElementCompute> const *alpha_ptr;       ///< pointer to accumulator scalar - if not null, loads it from memory
+    complex<ElementCompute> const *beta_ptr;        ///< pointer to source scalar - if not null, loads it from memory
+
+    //
+    // Methods
+    //
+
+    CUTLASS_HOST_DEVICE
+    Params(): 
+      alpha(ElementCompute(1)), 
+      beta(ElementCompute(0)), 
+      alpha_ptr(nullptr), 
+      beta_ptr(nullptr) { }
+
+    CUTLASS_HOST_DEVICE
+    Params(
+      complex<ElementCompute> alpha,
+      complex<ElementCompute> beta
+    ): alpha(alpha), beta(beta), alpha_ptr(nullptr), beta_ptr(nullptr) {
+
+    }
+
+    CUTLASS_HOST_DEVICE
+    Params(
+      complex<ElementCompute> const *alpha_ptr,
+      complex<ElementCompute> const *beta_ptr
+    ): alpha(complex<ElementCompute>()), beta(complex<ElementCompute>()), alpha_ptr(alpha_ptr), beta_ptr(beta_ptr) {
+
+    }
+  };
+
+private:
+
   //
   // Data members
   //
 
-  FragmentAlphaBeta const& s_prime_;
-  FragmentAlphaBeta const& m_prime_;
+  complex<ElementCompute> alpha_;
+  complex<ElementCompute> beta_;
 
- public:
-  /// Constructs the function object, possibly loading from pointers in host
-  /// memory
+public:
+
+  /// Constructs the function object, possibly loading from pointers in host memory
   CUTLASS_HOST_DEVICE
-  MemoryEfficientAttentionNormalize(
-      FragmentAlphaBeta const& s_prime,
-      FragmentAlphaBeta const& m_prime)
-      : s_prime_(s_prime), m_prime_(m_prime) {}
+  LinearCombinationPlanarComplex(Params const &params) {
+
+    alpha_ = (params.alpha_ptr ? *params.alpha_ptr : params.alpha);
+    beta_ = (params.beta_ptr ? *params.beta_ptr : params.beta);
+  }
 
   /// Returns true if source is needed
   CUTLASS_HOST_DEVICE
   bool is_source_needed() const {
-    return !isFirst;
+    return beta_.real() != ElementCompute(0) || beta_.imag() != ElementCompute(0);
   }
 
   /// Functionally required for serial reduction in the epilogue
   CUTLASS_HOST_DEVICE
-  void set_k_partition(int k_partition, int k_partition_count) {}
+  void set_k_partition(int k_partition, int k_partition_count) {
+    if (k_partition) {
+      beta_ = ElementCompute(1);
+    }
+  }
 
   /// Computes linear scaling: D = alpha * accumulator + beta * source
   CUTLASS_HOST_DEVICE
   FragmentOutput operator()(
-      int row,
-      FragmentAccumulator const& accumulator,
-      FragmentSource const& source) const {
-    assert(!isFirst);
+    FragmentAccumulator const &accumulator, 
+    FragmentOutput const &source) const {
 
     // Convert source to interal compute numeric type
-    NumericArrayConverter<ElementCompute, ElementSource, kCount, Round>
-        source_converter;
-    NumericArrayConverter<ElementCompute, ElementAccumulator, kCount, Round>
-        accumulator_converter;
+    NumericArrayConverter<ElementCompute, ElementOutput, kCount, Round> source_converter;
+    NumericArrayConverter<ElementCompute, ElementAccumulator, kCount, Round> accumulator_converter;
 
-    // Convert to destination numeric type
-    NumericArrayConverter<ElementOutput, ElementCompute, kCount, Round>
-        destination_converter;
-
-    ComputeFragment converted_source = source_converter(source);
-    ComputeFragment converted_accumulator = accumulator_converter(accumulator);
+    ComputeFragment converted_source(
+      source_converter(source.real), 
+      source_converter(source.imag));
+
+    ComputeFragment converted_accumulator(
+      accumulator_converter(accumulator.real), 
+      accumulator_converter(accumulator.imag));
 
     // Perform binary operations
     ComputeFragment intermediate;
 
-    multiplies<ComputeFragment> mul_add_source;
-    multiply_add<ComputeFragment> mul_add_accumulator;
+    multiplies<Array<ElementCompute, kCount> > mul_op;
+    multiply_add<Array<ElementCompute, kCount> > mul_add_op;
 
-    ElementCompute alpha = isLast ? (1 / s_prime_[row]) : 1;
-    ElementCompute beta = alpha * m_prime_[row];
+    // complex multiply: I = beta * C
+    intermediate.real = mul_op(beta_.real(), converted_source.real);
+    intermediate.imag = mul_op(beta_.real(), converted_source.imag);
+
+    intermediate.real = mul_add_op(-beta_.imag(), converted_source.imag, intermediate.real);
+    intermediate.imag = mul_add_op( beta_.imag(), converted_source.real, intermediate.imag);
+
+    // complex multiply-add: I = alpha * AB + I
+    intermediate.real = mul_add_op(alpha_.real(), converted_accumulator.real, intermediate.real);
+    intermediate.imag = mul_add_op(alpha_.real(), converted_accumulator.imag, intermediate.imag);
 
-    intermediate = mul_add_source(beta, converted_source); // X =  beta * C
+    intermediate.real = mul_add_op(-alpha_.imag(), converted_accumulator.imag, intermediate.real);
+    intermediate.imag = mul_add_op( alpha_.imag(), converted_accumulator.real, intermediate.imag);
 
-    intermediate = mul_add_accumulator(
-        alpha, converted_accumulator, intermediate); // D = alpha * Accum + X
+    // Convert to destination numeric type
+    NumericArrayConverter<ElementOutput, ElementCompute, kCount, Round> destination_converter;
 
-    return destination_converter(intermediate);
+    return FragmentOutput(
+      destination_converter(intermediate.real), 
+      destination_converter(intermediate.imag));
   }
 
-  /// Computes linear scaling: D = alpha * accumulator
+  /// Computes linear scaling: D = alpha * accumulator + beta * source
   CUTLASS_HOST_DEVICE
-  FragmentOutput operator()(int row, FragmentAccumulator const& accumulator)
-      const {
-    assert(isFirst);
+  FragmentOutput operator()(
+    FragmentAccumulator const &accumulator) const {
 
     // Convert source to interal compute numeric type
-    NumericArrayConverter<ElementCompute, ElementAccumulator, kCount, Round>
-        accumulator_converter;
-
-    // Convert to destination numeric type
-    NumericArrayConverter<ElementOutput, ElementCompute, kCount, Round>
-        destination_converter;
+    NumericArrayConverter<ElementCompute, ElementAccumulator, kCount, Round> accumulator_converter;
 
-    ComputeFragment converted_accumulator = accumulator_converter(accumulator);
+    ComputeFragment converted_accumulator(
+      accumulator_converter(accumulator.real), 
+      accumulator_converter(accumulator.imag));
 
+    // Perform binary operations
     ComputeFragment intermediate;
-    multiplies<ComputeFragment> mul_accumulator;
 
-    ElementCompute alpha = isLast ? (1 / s_prime_[row]) : 1;
+    multiplies<Array<ElementCompute, kCount> > mul_op;
+    multiply_add<Array<ElementCompute, kCount> > mul_add_op;
 
-    intermediate = mul_accumulator(
-        alpha, converted_accumulator); // X =  alpha * C + uniform
+    // complex multiply-add: I = alpha * AB + I
+    intermediate.real = mul_add_op(alpha_.real(), converted_accumulator.real);
+    intermediate.imag = mul_add_op(alpha_.real(), converted_accumulator.imag);
 
-    return destination_converter(intermediate);
-  }
-};
+    intermediate.real = mul_add_op(-alpha_.imag(), converted_accumulator.imag, intermediate.real);
+    intermediate.imag = mul_add_op( alpha_.imag(), converted_accumulator.real, intermediate.imag);
 
-} // namespace thread
+    // Convert to destination numeric type
+    NumericArrayConverter<ElementOutput, ElementCompute, kCount, Round> destination_converter;
 
-namespace threadblock {
-template <
-    typename EO,
-    typename ES,
-    int Count,
-    typename EA,
-    typename EC,
-    bool F,
-    bool L,
-    typename FAB,
-    FloatRoundStyle R>
-struct ApplyEpilogueOp<thread::MemoryEfficientAttentionNormalize<
-    EO,
-    ES,
-    Count,
-    EA,
-    EC,
-    F,
-    L,
-    FAB,
-    R>> {
-  using Op = thread::
-      MemoryEfficientAttentionNormalize<EO, ES, Count, EA, EC, F, L, FAB, R>;
-  static CUTLASS_DEVICE typename Op::FragmentOutput apply(
-      Op const& output_op,
-      int row_id,
-      typename Op::FragmentAccumulator const& accum,
-      typename Op::FragmentSource const& source) {
-    return output_op(row_id, accum, source);
-  }
-  static CUTLASS_DEVICE typename Op::FragmentOutput apply(
-      Op const& output_op,
-      int row_id,
-      typename Op::FragmentAccumulator const& accum) {
-    return output_op(row_id, accum);
+    return FragmentOutput(
+      destination_converter(intermediate.real), 
+      destination_converter(intermediate.imag));
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-} // namespace threadblock
+} // namespace thread
 } // namespace epilogue
 } // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_thread_apply_logsumexp.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/thread/testbed.h`

 * *Files 27% similar despite different names*

```diff
@@ -1,175 +1,242 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights
- *reserved. SPDX-License-Identifier: BSD-3-Clause
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
- * 1. Redistributions of source code must retain the above copyright notice,
- *this list of conditions and the following disclaimer.
+ * 1. Redistributions of source code must retain the above copyright notice, this
+ * list of conditions and the following disclaimer.
  *
  * 2. Redistributions in binary form must reproduce the above copyright notice,
  * this list of conditions and the following disclaimer in the documentation
  * and/or other materials provided with the distribution.
  *
  * 3. Neither the name of the copyright holder nor the names of its
  * contributors may be used to endorse or promote products derived from
  * this software without specific prior written permission.
  *
  * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
  * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- *ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
- *LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- *CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- *SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- *INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- *CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- *ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- *POSSIBILITY OF SUCH DAMAGE.
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+ * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-  \brief Functor performing linear combination operations used by epilogues.
+    \brief Unit tests for thread-level Reduction
 */
 
 #pragma once
 
-#include <cuda_fp16.h>
+#include "cutlass/reduction/thread/reduce.h"
 
-#include "cutlass/array.h"
-#include "cutlass/cutlass.h"
-#include "cutlass/epilogue/thread/activation.h"
-#include "cutlass/functional.h"
-#include "cutlass/numeric_conversion.h"
-#include "cutlass/numeric_types.h"
+#include "cutlass/layout/vector.h"
+#include "cutlass/util/host_tensor.h"
+#include "cutlass/util/tensor_view_io.h"
+
+#include "cutlass/util/reference/host/tensor_copy.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/reference/host/tensor_compare.h"
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-namespace cutlass {
-namespace epilogue {
+namespace test {
+namespace reduction {
 namespace thread {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-namespace detail {
+/// Structure to compute the reduction
+template <
+  /// Data type of elements
+  typename Element,
+  /// Number of elements
+  int N
+>
+struct Testbed_reduce_host {
+
+  /// Thread-level reduction operator
+  using Reduce = cutlass::reduction::thread::Reduce<
+    cutlass::plus<Element>,
+    cutlass::Array<Element, N>
+  >;
 
-template <typename Element, int ElementsPerAccess>
-struct ArrayExponential {
-  CUTLASS_HOST_DEVICE
-  Array<Element, ElementsPerAccess> operator()(
-      Array<Element, ElementsPerAccess> const& input) const {
-    Array<Element, ElementsPerAccess> result;
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < ElementsPerAccess; ++i) {
-      result[i] = expf(input[i]);
-    }
+  //
+  // Data members
+  //
 
-    return result;
+  cutlass::Array<Element, N> tensor_in;
+  cutlass::Array<Element, 1> reduced_tensor_computed;
+  cutlass::Array<Element, 1> reduced_tensor_reference;
+
+  //
+  // Methods
+  //
+
+  /// Allocates workspace in device memory
+  Testbed_reduce_host() {
+    tensor_in.clear();
+    reduced_tensor_computed.clear();
+    reduced_tensor_reference.clear();
   }
-};
 
-template <int ElementsPerAccess>
-struct ArrayExponential<half_t, ElementsPerAccess> {
-  CUTLASS_DEVICE
-  Array<half_t, ElementsPerAccess> operator()(
-      Array<half_t, ElementsPerAccess> const& input) const {
-    Array<half_t, ElementsPerAccess> result;
-
-    int const kVectorCount = ElementsPerAccess / 2;
-
-    __half2 const* input_ptr =
-        reinterpret_cast<__half2 const*>(input.raw_data());
-    __half2* res_ptr = reinterpret_cast<__half2*>(result.raw_data());
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < kVectorCount; ++i) {
-      res_ptr[i] = h2exp(input_ptr[i]);
-    }
+  /// Runs the test
+  bool run() {
 
-    return result;
+    //
+    // initialize memory
+    //
+
+    for(int i = 0; i < N; i++)
+      tensor_in.at(i) = Element(i);
+
+   
+    Reduce reduce;
+
+    cutlass::Array<Element, 1> *out_ptr = &reduced_tensor_computed;
+    out_ptr[0] = reduce(tensor_in);
+
+    //
+    // Reference implementation
+    //
+    Element e(0);
+    for (int i = 0; i < N; i++)
+       e = e + Element(i);
+
+    reduced_tensor_reference.at(0) = e;
+
+    //
+    // Verify equivalence
+    //
+
+    // compare
+    bool passed = reduced_tensor_reference[0] == reduced_tensor_computed[0];
+
+    EXPECT_TRUE(passed) 
+    << "Expected = " << float(reduced_tensor_reference.at(0)) << "\n\n"
+    << "Actual   = " << float(reduced_tensor_computed.at(0)) << "\n\n"
+    << std::endl;
+    
+    return passed;
   }
 };
-} // namespace detail
+
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Applies:
-/// output <- (input - lse).exp()
+/// Thread-level reduction kernel
+template <typename Element, int N>
+__global__ void kernel_reduce(Element const *array_in, Element *result) {
+
+  /// Thread-level reduction operator
+  using Reduce = cutlass::reduction::thread::Reduce<
+    cutlass::plus<Element>,
+    cutlass::Array<Element, N>
+  >;
+
+  Reduce reduce;
+
+  auto ptr_in = reinterpret_cast<cutlass::Array<Element , N> const *>(array_in);
+  auto result_ptr = reinterpret_cast<cutlass::Array<Element , 1> *>(result);
+  auto in = *ptr_in;
+  result_ptr[0] = reduce(in);
+}
+
+
+/// Structure to compute the reduction
 template <
-    typename ElementOutput_, // output
-    typename ElementLSE_, // accumulator from LSE
-    typename ElementAccumulator_, // accumulator from matmul
-    typename ElementCompute_, // intermediate compute (and exp calculation)
-    int ElementsPerAccess>
-class ApplyLogSumExp {
- public:
-  using ElementOutput = ElementOutput_;
-  using ElementAccumulator = ElementAccumulator_;
-  using ElementCompute = ElementCompute_;
-  using ElementLSE = ElementLSE_;
-
-  static int const kElementsPerAccess = ElementsPerAccess;
-  static int const kCount = kElementsPerAccess;
-  static const ScaleType::Kind kScale =
-      cutlass::epilogue::thread::ScaleType::NoBetaScaling;
-
-  using FragmentOutput = Array<ElementOutput, kCount>;
-  using FragmentAccumulator = Array<ElementAccumulator, kElementsPerAccess>;
-  using FragmentCompute = Array<ElementCompute, kElementsPerAccess>;
-  using FragmentLSE = Array<ElementLSE, kElementsPerAccess>;
-  using FragmentScaleBias = FragmentLSE; // Used by epilogue_smem_accumulator.h
+  /// Data type of elements
+  typename Element,
+  /// Number of elements
+  int N
+>
+struct Testbed_reduce_device {
+
+  using Layout = cutlass::layout::PackedVectorLayout;
+
+  //
+  // Data members
+  //
+
+  cutlass::HostTensor<Element, Layout> tensor_in;
+  cutlass::HostTensor<Element, Layout> reduced_tensor_computed;
+  cutlass::HostTensor<Element, Layout> reduced_tensor_reference;
 
- public:
   //
   // Methods
   //
 
-  CUTLASS_HOST_DEVICE
-  ApplyLogSumExp() {}
+  /// Allocates workspace in device memory
+  Testbed_reduce_device() {
 
-  /// Returns true if source is needed
-  CUTLASS_HOST_DEVICE
-  bool is_source_needed() const {
-    return true;
+    tensor_in.reset(cutlass::make_Coord(N), true);
+    reduced_tensor_computed.reset(cutlass::make_Coord(1), true);
+    reduced_tensor_reference.reset(cutlass::make_Coord(1), true);
   }
 
-  /// Functionally required for serial reduction in the epilogue
-  CUTLASS_HOST_DEVICE
-  void set_k_partition(int k_partition, int k_partition_count) {}
-
-  CUTLASS_HOST_DEVICE
-  FragmentOutput operator()(
-      FragmentAccumulator const& AB,
-      FragmentLSE const& scale_unused,
-      // bias used as LSE
-      FragmentLSE const& bias) const {
-    FragmentCompute frag_AB = NumericArrayConverter<
-        ElementCompute,
-        ElementAccumulator,
-        kElementsPerAccess>()(AB);
-    FragmentCompute frag_lse_compute =
-        NumericArrayConverter<ElementCompute, ElementLSE, kElementsPerAccess>()(
-            bias);
-    FragmentCompute frag_compute;
-
-    minus<FragmentCompute> minus_lse;
-    detail::ArrayExponential<ElementCompute, kElementsPerAccess> apply_exp;
-    frag_compute = minus_lse(frag_AB, frag_lse_compute);
-    frag_compute = apply_exp(frag_compute);
-
-    return NumericArrayConverter<
-        ElementOutput,
-        ElementCompute,
-        kElementsPerAccess>()(frag_compute);
+
+  /// Runs the test
+  bool run() {
+
+    //
+    // initialize memory
+    //
+
+    cutlass::reference::host::TensorFill(
+      tensor_in.host_view(),
+      Element(1)
+    );
+
+    cutlass::reference::host::TensorFill(
+      reduced_tensor_computed.host_view(),
+      Element(0)
+    );
+
+    cutlass::reference::host::TensorFill(
+      reduced_tensor_reference.host_view(),
+      Element(N)
+    );
+
+    tensor_in.sync_device();
+    reduced_tensor_computed.sync_device();
+    reduced_tensor_reference.sync_device();
+
+    /// call the kernel
+    kernel_reduce<Element, N><<< dim3(1, 1), dim3(1, 1, 1) >>> (
+        tensor_in.device_data(), 
+        reduced_tensor_computed.device_data()
+        );
+    
+    // verify no errors
+    cudaError_t result = cudaDeviceSynchronize();
+
+    EXPECT_EQ(result, cudaSuccess) << "CUDA ERROR: " << cudaGetErrorString(result);
+    if (result != cudaSuccess) {
+      return false;
+    }
+
+    // Copy back results
+    reduced_tensor_computed.sync_host();
+
+    // Verify equivalence
+    bool passed = cutlass::reference::host::TensorEquals(
+      reduced_tensor_computed.host_view(),
+      reduced_tensor_reference.host_view()
+    );
+
+    EXPECT_TRUE(passed) 
+    << "Expected = " << reduced_tensor_reference.host_view() << "\n\n"
+    << "Actual   = " << reduced_tensor_computed.host_view() << "\n\n"
+    << std::endl;
+    
+    return passed;
   }
 };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
 } // namespace thread
-} // namespace epilogue
-} // namespace cutlass
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
+} // namespace reduction
+} // namespace test
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/find_default_mma.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/find_default_mma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped_problem_visitor.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped_problem_visitor.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multi_head_attention_backward.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_planar_complex_testbed.h`

 * *Files 24% similar despite different names*

```diff
@@ -8,15 +8,15 @@
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
  *
  * 2. Redistributions in binary form must reproduce the above copyright notice,
  * this list of conditions and the following disclaimer in the documentation
  * and/or other materials provided with the distribution.
  *
- * 3. Neither the name of the copyright holdvr nor the names of its
+ * 3. Neither the name of the copyright holder nor the names of its
  * contributors may be used to endorse or promote products derived from
  * this software without specific prior written permission.
  *
  * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
  * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
  * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
@@ -24,272 +24,329 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
+/*! \file
+    \brief Unit testbed for kernel-level GEMM
+*/
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+#pragma once
 
-#include <vector>
-#include <iostream>
 #include <fstream>
 
-#include "kernel_backward.h"
+#include "../../common/cutlass_unit_test.h"
 
-#include "cutlass/util/device_memory.h"
-#include "cutlass/util/host_tensor.h"
+#include "cutlass/cutlass.h"
+#include "cutlass/platform/platform.h"
 
+#include "cutlass/aligned_buffer.h"
+#include "cutlass/gemm/gemm.h"
+#include "cutlass/layout/matrix.h"
+#include "cutlass/layout/vector.h"
+#include "cutlass/numeric_types.h"
+
+#include "cutlass/core_io.h"
+#include "cutlass/util/host_tensor_planar_complex.h"
+#include "cutlass/util/tensor_view_io.h"
+
+#include "cutlass/util/distribution.h"
+#include "cutlass/util/reference/host/gemm_planar_complex.h"
+#include "cutlass/util/reference/host/tensor_compare.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
 
-using Arch = cutlass::arch::Sm80;
-static constexpr int kMaxK = 128;
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-template <typename ArchTag, typename Element, int kMaxK>
-struct DefaultKernel {
-    // Some heuristics to select the best kernel (tested on Sm60, Sm70, Sm80)
-    // NOTE: Requires quite a lot of shmem for Sm80+,
-    // so might require tweaking those manually for Sm86/Sm89
-
-    static constexpr bool kSupports64x128 =
-        ArchTag::kMinComputeCapability >= 80 ||
-        (ArchTag::kMinComputeCapability >= 70 &&
-        cutlass::sizeof_bits<Element>::value <= 16);
-    static constexpr int kBlockSizeI = kSupports64x128 && kMaxK > 64 ? 128 : 64;
-    static constexpr bool kIsHalf = cutlass::sizeof_bits<Element>::value <= 16;
-    static constexpr bool kOutputInRF = kIsHalf && kMaxK <= kBlockSizeI;
-    static constexpr bool kPreload = kIsHalf && ArchTag::kMinComputeCapability >= 80 && kOutputInRF;
-    static constexpr int kBlockSizeJ = kPreload && kMaxK > 64 ? 128 : 64;
-
-    using Kernel = AttentionBackwardKernel<
-        Arch,
-        Element,
-        true,        // kIsAligned_
-        false,       // kApplyDropout_
-        kPreload,// kPreload_
-        kBlockSizeI, // kBlockSizeI_,
-        kBlockSizeJ, // kBlockSizeJ_,
-        kMaxK        // kMaxK
-    >;
-};
+namespace test {
+namespace gemm {
+namespace threadblock {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-namespace {
-template <typename T> struct TypeName;
-template <> struct TypeName<float> { static constexpr const char* Name = "f32"; };
-template <> struct TypeName<cutlass::half_t> { static constexpr const char* Name = "f16"; };
-template <> struct TypeName<cutlass::bfloat16_t> { static constexpr const char* Name = "b16"; };
-
-void readExpect(std::string const& expected) {
-    std::string read;
-    std::cin >> read;
-    if (read != expected) {
-        std::cerr << "FATAL: Read '" << read << "' but expected '" << expected << "'" << std::endl;
-        std::exit(1);
-    }
-}
+template <typename Mma>
+__global__ void kernel_mma_planar_complex(
+  cutlass::gemm::GemmCoord problem_size,
+  typename Mma::IteratorA::Params params_A,
+  typename Mma::IteratorA::Element *ptr_A,
+  int64_t imaginary_stride_A,
+  typename Mma::IteratorB::Params params_B,
+  typename Mma::IteratorB::Element *ptr_B,
+  int64_t imaginary_stride_B,
+  typename Mma::ElementC *ptr_C, 
+  typename Mma::LayoutC::Stride::Index ldc, int64_t imaginary_stride_C) {
+
+  // Shared storage needed by threadblock-scoped matrix multiply-accumulate
+  __shared__ typename Mma::SharedStorage shared_storage;
+
+  // Compute threadblock location
+  cutlass::gemm::GemmCoord tb_tile_offset = {int(blockIdx.x), int(blockIdx.y),
+                                             0};
+
+  cutlass::MatrixCoord tb_offset_A{tb_tile_offset.m() * Mma::Shape::kM,
+                                   tb_tile_offset.k()};
+
+  cutlass::MatrixCoord tb_offset_B{tb_tile_offset.k(),
+                                   tb_tile_offset.n() * Mma::Shape::kN};
+
+  // Compute position within threadblock
+  int tb_thread_id = threadIdx.y * blockDim.x + threadIdx.x;
+
+  // Construct iterators to A operand
+  typename Mma::IteratorA iterator_A_real(params_A, ptr_A,
+                                     {problem_size.m(), problem_size.k()},
+                                     tb_thread_id, tb_offset_A);
+  
+  typename Mma::IteratorA iterator_A_imag(params_A, ptr_A + imaginary_stride_A,
+                                     {problem_size.m(), problem_size.k()},
+                                     tb_thread_id, tb_offset_A);
+  
+  // Construct iterators to B operand
+  typename Mma::IteratorB iterator_B_real(params_B, ptr_B,
+                                     {problem_size.k(), problem_size.n()},
+                                     tb_thread_id, tb_offset_B);
+
+  typename Mma::IteratorB iterator_B_imag(params_B, ptr_B + imaginary_stride_B,
+                                     {problem_size.k(), problem_size.n()},
+                                     tb_thread_id, tb_offset_B);
+
+  int warp_id = threadIdx.y;
+  int lane_id = threadIdx.x;
+
+  // Construct thread-scoped matrix multiply
+  Mma mma(shared_storage, tb_thread_id, warp_id, threadIdx.x);
+
+  typename Mma::FragmentC accum;
+
+  accum.clear();
+
+  int gemm_k_iterations = (problem_size.k() + Mma::Shape::kK - 1) / Mma::Shape::kK;
+
+  // Compute threadblock-scoped matrix multiply-add
+  mma(gemm_k_iterations, accum, iterator_A_real, iterator_A_imag, iterator_B_real, iterator_B_imag, accum);
+
+  // Output results
+  typename Mma::Operator::IteratorC iterator_C({ptr_C, ldc}, lane_id);
+
+  iterator_C.add_tile_offset(
+      {(tb_tile_offset.m() * Mma::WarpCount::kM) +
+           (warp_id % Mma::WarpCount::kM),
+       (tb_tile_offset.n() * Mma::WarpCount::kN) +
+           (warp_id / Mma::WarpCount::kM)});
 
-/// Helpers to read from stdin
-template <typename Element>
-cutlass::HostTensor<Element, cutlass::layout::RowMajor> readTensorOnDevice(std::string const& expectedName) {
-    readExpect("tensor_begin");
-    readExpect(std::string(TypeName<Element>::Name) + ":" + expectedName);
-    uint64_t len = 0;
-    std::cin >> len;
-    readExpect("file");
-    std::string filename;
-    std::cin >> filename;
-
-    cutlass::HostTensor<Element, cutlass::layout::RowMajor> tensor({int64_t(1), int64_t(len / sizeof(Element))});
-    uint8_t* data = (uint8_t*)tensor.host_data();
-
-    std::fstream myFile(filename, std::ios::in | std::ios::binary );
-    myFile.read((char*)data, len);
-    readExpect("tensor_end");
-    tensor.sync_device();
-    return tensor;
-}
+  iterator_C.store(accum.real);
 
-int64_t readInt64(std::string const& expectedName) {
-    readExpect(expectedName);
-    int64_t s = 0;
-    std::cin >> s;
-    return s;
+  iterator_C.store_with_pointer_offset(accum.imag, imaginary_stride_C);
 }
 
-float readFloat(std::string const& expectedName) {
-    readExpect(expectedName);
-    float s = 0;
-    std::cin >> s;
-    return s;
-}
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-// Writing
-template <typename Element>
-void writeTensor(std::string const& name, cutlass::HostTensor<Element, cutlass::layout::RowMajor>& tensor) {
-    tensor.sync_host(); // device->host
-    size_t u8len = tensor.size() * sizeof(Element);
-
-    // Python is expected to provide a file name to write to
-    readExpect("tmpfile");
-    std::string tmpfile;
-    std::cin >> tmpfile;
-
-    uint8_t* data = (uint8_t*)tensor.host_data();
-    std::fstream myFile(tmpfile, std::ios::out | std::ios::binary );
-    myFile.write((char*)data, u8len);
-    myFile.close();
+/// Structure to compute the matrix product
+template <
+    /// Threadblock-level matrix multiply-accumulate
+    typename Mma_>
+struct TestbedPlanarComplex {
+
+  using Mma = Mma_;
+  using ThreadblockShape = typename Mma::Shape;
+  using IteratorA = typename Mma::IteratorA;
+  using ElementA = typename Mma::IteratorA::Element;
+  using LayoutA = typename Mma::IteratorA::Layout;
+  using IteratorB = typename Mma::IteratorB;
+  using ElementB = typename Mma::IteratorB::Element;
+  using LayoutB = typename Mma::IteratorB::Layout;
+  using ElementC = typename Mma::ElementC;
+  using ElementAccumulator = typename Mma::ElementC;
+  using LayoutC = typename Mma::LayoutC;
+  using ThreadMapA = typename Mma::IteratorA::ThreadMap;
+  using ThreadMapB = typename Mma::IteratorB::ThreadMap;
+  using AccessTypeA = cutlass::Array<ElementA, ThreadMapA::kElementsPerAccess>;
+  using AccessTypeB = cutlass::Array<ElementB, ThreadMapB::kElementsPerAccess>;
+  static int const Stages = Mma::kStages;
+  static cutlass::arch::CacheOperation::Kind const CacheOpA =
+      Mma::kCacheOpA;
+  static cutlass::arch::CacheOperation::Kind const CacheOpB =
+      Mma::kCacheOpB;
+
+  //
+  // Data members
+  //
+
+  cutlass::HostTensorPlanarComplex<ElementA, LayoutA> matrix_A;
+  cutlass::HostTensorPlanarComplex<ElementB, LayoutB> matrix_B;
+  cutlass::HostTensorPlanarComplex<ElementC, LayoutC> matrix_C_computed;
+  cutlass::HostTensorPlanarComplex<ElementC, LayoutC> matrix_C_reference;
+
+  cutlass::gemm::GemmCoord problem_size;
+
+  //
+  // Methods
+  //
+
+  /// Allocates workspace in device memory
+  TestbedPlanarComplex(int m, int n, int k)
+      : problem_size(m, n, k) {
+
+    matrix_A.reset(cutlass::make_Coord(m, k));
+    matrix_B.reset(cutlass::make_Coord(k, n));
+    matrix_C_computed.reset(cutlass::make_Coord(m, n));
+    matrix_C_reference.reset(cutlass::make_Coord(m, n), false);
+  }
+
+  /// Runs the test
+  bool run(
+      dim3 grid, dim3 block,
+      cutlass::Distribution::Kind init_A = cutlass::Distribution::Uniform,
+      cutlass::Distribution::Kind init_B = cutlass::Distribution::Uniform) {
+
+    //
+    // initialize device memory
+    //
+
+    if (init_A == cutlass::Distribution::Uniform) {
+      
+      int scope_max = 8;
+      int scope_min = -8;
+
+      if (cutlass::sizeof_bits<ElementA>::value == 4) {
+        scope_max = 2;
+        scope_min = -2;
+      } else if (cutlass::sizeof_bits<ElementA>::value == 1) {
+        scope_max = 2;
+        scope_min = 0;
+      }
+
+      uint64_t seed = 7;
+      cutlass::reference::host::TensorFillRandomUniform(
+          matrix_A.host_view(), seed, scope_max, scope_min, 0);
+      
+    } else if (init_A == cutlass::Distribution::Sequential) {
+      
+      for (int i = 0; i < matrix_A.capacity() * 2; ++i) {
+        matrix_A.host_data()[i] = cutlass::half_t(float(i % 5) - 2);
+      }
+      /*
+      cutlass::reference::host::BlockFillSequential(matrix_A.host_data(),
+                                                    matrix_A.capacity() * 2);
+      */
+    } else if (init_A == cutlass::Distribution::Identity) {
+      //cutlass::reference::host::TensorFillIdentity(matrix_A.host_view());
+    } else {
+      // TODO: Implement the rest
+      return false;
+    }
 
-    std::cout << "tensor_begin " << TypeName<Element>::Name << ":" << name << " ";
-    std::cout << u8len << " file " << tmpfile << " tensor_end" << std::endl;
-}
+    if (init_B == cutlass::Distribution::Uniform) {
 
-void writeInt64(std::string const& name, int64_t value) {
-    std::cout << name << " " << value << std::endl;
-}
-}
+      
+      int scope_max = 8;
+      int scope_min = -8;
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+      if (cutlass::sizeof_bits<ElementB>::value == 4) {
+        scope_max = 2;
+        scope_min = -2;
+      } else if (cutlass::sizeof_bits<ElementB>::value == 1) {
+        scope_max = 2;
+        scope_min = 0;
+      }
 
-template <typename Element>
-int runKernel() {
-    using Kernel = typename DefaultKernel<Arch, Element, kMaxK>::Kernel;
-
-#define READ_I64(NAME) p.NAME = (decltype(p.NAME))readInt64(#NAME)
-#define READ_TENSOR_AND_STRIDES_BMH(DT, NAME, NAME_XS) \
-    auto storage##NAME = readTensorOnDevice<DT>(#NAME); \
-    p.NAME##_ptr = storage##NAME.device_data(); \
-    READ_I64(NAME_XS##_strideB); \
-    READ_I64(NAME_XS##_strideM); \
-    READ_I64(NAME_XS##_strideH);
-
-#define CUDA_CHECK(FN) { \
-    auto cudaError = FN; \
-    if (cudaError != cudaSuccess) { \
-        std::cerr << "FATAL: " #FN " failed: " << cudaGetErrorString(cudaError) << std::endl; \
-        return -1; \
-    } \
-}
+      uint64_t seed = 7;
+      cutlass::reference::host::TensorFillRandomUniform(
+          matrix_B.host_view(), seed + 16, scope_max, scope_min, 0);
+      
 
-    typename Kernel::Params p;
-    p.scale = readFloat("scale");
-    READ_I64(head_dim);
-    READ_I64(head_dim_value);
-    READ_I64(num_queries);
-    READ_I64(num_keys);
-    READ_I64(num_heads);
-    READ_I64(custom_mask_type);
-    READ_I64(num_batches);
-    int64_t repeat_count = readInt64("repeat_count");
-
-    READ_TENSOR_AND_STRIDES_BMH(Element, query, q);
-    READ_TENSOR_AND_STRIDES_BMH(Element, key, k);
-    READ_TENSOR_AND_STRIDES_BMH(Element, value, v);
-    auto lse = readTensorOnDevice<typename Kernel::lse_scalar_t>("logsumexp");
-    p.logsumexp_ptr = lse.device_data();
-    p.lse_strideB = readInt64("lse_strideB");
-    p.lse_strideH = readInt64("lse_strideH");
-
-    // output
-    auto stOutput = readTensorOnDevice<Element>("output");
-    p.output_ptr = stOutput.device_data();
-    READ_I64(o_strideB);
-    auto o_strideM = readInt64("o_strideM");
-    if (o_strideM != p.o_strideM()) {
-        std::cerr << "Invalid `o_strideM`: " << o_strideM << " - expected " << p.o_strideM();
-        return 2;
-    }
-    READ_I64(o_strideH);
+    } else if (init_B == cutlass::Distribution::Sequential) {
 
-    READ_TENSOR_AND_STRIDES_BMH(Element, grad_output, gO);
+      cutlass::reference::host::BlockFillSequential(matrix_B.host_data(),
+                                                    matrix_B.capacity() * 2);
 
-    auto stDelta = readTensorOnDevice<typename Kernel::accum_t>("delta");
-    p.delta_ptr = stDelta.device_data();
-    READ_I64(delta_strideB);
-    READ_I64(delta_strideH);
-
-    // Allocate workspace
-    if (p.workspace_size()) {
-        cudaMalloc(&p.workspace, p.workspace_size());
-    }
+      for (int i = 0; i < matrix_B.capacity() * 2; ++i) {
+        matrix_B.host_data()[i] = cutlass::half_t(float((i + 3) % 5) - 2);
+      }
 
-    // Allocate outputs in BMHK format
-    p.gQKV_strideM_multiplier = 1;
-    p.gQ_strideH = p.head_dim;
-    p.gQ_strideB = p.gQ_strideM() * p.num_queries;
-    p.gK_strideH = p.head_dim;
-    p.gK_strideB = p.gK_strideM() * p.num_keys;
-    p.gV_strideH = p.head_dim_value;
-    p.gV_strideB = p.gV_strideM() * p.num_keys;
-
-    cutlass::HostTensor<Element, cutlass::layout::RowMajor> gQ({int64_t(1), p.gQ_strideB * p.num_batches});
-    cutlass::HostTensor<Element, cutlass::layout::RowMajor> gK({int64_t(1), p.gK_strideB * p.num_batches});
-    cutlass::HostTensor<Element, cutlass::layout::RowMajor> gV({int64_t(1), p.gV_strideB * p.num_batches});
-    p.grad_query_ptr = gQ.device_data();
-    p.grad_key_ptr = gK.device_data();
-    p.grad_value_ptr = gV.device_data();
-
-    if (!Kernel::check_supported(p)) {
-      std::cerr << "FATAL: Kernel does not support these inputs" << std::endl;
-      return 2;
-    }
 
-    // Run kernel
-    cudaDeviceSynchronize();
-    auto kernel_fn = attention_kernel_backward_batched_impl<Kernel>;
-    size_t smem_bytes = sizeof(typename Kernel::SharedStorage);
-    CUDA_CHECK(cudaFuncSetAttribute(kernel_fn, cudaFuncAttributeMaxDynamicSharedMemorySize, int(smem_bytes)));
-    kernel_fn<<<p.getBlocksGrid(), p.getThreadsGrid(), smem_bytes>>>(p);
-
-    // Write outputs
-    std::cout << "OK ";
-    writeTensor("grad_query", gQ);
-    writeInt64("gQ_strideB", p.gQ_strideB);
-    writeInt64("gQ_strideM", p.gQ_strideM());
-    writeInt64("gQ_strideH", p.gQ_strideH);
-    writeTensor("grad_key", gK);
-    writeInt64("gK_strideB", p.gK_strideB);
-    writeInt64("gK_strideM", p.gK_strideM());
-    writeInt64("gK_strideH", p.gK_strideH);
-    writeTensor("grad_value", gV);
-    writeInt64("gV_strideB", p.gV_strideB);
-    writeInt64("gV_strideM", p.gV_strideM());
-    writeInt64("gV_strideH", p.gV_strideH);
-
-    // Timing
-    cudaEvent_t events[2];
-    for (auto & event : events) {
-      CUDA_CHECK(cudaEventCreate(&event));
-    }
-    CUDA_CHECK(cudaEventRecord(events[0]));
-    for (int i = 0; i < repeat_count; ++i) {
-        kernel_fn<<<p.getBlocksGrid(), p.getThreadsGrid(), smem_bytes>>>(p);
-    }
-    CUDA_CHECK(cudaEventRecord(events[1]));
-    CUDA_CHECK(cudaEventSynchronize(events[1]));
-    // Measure elapsed runtime
-    float runtime_ms = 0;
-    CUDA_CHECK(cudaEventElapsedTime(&runtime_ms, events[0], events[1]));
-
-    std::cout << "runtime_ms " << runtime_ms / float(repeat_count) << std::endl;
-    return 0;
-}
+    } else if (init_B == cutlass::Distribution::Identity) {
 
-int main() {
-    std::ios_base::sync_with_stdio(false);
+      //cutlass::reference::host::TensorFillIdentity(matrix_B.host_view());
 
-    std::string dtype;
-    std::cin >> dtype;
-    std::cerr << "Running kernel with dtype: " << dtype << std::endl;
-    if (dtype == "f16") {
-        return runKernel<cutlass::half_t>();
-    } else if (dtype == "b16") {
-        return runKernel<cutlass::bfloat16_t>();
-    } else if (dtype == "f32") {
-        return runKernel<float>();
     } else {
-        std::cerr << "FATAL: Unknown dtype: " << dtype << std::endl;
-        return 3;
+      // TODO: Implement the rest
+      return false;
     }
-}
+
+    matrix_A.sync_device();
+    matrix_B.sync_device();
+    matrix_C_computed.sync_device();
+
+    typename IteratorA::Params params_A(matrix_A.layout());
+    typename IteratorB::Params params_B(matrix_B.layout());
+
+    test::gemm::threadblock::kernel_mma_planar_complex<Mma><<<grid, block>>>(
+        problem_size, 
+        params_A, 
+        matrix_A.device_data(),
+        matrix_A.imaginary_stride(),
+        params_B,
+        matrix_B.device_data(), 
+        matrix_B.imaginary_stride(),
+        matrix_C_computed.device_data(),
+        matrix_C_computed.layout().stride(0), 
+        matrix_C_computed.imaginary_stride()
+      );
+
+
+    //
+    // Check error code
+    //
+
+    cudaError_t result = cudaDeviceSynchronize();
+    EXPECT_EQ(result, cudaSuccess)
+        << " kernel error: " << cudaGetErrorString(result);
+
+    matrix_C_computed.sync_host();
+
+    cutlass::reference::host::GemmPlanarComplex<
+      ElementA, LayoutA,
+      ElementB, LayoutB,
+      ElementC, LayoutC,
+      ElementAccumulator
+    >(
+      problem_size,
+      cutlass::complex<ElementAccumulator>(ElementAccumulator(1)),
+      matrix_A.host_ref(),
+      Mma::kTransformA,
+      matrix_B.host_ref(),
+      Mma::kTransformB,
+      cutlass::complex<ElementAccumulator>(ElementAccumulator(0)),
+      matrix_C_reference.host_ref(),
+      matrix_C_reference.host_ref()
+    );
+    
+    bool passed = cutlass::reference::host::TensorEquals(
+      matrix_C_computed.host_view(), 
+      matrix_C_reference.host_view()
+    );
+
+    EXPECT_TRUE(passed);
+
+    if (!passed) {
+      std::ofstream output("mma_pipelined_testbed_errors.txt");
+
+      output
+        << "A:\n" << matrix_A.host_view() << "\n"
+        << "B:\n" << matrix_B.host_view() << "\n"
+        << "Reference:\n"
+        << matrix_C_reference.host_view() << "\n"
+        << "Computed:\n"
+        << matrix_C_computed.host_view() << "\n";
+    }
+
+    return passed;
+  }
+};
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
+
+}  // namespace threadblock
+}  // namespace gemm
+}  // namespace test
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_base.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_base.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_multistage.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_pipelined.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_pipelined.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/find_default_mma.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped_softmax_mainloop_fusion.h`

 * *Files 15% similar despite different names*

```diff
@@ -8,15 +8,15 @@
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
  *
  * 2. Redistributions in binary form must reproduce the above copyright notice,
  * this list of conditions and the following disclaimer in the documentation
  * and/or other materials provided with the distribution.
  *
- * 3. Neither the name of the copyright holdvr nor the names of its
+ * 3. Neither the name of the copyright holder nor the names of its
  * contributors may be used to endorse or promote products derived from
  * this software without specific prior written permission.
  *
  * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
  * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
  * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
@@ -26,166 +26,139 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-    \brief Cutlass provides helper template functions to figure out the right
-   datastructures to instanciate to run a GEMM with various parameters (see
-   `cutlass/gemm/threadblock/default_mma.h`). However, due to template
-   instantiation priority rules, it will only create an MmaMultiStage with
-   kStages=3 (otherwise creates an MmePipelined - which is not compatible with
-   FastF32). kStages=3 uses too much shared memory and we want to use kStages=2,
-   so we just copy-pasted some code from `default_mma.h` and
-   `default_mma_core.h` files and wrapped this template to allow our usecase.
-
-    This is really only for the FastF32 case - aka using TensorCores with fp32.
+    \brief 
+      Default kernel-level softmax-grouped-GEMM
 */
 
 #pragma once
 
-#include "cutlass/gemm/threadblock/default_mma.h"
-#include "cutlass/gemm/threadblock/default_mma_core_simt.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sm70.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sm75.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
+#include "cutlass/cutlass.h"
+
+#include "cutlass/complex.h"
+#include "cutlass/layout/matrix.h"
+#include "cutlass/numeric_types.h"
+
+#include "cutlass/gemm/kernel/gemm_grouped_softmax_mainloop_fusion.h"
+#include "cutlass/gemm/kernel/gemm_transpose_operands.h"
+#include "cutlass/gemm/kernel/default_gemm.h"
+#include "cutlass/gemm/kernel/default_gemm_complex.h"
+#include "cutlass/gemm/device/default_gemm_configuration.h"
+#include "cutlass/gemm/threadblock/default_mma_softmax_mainloop_fusion.h"
+
+#include "cutlass/layout/permute.h"
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
-namespace threadblock {
+namespace kernel {
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
     /// Element type for A matrix operand
-    typename ElementA,
+    typename ElementA_,
     /// Layout type for A matrix operand
-    typename LayoutA,
+    typename LayoutA_,
+    /// Complex elementwise transformation on A operand
+    ComplexTransform TransformA,
     /// Access granularity of A matrix in units of elements
     int kAlignmentA,
     /// Element type for B matrix operand
-    typename ElementB,
+    typename ElementB_,
     /// Layout type for B matrix operand
-    typename LayoutB,
+    typename LayoutB_,
+    /// Complex elementwise transformation on B operand
+    ComplexTransform TransformB,
     /// Access granularity of B matrix in units of elements
     int kAlignmentB,
+    /// Element type for Scale/Bias vectors
+    typename ElementScaleBias_,
+    /// Layout type for Scale/Bias vectors
+    typename LayoutScaleBias_,
+    /// Element type for C and D matrix operands
+    typename ElementC_,
+    /// Layout type for C and D matrix operands
+    typename LayoutC_,
     /// Element type for internal accumulation
     typename ElementAccumulator,
-    /// Layout type for C and D matrix operand
-    typename LayoutC,
     /// Operator class tag
     typename OperatorClass,
     /// Tag indicating architecture to tune for
     typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape,
-    /// Instruction-level tile size (concept: GemmShape)
+    /// Warp-level tile size (concept: GemmShape)
     typename InstructionShape,
+    /// Epilogue output operator
+    typename EpilogueOutputOp,
+    /// Threadblock-level swizzling operator
+    typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
-    /// Operation perfomed by GEMM
-    typename Operator,
-    typename Enable_ = void>
-struct FindDefaultMma {
-  static constexpr bool AccumulatorsInRowMajor = false;
-  static constexpr SharedMemoryClearOption SharedMemoryClear =
-      SharedMemoryClearOption::kNone;
-  using DefaultMma = cutlass::gemm::threadblock::DefaultMma<
-      ElementA,
-      LayoutA,
-      kAlignmentA,
-      ElementB,
-      LayoutB,
-      kAlignmentB,
-      ElementAccumulator,
-      LayoutC,
-      OperatorClass,
-      ArchTag,
-      ThreadblockShape,
-      WarpShape,
-      InstructionShape,
-      Stages,
-      Operator,
-      AccumulatorsInRowMajor,
-      SharedMemoryClear>;
-};
+    /// Whether the schedule of problems to visit has been precomputed
+    GroupScheduleMode GroupScheduleMode_ = GroupScheduleMode::kDeviceOnly,
+    /// Operation performed by GEMM
+    typename Operator = typename device::DefaultGemmConfiguration<
+        OperatorClass, ArchTag, ElementA_, ElementB_, ElementC_,
+        ElementAccumulator>::Operator,
+    /// Use zfill or predicate for out-of-bound cp.async
+    SharedMemoryClearOption SharedMemoryClear = SharedMemoryClearOption::kNone
+    >
+struct DefaultGemmGroupedSoftmaxMainloopFusion {
+  // If true, we must construct a 'transposed-and-exchanged' Mma operator.
+  static bool const kInternalTranspose = platform::is_same<LayoutC_, layout::ColumnMajor>::value;
 
-/// Specialization for sm80 / FastF32 / multistage with kStages=2
-template <
-    typename ElementA_,
-    /// Layout type for A matrix operand
-    typename LayoutA_,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentA,
-    typename ElementB_,
-    /// Layout type for B matrix operand
-    typename LayoutB_,
-    /// Access granularity of B matrix in units of elements
-    int kAlignmentB,
-    typename ElementAccumulator,
-    /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape,
-    /// Instruction-level tile size (concept: GemmShape)
-    typename InstructionShape,
-    int kStages,
-    typename Operator>
-struct FindDefaultMma<
+  using MapArguments = kernel::detail::MapArguments<
     ElementA_,
     LayoutA_,
+    ComplexTransform::kNone,
     kAlignmentA,
     ElementB_,
     LayoutB_,
+    ComplexTransform::kNone,
     kAlignmentB,
-    ElementAccumulator,
-    layout::RowMajor,
-    arch::OpClassTensorOp,
-    arch::Sm80,
-    ThreadblockShape,
-    WarpShape,
-    InstructionShape,
-    kStages,
-    Operator,
-    typename cutlass::platform::enable_if<(kAlignmentA > 1)>::type> {
-  using LayoutC = layout::RowMajor;
-  using OperatorClass = arch::OpClassTensorOp;
-  using ArchTag = arch::Sm80;
-
-  using DefaultMma_ = cutlass::gemm::threadblock::DefaultMma<
-      ElementA_,
-      LayoutA_,
-      kAlignmentA,
-      ElementB_,
-      LayoutB_,
-      kAlignmentB,
-      ElementAccumulator,
-      LayoutC,
-      OperatorClass,
-      ArchTag,
-      ThreadblockShape,
-      WarpShape,
-      InstructionShape,
-      3,
-      Operator>;
-  struct DefaultMma : DefaultMma_ {
-    using MmaCore_ = typename DefaultMma_::MmaCore;
-    // Define the threadblock-scoped multistage matrix multiply
-    using ThreadblockMma = cutlass::gemm::threadblock::MmaMultistage<
-        typename MmaCore_::Shape,
-        typename DefaultMma_::IteratorA,
-        typename MmaCore_::SmemIteratorA,
-        MmaCore_::kCacheOpA,
-        typename DefaultMma_::IteratorB,
-        typename MmaCore_::SmemIteratorB,
-        MmaCore_::kCacheOpB,
-        ElementAccumulator,
-        LayoutC,
-        typename MmaCore_::MmaPolicy,
-        kStages>;
-  };
+    LayoutC_,
+    kInternalTranspose
+  >;
+
+private:
+  /// Define the threadblock-scoped matrix multiply-accumulate
+  using Mma = typename cutlass::gemm::threadblock::DefaultMmaSoftmaxMainloopFusion<
+      typename MapArguments::ElementA, typename MapArguments::LayoutA, MapArguments::kAlignmentA,
+      typename MapArguments::ElementB, typename MapArguments::LayoutB, MapArguments::kAlignmentB,
+      ElementScaleBias_, LayoutScaleBias_, ElementAccumulator, layout::RowMajor, OperatorClass, ArchTag,
+      ThreadblockShape, WarpShape, InstructionShape, Stages, kInternalTranspose,
+      Operator, false, SharedMemoryClear>::ThreadblockMma;
+
+  static const int kPartitionsK = ThreadblockShape::kK / WarpShape::kK;
+
+  /// Define the epilogue
+  using Epilogue =
+      typename cutlass::epilogue::threadblock::DefaultEpilogueTensorOp<
+          ThreadblockShape, typename Mma::Operator, kPartitionsK, EpilogueOutputOp,
+          EpilogueOutputOp::kCount>::Epilogue;
+
+public:
+  using GemmKernel = kernel::GemmGroupedSoftmaxMainloopFusion<
+    Mma,
+    Epilogue,
+    ThreadblockSwizzle,
+    GroupScheduleMode_,
+    kInternalTranspose
+  >;
 };
 
-} // namespace threadblock
-} // namespace gemm
-} // namespace cutlass
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+}  // namespace kernel
+}  // namespace gemm
+}  // namespace cutlass
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/mma_accum_lambda_iterator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_fusion.h`

 * *Files 27% similar despite different names*

```diff
@@ -8,15 +8,15 @@
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
  *
  * 2. Redistributions in binary form must reproduce the above copyright notice,
  * this list of conditions and the following disclaimer in the documentation
  * and/or other materials provided with the distribution.
  *
- * 3. Neither the name of the copyright holdvr nor the names of its
+ * 3. Neither the name of the copyright holder nor the names of its
  * contributors may be used to endorse or promote products derived from
  * this software without specific prior written permission.
  *
  * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
  * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
  * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
@@ -24,355 +24,440 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
+/*! \file
+    \brief Template for a pipelined fused activation's scale+bias+relu and Implicit GEMM kernel.
+*/
 
 #pragma once
 
-#include "cutlass/functional.h"
-#include "cutlass/gemm/warp/mma_simt_tile_iterator.h"
-#include "cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm70.h"
-#include "cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm80.h"
+#include "cutlass/cutlass.h"
+
+#include "cutlass/aligned_buffer.h"
+#include "cutlass/array.h"
+#include "cutlass/numeric_types.h"
 #include "cutlass/matrix_shape.h"
+#include "cutlass/semaphore.h"
+#include "cutlass/tensor_ref.h"
+#include "cutlass/layout/tensor.h"
+#include "cutlass/gemm/gemm.h"
+#include "cutlass/conv/convolution.h"
+#include "cutlass/conv/conv2d_problem_size.h"
+#include "cutlass/conv/conv3d_problem_size.h"
+#include "cutlass/epilogue/threadblock/output_iterator_parameter.h"
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+namespace cutlass {
+namespace conv {
+namespace kernel {
 
-/*
-TensorCores have different accumulator layouts.
-This file provides a class to easily map the accumulator
-i-th element with the corresponding matrix row/col.
-*/
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-template <typename T, typename accum_t, int kWarpSize>
-struct AccumLambdaIteratorSm80 {
-  static_assert(
-      cutlass::platform::
-          is_same<typename T::Layout, cutlass::layout::RowMajor>::value,
-      "only RowMajor is supported");
-
-  using Policy = typename T::Policy;
-  using InstructionShape = typename T::InstructionShape;
-  using OpDelta = typename T::OpDelta;
-  using Shape = typename T::Shape;
-  static int const kElementsPerAccess = InstructionShape::kN / 4;
-  static int const kRowsPerTile = 8;
-  static int const kAccumulatorRows = InstructionShape::kM / kRowsPerTile;
-
-  static cutlass::MatrixCoord CUTLASS_DEVICE get_lane_offset(
-      int8_t lane_id,
-      int8_t warp_id,
-      typename T::TensorCoord const& tile_offset) {
-    int quad = (lane_id >> 2);
-    int lane_in_quad = (lane_id & 3);
-    return cutlass::MatrixCoord(
-        quad + tile_offset.row() * Shape::kRow,
-        lane_in_quad * kElementsPerAccess +
-            tile_offset.column() * Shape::kColumn);
-  }
-
-  template <typename FA, typename FB, typename FC>
-  CUTLASS_DEVICE static void iterateRows(
-      cutlass::MatrixCoord& lane_offset,
-      FA beginRow,
-      FB op,
-      FC endRow) {
-    // See cutlass/gemm/warp/mma_tensor_op_tile_iterator.h
-    CUTLASS_PRAGMA_UNROLL
-    for (int mma_m = 0; mma_m < Policy::MmaIterations::kRow; ++mma_m) {
-      CUTLASS_PRAGMA_UNROLL
-      for (int row = 0; row < kAccumulatorRows; ++row) {
-        int accum_m = mma_m * InstructionShape::kM * OpDelta::kRow +
-            row * kRowsPerTile + lane_offset.row();
-        beginRow(accum_m);
-
-        CUTLASS_PRAGMA_UNROLL
-        for (int mma_n = 0; mma_n < Policy::MmaIterations::kColumn; ++mma_n) {
-          int mma_accum_start = kAccumulatorRows * kElementsPerAccess *
-              (mma_n * Policy::MmaIterations::kRow + mma_m);
-          CUTLASS_PRAGMA_UNROLL
-          for (int col = 0; col < kElementsPerAccess; ++col) {
-            int accum_n = mma_n * InstructionShape::kN * OpDelta::kColumn +
-                col + lane_offset.column();
-            int idx = mma_accum_start + row * kElementsPerAccess + col;
-            op(accum_m, accum_n, idx);
-          }
-        }
+template <
+  typename Mma_,                                  ///! Threadblock-scoped matrix multiply-accumulate 
+  typename Epilogue_,                             ///! Epilogue
+  typename ThreadblockSwizzle_,                   ///! Threadblock swizzling function
+  conv::Operator ConvOperator,                    ///! Convolutional operator (Fprop, Dgrad, Wgrad)
+  typename ConvProblemSize_ = Conv2dProblemSize   ///! Convolutional operator on 2D or 3D problem
+>
+struct ImplicitGemmConvolutionFusion {
+
+  using Mma = Mma_;
+  using Epilogue = Epilogue_;
+  using EpilogueOutputOp = typename Epilogue::OutputOp;
+  using ThreadblockSwizzle = ThreadblockSwizzle_;
+  static Operator const kConvolutionalOperator = ConvOperator;
+
+  using ElementA = typename Mma::IteratorA::Element;
+  using LayoutA = typename Mma::IteratorA::Layout;
+  using ElementB = typename Mma::IteratorB::Element;
+  using LayoutB = typename Mma::IteratorB::Layout;
+
+  using ElementScaleBias = typename Mma::IteratorScaleBias::Element;
+  using LayoutScaleBias = typename Mma::IteratorScaleBias::Layout;
+
+  using ElementC = typename EpilogueOutputOp::ElementOutput;
+  using LayoutC = LayoutA;
+
+  using ElementAccumulator = typename EpilogueOutputOp::ElementAccumulator;
+  using ElementCompute = typename EpilogueOutputOp::ElementCompute;
+
+  using WarpMmaOperator = typename Mma::Policy::Operator;
+
+  using ArchMmaOperator = typename WarpMmaOperator::ArchMmaOperator;
+  using MathOperator = typename ArchMmaOperator::Operator;
+  
+  using OperatorClass = typename WarpMmaOperator::OperatorClass;
+  using ArchTag = typename WarpMmaOperator::ArchTag;
+
+  using ThreadblockShape = typename Mma::Shape;
+  using WarpShape = typename WarpMmaOperator::Shape;
+  using InstructionShape = typename ArchMmaOperator::Shape;
+
+  static int const kStages = Mma::kStages;
+  static IteratorAlgorithm const kIteratorAlgorithm = Mma::IteratorA::kIteratorAlgorithm; 
+ 
+  /// Warp count (concept: GemmShape)
+  using WarpCount = typename Mma::WarpCount;
+  static int const kThreadCount = 32 * WarpCount::kCount;
+
+  using TensorRefA = typename Mma::IteratorA::TensorRef;
+  using TensorRefB = typename Mma::IteratorB::TensorRef;
+  using TensorRefScaleBias = typename Mma::IteratorScaleBias::TensorRef;
+  using TensorRefC = cutlass::TensorRef<ElementC, LayoutC>;
+
+  /// Check iterator A and B convolution dimension are the same and 
+  // set device::ImplicitGemmConvolution::kConvDim
+  static_assert(Mma::IteratorA::kConvDim == Mma::IteratorB::kConvDim, 
+    "Convolution on different different dimensions is not supported");
+  static int const kConvDim = Mma::IteratorA::kConvDim;
+
+  /// Conv dimension and problem size structure (Conv2d or Conv3d)
+  using ConvProblemSize = ConvProblemSize_;
+
+  static conv::GroupMode const kGroupMode = conv::GroupMode::kNone;
+
+  /// Wgrad C stride idx for implicit gemm algorithm 
+  // Conv2d row-major matrix C (KxRSC) 
+  // Conv3d row-major matrix C (KxTRSC)
+  static int const kWgradCStrideIdx = 
+    platform::is_same<LayoutC, cutlass::layout::TensorNHWC>::value ? 2 : 3;
+
+  /// This chooses the appropriate stride element of the C tensor.
+  static int const kTensorCStrideIdx = 
+    (kConvolutionalOperator == conv::Operator::kWgrad ? kWgradCStrideIdx : 0);
+
+  //
+  //
+  //
+  using ConvOutputIteratorParameter = epilogue::threadblock::ConvOutputIteratorParameter<
+    LayoutC,
+    typename Epilogue::OutputTileIterator::Layout, 
+    TensorRefC,
+    ConvOperator,
+    ConvProblemSize
+    >;
+
+  /// Argument structure
+  struct Arguments {
+
+    //
+    // Data members
+    //
+
+    ConvProblemSize problem_size;
+    TensorRefA ref_A;
+    TensorRefB ref_B;
+    TensorRefScaleBias ref_scale;
+    TensorRefScaleBias ref_bias;
+    TensorRefC ref_C;
+    TensorRefC ref_D;
+    typename EpilogueOutputOp::Params output_op;
+    SplitKMode split_k_mode;
+
+    //
+    // Methods
+    //
+
+    /// Default ctor
+    CUTLASS_HOST_DEVICE
+    Arguments() { }
+   
+    CUTLASS_HOST_DEVICE 
+    Arguments(
+      ConvProblemSize const & problem_size
+    ):
+      problem_size(problem_size) { }
+
+    CUTLASS_HOST_DEVICE
+    Arguments(
+      ConvProblemSize const & problem_size,
+      TensorRefA const & ref_A,
+      TensorRefB const & ref_B,
+      TensorRefScaleBias const & ref_scale,
+      TensorRefScaleBias const & ref_bias,
+      TensorRefC const & ref_C,
+      TensorRefC const & ref_D,
+      typename EpilogueOutputOp::Params const & output_op,
+      SplitKMode const & split_k_mode = SplitKMode::kSerial
+    ):
+      problem_size(problem_size),
+      ref_A(ref_A),
+      ref_B(ref_B),
+      ref_scale(ref_scale),
+      ref_bias(ref_bias),
+      ref_C(ref_C),
+      ref_D(ref_D),
+      output_op(output_op),
+      split_k_mode(split_k_mode)
+    {
 
-        endRow(accum_m);
-      }
     }
-  }
 
-  template <typename DT, typename F>
-  CUTLASS_DEVICE static bool reduceSameRow(int lane_id, DT& myValue, F fn) {
-    // In each warp, 4 threads will work on the same row
-    // - the ones with the same `quad`
-    auto otherV = __shfl_xor_sync(0xffffffff, myValue, 1);
-    myValue = fn(myValue, otherV);
-    otherV = __shfl_xor_sync(0xffffffff, myValue, 2);
-    myValue = fn(myValue, otherV);
-    int lane_in_quad = (lane_id & 3);
-    return lane_in_quad == 0;
-  }
-};
+  };
 
-template <typename T, typename accum_t, int kWarpSize>
-struct AccumLambdaIteratorSm70 {
-  static_assert(
-      cutlass::platform::
-          is_same<typename T::Layout, cutlass::layout::RowMajor>::value,
-      "only RowMajor is supported");
-
-  using Policy = typename T::Policy;
-  using InstructionShape = typename T::InstructionShape;
-  using OpDelta = typename T::OpDelta;
-  using Shape = typename T::Shape;
-  using Element = accum_t;
-
-  static int const kElementsPerPartial = 4;
-  using EleShapePerPatial = typename cutlass::platform::conditional<
-      cutlass::platform::is_same<Element, float>::value,
-      cutlass::MatrixShape<2, 2>,
-      cutlass::MatrixShape<1, 4>>::type;
-  static int const kElementsPerMma = 8;
-  static int const kAccumulatorPatials = 2;
-  using QuadShapePerPatialMma = cutlass::MatrixShape<4, 4>;
-
-  static cutlass::MatrixCoord CUTLASS_DEVICE get_lane_offset(
-      int8_t lane_id,
-      int8_t warp_id,
-      typename T::TensorCoord const& tile_offset) {
-    int quad = (lane_id >> 2);
-    int lane_in_quad = (lane_id & 3);
-    int accum_m, accum_n;
-
-    if (cutlass::platform::is_same<Element, float>::value) {
-      // (quad[2],quad[0])+lane_in_quad[0]
-      accum_m = (((quad & 0x4) >> 1) + (quad & 0x1)) * 8 + (lane_in_quad & 1);
-      // (quad[1])+lane_in_quad[1]
-      accum_n =
-          ((quad >> 1) & 0x1) * kElementsPerPartial * kAccumulatorPatials +
-          (lane_in_quad & 2);
-    } else {
-      accum_m = (((quad & 0x4) >> 1) + (quad & 0x1)) * 8 +
-          lane_in_quad; // (quad[2],quad[0])
-      accum_n = ((quad >> 1) & 0x1) * kElementsPerPartial * kAccumulatorPatials;
+  /// Parameters structure
+  struct Params {
+    ConvProblemSize problem_size;
+    cutlass::gemm::GemmCoord grid_tiled_shape;
+    gemm::GemmCoord implicit_gemm_problem_size;
+    int swizzle_log_tile;
+    int gemm_k_iterations;
+    typename Mma::IteratorA::Params iterator_A;
+    typename Mma::IteratorA::Element const *ptr_A;
+    typename Mma::IteratorB::Params iterator_B;
+    typename Mma::IteratorB::Element const *ptr_B;
+    typename Mma::IteratorScaleBias::Params iterator_scale_bias;
+    typename Mma::IteratorScaleBias::Element const *ptr_scale;
+    typename Mma::IteratorScaleBias::Element const *ptr_bias;
+    typename Epilogue::OutputTileIterator::Params iterator_C;
+    typename Epilogue::OutputTileIterator::Element *ptr_C;
+    typename Epilogue::OutputTileIterator::Params iterator_D;
+    typename Epilogue::OutputTileIterator::Element *ptr_D;
+    typename EpilogueOutputOp::Params output_op;
+    int *semaphore;
+    SplitKMode split_k_mode;
+
+    //
+    // Methods
+    //
+
+    CUTLASS_HOST_DEVICE
+    Params(): swizzle_log_tile(0), gemm_k_iterations(0) { }
+
+    /// 
+    CUTLASS_HOST_DEVICE
+    Params(
+      Arguments const &args,
+      int *semaphore = nullptr
+    ):
+      problem_size(args.problem_size),
+      implicit_gemm_problem_size(cutlass::conv::implicit_gemm_problem_size(kConvolutionalOperator, args.problem_size)),
+      iterator_A(Mma::IteratorA::getParams(args.problem_size, args.ref_A.layout())),
+      ptr_A(args.ref_A.data()),
+      iterator_B(args.problem_size, args.ref_B.layout()),
+      ptr_B(args.ref_B.data()),
+      iterator_scale_bias(args.problem_size, args.ref_scale.layout()),
+      ptr_scale(args.ref_scale.data()),
+      ptr_bias(args.ref_bias.data()),
+      iterator_C(ConvOutputIteratorParameter::layout(args.ref_C)),
+      ptr_C(args.ref_C.data()),
+      iterator_D(ConvOutputIteratorParameter::layout(args.ref_D)),
+      ptr_D(args.ref_D.data()),
+      output_op(args.output_op),
+      semaphore(semaphore),
+      split_k_mode(args.split_k_mode)
+    {
+      gemm_k_iterations = implicit_gemm_k_iterations(kConvolutionalOperator, ThreadblockShape::kK, args.problem_size);
+
+      ThreadblockSwizzle threadblock_swizzle;
+
+      grid_tiled_shape = threadblock_swizzle.get_tiled_shape(
+        implicit_gemm_problem_size,
+        {ThreadblockShape::kM, ThreadblockShape::kN, ThreadblockShape::kK},
+        args.problem_size.split_k_slices);
+
+      swizzle_log_tile = threadblock_swizzle.get_log_tile(grid_tiled_shape);
     }
-    return cutlass::MatrixCoord(
-        accum_m + tile_offset.row() * Shape::kRow,
-        accum_n + tile_offset.column() * Shape::kColumn);
-  }
-
-  template <typename DT, typename F>
-  CUTLASS_DEVICE static bool reduceSameRow(int lane_id, DT& myValue, F fn) {
-    static_assert(
-        cutlass::platform::is_same<Element, float>::value,
-        "update to support non-float accum");
-    // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-fragment-mma-884-f16
-    // T0 & T2 share same line within a quad
-    auto otherV = __shfl_xor_sync(0xffffffff, myValue, 1 << 1);
-    myValue = fn(myValue, otherV);
-    // quad 0 and quad 2 are on the same lines
-    otherV = __shfl_xor_sync(0xffffffff, myValue, 1 << 3);
-    myValue = fn(myValue, otherV);
-    return (lane_id & ((1 << 1) | (1 << 3))) == 0;
-  }
-
-  template <typename FA, typename FB, typename FC>
-  CUTLASS_DEVICE static void iterateRows(
-      cutlass::MatrixCoord& lane_offset,
-      FA beginRow,
-      FB op,
-      FC endRow) {
-    CUTLASS_PRAGMA_UNROLL
-    for (int tile_m = 0; tile_m < Policy::TileIterations::kRow; ++tile_m) {
-      CUTLASS_PRAGMA_UNROLL
-      for (int mma_m = 0; mma_m < Policy::MmaIterations::kRow; ++mma_m) {
-        CUTLASS_PRAGMA_UNROLL
-        for (int m = 0; m < EleShapePerPatial::kRow; ++m) {
-          int accum_m = tile_m * Policy::InterleavedTile::kRow +
-              mma_m * QuadShapePerPatialMma::kRow + m * 2 + lane_offset.row();
-          beginRow(accum_m);
-
-          CUTLASS_PRAGMA_UNROLL
-          for (int tile_n = 0; tile_n < Policy::TileIterations::kColumn;
-               ++tile_n) {
-            CUTLASS_PRAGMA_UNROLL
-            for (int mma_n = 0; mma_n < Policy::MmaIterations::kColumn;
-                 ++mma_n) {
-              CUTLASS_PRAGMA_UNROLL
-              for (int p = 0; p < kAccumulatorPatials; ++p) {
-                CUTLASS_PRAGMA_UNROLL
-                for (int n = 0; n < EleShapePerPatial::kColumn; ++n) {
-                  int mma_accum_start =
-                      (((tile_n * Policy::TileIterations::kRow + tile_m) *
-                            Policy::MmaIterations::kColumn +
-                        mma_n) *
-                           Policy::MmaIterations::kRow +
-                       mma_m) *
-                      kElementsPerMma;
-                  int accum_n = tile_n * Policy::InterleavedTile::kColumn +
-                      mma_n * QuadShapePerPatialMma::kColumn +
-                      p * Policy::InterleavedTile::kColumn / 2 + n +
-                      lane_offset.column();
-                  int idx = mma_accum_start + p * kElementsPerPartial +
-                      m * EleShapePerPatial::kColumn + n;
-                  op(accum_m, accum_n, idx);
-                }
-              }
-            }
-          }
-          endRow(accum_m);
-        }
-      }
+  };
+
+  /// Shared memory storage structure
+  union SharedStorage {
+    typename Mma::SharedStorage main_loop;
+    typename Epilogue::SharedStorage epilogue;
+  };
+
+  //
+  // Methods
+  //
+
+  CUTLASS_HOST_DEVICE
+  ImplicitGemmConvolutionFusion() { } 
+
+  /// Executes one ImplicitGEMM
+  CUTLASS_DEVICE
+  void operator()(Params const &params, SharedStorage &shared_storage) {
+
+    // Compute threadblock location
+    ThreadblockSwizzle threadblock_swizzle;
+
+    cutlass::gemm::GemmCoord threadblock_tile_idx =
+        threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
+
+    // Early exit if CTA is out of range
+    if (params.grid_tiled_shape.m() <= threadblock_tile_idx.m() ||
+      params.grid_tiled_shape.n() <= threadblock_tile_idx.n()) {
+
+      return;
     }
-  }
-};
 
-template <typename T, typename accum_t, int kWarpSize>
-struct AccumLambdaIteratorSimt {
-  using Policy = typename T::Policy;
-  using Iterations = typename T::Iterations;
-  using Element = typename T::Element;
-  using Delta = typename T::Delta;
-  using Shape = typename T::Shape;
-  static_assert(
-      cutlass::platform::
-          is_same<typename T::Layout, cutlass::layout::RowMajor>::value,
-      "only RowMajor is supported");
-
-  template <typename DT, typename F>
-  CUTLASS_DEVICE static bool reduceSameRow(int lane_id, DT& myValue, F fn) {
-    CUTLASS_PRAGMA_UNROLL
-    for (int bit = 1; bit < Policy::WarpShape::kColumn; bit *= 2) {
-      auto otherV = __shfl_xor_sync(0xffffffff, myValue, bit);
-      myValue = fn(myValue, otherV);
+    // Compute position within threadblock
+    int thread_idx = threadIdx.x;
+
+    // Construct iterators to A operand
+    typename Mma::IteratorA iterator_A(
+      params.iterator_A,
+      params.problem_size,
+      params.ptr_A,
+      thread_idx,
+      MatrixCoord(
+        threadblock_tile_idx.m() * Mma::Shape::kM,
+        threadblock_tile_idx.k() * Mma::Shape::kK
+      )
+    );
+    
+    // Construct iterators to B operand
+    typename Mma::IteratorB iterator_B(
+      params.iterator_B,
+      params.problem_size,
+      params.ptr_B,
+      thread_idx,
+      MatrixCoord(
+        threadblock_tile_idx.k() * Mma::Shape::kK,
+        threadblock_tile_idx.n() * Mma::Shape::kN
+      )
+    );
+ 
+    // Construct iterators to A scale/bias vector
+    typename Mma::IteratorScaleBias iterator_scale_bias(
+      params.iterator_scale_bias,
+      params.problem_size,
+      params.ptr_scale,
+      params.ptr_bias,
+      thread_idx,
+      MatrixCoord(
+        0, (kConvolutionalOperator == conv::Operator::kFprop) ?
+                  (threadblock_tile_idx.k() * Mma::Shape::kK) :
+                  // Wgrad
+                  (threadblock_tile_idx.n() * Mma::Shape::kN)
+      )
+    );
+
+    // Broadcast the warp_id computed by lane 0 to ensure dependent code
+    // is compiled as warp-uniform.
+    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
+    int lane_idx = threadIdx.x % 32;
+
+    //
+    // Main loop
+    //
+
+    // Construct thread-scoped matrix multiply
+    Mma mma(shared_storage.main_loop, thread_idx, warp_idx, lane_idx);
+
+    typename Mma::FragmentC accumulators;
+
+    accumulators.clear();
+
+    // Compute threadblock-scoped matrix multiply-add
+    mma(params.gemm_k_iterations, accumulators, iterator_A,
+        iterator_B, iterator_scale_bias, accumulators);
+
+    //
+    // Epilogue
+    //
+
+    EpilogueOutputOp output_op(params.output_op);
+
+    // Construct the semaphore.
+    int block_idx = threadblock_tile_idx.m() + threadblock_tile_idx.n() * params.grid_tiled_shape.m();
+
+    Semaphore semaphore(params.semaphore + block_idx, thread_idx);
+    
+    // Compute logical position within grid
+    threadblock_tile_idx =
+        threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
+
+    // If performing a reduction via split-K, fetch the initial synchronization
+    if (params.split_k_mode == SplitKMode::kSerial && params.grid_tiled_shape.k() > 1) {
+        
+      // Fetch the synchronization lock initially but do not block.
+      semaphore.fetch();
+
+      // Indicate which position in a serial reduction the output operator is currently updating
+      output_op.set_k_partition(threadblock_tile_idx.k(), params.grid_tiled_shape.k());
     }
-    return (lane_id & (Policy::WarpShape::kColumn - 1)) == 0;
-  }
 
-  template <typename FA, typename FB, typename FC>
-  CUTLASS_DEVICE static void iterateRows(
-      cutlass::MatrixCoord& lane_offset,
-      FA beginRow,
-      FB op,
-      FC endRow) {
-    CUTLASS_PRAGMA_UNROLL
-    for (int mma_m = 0; mma_m < Iterations::kRow; ++mma_m) {
-      CUTLASS_PRAGMA_UNROLL
-      for (int m = 0; m < Policy::LaneMmaShape::kM; ++m) {
-        int accum_m = mma_m * Delta::kRow + m + lane_offset.row();
-        beginRow(accum_m);
-
-        CUTLASS_PRAGMA_UNROLL
-        for (int mma_n = 0; mma_n < Iterations::kColumn; ++mma_n) {
-          int accum_n =
-              mma_n * Policy::WarpShape::kColumn * Policy::LaneMmaShape::kN +
-              lane_offset.column();
-          CUTLASS_PRAGMA_UNROLL
-          for (int n = 0; n < Policy::LaneMmaShape::kN; ++n) {
-            int idx = n +
-                Policy::LaneMmaShape::kN *
-                    (mma_n +
-                     Iterations::kColumn *
-                         (m + mma_m * Policy::LaneMmaShape::kM));
-            op(accum_m, accum_n + n, idx);
-          }
-        }
-        endRow(accum_m);
+    MatrixCoord threadblock_offset(
+      threadblock_tile_idx.m() * Mma::Shape::kM,
+      threadblock_tile_idx.n() * Mma::Shape::kN
+    );
+
+    // Tile iterator writing to destination tensor
+    typename Epilogue::OutputTileIterator iterator_D(
+      params.iterator_D,
+      params.ptr_D,
+      ConvOutputIteratorParameter::extent(params.problem_size),
+      thread_idx,
+      threadblock_offset
+    );
+    
+    // Tile iterator reading from source accumulator tensor
+    typename Epilogue::OutputTileIterator iterator_C(
+      params.iterator_C,
+      params.ptr_C,
+      ConvOutputIteratorParameter::extent(params.problem_size),
+      thread_idx,
+      threadblock_offset
+    );
+
+    // Construct the epilogue
+    Epilogue epilogue(
+      shared_storage.epilogue, 
+      thread_idx, 
+      warp_idx, 
+      lane_idx);
+
+    // Wait on the semaphore - this latency may have been covered by iterator construction
+    if (params.split_k_mode == SplitKMode::kSerial && params.grid_tiled_shape.k() > 1) {
+        
+      // For subsequent threadblocks, the source matrix is held in the 'D' tensor.
+      if (threadblock_tile_idx.k()) {
+        iterator_C = iterator_D;
       }
+
+      semaphore.wait(threadblock_tile_idx.k());
+
+    }
+    // Each split-k-slice writes to a unique tensor location
+    else if (params.split_k_mode == SplitKMode::kParallel) {
+      iterator_D.add_pointer_offset(threadblock_tile_idx.k() * 
+        cutlass::conv::implicit_gemm_tensor_c_size(ConvOperator, params.problem_size));
     }
-  }
 
-  static cutlass::MatrixCoord CUTLASS_DEVICE get_lane_offset(
-      int8_t lane_id,
-      int8_t warp_id,
-      typename T::TensorCoord const& tile_offset) {
-    static_assert(
-        cutlass::platform::is_same<
-            typename Policy::LaneLayout,
-            cutlass::layout::RowMajorInterleaved<1>>::value,
-        "");
-    typename Policy::LaneLayout lane_layout = Policy::get_lane_layout();
-
-    cutlass::MatrixCoord lane_offset = lane_layout.inverse(lane_id) *
-        cutlass::MatrixCoord(Policy::LaneMmaShape::kM,
-                             Policy::LaneMmaShape::kN);
-    return lane_offset +
-        tile_offset * cutlass::MatrixCoord(Shape::kRow, Shape::kColumn);
-  }
-};
+    // Run efficient epilogue
+    epilogue(output_op, iterator_D, accumulators, iterator_C);
+  
+    //
+    // Release the semaphore
+    //
 
-template <typename T, typename accum_t, int kWarpSize>
-struct DefaultMmaAccumLambdaIterator;
+    if (params.split_k_mode == SplitKMode::kSerial && params.grid_tiled_shape.k() > 1) { 
 
-// Simt
-template <typename S, typename P, typename accum_t, int kWarpSize>
-struct DefaultMmaAccumLambdaIterator<
-    cutlass::gemm::warp::MmaSimtTileIterator<
-        S,
-        cutlass::gemm::Operand::kC,
-        accum_t,
-        cutlass::layout::RowMajor,
-        P,
-        1,
-        1>,
-    accum_t,
-    kWarpSize> {
-  using WarpIterator = typename cutlass::gemm::warp::MmaSimtTileIterator<
-      S,
-      cutlass::gemm::Operand::kC,
-      accum_t,
-      cutlass::layout::RowMajor,
-      P,
-      1,
-      1>;
-  using Iterator = AccumLambdaIteratorSimt<WarpIterator, accum_t, kWarpSize>;
-};
+      int lock = 0;
+      if (params.grid_tiled_shape.k() == threadblock_tile_idx.k() + 1) {
 
-// TensorOp - Volta
-template <typename S1, typename S2, typename accum_t, int kWarpSize>
-struct DefaultMmaAccumLambdaIterator<
-    cutlass::gemm::warp::MmaVoltaTensorOpAccumulatorTileIterator<
-        S1,
-        accum_t,
-        cutlass::layout::RowMajor,
-        S2,
-        cutlass::MatrixShape<1, 1>>,
-    accum_t,
-    kWarpSize> {
-  using WarpIterator =
-      typename cutlass::gemm::warp::MmaVoltaTensorOpAccumulatorTileIterator<
-          S1,
-          accum_t,
-          cutlass::layout::RowMajor,
-          S2,
-          cutlass::MatrixShape<1, 1>>;
-  using Iterator = AccumLambdaIteratorSm70<WarpIterator, accum_t, kWarpSize>;
+        // The final threadblock resets the semaphore for subsequent grids.
+        lock = 0;
+      }
+      else {
+        // Otherwise, the semaphore is incremented
+        lock = threadblock_tile_idx.k() + 1;
+      }
+      
+      semaphore.release(lock);
+    }
+  } 
 };
 
-// TensorOp - Sm75+
-template <
-    typename S1,
-    typename S2,
-    typename S3,
-    typename accum_t,
-    int kWarpSize>
-struct DefaultMmaAccumLambdaIterator<
-    cutlass::gemm::warp::MmaTensorOpAccumulatorTileIterator<
-        S1,
-        accum_t,
-        cutlass::layout::RowMajor,
-        S2,
-        S3>,
-    accum_t,
-    kWarpSize> {
-  using WarpIterator =
-      typename cutlass::gemm::warp::MmaTensorOpAccumulatorTileIterator<
-          S1,
-          accum_t,
-          cutlass::layout::RowMajor,
-          S2,
-          S3>;
-  using Iterator = AccumLambdaIteratorSm80<WarpIterator, accum_t, kWarpSize>;
-};
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+} // namespace kernel
+} // namespace conv
+} // namespace cutlass
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/mma_from_smem.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h`

 * *Files 8% similar despite different names*

```diff
@@ -39,34 +39,30 @@
 #include "cutlass/arch/memory.h"
 #include "cutlass/array.h"
 #include "cutlass/cutlass.h"
 #include "cutlass/epilogue/thread/linear_combination.h"
 #include "cutlass/epilogue/threadblock/default_epilogue_simt.h"
 #include "cutlass/epilogue/threadblock/default_epilogue_tensor_op.h"
 #include "cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h"
-#include "cutlass/functional.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/gemm/warp/mma_tensor_op_fragment_iterator.h"
 #include "cutlass/matrix_shape.h"
 #include "cutlass/numeric_conversion.h"
 #include "cutlass/numeric_types.h"
-#include "cutlass/platform/platform.h"
 #include "cutlass/transform/threadblock/vector_iterator.h"
 
-#include "../epilogue/epilogue_thread_apply_logsumexp.h"
-#include "../gemm/mma_accum_lambda_iterator.h"
-#include "../gemm_kernel_utils.h"
-#include "../iterators/make_residual_last.h"
-#include "../iterators/transpose_warp_iterator.h"
-#include "../iterators/warp_iterator_from_smem.h"
+#include "attention_scaling_coefs_updater.h"
 #include "cutlass/epilogue/threadblock/epilogue_smem_accumulator.h"
 #include "cutlass/gemm/threadblock/mma_base.h"
-#include "cutlass/gemm/threadblock/mma_multistage.h"
-#include "cutlass/gemm/threadblock/mma_pipelined.h"
 #include "cutlass/gemm/warp/mma_tensor_op_tile_access_iterator.h"
+#include "epilogue_thread_apply_logsumexp.h"
+#include "gemm_kernel_utils.h"
+#include "iterators/make_residual_last.h"
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace threadblock {
 
 /// Shared storage object needed by accumulator
 /// From 13_two_tensor_op_fusion/threadblock/b2b_mma_base_smem_accumulator.h
@@ -246,103 +242,27 @@
       ///< ID of warp
       int warp_idx,
       ///< ID of each thread within a warp
       int lane_idx)
       : warp_tile_iterator_B_(shared_storage.operand_B_ref(), lane_idx) {}
 };
 
-namespace {
-
-// has necessary trait compliance with WarpIteratorFromSmem but doesn't do
-// anything, can be default initialized, and uses fragment that takes up
-// (almost) no space. this warp iterator is selected at compile time when
-// elementwise on-the-fly scaling for operand A is disabled, in which case
-// operations related to loading scale factors for operand A get wiped out by
-// the compiler.
-template <typename TensorRef>
-class NoOpWarpIteratorScale {
- public:
-  // in pipelined+multistage MMA implementations we keep an array of fragments.
-  // if we aren't using scaling we don't want to waste registers on fragments
-  // of scale elements, so ideally this would be sized 0.
-  // using size 1 is kind of a hack to get around arrays of zero-sized objects
-  // not being allowed. the compiler is probably smart enough to wipe it out
-  // anyways.
-  using Fragment = cutlass::Array<char, 1>;
-
-  CUTLASS_HOST_DEVICE
-  NoOpWarpIteratorScale() {}
-
-  CUTLASS_HOST_DEVICE
-  NoOpWarpIteratorScale(TensorRef const&, int) {}
-
-  CUTLASS_HOST_DEVICE
-  NoOpWarpIteratorScale& add_tile_offset(
-      typename TensorRef::TensorCoord const&) {
-    return *this;
-  }
-
-  CUTLASS_HOST_DEVICE
-  NoOpWarpIteratorScale& operator++() {
-    return *this;
-  }
-
-  CUTLASS_DEVICE
-  void load(Fragment&) const {}
-};
-
-// if scaling is enabled, performs fragment elementwise multiplication between
-// fragment and its scaling factor.
-template <typename Fragment, typename FragmentScale, bool ScalingEnabled>
-class FragmentElementwiseScaler;
-
-// specialization for scaling being enabled.
-template <typename Fragment, typename FragmentScale>
-class FragmentElementwiseScaler<Fragment, FragmentScale, true> {
- public:
-  // cast scale_frag to correct type then apply elementwise to fragment
-  CUTLASS_DEVICE
-  static Fragment apply(Fragment frag, FragmentScale const& scale_frag) {
-    Fragment converted_scale_frag = cutlass::NumericArrayConverter<
-        typename Fragment::Element,
-        typename FragmentScale::Element,
-        FragmentScale::kElements>()(scale_frag);
-    return cutlass::multiplies<Fragment>()(frag, converted_scale_frag);
-  }
-};
-
-// specialization for scaling being disabled. doesn't do anything and should
-// just get wiped out by the compiler.
-template <typename Fragment, typename FragmentScale>
-class FragmentElementwiseScaler<Fragment, FragmentScale, false> {
- public:
-  CUTLASS_DEVICE
-  static Fragment apply(Fragment frag, FragmentScale const&) {
-    return frag;
-  }
-};
-} // namespace
-
 ////////////////////////////////////////////////////////////////////////////////
 // Taken from
 // https://github.com/NVIDIA/cutlass/blob/master/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined_smem_accumulator.h
 ////////////////////////////////////////////////////////////////////////////////
 
 /// Structure to compute the matrix product targeting CUDA cores and SIMT math
 /// instructions.
 template <
     /// Size of the Gemm problem - concept: gemm::GemmShape<>
     typename Shape_,
     // BEGIN smem
     /// Iterates over the intermediate accumulator tile in shared memory
     typename WarpIteratorA,
-    /// whether or not to perform elementwise multiplication of A
-    //  by another matrix (A_scale) that is also kept in shared memory prior
-    //  to matmul A @ B
-    bool ScaleOperandA_,
     // Accumulator type
     typename AccumulatorSharedStorage,
     // END smem
     /// Iterates over tiles of B operand in global memory
     //  (concept: ReadableTileIterator | ForwardTileIterator |
     //  MaskedTileIterator)
     typename IteratorB_,
@@ -373,23 +293,14 @@
       Shape_,
       AccumulatorSharedStorage::Shape::kN,
       Policy_,
       2>;
 
   using Shape =
       Shape_; ///< Size of the Gemm problem - concept: gemm::GemmShape<>
-  static constexpr bool ScaleOperandA = ScaleOperandA_;
-
-  ///< loads fragments of A_scale from shared memory if operand A scaling is
-  ///< enabled. otherwise no-op.
-  using WarpIteratorAScale = typename cutlass::platform::conditional<
-      ScaleOperandA,
-      WarpIteratorA,
-      NoOpWarpIteratorScale<typename WarpIteratorA::TensorRef>>::type;
-
   using IteratorB =
       IteratorB_; ///< Iterates over tiles of B operand in global memory
   using ElementC = ElementC_; ///< Data type of accumulator matrix
   using LayoutC = LayoutC_; ///< Layout of accumulator matrix
   using Policy = Policy_; ///< Policy describing tuning details
 
   using SmemIteratorB = SmemIteratorB_;
@@ -418,79 +329,28 @@
   // staticaly assert kStages for MmaPipelined is two (Double-buffered pipeline)
   static_assert(
       (Base::kStages == 2),
       "MmaPipelined requires kStages set to value 2");
 
  private:
   using WarpFragmentA = typename Operator::FragmentA;
-
-  /// fragment type of OperandA elementwise scaling matrix. (almost) empty
-  /// if operand A scaling is disabled.
-  using WarpFragmentAScale = typename WarpIteratorAScale::Fragment;
-
   using WarpFragmentB = typename Operator::FragmentB;
 
-  /// applies scaling factor to operand A fragment if operand A scaling is
-  /// enabled. otherwise no-op.
-  using FragmentAScaler = FragmentElementwiseScaler<
-      WarpFragmentA,
-      WarpFragmentAScale,
-      ScaleOperandA>;
-
  protected:
   // /// Iterator to write threadblock-scoped tile of A operand to shared memory
   // SmemIteratorA smem_iterator_A_;
 
   /// Iterator to write threadblock-scoped tile of B operand to shared memory
   SmemIteratorB smem_iterator_B_;
 
   /// Iterator to load a warp-scoped tile of A operand from intermediate
   /// accumulator tile
   WarpIteratorA warp_tile_iterator_A_;
 
-  /// Iterator to load a warp-scoped tile of A_scale from intermediate
-  /// accumulator tile (only used if ScaleOperandA_ is true)
-  WarpIteratorAScale warp_tile_iterator_A_scale_;
-
  public:
-  /// constructor for MMA with operand A scaling enabled.
-  CUTLASS_DEVICE
-  MmaPipelinedFromSharedMemory(
-      // shared storage needed for internal use by threadblock-scoped GEMM
-      typename Base::SharedStorage& shared_storage,
-      // warp iterator over A tile held in shared memory
-      WarpIteratorA warp_iter_a,
-      // warp iterator over A_scale tile held in shared memory
-      WarpIteratorAScale warp_iter_a_scale,
-      int thread_idx,
-      int warp_idx,
-      int lane_idx)
-      : Base(shared_storage, thread_idx, warp_idx, lane_idx),
-        warp_tile_iterator_A_(warp_iter_a),
-        warp_tile_iterator_A_scale_(warp_iter_a_scale),
-        smem_iterator_B_(shared_storage.operand_B_ref(), thread_idx) {
-    // Compute warp location within threadblock tile by mapping the warp_id to
-    // three coordinates:
-    //   _m: the warp's position within the threadblock along the M dimension
-    //   _n: the warp's position within the threadblock along the N dimension
-    //   _k: the warp's position within the threadblock along the K dimension
-    int warp_idx_mn = warp_idx % (Base::WarpCount::kM * Base::WarpCount::kN);
-    int warp_idx_k = warp_idx / (Base::WarpCount::kM * Base::WarpCount::kN);
-    int warp_idx_m = warp_idx_mn % Base::WarpCount::kM;
-    int warp_idx_n = warp_idx_mn / Base::WarpCount::kM;
-
-    // Add per-warp offsets in units of warp-level tiles
-    this->warp_tile_iterator_A_.add_tile_offset(
-        {warp_idx_m, Base::kWarpGemmIterations * warp_idx_k});
-    this->warp_tile_iterator_A_scale_.add_tile_offset(
-        {warp_idx_m, Base::kWarpGemmIterations * warp_idx_k});
-    this->warp_tile_iterator_B_.add_tile_offset(
-        {Base::kWarpGemmIterations * warp_idx_k, warp_idx_n});
-  }
-
   /// Construct from tensor references
   CUTLASS_DEVICE
   MmaPipelinedFromSharedMemory(
       typename Base::SharedStorage&
           shared_storage, ///< Shared storage needed for internal use by
                           ///< threadblock-scoped GEMM
       AccumulatorSharedStorage& accumulator_shared_storage,
@@ -520,15 +380,15 @@
         {Base::kWarpGemmIterations * warp_idx_k, warp_idx_n});
   }
 
   // For API compatibility with MmaMultistageFromSharedMemory
   // but not supported as it worsens perf: older gpus < sm80 don't
   // support async tranfers and have to waste registers
   CUTLASS_DEVICE
-  void set_prologue_done(bool value) {}
+  bool set_prologue_done(bool value) {}
   CUTLASS_DEVICE
   static void prologue(
       typename Base::SharedStorage& shared_storage,
       IteratorB iterator_B1,
       int thread_idx,
       int problem_size_0_n) {}
 
@@ -565,34 +425,27 @@
 
     this->smem_iterator_B_.store(transform_B(tb_frag_B));
 
     ++this->smem_iterator_B_;
 
     __syncthreads();
 
-    // remember that WarpFragmentAScale and WarpIteratorAScale are empty/no-op
-    // if scaling is disabled.
-
     // Pair of fragments used to overlap shared memory loads and math
     // instructions
     WarpFragmentA warp_frag_A[2];
-    WarpFragmentAScale warp_frag_A_scale[2];
     WarpFragmentB warp_frag_B[2];
     warp_frag_A[0].clear();
-    warp_frag_A_scale[0].clear();
     warp_frag_B[0].clear();
 
     this->warp_tile_iterator_B_.set_kgroup_index(0);
 
     this->warp_tile_iterator_A_.load(warp_frag_A[0]);
-    this->warp_tile_iterator_A_scale_.load(warp_frag_A_scale[0]);
     this->warp_tile_iterator_B_.load(warp_frag_B[0]);
 
     ++this->warp_tile_iterator_A_;
-    ++this->warp_tile_iterator_A_scale_;
     ++this->warp_tile_iterator_B_;
 
     Operator warp_mma;
 
     int smem_write_stage_idx = 1;
 
     // Avoid reading out of bounds
@@ -646,20 +499,17 @@
 
         // Only read the next if we need to
         if (hasNext) {
           this->warp_tile_iterator_B_.set_kgroup_index(
               (warp_mma_k + 1) % Base::kWarpGemmIterations);
 
           this->warp_tile_iterator_A_.load(warp_frag_A[(warp_mma_k + 1) % 2]);
-          this->warp_tile_iterator_A_scale_.load(
-              warp_frag_A_scale[(warp_mma_k + 1) % 2]);
           this->warp_tile_iterator_B_.load(warp_frag_B[(warp_mma_k + 1) % 2]);
 
           ++this->warp_tile_iterator_A_;
-          ++this->warp_tile_iterator_A_scale_;
           ++this->warp_tile_iterator_B_;
 
           if (warp_mma_k == 0) {
             iterator_B.load(tb_frag_B);
 
             ++iterator_B;
 
@@ -667,16 +517,15 @@
             iterator_B.set_residual_tile(gemm_k_iterations == 3);
             iterator_B.clear_mask(gemm_k_iterations <= 2);
           }
         }
 
         warp_mma(
             accum,
-            FragmentAScaler::apply(
-                warp_frag_A[warp_mma_k % 2], warp_frag_A_scale[warp_mma_k % 2]),
+            warp_frag_A[warp_mma_k % 2],
             warp_frag_B[warp_mma_k % 2],
             accum);
       }
     }
   }
 };
 
@@ -688,18 +537,14 @@
 /// Structure to compute the matrix product targeting CUDA cores and SIMT math
 /// instructions.
 template <
     /// Size of the Gemm problem - concept: gemm::GemmShape<>
     typename Shape1_,
     /// Iterates over the intermediate accumulator tile in shared memory
     typename WarpIteratorA1_,
-    /// whether or not to perform elementwise multiplication of A
-    //  by another matrix (A_scale) that is also kept in shared memory prior
-    //  to matmul A @ B
-    bool ScaleOperandA_,
     // Accumulator type
     typename AccumulatorSharedStorage,
     /// Iterates over tiles of B operand in global memory
     //  (concept: ReadableTileIterator | ForwardTileIterator |
     //  MaskedTileIterator)
     typename IteratorB1_,
     /// Iterates over tiles of B operand in shared memory
@@ -711,42 +556,41 @@
     typename ElementC_,
     /// Data type of accumulator matrix
     typename LayoutC_,
     /// Policy describing tuning details (concept: MmaPolicy)
     typename Policy1_,
     /// Number of stages,
     int Stages_,
-    int kMaxK_,
     /// Used for partial specialization
     typename Enable = bool>
-class MmaMultistageFromSharedMemory
-    : public MmaBaseFromSharedMemory<Shape1_, kMaxK_, Policy1_, Stages_> {
+class MmaMultistageFromSharedMemory : public MmaBaseFromSharedMemory<
+                                          Shape1_,
+                                          AccumulatorSharedStorage::Shape::kN,
+                                          Policy1_,
+                                          Stages_> {
  public:
   ///< Base class
-  using Base = MmaBaseFromSharedMemory<Shape1_, kMaxK_, Policy1_, Stages_>;
+  using Base = MmaBaseFromSharedMemory<
+      Shape1_,
+      AccumulatorSharedStorage::Shape::kN,
+      Policy1_,
+      Stages_>;
 
   ///< Size of the Gemm problem - concept: gemm::GemmShape<>
   using Shape1 = Shape1_;
   ///< Iterates over tiles of B operand in global memory
   using IteratorB1 = IteratorB1_;
   using IteratorB = IteratorB1;
   ///< Policy describing tuning details
   using Policy1 = Policy1_;
 
   using SmemIteratorB1 = SmemIteratorB1_;
   using WarpIteratorA1 = WarpIteratorA1_; ///< Iterates over the intermediate
                                           ///< accumulator tile in shared memory
-  static constexpr bool ScaleOperandA = ScaleOperandA_;
 
-  ///< warp level iterator over A_scale matrix tile kept in shared memory.
-  ///< if elementwise A scaling is disabled then everything this does is no-op.
-  using WarpIteratorAScale = typename cutlass::platform::conditional<
-      ScaleOperandA,
-      WarpIteratorA1,
-      NoOpWarpIteratorScale<typename WarpIteratorA1::TensorRef>>::type;
   ///< Data type of accumulator matrix
   using ElementC = ElementC_;
   ///< Layout of accumulator matrix
   using LayoutC = LayoutC_;
 
   static cutlass::arch::CacheOperation::Kind const kCacheOpB1 = CacheOpB1;
   static constexpr bool kSmemContainsEntireB = Base::kSmemContainsEntireB;
@@ -786,85 +630,33 @@
   };
 
   static constexpr int kNumStagesConcurrentLoad =
       kSmemContainsEntireB ? Base::kStages : Base::kStages - 1;
 
  private:
   using WarpLoadedFragmentA1 = typename Operator1::FragmentA;
-  /// fragment of OperandA scale matrix. if operand A scaling is disabled this
-  /// is (almost) empty.
-  using WarpLoadedFragmentA1Scale = typename WarpIteratorAScale::Fragment;
   using WarpLoadedFragmentB1 = typename Operator1::FragmentB;
   using WarpTransformedFragmentA1 = typename Operator1::TransformedFragmentA;
   using WarpTransformedFragmentB1 = typename Operator1::TransformedFragmentB;
 
-  /// applies elementwise scaling to fragment of A. if operand A scaling is
-  /// disabled this is a no-op.
-  using FragmentAScaler = FragmentElementwiseScaler<
-      WarpLoadedFragmentA1,
-      WarpLoadedFragmentA1Scale,
-      ScaleOperandA>;
-
  private:
   //
   // Data members
   //
 
   /// Iterator to load a warp-scoped tile of A1 operand from intermediate
   /// accumulator tile
   WarpIteratorA1 warp_tile_iterator_A1_;
 
-  /// Iterator to load a warp-scoped tile of A1_scale operand from shared memory
-  /// if operand A scaling is disabled everything this does is a no-op.
-  WarpIteratorAScale warp_tile_iterator_A1_scale_;
-
   /// Iterator to write threadblock-scoped tile of B operand to shared memory
   SmemIteratorB1 smem_iterator_B1_;
 
   bool prologue_done_;
 
  public:
-  /// constructor for MMA with operand A scaling enabled.
-  CUTLASS_DEVICE
-  MmaMultistageFromSharedMemory(
-      // shared storage needed for internal use by threadblock-scoped GEMM
-      typename Base::SharedStorage& shared_storage,
-      // warp level iterator over operand A tile kept in shared memory
-      WarpIteratorA1 warp_tile_iterator_A1,
-      // warp level iterator over operand A elementwise scale tile kept in
-      // shared memory.
-      WarpIteratorAScale warp_tile_iterator_A1_scale,
-      int thread_idx,
-      int warp_idx,
-      int lane_idx)
-      : Base(shared_storage, thread_idx, warp_idx, lane_idx),
-        warp_tile_iterator_A1_(warp_tile_iterator_A1),
-        warp_tile_iterator_A1_scale_(warp_tile_iterator_A1_scale),
-        smem_iterator_B1_(shared_storage.operand_B_ref(), thread_idx),
-        prologue_done_(false) {
-    // Compute warp location within threadblock tile by mapping the warp_id to
-    // three coordinates:
-    //   _m: the warp's position within the threadblock along the M dimension
-    //   _n: the warp's position within the threadblock along the N dimension
-    //   _k: the warp's position within the threadblock along the K dimension
-    int warp_idx_mn_1 =
-        warp_idx % (Base::WarpCount1::kM * Base::WarpCount1::kN);
-    int warp_idx_k_1 = warp_idx / (Base::WarpCount1::kM * Base::WarpCount1::kN);
-    int warp_idx_m_1 = warp_idx_mn_1 % Base::WarpCount1::kM;
-    int warp_idx_n_1 = warp_idx_mn_1 / Base::WarpCount1::kM;
-
-    // Add per-warp offsets in units of warp-level tiles
-    warp_tile_iterator_A1_.add_tile_offset(
-        {warp_idx_m_1, Base::kWarpGemmIterations1 * warp_idx_k_1});
-    warp_tile_iterator_A1_scale_.add_tile_offset(
-        {warp_idx_m_1, Base::kWarpGemmIterations1 * warp_idx_k_1});
-    this->warp_tile_iterator_B_.add_tile_offset(
-        {Base::kWarpGemmIterations1 * warp_idx_k_1, warp_idx_n_1});
-  }
-
   /// Construct from tensor references
   CUTLASS_DEVICE
   MmaMultistageFromSharedMemory(
       typename Base::SharedStorage&
           shared_storage, ///< Shared storage needed for internal use by
                           ///< threadblock-scoped GEMM
       AccumulatorSharedStorage& accumulator_shared_storage,
@@ -899,15 +691,15 @@
     warp_tile_iterator_A1_.add_tile_offset(
         {warp_idx_m_1, Base::kWarpGemmIterations1 * warp_idx_k_1});
     this->warp_tile_iterator_B_.add_tile_offset(
         {Base::kWarpGemmIterations1 * warp_idx_k_1, warp_idx_n_1});
   }
 
   CUTLASS_DEVICE
-  void set_prologue_done(bool value) {
+  bool set_prologue_done(bool value) {
     prologue_done_ = value;
   }
 
   CUTLASS_DEVICE
   static void prologue(
       typename Base::SharedStorage& shared_storage,
       IteratorB iterator_B1,
@@ -1052,45 +844,37 @@
       iterator_B1.clear_mask(gemm_k_iterations_1 <= 0);
     }
 
     // DEPBAR+SYNC
     cutlass::arch::cp_async_wait<kNumStagesConcurrentLoad - 1>();
     __syncthreads();
 
-    // remember that WarpFragmentAScale and WarpIteratorAScale are no-op/empty
-    // if scaling is disabled.
-
     // Pair of fragments used to overlap shared memory loads and math
     // instructions
     WarpLoadedFragmentA1 warp_loaded_frag_A1[2];
-    WarpLoadedFragmentA1Scale warp_loaded_frag_A1_scale[2];
     WarpLoadedFragmentB1 warp_loaded_frag_B1[2];
     WarpTransformedFragmentA1 warp_transformed_frag_A1[2];
     WarpTransformedFragmentB1 warp_transformed_frag_B1[2];
 
     Operator1 warp_mma1;
 
     warp_tile_iterator_A1_.load(warp_loaded_frag_A1[0]);
     ++warp_tile_iterator_A1_;
 
-    warp_tile_iterator_A1_scale_.load(warp_loaded_frag_A1_scale[0]);
-    ++warp_tile_iterator_A1_scale_;
-
     this->warp_tile_iterator_B_.set_kgroup_index(0);
     this->warp_tile_iterator_B_.load(warp_loaded_frag_B1[0]);
     ++this->warp_tile_iterator_B_;
 
     int smem_write_stage_idx = Base::kStages - 1;
     int smem_read_stage_idx = 0;
 
     warp_mma1.transform(
         warp_transformed_frag_A1[0],
         warp_transformed_frag_B1[0],
-        FragmentAScaler::apply(
-            warp_loaded_frag_A1[0], warp_loaded_frag_A1_scale[0]),
+        warp_loaded_frag_A1[0],
         warp_loaded_frag_B1[0]);
 
     // tf32x3 kernels use staging accumulation. warp_mma uses a temporary
     // accumulator and this temporary accumulator is added to the final
     // accumulator once in every mainloop iteration.
     plus<FragmentC1> plus_accum;
 
@@ -1127,30 +911,25 @@
         this->warp_tile_iterator_B_.set_kgroup_index(
             (warp_mma_k + 1) % Base::kWarpGemmIterations1);
         // skip warp tile loading for the last kgroup (we are out of the buf)
         if (gemm_k_iterations_1 > (-Base::kStages + 2) ||
             warp_mma_k < Base::kWarpGemmIterations1 - 1) {
           warp_tile_iterator_A1_.load(
               warp_loaded_frag_A1[(warp_mma_k + 1) % 2]);
-          warp_tile_iterator_A1_scale_.load(
-              warp_loaded_frag_A1_scale[(warp_mma_k + 1) % 2]);
           this->warp_tile_iterator_B_.load(
               warp_loaded_frag_B1[(warp_mma_k + 1) % 2]);
         }
         ++warp_tile_iterator_A1_;
-        ++warp_tile_iterator_A1_scale_;
         ++this->warp_tile_iterator_B_;
 
         if (warp_mma_k > 0)
           warp_mma1.transform(
               warp_transformed_frag_A1[warp_mma_k % 2],
               warp_transformed_frag_B1[warp_mma_k % 2],
-              FragmentAScaler::apply(
-                  warp_loaded_frag_A1[warp_mma_k % 2],
-                  warp_loaded_frag_A1_scale[warp_mma_k % 2]),
+              warp_loaded_frag_A1[warp_mma_k % 2],
               warp_loaded_frag_B1[warp_mma_k % 2]);
 
         if (platform::is_same<
                 typename Operator1::MathOperator,
                 arch::OpMultiplyAddFastF32>::value ||
             platform::is_same<
                 typename Operator1::MathOperator,
@@ -1232,17 +1011,15 @@
 
         // Do any conversions feeding the first stage at the end of the loop so
         // we can start right away on mma instructions
         if (warp_mma_k + 1 == Base::kWarpGemmIterations1)
           warp_mma1.transform(
               warp_transformed_frag_A1[(warp_mma_k + 1) % 2],
               warp_transformed_frag_B1[(warp_mma_k + 1) % 2],
-              FragmentAScaler::apply(
-                  warp_loaded_frag_A1[(warp_mma_k + 1) % 2],
-                  warp_loaded_frag_A1_scale[(warp_mma_k + 1) % 2]),
+              warp_loaded_frag_A1[(warp_mma_k + 1) % 2],
               warp_loaded_frag_B1[(warp_mma_k + 1) % 2]);
       }
     }
 
     if (platform::is_same<
             typename Operator1::MathOperator,
             arch::OpMultiplyAddFastF32>::value ||
@@ -1254,47 +1031,24 @@
   }
 };
 
 template <
     typename WarpShape,
     typename InstructionShape,
     typename RegularWarpIterator,
-    typename Policy,
-    typename Enable = void>
+    typename Policy>
 struct DefaultWarpIteratorAFromSharedMemory {};
 
-// TensorOp - Ampere half
-template <typename RegularWarpIterator, typename Policy>
-struct DefaultWarpIteratorAFromSharedMemory<
-    cutlass::gemm::GemmShape<32, 32, 32>,
-    cutlass::gemm::GemmShape<16, 8, 8>,
-    RegularWarpIterator,
-    Policy,
-    typename platform::enable_if<(
-        sizeof_bits<typename RegularWarpIterator::Element>::value == 16 &&
-        Policy::Operator::Policy::OpDelta::kRow == 1)>::type> {
-  static constexpr auto kWarpSize = 32;
-  using OpDelta = typename Policy::Operator::Policy::OpDelta;
-  using WarpShape = cutlass::MatrixShape<32, 32>;
-
-  using WarpIterator = cutlass::gemm::warp::WarpIteratorFromSmem<
-      cutlass::gemm::Operand::kA,
-      typename RegularWarpIterator::Element>;
-};
-
-// TensorOp - Ampere f32
+// TensorOp - Ampere
 template <typename WarpShape, typename RegularWarpIterator, typename Policy>
 struct DefaultWarpIteratorAFromSharedMemory<
     WarpShape,
     cutlass::gemm::GemmShape<16, 8, 8>,
     RegularWarpIterator,
-    Policy,
-    typename platform::enable_if<(
-        sizeof_bits<typename RegularWarpIterator::Element>::value != 16 ||
-        Policy::Operator::Policy::OpDelta::kRow != 1)>::type> {
+    Policy> {
   using InstructionShape = cutlass::gemm::GemmShape<16, 8, 8>;
   static constexpr auto kWarpSize = 32;
   using OpDelta = typename Policy::Operator::Policy::OpDelta;
 
   using WarpIterator =
       cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<
           cutlass::MatrixShape<WarpShape::kM, WarpShape::kK>,
@@ -1341,21 +1095,15 @@
 
   // We just use the same iterator, as we reproduced the same shared-memory
   // schema. Just modify it to handle non-complete tiles.
   using WarpIterator = RegularWarpIterator;
 };
 
 // Converts a "regular" Mma into their counterpart from shared memory
-template <
-    typename Mma_,
-    typename AccumulatorSharedStorage,
-    /// whether or not to apply elementwise multiplication of operand A by
-    /// another matrix in shared memory before usage in A @ B
-    bool kScaleOperandA,
-    bool kTransposeA = false>
+template <typename Mma_, typename AccumulatorSharedStorage>
 struct DefaultMmaFromSharedMemory;
 
 // Mma pipelined
 template <
     /// Size of the Gemm problem - concept: gemm::GemmShape<>
     typename Shape_,
     /// Iterates over tiles of A operand in global memory
@@ -1378,34 +1126,28 @@
     typename LayoutC_,
     /// Policy describing tuning details (concept: MmaPolicy)
     typename Policy_,
     /// Transformation applied to A operand
     typename TransformA_,
     /// Transformation applied to B operand
     typename TransformB_,
-    typename AccumulatorSharedStorage_,
-    /// whether or not to apply elementwise multiplication of operand A by
-    /// another matrix in shared memory before usage in A @ B
-    bool kScaleOperandA,
-    bool kTransposeA>
+    typename AccumulatorSharedStorage_>
 struct DefaultMmaFromSharedMemory<
     MmaPipelined<
         Shape_,
         IteratorA_,
         SmemIteratorA_,
         IteratorB_,
         SmemIteratorB_,
         ElementC_,
         LayoutC_,
         Policy_,
         TransformA_,
         TransformB_>,
-    AccumulatorSharedStorage_,
-    kScaleOperandA,
-    kTransposeA> {
+    AccumulatorSharedStorage_> {
   static constexpr int kWarpSize = 32;
   using SmemAccumulatorLayout = cutlass::layout::RowMajor;
 
   using RegularMma = MmaPipelined<
       Shape_,
       IteratorA_,
       SmemIteratorA_,
@@ -1417,28 +1159,26 @@
       TransformA_,
       TransformB_>;
 
   using WarpShape = typename Policy_::Operator::Shape;
   using InstructionShape = typename Policy_::Operator::InstructionShape;
   using ArchMmaOperator = typename Policy_::Operator;
 
-  static constexpr bool kIsTransposedA = false;
   using WarpIteratorA = typename DefaultWarpIteratorAFromSharedMemory<
       WarpShape,
       InstructionShape,
       typename RegularMma::Operator::IteratorA,
       Policy_>::WarpIterator;
   using IteratorB =
       typename cutlass::transform::threadblock::MakeIteratorResidualLast<
           IteratorB_>::Iterator;
 
   using Mma = typename cutlass::gemm::threadblock::MmaPipelinedFromSharedMemory<
       Shape_,
       WarpIteratorA,
-      kScaleOperandA,
       AccumulatorSharedStorage_,
       IteratorB,
       SmemIteratorB_,
       ElementC_,
       LayoutC_,
       Policy_>;
 };
@@ -1470,36 +1210,30 @@
     typename LayoutC_,
     /// Policy describing tuning details (concept: MmaPolicy)
     typename Policy_,
     /// Number of stages,
     int Stages,
     /// Use zfill or predicate for out-of-bound cp.async
     SharedMemoryClearOption SharedMemoryClear,
-    typename AccumulatorSharedStorage_,
-    /// whether or not to apply elementwise multiplication of operand A by
-    /// another matrix in shared memory before usage in A @ B
-    bool kScaleOperandA,
-    bool kTransposeA>
+    typename AccumulatorSharedStorage_>
 struct DefaultMmaFromSharedMemory<
     MmaMultistage<
         Shape_,
         IteratorA_,
         SmemIteratorA_,
         CacheOpA,
         IteratorB_,
         SmemIteratorB_,
         CacheOpB,
         ElementC_,
         LayoutC_,
         Policy_,
         Stages,
         SharedMemoryClear>,
-    AccumulatorSharedStorage_,
-    kScaleOperandA,
-    kTransposeA> {
+    AccumulatorSharedStorage_> {
   static constexpr int kWarpSize = 32;
 
   using RegularMma = MmaMultistage<
       Shape_,
       IteratorA_,
       SmemIteratorA_,
       CacheOpA,
@@ -1510,52 +1244,41 @@
       LayoutC_,
       Policy_,
       Stages,
       SharedMemoryClear>;
 
   using WarpShape = typename Policy_::Operator::Shape;
   using InstructionShape = typename Policy_::Operator::InstructionShape;
-  using WarpIteratorA_ = typename DefaultWarpIteratorAFromSharedMemory<
+  using WarpIteratorA = typename DefaultWarpIteratorAFromSharedMemory<
       WarpShape,
       InstructionShape,
       typename RegularMma::Operator::IteratorA,
       Policy_>::WarpIterator;
-  using WarpIteratorTranspose = TransposeWarpIterator<WarpIteratorA_>;
-  static constexpr bool kIsTransposedA =
-      WarpIteratorTranspose::kSupportsTranspose && kTransposeA;
-  using WarpIteratorA = typename platform::conditional<
-      kIsTransposedA,
-      typename WarpIteratorTranspose::Iterator,
-      WarpIteratorA_>::type;
-
-  static int constexpr kMaxK = kIsTransposedA
-      ? AccumulatorSharedStorage_::Shape::kM
-      : AccumulatorSharedStorage_::Shape::kN;
+
+  static int constexpr kMaxK = AccumulatorSharedStorage_::Shape::kN;
   // Reduce the number of stages if we don't need that many
   static int constexpr kStagesMax =
       (kMaxK + int(Shape_::kK) - 1) / int(Shape_::kK);
   static int constexpr kStages = cutlass::const_min(Stages, kStagesMax);
 
   using IteratorB =
       typename cutlass::transform::threadblock::MakeIteratorResidualLast<
           IteratorB_>::Iterator;
   using Mma =
       typename cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<
           Shape_,
           WarpIteratorA,
-          kScaleOperandA,
           AccumulatorSharedStorage_,
           IteratorB,
           SmemIteratorB_,
           RegularMma::kCacheOpB,
           ElementC_,
           LayoutC_,
           Policy_,
-          kStages,
-          kMaxK>;
+          kStages>;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
     typename IteratorC,
     typename Operator,
@@ -1875,25 +1598,26 @@
       int warp_id,
       int lane_id,
       cutlass::MatrixCoord const& tile_coords) {
     // Non-optimized way to apply LSE to registers
     // NOTE: accum is attn.T
     // TODO: Optimize for each architecture
     static constexpr int WarpSize = 32;
-    using AccumLambdaIterator =
-        typename DefaultMmaAccumLambdaIterator<IteratorC, accum_t, WarpSize>::
-            Iterator;
+    using RegistersIter = typename DefaultAttentionScalingCoefsUpdater<
+        IteratorC,
+        accum_t,
+        WarpSize>::Updater;
     auto lane_offset =
-        AccumLambdaIterator::get_lane_offset(lane_id, warp_id, tile_coords);
+        RegistersIter::get_lane_offset(lane_id, warp_id, tile_coords);
 
     cutlass::Array<lse_scalar_t, IteratorC::Fragment::kElements> lse_prefetched;
     lse_prefetched.clear();
     int rowIdx = 0;
     int colIdx = 0;
-    AccumLambdaIterator::iterateRows(
+    RegistersIter::iterateRows(
         lane_offset,
         [&](int accum_m) {
           ++rowIdx;
           colIdx = 0;
         },
         [&](int accum_m, int accum_n, int idx) {
           if (rowIdx == 1) {
@@ -2014,25 +1738,26 @@
       int warp_id,
       int lane_id,
       cutlass::MatrixCoord const& tile_coords) {
     // Non-optimized way to apply LSE to registers
     // NOTE: accum is attn.T
     // TODO: Optimize for each architecture
     static constexpr int WarpSize = 32;
-    using AccumLambdaIterator =
-        typename DefaultMmaAccumLambdaIterator<IteratorC, accum_t, WarpSize>::
-            Iterator;
+    using RegistersIter = typename DefaultAttentionScalingCoefsUpdater<
+        IteratorC,
+        accum_t,
+        WarpSize>::Updater;
     auto lane_offset =
-        AccumLambdaIterator::get_lane_offset(lane_id, warp_id, tile_coords);
+        RegistersIter::get_lane_offset(lane_id, warp_id, tile_coords);
 
     cutlass::Array<lse_scalar_t, IteratorC::Fragment::kElements> lse_prefetched;
     lse_prefetched.clear();
     int rowIdx = 0;
     int colIdx = 0;
-    AccumLambdaIterator::iterateRows(
+    RegistersIter::iterateRows(
         lane_offset,
         [&](int accum_m) {
           ++rowIdx;
           colIdx = 0;
         },
         [&](int accum_m, int accum_n, int idx) {
           if (rowIdx == 1) {
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm_kernel_utils.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm_kernel_utils.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/epilogue_predicated_tile_iterator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/epilogue_predicated_tile_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/make_residual_last.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/make_residual_last.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_access_iterator_residual_last.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_access_iterator_residual_last.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_iterator_residual_last.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_iterator_residual_last.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/transpose_warp_iterator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_policy.h`

 * *Files 22% similar despite different names*

```diff
@@ -8,15 +8,15 @@
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
  *
  * 2. Redistributions in binary form must reproduce the above copyright notice,
  * this list of conditions and the following disclaimer in the documentation
  * and/or other materials provided with the distribution.
  *
- * 3. Neither the name of the copyright holdvr nor the names of its
+ * 3. Neither the name of the copyright holder nor the names of its
  * contributors may be used to endorse or promote products derived from
  * this software without specific prior written permission.
  *
  * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
  * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
  * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
@@ -24,30 +24,42 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
+/*! \file
+    \brief Policy describing implementation details of warp-level GEMM targeting Tensor Cores.
+*/
 
 #pragma once
 
-#include "warp_iterator_from_smem.h"
+#include "cutlass/cutlass.h"
+#include "cutlass/matrix_shape.h"
+#include "cutlass/gemm/gemm.h"
 
-template <typename WarpIterator>
-struct TransposeWarpIterator {
-  using Iterator = char;
-  static bool constexpr kSupportsTranspose = false;
-};
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+namespace cutlass {
+namespace gemm {
+namespace warp {
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
+/// Policy 
 template <
-    /// Operand identity
-    cutlass::gemm::Operand Operand,
-    /// Data type of A elements
-    typename Element,
-    bool kTranspose>
-struct TransposeWarpIterator<
-    cutlass::gemm::warp::WarpIteratorFromSmem<Operand, Element, kTranspose>> {
-  using Iterator =
-      cutlass::gemm::warp::WarpIteratorFromSmem<Operand, Element, !kTranspose>;
-  static bool constexpr kSupportsTranspose = true;
+  typename Operator_,        ///< hardware instruction(s) performing TensorOp (concept: arch::Mma)
+  typename OpDelta_          ///< distance between operations (concept: MatrixShape)
+>
+struct MmaTensorOpPolicy {
+
+  using Operator = Operator_;    ///< hardware instruction(s) performing TensorOp (concept: arch::Mma)
+  using OpDelta = OpDelta_;      ///< distance between operations (concept: MatrixShape)
+  using MmaShape = typename Operator::Shape;
 };
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+} // namespace warp
+} // namespace gemm
+} // namespace cutlass
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/warp_iterator_from_smem.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_fixed_stride_dilation.h`

 * *Files 19% similar despite different names*

```diff
@@ -1,278 +1,314 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights
- *reserved. SPDX-License-Identifier: BSD-3-Clause
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
- * 1. Redistributions of source code must retain the above copyright notice,
- *this list of conditions and the following disclaimer.
+ * 1. Redistributions of source code must retain the above copyright notice, this
+ * list of conditions and the following disclaimer.
  *
  * 2. Redistributions in binary form must reproduce the above copyright notice,
  * this list of conditions and the following disclaimer in the documentation
  * and/or other materials provided with the distribution.
  *
  * 3. Neither the name of the copyright holder nor the names of its
  * contributors may be used to endorse or promote products derived from
  * this software without specific prior written permission.
  *
  * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
  * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- *ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
- *LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- *CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- *SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- *INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- *CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- *ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- *POSSIBILITY OF SUCH DAMAGE.
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+ * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Inspired from
-   "cutlass/gemm/warp/mma_tensor_op_tile_access_iterator.h" Loads tiles of GEMM
-   operands from a RowMajor shared-memory layout into registers to use by A100
-   TensorCores.
-
-    The difference with "mma_tensor_op_tile_access_iterator.h" is that:
-    (1) We use "ldmatrix" to load tiles, rather than manual loads (slightly
-   faster) (2) We support to transpose the operand (eg read `A.transpose()` when
-   the shared memory holds `A`)
+    \brief Templates implementing loading of convolution tiles mapped to GEMM A (activation tile)
+    matrix from memory.
 
-    This is only implemented for the specific shapes.
+    This iterator assumes TensorNHWC layout of tensors in Global Memory.
 */
+
 #pragma once
 
-#include <cutlass/gemm/gemm.h>
+#include "cutlass/array.h"
+#include "cutlass/conv/conv2d_problem_size.h"
+#include "cutlass/conv/convolution.h"
+#include "cutlass/conv/threadblock/depthwise_direct_conv_params.h"
+#include "cutlass/coord.h"
+#include "cutlass/cutlass.h"
+#include "cutlass/layout/matrix.h"
+#include "cutlass/layout/pitch_linear.h"
+#include "cutlass/layout/tensor.h"
+#include "cutlass/matrix_shape.h"
+#include "cutlass/predicate_vector.h"
+#include "cutlass/tensor_ref.h"
+#include "cutlass/tensor_view.h"
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-////////////////////////////////////////////////////////////////////////////////
 namespace cutlass {
-namespace gemm {
-namespace warp {
+namespace conv {
+namespace threadblock {
 
-template <
-    /// Operand identity
-    Operand Operand_,
-    /// Data type of A elements
-    typename Element_,
-    bool kTranspose = false>
-class WarpIteratorFromSmem {
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+template <typename Shape_,
+          typename OutputTileShape_,
+          typename StrideShape_,
+          typename DilationShape_,
+          typename ActivationShape_,
+          typename Element_,
+          typename Layout_,
+          typename ThreadMap_,
+          typename AccessType_ = cutlass::AlignedArray<Element_, ThreadMap_::kElementsPerAccess> >
+class DepthwiseFpropActivationDirect2dConvTileAccessIteratorFixedStrideDilation {
  public:
-  /// Shape of tile to load (concept: MatrixShape)
-  using Shape = cutlass::MatrixShape<32, 32>;
+  //
+  // Types
+  //
 
-  /// Operand tag
-  static Operand const kOperand = Operand_;
+  using Shape = Shape_;
+  using OutputTileShape = OutputTileShape_;
+  using Element = Element_;
+  using Layout = Layout_;
+  using TensorCoord = typename Layout::TensorCoord;
+  using ThreadMap = ThreadMap_;
+  using AccessType = AccessType_;
+  using TensorRef = cutlass::TensorRef<Element, Layout>;
+  using Index = typename Layout::Index;
+  using LongIndex = typename Layout::LongIndex;
+  static IteratorAlgorithm const kIteratorAlgorithm = conv::IteratorAlgorithm::kOptimized;
+  static StrideSupport const kStrideSupport = conv::StrideSupport::kStrided;
+  static int const kConvDim = 2;
+  using ConvProblemSize = typename conv::Conv2dProblemSize;
+
+  // Compilation value of stride , dialtion and activation shape
+  using StrideShape = StrideShape_;
+  using DilationShape = DilationShape_;
+  using ActivationShape = ActivationShape_;
+
+
+  static int const kAccessesPerVector = ThreadMap::kElementsPerAccess / AccessType::kElements;
+  static int const kActivationSize = ThreadMap::Iterations::kCount * ThreadMap::kElementsPerAccess * ThreadMap::kThreads *
+           sizeof_bits<Element>::value / 8;
 
-  /// Basic check
-  static_assert(
-      kOperand == Operand::kA || kOperand == Operand::kB,
-      "WarpIteratorFromSmem may only be instantiated for A or B operands to warp-level Mma.");
 
-  /// Element type
-  using Element = Element_;
-  static_assert(sizeof_bits<Element>::value == 16, "Only supported for half");
+  static_assert(!(ThreadMap::kElementsPerAccess % AccessType::kElements),
+                "Vectors implied by the thread map must be divisible by the access type.");
 
-  /// Layout of source tile
-  using Layout = cutlass::layout::RowMajor;
+  //
+  // Simplifying assertions
+  //
+  static_assert(ThreadMap::Iterations::kContiguous == 1, "Require Iterations::kContiguous == 1");
+  
+  static_assert(OutputTileShape::kN == 1, "Require OutputTileShape::kN == 1");
+  static_assert(OutputTileShape::kC == Shape::kColumn, "Require OutputTile shape == channels per threadblock");
 
-  /// Shape of one matrix product operation (concept: MatrixShape)
-  using InstructionShape = cutlass::MatrixShape<16, 8>;
+  //
+  // Parameters structure
+  //
 
-  /// Delta between *MMA operations (in units of *MMA operations, concept:
-  /// MatrixShape)
-  static int const kOpDelta = 1;
+  using Params = Depthwise2dFpropDirectConvActivationIteratorFixedStrideDilationParams<Layout>;
 
-  /// Number of participating threads
-  static int const kThreads = 32;
+ private:
+  Conv2dProblemSize const &problem_size_;
+  Params const &params_;
+  char const *pointer_;
+
+  // Base channels for current threadblock
+  int base_c_;
+  // Base activation index for current threadblock
+  int offset_intial_npq_;
+  // Base activation coord for current threadblock
+  TensorCoord activatioin_base_;
+  // Intial thread positioin
+  int offset_initial_hwc_;
+  // Overall load instruction per thread.
+  int iterator_load_;
+  // thread loading position.
+  int iterator_hwc_;
+  // activation N is inside the Tensor or not
+  bool valid_n_;
 
-  /// TensorRef type for loading element from a tensor
-  using TensorRef = TensorRef<Element, Layout>;
+ public:
 
-  /// Index type
-  using Index = typename TensorRef::Index;
 
-  /// Long Index type
-  using LongIndex = typename TensorRef::LongIndex;
+  CUTLASS_HOST_DEVICE
+  DepthwiseFpropActivationDirect2dConvTileAccessIteratorFixedStrideDilation(
+      Params const &params,
+      Conv2dProblemSize const &problem_size,
+      Element const *ptr,
+      int thread_idx,
+      MatrixCoord const &threadblock_offset =
+          MatrixCoord()
+      )
+      : params_(params),
+        problem_size_(problem_size),
+        pointer_(reinterpret_cast<char const *>(ptr)),
+        offset_intial_npq_(threadblock_offset.row()),
+        offset_initial_hwc_(thread_idx),
+        iterator_load_(0) {
+    
+    base_c_ = threadblock_offset.column();
 
-  /// Coordinate for an element in the tensor
-  using TensorCoord = typename TensorRef::TensorCoord;
+    set_iteration_index(0);
 
-  /// Number of elements accessed per Shared Memory load
-  static int const kElementsPerAccess =
-      (sizeof_bits<Element>::value >= 32 ? 1
-                                         : 32 / sizeof_bits<Element>::value);
+    set_activation_coord(offset_intial_npq_);
 
-  using InstructionCount = MatrixShape<
-      Shape::kRow / InstructionShape::kRow,
-      Shape::kColumn / InstructionShape::kColumn>;
+  }
 
-  static int const kIterations = (kOperand == Operand::kA)
-      ? InstructionCount::kColumn
-      : InstructionCount::kRow;
+  CUTLASS_HOST_DEVICE
+  void set_activation_coord(int offset_npq) {
+    int offset_inital_n, offset_inital_p, offset_inital_q;
+    int residual;
 
- public:
-  //
-  // Derived quantities
-  //
+    params_.pq_divmod(offset_inital_n, residual, offset_npq);
+    params_.q_divmod(offset_inital_p, offset_inital_q, residual);
 
-  /// Fragment object holding a thread's part of a tile
-  using Fragment = Array<
-      Element,
-      (kOperand == Operand::kA)
-          ? (Shape::kRow* InstructionShape::kColumn / kThreads)
-          : (Shape::kColumn* InstructionShape::kRow / kThreads)>;
-
-  /// Memory access type
-  // using AccessType = AlignedArray<Element, kElementsPerAccess>;
-  using AccessType = Array<unsigned, 4>;
-
-  static int constexpr kWarpShapeDivisibleInner =
-      (kOperand == Operand::kA ? InstructionShape::kColumn
-                               : InstructionShape::kRow);
-  static int constexpr kAccessesInner =
-      (kWarpShapeDivisibleInner / kElementsPerAccess) / 4;
-  static int const kTilesPerInstruction = InstructionShape::kRow / 8;
+    int base_n = offset_inital_n;
 
- private:
-  /// Underlying tensor reference
-  TensorRef ref_;
+    int base_h =
+        offset_inital_p * OutputTileShape::kH * StrideShape::kRow - problem_size_.pad_h;
 
-  /// Origin
-  MatrixCoord origin_;
+    int base_w =
+        offset_inital_q * OutputTileShape::kW * StrideShape::kColumn - problem_size_.pad_w;
 
-  /// Iterations in a tile
-  int iterations_;
+    activatioin_base_ = TensorCoord(base_n, base_h, base_w, base_c_);
+
+    valid_n_ = activatioin_base_.n() < problem_size_.N;
+  }
 
- public:
-  /// Constructor from TensorRef
   CUTLASS_HOST_DEVICE
-  WarpIteratorFromSmem(TensorRef const& ref, int lane_id)
-      : WarpIteratorFromSmem(ref, {Shape::kRow, Shape::kColumn}, lane_id) {}
+  static Params getParams(Conv2dProblemSize const &problem_size, Layout const &layout) {
+    return Params(
+        problem_size,
+        layout,
+        {Shape::kRow, Shape::kColumn},
+        {OutputTileShape::kN, OutputTileShape::kH, OutputTileShape::kW, OutputTileShape::kC},
+        kActivationSize);
+  }
+
+  /// Overrides the internal iteration index
   CUTLASS_HOST_DEVICE
-  WarpIteratorFromSmem(TensorRef const& ref, TensorCoord extent, int lane_id)
-      : ref_(ref), iterations_(0) {
-    int ldsm_vec_num = (lane_id >> 3);
-    if (kOperand == Operand::kA) {
-      origin_ = MatrixCoord(lane_id % 8, 0);
-      static_assert(
-          InstructionCount::kRow * kAccessesInner * kTilesPerInstruction == 4,
-          "");
-      CUTLASS_PRAGMA_UNROLL
-      for (int inst_m_idx = 0; inst_m_idx < InstructionCount::kRow;
-           ++inst_m_idx) {
-        CUTLASS_PRAGMA_UNROLL
-        for (int inner_idx = 0; inner_idx < kAccessesInner; ++inner_idx) {
-          CUTLASS_PRAGMA_UNROLL
-          for (int access_m_idx = 0; access_m_idx < kTilesPerInstruction;
-               ++access_m_idx) {
-            int access_idx = access_m_idx +
-                kTilesPerInstruction *
-                    (inner_idx + kAccessesInner * inst_m_idx);
-
-            MatrixCoord offset(
-                access_m_idx * 8 + inst_m_idx * InstructionShape::kRow,
-                inner_idx * 4 * kElementsPerAccess);
-
-            if (access_idx == ldsm_vec_num) {
-              if (kTranspose) {
-                offset = MatrixCoord(offset.column(), offset.row());
-              }
-              origin_ += offset;
-            }
-          }
-        }
-      }
-    } else {
-      origin_ = MatrixCoord(0, lane_id % 8);
-      static_assert(InstructionCount::kColumn * kAccessesInner == 4, "");
-      CUTLASS_PRAGMA_UNROLL
-      for (int inst_n_idx = 0; inst_n_idx < InstructionCount::kColumn;
-           ++inst_n_idx) {
-        CUTLASS_PRAGMA_UNROLL
-        for (int inner_idx = 0; inner_idx < kAccessesInner; ++inner_idx) {
-          int access_idx = inner_idx + kAccessesInner * inst_n_idx;
-
-          MatrixCoord offset(
-              inner_idx * 4 * kElementsPerAccess, inst_n_idx * 8);
-
-          if (access_idx == ldsm_vec_num) {
-            if (kTranspose) {
-              offset = MatrixCoord(offset.column(), offset.row());
-            }
-            origin_ += offset;
-          }
-        }
-      }
-    }
+  void set_iteration_index(Index index) {
+    iterator_hwc_ = offset_initial_hwc_ + index * ThreadMap::kThreads;
+    iterator_load_ = index;
+  }
 
-    ref_.add_coord_offset(origin_);
+  /// Adds a pointer offset in units of Element
+  CUTLASS_HOST_DEVICE
+  void add_pointer_offset(LongIndex pointer_offset) {
+    pointer_ += pointer_offset * sizeof_bits<Element>::value / 8;
   }
 
-  /// Advances an iterator along logical dimensions of matrix in units of whole
-  /// tiles
   CUTLASS_HOST_DEVICE
-  WarpIteratorFromSmem& add_tile_offset(TensorCoord const& tile_offset) {
-    TensorCoord coord_offset(
-        tile_offset.row() * Shape::kRow, tile_offset.column() * Shape::kColumn);
-    if (kTranspose) {
-      coord_offset = TensorCoord{coord_offset.column(), coord_offset.row()};
-    }
-    origin_ += coord_offset;
+  void advance() {
+    // Go to next threadblock
+    offset_intial_npq_ += problem_size_.split_k_slices;
 
-    ref_.add_coord_offset(coord_offset);
+    set_iteration_index(0);
 
-    return *this;
+    set_activation_coord(offset_intial_npq_);
   }
 
-  /// Advances the iterator along the advance dimension
-  CUTLASS_DEVICE
-  void advance() {
-    if (kOperand == Operand::kA) {
-      add_tile_offset({0, 1});
-    } else {
-      add_tile_offset({1, 0});
-    }
+  /// Returns the coordinate in the activations tensor X that is currently pointed to
+  /// by the iterator.
+  CUTLASS_HOST_DEVICE
+  TensorCoord at() const {
+    int c = iterator_hwc_ %  ThreadMap::Detail::ShapeVec::kContiguous ;
+    int next = iterator_hwc_ /  ThreadMap::Detail::ShapeVec::kContiguous ;
+    int h = next / ActivationShape::kW;
+    int w = next % ActivationShape::kW;
+
+    c = c * AccessType::kElements;
 
-    iterations_ = 0;
+    return activatioin_base_ + TensorCoord(0, h, w, c);
   }
 
-  /// increase iterations in a tile
+  /// Returns true if the current coordinate is within the activations tensor X
   CUTLASS_HOST_DEVICE
-  WarpIteratorFromSmem& operator++() {
-    iterations_++;
+  bool valid() const {
+    TensorCoord coord = at();
+    bool valid_c = coord.c() < problem_size_.C;
+    bool valid_h = coord.h() >= 0 && coord.h() < problem_size_.H;
+    bool valid_w = coord.w() >= 0 && coord.w() < problem_size_.W;
+    return valid_n_ ? valid_c & valid_h & valid_w : 0;
+  }
 
-    if (iterations_ >= kIterations)
-      advance();
+  /// Returns a pointer to the vector starting at the current coordinate
+  CUTLASS_HOST_DEVICE
+  AccessType const *get() const {
+    TensorCoord coord = at();
+    LongIndex offset = params_.layout(coord);
+
+    AccessType const *ptr =
+        reinterpret_cast<AccessType const *>(pointer_ + offset * sizeof_bits<Element>::value / 8);
+
+    return ptr;
+  }
+
+  /// Increments to the next memory access
+  CUTLASS_HOST_DEVICE
+  DepthwiseFpropActivationDirect2dConvTileAccessIteratorFixedStrideDilation &operator++() {
+
+    ++iterator_load_;
+    iterator_hwc_ += ThreadMap::kThreads;
+
+    if (iterator_load_ < ThreadMap::Iterations::kCount) {
+       return *this;
+    }
+    
+    iterator_load_ = 0;
+    iterator_hwc_ = offset_initial_hwc_;
 
     return *this;
   }
 
-  /// Loads a fragment from memory at the location pointed to by the iterator.
-  CUTLASS_DEVICE
-  void load(Fragment& frag) const {
-    AccessType* access_ptr = reinterpret_cast<AccessType*>(&frag);
-    using LoadLayout = typename platform::
-        conditional<kTranspose, layout::ColumnMajor, layout::RowMajor>::type;
-
-    MatrixCoord offset;
-    if (kOperand == Operand::kA) {
-      offset = MatrixCoord(0, iterations_ * InstructionShape::kColumn);
-    } else {
-      offset = MatrixCoord(iterations_ * InstructionShape::kRow, 0);
+  /// Determines the activation size loaded by iterator
+  CUTLASS_HOST_DEVICE
+  int get_load_size() {
+    return kActivationSize;
+  }
+
+  /// Determines the iterations needed
+  CUTLASS_HOST_DEVICE
+  int get_iteration_num() {
+    return ThreadMap::Iterations::kCount;
+  }
+
+  /// Determines whether the Depthwise fprop can execute the given problem.
+  CUTLASS_HOST_DEVICE
+  static Status can_implement(Conv2dProblemSize const &problem_size) {
+
+    // check stride and dilation constraint
+    if (problem_size.stride_h != StrideShape::kRow || problem_size.stride_w != StrideShape::kColumn) {
+      return Status::kErrorInvalidProblem;
     }
-    if (kTranspose) {
-      offset = MatrixCoord(offset.column(), offset.row());
+
+    if (problem_size.dilation_h != DilationShape::kRow || problem_size.dilation_w != DilationShape::kColumn) {
+      return Status::kErrorInvalidProblem;
     }
-    cutlass::arch::ldsm<LoadLayout, 4>(
-        access_ptr[0], ref_.data() + ref_.offset(offset));
+
+    // check alignment constraint on iterator's contiguous dimension
+    if (problem_size.C % AccessType::kElements) {
+      return Status::kErrorInvalidProblem;
+    }
+
+    return Status::kSuccess;
   }
 };
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+}  // namespace threadblock
+}  // namespace conv
+}  // namespace cutlass
 
-} // namespace warp
-} // namespace gemm
-} // namespace cutlass
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt_tile_iterator.h`

 * *Files 24% similar despite different names*

```diff
@@ -1,1780 +1,1890 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights
- *reserved. SPDX-License-Identifier: BSD-3-Clause
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
- * 1. Redistributions of source code must retain the above copyright notice,
- *this list of conditions and the following disclaimer.
+ * 1. Redistributions of source code must retain the above copyright notice, this
+ * list of conditions and the following disclaimer.
  *
  * 2. Redistributions in binary form must reproduce the above copyright notice,
  * this list of conditions and the following disclaimer in the documentation
  * and/or other materials provided with the distribution.
  *
  * 3. Neither the name of the copyright holder nor the names of its
  * contributors may be used to endorse or promote products derived from
  * this software without specific prior written permission.
  *
  * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
  * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- *ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
- *LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
- *CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
- *SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
- *INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
- *CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
- *ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
- *POSSIBILITY OF SUCH DAMAGE.
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+ * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Template for a double-buffered threadblock-scoped GEMM kernel.
+    \brief Describes the lane policy used by warp-level matrix multiply operators targeting SIMT
+      instructions
 */
 
 #pragma once
 
-#include "cutlass/aligned_buffer.h"
-#include "cutlass/arch/memory.h"
-#include "cutlass/array.h"
 #include "cutlass/cutlass.h"
-#include "cutlass/epilogue/thread/linear_combination.h"
-#include "cutlass/epilogue/threadblock/default_epilogue_simt.h"
-#include "cutlass/epilogue/threadblock/default_epilogue_tensor_op.h"
-#include "cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h"
-#include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/warp/mma_tensor_op_fragment_iterator.h"
+#include "cutlass/array.h"
+#include "cutlass/tensor_ref.h"
 #include "cutlass/matrix_shape.h"
-#include "cutlass/numeric_conversion.h"
-#include "cutlass/numeric_types.h"
-#include "cutlass/transform/threadblock/vector_iterator.h"
-
-#include "attention_scaling_coefs_updater.h"
-#include "cutlass/epilogue/threadblock/epilogue_smem_accumulator.h"
-#include "cutlass/gemm/threadblock/mma_base.h"
-#include "cutlass/gemm/warp/mma_tensor_op_tile_access_iterator.h"
-#include "epilogue_thread_apply_logsumexp.h"
-#include "gemm_kernel_utils.h"
-#include "iterators/make_residual_last.h"
+
+#include "cutlass/arch/memory_sm75.h"
+
+#include "cutlass/layout/matrix.h"
+
+#include "cutlass/gemm/gemm.h"
+#include "cutlass/gemm/warp/mma_simt_policy.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
-namespace threadblock {
+namespace warp {
 
-/// Shared storage object needed by accumulator
-/// From 13_two_tensor_op_fusion/threadblock/b2b_mma_base_smem_accumulator.h
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Iterates over operands to warp-level matrix multiply operations targeting SIMT instructions
+///
+/// concept: MutableRandomAccessContiguousTileIteratorConcept
+///
 template <
-    typename Shape_,
-    typename Element_,
-    typename Layout_,
-    typename Padding_>
-class AccumulatorSharedStorage {
- public:
-  //
-  // Type definitions
-  //
+  /// Size of the matrix to load (concept: MatrixShape)
+  typename Shape_,
+  /// Operand identity
+  Operand Operand,
+  /// Data type of A elements
+  typename Element_,
+  /// Layout of operand
+  typename Layout_,
+  /// Shape of the warp in units of thread (concept: MmaSimtPolicy)
+  typename Policy_,
+  /// Number of partitions along K dimension - used in sliced-K
+  int PartitionsK = 1,
+  /// Group Size along kPartition - used in sliced-K
+  int PartitionGroupSize = 1
+>
+class MmaSimtTileIterator;
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Specialization for A operands of column-major layouts
+///
+/// Concept: MutableRandomAccessContiguousTileIteratorConcept
+///
+template <
+  /// Size of the matrix to load (concept: MatrixShape)
+  typename Shape_,
+  /// Data type of A elements
+  typename Element_,
+  /// Shape of the warp in units of thread (concept: MmaSimtPolicy)
+  typename Policy_,
+  /// Number of partitions along K dimension - used in sliced-K
+  int PartitionsK,
+  /// Group Size along kPartition - used in sliced-K
+  int PartitionGroupSize
+>
+class MmaSimtTileIterator<Shape_, Operand::kA, Element_, layout::ColumnMajor, Policy_, PartitionsK, PartitionGroupSize> {
+public:
+
+  /// Shape of tile to load (concept: MatrixShape)
   using Shape = Shape_;
+
+  /// Operand tag
+  static Operand const kOperand = Operand::kA;
+
+  /// Element type
   using Element = Element_;
-  using Layout = Layout_;
-  using Padding = Padding_;
 
-  /// Tensor reference to the accumulator
-  using TensorRefAccum = cutlass::TensorRef<Element, Layout>;
+  /// Layout of policy
+  using Layout = layout::ColumnMajor;
 
-  /// Shape of the accumulator matrix in shared memory
-  using ShapeAccum = cutlass::
-      MatrixShape<Shape::kM + Padding::kRow, Shape::kN + Padding::kColumn>;
+  /// Decomposition of elements among threads
+  using Policy = Policy_;
 
- public:
-  //
-  // Data members
-  //
+  /// TensorRef type for loading element from a tensor
+  using TensorRef = TensorRef<Element, Layout>;
+
+  /// Index type
+  using Index = typename TensorRef::Index;
 
-  /// Buffer for accumulator
-  cutlass::AlignedBuffer<Element, ShapeAccum::kCount> accum;
+  /// Long Index type
+  using LongIndex = typename TensorRef::LongIndex;
+
+  /// Coordinate for an element in the tensor
+  using TensorCoord = typename TensorRef::TensorCoord;
 
- public:
   //
-  // Methods
+  // Derived quantities
   //
 
-  /// Returns a layout object for the Accum matrix
-  CUTLASS_DEVICE
-  static Layout LayoutAccum() {
-    return Layout::packed({ShapeAccum::kRow, ShapeAccum::kColumn});
+  static_assert(!(Shape::kRow % Policy::WarpShape::kRow), 
+    "The warp-level GEMM M size must be divisible by the number of threads arranged along the M dimension.");
+
+  static_assert(Shape::kRow > 0, "Shape::kRow must be greater than zero.");
+  static_assert(Shape::kColumn > 0, "Shape::kColumn must be greater than zero.");
+  static_assert(Policy::WarpShape::kRow > 0, "Policy::WarpShape::kRow must be greater than zero.");
+  static_assert(Shape::kRow / Policy::WarpShape::kRow > 0, "Shape::kRow / Policy::WarpShape::kRow must be greater than zero.");
+
+  /// Thread-level shape of a fragment
+  using ThreadShape = MatrixShape<
+    Shape::kRow / Policy::WarpShape::kRow,
+    Shape::kColumn
+  >;
+
+  static_assert(!(ThreadShape::kRow % Policy::LaneMmaShape::kM), 
+    "Thread-level GEMM must be divisible by Policy::LaneMmaShape.");
+
+  /// Number of individual loads
+  using Iterations = MatrixShape<
+    ThreadShape::kRow / Policy::LaneMmaShape::kM,
+    ThreadShape::kColumn
+  >;
+
+  /// Fragment object holding a thread's part of a tile
+  using Fragment = Array<Element, ThreadShape::kCount>;
+
+private:
+
+  /// Internal reference
+  cutlass::TensorRef<Array<Element, Policy::LaneMmaShape::kM>, layout::ColumnMajor> ref_;
+
+public:
+  
+  /// Default ctor constructs null iterator
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator() { }
+
+  /// Constructor from TensorRef
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator(
+    TensorRef ref, 
+    int lane_id
+  ) {
+
+    // compute offset based on thread ID and lane layout
+    typename Policy::LaneLayout lane_layout = Policy::get_lane_layout();
+
+    MatrixCoord lane_offset = lane_layout.inverse(lane_id) * 
+      MatrixCoord(Policy::LaneMmaShape::kM, 0);
+
+    ref.add_coord_offset(lane_offset);
+
+    ref_.reset(
+      reinterpret_cast<Array<Element, Policy::LaneMmaShape::kM> *>(ref.data()),
+      ref.stride(0) / Policy::LaneMmaShape::kM);
+  }
+  
+
+  /// Adds a pointer offset to internal pointer(s) to advance through memory
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator &add_pointer_offset(LongIndex offset) {
+    ref_.add_pointer_offset(offset);
+    return *this;
   }
 
-  /// Returns a TensorRef to the Accumulator
+  /// Advances an iterator along logical dimensions of matrix in units of whole tiles
   CUTLASS_HOST_DEVICE
-  TensorRefAccum accum_ref() {
-    return TensorRefAccum{accum.data(), LayoutAccum()};
+  MmaSimtTileIterator &add_tile_offset(TensorCoord const &coord) {
+
+    ref_.add_coord_offset({
+      coord.row() * Shape::kRow / Policy::LaneMmaShape::kM, 
+      coord.column() * Shape::kColumn});
+
+    return *this;
+  }
+
+  /// Advances the iterator along the advance dimension
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator & operator++() {
+
+    ref_.add_coord_offset({0, Shape::kColumn});
+
+    return *this;
+  }
+
+  /// Advances the iterator along the advance dimension
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator & operator--() {
+
+    ref_.add_coord_offset({0, -Shape::kColumn});
+
+    return *this;
+  }
+
+  /// Loads a fragment from memory at the location pointed to by the iterator. (vector loads)
+  CUTLASS_HOST_DEVICE
+  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) const {
+    Array<Element, Policy::LaneMmaShape::kM> *dst_ptr = 
+      reinterpret_cast<Array<Element, Policy::LaneMmaShape::kM> *>(&frag);
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int k = 0; k < Iterations::kColumn; ++k) {
+      CUTLASS_PRAGMA_UNROLL
+      for (int m = 0; m < Iterations::kRow; ++m) {
+
+        // This logic has been replaced with calls to inline PTX to guarantee vectorization.
+        #if 0
+        dst_ptr[m + k * Iterations::kRow] = 
+          *(ref_.data() + ref_.offset({m * Policy::WarpShape::kRow, k}) + pointer_offset / Policy::LaneMmaShape::kM);
+        #endif
+
+        auto ptr = ref_.data() + ref_.offset({m * Policy::WarpShape::kRow, k}) + pointer_offset / Policy::LaneMmaShape::kM;
+        arch::shared_load(dst_ptr[m + k * Iterations::kRow], ptr);
+      }
+    }
+  }
+  /// Loads a fragment from memory at the location pointed to by the iterator.
+  CUTLASS_HOST_DEVICE
+  void load(Fragment &frag) const {
+    load_with_pointer_offset(frag, 0);
+  }
+    
+  /// Stores a fragment to memory at the location pointed to by the iterator
+  CUTLASS_HOST_DEVICE
+  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) const {
+    
+    Array<Element, Policy::LaneMmaShape::kM> const *src_ptr = 
+      reinterpret_cast<Array<Element, Policy::LaneMmaShape::kM> *>(&frag);
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int k = 0; k < Iterations::kN; ++k) {
+      CUTLASS_PRAGMA_UNROLL
+      for (int m = 0; m < Iterations::kM; ++m) {
+        *(ref_.data() + ref_.offset(m * Policy::WarpShape::kM, k) + pointer_offset / Policy::LaneMmaShape::kM) = 
+          src_ptr[m + k * Iterations::kM];
+      }
+    }
+  }
+
+  /// Stores a fragment to memory at the location pointed to by the iterator
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag) const {
+    store_with_pointer_offset(frag, 0);
+  }
+
+  /// Notify the iterator which k-group it is currently pointing to.
+  ///
+  /// This does not advance the iterator. Rather, it overrides its internal
+  /// tracking with constant-valued k-group index to enable the compiler to
+  /// fold constants and achieve more efficient code.
+  ///
+  /// This is used by some nontrivial permuted layouts.
+  CUTLASS_DEVICE
+  void set_kgroup_index(int k_group) {
+    // no operation here
   }
 };
 
-////////////////////////////////////////////////////////////////////////////////
-// Taken from
-// https://github.com/NVIDIA/cutlass/blob/master/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base_smem_accumulator.h
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Structure to compute the matrix product targeting CUDA cores and SIMT math
-/// instructions.
+/// Specialization for A operands of row-major layouts
+///
+/// Concept: MutableRandomAccessContiguousTileIteratorConcept
+///
 template <
-    /// Size of the Gemm problem - concept: gemm::GemmShape<>
-    typename Shape_,
-    // Maximum value for K
-    int kMaxK,
-    /// Policy describing tuning details (concept: MmaPolicy)
-    typename Policy_,
-    /// Number of stages,
-    int Stages,
-    /// Used for partial specialization
-    typename Enable = bool>
-class MmaBaseFromSharedMemory {
- public:
-  ///< Size of the Gemm problem - concept: gemm::GemmShape<>
+  /// Size of the matrix to load (concept: MatrixShape)
+  typename Shape_,
+  /// Data type of A elements
+  typename Element_,
+  /// Shape of the warp in units of thread (concept: MmaSimtPolicy)
+  typename Policy_,
+  /// Number of partitions along K dimension - used in sliced-K
+  int PartitionsK,
+  /// Group Size along kPartition - used in sliced-K
+  int PartitionGroupSize
+>
+class MmaSimtTileIterator<Shape_, Operand::kA, Element_, layout::RowMajor, Policy_, PartitionsK, PartitionGroupSize> {
+public:
+
+  /// Shape of tile to load (concept: MatrixShape)
   using Shape = Shape_;
 
-  ///< Policy describing tuning details
+  /// Operand tag
+  static Operand const kOperand = Operand::kA;
+
+  /// Element type
+  using Element = Element_;
+
+  /// Layout of policy
+  using Layout = layout::RowMajor;
+
+  /// Decomposition of elements among threads
   using Policy = Policy_;
 
-  //
-  // Dependent types
-  //
+  /// TensorRef type for loading element from a tensor
+  using TensorRef = TensorRef<Element, Layout>;
 
-  /// Warp-level Mma
-  using Operator = typename Policy::Operator;
+  /// Index type
+  using Index = typename TensorRef::Index;
 
-  /// Shape describing the overall GEMM computed from shared memory
-  /// by each warp.
-  using WarpGemm = typename Policy::Operator::Shape;
-
-  /// Shape describing the number of warps filling the CTA
-  using WarpCount = GemmShape<
-      Shape::kM / WarpGemm::kM,
-      Shape::kN / WarpGemm::kN,
-      Shape::kK / WarpGemm::kK>;
-  using WarpCount1 = WarpCount;
-
-  /// Number of warp-level GEMM oeprations
-  static int const kWarpGemmIterations =
-      (WarpGemm::kK / Operator::Policy::MmaShape::kK);
-  static int const kWarpGemmIterations1 = kWarpGemmIterations;
-
-  /// Number of stages
-  static int const kStages = Stages;
-
-  /// If this is true, we fill the entire shmem buffer at start
-  /// and don't need to iterate through it in a circular fashion
-  static bool const kSmemContainsEntireB = kMaxK <= Shape::kK * kStages;
-
-  /// Tensor reference to the A operand
-  using TensorRefA =
-      TensorRef<typename Operator::ElementA, typename Operator::LayoutA>;
-
-  /// Tensor reference to the B operand
-  using TensorRefB =
-      TensorRef<typename Operator::ElementB, typename Operator::LayoutB>;
+  /// Long Index type
+  using LongIndex = typename TensorRef::LongIndex;
+
+  /// Coordinate for an element in the tensor
+  using TensorCoord = typename TensorRef::TensorCoord;
 
   //
-  // Nested structs
+  // Derived quantities
   //
 
-  /// Shared storage object needed by threadblock-scoped GEMM
-  class SharedStorage {
-   public:
-    //
-    // Type definitions
-    //
-
-    /// Shape of the B matrix operand in shared memory
-    using ShapeB = MatrixShape<
-        Shape::kK * kStages + Policy::SmemPaddingB::kRow,
-        Shape::kN + Policy::SmemPaddingB::kColumn>;
-
-   public:
-    //
-    // Data members
-    //
-
-    /// Buffer for B operand
-    AlignedBuffer<typename Operator::ElementB, ShapeB::kCount> operand_B;
-
-   public:
-    //
-    // Methods
-    //
-
-    /// Returns a layout object for the B matrix
-    CUTLASS_HOST_DEVICE
-    static typename Operator::LayoutB LayoutB() {
-      return Operator::LayoutB::packed({ShapeB::kRow, ShapeB::kColumn});
-    }
+  static_assert(!(Shape::kRow % Policy::WarpShape::kRow), 
+    "The warp-level GEMM M size must be divisible by the number of threads arranged along the M dimension.");
+
+  static_assert(Shape::kRow > 0, "Shape::kRow must be greater than zero.");
+  static_assert(Shape::kColumn > 0, "Shape::kColumn must be greater than zero.");
+  static_assert(Policy::WarpShape::kRow > 0, "Policy::WarpShape::kRow must be greater than zero.");
+  static_assert(Shape::kRow / Policy::WarpShape::kRow > 0, "Shape::kRow / Policy::WarpShape::kRow must be greater than zero.");
+
+  /// Thread-level shape of a fragment
+  using ThreadShape = MatrixShape<
+    Shape::kRow / Policy::WarpShape::kRow,
+    Shape::kColumn
+  >;
+
+  static_assert(!(ThreadShape::kRow % Policy::LaneMmaShape::kM), 
+    "Thread-level GEMM must be divisible by Policy::LaneMmaShape.");
+
+  /// Number of individual loads (scalar loads)
+  using Iterations = MatrixShape<
+    ThreadShape::kRow / Policy::LaneMmaShape::kM,
+    ThreadShape::kColumn
+  >;
+
+  /// Fragment object holding a thread's part of a tile
+  using Fragment = Array<Element, ThreadShape::kCount>;
+
+private:
+
+  /// Internal reference
+  cutlass::TensorRef<Element, layout::RowMajor> ref_;
+
+  /// Extent of tensor
+  MatrixCoord extent_;
+
+  /// Origin
+  MatrixCoord origin_;
+
+  /// Used to conditionally enable extents checking
+  bool divisible_;
+
+public:
+  
+  /// Default ctor constructs null iterator
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator() : divisible_(true) { }
+
+  /// Constructor from TensorRef
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator(
+    TensorRef ref, 
+    int lane_id
+  ) : extent_(Shape::kRow, Shape::kColumn), divisible_ (true) {
+
+    // compute offset based on thread ID and lane layout
+    typename Policy::LaneLayout lane_layout = Policy::get_lane_layout();
+
+    MatrixCoord lane_offset = lane_layout.inverse(lane_id) * 
+      MatrixCoord(Policy::LaneMmaShape::kM, 0);
+
+    origin_ = lane_offset;
+
+    ref.add_coord_offset(lane_offset);
+
+    ref_.reset(ref.data(), ref.stride(0));
+
+  }
+  
+  /// Constructor from TensorRef
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator(
+    TensorRef ref,
+    TensorCoord extent, 
+    int lane_id
+  ) : extent_(extent), divisible_ (false) {
+
+    // compute offset based on thread ID and lane layout
+    typename Policy::LaneLayout lane_layout = Policy::get_lane_layout();
 
-    /// Returns a TensorRef to the B operand
-    CUTLASS_HOST_DEVICE
-    TensorRefB operand_B_ref() {
-      return TensorRefB{operand_B.data(), LayoutB()};
+    MatrixCoord lane_offset = lane_layout.inverse(lane_id) * 
+      MatrixCoord(Policy::LaneMmaShape::kM, 0);
+
+    origin_ = lane_offset;
+    
+    ref.add_coord_offset(lane_offset);
+
+    ref_.reset(ref.data(), ref.stride(0));
+
+  }
+
+  /// Adds a pointer offset to internal pointer(s) to advance through memory
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator &add_pointer_offset(LongIndex offset) {
+    ref_.add_pointer_offset(offset);
+    return *this;
+  }
+
+  /// Advances an iterator along logical dimensions of matrix in units of whole tiles
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator &add_tile_offset(TensorCoord const &coord) {
+
+    TensorCoord coord_offset(
+      coord.row() * Shape::kRow, 
+      coord.column() * Shape::kColumn);
+    
+    origin_ += coord_offset;
+
+    ref_.add_coord_offset(coord_offset);
+
+    return *this;
+  }
+
+  /// Advances the iterator along the advance dimension
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator & operator++() {
+
+    ref_.add_coord_offset({0, Shape::kColumn});
+
+    return *this;
+  }
+
+  /// Advances the iterator along the advance dimension
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator & operator--() {
+
+    ref_.add_coord_offset({0, -Shape::kColumn});
+
+    return *this;
+  }
+
+  /// Loads a fragment from memory at the location pointed to by the iterator. (scalar loads)
+  CUTLASS_HOST_DEVICE
+  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) const {
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int k = 0; k < Iterations::kColumn; ++k) {
+      CUTLASS_PRAGMA_UNROLL
+      for (int m = 0; m < Iterations::kRow; ++m) {
+        CUTLASS_PRAGMA_UNROLL
+        for (int i = 0; i < Policy::LaneMmaShape::kM; i++) {
+          
+          MatrixCoord offset(m * Policy::WarpShape::kRow * Policy::LaneMmaShape::kM + i, k);
+            
+          MatrixCoord access_coord = origin_ + offset;
+
+          int frag_idx = m * Policy::LaneMmaShape::kM + i + k * Iterations::kRow;
+
+          if (divisible_ || 
+              (access_coord.row() < extent_.row() && access_coord.column() < extent_.column())) {
+          
+            frag[frag_idx] = *(ref_.data() + ref_.offset(offset) + pointer_offset);
+          }
+          else {
+            frag[frag_idx] = Element();
+          }
+        }
+      }
     }
-  };
+  }
+  /// Loads a fragment from memory at the location pointed to by the iterator. 
+  CUTLASS_HOST_DEVICE
+  void load(Fragment &frag) const {
+    load_with_pointer_offset(frag, 0);
+  }
+    
+  /// Stores a fragment to memory at the location pointed to by the iterator
+  CUTLASS_HOST_DEVICE
+  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) const {
 
- protected:
-  //
-  // Data members
-  //
+    CUTLASS_PRAGMA_UNROLL
+    for (int k = 0; k < Iterations::kColumn; ++k) {
+      CUTLASS_PRAGMA_UNROLL
+      for (int m = 0; m < Iterations::kRow; ++m) {
+        CUTLASS_PRAGMA_UNROLL
+        for (int i = 0; i < Policy::LaneMmaShape::kM; i++) {
 
-  // /// Iterator to load a warp-scoped tile of A operand from shared memory
-  // typename Operator::IteratorA warp_tile_iterator_A_;
+          *(ref_.data() + ref_.offset(m * Policy::WarpShape::kM * Policy::LaneMmaShape::kM + i, k) + pointer_offset) = 
+            frag[m * Policy::LaneMmaShape::kM + i + k * Iterations::kM];
+        }
+      }
+    }
+  }
 
-  /// Iterator to load a warp-scoped tile of B operand from shared memory
-  typename Operator::IteratorB warp_tile_iterator_B_;
+  /// Stores a fragment to memory at the location pointed to by the iterator
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag) const {
+    store_with_pointer_offset(frag, 0);
+  }
 
- public:
-  /// Construct from tensor references
+  /// Notify the iterator which k-group it is currently pointing to.
+  ///
+  /// This does not advance the iterator. Rather, it overrides its internal
+  /// tracking with constant-valued k-group index to enable the compiler to
+  /// fold constants and achieve more efficient code.
+  ///
+  /// This is used by some nontrivial permuted layouts.
   CUTLASS_DEVICE
-  MmaBaseFromSharedMemory(
-      ///< Shared storage needed for internal use by threadblock-scoped GEMM
-      SharedStorage& shared_storage,
-      ///< ID within the threadblock
-      int thread_idx,
-      ///< ID of warp
-      int warp_idx,
-      ///< ID of each thread within a warp
-      int lane_idx)
-      : warp_tile_iterator_B_(shared_storage.operand_B_ref(), lane_idx) {}
+  void set_kgroup_index(int k_group) {
+    // no operation here
+  }
 };
 
-////////////////////////////////////////////////////////////////////////////////
-// Taken from
-// https://github.com/NVIDIA/cutlass/blob/master/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined_smem_accumulator.h
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Structure to compute the matrix product targeting CUDA cores and SIMT math
-/// instructions.
+/// Specialization for B operands of row-major layouts
+///
+/// Concept: MutableRandomAccessContiguousTileIteratorConcept
+///
 template <
-    /// Size of the Gemm problem - concept: gemm::GemmShape<>
-    typename Shape_,
-    // BEGIN smem
-    /// Iterates over the intermediate accumulator tile in shared memory
-    typename WarpIteratorA,
-    // Accumulator type
-    typename AccumulatorSharedStorage,
-    // END smem
-    /// Iterates over tiles of B operand in global memory
-    //  (concept: ReadableTileIterator | ForwardTileIterator |
-    //  MaskedTileIterator)
-    typename IteratorB_,
-    /// Iterates over tiles of B operand in shared memory
-    /// (concept: WriteableTileIterator | RandomAccessTileIterator)
-    typename SmemIteratorB_,
-    /// Data type of accumulator matrix
-    typename ElementC_,
-    /// Data type of accumulator matrix
-    typename LayoutC_,
-    /// Policy describing tuning details (concept: MmaPolicy)
-    typename Policy_,
-    /// Transformation applied to B operand
-    typename TransformB_ = NumericArrayConverter<
-        typename SmemIteratorB_::Element,
-        typename IteratorB_::Element,
-        IteratorB_::Fragment::kElements>,
-    /// Used for partial specialization
-    typename Enable = bool>
-class MmaPipelinedFromSharedMemory : public MmaBaseFromSharedMemory<
-                                         Shape_,
-                                         AccumulatorSharedStorage::Shape::kN,
-                                         Policy_,
-                                         2> {
- public:
-  ///< Base class
-  using Base = MmaBaseFromSharedMemory<
-      Shape_,
-      AccumulatorSharedStorage::Shape::kN,
-      Policy_,
-      2>;
-
-  using Shape =
-      Shape_; ///< Size of the Gemm problem - concept: gemm::GemmShape<>
-  using IteratorB =
-      IteratorB_; ///< Iterates over tiles of B operand in global memory
-  using ElementC = ElementC_; ///< Data type of accumulator matrix
-  using LayoutC = LayoutC_; ///< Layout of accumulator matrix
-  using Policy = Policy_; ///< Policy describing tuning details
+  /// Size of the matrix to load (concept: MatrixShape)
+  typename Shape_,
+  /// Data type of A elements
+  typename Element_,
+  /// Shape of the warp in units of thread (concept: MmaSimtPolicy)
+  typename Policy_,
+  /// Number of partitions along K dimension
+  int PartitionsK,
+  /// Group Size along kPartition - used in sliced-K
+  int PartitionGroupSize
+>
+class MmaSimtTileIterator<Shape_, Operand::kB, Element_, layout::RowMajor, Policy_, PartitionsK, PartitionGroupSize> {
+public:
 
-  using SmemIteratorB = SmemIteratorB_;
+  /// Shape of tile to load (concept: MatrixShape)
+  using Shape = Shape_;
 
-  using TransformB = TransformB_;
+  /// Operand tag
+  static Operand const kOperand = Operand::kB;
+
+  /// Element type
+  using Element = Element_;
+
+  /// Layout of policy
+  using Layout = layout::RowMajor;
+
+  /// Decomposition of elements among threads
+  using Policy = Policy_;
+
+  /// TensorRef type for loading element from a tensor
+  using TensorRef = TensorRef<Element, Layout>;
+
+  /// Index type
+  using Index = typename TensorRef::Index;
+
+  /// Long Index type
+  using LongIndex = typename TensorRef::LongIndex;
+
+  /// Coordinate for an element in the tensor
+  using TensorCoord = typename TensorRef::TensorCoord;
 
   //
-  // Dependent types
+  // Derived quantities
   //
 
-  /// Fragment of operand B loaded from global memory
-  using FragmentB = typename IteratorB::Fragment;
+  static_assert(!(Shape::kColumn % Policy::WarpShape::kColumn), 
+    "The warp-level GEMM N size must be divisible by the number of threads arranged along the N dimension.");
+  
+  static_assert(Shape::kRow > 0, "Shape::kRow must be greater than zero.");
+  static_assert(Shape::kColumn > 0, "Shape::kColumn must be greater than zero.");
+  static_assert(Policy::WarpShape::kColumn > 0, "Policy::WarpShape::kColumn must be greater than zero.");
+  static_assert(Shape::kColumn / Policy::WarpShape::kColumn > 0, "Shape::kColumn / Policy::WarpShape::kColumn must be greater than zero.");
+
+  /// Thread-level shape of a fragment
+  using ThreadShape = MatrixShape<
+    Shape::kRow,
+    Shape::kColumn / Policy::WarpShape::kColumn
+  >;
+
+  static_assert(!(ThreadShape::kColumn % Policy::LaneMmaShape::kN), 
+    "Thread-level GEMM must be divisible by Policy::LaneMmaShape.");
+
+  /// Number of individual loads
+  using Iterations = MatrixShape<
+    ThreadShape::kRow,
+    ThreadShape::kColumn / Policy::LaneMmaShape::kN
+  >;
+
+  /// Fragment object holding a thread's part of a tile
+  using Fragment = Array<Element, ThreadShape::kCount>;
+
+protected:
+
+  /// Internal reference
+  cutlass::TensorRef<Array<Element, Policy::LaneMmaShape::kN>, layout::RowMajor> ref_;
+
+public:
+  
+  /// Default ctor constructs null iterator
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator() { }
+
+  /// Constructor from TensorRef
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator(
+    TensorRef ref, 
+    int lane_id
+  ) {
 
-  /// Fragment of accumulator tile
-  using FragmentC = typename Policy::Operator::FragmentC;
+    // compute offset based on thread ID and lane layout
+    typename Policy::LaneLayout lane_layout = Policy::get_lane_layout();
 
-  /// Warp-level Mma
-  using Operator = typename Policy::Operator;
+    MatrixCoord lane_offset = lane_layout.inverse(lane_id) * 
+      MatrixCoord(0, Policy::LaneMmaShape::kN);
 
-  /// Obtain the arch tag from the warp-level operator
-  using ArchTag = typename Policy::Operator::ArchTag;
+    ref.add_coord_offset(lane_offset);
 
-  /// Complex transform on B operand
-  static ComplexTransform const kTransformB = Operator::kTransformB;
+    ref_.reset(
+      reinterpret_cast<Array<Element, Policy::LaneMmaShape::kN> *>(ref.data()),
+      ref.stride(0) / Policy::LaneMmaShape::kN);
+  }
+  
+  /// Adds a pointer offset to internal pointer(s) to advance through memory
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator &add_pointer_offset(LongIndex offset) {
+    ref_.add_pointer_offset(offset);
+    return *this;
+  }
 
-  // staticaly assert kStages for MmaPipelined is two (Double-buffered pipeline)
-  static_assert(
-      (Base::kStages == 2),
-      "MmaPipelined requires kStages set to value 2");
+  /// Advances an iterator along logical dimensions of matrix in units of whole tiles
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator &add_tile_offset(TensorCoord const &coord) {
 
- private:
-  using WarpFragmentA = typename Operator::FragmentA;
-  using WarpFragmentB = typename Operator::FragmentB;
-
- protected:
-  // /// Iterator to write threadblock-scoped tile of A operand to shared memory
-  // SmemIteratorA smem_iterator_A_;
-
-  /// Iterator to write threadblock-scoped tile of B operand to shared memory
-  SmemIteratorB smem_iterator_B_;
-
-  /// Iterator to load a warp-scoped tile of A operand from intermediate
-  /// accumulator tile
-  WarpIteratorA warp_tile_iterator_A_;
+    ref_.add_coord_offset({
+      coord.row() * Shape::kRow, 
+      coord.column() * Shape::kColumn / Policy::LaneMmaShape::kN});
 
- public:
-  /// Construct from tensor references
-  CUTLASS_DEVICE
-  MmaPipelinedFromSharedMemory(
-      typename Base::SharedStorage&
-          shared_storage, ///< Shared storage needed for internal use by
-                          ///< threadblock-scoped GEMM
-      AccumulatorSharedStorage& accumulator_shared_storage,
-      int thread_idx, ///< ID within the threadblock
-      int warp_idx, ///< ID of warp
-      int lane_idx, ///< ID of each thread within a warp
-      int problem_size_0_n)
-      : Base(shared_storage, thread_idx, warp_idx, lane_idx),
-        warp_tile_iterator_A_(accumulator_shared_storage.accum_ref(), lane_idx),
-        smem_iterator_B_(shared_storage.operand_B_ref(), thread_idx) {
-    // Compute warp location within threadblock tile by mapping the warp_id to
-    // three coordinates:
-    //   _m: the warp's position within the threadblock along the M dimension
-    //   _n: the warp's position within the threadblock along the N dimension
-    //   _k: the warp's position within the threadblock along the K dimension
-
-    int warp_idx_mn = warp_idx % (Base::WarpCount::kM * Base::WarpCount::kN);
-    int warp_idx_k = warp_idx / (Base::WarpCount::kM * Base::WarpCount::kN);
-
-    int warp_idx_m = warp_idx_mn % Base::WarpCount::kM;
-    int warp_idx_n = warp_idx_mn / Base::WarpCount::kM;
-
-    // Add per-warp offsets in units of warp-level tiles
-    this->warp_tile_iterator_A_.add_tile_offset(
-        {warp_idx_m, Base::kWarpGemmIterations * warp_idx_k});
-    this->warp_tile_iterator_B_.add_tile_offset(
-        {Base::kWarpGemmIterations * warp_idx_k, warp_idx_n});
-  }
-
-  // For API compatibility with MmaMultistageFromSharedMemory
-  // but not supported as it worsens perf: older gpus < sm80 don't
-  // support async tranfers and have to waste registers
-  CUTLASS_DEVICE
-  bool set_prologue_done(bool value) {}
-  CUTLASS_DEVICE
-  static void prologue(
-      typename Base::SharedStorage& shared_storage,
-      IteratorB iterator_B1,
-      int thread_idx,
-      int problem_size_0_n) {}
+    return *this;
+  }
+
+  /// Advances the iterator along the advance dimension
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator & operator++() {
+
+    ref_.add_coord_offset({Shape::kRow, 0});
+
+    return *this;
+  }
+
+  /// Advances the iterator along the advance dimension
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator & operator--() {
+
+    ref_.add_coord_offset({-Shape::kRow, 0});
+
+    return *this;
+  }
+
+  /// Loads a fragment from memory at the location pointed to by the iterator. (vector loads)
+  CUTLASS_HOST_DEVICE
+  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) const {
+
+    Array<Element, Policy::LaneMmaShape::kN> *dst_ptr = 
+      reinterpret_cast<Array<Element, Policy::LaneMmaShape::kN> *>(&frag);
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int k = 0; k < Iterations::kRow; ++k) {
+      CUTLASS_PRAGMA_UNROLL
+      for (int n = 0; n < Iterations::kColumn; ++n) {
+
+        #if 0
+        dst_ptr[n + k * Iterations::kColumn] = 
+          *(ref_.data() + ref_.offset({k, n * Policy::WarpShape::kColumn}) + pointer_offset / Policy::LaneMmaShape::kN);
+        #endif
+
+        void const *ptr = ref_.data() + ref_.offset({k, n * Policy::WarpShape::kColumn}) + pointer_offset / Policy::LaneMmaShape::kN;
+        arch::shared_load(dst_ptr[n + k * Iterations::kColumn], ptr);
+      }
+    }
+  }
+
+  /// Loads a fragment from memory at the location pointed to by the iterator.
+  CUTLASS_HOST_DEVICE
+  void load(Fragment &frag) const {
+    load_with_pointer_offset(frag, 0);
+  }
+  
+  /// Stores a fragment to memory at the location pointed to by the iterator
+  CUTLASS_HOST_DEVICE
+  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) const {
+
+    Array<Element, Policy::LaneMmaShape::kN> const *src_ptr = 
+      reinterpret_cast<Array<Element, Policy::LaneMmaShape::kN> *>(&frag);
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int k = 0; k < Iterations::kM; ++k) {
+      CUTLASS_PRAGMA_UNROLL
+      for (int n = 0; n < Iterations::kN; ++n) {
+        *(ref_.data() + ref_.offset({k, n * Policy::WarpShape::kN}) + pointer_offset / Policy::LaneMmaShape::kN) = 
+          src_ptr[n + k * Iterations::kN];
+      }
+    }
+  }
+
+  /// Stores a fragment to memory at the location pointed to by the iterator
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag, Index pointer_offset) const {
+    store_with_pointer_offset(frag, 0);
+  }
 
-  /// Perform a threadblock-scoped matrix multiply-accumulate
+  /// Notify the iterator which k-group it is currently pointing to.
+  ///
+  /// This does not advance the iterator. Rather, it overrides its internal
+  /// tracking with constant-valued k-group index to enable the compiler to
+  /// fold constants and achieve more efficient code.
+  ///
+  /// This is used by some nontrivial permuted layouts.
   CUTLASS_DEVICE
-  void operator()(
-      int gemm_k_iterations, ///< number of iterations of the mainloop
-      FragmentC& accum, ///< destination accumulator tile
-      // IteratorA iterator_A,                             ///< iterator over A
-      // operand in global memory
-      IteratorB iterator_B, ///< iterator over B operand in global memory
-      FragmentC const& src_accum, ///< source accumulator tile
-      // TransformA transform_A = TransformA(),            ///< transformation
-      // applied to A fragment
-      TransformB transform_B =
-          TransformB()) { ///< transformation applied to B fragment
+  void set_kgroup_index(int k_group) {
+    // no operation here
+  }
+};
 
-    //
-    // Prologue
-    //
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-    // Perform accumulation in the 'd' output operand
-    accum = src_accum;
+/// Specialization for B operands of column-major layouts
+///
+/// Concept: MutableRandomAccessContiguousTileIteratorConcept
+///
+template <
+  /// Size of the matrix to load (concept: MatrixShape)
+  typename Shape_,
+  /// Data type of A elements
+  typename Element_,
+  /// Shape of the warp in units of thread (concept: MmaSimtPolicy)
+  typename Policy_,
+  /// Number of partitions along K dimension
+  int PartitionsK,
+  /// Group Size along kPartition - used in sliced-K
+  int PartitionGroupSize
+>
+class MmaSimtTileIterator<Shape_, Operand::kB, Element_, layout::ColumnMajor, Policy_, PartitionsK, PartitionGroupSize> {
+public:
 
-    FragmentB tb_frag_B;
+  /// Shape of tile to load (concept: MatrixShape)
+  using Shape = Shape_;
 
-    tb_frag_B.clear();
+  /// Operand tag
+  static Operand const kOperand = Operand::kB;
 
-    // The last kblock is loaded in the prolog
-    iterator_B.set_residual_tile(gemm_k_iterations == 1);
-    iterator_B.load(tb_frag_B);
+  /// Element type
+  using Element = Element_;
 
-    ++iterator_B;
+  /// Layout of policy
+  using Layout = layout::ColumnMajor;
 
-    this->smem_iterator_B_.store(transform_B(tb_frag_B));
+  /// Decomposition of elements among threads
+  using Policy = Policy_;
 
-    ++this->smem_iterator_B_;
+  /// TensorRef type for loading element from a tensor
+  using TensorRef = TensorRef<Element, Layout>;
 
-    __syncthreads();
+  /// Index type
+  using Index = typename TensorRef::Index;
 
-    // Pair of fragments used to overlap shared memory loads and math
-    // instructions
-    WarpFragmentA warp_frag_A[2];
-    WarpFragmentB warp_frag_B[2];
-    warp_frag_A[0].clear();
-    warp_frag_B[0].clear();
+  /// Long Index type
+  using LongIndex = typename TensorRef::LongIndex;
 
-    this->warp_tile_iterator_B_.set_kgroup_index(0);
+  /// Coordinate for an element in the tensor
+  using TensorCoord = typename TensorRef::TensorCoord;
 
-    this->warp_tile_iterator_A_.load(warp_frag_A[0]);
-    this->warp_tile_iterator_B_.load(warp_frag_B[0]);
+  //
+  // Derived quantities
+  //
 
-    ++this->warp_tile_iterator_A_;
-    ++this->warp_tile_iterator_B_;
+  static_assert(!(Shape::kColumn % Policy::WarpShape::kColumn), 
+    "The warp-level GEMM N size must be divisible by the number of threads arranged along the N dimension.");
+  
+  static_assert(Shape::kRow > 0, "Shape::kRow must be greater than zero.");
+  static_assert(Shape::kColumn > 0, "Shape::kColumn must be greater than zero.");
+  static_assert(Policy::WarpShape::kColumn > 0, "Policy::WarpShape::kColumn must be greater than zero.");
+  static_assert(Shape::kColumn / Policy::WarpShape::kColumn > 0, "Shape::kColumn / Policy::WarpShape::kColumn must be greater than zero.");
+
+  /// Thread-level shape of a fragment
+  using ThreadShape = MatrixShape<
+    Shape::kRow,
+    Shape::kColumn / Policy::WarpShape::kColumn
+  >;
+
+  static_assert(!(ThreadShape::kColumn % Policy::LaneMmaShape::kN), 
+    "Thread-level GEMM must be divisible by Policy::LaneMmaShape.");
+
+  /// Number of individual loads
+  using Iterations = MatrixShape<
+    ThreadShape::kRow,
+    ThreadShape::kColumn / Policy::LaneMmaShape::kN
+  >;
+
+  /// Fragment object holding a thread's part of a tile
+  using Fragment = Array<Element, ThreadShape::kCount>;
+
+private:
+
+  /// Internal reference
+  cutlass::TensorRef<Element, layout::ColumnMajor> ref_;
+
+  /// Extent of tensor
+  MatrixCoord extent_;
+
+  /// Origin
+  MatrixCoord origin_;
+
+  /// Used to conditionally enable extents checking
+  bool divisible_;
+
+public:
+  
+  /// Default ctor constructs null iterator
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator(): divisible_(true) { }
 
-    Operator warp_mma;
+  /// Constructor from TensorRef
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator(
+    TensorRef ref, 
+    int lane_id
+  ): extent_(Shape::kRow, Shape::kColumn), divisible_(true) {
 
-    int smem_write_stage_idx = 1;
+    // compute offset based on thread ID and lane layout
+    typename Policy::LaneLayout lane_layout = Policy::get_lane_layout();
 
-    // Avoid reading out of bounds
-    iterator_B.set_residual_tile(gemm_k_iterations == 2);
-    iterator_B.clear_mask(gemm_k_iterations <= 1);
+    MatrixCoord lane_offset = lane_layout.inverse(lane_id) * 
+      MatrixCoord(0, Policy::LaneMmaShape::kN);
 
-    // Issue loads during the first warp-level matrix multiply-add *AFTER*
-    // issuing shared memory loads (which have the tighest latency requirement).
+    origin_ = lane_offset;
 
-    //
-    // Mainloop
-    //
+    ref.add_coord_offset(lane_offset);
 
-    // Note: The main loop does not support Base::kWarpGemmIterations == 2.
-    CUTLASS_GEMM_LOOP
-    for (; gemm_k_iterations > 0; --gemm_k_iterations) {
-      //
-      // Loop over GEMM K dimension
-      //
+    ref_.reset(ref.data(), ref.stride(0));
+  }
 
-      CUTLASS_PRAGMA_UNROLL
-      for (int warp_mma_k = 0; warp_mma_k < Base::kWarpGemmIterations;
-           ++warp_mma_k) {
-        // Load warp-level tiles from shared memory, wrapping to k offset if
-        // this is the last group as the case may be.
-        bool hasNext = true;
-
-        if (warp_mma_k == Base::kWarpGemmIterations - 1) {
-          // Write fragments to shared memory
-          this->smem_iterator_B_.store(transform_B(tb_frag_B));
-
-          __syncthreads();
-
-          ++this->smem_iterator_B_;
-
-          // Add negative offsets to return iterators to the 'start' of the
-          // circular buffer in shared memory SMEM: Don't reset iterator A, as
-          // we are continuing our iteration at this point
-          if (smem_write_stage_idx == 1) {
-            this->smem_iterator_B_.add_tile_offset({-Base::kStages, 0});
-          } else {
-            this->warp_tile_iterator_B_.add_tile_offset(
-                {-Base::kStages * Policy::kPartitionsK *
-                     Base::kWarpGemmIterations,
-                 0});
-          }
+  /// Constructor from TensorRef
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator(
+    TensorRef ref,
+    TensorCoord extent, 
+    int lane_id
+  ): extent_(extent), divisible_(false) {
 
-          smem_write_stage_idx ^= 1;
-          hasNext = gemm_k_iterations > 1;
-        }
+    // compute offset based on thread ID and lane layout
+    typename Policy::LaneLayout lane_layout = Policy::get_lane_layout();
+
+    MatrixCoord lane_offset = lane_layout.inverse(lane_id) * 
+      MatrixCoord(0, Policy::LaneMmaShape::kN);
+
+    origin_ = lane_offset;
+
+    ref.add_coord_offset(lane_offset);
+
+    ref_.reset(ref.data(), ref.stride(0));
+  }
+
+  /// Adds a pointer offset to internal pointer(s) to advance through memory
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator &add_pointer_offset(LongIndex offset) {
+    ref_.add_pointer_offset(offset);
+    return *this;
+  }
 
-        // Only read the next if we need to
-        if (hasNext) {
-          this->warp_tile_iterator_B_.set_kgroup_index(
-              (warp_mma_k + 1) % Base::kWarpGemmIterations);
+  /// Advances an iterator along logical dimensions of matrix in units of whole tiles
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator &add_tile_offset(TensorCoord const &coord) {
+
+    TensorCoord coord_offset(
+      coord.row() * Shape::kRow, 
+      coord.column() * Shape::kColumn);
 
-          this->warp_tile_iterator_A_.load(warp_frag_A[(warp_mma_k + 1) % 2]);
-          this->warp_tile_iterator_B_.load(warp_frag_B[(warp_mma_k + 1) % 2]);
+    origin_ += coord_offset;
 
-          ++this->warp_tile_iterator_A_;
-          ++this->warp_tile_iterator_B_;
+    ref_.add_coord_offset(coord_offset);
 
-          if (warp_mma_k == 0) {
-            iterator_B.load(tb_frag_B);
+    return *this;
+  }
+
+  /// Advances the iterator along the advance dimension
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator & operator++() {
 
-            ++iterator_B;
+    ref_.add_coord_offset({Shape::kRow, 0});
+
+    return *this;
+  }
+
+  /// Advances the iterator along the advance dimension
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator & operator--() {
 
-            // Avoid reading out of bounds if this was the last loop iteration
-            iterator_B.set_residual_tile(gemm_k_iterations == 3);
-            iterator_B.clear_mask(gemm_k_iterations <= 2);
+    ref_.add_coord_offset({-Shape::kRow, 0});
+
+    return *this;
+  }
+
+  /// Loads a fragment from memory at the location pointed to by the iterator. (scalar loads)
+  CUTLASS_HOST_DEVICE
+  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) const {
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int k = 0; k < Iterations::kRow; ++k) {
+      CUTLASS_PRAGMA_UNROLL
+      for (int n = 0; n < Iterations::kColumn; ++n) {
+        CUTLASS_PRAGMA_UNROLL
+        for (int i = 0; i < Policy::LaneMmaShape::kN; ++i) {
+
+          MatrixCoord offset(k, n * Policy::WarpShape::kColumn * Policy::LaneMmaShape::kN + i);
+            
+          MatrixCoord access_coord = origin_ + offset;
+
+          int frag_idx = n * Policy::LaneMmaShape::kN + i + k * Iterations::kColumn;
+
+          if (divisible_ || 
+              (access_coord.row() < extent_.row() && access_coord.column() < extent_.column())) {
+
+            frag[frag_idx] = *(ref_.data() + ref_.offset(offset) + pointer_offset);
+          }
+          else {
+            frag[frag_idx] = Element();
           }
         }
+      }
+    }
+  }
+
+  /// Loads a fragment from memory at the location pointed to by the iterator.
+  CUTLASS_HOST_DEVICE
+  void load(Fragment &frag) const {
+    load_with_pointer_offset(frag, 0);
+  }
+  
+  /// Stores a fragment to memory at the location pointed to by the iterator
+  CUTLASS_HOST_DEVICE
+  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) const {
+
+    Array<Element, Policy::LaneMmaShape::kN> const *src_ptr = 
+      reinterpret_cast<Array<Element, Policy::LaneMmaShape::kN> *>(&frag);
 
-        warp_mma(
-            accum,
-            warp_frag_A[warp_mma_k % 2],
-            warp_frag_B[warp_mma_k % 2],
-            accum);
+    CUTLASS_PRAGMA_UNROLL
+    for (int k = 0; k < Iterations::kM; ++k) {
+      CUTLASS_PRAGMA_UNROLL
+      for (int n = 0; n < Iterations::kN; ++n) {
+        *(ref_.data() + ref_.offset({k, n * Policy::WarpShape::kN}) + pointer_offset / Policy::LaneMmaShape::kN) = 
+          src_ptr[n + k * Iterations::kN];
       }
     }
   }
+
+  /// Stores a fragment to memory at the location pointed to by the iterator
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag, Index pointer_offset) const {
+    store_with_pointer_offset(frag, 0);
+  }
+
+  /// Notify the iterator which k-group it is currently pointing to.
+  ///
+  /// This does not advance the iterator. Rather, it overrides its internal
+  /// tracking with constant-valued k-group index to enable the compiler to
+  /// fold constants and achieve more efficient code.
+  ///
+  /// This is used by some nontrivial permuted layouts.
+  CUTLASS_DEVICE
+  void set_kgroup_index(int k_group) {
+    // no operation here
+  }
 };
 
-////////////////////////////////////////////////////////////////////////////////
-// Taken from
-// https://github.com/NVIDIA/cutlass/blob/master/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage_smem_accumulator.h
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Structure to compute the matrix product targeting CUDA cores and SIMT math
-/// instructions.
+/// Specialization for C operands of column-major layouts
+///
+/// Concept: MutableRandomAccessContiguousTileIteratorConcept
+///
 template <
-    /// Size of the Gemm problem - concept: gemm::GemmShape<>
-    typename Shape1_,
-    /// Iterates over the intermediate accumulator tile in shared memory
-    typename WarpIteratorA1_,
-    // Accumulator type
-    typename AccumulatorSharedStorage,
-    /// Iterates over tiles of B operand in global memory
-    //  (concept: ReadableTileIterator | ForwardTileIterator |
-    //  MaskedTileIterator)
-    typename IteratorB1_,
-    /// Iterates over tiles of B operand in shared memory
-    /// (concept: WriteableTileIterator | RandomAccessTileIterator)
-    typename SmemIteratorB1_,
-    /// Cache operation for operand B
-    cutlass::arch::CacheOperation::Kind CacheOpB1,
-    /// Data type of accumulator matrix
-    typename ElementC_,
-    /// Data type of accumulator matrix
-    typename LayoutC_,
-    /// Policy describing tuning details (concept: MmaPolicy)
-    typename Policy1_,
-    /// Number of stages,
-    int Stages_,
-    /// Used for partial specialization
-    typename Enable = bool>
-class MmaMultistageFromSharedMemory : public MmaBaseFromSharedMemory<
-                                          Shape1_,
-                                          AccumulatorSharedStorage::Shape::kN,
-                                          Policy1_,
-                                          Stages_> {
- public:
-  ///< Base class
-  using Base = MmaBaseFromSharedMemory<
-      Shape1_,
-      AccumulatorSharedStorage::Shape::kN,
-      Policy1_,
-      Stages_>;
-
-  ///< Size of the Gemm problem - concept: gemm::GemmShape<>
-  using Shape1 = Shape1_;
-  ///< Iterates over tiles of B operand in global memory
-  using IteratorB1 = IteratorB1_;
-  using IteratorB = IteratorB1;
-  ///< Policy describing tuning details
-  using Policy1 = Policy1_;
-
-  using SmemIteratorB1 = SmemIteratorB1_;
-  using WarpIteratorA1 = WarpIteratorA1_; ///< Iterates over the intermediate
-                                          ///< accumulator tile in shared memory
-
-  ///< Data type of accumulator matrix
-  using ElementC = ElementC_;
-  ///< Layout of accumulator matrix
-  using LayoutC = LayoutC_;
+  /// Size of the matrix to load (concept: MatrixShape)
+  typename Shape_,
+  /// Data type of A elements
+  typename Element_,
+  /// Shape of the warp in units of thread (concept: MmaSimtPolicy)
+  typename Policy_
+>
+class MmaSimtTileIterator<Shape_, Operand::kC, Element_, layout::ColumnMajor, Policy_> {
+public:
 
-  static cutlass::arch::CacheOperation::Kind const kCacheOpB1 = CacheOpB1;
-  static constexpr bool kSmemContainsEntireB = Base::kSmemContainsEntireB;
+  /// Shape of tile to load (concept: MatrixShape)
+  using Shape = Shape_;
 
-  //
-  // Dependent types
-  //
+  /// Operand tag
+  static Operand const kOperand = Operand::kC;
+
+  /// Element type
+  using Element = Element_;
 
-  /// Fragment of accumulator tile
-  using FragmentC1 = typename Policy1::Operator::FragmentC;
-  using FragmentC = FragmentC1;
-
-  /// Warp-level Mma
-  using Operator1 = typename Policy1::Operator;
-
-  /// Minimum architecture is Sm80 to support cp.async
-  using ArchTag = arch::Sm80;
-
-  /// Complex transform on B operand
-  static ComplexTransform const kTransformB1 = Operator1::kTransformB;
-
-  /// Internal structure exposed for introspection.
-  struct Detail {
-    static_assert(
-        Base::kWarpGemmIterations1 > 1,
-        "The pipelined structure requires at least two warp-level "
-        "GEMM operations.");
-
-    /// Number of cp.async instructions to load one stage of operand B
-    static int const TBLoadIterationsB1 =
-        IteratorB1::ThreadMap::Iterations::kCount;
-
-    /// Number of cp.async instructions to load on group of operand B
-    static int const kAccessesPerGroupB1 =
-        (TBLoadIterationsB1 + Base::kWarpGemmIterations1 - 1) /
-        Base::kWarpGemmIterations1;
-  };
-
-  static constexpr int kNumStagesConcurrentLoad =
-      kSmemContainsEntireB ? Base::kStages : Base::kStages - 1;
-
- private:
-  using WarpLoadedFragmentA1 = typename Operator1::FragmentA;
-  using WarpLoadedFragmentB1 = typename Operator1::FragmentB;
-  using WarpTransformedFragmentA1 = typename Operator1::TransformedFragmentA;
-  using WarpTransformedFragmentB1 = typename Operator1::TransformedFragmentB;
+  /// Layout of accumulators in memory
+  using Layout = layout::ColumnMajor;
+
+  /// Decomposition of elements among threads
+  using Policy = Policy_;
+
+  /// TensorRef type for loading element from a tensor
+  using TensorRef = TensorRef<Element, Layout>;
+
+  /// Index type
+  using Index = typename TensorRef::Index;
+
+  /// Long Index type
+  using LongIndex = typename TensorRef::LongIndex;
+
+  /// Coordinate for an element in the tensor
+  using TensorCoord = typename TensorRef::TensorCoord;
 
- private:
   //
-  // Data members
+  // Derived quantities
   //
 
-  /// Iterator to load a warp-scoped tile of A1 operand from intermediate
-  /// accumulator tile
-  WarpIteratorA1 warp_tile_iterator_A1_;
+  static_assert(
+    (!(Shape::kRow % Policy::WarpShape::kRow)) && (!(Shape::kColumn % Policy::WarpShape::kColumn)),
+    "Warp-level GEMM shape must be divisible by the arrangement of threads in the warp.");
 
-  /// Iterator to write threadblock-scoped tile of B operand to shared memory
-  SmemIteratorB1 smem_iterator_B1_;
+  static_assert(Shape::kRow > 0, "Shape::kRow must be greater than zero.");
+  static_assert(Shape::kColumn > 0, "Shape::kColumn must be greater than zero.");
+  static_assert(Policy::WarpShape::kRow > 0, "Policy::WarpShape::kRow must be greater than zero.");
+  static_assert(Policy::WarpShape::kColumn > 0, "Policy::WarpShape::kColumn must be greater than zero.");
+  static_assert(Shape::kRow / Policy::WarpShape::kRow > 0, "Shape::kRow / Policy::WarpShape::kRow must be greater than zero.");
+  static_assert(Shape::kColumn / Policy::WarpShape::kColumn > 0, "Shape::kColumn / Policy::WarpShape::kColumn must be greater than zero.");
+
+  /// Thraed-level shape of a fragment
+  using ThreadShape = MatrixShape<
+    Shape::kRow / Policy::WarpShape::kRow,
+    Shape::kColumn / Policy::WarpShape::kColumn
+  >;
 
-  bool prologue_done_;
+  static_assert(
+    (!(ThreadShape::kRow % Policy::LaneMmaShape::kM)) && (!(ThreadShape::kColumn % Policy::LaneMmaShape::kN)),
+    "Warp-level GEMM shape must be divisible by the arrangement of threads in the warp.");
+  
+  /// Number of individual loads
+  using Iterations = MatrixShape<
+    ThreadShape::kRow / Policy::LaneMmaShape::kM,
+    ThreadShape::kColumn / Policy::LaneMmaShape::kN
+  >;
+
+  using Delta = MatrixShape<
+    Policy::WarpShape::kRow * Policy::LaneMmaShape::kM,
+    Policy::WarpShape::kColumn * Policy::LaneMmaShape::kN
+  >;
+
+  /// Fragment object holding a thread's part of a tile
+  using Fragment = Array<Element, ThreadShape::kCount>;
+
+private:
+
+  TensorRef ref_;
+
+public:
+  
+  /// Default ctor constructs null iterator
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator() { }
 
- public:
-  /// Construct from tensor references
-  CUTLASS_DEVICE
-  MmaMultistageFromSharedMemory(
-      typename Base::SharedStorage&
-          shared_storage, ///< Shared storage needed for internal use by
-                          ///< threadblock-scoped GEMM
-      AccumulatorSharedStorage& accumulator_shared_storage,
-      ///< ID within the threadblock
-      int thread_idx,
-      ///< ID of warp
-      int warp_idx,
-      ///< ID of each thread within a warp
-      int lane_idx,
-      ///< GEMM0 N is used for accumulator extent
-      int problem_size_0_n)
-      : Base(shared_storage, thread_idx, warp_idx, lane_idx),
-        warp_tile_iterator_A1_(
-            accumulator_shared_storage.accum_ref(),
-            lane_idx),
-        smem_iterator_B1_(shared_storage.operand_B_ref(), thread_idx),
-        prologue_done_(false) {
-    // Compute warp location within threadblock tile by mapping the warp_id to
-    // three coordinates:
-    //   _m: the warp's position within the threadblock along the M dimension
-    //   _n: the warp's position within the threadblock along the N dimension
-    //   _k: the warp's position within the threadblock along the K dimension
-
-    int warp_idx_mn_1 =
-        warp_idx % (Base::WarpCount1::kM * Base::WarpCount1::kN);
-    int warp_idx_k_1 = warp_idx / (Base::WarpCount1::kM * Base::WarpCount1::kN);
-
-    int warp_idx_m_1 = warp_idx_mn_1 % Base::WarpCount1::kM;
-    int warp_idx_n_1 = warp_idx_mn_1 / Base::WarpCount1::kM;
-
-    // Add per-warp offsets in units of warp-level tiles
-    warp_tile_iterator_A1_.add_tile_offset(
-        {warp_idx_m_1, Base::kWarpGemmIterations1 * warp_idx_k_1});
-    this->warp_tile_iterator_B_.add_tile_offset(
-        {Base::kWarpGemmIterations1 * warp_idx_k_1, warp_idx_n_1});
+  /// Constructor from TensorRef
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator(
+    TensorRef const &ref, 
+    int lane_id
+  ):
+    ref_(ref) {
+
+    // compute offset based on thread ID and lane layout
+    typename Policy::LaneLayout lane_layout = Policy::get_lane_layout();
+
+    MatrixCoord lane_offset = lane_layout.inverse(lane_id) * 
+      MatrixCoord(Policy::LaneMmaShape::kM, Policy::LaneMmaShape::kN);
+
+    ref_.add_coord_offset(lane_offset);
+  }
+  
+  /// Adds a pointer offset to internal pointer(s) to advance through memory
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator &add_pointer_offset(LongIndex offset) {
+    ref_.add_pointer_offset(offset);
+    return *this;
   }
 
-  CUTLASS_DEVICE
-  bool set_prologue_done(bool value) {
-    prologue_done_ = value;
+  /// Advances an iterator along logical dimensions of matrix in units of whole tiles
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator &add_tile_offset(TensorCoord const &coord) {
+
+    ref_.add_coord_offset({
+      coord.row() * Shape::kRow, 
+      coord.column() * Shape::kColumn});
+
+    return *this;
   }
 
-  CUTLASS_DEVICE
-  static void prologue(
-      typename Base::SharedStorage& shared_storage,
-      IteratorB iterator_B1,
-      int thread_idx,
-      int problem_size_0_n) {
-    SmemIteratorB1 smem_iterator_B1(shared_storage.operand_B_ref(), thread_idx);
-    _prologue(
-        iterator_B1,
-        (problem_size_0_n + Base::Shape::kK - 1) / Base::Shape::kK,
-        smem_iterator_B1);
+  /// Advances the iterator along the advance dimension
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator & operator++() {
+
+    ref_.add_coord_offset({Shape::kRow, 0});
+
+    return *this;
   }
 
-  CUTLASS_DEVICE
-  void copy_tiles_and_advance_1(
-      IteratorB1& iterator_B1,
-      int group_start_B1 = 0) {
-    iterator_B1.set_iteration_index(
-        group_start_B1 * IteratorB1::kAccessesPerVector);
-    this->smem_iterator_B1_.set_iteration_index(group_start_B1);
+  /// Advances the iterator along the advance dimension
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator & operator--() {
+
+    ref_.add_coord_offset({-Shape::kRow, 0});
+
+    return *this;
+  }
+
+  /// Loads a fragment from memory with additional logical offset
+  CUTLASS_HOST_DEVICE
+  void load_with_pointer_offset(
+    Fragment &frag,                             ///< fragment to be loaded from memory
+    Index pointer_offset) const {               ///< linear offset (in units of Element) when loading
 
-    // Load for operand B
     CUTLASS_PRAGMA_UNROLL
-    for (int j = 0; j < Detail::kAccessesPerGroupB1; ++j) {
-      if (group_start_B1 + j < Detail::TBLoadIterationsB1) {
-        typename IteratorB1::AccessType* dst_ptr =
-            reinterpret_cast<typename IteratorB1::AccessType*>(
-                this->smem_iterator_B1_.get());
-
-        int const kSrcBytes = sizeof_bits<typename IteratorB1::Element>::value *
-            IteratorB1::ThreadMap::kElementsPerAccess /
-            IteratorB1::kAccessesPerVector / 8;
+    for (int mma_n = 0; mma_n < Iterations::kN; ++mma_n) {
+      CUTLASS_PRAGMA_UNROLL
+      for (int n = 0; n < Policy::LaneMmaShape::kN; ++n) {
+
+        Array<Element, Policy::LaneMmaShape::kM> const *src_ptr = 
+          reinterpret_cast<Array<Element, Policy::LaneMmaShape::kM> const *>(
+            ref_.data() + pointer_offset + ref_.offset({0, mma_n * Delta::kN + n}));
 
         CUTLASS_PRAGMA_UNROLL
-        for (int v = 0; v < IteratorB1::kAccessesPerVector; ++v) {
-          auto gmem_ptr = iterator_B1.get();
+        for (int mma_m = 0; mma_m < Iterations::kM; ++mma_m) {
 
-          cutlass::arch::cp_async_zfill<kSrcBytes, kCacheOpB1>(
-              dst_ptr + v, gmem_ptr, iterator_B1.valid());
+          Array<Element, Policy::LaneMmaShape::kM> *dst_ptr = 
+            reinterpret_cast<Array<Element, Policy::LaneMmaShape::kM> *>(&frag) + 
+            mma_m + Iterations::kM * (n + mma_n * Policy::LaneMmaShape::kN);
 
-          ++iterator_B1;
+          *dst_ptr = src_ptr[mma_m * Policy::WarpShape::kM];
         }
-        ++this->smem_iterator_B1_;
       }
     }
   }
+    
+  /// Loads a fragment from memory at the location pointed to by the iterator.
+  CUTLASS_HOST_DEVICE
+  void load(Fragment &frag) const {
+    load_with_pointer_offset(frag, 0);
+  }
 
-  CUTLASS_DEVICE
-  static void _prologue(
-      IteratorB& iterator_B1,
-      int32_t gemm_k_iterations_1,
-      SmemIteratorB1& smem_iterator_B1_) {
-    // Issue several complete stages
+  /// Stores a fragment to memory at the location pointed to by the iterator
+  CUTLASS_HOST_DEVICE
+  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) const {
+    
     CUTLASS_PRAGMA_UNROLL
-    for (int stage = 0; stage < kNumStagesConcurrentLoad;
-         ++stage, --gemm_k_iterations_1) {
-      iterator_B1.set_residual_tile(gemm_k_iterations_1 == 1);
-      iterator_B1.clear_mask(gemm_k_iterations_1 == 0);
-
-      iterator_B1.set_iteration_index(0);
-      smem_iterator_B1_.set_iteration_index(0);
-
-      // Load for operand B
+    for (int mma_n = 0; mma_n < Iterations::kColumn; ++mma_n) {
       CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::TBLoadIterationsB1; ++j) {
-        typename IteratorB1::AccessType* dst_ptr =
-            reinterpret_cast<typename IteratorB1::AccessType*>(
-                smem_iterator_B1_.get());
+      for (int n = 0; n < Policy::LaneMmaShape::kN; ++n) {
+
+        Array<Element, Policy::LaneMmaShape::kM> *dst_ptr= 
+          reinterpret_cast<Array<Element, Policy::LaneMmaShape::kM> *>(
+            ref_.data() + pointer_offset + ref_.offset({0, mma_n * Delta::kColumn + n}));
 
         CUTLASS_PRAGMA_UNROLL
-        for (int v = 0; v < IteratorB1::kAccessesPerVector; ++v) {
-          int const kSrcBytes =
-              sizeof_bits<typename IteratorB1::Element>::value *
-              IteratorB1::ThreadMap::kElementsPerAccess /
-              IteratorB1::kAccessesPerVector / 8;
+        for (int mma_m = 0; mma_m < Iterations::kRow; ++mma_m) {
 
-          cutlass::arch::cp_async_zfill<kSrcBytes, kCacheOpB1>(
-              dst_ptr + v, iterator_B1.get(), iterator_B1.valid());
+          Array<Element, Policy::LaneMmaShape::kM> const *src_ptr = 
+            reinterpret_cast<Array<Element, Policy::LaneMmaShape::kM> const *>(&frag) + 
+            mma_m + Iterations::kRow * (n + mma_n * Policy::LaneMmaShape::kN);
 
-          ++iterator_B1;
+          dst_ptr[mma_m * Policy::WarpShape::kRow] = *src_ptr;
         }
-
-        ++smem_iterator_B1_;
       }
+    }
+  }
+  /// Stores a fragment to memory at the location pointed to by the iterator
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag) const {
+    store_with_pointer_offset(frag, 0);
+  }
+};
 
-      // Move to the next stage
-      iterator_B1.add_tile_offset({1, 0});
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-      smem_iterator_B1_.add_tile_offset({1, 0});
+/// Specialization for C operands of row-major layouts
+///
+/// Concept: MutableRandomAccessContiguousTileIteratorConcept
+///
+template <
+  /// Size of the matrix to load (concept: MatrixShape)
+  typename Shape_,
+  /// Data type of A elements
+  typename Element_,
+  /// Shape of the warp in units of thread (concept: MmaSimtPolicy)
+  typename Policy_
+>
+class MmaSimtTileIterator<Shape_, Operand::kC, Element_, layout::RowMajor, Policy_> {
+public:
 
-      // Defines the boundary of a stage of cp.async.
-      cutlass::arch::cp_async_fence();
-    }
-    iterator_B1.set_residual_tile(gemm_k_iterations_1 == 1);
-    iterator_B1.clear_mask(gemm_k_iterations_1 == 0);
+  /// Shape of tile to load (concept: MatrixShape)
+  using Shape = Shape_;
+
+  /// Operand tag
+  static Operand const kOperand = Operand::kC;
+
+  /// Element type
+  using Element = Element_;
+
+  /// Layout of accumulators in memory
+  using Layout = layout::RowMajor;
+
+  /// Decomposition of elements among threads
+  using Policy = Policy_;
+
+  /// TensorRef type for loading element from a tensor
+  using TensorRef = TensorRef<Element, Layout>;
+
+  /// Index type
+  using Index = typename TensorRef::Index;
+
+  /// Long Index type
+  using LongIndex = typename TensorRef::LongIndex;
+
+  /// Coordinate for an element in the tensor
+  using TensorCoord = typename TensorRef::TensorCoord;
+
+  //
+  // Derived quantities
+  //
+
+  static_assert(
+    (!(Shape::kRow % Policy::WarpShape::kRow)) && (!(Shape::kColumn % Policy::WarpShape::kColumn)),
+    "Warp-level GEMM shape must be divisible by the arrangement of threads in the warp.");
+
+  static_assert(Shape::kRow > 0, "Shape::kRow must be greater than zero.");
+  static_assert(Shape::kColumn > 0, "Shape::kColumn must be greater than zero.");
+  static_assert(Policy::WarpShape::kRow > 0, "Policy::WarpShape::kRow must be greater than zero.");
+  static_assert(Policy::WarpShape::kColumn > 0, "Policy::WarpShape::kColumn must be greater than zero.");
+  static_assert(Shape::kRow / Policy::WarpShape::kRow > 0, "Shape::kRow / Policy::WarpShape::kRow must be greater than zero.");
+  static_assert(Shape::kColumn / Policy::WarpShape::kColumn > 0, "Shape::kColumn / Policy::WarpShape::kColumn must be greater than zero.");
+
+  /// Thraed-level shape of a fragment
+  using ThreadShape = MatrixShape<
+    Shape::kRow / Policy::WarpShape::kRow,
+    Shape::kColumn / Policy::WarpShape::kColumn
+  >;
+
+  static_assert(
+    (!(ThreadShape::kRow % Policy::LaneMmaShape::kM)) && (!(ThreadShape::kColumn % Policy::LaneMmaShape::kN)),
+    "Warp-level GEMM shape must be divisible by the arrangement of threads in the warp.");
+  
+  /// Number of individual loads
+  using Iterations = MatrixShape<
+    ThreadShape::kRow / Policy::LaneMmaShape::kM,
+    ThreadShape::kColumn / Policy::LaneMmaShape::kN
+  >;
+
+  using Delta = MatrixShape<
+    Policy::WarpShape::kRow * Policy::LaneMmaShape::kM,
+    Policy::WarpShape::kColumn * Policy::LaneMmaShape::kN
+  >;
+
+  /// Fragment object holding a thread's part of a tile
+  using Fragment = Array<Element, ThreadShape::kCount>;
+
+private:
+
+  TensorRef ref_;
+
+public:
+  
+  /// Default ctor constructs null iterator
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator() { }
+
+  /// Constructor from TensorRef
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator(
+    TensorRef const &ref, 
+    int lane_id
+  ):
+    ref_(ref) {
+
+    // compute offset based on thread ID and lane layout
+    typename Policy::LaneLayout lane_layout = Policy::get_lane_layout();
+
+    MatrixCoord lane_offset = lane_layout.inverse(lane_id) * 
+      MatrixCoord(Policy::LaneMmaShape::kM, Policy::LaneMmaShape::kN);
+    
+    ref_.add_coord_offset(lane_offset);
+  }
+  
+  /// Adds a pointer offset to internal pointer(s) to advance through memory
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator &add_pointer_offset(LongIndex offset) {
+    ref_.add_pointer_offset(offset);
+    return *this;
   }
 
-  /// Perform a threadblock-scoped matrix multiply-accumulate
-  CUTLASS_DEVICE
-  void operator()(
-      ///< problem size of GEMM
-      int gemm_k_iterations_1_,
-      ///< destination accumulator tile
-      FragmentC1& accum,
-      ///< iterator over B1 operand in global memory
-      IteratorB1 iterator_B1,
-      ///< initial value of accumulator
-      FragmentC1 const& src_accum) {
-    // 2nd Gemm
-
-    //
-    // Prologue
-    //
-    // Perform accumulation in the 'd' output operand
-    accum = src_accum;
-
-    if (!prologue_done_) {
-      _prologue(iterator_B1, gemm_k_iterations_1_, smem_iterator_B1_);
-    } else if (!kSmemContainsEntireB) {
-      // Restore the iterators increments
+  /// Advances an iterator along logical dimensions of matrix in units of whole tiles
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator &add_tile_offset(TensorCoord const &coord) {
 
-      int gemm_k_iterations_1 = gemm_k_iterations_1_;
-      // Issue several complete stages
-      CUTLASS_PRAGMA_UNROLL
-      for (int stage = 0; stage < kNumStagesConcurrentLoad;
-           ++stage, --gemm_k_iterations_1) {
-        iterator_B1.set_iteration_index(0);
-        this->smem_iterator_B1_.set_iteration_index(0);
+    ref_.add_coord_offset({
+      coord.row() * Shape::kRow, 
+      coord.column() * Shape::kColumn});
 
-        // Load for operand B
-        CUTLASS_PRAGMA_UNROLL
-        for (int j = 0; j < Detail::TBLoadIterationsB1; ++j) {
-          CUTLASS_PRAGMA_UNROLL
-          for (int v = 0; v < IteratorB1::kAccessesPerVector; ++v) {
-            ++iterator_B1;
-          }
-          ++this->smem_iterator_B1_;
-        }
-        iterator_B1.add_tile_offset({1, 0});
-        this->smem_iterator_B1_.add_tile_offset({1, 0});
-      }
-      iterator_B1.set_residual_tile(gemm_k_iterations_1 <= 1);
-      iterator_B1.clear_mask(gemm_k_iterations_1 <= 0);
-    }
+    return *this;
+  }
 
-    // DEPBAR+SYNC
-    cutlass::arch::cp_async_wait<kNumStagesConcurrentLoad - 1>();
-    __syncthreads();
-
-    // Pair of fragments used to overlap shared memory loads and math
-    // instructions
-    WarpLoadedFragmentA1 warp_loaded_frag_A1[2];
-    WarpLoadedFragmentB1 warp_loaded_frag_B1[2];
-    WarpTransformedFragmentA1 warp_transformed_frag_A1[2];
-    WarpTransformedFragmentB1 warp_transformed_frag_B1[2];
-
-    Operator1 warp_mma1;
-
-    warp_tile_iterator_A1_.load(warp_loaded_frag_A1[0]);
-    ++warp_tile_iterator_A1_;
-
-    this->warp_tile_iterator_B_.set_kgroup_index(0);
-    this->warp_tile_iterator_B_.load(warp_loaded_frag_B1[0]);
-    ++this->warp_tile_iterator_B_;
-
-    int smem_write_stage_idx = Base::kStages - 1;
-    int smem_read_stage_idx = 0;
-
-    warp_mma1.transform(
-        warp_transformed_frag_A1[0],
-        warp_transformed_frag_B1[0],
-        warp_loaded_frag_A1[0],
-        warp_loaded_frag_B1[0]);
-
-    // tf32x3 kernels use staging accumulation. warp_mma uses a temporary
-    // accumulator and this temporary accumulator is added to the final
-    // accumulator once in every mainloop iteration.
-    plus<FragmentC1> plus_accum;
-
-    FragmentC1 tmp_accum;
-
-    if (platform::is_same<
-            typename Operator1::MathOperator,
-            arch::OpMultiplyAddFastF32>::value ||
-        platform::is_same<
-            typename Operator1::MathOperator,
-            arch::OpMultiplyAddComplexFastF32>::value) {
-      tmp_accum.clear();
-    }
+  /// Advances the iterator along the advance dimension
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator & operator++() {
 
-    //
-    // Mainloop
-    //
+    ref_.add_coord_offset({Shape::kRow, 0});
 
-    CUTLASS_PRAGMA_UNROLL
-    for (int gemm_k_iterations_1 = gemm_k_iterations_1_ - (Base::kStages - 1);
-         gemm_k_iterations_1 > (-Base::kStages + 1);
-         gemm_k_iterations_1--) {
-      //
-      // Loop over GEMM K dimension
-      //
+    return *this;
+  }
+
+  /// Advances the iterator along the advance dimension
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator & operator--() {
+
+    ref_.add_coord_offset({-Shape::kRow, 0});
+
+    return *this;
+  }
+
+  /// Loads a fragment from memory with additional logical offset
+  CUTLASS_HOST_DEVICE
+  void load_with_pointer_offset(
+    Fragment &frag,                             ///< fragment to be loaded from memory
+    Index pointer_offset) const {               ///< linear offset (in units of Element) when loading
 
-      // Computes a warp-level GEMM on data held in shared memory
-      // Each "warp_mma_k" refers to a warp-level matrix multiply-accumulate
+    CUTLASS_PRAGMA_UNROLL
+    for (int mma_m = 0; mma_m < Iterations::kRow; ++mma_m) {
       CUTLASS_PRAGMA_UNROLL
-      for (int warp_mma_k = 0; warp_mma_k < Base::kWarpGemmIterations1;
-           ++warp_mma_k) {
-        // Load warp-level tile from accumulator fragment (A)
-        // or shared memory (operand B)
-        this->warp_tile_iterator_B_.set_kgroup_index(
-            (warp_mma_k + 1) % Base::kWarpGemmIterations1);
-        // skip warp tile loading for the last kgroup (we are out of the buf)
-        if (gemm_k_iterations_1 > (-Base::kStages + 2) ||
-            warp_mma_k < Base::kWarpGemmIterations1 - 1) {
-          warp_tile_iterator_A1_.load(
-              warp_loaded_frag_A1[(warp_mma_k + 1) % 2]);
-          this->warp_tile_iterator_B_.load(
-              warp_loaded_frag_B1[(warp_mma_k + 1) % 2]);
-        }
-        ++warp_tile_iterator_A1_;
-        ++this->warp_tile_iterator_B_;
+      for (int m = 0; m < Policy::LaneMmaShape::kM; ++m) {
 
-        if (warp_mma_k > 0)
-          warp_mma1.transform(
-              warp_transformed_frag_A1[warp_mma_k % 2],
-              warp_transformed_frag_B1[warp_mma_k % 2],
-              warp_loaded_frag_A1[warp_mma_k % 2],
-              warp_loaded_frag_B1[warp_mma_k % 2]);
-
-        if (platform::is_same<
-                typename Operator1::MathOperator,
-                arch::OpMultiplyAddFastF32>::value ||
-            platform::is_same<
-                typename Operator1::MathOperator,
-                arch::OpMultiplyAddComplexFastF32>::value) {
-          warp_mma1(
-              tmp_accum,
-              warp_transformed_frag_A1[warp_mma_k % 2],
-              warp_transformed_frag_B1[warp_mma_k % 2],
-              tmp_accum);
-
-          if (warp_mma_k == 0) {
-            accum = plus_accum(accum, tmp_accum);
-            tmp_accum.clear();
-          }
-        } else {
-          warp_mma1(
-              accum,
-              warp_transformed_frag_A1[warp_mma_k % 2],
-              warp_transformed_frag_B1[warp_mma_k % 2],
-              accum);
-        }
+        Array<Element, Policy::LaneMmaShape::kN> const *src_ptr = 
+          reinterpret_cast<Array<Element, Policy::LaneMmaShape::kN> const *>(
+            ref_.data() + pointer_offset + ref_.offset({mma_m * Delta::kRow + m, 0}));
 
-        // Issue global->shared copies for the this stage
-        if (warp_mma_k < Base::kWarpGemmIterations1 - 1) {
-          int group_start_iteration_B1;
+        CUTLASS_PRAGMA_UNROLL
+        for (int mma_n = 0; mma_n < Iterations::kColumn; ++mma_n) {
 
-          group_start_iteration_B1 = warp_mma_k * Detail::kAccessesPerGroupB1;
+          Array<Element, Policy::LaneMmaShape::kN> *dst_ptr = 
+            reinterpret_cast<Array<Element, Policy::LaneMmaShape::kN> *>(&frag) + 
+            mma_n + Iterations::kColumn * (m + mma_m * Policy::LaneMmaShape::kM);
 
-          if (!kSmemContainsEntireB) {
-            copy_tiles_and_advance_1(iterator_B1, group_start_iteration_B1);
-          }
+          *dst_ptr = src_ptr[mma_n * Policy::WarpShape::kColumn];
         }
+      }
+    }
+  }
+    
+  /// Loads a fragment from memory at the location pointed to by the iterator.
+  CUTLASS_HOST_DEVICE
+  void load(Fragment &frag) const {
+    load_with_pointer_offset(frag, 0);
+  }
 
-        if (warp_mma_k + 2 == Base::kWarpGemmIterations1) {
-          int group_start_iteration_B1;
-          group_start_iteration_B1 =
-              (warp_mma_k + 1) * Detail::kAccessesPerGroupB1;
+  /// Stores a fragment to memory at the location pointed to by the iterator
+  CUTLASS_HOST_DEVICE
+  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) const {
+    
+    CUTLASS_PRAGMA_UNROLL
+    for (int mma_m = 0; mma_m < Iterations::kRow; ++mma_m) {
+      CUTLASS_PRAGMA_UNROLL
+      for (int m = 0; m < Policy::LaneMmaShape::kM; ++m) {
 
-          if (!kSmemContainsEntireB) {
-            copy_tiles_and_advance_1(iterator_B1, group_start_iteration_B1);
-          }
+        Array<Element, Policy::LaneMmaShape::kN> *dst_ptr = 
+          reinterpret_cast<Array<Element, Policy::LaneMmaShape::kN> *>(
+            ref_.data() + pointer_offset + ref_.offset({mma_m * Delta::kRow + m, 0}));
 
-          // Inserts a memory fence between stages of cp.async instructions.
-          cutlass::arch::cp_async_fence();
+        CUTLASS_PRAGMA_UNROLL
+        for (int mma_n = 0; mma_n < Iterations::kColumn; ++mma_n) {
 
-          // Waits until kStages-2 stages have committed.
-          arch::cp_async_wait<kNumStagesConcurrentLoad - 1>();
-          __syncthreads();
-
-          // Move to the next stage
-          iterator_B1.add_tile_offset({1, 0});
-
-          this->smem_iterator_B1_.add_tile_offset({1, 0});
-
-          // Add negative offsets to return iterators to the 'start' of the
-          // circular buffer in shared memory
-          if (!kSmemContainsEntireB) {
-            if (smem_write_stage_idx == (Base::kStages - 1)) {
-              this->smem_iterator_B1_.add_tile_offset({-Base::kStages, 0});
-              smem_write_stage_idx = 0;
-            } else {
-              ++smem_write_stage_idx;
-            }
-
-            if (smem_read_stage_idx == (Base::kStages - 1)) {
-              this->warp_tile_iterator_B_.add_tile_offset(
-                  {-Base::kStages * Policy1::kPartitionsK *
-                       Base::kWarpGemmIterations1,
-                   0});
-              smem_read_stage_idx = 0;
-            } else {
-              ++smem_read_stage_idx;
-            }
-          }
+          Array<Element, Policy::LaneMmaShape::kN> const *src_ptr = 
+            reinterpret_cast<Array<Element, Policy::LaneMmaShape::kN> const *>(&frag) + 
+            mma_n + Iterations::kColumn * (m + mma_m * Policy::LaneMmaShape::kM);
 
-          iterator_B1.set_residual_tile(gemm_k_iterations_1 == 2);
-          iterator_B1.clear_mask(gemm_k_iterations_1 == 1);
+          dst_ptr[mma_n * Policy::WarpShape::kColumn] = *src_ptr;
         }
-
-        // Do any conversions feeding the first stage at the end of the loop so
-        // we can start right away on mma instructions
-        if (warp_mma_k + 1 == Base::kWarpGemmIterations1)
-          warp_mma1.transform(
-              warp_transformed_frag_A1[(warp_mma_k + 1) % 2],
-              warp_transformed_frag_B1[(warp_mma_k + 1) % 2],
-              warp_loaded_frag_A1[(warp_mma_k + 1) % 2],
-              warp_loaded_frag_B1[(warp_mma_k + 1) % 2]);
       }
     }
-
-    if (platform::is_same<
-            typename Operator1::MathOperator,
-            arch::OpMultiplyAddFastF32>::value ||
-        platform::is_same<
-            typename Operator1::MathOperator,
-            arch::OpMultiplyAddComplexFastF32>::value) {
-      accum = plus_accum(accum, tmp_accum);
-    }
+  }
+  
+  /// Stores a fragment to memory at the location pointed to by the iterator
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag) const {
+    store_with_pointer_offset(frag, 0);
   }
 };
 
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Specialization for A operands of column-major-K interleaved layouts
+///
+/// Concept: MutableRandomAccessContiguousTileIteratorConcept
+///
 template <
-    typename WarpShape,
-    typename InstructionShape,
-    typename RegularWarpIterator,
-    typename Policy>
-struct DefaultWarpIteratorAFromSharedMemory {};
-
-// TensorOp - Ampere
-template <typename WarpShape, typename RegularWarpIterator, typename Policy>
-struct DefaultWarpIteratorAFromSharedMemory<
-    WarpShape,
-    cutlass::gemm::GemmShape<16, 8, 8>,
-    RegularWarpIterator,
-    Policy> {
-  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 8>;
-  static constexpr auto kWarpSize = 32;
-  using OpDelta = typename Policy::Operator::Policy::OpDelta;
-
-  using WarpIterator =
-      cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<
-          cutlass::MatrixShape<WarpShape::kM, WarpShape::kK>,
-          cutlass::gemm::Operand::kA,
-          typename RegularWarpIterator::Element,
-          cutlass::layout::RowMajor,
-          cutlass::MatrixShape<InstructionShape::kM, InstructionShape::kK>,
-          OpDelta::kRow,
-          kWarpSize>;
-};
+  /// Size of the matrix to load (concept: MatrixShape)
+  typename Shape_,
+  /// Data type of A elements
+  typename Element_,
+  /// Shape of the warp in units of thread (concept: MmaSimtPolicy)
+  typename Policy_,
+  /// Number of partitions along K dimension
+  int PartitionsK,
+  /// Number of KGroups per kPartition
+  int PartitionGroupSize
+>
+class MmaSimtTileIterator<Shape_, Operand::kA, Element_, layout::ColumnMajorInterleaved<4>, Policy_, PartitionsK, PartitionGroupSize> {
+public:
 
-// TensorOp - Volta
-template <typename WarpShape, typename RegularWarpIterator, typename Policy>
-struct DefaultWarpIteratorAFromSharedMemory<
-    WarpShape,
-    cutlass::gemm::GemmShape<16, 16, 4>,
-    RegularWarpIterator,
-    Policy> {
-  using InstructionShape = cutlass::gemm::GemmShape<16, 16, 4>;
-  static constexpr auto kWarpSize = 32;
-  using OpDelta = typename Policy::Operator::Policy::OpDelta;
-
-  using WarpIterator =
-      cutlass::gemm::warp::MmaVoltaTensorOpMultiplicandTileIterator<
-          cutlass::MatrixShape<32, 32>, // MatrixShape<WarpShape::kM,
-                                        // WarpShape::kK>,
-          cutlass::gemm::Operand::kA,
-          typename RegularWarpIterator::Element,
-          cutlass::layout::RowMajorVoltaTensorOpMultiplicandCrosswise<16, 32>,
-          cutlass::MatrixShape<16, 4>,
-          OpDelta::kRow,
-          kWarpSize>;
-};
+  /// Shape of tile to load (concept: MatrixShape)
+  using Shape = Shape_;
 
-// Simt
-template <typename WarpShape, typename RegularWarpIterator, typename Policy>
-struct DefaultWarpIteratorAFromSharedMemory<
-    WarpShape,
-    cutlass::gemm::GemmShape<1, 1, 1>,
-    RegularWarpIterator,
-    Policy> {
-  using InstructionShape = cutlass::gemm::GemmShape<1, 1, 1>;
-  static constexpr auto kWarpSize = 32;
-
-  // We just use the same iterator, as we reproduced the same shared-memory
-  // schema. Just modify it to handle non-complete tiles.
-  using WarpIterator = RegularWarpIterator;
-};
+  /// Operand tag
+  static Operand const kOperand = Operand::kA;
 
-// Converts a "regular" Mma into their counterpart from shared memory
-template <typename Mma_, typename AccumulatorSharedStorage>
-struct DefaultMmaFromSharedMemory;
+  /// Element type
+  using Element = Element_;
 
-// Mma pipelined
-template <
-    /// Size of the Gemm problem - concept: gemm::GemmShape<>
-    typename Shape_,
-    /// Iterates over tiles of A operand in global memory
-    //  (concept: ReadableTileIterator | ForwardTileIterator |
-    //  MaskedTileIterator)
-    typename IteratorA_,
-    /// Iterates over tiles of A operand in shared memory
-    /// (concept: WriteableTileIterator | RandomAccessTileIterator)
-    typename SmemIteratorA_,
-    /// Iterates over tiles of B operand in global memory
-    //  (concept: ReadableTileIterator | ForwardTileIterator |
-    //  MaskedTileIterator)
-    typename IteratorB_,
-    /// Iterates over tiles of B operand in shared memory
-    /// (concept: WriteableTileIterator | RandomAccessTileIterator)
-    typename SmemIteratorB_,
-    /// Data type of accumulator matrix
-    typename ElementC_,
-    /// Data type of accumulator matrix
-    typename LayoutC_,
-    /// Policy describing tuning details (concept: MmaPolicy)
-    typename Policy_,
-    /// Transformation applied to A operand
-    typename TransformA_,
-    /// Transformation applied to B operand
-    typename TransformB_,
-    typename AccumulatorSharedStorage_>
-struct DefaultMmaFromSharedMemory<
-    MmaPipelined<
-        Shape_,
-        IteratorA_,
-        SmemIteratorA_,
-        IteratorB_,
-        SmemIteratorB_,
-        ElementC_,
-        LayoutC_,
-        Policy_,
-        TransformA_,
-        TransformB_>,
-    AccumulatorSharedStorage_> {
-  static constexpr int kWarpSize = 32;
-  using SmemAccumulatorLayout = cutlass::layout::RowMajor;
-
-  using RegularMma = MmaPipelined<
-      Shape_,
-      IteratorA_,
-      SmemIteratorA_,
-      IteratorB_,
-      SmemIteratorB_,
-      ElementC_,
-      LayoutC_,
-      Policy_,
-      TransformA_,
-      TransformB_>;
-
-  using WarpShape = typename Policy_::Operator::Shape;
-  using InstructionShape = typename Policy_::Operator::InstructionShape;
-  using ArchMmaOperator = typename Policy_::Operator;
-
-  using WarpIteratorA = typename DefaultWarpIteratorAFromSharedMemory<
-      WarpShape,
-      InstructionShape,
-      typename RegularMma::Operator::IteratorA,
-      Policy_>::WarpIterator;
-  using IteratorB =
-      typename cutlass::transform::threadblock::MakeIteratorResidualLast<
-          IteratorB_>::Iterator;
-
-  using Mma = typename cutlass::gemm::threadblock::MmaPipelinedFromSharedMemory<
-      Shape_,
-      WarpIteratorA,
-      AccumulatorSharedStorage_,
-      IteratorB,
-      SmemIteratorB_,
-      ElementC_,
-      LayoutC_,
-      Policy_>;
-};
+  /// Layout of policy
+  using Layout = layout::ColumnMajorInterleaved<4> ;
 
-template <
-    /// Size of the Gemm problem - concept: gemm::GemmShape<>
-    typename Shape_,
-    /// Iterates over tiles of A operand in global memory
-    //  (concept: ReadableTileIterator | ForwardTileIterator |
-    //  MaskedTileIterator)
-    typename IteratorA_,
-    /// Iterates over tiles of A operand in shared memory
-    /// (concept: WriteableTileIterator | RandomAccessTileIterator)
-    typename SmemIteratorA_,
-    /// Cache operation for operand A
-    cutlass::arch::CacheOperation::Kind CacheOpA,
-    /// Iterates over tiles of B operand in global memory
-    //  (concept: ReadableTileIterator | ForwardTileIterator |
-    //  MaskedTileIterator)
-    typename IteratorB_,
-    /// Iterates over tiles of B operand in shared memory
-    /// (concept: WriteableTileIterator | RandomAccessTileIterator)
-    typename SmemIteratorB_,
-    /// Cache operation for operand B
-    cutlass::arch::CacheOperation::Kind CacheOpB,
-    /// Data type of accumulator matrix
-    typename ElementC_,
-    /// Data type of accumulator matrix
-    typename LayoutC_,
-    /// Policy describing tuning details (concept: MmaPolicy)
-    typename Policy_,
-    /// Number of stages,
-    int Stages,
-    /// Use zfill or predicate for out-of-bound cp.async
-    SharedMemoryClearOption SharedMemoryClear,
-    typename AccumulatorSharedStorage_>
-struct DefaultMmaFromSharedMemory<
-    MmaMultistage<
-        Shape_,
-        IteratorA_,
-        SmemIteratorA_,
-        CacheOpA,
-        IteratorB_,
-        SmemIteratorB_,
-        CacheOpB,
-        ElementC_,
-        LayoutC_,
-        Policy_,
-        Stages,
-        SharedMemoryClear>,
-    AccumulatorSharedStorage_> {
-  static constexpr int kWarpSize = 32;
-
-  using RegularMma = MmaMultistage<
-      Shape_,
-      IteratorA_,
-      SmemIteratorA_,
-      CacheOpA,
-      IteratorB_,
-      SmemIteratorB_,
-      CacheOpB,
-      ElementC_,
-      LayoutC_,
-      Policy_,
-      Stages,
-      SharedMemoryClear>;
-
-  using WarpShape = typename Policy_::Operator::Shape;
-  using InstructionShape = typename Policy_::Operator::InstructionShape;
-  using WarpIteratorA = typename DefaultWarpIteratorAFromSharedMemory<
-      WarpShape,
-      InstructionShape,
-      typename RegularMma::Operator::IteratorA,
-      Policy_>::WarpIterator;
-
-  static int constexpr kMaxK = AccumulatorSharedStorage_::Shape::kN;
-  // Reduce the number of stages if we don't need that many
-  static int constexpr kStagesMax =
-      (kMaxK + int(Shape_::kK) - 1) / int(Shape_::kK);
-  static int constexpr kStages = cutlass::const_min(Stages, kStagesMax);
-
-  using IteratorB =
-      typename cutlass::transform::threadblock::MakeIteratorResidualLast<
-          IteratorB_>::Iterator;
-  using Mma =
-      typename cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<
-          Shape_,
-          WarpIteratorA,
-          AccumulatorSharedStorage_,
-          IteratorB,
-          SmemIteratorB_,
-          RegularMma::kCacheOpB,
-          ElementC_,
-          LayoutC_,
-          Policy_,
-          kStages>;
-};
+  /// Decomposition of elements among threads
+  using Policy = Policy_;
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+  /// TensorRef type for loading element from a tensor
+  using TensorRef = TensorRef<Element, Layout>;
 
-template <
-    typename IteratorC,
-    typename Operator,
-    typename scalar_t,
-    typename WarpShape_,
-    typename ThreadblockShape_>
-struct B2bGemm;
-
-// Tensor Cores >= Sm75 specialization (Ampere ...)
-template < /// Size of the matrix to load (concept: MatrixShape)
-    typename Shape_,
-    /// Element type
-    typename Element_,
-    /// Layout of operand in memory
-    typename Layout_,
-    /// Shape of one matrix product operation (concept: MatrixShape)
-    typename InstructionShape_,
-    /// Interval between adjacent *MMA instructions (in units of MMA
-    /// instructions, concept: MatrixShape)
-    typename OpDelta_,
-    typename Operator,
-    typename scalar_t,
-    typename WarpShape_,
-    typename ThreadblockShape_>
-struct B2bGemm<
-    cutlass::gemm::warp::MmaTensorOpAccumulatorTileIterator<
-        Shape_,
-        Element_,
-        Layout_,
-        InstructionShape_,
-        OpDelta_>,
-    Operator,
-    scalar_t,
-    WarpShape_,
-    ThreadblockShape_> {
-  using IteratorC =
-      typename cutlass::gemm::warp::MmaTensorOpAccumulatorTileIterator<
-          Shape_,
-          Element_,
-          Layout_,
-          InstructionShape_,
-          OpDelta_>;
-  using FragmentC = typename IteratorC::Fragment;
-  using InstructionShape = InstructionShape_;
-  using WarpShape = WarpShape_;
-  using ThreadblockShape = ThreadblockShape_;
-  using accum_t = Element_;
-  using lse_scalar_t = float;
-
-  using SmemAccumulatorLayout = cutlass::layout::RowMajor;
-
-  // Iterator to load accumulators (results of matmul in registers)
-  using FragmentIteratorAccumulator =
-      cutlass::epilogue::warp::FragmentIteratorTensorOp<
-          WarpShape,
-          InstructionShape,
-          accum_t,
-          typename Operator::Policy::Operator::FragmentC,
-          cutlass::layout::RowMajor>;
-
-  // Iterator to store to shared-memory
-  using SmemIteratorD0 = typename cutlass::epilogue::warp::TileIteratorTensorOp<
-      WarpShape,
-      InstructionShape,
-      scalar_t, // accum_t,
-      SmemAccumulatorLayout>;
-  using AccumulatorSharedStorage =
-      cutlass::gemm::threadblock::AccumulatorSharedStorage<
-          ThreadblockShape,
-          typename SmemIteratorD0::Element,
-          typename SmemIteratorD0::TensorLayout,
-          typename SmemIteratorD0::Padding>;
-  // We need to provide an operation for the epilogue. Let's create an
-  // operation that does nothing (ScaleType::Nothing), just converts
-  // from accum_t (float) -> scalar_t (can be half)
-  using OutputOpNoOp = cutlass::epilogue::thread::LinearCombination<
-      typename SmemIteratorD0::Element, // ElementOutput
-      FragmentIteratorAccumulator::Fragment::kElements,
-      accum_t, // ElementAccumulator
-      typename SmemIteratorD0::Element, // ElementCompute
-      cutlass::epilogue::thread::ScaleType::Nothing>;
-  using Epilogue = cutlass::epilogue::threadblock::EpilogueSmemAccumulator<
-      SmemIteratorD0,
-      FragmentIteratorAccumulator,
-      SmemIteratorD0, // ScaleBiasIterator - not used
-      OutputOpNoOp>;
-
-  // Epilogue 2: with LSE (for backwards pass)
-  static int const kElementsPerAccess = 2; // TODO: Why 2?
-  using IteratorAccumulatorLSE =
-      cutlass::transform::threadblock::VectorIterator<
-          cutlass::transform::threadblock::PredicatedVectorAccessIterator<
-              // Shape
-              cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kN>,
-              // WarpShape
-              cutlass::MatrixShape<WarpShape::kM, WarpShape::kN>,
-              lse_scalar_t,
-              cutlass::layout::RowMajor,
-              kElementsPerAccess>>;
-  using EpilogueOpApplyLSE = cutlass::epilogue::thread::ApplyLogSumExp<
-      scalar_t, // ElementOutput_
-      lse_scalar_t, // ElementLSE_
-      accum_t, // ElementAccumulator_
-      accum_t, // ElementCompute_
-      128 / cutlass::sizeof_bits<scalar_t>::value
-      // FragmentIteratorAccumulator::Fragment::kElements
-      // InstructionShape::kM * InstructionShape::kN / 32
-      >;
-  using EpilogueWithLSE =
-      cutlass::epilogue::threadblock::EpilogueSmemAccumulator<
-          SmemIteratorD0,
-          FragmentIteratorAccumulator,
-          IteratorAccumulatorLSE,
-          EpilogueOpApplyLSE>;
-
-  static void CUTLASS_DEVICE accumToSmem(
-      AccumulatorSharedStorage& shared_storage,
-      FragmentC const& accum,
-      int lane_id,
-      cutlass::MatrixCoord const& tile_coords) {
-    SmemIteratorD0 smem_iterator_attn(shared_storage.accum_ref(), lane_id);
-    smem_iterator_attn.add_tile_offset(
-        tile_coords *
-        cutlass::MatrixCoord{
-            SmemIteratorD0::TileIterations::kRow,
-            SmemIteratorD0::TileIterations::kColumn});
-    Epilogue epilogue;
-    epilogue(OutputOpNoOp({}), smem_iterator_attn, accum);
-  }
-
-  static void CUTLASS_DEVICE accumApplyLSEToSmem(
-      AccumulatorSharedStorage& shared_storage,
-      FragmentC& accum,
-      lse_scalar_t const* lse,
-      int32_t lse_extents,
-      int thread_id,
-      int warp_id,
-      int lane_id,
-      cutlass::MatrixCoord const& tile_coords) {
-    constexpr int32_t kAlignLSE = 32;
-    IteratorAccumulatorLSE iterator_lse(
-        lse,
-        {(int32_t)0, (int32_t)ceil_div(lse_extents, kAlignLSE) * kAlignLSE},
-        thread_id,
-        warp_id,
-        cutlass::MatrixCoord{0, 0} // offset
-    );
-
-    SmemIteratorD0 smem_iterator_attn(shared_storage.accum_ref(), lane_id);
-    smem_iterator_attn.add_tile_offset(
-        tile_coords *
-        cutlass::MatrixCoord{
-            SmemIteratorD0::TileIterations::kRow,
-            SmemIteratorD0::TileIterations::kColumn});
-    EpilogueWithLSE epilogue;
-    EpilogueOpApplyLSE minus_lse_exp({});
-    epilogue(
-        minus_lse_exp,
-        smem_iterator_attn,
-        accum,
-        // scale - unused
-        iterator_lse,
-        // bias
-        iterator_lse);
+  /// Index type
+  using Index = typename TensorRef::Index;
+
+  /// Long Index type
+  using LongIndex = typename TensorRef::LongIndex;
+
+  /// Coordinate for an element in the tensor
+  using TensorCoord = typename TensorRef::TensorCoord;
+
+  /// Iterleave factor
+  static const int kInterleave = 4;
+  
+  /// Number of partitions along K dimension
+  static const int kPartitionsK = PartitionsK;
+
+  /// Number of KGroups per kPartition
+  static const int kGroupPerTile = PartitionGroupSize / Shape::kColumn;
+
+  //
+  // Derived quantities
+  //
+
+  static_assert(!(Shape::kRow % Policy::WarpShape::kRow), 
+    "The warp-level GEMM M size must be divisible by the number of threads arranged along the M dimension.");
+
+  static_assert(Shape::kRow > 0, "Shape::kRow must be greater than zero.");
+  static_assert(Shape::kColumn > 0, "Shape::kColumn must be greater than zero.");
+  static_assert(Policy::WarpShape::kRow > 0, "Policy::WarpShape::kRow must be greater than zero.");
+  static_assert(Shape::kRow / Policy::WarpShape::kRow > 0, "Shape::kRow / Policy::WarpShape::kRow must be greater than zero.");
+
+  /// Thread-level shape of a fragment
+  using ThreadShape = MatrixShape<
+    Shape::kRow / Policy::WarpShape::kRow,
+    Shape::kColumn
+  >;
+
+  static_assert(!(ThreadShape::kRow % Policy::LaneMmaShape::kM) && !(ThreadShape::kColumn % Policy::LaneMmaShape::kK), 
+    "Thread-level GEMM must be divisible by Policy::LaneMmaShape.");
+
+  /// Number of individual loads
+  using Iterations = MatrixShape<
+    ThreadShape::kRow / Policy::LaneMmaShape::kM,
+    ThreadShape::kColumn / Policy::LaneMmaShape::kK
+  >;
+
+  /// Fragment object holding a thread's part of a tile
+  using Fragment = Array<Element, ThreadShape::kCount>;
+
+private:
+
+  /// Internal reference
+  cutlass::TensorRef<Array<Element, Policy::LaneMmaShape::kMK>, layout::ColumnMajorInterleaved<4>> ref_;
+
+  /// group index within tile
+  int k_group_idx_;
+
+public:
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator() { }
+
+  /// Constructor from TensorRef
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator(
+    TensorRef ref, 
+    int lane_id
+  ) {
+
+    // compute offset based on thread ID and lane layout
+    typename Policy::LaneLayout lane_layout = Policy::get_lane_layout();
+
+    MatrixCoord lane_offset = lane_layout.inverse(lane_id) * 
+      MatrixCoord(Policy::LaneMmaShape::kM, 0);
+
+    ref.add_coord_offset(lane_offset);
+
+    k_group_idx_ = 0;
+    ref_.reset(reinterpret_cast<Array<Element, Policy::LaneMmaShape::kMK> *>(ref.data()), ref.stride(0)/Policy::LaneMmaShape::kMK);
   }
-};
+  
+
+  /// Adds a pointer offset to internal pointer(s) to advance through memory
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator &add_pointer_offset(LongIndex offset) {
+    ref_.add_pointer_offset(offset);
+    return *this;
+  }
+
+  /// Advances an iterator along logical dimensions of matrix in units of whole tiles
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator &add_tile_offset(TensorCoord const &coord) {
+
+    ref_.add_coord_offset({
+      coord.row() * Shape::kRow / Policy::LaneMmaShape::kMK, 
+      coord.column() * Shape::kColumn});
 
-// Volta Specialization
-// only supported for f16
-template <typename Operator, typename WarpShape_, typename ThreadblockShape_>
-struct B2bGemm<
-    cutlass::gemm::warp::MmaVoltaTensorOpAccumulatorTileIterator<
-        cutlass::MatrixShape<32, 32>,
-        float,
-        cutlass::layout::RowMajor,
-        cutlass::gemm::GemmShape<16, 16, 4>,
-        cutlass::MatrixShape<1, 1>>,
-    Operator,
-    cutlass::half_t,
-    WarpShape_,
-    ThreadblockShape_> {
-  using IteratorC =
-      cutlass::gemm::warp::MmaVoltaTensorOpAccumulatorTileIterator<
-          cutlass::MatrixShape<32, 32>,
-          float,
-          cutlass::layout::RowMajor,
-          cutlass::gemm::GemmShape<16, 16, 4>,
-          cutlass::MatrixShape<1, 1>>;
-  using scalar_t = cutlass::half_t;
-  using accum_t = IteratorC::Element;
-  using WarpShape = WarpShape_;
-  using ThreadblockShape = ThreadblockShape_;
-  using FragmentC = IteratorC::Fragment;
-  using lse_scalar_t = float;
-
-  using SmemAccumulatorLayout = cutlass::layout::RowMajor;
-  using SmemIteratorD0 = cutlass::epilogue::warp::TileIteratorVoltaTensorOp<
-      WarpShape,
-      cutlass::gemm::GemmShape<32, 32, 4>,
-      scalar_t,
-      SmemAccumulatorLayout>;
-
-  // // Storage in shared-memory for Q.Kt
-  using AccumulatorSharedStorage =
-      cutlass::gemm::threadblock::AccumulatorSharedStorage<
-          ThreadblockShape,
-          scalar_t,
-          cutlass::layout::RowMajorVoltaTensorOpMultiplicandCrosswise<
-              16,
-              32>, // typename SmemIteratorD0::TensorLayout,
-          cutlass::MatrixShape<0, 0> // Padding
-          >;
-
-  using OutputLayout =
-      cutlass::layout::RowMajorVoltaTensorOpMultiplicandCrosswise<16, 32>;
-  using TensorRef = cutlass::TensorRef<scalar_t, OutputLayout>;
-  using Policy = typename IteratorC::Policy;
-  using Element = accum_t;
-  // Those are MmaVoltaTensorOpAccumulatorTileIterator private fields
-  // Let's copy their values
-  static int const kElementsPerPartial = 4;
-  using EleShapePerPatial = typename cutlass::platform::conditional<
-      cutlass::platform::is_same<Element, float>::value,
-      cutlass::MatrixShape<2, 2>,
-      cutlass::MatrixShape<1, 4>>::type;
-  static int const kElementsPerMma = 8;
-  static int const kAccumulatorPatials = 2;
-  using QuadShapePerPatialMma = cutlass::MatrixShape<4, 4>;
-
-  static void CUTLASS_DEVICE accumToSmem(
-      AccumulatorSharedStorage& shared_storage,
-      FragmentC const& accum,
-      int lane_id,
-      cutlass::MatrixCoord const& tile_coords) {
-    // ctor - from MmaVoltaTensorOpAccumulatorTileIterator
-    TensorRef ref_(shared_storage.accum_ref());
-    int quad = (lane_id >> 2);
-    int lane_in_quad = (lane_id & 3);
-    int accum_m, accum_n;
-
-    if (cutlass::platform::is_same<Element, float>::value) {
-      // (quad[2],quad[0])+lane_in_quad[0]
-      accum_m = (((quad & 0x4) >> 1) + (quad & 0x1)) * 8 + (lane_in_quad & 1);
-      // (quad[1])+lane_in_quad[1]
-      accum_n =
-          ((quad >> 1) & 0x1) * kElementsPerPartial * kAccumulatorPatials +
-          (lane_in_quad & 2);
-    } else {
-      accum_m = (((quad & 0x4) >> 1) + (quad & 0x1)) * 8 +
-          lane_in_quad; // (quad[2],quad[0])
-      accum_n = ((quad >> 1) & 0x1) * kElementsPerPartial * kAccumulatorPatials;
+    return *this;
+  }
+
+  /// Advances the iterator along the advance dimension
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator & operator++() {
+
+    add_tile_offset({0, 1});
+
+    if (kPartitionsK > 1) {
+      ++k_group_idx_;
+      // Jump to next stage
+      if (k_group_idx_ == kGroupPerTile) {
+        k_group_idx_ = 0;
+        add_tile_offset({0, kGroupPerTile * (kPartitionsK-1)});
+      }
     }
-    cutlass::MatrixCoord lane_offset(accum_m, accum_n);
 
-    // Tile offset
-    ref_.add_coord_offset(
-        tile_coords *
-        cutlass::MatrixCoord(
-            {IteratorC::Shape::kRow, IteratorC::Shape::kColumn}));
+    return *this;
+  }
+
+  /// Advances the iterator along the advance dimension
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator & operator--() {
 
-    using AccessType = cutlass::Array<scalar_t, EleShapePerPatial::kColumn>;
+    ref_.add_coord_offset({0, -Shape::kColumn});
+
+    return *this;
+  }
+
+  /// Loads a fragment from memory at the location pointed to by the iterator.
+  CUTLASS_HOST_DEVICE
+  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) const {
+
+    Array<Element, Policy::LaneMmaShape::kMK > *dst_ptr = 
+      reinterpret_cast<Array<Element, Policy::LaneMmaShape::kMK> *>(&frag);
 
-    // store - from MmaVoltaTensorOpAccumulatorTileIterator
     CUTLASS_PRAGMA_UNROLL
-    for (int tile_n = 0; tile_n < Policy::TileIterations::kColumn; ++tile_n) {
+    for (int k = 0; k < Iterations::kColumn; ++k) {
+
       CUTLASS_PRAGMA_UNROLL
-      for (int tile_m = 0; tile_m < Policy::TileIterations::kRow; ++tile_m) {
-        CUTLASS_PRAGMA_UNROLL
-        for (int mma_n = 0; mma_n < Policy::MmaIterations::kColumn; ++mma_n) {
-          CUTLASS_PRAGMA_UNROLL
-          for (int mma_m = 0; mma_m < Policy::MmaIterations::kRow; ++mma_m) {
-            int mma_accum_start =
-                (((tile_n * Policy::TileIterations::kRow + tile_m) *
-                      Policy::MmaIterations::kColumn +
-                  mma_n) *
-                     Policy::MmaIterations::kRow +
-                 mma_m) *
-                kElementsPerMma;
-
-            CUTLASS_PRAGMA_UNROLL
-            for (int p = 0; p < kAccumulatorPatials; ++p) {
-              CUTLASS_PRAGMA_UNROLL
-              for (int m = 0; m < EleShapePerPatial::kRow; ++m) {
-                int accum_m = tile_m * Policy::InterleavedTile::kRow +
-                    mma_m * QuadShapePerPatialMma::kRow + m * 2;
-                int accum_n = tile_n * Policy::InterleavedTile::kColumn +
-                    mma_n * QuadShapePerPatialMma::kColumn +
-                    p * Policy::InterleavedTile::kColumn / 2;
-                int r = (accum_m + lane_offset.row());
-                AccessType to_store;
-                CUTLASS_PRAGMA_UNROLL
-                for (int n = 0; n < EleShapePerPatial::kColumn; ++n) {
-                  int idx = mma_accum_start + p * kElementsPerPartial +
-                      m * EleShapePerPatial::kColumn + n;
-                  int c = (accum_n + n + lane_offset.column());
-                  to_store[n] = scalar_t(accum[idx]);
-                }
-                int c = (accum_n + lane_offset.column());
-                assert(r < 32);
-                assert(c < 32);
-                *reinterpret_cast<AccessType*>(
-                    ref_.data() + ref_.offset({r, c})) = to_store;
-              }
-            }
-          }
-        }
+      for (int m = 0; m < Iterations::kRow; ++m) {
+
+        dst_ptr[m + k * Iterations::kRow] = 
+          *((ref_.data() + ref_.offset({m * Policy::WarpShape::kRow / kInterleave, 
+                  k*Policy::LaneMmaShape::kK}) + pointer_offset / Policy::LaneMmaShape::kM));
       }
     }
   }
 
-  static void CUTLASS_DEVICE accumApplyLSEToSmem(
-      AccumulatorSharedStorage& shared_storage,
-      typename IteratorC::Fragment& accum,
-      lse_scalar_t const* lse,
-      int lse_extent,
-      int thread_id,
-      int warp_id,
-      int lane_id,
-      cutlass::MatrixCoord const& tile_coords) {
-    // Non-optimized way to apply LSE to registers
-    // NOTE: accum is attn.T
-    // TODO: Optimize for each architecture
-    static constexpr int WarpSize = 32;
-    using RegistersIter = typename DefaultAttentionScalingCoefsUpdater<
-        IteratorC,
-        accum_t,
-        WarpSize>::Updater;
-    auto lane_offset =
-        RegistersIter::get_lane_offset(lane_id, warp_id, tile_coords);
-
-    cutlass::Array<lse_scalar_t, IteratorC::Fragment::kElements> lse_prefetched;
-    lse_prefetched.clear();
-    int rowIdx = 0;
-    int colIdx = 0;
-    RegistersIter::iterateRows(
-        lane_offset,
-        [&](int accum_m) {
-          ++rowIdx;
-          colIdx = 0;
-        },
-        [&](int accum_m, int accum_n, int idx) {
-          if (rowIdx == 1) {
-            lse_prefetched[colIdx] = accum_n < lse_extent
-                ? lse[accum_n]
-                : platform::numeric_limits<accum_t>::infinity();
-          }
-          accum[idx] = expf(accum[idx] - lse_prefetched[colIdx]);
-          ++colIdx;
-        },
-        [&](int accum_m) {});
-    accumToSmem(shared_storage, accum, lane_id, tile_coords);
+  /// Loads a fragment from memory at the location pointed to by the iterator.
+  CUTLASS_HOST_DEVICE
+  void load(Fragment &frag) const {
+    load_with_pointer_offset(frag, 0);
+  }
+    
+  /// Stores a fragment to memory at the location pointed to by the iterator
+  CUTLASS_HOST_DEVICE
+  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) const {
+    
+    Array<Element, Policy::LaneMmaShape::kMK> const *src_ptr = 
+      reinterpret_cast<Array<Element, Policy::LaneMmaShape::kMK > *>(&frag);
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int k = 0; k < Iterations::kN; ++k) {
+      CUTLASS_PRAGMA_UNROLL
+      for (int m = 0; m < Iterations::kM; ++m) {
+        *(ref_.data() + ref_.offset(m * Policy::WarpShape::kM, k) + pointer_offset / Policy::LaneMmaShape::kM) = 
+          src_ptr[m + k * Iterations::kM];
+      }
+    }
+  }
+
+  /// Stores a fragment to memory at the location pointed to by the iterator
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag) const {
+    store_with_pointer_offset(frag, 0);
+  }
+
+  /// Notify the iterator which k-group it is currently pointing to.
+  ///
+  /// This does not advance the iterator. Rather, it overrides its internal
+  /// tracking with constant-valued k-group index to enable the compiler to
+  /// fold constants and achieve more efficient code.
+  ///
+  /// This is used by some nontrivial permuted layouts.
+  CUTLASS_DEVICE
+  void set_kgroup_index(int k_group) {
+    // no operation here
   }
 };
 
-// Simt Specialization
-// for f32 on Sm70-Sm75 and f16/f32 below
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
+/// Specialization for B operands of row-major k-interleaved layouts
+///
+/// Concept: MutableRandomAccessContiguousTileIteratorConcept
+///
 template <
-    typename Operator,
-    typename OperatorPolicy,
-    typename scalar_t,
-    typename WarpShape_,
-    typename ThreadblockShape_>
-struct B2bGemm<
-    cutlass::gemm::warp::MmaSimtTileIterator<
-        cutlass::MatrixShape<32, 32>,
-        cutlass::gemm::Operand::kC,
-        float,
-        cutlass::layout::RowMajor,
-        OperatorPolicy,
-        1,
-        1>,
-    Operator,
-    scalar_t,
-    WarpShape_,
-    ThreadblockShape_> {
-  using IteratorC = cutlass::gemm::warp::MmaSimtTileIterator<
-      cutlass::MatrixShape<32, 32>,
-      cutlass::gemm::Operand::kC,
-      float,
-      cutlass::layout::RowMajor,
-      OperatorPolicy,
-      1,
-      1>;
-  using accum_t = typename IteratorC::Element;
-  using WarpShape = WarpShape_;
-  using ThreadblockShape = ThreadblockShape_;
-  using FragmentC = typename IteratorC::Fragment;
-  using lse_scalar_t = float;
-
-  // Storage in shared-memory for Q.Kt
-  using AccumulatorSharedStorage =
-      cutlass::gemm::threadblock::AccumulatorSharedStorage<
-          ThreadblockShape,
-          scalar_t,
-          cutlass::layout::ColumnMajor,
-          cutlass::MatrixShape<0, 0> // Padding
-          >;
-
-  static void CUTLASS_DEVICE accumToSmem(
-      AccumulatorSharedStorage& shared_storage,
-      FragmentC const& accum,
-      int lane_id,
-      cutlass::MatrixCoord const& tile_coords) {
-    using Policy = typename IteratorC::Policy;
-    using Element = typename IteratorC::Element;
-    using Iterations = typename IteratorC::Iterations;
-    using Delta = typename IteratorC::Delta;
+  /// Size of the matrix to load (concept: MatrixShape)
+  typename Shape_,
+  /// Data type of A elements
+  typename Element_,
+  /// Shape of the warp in units of thread (concept: MmaSimtPolicy)
+  typename Policy_,
+  /// Number of partitions along K dimension
+  int PartitionsK,
+  /// Number of KGroups per kPartition
+  int PartitionGroupSize
+>
+class MmaSimtTileIterator<Shape_, Operand::kB, Element_, layout::RowMajorInterleaved<4>, Policy_, PartitionsK, PartitionGroupSize> {
+public:
+
+  /// Shape of tile to load (concept: MatrixShape)
+  using Shape = Shape_;
+
+  /// Operand tag
+  static Operand const kOperand = Operand::kB;
+
+  /// Element type
+  using Element = Element_;
+
+  /// Layout of policy
+  using Layout = layout::RowMajorInterleaved<4>;
+
+  /// Decomposition of elements among threads
+  using Policy = Policy_;
+
+  /// TensorRef type for loading element from a tensor
+  using TensorRef = TensorRef<Element, Layout>;
+
+  /// Index type
+  using Index = typename TensorRef::Index;
+
+  /// Long Index type
+  using LongIndex = typename TensorRef::LongIndex;
+
+  /// Coordinate for an element in the tensor
+  using TensorCoord = typename TensorRef::TensorCoord;
+
+  /// Interleave factor
+  static const int kInterleave = 4;
+
+  /// Number of partitions along K dimension
+  static const int kPartitionsK = PartitionsK;
+
+  /// Number of KGroups per kPartition
+  static const int kGroupPerTile = PartitionGroupSize / Shape::kRow;
+
+  //
+  // Derived quantities
+  //
+
+  static_assert(!(Shape::kColumn % Policy::WarpShape::kColumn), 
+    "The warp-level GEMM N size must be divisible by the number of threads arranged along the N dimension.");
+
+  static_assert(Shape::kRow > 0, "Shape::kRow must be greater than zero.");
+  static_assert(Shape::kColumn > 0, "Shape::kColumn must be greater than zero.");
+  static_assert(Policy::WarpShape::kColumn > 0, "Policy::WarpShape::kColumn must be greater than zero.");
+  static_assert(Shape::kColumn / Policy::WarpShape::kColumn > 0, "Shape::kColumn / Policy::WarpShape::kColumn must be greater than zero.");
+
+  /// Thread-level shape of a fragment
+  using ThreadShape = MatrixShape<
+    Shape::kRow,
+    Shape::kColumn / Policy::WarpShape::kColumn
+  >;
+
+  static_assert(!(ThreadShape::kColumn % Policy::LaneMmaShape::kN) && !(ThreadShape::kRow % Policy::LaneMmaShape::kK), 
+    "Thread-level GEMM must be divisible by Policy::LaneMmaShape.");
+
+  /// Number of individual loads
+  using Iterations = MatrixShape<
+    ThreadShape::kRow / Policy::LaneMmaShape::kK,
+    ThreadShape::kColumn / Policy::LaneMmaShape::kN
+  >;
+
+  /// Fragment object holding a thread's part of a tile
+  using Fragment = Array<Element, ThreadShape::kCount>;
+
+
+private:
+
+  /// Internal reference
+  cutlass::TensorRef<Array<Element, Policy::LaneMmaShape::kKN>, layout::RowMajorInterleaved<4>> ref_;
+
+  /// group index within tile
+  int k_group_idx_;
+
+public:
+  
+  /// Default ctor constructs null iterator
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator() { }
+
+  /// Constructor from TensorRef
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator(
+    TensorRef ref, 
+    int lane_id
+  ) {
 
-    auto ref_ = shared_storage.accum_ref();
-    // ctor - MmaSimtTileIterator
     // compute offset based on thread ID and lane layout
     typename Policy::LaneLayout lane_layout = Policy::get_lane_layout();
 
-    MatrixCoord lane_offset = lane_layout.inverse(lane_id) *
-        MatrixCoord(Policy::LaneMmaShape::kM, Policy::LaneMmaShape::kN);
+    MatrixCoord lane_offset = lane_layout.inverse(lane_id) * 
+      MatrixCoord(0, Policy::LaneMmaShape::kN);
 
-    ref_.add_coord_offset(lane_offset);
+    ref.add_coord_offset(lane_offset);
+
+    k_group_idx_ = 0;
+
+    ref_.reset(
+      reinterpret_cast<Array<Element, Policy::LaneMmaShape::kKN> *>(ref.data()),
+      ref.stride(0) / Policy::LaneMmaShape::kKN);
+  }
+  
+  /// Adds a pointer offset to internal pointer(s) to advance through memory
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator &add_pointer_offset(LongIndex offset) {
+    ref_.add_pointer_offset(offset);
+    return *this;
+  }
 
-    // Tile offset
-    ref_.add_coord_offset(
-        tile_coords *
-        cutlass::MatrixCoord(
-            {IteratorC::Shape::kRow, IteratorC::Shape::kColumn}));
+  /// Advances an iterator along logical dimensions of matrix in units of whole tiles
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator &add_tile_offset(TensorCoord const &coord) {
+
+    ref_.add_coord_offset({
+      coord.row() * Shape::kRow, 
+      coord.column() * Shape::kColumn / Policy::LaneMmaShape::kKN});
+
+    return *this;
+  }
+
+  /// Advances the iterator along the advance dimension
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator & operator++() {
+
+    add_tile_offset({1, 0});
+
+    if (kPartitionsK > 1) {
+      ++k_group_idx_;
+      // Jump to next stage
+      if (k_group_idx_ == kGroupPerTile) {
+        k_group_idx_ = 0;
+        add_tile_offset({kGroupPerTile * (kPartitionsK-1), 0});
+      }
+    }
+
+    return *this;
+  }
+
+  /// Advances the iterator along the advance dimension
+  CUTLASS_HOST_DEVICE
+  MmaSimtTileIterator & operator--() {
+
+    ref_.add_coord_offset({-Shape::kRow, 0});
+
+    return *this;
+  }
+
+  /// Loads a fragment from memory at the location pointed to by the iterator.
+  CUTLASS_HOST_DEVICE
+  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) const {
+
+    Array<Element, Policy::LaneMmaShape::kKN> *dst_ptr = 
+      reinterpret_cast<Array<Element, Policy::LaneMmaShape::kKN> *>(&frag);
 
-    // store - MmaSimtTileIterator
     CUTLASS_PRAGMA_UNROLL
-    for (int mma_n = 0; mma_n < Iterations::kColumn; ++mma_n) {
+    for (int k = 0; k < Iterations::kRow; ++k) {
       CUTLASS_PRAGMA_UNROLL
-      for (int n = 0; n < Policy::LaneMmaShape::kN; ++n) {
-        CUTLASS_PRAGMA_UNROLL
-        for (int mma_m = 0; mma_m < Iterations::kRow; ++mma_m) {
-          CUTLASS_PRAGMA_UNROLL
-          for (int m = 0; m < Policy::LaneMmaShape::kM; ++m) {
-            int r =
-                Policy::LaneMmaShape::kM * (mma_m * Policy::WarpShape::kRow) +
-                m;
-            int c = mma_n * Delta::kColumn + n;
-            int idx = n +
-                Policy::LaneMmaShape::kN *
-                    (mma_n +
-                     Iterations::kColumn *
-                         (m + mma_m * Policy::LaneMmaShape::kM));
-            ref_.at({r, c}) = scalar_t(accum[idx]);
-          }
-        }
+      for (int n = 0; n < Iterations::kColumn; ++n) {
+        dst_ptr[n + k * Iterations::kColumn] = 
+          *(ref_.data() + ref_.offset({k * Policy::LaneMmaShape::kK, 
+                n * Policy::WarpShape::kColumn / kInterleave}) + pointer_offset / Policy::LaneMmaShape::kN);
       }
     }
   }
 
-  static void CUTLASS_DEVICE accumApplyLSEToSmem(
-      AccumulatorSharedStorage& shared_storage,
-      typename IteratorC::Fragment& accum,
-      lse_scalar_t const* lse,
-      int lse_extent,
-      int thread_id,
-      int warp_id,
-      int lane_id,
-      cutlass::MatrixCoord const& tile_coords) {
-    // Non-optimized way to apply LSE to registers
-    // NOTE: accum is attn.T
-    // TODO: Optimize for each architecture
-    static constexpr int WarpSize = 32;
-    using RegistersIter = typename DefaultAttentionScalingCoefsUpdater<
-        IteratorC,
-        accum_t,
-        WarpSize>::Updater;
-    auto lane_offset =
-        RegistersIter::get_lane_offset(lane_id, warp_id, tile_coords);
-
-    cutlass::Array<lse_scalar_t, IteratorC::Fragment::kElements> lse_prefetched;
-    lse_prefetched.clear();
-    int rowIdx = 0;
-    int colIdx = 0;
-    RegistersIter::iterateRows(
-        lane_offset,
-        [&](int accum_m) {
-          ++rowIdx;
-          colIdx = 0;
-        },
-        [&](int accum_m, int accum_n, int idx) {
-          if (rowIdx == 1) {
-            lse_prefetched[colIdx] = accum_n < lse_extent
-                ? lse[accum_n]
-                : platform::numeric_limits<accum_t>::infinity();
-          }
-          accum[idx] = expf(accum[idx] - lse_prefetched[colIdx]);
-          ++colIdx;
-        },
-        [&](int accum_m) {});
-    accumToSmem(shared_storage, accum, lane_id, tile_coords);
+  /// Loads a fragment from memory at the location pointed to by the iterator.
+  CUTLASS_HOST_DEVICE
+  void load(Fragment &frag) const {
+    load_with_pointer_offset(frag, 0);
+  }
+  
+  /// Stores a fragment to memory at the location pointed to by the iterator
+  CUTLASS_HOST_DEVICE
+  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) const {
+
+    Array<Element, Policy::LaneMmaShape::kN> const *src_ptr = 
+      reinterpret_cast<Array<Element, Policy::LaneMmaShape::kN> *>(&frag);
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int k = 0; k < Iterations::kM; ++k) {
+      CUTLASS_PRAGMA_UNROLL
+      for (int n = 0; n < Iterations::kN; ++n) {
+        *(ref_.data() + ref_.offset({k, n * Policy::WarpShape::kN}) + pointer_offset / Policy::LaneMmaShape::kN) = 
+          src_ptr[n + k * Iterations::kN];
+      }
+    }
+  }
+
+  /// Stores a fragment to memory at the location pointed to by the iterator
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag, Index pointer_offset) const {
+    store_with_pointer_offset(frag, 0);
+  }
+
+  /// Notify the iterator which k-group it is currently pointing to.
+  ///
+  /// This does not advance the iterator. Rather, it overrides its internal
+  /// tracking with constant-valued k-group index to enable the compiler to
+  /// fold constants and achieve more efficient code.
+  ///
+  /// This is used by some nontrivial permuted layouts.
+  CUTLASS_DEVICE
+  void set_kgroup_index(int k_group) {
+    // no operation here
   }
 };
 
-} // namespace threadblock
+///////////////////////////////////////////////////////////////////////////////////////////////////
+
+} // namespace warp
 } // namespace gemm
 } // namespace cutlass
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/transform/tile_smem_loader.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm75.cu`

 * *Files 22% similar despite different names*

```diff
@@ -8,15 +8,15 @@
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
  *
  * 2. Redistributions in binary form must reproduce the above copyright notice,
  * this list of conditions and the following disclaimer in the documentation
  * and/or other materials provided with the distribution.
  *
- * 3. Neither the name of the copyright holdvr nor the names of its
+ * 3. Neither the name of the copyright holder nor the names of its
  * contributors may be used to endorse or promote products derived from
  * this software without specific prior written permission.
  *
  * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
  * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
  * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
@@ -24,65 +24,65 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
+/*! \file
+    \brief Tests for device-wide GEMM interface
+*/
+
+#include <iostream>
+
+#include "cutlass/cutlass.h"
+#include "cutlass/gemm/device/gemm.h"
+
+#include "../../common/cutlass_unit_test.h"
+
+#include "cutlass/util/host_tensor.h"
+#include "cutlass/util/tensor_view_io.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/reference/host/tensor_copy.h"
+#include "cutlass/util/reference/host/tensor_compare.h"
+#include "cutlass/util/reference/host/gemm.h"
+
+#include "testbed.h"
+
+#if defined(CUTLASS_ARCH_MMA_SM75_SUPPORTED)
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM75_Device_Gemm_f16n_f16t_f16t_tensor_op_f16_sliced_k, 64x64x64_64x32x32) {
+
+  using ElementOutput = cutlass::half_t;
+  using ElementAccumulator = cutlass::half_t;
+
+  using Gemm = cutlass::gemm::device::Gemm<
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor,
+    cutlass::half_t,
+    cutlass::layout::RowMajor,
+    ElementOutput,
+    cutlass::layout::RowMajor,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm75,
+    cutlass::gemm::GemmShape<64, 64, 64>,
+    cutlass::gemm::GemmShape<64, 32, 32>,
+    cutlass::gemm::GemmShape<16, 8, 8>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementOutput,
+      64 / cutlass::sizeof_bits<ElementOutput>::value,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    2
+  >;
 
-#include <cutlass/cutlass.h>
-#include "cutlass/aligned_buffer.h"
-#include "cutlass/array.h"
-#include "cutlass/layout/matrix.h"
-#include "cutlass/layout/pitch_linear.h"
-#include "cutlass/numeric_types.h"
-#include "cutlass/transform/pitch_linear_thread_map.h"
-#include "cutlass/transform/threadblock/predicated_tile_iterator.h"
-#include "cutlass/transform/threadblock/regular_tile_iterator.h"
-
-template <
-    typename scalar_t, // scalar type
-    typename ThreadblockTileShape, // size of tile to load
-    int Threads, // number of participating threads
-    int ElementsPerAccess> // thread access width in elements
-class TileSmemLoader {
- public:
-  using SmemTile =
-      cutlass::AlignedBuffer<scalar_t, ThreadblockTileShape::kCount>;
-
-  using ThreadMap = cutlass::transform::PitchLinearStripminedThreadMap<
-      cutlass::layout::PitchLinearShape<
-          ThreadblockTileShape::kColumn, // contiguous
-          ThreadblockTileShape::kRow>, // strided
-      Threads, // Threads
-      ElementsPerAccess>; // ElementsPerAccess
-
-  using GmemTileIterator =
-      cutlass::transform::threadblock::PredicatedTileIterator<
-          ThreadblockTileShape, // Shape
-          scalar_t, // Element
-          cutlass::layout::RowMajor, // Layout
-          0, // AdvanceRank
-          ThreadMap>; // ThreadMap
-
-  using SmemTileIterator = cutlass::transform::threadblock::RegularTileIterator<
-      ThreadblockTileShape, // Shape
-      scalar_t, // Element
-      cutlass::layout::RowMajor, // Layout
-      0, // AdvanceRank
-      ThreadMap>; // ThreadMap
-
-  using Fragment = typename GmemTileIterator::Fragment;
-
-  /// load a tile from global memory into shared memory
-  CUTLASS_DEVICE
-  static void load(
-      GmemTileIterator tile_load_iter,
-      SmemTileIterator tile_store_iter) {
-    Fragment tb_frag;
-    tb_frag.clear();
-    tile_load_iter.load(tb_frag);
-    tile_store_iter.store(tb_frag);
-
-    __syncthreads();
-  }
-};
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+}
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+#endif  // if (CUTLASS_ARCH_MMA_SM75_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_multi_head_attention/fused_multihead_attention.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/handle.cu`

 * *Files 26% similar despite different names*

```diff
@@ -1,22 +1,22 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
  *
  * 2. Redistributions in binary form must reproduce the above copyright notice,
  * this list of conditions and the following disclaimer in the documentation
  * and/or other materials provided with the distribution.
  *
- * 3. Neither the name of the copyright holdvr nor the names of its
+ * 3. Neither the name of the copyright holder nor the names of its
  * contributors may be used to endorse or promote products derived from
  * this software without specific prior written permission.
  *
  * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
  * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
  * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
@@ -26,1120 +26,1141 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-    \brief CUTLASS Attention Example.
+    \brief CUTLASS Library handle.
+*/
+#include <iostream> 
+#include <stdexcept>
+#include <cstdint>
+
+#include "cutlass/library/handle.h"
+#include "cutlass/library/singleton.h"
+#include "cutlass/library/util.h"
 
-    This workload computes an attention example with non-fixed sequence length input. Pointers of arrays
-    are fed into grouped-GEMM functions fused with softmax for computation.
+namespace cutlass {
+namespace library {
 
-    Examples:
+///////////////////////////////////////////////////////////////////////////////////////////////////
 
-      # Run an attention example with default setup (max sequence length = 1024, batch size = 16, head size = 64, head number = 12)
-      $ ./examples/41_multi_head_attention/41_multi_head_attention
+/// Constructor
+Handle::Handle(
+  cudaStream_t stream, 
+  size_t workspace_size
+):
+  provider_(Provider::kCUTLASS), 
+  stream_(stream), 
+  workspace_(nullptr), 
+  workspace_size_(0), 
+  scalar_pointer_mode_(ScalarPointerMode::kHost), 
+  last_operation_(nullptr) {
 
-      # Run an attention example with batch size = 64 and head number = 16 without checking the correctness
-      $ ./examples/41_multi_head_attention/41_multi_head_attention --head_number=16 --batch_size=64 --reference-check=false
+  int device_idx = -1;
 
-*/
+  cudaError_t error = cudaGetDevice(&device_idx);
+  if (error != cudaSuccess) {
+    throw std::runtime_error("cudaGetDevice() failed");
+  }
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+  error = cudaGetDeviceProperties(&device_, device_idx);
+  if (error != cudaSuccess) {
+    throw std::runtime_error("cudaGetDeviceProperties() failed");
+  }
 
-#include <iostream>
-#include <fstream>
-#include <sstream>
-#include <vector>
-#include <map>
-#include <unordered_map>
-
-#include "cutlass/cutlass.h"
-#include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/kernel/gemm_grouped.h"
-#include "cutlass/gemm/kernel/default_gemm_grouped.h"
-#include "cutlass/gemm/device/gemm_grouped.h"
-#include "cutlass/gemm/device/gemm_universal.h"
-
-#include "cutlass/util/command_line.h"
-#include "cutlass/util/distribution.h"
-#include "cutlass/util/device_memory.h"
-#include "cutlass/util/tensor_view_io.h"
-#include "cutlass/util/host_tensor.h"
-#include "cutlass/util/reference/host/gemm_complex.h"
-#include "cutlass/util/reference/device/gemm_complex.h"
-#include "cutlass/util/reference/host/tensor_compare.h"
-#include "cutlass/util/reference/host/tensor_copy.h"
-#include "cutlass/util/reference/device/tensor_fill.h"
-#include "cutlass/util/reference/host/tensor_norm.h"
-
-#include "cutlass/layout/matrix.h"
-#include "cutlass/gemm/kernel/gemm_grouped.h"
-#include "cutlass/gemm/kernel/gemm_transpose_operands.h"
-#include "cutlass/gemm/kernel/default_gemm.h"
-#include "cutlass/gemm/kernel/default_gemm_complex.h"
-#include "cutlass/gemm/device/default_gemm_configuration.h"
-#include "cutlass/gemm/gemm.h"
-
-#include "cutlass/epilogue/threadblock/epilogue_with_visitor.h"
-#include "cutlass/fast_math.h"
-#include "gemm_attention.h"
+  set_workspace_size(workspace_size);
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+  Singleton::get();
+}
 
-/// Result structure
-struct Result {
+/// Destructor
+Handle::~Handle() {
+  if (workspace_) {
 
-  double runtime_ms;
-  double gflops;
-  cutlass::Status status;
-  cudaError_t error;
-  bool passed;
-
-  //
-  // Methods
-  //
-
-  Result(
-    double runtime_ms = 0,
-    double gflops = 0,
-    cutlass::Status status = cutlass::Status::kSuccess,
-    cudaError_t error = cudaSuccess
-  ):
-    runtime_ms(runtime_ms), gflops(gflops), status(status), error(error), passed(true) { }
-};
+    if (workspace_) {
+      cudaFree(workspace_);
+    }
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+    workspace_ = nullptr;
+    workspace_size_ = 0;
+  }
+}
 
-// Command line options parsing
-struct Options {
+/// Move constructor
+Handle::Handle(Handle && handle) {
+  device_ = handle.device_;
+  workspace_size_ = handle.workspace_size_;
+  workspace_ = handle.workspace_;
+  stream_ = handle.stream_;
+  scalar_pointer_mode_ = handle.scalar_pointer_mode_;
+  
+  handle.workspace_ = nullptr;
+  handle.workspace_size_ = 0;
+}
 
-  bool help;
-  bool error;
-  bool reference_check;
-  bool use_mask;
-
-  std::vector<cutlass::gemm::GemmCoord> problem_sizes0;
-  std::vector<cutlass::gemm::GemmCoord> problem_sizes1;
-
-  std::vector<cutlass::gemm::GemmCoord> problem_sizes0_real;
-  std::vector<cutlass::gemm::GemmCoord> problem_sizes1_real;
-
-  int alignment;
-  int head_number;
-  int batch_size;
-  int head_size;
-  int seq_length;
-  int iterations;
-  int cuda_streams;
-
-  // alpha0, alpha1 and beta are fixed 
-  // in this multi-head attention example
-  float alpha0;
-  float alpha1;
-  float beta;
-
-  //
-  // Methods
-  // 
-
-  Options():
-    help(false),
-    error(false),
-    alignment(16),
-    reference_check(true),
-    head_number(12),
-    batch_size(16),
-    head_size(64),
-    seq_length(1024),
-    use_mask(false),
-    iterations(20),
-    cuda_streams(0)
-  { }
-
-  // Parses the command line
-  void parse(int argc, char const **args) {
-    cutlass::CommandLine cmd(argc, args);
-
-    if (cmd.check_cmd_line_flag("help")) {
-      help = true;
-      return;
-    }
+/// Move assignment operator
+Handle & Handle::operator=(Handle && handle) {
 
-    cmd.get_cmd_line_argument("alignment", alignment, 16);
-    cmd.get_cmd_line_argument("head_number", head_number, 12);
-    cmd.get_cmd_line_argument("batch_size", batch_size, 16);
-    cmd.get_cmd_line_argument("head_size", head_size, 64);
-    cmd.get_cmd_line_argument("seq_length", seq_length, 1024);
-    cmd.get_cmd_line_argument("use_mask", use_mask, false);
-    cmd.get_cmd_line_argument("iterations", iterations, 20);
-    cmd.get_cmd_line_argument("streams", cuda_streams, 0);
-    cmd.get_cmd_line_argument("reference-check", reference_check, true);
+  provider_ = handle.provider_;
+  device_ = handle.device_;
+  workspace_size_ = handle.workspace_size_;
+  workspace_ = handle.workspace_;
+  stream_ = handle.stream_;
+  scalar_pointer_mode_ = handle.scalar_pointer_mode_;
 
-    randomize_problems();
+  handle.workspace_ = nullptr;
+  handle.workspace_size_ = 0;
 
-  }
+  return *this;
+}
 
-  void randomize_problems() {
+int Handle::compute_capability() const {
+  return device_.major * 10 + device_.minor;
+}
 
-    int problem_count = head_number * batch_size;
+/// Sets the current CUDA stream
+void Handle::set_stream(cudaStream_t stream) {
+  stream_ = stream;
+}
 
-    problem_sizes0.reserve(problem_count);
-    problem_sizes1.reserve(problem_count);
+/// Gets the current CUDA stream
+cudaStream_t Handle::get_stream() const {
+  return stream_;
+}
 
-    // When using mask, the original inputs are not padded
-    // and we need to save these info.
-    if (use_mask) {
-      problem_sizes0_real.reserve(problem_count);
-      problem_sizes1_real.reserve(problem_count);
-    }
+/// Gets the current provider
+Provider Handle::get_provider() const {
+  return provider_;
+}
 
-    for (int i = 0; i < batch_size; ++i) {
-      // problems belonging to the same batch share the same seq len
-      int m_real = (rand() % seq_length);
-      int m = (m_real + 1 + alignment - 1) / alignment * alignment;
-      int n = m;
-      int k = head_size;
-
-      for (int j = 0; j < head_number; ++j) {
-        cutlass::gemm::GemmCoord problem0(m, n, k);
-        cutlass::gemm::GemmCoord problem1(m, k, n);
-        problem_sizes0.push_back(problem0);
-        problem_sizes1.push_back(problem1);
-
-        if (use_mask) {
-          cutlass::gemm::GemmCoord problem0_real(m_real, m_real, k);
-          cutlass::gemm::GemmCoord problem1_real(m_real, k, m_real);
-          problem_sizes0_real.push_back(problem0_real);
-          problem_sizes1_real.push_back(problem1_real);
-        }
+/// Sets the provider of operations
+void Handle::set_provider(Provider provider) {
+  provider_ = provider;
+}
 
+/// Gets the device workspace size
+size_t Handle::get_workspace_size() const {
+  return workspace_size_;
+}
+
+/// Gets a pointer to the device workspace allocation in Global Memory
+void *Handle::get_workspace() const {
+  return workspace_;
+}
+
+/// Sets the size of device workspace, invalidating previous calls to get_device_workspace()
+void Handle::set_workspace_size(size_t bytes) {
+  if (bytes != workspace_size_) {
+
+    if (workspace_) {
+      cudaFree(workspace_);
+    }
+      
+    workspace_ = nullptr;
+    workspace_size_ = bytes;
+
+    if (workspace_size_) {
+  
+      cudaError_t error = cudaMalloc((void **)&workspace_, workspace_size_);
+  
+      if (error != cudaSuccess) {
+        throw std::runtime_error("Failed to allocate workspace");
       }
     }
   }
 
-  /// Prints the usage statement.
-  std::ostream & print_usage(std::ostream &out) const {
+  if (workspace_) {
+    cudaError_t error = cudaMemset(workspace_, 0, workspace_size_);
 
-    out << "41_multi_head_attention\n\n"
-      << "Options:\n\n"
-      << "  --help                      If specified, displays this usage statement.\n\n"
-      << "  --head_number=<int>         Head number in multi-head attention (default: --head_number=12)\n"
-      << "  --batch_size=<int>          Batch size in multi-head attention (default: --batch_size=16)\n"
-      << "  --head_size=<int>           Head size in multi-head attention (default: --head_size=64)\n"
-      << "  --seq_length=<int>          Max sequence length in multi-head attention (default: --seq_length=1024)\n"
-      << "  --use_mask=<bool>           If true, performs padding-like masking in softmax.\n"
-      << "  --iterations=<int>          Number of profiling iterations to perform.\n"
-      << "  --reference-check=<bool>    If true, performs reference check.\n";
-
-    return out;
-  }
-
-  /// Compute performance in GFLOP/s
-  double gflops(double runtime_s) const {
-
-    // Number of real-valued multiply-adds 
-    int64_t fmas = int64_t();
-
-    for (auto const & problem : problem_sizes0) {
-      // Two flops per multiply-add
-      fmas += problem.product() * 2;
+    if (error != cudaSuccess) {
+      throw std::runtime_error("Failed to clear workspace");
     }
-    
-    // Multiply another '2' because of the back-to-back GEMM problems in attention
-    return 2.0 * double(fmas) / double(1.0e9) / runtime_s;
   }
-};
+}
 
+/// Gets the scalar pointer mode
+ScalarPointerMode Handle::get_scalar_pointer_mode() const {
+  return scalar_pointer_mode_;
+}
 
+/// Sets the scalar pointer mode
+void Handle::set_scalar_pointer_mode(ScalarPointerMode mode) {
+  scalar_pointer_mode_ = mode;
+}
+
+/// Gets the last operation
+Operation const *Handle::get_last_operation() const {
+  return last_operation_;
+}
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
-template <typename Attention>
-class TestbedAttention {
-public:
-
-  //
-  // Type definitions
-  //
-
-  using ElementQ = typename Attention::ElementQ;
-  using ElementK = typename Attention::ElementK;
-  using ElementP = typename Attention::ElementP;
-  using ElementAccumulator = typename Attention::GemmGrouped0::ElementAccumulator;
-  using ElementV = typename Attention::ElementV;
-  using ElementO = typename Attention::ElementOutput;
-
-  using EpilogueOutputOp = typename Attention::GemmGrouped0::GemmKernel::EpilogueVisitor::ElementwiseFunctor;
-  using ElementCompute = typename EpilogueOutputOp::ElementCompute;
-
-  using ElementNorm = typename Attention::ElementNorm;
-  using ElementSum = typename Attention::ElementSum;
-  using ElementSoftmaxCompute = typename Attention::ElementSoftmaxCompute;
-
-  using LayoutQ = typename Attention::LayoutQ;
-  using LayoutK = typename Attention::LayoutK;
-  using LayoutP = typename Attention::LayoutP;
-  using LayoutV = typename Attention::LayoutV;
-  using LayoutO = typename Attention::LayoutO;
-
-  using MatrixCoord = typename LayoutP::TensorCoord;
-
-  using ProblemVisitor0 = typename Attention::GemmKernel0::ProblemVisitor;
-  using ProblemVisitor1 = typename Attention::GemmKernel1::ProblemVisitor;
-
-private:
-
-  //
-  // Data members
-  //
-
-  Options & options;
-
-  /// Initialization
-  cutlass::Distribution::Kind init_Q;
-  cutlass::Distribution::Kind init_K;
-  cutlass::Distribution::Kind init_P;
-  cutlass::Distribution::Kind init_V;
-  cutlass::Distribution::Kind init_O;
-  uint32_t seed;
-
-  cutlass::DeviceAllocation<cutlass::gemm::GemmCoord> problem_sizes_device0;
-  cutlass::DeviceAllocation<cutlass::gemm::GemmCoord> problem_sizes_device1;
-  cutlass::DeviceAllocation<cutlass::gemm::GemmCoord> problem_sizes_device0_real;
-
-  std::vector<int64_t> offset_Q;
-  std::vector<int64_t> offset_K;
-  std::vector<int64_t> offset_P;
-  std::vector<int64_t> offset_V;
-  std::vector<int64_t> offset_O;
-  std::vector<int64_t> offset_Norm;
-  std::vector<int64_t> offset_Sum;
-
-  std::vector<int64_t> ldq_host;
-  std::vector<int64_t> ldk_host;
-  std::vector<int64_t> ldp_host;
-  std::vector<int64_t> ldv_host;
-  std::vector<int64_t> ldo_host;
-  std::vector<int64_t> seqlen_host;
-
-  cutlass::DeviceAllocation<int64_t> ldq;
-  cutlass::DeviceAllocation<int64_t> ldk;
-  cutlass::DeviceAllocation<int64_t> ldp;
-  cutlass::DeviceAllocation<int64_t> ldv;
-  cutlass::DeviceAllocation<int64_t> ldo;
-  cutlass::DeviceAllocation<int64_t> seqlen;
-
-  cutlass::DeviceAllocation<ElementQ> block_Q;
-  cutlass::DeviceAllocation<ElementK> block_K;
-  cutlass::DeviceAllocation<ElementP> block_P;
-  cutlass::DeviceAllocation<ElementV> block_V;
-  cutlass::DeviceAllocation<ElementO> block_O;
-  cutlass::DeviceAllocation<ElementNorm> block_Norm;
-  cutlass::DeviceAllocation<ElementSum> block_Sum;
-
-  cutlass::DeviceAllocation<int64_t> offset_P_Device;
-  cutlass::DeviceAllocation<int64_t> offset_Norm_Device;
-  cutlass::DeviceAllocation<int64_t> offset_Sum_Device;
-
-  cutlass::DeviceAllocation<ElementQ *> ptr_Q;
-  cutlass::DeviceAllocation<ElementK *> ptr_K;
-  cutlass::DeviceAllocation<ElementP *> ptr_P;
-  cutlass::DeviceAllocation<ElementV *> ptr_V;
-  cutlass::DeviceAllocation<ElementO *> ptr_O;
-  cutlass::DeviceAllocation<ElementNorm *> ptr_Max;
-  cutlass::DeviceAllocation<ElementSum *> ptr_Sum;
-
-public:
-
-  //
-  // Methods
-  //
-
-  TestbedAttention(
-    Options &options_,
-    cutlass::Distribution::Kind init_Q_ = cutlass::Distribution::Uniform,
-    cutlass::Distribution::Kind init_K_ = cutlass::Distribution::Uniform,
-    cutlass::Distribution::Kind init_P_ = cutlass::Distribution::Uniform,
-    cutlass::Distribution::Kind init_V_ = cutlass::Distribution::Uniform,
-    cutlass::Distribution::Kind init_O_ = cutlass::Distribution::Uniform,
-    uint32_t seed_ = 3080
-  ):
-    options(options_), init_Q(init_Q_), init_K(init_K_), init_P(init_P_), init_V(init_V_), init_O(init_O_), seed(seed_) { }
-
-  int problem_count() const {
-    return (options.head_number * options.batch_size);
-  }
-
-private:
-
-  /// Helper to initialize a tensor view
-  template <typename Element>
-  void initialize_tensor_(
-    Element *ptr,
-    size_t capacity, 
-    cutlass::Distribution::Kind dist_kind,
-    uint32_t seed) {
-
-    if (dist_kind == cutlass::Distribution::Uniform) {
-
-      Element scope_max, scope_min;
-      int bits_input = cutlass::sizeof_bits<Element>::value;
-      int bits_output = cutlass::sizeof_bits<typename Attention::ElementP>::value;
-
-      if (bits_input == 1) {
-        scope_max = 2;
-        scope_min = 0;
-      } else if (bits_input <= 8) {
-        scope_max = 2;
-        scope_min = -2;
-      } else if (bits_output == 16) {
-        scope_max = 8;
-        scope_min = -8;
-      } else {
-        scope_max = 8;
-        scope_min = -8;
-      }
+/// Returns the maximum required alignment for each operator
+static int maximum_alignment_requirement(GemmDescription const &desc) {
+  return std::max(
+    std::max(desc.A.alignment, desc.B.alignment), desc.C.alignment);
+}
 
-      cutlass::reference::device::BlockFillRandomUniform(
-        ptr, capacity, seed, scope_max, scope_min, 0);
-    } 
-    else if (dist_kind == cutlass::Distribution::Gaussian) {
+/// Returns the largest alignment (in units of elements) the problem satisfies, starting from a
+/// given upper limit.
+static int gemm_problem_alignment(
+  int M,
+  int N,
+  int K,
+  NumericTypeID element_A,
+  void const *ptr_A,
+  int64_t lda,
+  int64_t batch_stride_A,
+  NumericTypeID element_B,
+  void const *ptr_B,
+  int64_t ldb,
+  int64_t batch_stride_B,
+  NumericTypeID element_C,
+  void const * ptr_C,
+  int64_t ldc,
+  int64_t batch_stride_C,
+  void const * ptr_D,
+  int64_t ldd,
+  int64_t batch_stride_D,
+  int max_alignment_in_bytes = 16
+) {
+
+  void const *pointers[] = {
+    ptr_A, ptr_B, ptr_C, ptr_D
+  };
+
+  int64_t extents[] = {
+    M, N, K, lda, ldb, ldc, ldd, batch_stride_A, batch_stride_B, batch_stride_C, batch_stride_D
+  };
+
+  NumericTypeID elements[] = {
+    element_A, element_B, element_C
+  };
 
-      cutlass::reference::device::BlockFillRandomGaussian(
-        ptr, capacity, seed, Element(), Element(0.5f));
-    }
-    else if (dist_kind == cutlass::Distribution::Sequential) {
+  for (; max_alignment_in_bytes > 0; max_alignment_in_bytes /= 2) {
+    
+    bool satisfied = true;
 
-      // Fill with increasing elements
-      cutlass::reference::device::BlockFillSequential(
-        ptr, capacity, Element(1), Element());
-    } 
-    else {
-
-      // Fill with all 1s
-      cutlass::reference::device::BlockFillSequential(
-        ptr, capacity, Element(), Element(1));
+    // Can pointers satisfy this?
+    for (void const *ptr : pointers) {
+      std::uintptr_t int_ptr = reinterpret_cast<std::uintptr_t>(ptr);
+
+      if (int_ptr % max_alignment_in_bytes) {
+        satisfied = false;
+        break;
+      }
     }
-  }
 
-  /// Initializes data structures
-  void initialize_() {
+    if (!satisfied) {
+      continue;
+    }
 
-    //
-    // Set scalors for the mha example
-    //
-
-    options.alpha0 = 1.0f / sqrt(float(options.head_size));
-    options.alpha1 = 1.0f;
-    options.beta = 0;
-
-    //
-    // Choose random problem sizes
-    //
-
-    // construct a few problems of random sizes
-    srand(seed);
-
-    int64_t total_elements_Q = 0;
-    int64_t total_elements_K = 0;
-    int64_t total_elements_P = 0;
-    int64_t total_elements_V = 0;
-    int64_t total_elements_O = 0;
-
-    int64_t total_elements_partial_norm = 0;
-
-    ldq_host.resize(problem_count());
-    ldk_host.resize(problem_count());
-    ldp_host.resize(problem_count());
-    ldv_host.resize(problem_count());
-    ldo_host.resize(problem_count());
-    seqlen_host.resize(problem_count());
-
-    for (int32_t i = 0; i < problem_count(); ++i) {
-
-      auto problem = options.problem_sizes0.at(i);
-
-      ldq_host.at(i) = LayoutQ::packed({problem.m(), problem.k()}).stride(0);
-      ldk_host.at(i) = LayoutK::packed({problem.k(), problem.n()}).stride(0);
-      ldp_host.at(i) = LayoutP::packed({problem.m(), problem.n()}).stride(0);
-      ldv_host.at(i) = LayoutV::packed({problem.n(), problem.k()}).stride(0);
-      ldo_host.at(i) = LayoutO::packed({problem.m(), problem.k()}).stride(0);
-
-      // m = n for attention problems.
-      int64_t non_leading_dim = ldp_host.at(i);
-      int64_t threadblock_n = Attention::GemmGrouped0::GemmKernel::EpilogueVisitor::ThreadblockShape::kN;
-      int64_t threadblock_num = (ldp_host.at(i) + threadblock_n - 1) / threadblock_n;
-
-      seqlen_host.at(i) = problem.m();
-
-      offset_Q.push_back(total_elements_Q);
-      offset_K.push_back(total_elements_K);
-      offset_P.push_back(total_elements_P);
-      offset_V.push_back(total_elements_V);
-      offset_O.push_back(total_elements_O);
-      offset_Norm.push_back(total_elements_partial_norm);
-      offset_Sum.push_back(total_elements_partial_norm);
-
-      int64_t elements_Q = problem.m() * problem.k();
-      int64_t elements_K = problem.k() * problem.n();
-      int64_t elements_P = problem.m() * problem.n();
-      int64_t elements_V = problem.n() * problem.k();
-      int64_t elements_O = problem.m() * problem.k();
-      int64_t elements_norm = non_leading_dim * threadblock_num;
-
-      total_elements_Q += elements_Q;
-      total_elements_K += elements_K;
-      total_elements_P += elements_P;
-      total_elements_V += elements_V;
-      total_elements_O += elements_O;
-      total_elements_partial_norm += elements_norm;
+    // Compute the maximum alignment based on element data types
+    int max_element_alignment = 0;
 
+    for (NumericTypeID type_id : elements) {
+      int element_alignment = max_alignment_in_bytes * 8 / library::sizeof_bits(type_id); 
+      max_element_alignment = std::max(max_element_alignment, element_alignment);
     }
 
-    problem_sizes_device0.reset(problem_count());
-    problem_sizes_device1.reset(problem_count());
-    problem_sizes_device0.copy_from_host(options.problem_sizes0.data());
-    problem_sizes_device1.copy_from_host(options.problem_sizes1.data());
-
-    if (options.use_mask) {
-      problem_sizes_device0_real.reset(problem_count());
-      problem_sizes_device0_real.copy_from_host(options.problem_sizes0_real.data());
+    // Can the problem size and leading dimensions satisfy this?
+    for (int64_t extent : extents) {
+      if (extent % max_element_alignment) {
+        satisfied = false;
+        break;
+      }
     }
 
-    ldq.reset(problem_count());
-    ldk.reset(problem_count());
-    ldp.reset(problem_count());
-    ldv.reset(problem_count());
-    ldo.reset(problem_count());
-    seqlen.reset(problem_count());
-
-    ldq.copy_from_host(ldq_host.data());
-    ldk.copy_from_host(ldk_host.data());
-    ldp.copy_from_host(ldp_host.data());
-    ldv.copy_from_host(ldv_host.data());
-    ldo.copy_from_host(ldo_host.data());
-    seqlen.copy_from_host(seqlen_host.data());
-
-    //
-    // Assign pointers
-    //
-
-    block_Q.reset(total_elements_Q);
-    block_K.reset(total_elements_K);
-    block_P.reset(total_elements_P);
-    block_V.reset(total_elements_V);
-    block_O.reset(total_elements_O);
-    block_Norm.reset(total_elements_partial_norm);
-    block_Sum.reset(total_elements_partial_norm);
-
-    offset_P_Device.reset(problem_count());
-    offset_Norm_Device.reset(problem_count());
-    offset_Sum_Device.reset(problem_count());
-
-    // sync offset with device
-    cutlass::device_memory::copy_to_device(offset_P_Device.get(), offset_P.data(), offset_P.size());
-    cutlass::device_memory::copy_to_device(offset_Norm_Device.get(), offset_Norm.data(), offset_Norm.size());
-    cutlass::device_memory::copy_to_device(offset_Sum_Device.get(), offset_Sum.data(), offset_Sum.size());
-
-    std::vector<ElementQ *> ptr_Q_host(problem_count());
-    std::vector<ElementK *> ptr_K_host(problem_count());
-    std::vector<ElementP *> ptr_P_host(problem_count());
-    std::vector<ElementV *> ptr_V_host(problem_count());
-    std::vector<ElementO *> ptr_O_host(problem_count());
-    std::vector<ElementNorm *> ptr_norm_host(problem_count());
-    std::vector<ElementSum *> ptr_sum_host(problem_count());
-
-    for (int32_t i = 0; i < problem_count(); ++i) {
-      ptr_Q_host.at(i) = block_Q.get() + offset_Q.at(i);
-      ptr_K_host.at(i) = block_K.get() + offset_K.at(i);
-      ptr_P_host.at(i) = block_P.get() + offset_P.at(i);
-      ptr_V_host.at(i) = block_V.get() + offset_V.at(i);
-      ptr_O_host.at(i) = block_O.get() + offset_O.at(i);
-      ptr_norm_host.at(i) = block_Norm.get() + offset_Norm.at(i);
-      ptr_sum_host.at(i) = block_Sum.get() + offset_Sum.at(i);
+    if (!satisfied) {
+      continue;
     }
 
-    ptr_Q.reset(problem_count());
-    ptr_Q.copy_from_host(ptr_Q_host.data());
-    
-    ptr_K.reset(problem_count());
-    ptr_K.copy_from_host(ptr_K_host.data());
-    
-    ptr_P.reset(problem_count());
-    ptr_P.copy_from_host(ptr_P_host.data());
+    // Yes
+    return max_element_alignment;
+  }
 
-    ptr_V.reset(problem_count());
-    ptr_V.copy_from_host(ptr_V_host.data());
+  // No alignment satisfies this problem
+  return 0;
+}
 
-    ptr_O.reset(problem_count());
-    ptr_O.copy_from_host(ptr_O_host.data());
+/// Find the best kernel in descending order of preference.
+static Operation const * find_gemm_operation(
+  GemmOperationFunctionalMap::const_iterator operators_it, 
+  GemmPreferenceKey const preference_key) {
 
-    ptr_Max.reset(problem_count());
-    ptr_Max.copy_from_host(ptr_norm_host.data());
-
-    ptr_Sum.reset(problem_count());
-    ptr_Sum.copy_from_host(ptr_sum_host.data());
-
-    //
-    // Initialize the problems of the workspace
-    //
-
-    initialize_tensor_(block_Q.get(), total_elements_Q, init_Q, seed + 1);
-    initialize_tensor_(block_K.get(), total_elements_K, init_K, seed + 2);
-    initialize_tensor_(block_V.get(), total_elements_V, init_V, seed + 3);
+  auto cc_it = operators_it->second.upper_bound(preference_key);
 
+  if (cc_it == operators_it->second.begin()) {
+    return nullptr;
   }
 
-  template<typename Element>
-  bool verify_tensor_(std::vector<Element> vector_Input, \
-                       std::vector<Element> vector_Input_Ref,
-                       int64_t verify_length = -1) {
-
-    int64_t size = (vector_Input.size() < vector_Input_Ref.size()) ? vector_Input.size() : vector_Input_Ref.size();
-    size = (verify_length == -1) ? size : verify_length;
-
-    // 0.05 for absolute error
-    float abs_tol = 5e-2f;
-    // 10% for relative error
-    float rel_tol = 1e-1f;
-    for (int64_t i = 0; i < size; ++i) {
-      float diff = (float)(vector_Input.at(i) - vector_Input_Ref.at(i));
-      float abs_diff = fabs(diff);
-      float abs_ref = fabs((float)vector_Input_Ref.at(i) + 1e-5f);
-      float relative_diff = abs_diff / abs_ref;
-      if ( (isnan(abs_diff) || isinf(abs_diff)) ||  (abs_diff > abs_tol && relative_diff > rel_tol)) {
-        printf("diff = %f, rel_diff = %f, {%f, %f}.\n", abs_diff, relative_diff, (float)(vector_Input.at(i)), (float)(vector_Input_Ref.at(i)));
-        return false;
-      }
+  Operation const *operation = nullptr;
 
-    }
+  // Search in descending order of compute capability
+  do {
+    --cc_it;
 
-    return true;
-  }
+    // Search tile sizes in order, for now.
+    for (auto const * op : cc_it->second) {
 
-  /// Verifies the result is a GEMM
-  bool verify_() {
+      GemmDescription const &desc = static_cast<GemmDescription const &>(op->description());
 
-    bool passed = true;
+      int min_cc = desc.tile_description.minimum_compute_capability;
+      int max_cc = desc.tile_description.maximum_compute_capability;
 
-    for (int32_t i = 0; i < problem_count(); ++i) {
-      cutlass::gemm::GemmCoord problem = options.problem_sizes0.at(i);
-      cutlass::gemm::GemmCoord problem1 = options.problem_sizes1.at(i);
-
-      LayoutQ layout_Q(ldq_host.at(i));
-      LayoutK layout_K(ldk_host.at(i));
-      LayoutP layout_P(ldp_host.at(i));
-      LayoutV layout_V(ldv_host.at(i));
-      LayoutO layout_O(ldo_host.at(i));
-
-      MatrixCoord extent_Q{problem.m(), problem.k()};
-      MatrixCoord extent_K{problem.k(), problem.n()};
-      MatrixCoord extent_P{problem.m(), problem.n()};
-      MatrixCoord extent_V{problem.n(), problem.k()};
-      MatrixCoord extent_O{problem.m(), problem.k()};
-
-      cutlass::TensorView<ElementQ, LayoutQ> view_Q(block_Q.get() + offset_Q.at(i), layout_Q, extent_Q);
-      cutlass::TensorView<ElementK, LayoutK> view_K(block_K.get() + offset_K.at(i), layout_K, extent_K);
-      cutlass::TensorView<ElementP, LayoutP> view_P(block_P.get() + offset_P.at(i), layout_P, extent_P);
-      cutlass::TensorView<ElementV, LayoutV> view_V(block_V.get() + offset_V.at(i), layout_V, extent_V);
-
-      cutlass::DeviceAllocation<ElementP>    block_Ref(layout_P.capacity(extent_P));
-      cutlass::TensorView<ElementP, LayoutP> view_Ref_device(block_Ref.get(), layout_P, extent_P);
-
-      cutlass::DeviceAllocation<ElementO>    block_Ref_O(layout_O.capacity(extent_O));
-      cutlass::TensorView<ElementO, LayoutO> view_Ref_O_device(block_Ref_O.get(), layout_O, extent_O);
-
-      // Reference GEMM
-      cutlass::reference::device::GemmComplex<
-          ElementQ, LayoutQ,
-          ElementK, LayoutK,
-          ElementP, LayoutP, 
-          ElementCompute, ElementAccumulator
-      >(
-        problem,
-        ElementAccumulator(options.alpha0), 
-        view_Q,
-        Attention::GemmGrouped0::kTransformA,
-        view_K,
-        Attention::GemmGrouped0::kTransformB,
-        ElementAccumulator(options.beta), 
-        view_P, 
-        view_Ref_device, 
-        ElementAccumulator(0)
-      );
-
-      // Compute softmax for P. We need to explicitly compute softmax
-      // over P because softmax is fused to the second GEMM in the
-      // profiled implementation.
-      std::vector<ElementP> matrix_Ref(layout_P.capacity(extent_P));
-      cutlass::device_memory::copy_to_host(matrix_Ref.data(), block_Ref.get(), matrix_Ref.size());
-      cutlass::TensorView<ElementP, LayoutP> view_Ref_host(matrix_Ref.data(), layout_P, extent_P);
-      std::vector<ElementNorm> vector_Norm_Ref(problem.m());
-      std::vector<ElementSum> vector_Sum_Ref(problem.m());
-
-      int n_dim = options.use_mask ? options.problem_sizes0_real.at(i).n() : problem.n();
-
-      // Compute softmax for referece matrix
-      // Assumed a row-major storage
-      for (int m = 0; m < problem.m(); m++) {
-        ElementSoftmaxCompute max = ElementSoftmaxCompute(view_Ref_host.ref().at({m, 0}));
-        for (int n = 1; n < n_dim; n++) {
-           max = std::max(max, ElementSoftmaxCompute(view_Ref_host.ref().at({m, n})));
-        }
-
-        vector_Norm_Ref.at(m) = ElementNorm(max);
-
-        ElementSoftmaxCompute sum = ElementSoftmaxCompute();
-        for (int n = 0; n < n_dim; n++) {
-          sum += std::exp( ElementSoftmaxCompute(view_Ref_host.ref().at({m, n})) - max );
-        }
-        ElementSoftmaxCompute inv_sum = ElementSoftmaxCompute(1.0f / sum);
-
-        vector_Sum_Ref.at(m) = ElementSum(inv_sum);
-
-        for (int n = 0; n < n_dim; n++) {
-          view_Ref_host.ref().at({m, n}) = ElementP(
-            std::exp( ElementSoftmaxCompute(view_Ref_host.ref().at({m, n})) - max ) * inv_sum
-          );
-        }
+      int op_alignment = maximum_alignment_requirement(desc);
 
-      }
+      if ((min_cc <= preference_key.compute_capability) &&
+        (preference_key.compute_capability <= max_cc) &&
+        (op_alignment <= preference_key.alignment)) {
 
-      // when not using mask, problem_real and problem share the same sizes
-      if (options.use_mask) {
-        for (int m = 0; m < problem.m(); m++) {
-          for (int n = n_dim; n < problem.n(); n++) {
-            view_Ref_host.ref().at({m, n}) = ElementP(0);
-          }
-        }
+        operation = op;
+        break;
       }
+    }
+  } while (!operation && cc_it != operators_it->second.begin());
 
-      cutlass::device_memory::copy_to_device(block_P.get() + offset_P.at(i), matrix_Ref.data(), matrix_Ref.size());
+  return operation;
+}
 
-      // Reference GEMM
-      cutlass::reference::device::GemmComplex<
-          ElementP, LayoutP,
-          ElementV, LayoutV,
-          ElementO, LayoutO, 
-          ElementCompute, ElementAccumulator
-      >(
-        problem1,
-        ElementAccumulator(options.alpha1), 
-        view_P,
-        Attention::GemmGrouped0::kTransformA,
-        view_V,
-        Attention::GemmGrouped0::kTransformB,
-        ElementAccumulator(options.beta), 
-        view_Ref_O_device, 
-        view_Ref_O_device, 
-        ElementAccumulator(0)
-      );
-
-      // Copy to host memory
-
-      int64_t threadblock_n = Attention::GemmGrouped0::GemmKernel::EpilogueVisitor::ThreadblockShape::kN;
-      int64_t threadblock_num = (problem.m() + threadblock_n - 1) / threadblock_n;
-
-      std::vector<ElementNorm> vector_Norm(problem.m() * threadblock_num);
-      std::vector<ElementSum> vector_Sum(problem.m() * threadblock_num);
-
-      cutlass::device_memory::copy_to_host(vector_Norm.data(),   block_Norm.get() + offset_Norm.at(i), vector_Norm.size());
-      cutlass::device_memory::copy_to_host(vector_Sum.data(),   block_Sum.get() + offset_Sum.at(i), vector_Sum.size());
-
-      cutlass::TensorView<ElementP, LayoutP> view_Ref(matrix_Ref.data(), layout_P, extent_P);
-
-      std::vector<ElementO> matrix_O(layout_O.capacity(extent_O));
-      cutlass::device_memory::copy_to_host(matrix_O.data(),   block_O.get() + offset_O.at(i), matrix_O.size());
-      std::vector<ElementP> matrix_Ref_O(layout_O.capacity(extent_O));
-      cutlass::device_memory::copy_to_host(matrix_Ref_O.data(), block_Ref_O.get(), matrix_Ref_O.size());
-
-      bool verified_N = false;
-      bool verified_S = false;
-      bool verified_O = false;
+///////////////////////////////////////////////////////////////////////////////////////////////////
 
-      if (!verified_N) {
-        verified_N = verify_tensor_<ElementNorm>(vector_Norm, vector_Norm_Ref);
-      }
-      
-      if (!verified_S) {
-        verified_S = verify_tensor_<ElementSum>(vector_Sum, vector_Sum_Ref);
-      }
+/// Executes a GEMM computation: D <= alpha * A*B + beta * C
+Status Handle::gemm(
 
+  int M,                                    /// GEMM M dimension
+  int N,                                    /// GEMM N dimension
+  int K,                                    /// GEMM K dimension
 
-      if (!verified_O) {
-        verified_O = verify_tensor_<ElementO>(matrix_O, matrix_Ref_O);
-      }
+  NumericTypeID element_compute,            /// Data type of internal accumulation
 
-      passed = passed && verified_N && verified_S && verified_O;
+  NumericTypeID element_scalar,             /// Data type of alpha/beta scalars
 
-      if (!passed) {
-        std::cerr << "\n***\nError - problem " << i << " failed the QA check\n***\n" << std::endl;
+  void const *alpha,                        /// Pointer to alpha scalar
 
-        if (!verified_O) {
-          std::cout << "Final matrix output is incorrect" << std::endl;
-        }
+  NumericTypeID element_A,                  /// Data type of A matrix elements
+  LayoutTypeID layout_A,                    /// Layout of A matrix
+  ComplexTransform transform_A,             /// Complex transformation applied to A matrix - ignored for real-valued matrices
 
-        if (!verified_N) {
-          std::cout << "Max is incorrect" << std::endl;
-        }
+  void const * ptr_A,                       /// Pointer to A matrix in Global Memory
+  int64_t lda,                              /// Leading dimension of A matrix
 
-        if (!verified_S) {
-          std::cout << "Sum is incorrect" << std::endl;
-        }
+  NumericTypeID element_B,                  /// Data type of B matrix elements
+  LayoutTypeID layout_B,                    /// Layout of B matrix
+  ComplexTransform transform_B,             /// Complex transformation applied to B matrix - ignored for real-valued matrices
 
-        return passed;
-      }
+  void const * ptr_B,                       /// Pointer to B matrix in Global Memory
+  int64_t ldb,                              /// Leading dimension of B matrix
 
-    }
+  void const * beta,                        /// Pointer to beta scalar
 
-    return passed;
+  NumericTypeID element_C,                  /// Data type of C and D matrices
+
+  void const * ptr_C,                       /// Pointer to C matrix
+  int64_t ldc,                              /// Leading dimension of C matrix
+
+  void * ptr_D,                             /// Pointer to D matrix
+  int64_t ldd                               /// Leading dimension of D matrix
+) {
+  
+  //
+  // Find the operation
+  //
+
+  GemmFunctionalKey key(
+    provider_,
+    GemmKind::kGemm,
+    element_compute,
+    element_scalar,
+    element_A,
+    layout_A,
+    transform_A,
+    element_B,
+    layout_B,
+    transform_B,
+    element_C
+  );
+
+  auto operators_it = Singleton::get().operation_table.gemm_operations.find(key);
+
+  if (operators_it == Singleton::get().operation_table.gemm_operations.end()) {
+    return cutlass::Status::kErrorNotSupported;
+  }
+  
+  if (operators_it->second.empty()) {
+    return cutlass::Status::kErrorNotSupported;
   }
 
-public:
+  //
+  // Compute the largest alignment restriction the kernel can satisfy.
+  //
 
-  /// Returns the number of threadblocks to launch if the kernel can run on the target
-  /// device. Otherwise, returns zero.
-  int sufficient() const {
-    cudaDeviceProp properties;
-    int device_idx;
-    cudaError_t result = cudaGetDevice(&device_idx);
+  // Maximum alignment expectation among all kernels (in units of bytes)
+  int const kMaximumAlignmentSize = 16;
 
-    if (result != cudaSuccess) {
-      throw std::runtime_error("cudaGetDevice() API call failed.");
-    }
+  int alignment = gemm_problem_alignment(
+    M, N, K, 
+    element_A, ptr_A, lda, 0,
+    element_B, ptr_B, ldb, 0,
+    element_C, ptr_C, ldc, 0,
+    ptr_D, ldd, 0, kMaximumAlignmentSize
+  );
 
-    result = cudaGetDeviceProperties(&properties, device_idx);
+  //
+  // Find the best kernel in descending order of preference.
+  //
 
-    if (result != cudaSuccess) {
-      throw std::runtime_error("cudaGetDeviceProperties() failed");
-    }
+  GemmPreferenceKey preference_key(compute_capability(), alignment);
 
-    int occupancy = Attention::GemmGrouped0::maximum_active_blocks();
+  Operation const *operation = find_gemm_operation(operators_it, preference_key);
 
-    return properties.multiProcessorCount * occupancy;
+  if (!operation) {
+    return cutlass::Status::kErrorNotSupported;
+  }
 
+  last_operation_ = operation;
+
+  //
+  // Configure operation
+  //
+
+  GemmConfiguration configuration{
+    {M, N, K},
+    lda,
+    ldb,
+    ldc,
+    ldd,
+    1
+  };
+
+  // Query host work space size
+  uint64_t host_workspace_size_needed = operation->get_host_workspace_size(&configuration);
+
+  if (uint64_t(kHostWorkspaceSize) < host_workspace_size_needed) {
+    return cutlass::Status::kErrorNotSupported;
   }
 
+  char host_workspace[kHostWorkspaceSize];
 
-  /// Executes a CUTLASS Attention kernel and measures runtime.
-  Result profile_grouped() {
+  // Query device workspace size
+  uint64_t device_workspace_size_needed = operation->get_device_workspace_size(&configuration);
 
-    Result result;
+  if (uint64_t(workspace_size_) < device_workspace_size_needed) {
+    return cutlass::Status::kErrorNotSupported;
+  }
 
-    int threadblock_count = sufficient();
+  // Initialize host and device workspaces
+  Status status = operation->initialize(
+    &configuration,
+    host_workspace,
+    workspace_,
+    stream_);
 
-    // Early exit
-    if (!threadblock_count) {
-      std::cout << "Active CUDA device lacks hardware resources to run CUTLASS Attention kernel." << std::endl;
-      return result;
-    }
+  if (status != cutlass::Status::kSuccess) {
+    return status;
+  }
 
-    result.passed = false;
+  // Run the operator
+  GemmArguments arguments{
+    ptr_A,
+    ptr_B,
+    ptr_C,
+    ptr_D,
+    alpha,
+    beta,
+    scalar_pointer_mode_
+  };
 
-    // Initialize the problem
-    initialize_();
+  return operation->run(&arguments, host_workspace, workspace_, stream_);
+}
 
-    typename Attention::Arguments args(
-      problem_sizes_device0.get(),
-      problem_sizes_device1.get(),
-      problem_count(),
-      threadblock_count,
-      ptr_Q.get(),
-      ptr_K.get(),
-      ptr_P.get(),
-      ptr_V.get(),
-      ptr_O.get(),
-      ptr_Max.get(),
-      ptr_Sum.get(),
-      block_P.get(),
-      block_Norm.get(),
-      block_Sum.get(),
-      offset_P_Device.get(),
-      offset_Norm_Device.get(),
-      offset_Sum_Device.get(),
-      ldq.get(),
-      ldk.get(),
-      ldp.get(),
-      ldv.get(),
-      ldo.get(),
-      ElementAccumulator(options.alpha0),
-      ElementAccumulator(options.alpha1),
-      ElementAccumulator(options.beta),
-      options.head_number,
-      options.batch_size,
-      options.seq_length,
-      options.problem_sizes0.data(),
-      options.problem_sizes1.data(),
-      problem_sizes_device0_real.get()
-    );
-
-    size_t workspace_size0 = ProblemVisitor0::kRequiresPrecomputation ?\
-      ProblemVisitor0::get_workspace_size(options.problem_sizes0.data(),\
-                                          problem_count(),\
-                                          threadblock_count)\
-      : 0;
-
-    size_t workspace_size1 = ProblemVisitor1::kRequiresPrecomputation ?\
-      ProblemVisitor1::get_workspace_size(options.problem_sizes1.data(),\
-                                          problem_count(),\
-                                          threadblock_count)\
-      : 0;
-
-    cutlass::DeviceAllocation<uint8_t> workspace0(workspace_size0);
-    cutlass::DeviceAllocation<uint8_t> workspace1(workspace_size1);
-
-    Attention attention;
-
-    result.status = attention.initialize(args, workspace0.get(), workspace1.get());
-
-    if (result.status != cutlass::Status::kSuccess) {
-      std::cerr << "Failed to initialize CUTLASS Attention kernel." << std::endl;
-      return result;
-    }
+///////////////////////////////////////////////////////////////////////////////////////////////////
 
-    result.status = attention.run();
+/// Executes a GEMM computation: D <= alpha * A*B + beta * C.
+//
+// Supports batched-strided, batched array or split-K serial or split-K parallel.
+//
+Status Handle::gemm_universal(
 
-    if (result.status != cutlass::Status::kSuccess) {
-      std::cerr << "Failed to initialize CUTLASS Attention kernel." << std::endl;
-      return result;
-    }
+  GemmUniversalMode mode,                   /// indicates the mode in which the kUniversal GEMM is launched
 
-    // Wait for completion
-    result.error = cudaDeviceSynchronize();
+  int M,                                    /// GEMM M dimension
+  int N,                                    /// GEMM N dimension
+  int K,                                    /// GEMM K dimension
 
-    if (result.error != cudaSuccess)  {
-      std::cerr << "Kernel execution error: " << cudaGetErrorString(result.error);
-      return result;
-    }
+  NumericTypeID element_compute,            /// Data type of internal accumulation
 
-    //
-    // Verify correctness
-    //
-    result.passed = true;
+  NumericTypeID element_scalar,             /// Data type of alpha/beta scalars
 
-    if (options.reference_check) {
-      result.passed = verify_();
-    }
+  void const *alpha,                        /// Pointer to alpha scalar
 
-    //
-    // Warm-up run of the grouped GEMM object
-    //
-
-    result.status = attention.run();
-
-    if (result.status != cutlass::Status::kSuccess) {
-      std::cerr << "Failed to run CUTLASS Attention kernel." << std::endl;
-      return result;
-    }
+  NumericTypeID element_A,                  /// Data type of A matrix elements
+  LayoutTypeID layout_A,                    /// Layout of A matrix
+  ComplexTransform transform_A,             /// Complex transformation applied to A matrix - ignored for real-valued matrices
 
-    //
-    // Construct events
-    //
-
-    cudaEvent_t events[2];
-
-    for (auto & event : events) {
-      result.error = cudaEventCreate(&event);
-      if (result.error != cudaSuccess) {
-        std::cerr << "cudaEventCreate() failed: " << cudaGetErrorString(result.error) << std::endl;
-        return -1;
-      }
-    }
+  void const * ptr_A,                       /// Pointer to A matrix in Global Memory
+  int64_t lda,                                  /// Leading dimension of A matrix
 
-    // Record an event at the start of a series of GEMM operations
-    result.error = cudaEventRecord(events[0]);
-    if (result.error != cudaSuccess) {
-      std::cerr << "cudaEventRecord() failed: " << cudaGetErrorString(result.error) << std::endl;
-      return result;
-    }
+  NumericTypeID element_B,                  /// Data type of B matrix elements
+  LayoutTypeID layout_B,                    /// Layout of B matrix
+  ComplexTransform transform_B,             /// Complex transformation applied to B matrix - ignored for real-valued matrices
 
-    //
-    // Run profiling loop
-    //
+  void const * ptr_B,                       /// Pointer to B matrix in Global Memory
+  int64_t ldb,                                  /// Leading dimension of B matrix
 
-    for (int iter = 0; iter < options.iterations; ++iter) {
-      attention();
-    }
+  void const * beta,                        /// Pointer to beta scalar
 
-    //
-    // Stop profiling loop
-    //
-
-    // Record an event when the GEMM operations have been launched.
-    result.error = cudaEventRecord(events[1]);
-    if (result.error != cudaSuccess) {
-      std::cerr << "cudaEventRecord() failed: " << cudaGetErrorString(result.error) << std::endl;
-      return result;
-    }
+  NumericTypeID element_C,                  /// Data type of C and D matrices
 
-    // Wait for work on the device to complete.
-    result.error = cudaEventSynchronize(events[1]);
-    if (result.error != cudaSuccess) {
-      std::cerr << "cudaEventSynchronize() failed: " << cudaGetErrorString(result.error) << std::endl;
-      return result;
-    }
+  void const * ptr_C,                       /// Pointer to C matrix
+  int64_t ldc,                                  /// Leading dimension of C matrix
 
-    // Measure elapsed runtime
-    float runtime_ms = 0;
-    result.error = cudaEventElapsedTime(&runtime_ms, events[0], events[1]);
-    if (result.error != cudaSuccess) {
-      std::cerr << "cudaEventElapsed() failed: " << cudaGetErrorString(result.error) << std::endl;
-      return result;
-    }
+  void * ptr_D,                             /// Pointer to D matrix
+  int64_t ldd,                                  /// Leading dimension of D matrix
 
-    // Compute average runtime and GFLOPs.
-    result.runtime_ms = double(runtime_ms) / double(options.iterations);
-    result.gflops = options.gflops(result.runtime_ms / 1000.0);
-
-    //
-    // Cleanup
-    //
+  int batch_count,                          /// Batch count or number of split-K slices
 
-    for (auto event : events) {
-      (void)cudaEventDestroy(event);
-    }
+  int64_t batch_stride_A,                   /// Batch stride of A operand
+  int64_t batch_stride_B,                   /// Batch stride of B operand
+  int64_t batch_stride_C,                   /// Batch stride of C operand
+  int64_t batch_stride_D                    /// Batch stride of D operand
+) {
+  
+  //
+  // Find the operation
+  //
 
-    std::cout << std::endl;
-    std::cout << "CUTLASS Attention:\n"
-      << "====================================================" << std::endl;
-    std::cout << "    " << " {max sequence length, head size, head number, batch size} = {" << options.seq_length \
-      << ", " << options.head_size << ", " << options.head_number << ", " << options.batch_size << "}." << std::endl;
-    std::cout << std::endl;
-    std::cout << "    " << "Runtime: " << result.runtime_ms << " ms" << std::endl;
-    std::cout << "    " << "GFLOPs: " << result.gflops << std::endl;
+  GemmFunctionalKey key(
+    provider_,
+    GemmKind::kUniversal,
+    element_compute,
+    element_scalar,
+    element_A,
+    layout_A,
+    transform_A,
+    element_B,
+    layout_B,
+    transform_B,
+    element_C
+  );
 
-    return result;
+  auto operators_it = Singleton::get().operation_table.gemm_operations.find(key);
+
+  if (operators_it == Singleton::get().operation_table.gemm_operations.end()) {
+    return cutlass::Status::kErrorNotSupported;
+  }
+  
+  if (operators_it->second.empty()) {
+    return cutlass::Status::kErrorNotSupported;
   }
 
+  //
+  // Compute the largest alignment restriction the kernel can satisfy.
+  //
 
-};
+  // Maximum alignment expectation among all kernels (in units of bytes)
+  int const kMaximumAlignmentSize = 16;
 
-///////////////////////////////////////////////////////////////////////////////////////////////////
+  void const *ptr_A_check = ptr_A;
+  void const *ptr_B_check = ptr_B;
+  void const *ptr_C_check = ptr_C;
+  void *      ptr_D_check = ptr_D;
+
+  // Ignore alignment of pointers to pointers. We can't check this from the host,
+  // as each batch index has its own pointer in device memory.
+  if (mode == GemmUniversalMode::kArray) {
+    ptr_A_check = nullptr; 
+    ptr_B_check = nullptr; 
+    ptr_C_check = nullptr; 
+    ptr_D_check = nullptr; 
+  }
 
-int main(int argc, char const **args) {
+  int alignment = gemm_problem_alignment(
+    M, N, K, 
+    element_A, ptr_A_check, lda, 0,
+    element_B, ptr_B_check, ldb, 0,
+    element_C, ptr_C_check, ldc, 0,
+    ptr_D_check, ldd, 0, kMaximumAlignmentSize
+  );
 
   //
-  // This example uses mma.sync to directly access Tensor Cores to achieve peak performance.
+  // Find the best kernel in descending order of preference.
   //
 
-  cudaDeviceProp props;
+  GemmPreferenceKey preference_key(compute_capability(), alignment);
 
-  cudaError_t error = cudaGetDeviceProperties(&props, 0);
-  if (error != cudaSuccess) {
-    std::cerr << "cudaGetDeviceProperties() returned an error: " << cudaGetErrorString(error) << std::endl;
-    return -1;
+  Operation const *operation = find_gemm_operation(operators_it, preference_key);
+
+  if (!operation) {
+    return cutlass::Status::kErrorNotSupported;
+  }
+
+  last_operation_ = operation;
+
+  //
+  // Configure operation
+  //
+
+  GemmUniversalConfiguration configuration{
+    mode,
+    {M, N, K},
+    batch_count,
+    lda,
+    ldb,
+    ldc,
+    ldd
+  };
+
+  // Query host work space size
+  uint64_t host_workspace_size_needed = operation->get_host_workspace_size(&configuration);
+
+  if (uint64_t(kHostWorkspaceSize) < host_workspace_size_needed) {
+    return cutlass::Status::kErrorNotSupported;
+  }
+
+  char host_workspace[kHostWorkspaceSize];
+
+  GemmUniversalArguments arguments{
+    ptr_A,
+    ptr_B,
+    ptr_C,
+    ptr_D,
+    alpha,
+    beta,
+    scalar_pointer_mode_,
+    batch_stride_A,
+    batch_stride_B,
+    batch_stride_C,
+    batch_stride_D
+  };
+
+  // Query device workspace size
+  uint64_t device_workspace_size_needed = operation->get_device_workspace_size(&configuration, &arguments);
+
+  if (uint64_t(workspace_size_) < device_workspace_size_needed) {
+    return cutlass::Status::kErrorNotSupported;
   }
 
-  if (__CUDACC_VER_MAJOR__ < 11 || props.major < 8) {
+  // Initialize host and device workspaces
+  Status status = operation->initialize(
+    &configuration,
+    host_workspace,
+    workspace_,
+    stream_);
+
+  if (status != cutlass::Status::kSuccess) {
+    return status;
+  }
+
+  // Run the operator
+
+  return operation->run(&arguments, host_workspace, workspace_, stream_);
+}
+
+///////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Planar complex GEMM
+Status Handle::gemm_planar_complex(
+
+  int M,                                    /// GEMM M dimension
+  int N,                                    /// GEMM N dimension
+  int K,                                    /// GEMM K dimension
+
+  NumericTypeID element_compute,            /// Data type of internal accumulation
+
+  NumericTypeID element_scalar,             /// Data type of alpha/beta scalars
+
+  void const *alpha,                        /// Pointer to alpha scalar
+
+  NumericTypeID element_A,                  /// Data type of A matrix elements
+  LayoutTypeID layout_A,                    /// Layout of A matrix
+  ComplexTransform transform_A,             /// Complex transformation applied to A matrix
+
+  void const * ptr_A_real,                  /// Pointer to real part of A matrix
+  void const * ptr_A_imag,                  /// Pointer to imaginary part of A matrix
+  int64_t lda_real,                         /// Leading dimension of real part of A matrix
+  int64_t lda_imag,                         /// Leading dimension of imaginary part of A matrix
+
+  NumericTypeID element_B,                  /// Data type of B matrix elements
+  LayoutTypeID layout_B,                    /// Layout of B matrix
+  ComplexTransform transform_B,             /// Complex transformation applied to B matrix
+
+  void const * ptr_B_real,                  /// Pointer to real part of B matrix
+  void const * ptr_B_imag,                  /// Pointer to imaginary part of B matrix
+  int64_t ldb_real,                             /// Leading dimension of real part of B matrix
+  int64_t ldb_imag,                             /// Leading dimension of imaginary part of B matrix
+
+  void const * beta,                        /// Pointer to beta scalar
+
+  NumericTypeID element_C,                  /// Data type of C and D matrix
+
+  void const * ptr_C_real,                  /// Pointer to real part of C matrix
+  void const * ptr_C_imag,                  /// Pointer to imaginary part of C matrix
+  int64_t ldc_real,                             /// Leading dimension of real part of C matrix
+  int64_t ldc_imag,                             /// Leading dimension of imaginary part of C matrix
+
+  void * ptr_D_real,                        /// Pointer to real part of D matrix
+  void * ptr_D_imag,                        /// Pointer to imaginary part of D matrix
+  int64_t ldd_real,                             /// Leading dimension of real part of D matrix
+  int64_t ldd_imag,                             /// Leading dimension of imaginary part of D matrix
+
+  int batch_count,                          /// Number of batched GEMMs to execute
+
+  int64_t batch_stride_A_real,
+  int64_t batch_stride_A_imag,
+
+  int64_t batch_stride_B_real,
+  int64_t batch_stride_B_imag,
+
+  int64_t batch_stride_C_real,
+  int64_t batch_stride_C_imag,
+
+  int64_t batch_stride_D_real,
+  int64_t batch_stride_D_imag
+) {
+
+  //
+  // Find the operation
+  //
+
+  GemmFunctionalKey key(
+    provider_,
+    GemmKind::kPlanarComplex,
+    element_compute,
+    element_scalar,
+    element_A,
+    layout_A,
+    transform_A,
+    element_B,
+    layout_B,
+    transform_B,
+    element_C
+  );
+
+  auto operators_it = Singleton::get().operation_table.gemm_operations.find(key);
+
+  if (operators_it == Singleton::get().operation_table.gemm_operations.end()) {
+    return cutlass::Status::kErrorNotSupported;
+  }
   
-    //
-    // This example requires an NVIDIA Ampere-architecture GPU.
-    //
+  if (operators_it->second.empty()) {
+    return cutlass::Status::kErrorNotSupported;
+  }
 
-    std::cout 
-      << "CUTLASS's CUTLASS Attention example requires a GPU of NVIDIA's Ampere Architecture or "
-      << "later (compute capability 80 or greater).\n";
+  //
+  // Compute the largest alignment restriction the kernel can satisfy.
+  //
+
+  // Maximum alignment expectation among all kernels (in units of bytes)
+  int const kMaximumAlignmentSize = 16;
+
+  int alignment = std::max(
+    gemm_problem_alignment(
+      M, N, K, 
+      element_A, ptr_A_real, lda_real, batch_stride_A_real,
+      element_B, ptr_B_real, ldb_real, batch_stride_B_real,
+      element_C, ptr_C_real, ldc_real, batch_stride_C_real,
+      ptr_D_real, ldd_real, batch_stride_D_real, kMaximumAlignmentSize
+    ),
+    gemm_problem_alignment(
+      M, N, K, 
+      element_A, ptr_A_imag, lda_imag, batch_stride_A_imag,
+      element_B, ptr_B_imag, ldb_imag, batch_stride_B_imag,
+      element_C, ptr_C_imag, ldc_imag, batch_stride_C_imag,
+      ptr_D_imag, ldd_imag, batch_stride_D_imag, kMaximumAlignmentSize
+    )
+  );
 
-    return 0;
+  //
+  // Find the best kernel in descending order of preference.
+  //
+
+  GemmPreferenceKey preference_key(compute_capability(), alignment);
+
+  Operation const *operation = find_gemm_operation(operators_it, preference_key);
+
+  if (!operation) {
+    return cutlass::Status::kErrorNotSupported;
   }
 
+  last_operation_ = operation;
+
   //
-  // Parse options
+  // Configure operation
   //
 
-  Options options;
+  GemmPlanarComplexConfiguration configuration{
+    GemmUniversalMode::kBatched,
+    {M, N, K},
+    batch_count,
+    lda_real,
+    lda_imag,
+    ldb_real,
+    ldb_imag,
+    ldc_real,
+    ldc_imag,
+    ldd_real,
+    ldd_imag
+  };
+
+  // Query host work space size
+  uint64_t host_workspace_size_needed = operation->get_host_workspace_size(&configuration);
+
+  if (uint64_t(kHostWorkspaceSize) < host_workspace_size_needed) {
+    return cutlass::Status::kErrorNotSupported;
+  }
+
+  char host_workspace[kHostWorkspaceSize];
+
+  // Query device workspace size
+  uint64_t device_workspace_size_needed = operation->get_device_workspace_size(&configuration);
+
+  if (uint64_t(workspace_size_) < device_workspace_size_needed) {
+    return cutlass::Status::kErrorNotSupported;
+  }
+
+  // Initialize host and device workspaces
+  Status status = operation->initialize(
+    &configuration,
+    host_workspace,
+    workspace_,
+    stream_);
+
+  if (status != cutlass::Status::kSuccess) {
+    return status;
+  }
+
+  // Run the operator
+  GemmPlanarComplexArguments arguments{
+    ptr_A_real,
+    ptr_A_imag,
+    ptr_B_real,
+    ptr_B_imag,
+    ptr_C_real,
+    ptr_C_imag,
+    ptr_D_real,
+    ptr_D_imag,
+    alpha,
+    beta,
+    scalar_pointer_mode_,
+    batch_stride_A_real,
+    batch_stride_A_imag,
+    batch_stride_B_real,
+    batch_stride_B_imag,
+    batch_stride_C_real,
+    batch_stride_C_imag,
+    batch_stride_D_real,
+    batch_stride_D_imag
+  };
+
+  return operation->run(&arguments, host_workspace, workspace_, stream_);
+}
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Planar complex batched GEMM loading pointers from arrays in global memory
+Status Handle::gemm_planar_complex_array(
+
+  int expected_M,                           /// Expected GEMM M dimension (used for sizing CUDA grid)
+  int expected_N,                           /// Expected GEMM N dimension (used for sizing CUDA grid)
+  int expected_K,                           /// Expected GEMM K dimension
+  int batch_count,                          /// Number of independent GEMM computations to execute
+
+  int const *M,                             /// Array containing the GEMM M dimension for each batch index
+  int const *N,                             /// Array containing the GEMM N dimension for each batch index
+  int const *K,                             /// Array containing the GEMM K dimension for each batch index
+
+  NumericTypeID element_compute,            /// Data type of internal accumulation
+
+  NumericTypeID element_scalar,             /// Data type of alpha/beta scalars
+
+  void const *alpha,                        /// Pointer to alpha scalar
+
+  NumericTypeID element_A,                  /// Data type of A matrix elements
+  LayoutTypeID layout_A,                    /// Layout of A matrix
+  ComplexTransform transform_A,             /// Complex transformation applied to A matrix
+
+  void const * const * ptr_A_real,          /// Pointer to array containing pointers to real part of A matrices
+  void const * const * ptr_A_imag,          /// Pointer to array containing pointers to imaginary part of A matrices
+
+  int64_t lda_real,                             /// Leading dimension of real part of A matrix
+  int64_t lda_imag,                             /// Leading dimension of imaginary part of A matrix
+
+  NumericTypeID element_B,                  /// Data type of B matrix elements
+  LayoutTypeID layout_B,                    /// Layout of B matrix
+  ComplexTransform transform_B,             /// Complex transformation applied to B matrix
+
+  void const * const * ptr_B_real,          /// Pointer to array containing pointers to real part of B matrices
+  void const * const * ptr_B_imag,          /// Pointer to array containing pointers to imaginary part of B matrices
+
+  int64_t ldb_real,                             /// Leading dimension of real part of B matrix
+  int64_t ldb_imag,                             /// Leading dimension of imaginary part of B matrix
+
+  void const * beta,                        /// Pointer to beta scalar
+
+  NumericTypeID element_C,                  /// Data type of C and D matrix
+
+  void const * const * ptr_C_real,          /// Pointer to array containing pointers to real part of C matrices
+  void const * const * ptr_C_imag,          /// Pointer to array containing poitners to imaginary part of C matrices
+
+  int64_t ldc_real,                             /// Leading dimension of real part of C matrix
+  int64_t ldc_imag,                             /// Leading dimension of imaginary part of C matrix
+
+  void * const * ptr_D_real,                /// Pointer to array containing pointers to real part of D matrices
+  void * const * ptr_D_imag,                /// Pointer to array containing poitners to imaginary part of D matrices
+
+  int64_t ldd_real,                             /// Leading dimension of real part of D matrix
+  int64_t ldd_imag                              /// Leading dimension of imaginary part of D matrix
+) {
   
-  options.parse(argc, args);
+  //
+  // Find the operation
+  //
+
+  GemmFunctionalKey key(
+    provider_,
+    GemmKind::kPlanarComplexArray,
+    element_compute,
+    element_scalar,
+    element_A,
+    layout_A,
+    transform_A,
+    element_B,
+    layout_B,
+    transform_B,
+    element_C
+  );
+
+  auto operators_it = Singleton::get().operation_table.gemm_operations.find(key);
 
-  if (options.help) {
-    options.print_usage(std::cout) << std::endl;
-    return 0;
+  if (operators_it == Singleton::get().operation_table.gemm_operations.end()) {
+    return cutlass::Status::kErrorNotSupported;
+  }
+  
+  if (operators_it->second.empty()) {
+    return cutlass::Status::kErrorNotSupported;
   }
 
-  if (options.error) {
-    std::cerr << "Aborting execution." << std::endl;
-    return -1;
+  //
+  // Compute the largest alignment restriction the kernel can satisfy.
+  //
+
+  // Maximum alignment expectation among all kernels (in units of bytes)
+  int const kMaximumAlignmentSize = 16;
+
+  int alignment = std::max(
+    gemm_problem_alignment(
+      expected_M, expected_N, expected_K, 
+      element_A, nullptr, lda_real, 0,
+      element_B, nullptr, ldb_real, 0,
+      element_C, nullptr, ldc_real, 0,
+      nullptr, ldd_real, 0, kMaximumAlignmentSize
+    ),
+    gemm_problem_alignment(
+      expected_M, expected_N, expected_K, 
+      element_A, nullptr, lda_imag, 0,
+      element_B, nullptr, ldb_imag, 0,
+      element_C, nullptr, ldc_imag, 0,
+      nullptr, ldd_imag, 0, kMaximumAlignmentSize
+    )
+  );
+
+  //
+  // Find the best kernel in descending order of preference.
+  //
+
+  GemmPreferenceKey preference_key(compute_capability(), alignment);
+
+  Operation const *operation = find_gemm_operation(operators_it, preference_key);
+
+  if (!operation) {
+    return cutlass::Status::kErrorNotSupported;
   }
 
+  last_operation_ = operation;
+
   //
-  // Define the CUTLASS Attention type
+  // Configure operation
   //
 
-  using ElementOutput = cutlass::half_t;
-  using ElementAccumulator = cutlass::half_t;
+  GemmPlanarComplexArrayConfiguration configuration{
+    {expected_M, expected_N, expected_K},
+    batch_count,
+    lda_real,
+    lda_imag,
+    ldb_real,
+    ldb_imag,
+    ldc_real,
+    ldc_imag,
+    ldd_real,
+    ldd_imag
+  };
+
+  // Query host work space size
+  uint64_t host_workspace_size_needed = operation->get_host_workspace_size(&configuration);
+
+  if (uint64_t(kHostWorkspaceSize) < host_workspace_size_needed) {
+    return cutlass::Status::kErrorNotSupported;
+  }
+
+  char host_workspace[kHostWorkspaceSize];
+
+  // Query device workspace size
+  uint64_t device_workspace_size_needed = operation->get_device_workspace_size(&configuration);
+
+  if (uint64_t(workspace_size_) < device_workspace_size_needed) {
+    return cutlass::Status::kErrorNotSupported;
+  }
+
+  // Initialize host and device workspaces
+  Status status = operation->initialize(
+    &configuration,
+    host_workspace,
+    workspace_,
+    stream_);
 
-  using ElementQ = cutlass::half_t;
-  using ElementK = cutlass::half_t;
-  using ElementP = ElementOutput;
+  if (status != cutlass::Status::kSuccess) {
+    return status;
+  }
 
-  using LayoutQ = cutlass::layout::RowMajor;
-  using LayoutK = cutlass::layout::ColumnMajor;
-  using LayoutP = cutlass::layout::RowMajor;
+  // Run the operator
+  GemmPlanarComplexArrayArguments arguments{
+    M, N, K,
+    ptr_A_real,
+    ptr_A_imag,
+    ptr_B_real,
+    ptr_B_imag,
+    ptr_C_real,
+    ptr_C_imag,
+    ptr_D_real,
+    ptr_D_imag,
+    alpha,
+    beta,
+    scalar_pointer_mode_
+  };
 
-  static bool const UseMask = false;
+  return operation->run(&arguments, host_workspace, workspace_, stream_);
+}
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-  if (UseMask != options.use_mask) {
-    std::cerr << "UseMask and user-defined use_mask need to be consistant, "
-    << " aborted execution.\n";
-    return -2;
+/// Finds conv operation instances with Conv::ElementC = Reduction::ElementWorkspace
+Operation const* find_conv_operation_for_parallel_reduction(Operation const *operation) {
+
+  ConvDescription const &conv_desc = 
+    static_cast<ConvDescription const &>(operation->description());
+
+  // if the curren conv operation accumulator and output data type match return operation
+  if(conv_desc.tile_description.math_instruction.element_accumulator == conv_desc.C.element) {
+    return operation;
   }
 
-  using OperatorClass = cutlass::arch::OpClassTensorOp;
-  using ArchTag = cutlass::arch::Sm80;
+  // find conv operation to match conv output and reduction workspace data type
+  ConvFunctionalKey key(
+    library::Provider::kCUTLASS,
+    conv_desc.conv_kind,        
+    conv_desc.A.element,
+    conv_desc.A.layout,
+    conv_desc.B.element,
+    conv_desc.B.layout,
+    conv_desc.tile_description.math_instruction.element_accumulator,
+    conv_desc.C.layout,
+    conv_desc.tile_description.math_instruction.element_accumulator, 
+    conv_desc.element_epilogue);
+
+  // conv operation table for conv2d or conv3d
+  auto conv_operations = (conv_desc.kind == OperationKind::kConv2d) ? 
+                          Singleton::get().operation_table.conv2d_operations : 
+                          Singleton::get().operation_table.conv3d_operations;
 
-  using ThreadblockShape0 = cutlass::gemm::GemmShape<128, 128, 32>;
-  using WarpShape0 = cutlass::gemm::GemmShape<64, 64, 32>;
+  // find ConvFunctionalKey in convolution operation table
+  auto operators_it = conv_operations.find(key);
 
-  using ThreadblockShape1 = cutlass::gemm::GemmShape<64, 64, 32>;
-  using WarpShape1 = cutlass::gemm::GemmShape<32, 32, 32>;
+  if (operators_it == conv_operations.end()) {
+    return nullptr;
+  }
   
-  static int const Stages0 = 3;
-  static int const Stages1 = 4;
+  if (operators_it->second.empty()) {
+    return nullptr;
+  }
 
-  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 16>;
+  // conv operation for same compute capability and iterator algorithm
+  ConvPreferenceKey preference_key(
+    conv_desc.tile_description.minimum_compute_capability, 
+    conv_desc.iterator_algorithm);
 
-  using Attention = cutlass::FusedMultiHeadAttention<
-    ElementQ,
-    LayoutQ,
-    ElementK,
-    LayoutK,
-    ElementP,
-    LayoutP,
-    ElementAccumulator,
-    OperatorClass,
-    ArchTag,
-    ThreadblockShape0,
-    ThreadblockShape1,
-    WarpShape0,
-    WarpShape1,
-    InstructionShape,
-    Stages0,
-    Stages1,
-    UseMask
-  >;
-
-  //
-  // Test and profile
-  //
-
-  TestbedAttention<Attention> testbed(options);
-
-  if (!testbed.sufficient()) {
-    std::cout << "The active CUDA device lacks sufficient hardware resources to execute this kernel.\n";
-    return 0;
-  }
-
-  Result result = testbed.profile_grouped();
-  if (!result.passed) {
-    std::cout << "Profiling CUTLASS attention has failed.\n";
-    std::cout << "\nFailed\n";
-    return -1;
+  auto it = operators_it->second.find(preference_key);
+  
+  if(it == operators_it->second.end()) {
+    return nullptr;
   }
 
-  std::cout << "\nPassed\n";
+  // return matching conv opertion (same tile sizes and instruction)
+  for (auto op : it->second) {
+    if (op->description().tile_description == operation->description().tile_description) {
+      return op;
+    }
+  }
 
-  return 0;
+  return nullptr;
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Finds gemm operation instances with Gemm::ElementC = Reduction::ElementWorkspace
+Operation const* find_gemm_operation_for_parallel_reduction(Operation const *operation) {
+
+  GemmDescription const &gemm_desc = 
+    static_cast<GemmDescription const &>(operation->description());
+
+  // if the curren gemm operation accumulator and output data type match return operation
+  if(gemm_desc.tile_description.math_instruction.element_accumulator == gemm_desc.C.element) {
+    return operation;
+  }
+
+  // find gemm operation to match gemm output and reduction workspace data type
+  GemmFunctionalKey key(
+    library::Provider::kCUTLASS,
+    gemm_desc.gemm_kind,
+    gemm_desc.tile_description.math_instruction.element_accumulator,
+    gemm_desc.element_epilogue,
+    gemm_desc.A.element,
+    gemm_desc.A.layout,
+    gemm_desc.transform_A,
+    gemm_desc.B.element,
+    gemm_desc.B.layout,
+    gemm_desc.transform_B,
+    gemm_desc.tile_description.math_instruction.element_accumulator);
+
+  // gemm operation table
+  auto gemm_operations = Singleton::get().operation_table.gemm_operations;
+
+  // find ConvFunctionalKey in gemm operation table
+  auto operators_it = gemm_operations.find(key);
+
+  if (operators_it == gemm_operations.end()) {
+    return nullptr;
+  }
+
+  if (operators_it->second.empty()) {
+    return nullptr;
+  }
+
+  // A and B uses the same alignment in the generator.py
+  int alignment = gemm_desc.A.alignment;
+
+  // gemm operation for same compute capability and iterator algorithm
+  GemmPreferenceKey preference_key(
+    gemm_desc.tile_description.minimum_compute_capability, 
+    alignment);
+
+  return find_gemm_operation(operators_it, preference_key);
+}
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+} // namespace library
+} // namespace cutlass
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_multi_head_attention/gemm_attention.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_splitk_parallel.h`

 * *Files 23% similar despite different names*

```diff
@@ -1,22 +1,22 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
  *
  * 2. Redistributions in binary form must reproduce the above copyright notice,
  * this list of conditions and the following disclaimer in the documentation
  * and/or other materials provided with the distribution.
  *
- * 3. Neither the name of the copyright holdvr nor the names of its
+ * 3. Neither the name of the copyright holder nor the names of its
  * contributors may be used to endorse or promote products derived from
  * this software without specific prior written permission.
  *
  * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
  * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
  * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
@@ -24,603 +24,615 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
 /*! \file
-    \brief Defines the FusedMultiHeadAttention Class
-
-    The class contains the following:
-    1) GEMM0 with epilogue fusion,
-    2) GEMM1 with mainloop fusion, and 
-    3) A lightweight full softmax reduction kernel.
-
+    \brief Template for GEMM performing a reduction over K partitions in parallel.
 */
 
 #pragma once
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-#include <cmath>
-#include <iostream>
-#include <vector>
-#include <limits>
-
 #include "cutlass/cutlass.h"
-#include "cutlass/arch/memory.h"
-#include "cutlass/arch/memory_sm75.h"
-#include "cutlass/epilogue/threadblock/epilogue_visitor_with_softmax.h"
-#include "cutlass/epilogue/thread/scale_type.h"
-#include "cutlass/gemm/kernel/default_gemm_grouped_softmax_mainloop_fusion.h"
-#include "cutlass/reduction/kernel/reduce_softmax_final.h"
-#include "gemm_grouped_with_softmax_visitor.h"
+#include "cutlass/numeric_types.h"
+#include "cutlass/arch/arch.h"
+#include "cutlass/device_kernel.h"
 
-namespace cutlass {
+#include "cutlass/gemm/threadblock/threadblock_swizzle.h"
+#include "cutlass/gemm/kernel/gemm.h"
 
-template <
-  typename ElementQ_,
-  typename LayoutQ_,
-  typename ElementK_,
-  typename LayoutK_,
-  typename ElementP_,
-  typename LayoutP_,
-  typename ElementCompute_,
-  typename OperatorClass_,
-  typename ArchTag_,
-  typename ThreadblockShape0_,
-  typename ThreadblockShape1_,
-  typename WarpShape0_,
-  typename WarpShape1_,
-  typename InstructionShape_,
-  int kStages0_,
-  int kStages1_,
-  bool UseMasking_ = false,
-  cutlass::gemm::kernel::GroupScheduleMode GroupScheduleMode0_ = cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute,
-  cutlass::gemm::kernel::GroupScheduleMode GroupScheduleMode1_ = cutlass::gemm::kernel::GroupScheduleMode::kHostPrecompute,
-  int Alignment = 128 / cutlass::sizeof_bits<ElementQ_>::value,
-  typename ElementSoftmax_ = ElementP_
->
-class FusedMultiHeadAttention {
-public:
+#include "cutlass/gemm/kernel/default_gemm_splitk_parallel.h"
+#include "cutlass/gemm/device/default_gemm_configuration.h"
 
-  using ElementQ = ElementQ_;
-  using ElementK = ElementK_;
-  using ElementP = ElementP_;
-  using ElementV = ElementK;
-  using ElementOutput = ElementP;
-  using ElementAccumulator = ElementCompute_;
-
-  using LayoutQ = LayoutQ_;
-  using LayoutK = LayoutK_;
-  using LayoutP = LayoutP_;
-  using LayoutV = LayoutK;
-  using LayoutO = LayoutP;
-
-  using ElementNorm = cutlass::half_t;
-  using ElementSum = cutlass::half_t;
-  using ElementSoftmaxCompute = float;
-  using LayoutNorm = cutlass::layout::RowMajor;
+#include "cutlass/epilogue/thread/conversion_op.h"
+#include "cutlass/reduction/kernel/reduce_split_k.h"
+#include "cutlass/reduction/thread/reduction_operators.h"
 
-  using ThreadblockSwizzle = cutlass::gemm::threadblock::GemmBatchedIdentityThreadblockSwizzle;
+////////////////////////////////////////////////////////////////////////////////
 
-  using OperatorClass = OperatorClass_;
-  using ArchTag = ArchTag_;
+namespace cutlass {
+namespace gemm {
+namespace device {
 
-  using ThreadblockShape0 = ThreadblockShape0_;
-  using WarpShape0 = WarpShape0_;
+////////////////////////////////////////////////////////////////////////////////
 
-  using ThreadblockShape1 = ThreadblockShape1_;
-  using WarpShape1 = WarpShape1_;
-  
-  static int const Stages0 = kStages0_;
-  static int const Stages1 = kStages1_;
+/*! 
+  Gemm device-level operator performing parallel reduction over the K partition.
 
+*/
+template <
+    /// Element type for A matrix operand
+    typename ElementA_,
+    /// Layout type for A matrix operand
+    typename LayoutA_,
+    /// Element type for B matrix operand
+    typename ElementB_,
+    /// Layout type for B matrix operand
+    typename LayoutB_,
+    /// Element type for C and D matrix operands
+    typename ElementC_,
+    /// Layout type for C and D matrix operands
+    typename LayoutC_,
+    /// Element type for internal accumulation
+    typename ElementAccumulator_ = ElementC_,
+    /// Operator class tag
+    typename OperatorClass_ = arch::OpClassSimt,
+    /// Tag indicating architecture to tune for.  This is the minimum SM that
+      /// supports the intended feature. The device kernel can be built
+      /// targeting any SM larger than this number.
+    typename ArchTag_ = arch::Sm70,
+    /// Threadblock-level tile size (concept: GemmShape)
+    typename ThreadblockShape_ = typename DefaultGemmConfiguration<
+        OperatorClass_, ArchTag_, ElementA_, ElementB_, ElementC_,
+        ElementAccumulator_>::ThreadblockShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename WarpShape_ = typename DefaultGemmConfiguration<
+        OperatorClass_, ArchTag_, ElementA_, ElementB_, ElementC_,
+        ElementAccumulator_>::WarpShape,
+    /// Instruction-level tile size (concept: GemmShape)
+    typename InstructionShape_ = typename DefaultGemmConfiguration<
+        OperatorClass_, ArchTag_, ElementA_, ElementB_, ElementC_,
+        ElementAccumulator_>::InstructionShape,
+    /// Epilogue output operator
+    typename EpilogueOutputOp_ = typename DefaultGemmConfiguration<
+        OperatorClass_, ArchTag_, ElementA_, ElementB_, ElementC_,
+        ElementAccumulator_>::EpilogueOutputOp,
+    /// Epilogue output operator
+    typename ConvertScaledOp_ = cutlass::epilogue::thread::Convert<
+        ElementAccumulator_,
+        DefaultGemmConfiguration<OperatorClass_, ArchTag_, ElementA_, ElementB_,
+                                 ElementAccumulator_,
+                                 ElementAccumulator_>::EpilogueOutputOp::kCount,
+        ElementAccumulator_>,
+    /// Reduction operator
+    typename ReductionOp_ = cutlass::reduction::thread::ReduceAdd<
+        ElementAccumulator_, typename EpilogueOutputOp_::ElementAccumulator,
+        EpilogueOutputOp_::kCount>,
+    /// Threadblock-level swizzling operator
+    typename ThreadblockSwizzle_ =
+        threadblock::GemmSplitKHorizontalThreadblockSwizzle,
+    /// Number of stages used in the pipelined mainloop
+    int Stages =
+        DefaultGemmConfiguration<OperatorClass_, ArchTag_, ElementA_, ElementB_,
+                                 ElementC_, ElementAccumulator_>::kStages,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentA =
+        DefaultGemmConfiguration<OperatorClass_, ArchTag_, ElementA_, ElementB_,
+                                 ElementC_, ElementAccumulator_>::kAlignmentA,
+    /// Access granularity of B matrix in units of elements
+    int kAlignmentB =
+        DefaultGemmConfiguration<OperatorClass_, ArchTag_, ElementA_, ElementB_,
+                                 ElementC_, ElementAccumulator_>::kAlignmentB,
+    /// Operation performed by GEMM
+    typename Operator_ = typename DefaultGemmConfiguration<
+        OperatorClass_, ArchTag_, ElementA_, ElementB_, ElementC_,
+        ElementAccumulator_>::Operator>
+class GemmSplitKParallel {
+ public:
+
+  using ElementA = ElementA_;
+  using LayoutA = LayoutA_;
+  using ElementB = ElementB_;
+  using LayoutB = LayoutB_;
+  using ElementC = ElementC_;
+  using LayoutC = LayoutC_;
+  using ElementAccumulator = ElementAccumulator_;
+  using OperatorClass = OperatorClass_;
+  using ArchTag = ArchTag_;
+  using ThreadblockShape = ThreadblockShape_;
+  using WarpShape = WarpShape_;
   using InstructionShape = InstructionShape_;
-
-  using EpilogueOutputOp0 = cutlass::epilogue::thread::LinearCombination<
-        ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
-        ElementAccumulator, ElementAccumulator, cutlass::epilogue::thread::ScaleType::OnlyAlphaScaling>;
-
-  using EpilogueOutputOp1 = cutlass::epilogue::thread::LinearCombination<
-        ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
-        ElementAccumulator, ElementAccumulator, cutlass::epilogue::thread::ScaleType::Nothing>;
-
-  using Operator = typename cutlass::gemm::device::DefaultGemmConfiguration<
-        OperatorClass, ArchTag, ElementQ, ElementK, ElementP,
-        ElementAccumulator>::Operator;
-  static bool const kInternalTranspose = cutlass::platform::is_same<LayoutP, cutlass::layout::ColumnMajor>::value;
-
-  static bool const kUseMasking = UseMasking_;
-
-  static cutlass::gemm::kernel::GroupScheduleMode const kGroupScheduleMode0 = GroupScheduleMode0_;
-  static cutlass::gemm::kernel::GroupScheduleMode const kGroupScheduleMode1 = GroupScheduleMode1_;
-
-  using MapArguments = cutlass::gemm::kernel::detail::MapArguments<
-    ElementQ,
-    LayoutQ,
-    cutlass::ComplexTransform::kNone,
-    8,
-    ElementK,
-    LayoutK,
-    cutlass::ComplexTransform::kNone,
-    8,
-    LayoutP,
-    kInternalTranspose
-  >;
-
-  using DefaultGemmKernel = typename cutlass::gemm::kernel::DefaultGemm<
-    typename MapArguments::ElementA,
-    typename MapArguments::LayoutA,
-    MapArguments::kAlignmentA,
-    typename MapArguments::ElementB,
-    typename MapArguments::LayoutB,
-    MapArguments::kAlignmentB,
-    ElementP,
-    typename MapArguments::LayoutC,
+  using ConvertScaledOp = ConvertScaledOp_;
+  using EpilogueOutputOp = EpilogueOutputOp_;
+  using ReductionOp = ReductionOp_;
+  using ThreadblockSwizzle = ThreadblockSwizzle_;
+  using Operator = Operator_;
+  static int const kStages = Stages;
+
+  /// GEMM kernel 
+  using GemmKernel = typename kernel::DefaultGemmSplitKParallel<
+    ElementA,
+    LayoutA,
+    kAlignmentA,
+    ElementB,
+    LayoutB,
+    kAlignmentB,
+    ElementAccumulator,
+    LayoutC,
     ElementAccumulator,
     OperatorClass,
     ArchTag,
-    ThreadblockShape0,
-    WarpShape0,
+    ThreadblockShape,
+    WarpShape,
     InstructionShape,
-    EpilogueOutputOp0,
+    ConvertScaledOp,
     ThreadblockSwizzle,
-    Stages0,
-    true,
-    Operator,
-    cutlass::gemm::SharedMemoryClearOption::kNone
+    kStages,
+    Operator
   >::GemmKernel;
 
-  using EpilogueVisitor = typename cutlass::epilogue::threadblock::EpilogueVisitorSoftmax<
-    ThreadblockShape0,
-    DefaultGemmKernel::kThreadCount,
-    typename DefaultGemmKernel::Epilogue::OutputTileIterator,
-    typename EpilogueOutputOp0::ElementCompute,
-    ElementNorm,
-    ElementSum,
-    ElementSoftmaxCompute,
-    EpilogueOutputOp0,
-    kUseMasking
-  >;
-
-  using Epilogue = typename cutlass::epilogue::threadblock::EpilogueWithVisitorFromExistingEpilogue<
-    EpilogueVisitor,
-    typename DefaultGemmKernel::Epilogue
-  >::Epilogue;
-
-  using GemmKernel0 = cutlass::gemm::kernel::GemmGroupedWithEpilogueVistor<
-    typename DefaultGemmKernel::Mma,
-    Epilogue,
-    ThreadblockSwizzle,
-    kGroupScheduleMode0,
-    kInternalTranspose,
-    kUseMasking
-  >;
-
-  using GemmGrouped0 = cutlass::gemm::device::GemmGrouped<GemmKernel0>;
-
-  using ApplyFinalReductionDevice = cutlass::reduction::kernel::ApplySoftmaxFinalReduction<
-    ElementNorm,
-    ElementSum,
-    typename GemmGrouped0::GemmKernel::EpilogueVisitor::ElementSoftmaxCompute,
-    typename GemmGrouped0::GemmKernel::EpilogueVisitor::ThreadblockShape,
-    true
+  /// Reduction kernel
+  using ReductionKernel = cutlass::reduction::kernel::ReduceSplitK<
+    cutlass::MatrixShape<4, 32 * EpilogueOutputOp::kCount>,
+    EpilogueOutputOp,
+    ReductionOp
   >;
 
-  using GemmKernel1 = typename cutlass::gemm::kernel::DefaultGemmGroupedSoftmaxMainloopFusion<
-    ElementP, 
-    LayoutP,
-    cutlass::ComplexTransform::kNone,
-    128 / cutlass::sizeof_bits<ElementQ>::value,
-    ElementV,
-    LayoutV,
-    cutlass::ComplexTransform::kNone,
-    128 / cutlass::sizeof_bits<ElementK>::value,
-    ElementNorm,
-    LayoutNorm,
-    ElementOutput,
-    LayoutO,
-    ElementAccumulator,
-    OperatorClass, 
-    ArchTag,
-    ThreadblockShape1,
-    WarpShape1,
-    InstructionShape,
-    EpilogueOutputOp1,
-    ThreadblockSwizzle, 
-    Stages1,
-    kGroupScheduleMode1
-  >::GemmKernel;
+  //
+  //
+  //
 
-  using GemmGrouped1 = cutlass::gemm::device::GemmGrouped<GemmKernel1>;
-
-public:
-
-  /// Arguments class
+  /// Argument structure
   struct Arguments {
-    cutlass::gemm::GemmCoord *problem_sizes0;
-    cutlass::gemm::GemmCoord *problem_sizes0_real;
-    cutlass::gemm::GemmCoord *problem_sizes1;
-    int problem_count;
-    int threadblock_count;
-
-    ElementQ ** ptr_Q;
-    ElementK ** ptr_K;
-    ElementP ** ptr_P;
-    ElementP ** ptr_V;
-    ElementP ** ptr_O;
-
-    ElementNorm **ptr_Max;
-    ElementSum **ptr_Sum;
-
-    ElementP *block_P;
-    ElementNorm *block_Norm;
-    ElementSum *block_Sum;
-    int64_t *offset_P;
-    int64_t *offset_Norm_Device;
-    int64_t *offset_Sum_Device;
-
-    typename LayoutQ::Stride::LongIndex *ldq;
-    typename LayoutK::Stride::LongIndex *ldk;
-    typename LayoutP::Stride::LongIndex *ldp;
-    typename LayoutP::Stride::LongIndex *ldv;
-    typename LayoutP::Stride::LongIndex *ldo;
-
-    cutlass::gemm::GemmCoord *problem_sizes0_host;
-    cutlass::gemm::GemmCoord *problem_sizes1_host;
-
-    ElementAccumulator alpha0;
-    ElementAccumulator alpha1;
-    ElementAccumulator beta;
-
-    int head_number;
-    int batch_size;
-    int seq_length;
 
-    typename ApplyFinalReductionDevice::Arguments reduction;
+    //
+    // Data members
+    //
+
+    GemmCoord problem_size;
+    TensorRef<ElementA const, LayoutA> ref_A;
+    TensorRef<ElementB const, LayoutB> ref_B;
+    TensorRef<ElementC const, LayoutC> ref_C;
+    TensorRef<ElementC, LayoutC> ref_D;
+    typename EpilogueOutputOp::Params epilogue;
+    int split_k_slices;
+    typename ConvertScaledOp::Params convert;
+    typename ReductionOp::Params reduction;
 
     //
     // Methods
     //
-    Arguments(): 
-      problem_count(0), 
-      threadblock_count(0), 
-      ptr_Q(nullptr),
-      ptr_K(nullptr),
-      ptr_P(nullptr),
-      ptr_V(nullptr),
-      ptr_O(nullptr),
-      ptr_Max(nullptr),
-      ptr_Sum(nullptr),
-      block_P(nullptr),
-      block_Norm(nullptr),
-      block_Sum(nullptr),
-      offset_P(nullptr),
-      offset_Norm_Device(nullptr),
-      offset_Sum_Device(nullptr),
-      ldq(nullptr),
-      ldk(nullptr),
-      ldp(nullptr),
-      ldv(nullptr),
-      ldo(nullptr),
-      head_number(0),
-      batch_size(0),
-      seq_length(0)
-    {
 
-    }
+    /// Default ctor
+    CUTLASS_HOST_DEVICE
+    Arguments() { }
 
+    /// Constructs an Arguments structure 
+    CUTLASS_HOST_DEVICE
     Arguments(
-      cutlass::gemm::GemmCoord *problem_sizes0,
-      cutlass::gemm::GemmCoord *problem_sizes1,
-      int problem_count,
-      int threadblock_count,
-      ElementQ ** ptr_Q,
-      ElementK ** ptr_K,
-      ElementP ** ptr_P,
-      ElementP ** ptr_V,
-      ElementP ** ptr_O,
-      ElementNorm **ptr_Max,
-      ElementSum **ptr_Sum,
-      ElementP *block_P,
-      ElementNorm *block_Norm,
-      ElementSum *block_Sum,
-      int64_t *offset_P,
-      int64_t *offset_Norm_Device,
-      int64_t *offset_Sum_Device,
-      typename LayoutQ::Stride::LongIndex *ldq,
-      typename LayoutK::Stride::LongIndex *ldk,
-      typename LayoutP::Stride::LongIndex *ldp,
-      typename LayoutP::Stride::LongIndex *ldv,
-      typename LayoutP::Stride::LongIndex *ldo,
-      ElementAccumulator alpha0,
-      ElementAccumulator alpha1,
-      ElementAccumulator beta,
-      int head_number,
-      int batch_size,
-      int seq_length,
-      cutlass::gemm::GemmCoord *problem_sizes0_host = nullptr,
-      cutlass::gemm::GemmCoord *problem_sizes1_host = nullptr,
-      cutlass::gemm::GemmCoord *problem_sizes0_real = nullptr
+      GemmCoord problem_size_,
+      TensorRef<ElementA const, LayoutA> ref_A_,
+      TensorRef<ElementB const, LayoutB> ref_B_,
+      TensorRef<ElementC const, LayoutC> ref_C_,
+      TensorRef<ElementC, LayoutC> ref_D_,
+      typename EpilogueOutputOp::Params epilogue_ = 
+        typename EpilogueOutputOp::Params(),
+      int split_k_slices = 1,
+      typename ConvertScaledOp::Params convert_ = 
+        typename ConvertScaledOp::Params(),
+      typename ReductionOp::Params reduction_ =
+        typename ReductionOp::Params()
     ):
-      problem_sizes0(problem_sizes0),
-      problem_sizes1(problem_sizes1),
-      problem_count(problem_count),
-      threadblock_count(threadblock_count),
-      ptr_Q(ptr_Q),
-      ptr_K(ptr_K),
-      ptr_P(ptr_P),
-      ptr_V(ptr_V),
-      ptr_O(ptr_O),
-      ptr_Max(ptr_Max),
-      ptr_Sum(ptr_Sum),
-      block_P(block_P),
-      block_Norm(block_Norm),
-      block_Sum(block_Sum),
-      offset_P(offset_P),
-      offset_Norm_Device(offset_Norm_Device),
-      offset_Sum_Device(offset_Sum_Device),
-      ldq(ldq),
-      ldk(ldk),
-      ldp(ldp),
-      ldv(ldv),
-      ldo(ldo),
-      alpha0(alpha0),
-      alpha1(alpha1),
-      beta(beta),
-      head_number(head_number),
-      batch_size(batch_size),
-      seq_length(seq_length),
-      problem_sizes0_host(problem_sizes0_host),
-      problem_sizes1_host(problem_sizes1_host),
-      problem_sizes0_real(problem_sizes0_real),
-      reduction(
-        problem_sizes0,
-        block_Norm,
-        block_Sum,
-        offset_Norm_Device,
-        offset_Sum_Device
-      )
-    {
-        
-    }
+      problem_size(problem_size_),
+      ref_A(ref_A_),
+      ref_B(ref_B_),
+      ref_C(ref_C_),
+      ref_D(ref_D_),
+      epilogue(epilogue_),
+      split_k_slices(split_k_slices),
+      convert(convert_),
+      reduction(reduction_) { }
+  };
 
+private:
 
-  };
+  /// Kernel parameters object
+  typename GemmKernel::Params gemm_params_;
 
-  struct Params {
-    cutlass::gemm::GemmCoord *problem_sizes0;
-    cutlass::gemm::GemmCoord *problem_sizes0_real;
-    cutlass::gemm::GemmCoord *problem_sizes1;
-    int problem_count;
-    int threadblock_count;
-
-    ElementQ ** ptr_Q;
-    ElementK ** ptr_K;
-    ElementP ** ptr_P;
-    ElementP ** ptr_V;
-    ElementP ** ptr_O;
-
-    ElementNorm **ptr_Max;
-    ElementSum **ptr_Sum;
-
-    ElementP *block_P;
-    ElementNorm *block_Norm;
-    ElementSum *block_Sum;
-    int64_t *offset_P;
-    int64_t *offset_Norm_Device;
-    int64_t *offset_Sum_Device;
-
-    typename LayoutQ::Stride::LongIndex *ldq;
-    typename LayoutK::Stride::LongIndex *ldk;
-    typename LayoutP::Stride::LongIndex *ldp;
-    typename LayoutP::Stride::LongIndex *ldv;
-    typename LayoutP::Stride::LongIndex *ldo;
-
-    cutlass::gemm::GemmCoord *problem_sizes0_host;
-    cutlass::gemm::GemmCoord *problem_sizes1_host;
-
-    ElementAccumulator alpha0;
-    ElementAccumulator alpha1;
-    ElementAccumulator beta;
-
-    int head_number;
-    int batch_size;
-    int seq_length;
-
-    typename ApplyFinalReductionDevice::Params reduction;
-
-    Params(): 
-      problem_count(0), 
-      threadblock_count(0), 
-      ptr_Q(nullptr),
-      ptr_K(nullptr),
-      ptr_P(nullptr),
-      ptr_V(nullptr),
-      ptr_O(nullptr),
-      ptr_Max(nullptr),
-      ptr_Sum(nullptr),
-      block_P(nullptr),
-      block_Norm(nullptr),
-      block_Sum(nullptr),
-      offset_P(nullptr),
-      offset_Norm_Device(nullptr),
-      offset_Sum_Device(nullptr),
-      ldq(nullptr),
-      ldk(nullptr),
-      ldp(nullptr),
-      ldv(nullptr),
-      ldo(nullptr),
-      problem_sizes0(nullptr),
-      problem_sizes1(nullptr),
-      problem_sizes0_real(nullptr),
-      head_number(0),
-      batch_size(0),
-      seq_length(0)
-    {
+  /// Reduction kernel parameters object
+  typename ReductionKernel::Params reduction_params_;
 
-    }
+public:
 
-    Params(Arguments const &args, void *workspace = nullptr):
-      problem_sizes0(args.problem_sizes0),
-      problem_sizes1(args.problem_sizes1),
-      problem_count(args.problem_count),
-      threadblock_count(args.threadblock_count),
-      ptr_Q(args.ptr_Q),
-      ptr_K(args.ptr_K),
-      ptr_P(args.ptr_P),
-      ptr_V(args.ptr_V),
-      ptr_O(args.ptr_O),
-      ptr_Max(args.ptr_Max),
-      ptr_Sum(args.ptr_Sum),
-      block_P(args.block_P),
-      block_Norm(args.block_Norm),
-      block_Sum(args.block_Sum),
-      offset_P(args.offset_P),
-      offset_Norm_Device(args.offset_Norm_Device),
-      offset_Sum_Device(args.offset_Sum_Device),
-      ldq(args.ldq),
-      ldk(args.ldk),
-      ldp(args.ldp),
-      ldv(args.ldv),
-      ldo(args.ldo),
-      problem_sizes0_host(args.problem_sizes0_host),
-      problem_sizes1_host(args.problem_sizes1_host),
-      problem_sizes0_real(args.problem_sizes0_real),
-      alpha0(args.alpha0),
-      alpha1(args.alpha1),
-      beta(args.beta),
-      head_number(args.head_number),
-      batch_size(args.batch_size),
-      seq_length(args.seq_length),
-      reduction(args.reduction)
-    {
+  /// Constructs the GEMM.
+  GemmSplitKParallel() { }
 
-    }
-  };
+  /// Determines whether the GEMM can execute the given problem.
+  static Status can_implement(Arguments const &args) {
 
+    // TODO
 
-private:
+    return Status::kSuccess;
+  }
 
-  Params params_;
-  GemmGrouped0 gemm_grouped0;
-  GemmGrouped1 gemm_grouped1;
+  /// Gets the workspace size
+  static size_t get_workspace_size(Arguments const &args) {
+    
+    // Determine grid shape
+    ThreadblockSwizzle threadblock_swizzle;
+
+    cutlass::gemm::GemmCoord grid_shape = threadblock_swizzle.get_tiled_shape(
+      args.problem_size, 
+      {ThreadblockShape::kM, ThreadblockShape::kN, ThreadblockShape::kK},
+      args.split_k_slices);
 
+    return sizeof(ElementAccumulator_) * size_t(args.problem_size.m()) * size_t(args.problem_size.n()) * grid_shape.k();
+  }
 
-public:
+  /// Initializes GEMM state from arguments.
+  Status initialize(Arguments const &args, void *workspace) {
 
-  /// Ctor
-  FusedMultiHeadAttention() {
+    // Determine grid shape
+    ThreadblockSwizzle threadblock_swizzle;
 
+    cutlass::gemm::GemmCoord grid_shape = threadblock_swizzle.get_tiled_shape(
+      args.problem_size, 
+      {ThreadblockShape::kM, ThreadblockShape::kN, ThreadblockShape::kK},
+      args.split_k_slices);
+
+    // Define a reference to the workspace - this is an aligned region in device memory.
+    if (!workspace) {
+      return Status::kErrorWorkspaceNull;
+    }
+    
+    TensorRef<ElementAccumulator_, layout::RowMajor> ref_workspace(
+      static_cast<ElementAccumulator_ *>(workspace), 
+      args.problem_size.n());
+
+    int64_t partition_stride = int64_t(args.problem_size.m()) * int64_t(args.problem_size.n());
+
+    // Initialize the Params structure
+    gemm_params_ = typename GemmKernel::Params{
+      args.problem_size,
+      grid_shape,
+      args.ref_A.non_const_ref(),
+      args.ref_B.non_const_ref(),
+      ref_workspace,
+      args.convert,
+      partition_stride
+    };
+
+    reduction_params_ = typename ReductionKernel::Params(
+      args.problem_size.mn(),
+      grid_shape.k(),
+      partition_stride,
+      ref_workspace,
+      args.ref_D,
+      args.ref_C.non_const_ref(),
+      args.epilogue
+    );
+
+    return Status::kSuccess;
   }
 
-  /// Initialize
-  Status initialize(Arguments const &args, 
-                    void *workspace0 = nullptr,
-                    void *workspace1 = nullptr) {
-
-    params_ = Params(args);
-
-    typename GemmGrouped0::Arguments args_gemm0(
-      params_.problem_sizes0,
-      params_.problem_count,
-      params_.threadblock_count,
-      params_.ptr_Q,
-      params_.ptr_K,
-      params_.ptr_P,
-      params_.ptr_P,
-      params_.ptr_Max,
-      params_.ptr_Sum,
-      params_.ldq,
-      params_.ldk,
-      params_.ldp,
-      params_.ldp,
-      typename GemmGrouped0::GemmKernel::EpilogueVisitor::Arguments(
-        {
-          params_.alpha0,
-          params_.beta
-        }
-      ),
-      params_.problem_sizes0_host,
-      params_.problem_sizes0_real
-    );
+  /// Lightweight update given a subset of arguments
+  Status update(Arguments const &args, void *workspace = nullptr) {
 
+    if (!workspace) {
+      return Status::kErrorWorkspaceNull;
+    }
 
-    Status result0 = gemm_grouped0.initialize(args_gemm0, workspace0);
+    gemm_params_.ref_A.reset(args.ref_A.data());
+    gemm_params_.ref_B.reset(args.ref_B.data());
+    gemm_params_.ref_D.reset(workspace);     
 
-    typename EpilogueOutputOp1::Params epilogue_op1(params_.alpha1, params_.beta);
+    reduction_params_.ref_D.reset(args.ref_D.data());
+    reduction_params_.ref_C.reset(args.ref_C.data());
 
-    typename GemmGrouped1::Arguments args_gemm1(
-      params_.problem_sizes1,
-      params_.problem_count,
-      params_.threadblock_count,
-      epilogue_op1,
-      params_.ptr_P,
-      params_.ptr_V,
-      params_.ptr_O,
-      params_.ptr_O,
-      (void**)params_.ptr_Max,
-      (void**)params_.ptr_Sum,
-      params_.ldp,
-      params_.ldv,
-      params_.ldo,
-      params_.ldo,
-      params_.problem_sizes1_host
-    );
+    return Status::kSuccess;
+  }
+
+  /// Runs the kernel using initialized state.
+  Status run(cudaStream_t stream = nullptr) {
+
+    //
+    // Launch GEMM kernel
+    //
+
+    ThreadblockSwizzle threadblock_swizzle;
+
+    dim3 grid = threadblock_swizzle.get_grid_shape(gemm_params_.grid_tiled_shape);
+    dim3 block(GemmKernel::kThreadCount, 1, 1);
 
-    Status result1 = gemm_grouped1.initialize(args_gemm1, workspace1);
+    cudaError_t result;
 
-    if ((result0 == cutlass::Status::kSuccess) && (result1 == cutlass::Status::kSuccess) ) {
-      return cutlass::Status::kSuccess;
-    }else{
-      if (result0 != cutlass::Status::kSuccess) {
-        return result0;
-      }else{
-        return result1;
+    int smem_size = int(sizeof(typename GemmKernel::SharedStorage));
+    if (smem_size >= (48 << 10)) {
+
+      result = cudaFuncSetAttribute(
+        Kernel<GemmKernel>,
+        cudaFuncAttributeMaxDynamicSharedMemorySize,
+        smem_size);
+
+      if (result != cudaSuccess) {
+        return Status::kErrorInternal;
       }
     }
-  }
 
-  /// Run
-  Status run(cudaStream_t stream = nullptr) {
+    Kernel<GemmKernel><<<grid, block, smem_size, stream>>>(gemm_params_);
 
-    Status result = gemm_grouped0.run();
-    cudaError_t error_info;
+    result = cudaGetLastError();
 
-    if (result != cutlass::Status::kSuccess) {
-      return cutlass::Status::kErrorInternal;
+    if (result != cudaSuccess) {
+      return Status::kErrorInternal;
     }
 
-    int thread_per_block = 1024;
+    //
+    // Launch reduction kernel
+    //
 
-    dim3 final_reduction_grid(params_.head_number * params_.batch_size);
-    dim3 final_reduction_block(thread_per_block);
+    block = ReductionKernel::block_shape();
+    grid = ReductionKernel::grid_shape(gemm_params_.problem_size.mn());
 
-    cutlass::Kernel<ApplyFinalReductionDevice><<<
-        final_reduction_grid, final_reduction_block, sizeof(typename ApplyFinalReductionDevice::SharedStorage), stream
-      >>>(params_.reduction);
+    Kernel<ReductionKernel><<< grid, block, 0, stream >>>(reduction_params_);
 
-    error_info = cudaGetLastError();
+    result = cudaGetLastError();
 
-    if (error_info != cudaSuccess) {
-      return cutlass::Status::kErrorInternal;
+    if (result != cudaSuccess) {
+      return Status::kErrorInternal;
     }
 
-    result = gemm_grouped1.run();
+    return result == cudaSuccess ? Status::kSuccess : Status::kErrorInternal;
+  }
 
-    if (result != cutlass::Status::kSuccess) {
-      return cutlass::Status::kErrorInternal;
+  /// Runs the kernel using initialized state.
+  Status operator()(cudaStream_t stream = nullptr) {
+    return run(stream);
+  }
+
+  /// Runs the kernel using initialized state.
+  Status operator()(
+    Arguments const &args, 
+    void *workspace = nullptr, 
+    cudaStream_t stream = nullptr) {
+    
+    Status status = initialize(args, workspace);
+    
+    if (status == Status::kSuccess) {
+      status = run(stream);
     }
 
-    return cutlass::Status::kSuccess;
+    return status;
+  }
+};
+
+////////////////////////////////////////////////////////////////////////////////
+
+/// Partial specialization for column-major output
+template <
+    /// Element type for A matrix operand
+    typename ElementA_,
+    /// Layout type for A matrix operand
+    typename LayoutA_,
+    /// Element type for B matrix operand
+    typename ElementB_,
+    /// Layout type for B matrix operand
+    typename LayoutB_,
+    /// Element type for C and D matrix operands
+    typename ElementC_,
+    /// Element type for internal accumulation
+    typename ElementAccumulator_,
+    /// Operator class tag
+    typename OperatorClass_,
+    /// Tag indicating architecture to tune for.  This is the minimum SM that
+      /// supports the intended feature. The device kernel can be built
+      /// targeting any SM larger than this number.
+    typename ArchTag_,
+    /// Threadblock-level tile size (concept: GemmShape)
+    typename ThreadblockShape_,
+    /// Warp-level tile size (concept: GemmShape)
+    typename WarpShape_,
+    /// Instruction-level tile size (concept: GemmShape)
+    typename InstructionShape_,
+    /// Epilogue output operator
+    typename EpilogueOutputOp_,
+    /// Epilogue output operator
+    typename ConvertScaledOp_,
+    /// Reduction operator
+    typename ReductionOp_,
+    /// Threadblock-level swizzling operator
+    typename ThreadblockSwizzle_,
+    /// Number of stages used in the pipelined mainloop
+    int Stages, int kAlignmentA, int kAlignmentB,
+    /// Operation performed by GEMM
+    typename Operator_>
+class GemmSplitKParallel<ElementA_, LayoutA_, ElementB_, LayoutB_, ElementC_,
+                         layout::ColumnMajor, ElementAccumulator_,
+                         OperatorClass_, ArchTag_, ThreadblockShape_,
+                         WarpShape_, InstructionShape_, EpilogueOutputOp_,
+                         ConvertScaledOp_, ReductionOp_, ThreadblockSwizzle_,
+                         Stages, kAlignmentA, kAlignmentB, Operator_> {
+ public:
+
+  using ElementA = ElementA_;
+  using LayoutA = LayoutA_;
+  using ElementB = ElementB_;
+  using LayoutB = LayoutB_;
+  using ElementC = ElementC_;
+  using LayoutC = layout::ColumnMajor;
+  using ElementAccumulator = ElementAccumulator_;
+  using OperatorClass = OperatorClass_;
+  using ArchTag = ArchTag_;
+  using ThreadblockShape = ThreadblockShape_;
+  using WarpShape = WarpShape_;
+  using InstructionShape = InstructionShape_;
+  using ConvertScaledOp = ConvertScaledOp_;
+  using EpilogueOutputOp = EpilogueOutputOp_;
+  using ReductionOp = ReductionOp_;
+  using ThreadblockSwizzle = ThreadblockSwizzle_;
+  using Operator = Operator_;
+  static int const kStages = Stages;
+
+  using UnderlyingOperator = GemmSplitKParallel< 
+    ElementB,
+    typename layout::LayoutTranspose<LayoutB>::type,
+    ElementA,
+    typename layout::LayoutTranspose<LayoutA>::type,
+    ElementC,
+    layout::RowMajor,    
+    ElementAccumulator,
+    OperatorClass,
+    ArchTag,
+    ThreadblockShape,
+    WarpShape,
+    InstructionShape,
+    EpilogueOutputOp,
+    ConvertScaledOp,
+    ReductionOp,
+    ThreadblockSwizzle,
+    Stages,
+    kAlignmentA,
+    kAlignmentB,
+    Operator
+  >;
+
+  using UnderlyingArguments = typename UnderlyingOperator::Arguments;
+  using GemmKernel = typename UnderlyingOperator::GemmKernel;
+  using ReductionKernel = typename UnderlyingOperator::ReductionKernel;
+
+  /// Argument structure
+  struct Arguments {
+
+    //
+    // Data members
+    //
+
+    GemmCoord problem_size;
+    TensorRef<ElementA const, LayoutA> ref_A;
+    TensorRef<ElementB const, LayoutB> ref_B;
+    TensorRef<ElementC const, LayoutC> ref_C;
+    TensorRef<ElementC, LayoutC> ref_D;
+    typename EpilogueOutputOp::Params epilogue;
+    int split_k_slices;
+    typename ConvertScaledOp::Params convert;
+    typename ReductionOp::Params reduction;
+
+    //
+    // Methods
+    //
+
+    /// Default ctor
+    CUTLASS_HOST_DEVICE
+    Arguments() { }
+
+    /// Constructs an Arguments structure 
+    CUTLASS_HOST_DEVICE
+    Arguments(
+      GemmCoord problem_size_,
+      TensorRef<ElementA const, LayoutA> ref_A_,
+      TensorRef<ElementB const, LayoutB> ref_B_,
+      TensorRef<ElementC const, LayoutC> ref_C_,
+      TensorRef<ElementC, LayoutC> ref_D_,
+      typename EpilogueOutputOp::Params epilogue_ = 
+        typename EpilogueOutputOp::Params(),
+      int split_k_slices = 1,
+      typename ConvertScaledOp::Params convert_ = 
+        typename ConvertScaledOp::Params(),
+      typename ReductionOp::Params reduction_ =
+        typename ReductionOp::Params()
+    ):
+      problem_size(problem_size_),
+      ref_A(ref_A_),
+      ref_B(ref_B_),
+      ref_C(ref_C_),
+      ref_D(ref_D_),
+      epilogue(epilogue_),
+      split_k_slices(split_k_slices),
+      convert(convert_),
+      reduction(reduction_) { }
+  };
+
+private:
+
+  /// Kernel parameters object
+  UnderlyingOperator underlying_operator_;
+
+public:
+
+  /// Constructs the GEMM.
+  GemmSplitKParallel() { }
+
+  /// Helper to construct a transposed equivalent for the underying GEMM operator
+  static UnderlyingArguments to_underlying_arguments(Arguments const &args) {
+    return UnderlyingArguments(
+      {args.problem_size.n(), args.problem_size.m(), args.problem_size.k()},
+      {args.ref_B.data(), args.ref_B.stride(0)},
+      {args.ref_A.data(), args.ref_A.stride(0)},
+      {args.ref_C.data(), args.ref_C.stride(0)},
+      {args.ref_D.data(), args.ref_D.stride(0)},
+      args.epilogue,
+      args.split_k_slices,
+      args.convert,
+      args.reduction
+    );
+  }
+
+  /// Determines whether the GEMM can execute the given problem.
+  static Status can_implement(Arguments const &args) {
+
+    return UnderlyingOperator::can_implement(to_underlying_arguments(args));
+  }
+
+  /// Gets the workspace size
+  static size_t get_workspace_size(Arguments const &args) {
+    
+    return UnderlyingOperator::get_workspace_size(to_underlying_arguments(args));
   }
 
-  /// Function call operator
+  /// Initializes GEMM state from arguments.
+  Status initialize(Arguments const &args, void *workspace) {
+
+    return underlying_operator_.initialize(to_underlying_arguments(args), workspace);
+  }
+
+  /// Lightweight update given a subset of arguments
+  Status update(Arguments const &args, void *workspace = nullptr) {
+
+    return underlying_operator_.update(to_underlying_arguments(args), workspace);
+  }
+
+  /// Runs the kernel using initialized state.
+  Status run(cudaStream_t stream = nullptr) {
+
+    return underlying_operator_.run(stream);
+  }
+
+  /// Runs the kernel using initialized state.
   Status operator()(cudaStream_t stream = nullptr) {
     return run(stream);
   }
+
+  /// Runs the kernel using initialized state.
+  Status operator()(
+    Arguments const &args, 
+    void *workspace = nullptr, 
+    cudaStream_t stream = nullptr) {
+    
+    Status status = initialize(args, workspace, stream);
+    
+    if (status == Status::kSuccess) {
+      status = run(stream);
+    }
+
+    return status;
+  }
 };
 
-}
+////////////////////////////////////////////////////////////////////////////////
+
+} // namespace device
+} // namespace gemm
+} // namespace cutlass
+
+////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/41_multi_head_attention/gemm_grouped_with_softmax_visitor.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped.h`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -26,59 +26,55 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-    \brief Grouped GEMM kernel with epilogue visitor customized for softmax
+    \brief Problem visitor for grouped GEMMs
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/fast_math.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/matrix_coord.h"
 #include "cutlass/complex.h"
 #include "cutlass/semaphore.h"
-#include "cutlass/util/device_memory.h"
+
 #include "cutlass/layout/matrix.h"
 #include "cutlass/trace.h"
 #include "cutlass/gemm/kernel/gemm_transpose_operands.h"
 #include "cutlass/gemm/kernel/gemm_grouped_problem_visitor.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace kernel {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
-  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate 
-  typename Epilogue_,             ///! Epilogue
-  typename ThreadblockSwizzle_,   ///! Threadblock swizzling function
-  GroupScheduleMode GroupScheduleMode_, ///! Type of scheduling to perform
-  bool Transposed_ = false,
-  bool UseMask_ = false
+  typename Mma_,                           ///! Threadblock-scoped matrix multiply-accumulate
+  typename Epilogue_,                      ///! Epilogue
+  typename ThreadblockSwizzle_,            ///! Threadblock swizzling function
+  GroupScheduleMode GroupScheduleMode_,    ///! Type of scheduling to perform
+  bool Transposed = false
 >
-struct GemmGroupedWithEpilogueVistor {
+struct GemmGrouped {
 public:
 
   using Mma = Mma_;
   using Epilogue = Epilogue_;
+  using EpilogueOutputOp = typename Epilogue::OutputOp;
   using ThreadblockSwizzle = ThreadblockSwizzle_;
-
   static GroupScheduleMode const kGroupScheduleMode = GroupScheduleMode_;
-
-  using EpilogueVisitor = typename Epilogue::Visitor;
-  using EpilogueOutputOp = typename EpilogueVisitor::ElementwiseFunctor;
-  static bool const kTransposed = Transposed_;
+  static bool const kTransposed = Transposed;
 
   // Optional transpose
   using MapArguments = kernel::detail::MapArguments<
     typename Mma::IteratorA::Element,
     typename Mma::IteratorA::Layout,
     Mma::kTransformA,
     Mma::IteratorA::AccessType::kElements,
@@ -92,35 +88,32 @@
 
   // Public-facing type definitions related to operand element type, layout, and complex conjugate
   // operation. Must interact with the 'kTransposed' notion.
   using ElementA = typename MapArguments::ElementA;
   using LayoutA = typename MapArguments::LayoutA;
   using ElementB = typename MapArguments::ElementB;
   using LayoutB = typename MapArguments::LayoutB;
-  using ElementC = typename EpilogueVisitor::ElementOutput;
+  using ElementC = typename Epilogue::OutputTileIterator::Element;
   using LayoutC = typename MapArguments::LayoutC;
 
-  using ElementNorm = typename EpilogueVisitor::ElementNorm;
-  using ElementSum = typename EpilogueVisitor::ElementSum;
-
   static ComplexTransform const kTransformA = MapArguments::kTransformA;
   static ComplexTransform const kTransformB = MapArguments::kTransformB;
 
   // Type definitions about the mainloop.
   using Operator = typename Mma::Operator;
   using OperatorClass = typename Mma::Operator::OperatorClass;
   using ThreadblockShape = typename Mma::Shape;
   using WarpShape = typename Mma::Operator::Shape;
   using InstructionShape = typename Mma::Policy::Operator::InstructionShape;
   using ArchTag = typename Mma::ArchTag;
 
   static int const kStages = Mma::kStages;
   static int const kAlignmentA = MapArguments::kAlignmentA;
   static int const kAlignmentB = MapArguments::kAlignmentB;
-  static int const kAlignmentC = EpilogueVisitor::kElementsPerAccess;
+  static int const kAlignmentC = Epilogue::OutputTileIterator::kElementsPerAccess;
 
   /// Warp count (concept: GemmShape)
   using WarpCount = typename Mma::WarpCount;
   static int const kThreadCount = 32 * WarpCount::kCount;
 
   using ProblemVisitor = GemmGroupedProblemVisitor<
                             ThreadblockShape,
@@ -137,53 +130,45 @@
   struct Arguments {
 
     //
     // Data members
     //
 
     GemmCoord *problem_sizes;
-    // when using mask, real problem sizes may not be aligned
-    // then we need to mask out unpadded elements in softmax
-    GemmCoord *problem_sizes_real;
     int problem_count;
     int threadblock_count;
 
+    typename EpilogueOutputOp::Params output_op;
+
     ElementA ** ptr_A;
     ElementB ** ptr_B;
     ElementC ** ptr_C;
     ElementC ** ptr_D;
 
-    ElementNorm **ptr_Max;
-    ElementSum **ptr_Sum;
-
     typename LayoutA::Stride::LongIndex *lda;
     typename LayoutB::Stride::LongIndex *ldb;
     typename LayoutC::Stride::LongIndex *ldc;
     typename LayoutC::Stride::LongIndex *ldd;
 
-    typename EpilogueVisitor::Arguments epilogue_visitor;
-
     // Only used by device-level operator
     GemmCoord *host_problem_sizes;
 
     //
     // Methods
     //
 
     /// Default ctor
     CUTLASS_HOST_DEVICE
     Arguments(): 
-      problem_count(0), 
+      problem_count(0),
       threadblock_count(0), 
-      ptr_A(nullptr),
-      ptr_B(nullptr),
-      ptr_C(nullptr),
-      ptr_D(nullptr),
-      ptr_Max(nullptr),
-      ptr_Sum(nullptr),
+      ptr_A(nullptr), 
+      ptr_B(nullptr), 
+      ptr_C(nullptr), 
+      ptr_D(nullptr), 
       lda(nullptr),
       ldb(nullptr),
       ldc(nullptr),
       ldd(nullptr),
       host_problem_sizes(nullptr)
     {
 
@@ -191,205 +176,179 @@
 
     /// Ctor
     CUTLASS_HOST_DEVICE
     Arguments(    
       GemmCoord *problem_sizes,
       int problem_count,
       int threadblock_count,
+      typename EpilogueOutputOp::Params output_op,
       ElementA ** ptr_A,
       ElementB ** ptr_B,
       ElementC ** ptr_C,
       ElementC ** ptr_D,
-      ElementNorm **ptr_Max,
-      ElementSum **ptr_Sum,
       typename LayoutA::Stride::LongIndex *lda,
       typename LayoutB::Stride::LongIndex *ldb,
       typename LayoutC::Stride::LongIndex *ldc,
       typename LayoutC::Stride::LongIndex *ldd,
-      typename EpilogueVisitor::Arguments epilogue_visitor_,
-      GemmCoord *host_problem_sizes=nullptr,
-      GemmCoord *problem_sizes_real=nullptr
+      GemmCoord *host_problem_sizes=nullptr
     ): 
       problem_sizes(problem_sizes),
       problem_count(problem_count),
       threadblock_count(threadblock_count),
+      output_op(output_op),
       ptr_A(ptr_A),
       ptr_B(ptr_B),
       ptr_C(ptr_C),
       ptr_D(ptr_D),
-      ptr_Max(ptr_Max),
-      ptr_Sum(ptr_Sum),
       lda(lda),
       ldb(ldb),
       ldc(ldc),
       ldd(ldd),
-      epilogue_visitor(epilogue_visitor_),
-      host_problem_sizes(host_problem_sizes),
-      problem_sizes_real(problem_sizes_real)
+      host_problem_sizes(host_problem_sizes)
     {
 
     }
   };
 
   //
   // Structure for precomputing values in host memory and passing to kernels
   //
 
   /// Parameters structure
   struct Params {
 
     typename ProblemVisitor::Params problem_visitor;
-    GemmCoord *problem_sizes_real;
     int threadblock_count;
 
+    typename EpilogueOutputOp::Params output_op;
+
     ElementA ** ptr_A;
     ElementB ** ptr_B;
     ElementC ** ptr_C;
     ElementC ** ptr_D;
 
-    ElementNorm **ptr_Max;
-    ElementSum **ptr_Sum;
-
     typename LayoutA::Stride::LongIndex *lda;
     typename LayoutB::Stride::LongIndex *ldb;
     typename LayoutC::Stride::LongIndex *ldc;
     typename LayoutC::Stride::LongIndex *ldd;
 
-    typename EpilogueVisitor::Params epilogue_visitor;
-
     //
     // Methods
     //
 
     CUTLASS_HOST_DEVICE
     Params():
       ptr_A(nullptr),
       ptr_B(nullptr),
       ptr_C(nullptr),
       ptr_D(nullptr),
-      ptr_Max(nullptr),
-      ptr_Sum(nullptr),
       lda(nullptr),
       ldb(nullptr),
       ldc(nullptr),
-      ldd(nullptr),
-      problem_sizes_real(problem_sizes_real)
+      ldd(nullptr)
     { }
 
     CUTLASS_HOST_DEVICE
-    Params(Arguments const &args, void *workspace = nullptr, int32_t tile_count = 0):
+    Params(Arguments const &args,
+          void *workspace = nullptr,
+          int tile_count = 0):
       problem_visitor(args.problem_sizes, args.problem_count, workspace, tile_count),
       threadblock_count(args.threadblock_count),
+      output_op(args.output_op),
       ptr_A(args.ptr_A),
       ptr_B(args.ptr_B),
       ptr_C(args.ptr_C),
       ptr_D(args.ptr_D),
-      ptr_Max(args.ptr_Max),
-      ptr_Sum(args.ptr_Sum),
       lda(args.lda),
       ldb(args.ldb),
       ldc(args.ldc),
-      ldd(args.ldd),
-      epilogue_visitor(args.epilogue_visitor),
-      problem_sizes_real(args.problem_sizes_real)
+      ldd(args.ldd)
     { 
 
     }
 
     CUTLASS_HOST_DEVICE
     void update(
       Arguments const &args,
       void *workspace = nullptr,
-      int32_t tile_count = -1) {
+      int tile_count = 0) {
 
-      problem_visitor = typename ProblemVisitor::Params(args.problem_sizes, args.problem_count, workspace, tile_count);
+      problem_visitor = typename ProblemVisitor::Params(args.problem_sizes, args.problem_count,
+                                                        workspace, tile_count);
       threadblock_count = args.threadblock_count;
+      output_op = args.output_op;
       ptr_A = args.ptr_A;
       ptr_B = args.ptr_B;
       ptr_C = args.ptr_C;
       ptr_D = args.ptr_D;
-      ptr_Max = args.ptr_Max;
-      ptr_Sum = args.ptr_Sum;
       lda = args.lda;
       ldb = args.ldb;
       ldc = args.ldc;
       ldd = args.ldd;
-      problem_sizes_real = args.problem_sizes_real;
     }
   };
 
   /// Shared memory storage structure
   struct SharedStorage {
     union {
       typename Mma::SharedStorage main_loop;
-      struct {
-        typename Epilogue::SharedStorage epilogue;
-        typename EpilogueVisitor::SharedStorage visitor;
-      } epilogue;
+      typename Epilogue::SharedStorage epilogue;
     } kernel;
 
     // ProblemVisitor shared storage can't be overlapped with others
     typename ProblemVisitor::SharedStorage problem_visitor;
   };
 
-
 public:
 
   //
   // Methods
   //
 
   CUTLASS_DEVICE
-  GemmGroupedWithEpilogueVistor() { } 
+  GemmGrouped() { } 
 
   /// Determines whether kernel satisfies alignment
   static Status can_implement(cutlass::gemm::GemmCoord const & problem_size) {
     return Status::kSuccess;
   }
 
   static Status can_implement(Arguments const &args) {
     return Status::kSuccess;
   }
-
-  static size_t get_extra_workspace_size(
-    Arguments const &args,
-    cutlass::gemm::GemmCoord const &grid_tiled_shape) {
-
-    return 0;
-  }
  
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
     //
     // These types shadow the type-level definitions and support the ability to implement
     // a 'transposed' GEMM that computes the transposed problems.
     //
     using ElementA = typename Mma::IteratorA::Element;
     using LayoutA = typename Mma::IteratorA::Layout;
     using ElementB = typename Mma::IteratorB::Element;
     using LayoutB = typename Mma::IteratorB::Layout;
-    using ElementC = typename EpilogueVisitor::ElementOutput;
-    using LayoutC = typename Mma::LayoutC;
+    using ElementC = typename Epilogue::OutputTileIterator::Element;
+    using LayoutC = typename Epilogue::OutputTileIterator::Layout;
 
     //
     // Problem visitor.
     //
     ProblemVisitor problem_visitor(
       params.problem_visitor,
       shared_storage.problem_visitor,
       blockIdx.x);
 
     // Outer 'persistent' loop to iterate over tiles
     while (problem_visitor.next_tile()) {
 
-      GemmCoord problem_size = problem_visitor.problem_size();
-      int32_t problem_idx    = problem_visitor.problem_index();
-      int32_t threadblock_idx        = int32_t(problem_visitor.threadblock_idx());
+      GemmCoord problem_size  = problem_visitor.problem_size();
+      int32_t problem_idx     = problem_visitor.problem_index();
+      int32_t threadblock_idx = int32_t(problem_visitor.threadblock_idx());
 
       GemmCoord grid_shape = problem_visitor.grid_shape(problem_size);
 
       cutlass::gemm::GemmCoord threadblock_offset(
         int(threadblock_idx / grid_shape.n()) * Mma::Shape::kM,
         int(threadblock_idx % grid_shape.n()) * Mma::Shape::kN,
         0);
@@ -457,59 +416,59 @@
       mma(
         gemm_k_iterations, 
         accumulators, 
         iterator_A, 
         iterator_B, 
         accumulators);
 
+      //
+      // Epilogue
+      //
+
+      EpilogueOutputOp output_op(params.output_op);
+
       ElementC *ptr_C = params.ptr_C[problem_idx];
       ElementC *ptr_D = params.ptr_D[problem_idx];
 
-      ElementNorm *ptr_Max = params.ptr_Max[problem_idx];
-      ElementSum *ptr_Sum = params.ptr_Sum[problem_idx];
-
       LayoutC layout_C(params.ldc[problem_idx]);
       LayoutC layout_D(params.ldd[problem_idx]);
 
-      int column_offset = (threadblock_offset.n() / ThreadblockShape::kN) * problem_size.m();
-
-      typename EpilogueVisitor::OutputTileIterator::Params params_C(layout_C);
-      typename EpilogueVisitor::OutputTileIterator::Params params_D(layout_D);
-
-      //
-      // Construct the epilogue visitor
-      //
+      typename Epilogue::OutputTileIterator::Params params_C(layout_C);
+      typename Epilogue::OutputTileIterator::Params params_D(layout_D);
 
-      EpilogueVisitor epilogue_visitor(
-        params.epilogue_visitor,
-        shared_storage.kernel.epilogue.visitor,
+      // Tile iterator loading from source tensor.
+      typename Epilogue::OutputTileIterator iterator_C(
+        params_C,
+        ptr_C,
         problem_size.mn(),
         thread_idx,
-        warp_idx,
-        lane_idx,
-        params_C,
+        threadblock_offset.mn()
+      );
+
+      // Tile iterator writing to destination tensor.
+      typename Epilogue::OutputTileIterator iterator_D(
         params_D,
-        ptr_C,
         ptr_D,
-        ptr_Max,
-        ptr_Sum,
-        threadblock_offset.mn(),
-        column_offset,
-        params.problem_sizes_real[problem_idx].mn()
+        problem_size.mn(),
+        thread_idx,
+        threadblock_offset.mn()
       );
 
-      // Construct the epilogue
       Epilogue epilogue(
-        shared_storage.kernel.epilogue.epilogue, 
+        shared_storage.kernel.epilogue, 
         thread_idx, 
         warp_idx, 
         lane_idx);
 
-      // Execute the epilogue operator to update the destination tensor
-      epilogue(epilogue_visitor, accumulators);
+      // Execute the epilogue operator to update the destination tensor.
+      epilogue(
+        output_op, 
+        iterator_D, 
+        accumulators, 
+        iterator_C); 
 
       // Next tile
       problem_visitor.advance(gridDim.x);
     }
   }
 };
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/42_ampere_tensorop_group_conv/ampere_tensorop_group_conv.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/42_ampere_tensorop_group_conv/ampere_tensorop_group_conv.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/43_ell_block_sparse_gemm/ell_block_sparse_gemm.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/43_ell_block_sparse_gemm/ell_block_sparse_gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_bias_act_epilogue_tensor_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_bias_act_epilogue_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_thread_map_tensor_op_for_fused_bias.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_thread_map_tensor_op_for_fused_bias.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/fused_bias_act_epilogue.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/fused_bias_act_epilogue.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/output_tile_thread_map_for_fused_bias.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/output_tile_thread_map_for_fused_bias.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/warp/fused_bias_act_fragment_iterator_tensor_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/warp/fused_bias_act_fragment_iterator_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/warp/mma_tensor_op_fragment_iterator_without_output_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/warp/mma_tensor_op_fragment_iterator_without_output_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/leaky_bias.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/leaky_bias.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/utils.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/utils.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/45_dual_gemm/device/dual_gemm.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/device/dual_gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm_common.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/index_sequence.h`

 * *Files 10% similar despite different names*

```diff
@@ -24,29 +24,15 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/*! \file
-    \brief Defines common types used for all DualGemm operators.
-*/
-#pragma once
-
-namespace cutlass {
-namespace gemm {
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-enum class DualGemmMode {
-  kGemm,
-  kBatched,
-  kInvalid
-};
+#pragma once
 
-////////////////////////////////////////////////////////////////////////////////////////////////////
+#include "cutlass/cutlass.h"
+#include "cutlass/numeric_types.h"
 
-} // namespace gemm
-} // namespace cutlass
+// integer_sequence moved to cutlass/numeric_types.h
 
-////////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm_run.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm_run.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/45_dual_gemm/kernel/dual_gemm.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/kernel/dual_gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/45_dual_gemm/test_run.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/test_run.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/45_dual_gemm/thread/left_silu_and_mul.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/thread/left_silu_and_mul.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_epilogue.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_epilogue.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_base.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_base.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_multistage.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/46_depthwise_simt_conv2dfprop/depthwise_simt_conv2dfprop.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/46_depthwise_simt_conv2dfprop/depthwise_simt_conv2dfprop.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/47_ampere_gemm_universal_streamk/ampere_gemm_universal_streamk.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/47_ampere_gemm_universal_streamk/ampere_gemm_universal_streamk.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/48_hopper_warp_specialized_gemm/48_hopper_warp_specialized_gemm.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_rank_k_universal.h`

 * *Files 22% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2023 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,440 +24,486 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
 /*! \file
-    \brief Simple Hopper GEMM example using CUTLASS 3.0 APIs for NVIDIA Hopper architecture
-
-    This example demonstrate a simple way to instantiate and run a TF32 GEMM using the new CUTLASS 3.0
-    APIs on NVIDIA Hopper architecture. New features that will be showcased in this example are as follows:
-
-    1. NVIDIA Hopper architecture introduces a new series of tensor core instructions (GMMA) 
-    which are more efficient than the Ampere tensor core instructions.
-
-    2. NVIDIA Hopper architecture includes new Tensor Memory Accelerator (TMA) unit to transfer large 
-    blocks of data efficiently between global memory and shared memory. TMA also supports asynchronous
-    copies between thread blocks in a cluster. Another advantage is that TMA can load in FP32 data and
-    convert them implicitly to TF32.
-
-    3. This example uses the Warp Specialized kernel design (see /media/docs/efficient_gemm.md for details).
-
-    Examples:
-
-      $ ./examples/48_hopper_warp_specialized_gemm/48_hopper_warp_specialized_gemm --m=2048 --n=2048 --k=2048
+    \brief Tests for device-wide Rank 2k update interface
+  
 */
 
-#include <iostream>
+#pragma once
 
-#include "cutlass/cutlass.h"
+#include <iostream>
+#include <fstream>
+#include <sstream>
 
-#include "cute/tensor.hpp"
-#include "cutlass/tensor_ref.h"
-#include "cutlass/epilogue/collective/default_epilogue.hpp"
-#include "cutlass/epilogue/thread/linear_combination.h"
-#include "cutlass/gemm/dispatch_policy.hpp"
-#include "cutlass/gemm/collective/collective_builder.hpp"
-#include "cutlass/gemm/device/gemm_universal_adapter.h"
-#include "cutlass/gemm/kernel/gemm_universal.hpp"
+#include "../../common/cutlass_unit_test.h"
+#include "cutlass/blas3.h"
 
-#include "cutlass/util/command_line.h"
-#include "cutlass/util/distribution.h"
 #include "cutlass/util/host_tensor.h"
-#include "cutlass/util/packed_stride.hpp"
 #include "cutlass/util/tensor_view_io.h"
-#include "cutlass/util/reference/device/gemm.h"
-#include "cutlass/util/reference/device/tensor_compare.h"
-#include "cutlass/util/reference/device/tensor_fill.h"
-
-#include "helper.h"
-
-using namespace cute;
-
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#include "cutlass/util/distribution.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/reference/host/tensor_copy.h"
+#include "cutlass/util/reference/host/tensor_compare.h"
+#include "cutlass/util/reference/host/tensor_norm.h"
+#include "cutlass/util/reference/host/error_metrics.h"
+#include "cutlass/util/reference/host/rank_k_complex.h"
+
+#include "testbed_utils.h"
+
+namespace test {
+namespace gemm {
+namespace device {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-/// GEMM kernel configurations
-/////////////////////////////////////////////////////////////////////////////////////////////////
 
-// A matrix configuration
-using         ElementA    = float;                                          // Element type for A matrix operand
-using         LayoutA     = cutlass::layout::RowMajor;                      // Layout type for A matrix operand
-constexpr int AlignmentA  = 128 / cutlass::sizeof_bits<ElementA>::value;    // Memory access granularity/alignment of A matrix in units of elements (up to 16 bytes)
-
-// B matrix configuration
-using         ElementB    = float;                                          // Element type for B matrix operand
-using         LayoutB     = cutlass::layout::ColumnMajor;                   // Layout type for B matrix operand
-constexpr int AlignmentB  = 128 / cutlass::sizeof_bits<ElementB>::value;    // Memory access granularity/alignment of B matrix in units of elements (up to 16 bytes)
-
-// C/D matrix configuration
-using         ElementC    = float;                                          // Element type for C and D matrix operands
-using         LayoutC     = cutlass::layout::ColumnMajor;                   // Layout type for C and D matrix operands
-
-// Core kernel configurations
-using ElementAccumulator  = float;                                          // Element type for internal accumulation
-using ArchTag             = cutlass::arch::Sm90;                            // Tag indicating the minimum SM that supports the intended feature
-using OperatorClass       = cutlass::arch::OpClassTensorOp;                 // Operator class tag
-using TilesShape          = Shape<_128,_128,_32>;                           // Threadblock-level tile size
-using ClusterShape        = Shape<_1,_2,_1>;                                // Shape of the threadblocks in a cluster
-using StageCountType = cutlass::gemm::collective::StageCountAuto;           // Stage count maximized based on the tile size
-using KernelSchedule = cutlass::gemm::collective::KernelScheduleAuto;       // Kernel to launch based on the default setting in the Collective Builder 
-
-using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
-    ArchTag, OperatorClass,
-    ElementA, LayoutA, AlignmentA,
-    ElementB, LayoutB, AlignmentB,
-    ElementAccumulator,
-    TilesShape, ClusterShape,
-    cutlass::gemm::collective::StageCountAuto,
-    cutlass::gemm::collective::KernelScheduleAuto
-  >::CollectiveOp;
-
-using CollectiveEpilogue = cutlass::epilogue::collective::DefaultEpilogue<
-    cutlass::gemm::TagToStrideC_t<LayoutC>,
-    cutlass::gemm::TagToStrideC_t<LayoutC>,
-    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>>;
-
-using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-    Shape<int,int,int>, // Indicates ProblemShape
-    CollectiveMainloop,
-    CollectiveEpilogue
->;
-
-using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-
-// Reference device GEMM implementation type
-using DeviceGemmReference = cutlass::reference::device::Gemm<
-  ElementA,
-  LayoutA,
-  ElementB,
-  LayoutB,
-  ElementC,
-  LayoutC,
-  ElementAccumulator,
-  ElementAccumulator>;
-
-using StrideA = typename Gemm::GemmKernel::StrideA;
-using StrideB = typename Gemm::GemmKernel::StrideB;
-using StrideC = typename Gemm::GemmKernel::StrideC;
-using StrideD = typename Gemm::GemmKernel::StrideD;
-
-//
-// Data members
-//
-
-/// Initialization
-StrideA stride_A;
-StrideB stride_B;
-StrideC stride_C;
-StrideD stride_D;
-uint64_t seed;
-
-cutlass::DeviceAllocation<typename Gemm::ElementA> block_A;
-cutlass::DeviceAllocation<typename Gemm::ElementB> block_B;
-cutlass::DeviceAllocation<typename Gemm::ElementC> block_C;
-cutlass::DeviceAllocation<typename Gemm::EpilogueOutputOp::ElementOutput> block_D;
-cutlass::DeviceAllocation<typename Gemm::EpilogueOutputOp::ElementOutput> block_ref_D;
+template <typename RankK>
+struct TestbedRank2KUniversal {
 
-#endif // defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+  using ElementAccumulator = typename RankK::ElementAccumulator;
+  using ElementCompute = typename RankK::RankKkernel::Epilogue::OutputOp::ElementCompute;
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-/// Testbed utility types
-/////////////////////////////////////////////////////////////////////////////////////////////////
+  /// Initialization
+  cutlass::Distribution::Kind init_A;
+  cutlass::Distribution::Kind init_C;
+  uint64_t seed;
+
+  cutlass::HostTensor<typename RankK::ElementA, typename RankK::LayoutA> tensor_A;
+  cutlass::HostTensor<typename RankK::ElementC, typename RankK::LayoutC> tensor_C;
+  cutlass::HostTensor<typename RankK::ElementC, typename RankK::LayoutC> tensor_D;
+  cutlass::HostTensor<typename RankK::ElementC, typename RankK::LayoutC> reference_D;
 
-// Command line options parsing
-struct Options {
+  //
+  // Methods
+  //
 
-  bool help;
+  TestbedRank2KUniversal(
+    cutlass::Distribution::Kind init_A_ = cutlass::Distribution::Uniform,
+    cutlass::Distribution::Kind init_C_ = cutlass::Distribution::Uniform,
+    uint64_t seed_ = 2080
+  ):
+    init_A(init_A_), init_C(init_C_), seed(seed_) { }
+
+  /// Helper to initialize a tensor view
+  template <typename Element, typename Layout>
+  bool initialize_tensor(
+    cutlass::TensorView<Element, Layout> view, 
+    cutlass::Distribution::Kind dist_kind,
+    uint64_t seed,
+    int mantissa_in_bits) {
+
+    if (dist_kind == cutlass::Distribution::Uniform) {
+
+      double scope_max, scope_min;
+      int bits_input = cutlass::sizeof_bits<Element>::value;
+      int bits_output = cutlass::sizeof_bits<typename RankK::ElementC>::value;
+
+      if (bits_input == 1) {
+        scope_max = 2;
+        scope_min = 0;
+      } else if (bits_input <= 8) {
+        scope_max = 2;
+        scope_min = -2;
+      } else if (bits_output == 16) {
+        scope_max = 5;
+        scope_min = -5;
+      } else {
+        scope_max = 8;
+        scope_min = -8;
+      }
+
+      cutlass::reference::host::TensorFillRandomUniform(
+        view, seed, scope_max, scope_min, mantissa_in_bits);
+    } 
+    else if (dist_kind == cutlass::Distribution::Identity) {
+
+      cutlass::reference::host::TensorFillIdentity(view);
+    } 
+    else if (dist_kind == cutlass::Distribution::Gaussian) {
 
-  float alpha, beta;
-  int iterations;
-  int m, n, k;
-
-  Options():
-    help(false),
-    m(5120), n(4096), k(4096),
-    alpha(1.f), beta(0.f),
-    iterations(1000)
-  { }
-
-  // Parses the command line
-  void parse(int argc, char const **args) {
-    cutlass::CommandLine cmd(argc, args);
-
-    if (cmd.check_cmd_line_flag("help")) {
-      help = true;
-      return;
+      cutlass::reference::host::TensorFillRandomGaussian(view, seed, 0, 0.5, mantissa_in_bits);
     }
+    else if (dist_kind == cutlass::Distribution::Sequential) {
 
-    cmd.get_cmd_line_argument("m", m);
-    cmd.get_cmd_line_argument("n", n);
-    cmd.get_cmd_line_argument("k", k);
-    cmd.get_cmd_line_argument("alpha", alpha, 1.f);
-    cmd.get_cmd_line_argument("beta", beta, 0.f);
-    cmd.get_cmd_line_argument("iterations", iterations);
-  }
+      cutlass::reference::host::BlockFillSequential(
+        view.data(), view.capacity());
+    } 
+    else {
 
-  /// Prints the usage statement.
-  std::ostream & print_usage(std::ostream &out) const {
-
-    out << "48_hopper_warp_specialized_gemm\n\n"
-      << "  Hopper FP32 GEMM using a Warp Specialized kernel.\n\n"
-      << "Options:\n\n"
-      << "  --help                      If specified, displays this usage statement\n\n"
-      << "  --m=<int>                   Sets the M extent of the GEMM\n"
-      << "  --n=<int>                   Sets the N extent of the GEMM\n"
-      << "  --k=<int>                   Sets the K extent of the GEMM\n"
-      << "  --alpha=<f32>               Epilogue scalar alpha\n"
-      << "  --beta=<f32>                Epilogue scalar beta\n\n"
-      << "  --iterations=<int>          Number of profiling iterations to perform.\n\n";
-
-    out
-      << "\n\nExamples:\n\n"
-      << "$ " << "48_hopper_warp_specialized_gemm" << " --m=1024 --n=512 --k=1024 --alpha=2 --beta=0.707 \n\n";
-
-    return out;
-  }
+      EXPECT_TRUE(false) << "Input distribution not implemented";
+      return false;
+    }
 
-  /// Compute performance in GFLOP/s
-  double gflops(double runtime_s) const
-  {
-    // Two flops per multiply-add
-    uint64_t flop = uint64_t(2) * m * n * k;
-    double gflop = double(flop) / double(1.0e9);
-    return gflop / runtime_s;
+    return true;
   }
-};
 
-/// Result structure
-struct Result
-{
-  double avg_runtime_ms;
-  double gflops;
-  cutlass::Status status;
-  cudaError_t error;
-  bool passed;
-
-  Result(
-    double avg_runtime_ms = 0,
-    double gflops = 0,
-    cutlass::Status status = cutlass::Status::kSuccess,
-    cudaError_t error = cudaSuccess)
-  :
-    avg_runtime_ms(avg_runtime_ms), gflops(gflops), status(status), error(error), passed(false)
-  {}
 
-};
+  /// Helper to initialize a tensor view
+  template <typename Element, typename Layout>
+  bool initialize_symmetric_tensor(
+    cutlass::TensorView<Element, Layout> view, 
+    cutlass::Distribution::Kind dist_kind,
+    uint64_t seed,
+    int mantissa_in_bits) {
+
+    if (dist_kind == cutlass::Distribution::Uniform) {
+
+      double scope_max, scope_min;
+      int bits_input = cutlass::sizeof_bits<Element>::value;
+      int bits_output = cutlass::sizeof_bits<typename RankK::ElementC>::value;
+
+      if (bits_input == 1) {
+        scope_max = 2;
+        scope_min = 0;
+      } else if (bits_input <= 8) {
+        scope_max = 2;
+        scope_min = -2;
+      } else if (bits_output == 16) {
+        scope_max = 5;
+        scope_min = -5;
+      } else {
+        scope_max = 8;
+        scope_min = -8;
+      }
+
+      cutlass::reference::host::TensorFillSymmetricRandomUniform(
+        view, seed, RankK::kFillModeC, scope_max, scope_min, mantissa_in_bits);
+    } 
+    else if (dist_kind == cutlass::Distribution::Gaussian) {
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+      cutlass::reference::host::TensorFillSymmetricRandomGaussian(
+        view, seed, RankK::kFillModeC, 0, 0.5, mantissa_in_bits);
+    }
+    else {
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-/// GEMM setup and evaluation
-/////////////////////////////////////////////////////////////////////////////////////////////////
+      EXPECT_TRUE(false) << "Input distribution (symmetric tensor) not implemented";
+      return false;
+    }
 
-/// Helper to initialize a block of device data
-template <class Element>
-bool initialize_block(
-  cutlass::DeviceAllocation<Element>& block,
-  uint64_t seed=2023) {
-
-  Element scope_max, scope_min;
-  int bits_input = cutlass::sizeof_bits<Element>::value;
-
-  if (bits_input == 1) {
-    scope_max = 2;
-    scope_min = 0;
-  } else if (bits_input <= 8) {
-    scope_max = 2;
-    scope_min = -2;
-  } else {
-    scope_max = 8;
-    scope_min = -8;
+    return true;
+  }
+  /// Initializes data structures
+  void initialize(cutlass::gemm::GemmCoord problem_size) {
+    //
+    // Allocate the RankK workspace
+    //
+
+    tensor_A.resize(problem_size.mk());
+    tensor_C.resize(problem_size.mn());
+    tensor_D.resize(problem_size.mn());
+    reference_D.resize(problem_size.mn(), false);
+
+    EXPECT_TRUE(initialize_tensor(tensor_A.host_view(), init_A, seed + 2019, cutlass::MantissaInBits<typename RankK::ElementA>::bits));
+    EXPECT_TRUE(initialize_symmetric_tensor(tensor_C.host_view(), init_C, seed + 2017, cutlass::MantissaInBits<typename RankK::ElementC>::bits));
+
+    // It is possible to randomly initialize to all zeros, so override this with non-zeros
+    // in the upper left corner of each operand.
+    tensor_A.host_view().at({0, 0}) = typename RankK::ElementA(1);
+    tensor_C.host_view().at({0, 0}) = typename RankK::ElementC(1);
+
+    cutlass::reference::host::TensorCopy(reference_D.host_view(), tensor_C.host_view());
+
+    tensor_A.sync_device();
+    tensor_C.sync_device();
+    tensor_D.sync_device();
   }
 
-  cutlass::reference::device::BlockFillRandomUniform(
-    block.get(), block.size(), seed, scope_max, scope_min, 0);
+  /// Compares computed reference with device reference and outputs to a file if incorrect
+  bool compare_reference(
+    cutlass::gemm::GemmCoord problem_size,
+    ElementCompute alpha, 
+    ElementCompute beta) {
 
-  return true;
-}
+    tensor_D.sync_host();
 
-/// Initialize operands to be used in the GEMM and reference GEMM
-void initialize(const Options &options) {
+    EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_A.host_view()), 0);
+    EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_C.host_view()), 0);
 
-  stride_A = make_cute_packed_stride(StrideA{}, cute::make_shape(options.m, options.k, Int<1>{}));
-  stride_B = make_cute_packed_stride(StrideB{}, cute::make_shape(options.n, options.k, Int<1>{}));
-  stride_C = make_cute_packed_stride(StrideC{}, cute::make_shape(options.m, options.n, Int<1>{}));
-  stride_D = make_cute_packed_stride(StrideD{}, cute::make_shape(options.m, options.n, Int<1>{}));
-
-  block_A.reset(options.m * options.k);
-  block_B.reset(options.k * options.n);
-  block_C.reset(options.m * options.n);
-  block_D.reset(options.m * options.n);
-  block_ref_D.reset(options.m * options.n);
-
-  initialize_block(block_A, seed + 2023);
-  initialize_block(block_B, seed + 2022);
-  initialize_block(block_C, seed + 2021);
-}
+    if (tensor_D.size() > 1)
+      EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_D.host_view()), 0);
 
-/// Populates a Gemm::Arguments structure from the given commandline options
-typename Gemm::Arguments args_from_options(const Options &options)
-{
-  typename Gemm::Arguments arguments{
-    cutlass::gemm::GemmUniversalMode::kGemm,
-    {options.m, options.n, options.k},
-    block_A.get(),
-    stride_A,
-    block_B.get(),
-    stride_B,
-    {block_C.get(), stride_C, block_D.get(), stride_D, {options.alpha, options.beta}}
-  };
+    if (reference_D.size() > 1)
+      EXPECT_GT(cutlass::reference::host::TensorNorm(reference_D.host_view()), 0);
 
-  return arguments;
-}
+    double l2_norm = cutlass::reference::host::TensorRelativeErrorMetric(reference_D.host_view(), tensor_D.host_view());
 
-bool verify(const Options &options) {
-  cutlass::TensorRef ref_A(block_A.get(), Gemm::LayoutA::packed({options.m, options.k}));
-  cutlass::TensorRef ref_B(block_B.get(), Gemm::LayoutB::packed({options.n, options.k}));
-  cutlass::TensorRef ref_C(block_C.get(), Gemm::LayoutC::packed({options.m, options.n}));
-  cutlass::TensorRef ref_D(block_ref_D.get(), Gemm::LayoutD::packed({options.m, options.n}));
+    bool passed = l2_norm < cutlass::MantissaInBits<typename RankK::ElementA>::error;
 
-  //
-  // Compute reference output
-  //
+    return passed;
+  }
 
-  // Create instantiation for device reference gemm kernel
-  DeviceGemmReference gemm_reference;
+  /// Verifies the result is a RankK
+  bool verify(
+    cutlass::gemm::GemmCoord problem_size, 
+    ElementCompute alpha, 
+    ElementCompute beta) {
+
+    //
+    // Verify
+    //
+    cutlass::reference::host::Rank2KComplex<
+        typename RankK::ElementA, typename RankK::LayoutA,
+        typename RankK::ElementC, typename RankK::LayoutC, 
+        ElementCompute, ElementAccumulator
+    >(
+      problem_size,
+      alpha, 
+      tensor_A.host_ref(),
+      RankK::kTransformA,
+      beta, 
+      tensor_C.host_ref(), 
+      reference_D.host_ref(),
+      ElementAccumulator(0),
+      RankK::kFillModeC,
+      RankK::kBlasMode
+    );
 
-  // Launch device reference gemm kernel
-  gemm_reference(
-    {options.m, options.n, options.k},
-    ElementAccumulator(options.alpha),
-    ref_A,
-    ref_B,
-    ElementAccumulator(options.beta),
-    ref_C,
-    ref_D);
+    return compare_reference(problem_size, alpha, beta);
+  }
 
-  // Wait for kernel to finish
-  CUDA_CHECK(cudaDeviceSynchronize());
+  /// Returns true if the CUDA device is sufficient to execute the kernel.
+  bool sufficient() const {
+    //
+    // Determine SMEM requirements and waive if not satisfied
+    //
+
+    int smem_size = int(sizeof(typename RankK::RankKkernel::SharedStorage));
+
+    cudaDeviceProp properties;
+    int device_idx;
+    cudaError_t result = cudaGetDevice(&device_idx);
 
-  // Check if output from CUTLASS kernel and reference kernel are equal or not
-  bool passed = cutlass::reference::device::BlockCompareEqual(block_ref_D.get(), block_D.get(), block_D.size());
+    if (result != cudaSuccess) {
+      throw std::runtime_error("cudaGetDevice() API call failed.");
+    }
 
-  return passed;
-}
+    result = cudaGetDeviceProperties(&properties, device_idx);
 
-/// Execute a given example GEMM computation
-template <typename Gemm>
-int run(Options &options)
-{
-  initialize(options);
+    if (result != cudaSuccess) {
+      throw std::runtime_error("cudaGetDeviceProperties() failed");
+    }
 
-  // Instantiate CUTLASS kernel depending on templates
-  Gemm gemm;
+    if (properties.sharedMemPerBlockOptin < smem_size) {
+      return false;
+    }
 
-  // Create a structure of gemm kernel arguments suitable for invoking an instance of Gemm
-  auto arguments = args_from_options(options);
+    return true;
+  }
 
-  // Using the arguments, query for extra workspace required for matrix multiplication computation
-  size_t workspace_size = Gemm::get_workspace_size(arguments);
+  /// Executes one test
+  bool run(
+    cutlass::gemm::GemmUniversalMode mode,
+    cutlass::gemm::GemmCoord problem_size,
+    int batch_count = 1,
+    ElementCompute alpha = ElementCompute(1), 
+    ElementCompute beta = ElementCompute(0)) {
+
+    // Waive test if insufficient CUDA device
+    if (!sufficient()) {
+      if (CUTLASS_TEST_UNIT_ENABLE_WARNINGS) {
+        std::cerr << "Test waived due to insufficient CUDA device." << std::endl;
+      }
+      return true;
+    }
 
-  // Allocate workspace memory
-  cutlass::device_memory::allocation<uint8_t> workspace(workspace_size);
+#if 0
+    std::cout << "[TestbedRankKUniversal::run()] problem(m, n, k): " << problem_size
+              << " alpha: " << ElementCompute(alpha)
+              << " beta: " << ElementCompute(beta) << std::endl;
+#endif
 
-  // Check if the problem size is supported or not
-  CUTLASS_CHECK(gemm.can_implement(arguments));
+    this->initialize(problem_size);
 
-  // Initialize CUTLASS kernel with arguments and workspace pointer
-  CUTLASS_CHECK(gemm.initialize(arguments, workspace.get()));
+    //
+    // Initialize the RankK operator
+    //
+
+    typename RankK::Arguments arguments{
+      mode,
+      problem_size,
+      batch_count,
+      {alpha, beta},
+      tensor_A.device_data(),
+      tensor_C.device_data(),
+      tensor_D.device_data(),
+      problem_size.n() * problem_size.k(),
+      problem_size.m() * problem_size.n(),
+      problem_size.m() * problem_size.n(),
+      tensor_A.layout().stride(0),
+      tensor_C.layout().stride(0),
+      tensor_D.layout().stride(0)
+    };
+
+    RankK rank2k_op;
+
+    size_t workspace_size = RankK::get_workspace_size(arguments);
+
+    cutlass::device_memory::allocation<uint8_t> workspace(workspace_size);
+
+    cutlass::Status status = rank2k_op.initialize(arguments, workspace.get());
+
+    EXPECT_TRUE(status == cutlass::Status::kSuccess) << to_string(status);
+
+    //
+    // Run the RankK
+    //
+
+    status = rank2k_op();
+
+    EXPECT_TRUE(status == cutlass::Status::kSuccess) << to_string(status);
+
+    //
+    // Verify
+    //
+
+    bool passed = this->verify(problem_size, alpha, beta);
+
+    //if (true) {
+    if (!passed) {
+      std::stringstream fname;
+
+      fname << "error_RankK_device_"
+            << "fill_mode_c_"
+            << (RankK::kFillModeC == cutlass::FillMode::kLower ? "lower_" :
+                (RankK::kFillModeC == cutlass::FillMode::kUpper ? "upper_" : "invalid_"))
+            << "mnk_"
+            << problem_size.m() << "x"
+            << problem_size.n() << "x"
+            << problem_size.k() << "_"
+            << RankK::ThreadblockShape::kM << "x"  
+            << RankK::ThreadblockShape::kN << "x"  
+            << RankK::ThreadblockShape::kK << "_"
+            << RankK::WarpShape::kM << "x"  
+            << RankK::WarpShape::kN << "x"  
+            << RankK::WarpShape::kK << ".txt";
+
+      std::cout << fname.str() << std::endl;
+
+      std::ofstream results(fname.str());
+
+      results << problem_size << std::endl;
+
+      results
+        << "\nA:\n" << tensor_A.host_view() << "\n"
+        << "\nC:\n" << tensor_C.host_view() << "\n"
+        << "\nD reference:\n" << reference_D.host_view() << "\n"
+        << "\nD computed:\n" << tensor_D.host_view() << "\n";
 
-  // Correctness / Warmup iteration
-  CUTLASS_CHECK(gemm.run());
+    }
 
-  // Check if output from CUTLASS kernel and reference kernel are equal or not
-  Result result;
-  result.passed = verify(options);
+    return passed;
+  }
+};
 
-  std::cout << "  Disposition: " << (result.passed ? "Passed" : "Failed") << std::endl;
+/////////////////////////////////////////////////////////////////////////////////////////////////
+template <typename RankK>
+bool TestRank2kUniversal(
+  cutlass::gemm::GemmCoord const & problem_size,
+  cutlass::gemm::GemmUniversalMode mode,
+  int batch_count,
+  double alpha = 1.0, 
+  double beta = 2.0) {
+
+  bool passed = true;
+
+  TestbedRank2KUniversal<RankK> testbed;
+  
+  using ElementCompute = typename RankK::EpilogueOutputOp::ElementCompute;
+
+  passed = testbed.run(
+    mode,
+    problem_size,
+    batch_count,
+    cutlass::from_real<ElementCompute>(alpha), 
+    cutlass::from_real<ElementCompute>(beta)
+  );
 
-  if (!result.passed) {
-    exit(-1);
-  }
+  return passed;
+}
 
-  // Run profiling loop
-  if (options.iterations > 0)
-  {
-    GpuTimer timer;
-    timer.start();
-    for (int iter = 0; iter < options.iterations; ++iter) {
-      CUTLASS_CHECK(gemm.run());
-    }
-    timer.stop();
+template <typename RankK>
+bool TestAllRankKUniversal() {
+  bool passed = true;
 
-    // Compute average runtime and GFLOPs.
-    float elapsed_ms = timer.elapsed_millis();
-    result.avg_runtime_ms = double(elapsed_ms) / double(options.iterations);
-    result.gflops = options.gflops(result.avg_runtime_ms / 1000.0);
-
-    std::cout << "  Problem Size: " << options.m << 'x' << options.n << 'x' << options.k << std::endl;
-    std::cout << "  Avg runtime: " << result.avg_runtime_ms << " ms" << std::endl;
-    std::cout << "  GFLOPS: " << result.gflops << std::endl;
-  }
 
-  return 0;
-}
+  int const kMinimumOperandElementSize = int(cutlass::sizeof_bits<typename RankK::ElementA>::value);
+  int const kAlignmentN = 128 / kMinimumOperandElementSize;
+  int const kAlignmentK = 128 / kMinimumOperandElementSize;
 
-#endif // defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+  cutlass::gemm::GemmUniversalMode modes[] = {
+    cutlass::gemm::GemmUniversalMode::kGemm,
+  };
 
-///////////////////////////////////////////////////////////////////////////////////////////////////
+  int problem_size_n[] = {
+    kAlignmentN, 512 - 2*kAlignmentN
+  };
 
-int main(int argc, char const **args) {
+  int problem_size_k[] = {
+    kAlignmentK, 
+    RankK::ThreadblockShape::kK * RankK::kStages - kAlignmentK, 
+    RankK::ThreadblockShape::kK * RankK::kStages * 3 - kAlignmentK
+  };
 
-  // CUTLASS must be compiled with CUDA 12.0 Toolkit to run this example
-  // and must have compute capability at least 90.
-  if (__CUDACC_VER_MAJOR__ < 12) {
-    std::cerr << "This example requires CUDA 12 or newer.\n";
-    // Returning zero so this test passes on older Toolkits. Its actions are no-op.
-    return 0;
-  }
+  int batch_counts[] = {      // may be interpretted as batch count or split-K slices
+    1                         // Just running one batch for now (removing 2, 3, 5, 7)
+  };
 
-  cudaDeviceProp props;
-  int current_device_id;
-  CUDA_CHECK(cudaGetDevice(&current_device_id));
-  CUDA_CHECK(cudaGetDeviceProperties(&props, current_device_id));
-  cudaError_t error = cudaGetDeviceProperties(&props, 0);
-  if (props.major < 9) {
-    std::cerr
-      << "This example requires a GPU of NVIDIA's Hopper Architecture or "
-      << "later (compute capability 90 or greater).\n";
-    return 0;
-  }
+  double problem_alpha[] = {
+    1.0
+  };
 
-  //
-  // Parse options
-  //
+  double problem_beta[] = {
+    2.0
+  };
 
-  Options options;
 
-  options.parse(argc, args);
+  using ElementCompute = typename RankK::EpilogueOutputOp::ElementCompute;
 
-  if (options.help) {
-    options.print_usage(std::cout) << std::endl;
-    return 0;
+  for (cutlass::gemm::GemmUniversalMode mode : modes) {
+    for (int n : problem_size_n) {
+      for (int k : problem_size_k) {
+        for (int batch_count : batch_counts) {
+
+          for (auto alpha : problem_alpha) {
+            for (auto beta : problem_beta) {
+
+              if (mode == cutlass::gemm::GemmUniversalMode::kGemm ||
+                mode == cutlass::gemm::GemmUniversalMode::kGemmSplitKParallel) {
+              }
+
+              cutlass::gemm::GemmCoord problem_size(n, n, k);
+
+              TestbedRank2KUniversal<RankK> testbed;
+
+              passed = testbed.run(
+                mode,
+                problem_size,
+                batch_count,
+                cutlass::from_real<ElementCompute>(alpha), 
+                cutlass::from_real<ElementCompute>(beta)
+              );
+
+              if (!passed) {
+                return false;
+              }
+            }
+          }
+        }
+      }
+    }
   }
 
-  //
-  // Evaluate CUTLASS kernels
-  //
+  return passed;
+}
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
-  run<Gemm>(options);
-#endif
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-  return 0;
-}
+} // namespace device
+} // namespace gemm
+} // namespace test
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
+
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/50_hopper_gemm_with_epilogue_swizzle/50_hopper_gemm_with_epilogue_swizzle.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed.h`

 * *Files 22% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2023 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,506 +24,574 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
 /*! \file
-    \brief Hopper GEMM example to create a GEMM kernel with custom Collectives
-
-    The following example shows how to assemble a custom GEMM kernel that spells out the Collectives
-    directly instead of using a builder and, in the process, instance a more efficient Epilogue
-    (from `cutlass/epilogue/collective/epilogue.hpp`) instead of using the default epilogue.
-
-    The GemmUniversal API takes 3 main template arguments:
-      (1) the problem shape / extents
-      (2) the collective mainloop type
-      (3) the collective epilogue type
-
-    While the collecive mainloop can be stamped out using a CollectiveBuilder interface, it is
-    possible to build a custom collective mainloop directly as well. Furthermore, since epilogues
-    do not yet have a builder interface, this example shows how to instantiate a more-efficient
-    epilogue alongside the collective mainloop.
-
-    Note: there are several ways to implement the GEMM epilogue in Hopper - each with its own set
-    of trade-offs. So it is recommended that users look at the options available under
-    cutlass/epilogue/collective and evaluate for their particular scenario.
-
-    Please refer to examples 48, 49 to learn more about kernel schedules and other CuTe examples
-    present in `test/unit/cute` to famialiarize with the basics of CuTe.
-
-    Examples:
-
-      $ ./examples/50_hopper_gemm_with_epilogue_swizzle/50_hopper_gemm_with_epilogue_swizzle
+    \brief Tests for device-wide GEMM interface
 */
 
-#include <iostream>
+#pragma once
 
-#include "cutlass/cutlass.h"
+#include <iostream>
+#include <fstream>
+#include <sstream>
 
-#include "cute/tensor.hpp"
-#include "cutlass/util/command_line.h"
-#include "cutlass/tensor_ref.h"
-#include "cutlass/epilogue/collective/epilogue.hpp"
-#include "cutlass/epilogue/thread/linear_combination.h"
-#include "cutlass/gemm/dispatch_policy.hpp"
-#include "cutlass/gemm/collective/collective_builder.hpp"
-#include "cutlass/gemm/device/gemm_universal_adapter.h"
-#include "cutlass/gemm/kernel/gemm_universal.hpp"
-#include "cutlass/gemm/dispatch_policy.hpp"
+#include "../../common/cutlass_unit_test.h"
 
-#include "cutlass/util/command_line.h"
-#include "cutlass/util/distribution.h"
 #include "cutlass/util/host_tensor.h"
-#include "cutlass/util/packed_stride.hpp"
 #include "cutlass/util/tensor_view_io.h"
-#include "cutlass/util/reference/device/gemm_complex.h"
-#include "cutlass/util/reference/device/tensor_compare.h"
-#include "cutlass/util/reference/device/tensor_fill.h"
+#include "cutlass/util/distribution.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/reference/host/tensor_copy.h"
+#include "cutlass/util/reference/host/tensor_compare.h"
+#include "cutlass/util/reference/host/tensor_norm.h"
+#include "cutlass/util/reference/host/gemm.h"
 
-using namespace cute;
+#include "testbed_utils.h"
+#include "testbed_universal.h"
 
-///////////////////////////////////////////////////////////////////////////////////////////////////
+#include "cutlass/layout/matrix.h"
+#include "cutlass/matrix_coord.h"
+#include "cutlass/gemm/device/gemm_universal_adapter.h"
 
-// Command line options parsing
-struct Options {
+namespace test {
+namespace gemm {
+namespace device {
 
-  bool help;
-  bool error;
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-  int m, n, k, l;
-  int alpha, beta;
+template <typename Gemm, bool Relu = false>
+struct Testbed {
 
-  Options():
-    help(false),
-    error(false),
-    m(2048), n(2048), k(2048), l(1),
-    alpha(1), beta(0)
-  { }
+  using ElementAccumulator = typename Gemm::ElementAccumulator;
+  using ElementCompute = typename Gemm::GemmKernel::Epilogue::OutputOp::ElementCompute;
 
-  // Parses the command line
-  void parse(int argc, char const **args) {
-    cutlass::CommandLine cmd(argc, args);
+  /// Initialization
+  typename Gemm::LayoutA::Stride stride_factor_A;
+  typename Gemm::LayoutB::Stride stride_factor_B;
+  typename Gemm::LayoutC::Stride stride_factor_C;
+  cutlass::Distribution::Kind init_A;
+  cutlass::Distribution::Kind init_B;
+  cutlass::Distribution::Kind init_C;
+  uint64_t seed;
+
+  cutlass::HostTensor<typename Gemm::ElementA, typename Gemm::LayoutA> tensor_A;
+  cutlass::HostTensor<typename Gemm::ElementB, typename Gemm::LayoutB> tensor_B;
+  cutlass::HostTensor<typename Gemm::ElementC, typename Gemm::LayoutC> tensor_C;
+  cutlass::HostTensor<typename Gemm::ElementC, typename Gemm::LayoutC> tensor_D;
+  cutlass::HostTensor<typename Gemm::ElementC, typename Gemm::LayoutC> reference_D;
 
-    if (cmd.check_cmd_line_flag("help")) {
-      help = true;
-      return;
-    }
+  //
+  // Methods
+  //
 
-    cmd.get_cmd_line_argument("m", m, 2048);
-    cmd.get_cmd_line_argument("n", n, 2048);
-    cmd.get_cmd_line_argument("k", k, 2048);
-    cmd.get_cmd_line_argument("l", l, 1);
-    cmd.get_cmd_line_argument("alpha", alpha, 1);
-    cmd.get_cmd_line_argument("beta", beta, 0);
-  }
+  Testbed(
+    cutlass::Distribution::Kind init_A_ = cutlass::Distribution::Uniform,
+    cutlass::Distribution::Kind init_B_ = cutlass::Distribution::Uniform,
+    cutlass::Distribution::Kind init_C_ = cutlass::Distribution::Uniform,
+    uint64_t seed_ = 2080
+  ):
+    stride_factor_A(typename Gemm::LayoutA::Stride()),
+    stride_factor_B(typename Gemm::LayoutB::Stride()),
+    stride_factor_C(typename Gemm::LayoutC::Stride()),
+    init_A(init_A_), init_B(init_B_), init_C(init_C_), seed(seed_) { }
+
+  Testbed(
+    typename Gemm::LayoutA::Stride stride_factor_A_,
+    typename Gemm::LayoutB::Stride stride_factor_B_,
+    typename Gemm::LayoutC::Stride stride_factor_C_,
+    cutlass::Distribution::Kind init_A_ = cutlass::Distribution::Uniform,
+    cutlass::Distribution::Kind init_B_ = cutlass::Distribution::Uniform,
+    cutlass::Distribution::Kind init_C_ = cutlass::Distribution::Uniform,
+    uint64_t seed_ = 2080
+  ):
+    stride_factor_A(stride_factor_A_),
+    stride_factor_B(stride_factor_B_),
+    stride_factor_C(stride_factor_C_),
+    init_A(init_A_), init_B(init_B_), init_C(init_C_), seed(seed_) { }
+
+  /// Helper to initialize a tensor view
+  template <typename Element, typename Layout>
+  bool initialize_tensor(
+    cutlass::TensorView<Element, Layout> view, 
+    cutlass::Distribution::Kind dist_kind,
+    uint64_t seed) {
+
+    if (dist_kind == cutlass::Distribution::Uniform) {
+
+      double scope_max, scope_min;
+      int bits_input = cutlass::sizeof_bits<Element>::value;
+      int bits_output = cutlass::sizeof_bits<typename Gemm::ElementC>::value;
+
+      if (bits_input == 1) {
+        scope_max = 2;
+        scope_min = 0;
+      } else if (bits_input <= 8) {
+        scope_max = 2;
+        scope_min = -2;
+      } else if (bits_output == 16) {
+        scope_max = 5;
+        scope_min = -5;
+      } else {
+        scope_max = 8;
+        scope_min = -8;
+      }
+
+      cutlass::reference::host::TensorFillRandomUniform(
+        view, seed, scope_max, scope_min, 0);
+    } 
+    else if (dist_kind == cutlass::Distribution::Identity) {
+
+      cutlass::reference::host::TensorFillIdentity(view);
+    } 
+    else if (dist_kind == cutlass::Distribution::Gaussian) {
 
-  /// Prints the usage statement.
-  std::ostream & print_usage(std::ostream &out) const {
+      cutlass::reference::host::TensorFillRandomGaussian(view, seed, 0, 0.5);
+    }
+    else if (dist_kind == cutlass::Distribution::Sequential) {
 
-    out << "50_hopper_gemm_with_vectorized_epilogue\n\n"
-      << "Hopper GEMM Example with Epilogue Swizzle.\n\n"
-      << "Options:\n\n"
-      << "  --help                      If specified, displays this usage statement\n\n"
-      << "  --m=<int>                   Sets the M extent of the GEMM\n"
-      << "  --n=<int>                   Sets the N extent of the GEMM\n"
-      << "  --k=<int>                   Sets the K extent of the GEMM\n"
-      << "  --l=<int>                   Sets the L extent (batch count) of the GEMM\n"
-      << "  --alpha=<s32>               Epilogue scalar alpha\n"
-      << "  --beta=<s32>                Epilogue scalar beta\n\n";
+      cutlass::reference::host::BlockFillSequential(
+        view.data(), view.capacity());
+    } 
+    else {
+      // TODO: Implement the rest
+      EXPECT_TRUE(false) << "Not implemented";
+      return false;
+    }
 
-    return out;
+    return true;
   }
-};
 
-///////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Helper to initialize a block of device data
-template <class Element>
-bool initialize_block(
-  cutlass::DeviceAllocation<Element>& block,
-  uint64_t seed=2023) {
-
-  Element scope_max, scope_min;
-  int bits_input = cutlass::sizeof_bits<Element>::value;
-
-  if (bits_input == 1) {
-    scope_max = 2;
-    scope_min = 0;
-  } else if (bits_input <= 8) {
-    scope_max = 2;
-    scope_min = -2;
-  } else {
-    scope_max = 8;
-    scope_min = -8;
+  /// Initializes data structures
+  void initialize(cutlass::gemm::GemmCoord problem_size) {
+    //
+    // Allocate the GEMM workspace
+    //
+
+    tensor_A.resize(problem_size.mk(), cutlass::layout::Affine2Layout_Factory<typename Gemm::LayoutA>::layout_factory(problem_size.mk(), stride_factor_A));
+    tensor_B.resize(problem_size.kn(), cutlass::layout::Affine2Layout_Factory<typename Gemm::LayoutB>::layout_factory(problem_size.kn(), stride_factor_B));
+    tensor_C.resize(problem_size.mn(), cutlass::layout::Affine2Layout_Factory<typename Gemm::LayoutC>::layout_factory(problem_size.mn(), stride_factor_C));
+    tensor_D.resize(problem_size.mn(), cutlass::layout::Affine2Layout_Factory<typename Gemm::LayoutC>::layout_factory(problem_size.mn(), stride_factor_C));
+    reference_D.resize(problem_size.mn(), cutlass::layout::Affine2Layout_Factory<typename Gemm::LayoutC>::layout_factory(problem_size.mn(), stride_factor_C), false);
+
+    EXPECT_TRUE(initialize_tensor(tensor_A.host_view(), init_A, seed + 2019));
+    EXPECT_TRUE(initialize_tensor(tensor_B.host_view(), init_B, seed + 2018));
+    EXPECT_TRUE(initialize_tensor(tensor_C.host_view(), init_C, seed + 2017));
+
+    // It is possible to randomly initialize to all zeros, so override this with non-zeros
+    // in the upper left corner of each operand.
+    tensor_A.host_view().at({0, 0}) = typename Gemm::ElementA(1);
+    tensor_B.host_view().at({0, 0}) = typename Gemm::ElementB(1);
+    tensor_C.host_view().at(cutlass::make_Coord(0, 0)) = typename Gemm::ElementC(1);
+
+    cutlass::reference::host::TensorCopy(reference_D.host_view(), tensor_C.host_view());
+
+    tensor_A.sync_device();
+    tensor_B.sync_device();
+    tensor_C.sync_device();
+    tensor_D.sync_device();
   }
 
-  cutlass::reference::device::BlockFillRandomUniform(
-    block.get(), block.size(), seed, scope_max, scope_min, 0);
+  /// Compares computed reference with device reference and outputs to a file if incorrect
+  bool compare_reference(
+    cutlass::gemm::GemmCoord problem_size, 
+    ElementCompute alpha, 
+    ElementCompute beta) {
 
-  return true;
-}
+    tensor_D.sync_host();
 
-///////////////////////////////////////////////////////////////////////////////////////////////////
+    EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_A.host_view()), 0);
+    EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_B.host_view()), 0);
+    EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_C.host_view()), 0);
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+    if (tensor_D.size() > 1)
+      EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_D.host_view()), 0);
 
-// Wrapper to run and verify a GEMM.
-template <
-  class Gemm
->
-struct ExampleRunner {
-
-  using StrideA = typename Gemm::GemmKernel::StrideA;
-  using StrideB = typename Gemm::GemmKernel::StrideB;
-  using StrideC = typename Gemm::GemmKernel::StrideC;
-  using StrideD = typename Gemm::GemmKernel::StrideD;
-
-  using LayoutA = typename Gemm::LayoutA;
-  using LayoutB = typename Gemm::LayoutB;
-  using LayoutC = typename Gemm::LayoutC;
-  using LayoutD = typename Gemm::LayoutD;
-
-  using ElementA = typename Gemm::ElementA;
-  using ElementB = typename Gemm::ElementB;
-  using ElementAcc = typename Gemm::ElementAccumulator;
-
-  using CollectiveEpilogue = typename Gemm::CollectiveEpilogue;
-  using ElementC = typename Gemm::ElementC;
-  using ElementOutput = typename CollectiveEpilogue::ElementOutput;
-  using ElementCompute = typename CollectiveEpilogue::ElementCompute;
-  using ElementAccumulator = typename CollectiveEpilogue::ElementAccumulator;
+    if (reference_D.size() > 1)
+      EXPECT_GT(cutlass::reference::host::TensorNorm(reference_D.host_view()), 0);
 
-  using ProblemShapeType = typename Gemm::GemmKernel::ProblemShape;
+    bool passed = cutlass::reference::host::TensorEquals(reference_D.host_view(), tensor_D.host_view());
 
-  //
-  // Data members
-  //
-
-  /// Initialization
-  StrideA stride_A;
-  StrideB stride_B;
-  StrideC stride_C;
-  StrideD stride_D;
-  uint64_t seed = 0;
-
-  cutlass::DeviceAllocation<ElementA> block_A;
-  cutlass::DeviceAllocation<ElementB> block_B;
-  cutlass::DeviceAllocation<ElementC> block_C;
-  cutlass::DeviceAllocation<ElementOutput> block_D;
-  cutlass::DeviceAllocation<ElementOutput> block_ref_D;
-
-  //
-  // Methods
-  //
+    EXPECT_TRUE(passed);
 
-  bool verify(const ProblemShapeType& problem_size, int32_t alpha, int32_t beta) {
-    auto [M, N, K, L] = problem_size;
+    if (!passed) {
 
-    cutlass::TensorRef ref_A(block_A.get(), LayoutA::packed({M, K}));
-    cutlass::TensorRef ref_B(block_B.get(), LayoutB::packed({K, N}));
-    cutlass::TensorRef ref_C(block_C.get(), LayoutC::packed({M, N}));
-    cutlass::TensorRef ref_D(block_ref_D.get(), LayoutD::packed({M, N}));
-
-    cutlass::reference::device::GemmComplex(
-          {M, N, K},
-          ElementCompute(alpha),
-          ref_A,
-          cutlass::ComplexTransform::kNone,
-          ref_B,
-          cutlass::ComplexTransform::kNone,
-          ElementCompute(beta),
-          ref_C,
-          ref_D,
-          ElementAccumulator(0),
-          L,     // batch_count
-          M * K, // batch_stride_A
-          K * N, // batch_stride_B
-          M * N, // batch_stride_C
-          M * N  // batch_stride_D
-        );
+      std::stringstream fname;
 
-    cudaError_t result = cudaDeviceSynchronize();
-    if (result != cudaSuccess) {
-      std::cerr << "Reference kernel failed. Last CUDA error: "
-                << cudaGetErrorString(result) << std::endl;
-      return false;
+      fname << "error_Gemm_device_" 
+        << problem_size.m() << "x"
+        << problem_size.n() << "x"
+        << problem_size.k() << "_"
+        << Gemm::ThreadblockShape::kM << "x"  
+        << Gemm::ThreadblockShape::kN << "x"  
+        << Gemm::ThreadblockShape::kK << "_"
+        << Gemm::WarpShape::kM << "x"  
+        << Gemm::WarpShape::kN << "x"  
+        << Gemm::WarpShape::kK << ".txt";
+
+      std::ofstream file(fname.str());
+
+      file
+        << "problem: " << problem_size 
+        << ", alpha: " << alpha << ", beta: " << beta << "\n\n";
+
+      file 
+        << "A =\n" << tensor_A.host_view()
+        << "\nB =\n" << tensor_B.host_view()
+        << "\nC =\n" << tensor_C.host_view()
+        << "\n\nReference =\n" << reference_D.host_view()
+        << "\nComputed =\n" << tensor_D.host_view();
     }
 
-    // Check if output from CUTLASS kernel and reference kernel are equal or not
-    bool passed = cutlass::reference::device::BlockCompareEqual(block_ref_D.get(), block_D.get(), block_D.size());
-
     return passed;
   }
 
-  /// Initialize operands to be used in the GEMM and reference GEMM
-  void initialize(const ProblemShapeType& problem_size) {
-    auto problem_shape_MNKL = cute::append<4>(problem_size, 1);
-    auto [M, N, K, L] = problem_shape_MNKL;
-
-    stride_A = make_cute_packed_stride(StrideA{}, cute::make_shape(M, K, L));
-    stride_B = make_cute_packed_stride(StrideB{}, cute::make_shape(N, K, L));
-    stride_C = make_cute_packed_stride(StrideC{}, cute::make_shape(M, N, L));
-    stride_D = make_cute_packed_stride(StrideD{}, cute::make_shape(M, N, L));
-
-    block_A.reset(M * K * L);
-    block_B.reset(K * N * L);
-    block_C.reset(M * N * L);
-    block_D.reset(M * N * L);
-    block_ref_D.reset(M * N * L);
-
-    initialize_block(block_A, seed + 2023);
-    initialize_block(block_B, seed + 2022);
-    initialize_block(block_C, seed + 2021);
-  }
-
-  bool run(const Options& options, const cutlass::KernelHardwareInfo& hw_info) {
-    ProblemShapeType problem_size = ProblemShapeType{options.m, options.n, options.k, options.l};
-
-    initialize(problem_size);
+  /// Verifies the result is a GEMM
+  bool verify(
+    cutlass::gemm::GemmCoord problem_size, 
+    ElementCompute alpha, 
+    ElementCompute beta) {
+
+    //
+    // Verify
+    //
+    
+    cutlass::reference::host::Gemm<
+        typename Gemm::ElementA, typename Gemm::LayoutA,
+        typename Gemm::ElementB, typename Gemm::LayoutB,
+        typename Gemm::ElementC, typename Gemm::LayoutC, ElementCompute,
+        ElementAccumulator, typename Gemm::Operator>
+        reference_gemm;
 
-    typename Gemm::GemmKernel::Arguments arguments{
-      cutlass::gemm::GemmUniversalMode::kGemm,
+    reference_gemm(
       problem_size,
-      block_A.get(),
-      stride_A,
-      block_B.get(),
-      stride_B,
-      {block_C.get(), stride_C, block_D.get(), stride_D, {options.alpha, options.beta}},
-      hw_info
-    };
+      alpha, 
+      tensor_A.host_ref(), 
+      tensor_B.host_ref(), 
+      beta, 
+      reference_D.host_ref(), 
+      ElementAccumulator(0)
+    );
+
+    if (Relu) {
+      for (int i = 0; i < problem_size.m(); ++i) {
+        for (int j = 0; j < problem_size.n(); ++j) {
+           reference_D.at(cutlass::MatrixCoord(i, j)) = 
+                  ((ElementCompute)reference_D.at(cutlass::MatrixCoord(i, j)) < (ElementCompute)0)
+                  ? (typename Gemm::ElementC)0
+                  : reference_D.at(cutlass::MatrixCoord(i, j));
+        }
+      }
+    }
 
-    Gemm gemm_op;
+    return compare_reference(problem_size, alpha, beta);
+  }
 
-    size_t workspace_size = Gemm::get_workspace_size(arguments);
-    cutlass::device_memory::allocation<uint8_t> workspace(workspace_size);
+	/// Determine if the CUDA device is sufficient to run the kernel
+  bool sufficient() const {
+    //
+    // Determine SMEM requirements and waive if not satisfied
+    //
+
+    int smem_size = int(sizeof(typename Gemm::GemmKernel::SharedStorage));
+
+    cudaDeviceProp properties;
+    int device_idx;
+    cudaError_t result = cudaGetDevice(&device_idx);
 
-    cutlass::Status status = gemm_op.can_implement(arguments);
-    if (status != cutlass::Status::kSuccess) {
-      std::cerr << "This kernel is not supported. Last CUDA error is: "
-                << cudaGetErrorString(cudaGetLastError()) << std::endl;
-      return false;
+    if (result != cudaSuccess) {
+      throw std::runtime_error("cudaGetDevice() API call failed.");
     }
 
-    status = gemm_op.initialize(arguments, workspace.get());
-    if (status != cutlass::Status::kSuccess) {
-      std::cerr << "Failed to initialize the CUTLASS kernel. Last CUDA error is: "
-                << cudaGetErrorString(cudaGetLastError()) << std::endl;
-      return false;
-    }
+    result = cudaGetDeviceProperties(&properties, device_idx);
 
-    // Run the GEMM
-    status = gemm_op.run();
-    if (status != cutlass::Status::kSuccess) {
-      std::cerr << "Failed to launch the CUTLASS kernel. Last CUDA error is: "
-                << cudaGetErrorString(cudaGetLastError()) << std::endl;
-      return false;
-    }
-
-    cudaError_t result = cudaDeviceSynchronize();
     if (result != cudaSuccess) {
-      std::cerr << "Error running the CUTLASS kernel. Last CUDA error is: "
-                << cudaGetErrorString(result) << std::endl;
-      return false;
+      throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    // Verify that the result is correct
-    bool passed = verify(problem_size, options.alpha, options.beta);
-    if (!passed) {
-      std::cerr << "Reference check failed" << std::endl;
+    if (properties.sharedMemPerBlockOptin < smem_size) {
+      return false;
     }
 
-    return passed;
+    return true;
   }
 
-};
 
-#endif // defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+  /// Executes one test
+  bool run(
+    cutlass::gemm::GemmCoord problem_size,
+    int split_k_slices = 1,
+    ElementCompute alpha = ElementCompute(1),
+    ElementCompute beta = ElementCompute(0))
+  {
+/*
+    std::cout << "\n-----------------------\n";
+    std::cout << "problem size: " << problem_size << "\n";
+    std::cout << "split_k_slices: " << split_k_slices << "\n";
+    std::cout << "alpha: " << alpha << "\n";
+    std::cout << "beta: " << beta << "\n";
+    std::cout << "-----------------------\n\n";
+*/
 
-///////////////////////////////////////////////////////////////////////////////////////////////////
+    // Waive test if insufficient CUDA device
+    if (!sufficient()) {
+      if (CUTLASS_TEST_UNIT_ENABLE_WARNINGS) {
+        std::cerr << "Test waived due to insufficient CUDA device." << std::endl;
+      }
+      return true;
+    }
 
-int main(int argc, char const **args) {
+    this->initialize(problem_size);
 
-  cudaDeviceProp props;
+    //
+    // Initialize the GEMM operator
+    //
 
-  cudaError_t error = cudaGetDeviceProperties(&props, 0);
-  if (error != cudaSuccess) {
-    std::cerr << "cudaGetDeviceProperties() returned an error: " << cudaGetErrorString(error) << std::endl;
-    return -1;
-  }
+    typename Gemm::Arguments arguments{
+      problem_size,
+      tensor_A.device_ref(),
+      tensor_B.device_ref(),
+      tensor_C.device_ref(),
+      tensor_D.device_ref(),
+      {alpha, beta},
+      split_k_slices
+    };
 
-  if (__CUDACC_VER_MAJOR__ < 12 || props.major < 9) {
-    std::cout
-      << "This example requires a GPU of NVIDIA's Hopper Architecture or "
-      << "later (compute capability 90 or greater) and CUDA 12.0 or greater.\n";
-    return 0;
-  }
+    Gemm gemm_op;
 
-  //
-  // Parse options
-  //
+    size_t workspace_size = Gemm::get_workspace_size(arguments);
 
-  Options options;
+    cutlass::device_memory::allocation<uint8_t> workspace(workspace_size);
 
-  options.parse(argc, args);
+    cutlass::Status status = gemm_op.initialize(arguments, workspace.get());
 
-  if (options.help) {
-    options.print_usage(std::cout) << std::endl;
-    return 0;
-  }
+    if (status != cutlass::Status::kSuccess) {
+      cudaError_t error = cudaGetLastError();
+      std::cerr << "This test is not supported: " << cudaGetErrorString(error) << "\n";
+      return true;
+    }
 
-  if (options.error) {
-    std::cerr << "Aborting execution." << std::endl;
-    return -1;
-  }
+    //
+    // Run the GEMM
+    //
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+    status = gemm_op();
 
-  //
-  // Run examples
-  //
+    EXPECT_TRUE(status == cutlass::Status::kSuccess) << to_string(status);
 
-  // The KernelHardwareInfo struct holds the number of SMs on the GPU with a given device ID. This
-  // information is used by the underlying kernel.
-  cutlass::KernelHardwareInfo hw_info;
-
-  // Change device_id to another value if you are running on a machine with multiple GPUs and wish
-  // to use a GPU other than that with device ID 0.
-  hw_info.device_id = 0;
-  hw_info.sm_count = cutlass::KernelHardwareInfo::query_device_multiprocessor_count(hw_info.device_id);
-
-  bool passed;
-
-  // Problem configuration
-  using ElementA = int8_t;
-  using ElementB = int8_t;
-  using ElementAcc = int32_t;
-  using ElementOutput = int8_t;
-
-  // Note : Only TN WGMMA Gemm is supported currently in 3.0
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-  using LayoutD = cutlass::layout::ColumnMajor;
+    //
+    // Verify
+    //
 
-  // Tiling configuration selection
-  using TileShape = Shape<_128,_64,_128>;
+    bool passed = this->verify(problem_size, alpha, beta);
 
-  // Choosing a thread block cluster larger than 1 allows us to Multicast data across thread blocks
-  using ClusterShape = Shape<_1,_2,_1>;
+    if (!passed) {
+      std::cout << "Error with split_k_slices = " << split_k_slices << ", alpha: " << alpha << std::endl;
+    }
 
-  //
-  // Assembling the CollectiveMainloop type
-  //
+    return passed;
+  }
+};
 
-  // Pipeline Depth to be used i.e number of A, B buffers in shared memory
-  constexpr int PipelineStages = 8;
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-  // Let's choose a Warp-Specialized Mainloop implemention which uses TMA
-  // Note : This requires / assumes the tensors to be 16B aligned
-  using DispatchPolicy = cutlass::gemm::MainloopSm90TmaGmmaWarpSpecialized<PipelineStages, ClusterShape,
-                           cutlass::gemm::KernelTmaWarpSpecialized>;
-
-  // TN => K Major for both A & B
-  static constexpr cute::GMMA::Major GmmaMajorA = cute::GMMA::Major::K;
-  static constexpr cute::GMMA::Major GmmaMajorB = cute::GMMA::Major::K;
-
-  // We use the SS op selector as both A, B operands are read directly from SMEM (for TN WGMMA)
-  using TiledMma = decltype(cute::make_tiled_mma(cute::GMMA::ss_op_selector<
-      ElementA, ElementB, ElementAcc, TileShape, GmmaMajorA, GmmaMajorB>()));
-
-  // A loads can be optimized with multicast if cluster-n > 1
-  using GmemTiledCopyA = std::conditional< cute::size(shape<1>(ClusterShape{})) == 1,
-                           cute::SM90_TMA_LOAD,
-                           cute::SM90_TMA_LOAD_MULTICAST>::type;
-
-  // B loads can be optimized with multicast if cluster-m > 1
-  using GmemTiledCopyB = std::conditional< cute::size(shape<0>(ClusterShape{})) == 1,
-                           cute::SM90_TMA_LOAD,
-                           cute::SM90_TMA_LOAD_MULTICAST>::type;
-
-  using SmemLayoutAtomA = decltype(cute::GMMA::smem_selector<
-      GmmaMajorA, ElementA, decltype(cute::get<0>(TileShape{})), decltype(cute::get<2>(TileShape{}))
-    >());
-
-  using SmemLayoutAtomB = decltype(cute::GMMA::smem_selector<
-      GmmaMajorB, ElementB, decltype(cute::get<1>(TileShape{})), decltype(cute::get<2>(TileShape{}))
-    >());
-
-  using CollectiveMainloop = cutlass::gemm::collective::CollectiveMma<
-      DispatchPolicy,
-      TileShape,
-      ElementA,
-      cutlass::gemm::TagToStrideA_t<LayoutA>,
-      ElementB,
-      cutlass::gemm::TagToStrideB_t<LayoutB>,
-      TiledMma,
-      GmemTiledCopyA,
-      SmemLayoutAtomA,
-      void, // Does not need a SmemCopyAtom, since A is read directly from SMEM
-      cute::identity,
-      GmemTiledCopyB,
-      SmemLayoutAtomB,
-      void, // Does not need a SmemCopyAtom, since B is read directly from SMEM
-      cute::identity
-    >;
+template <typename Gemm, bool Relu=false>
+bool TestAllGemmBasic(
+    const typename Gemm::LayoutA::Stride& stride_factor_A = typename Gemm::LayoutA::Stride(),
+    const typename Gemm::LayoutB::Stride& stride_factor_B = typename Gemm::LayoutB::Stride(),
+    const typename Gemm::LayoutC::Stride& stride_factor_C = typename Gemm::LayoutC::Stride()) {
+  bool passed = true;
+
+  int const kMinimumOperandElementSize = 
+    std::min(
+      int(cutlass::sizeof_bits<typename Gemm::ElementA>::value), 
+      int(cutlass::sizeof_bits<typename Gemm::ElementB>::value));
+
+  int const kAlignment = cutlass::platform::is_same<
+                              typename Gemm::OperatorClass, 
+                              cutlass::arch::OpClassSimt>::value ? 1 : 128 / kMinimumOperandElementSize;
+
+  // int8_t gemm alignment constraints
+  int const kAlignmentM = cutlass::platform::is_same<typename Gemm::OperatorClass, cutlass::arch::OpClassSimt>::value &&
+                          cutlass::platform::is_same<typename Gemm::ElementA, int8_t>::value &&
+                          cutlass::platform::is_same<typename Gemm::LayoutA, cutlass::layout::ColumnMajor>::value ? 4 : kAlignment;
+
+  int const kAlignmentN = cutlass::platform::is_same<typename Gemm::OperatorClass, cutlass::arch::OpClassSimt>::value &&
+                          cutlass::platform::is_same<typename Gemm::ElementB, int8_t>::value &&
+                          cutlass::platform::is_same<typename Gemm::LayoutB, cutlass::layout::RowMajor>::value ? 4 : kAlignment;
+
+  int const kAlignmentK = cutlass::platform::is_same<typename Gemm::OperatorClass, cutlass::arch::OpClassSimt>::value &&
+                          cutlass::platform::is_same<typename Gemm::ElementA, int8_t>::value &&
+                          cutlass::platform::is_same<typename Gemm::ElementB, int8_t>::value &&
+                          (cutlass::platform::is_same<typename Gemm::LayoutA, cutlass::layout::RowMajor>::value ||
+                          cutlass::platform::is_same<typename Gemm::LayoutB, cutlass::layout::ColumnMajor>::value) ? 4 : kAlignment;
+
+  int problem_size_m[] = {kAlignmentM, 512 - 3 * kAlignmentM};
+
+  int problem_size_n[] = {kAlignmentN, 512 - 2 * kAlignmentN};
+
+  int problem_size_k[] = {
+      kAlignmentK, Gemm::ThreadblockShape::kK * (Gemm::kStages + 1) - kAlignmentK};
+
+  int split_k_slices[] = {
+    1, 2, 3
+  };
+
+  double problem_alpha[] = {
+    1
+  };
+
+  double problem_beta[] = {
+    2.0
+  };
+
+  Testbed<Gemm, Relu> testbed(stride_factor_A, stride_factor_B, stride_factor_C);
+
+  using ElementCompute = typename Gemm::EpilogueOutputOp::ElementCompute;
+
+  for (int m : problem_size_m) {
+    for (int n : problem_size_n) {
+      for (int k : problem_size_k) {
+        for (int split_k : split_k_slices) {
+
+          if (!Gemm::kSplitKSerial && split_k > 1) {
+            continue;
+          }
+
+          if (split_k > 1 && k / Gemm::ThreadblockShape::kK < split_k) {
+            continue;
+          }
+
+          for (auto alpha : problem_alpha) {
+            for (auto beta : problem_beta) {
+
+              cutlass::gemm::GemmCoord problem_size(m, n, k);
+              passed = testbed.run(
+                problem_size, 
+                split_k,
+                cutlass::from_real<ElementCompute>(alpha), 
+                cutlass::from_real<ElementCompute>(beta)
+              );
+
+              if (!passed) {
+                return false;
+              }
+            }
+          }
+        }
+      }
+    }
+  }
 
-  //
-  // Assembling the Collective Epilogue Type
-  //
+  return passed;
+}
 
-  // Break the 128 along TILE_M into chunks of 32, to get a 128B leading dimension
-  using PreSwizzleLayout = Layout< Shape< Shape <_32,_4   >,_64>,
-                                   Stride<Stride< _1,_2048>,_32>>;
-
-  // 128 threads loading 16 elements each (to get vectorized global stores)
-  using TileShapeS2R = Shape<_128,_16>;
-
-  // Layout to ensure bank-conflict free loads & stores
-  using SmemLayout = ComposedLayout<
-                       Swizzle<3,4,3>,
-                       smem_ptr_flag_bits<sizeof_bits<ElementAcc>::value>,
-                       PreSwizzleLayout>;
-
-  // Tiled copy from Smem to Registers
-  // Note : CuTe will vectorize this copy if the tiling + swizzling above were right
-  using TiledCopyS2R = TiledCopy<
-                         Copy_Atom<DefaultCopy, ElementAcc>,
-                         Layout< Shape<_128,_16>, 
-                                 Stride<_16,_1>>,
-                         TileShapeS2R>;
-
-  using Epilogue = cutlass::epilogue::collective::Epilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutD>,
-      cutlass::epilogue::thread::LinearCombination<int32_t, 1, int32_t, int32_t>,
-      SmemLayout,
-      Copy_Atom<DefaultCopy, ElementAcc>,
-      TiledCopyS2R,
-      Copy_Atom<DefaultCopy, ElementOutput>>;
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-  //
-  // Assembling the GemmKernel
-  //
+template <typename Gemm, bool Relu=false>
+bool TestAllGemm(
+    const typename Gemm::LayoutA::Stride& stride_factor_A,
+    const typename Gemm::LayoutB::Stride& stride_factor_B = typename Gemm::LayoutB::Stride(),
+    const typename Gemm::LayoutC::Stride& stride_factor_C = typename Gemm::LayoutC::Stride())
+{
+  // Test basic GEMM with non-default stride factors
+  return TestAllGemmBasic<Gemm, Relu>(stride_factor_A, stride_factor_B, stride_factor_C);
+}
 
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveMainloop,
-      Epilogue
-  >;
+template <typename Gemm, bool Relu=false>
+bool TestAllGemm()
+{
+#ifdef NDEBUG
+  // Non-debug builds also test basic GEMM with default stride factors
+  if (!TestAllGemmBasic<Gemm, Relu>()) {
+    return false;
+  }
+#endif // NDEBUG
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  
-  ExampleRunner<Gemm> runner;
+  // Test universal GEMM
+#if 0
+  // Define the universal kernel
+  using UniversalKernel = cutlass::gemm::kernel::GemmUniversal<
+    typename Gemm::GemmKernel::Mma,                                 // Mma
+    typename Gemm::GemmKernel::Epilogue,                            // Epilogue
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>    // ThreadblockSwizzle
+  >;
+#else
+  // Define the streamk universal kernel
+  using UniversalKernel = cutlass::gemm::kernel::GemmUniversalStreamk<
+    typename Gemm::GemmKernel::Mma,                                 // Mma
+    typename Gemm::GemmKernel::Epilogue,                            // Epilogue
+    cutlass::gemm::threadblock::ThreadblockSwizzleStreamK           // ThreadblockSwizzle
+  >;
+#endif
 
-  passed = runner.run(options, hw_info);
+  // Define the universal adaptor
+  using UniversalGemm = cutlass::gemm::device::GemmUniversalAdapter<UniversalKernel>;
 
-  std::cout << "WGMMA GEMM with Epilogue Swizzle : " << (passed ? "Passed" : "Failed") << std::endl;
+  // Test universal GEMM
+  return TestAllGemmUniversal<UniversalGemm, Relu>();
+}
 
-#endif // defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+/////////////////////////////////////////////////////////////////////////////////////////////////
+template <typename Gemm>
+bool TestGemmPerf(int iterations = 1) {
+  bool passed = true;
+
+  int problem_size_m[] = { 2048 };
+
+  int problem_size_n[] = { 4352 };
+
+  int problem_size_k[] = { 4096  };
+
+  int split_k_slices[] = { 1 };
+  double problem_alpha[] = { 1 };
+  double problem_beta[] = { 0.0 };
+
+  Testbed<Gemm> testbed;
+
+  using ElementCompute = typename Gemm::EpilogueOutputOp::ElementCompute;
+
+  for (int m : problem_size_m) {
+    for (int n : problem_size_n) {
+      for (int k : problem_size_k) {
+        for (int split_k : split_k_slices) {
+
+          if (!Gemm::kSplitKSerial && split_k > 1) {
+            continue;
+          }
+
+          for (auto alpha : problem_alpha) {
+            for (auto beta : problem_beta) {
+
+              cutlass::gemm::GemmCoord problem_size(m, n, k);
+
+              for (int i = 0; i < iterations; i++){
+                passed = testbed.run(
+                  problem_size, 
+                  split_k,
+                  cutlass::from_real<ElementCompute>(alpha), 
+                  cutlass::from_real<ElementCompute>(beta)
+                );
+              }
+
+              if (!passed) {
+                return false;
+              }
+            }
+          }
+        }
+      }
+    }
+  }
 
-  return 0;
+  return passed;
 }
 
+} // namespace device
+} // namespace gemm
+} // namespace test
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
+
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/51_hopper_gett/gett_kernel.cuh` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm70.cu`

 * *Files 22% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2023 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,114 +24,104 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-#pragma once
-
-#include "cute/tensor.hpp"
-
-#include "cutlass/arch/arch.h"
-#include "cutlass/gemm/device/gemm_universal_adapter.h"
-#include "cutlass/gemm/kernel/gemm_universal.hpp"
-#include "cutlass/gemm/collective/collective_builder.hpp"
-
-#include "cutlass/epilogue/collective/collective_epilogue.hpp"
-#include "cutlass/epilogue/thread/linear_combination.h"
-
-namespace example {
-
-//
-// GETT entry point
-//
+/*! \file
+    \brief Tests for device-wide Implicit GEMM interface
+*/
+
+#include "../../common/cutlass_unit_test.h"
+#include "cutlass/cutlass.h"
+#include "cutlass/array.h"
+#include "cutlass/epilogue/thread/linear_combination_bias_elementwise.h"
+#include "cutlass/epilogue/thread/linear_combination_residual_block.h"
+#include "cutlass/epilogue/thread/activation.h"
+
+#include "cutlass/conv/kernel/default_conv2d_fprop_with_broadcast.h"
+#include "cutlass/conv/device/implicit_gemm_convolution.h"
+
+#include "conv2d_with_broadcast_testbed.h"
+
+#if defined(CUTLASS_ARCH_MMA_SM70_SUPPORTED)
+
+// Test residual block fusion: UnaryOp(BinaryOp(ActivationOp(Conv2d(X) + bias), residual))
+// LinearCombinationResidualBlock does not support the split-k mode unless ActivationOp is Identity.
+// This is because the activation needs to be applied to the fully accumulated output of the Conv2d op,
+// which only the last thread block would have an access to, before applying BinaryOp.
+// The epilogue functor in the last thread block would have to be given three inputs, namely
+// partial outputs, bias, and residual, but this is not supported in the current interface.
+// Set TestSplitK = false to skip split-k tests with non-trivial ActivationOp.
 template <
-  class ProblemShapeMNKL,
-  class ElementA,
-  class StrideA,
-  class ElementB,
-  class StrideB,
-  class ElementAccumulator,
-  class ElementC,
-  class StrideC,
-  class ElementD,
-  class StrideD,
-  class ElementEpilogue>
-cutlass::Status
-gett_kernel(
-    ProblemShapeMNKL problem_shape_mnkl,
-    ElementA const* ptr_A, StrideA stride_a_mkl,
-    ElementB const* ptr_B, StrideB stride_b_nkl,
-    ElementAccumulator _,
-    ElementC const* ptr_C, StrideC stride_c_mnl,
-    ElementD      * ptr_D, StrideD stride_d_mnl,
-    ElementEpilogue alpha, ElementEpilogue beta,
-    cudaStream_t stream = 0) {
-  using namespace cute;
-
-  // TileShape -- GETT configuration
-  // Specify the number of elements to take from each mode 
-  // BLK_M = (M0,M1,...)  BLK_N = (M0,M1,...)  BLK_K = (K0,K1,...)
-
-  // Take 128 from m0, 128 from n0, 64 from k0
-  using TileShape = Shape<Shape<_128>, Shape<_128>, Shape<_64>>;
-
-  /* Other examples:
-   * Take 32 elements from m0 and 4 elements from m1
-   * Take 64 elements from n0 and 2 elements from n1
-   * Take  8 elements from k0 and 8 elements from k1
-  **/
-  // using TileShape = Shape<Shape<_32,_4>, Shape<_64,_2>, Shape<_8,_8>>;
-  
-  using EpilogueThreadOp = cutlass::epilogue::thread::LinearCombination<
-      ElementD, 1, ElementAccumulator, ElementEpilogue, cutlass::epilogue::thread::ScaleType::Default,
-      cutlass::FloatRoundStyle::round_to_nearest, ElementC>;
-
-  // No changes are required to the default epilogue
-  using CollectiveEpilogue = cutlass::epilogue::collective::detail::Sm90TmaWarpSpecializedAdapter<
-    cutlass::epilogue::collective::DefaultEpilogue<
-      StrideC,
-      StrideD,
-      EpilogueThreadOp,
-      cutlass::gemm::EpilogueDefault>>;
-
-  // CollectiveMma for GETTs can be built using the CollectiveBuilders
-  using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      ElementA, StrideA, 128 / cutlass::sizeof_bits<ElementA>::value,
-      ElementB, StrideB, 128 / cutlass::sizeof_bits<ElementB>::value,
-      ElementAccumulator,
-      TileShape, Shape<_1,_2,_1>,
-      cutlass::gemm::collective::StageCountAutoCarveout<sizeof(typename CollectiveEpilogue::SharedStorage)>,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  // The GETT kernel is a composition of a collective mainloop and epilogue, just like any 3.x GEMM
-  using GettKernel = cutlass::gemm::kernel::GemmUniversal<
-      ProblemShapeMNKL,
-      CollectiveMainloop,
-      CollectiveEpilogue>;
-
-  using GettOperator = cutlass::gemm::device::GemmUniversalAdapter<GettKernel>;
-
-  typename GettOperator::Arguments args {
-    cutlass::gemm::GemmUniversalMode::kBatched,
-    problem_shape_mnkl,
-    { ptr_A, stride_a_mkl, ptr_B, stride_b_nkl }, 
-    { {alpha, beta}, ptr_C, stride_c_mnl, ptr_D, stride_d_mnl }
+ typename ElementAccumulator,
+ template<typename T> class ActivationOp,
+ template<typename T> class BinaryOp,
+ template<typename T> class UnaryOp,
+ bool TestSplitK = false 
+>
+void TestResidaulBlock() {
+  using ElementA = cutlass::half_t;
+  using ElementB = cutlass::half_t;
+  using ElementC = cutlass::half_t;
+  using ElementD = ElementC;
+  using ElementCompute = ElementAccumulator;
+
+  using EpilogueOutputOp = cutlass::epilogue::thread::LinearCombinationResidualBlock<
+    ElementD,
+    ElementAccumulator,
+    ElementCompute,
+    ElementC,
+    8,
+    ActivationOp,
+    BinaryOp,
+    UnaryOp
+  >;
+
+  using Conv2dFpropKernel = typename cutlass::conv::kernel::DefaultConv2dFpropWithBroadcast<
+    ElementA, cutlass::layout::TensorNHWC,
+    ElementB, cutlass::layout::TensorNHWC,
+    ElementC, cutlass::layout::TensorNHWC,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm70,
+    cutlass::gemm::GemmShape<128, 128, 32>,
+    cutlass::gemm::GemmShape<64, 64, 32>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
+    EpilogueOutputOp,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    2,
+    cutlass::arch::OpMultiplyAdd,
+    cutlass::conv::IteratorAlgorithm::kOptimized
+  >::Kernel;
+
+  using Conv2dFprop = cutlass::conv::device::ImplicitGemmConvolution<Conv2dFpropKernel>;
+
+  struct ReferenceOp {
+    using OutputOp = typename Conv2dFprop::EpilogueOutputOp;
+    using ElementZ = typename OutputOp::ElementZ;
+
+    ActivationOp<ElementCompute> activation;
+    BinaryOp<ElementCompute> binary_op;
+    UnaryOp<ElementCompute> unary_op;
+
+    void operator()(ElementZ &Z, ElementZ&, ElementCompute conv2d, ElementCompute residual) {
+      Z = ElementZ(unary_op(binary_op(activation(conv2d), residual)));
+    }
   };
 
-#if CUTLASS_DEBUG_TRACE_LEVEL > 0
-  print("Problem shape:");
-  print("\tM: "); print(cute::get<0>(problem_shape_mnkl)); print("\n");
-  print("\tN: "); print(cute::get<1>(problem_shape_mnkl)); print("\n");
-  print("\tK: "); print(cute::get<2>(problem_shape_mnkl)); print("\n");
-  print("\tL: "); print(cute::get<3>(problem_shape_mnkl)); print("\n");
-  print("TileSape:"); print(TileShape{}); print("\n");
-#endif
+  bool passed = test::conv::device::TestAllConv2dWithBroadcast<Conv2dFprop, ReferenceOp, true, TestSplitK>();
+  EXPECT_TRUE(passed);
+}
 
-  GettOperator op;
-  return op(args, stream);
+TEST(SM70_Device_Conv2d_Fprop_With_Residual_Block_Plus_Optimized_ImplicitGemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32,
+     128x128_32x2_64x64x32) {
+  // Resnet
+  TestResidaulBlock<float, cutlass::epilogue::thread::ReLu, cutlass::plus, cutlass::epilogue::thread::Identity>();
 }
 
-} // namespace example
+////////////////////////////////////////////////////////////////////////////////
+
+#endif  // CUTLASS_ARCH_MMA_SM70_SUPPORTED
+
+////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/60_cutlass_import/main.cpp` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/60_cutlass_import/main.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/examples/common/helper.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/common/helper.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/aligned_buffer.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/aligned_buffer.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/arch.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/arch.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/cache_operation.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/cache_operation.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/memory.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/memory.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm75.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm75.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm80.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm80.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/mma.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm50.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm50.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm60.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm60.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm61.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm61.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm70.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm70.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm75.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm75.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm80.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm80.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm90.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm90.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sparse_sm80.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sparse_sm80.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/reg_reconfig.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_grouped.h`

 * *Files 17% similar despite different names*

```diff
@@ -24,45 +24,38 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
-/*! \file
-  \brief PTX for CTA Reconfiguration
+/*!
+  \file
+  \brief Device-level grouped GEMM.
 */
 
 #pragma once
 
-#include "cutlass/cutlass.h"
+#include "cutlass/gemm/device/base_grouped.h"
 
-#if (defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 900) && (__CUDACC_VER_MAJOR__ >= 12))
-  #if (defined(__CUDA_ARCH_FEAT_SM90_ALL))
-    #define CUDA_CTA_RECONFIG_ACTIVATED 1
-  #endif
-#else
-  #define CUDA_CTA_RECONFIG_ACTIVATED 0
-#endif
+////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
-namespace arch {
+namespace gemm {
+namespace device {
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-template<uint32_t RegCount>
-CUTLASS_DEVICE
-void warpgroup_reg_alloc(){
-#if CUDA_CTA_RECONFIG_ACTIVATED
-  asm volatile( "setmaxnreg.inc.sync.aligned.u32 %0;\n" : : "n"(RegCount) );
-#endif
-}
-
-template<uint32_t RegCount>
-CUTLASS_DEVICE
-void warpgroup_reg_dealloc(){
-#if CUDA_CTA_RECONFIG_ACTIVATED
-  asm volatile( "setmaxnreg.dec.sync.aligned.u32 %0;\n" : : "n"(RegCount) );
-#endif
-}
+/// GEMM Grouped
+template <typename GemmKernel_>
+class GemmGrouped : public BaseGrouped<GemmKernel_> {
+public:
+  using GemmKernel = GemmKernel_;
+};
 
-} // namespace arch
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+} // namespace device
+} // namespace gemm
 } // namespace cutlass
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/simd.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/simd.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/simd_sm60.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/simd_sm60.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/simd_sm61.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/simd_sm61.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/wmma.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/wmma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm70.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm70.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm72.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm72.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm75.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm75.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/array.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/array.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/array_planar_complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/array_planar_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/array_subbyte.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/array_subbyte.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/barrier.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/barrier.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/bfloat16.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/bfloat16.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/blas3.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/blas3.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/block_striped.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/block_striped.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/constants.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/constants.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/conv2d_problem_size.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/conv2d_problem_size.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/conv3d_problem_size.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/conv3d_problem_size.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/convolution.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/convolution.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/device/direct_convolution.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/device/direct_convolution.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/device/implicit_gemm_convolution.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/device/implicit_gemm_convolution.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/device/implicit_gemm_convolution_fusion.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/device/implicit_gemm_convolution_fusion.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_dgrad.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_dgrad.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_fusion.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_fusion.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_broadcast.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_broadcast.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_reduction.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_reduction.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_group_fprop.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_group_fprop.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad_fusion.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad_fusion.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_dgrad.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_dgrad.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop_fusion.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop_fusion.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_wgrad.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_wgrad.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_depthwise_fprop.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_depthwise_fprop.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/direct_convolution.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/direct_convolution.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_fusion.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_strided_dgrad.h`

 * *Files 10% similar despite different names*

```diff
@@ -25,21 +25,21 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Template for a pipelined fused activation's scale+bias+relu and Implicit GEMM kernel.
+    \brief Template for a pipelined Implicit GEMM kernel.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
-
+#include "cutlass/fast_math.h"
 #include "cutlass/aligned_buffer.h"
 #include "cutlass/array.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/matrix_shape.h"
 #include "cutlass/semaphore.h"
 #include "cutlass/tensor_ref.h"
 #include "cutlass/layout/tensor.h"
@@ -60,31 +60,29 @@
 template <
   typename Mma_,                                  ///! Threadblock-scoped matrix multiply-accumulate 
   typename Epilogue_,                             ///! Epilogue
   typename ThreadblockSwizzle_,                   ///! Threadblock swizzling function
   conv::Operator ConvOperator,                    ///! Convolutional operator (Fprop, Dgrad, Wgrad)
   typename ConvProblemSize_ = Conv2dProblemSize   ///! Convolutional operator on 2D or 3D problem
 >
-struct ImplicitGemmConvolutionFusion {
+struct ImplicitGemmConvolutionStridedDgrad {
 
   using Mma = Mma_;
   using Epilogue = Epilogue_;
   using EpilogueOutputOp = typename Epilogue::OutputOp;
   using ThreadblockSwizzle = ThreadblockSwizzle_;
   static Operator const kConvolutionalOperator = ConvOperator;
 
   using ElementA = typename Mma::IteratorA::Element;
   using LayoutA = typename Mma::IteratorA::Layout;
   using ElementB = typename Mma::IteratorB::Element;
   using LayoutB = typename Mma::IteratorB::Layout;
-
-  using ElementScaleBias = typename Mma::IteratorScaleBias::Element;
-  using LayoutScaleBias = typename Mma::IteratorScaleBias::Layout;
-
   using ElementC = typename EpilogueOutputOp::ElementOutput;
+
+  /// Set output tensor C layout
   using LayoutC = LayoutA;
 
   using ElementAccumulator = typename EpilogueOutputOp::ElementAccumulator;
   using ElementCompute = typename EpilogueOutputOp::ElementCompute;
 
   using WarpMmaOperator = typename Mma::Policy::Operator;
 
@@ -96,22 +94,22 @@
 
   using ThreadblockShape = typename Mma::Shape;
   using WarpShape = typename WarpMmaOperator::Shape;
   using InstructionShape = typename ArchMmaOperator::Shape;
 
   static int const kStages = Mma::kStages;
   static IteratorAlgorithm const kIteratorAlgorithm = Mma::IteratorA::kIteratorAlgorithm; 
- 
+  static StrideSupport const kStrideSupport = Mma::IteratorA::kStrideSupport;
+  
   /// Warp count (concept: GemmShape)
   using WarpCount = typename Mma::WarpCount;
   static int const kThreadCount = 32 * WarpCount::kCount;
 
   using TensorRefA = typename Mma::IteratorA::TensorRef;
   using TensorRefB = typename Mma::IteratorB::TensorRef;
-  using TensorRefScaleBias = typename Mma::IteratorScaleBias::TensorRef;
   using TensorRefC = cutlass::TensorRef<ElementC, LayoutC>;
 
   /// Check iterator A and B convolution dimension are the same and 
   // set device::ImplicitGemmConvolution::kConvDim
   static_assert(Mma::IteratorA::kConvDim == Mma::IteratorB::kConvDim, 
     "Convolution on different different dimensions is not supported");
   static int const kConvDim = Mma::IteratorA::kConvDim;
@@ -127,14 +125,25 @@
   static int const kWgradCStrideIdx = 
     platform::is_same<LayoutC, cutlass::layout::TensorNHWC>::value ? 2 : 3;
 
   /// This chooses the appropriate stride element of the C tensor.
   static int const kTensorCStrideIdx = 
     (kConvolutionalOperator == conv::Operator::kWgrad ? kWgradCStrideIdx : 0);
 
+  // Strided dgrad uses a specialized threadblock swizzle for functionality and performance
+  static_assert((platform::is_same<ThreadblockSwizzle,
+                      threadblock::StridedDgradHorizontalThreadblockSwizzle>::value) ||
+                (platform::is_same<ThreadblockSwizzle,
+                      threadblock::StridedDgradIdentityThreadblockSwizzle<1>>::value) ||
+                (platform::is_same<ThreadblockSwizzle,
+                      threadblock::StridedDgradIdentityThreadblockSwizzle<4>>::value) ||
+                (platform::is_same<ThreadblockSwizzle,
+                      threadblock::StridedDgradIdentityThreadblockSwizzle<8>>::value),
+    "Needs ThreadblockSwizzle type specialized for strided dgrad");
+
   //
   //
   //
   using ConvOutputIteratorParameter = epilogue::threadblock::ConvOutputIteratorParameter<
     LayoutC,
     typename Epilogue::OutputTileIterator::Layout, 
     TensorRefC,
@@ -148,16 +157,14 @@
     //
     // Data members
     //
 
     ConvProblemSize problem_size;
     TensorRefA ref_A;
     TensorRefB ref_B;
-    TensorRefScaleBias ref_scale;
-    TensorRefScaleBias ref_bias;
     TensorRefC ref_C;
     TensorRefC ref_D;
     typename EpilogueOutputOp::Params output_op;
     SplitKMode split_k_mode;
 
     //
     // Methods
@@ -174,211 +181,227 @@
       problem_size(problem_size) { }
 
     CUTLASS_HOST_DEVICE
     Arguments(
       ConvProblemSize const & problem_size,
       TensorRefA const & ref_A,
       TensorRefB const & ref_B,
-      TensorRefScaleBias const & ref_scale,
-      TensorRefScaleBias const & ref_bias,
       TensorRefC const & ref_C,
       TensorRefC const & ref_D,
       typename EpilogueOutputOp::Params const & output_op,
       SplitKMode const & split_k_mode = SplitKMode::kSerial
     ):
       problem_size(problem_size),
       ref_A(ref_A),
       ref_B(ref_B),
-      ref_scale(ref_scale),
-      ref_bias(ref_bias),
       ref_C(ref_C),
       ref_D(ref_D),
       output_op(output_op),
       split_k_mode(split_k_mode)
     {
 
     }
 
   };
 
   /// Parameters structure
   struct Params {
     ConvProblemSize problem_size;
     cutlass::gemm::GemmCoord grid_tiled_shape;
-    gemm::GemmCoord implicit_gemm_problem_size;
-    int swizzle_log_tile;
+    FastDivmod stride_h_divmod;
+    FastDivmod stride_w_divmod;
     int gemm_k_iterations;
     typename Mma::IteratorA::Params iterator_A;
     typename Mma::IteratorA::Element const *ptr_A;
     typename Mma::IteratorB::Params iterator_B;
     typename Mma::IteratorB::Element const *ptr_B;
-    typename Mma::IteratorScaleBias::Params iterator_scale_bias;
-    typename Mma::IteratorScaleBias::Element const *ptr_scale;
-    typename Mma::IteratorScaleBias::Element const *ptr_bias;
     typename Epilogue::OutputTileIterator::Params iterator_C;
     typename Epilogue::OutputTileIterator::Element *ptr_C;
     typename Epilogue::OutputTileIterator::Params iterator_D;
     typename Epilogue::OutputTileIterator::Element *ptr_D;
     typename EpilogueOutputOp::Params output_op;
     int *semaphore;
     SplitKMode split_k_mode;
 
     //
     // Methods
     //
 
     CUTLASS_HOST_DEVICE
-    Params(): swizzle_log_tile(0), gemm_k_iterations(0) { }
+    Params(): gemm_k_iterations(0) { }
 
     /// 
     CUTLASS_HOST_DEVICE
     Params(
       Arguments const &args,
       int *semaphore = nullptr
     ):
       problem_size(args.problem_size),
-      implicit_gemm_problem_size(cutlass::conv::implicit_gemm_problem_size(kConvolutionalOperator, args.problem_size)),
+      stride_h_divmod(args.problem_size.stride_h),
+      stride_w_divmod(args.problem_size.stride_w),
       iterator_A(Mma::IteratorA::getParams(args.problem_size, args.ref_A.layout())),
       ptr_A(args.ref_A.data()),
       iterator_B(args.problem_size, args.ref_B.layout()),
       ptr_B(args.ref_B.data()),
-      iterator_scale_bias(args.problem_size, args.ref_scale.layout()),
-      ptr_scale(args.ref_scale.data()),
-      ptr_bias(args.ref_bias.data()),
-      iterator_C(ConvOutputIteratorParameter::layout(args.ref_C)),
+      iterator_C(ConvOutputIteratorParameter::layout(args.ref_C), args.problem_size, ThreadblockShape::kM),
       ptr_C(args.ref_C.data()),
-      iterator_D(ConvOutputIteratorParameter::layout(args.ref_D)),
+      iterator_D(ConvOutputIteratorParameter::layout(args.ref_D), args.problem_size, ThreadblockShape::kM),
       ptr_D(args.ref_D.data()),
       output_op(args.output_op),
       semaphore(semaphore),
       split_k_mode(args.split_k_mode)
     {
       gemm_k_iterations = implicit_gemm_k_iterations(kConvolutionalOperator, ThreadblockShape::kK, args.problem_size);
 
       ThreadblockSwizzle threadblock_swizzle;
 
       grid_tiled_shape = threadblock_swizzle.get_tiled_shape(
-        implicit_gemm_problem_size,
+        kConvolutionalOperator,
+        args.problem_size,
         {ThreadblockShape::kM, ThreadblockShape::kN, ThreadblockShape::kK},
         args.problem_size.split_k_slices);
-
-      swizzle_log_tile = threadblock_swizzle.get_log_tile(grid_tiled_shape);
     }
   };
 
   /// Shared memory storage structure
   union SharedStorage {
     typename Mma::SharedStorage main_loop;
     typename Epilogue::SharedStorage epilogue;
   };
-
+  
   //
   // Methods
   //
 
   CUTLASS_HOST_DEVICE
-  ImplicitGemmConvolutionFusion() { } 
+  ImplicitGemmConvolutionStridedDgrad() { } 
 
   /// Executes one ImplicitGEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
     // Compute threadblock location
     ThreadblockSwizzle threadblock_swizzle;
 
     cutlass::gemm::GemmCoord threadblock_tile_idx =
-        threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
+        threadblock_swizzle.get_tile_offset(params.grid_tiled_shape);
 
     // Early exit if CTA is out of range
     if (params.grid_tiled_shape.m() <= threadblock_tile_idx.m() ||
       params.grid_tiled_shape.n() <= threadblock_tile_idx.n()) {
 
       return;
     }
 
     // Compute position within threadblock
     int thread_idx = threadIdx.x;
 
-    // Construct iterators to A operand
-    typename Mma::IteratorA iterator_A(
-      params.iterator_A,
-      params.problem_size,
-      params.ptr_A,
-      thread_idx,
-      MatrixCoord(
-        threadblock_tile_idx.m() * Mma::Shape::kM,
-        threadblock_tile_idx.k() * Mma::Shape::kK
-      )
-    );
+    // Compute starting filter position for strided dgrad
+    int tile_m_per_filter = strided_dgrad_tile_m_per_filter(params.problem_size, 
+                                                            ThreadblockShape::kM);
+    int filter_tile_m = (threadblock_tile_idx.m() / tile_m_per_filter);
     
-    // Construct iterators to B operand
-    typename Mma::IteratorB iterator_B(
-      params.iterator_B,
-      params.problem_size,
-      params.ptr_B,
-      thread_idx,
-      MatrixCoord(
-        threadblock_tile_idx.k() * Mma::Shape::kK,
-        threadblock_tile_idx.n() * Mma::Shape::kN
-      )
-    );
- 
-    // Construct iterators to A scale/bias vector
-    typename Mma::IteratorScaleBias iterator_scale_bias(
-      params.iterator_scale_bias,
+
+    // The subsequent fast_divmod() operations are equivalent to the following logical computation:
+    //
+    // int start_r = filter_tile_m / (params.problem_size.stride_w);
+    // int start_s = filter_tile_m % (params.problem_size.stride_w);
+
+    int start_r, start_s;
+    params.stride_w_divmod(start_r, start_s, filter_tile_m);
+
+    int filter_r = start_r;
+    int filter_s = start_s;
+
+    if (params.problem_size.mode == Mode::kConvolution) {
+      filter_r = (params.problem_size.R - 1 - filter_r);
+      filter_s = (params.problem_size.S - 1 - filter_s);
+    }
+
+    // Starting h, w positions for filter position in gemm_k=0
+    int start_h, start_w;
+    strided_dgrad_starting_coords(
       params.problem_size,
-      params.ptr_scale,
-      params.ptr_bias,
-      thread_idx,
-      MatrixCoord(
-        0, (kConvolutionalOperator == conv::Operator::kFprop) ?
-                  (threadblock_tile_idx.k() * Mma::Shape::kK) :
-                  // Wgrad
-                  (threadblock_tile_idx.n() * Mma::Shape::kN)
-      )
-    );
+      params.stride_h_divmod, params.stride_w_divmod,
+      filter_r, filter_s,
+      start_h, start_w);
+
+    if (start_h >= params.problem_size.H || start_w >= params.problem_size.W) {
+      return;
+    }
+
+    typename Mma::FragmentC accumulators;
+
+    accumulators.clear();
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
     int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
     int lane_idx = threadIdx.x % 32;
 
-    //
-    // Main loop
-    //
-
-    // Construct thread-scoped matrix multiply
-    Mma mma(shared_storage.main_loop, thread_idx, warp_idx, lane_idx);
+    // Check if CTA contributes valid MMA (Dy * w) and accumulator will be non-zero after MMA
+    if (start_r < params.problem_size.R && start_s < params.problem_size.S) {
+      // Scale gemm_k_iterations for strided dgrad
+      int gemm_k_iterations = (params.gemm_k_iterations / (params.problem_size.R * params.problem_size.S)
+                              ) * params.problem_size.num_gemm_k_filter_positions(start_r, start_s);
+      
+      // Construct iterators to A and B operands
+      typename Mma::IteratorA iterator_A(
+        params.iterator_A,
+        params.problem_size,
+        params.ptr_A,
+        thread_idx,
+        params.stride_h_divmod, params.stride_w_divmod,
+        start_r, start_s,
+        MatrixCoord(
+          threadblock_tile_idx.m() * Mma::Shape::kM,
+          threadblock_tile_idx.k() * Mma::Shape::kK
+        ) 
+      );
+      
+      typename Mma::IteratorB iterator_B(
+        params.iterator_B,
+        params.problem_size,
+        params.ptr_B,
+        thread_idx,
+        start_r, start_s,
+        MatrixCoord(
+          threadblock_tile_idx.k() * Mma::Shape::kK,
+          threadblock_tile_idx.n() * Mma::Shape::kN
+        )
+      );
+
+      //
+      // Main loop
+      //
 
-    typename Mma::FragmentC accumulators;
+      // Construct thread-scoped matrix multiply
+      Mma mma(shared_storage.main_loop, thread_idx, warp_idx, lane_idx);
 
-    accumulators.clear();
-
-    // Compute threadblock-scoped matrix multiply-add
-    mma(params.gemm_k_iterations, accumulators, iterator_A,
-        iterator_B, iterator_scale_bias, accumulators);
+      // Compute threadblock-scoped matrix multiply-add
+      mma(gemm_k_iterations, accumulators, iterator_A, iterator_B, accumulators);
+    }
 
     //
     // Epilogue
     //
 
     EpilogueOutputOp output_op(params.output_op);
 
     // Construct the semaphore.
     int block_idx = threadblock_tile_idx.m() + threadblock_tile_idx.n() * params.grid_tiled_shape.m();
-
     Semaphore semaphore(params.semaphore + block_idx, thread_idx);
-    
+
     // Compute logical position within grid
     threadblock_tile_idx =
-        threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
+        threadblock_swizzle.get_tile_offset(params.grid_tiled_shape);
 
     // If performing a reduction via split-K, fetch the initial synchronization
     if (params.split_k_mode == SplitKMode::kSerial && params.grid_tiled_shape.k() > 1) {
-        
+
       // Fetch the synchronization lock initially but do not block.
       semaphore.fetch();
 
       // Indicate which position in a serial reduction the output operator is currently updating
       output_op.set_k_partition(threadblock_tile_idx.k(), params.grid_tiled_shape.k());
     }
 
@@ -389,73 +412,79 @@
 
     // Tile iterator writing to destination tensor
     typename Epilogue::OutputTileIterator iterator_D(
       params.iterator_D,
       params.ptr_D,
       ConvOutputIteratorParameter::extent(params.problem_size),
       thread_idx,
-      threadblock_offset
-    );
-    
-    // Tile iterator reading from source accumulator tensor
-    typename Epilogue::OutputTileIterator iterator_C(
-      params.iterator_C,
-      params.ptr_C,
-      ConvOutputIteratorParameter::extent(params.problem_size),
-      thread_idx,
+      params.stride_h_divmod, params.stride_w_divmod,
+      start_r, start_s,
       threadblock_offset
     );
 
     // Construct the epilogue
     Epilogue epilogue(
-      shared_storage.epilogue, 
-      thread_idx, 
-      warp_idx, 
+      shared_storage.epilogue,
+      thread_idx,
+      warp_idx,
       lane_idx);
 
-    // Wait on the semaphore - this latency may have been covered by iterator construction
-    if (params.split_k_mode == SplitKMode::kSerial && params.grid_tiled_shape.k() > 1) {
-        
-      // For subsequent threadblocks, the source matrix is held in the 'D' tensor.
-      if (threadblock_tile_idx.k()) {
-        iterator_C = iterator_D;
-      }
+    if (output_op.is_source_needed())
+    {
+      // Tile iterator reading from source accumulator tensor
+      typename Epilogue::OutputTileIterator iterator_C(
+        params.iterator_C,
+        params.ptr_C,
+        ConvOutputIteratorParameter::extent(params.problem_size),
+        thread_idx,
+        params.stride_h_divmod, params.stride_w_divmod,
+        start_r, start_s,
+        threadblock_offset);
+
+      // Wait on the semaphore - this latency may have been covered by iterator construction
+      if (params.split_k_mode == SplitKMode::kSerial && params.grid_tiled_shape.k() > 1) {
+
+        // For subsequent threadblocks, the source matrix is held in the 'D' tensor.
+        if (threadblock_tile_idx.k()) {
+          iterator_C = iterator_D;
+        }
 
-      semaphore.wait(threadblock_tile_idx.k());
+        semaphore.wait(threadblock_tile_idx.k());
+      }
 
+      // Run epilogue with addend source iterator
+      epilogue(output_op, iterator_D, accumulators, iterator_C);
     }
-    // Each split-k-slice writes to a unique tensor location
-    else if (params.split_k_mode == SplitKMode::kParallel) {
-      iterator_D.add_pointer_offset(threadblock_tile_idx.k() * 
-        cutlass::conv::implicit_gemm_tensor_c_size(ConvOperator, params.problem_size));
+    else
+    {
+      // Run epilogue without addend source iterator
+      epilogue(output_op, iterator_D, accumulators);
     }
 
-    // Run efficient epilogue
-    epilogue(output_op, iterator_D, accumulators, iterator_C);
-  
     //
     // Release the semaphore
     //
 
-    if (params.split_k_mode == SplitKMode::kSerial && params.grid_tiled_shape.k() > 1) { 
+    if (params.split_k_mode == SplitKMode::kSerial && params.grid_tiled_shape.k() > 1) {
 
       int lock = 0;
       if (params.grid_tiled_shape.k() == threadblock_tile_idx.k() + 1) {
 
         // The final threadblock resets the semaphore for subsequent grids.
         lock = 0;
       }
       else {
         // Otherwise, the semaphore is incremented
         lock = threadblock_tile_idx.k() + 1;
       }
-      
+
       semaphore.release(lock);
     }
-  } 
+
+  }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace kernel
 } // namespace conv
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_strided_dgrad.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_with_fused_epilogue.h`

 * *Files 15% similar despite different names*

```diff
@@ -31,15 +31,15 @@
 /*! \file
     \brief Template for a pipelined Implicit GEMM kernel.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
-#include "cutlass/fast_math.h"
+
 #include "cutlass/aligned_buffer.h"
 #include "cutlass/array.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/matrix_shape.h"
 #include "cutlass/semaphore.h"
 #include "cutlass/tensor_ref.h"
 #include "cutlass/layout/tensor.h"
@@ -60,15 +60,15 @@
 template <
   typename Mma_,                                  ///! Threadblock-scoped matrix multiply-accumulate 
   typename Epilogue_,                             ///! Epilogue
   typename ThreadblockSwizzle_,                   ///! Threadblock swizzling function
   conv::Operator ConvOperator,                    ///! Convolutional operator (Fprop, Dgrad, Wgrad)
   typename ConvProblemSize_ = Conv2dProblemSize   ///! Convolutional operator on 2D or 3D problem
 >
-struct ImplicitGemmConvolutionStridedDgrad {
+struct ImplicitGemmConvolutionWithFusedEpilogue {
 
   using Mma = Mma_;
   using Epilogue = Epilogue_;
   using EpilogueOutputOp = typename Epilogue::OutputOp;
   using ThreadblockSwizzle = ThreadblockSwizzle_;
   static Operator const kConvolutionalOperator = ConvOperator;
 
@@ -95,15 +95,15 @@
   using ThreadblockShape = typename Mma::Shape;
   using WarpShape = typename WarpMmaOperator::Shape;
   using InstructionShape = typename ArchMmaOperator::Shape;
 
   static int const kStages = Mma::kStages;
   static IteratorAlgorithm const kIteratorAlgorithm = Mma::IteratorA::kIteratorAlgorithm; 
   static StrideSupport const kStrideSupport = Mma::IteratorA::kStrideSupport;
-  
+
   /// Warp count (concept: GemmShape)
   using WarpCount = typename Mma::WarpCount;
   static int const kThreadCount = 32 * WarpCount::kCount;
 
   using TensorRefA = typename Mma::IteratorA::TensorRef;
   using TensorRefB = typename Mma::IteratorB::TensorRef;
   using TensorRefC = cutlass::TensorRef<ElementC, LayoutC>;
@@ -125,25 +125,14 @@
   static int const kWgradCStrideIdx = 
     platform::is_same<LayoutC, cutlass::layout::TensorNHWC>::value ? 2 : 3;
 
   /// This chooses the appropriate stride element of the C tensor.
   static int const kTensorCStrideIdx = 
     (kConvolutionalOperator == conv::Operator::kWgrad ? kWgradCStrideIdx : 0);
 
-  // Strided dgrad uses a specialized threadblock swizzle for functionality and performance
-  static_assert((platform::is_same<ThreadblockSwizzle,
-                      threadblock::StridedDgradHorizontalThreadblockSwizzle>::value) ||
-                (platform::is_same<ThreadblockSwizzle,
-                      threadblock::StridedDgradIdentityThreadblockSwizzle<1>>::value) ||
-                (platform::is_same<ThreadblockSwizzle,
-                      threadblock::StridedDgradIdentityThreadblockSwizzle<4>>::value) ||
-                (platform::is_same<ThreadblockSwizzle,
-                      threadblock::StridedDgradIdentityThreadblockSwizzle<8>>::value),
-    "Needs ThreadblockSwizzle type specialized for strided dgrad");
-
   //
   //
   //
   using ConvOutputIteratorParameter = epilogue::threadblock::ConvOutputIteratorParameter<
     LayoutC,
     typename Epilogue::OutputTileIterator::Layout, 
     TensorRefC,
@@ -159,17 +148,24 @@
     //
 
     ConvProblemSize problem_size;
     TensorRefA ref_A;
     TensorRefB ref_B;
     TensorRefC ref_C;
     TensorRefC ref_D;
+
     typename EpilogueOutputOp::Params output_op;
     SplitKMode split_k_mode;
 
+    void * ptr_Vector;
+    void * ptr_Tensor;
+
+    typename LayoutC::Stride::Index ldr;
+    typename LayoutC::Stride::Index ldt;
+
     //
     // Methods
     //
 
     /// Default ctor
     CUTLASS_HOST_DEVICE
     Arguments() { }
@@ -184,224 +180,206 @@
     Arguments(
       ConvProblemSize const & problem_size,
       TensorRefA const & ref_A,
       TensorRefB const & ref_B,
       TensorRefC const & ref_C,
       TensorRefC const & ref_D,
       typename EpilogueOutputOp::Params const & output_op,
-      SplitKMode const & split_k_mode = SplitKMode::kSerial
+      SplitKMode const & split_k_mode = SplitKMode::kSerial,
+      void * ptr_Vector = nullptr,
+      void * ptr_Tensor = nullptr,
+      typename LayoutC::Stride::Index ldr = 0,
+      typename LayoutC::Stride::Index ldt = 0
     ):
       problem_size(problem_size),
       ref_A(ref_A),
       ref_B(ref_B),
       ref_C(ref_C),
       ref_D(ref_D),
       output_op(output_op),
-      split_k_mode(split_k_mode)
+      split_k_mode(split_k_mode),
+      ptr_Vector(ptr_Vector),
+      ptr_Tensor(ptr_Tensor),
+      ldr(ldr),
+      ldt(ldt)
     {
 
     }
 
   };
 
   /// Parameters structure
   struct Params {
     ConvProblemSize problem_size;
     cutlass::gemm::GemmCoord grid_tiled_shape;
-    FastDivmod stride_h_divmod;
-    FastDivmod stride_w_divmod;
+    gemm::GemmCoord implicit_gemm_problem_size;
+    int swizzle_log_tile;
+
     int gemm_k_iterations;
     typename Mma::IteratorA::Params iterator_A;
     typename Mma::IteratorA::Element const *ptr_A;
     typename Mma::IteratorB::Params iterator_B;
     typename Mma::IteratorB::Element const *ptr_B;
     typename Epilogue::OutputTileIterator::Params iterator_C;
     typename Epilogue::OutputTileIterator::Element *ptr_C;
     typename Epilogue::OutputTileIterator::Params iterator_D;
     typename Epilogue::OutputTileIterator::Element *ptr_D;
     typename EpilogueOutputOp::Params output_op;
     int *semaphore;
     SplitKMode split_k_mode;
 
+    typename Epilogue::TensorTileIterator::Params params_Tensor;
+    void * ptr_Vector;
+    typename LayoutC::Stride::Index ldr;
+    void * ptr_Tensor;
+
     //
     // Methods
     //
 
     CUTLASS_HOST_DEVICE
-    Params(): gemm_k_iterations(0) { }
+    Params():
+      swizzle_log_tile(0), 
+      gemm_k_iterations(0),
+      ptr_Vector(nullptr),
+      ldr(0),
+      ptr_Tensor(nullptr)
+    { }
 
     /// 
     CUTLASS_HOST_DEVICE
     Params(
       Arguments const &args,
       int *semaphore = nullptr
     ):
       problem_size(args.problem_size),
-      stride_h_divmod(args.problem_size.stride_h),
-      stride_w_divmod(args.problem_size.stride_w),
+      implicit_gemm_problem_size(cutlass::conv::implicit_gemm_problem_size(kConvolutionalOperator, args.problem_size)),
       iterator_A(Mma::IteratorA::getParams(args.problem_size, args.ref_A.layout())),
       ptr_A(args.ref_A.data()),
       iterator_B(args.problem_size, args.ref_B.layout()),
       ptr_B(args.ref_B.data()),
-      iterator_C(ConvOutputIteratorParameter::layout(args.ref_C), args.problem_size, ThreadblockShape::kM),
+      iterator_C(ConvOutputIteratorParameter::layout(args.ref_C)),
       ptr_C(args.ref_C.data()),
-      iterator_D(ConvOutputIteratorParameter::layout(args.ref_D), args.problem_size, ThreadblockShape::kM),
+      iterator_D(ConvOutputIteratorParameter::layout(args.ref_D)),
       ptr_D(args.ref_D.data()),
       output_op(args.output_op),
       semaphore(semaphore),
-      split_k_mode(args.split_k_mode)
+      split_k_mode(args.split_k_mode),
+      params_Tensor(args.ldt),
+      ptr_Vector(args.ptr_Vector), 
+      ldr(args.ldr),
+      ptr_Tensor(args.ptr_Tensor)
+
     {
       gemm_k_iterations = implicit_gemm_k_iterations(kConvolutionalOperator, ThreadblockShape::kK, args.problem_size);
 
       ThreadblockSwizzle threadblock_swizzle;
 
       grid_tiled_shape = threadblock_swizzle.get_tiled_shape(
-        kConvolutionalOperator,
-        args.problem_size,
+        implicit_gemm_problem_size,
         {ThreadblockShape::kM, ThreadblockShape::kN, ThreadblockShape::kK},
         args.problem_size.split_k_slices);
+
+      swizzle_log_tile = threadblock_swizzle.get_log_tile(grid_tiled_shape);
     }
   };
 
   /// Shared memory storage structure
   union SharedStorage {
     typename Mma::SharedStorage main_loop;
     typename Epilogue::SharedStorage epilogue;
   };
-  
+
   //
   // Methods
   //
 
   CUTLASS_HOST_DEVICE
-  ImplicitGemmConvolutionStridedDgrad() { } 
+  ImplicitGemmConvolutionWithFusedEpilogue() { } 
 
   /// Executes one ImplicitGEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
     // Compute threadblock location
     ThreadblockSwizzle threadblock_swizzle;
 
     cutlass::gemm::GemmCoord threadblock_tile_idx =
-        threadblock_swizzle.get_tile_offset(params.grid_tiled_shape);
+        threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
 
     // Early exit if CTA is out of range
     if (params.grid_tiled_shape.m() <= threadblock_tile_idx.m() ||
       params.grid_tiled_shape.n() <= threadblock_tile_idx.n()) {
 
       return;
     }
 
     // Compute position within threadblock
     int thread_idx = threadIdx.x;
 
-    // Compute starting filter position for strided dgrad
-    int tile_m_per_filter = strided_dgrad_tile_m_per_filter(params.problem_size, 
-                                                            ThreadblockShape::kM);
-    int filter_tile_m = (threadblock_tile_idx.m() / tile_m_per_filter);
+    // Construct iterators to A and B operands
+    typename Mma::IteratorA iterator_A(
+      params.iterator_A,
+      params.problem_size,
+      params.ptr_A,
+      thread_idx,
+      MatrixCoord(
+        threadblock_tile_idx.m() * Mma::Shape::kM,
+        threadblock_tile_idx.k() * Mma::Shape::kK
+      )
+    );
     
-
-    // The subsequent fast_divmod() operations are equivalent to the following logical computation:
-    //
-    // int start_r = filter_tile_m / (params.problem_size.stride_w);
-    // int start_s = filter_tile_m % (params.problem_size.stride_w);
-
-    int start_r, start_s;
-    params.stride_w_divmod(start_r, start_s, filter_tile_m);
-
-    int filter_r = start_r;
-    int filter_s = start_s;
-
-    if (params.problem_size.mode == Mode::kConvolution) {
-      filter_r = (params.problem_size.R - 1 - filter_r);
-      filter_s = (params.problem_size.S - 1 - filter_s);
-    }
-
-    // Starting h, w positions for filter position in gemm_k=0
-    int start_h, start_w;
-    strided_dgrad_starting_coords(
+    typename Mma::IteratorB iterator_B(
+      params.iterator_B,
       params.problem_size,
-      params.stride_h_divmod, params.stride_w_divmod,
-      filter_r, filter_s,
-      start_h, start_w);
-
-    if (start_h >= params.problem_size.H || start_w >= params.problem_size.W) {
-      return;
-    }
-
-    typename Mma::FragmentC accumulators;
-
-    accumulators.clear();
+      params.ptr_B,
+      thread_idx,
+      MatrixCoord(
+        threadblock_tile_idx.k() * Mma::Shape::kK,
+        threadblock_tile_idx.n() * Mma::Shape::kN
+      )
+    );
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
     int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
     int lane_idx = threadIdx.x % 32;
 
-    // Check if CTA contributes valid MMA (Dy * w) and accumulator will be non-zero after MMA
-    if (start_r < params.problem_size.R && start_s < params.problem_size.S) {
-      // Scale gemm_k_iterations for strided dgrad
-      int gemm_k_iterations = (params.gemm_k_iterations / (params.problem_size.R * params.problem_size.S)
-                              ) * params.problem_size.num_gemm_k_filter_positions(start_r, start_s);
-      
-      // Construct iterators to A and B operands
-      typename Mma::IteratorA iterator_A(
-        params.iterator_A,
-        params.problem_size,
-        params.ptr_A,
-        thread_idx,
-        params.stride_h_divmod, params.stride_w_divmod,
-        start_r, start_s,
-        MatrixCoord(
-          threadblock_tile_idx.m() * Mma::Shape::kM,
-          threadblock_tile_idx.k() * Mma::Shape::kK
-        ) 
-      );
-      
-      typename Mma::IteratorB iterator_B(
-        params.iterator_B,
-        params.problem_size,
-        params.ptr_B,
-        thread_idx,
-        start_r, start_s,
-        MatrixCoord(
-          threadblock_tile_idx.k() * Mma::Shape::kK,
-          threadblock_tile_idx.n() * Mma::Shape::kN
-        )
-      );
-
-      //
-      // Main loop
-      //
+    //
+    // Main loop
+    //
 
-      // Construct thread-scoped matrix multiply
-      Mma mma(shared_storage.main_loop, thread_idx, warp_idx, lane_idx);
+    // Construct thread-scoped matrix multiply
+    Mma mma(shared_storage.main_loop, thread_idx, warp_idx, lane_idx);
 
-      // Compute threadblock-scoped matrix multiply-add
-      mma(gemm_k_iterations, accumulators, iterator_A, iterator_B, accumulators);
-    }
+    typename Mma::FragmentC accumulators;
+
+    accumulators.clear();
+
+    // Compute threadblock-scoped matrix multiply-add
+    mma(params.gemm_k_iterations, accumulators, iterator_A, iterator_B, accumulators);
 
     //
     // Epilogue
     //
 
     EpilogueOutputOp output_op(params.output_op);
 
     // Construct the semaphore.
     int block_idx = threadblock_tile_idx.m() + threadblock_tile_idx.n() * params.grid_tiled_shape.m();
-    Semaphore semaphore(params.semaphore + block_idx, thread_idx);
 
+    Semaphore semaphore(params.semaphore + block_idx, thread_idx);
+    
     // Compute logical position within grid
     threadblock_tile_idx =
-        threadblock_swizzle.get_tile_offset(params.grid_tiled_shape);
+        threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
 
     // If performing a reduction via split-K, fetch the initial synchronization
     if (params.split_k_mode == SplitKMode::kSerial && params.grid_tiled_shape.k() > 1) {
-
+        
       // Fetch the synchronization lock initially but do not block.
       semaphore.fetch();
 
       // Indicate which position in a serial reduction the output operator is currently updating
       output_op.set_k_partition(threadblock_tile_idx.k(), params.grid_tiled_shape.k());
     }
 
@@ -412,79 +390,108 @@
 
     // Tile iterator writing to destination tensor
     typename Epilogue::OutputTileIterator iterator_D(
       params.iterator_D,
       params.ptr_D,
       ConvOutputIteratorParameter::extent(params.problem_size),
       thread_idx,
-      params.stride_h_divmod, params.stride_w_divmod,
-      start_r, start_s,
       threadblock_offset
     );
-
-    // Construct the epilogue
-    Epilogue epilogue(
-      shared_storage.epilogue,
+    
+    // Tile iterator reading from source accumulator tensor
+    typename Epilogue::OutputTileIterator iterator_C(
+      params.iterator_C,
+      params.ptr_C,
+      ConvOutputIteratorParameter::extent(params.problem_size),
       thread_idx,
-      warp_idx,
-      lane_idx);
+      threadblock_offset
+    );
 
-    if (output_op.is_source_needed())
-    {
-      // Tile iterator reading from source accumulator tensor
-      typename Epilogue::OutputTileIterator iterator_C(
-        params.iterator_C,
-        params.ptr_C,
+    typename Epilogue::ElementTensor *ptr_Tensor = 
+      static_cast<typename Epilogue::ElementTensor *>(params.ptr_Tensor);
+
+    // Define the reduction output pointer and move to the appropriate place
+    typename Epilogue::ElementVector *ptr_Vector = 
+      static_cast<typename Epilogue::ElementVector *>(params.ptr_Vector);
+
+    // Additional tensor to load from
+    typename Epilogue::TensorTileIterator tensor_iterator(
+        params.params_Tensor,
+        // Only the final block outputs Tensor
+        ((params.split_k_mode == SplitKMode::kSerial && params.grid_tiled_shape.k() > 1) &&
+         (params.grid_tiled_shape.k() != threadblock_tile_idx.k() + 1))
+            ? nullptr
+            : ptr_Tensor,
         ConvOutputIteratorParameter::extent(params.problem_size),
         thread_idx,
-        params.stride_h_divmod, params.stride_w_divmod,
-        start_r, start_s,
         threadblock_offset);
 
-      // Wait on the semaphore - this latency may have been covered by iterator construction
-      if (params.split_k_mode == SplitKMode::kSerial && params.grid_tiled_shape.k() > 1) {
+    // Construct the epilogue
+    Epilogue epilogue(
+      shared_storage.epilogue, 
+      thread_idx, 
+      warp_idx, 
+      lane_idx);
 
-        // For subsequent threadblocks, the source matrix is held in the 'D' tensor.
-        if (threadblock_tile_idx.k()) {
-          iterator_C = iterator_D;
-        }
+    // Move to appropriate location for this output tile
+    if (ptr_Vector) {
+      ptr_Vector += threadblock_offset.column() + threadblock_tile_idx.m() * params.ldr;
+    }
 
-        semaphore.wait(threadblock_tile_idx.k());
+    // Wait on the semaphore - this latency may have been covered by iterator construction
+    if (params.split_k_mode == SplitKMode::kSerial && params.grid_tiled_shape.k() > 1) {
+        
+      // For subsequent threadblocks, the source matrix is held in the 'D' tensor.
+      if (threadblock_tile_idx.k()) {
+        iterator_C = iterator_D;
       }
 
-      // Run epilogue with addend source iterator
-      epilogue(output_op, iterator_D, accumulators, iterator_C);
+      semaphore.wait(threadblock_tile_idx.k());
+
     }
-    else
-    {
-      // Run epilogue without addend source iterator
-      epilogue(output_op, iterator_D, accumulators);
+    // Each split-k-slice writes to a unique tensor location
+    else if (params.split_k_mode == SplitKMode::kParallel) {
+      iterator_D.add_pointer_offset(threadblock_tile_idx.k() * 
+        cutlass::conv::implicit_gemm_tensor_c_size(ConvOperator, params.problem_size));
     }
 
+    // Execute the epilogue operator to update the destination tensor.
+    epilogue(output_op,
+             // Only the final block uses Vector
+             ((params.split_k_mode == SplitKMode::kSerial && params.grid_tiled_shape.k() > 1) &&
+              (params.grid_tiled_shape.k() != threadblock_tile_idx.k() + 1))
+                 ? nullptr
+                 : ptr_Vector,
+             iterator_D,
+             accumulators,
+             iterator_C,
+             tensor_iterator,
+            ConvOutputIteratorParameter::extent(params.problem_size),
+             threadblock_offset);
+  
     //
     // Release the semaphore
     //
 
-    if (params.split_k_mode == SplitKMode::kSerial && params.grid_tiled_shape.k() > 1) {
+    if (params.split_k_mode == SplitKMode::kSerial && params.grid_tiled_shape.k() > 1) { 
 
       int lock = 0;
       if (params.grid_tiled_shape.k() == threadblock_tile_idx.k() + 1) {
 
         // The final threadblock resets the semaphore for subsequent grids.
         lock = 0;
       }
       else {
         // Otherwise, the semaphore is incremented
         lock = threadblock_tile_idx.k() + 1;
       }
-
+      
       semaphore.release(lock);
     }
-
-  }
+  } 
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace kernel
 } // namespace conv
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_with_fused_epilogue.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_planar_complex.h`

 * *Files 26% similar despite different names*

```diff
@@ -25,475 +25,377 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Template for a pipelined Implicit GEMM kernel.
+  \brief Epilogue for threadblock scoped GEMMs using Tensor Ops.
+
+  The epilogue rearranges the result of a matrix product through shared memory to match canonical
+  tensor layouts in global memory. Epilogues support conversion and reduction operations.
+
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
-
-#include "cutlass/aligned_buffer.h"
-#include "cutlass/array.h"
 #include "cutlass/numeric_types.h"
-#include "cutlass/matrix_shape.h"
-#include "cutlass/semaphore.h"
-#include "cutlass/tensor_ref.h"
+#include "cutlass/array.h"
+#include "cutlass/array_planar_complex.h"
+#include "cutlass/layout/vector.h"
 #include "cutlass/layout/tensor.h"
+#include "cutlass/tensor_coord.h"
+#include "cutlass/aligned_buffer.h"
+#include "cutlass/functional.h"
+
 #include "cutlass/gemm/gemm.h"
-#include "cutlass/conv/convolution.h"
-#include "cutlass/conv/conv2d_problem_size.h"
-#include "cutlass/conv/conv3d_problem_size.h"
-#include "cutlass/epilogue/threadblock/output_iterator_parameter.h"
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+#include "cutlass/transform/pitch_linear_thread_map.h"
+#include "cutlass/transform/threadblock/regular_tile_iterator.h"
+
+#include "cutlass/epilogue/threadblock/epilogue_base.h"
+#include "cutlass/epilogue/threadblock/predicated_tile_iterator.h"
+
+////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
-namespace conv {
-namespace kernel {
+namespace epilogue {
+namespace threadblock {
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
+/// Epilogue operator for planar-complex output representations.
+///
+/// Note, as with most CUTLASS components for planar complex, the template arguments describe
+/// the underlying real data type.
 template <
-  typename Mma_,                                  ///! Threadblock-scoped matrix multiply-accumulate 
-  typename Epilogue_,                             ///! Epilogue
-  typename ThreadblockSwizzle_,                   ///! Threadblock swizzling function
-  conv::Operator ConvOperator,                    ///! Convolutional operator (Fprop, Dgrad, Wgrad)
-  typename ConvProblemSize_ = Conv2dProblemSize   ///! Convolutional operator on 2D or 3D problem
+  typename Shape_,                          ///< Shape of threadblock tile (concept: GemmShape)
+  typename WarpMmaOperator_,                ///< Warp-level MMA operator (concept: gemm::warp::MmaTensorOp)
+  int PartitionsK,                          ///< Number of partitions of the K dimension
+  typename OutputTileIterator_,             ///< Tile iterator reading and writing output tensors
+  typename AccumulatorFragmentIterator_,    ///< Fragment iterator selecting accumulators
+  typename WarpTileIterator_,               ///< Warp-scoped tile iterator writing accumulators to SMEM
+  typename SharedLoadIterator_,             ///< Threadblock-scoped tile iterator loading from SMEM
+  typename OutputOp_,                       ///< Output operator
+  typename Padding_                         ///< Padding added to SMEM allocation to avoid bank conflicts (concept: MatrixShape)
 >
-struct ImplicitGemmConvolutionWithFusedEpilogue {
-
-  using Mma = Mma_;
-  using Epilogue = Epilogue_;
-  using EpilogueOutputOp = typename Epilogue::OutputOp;
-  using ThreadblockSwizzle = ThreadblockSwizzle_;
-  static Operator const kConvolutionalOperator = ConvOperator;
-
-  using ElementA = typename Mma::IteratorA::Element;
-  using LayoutA = typename Mma::IteratorA::Layout;
-  using ElementB = typename Mma::IteratorB::Element;
-  using LayoutB = typename Mma::IteratorB::Layout;
-  using ElementC = typename EpilogueOutputOp::ElementOutput;
-
-  /// Set output tensor C layout
-  using LayoutC = LayoutA;
-
-  using ElementAccumulator = typename EpilogueOutputOp::ElementAccumulator;
-  using ElementCompute = typename EpilogueOutputOp::ElementCompute;
-
-  using WarpMmaOperator = typename Mma::Policy::Operator;
-
-  using ArchMmaOperator = typename WarpMmaOperator::ArchMmaOperator;
-  using MathOperator = typename ArchMmaOperator::Operator;
+class EpiloguePlanarComplex {
+public:
   
-  using OperatorClass = typename WarpMmaOperator::OperatorClass;
-  using ArchTag = typename WarpMmaOperator::ArchTag;
+  using Shape = Shape_;
+  using WarpMmaOperator = WarpMmaOperator_;
+  static int const kPartitionsK = PartitionsK;
+  using OutputTileIterator = OutputTileIterator_;
+  using AccumulatorFragmentIterator = AccumulatorFragmentIterator_;
+  using WarpTileIterator = WarpTileIterator_;
+  using SharedLoadIterator = SharedLoadIterator_;
+  using OutputOp = OutputOp_;
+  using Padding = Padding_;
+
+  /// Output layout is always row-major
+  using Layout = layout::RowMajor;
+  using LongIndex = typename Layout::LongIndex;
+
+  /// The complete warp-level accumulator tile
+  using AccumulatorTile = ArrayPlanarComplex<
+    typename WarpMmaOperator::FragmentC::Element, 
+    WarpMmaOperator::FragmentC::kElements
+  >;
+
+  /// Accumulator element
+  using ElementAccumulator = typename WarpTileIterator::Element;
+
+  /// Output element
+  using ElementOutput = typename OutputTileIterator::Element;
+
+  /// Output access size
+  static int const kElementsPerAccess = OutputTileIterator::kElementsPerAccess;
+
+  /// Tensor reference to destination tensor
+  using TensorRef = typename OutputTileIterator::TensorRef;
+
+  /// Tensor reference to sync tensor
+  using SyncTensorRef = typename cutlass::TensorRef<int, cutlass::layout::PackedVectorLayout>;
+
+  /// Const tensor reference to source tensor
+  using ConstTensorRef = typename OutputTileIterator::ConstTensorRef;
+
+  /// Array type used to output
+  using OutputAccessType = Array<
+    typename OutputTileIterator::Element, OutputTileIterator::kElementsPerAccess>;
 
-  using ThreadblockShape = typename Mma::Shape;
+  /// Array type used by output functor
+  using AccumulatorAccessType = Array<typename WarpTileIterator::Element, OutputTileIterator::kElementsPerAccess>; 
+  
+  /// Shape of each warp-level operation
   using WarpShape = typename WarpMmaOperator::Shape;
-  using InstructionShape = typename ArchMmaOperator::Shape;
-
-  static int const kStages = Mma::kStages;
-  static IteratorAlgorithm const kIteratorAlgorithm = Mma::IteratorA::kIteratorAlgorithm; 
-  static StrideSupport const kStrideSupport = Mma::IteratorA::kStrideSupport;
-
-  /// Warp count (concept: GemmShape)
-  using WarpCount = typename Mma::WarpCount;
-  static int const kThreadCount = 32 * WarpCount::kCount;
-
-  using TensorRefA = typename Mma::IteratorA::TensorRef;
-  using TensorRefB = typename Mma::IteratorB::TensorRef;
-  using TensorRefC = cutlass::TensorRef<ElementC, LayoutC>;
-
-  /// Check iterator A and B convolution dimension are the same and 
-  // set device::ImplicitGemmConvolution::kConvDim
-  static_assert(Mma::IteratorA::kConvDim == Mma::IteratorB::kConvDim, 
-    "Convolution on different different dimensions is not supported");
-  static int const kConvDim = Mma::IteratorA::kConvDim;
-
-  /// Conv dimension and problem size structure (Conv2d or Conv3d)
-  using ConvProblemSize = ConvProblemSize_;
-
-  static conv::GroupMode const kGroupMode = conv::GroupMode::kNone;
-
-  /// Wgrad C stride idx for implicit gemm algorithm 
-  // Conv2d row-major matrix C (KxRSC) 
-  // Conv3d row-major matrix C (KxTRSC)
-  static int const kWgradCStrideIdx = 
-    platform::is_same<LayoutC, cutlass::layout::TensorNHWC>::value ? 2 : 3;
-
-  /// This chooses the appropriate stride element of the C tensor.
-  static int const kTensorCStrideIdx = 
-    (kConvolutionalOperator == conv::Operator::kWgrad ? kWgradCStrideIdx : 0);
 
-  //
-  //
-  //
-  using ConvOutputIteratorParameter = epilogue::threadblock::ConvOutputIteratorParameter<
-    LayoutC,
-    typename Epilogue::OutputTileIterator::Layout, 
-    TensorRefC,
-    ConvOperator,
-    ConvProblemSize
-    >;
+  /// Number of warps
+  using WarpCount = gemm::GemmShape<
+    Shape::kM / WarpShape::kM,
+    Shape::kN / WarpShape::kN,
+    kPartitionsK
+  >;
 
-  /// Argument structure
-  struct Arguments {
+  /// Shared memory allocation
+  struct SharedStorage {
 
     //
-    // Data members
+    // Type definitions
     //
 
-    ConvProblemSize problem_size;
-    TensorRefA ref_A;
-    TensorRefB ref_B;
-    TensorRefC ref_C;
-    TensorRefC ref_D;
+    /// Element type of shared memory
+    using Element = typename WarpTileIterator::Element;
+
+    /// Tensor reference to shared memory allocation
+    using TensorRef = typename WarpTileIterator::TensorRef;
 
-    typename EpilogueOutputOp::Params output_op;
-    SplitKMode split_k_mode;
+    /// Layout of shared memory allocation
+    using Layout = typename WarpTileIterator::Layout;
+    
+    /// Logical shape of the shared memory tile written to by all warps.
+    using Shape = MatrixShape<
+      WarpCount::kM * WarpTileIterator::Shape::kRow * WarpCount::kK,
+      WarpCount::kN * WarpTileIterator::Shape::kColumn
+    >;
 
-    void * ptr_Vector;
-    void * ptr_Tensor;
+    /// Shape of the shared memory allocation for the epilogue    
+    using StorageShape = MatrixShape<
+      Shape::kRow + Padding::kRow, 
+      Shape::kColumn + Padding::kColumn
+    >;
 
-    typename LayoutC::Stride::Index ldr;
-    typename LayoutC::Stride::Index ldt;
+    static int const kImaginaryStride = StorageShape::kCount;
 
     //
-    // Methods
+    // Data members
     //
 
-    /// Default ctor
-    CUTLASS_HOST_DEVICE
-    Arguments() { }
-   
-    CUTLASS_HOST_DEVICE 
-    Arguments(
-      ConvProblemSize const & problem_size
-    ):
-      problem_size(problem_size) { }
-
-    CUTLASS_HOST_DEVICE
-    Arguments(
-      ConvProblemSize const & problem_size,
-      TensorRefA const & ref_A,
-      TensorRefB const & ref_B,
-      TensorRefC const & ref_C,
-      TensorRefC const & ref_D,
-      typename EpilogueOutputOp::Params const & output_op,
-      SplitKMode const & split_k_mode = SplitKMode::kSerial,
-      void * ptr_Vector = nullptr,
-      void * ptr_Tensor = nullptr,
-      typename LayoutC::Stride::Index ldr = 0,
-      typename LayoutC::Stride::Index ldt = 0
-    ):
-      problem_size(problem_size),
-      ref_A(ref_A),
-      ref_B(ref_B),
-      ref_C(ref_C),
-      ref_D(ref_D),
-      output_op(output_op),
-      split_k_mode(split_k_mode),
-      ptr_Vector(ptr_Vector),
-      ptr_Tensor(ptr_Tensor),
-      ldr(ldr),
-      ldt(ldt)
-    {
-
-    }
-
-  };
-
-  /// Parameters structure
-  struct Params {
-    ConvProblemSize problem_size;
-    cutlass::gemm::GemmCoord grid_tiled_shape;
-    gemm::GemmCoord implicit_gemm_problem_size;
-    int swizzle_log_tile;
-
-    int gemm_k_iterations;
-    typename Mma::IteratorA::Params iterator_A;
-    typename Mma::IteratorA::Element const *ptr_A;
-    typename Mma::IteratorB::Params iterator_B;
-    typename Mma::IteratorB::Element const *ptr_B;
-    typename Epilogue::OutputTileIterator::Params iterator_C;
-    typename Epilogue::OutputTileIterator::Element *ptr_C;
-    typename Epilogue::OutputTileIterator::Params iterator_D;
-    typename Epilogue::OutputTileIterator::Element *ptr_D;
-    typename EpilogueOutputOp::Params output_op;
-    int *semaphore;
-    SplitKMode split_k_mode;
-
-    typename Epilogue::TensorTileIterator::Params params_Tensor;
-    void * ptr_Vector;
-    typename LayoutC::Stride::Index ldr;
-    void * ptr_Tensor;
+    AlignedBuffer<Element, kImaginaryStride * 2> storage;
 
     //
     // Methods
     //
 
-    CUTLASS_HOST_DEVICE
-    Params():
-      swizzle_log_tile(0), 
-      gemm_k_iterations(0),
-      ptr_Vector(nullptr),
-      ldr(0),
-      ptr_Tensor(nullptr)
-    { }
-
-    /// 
-    CUTLASS_HOST_DEVICE
-    Params(
-      Arguments const &args,
-      int *semaphore = nullptr
-    ):
-      problem_size(args.problem_size),
-      implicit_gemm_problem_size(cutlass::conv::implicit_gemm_problem_size(kConvolutionalOperator, args.problem_size)),
-      iterator_A(Mma::IteratorA::getParams(args.problem_size, args.ref_A.layout())),
-      ptr_A(args.ref_A.data()),
-      iterator_B(args.problem_size, args.ref_B.layout()),
-      ptr_B(args.ref_B.data()),
-      iterator_C(ConvOutputIteratorParameter::layout(args.ref_C)),
-      ptr_C(args.ref_C.data()),
-      iterator_D(ConvOutputIteratorParameter::layout(args.ref_D)),
-      ptr_D(args.ref_D.data()),
-      output_op(args.output_op),
-      semaphore(semaphore),
-      split_k_mode(args.split_k_mode),
-      params_Tensor(args.ldt),
-      ptr_Vector(args.ptr_Vector), 
-      ldr(args.ldr),
-      ptr_Tensor(args.ptr_Tensor)
-
-    {
-      gemm_k_iterations = implicit_gemm_k_iterations(kConvolutionalOperator, ThreadblockShape::kK, args.problem_size);
-
-      ThreadblockSwizzle threadblock_swizzle;
-
-      grid_tiled_shape = threadblock_swizzle.get_tiled_shape(
-        implicit_gemm_problem_size,
-        {ThreadblockShape::kM, ThreadblockShape::kN, ThreadblockShape::kK},
-        args.problem_size.split_k_slices);
+    /// Returns a pointer to the shared memory buffer
+    CUTLASS_DEVICE
+    Element *data() {
+      return storage.data();
+    }
 
-      swizzle_log_tile = threadblock_swizzle.get_log_tile(grid_tiled_shape);
+    /// Returns a tensor reference to the shared memory buffer
+    CUTLASS_DEVICE
+    TensorRef reference() {
+      return TensorRef(
+        storage.data(), 
+        Layout::packed({StorageShape::kRow, StorageShape::kColumn}));
     }
   };
 
-  /// Shared memory storage structure
-  union SharedStorage {
-    typename Mma::SharedStorage main_loop;
-    typename Epilogue::SharedStorage epilogue;
-  };
+private:
 
   //
-  // Methods
+  // Data members
   //
 
-  CUTLASS_HOST_DEVICE
-  ImplicitGemmConvolutionWithFusedEpilogue() { } 
+  SharedStorage &shared_storage_;
 
-  /// Executes one ImplicitGEMM
-  CUTLASS_DEVICE
-  void operator()(Params const &params, SharedStorage &shared_storage) {
+  /// Loads fragment from shared memory aligned with output tensor
+  SharedLoadIterator shared_load_iterator_;
 
-    // Compute threadblock location
-    ThreadblockSwizzle threadblock_swizzle;
+  /// Stores a warp's fragment of accumulators to SMEM
+  WarpTileIterator warp_tile_iterator_;
 
-    cutlass::gemm::GemmCoord threadblock_tile_idx =
-        threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
+public:
 
-    // Early exit if CTA is out of range
-    if (params.grid_tiled_shape.m() <= threadblock_tile_idx.m() ||
-      params.grid_tiled_shape.n() <= threadblock_tile_idx.n()) {
+  /// Constructor
+  CUTLASS_DEVICE
+  EpiloguePlanarComplex(
+    SharedStorage &shared_storage,    ///< Shared storage object    
+    int thread_idx,                   ///< ID of a thread within the threadblock
+    int warp_idx,                     ///< ID of warp within threadblock
+    int lane_idx                      ///< Id of thread within warp
+  ):
+    shared_storage_(shared_storage),
+    shared_load_iterator_(shared_storage.reference(), thread_idx),
+    warp_tile_iterator_(shared_storage.reference(), lane_idx) {
+
+    // Compute warp location within threadblock tile by mapping the warp_id to three coordinates:
+    //
+    //   _m: the warp's position within the threadblock along the M dimension
+    //   _n: the warp's position within the threadblock along the N dimension
+    //   _k: the warp's position within the threadblock along the K dimension
+
+    int warp_k = warp_idx / (WarpCount::kM * WarpCount::kN);
+    int warp_mn = warp_idx % (WarpCount::kM * WarpCount::kN);
+    int warp_m = warp_mn % WarpCount::kM;
+    int warp_n = warp_mn / WarpCount::kM;
 
-      return;
-    }
+    MatrixCoord warp_offset{warp_k * WarpCount::kM + warp_m, warp_n};
 
-    // Compute position within threadblock
-    int thread_idx = threadIdx.x;
+    warp_tile_iterator_.add_tile_offset(warp_offset);
+  }
 
-    // Construct iterators to A and B operands
-    typename Mma::IteratorA iterator_A(
-      params.iterator_A,
-      params.problem_size,
-      params.ptr_A,
-      thread_idx,
-      MatrixCoord(
-        threadblock_tile_idx.m() * Mma::Shape::kM,
-        threadblock_tile_idx.k() * Mma::Shape::kK
-      )
-    );
-    
-    typename Mma::IteratorB iterator_B(
-      params.iterator_B,
-      params.problem_size,
-      params.ptr_B,
-      thread_idx,
-      MatrixCoord(
-        threadblock_tile_idx.k() * Mma::Shape::kK,
-        threadblock_tile_idx.n() * Mma::Shape::kN
-      )
-    );
-
-    // Broadcast the warp_id computed by lane 0 to ensure dependent code
-    // is compiled as warp-uniform.
-    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
-    int lane_idx = threadIdx.x % 32;
+  /// Streams the result to global memory
+  CUTLASS_DEVICE
+  void operator()(
+    OutputOp const &output_op,                        ///< Output operator
+    OutputTileIterator destination_iterator_real,     ///< Tile iterator for destination
+    OutputTileIterator destination_iterator_imag,     ///< Tile iterator for destination
+    AccumulatorTile const &accumulators,              ///< Complete warp-level accumulator tile
+    OutputTileIterator source_iterator_real,          ///< Threadblock tile coordinate in GEMM (in units of threadblock tiles)
+    OutputTileIterator source_iterator_imag) {        ///< Threadblock tile coordinate in GEMM (in units of threadblock tiles)
+
+    typename OutputTileIterator::Fragment source_fragment_real;
+    typename OutputTileIterator::Fragment source_fragment_imag;
+
+    if (!output_op.is_source_needed()) {
+      source_iterator_real.clear_mask();
+      source_iterator_imag.clear_mask();
+    }
+
+    source_fragment_real.clear();
+    source_fragment_imag.clear();
 
     //
-    // Main loop
+    // Iterator over warp-level accumulator fragment
     //
 
-    // Construct thread-scoped matrix multiply
-    Mma mma(shared_storage.main_loop, thread_idx, warp_idx, lane_idx);
+    AccumulatorFragmentIterator accum_fragment_iterator_real(accumulators.real);
+    AccumulatorFragmentIterator accum_fragment_iterator_imag(accumulators.imag);
 
-    typename Mma::FragmentC accumulators;
+    //
+    // Iterate over accumulator tile
+    // 
 
-    accumulators.clear();
+    CUTLASS_PRAGMA_UNROLL
+    for (int iter = 0; iter < OutputTileIterator::kIterations; ++iter) {
 
-    // Compute threadblock-scoped matrix multiply-add
-    mma(params.gemm_k_iterations, accumulators, iterator_A, iterator_B, accumulators);
+      //
+      // Load the source
+      //
 
-    //
-    // Epilogue
-    //
+      source_iterator_real.load(source_fragment_real);
+      source_iterator_imag.load(source_fragment_imag);
 
-    EpilogueOutputOp output_op(params.output_op);
+      ++source_iterator_real;
+      ++source_iterator_imag;
 
-    // Construct the semaphore.
-    int block_idx = threadblock_tile_idx.m() + threadblock_tile_idx.n() * params.grid_tiled_shape.m();
+      //
+      // Convert and store fragment
+      //
+      
+      __syncthreads();
 
-    Semaphore semaphore(params.semaphore + block_idx, thread_idx);
-    
-    // Compute logical position within grid
-    threadblock_tile_idx =
-        threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
-
-    // If performing a reduction via split-K, fetch the initial synchronization
-    if (params.split_k_mode == SplitKMode::kSerial && params.grid_tiled_shape.k() > 1) {
-        
-      // Fetch the synchronization lock initially but do not block.
-      semaphore.fetch();
+      typename AccumulatorFragmentIterator::Fragment accum_fragment_real;
+      typename AccumulatorFragmentIterator::Fragment accum_fragment_imag;
 
-      // Indicate which position in a serial reduction the output operator is currently updating
-      output_op.set_k_partition(threadblock_tile_idx.k(), params.grid_tiled_shape.k());
-    }
+      accum_fragment_iterator_real.load(accum_fragment_real);
+      accum_fragment_iterator_imag.load(accum_fragment_imag);
+      
+      ++accum_fragment_iterator_real;
+      ++accum_fragment_iterator_imag;
 
-    MatrixCoord threadblock_offset(
-      threadblock_tile_idx.m() * Mma::Shape::kM,
-      threadblock_tile_idx.n() * Mma::Shape::kN
-    );
-
-    // Tile iterator writing to destination tensor
-    typename Epilogue::OutputTileIterator iterator_D(
-      params.iterator_D,
-      params.ptr_D,
-      ConvOutputIteratorParameter::extent(params.problem_size),
-      thread_idx,
-      threadblock_offset
-    );
-    
-    // Tile iterator reading from source accumulator tensor
-    typename Epilogue::OutputTileIterator iterator_C(
-      params.iterator_C,
-      params.ptr_C,
-      ConvOutputIteratorParameter::extent(params.problem_size),
-      thread_idx,
-      threadblock_offset
-    );
-
-    typename Epilogue::ElementTensor *ptr_Tensor = 
-      static_cast<typename Epilogue::ElementTensor *>(params.ptr_Tensor);
-
-    // Define the reduction output pointer and move to the appropriate place
-    typename Epilogue::ElementVector *ptr_Vector = 
-      static_cast<typename Epilogue::ElementVector *>(params.ptr_Vector);
-
-    // Additional tensor to load from
-    typename Epilogue::TensorTileIterator tensor_iterator(
-        params.params_Tensor,
-        // Only the final block outputs Tensor
-        ((params.split_k_mode == SplitKMode::kSerial && params.grid_tiled_shape.k() > 1) &&
-         (params.grid_tiled_shape.k() != threadblock_tile_idx.k() + 1))
-            ? nullptr
-            : ptr_Tensor,
-        ConvOutputIteratorParameter::extent(params.problem_size),
-        thread_idx,
-        threadblock_offset);
-
-    // Construct the epilogue
-    Epilogue epilogue(
-      shared_storage.epilogue, 
-      thread_idx, 
-      warp_idx, 
-      lane_idx);
-
-    // Move to appropriate location for this output tile
-    if (ptr_Vector) {
-      ptr_Vector += threadblock_offset.column() + threadblock_tile_idx.m() * params.ldr;
-    }
+      this->warp_tile_iterator_.store(accum_fragment_real);
+      this->warp_tile_iterator_.store_with_pointer_offset(accum_fragment_imag, SharedStorage::kImaginaryStride);
 
-    // Wait on the semaphore - this latency may have been covered by iterator construction
-    if (params.split_k_mode == SplitKMode::kSerial && params.grid_tiled_shape.k() > 1) {
-        
-      // For subsequent threadblocks, the source matrix is held in the 'D' tensor.
-      if (threadblock_tile_idx.k()) {
-        iterator_C = iterator_D;
-      }
+      __syncthreads();
 
-      semaphore.wait(threadblock_tile_idx.k());
+      //
+      // Load fragments from shared memory
+      //
 
-    }
-    // Each split-k-slice writes to a unique tensor location
-    else if (params.split_k_mode == SplitKMode::kParallel) {
-      iterator_D.add_pointer_offset(threadblock_tile_idx.k() * 
-        cutlass::conv::implicit_gemm_tensor_c_size(ConvOperator, params.problem_size));
-    }
+      typename SharedLoadIterator::Fragment aligned_accum_fragment_real[kPartitionsK];
+      typename SharedLoadIterator::Fragment aligned_accum_fragment_imag[kPartitionsK];
 
-    // Execute the epilogue operator to update the destination tensor.
-    epilogue(output_op,
-             // Only the final block uses Vector
-             ((params.split_k_mode == SplitKMode::kSerial && params.grid_tiled_shape.k() > 1) &&
-              (params.grid_tiled_shape.k() != threadblock_tile_idx.k() + 1))
-                 ? nullptr
-                 : ptr_Vector,
-             iterator_D,
-             accumulators,
-             iterator_C,
-             tensor_iterator,
-            ConvOutputIteratorParameter::extent(params.problem_size),
-             threadblock_offset);
-  
-    //
-    // Release the semaphore
-    //
+      shared_load_iterator_.load(aligned_accum_fragment_real[0]);
+      shared_load_iterator_.load_with_pointer_offset(aligned_accum_fragment_imag[0], SharedStorage::kImaginaryStride);
 
-    if (params.split_k_mode == SplitKMode::kSerial && params.grid_tiled_shape.k() > 1) { 
+      // If the number of k-slices is > 1 - perform a reduction amongst the k-slices
+      static_assert(kPartitionsK  == 1, "Sliced-K not supported for planar complex at this time");
+    
+      //
+      // Compute the output result
+      //
+     
+      typename OutputTileIterator::Fragment output_fragment_real;
+      typename OutputTileIterator::Fragment output_fragment_imag;
+
+      apply_output_operator_(
+        output_fragment_real, 
+        output_fragment_imag, 
+        output_op, 
+        aligned_accum_fragment_real[0],
+        aligned_accum_fragment_imag[0], 
+        source_fragment_real,
+        source_fragment_imag);
+
+      //
+      // Store the final result
+      //
 
-      int lock = 0;
-      if (params.grid_tiled_shape.k() == threadblock_tile_idx.k() + 1) {
+      destination_iterator_real.store(output_fragment_real);
+      destination_iterator_imag.store(output_fragment_imag);
 
-        // The final threadblock resets the semaphore for subsequent grids.
-        lock = 0;
-      }
-      else {
-        // Otherwise, the semaphore is incremented
-        lock = threadblock_tile_idx.k() + 1;
-      }
-      
-      semaphore.release(lock);
+      ++destination_iterator_real;
+      ++destination_iterator_imag;
     }
-  } 
+  }
+
+private:
+
+  /// Helper to invoke the output functor over each vector of output
+  CUTLASS_DEVICE
+  void apply_output_operator_(
+    typename OutputTileIterator::Fragment &output_fragment_real,
+    typename OutputTileIterator::Fragment &output_fragment_imag,
+    OutputOp const &output_op,                    ///< Output operator
+    typename SharedLoadIterator::Fragment const &aligned_accum_fragment_real,
+    typename SharedLoadIterator::Fragment const &aligned_accum_fragment_imag,
+    typename OutputTileIterator::Fragment const &source_fragment_real,
+    typename OutputTileIterator::Fragment const &source_fragment_imag) {
+
+    OutputAccessType *output_frag_real_ptr = 
+      reinterpret_cast<OutputAccessType *>(&output_fragment_real);
+
+    OutputAccessType *output_frag_imag_ptr = 
+      reinterpret_cast<OutputAccessType *>(&output_fragment_imag);
+
+    AccumulatorAccessType const *compute_frag_real_ptr = 
+      reinterpret_cast<AccumulatorAccessType const *>(&aligned_accum_fragment_real);
+
+    AccumulatorAccessType const *compute_frag_imag_ptr = 
+      reinterpret_cast<AccumulatorAccessType const *>(&aligned_accum_fragment_imag);
+
+    OutputAccessType const *source_frag_real_ptr = 
+      reinterpret_cast<OutputAccessType const *>(&source_fragment_real);
+
+    OutputAccessType const *source_frag_imag_ptr = 
+      reinterpret_cast<OutputAccessType const *>(&source_fragment_imag);
+
+    int const kOutputOpIterations = 
+      OutputTileIterator::Fragment::kElements / OutputTileIterator::kElementsPerAccess;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < kOutputOpIterations; ++i) {
+
+      // Call the output operator
+      auto result_fragment = output_op(
+        make_ArrayPlanarComplex(compute_frag_real_ptr[i], compute_frag_imag_ptr[i]), 
+        make_ArrayPlanarComplex(source_frag_real_ptr[i], source_frag_imag_ptr[i])
+      );
+
+      output_frag_real_ptr[i] = result_fragment.real;
+      output_frag_imag_ptr[i] = result_fragment.imag;
+    }
+  }
+
 };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
-} // namespace kernel
-} // namespace conv
+} // namespace threadblock
+} // namespace epilogue
 } // namespace cutlass
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/thread/depthwise_mma.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/thread/depthwise_mma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_analytic.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_analytic.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_optimized.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_analytic.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_analytic.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_optimized.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_analytic.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_analytic.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_few_channels.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_few_channels.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_fixed_channels.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_fixed_channels.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_optimized.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_analytic.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_analytic.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_few_channels.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_few_channels.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_fixed_channels.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_fixed_channels.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_optimized.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_params.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_params.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_tile_iterator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_tile_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_analytic.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_analytic.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_optimized.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_analytic.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_analytic.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_optimized.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_analytic.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_analytic.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_optimized.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_analytic.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_analytic.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_optimized.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_analytic.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_analytic.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_optimized.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_analytic.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_analytic.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_optimized.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_params.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_params.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_analytic.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_analytic.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_optimized.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_analytic.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_analytic.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_optimized.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_direct_conv_params.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_direct_conv_params.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_fixed_stride_dilation.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_optimized.h`

 * *Files 13% similar despite different names*

```diff
@@ -57,22 +57,19 @@
 namespace conv {
 namespace threadblock {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename Shape_,
           typename OutputTileShape_,
-          typename StrideShape_,
-          typename DilationShape_,
-          typename ActivationShape_,
           typename Element_,
           typename Layout_,
           typename ThreadMap_,
           typename AccessType_ = cutlass::AlignedArray<Element_, ThreadMap_::kElementsPerAccess> >
-class DepthwiseFpropActivationDirect2dConvTileAccessIteratorFixedStrideDilation {
+class DepthwiseFpropActivationDirect2dConvTileAccessIteratorOptimized {
  public:
   //
   // Types
   //
 
   using Shape = Shape_;
   using OutputTileShape = OutputTileShape_;
@@ -85,24 +82,15 @@
   using Index = typename Layout::Index;
   using LongIndex = typename Layout::LongIndex;
   static IteratorAlgorithm const kIteratorAlgorithm = conv::IteratorAlgorithm::kOptimized;
   static StrideSupport const kStrideSupport = conv::StrideSupport::kStrided;
   static int const kConvDim = 2;
   using ConvProblemSize = typename conv::Conv2dProblemSize;
 
-  // Compilation value of stride , dialtion and activation shape
-  using StrideShape = StrideShape_;
-  using DilationShape = DilationShape_;
-  using ActivationShape = ActivationShape_;
-
-
   static int const kAccessesPerVector = ThreadMap::kElementsPerAccess / AccessType::kElements;
-  static int const kActivationSize = ThreadMap::Iterations::kCount * ThreadMap::kElementsPerAccess * ThreadMap::kThreads *
-           sizeof_bits<Element>::value / 8;
-
 
   static_assert(!(ThreadMap::kElementsPerAccess % AccessType::kElements),
                 "Vectors implied by the thread map must be divisible by the access type.");
 
   //
   // Simplifying assertions
   //
@@ -111,15 +99,15 @@
   static_assert(OutputTileShape::kN == 1, "Require OutputTileShape::kN == 1");
   static_assert(OutputTileShape::kC == Shape::kColumn, "Require OutputTile shape == channels per threadblock");
 
   //
   // Parameters structure
   //
 
-  using Params = Depthwise2dFpropDirectConvActivationIteratorFixedStrideDilationParams<Layout>;
+  using Params = Depthwise2dFpropDirectConvParams<Layout>;
 
  private:
   Conv2dProblemSize const &problem_size_;
   Params const &params_;
   char const *pointer_;
 
   // Base channels for current threadblock
@@ -130,73 +118,74 @@
   TensorCoord activatioin_base_;
   // Intial thread positioin
   int offset_initial_hwc_;
   // Overall load instruction per thread.
   int iterator_load_;
   // thread loading position.
   int iterator_hwc_;
-  // activation N is inside the Tensor or not
-  bool valid_n_;
+  // Number of loads for activations tensor X.
+  const int number_of_loads_;
 
  public:
 
 
   CUTLASS_HOST_DEVICE
-  DepthwiseFpropActivationDirect2dConvTileAccessIteratorFixedStrideDilation(
+  DepthwiseFpropActivationDirect2dConvTileAccessIteratorOptimized(
       Params const &params,
       Conv2dProblemSize const &problem_size,
       Element const *ptr,
       int thread_idx,
       MatrixCoord const &threadblock_offset =
           MatrixCoord()
       )
       : params_(params),
         problem_size_(problem_size),
         pointer_(reinterpret_cast<char const *>(ptr)),
         offset_intial_npq_(threadblock_offset.row()),
         offset_initial_hwc_(thread_idx),
-        iterator_load_(0) {
+        iterator_load_(0),
+        number_of_loads_(params.activation_load_count) {
     
     base_c_ = threadblock_offset.column();
 
-    set_iteration_index(0);
-
     set_activation_coord(offset_intial_npq_);
 
+    set_iteration_index(0);
   }
 
   CUTLASS_HOST_DEVICE
   void set_activation_coord(int offset_npq) {
     int offset_inital_n, offset_inital_p, offset_inital_q;
     int residual;
 
     params_.pq_divmod(offset_inital_n, residual, offset_npq);
     params_.q_divmod(offset_inital_p, offset_inital_q, residual);
 
     int base_n = offset_inital_n;
 
     int base_h =
-        offset_inital_p * OutputTileShape::kH * StrideShape::kRow - problem_size_.pad_h;
+        offset_inital_p * OutputTileShape::kH * problem_size_.stride_h - problem_size_.pad_h;
 
     int base_w =
-        offset_inital_q * OutputTileShape::kW * StrideShape::kColumn - problem_size_.pad_w;
+        offset_inital_q * OutputTileShape::kW * problem_size_.stride_w - problem_size_.pad_w;
 
     activatioin_base_ = TensorCoord(base_n, base_h, base_w, base_c_);
-
-    valid_n_ = activatioin_base_.n() < problem_size_.N;
   }
 
   CUTLASS_HOST_DEVICE
   static Params getParams(Conv2dProblemSize const &problem_size, Layout const &layout) {
     return Params(
         problem_size,
         layout,
         {Shape::kRow, Shape::kColumn},
         {OutputTileShape::kN, OutputTileShape::kH, OutputTileShape::kW, OutputTileShape::kC},
-        kActivationSize);
+        sizeof_bits<Element>::value,
+        ThreadMap::kThreads,
+        ThreadMap::Detail::ShapeVec::kContiguous,
+        ThreadMap::kElementsPerAccess);
   }
 
   /// Overrides the internal iteration index
   CUTLASS_HOST_DEVICE
   void set_iteration_index(Index index) {
     iterator_hwc_ = offset_initial_hwc_ + index * ThreadMap::kThreads;
     iterator_load_ = index;
@@ -209,41 +198,39 @@
   }
 
   CUTLASS_HOST_DEVICE
   void advance() {
     // Go to next threadblock
     offset_intial_npq_ += problem_size_.split_k_slices;
 
-    set_iteration_index(0);
-
     set_activation_coord(offset_intial_npq_);
   }
 
   /// Returns the coordinate in the activations tensor X that is currently pointed to
   /// by the iterator.
   CUTLASS_HOST_DEVICE
   TensorCoord at() const {
+    
     int c = iterator_hwc_ %  ThreadMap::Detail::ShapeVec::kContiguous ;
     int next = iterator_hwc_ /  ThreadMap::Detail::ShapeVec::kContiguous ;
-    int h = next / ActivationShape::kW;
-    int w = next % ActivationShape::kW;
+    int h, w;
+    params_.activation_tile_w_divmod(h, w, next) ;
 
     c = c * AccessType::kElements;
 
     return activatioin_base_ + TensorCoord(0, h, w, c);
   }
 
   /// Returns true if the current coordinate is within the activations tensor X
   CUTLASS_HOST_DEVICE
   bool valid() const {
     TensorCoord coord = at();
-    bool valid_c = coord.c() < problem_size_.C;
-    bool valid_h = coord.h() >= 0 && coord.h() < problem_size_.H;
-    bool valid_w = coord.w() >= 0 && coord.w() < problem_size_.W;
-    return valid_n_ ? valid_c & valid_h & valid_w : 0;
+
+    return coord.n() < problem_size_.N && coord.h() >= 0 && coord.h() < problem_size_.H &&
+           coord.w() >= 0 && coord.w() < problem_size_.W && coord.c() < problem_size_.C;
   }
 
   /// Returns a pointer to the vector starting at the current coordinate
   CUTLASS_HOST_DEVICE
   AccessType const *get() const {
     TensorCoord coord = at();
     LongIndex offset = params_.layout(coord);
@@ -252,54 +239,44 @@
         reinterpret_cast<AccessType const *>(pointer_ + offset * sizeof_bits<Element>::value / 8);
 
     return ptr;
   }
 
   /// Increments to the next memory access
   CUTLASS_HOST_DEVICE
-  DepthwiseFpropActivationDirect2dConvTileAccessIteratorFixedStrideDilation &operator++() {
+  DepthwiseFpropActivationDirect2dConvTileAccessIteratorOptimized &operator++() {
 
     ++iterator_load_;
     iterator_hwc_ += ThreadMap::kThreads;
 
-    if (iterator_load_ < ThreadMap::Iterations::kCount) {
+    if (iterator_load_ < number_of_loads_) {
        return *this;
     }
     
     iterator_load_ = 0;
     iterator_hwc_ = offset_initial_hwc_;
 
     return *this;
   }
 
   /// Determines the activation size loaded by iterator
   CUTLASS_HOST_DEVICE
   int get_load_size() {
-    return kActivationSize;
+    return params_.activation_size;
   }
 
   /// Determines the iterations needed
   CUTLASS_HOST_DEVICE
   int get_iteration_num() {
-    return ThreadMap::Iterations::kCount;
+    return number_of_loads_;
   }
 
   /// Determines whether the Depthwise fprop can execute the given problem.
   CUTLASS_HOST_DEVICE
   static Status can_implement(Conv2dProblemSize const &problem_size) {
-
-    // check stride and dilation constraint
-    if (problem_size.stride_h != StrideShape::kRow || problem_size.stride_w != StrideShape::kColumn) {
-      return Status::kErrorInvalidProblem;
-    }
-
-    if (problem_size.dilation_h != DilationShape::kRow || problem_size.dilation_w != DilationShape::kColumn) {
-      return Status::kErrorInvalidProblem;
-    }
-
     // check alignment constraint on iterator's contiguous dimension
     if (problem_size.C % AccessType::kElements) {
       return Status::kErrorInvalidProblem;
     }
 
     return Status::kSuccess;
   }
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_optimized.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_base.h`

 * *Files 23% similar despite different names*

```diff
@@ -25,267 +25,216 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Templates implementing loading of convolution tiles mapped to GEMM A (activation tile)
-    matrix from memory.
+  \brief Epilogue for threadblock scoped GEMMs using Tensor Ops.
+
+  The epilogue rearranges the result of a matrix product through shared memory to match canonical
+  tensor layouts in global memory. Epilogues support conversion and reduction operations.
 
-    This iterator assumes TensorNHWC layout of tensors in Global Memory.
 */
 
 #pragma once
 
-#include "cutlass/array.h"
-#include "cutlass/conv/conv2d_problem_size.h"
-#include "cutlass/conv/convolution.h"
-#include "cutlass/conv/threadblock/depthwise_direct_conv_params.h"
-#include "cutlass/coord.h"
+#if !defined(__CUDACC_RTC__)
+#include <type_traits>
+#include <utility>
+#endif
+
+#if defined(__CUDACC_RTC__)
+#include <cuda/std/cassert>
+#else
+#include <assert.h>
+#endif
+
 #include "cutlass/cutlass.h"
-#include "cutlass/layout/matrix.h"
-#include "cutlass/layout/pitch_linear.h"
-#include "cutlass/layout/tensor.h"
 #include "cutlass/matrix_shape.h"
-#include "cutlass/predicate_vector.h"
-#include "cutlass/tensor_ref.h"
-#include "cutlass/tensor_view.h"
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-namespace cutlass {
-namespace conv {
-namespace threadblock {
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-template <typename Shape_,
-          typename OutputTileShape_,
-          typename Element_,
-          typename Layout_,
-          typename ThreadMap_,
-          typename AccessType_ = cutlass::AlignedArray<Element_, ThreadMap_::kElementsPerAccess> >
-class DepthwiseFpropActivationDirect2dConvTileAccessIteratorOptimized {
- public:
-  //
-  // Types
-  //
-
-  using Shape = Shape_;
-  using OutputTileShape = OutputTileShape_;
-  using Element = Element_;
-  using Layout = Layout_;
-  using TensorCoord = typename Layout::TensorCoord;
-  using ThreadMap = ThreadMap_;
-  using AccessType = AccessType_;
-  using TensorRef = cutlass::TensorRef<Element, Layout>;
-  using Index = typename Layout::Index;
-  using LongIndex = typename Layout::LongIndex;
-  static IteratorAlgorithm const kIteratorAlgorithm = conv::IteratorAlgorithm::kOptimized;
-  static StrideSupport const kStrideSupport = conv::StrideSupport::kStrided;
-  static int const kConvDim = 2;
-  using ConvProblemSize = typename conv::Conv2dProblemSize;
-
-  static int const kAccessesPerVector = ThreadMap::kElementsPerAccess / AccessType::kElements;
-
-  static_assert(!(ThreadMap::kElementsPerAccess % AccessType::kElements),
-                "Vectors implied by the thread map must be divisible by the access type.");
-
-  //
-  // Simplifying assertions
-  //
-  static_assert(ThreadMap::Iterations::kContiguous == 1, "Require Iterations::kContiguous == 1");
-  
-  static_assert(OutputTileShape::kN == 1, "Require OutputTileShape::kN == 1");
-  static_assert(OutputTileShape::kC == Shape::kColumn, "Require OutputTile shape == channels per threadblock");
-
-  //
-  // Parameters structure
-  //
+#include "cutlass/numeric_types.h"
+#include "cutlass/array.h"
+#include "cutlass/layout/vector.h"
+#include "cutlass/layout/tensor.h"
+#include "cutlass/tensor_coord.h"
+#include "cutlass/aligned_buffer.h"
 
-  using Params = Depthwise2dFpropDirectConvParams<Layout>;
+#include "cutlass/gemm/gemm.h"
 
- private:
-  Conv2dProblemSize const &problem_size_;
-  Params const &params_;
-  char const *pointer_;
-
-  // Base channels for current threadblock
-  int base_c_;
-  // Base activation index for current threadblock
-  int offset_intial_npq_;
-  // Base activation coord for current threadblock
-  TensorCoord activatioin_base_;
-  // Intial thread positioin
-  int offset_initial_hwc_;
-  // Overall load instruction per thread.
-  int iterator_load_;
-  // thread loading position.
-  int iterator_hwc_;
-  // Number of loads for activations tensor X.
-  const int number_of_loads_;
-
- public:
-
-
-  CUTLASS_HOST_DEVICE
-  DepthwiseFpropActivationDirect2dConvTileAccessIteratorOptimized(
-      Params const &params,
-      Conv2dProblemSize const &problem_size,
-      Element const *ptr,
-      int thread_idx,
-      MatrixCoord const &threadblock_offset =
-          MatrixCoord()
-      )
-      : params_(params),
-        problem_size_(problem_size),
-        pointer_(reinterpret_cast<char const *>(ptr)),
-        offset_intial_npq_(threadblock_offset.row()),
-        offset_initial_hwc_(thread_idx),
-        iterator_load_(0),
-        number_of_loads_(params.activation_load_count) {
-    
-    base_c_ = threadblock_offset.column();
+#include "cutlass/transform/pitch_linear_thread_map.h"
 
-    set_activation_coord(offset_intial_npq_);
+////////////////////////////////////////////////////////////////////////////////
 
-    set_iteration_index(0);
-  }
+namespace cutlass {
+namespace epilogue {
+namespace threadblock {
 
-  CUTLASS_HOST_DEVICE
-  void set_activation_coord(int offset_npq) {
-    int offset_inital_n, offset_inital_p, offset_inital_q;
-    int residual;
+////////////////////////////////////////////////////////////////////////////////
 
-    params_.pq_divmod(offset_inital_n, residual, offset_npq);
-    params_.q_divmod(offset_inital_p, offset_inital_q, residual);
+//
+// This is used for metaprogramming epilogue functors. If they define 
+// `static bool const kIsHeavy = true;`, then the epilogue functor itself is
+// not inlined. This results in smaller code and is advantageous if the epilogue
+// functor consists of many instructions.
+//
+// If the epilogue functor does not define `kIsHeavy` or if it is `false`, then
+// the behavior from CUTLASS 2.5 and before is retained. The epilogue is fully
+// unrolled and inlined.
+//
 
-    int base_n = offset_inital_n;
+template<class> 
+struct TypeSink {  typedef void type; };
 
-    int base_h =
-        offset_inital_p * OutputTileShape::kH * problem_size_.stride_h - problem_size_.pad_h;
+template<class T> using TypeSinkT = typename TypeSink<T>::type;
 
-    int base_w =
-        offset_inital_q * OutputTileShape::kW * problem_size_.stride_w - problem_size_.pad_w;
+template<class T, class=void> struct IsEpilogueFunctorHeavy {
+  static bool const value = false;
+};
 
-    activatioin_base_ = TensorCoord(base_n, base_h, base_w, base_c_);
-  }
+template<class T> struct IsEpilogueFunctorHeavy<T, TypeSinkT< decltype( T::kIsHeavy ) > > {
+  static bool const value = T::kIsHeavy;
+};
 
-  CUTLASS_HOST_DEVICE
-  static Params getParams(Conv2dProblemSize const &problem_size, Layout const &layout) {
-    return Params(
-        problem_size,
-        layout,
-        {Shape::kRow, Shape::kColumn},
-        {OutputTileShape::kN, OutputTileShape::kH, OutputTileShape::kW, OutputTileShape::kC},
-        sizeof_bits<Element>::value,
-        ThreadMap::kThreads,
-        ThreadMap::Detail::ShapeVec::kContiguous,
-        ThreadMap::kElementsPerAccess);
-  }
+////////////////////////////////////////////////////////////////////////////////
 
-  /// Overrides the internal iteration index
-  CUTLASS_HOST_DEVICE
-  void set_iteration_index(Index index) {
-    iterator_hwc_ = offset_initial_hwc_ + index * ThreadMap::kThreads;
-    iterator_load_ = index;
-  }
+/// Base class for epilogues defining warp-level 
+template <
+  typename Shape_,                          ///< Shape of threadblock tile (concept: GemmShape)
+  typename WarpShape_,                      ///< Warp-level MMA operator (concept: gemm::warp::MmaTensorOp)
+  int PartitionsK,                          ///< Number of partitions of the K dimension
+  typename AccumulatorFragmentIterator_,    ///< Fragment iterator selecting accumulators
+  typename WarpTileIterator_,               ///< Warp-scoped tile iterator writing accumulators to SMEM
+  typename Padding_,                        ///< Padding added to SMEM allocation to avoid bank conflicts (concept: MatrixShape)
+  int FragmentsPerIteration = 1
+>
+class EpilogueBase {
+public:
 
-  /// Adds a pointer offset in units of Element
-  CUTLASS_HOST_DEVICE
-  void add_pointer_offset(LongIndex pointer_offset) {
-    pointer_ += pointer_offset * sizeof_bits<Element>::value / 8;
-  }
+  using Shape = Shape_;
+  using WarpShape = WarpShape_;
+  static int const kPartitionsK = PartitionsK;
+  using AccumulatorFragmentIterator = AccumulatorFragmentIterator_;
+  using WarpTileIterator = WarpTileIterator_;
+  using Padding = Padding_;
+
+  /// Output layout is always row-major
+  using Layout = layout::RowMajor;
+
+  /// The complete warp-level accumulator tile
+  using AccumulatorTile = typename AccumulatorFragmentIterator::AccumulatorTile;
+
+  /// Accumulator element
+  using ElementAccumulator = typename AccumulatorTile::Element;
+
+  /// Number of warps
+  using WarpCount = gemm::GemmShape<
+    Shape::kM / WarpShape::kM,
+    Shape::kN / WarpShape::kN,
+    kPartitionsK
+  >;
 
-  CUTLASS_HOST_DEVICE
-  void advance() {
-    // Go to next threadblock
-    offset_intial_npq_ += problem_size_.split_k_slices;
+  /// Use this to control the granularity of one epilogue 'iteration'
+  static int const kFragmentsPerIteration = FragmentsPerIteration;
 
-    set_activation_coord(offset_intial_npq_);
-  }
+public:
 
-  /// Returns the coordinate in the activations tensor X that is currently pointed to
-  /// by the iterator.
-  CUTLASS_HOST_DEVICE
-  TensorCoord at() const {
+  /// Shared storage allocation needed by the epilogue
+  struct SharedStorage {
     
-    int c = iterator_hwc_ %  ThreadMap::Detail::ShapeVec::kContiguous ;
-    int next = iterator_hwc_ /  ThreadMap::Detail::ShapeVec::kContiguous ;
-    int h, w;
-    params_.activation_tile_w_divmod(h, w, next) ;
+    //
+    // Type definitions
+    //
 
-    c = c * AccessType::kElements;
+    /// Element type of shared memory
+    using Element = typename WarpTileIterator::Element;
 
-    return activatioin_base_ + TensorCoord(0, h, w, c);
-  }
-
-  /// Returns true if the current coordinate is within the activations tensor X
-  CUTLASS_HOST_DEVICE
-  bool valid() const {
-    TensorCoord coord = at();
+    /// Tensor reference to shared memory allocation
+    using TensorRef = typename WarpTileIterator::TensorRef;
 
-    return coord.n() < problem_size_.N && coord.h() >= 0 && coord.h() < problem_size_.H &&
-           coord.w() >= 0 && coord.w() < problem_size_.W && coord.c() < problem_size_.C;
-  }
+    /// Layout of shared memory allocation
+    using Layout = typename WarpTileIterator::Layout;
+    
+    /// Logical shape of the shared memory tile written to by all warps.
+    using Shape = MatrixShape<
+      WarpCount::kM * WarpTileIterator::Shape::kRow * WarpCount::kK,
+      WarpCount::kN * WarpTileIterator::Shape::kColumn
+    >;
+
+    /// Shape of the shared memory allocation for the epilogue    
+    using StorageShape = MatrixShape<
+      (Shape::kRow + Padding::kRow) * kFragmentsPerIteration, 
+      Shape::kColumn + Padding::kColumn
+    >;
+
+    //
+    // Data members
+    //
+
+    AlignedBuffer<Element, StorageShape::kCount> storage;
+
+    //
+    // Methods
+    //
+
+    /// Returns a pointer to the shared memory buffer
+    CUTLASS_DEVICE
+    Element *data() {
+      return storage.data();
+    }
 
-  /// Returns a pointer to the vector starting at the current coordinate
-  CUTLASS_HOST_DEVICE
-  AccessType const *get() const {
-    TensorCoord coord = at();
-    LongIndex offset = params_.layout(coord);
+    /// Returns a tensor reference to the shared memory buffer
+    CUTLASS_DEVICE
+    TensorRef reference() {
+      return TensorRef(
+        storage.data(), 
+        Layout::packed({StorageShape::kRow, StorageShape::kColumn}));
+    }
+  };
 
-    AccessType const *ptr =
-        reinterpret_cast<AccessType const *>(pointer_ + offset * sizeof_bits<Element>::value / 8);
+protected:
 
-    return ptr;
-  }
+  //
+  // Data members
+  //
 
-  /// Increments to the next memory access
-  CUTLASS_HOST_DEVICE
-  DepthwiseFpropActivationDirect2dConvTileAccessIteratorOptimized &operator++() {
+  SharedStorage &shared_storage_;
 
-    ++iterator_load_;
-    iterator_hwc_ += ThreadMap::kThreads;
+  /// Stores a warp's fragment of accumulators to SMEM
+  WarpTileIterator warp_tile_iterator_;
 
-    if (iterator_load_ < number_of_loads_) {
-       return *this;
-    }
-    
-    iterator_load_ = 0;
-    iterator_hwc_ = offset_initial_hwc_;
+public:
 
-    return *this;
-  }
+  /// Constructor
+  CUTLASS_DEVICE
+  EpilogueBase(
+    SharedStorage &shared_storage,    ///< Shared storage object    
+    int thread_idx,                   ///< ID of a thread within the threadblock
+    int warp_idx,                     ///< ID of warp within threadblock
+    int lane_idx                      ///< Id of thread within warp
+  ):
+    shared_storage_(shared_storage),
+    warp_tile_iterator_(shared_storage.reference(), lane_idx) {
 
-  /// Determines the activation size loaded by iterator
-  CUTLASS_HOST_DEVICE
-  int get_load_size() {
-    return params_.activation_size;
-  }
+    // Compute warp location within threadblock tile by mapping the warp_id to three coordinates:
+    //
+    //   _m: the warp's position within the threadblock along the M dimension
+    //   _n: the warp's position within the threadblock along the N dimension
+    //   _k: the warp's position within the threadblock along the K dimension
 
-  /// Determines the iterations needed
-  CUTLASS_HOST_DEVICE
-  int get_iteration_num() {
-    return number_of_loads_;
-  }
+    int warp_k = warp_idx / (WarpCount::kM * WarpCount::kN);
+    int warp_mn = warp_idx % (WarpCount::kM * WarpCount::kN);
+    int warp_m = warp_mn % WarpCount::kM;
+    int warp_n = warp_mn / WarpCount::kM;
 
-  /// Determines whether the Depthwise fprop can execute the given problem.
-  CUTLASS_HOST_DEVICE
-  static Status can_implement(Conv2dProblemSize const &problem_size) {
-    // check alignment constraint on iterator's contiguous dimension
-    if (problem_size.C % AccessType::kElements) {
-      return Status::kErrorInvalidProblem;
-    }
+    MatrixCoord warp_offset{warp_k * WarpCount::kM + warp_m, warp_n};
 
-    return Status::kSuccess;
+    warp_tile_iterator_.add_tile_offset(warp_offset);
   }
 };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
-}  // namespace threadblock
-}  // namespace conv
-}  // namespace cutlass
+} // namespace threadblock
+} // namespace epilogue
+} // namespace cutlass
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_direct_conv_multistage.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_direct_conv_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_filter_tile_access_iterator_direct_conv_optimized.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_filter_tile_access_iterator_direct_conv_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_pipelined.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_pipelined.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_mma_base.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_mma_base.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_mma_core_with_lane_access_size.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_mma_core_with_lane_access_size.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_fprop_fusion_multistage.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_fprop_fusion_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_multistage.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_pipelined.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_pipelined.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_wgrad_fusion_multistage.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_wgrad_fusion_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_access_iterator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_access_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_iterator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/threadblock_swizzle.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/threadblock_swizzle.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/warp/mma_depthwise_simt.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/warp/mma_depthwise_simt.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/warp/mma_depthwise_simt_tile_iterator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/warp/mma_depthwise_simt_tile_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/conv/warp/scale_bias_relu_transform.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/warp/scale_bias_relu_transform.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/coord.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/coord.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/core_io.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/core_io.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/cutlass.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/cutlass.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/device_kernel.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/device_kernel.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/activation.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/activation.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/conversion_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/conversion_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_elementwise.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_elementwise.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_relu.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_relu.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_clamp.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_clamp.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_dgelu.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_dgelu.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_drelu.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_drelu.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_gelu.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_gelu.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_generic.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_generic.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_hardswish.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_hardswish.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_leaky_relu.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_leaky_relu.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_params.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_params.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_planar_complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_with_elementwise.h`

 * *Files 23% similar despite different names*

```diff
@@ -25,210 +25,207 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-  \brief Functor performing linear combination operations on planar-complex arrays
+  
+  \brief Functor performing linear combination with elementwise
 */
 
 #pragma once
 
+#include <cutlass/half.h>
 #include "cutlass/cutlass.h"
 #include "cutlass/numeric_types.h"
-#include "cutlass/complex.h"
-#include "cutlass/array_planar_complex.h"
+#include "cutlass/array.h"
+#include "cutlass/constants.h"
+#include "cutlass/fast_math.h"
 #include "cutlass/functional.h"
 #include "cutlass/numeric_conversion.h"
+#include "cutlass/epilogue/thread/activation.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace thread {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Applies a linear combination operator to arrays of planar-complex elements.
+/// Applies a linear combination operator to an array of elements.
 ///
 /// D = alpha * accumulator + beta * source + uniform
 ///
-/// Note, as with most CUTLASS components for planar complex, the template arguments describe
-/// the underlying real data type.
 template <
-  typename ElementOutput_,                             ///< Data type used to load and store tensors
+  typename ElementCompute_,                            ///< Data type returned by this functor
+  typename ElementAccumulator_,                        ///< Data type of accumulators
+  typename ElementSource_,                             ///< Data type of source tensor
+  typename ElementTensor_,                             ///< Data type of additional tensor
   int Count,                                           ///< Number of elements computed per operation
                                                        ///< Usually it is 128/sizeof_bits<ElementOutput_>,
                                                        ///< but we use 64 or 32 sometimes when there are not enough data to store
-  typename ElementAccumulator_ = ElementOutput_,       ///< Accumulator data type
-  typename ElementCompute_ = ElementOutput_,           ///< Data type used to compute linear combination
   FloatRoundStyle Round = FloatRoundStyle::round_to_nearest
 >
-class LinearCombinationPlanarComplex {
+class LinearCombinationWithElementwise {
 public:
 
-  using ElementOutput = ElementOutput_;
-  using ElementAccumulator = ElementAccumulator_;
+  using ElementOutput = ElementSource_;
   using ElementCompute = ElementCompute_;
+  using ElementAccumulator = ElementAccumulator_;
+  using ElementSource = ElementSource_;
+  using ElementTensor = ElementTensor_;
+
+  static bool const kIsHeavy = true;
 
   static int const kCount = Count;
 
-  using FragmentOutput = ArrayPlanarComplex<ElementOutput, kCount>;
-  using FragmentAccumulator = ArrayPlanarComplex<ElementAccumulator, kCount>;
-  using ComputeFragment = ArrayPlanarComplex<ElementCompute, kCount>;
+  using FragmentCompute = Array<ElementCompute, kCount>;
+  using FragmentAccumulator = Array<ElementAccumulator, kCount>;
+  using FragmentSource = Array<ElementSource, kCount>;
+  using FragmentTensor = Array<ElementTensor, kCount>;
 
   static FloatRoundStyle const kRound = Round;
 
   /// Host-constructable parameters structure
   struct Params {
 
-    complex<ElementCompute> alpha;                  ///< scales accumulators
-    complex<ElementCompute> beta;                   ///< scales source tensor
-    complex<ElementCompute> const *alpha_ptr;       ///< pointer to accumulator scalar - if not null, loads it from memory
-    complex<ElementCompute> const *beta_ptr;        ///< pointer to source scalar - if not null, loads it from memory
-
+    ElementCompute alpha;                  ///< scales accumulators
+    ElementCompute beta;                   ///< scales source tensor
+    ElementCompute threshold;              ///< minimum value that is output 
+    ElementCompute const *alpha_ptr;       ///< pointer to accumulator scalar - if not null, loads it from memory
+    ElementCompute const *beta_ptr;        ///< pointer to source scalar - if not null, loads it from memory
     //
     // Methods
     //
 
     CUTLASS_HOST_DEVICE
     Params(): 
       alpha(ElementCompute(1)), 
-      beta(ElementCompute(0)), 
+      beta(ElementCompute(0)),
+      threshold(ElementCompute(0)), 
       alpha_ptr(nullptr), 
       beta_ptr(nullptr) { }
 
     CUTLASS_HOST_DEVICE
     Params(
-      complex<ElementCompute> alpha,
-      complex<ElementCompute> beta
-    ): alpha(alpha), beta(beta), alpha_ptr(nullptr), beta_ptr(nullptr) {
+      ElementCompute alpha,
+      ElementCompute beta,
+      ElementCompute threshold = ElementCompute(0)
+    ): alpha(alpha), beta(beta), threshold(threshold), alpha_ptr(nullptr), beta_ptr(nullptr) {
 
     }
 
     CUTLASS_HOST_DEVICE
     Params(
-      complex<ElementCompute> const *alpha_ptr,
-      complex<ElementCompute> const *beta_ptr
-    ): alpha(complex<ElementCompute>()), beta(complex<ElementCompute>()), alpha_ptr(alpha_ptr), beta_ptr(beta_ptr) {
+      ElementCompute const *alpha_ptr,
+      ElementCompute const *beta_ptr,
+      ElementCompute threshold = ElementCompute(0)
+    ): alpha(0), beta(0), threshold(threshold), alpha_ptr(alpha_ptr), beta_ptr(beta_ptr) {
 
     }
   };
 
 private:
 
   //
   // Data members
   //
 
-  complex<ElementCompute> alpha_;
-  complex<ElementCompute> beta_;
+  ElementCompute alpha_;
+  ElementCompute beta_;
+  ElementCompute threshold_;
+  bool participates_in_reduction_;
 
 public:
 
   /// Constructs the function object, possibly loading from pointers in host memory
   CUTLASS_HOST_DEVICE
-  LinearCombinationPlanarComplex(Params const &params) {
+  LinearCombinationWithElementwise(Params const &params) {
 
     alpha_ = (params.alpha_ptr ? *params.alpha_ptr : params.alpha);
     beta_ = (params.beta_ptr ? *params.beta_ptr : params.beta);
+    threshold_ = params.threshold;
+    participates_in_reduction_ = true;
   }
 
   /// Returns true if source is needed
   CUTLASS_HOST_DEVICE
   bool is_source_needed() const {
-    return beta_.real() != ElementCompute(0) || beta_.imag() != ElementCompute(0);
+    return beta_ != ElementCompute(0);
+  }
+
+  /// Returns true if the threadblock computes the reduction
+  CUTLASS_HOST_DEVICE
+  bool participates_in_reduction() const {
+    return participates_in_reduction_;
   }
 
   /// Functionally required for serial reduction in the epilogue
   CUTLASS_HOST_DEVICE
   void set_k_partition(int k_partition, int k_partition_count) {
     if (k_partition) {
       beta_ = ElementCompute(1);
     }
-  }
 
+    if (k_partition != k_partition_count - 1) {
+      // set to NaN to make ReLU no-op for all except last k partitions
+      int64_t allones = -1;
+      threshold_ = reinterpret_cast<ElementCompute const &>(allones);
+      // Avoid computing the reduction if this isn't the final Split-K slice
+      participates_in_reduction_ = false;
+    }
+  }
+  
   /// Computes linear scaling: D = alpha * accumulator + beta * source
   CUTLASS_HOST_DEVICE
-  FragmentOutput operator()(
+  FragmentCompute operator()(
     FragmentAccumulator const &accumulator, 
-    FragmentOutput const &source) const {
+    FragmentSource const &source,
+    FragmentTensor const &tensor) const {
 
     // Convert source to interal compute numeric type
-    NumericArrayConverter<ElementCompute, ElementOutput, kCount, Round> source_converter;
+    NumericArrayConverter<ElementCompute, ElementSource, kCount, Round> source_converter;
     NumericArrayConverter<ElementCompute, ElementAccumulator, kCount, Round> accumulator_converter;
 
-    ComputeFragment converted_source(
-      source_converter(source.real), 
-      source_converter(source.imag));
-
-    ComputeFragment converted_accumulator(
-      accumulator_converter(accumulator.real), 
-      accumulator_converter(accumulator.imag));
+    FragmentCompute converted_source = source_converter(source);
+    FragmentCompute converted_accumulator = accumulator_converter(accumulator);
 
     // Perform binary operations
-    ComputeFragment intermediate;
-
-    multiplies<Array<ElementCompute, kCount> > mul_op;
-    multiply_add<Array<ElementCompute, kCount> > mul_add_op;
-
-    // complex multiply: I = beta * C
-    intermediate.real = mul_op(beta_.real(), converted_source.real);
-    intermediate.imag = mul_op(beta_.real(), converted_source.imag);
-
-    intermediate.real = mul_add_op(-beta_.imag(), converted_source.imag, intermediate.real);
-    intermediate.imag = mul_add_op( beta_.imag(), converted_source.real, intermediate.imag);
+    FragmentCompute intermediate;
 
-    // complex multiply-add: I = alpha * AB + I
-    intermediate.real = mul_add_op(alpha_.real(), converted_accumulator.real, intermediate.real);
-    intermediate.imag = mul_add_op(alpha_.real(), converted_accumulator.imag, intermediate.imag);
+    multiplies<FragmentCompute> mul_add_source;
+    multiply_add<FragmentCompute> mul_add_accumulator;
 
-    intermediate.real = mul_add_op(-alpha_.imag(), converted_accumulator.imag, intermediate.real);
-    intermediate.imag = mul_add_op( alpha_.imag(), converted_accumulator.real, intermediate.imag);
+    intermediate = mul_add_source(beta_, converted_source);                             // X =  beta * C + uniform
+    intermediate = mul_add_accumulator(alpha_, converted_accumulator, intermediate);    // D = alpha * Accum + X
 
-    // Convert to destination numeric type
-    NumericArrayConverter<ElementOutput, ElementCompute, kCount, Round> destination_converter;
-
-    return FragmentOutput(
-      destination_converter(intermediate.real), 
-      destination_converter(intermediate.imag));
+    return intermediate;
   }
 
-  /// Computes linear scaling: D = alpha * accumulator + beta * source
+  /// Computes linear scaling: D = alpha * accumulator
   CUTLASS_HOST_DEVICE
-  FragmentOutput operator()(
-    FragmentAccumulator const &accumulator) const {
+  FragmentCompute operator()(
+    FragmentAccumulator const &accumulator,
+    FragmentTensor const &tensor) const {
 
     // Convert source to interal compute numeric type
     NumericArrayConverter<ElementCompute, ElementAccumulator, kCount, Round> accumulator_converter;
 
-    ComputeFragment converted_accumulator(
-      accumulator_converter(accumulator.real), 
-      accumulator_converter(accumulator.imag));
+    FragmentCompute converted_accumulator = accumulator_converter(accumulator);
 
     // Perform binary operations
-    ComputeFragment intermediate;
-
-    multiplies<Array<ElementCompute, kCount> > mul_op;
-    multiply_add<Array<ElementCompute, kCount> > mul_add_op;
-
-    // complex multiply-add: I = alpha * AB + I
-    intermediate.real = mul_add_op(alpha_.real(), converted_accumulator.real);
-    intermediate.imag = mul_add_op(alpha_.real(), converted_accumulator.imag);
+    FragmentCompute intermediate;
 
-    intermediate.real = mul_add_op(-alpha_.imag(), converted_accumulator.imag, intermediate.real);
-    intermediate.imag = mul_add_op( alpha_.imag(), converted_accumulator.real, intermediate.imag);
+    multiplies<FragmentCompute> mul_accumulator;
 
-    // Convert to destination numeric type
-    NumericArrayConverter<ElementOutput, ElementCompute, kCount, Round> destination_converter;
+    intermediate = mul_accumulator(alpha_, converted_accumulator);    // D = alpha * Accum
 
-    return FragmentOutput(
-      destination_converter(intermediate.real), 
-      destination_converter(intermediate.imag));
+    return intermediate;
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace thread
 } // namespace epilogue
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_relu.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_relu.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_relu0.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_relu0.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_residual_block.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_residual_block.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_sigmoid.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_sigmoid.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_silu.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_silu.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_with_elementwise.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_base_streamk.h`

 * *Files 24% similar despite different names*

```diff
@@ -25,210 +25,173 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-  
-  \brief Functor performing linear combination with elementwise
+  \brief Basic subset of epilogue functionality for supporting StreamK decompositions
 */
 
+
 #pragma once
 
-#include <cutlass/half.h>
 #include "cutlass/cutlass.h"
-#include "cutlass/numeric_types.h"
-#include "cutlass/array.h"
-#include "cutlass/constants.h"
-#include "cutlass/fast_math.h"
 #include "cutlass/functional.h"
-#include "cutlass/numeric_conversion.h"
-#include "cutlass/epilogue/thread/activation.h"
+#include "cutlass/block_striped.h"
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
-namespace thread {
+namespace threadblock {
+
+////////////////////////////////////////////////////////////////////////////////
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Applies a linear combination operator to an array of elements.
-///
-/// D = alpha * accumulator + beta * source + uniform
-///
+/// StreamK epilogue functionality for cross-block accumulator fragment reduction
 template <
-  typename ElementCompute_,                            ///< Data type returned by this functor
-  typename ElementAccumulator_,                        ///< Data type of accumulators
-  typename ElementSource_,                             ///< Data type of source tensor
-  typename ElementTensor_,                             ///< Data type of additional tensor
-  int Count,                                           ///< Number of elements computed per operation
-                                                       ///< Usually it is 128/sizeof_bits<ElementOutput_>,
-                                                       ///< but we use 64 or 32 sometimes when there are not enough data to store
-  FloatRoundStyle Round = FloatRoundStyle::round_to_nearest
->
-class LinearCombinationWithElementwise {
-public:
+  typename Shape,                          ///< Shape of threadblock tile (concept: GemmShape)
+  int PartitionsK,
+  typename WarpMmaOperator,                ///< Warp-level MMA operator (concept: gemm::warp::MmaTensorOp)
+  typename AccumulatorFragmentIterator>    ///< Iterator for enumerating fragments within the per-thread tile of raw accumulators
+class EpilogueBaseStreamK
+{
+
+protected:
+
+  /// The per-thread tile of raw accumulators
+  using AccumulatorTile = typename AccumulatorFragmentIterator::AccumulatorTile;
+
+  /// Number of warps
+  using WarpCount = gemm::GemmShape<
+                        Shape::kM / WarpMmaOperator::Shape::kM,
+                        Shape::kN / WarpMmaOperator::Shape::kN,
+                        PartitionsK>;
 
-  using ElementOutput = ElementSource_;
-  using ElementCompute = ElementCompute_;
-  using ElementAccumulator = ElementAccumulator_;
-  using ElementSource = ElementSource_;
-  using ElementTensor = ElementTensor_;
-
-  static bool const kIsHeavy = true;
-
-  static int const kCount = Count;
-
-  using FragmentCompute = Array<ElementCompute, kCount>;
-  using FragmentAccumulator = Array<ElementAccumulator, kCount>;
-  using FragmentSource = Array<ElementSource, kCount>;
-  using FragmentTensor = Array<ElementTensor, kCount>;
-
-  static FloatRoundStyle const kRound = Round;
-
-  /// Host-constructable parameters structure
-  struct Params {
-
-    ElementCompute alpha;                  ///< scales accumulators
-    ElementCompute beta;                   ///< scales source tensor
-    ElementCompute threshold;              ///< minimum value that is output 
-    ElementCompute const *alpha_ptr;       ///< pointer to accumulator scalar - if not null, loads it from memory
-    ElementCompute const *beta_ptr;        ///< pointer to source scalar - if not null, loads it from memory
-    //
-    // Methods
-    //
-
-    CUTLASS_HOST_DEVICE
-    Params(): 
-      alpha(ElementCompute(1)), 
-      beta(ElementCompute(0)),
-      threshold(ElementCompute(0)), 
-      alpha_ptr(nullptr), 
-      beta_ptr(nullptr) { }
-
-    CUTLASS_HOST_DEVICE
-    Params(
-      ElementCompute alpha,
-      ElementCompute beta,
-      ElementCompute threshold = ElementCompute(0)
-    ): alpha(alpha), beta(beta), threshold(threshold), alpha_ptr(nullptr), beta_ptr(nullptr) {
+  /// Number of threads per block
+  static int const kBlockThreads = 32 * WarpCount::kCount;
 
-    }
+  /// Numerical accumulation element type
+  using ElementAccumulator = typename WarpMmaOperator::ElementC;
 
-    CUTLASS_HOST_DEVICE
-    Params(
-      ElementCompute const *alpha_ptr,
-      ElementCompute const *beta_ptr,
-      ElementCompute threshold = ElementCompute(0)
-    ): alpha(0), beta(0), threshold(threshold), alpha_ptr(alpha_ptr), beta_ptr(beta_ptr) {
+  /// Fragment type used by the accumulator tile's fragment iterator
+  using AccumulatorFragment = typename AccumulatorFragmentIterator::Fragment;
 
-    }
-  };
+public:
 
-private:
+  /// Number of AccumulatorTile fragments per thread
+  static int const kAccumulatorFragments = AccumulatorFragmentIterator::Policy::kIterations;
 
-  //
-  // Data members
-  //
-
-  ElementCompute alpha_;
-  ElementCompute beta_;
-  ElementCompute threshold_;
-  bool participates_in_reduction_;
+protected:
 
-public:
+  /// Number of AccumulatorTile fragments per block output tile
+  static int const kOutputTileFragments = kBlockThreads * kAccumulatorFragments;
 
-  /// Constructs the function object, possibly loading from pointers in host memory
-  CUTLASS_HOST_DEVICE
-  LinearCombinationWithElementwise(Params const &params) {
-
-    alpha_ = (params.alpha_ptr ? *params.alpha_ptr : params.alpha);
-    beta_ = (params.beta_ptr ? *params.beta_ptr : params.beta);
-    threshold_ = params.threshold;
-    participates_in_reduction_ = true;
-  }
+  /// Block-striped transfer utility for sharing AccumulatorFragment
+  using BlockStripedT = BlockStriped<kBlockThreads, AccumulatorFragment>;
 
-  /// Returns true if source is needed
-  CUTLASS_HOST_DEVICE
-  bool is_source_needed() const {
-    return beta_ != ElementCompute(0);
-  }
+  /// AccumulatorFragment stride in the shared workspace between different peer blocks (each thread block can share accumulators for up to two block output tiles)
+  static const int kPeerFragmentStride = kOutputTileFragments * 2;
 
-  /// Returns true if the threadblock computes the reduction
-  CUTLASS_HOST_DEVICE
-  bool participates_in_reduction() const {
-    return participates_in_reduction_;
-  }
+public:
 
-  /// Functionally required for serial reduction in the epilogue
-  CUTLASS_HOST_DEVICE
-  void set_k_partition(int k_partition, int k_partition_count) {
-    if (k_partition) {
-      beta_ = ElementCompute(1);
-    }
+  /// Workspace bytes per thread block
+  static size_t const kWorkspaceBytesPerBlock =sizeof(AccumulatorFragment) * kPeerFragmentStride;
 
-    if (k_partition != k_partition_count - 1) {
-      // set to NaN to make ReLU no-op for all except last k partitions
-      int64_t allones = -1;
-      threshold_ = reinterpret_cast<ElementCompute const &>(allones);
-      // Avoid computing the reduction if this isn't the final Split-K slice
-      participates_in_reduction_ = false;
-    }
-  }
-  
-  /// Computes linear scaling: D = alpha * accumulator + beta * source
-  CUTLASS_HOST_DEVICE
-  FragmentCompute operator()(
-    FragmentAccumulator const &accumulator, 
-    FragmentSource const &source,
-    FragmentTensor const &tensor) const {
-
-    // Convert source to interal compute numeric type
-    NumericArrayConverter<ElementCompute, ElementSource, kCount, Round> source_converter;
-    NumericArrayConverter<ElementCompute, ElementAccumulator, kCount, Round> accumulator_converter;
-
-    FragmentCompute converted_source = source_converter(source);
-    FragmentCompute converted_accumulator = accumulator_converter(accumulator);
-
-    // Perform binary operations
-    FragmentCompute intermediate;
+public:
 
-    multiplies<FragmentCompute> mul_add_source;
-    multiply_add<FragmentCompute> mul_add_accumulator;
+  /// Thread index in the threadblock
+  int thread_idx;
 
-    intermediate = mul_add_source(beta_, converted_source);                             // X =  beta * C + uniform
-    intermediate = mul_add_accumulator(alpha_, converted_accumulator, intermediate);    // D = alpha * Accum + X
+public:
 
-    return intermediate;
-  }
+  /// Constructor
+  CUTLASS_DEVICE
+  EpilogueBaseStreamK(
+      int thread_idx)                                       ///< ID of a thread within the threadblock
+  :
+      thread_idx(thread_idx)
+  {}
+
+
+  /// Aggregates the accumulator sets shared by peer blocks in the global workspace
+  CUTLASS_DEVICE
+  void reduce(
+      AccumulatorFragment &accum_fragment,                  ///< [out] sum of all shared accumulator fragments for these peer partials
+      int peer_idx_begin,
+      int peer_idx_end,
+      int reduce_fragment_idx,
+      void *workspace_ptr)
+  {
+    plus<AccumulatorFragment> add_fragments;
+
+    AccumulatorFragment *fragment_workspace = reinterpret_cast<AccumulatorFragment *>(workspace_ptr);
+
+    int fragment_offset = (peer_idx_begin * kPeerFragmentStride) + (reduce_fragment_idx * kBlockThreads);
+
+    // Load first peer fragment
+    BlockStripedT::load(accum_fragment, fragment_workspace + fragment_offset, this->thread_idx);
+
+    fragment_offset += kPeerFragmentStride;         // Move to next peer
+    fragment_offset += kOutputTileFragments;        // Move to the set of fragments for this peer's "non-started" output tile
+
+    // Reduce fragments from additional peers
+    #pragma unroll 2
+    for (; fragment_offset < peer_idx_end * kPeerFragmentStride; fragment_offset += kPeerFragmentStride)
+    {
+      // Load peer fragment
+      AccumulatorFragment addend_fragment;
+      BlockStripedT::load(addend_fragment, fragment_workspace + fragment_offset, this->thread_idx);
 
-  /// Computes linear scaling: D = alpha * accumulator
-  CUTLASS_HOST_DEVICE
-  FragmentCompute operator()(
-    FragmentAccumulator const &accumulator,
-    FragmentTensor const &tensor) const {
+      // Add peer fragment
+      accum_fragment = add_fragments(accum_fragment, addend_fragment);
+    }
+  }
 
-    // Convert source to interal compute numeric type
-    NumericArrayConverter<ElementCompute, ElementAccumulator, kCount, Round> accumulator_converter;
 
-    FragmentCompute converted_accumulator = accumulator_converter(accumulator);
+  /// Shares the accumulator set with peers in the global workspace
+  CUTLASS_DEVICE
+  void share(
+      int peer_idx,
+      void *workspace_ptr,
+      AccumulatorTile const &accumulators,
+      bool started_tile)                      ///< Whether this thread block computed the first work volume for the current output tile
+  {
+    AccumulatorFragment *fragment_workspace = reinterpret_cast<AccumulatorFragment *>(workspace_ptr);
+
+    int fragment_offset = peer_idx * kPeerFragmentStride;
+
+    if (!started_tile) {
+      // Move to the set of fragments for the "non-started" output tile
+      fragment_offset += kOutputTileFragments;
+    }
 
-    // Perform binary operations
-    FragmentCompute intermediate;
+    AccumulatorFragmentIterator accum_fragment_iterator(accumulators);
 
-    multiplies<FragmentCompute> mul_accumulator;
+    // Convert raw accumulator tile to fragments and store
+    CUTLASS_PRAGMA_UNROLL
+    for (int iter = 0; iter < kAccumulatorFragments; ++iter)
+    {
+      // Acquire reordered accumulator fragment
+      AccumulatorFragment accum_fragment;
+      accum_fragment_iterator.load(accum_fragment);
+      ++accum_fragment_iterator;
 
-    intermediate = mul_accumulator(alpha_, converted_accumulator);    // D = alpha * Accum
+      // Store accumulator fragment
+      BlockStripedT::store(fragment_workspace + fragment_offset, accum_fragment, this->thread_idx);
 
-    return intermediate;
+      fragment_offset += kBlockThreads;
+    }
   }
+
 };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
 
-} // namespace thread
+
+////////////////////////////////////////////////////////////////////////////////
+
+} // namespace threadblock
 } // namespace epilogue
 } // namespace cutlass
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/reduction_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/reduction_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/scale_type.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/scale_type.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op_blas3.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op_blas3.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_direct_store.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_direct_store.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_planar_complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_planar_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_simt.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_simt.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op_blas3.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op_blas3.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_broadcast.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_broadcast.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_reduction.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_reduction.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_simt.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_simt.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_tensor_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_volta_tensor_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_volta_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_wmma_tensor_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_wmma_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/direct_store_epilogue_iterator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/direct_store_epilogue_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/interleaved_epilogue.h`

 * *Files 14% similar despite different names*

```diff
@@ -30,166 +30,126 @@
  **************************************************************************************************/
 /*! \file
   \brief Epilogue for threadblock scoped GEMMs using Tensor Ops.
 
   The epilogue rearranges the result of a matrix product through shared memory to match canonical
   tensor layouts in global memory. Epilogues support conversion and reduction operations.
 
-  The shared memory resource is time-sliced across warps.
 */
 
 #pragma once
 
-#if defined(__CUDACC_RTC__)
-#include <cuda/std/cassert>
-#else
-#include <assert.h>
-#endif
-
 #include "cutlass/cutlass.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/array.h"
 #include "cutlass/layout/vector.h"
 #include "cutlass/layout/tensor.h"
 #include "cutlass/tensor_coord.h"
 #include "cutlass/aligned_buffer.h"
-#include "cutlass/functional.h"
 
 #include "cutlass/gemm/gemm.h"
 
 #include "cutlass/transform/pitch_linear_thread_map.h"
 #include "cutlass/transform/threadblock/regular_tile_iterator.h"
 
-#include "cutlass/epilogue/threadblock/epilogue_base.h"
 #include "cutlass/epilogue/threadblock/epilogue_base_streamk.h"
 #include "cutlass/epilogue/threadblock/predicated_tile_iterator.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace threadblock {
 
-
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Epilogue operator
+/// Epilogue operator without splitk
 template <
-  typename Shape_,                          ///< Shape of threadblock tile (concept: GemmShape)
-  typename WarpMmaOperator_,                ///< Warp-level MMA operator (concept: gemm::warp::MmaTensorOp)
-  int PartitionsK,                          ///< Number of partitions of the K dimension
-  typename OutputTileIterator_,             ///< Tile iterator reading and writing output tensors
-  typename AccumulatorFragmentIterator_,    ///< Fragment iterator selecting accumulators
-  typename WarpTileIterator_,               ///< Warp-scoped tile iterator writing accumulators to SMEM
-  typename SharedLoadIterator_,             ///< Threadblock-scoped tile iterator loading from SMEM
-  typename OutputOp_,                       ///< Output operator
-  typename Padding_,                        ///< Padding added to SMEM allocation to avoid bank conflicts (concept: MatrixShape)
-  int FragmentsPerPartition = 1,            ///< Used to coarsten the epilogue granularity
-  int IterationsUnroll =                    ///< Used to reduce binary size when epilogue op is large
-    (!IsEpilogueFunctorHeavy<OutputOp_>::value)
->
-class Epilogue :
-  public EpilogueBase<
-    Shape_,
-    typename WarpMmaOperator_::Shape,
-    PartitionsK,
-    AccumulatorFragmentIterator_,
-    WarpTileIterator_,
-    Padding_,
-    FragmentsPerPartition>,
+    /// Shape of threadblock tile (concept: GemmShape)
+    typename Shape_,
+    /// Warp-level MMA operator (concept: gemm::warp::MmaTensorOp)
+    typename WarpMmaOperator_,
+    /// Number of partitions of the K dimension
+    int PartitionsK,
+    /// Tile iterator reading and writing output tensors
+    typename OutputTileIterator_,
+    /// Fragment iterator selecting accumulators
+    typename AccumulatorFragmentIterator_,
+    /// Output operator
+    typename OutputOp_,
+    /// Number of interleaved k
+    int InterleavedK>
+class InterleavedEpilogue :
   public EpilogueBaseStreamK<
     Shape_,
     PartitionsK,
     WarpMmaOperator_,
     AccumulatorFragmentIterator_>
 {
-
 public:
 
-  using Base = EpilogueBase<
-    Shape_,
-    typename WarpMmaOperator_::Shape,
-    PartitionsK,
-    AccumulatorFragmentIterator_,
-    WarpTileIterator_,
-    Padding_,
-    FragmentsPerPartition>;
-
   using BaseStreamK = EpilogueBaseStreamK<
     Shape_,
     PartitionsK,
     WarpMmaOperator_,
     AccumulatorFragmentIterator_>;
 
   using Shape = Shape_;
   using WarpMmaOperator = WarpMmaOperator_;
   static int const kPartitionsK = PartitionsK;
-  using OutputTileIterator = OutputTileIterator_;
   using AccumulatorFragmentIterator = AccumulatorFragmentIterator_;
-  using WarpTileIterator = WarpTileIterator_;
-  using SharedLoadIterator = SharedLoadIterator_;
+  using OutputTileIterator = OutputTileIterator_;
   using OutputOp = OutputOp_;
-  using Padding = Padding_;
-  using Layout = layout::RowMajor;
-  using LongIndex = typename Layout::LongIndex;
-
-  /// Number of warps per block
-  using WarpCount = typename Base::WarpCount;
 
-  /// Number of threads per block
-  static int const kBlockThreads = 32 * WarpCount::kCount;
-
-  /// Per-thread accumulator tile type
-  using AccumulatorTile = typename Base::AccumulatorTile;
-
-  /// Numerical accumulation element type
-  using ElementAccumulator = typename WarpMmaOperator::ElementC;
+  /// The complete warp-level accumulator tile
+  using AccumulatorTile = typename AccumulatorFragmentIterator::AccumulatorTile;
 
   /// Fragment type used by the accumulator tile's fragment iterator
   using AccumulatorFragment = typename AccumulatorFragmentIterator::Fragment;
 
+  /// Accumulator element
+  using ElementAccumulator = typename AccumulatorTile::Element;
+
   /// Output element
   using ElementOutput = typename OutputTileIterator::Element;
 
   /// Output access size
   static int const kElementsPerAccess = OutputTileIterator::kElementsPerAccess;
 
   /// Tensor reference to destination tensor
   using TensorRef = typename OutputTileIterator::TensorRef;
 
   /// Tensor reference to sync tensor
-  using SyncTensorRef = typename cutlass::TensorRef<int, cutlass::layout::PackedVectorLayout>;
+  using SyncTensorRef =
+      typename cutlass::TensorRef<int, cutlass::layout::PackedVectorLayout>;
 
   /// Const tensor reference to source tensor
   using ConstTensorRef = typename OutputTileIterator::ConstTensorRef;
 
-  /// Vector type used by the global output iterator
-  using OutputAccessType = Array<
-    typename OutputTileIterator::Element, OutputTileIterator::kElementsPerAccess>;
-
-  /// Vector type used by the shared output iterator
-  using AccumulatorAccessType = Array<typename WarpTileIterator::Element, OutputTileIterator::kElementsPerAccess>;
-
-  static int constexpr kSmemTiles = Base::kFragmentsPerIteration > 1 ? Base::kFragmentsPerIteration : kPartitionsK;
-
-  static int constexpr kSmemPointerOffset = Base::SharedStorage::StorageShape::kCount / kSmemTiles;
-
+  /// Array type used to output
+  using OutputAccessType = Array<typename OutputTileIterator::Element,
+                                 OutputTileIterator::kElementsPerAccess>;
+
+  /// Array type used by output functor
+  using AccumulatorAccessType =
+      Array<ElementAccumulator, OutputTileIterator::kElementsPerAccess>;
+
+  /// Number of warps
+  using WarpCount =
+      gemm::GemmShape<Shape::kM / WarpMmaOperator::Shape::kM,
+                      Shape::kN / WarpMmaOperator::Shape::kN, kPartitionsK>;
 
 public:
 
-  static_assert(SharedLoadIterator::Fragment::kElements == OutputTileIterator::Fragment::kElements,
-    "Mismatch between shared load iterator and output tile iterator.");
-
-  static_assert(OutputTileIterator::kElementsPerAccess, "OutputTileIterator::kElementsPerAccess must not be zero.");
-
-  static_assert(!(OutputTileIterator::Fragment::kElements % OutputTileIterator::kElementsPerAccess), 
-    "Divisibility");
-
-  static_assert(kPartitionsK == 1 || Base::kFragmentsPerIteration == 1, "One of these must be exactly 1.");
+  static_assert(OutputTileIterator::kElementsPerAccess,
+                "This must not be zero.");
 
+  static_assert(!(OutputTileIterator::Fragment::kElements %
+                  OutputTileIterator::kElementsPerAccess),
+                "Divisibility");
 
 public:
 
   /// Aspect for when epilogue source is not needed
   struct SourceAspectNotNeeded
   {
     /// Constructor
@@ -198,15 +158,15 @@
     {}
 
     /// Invoke the output functor over each vector of output
     CUTLASS_DEVICE
     void apply_output_operator(
       typename OutputTileIterator::Fragment &output_fragment,
       OutputOp const &output_op,
-      typename SharedLoadIterator::Fragment const &aligned_accum_fragment)
+      typename AccumulatorFragmentIterator::Fragment const &aligned_accum_fragment)
     {
       OutputAccessType *output_frag_ptr =
         reinterpret_cast<OutputAccessType *>(&output_fragment);
 
       AccumulatorAccessType const *compute_frag_ptr =
         reinterpret_cast<AccumulatorAccessType const *>(&aligned_accum_fragment);
 
@@ -231,15 +191,15 @@
     typename OutputTileIterator::Fragment source_fragment;
 
     /// Invoke the output functor over each vector of output
     CUTLASS_DEVICE
     static void apply_output_operator(
       typename OutputTileIterator::Fragment &output_fragment,
       OutputOp const &output_op,
-      typename SharedLoadIterator::Fragment const &aligned_accum_fragment,
+      typename AccumulatorFragmentIterator::Fragment const &aligned_accum_fragment,
       typename OutputTileIterator::Fragment const &source_fragment)
     {
       OutputAccessType *output_frag_ptr =
         reinterpret_cast<OutputAccessType *>(&output_fragment);
 
       AccumulatorAccessType const *compute_frag_ptr =
         reinterpret_cast<AccumulatorAccessType const *>(&aligned_accum_fragment);
@@ -267,51 +227,40 @@
     }
 
     /// Invoke the output functor over each vector of output
     CUTLASS_DEVICE
     void apply_output_operator(
       typename OutputTileIterator::Fragment &output_fragment,
       OutputOp const &output_op,
-      typename SharedLoadIterator::Fragment const &aligned_accum_fragment)
+      typename AccumulatorFragmentIterator::Fragment const &aligned_accum_fragment)
     {
       // Load addend source fragment from global memory
       source_iterator.load(source_fragment);
       ++source_iterator;
 
       apply_output_operator(output_fragment, output_op, aligned_accum_fragment, source_fragment);
     }
   };
 
 
-private:
+  /// Shared storage allocation needed by the epilogue
+  struct SharedStorage {};
 
-  /// Loads fragment from shared memory aligned with output tensor
-  SharedLoadIterator shared_load_iterator_;
-
-  /// Thread index in the threadblock
-  int thread_idx;
-
-  /// Warp index in the threadblock
-  int warp_idx;
 
 public:
 
   /// Constructor
   CUTLASS_DEVICE
-  Epilogue(
-      typename Base::SharedStorage &shared_storage,   ///< Shared storage object
-      int thread_idx,                                 ///< ID of a thread within the threadblock
-      int warp_idx,                                   ///< ID of warp within threadblock
-      int lane_idx)                                   ///< Id of thread within warp
+  InterleavedEpilogue(
+      SharedStorage &shared_storage,  ///< Shared storage object
+      int thread_idx,                 ///< ID of a thread within the threadblock
+      int warp_idx,                   ///< ID of warp within threadblock
+      int lane_idx)                   ///< Id of thread within warp
   :
-      Base(shared_storage, thread_idx, warp_idx, lane_idx),
-      BaseStreamK(thread_idx),
-      shared_load_iterator_(shared_storage.reference(), thread_idx),
-      thread_idx(thread_idx),
-      warp_idx(warp_idx)
+      BaseStreamK(thread_idx)
   {}
 
 
   /// Aggregates the accumulator sets shared by peer blocks in the global workspace,
   /// performing epilogue computations, writing to output
   CUTLASS_DEVICE
   void reduce(
@@ -323,56 +272,30 @@
       OutputTileIterator destination_iterator,        ///< Tile iterator for destination
       OutputTileIterator source_iterator)             ///< Threadblock tile coordinate in GEMM (in units of threadblock tiles)
   {
     // Redcuce peer accumulator fragments into one fragment
     AccumulatorFragment accum_fragment;
     BaseStreamK::reduce(accum_fragment, peer_idx_begin, peer_idx_end, reduce_fragment_idx, element_workspace);
 
-    // Store fragment to shared memory
-    this->warp_tile_iterator_.store(accum_fragment);
-
-    __syncthreads();
-
-    // Initialize/load source-fragment data
+    // Source-fragment data (zero-initialized for scenarios where the
+    // output operator allows us to skip loading it from global input)
     typename OutputTileIterator::Fragment source_fragment;
     source_fragment.clear();
 
     if (output_op.is_source_needed())
     {
       source_iterator += reduce_fragment_idx;
       source_iterator.load(source_fragment);
     }
 
-    // Load fragment from shared memory
-    typename SharedLoadIterator::Fragment aligned_accum_fragment;
-    shared_load_iterator_.load(aligned_accum_fragment);
-
-    // Add fragments shared by other k partitions
-    if (kPartitionsK > 1)
-    {
-      plus <typename SharedLoadIterator::Fragment> add_fragments;
-
-      CUTLASS_PRAGMA_UNROLL
-      for ( int i = 1; i < kPartitionsK; ++i) {
-        typename SharedLoadIterator::Fragment aligned_addend_fragment;
-        shared_load_iterator_.add_pointer_offset(kSmemPointerOffset);
-        shared_load_iterator_.load(aligned_addend_fragment);
-        aligned_accum_fragment = add_fragments(aligned_accum_fragment, aligned_addend_fragment);
-      }
-    }
-
     // Compute the output result
     typename OutputTileIterator::Fragment output_fragment;
 
     // Apply the output operator
-    SourceAspectNeeded::apply_output_operator(
-        output_fragment,
-        output_op,
-        aligned_accum_fragment,
-        source_fragment);
+    SourceAspectNeeded::apply_output_operator(output_fragment, output_op, accum_fragment, source_fragment);
 
     // Store the final result
     destination_iterator += reduce_fragment_idx;
     destination_iterator.store(output_fragment);
   }
 
 
@@ -431,103 +354,52 @@
   CUTLASS_DEVICE
   void operator()(
     OutputOp const &output_op,                      ///< Output operator
     OutputTileIterator destination_iterator,        ///< Tile iterator for destination
     AccumulatorTile const &accumulators,            ///< Complete warp-level accumulator tile
     SourceAspect source)
   {
+    //
     // Iterator over warp-level accumulator fragment
+    //
+
     AccumulatorFragmentIterator accum_fragment_iterator(accumulators);
 
     //
     // Iterate over accumulator tile
     //
 
-    #pragma unroll(IterationsUnroll ? OutputTileIterator::kIterations / Base::kFragmentsPerIteration : 1)
-    for (int iter = 0; iter < OutputTileIterator::kIterations; iter += Base::kFragmentsPerIteration)
-    {
+    CUTLASS_PRAGMA_UNROLL
+    for (int iter = 0; iter < OutputTileIterator::kIterations; ++iter) {
 
       //
-      // Convert and store fragment
+      // Convert fragment
       //
 
-      __syncthreads();
-
-      CUTLASS_PRAGMA_UNROLL
-      for (int p = 0; p < Base::kFragmentsPerIteration; ++p)
-      {
-        typename AccumulatorFragmentIterator::Fragment accum_fragment;
-
-        accum_fragment_iterator.load(accum_fragment);
-        ++accum_fragment_iterator;
-
-        this->warp_tile_iterator_.store(accum_fragment);
-
-        if (p < Base::kFragmentsPerIteration - 1) {
-          this->warp_tile_iterator_.add_pointer_offset(kSmemPointerOffset);
-        }
-      }
-
-      if (Base::kFragmentsPerIteration > 1) {
-        this->warp_tile_iterator_.add_pointer_offset(kSmemPointerOffset * (1 - Base::kFragmentsPerIteration));
-      }
+      typename AccumulatorFragmentIterator::Fragment accum_fragment;
 
+      accum_fragment_iterator.load(accum_fragment);
+      ++accum_fragment_iterator;
 
       //
-      // Load fragments from shared memory
+      // Compute the output result
       //
 
-      __syncthreads();
-
-      CUTLASS_PRAGMA_UNROLL
-      for (int p = 0; p < Base::kFragmentsPerIteration; ++p)
-      {
-        typename SharedLoadIterator::Fragment aligned_accum_fragment;
-        shared_load_iterator_.load(aligned_accum_fragment);
+      typename OutputTileIterator::Fragment output_fragment;
+      source.apply_output_operator(output_fragment, output_op, accum_fragment);
 
-        if (p < Base::kFragmentsPerIteration - 1)
-        {
-          shared_load_iterator_.add_pointer_offset(kSmemPointerOffset);
-        }
-        else if (kPartitionsK > 1)
-        {
-          plus <typename SharedLoadIterator::Fragment> add_fragments;
-
-          CUTLASS_PRAGMA_UNROLL
-          for ( int i = 1; i < kPartitionsK; ++i) {
-            typename SharedLoadIterator::Fragment aligned_accum_fragment_addend;
-            shared_load_iterator_.add_pointer_offset(kSmemPointerOffset);
-            shared_load_iterator_.load(aligned_accum_fragment_addend);
-            aligned_accum_fragment = add_fragments(aligned_accum_fragment, aligned_accum_fragment_addend);
-          }
-
-          shared_load_iterator_.add_pointer_offset((1 - kPartitionsK) * kSmemPointerOffset);
-        }
-
-        //
-        // Compute the output result
-        //
-
-        typename OutputTileIterator::Fragment output_fragment;
-        source.apply_output_operator(output_fragment, output_op, aligned_accum_fragment);
-
-        //
-        // Store the final result
-        //
-
-        destination_iterator.store(output_fragment);
-        ++destination_iterator;
-      }
+      //
+      // Store the final result
+      //
 
-      if (Base::kFragmentsPerIteration > 1) {
-        shared_load_iterator_.add_pointer_offset(kSmemPointerOffset * (1 - Base::kFragmentsPerIteration));
-      }
+      destination_iterator.set_iteration_index(iter);
+      destination_iterator.store(output_fragment);
+      ++destination_iterator;
     }
   }
-
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
 } // namespace threadblock
 } // namespace epilogue
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_base.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_singlestage.h`

 * *Files 20% similar despite different names*

```diff
@@ -25,216 +25,241 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-  \brief Epilogue for threadblock scoped GEMMs using Tensor Ops.
-
-  The epilogue rearranges the result of a matrix product through shared memory to match canonical
-  tensor layouts in global memory. Epilogues support conversion and reduction operations.
-
+    \brief Template for a double-buffered threadblock-scoped GEMM kernel.
 */
 
 #pragma once
 
-#if !defined(__CUDACC_RTC__)
-#include <type_traits>
-#include <utility>
-#endif
-
-#if defined(__CUDACC_RTC__)
-#include <cuda/std/cassert>
-#else
-#include <assert.h>
-#endif
-
 #include "cutlass/cutlass.h"
-#include "cutlass/matrix_shape.h"
-#include "cutlass/numeric_types.h"
 #include "cutlass/array.h"
-#include "cutlass/layout/vector.h"
-#include "cutlass/layout/tensor.h"
-#include "cutlass/tensor_coord.h"
 #include "cutlass/aligned_buffer.h"
 
+#include "cutlass/numeric_types.h"
+#include "cutlass/matrix_shape.h"
+
 #include "cutlass/gemm/gemm.h"
+#include "cutlass/gemm/threadblock/mma_base.h"
 
-#include "cutlass/transform/pitch_linear_thread_map.h"
 
-////////////////////////////////////////////////////////////////////////////////
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
-namespace epilogue {
+namespace gemm {
 namespace threadblock {
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-//
-// This is used for metaprogramming epilogue functors. If they define 
-// `static bool const kIsHeavy = true;`, then the epilogue functor itself is
-// not inlined. This results in smaller code and is advantageous if the epilogue
-// functor consists of many instructions.
-//
-// If the epilogue functor does not define `kIsHeavy` or if it is `false`, then
-// the behavior from CUTLASS 2.5 and before is retained. The epilogue is fully
-// unrolled and inlined.
-//
+/// Structure to compute the matrix product targeting CUDA cores and SIMT math instructions.
+template <
+  /// Size of the Gemm problem - concept: gemm::GemmShape<>
+  typename Shape_,
+  /// Iterates over tiles of A operand in global memory 
+  //  (concept: ReadableTileIterator | ForwardTileIterator | MaskedTileIterator)
+  typename IteratorA_,
+  /// Iterates over tiles of A operand in shared memory
+  /// (concept: WriteableTileIterator | RandomAccessTileIterator)
+  typename SmemIteratorA_,
+  /// Iterates over tiles of B operand in global memory
+  //  (concept: ReadableTileIterator | ForwardTileIterator | MaskedTileIterator)
+  typename IteratorB_,
+  /// Iterates over tiles of B operand in shared memory
+  /// (concept: WriteableTileIterator | RandomAccessTileIterator)
+  typename SmemIteratorB_,
+  /// Data type of accumulator matrix
+  typename ElementC_,
+  /// Data type of accumulator matrix
+  typename LayoutC_,
+  /// Policy describing tuning details (concept: MmaPolicy)
+  typename Policy_,
+  /// Used for partial specialization
+  typename Enable = bool
+>
+class MmaSingleStage : public MmaBase<Shape_, Policy_, 1> {
+public:
 
-template<class> 
-struct TypeSink {  typedef void type; };
+  ///< Base class
+  using Base = MmaBase<Shape_, Policy_, 1>;
 
-template<class T> using TypeSinkT = typename TypeSink<T>::type;
+  using Shape = Shape_;             ///< Size of the Gemm problem - concept: gemm::GemmShape<>
+  using IteratorA = IteratorA_;     ///< Iterates over tiles of A operand in global memory
+  using IteratorB = IteratorB_;     ///< Iterates over tiles of B operand in global memory
+  using ElementC = ElementC_;       ///< Data type of accumulator matrix
+  using LayoutC = LayoutC_;         ///< Layout of accumulator matrix
+  using Policy = Policy_;           ///< Policy describing tuning details
 
-template<class T, class=void> struct IsEpilogueFunctorHeavy {
-  static bool const value = false;
-};
+  using SmemIteratorA = SmemIteratorA_;
+  using SmemIteratorB = SmemIteratorB_;
 
-template<class T> struct IsEpilogueFunctorHeavy<T, TypeSinkT< decltype( T::kIsHeavy ) > > {
-  static bool const value = T::kIsHeavy;
-};
+  //
+  // Dependent types
+  //
 
-////////////////////////////////////////////////////////////////////////////////
+  /// Fragment of operand A loaded from global memory
+  using FragmentA = typename IteratorA::Fragment;
 
-/// Base class for epilogues defining warp-level 
-template <
-  typename Shape_,                          ///< Shape of threadblock tile (concept: GemmShape)
-  typename WarpShape_,                      ///< Warp-level MMA operator (concept: gemm::warp::MmaTensorOp)
-  int PartitionsK,                          ///< Number of partitions of the K dimension
-  typename AccumulatorFragmentIterator_,    ///< Fragment iterator selecting accumulators
-  typename WarpTileIterator_,               ///< Warp-scoped tile iterator writing accumulators to SMEM
-  typename Padding_,                        ///< Padding added to SMEM allocation to avoid bank conflicts (concept: MatrixShape)
-  int FragmentsPerIteration = 1
->
-class EpilogueBase {
-public:
+  /// Fragment of operand B loaded from global memory
+  using FragmentB = typename IteratorB::Fragment;
+
+  /// Fragment of accumulator tile
+  using FragmentC = typename Policy::Operator::FragmentC;
+
+  /// Warp-level Mma
+  using Operator = typename Policy::Operator;
+
+  using ArchTag = arch::Sm70;
+
+  /// Complex transform on A operand
+  static ComplexTransform const kTransformA = Operator::kTransformA;
+
+  /// Complex transform on B operand
+  static ComplexTransform const kTransformB = Operator::kTransformB;
 
-  using Shape = Shape_;
-  using WarpShape = WarpShape_;
-  static int const kPartitionsK = PartitionsK;
-  using AccumulatorFragmentIterator = AccumulatorFragmentIterator_;
-  using WarpTileIterator = WarpTileIterator_;
-  using Padding = Padding_;
-
-  /// Output layout is always row-major
-  using Layout = layout::RowMajor;
-
-  /// The complete warp-level accumulator tile
-  using AccumulatorTile = typename AccumulatorFragmentIterator::AccumulatorTile;
-
-  /// Accumulator element
-  using ElementAccumulator = typename AccumulatorTile::Element;
-
-  /// Number of warps
-  using WarpCount = gemm::GemmShape<
-    Shape::kM / WarpShape::kM,
-    Shape::kN / WarpShape::kN,
-    kPartitionsK
-  >;
+  // staticaly assert kStages for MmaSingleStage is 1 (single stage mma pipeline)
+  static_assert((Base::kStages==1), "MmaSingleStage requires kStages set to value 1");
+private:
 
-  /// Use this to control the granularity of one epilogue 'iteration'
-  static int const kFragmentsPerIteration = FragmentsPerIteration;
+  using WarpFragmentA = typename Operator::FragmentA;
+  using WarpFragmentB = typename Operator::FragmentB;
+
+protected:
+
+  /// Iterator to write threadblock-scoped tile of A operand to shared memory
+  SmemIteratorA smem_iterator_A_;
+
+  /// Iterator to write threadblock-scoped tile of B operand to shared memory
+  SmemIteratorB smem_iterator_B_;
 
 public:
 
-  /// Shared storage allocation needed by the epilogue
-  struct SharedStorage {
-    
-    //
-    // Type definitions
-    //
+  /// Construct from tensor references
+  CUTLASS_DEVICE
+  MmaSingleStage(
+    typename Base::SharedStorage &shared_storage,       ///< Shared storage needed for internal use by threadblock-scoped GEMM
+    int thread_idx,                                     ///< ID within the threadblock
+    int warp_idx,                                       ///< ID of warp
+    int lane_idx                                        ///< ID of each thread within a warp
+  ):
+    Base(shared_storage, thread_idx, warp_idx, lane_idx),
+    smem_iterator_A_(shared_storage.operand_A_ref(), thread_idx),
+    smem_iterator_B_(shared_storage.operand_B_ref(), thread_idx) {
 
-    /// Element type of shared memory
-    using Element = typename WarpTileIterator::Element;
+    // Compute warp location within threadblock tile by mapping the warp_id to
+    // three coordinates:
+    //   _m: the warp's position within the threadblock along the M dimension
+    //   _n: the warp's position within the threadblock along the N dimension
+    //   _k: the warp's position within the threadblock along the K dimension
+
+    int warp_idx_mn = warp_idx % (Base::WarpCount::kM * Base::WarpCount::kN);
+    int warp_idx_k = warp_idx / (Base::WarpCount::kM * Base::WarpCount::kN);
 
-    /// Tensor reference to shared memory allocation
-    using TensorRef = typename WarpTileIterator::TensorRef;
+    int warp_idx_m = warp_idx_mn % Base::WarpCount::kM;
+    int warp_idx_n = warp_idx_mn / Base::WarpCount::kM;
 
-    /// Layout of shared memory allocation
-    using Layout = typename WarpTileIterator::Layout;
-    
-    /// Logical shape of the shared memory tile written to by all warps.
-    using Shape = MatrixShape<
-      WarpCount::kM * WarpTileIterator::Shape::kRow * WarpCount::kK,
-      WarpCount::kN * WarpTileIterator::Shape::kColumn
-    >;
-
-    /// Shape of the shared memory allocation for the epilogue    
-    using StorageShape = MatrixShape<
-      (Shape::kRow + Padding::kRow) * kFragmentsPerIteration, 
-      Shape::kColumn + Padding::kColumn
-    >;
+    // Add per-warp offsets in units of warp-level tiles
+    this->warp_tile_iterator_A_.add_tile_offset({warp_idx_m, Base::kWarpGemmIterations * warp_idx_k});
+    this->warp_tile_iterator_B_.add_tile_offset({Base::kWarpGemmIterations * warp_idx_k, warp_idx_n});
+
+  }
+
+  /// Perform a threadblock-scoped matrix multiply-accumulate
+  CUTLASS_DEVICE
+  void operator()(
+    int gemm_k_iterations,            ///< number of iterations of the mainloop
+    FragmentC &accum,                 ///< destination accumulator tile
+    IteratorA iterator_A,             ///< iterator over A operand in global memory
+    IteratorB iterator_B,             ///< iterator over B operand in global memory
+    FragmentC const &src_accum) {     ///< source accumualtor tile
 
     //
-    // Data members
+    // Prologue
     //
 
-    AlignedBuffer<Element, StorageShape::kCount> storage;
+    // Perform accumulation in the 'd' output operand
+    accum = src_accum;
+
+    FragmentA tb_frag_A;
+    FragmentB tb_frag_B;
+
+    tb_frag_A.clear();
+    tb_frag_B.clear();
+
+    // The last kblock is loaded in the prolog
+    iterator_A.load(tb_frag_A);
+    iterator_B.load(tb_frag_B);
+
+    ++iterator_A;
+    ++iterator_B;
+
+    // Pair of fragments used to overlap shared memory loads and math instructions
+    WarpFragmentA warp_frag_A;
+    WarpFragmentB warp_frag_B;
+
+    Operator warp_mma;
+
+    // Avoid reading out of bounds
+    iterator_A.clear_mask(gemm_k_iterations <= 1);
+    iterator_B.clear_mask(gemm_k_iterations <= 1);
 
     //
-    // Methods
+    // Mainloop
     //
 
-    /// Returns a pointer to the shared memory buffer
-    CUTLASS_DEVICE
-    Element *data() {
-      return storage.data();
-    }
+    CUTLASS_GEMM_LOOP
+    for (; gemm_k_iterations > 0; --gemm_k_iterations) {
+      this->smem_iterator_A_.store(tb_frag_A);
+      this->smem_iterator_B_.store(tb_frag_B);
 
-    /// Returns a tensor reference to the shared memory buffer
-    CUTLASS_DEVICE
-    TensorRef reference() {
-      return TensorRef(
-        storage.data(), 
-        Layout::packed({StorageShape::kRow, StorageShape::kColumn}));
-    }
-  };
+      __syncthreads();
 
-protected:
+      //
+      // Loop over GEMM K dimension
+      //
 
-  //
-  // Data members
-  //
+      CUTLASS_PRAGMA_UNROLL
+      for (int warp_mma_k = 0; warp_mma_k < Base::kWarpGemmIterations; ++warp_mma_k) {
 
-  SharedStorage &shared_storage_;
+        // Load warp-level tiles from shared memory, wrapping to k offset if this is the last group
+        // as the case may be.
+        
+        this->warp_tile_iterator_A_.set_kgroup_index(warp_mma_k % Base::kWarpGemmIterations);
+        this->warp_tile_iterator_B_.set_kgroup_index(warp_mma_k % Base::kWarpGemmIterations);
 
-  /// Stores a warp's fragment of accumulators to SMEM
-  WarpTileIterator warp_tile_iterator_;
+        this->warp_tile_iterator_A_.load(warp_frag_A);
+        this->warp_tile_iterator_B_.load(warp_frag_B);
 
-public:
+        ++this->warp_tile_iterator_A_;
+        ++this->warp_tile_iterator_B_;
 
-  /// Constructor
-  CUTLASS_DEVICE
-  EpilogueBase(
-    SharedStorage &shared_storage,    ///< Shared storage object    
-    int thread_idx,                   ///< ID of a thread within the threadblock
-    int warp_idx,                     ///< ID of warp within threadblock
-    int lane_idx                      ///< Id of thread within warp
-  ):
-    shared_storage_(shared_storage),
-    warp_tile_iterator_(shared_storage.reference(), lane_idx) {
+        warp_mma(accum, warp_frag_A, warp_frag_B, accum);
+      }
 
-    // Compute warp location within threadblock tile by mapping the warp_id to three coordinates:
-    //
-    //   _m: the warp's position within the threadblock along the M dimension
-    //   _n: the warp's position within the threadblock along the N dimension
-    //   _k: the warp's position within the threadblock along the K dimension
+      // Add negative offsets to return smem load iterators to the 'start' of the shared memory
+      this->warp_tile_iterator_A_.add_tile_offset({0, -Policy::kPartitionsK * Base::kWarpGemmIterations});
+      this->warp_tile_iterator_B_.add_tile_offset({-Policy::kPartitionsK * Base::kWarpGemmIterations, 0});
+
+      __syncthreads();
 
-    int warp_k = warp_idx / (WarpCount::kM * WarpCount::kN);
-    int warp_mn = warp_idx % (WarpCount::kM * WarpCount::kN);
-    int warp_m = warp_mn % WarpCount::kM;
-    int warp_n = warp_mn / WarpCount::kM;
+      iterator_A.load(tb_frag_A);
+      iterator_B.load(tb_frag_B);
 
-    MatrixCoord warp_offset{warp_k * WarpCount::kM + warp_m, warp_n};
+      ++iterator_A;
+      ++iterator_B;
+
+      // Avoid reading out of bounds if this was the last loop iteration
+      iterator_A.clear_mask(gemm_k_iterations <= 2);
+      iterator_B.clear_mask(gemm_k_iterations <= 2);
+    }
 
-    warp_tile_iterator_.add_tile_offset(warp_offset);
   }
 };
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace threadblock
-} // namespace epilogue
+} // namespace gemm
 } // namespace cutlass
-
-////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_base_streamk.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_unary.h`

 * *Files 25% similar despite different names*

```diff
@@ -10,188 +10,217 @@
  *
  * 2. Redistributions in binary form must reproduce the above copyright notice,
  * this list of conditions and the following disclaimer in the documentation
  * and/or other materials provided with the distribution.
  *
  * 3. Neither the name of the copyright holder nor the names of its
  * contributors may be used to endorse or promote products derived from
- * this software without specific prior written permission.
+ * this layernormware without specific prior written permission.
  *
  * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
  * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
  * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
  * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
+
 /*! \file
-  \brief Basic subset of epilogue functionality for supporting StreamK decompositions
-*/
 
+  \brief Epilogue visitor operator performing a unary operation atop a visitor node
+*/
 
 #pragma once
-
 #include "cutlass/cutlass.h"
-#include "cutlass/functional.h"
-#include "cutlass/block_striped.h"
+#include "unary_ops.h"
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace threadblock {
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 
-/// StreamK epilogue functionality for cross-block accumulator fragment reduction
+/// Epilogue Visitor operator for the following computation:
+///
+///  ElementCompute alpha;
+///  ElementCompute beta;
+///  ElementCompute C = UnaryOp(ElementCompute(Visitor)) 
+///  Return C;
+///
 template <
-  typename Shape,                          ///< Shape of threadblock tile (concept: GemmShape)
-  int PartitionsK,
-  typename WarpMmaOperator,                ///< Warp-level MMA operator (concept: gemm::warp::MmaTensorOp)
-  typename AccumulatorFragmentIterator>    ///< Iterator for enumerating fragments within the per-thread tile of raw accumulators
-class EpilogueBaseStreamK
-{
-
-protected:
-
-  /// The per-thread tile of raw accumulators
-  using AccumulatorTile = typename AccumulatorFragmentIterator::AccumulatorTile;
-
-  /// Number of warps
-  using WarpCount = gemm::GemmShape<
-                        Shape::kM / WarpMmaOperator::Shape::kM,
-                        Shape::kN / WarpMmaOperator::Shape::kN,
-                        PartitionsK>;
-
-  /// Number of threads per block
-  static int const kBlockThreads = 32 * WarpCount::kCount;
-
-  /// Numerical accumulation element type
-  using ElementAccumulator = typename WarpMmaOperator::ElementC;
-
-  /// Fragment type used by the accumulator tile's fragment iterator
-  using AccumulatorFragment = typename AccumulatorFragmentIterator::Fragment;
-
+    typename ElementAccumulator_,  ///< Data type of the Accumulator
+    typename ElementCompute_,      ///< Data type used to compute linear combination
+    int      kElementsPerAccess_,  ///< Number of elements computed per operation
+    typename Visitor_,              ///< Child node
+    template<typename T, int N> typename UnaryOp_
+>
+class VisitorOpUnary{
 public:
+    using ElementAccumulator = ElementAccumulator_;
+    using ElementCompute = ElementCompute_;
+    static int const kElementsPerAccess = kElementsPerAccess_;
+
+    using Visitor = Visitor_;
+
+    /// Fragment type returned from Visitor.visit
+    using VisitAccessTypeVisitor = typename Visitor::VisitAccessType;
+    using ElementVisit = typename VisitAccessTypeVisitor::Element;
+
+    /// Fragment type returned by this visitor
+    using VisitAccessType = Array<ElementCompute, kElementsPerAccess>; 
+
+    /// Fragment type of accumulator
+    using AccumulatorAccessType = Array<ElementAccumulator, kElementsPerAccess>;
+
+    /// Combination Op TODO: generalize this
+    using UnaryOp = UnaryOp_<ElementCompute, kElementsPerAccess>;
+
+    static_assert(kElementsPerAccess==VisitAccessTypeVisitor::kElements, "kElementsPerAccess mismatches with Visitor");
+
+    /// SMEM buffer class required in the epilogue visitor
+    struct SharedStorage {
+        typename Visitor::SharedStorage storage_visitor;
+
+        CUTLASS_HOST_DEVICE
+        SharedStorage() {}
+    };
+
+
+    /// Host-constructable Arguments structure
+    struct Arguments {
+        typename UnaryOp::Arguments unary_arg;
+        typename Visitor::Arguments visitor_arg;    ///< Argument type for visitor
+
+        //
+        // Methods
+        //
+        CUTLASS_HOST_DEVICE
+        Arguments():unary_arg() { }
+        
+        CUTLASS_HOST_DEVICE
+        Arguments(
+            typename UnaryOp::Arguments unary_arg,
+            typename Visitor::Arguments visitor_arg
+        ):
+            unary_arg(unary_arg),
+            visitor_arg(visitor_arg)
+        { }
+    };
+
+    /// Parameter structure
+    struct Params {
+        typename UnaryOp::Params unary_param;
+        typename Visitor::Params visitor_param;    ///< Argument type for visitor
+
+        //
+        // Methods
+        //
+        CUTLASS_HOST_DEVICE
+        Params():unary_param() { }
+        
+        CUTLASS_HOST_DEVICE
+        Params(Arguments const &args):
+            unary_param(args.unary_arg),
+            visitor_param(args.visitor_arg)
+        { }
+    };
+
+private:
+    //
+    // Data members
+    //
+    UnaryOp unary_op;
 
-  /// Number of AccumulatorTile fragments per thread
-  static int const kAccumulatorFragments = AccumulatorFragmentIterator::Policy::kIterations;
-
-protected:
+    Visitor visitor_op;
 
-  /// Number of AccumulatorTile fragments per block output tile
-  static int const kOutputTileFragments = kBlockThreads * kAccumulatorFragments;
+public:
 
-  /// Block-striped transfer utility for sharing AccumulatorFragment
-  using BlockStripedT = BlockStriped<kBlockThreads, AccumulatorFragment>;
+    /// Constructs the function object
+    CUTLASS_HOST_DEVICE
+    VisitorOpUnary(
+        Params const &params,
+        SharedStorage &shared_storage,
+        int thread_idx,
+        MatrixCoord threadblock_offset,
+        MatrixCoord problem_size
+    ):
+        unary_op(params.unary_param),
+        visitor_op(params.visitor_param, shared_storage.storage_visitor, thread_idx, threadblock_offset, problem_size)
+    { }
+
+    CUTLASS_DEVICE
+    void set_batch_index(int batch_idx) {
+        visitor_op.set_batch_index(batch_idx);
+    }
 
-  /// AccumulatorFragment stride in the shared workspace between different peer blocks (each thread block can share accumulators for up to two block output tiles)
-  static const int kPeerFragmentStride = kOutputTileFragments * 2;
+    CUTLASS_DEVICE
+    void begin_epilogue() {
+        if (unary_op.guard()) visitor_op.begin_epilogue();
+    }
 
-public:
+    CUTLASS_DEVICE
+    void begin_step(int step_idx) {
+        if (unary_op.guard()) visitor_op.begin_step(step_idx);
+    }
 
-  /// Workspace bytes per thread block
-  static size_t const kWorkspaceBytesPerBlock =sizeof(AccumulatorFragment) * kPeerFragmentStride;
+    CUTLASS_DEVICE
+    void begin_row(int row_idx) {
+        if (unary_op.guard()) visitor_op.begin_row(row_idx);
+    }
 
-public:
+    CUTLASS_DEVICE
+    VisitAccessType visit(
+        int iter_idx,
+        int row_idx,
+        int column_idx,
+        int frag_idx,
+        AccumulatorAccessType const &accum
+    ) { 
+        /// Get result from visitor A and visitor B
+        VisitAccessTypeVisitor result;
+
+        if (unary_op.guard()){
+            result = visitor_op.visit(iter_idx, row_idx, column_idx, frag_idx, accum);
+        } else {
+            result.clear();
+        }
 
-  /// Thread index in the threadblock
-  int thread_idx;
+        /// Type conversion
+        NumericArrayConverter<ElementCompute, ElementVisit, kElementsPerAccess> source_converter;
 
-public:
+        cutlass::multiplies<VisitAccessType> multiply_op;
 
-  /// Constructor
-  CUTLASS_DEVICE
-  EpilogueBaseStreamK(
-      int thread_idx)                                       ///< ID of a thread within the threadblock
-  :
-      thread_idx(thread_idx)
-  {}
-
-
-  /// Aggregates the accumulator sets shared by peer blocks in the global workspace
-  CUTLASS_DEVICE
-  void reduce(
-      AccumulatorFragment &accum_fragment,                  ///< [out] sum of all shared accumulator fragments for these peer partials
-      int peer_idx_begin,
-      int peer_idx_end,
-      int reduce_fragment_idx,
-      void *workspace_ptr)
-  {
-    plus<AccumulatorFragment> add_fragments;
-
-    AccumulatorFragment *fragment_workspace = reinterpret_cast<AccumulatorFragment *>(workspace_ptr);
-
-    int fragment_offset = (peer_idx_begin * kPeerFragmentStride) + (reduce_fragment_idx * kBlockThreads);
-
-    // Load first peer fragment
-    BlockStripedT::load(accum_fragment, fragment_workspace + fragment_offset, this->thread_idx);
-
-    fragment_offset += kPeerFragmentStride;         // Move to next peer
-    fragment_offset += kOutputTileFragments;        // Move to the set of fragments for this peer's "non-started" output tile
-
-    // Reduce fragments from additional peers
-    #pragma unroll 2
-    for (; fragment_offset < peer_idx_end * kPeerFragmentStride; fragment_offset += kPeerFragmentStride)
-    {
-      // Load peer fragment
-      AccumulatorFragment addend_fragment;
-      BlockStripedT::load(addend_fragment, fragment_workspace + fragment_offset, this->thread_idx);
-
-      // Add peer fragment
-      accum_fragment = add_fragments(accum_fragment, addend_fragment);
-    }
-  }
-
-
-  /// Shares the accumulator set with peers in the global workspace
-  CUTLASS_DEVICE
-  void share(
-      int peer_idx,
-      void *workspace_ptr,
-      AccumulatorTile const &accumulators,
-      bool started_tile)                      ///< Whether this thread block computed the first work volume for the current output tile
-  {
-    AccumulatorFragment *fragment_workspace = reinterpret_cast<AccumulatorFragment *>(workspace_ptr);
-
-    int fragment_offset = peer_idx * kPeerFragmentStride;
-
-    if (!started_tile) {
-      // Move to the set of fragments for the "non-started" output tile
-      fragment_offset += kOutputTileFragments;
-    }
-
-    AccumulatorFragmentIterator accum_fragment_iterator(accumulators);
-
-    // Convert raw accumulator tile to fragments and store
-    CUTLASS_PRAGMA_UNROLL
-    for (int iter = 0; iter < kAccumulatorFragments; ++iter)
-    {
-      // Acquire reordered accumulator fragment
-      AccumulatorFragment accum_fragment;
-      accum_fragment_iterator.load(accum_fragment);
-      ++accum_fragment_iterator;
+        return unary_op(source_converter(result));
+    }
 
-      // Store accumulator fragment
-      BlockStripedT::store(fragment_workspace + fragment_offset, accum_fragment, this->thread_idx);
+    CUTLASS_DEVICE
+    void end_row(int row_idx) {
+        if (unary_op.guard()) visitor_op.end_row(row_idx);
+    }
 
-      fragment_offset += kBlockThreads;
+    CUTLASS_DEVICE
+    void end_step(int step_idx) {
+        if (unary_op.guard()) visitor_op.end_step(step_idx);
     }
-  }
 
+    CUTLASS_DEVICE
+    void end_epilogue() {
+        if (unary_op.guard()) visitor_op.end_epilogue();
+    }
 };
 
 
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-////////////////////////////////////////////////////////////////////////////////
-
-} // namespace threadblock
-} // namespace epilogue
+} // namespace kernel
+} // namespace gemm
 } // namespace cutlass
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_depthwise.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_depthwise.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_direct_store.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_direct_store.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_gemm_k_reduction.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_gemm_k_reduction.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_planar_complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h`

 * *Files 19% similar despite different names*

```diff
@@ -25,377 +25,385 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-  \brief Epilogue for threadblock scoped GEMMs using Tensor Ops.
-
-  The epilogue rearranges the result of a matrix product through shared memory to match canonical
-  tensor layouts in global memory. Epilogues support conversion and reduction operations.
-
+    \brief Generic epilogue for implementing certain kinds of fused epilogue behavior.
 */
 
 #pragma once
 
-#include "cutlass/cutlass.h"
-#include "cutlass/numeric_types.h"
-#include "cutlass/array.h"
-#include "cutlass/array_planar_complex.h"
-#include "cutlass/layout/vector.h"
-#include "cutlass/layout/tensor.h"
-#include "cutlass/tensor_coord.h"
-#include "cutlass/aligned_buffer.h"
-#include "cutlass/functional.h"
-
-#include "cutlass/gemm/gemm.h"
-
-#include "cutlass/transform/pitch_linear_thread_map.h"
-#include "cutlass/transform/threadblock/regular_tile_iterator.h"
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
+#include "cutlass/cutlass.h"
+#include "cutlass/fast_math.h"
+#include "cutlass/matrix_coord.h"
+#include "cutlass/semaphore.h"
 #include "cutlass/epilogue/threadblock/epilogue_base.h"
-#include "cutlass/epilogue/threadblock/predicated_tile_iterator.h"
 
-////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace threadblock {
 
-////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////////////////////////
+
+class EpilogueFusedVisitorConcept {
+public:
+
+  static int const kIterations = 1;
+  static int const kElementsPerAccess = 4;
+  using ElementOutput = float;
+  using ElementAccumulator = float;
+  using AccumulatorFragment = Array<ElementAccumulator, kElementsPerAccess>;
+
+  /// Arguments structure
+  struct Arguments {  };
+
+  /// Params structure
+  struct Params {
+
+    Params() { }
+    Params(Arguments const &args) { }
+  };
+
+  /// Shared storage
+  struct SharedStorage { };
+
+public:
+
+  CUTLASS_DEVICE
+  EpilogueFusedVisitorConcept(
+    Params const &params,                                         ///< Parameters routed to the epilogue
+    SharedStorage &shared_storage,                                ///< Shared storage needed by the functors here
+    MatrixCoord const &problem_size,                              ///< Problem size of the output
+    int thread_idx,                                               ///< Thread index within the threadblock
+    int warp_idx,                                                 ///< Warp index within the threadblock
+    int lane_idx,                                                 ///< Lane index within the warp
+    MatrixCoord const &threadblock_offset = MatrixCoord(0, 0)) {  ///< Coordinate
+
+  }
+
+  /// Helper to indicate split-K behavior
+  CUTLASS_DEVICE
+  void set_k_partition(
+    int split_k_index,                                            ///< Index of this threadblock within split-K partitioned scheme
+    int split_k_slices) {                                         ///< Total number of split-K slices
+
+  }
+
+  /// Called to set the batch index
+  CUTLASS_DEVICE
+  void set_batch_index(int batch_idx) {
+
+  }
+
+  /// Called at the start of the epilogue just before iterating over accumulator slices
+  CUTLASS_DEVICE
+  void begin_epilogue() {
+
+  }
 
-/// Epilogue operator for planar-complex output representations.
-///
-/// Note, as with most CUTLASS components for planar complex, the template arguments describe
-/// the underlying real data type.
+  /// Called at the start of one step before starting accumulator exchange
+  CUTLASS_DEVICE
+  void begin_step(int step_idx) {
+
+  }
+
+  /// Called at the start of a row
+  CUTLASS_DEVICE
+  void begin_row(int row_idx) {
+
+  }
+
+  /// Called after accumulators have been exchanged for each accumulator vector
+  CUTLASS_DEVICE
+  void visit(
+    int iter_idx,
+    int row_idx,
+    int column_idx,
+    int frag_idx,
+    AccumulatorFragment const &accum) {
+
+  }
+
+  /// Called at the end of a row
+  CUTLASS_DEVICE
+  void end_row(int row_idx) {
+
+  }
+
+  /// Called after all accumulator elements have been visited
+  CUTLASS_DEVICE
+  void end_step(int step_idx) {
+
+  }
+
+  /// Called after all steps have been completed
+  CUTLASS_DEVICE
+  void end_epilogue() {
+
+  }
+};
+
+////////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Epilogue operator
 template <
+  typename Visitor_,                        ///< Functor containing fused operations (satisfies EpilogueFusedVisitorConcept)
   typename Shape_,                          ///< Shape of threadblock tile (concept: GemmShape)
   typename WarpMmaOperator_,                ///< Warp-level MMA operator (concept: gemm::warp::MmaTensorOp)
   int PartitionsK,                          ///< Number of partitions of the K dimension
-  typename OutputTileIterator_,             ///< Tile iterator reading and writing output tensors
   typename AccumulatorFragmentIterator_,    ///< Fragment iterator selecting accumulators
   typename WarpTileIterator_,               ///< Warp-scoped tile iterator writing accumulators to SMEM
   typename SharedLoadIterator_,             ///< Threadblock-scoped tile iterator loading from SMEM
-  typename OutputOp_,                       ///< Output operator
-  typename Padding_                         ///< Padding added to SMEM allocation to avoid bank conflicts (concept: MatrixShape)
+  typename Padding_,                        ///< Padding added to SMEM allocation to avoid bank conflicts (concept: MatrixShape)
+  int FragmentsPerPartition = 1,            ///< Used to coarsten the epilogue granularity
+  int IterationsUnroll =                    ///< Used to reduce binary size when epilogue op is large
+    (true || !IsEpilogueFunctorHeavy<Visitor_>::value)
 >
-class EpiloguePlanarComplex {
+class EpilogueWithVisitor :
+  public EpilogueBase<
+    Shape_,
+    typename WarpMmaOperator_::Shape,
+    PartitionsK,
+    AccumulatorFragmentIterator_,
+    WarpTileIterator_,
+    Padding_,
+    FragmentsPerPartition> {
+
 public:
-  
+
+  using Visitor = Visitor_;
+
+  using Base = EpilogueBase<
+    Shape_,
+    typename WarpMmaOperator_::Shape,
+    PartitionsK,
+    AccumulatorFragmentIterator_,
+    WarpTileIterator_,
+    Padding_,
+    FragmentsPerPartition>;
+
   using Shape = Shape_;
   using WarpMmaOperator = WarpMmaOperator_;
   static int const kPartitionsK = PartitionsK;
-  using OutputTileIterator = OutputTileIterator_;
+
   using AccumulatorFragmentIterator = AccumulatorFragmentIterator_;
   using WarpTileIterator = WarpTileIterator_;
   using SharedLoadIterator = SharedLoadIterator_;
-  using OutputOp = OutputOp_;
   using Padding = Padding_;
 
-  /// Output layout is always row-major
   using Layout = layout::RowMajor;
   using LongIndex = typename Layout::LongIndex;
 
   /// The complete warp-level accumulator tile
-  using AccumulatorTile = ArrayPlanarComplex<
-    typename WarpMmaOperator::FragmentC::Element, 
-    WarpMmaOperator::FragmentC::kElements
-  >;
+  using AccumulatorTile = typename Base::AccumulatorTile;
 
   /// Accumulator element
   using ElementAccumulator = typename WarpTileIterator::Element;
 
-  /// Output element
-  using ElementOutput = typename OutputTileIterator::Element;
-
   /// Output access size
-  static int const kElementsPerAccess = OutputTileIterator::kElementsPerAccess;
-
-  /// Tensor reference to destination tensor
-  using TensorRef = typename OutputTileIterator::TensorRef;
+  static int const kElementsPerAccess = Visitor::kElementsPerAccess;
 
   /// Tensor reference to sync tensor
   using SyncTensorRef = typename cutlass::TensorRef<int, cutlass::layout::PackedVectorLayout>;
 
-  /// Const tensor reference to source tensor
-  using ConstTensorRef = typename OutputTileIterator::ConstTensorRef;
-
-  /// Array type used to output
-  using OutputAccessType = Array<
-    typename OutputTileIterator::Element, OutputTileIterator::kElementsPerAccess>;
-
   /// Array type used by output functor
-  using AccumulatorAccessType = Array<typename WarpTileIterator::Element, OutputTileIterator::kElementsPerAccess>; 
-  
-  /// Shape of each warp-level operation
-  using WarpShape = typename WarpMmaOperator::Shape;
+  using AccumulatorAccessType = Array<
+    typename WarpTileIterator::Element, kElementsPerAccess>;
 
   /// Number of warps
-  using WarpCount = gemm::GemmShape<
-    Shape::kM / WarpShape::kM,
-    Shape::kN / WarpShape::kN,
-    kPartitionsK
-  >;
-
-  /// Shared memory allocation
-  struct SharedStorage {
-
-    //
-    // Type definitions
-    //
-
-    /// Element type of shared memory
-    using Element = typename WarpTileIterator::Element;
-
-    /// Tensor reference to shared memory allocation
-    using TensorRef = typename WarpTileIterator::TensorRef;
+  using WarpCount = typename Base::WarpCount;
 
-    /// Layout of shared memory allocation
-    using Layout = typename WarpTileIterator::Layout;
-    
-    /// Logical shape of the shared memory tile written to by all warps.
-    using Shape = MatrixShape<
-      WarpCount::kM * WarpTileIterator::Shape::kRow * WarpCount::kK,
-      WarpCount::kN * WarpTileIterator::Shape::kColumn
-    >;
-
-    /// Shape of the shared memory allocation for the epilogue    
-    using StorageShape = MatrixShape<
-      Shape::kRow + Padding::kRow, 
-      Shape::kColumn + Padding::kColumn
-    >;
-
-    static int const kImaginaryStride = StorageShape::kCount;
-
-    //
-    // Data members
-    //
+  static int constexpr kSmemTiles = Base::kFragmentsPerIteration > 1 ? Base::kFragmentsPerIteration : kPartitionsK;
+  static int constexpr kSmemPointerOffset = Base::SharedStorage::StorageShape::kCount / kSmemTiles;
 
-    AlignedBuffer<Element, kImaginaryStride * 2> storage;
-
-    //
-    // Methods
-    //
-
-    /// Returns a pointer to the shared memory buffer
-    CUTLASS_DEVICE
-    Element *data() {
-      return storage.data();
-    }
-
-    /// Returns a tensor reference to the shared memory buffer
-    CUTLASS_DEVICE
-    TensorRef reference() {
-      return TensorRef(
-        storage.data(), 
-        Layout::packed({StorageShape::kRow, StorageShape::kColumn}));
-    }
-  };
+  using SharedStorage = typename Base::SharedStorage;
 
 private:
 
-  //
-  // Data members
-  //
-
-  SharedStorage &shared_storage_;
-
   /// Loads fragment from shared memory aligned with output tensor
   SharedLoadIterator shared_load_iterator_;
 
-  /// Stores a warp's fragment of accumulators to SMEM
-  WarpTileIterator warp_tile_iterator_;
-
 public:
 
   /// Constructor
   CUTLASS_DEVICE
-  EpiloguePlanarComplex(
-    SharedStorage &shared_storage,    ///< Shared storage object    
+  EpilogueWithVisitor(
+    SharedStorage &shared_storage,    ///< Shared storage object
     int thread_idx,                   ///< ID of a thread within the threadblock
     int warp_idx,                     ///< ID of warp within threadblock
     int lane_idx                      ///< Id of thread within warp
   ):
-    shared_storage_(shared_storage),
-    shared_load_iterator_(shared_storage.reference(), thread_idx),
-    warp_tile_iterator_(shared_storage.reference(), lane_idx) {
-
-    // Compute warp location within threadblock tile by mapping the warp_id to three coordinates:
-    //
-    //   _m: the warp's position within the threadblock along the M dimension
-    //   _n: the warp's position within the threadblock along the N dimension
-    //   _k: the warp's position within the threadblock along the K dimension
-
-    int warp_k = warp_idx / (WarpCount::kM * WarpCount::kN);
-    int warp_mn = warp_idx % (WarpCount::kM * WarpCount::kN);
-    int warp_m = warp_mn % WarpCount::kM;
-    int warp_n = warp_mn / WarpCount::kM;
+    Base(shared_storage, thread_idx, warp_idx, lane_idx),
+    shared_load_iterator_(shared_storage.reference(), thread_idx)
+  {
 
-    MatrixCoord warp_offset{warp_k * WarpCount::kM + warp_m, warp_n};
-
-    warp_tile_iterator_.add_tile_offset(warp_offset);
   }
 
   /// Streams the result to global memory
   CUTLASS_DEVICE
   void operator()(
-    OutputOp const &output_op,                        ///< Output operator
-    OutputTileIterator destination_iterator_real,     ///< Tile iterator for destination
-    OutputTileIterator destination_iterator_imag,     ///< Tile iterator for destination
-    AccumulatorTile const &accumulators,              ///< Complete warp-level accumulator tile
-    OutputTileIterator source_iterator_real,          ///< Threadblock tile coordinate in GEMM (in units of threadblock tiles)
-    OutputTileIterator source_iterator_imag) {        ///< Threadblock tile coordinate in GEMM (in units of threadblock tiles)
-
-    typename OutputTileIterator::Fragment source_fragment_real;
-    typename OutputTileIterator::Fragment source_fragment_imag;
-
-    if (!output_op.is_source_needed()) {
-      source_iterator_real.clear_mask();
-      source_iterator_imag.clear_mask();
-    }
+    Visitor & visitor,
+    AccumulatorTile const &accumulators) {         ///< Threadblock tile coordinate in GEMM (in units of threadblock tiles)
 
-    source_fragment_real.clear();
-    source_fragment_imag.clear();
+    visitor.begin_epilogue();
 
     //
     // Iterator over warp-level accumulator fragment
     //
 
-    AccumulatorFragmentIterator accum_fragment_iterator_real(accumulators.real);
-    AccumulatorFragmentIterator accum_fragment_iterator_imag(accumulators.imag);
+    AccumulatorFragmentIterator accum_fragment_iterator(accumulators);
 
     //
     // Iterate over accumulator tile
-    // 
+    //
 
-    CUTLASS_PRAGMA_UNROLL
-    for (int iter = 0; iter < OutputTileIterator::kIterations; ++iter) {
+    #pragma unroll(IterationsUnroll ? Visitor::kIterations : 1)
+    for (int iter_idx = 0; iter_idx < Visitor::kIterations; ++iter_idx) {
 
       //
       // Load the source
       //
 
-      source_iterator_real.load(source_fragment_real);
-      source_iterator_imag.load(source_fragment_imag);
-
-      ++source_iterator_real;
-      ++source_iterator_imag;
+      visitor.begin_step(iter_idx);
 
       //
       // Convert and store fragment
       //
-      
-      __syncthreads();
-
-      typename AccumulatorFragmentIterator::Fragment accum_fragment_real;
-      typename AccumulatorFragmentIterator::Fragment accum_fragment_imag;
 
-      accum_fragment_iterator_real.load(accum_fragment_real);
-      accum_fragment_iterator_imag.load(accum_fragment_imag);
-      
-      ++accum_fragment_iterator_real;
-      ++accum_fragment_iterator_imag;
+      __syncthreads();
 
-      this->warp_tile_iterator_.store(accum_fragment_real);
-      this->warp_tile_iterator_.store_with_pointer_offset(accum_fragment_imag, SharedStorage::kImaginaryStride);
+      acc2smem_source_needed<cutlass::make_index_sequence<Visitor::kIterations>>::push(
+          iter_idx, accum_fragment_iterator, this->warp_tile_iterator_);
 
       __syncthreads();
 
       //
       // Load fragments from shared memory
       //
 
-      typename SharedLoadIterator::Fragment aligned_accum_fragment_real[kPartitionsK];
-      typename SharedLoadIterator::Fragment aligned_accum_fragment_imag[kPartitionsK];
+      typename SharedLoadIterator::Fragment aligned_accum_fragment[kPartitionsK];
 
-      shared_load_iterator_.load(aligned_accum_fragment_real[0]);
-      shared_load_iterator_.load_with_pointer_offset(aligned_accum_fragment_imag[0], SharedStorage::kImaginaryStride);
+      shared_load_iterator_.load(aligned_accum_fragment[0]);
 
       // If the number of k-slices is > 1 - perform a reduction amongst the k-slices
-      static_assert(kPartitionsK  == 1, "Sliced-K not supported for planar complex at this time");
-    
-      //
-      // Compute the output result
-      //
-     
-      typename OutputTileIterator::Fragment output_fragment_real;
-      typename OutputTileIterator::Fragment output_fragment_imag;
-
-      apply_output_operator_(
-        output_fragment_real, 
-        output_fragment_imag, 
-        output_op, 
-        aligned_accum_fragment_real[0],
-        aligned_accum_fragment_imag[0], 
-        source_fragment_real,
-        source_fragment_imag);
+      if (kPartitionsK > 1) {
 
-      //
-      // Store the final result
-      //
+        plus <typename SharedLoadIterator::Fragment> add_fragments;
 
-      destination_iterator_real.store(output_fragment_real);
-      destination_iterator_imag.store(output_fragment_imag);
+        CUTLASS_PRAGMA_UNROLL
+        for ( int i = 1; i < kPartitionsK; ++i) {
+          shared_load_iterator_.add_pointer_offset(kSmemPointerOffset);
+          shared_load_iterator_.load(aligned_accum_fragment[i]);
+          aligned_accum_fragment[0] = add_fragments(aligned_accum_fragment[0], aligned_accum_fragment[i]);
+        }
 
-      ++destination_iterator_real;
-      ++destination_iterator_imag;
-    }
-  }
+        shared_load_iterator_.add_pointer_offset((1 - kPartitionsK) * kSmemPointerOffset);
+      }
 
-private:
+      //
+      // Iterate over output fragments
+      //
 
-  /// Helper to invoke the output functor over each vector of output
-  CUTLASS_DEVICE
-  void apply_output_operator_(
-    typename OutputTileIterator::Fragment &output_fragment_real,
-    typename OutputTileIterator::Fragment &output_fragment_imag,
-    OutputOp const &output_op,                    ///< Output operator
-    typename SharedLoadIterator::Fragment const &aligned_accum_fragment_real,
-    typename SharedLoadIterator::Fragment const &aligned_accum_fragment_imag,
-    typename OutputTileIterator::Fragment const &source_fragment_real,
-    typename OutputTileIterator::Fragment const &source_fragment_imag) {
+      AccumulatorAccessType const *accum_frag_ptr =
+        reinterpret_cast<AccumulatorAccessType const *>(&aligned_accum_fragment[0]);
 
-    OutputAccessType *output_frag_real_ptr = 
-      reinterpret_cast<OutputAccessType *>(&output_fragment_real);
+      int const kAccumulatorFragmentCount = AccumulatorTile::kElements / (Visitor::kIterations * AccumulatorAccessType::kElements);
 
-    OutputAccessType *output_frag_imag_ptr = 
-      reinterpret_cast<OutputAccessType *>(&output_fragment_imag);
+      CUTLASS_PRAGMA_UNROLL
+      for (int idx = 0; idx < kAccumulatorFragmentCount; ++idx) {
 
-    AccumulatorAccessType const *compute_frag_real_ptr = 
-      reinterpret_cast<AccumulatorAccessType const *>(&aligned_accum_fragment_real);
+        int row_idx = idx / SharedLoadIterator::ThreadMap::Iterations::kColumn;
+        int col_idx = idx % SharedLoadIterator::ThreadMap::Iterations::kColumn;
+
+        // Start a new row of the output fragment
+        if (!col_idx) {
+          visitor.begin_row(row_idx);
+        }
+
+        visitor.visit(
+          iter_idx,
+          row_idx,
+          col_idx,
+          idx,
+          accum_frag_ptr[idx]
+        );
+
+        // End the row of the output fragment
+        if (col_idx + 1 == SharedLoadIterator::ThreadMap::Iterations::kColumn) {
+          visitor.end_row(row_idx);
+        }
+      }
 
-    AccumulatorAccessType const *compute_frag_imag_ptr = 
-      reinterpret_cast<AccumulatorAccessType const *>(&aligned_accum_fragment_imag);
+      //
+      // Conclude the step
+      //
 
-    OutputAccessType const *source_frag_real_ptr = 
-      reinterpret_cast<OutputAccessType const *>(&source_fragment_real);
+      visitor.end_step(iter_idx);
+    }
 
-    OutputAccessType const *source_frag_imag_ptr = 
-      reinterpret_cast<OutputAccessType const *>(&source_fragment_imag);
+    visitor.end_epilogue();
+  }
 
-    int const kOutputOpIterations = 
-      OutputTileIterator::Fragment::kElements / OutputTileIterator::kElementsPerAccess;
+private:
 
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < kOutputOpIterations; ++i) {
 
-      // Call the output operator
-      auto result_fragment = output_op(
-        make_ArrayPlanarComplex(compute_frag_real_ptr[i], compute_frag_imag_ptr[i]), 
-        make_ArrayPlanarComplex(source_frag_real_ptr[i], source_frag_imag_ptr[i])
-      );
+  template<class Seq>
+  struct acc2smem_source_needed;
 
-      output_frag_real_ptr[i] = result_fragment.real;
-      output_frag_imag_ptr[i] = result_fragment.imag;
+  template <size_t... Seq>
+  struct acc2smem_source_needed<cutlass::index_sequence<Seq...>> {
+    template<int Advance>
+    CUTLASS_DEVICE
+    static void helper(AccumulatorFragmentIterator accum_fragment_iterator,
+                       WarpTileIterator &warp_tile_iterator) {
+      CUTLASS_PRAGMA_UNROLL
+      for (int i = 0; i < Advance; i++) {
+        ++accum_fragment_iterator;
+      }
+
+      typename AccumulatorFragmentIterator::Fragment accum_fragment;
+      accum_fragment_iterator.load(accum_fragment);
+      warp_tile_iterator.store(accum_fragment);
     }
-  }
 
+    CUTLASS_DEVICE
+    static void push(size_t pos,
+                     AccumulatorFragmentIterator const &iterator_begin,
+                     WarpTileIterator &warp_tile_iterator) {
+      int dummy[] = {(pos == Seq) && (helper<Seq>(iterator_begin, warp_tile_iterator), 0)...};
+    }
+  };
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Helper to create an EpilogueWithVisitor from an existing epilogue
+template <typename Visitor_, typename Existing_, bool IterationsUnroll = true>
+struct EpilogueWithVisitorFromExistingEpilogue  {
+
+  using Epilogue = EpilogueWithVisitor<
+    Visitor_,
+    typename Existing_::Shape,
+    typename Existing_::WarpMmaOperator,
+    Existing_::kPartitionsK,
+    typename Existing_::AccumulatorFragmentIterator,
+    typename Existing_::WarpTileIterator,
+    typename Existing_::SharedLoadIterator,
+    typename Existing_::Padding,
+    Existing_::kFragmentsPerIteration,
+    IterationsUnroll
+  >;
 };
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace threadblock
 } // namespace epilogue
 } // namespace cutlass
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_smem_accumulator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_smem_accumulator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_visitor_with_softmax.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_visitor_with_softmax.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_broadcast.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_broadcast.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_reduction.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_reduction.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear.h`

 * *Files 22% similar despite different names*

```diff
@@ -25,385 +25,384 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Generic epilogue for implementing certain kinds of fused epilogue behavior.
+    \brief Templates implementing computing the addresses of storing of tiles
+   from pitch-linear rank=2 tensors.
 */
 
 #pragma once
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
 #include "cutlass/cutlass.h"
-#include "cutlass/fast_math.h"
+#include "cutlass/array.h"
+#include "cutlass/layout/pitch_linear.h"
+#include "cutlass/layout/matrix.h"
 #include "cutlass/matrix_coord.h"
-#include "cutlass/semaphore.h"
-#include "cutlass/epilogue/threadblock/epilogue_base.h"
+#include "cutlass/matrix_shape.h"
+#include "cutlass/tensor_ref.h"
+
+#include "cutlass/transform/threadblock/regular_tile_access_iterator.h"
 
-////////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
-namespace epilogue {
+namespace transform {
 namespace threadblock {
 
-////////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
-class EpilogueFusedVisitorConcept {
-public:
+/// Tile iterator specialized for congruous arrangements for TensorOps
+///
+///
+/// Satisfies: ForwardTileIteratorConcept |
+///            ReadableContiguousTileIteratorConcept |
+///            WriteableContiguousTileIteratorConcept
+///
+template <typename Shape_, typename Element_, int AdvanceRank,
+          typename ThreadMap_, int Alignment>
+class RegularTileAccessIterator<
+    Shape_, Element_,
+    layout::PitchLinear,
+    AdvanceRank, ThreadMap_, Alignment> {
+ public:
+  static_assert(
+      AdvanceRank == 0 || AdvanceRank == 1,
+      "Specialization for pitch-linear iterator may along advance along the "
+      "contiguous(rank=0) or strided(rank=1) dimension.");
 
-  static int const kIterations = 1;
-  static int const kElementsPerAccess = 4;
-  using ElementOutput = float;
-  using ElementAccumulator = float;
-  using AccumulatorFragment = Array<ElementAccumulator, kElementsPerAccess>;
-
-  /// Arguments structure
-  struct Arguments {  };
+  using Shape = Shape_;
+  using Element = Element_;
+  using Layout = layout::PitchLinear;
+  static int const kAdvanceRank = AdvanceRank;
+  static int const kAlignment = Alignment;
 
-  /// Params structure
-  struct Params {
+  using Index = typename Layout::Index;
+  using LongIndex = typename Layout::LongIndex;
+  using StrideIndex = typename Layout::Stride::Index;
 
-    Params() { }
-    Params(Arguments const &args) { }
-  };
+  using TensorRef = TensorRef<Element, Layout>;
+  using TensorCoord = typename Layout::TensorCoord;
 
-  /// Shared storage
-  struct SharedStorage { };
+  using ThreadMap = ThreadMap_;
 
-public:
+  /// Element type per access
+  using AccessType = Array<Element, ThreadMap::kElementsPerAccess>;
 
-  CUTLASS_DEVICE
-  EpilogueFusedVisitorConcept(
-    Params const &params,                                         ///< Parameters routed to the epilogue
-    SharedStorage &shared_storage,                                ///< Shared storage needed by the functors here
-    MatrixCoord const &problem_size,                              ///< Problem size of the output
-    int thread_idx,                                               ///< Thread index within the threadblock
-    int warp_idx,                                                 ///< Warp index within the threadblock
-    int lane_idx,                                                 ///< Lane index within the warp
-    MatrixCoord const &threadblock_offset = MatrixCoord(0, 0)) {  ///< Coordinate
+ private:
+  //
+  // Data members
+  //
 
-  }
+  /// Stride value
+  StrideIndex stride_;
 
-  /// Helper to indicate split-K behavior
-  CUTLASS_DEVICE
-  void set_k_partition(
-    int split_k_index,                                            ///< Index of this threadblock within split-K partitioned scheme
-    int split_k_slices) {                                         ///< Total number of split-K slices
+  /// Internal pointer to first access of tile
+  AccessType *pointer_;
 
-  }
+  /// Internal byte offset
+  Index byte_offset_;
 
-  /// Called to set the batch index
-  CUTLASS_DEVICE
-  void set_batch_index(int batch_idx) {
+  /// Iteration in the contiguous dimension
+  int iteration_contiguous_;
 
-  }
+  /// Iteration in the strided dimension
+  int iteration_strided_;
 
-  /// Called at the start of the epilogue just before iterating over accumulator slices
-  CUTLASS_DEVICE
-  void begin_epilogue() {
+ public:
+  /// Construct a TileIterator with zero threadblock offset
+  CUTLASS_HOST_DEVICE
+  RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
+                            int thread_id   ///< ID of each participating thread
+                            )
+      : stride_(ref.stride(0) / ThreadMap::kElementsPerAccess),
+        byte_offset_(0) {
 
-  }
+    layout::PitchLinearCoord thread_offset_base = ThreadMap::initial_offset(thread_id);
 
-  /// Called at the start of one step before starting accumulator exchange
-  CUTLASS_DEVICE
-  void begin_step(int step_idx) {
+    // initialize pointer
+    pointer_ = reinterpret_cast<AccessType *>(ref.data() + ref.offset(thread_offset_base));
 
+    set_iteration_index(0);
   }
 
-  /// Called at the start of a row
-  CUTLASS_DEVICE
-  void begin_row(int row_idx) {
+  /// Overrides the internal iteration index
+  CUTLASS_HOST_DEVICE
+  void set_iteration_index(int index) {
+    iteration_contiguous_ = index % ThreadMap::Iterations::kContiguous;
+    iteration_strided_ = index / ThreadMap::Iterations::kContiguous;
+  }
 
+  /// Adds a pointer offset in units of Element
+  CUTLASS_HOST_DEVICE
+  void add_pointer_offset(LongIndex pointer_offset) {
+    byte_offset_ += pointer_offset * sizeof(Element);
   }
 
-  /// Called after accumulators have been exchanged for each accumulator vector
+  /// Returns a pointer
   CUTLASS_DEVICE
-  void visit(
-    int iter_idx,
-    int row_idx,
-    int column_idx,
-    int frag_idx,
-    AccumulatorFragment const &accum) {
+  AccessType *get() const {
 
-  }
+    AccessType *access_ptr = pointer_;
 
-  /// Called at the end of a row
-  CUTLASS_DEVICE
-  void end_row(int row_idx) {
+    int access_offset = iteration_strided_ * ThreadMap::Delta::kStrided * stride_ +
+                        iteration_contiguous_ * ThreadMap::Delta::kContiguous /
+                            ThreadMap::kElementsPerAccess;
 
+    char *access_byte_ptr =
+        reinterpret_cast<char *>(access_ptr + access_offset);
+
+    return reinterpret_cast<AccessType *>(access_byte_ptr + byte_offset_);
   }
 
-  /// Called after all accumulator elements have been visited
-  CUTLASS_DEVICE
-  void end_step(int step_idx) {
+  /// Advances to the next tile in memory.
+  CUTLASS_HOST_DEVICE
+  RegularTileAccessIterator &operator++() {
+    ++iteration_contiguous_;
 
-  }
+    if (iteration_contiguous_ < ThreadMap::Iterations::kContiguous)
+      return *this;
 
-  /// Called after all steps have been completed
-  CUTLASS_DEVICE
-  void end_epilogue() {
+    // Enter here only if (iteration_contiguous_ ==
+    // ThreadMap::Iteration::kContiguous)
+    iteration_contiguous_ = 0;
+    ++iteration_strided_;
+
+    if (iteration_strided_ < ThreadMap::Iterations::kStrided) {
+      return *this;
+    }
 
+    // Enter here only if (iteration_stride_ == ThreadMap::Iteration::kStrided)
+    // which means we enter the next tile.
+    iteration_strided_ = 0;
+
+    return *this;
+  }
+
+  /// Advances to the next tile in memory.
+  CUTLASS_HOST_DEVICE
+  RegularTileAccessIterator operator++(int) {
+    RegularTileAccessIterator prev(*this);
+    this->operator++();
+
+    return prev;
+  }
+
+  /// Adds a tile offset in the unit of tile.
+  /// In GEMM/Conv implementation, this is used to move in the k dimension in the shared memory.
+  /// Below layouts are the shared memory layouts.  Current SM50 SIMT kernels only use col major A and row major B.
+  ///   For row major A operand, k dimension is contiguous dimension;
+  ///   For col major A operand, k dimension is strided dimension;
+  ///   For row major B operand, k dimension is strided dimension;
+  ///   For col major B operand, k dimension is contiguous dimension.
+  /// Below two classes map col/row major to the pitch linear coordinates used
+  /// in this base class.
+  CUTLASS_DEVICE
+  void add_tile_offset(TensorCoord const &coord) {
+    add_pointer_offset(coord.contiguous() * Shape::kContiguous +
+                       coord.strided() * Shape::kStrided * stride_ *
+                           ThreadMap::kElementsPerAccess);
   }
 };
 
-////////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
-/// Epilogue operator
-template <
-  typename Visitor_,                        ///< Functor containing fused operations (satisfies EpilogueFusedVisitorConcept)
-  typename Shape_,                          ///< Shape of threadblock tile (concept: GemmShape)
-  typename WarpMmaOperator_,                ///< Warp-level MMA operator (concept: gemm::warp::MmaTensorOp)
-  int PartitionsK,                          ///< Number of partitions of the K dimension
-  typename AccumulatorFragmentIterator_,    ///< Fragment iterator selecting accumulators
-  typename WarpTileIterator_,               ///< Warp-scoped tile iterator writing accumulators to SMEM
-  typename SharedLoadIterator_,             ///< Threadblock-scoped tile iterator loading from SMEM
-  typename Padding_,                        ///< Padding added to SMEM allocation to avoid bank conflicts (concept: MatrixShape)
-  int FragmentsPerPartition = 1,            ///< Used to coarsten the epilogue granularity
-  int IterationsUnroll =                    ///< Used to reduce binary size when epilogue op is large
-    (true || !IsEpilogueFunctorHeavy<Visitor_>::value)
->
-class EpilogueWithVisitor :
-  public EpilogueBase<
-    Shape_,
-    typename WarpMmaOperator_::Shape,
-    PartitionsK,
-    AccumulatorFragmentIterator_,
-    WarpTileIterator_,
-    Padding_,
-    FragmentsPerPartition> {
-
-public:
-
-  using Visitor = Visitor_;
-
-  using Base = EpilogueBase<
-    Shape_,
-    typename WarpMmaOperator_::Shape,
-    PartitionsK,
-    AccumulatorFragmentIterator_,
-    WarpTileIterator_,
-    Padding_,
-    FragmentsPerPartition>;
+/// Tile iterator specialized for column major layouts
+///
+///
+/// Satisfies: ForwardTileIteratorConcept |
+///            ReadableContiguousTileIteratorConcept |
+///            WriteableContiguousTileIteratorConcept
+///
+template <typename Shape_, typename Element_, int AdvanceRank,
+          typename ThreadMap_, int Alignment>
+class RegularTileAccessIterator<
+    Shape_, Element_,
+    layout::ColumnMajor,
+    AdvanceRank, ThreadMap_, Alignment> {
+ public:
+  static_assert(
+      AdvanceRank == 0 || AdvanceRank == 1,
+      "Specialization for pitch-linear iterator may along advance along the "
+      "contiguous(rank=0) or strided(rank=1) dimension.");
 
   using Shape = Shape_;
-  using WarpMmaOperator = WarpMmaOperator_;
-  static int const kPartitionsK = PartitionsK;
+  using Element = Element_;
+  using Layout = layout::ColumnMajor;
+  static int const kAdvanceRank = AdvanceRank;
+  static int const kAlignment = Alignment;
 
-  using AccumulatorFragmentIterator = AccumulatorFragmentIterator_;
-  using WarpTileIterator = WarpTileIterator_;
-  using SharedLoadIterator = SharedLoadIterator_;
-  using Padding = Padding_;
-
-  using Layout = layout::RowMajor;
+  using Index = typename Layout::Index;
   using LongIndex = typename Layout::LongIndex;
 
-  /// The complete warp-level accumulator tile
-  using AccumulatorTile = typename Base::AccumulatorTile;
-
-  /// Accumulator element
-  using ElementAccumulator = typename WarpTileIterator::Element;
-
-  /// Output access size
-  static int const kElementsPerAccess = Visitor::kElementsPerAccess;
+  using TensorRef = TensorRef<Element, Layout>;
+  using TensorCoord = typename Layout::TensorCoord;
 
-  /// Tensor reference to sync tensor
-  using SyncTensorRef = typename cutlass::TensorRef<int, cutlass::layout::PackedVectorLayout>;
+  using ThreadMap = ThreadMap_;
 
-  /// Array type used by output functor
-  using AccumulatorAccessType = Array<
-    typename WarpTileIterator::Element, kElementsPerAccess>;
+  /// Underlying iterator type
+  using UnderlyingIterator = RegularTileAccessIterator<
+      layout::PitchLinearShape<Shape::kRow, Shape::kColumn>, Element,
+      layout::PitchLinear,
+      (kAdvanceRank == 0 ? 0 : 1), 
+      ThreadMap_>;
 
-  /// Number of warps
-  using WarpCount = typename Base::WarpCount;
+  using AccessType = typename UnderlyingIterator::AccessType;
 
-  static int constexpr kSmemTiles = Base::kFragmentsPerIteration > 1 ? Base::kFragmentsPerIteration : kPartitionsK;
-  static int constexpr kSmemPointerOffset = Base::SharedStorage::StorageShape::kCount / kSmemTiles;
+ private:
 
-  using SharedStorage = typename Base::SharedStorage;
+  /// Underlying iterator
+  UnderlyingIterator iterator_;
 
-private:
+ public:
+  /// Construct a TileIterator with zero threadblock offset
+  CUTLASS_HOST_DEVICE
+  RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
+                            int thread_id   ///< ID of each participating thread
+                            )
+      : iterator_({ref.data(), ref.stride()}, thread_id) {}
 
-  /// Loads fragment from shared memory aligned with output tensor
-  SharedLoadIterator shared_load_iterator_;
+  /// Overrides the internal iteration index
+  CUTLASS_HOST_DEVICE
+  void set_iteration_index(int index) { iterator_.set_iteration_index(index); }
 
-public:
-
-  /// Constructor
-  CUTLASS_DEVICE
-  EpilogueWithVisitor(
-    SharedStorage &shared_storage,    ///< Shared storage object
-    int thread_idx,                   ///< ID of a thread within the threadblock
-    int warp_idx,                     ///< ID of warp within threadblock
-    int lane_idx                      ///< Id of thread within warp
-  ):
-    Base(shared_storage, thread_idx, warp_idx, lane_idx),
-    shared_load_iterator_(shared_storage.reference(), thread_idx)
-  {
+  /// Adds a pointer offset in units of Element
+  CUTLASS_HOST_DEVICE
+  void add_pointer_offset(LongIndex pointer_offset) {
+    iterator_.add_pointer_offset(pointer_offset);
+  }
 
+  /// Returns a pointer
+  CUTLASS_HOST_DEVICE
+  AccessType *get() const {
+    return reinterpret_cast<AccessType *>(iterator_.get());
   }
 
-  /// Streams the result to global memory
+  /// Adds a tile offset
   CUTLASS_DEVICE
-  void operator()(
-    Visitor & visitor,
-    AccumulatorTile const &accumulators) {         ///< Threadblock tile coordinate in GEMM (in units of threadblock tiles)
-
-    visitor.begin_epilogue();
-
-    //
-    // Iterator over warp-level accumulator fragment
-    //
-
-    AccumulatorFragmentIterator accum_fragment_iterator(accumulators);
-
-    //
-    // Iterate over accumulator tile
-    //
-
-    #pragma unroll(IterationsUnroll ? Visitor::kIterations : 1)
-    for (int iter_idx = 0; iter_idx < Visitor::kIterations; ++iter_idx) {
-
-      //
-      // Load the source
-      //
-
-      visitor.begin_step(iter_idx);
-
-      //
-      // Convert and store fragment
-      //
-
-      __syncthreads();
-
-      acc2smem_source_needed<cutlass::make_index_sequence<Visitor::kIterations>>::push(
-          iter_idx, accum_fragment_iterator, this->warp_tile_iterator_);
-
-      __syncthreads();
-
-      //
-      // Load fragments from shared memory
-      //
+  void add_tile_offset(TensorCoord const &coord) {
+    iterator_.add_tile_offset({coord.row(), coord.column()});
+  }
 
-      typename SharedLoadIterator::Fragment aligned_accum_fragment[kPartitionsK];
+  /// Advances to the next tile in memory.
+  CUTLASS_HOST_DEVICE
+  RegularTileAccessIterator &operator++() {
+    ++iterator_;
+    return *this;
+  }
 
-      shared_load_iterator_.load(aligned_accum_fragment[0]);
+  /// Advances to the next tile in memory.
+  CUTLASS_HOST_DEVICE
+  RegularTileAccessIterator operator++(int) {
+    RegularTileAccessIterator prev(*this);
+    ++iterator_;
 
-      // If the number of k-slices is > 1 - perform a reduction amongst the k-slices
-      if (kPartitionsK > 1) {
+    return prev;
+  }
+};
 
-        plus <typename SharedLoadIterator::Fragment> add_fragments;
 
-        CUTLASS_PRAGMA_UNROLL
-        for ( int i = 1; i < kPartitionsK; ++i) {
-          shared_load_iterator_.add_pointer_offset(kSmemPointerOffset);
-          shared_load_iterator_.load(aligned_accum_fragment[i]);
-          aligned_accum_fragment[0] = add_fragments(aligned_accum_fragment[0], aligned_accum_fragment[i]);
-        }
+////////////////////////////////////////////////////////////////////////////////
 
-        shared_load_iterator_.add_pointer_offset((1 - kPartitionsK) * kSmemPointerOffset);
-      }
+/// Tile iterator specialized for row major layouts
+///
+///
+/// Satisfies: ForwardTileIteratorConcept |
+///            ReadableContiguousTileIteratorConcept |
+///            WriteableContiguousTileIteratorConcept
+///
+template <typename Shape_, typename Element_, int AdvanceRank,
+          typename ThreadMap_, int Alignment>
+class RegularTileAccessIterator<
+    Shape_, Element_,
+    layout::RowMajor,
+    AdvanceRank, ThreadMap_, Alignment> {
+ public:
+  static_assert(
+      AdvanceRank == 0 || AdvanceRank == 1,
+      "Specialization for pitch-linear iterator may along advance along the "
+      "contiguous(rank=0) or strided(rank=1) dimension.");
 
-      //
-      // Iterate over output fragments
-      //
+  using Shape = Shape_;
+  using Element = Element_;
+  using Layout = layout::RowMajor;
+  static int const kAdvanceRank = AdvanceRank;
+  static int const kAlignment = Alignment;
 
-      AccumulatorAccessType const *accum_frag_ptr =
-        reinterpret_cast<AccumulatorAccessType const *>(&aligned_accum_fragment[0]);
+  using Index = typename Layout::Index;
+  using LongIndex = typename Layout::LongIndex;
 
-      int const kAccumulatorFragmentCount = AccumulatorTile::kElements / (Visitor::kIterations * AccumulatorAccessType::kElements);
+  using TensorRef = TensorRef<Element, Layout>;
+  using TensorCoord = typename Layout::TensorCoord;
 
-      CUTLASS_PRAGMA_UNROLL
-      for (int idx = 0; idx < kAccumulatorFragmentCount; ++idx) {
+  using ThreadMap = ThreadMap_;
 
-        int row_idx = idx / SharedLoadIterator::ThreadMap::Iterations::kColumn;
-        int col_idx = idx % SharedLoadIterator::ThreadMap::Iterations::kColumn;
+  /// Underlying iterator type
+  using UnderlyingIterator = RegularTileAccessIterator<
+      layout::PitchLinearShape<Shape::kColumn, Shape::kRow>, Element,
+      layout::PitchLinear,
+      (kAdvanceRank == 0 ? 1 : 0), 
+      ThreadMap_>;
 
-        // Start a new row of the output fragment
-        if (!col_idx) {
-          visitor.begin_row(row_idx);
-        }
+  using AccessType = typename UnderlyingIterator::AccessType;
 
-        visitor.visit(
-          iter_idx,
-          row_idx,
-          col_idx,
-          idx,
-          accum_frag_ptr[idx]
-        );
+ private:
 
-        // End the row of the output fragment
-        if (col_idx + 1 == SharedLoadIterator::ThreadMap::Iterations::kColumn) {
-          visitor.end_row(row_idx);
-        }
-      }
+  /// Underlying iterator
+  UnderlyingIterator iterator_;
 
-      //
-      // Conclude the step
-      //
+ public:
+  /// Construct a TileIterator with zero threadblock offset
+  CUTLASS_HOST_DEVICE
+  RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
+                            int thread_id   ///< ID of each participating thread
+                            )
+      : iterator_({ref.data(), ref.stride()}, thread_id) {}
 
-      visitor.end_step(iter_idx);
-    }
+  /// Overrides the internal iteration index
+  CUTLASS_HOST_DEVICE
+  void set_iteration_index(int index) { iterator_.set_iteration_index(index); }
 
-    visitor.end_epilogue();
+  /// Adds a pointer offset in units of Element
+  CUTLASS_HOST_DEVICE
+  void add_pointer_offset(LongIndex pointer_offset) {
+    iterator_.add_pointer_offset(pointer_offset);
   }
 
-private:
-
-
-  template<class Seq>
-  struct acc2smem_source_needed;
-
-  template <size_t... Seq>
-  struct acc2smem_source_needed<cutlass::index_sequence<Seq...>> {
-    template<int Advance>
-    CUTLASS_DEVICE
-    static void helper(AccumulatorFragmentIterator accum_fragment_iterator,
-                       WarpTileIterator &warp_tile_iterator) {
-      CUTLASS_PRAGMA_UNROLL
-      for (int i = 0; i < Advance; i++) {
-        ++accum_fragment_iterator;
-      }
+  /// Returns a pointer
+  CUTLASS_HOST_DEVICE
+  AccessType *get() const {
+    return reinterpret_cast<AccessType *>(iterator_.get());
+  }
 
-      typename AccumulatorFragmentIterator::Fragment accum_fragment;
-      accum_fragment_iterator.load(accum_fragment);
-      warp_tile_iterator.store(accum_fragment);
-    }
+  /// Adds a tile offset
+  CUTLASS_DEVICE
+  void add_tile_offset(TensorCoord const &coord) {
+    iterator_.add_tile_offset({coord.column(), coord.row()});
+  }
 
-    CUTLASS_DEVICE
-    static void push(size_t pos,
-                     AccumulatorFragmentIterator const &iterator_begin,
-                     WarpTileIterator &warp_tile_iterator) {
-      int dummy[] = {(pos == Seq) && (helper<Seq>(iterator_begin, warp_tile_iterator), 0)...};
-    }
-  };
-};
+  /// Advances to the next tile in memory.
+  CUTLASS_HOST_DEVICE
+  RegularTileAccessIterator &operator++() {
+    ++iterator_;
+    return *this;
+  }
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+  /// Advances to the next tile in memory.
+  CUTLASS_HOST_DEVICE
+  RegularTileAccessIterator operator++(int) {
+    RegularTileAccessIterator prev(*this);
+    ++iterator_;
 
-/// Helper to create an EpilogueWithVisitor from an existing epilogue
-template <typename Visitor_, typename Existing_, bool IterationsUnroll = true>
-struct EpilogueWithVisitorFromExistingEpilogue  {
-
-  using Epilogue = EpilogueWithVisitor<
-    Visitor_,
-    typename Existing_::Shape,
-    typename Existing_::WarpMmaOperator,
-    Existing_::kPartitionsK,
-    typename Existing_::AccumulatorFragmentIterator,
-    typename Existing_::WarpTileIterator,
-    typename Existing_::SharedLoadIterator,
-    typename Existing_::Padding,
-    Existing_::kFragmentsPerIteration,
-    IterationsUnroll
-  >;
+    return prev;
+  }
 };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
-} // namespace threadblock
-} // namespace epilogue
-} // namespace cutlass
+}  // namespace threadblock
+}  // namespace transform
+}  // namespace cutlass
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_workspace.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_workspace.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/interleaved_epilogue.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_vector_access_iterator.h`

 * *Files 25% similar despite different names*

```diff
@@ -24,384 +24,394 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/*! \file
-  \brief Epilogue for threadblock scoped GEMMs using Tensor Ops.
-
-  The epilogue rearranges the result of a matrix product through shared memory to match canonical
-  tensor layouts in global memory. Epilogues support conversion and reduction operations.
 
+/*! \file
+    \brief Templates implementing computing the addresses of loading small
+    vectors from the global memory.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
-#include "cutlass/numeric_types.h"
 #include "cutlass/array.h"
-#include "cutlass/layout/vector.h"
+#include "cutlass/coord.h"
+#include "cutlass/layout/pitch_linear.h"
+#include "cutlass/layout/matrix.h"
 #include "cutlass/layout/tensor.h"
-#include "cutlass/tensor_coord.h"
-#include "cutlass/aligned_buffer.h"
-
-#include "cutlass/gemm/gemm.h"
-
-#include "cutlass/transform/pitch_linear_thread_map.h"
-#include "cutlass/transform/threadblock/regular_tile_iterator.h"
-
-#include "cutlass/epilogue/threadblock/epilogue_base_streamk.h"
-#include "cutlass/epilogue/threadblock/predicated_tile_iterator.h"
+#include "cutlass/matrix_coord.h"
+#include "cutlass/matrix_shape.h"
+#include "cutlass/tensor_ref.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
-namespace epilogue {
+namespace transform {
 namespace threadblock {
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Epilogue operator without splitk
+/// PredicatedVectorAccessIterator
+///
 template <
-    /// Shape of threadblock tile (concept: GemmShape)
-    typename Shape_,
-    /// Warp-level MMA operator (concept: gemm::warp::MmaTensorOp)
-    typename WarpMmaOperator_,
-    /// Number of partitions of the K dimension
-    int PartitionsK,
-    /// Tile iterator reading and writing output tensors
-    typename OutputTileIterator_,
-    /// Fragment iterator selecting accumulators
-    typename AccumulatorFragmentIterator_,
-    /// Output operator
-    typename OutputOp_,
-    /// Number of interleaved k
-    int InterleavedK>
-class InterleavedEpilogue :
-  public EpilogueBaseStreamK<
-    Shape_,
-    PartitionsK,
-    WarpMmaOperator_,
-    AccumulatorFragmentIterator_>
-{
-public:
-
-  using BaseStreamK = EpilogueBaseStreamK<
-    Shape_,
-    PartitionsK,
-    WarpMmaOperator_,
-    AccumulatorFragmentIterator_>;
+    /// Shape of the vector accessed by the entire threadblock
+    typename Shape,
+    /// Shape of the vector accessed by the warp
+    typename WarpShape,
+    /// Type of Element
+    typename Element,
+    /// Layout of the vector
+    typename Layout,
+    /// Number of elements for each access
+    int ElementsPerAccess,
+    /// Support residual tile
+    bool EnableResidualAccess = false
+>
+class PredicatedVectorAccessIterator;
 
-  using Shape = Shape_;
-  using WarpMmaOperator = WarpMmaOperator_;
-  static int const kPartitionsK = PartitionsK;
-  using AccumulatorFragmentIterator = AccumulatorFragmentIterator_;
-  using OutputTileIterator = OutputTileIterator_;
-  using OutputOp = OutputOp_;
-
-  /// The complete warp-level accumulator tile
-  using AccumulatorTile = typename AccumulatorFragmentIterator::AccumulatorTile;
-
-  /// Fragment type used by the accumulator tile's fragment iterator
-  using AccumulatorFragment = typename AccumulatorFragmentIterator::Fragment;
-
-  /// Accumulator element
-  using ElementAccumulator = typename AccumulatorTile::Element;
-
-  /// Output element
-  using ElementOutput = typename OutputTileIterator::Element;
-
-  /// Output access size
-  static int const kElementsPerAccess = OutputTileIterator::kElementsPerAccess;
-
-  /// Tensor reference to destination tensor
-  using TensorRef = typename OutputTileIterator::TensorRef;
-
-  /// Tensor reference to sync tensor
-  using SyncTensorRef =
-      typename cutlass::TensorRef<int, cutlass::layout::PackedVectorLayout>;
-
-  /// Const tensor reference to source tensor
-  using ConstTensorRef = typename OutputTileIterator::ConstTensorRef;
-
-  /// Array type used to output
-  using OutputAccessType = Array<typename OutputTileIterator::Element,
-                                 OutputTileIterator::kElementsPerAccess>;
-
-  /// Array type used by output functor
-  using AccumulatorAccessType =
-      Array<ElementAccumulator, OutputTileIterator::kElementsPerAccess>;
-
-  /// Number of warps
-  using WarpCount =
-      gemm::GemmShape<Shape::kM / WarpMmaOperator::Shape::kM,
-                      Shape::kN / WarpMmaOperator::Shape::kN, kPartitionsK>;
-
-public:
-
-  static_assert(OutputTileIterator::kElementsPerAccess,
-                "This must not be zero.");
-
-  static_assert(!(OutputTileIterator::Fragment::kElements %
-                  OutputTileIterator::kElementsPerAccess),
-                "Divisibility");
-
-public:
-
-  /// Aspect for when epilogue source is not needed
-  struct SourceAspectNotNeeded
-  {
-    /// Constructor
-    CUTLASS_DEVICE
-    SourceAspectNotNeeded()
-    {}
-
-    /// Invoke the output functor over each vector of output
-    CUTLASS_DEVICE
-    void apply_output_operator(
-      typename OutputTileIterator::Fragment &output_fragment,
-      OutputOp const &output_op,
-      typename AccumulatorFragmentIterator::Fragment const &aligned_accum_fragment)
-    {
-      OutputAccessType *output_frag_ptr =
-        reinterpret_cast<OutputAccessType *>(&output_fragment);
-
-      AccumulatorAccessType const *compute_frag_ptr =
-        reinterpret_cast<AccumulatorAccessType const *>(&aligned_accum_fragment);
-
-      int const kOutputOpIterations =
-        OutputTileIterator::Fragment::kElements / OutputTileIterator::kElementsPerAccess;
-
-      CUTLASS_PRAGMA_UNROLL
-      for (int i = 0; i < kOutputOpIterations; ++i)
-      {
-        // Call the output operator
-        output_frag_ptr[i] = output_op(compute_frag_ptr[i]);
-      }
-    }
-  };
+////////////////////////////////////////////////////////////////////////////////
 
+/// Vector access iterator specialized for vectors, e.g. scale and bias
+/// Thread arrangements are for TensorOps
+///
+template <
+  typename Shape_, 
+  typename WarpShape_, 
+  typename Element_, 
+  int ElementsPerAccess, 
+  bool EnableResidualAccess
+>
+class PredicatedVectorAccessIterator <
+  Shape_,
+  WarpShape_,
+  Element_,
+  layout::PitchLinear,
+  ElementsPerAccess,
+  EnableResidualAccess
+> {
+  public:
 
-  /// Aspect for when epilogue source is needed
-  struct SourceAspectNeeded
-  {
-    OutputTileIterator source_iterator;
-
-    typename OutputTileIterator::Fragment source_fragment;
-
-    /// Invoke the output functor over each vector of output
-    CUTLASS_DEVICE
-    static void apply_output_operator(
-      typename OutputTileIterator::Fragment &output_fragment,
-      OutputOp const &output_op,
-      typename AccumulatorFragmentIterator::Fragment const &aligned_accum_fragment,
-      typename OutputTileIterator::Fragment const &source_fragment)
-    {
-      OutputAccessType *output_frag_ptr =
-        reinterpret_cast<OutputAccessType *>(&output_fragment);
-
-      AccumulatorAccessType const *compute_frag_ptr =
-        reinterpret_cast<AccumulatorAccessType const *>(&aligned_accum_fragment);
-
-      OutputAccessType const *source_frag_ptr =
-        reinterpret_cast<OutputAccessType const *>(&source_fragment);
-
-      int const kOutputOpIterations =
-        OutputTileIterator::Fragment::kElements / OutputTileIterator::kElementsPerAccess;
-
-      CUTLASS_PRAGMA_UNROLL
-      for (int i = 0; i < kOutputOpIterations; ++i)
-      {
-        // Call the output operator
-        output_frag_ptr[i] = output_op(compute_frag_ptr[i], source_frag_ptr[i]);
+  using Shape = Shape_;
+  using WarpShape = WarpShape_;
+  using Element = Element_;
+  using Layout = layout::PitchLinear;
+
+  using Index = typename Layout::Index;
+  using LongIndex = typename Layout::LongIndex;
+
+  using TensorRef = TensorRef<Element, Layout>;
+  using TensorView = TensorView<Element, Layout>;
+  using TensorCoord = typename Layout::TensorCoord;
+
+  using ConstPointer = const Element *;
+  using NonConstPointer = typename platform::remove_const<Element>::type *;
+
+//  static int const kElementsPerAccess = 128 / sizeof_bits<Element>::value;
+  static int const kElementsPerAccess = ElementsPerAccess;
+  static int const kThreads = 32;
+  static int const kRowsPerIteration = 8;
+  static int const kThreadsPerRow = kThreads / kRowsPerIteration;
+  static int const kThreadsPerRowMask = 0x3;
+  static int const kIterations = WarpShape::kContiguous / (kThreadsPerRow * kElementsPerAccess); 
+  static int const kWarpCountStrided = Shape::kStrided / WarpShape::kStrided;
+
+  using AccessType = AlignedArray<Element, kElementsPerAccess>;
+
+ private:
+  /// Internal pointer type permits fast address arithmetic
+  using BytePointer = char *;
+
+ private:
+  //
+  // Data members
+  //
+
+  /// Internal pointer to first access of tile
+  BytePointer pointer_;
+
+  /// Extent of tensor
+  TensorCoord extent_;
+
+  /// pointer offset of each thread
+  TensorCoord thread_offset_;
+
+  /// iteration index
+  LongIndex iteration_;
+
+  /// residual access
+  bool is_residual_;
+
+  /// residual offset of each thread
+  TensorCoord residual_offset_;
+
+ public:
+  /// Constructs a vector access iterator
+  CUTLASS_HOST_DEVICE
+  PredicatedVectorAccessIterator(
+    /// Pointer to the start of the vector
+    ConstPointer pointer,
+    /// Extent of vector
+    TensorCoord extent,
+    /// ID of each participating thread
+    int thread_id,
+    /// ID of each participating warp
+    int warp_id,
+    /// Initial offset of threadblock
+    TensorCoord const &threadblock_offset)
+    : pointer_(reinterpret_cast<BytePointer>(
+                       const_cast<NonConstPointer>(pointer))),
+      extent_(extent),
+      is_residual_(false) {
+
+
+    int warp_offset = (warp_id / kWarpCountStrided) * WarpShape::kContiguous;
+
+    // Per-thread offset in logical coordinates of tensor
+
+    thread_offset_ = threadblock_offset + TensorCoord(warp_offset, 0) +
+        TensorCoord((thread_id & kThreadsPerRowMask) * kElementsPerAccess, 0);
+
+    set_iteration_index(0);
+
+    if(EnableResidualAccess) {
+      // compute residual offset
+      typename TensorCoord::Index residual_size = extent_.contiguous() % WarpShape::kContiguous;
+      if (residual_size) {
+        is_residual_ = true;
+        residual_offset_ = make_Coord(residual_size, 0);
       }
     }
+  }
 
-    /// Constructor
-    CUTLASS_DEVICE
-    SourceAspectNeeded(OutputTileIterator source_iterator) :
-      source_iterator(source_iterator)
-    {
-      source_fragment.clear();
-    }
-
-    /// Invoke the output functor over each vector of output
-    CUTLASS_DEVICE
-    void apply_output_operator(
-      typename OutputTileIterator::Fragment &output_fragment,
-      OutputOp const &output_op,
-      typename AccumulatorFragmentIterator::Fragment const &aligned_accum_fragment)
-    {
-      // Load addend source fragment from global memory
-      source_iterator.load(source_fragment);
-      ++source_iterator;
-
-      apply_output_operator(output_fragment, output_op, aligned_accum_fragment, source_fragment);
-    }
-  };
-
-
-  /// Shared storage allocation needed by the epilogue
-  struct SharedStorage {};
-
-
-public:
+  /// Construct a PredicatedVectorAccessIterator with zero threadblock offset
+  CUTLASS_HOST_DEVICE
+  PredicatedVectorAccessIterator(
+    /// Pointer to start of vector
+    ConstPointer pointer,
+    /// Extent of vector
+    TensorCoord extent,
+    ///< ID of each participating thread
+    int thread_id,
+    /// ID of each participating warp
+    int warp_id)
+    : PredicatedVectorAccessIterator(pointer, extent, thread_id, warp_id,
+                                     make_Coord(0, 0)) {}
+
+
+  /// Overrides the internal iteration index
+  CUTLASS_HOST_DEVICE
+  void set_iteration_index(int index) {
+    iteration_ = index;
+  }
 
-  /// Constructor
+  /// Advances an iterator along logical dimensions of matrix in units of whole tiles
   CUTLASS_DEVICE
-  InterleavedEpilogue(
-      SharedStorage &shared_storage,  ///< Shared storage object
-      int thread_idx,                 ///< ID of a thread within the threadblock
-      int warp_idx,                   ///< ID of warp within threadblock
-      int lane_idx)                   ///< Id of thread within warp
-  :
-      BaseStreamK(thread_idx)
-  {}
+  void add_tile_offset(
+      TensorCoord const &tile_offset) {
 
+    thread_offset_ =
+        thread_offset_ +
+        TensorCoord(WarpShape::kContiguous * tile_offset.contiguous(), 0);
+  }
 
-  /// Aggregates the accumulator sets shared by peer blocks in the global workspace,
-  /// performing epilogue computations, writing to output
-  CUTLASS_DEVICE
-  void reduce(
-      int peer_idx_begin,
-      int peer_idx_end,
-      int reduce_fragment_idx,
-      void *element_workspace,
-      OutputOp const &output_op,                      ///< Output operator
-      OutputTileIterator destination_iterator,        ///< Tile iterator for destination
-      OutputTileIterator source_iterator)             ///< Threadblock tile coordinate in GEMM (in units of threadblock tiles)
-  {
-    // Redcuce peer accumulator fragments into one fragment
-    AccumulatorFragment accum_fragment;
-    BaseStreamK::reduce(accum_fragment, peer_idx_begin, peer_idx_end, reduce_fragment_idx, element_workspace);
-
-    // Source-fragment data (zero-initialized for scenarios where the
-    // output operator allows us to skip loading it from global input)
-    typename OutputTileIterator::Fragment source_fragment;
-    source_fragment.clear();
-
-    if (output_op.is_source_needed())
-    {
-      source_iterator += reduce_fragment_idx;
-      source_iterator.load(source_fragment);
-    }
+  /// Returns a pointer
+  CUTLASS_HOST_DEVICE
+  AccessType *get() const {
+
+    return reinterpret_cast<AccessType *>(
+        pointer_ +
+        ((thread_offset_.contiguous() + iteration_ * kThreadsPerRow * kElementsPerAccess) 
+        * sizeof_bits<Element>::value / 8));
+  }
 
-    // Compute the output result
-    typename OutputTileIterator::Fragment output_fragment;
+  /// Increment and return an instance to self.
+  CUTLASS_HOST_DEVICE
+  PredicatedVectorAccessIterator &operator++() {
+    ++iteration_;
+    if(iteration_ >= kIterations)
+      iteration_ = 0; 
 
-    // Apply the output operator
-    SourceAspectNeeded::apply_output_operator(output_fragment, output_op, accum_fragment, source_fragment);
+    return *this;
+  }
 
-    // Store the final result
-    destination_iterator += reduce_fragment_idx;
-    destination_iterator.store(output_fragment);
+  /// Increment and return an instance to self.
+  CUTLASS_HOST_DEVICE
+  void advance() {
+    if(EnableResidualAccess && is_residual_) {
+      is_residual_ = false;
+      thread_offset_ += residual_offset_; 
+    }
+    else
+      add_tile_offset(TensorCoord(1, 0));
   }
 
+  /// Increment and return an instance to self.
+  CUTLASS_HOST_DEVICE
+  PredicatedVectorAccessIterator operator++(int) {
+    PredicatedVectorAccessIterator self(*this);
+    operator++();
+    return self;
+  }
 
-  /// Perform the epilogue computations and stream the result to global memory.
-  CUTLASS_DEVICE
-  void operator()(
-    OutputOp const &output_op,                      ///< Output operator
-    OutputTileIterator destination_iterator,        ///< Tile iterator for destination
-    AccumulatorTile const &accumulators)            ///< Complete warp-level accumulator tile
-  {
-    operator()(output_op, destination_iterator, accumulators, SourceAspectNotNeeded());
+  /// Returns whether access is valid or not
+  CUTLASS_HOST_DEVICE
+  bool valid() {
+    return ((thread_offset_.contiguous() + 
+              iteration_ * kThreadsPerRow * kElementsPerAccess) < extent_.contiguous());
   }
+};
 
+////////////////////////////////////////////////////////////////////////////////
 
-  /// Perform the epilogue computations and stream the result to global memory.  Implements
-  /// two alternative codepaths, depending on whether the output op requires addend data to be loaded.
-  CUTLASS_DEVICE
-  void operator()(
-    OutputOp const &output_op,                      ///< Output operator
-    OutputTileIterator destination_iterator,        ///< Tile iterator for destination
-    AccumulatorTile const &accumulators,            ///< Complete warp-level accumulator tile
-    OutputTileIterator source_iterator )            ///< Tile iterator for addend source
-  {
-    if (output_op.is_source_needed())
-    {
-      operator()(output_op, destination_iterator, accumulators, SourceAspectNeeded(source_iterator));
-    }
-    else
-    {
-      operator()(output_op, destination_iterator, accumulators, SourceAspectNotNeeded());
-    }
+/// Specialization of PredicatedVectorAccessIterator for row-major data.
+///
+template <
+  typename Shape_,
+  typename WarpShape_,
+  typename Element_,
+  int ElementsPerAccess,
+  bool EnableResidualAccess
+>
+class PredicatedVectorAccessIterator<
+  Shape_,
+  WarpShape_,
+  Element_,
+  layout::RowMajor,
+  ElementsPerAccess,
+  EnableResidualAccess
+> {
+ public:
+
+  using Shape = Shape_;
+  using WarpShape = WarpShape_;
+  using Element = Element_;
+  using Layout = layout::RowMajor;
+
+  using Index = typename Layout::Index;
+  using LongIndex = typename Layout::LongIndex;
+
+  using TensorRef = TensorRef<Element, Layout>;
+  using TensorView = TensorView<Element, Layout>;
+  using TensorCoord = typename Layout::TensorCoord;
+
+  using ConstPointer = const Element *;
+  using NonConstPointer = typename platform::remove_const<Element>::type *;
+
+  using UnderlyingIterator = PredicatedVectorAccessIterator<
+      layout::PitchLinearShape<Shape::kColumn, Shape::kRow>, 
+      layout::PitchLinearShape<WarpShape::kColumn, WarpShape::kRow>, 
+      Element,
+      layout::PitchLinear,
+      ElementsPerAccess,
+      EnableResidualAccess>;
+
+  using AccessType = typename UnderlyingIterator::AccessType;
+  static int const kElementsPerAccess = UnderlyingIterator::kElementsPerAccess;
+  static int const kRowsPerIteration = UnderlyingIterator::kRowsPerIteration;
+  static int const kThreads = UnderlyingIterator::kThreads;
+  static int const kIterations = UnderlyingIterator::kIterations;
+
+ private:
+  //
+  // Data members
+  //
+
+  /// Underlying pitch-linear tile iterator
+  UnderlyingIterator iterator_;
+
+ public:
+  /// Constructs a TileIterator from its precomputed state, threadblock offset,
+  /// and thread ID
+  CUTLASS_HOST_DEVICE
+  PredicatedVectorAccessIterator(
+      ///< Pointer to the start of the vector
+      ConstPointer pointer,
+      ///< Extent of tensor
+      TensorCoord extent,
+      ///< ID of each participating thread
+      int thread_id,
+      ///< ID of each participating warp
+      int warp_id,
+      ///< Initial offset of threadblock
+      TensorCoord const &threadblock_offset)
+      : iterator_(pointer, layout::PitchLinearCoord(extent.column(), extent.row()),
+                  thread_id, warp_id,
+                  layout::PitchLinearCoord(threadblock_offset.column(),
+                                           threadblock_offset.row())) {}
+
+  /// Construct a PredicatedVectorAccessIterator with zero threadblock offset
+  CUTLASS_HOST_DEVICE
+  PredicatedVectorAccessIterator(
+      ConstPointer pointer,   ///< Pointer to the start of the vector
+      TensorCoord extent,     ///< Extent of tensor
+      int thread_id,          ///< ID of each participating thread
+      int warp_id             ///< ID of each participating warp
+      )
+      : PredicatedVectorAccessIterator(pointer, extent, thread_id, warp_id, 
+                                        make_Coord(0, 0)) {}
+
+  /// Overrides the internal iteration index
+  CUTLASS_HOST_DEVICE
+  void set_iteration_index(int index) { iterator_.set_iteration_index(index); }
+
+  /// Advances an iterator along logical dimensions of matrix in units of whole
+  /// tiles
+  CUTLASS_HOST_DEVICE
+  void add_tile_offset(TensorCoord const &tile_offset) {
+    iterator_.add_tile_offset({tile_offset.column(), tile_offset.row()});
   }
 
+  /// Returns a pointer
+  CUTLASS_HOST_DEVICE
+  AccessType *get() const {
+    return reinterpret_cast<AccessType *>(iterator_.get());
+  }
 
-  /// Perform the epilogue computations and stream the result to global memory.  Implements a
-  /// single codepath, regardless of whether the output op requires addend data to be loaded
-  CUTLASS_DEVICE
-  void unified(
-    OutputOp const &output_op,                      ///< Output operator
-    OutputTileIterator destination_iterator,        ///< Tile iterator for destination
-    AccumulatorTile const &accumulators,            ///< Complete warp-level accumulator tile
-    OutputTileIterator source_iterator )            ///< Tile iterator for addend source
-  {
-    if (!output_op.is_source_needed())
-    {
-      source_iterator.clear_mask();
-      __syncthreads();  // Dummy (CUDA 11.0)
-    }
+  /// Advances to the next tile in memory.
+  ///
+  /// The first time this method is called, predicates are updated, and the
+  /// iterator's internal pointer is reverted to the first "steady state" tile.
+  /// Subsequent calls are lightweight and must only update the internal
+  /// pointer.
+  CUTLASS_HOST_DEVICE
+  PredicatedVectorAccessIterator &operator++() {
+    ++iterator_;
+    return *this;
+  }
 
-    operator()(output_op, destination_iterator, accumulators, SourceAspectNeeded(source_iterator));
+  /// Advances to the next tile in memory.
+  ///
+  /// The first time this method is called, predicates are updated, and the
+  /// iterator's internal pointer is reverted to the first "steady state" tile.
+  /// Subsequent calls are lightweight and must only update the internal
+  /// pointer.
+  CUTLASS_HOST_DEVICE
+  PredicatedVectorAccessIterator operator++(int) {
+    PredicatedVectorAccessIterator self(*this);
+    operator++();
+    return self;
   }
 
+  /// Increment and return an instance to self.
+  CUTLASS_HOST_DEVICE
+  void advance() {
+    iterator_.advance();
+  }
 
-  /// Streams the result to global memory
-  template <typename SourceAspect>
-  CUTLASS_DEVICE
-  void operator()(
-    OutputOp const &output_op,                      ///< Output operator
-    OutputTileIterator destination_iterator,        ///< Tile iterator for destination
-    AccumulatorTile const &accumulators,            ///< Complete warp-level accumulator tile
-    SourceAspect source)
-  {
-    //
-    // Iterator over warp-level accumulator fragment
-    //
-
-    AccumulatorFragmentIterator accum_fragment_iterator(accumulators);
-
-    //
-    // Iterate over accumulator tile
-    //
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int iter = 0; iter < OutputTileIterator::kIterations; ++iter) {
-
-      //
-      // Convert fragment
-      //
-
-      typename AccumulatorFragmentIterator::Fragment accum_fragment;
-
-      accum_fragment_iterator.load(accum_fragment);
-      ++accum_fragment_iterator;
-
-      //
-      // Compute the output result
-      //
-
-      typename OutputTileIterator::Fragment output_fragment;
-      source.apply_output_operator(output_fragment, output_op, accum_fragment);
-
-      //
-      // Store the final result
-      //
-
-      destination_iterator.set_iteration_index(iter);
-      destination_iterator.store(output_fragment);
-      ++destination_iterator;
-    }
+  /// Returns whether access is valid or not
+  CUTLASS_HOST_DEVICE
+  bool valid() {
+    return iterator_.valid();
   }
 };
 
+
 ////////////////////////////////////////////////////////////////////////////////
 
-} // namespace threadblock
-} // namespace epilogue
-} // namespace cutlass
+}  // namespace threadblock
+}  // namespace transform 
+}  // namespace cutlass
 
-////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/output_iterator_parameter.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/output_iterator_parameter.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/output_tile_thread_map.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/output_tile_thread_map.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine_layout_params.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine_layout_params.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_blas3.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_blas3.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_direct_conv.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_direct_conv.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_params.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_params.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_predicates.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_predicates.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_strided_dgrad.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_strided_dgrad.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_mixed.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_mixed.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_pitch_liner.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_pitch_liner.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_complex_tensor_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_complex_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_gaussian_complex_tensor_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_gaussian_complex_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_simt.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_simt.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_tensor_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_volta_tensor_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_volta_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_wmma_tensor_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_wmma_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/simt_policy.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/simt_policy.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tensor_op_policy.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tensor_op_policy.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_simt.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_simt.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op_mixed.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op_mixed.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_volta_tensor_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_volta_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_wmma_tensor_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_wmma_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/volta_tensor_op_policy.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/volta_tensor_op_policy.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/wmma_tensor_op_policy.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/wmma_tensor_op_policy.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/fast_math.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/fast_math.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/float8.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/float8.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/floating_point_nvrtc.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/floating_point_nvrtc.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/functional.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/functional.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/base_grouped.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/base_grouped.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/default_gemm_configuration.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/default_gemm_configuration.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/ell_gemm.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/ell_gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_array.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_array.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_batched.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_batched.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_grouped.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma.h`

 * *Files 6% similar despite different names*

```diff
@@ -24,38 +24,37 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/*!
-  \file
-  \brief Device-level grouped GEMM.
+/*! \file
+    \brief Templates exposing architecture support for warp-level multiply-add operations
 */
 
 #pragma once
 
-#include "cutlass/gemm/device/base_grouped.h"
+#include "cutlass/cutlass.h"
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
-namespace device {
+namespace warp {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// GEMM Grouped
-template <typename GemmKernel_>
-class GemmGrouped : public BaseGrouped<GemmKernel_> {
-public:
-  using GemmKernel = GemmKernel_;
+/// Query the number of threads per warp
+template <typename OperatorClass>
+struct WarpSize {
+  static int const value = 32;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-} // namespace device
+} // namespace warp
 } // namespace gemm
 } // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
+
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_layernorm_mainloop_fusion.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_layernorm_mainloop_fusion.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_sparse.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_sparse.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_splitk_parallel.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/symm.h`

 * *Files 18% similar despite different names*

```diff
@@ -25,367 +25,325 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Template for GEMM performing a reduction over K partitions in parallel.
+    \brief Template for a pipelined SYMM and HEMM kernels. Does not compute batching or support split-K.
+
+  
 */
 
 #pragma once
 
-#include "cutlass/cutlass.h"
-#include "cutlass/numeric_types.h"
+#include "cutlass/blas3.h"
 #include "cutlass/arch/arch.h"
 #include "cutlass/device_kernel.h"
 
 #include "cutlass/gemm/threadblock/threadblock_swizzle.h"
-#include "cutlass/gemm/kernel/gemm.h"
+#include "cutlass/gemm/kernel/symm_universal.h"
 
-#include "cutlass/gemm/kernel/default_gemm_splitk_parallel.h"
+#include "cutlass/gemm/kernel/default_symm_universal.h"
 #include "cutlass/gemm/device/default_gemm_configuration.h"
 
-#include "cutlass/epilogue/thread/conversion_op.h"
-#include "cutlass/reduction/kernel/reduce_split_k.h"
-#include "cutlass/reduction/thread/reduction_operators.h"
-
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace device {
 
-////////////////////////////////////////////////////////////////////////////////
-
-/*! 
-  Gemm device-level operator performing parallel reduction over the K partition.
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-*/
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
     typename LayoutA_,
+    /// Side Mode for A (kLeft or kRight)
+    SideMode SideModeA,
+    /// Fill Mode for A (kLower or kUpper)
+    FillMode FillModeA,
     /// Element type for B matrix operand
     typename ElementB_,
     /// Layout type for B matrix operand
     typename LayoutB_,
     /// Element type for C and D matrix operands
     typename ElementC_,
     /// Layout type for C and D matrix operands
     typename LayoutC_,
     /// Element type for internal accumulation
     typename ElementAccumulator_ = ElementC_,
     /// Operator class tag
-    typename OperatorClass_ = arch::OpClassSimt,
-    /// Tag indicating architecture to tune for.  This is the minimum SM that
-      /// supports the intended feature. The device kernel can be built
-      /// targeting any SM larger than this number.
-    typename ArchTag_ = arch::Sm70,
+    typename OperatorClass_ = arch::OpClassTensorOp,
+    /// Tag indicating architecture to tune for
+    typename ArchTag_ = arch::Sm80,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape_ = typename DefaultGemmConfiguration<
         OperatorClass_, ArchTag_, ElementA_, ElementB_, ElementC_,
         ElementAccumulator_>::ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape_ = typename DefaultGemmConfiguration<
         OperatorClass_, ArchTag_, ElementA_, ElementB_, ElementC_,
         ElementAccumulator_>::WarpShape,
     /// Instruction-level tile size (concept: GemmShape)
     typename InstructionShape_ = typename DefaultGemmConfiguration<
         OperatorClass_, ArchTag_, ElementA_, ElementB_, ElementC_,
         ElementAccumulator_>::InstructionShape,
     /// Epilogue output operator
-    typename EpilogueOutputOp_ = typename DefaultGemmConfiguration<
-        OperatorClass_, ArchTag_, ElementA_, ElementB_, ElementC_,
-        ElementAccumulator_>::EpilogueOutputOp,
-    /// Epilogue output operator
-    typename ConvertScaledOp_ = cutlass::epilogue::thread::Convert<
-        ElementAccumulator_,
-        DefaultGemmConfiguration<OperatorClass_, ArchTag_, ElementA_, ElementB_,
-                                 ElementAccumulator_,
-                                 ElementAccumulator_>::EpilogueOutputOp::kCount,
-        ElementAccumulator_>,
-    /// Reduction operator
-    typename ReductionOp_ = cutlass::reduction::thread::ReduceAdd<
-        ElementAccumulator_, typename EpilogueOutputOp_::ElementAccumulator,
-        EpilogueOutputOp_::kCount>,
+    typename EpilogueOutputOp_ = epilogue::thread::LinearCombination<
+      ElementC_,
+      128 / sizeof_bits<ElementC_>::value,
+      ElementAccumulator_,
+      ElementAccumulator_,
+      epilogue::thread::ScaleType::OnlyAlphaScaling
+    >,
     /// Threadblock-level swizzling operator
-    typename ThreadblockSwizzle_ =
-        threadblock::GemmSplitKHorizontalThreadblockSwizzle,
+    typename ThreadblockSwizzle_ = threadblock::GemmIdentityThreadblockSwizzle<>,
     /// Number of stages used in the pipelined mainloop
     int Stages =
         DefaultGemmConfiguration<OperatorClass_, ArchTag_, ElementA_, ElementB_,
                                  ElementC_, ElementAccumulator_>::kStages,
     /// Access granularity of A matrix in units of elements
-    int kAlignmentA =
+    int AlignmentA =
         DefaultGemmConfiguration<OperatorClass_, ArchTag_, ElementA_, ElementB_,
                                  ElementC_, ElementAccumulator_>::kAlignmentA,
     /// Access granularity of B matrix in units of elements
-    int kAlignmentB =
+    int AlignmentB =
         DefaultGemmConfiguration<OperatorClass_, ArchTag_, ElementA_, ElementB_,
                                  ElementC_, ElementAccumulator_>::kAlignmentB,
-    /// Operation performed by GEMM
+    /// If true, kernel supports split-K with serial reduction
+    bool SplitKSerial = false,
+    /// Operation performed by SYMM
     typename Operator_ = typename DefaultGemmConfiguration<
         OperatorClass_, ArchTag_, ElementA_, ElementB_, ElementC_,
-        ElementAccumulator_>::Operator>
-class GemmSplitKParallel {
+        ElementAccumulator_>::Operator,
+    /// Blas3 computation mode (symmetric/hermitian)
+    BlasMode BlasMode_ = BlasMode::kSymmetric>
+class Symm {
  public:
 
   using ElementA = ElementA_;
   using LayoutA = LayoutA_;
+  using ElementAKernel = typename platform::conditional<(SideModeA == SideMode::kRight), ElementB_, ElementA_>::type;
+  using LayoutAKernel = typename platform::conditional<(SideModeA == SideMode::kRight), LayoutB_, LayoutA_>::type;
   using ElementB = ElementB_;
   using LayoutB = LayoutB_;
+  using ElementBKernel = typename platform::conditional<(SideModeA == SideMode::kRight), ElementA_, ElementB_>::type;
+  using LayoutBKernel = typename platform::conditional<(SideModeA == SideMode::kRight), LayoutA_, LayoutB_>::type;
   using ElementC = ElementC_;
   using LayoutC = LayoutC_;
   using ElementAccumulator = ElementAccumulator_;
   using OperatorClass = OperatorClass_;
   using ArchTag = ArchTag_;
   using ThreadblockShape = ThreadblockShape_;
   using WarpShape = WarpShape_;
   using InstructionShape = InstructionShape_;
-  using ConvertScaledOp = ConvertScaledOp_;
   using EpilogueOutputOp = EpilogueOutputOp_;
-  using ReductionOp = ReductionOp_;
   using ThreadblockSwizzle = ThreadblockSwizzle_;
   using Operator = Operator_;
+  static SideMode const kSideModeA = SideModeA;
+  static FillMode const kFillModeA = FillModeA;
   static int const kStages = Stages;
-
-  /// GEMM kernel 
-  using GemmKernel = typename kernel::DefaultGemmSplitKParallel<
-    ElementA,
-    LayoutA,
-    kAlignmentA,
-    ElementB,
-    LayoutB,
-    kAlignmentB,
-    ElementAccumulator,
+  static int const kAlignmentA = AlignmentA;
+  static int const kAlignmentAKernel = (SideModeA == SideMode::kRight) ? AlignmentB : AlignmentA;
+  static int const kAlignmentB = AlignmentB;
+  static int const kAlignmentBKernel = (SideModeA == SideMode::kRight) ? AlignmentA : AlignmentB;
+  static int const kAlignmentC = EpilogueOutputOp::kCount;
+  static bool const kSplitKSerial = SplitKSerial;
+  static BlasMode const kBlasMode = BlasMode_;
+
+  // static asserts for symm update kernel
+  static_assert(platform::is_same<LayoutA, LayoutB>::value,
+    "SYMM update operator support same layouts for operand A and B");
+
+  /// Define the kernel
+  using SymmKernel = typename kernel::DefaultSymmUniversal<
+    ElementAKernel,
+    LayoutAKernel,
+    kSideModeA,
+    kFillModeA,
+    kAlignmentAKernel,
+    ElementBKernel,
+    LayoutBKernel,
+    kAlignmentBKernel,
+    ElementC,
     LayoutC,
     ElementAccumulator,
     OperatorClass,
     ArchTag,
     ThreadblockShape,
     WarpShape,
     InstructionShape,
-    ConvertScaledOp,
+    EpilogueOutputOp,
     ThreadblockSwizzle,
     kStages,
-    Operator
-  >::GemmKernel;
-
-  /// Reduction kernel
-  using ReductionKernel = cutlass::reduction::kernel::ReduceSplitK<
-    cutlass::MatrixShape<4, 32 * EpilogueOutputOp::kCount>,
-    EpilogueOutputOp,
-    ReductionOp
-  >;
-
-  //
-  //
-  //
-
-  /// Argument structure
-  struct Arguments {
-
-    //
-    // Data members
-    //
-
-    GemmCoord problem_size;
-    TensorRef<ElementA const, LayoutA> ref_A;
-    TensorRef<ElementB const, LayoutB> ref_B;
-    TensorRef<ElementC const, LayoutC> ref_C;
-    TensorRef<ElementC, LayoutC> ref_D;
-    typename EpilogueOutputOp::Params epilogue;
-    int split_k_slices;
-    typename ConvertScaledOp::Params convert;
-    typename ReductionOp::Params reduction;
-
-    //
-    // Methods
-    //
-
-    /// Default ctor
-    CUTLASS_HOST_DEVICE
-    Arguments() { }
-
-    /// Constructs an Arguments structure 
-    CUTLASS_HOST_DEVICE
-    Arguments(
-      GemmCoord problem_size_,
-      TensorRef<ElementA const, LayoutA> ref_A_,
-      TensorRef<ElementB const, LayoutB> ref_B_,
-      TensorRef<ElementC const, LayoutC> ref_C_,
-      TensorRef<ElementC, LayoutC> ref_D_,
-      typename EpilogueOutputOp::Params epilogue_ = 
-        typename EpilogueOutputOp::Params(),
-      int split_k_slices = 1,
-      typename ConvertScaledOp::Params convert_ = 
-        typename ConvertScaledOp::Params(),
-      typename ReductionOp::Params reduction_ =
-        typename ReductionOp::Params()
-    ):
-      problem_size(problem_size_),
-      ref_A(ref_A_),
-      ref_B(ref_B_),
-      ref_C(ref_C_),
-      ref_D(ref_D_),
-      epilogue(epilogue_),
-      split_k_slices(split_k_slices),
-      convert(convert_),
-      reduction(reduction_) { }
-  };
+    kSplitKSerial,
+    Operator,
+    kBlasMode
+  >::SymmKernel;
+  
+  using Arguments = typename SymmKernel::Arguments;
 
 private:
 
   /// Kernel parameters object
-  typename GemmKernel::Params gemm_params_;
-
-  /// Reduction kernel parameters object
-  typename ReductionKernel::Params reduction_params_;
-
+  typename SymmKernel::Params params_;
 public:
 
-  /// Constructs the GEMM.
-  GemmSplitKParallel() { }
+  /// Constructs the SYMM.
+  Symm() { }
 
-  /// Determines whether the GEMM can execute the given problem.
+  /// Determines whether the SYMM can execute the given problem.
   static Status can_implement(Arguments const &args) {
 
-    // TODO
+    if (!kSplitKSerial && args.batch_count > 1) {
+      return Status::kErrorInvalidProblem;
+    }
+
+    Status status = SymmKernel::can_implement(args);
+
+    if (SideModeA == SideMode::kInvalid) {
+      return Status::kErrorInvalidProblem;
+    }
+   
+    if (FillModeA != FillMode::kLower && FillModeA != FillMode::kUpper) {
+      return Status::kErrorInvalidProblem;
+    }
+
+    if (status != Status::kSuccess) {
+      return status;
+    }
 
     return Status::kSuccess;
   }
 
   /// Gets the workspace size
   static size_t get_workspace_size(Arguments const &args) {
     
+    size_t bytes = 0;
+
     // Determine grid shape
     ThreadblockSwizzle threadblock_swizzle;
 
-    cutlass::gemm::GemmCoord grid_shape = threadblock_swizzle.get_tiled_shape(
+    cutlass::gemm::GemmCoord tiled_shape = threadblock_swizzle.get_tiled_shape(
       args.problem_size, 
       {ThreadblockShape::kM, ThreadblockShape::kN, ThreadblockShape::kK},
-      args.split_k_slices);
+      args.batch_count);
+    
+    if (kSplitKSerial && args.batch_count > 1) {
 
-    return sizeof(ElementAccumulator_) * size_t(args.problem_size.m()) * size_t(args.problem_size.n()) * grid_shape.k();
-  }
+      bytes += sizeof(int) * size_t(tiled_shape.m()) * size_t(tiled_shape.n());
+    }
 
-  /// Initializes GEMM state from arguments.
-  Status initialize(Arguments const &args, void *workspace) {
+    return bytes;
+  }
 
+  /// Initializes SYMM state from arguments.
+  Status initialize(Arguments const &args, void *workspace = nullptr, cudaStream_t stream = nullptr) {
+    
     // Determine grid shape
     ThreadblockSwizzle threadblock_swizzle;
 
-    cutlass::gemm::GemmCoord grid_shape = threadblock_swizzle.get_tiled_shape(
+    cutlass::gemm::GemmCoord grid_tiled_shape = threadblock_swizzle.get_tiled_shape(
       args.problem_size, 
       {ThreadblockShape::kM, ThreadblockShape::kN, ThreadblockShape::kK},
-      args.split_k_slices);
+      args.batch_count);
 
-    // Define a reference to the workspace - this is an aligned region in device memory.
-    if (!workspace) {
-      return Status::kErrorWorkspaceNull;
+    if (kSplitKSerial) {
+      if (args.batch_count > 1) {
+        if (!workspace) {
+          return Status::kErrorWorkspaceNull;
+        }
+
+        size_t bytes = get_workspace_size(args);
+      
+        cudaError_t result = cudaMemsetAsync(workspace, 0, bytes, stream);
+
+        if (result != cudaSuccess) {
+          return Status::kErrorInternal;
+        }
+      }
+    }
+    else {
+
+      if (args.batch_count > 1) {
+        return Status::kErrorInvalidProblem;
+      }
     }
     
-    TensorRef<ElementAccumulator_, layout::RowMajor> ref_workspace(
-      static_cast<ElementAccumulator_ *>(workspace), 
-      args.problem_size.n());
+    int gemm_k_size = args.problem_size.k();
 
-    int64_t partition_stride = int64_t(args.problem_size.m()) * int64_t(args.problem_size.n());
+   // Swapping argument for A and B, if A was on the right side (problem size doesn't need to change here).
+    if (kSideModeA == SideMode::kRight) {
+      // Initialize the Params structure
+      params_ = typename SymmKernel::Params{
+        args.swapped_matrices(),
+        grid_tiled_shape,
+        gemm_k_size,
+        static_cast<int *>(workspace)
+      };
+
+      return Status::kSuccess;
+    }
 
     // Initialize the Params structure
-    gemm_params_ = typename GemmKernel::Params{
-      args.problem_size,
-      grid_shape,
-      args.ref_A.non_const_ref(),
-      args.ref_B.non_const_ref(),
-      ref_workspace,
-      args.convert,
-      partition_stride
+    params_ = typename SymmKernel::Params{
+      args,
+      grid_tiled_shape,
+      gemm_k_size,
+      static_cast<int *>(workspace)
     };
-
-    reduction_params_ = typename ReductionKernel::Params(
-      args.problem_size.mn(),
-      grid_shape.k(),
-      partition_stride,
-      ref_workspace,
-      args.ref_D,
-      args.ref_C.non_const_ref(),
-      args.epilogue
-    );
-
+    
     return Status::kSuccess;
   }
 
   /// Lightweight update given a subset of arguments
   Status update(Arguments const &args, void *workspace = nullptr) {
+    
+    if (kSplitKSerial && args.batch_count > 1) {  
+      if (!workspace) {
+        return Status::kErrorWorkspaceNull;
+      }
+    }
+
+    size_t workspace_bytes = get_workspace_size(args);
 
-    if (!workspace) {
+    if (workspace_bytes && !workspace) {
       return Status::kErrorWorkspaceNull;
     }
 
-    gemm_params_.ref_A.reset(args.ref_A.data());
-    gemm_params_.ref_B.reset(args.ref_B.data());
-    gemm_params_.ref_D.reset(workspace);     
-
-    reduction_params_.ref_D.reset(args.ref_D.data());
-    reduction_params_.ref_C.reset(args.ref_C.data());
+    params_.update(args, workspace);
 
     return Status::kSuccess;
   }
 
   /// Runs the kernel using initialized state.
   Status run(cudaStream_t stream = nullptr) {
 
-    //
-    // Launch GEMM kernel
-    //
-
     ThreadblockSwizzle threadblock_swizzle;
 
-    dim3 grid = threadblock_swizzle.get_grid_shape(gemm_params_.grid_tiled_shape);
-    dim3 block(GemmKernel::kThreadCount, 1, 1);
+    dim3 grid = threadblock_swizzle.get_grid_shape(params_.grid_tiled_shape);
+    dim3 block(SymmKernel::kThreadCount, 1, 1);
 
-    cudaError_t result;
+    int smem_size = int(sizeof(typename SymmKernel::SharedStorage));
 
-    int smem_size = int(sizeof(typename GemmKernel::SharedStorage));
     if (smem_size >= (48 << 10)) {
-
-      result = cudaFuncSetAttribute(
-        Kernel<GemmKernel>,
-        cudaFuncAttributeMaxDynamicSharedMemorySize,
-        smem_size);
+      cudaError_t result = cudaFuncSetAttribute(Kernel<SymmKernel>,
+                                    cudaFuncAttributeMaxDynamicSharedMemorySize,
+                                    smem_size);
 
       if (result != cudaSuccess) {
         return Status::kErrorInternal;
       }
     }
 
-    Kernel<GemmKernel><<<grid, block, smem_size, stream>>>(gemm_params_);
-
-    result = cudaGetLastError();
-
-    if (result != cudaSuccess) {
-      return Status::kErrorInternal;
-    }
-
-    //
-    // Launch reduction kernel
-    //
-
-    block = ReductionKernel::block_shape();
-    grid = ReductionKernel::grid_shape(gemm_params_.problem_size.mn());
+    cutlass::Kernel<SymmKernel><<<grid, block, smem_size, stream>>>(params_);
 
-    Kernel<ReductionKernel><<< grid, block, 0, stream >>>(reduction_params_);
-
-    result = cudaGetLastError();
-
-    if (result != cudaSuccess) {
-      return Status::kErrorInternal;
-    }
+    cudaError_t result = cudaGetLastError();
 
     return result == cudaSuccess ? Status::kSuccess : Status::kErrorInternal;
   }
 
   /// Runs the kernel using initialized state.
   Status operator()(cudaStream_t stream = nullptr) {
     return run(stream);
@@ -402,202 +360,208 @@
     if (status == Status::kSuccess) {
       status = run(stream);
     }
 
     return status;
   }
 };
-
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for column-major output
+/********************************************************************************************************
+  SYMM/HEMM has 4 combinations based on Layouts {RowMajor, ColumnMajor} x Side mode {LeftSide, RightSide}
+  In templates and arguments to cutlass kernel, `matrix A` is always symmetric/hermitian, and `matrix B` is rectangular. 
+  (adhering to the cuBLAS convention)
+
+  Although, cuBLAS SYMM/HEMM only supports ColumnMajor layouts for all matrices (A, B, C/D).
+
+  For the mainloop and symm kernel, `A` and `B` points to left-side and right-side matrices, respectively.
+  
+  Thus, for LeftSide mode `A` and `B` points to `matrix A` and `matrix B`, respectively. While for 
+  the RightSide mode `A` and `B` points to `matrix B` and `matrix A`, respectively. 
+  
+  Additionally, CUTLASS GEMM epilogue is always RowMajor, and ColumnMajor output is achieved by 
+  transposing the GEMM problem. Thus, ColumnMajor output layout for SYMM/HEMM requires:
+   - Transposing `matrix A` and `matrix B` layouts
+   - Swapping problem size m and n values
+   - Swapping LeftSide and RightSide mode
+  
+  RowMajor output:    D = matrix A x matrix B
+  ColumnMajor output: D = matrix A x matrix B -> Transpose (D) = Transpose(matrix B) x Transpose(matrix A)
+
+  {RowMajor, ColumnMajor} x Side Mode {LeftSide, RightSide} 4 cases:
+    1.  LeftSide mode and RowMajor output (default template)
+    2.  LeftSide mode and ColumnMajor output 
+    3.  RightSide mode and RowMajor output
+    4.  RightSide mode and ColumnMajor output
+  
+  Mapping ColumnMajor output layout cases 2 and 4 to RowMajor efficient epilogue implementation:
+  
+  Case 2 -> Case 3:
+      D_col = matrix A x matrix B (LeftSide mode) 
+   => Transpose(D_col) = Transpose(matrix B) x Transpose(matrix A) (RightSide mode)
+
+  swap pointers for `A` and `B` call GEMM mainloop with RowMajor efficient-epilogue
+
+  Case 4 -> Case 1:
+      D_col = matrix B x matrix A (RightSide mode) 
+   => Transpose(D_col) = Transpose(matrix A) x Transpose(matrix B) (LeftSide mode)
+
+   call GEMM mainloop for with RowMajor efficient-epilogue
+********************************************************************************************************/
+
+/// Parital specialization for column-major output exchanges problem size and operand.
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
     typename LayoutA_,
+    /// Side Mode for A (kLeft or kRight)
+    SideMode SideModeA,
+    /// Fill Mode for A (kLower or kUpper)
+    FillMode FillModeA,
     /// Element type for B matrix operand
     typename ElementB_,
     /// Layout type for B matrix operand
     typename LayoutB_,
     /// Element type for C and D matrix operands
     typename ElementC_,
     /// Element type for internal accumulation
     typename ElementAccumulator_,
     /// Operator class tag
     typename OperatorClass_,
     /// Tag indicating architecture to tune for.  This is the minimum SM that
-      /// supports the intended feature. The device kernel can be built
-      /// targeting any SM larger than this number.
+    /// supports the intended feature. The device kernel can be built
+    /// targeting any SM larger than this number.
     typename ArchTag_,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape_,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape_,
     /// Instruction-level tile size (concept: GemmShape)
     typename InstructionShape_,
     /// Epilogue output operator
     typename EpilogueOutputOp_,
-    /// Epilogue output operator
-    typename ConvertScaledOp_,
-    /// Reduction operator
-    typename ReductionOp_,
     /// Threadblock-level swizzling operator
     typename ThreadblockSwizzle_,
     /// Number of stages used in the pipelined mainloop
-    int Stages, int kAlignmentA, int kAlignmentB,
-    /// Operation performed by GEMM
-    typename Operator_>
-class GemmSplitKParallel<ElementA_, LayoutA_, ElementB_, LayoutB_, ElementC_,
-                         layout::ColumnMajor, ElementAccumulator_,
-                         OperatorClass_, ArchTag_, ThreadblockShape_,
-                         WarpShape_, InstructionShape_, EpilogueOutputOp_,
-                         ConvertScaledOp_, ReductionOp_, ThreadblockSwizzle_,
-                         Stages, kAlignmentA, kAlignmentB, Operator_> {
+    int Stages,
+    /// Access granularity of A matrix in units of elements
+    int AlignmentA,
+    /// Access granularity of B matrix in units of elements
+    int AlignmentB,
+    /// If true, kernel supports split-K with serial reduction
+    bool SplitKSerial,
+    /// Operation performed by Symm update kernel
+    typename Operator_,
+    /// Blas3 computation mode (symmetric/hermitian)
+    BlasMode BlasMode_
+    >
+class Symm<ElementA_, LayoutA_, SideModeA, FillModeA, ElementB_, LayoutB_, ElementC_,
+           layout::ColumnMajor,  // partially specialized on LayoutC
+           ElementAccumulator_, OperatorClass_, ArchTag_, ThreadblockShape_,
+           WarpShape_, InstructionShape_, EpilogueOutputOp_,
+           ThreadblockSwizzle_, Stages, AlignmentA, AlignmentB,
+           SplitKSerial, Operator_, BlasMode_> {
  public:
 
   using ElementA = ElementA_;
   using LayoutA = LayoutA_;
   using ElementB = ElementB_;
   using LayoutB = LayoutB_;
   using ElementC = ElementC_;
   using LayoutC = layout::ColumnMajor;
   using ElementAccumulator = ElementAccumulator_;
   using OperatorClass = OperatorClass_;
   using ArchTag = ArchTag_;
   using ThreadblockShape = ThreadblockShape_;
   using WarpShape = WarpShape_;
   using InstructionShape = InstructionShape_;
-  using ConvertScaledOp = ConvertScaledOp_;
   using EpilogueOutputOp = EpilogueOutputOp_;
-  using ReductionOp = ReductionOp_;
   using ThreadblockSwizzle = ThreadblockSwizzle_;
   using Operator = Operator_;
+  static SideMode const kSideModeA = SideModeA;
+  static FillMode const kFillModeA = FillModeA;
   static int const kStages = Stages;
-
-  using UnderlyingOperator = GemmSplitKParallel< 
-    ElementB,
-    typename layout::LayoutTranspose<LayoutB>::type,
+  static int const kAlignmentA = AlignmentA;
+  static int const kAlignmentB = AlignmentB;
+  static int const kAlignmentC = EpilogueOutputOp::kCount;
+  static bool const kSplitKSerial = SplitKSerial;
+  static BlasMode const kBlasMode = BlasMode_;
+  
+  /// Define the kernel
+  using UnderlyingOperator = typename cutlass::gemm::device::Symm<
     ElementA,
     typename layout::LayoutTranspose<LayoutA>::type,
+    InvertSideMode<kSideModeA>::mode,
+    InvertFillMode<kFillModeA>::mode,
+    ElementB,
+    typename layout::LayoutTranspose<LayoutB>::type, 
     ElementC,
-    layout::RowMajor,    
+    layout::RowMajor,
     ElementAccumulator,
     OperatorClass,
     ArchTag,
     ThreadblockShape,
     WarpShape,
     InstructionShape,
     EpilogueOutputOp,
-    ConvertScaledOp,
-    ReductionOp,
     ThreadblockSwizzle,
-    Stages,
+    kStages,
     kAlignmentA,
     kAlignmentB,
-    Operator
+    kSplitKSerial,
+    Operator,
+    kBlasMode
   >;
-
-  using UnderlyingArguments = typename UnderlyingOperator::Arguments;
-  using GemmKernel = typename UnderlyingOperator::GemmKernel;
-  using ReductionKernel = typename UnderlyingOperator::ReductionKernel;
+  
 
   /// Argument structure
-  struct Arguments {
-
-    //
-    // Data members
-    //
-
-    GemmCoord problem_size;
-    TensorRef<ElementA const, LayoutA> ref_A;
-    TensorRef<ElementB const, LayoutB> ref_B;
-    TensorRef<ElementC const, LayoutC> ref_C;
-    TensorRef<ElementC, LayoutC> ref_D;
-    typename EpilogueOutputOp::Params epilogue;
-    int split_k_slices;
-    typename ConvertScaledOp::Params convert;
-    typename ReductionOp::Params reduction;
-
-    //
-    // Methods
-    //
-
-    /// Default ctor
-    CUTLASS_HOST_DEVICE
-    Arguments() { }
-
-    /// Constructs an Arguments structure 
-    CUTLASS_HOST_DEVICE
-    Arguments(
-      GemmCoord problem_size_,
-      TensorRef<ElementA const, LayoutA> ref_A_,
-      TensorRef<ElementB const, LayoutB> ref_B_,
-      TensorRef<ElementC const, LayoutC> ref_C_,
-      TensorRef<ElementC, LayoutC> ref_D_,
-      typename EpilogueOutputOp::Params epilogue_ = 
-        typename EpilogueOutputOp::Params(),
-      int split_k_slices = 1,
-      typename ConvertScaledOp::Params convert_ = 
-        typename ConvertScaledOp::Params(),
-      typename ReductionOp::Params reduction_ =
-        typename ReductionOp::Params()
-    ):
-      problem_size(problem_size_),
-      ref_A(ref_A_),
-      ref_B(ref_B_),
-      ref_C(ref_C_),
-      ref_D(ref_D_),
-      epilogue(epilogue_),
-      split_k_slices(split_k_slices),
-      convert(convert_),
-      reduction(reduction_) { }
-  };
+  using Arguments = typename UnderlyingOperator::Arguments;
+  using SymmKernel = typename UnderlyingOperator::SymmKernel;
 
 private:
 
-  /// Kernel parameters object
   UnderlyingOperator underlying_operator_;
 
 public:
 
-  /// Constructs the GEMM.
-  GemmSplitKParallel() { }
+  /// Constructs the Symm.
+  Symm() { }
 
-  /// Helper to construct a transposed equivalent for the underying GEMM operator
-  static UnderlyingArguments to_underlying_arguments(Arguments const &args) {
-    return UnderlyingArguments(
-      {args.problem_size.n(), args.problem_size.m(), args.problem_size.k()},
-      {args.ref_B.data(), args.ref_B.stride(0)},
-      {args.ref_A.data(), args.ref_A.stride(0)},
-      {args.ref_C.data(), args.ref_C.stride(0)},
-      {args.ref_D.data(), args.ref_D.stride(0)},
-      args.epilogue,
-      args.split_k_slices,
-      args.convert,
-      args.reduction
-    );
+  /// Helper to construct a transposed equivalent for the underying SYMM operator
+  static Arguments to_underlying_arguments(Arguments const &args) {
+    return args.transposed_problem_size();
   }
 
-  /// Determines whether the GEMM can execute the given problem.
+  /// Determines whether the Symm can execute the given problem.
   static Status can_implement(Arguments const &args) {
 
     return UnderlyingOperator::can_implement(to_underlying_arguments(args));
   }
 
   /// Gets the workspace size
   static size_t get_workspace_size(Arguments const &args) {
     
     return UnderlyingOperator::get_workspace_size(to_underlying_arguments(args));
   }
 
-  /// Initializes GEMM state from arguments.
-  Status initialize(Arguments const &args, void *workspace) {
+  /// Computes the grid shape
+  static dim3 get_grid_shape(Arguments const &args) { 
+    return UnderlyingOperator::get_grid_shape(to_underlying_arguments(args));
+  }
+
+  /// Computes the maximum number of active blocks per multiprocessor
+  static int maximum_active_blocks(int smem_capacity = -1) {
+    return UnderlyingOperator::maximum_active_blocks(smem_capacity);
+  }
+
+  /// Initializes Symm state from arguments.
+  Status initialize(Arguments const &args, void *workspace = nullptr, cudaStream_t stream = nullptr) {
 
-    return underlying_operator_.initialize(to_underlying_arguments(args), workspace);
+    return underlying_operator_.initialize(to_underlying_arguments(args), workspace, stream);
   }
 
   /// Lightweight update given a subset of arguments
   Status update(Arguments const &args, void *workspace = nullptr) {
 
     return underlying_operator_.update(to_underlying_arguments(args), workspace);
   }
@@ -628,11 +592,11 @@
     return status;
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
 } // namespace device
-} // namespace gemm
+} // namespace Symm
 } // namespace cutlass
 
 ////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_adapter.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_adapter.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_base.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_base.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_with_broadcast.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_with_broadcast.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_with_k_reduction.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_with_k_reduction.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemv.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemv.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_2k.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_2k.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_2k_grouped.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_2k_grouped.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_k.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_k.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/device/symm.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/trmm.h`

 * *Files 19% similar despite different names*

```diff
@@ -25,48 +25,194 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Template for a pipelined SYMM and HEMM kernels. Does not compute batching or support split-K.
+    \brief Template for a TRMM kernel. Does not compute batching or support split-K.
 
   
 */
 
 #pragma once
 
 #include "cutlass/blas3.h"
 #include "cutlass/arch/arch.h"
 #include "cutlass/device_kernel.h"
 
 #include "cutlass/gemm/threadblock/threadblock_swizzle.h"
-#include "cutlass/gemm/kernel/symm_universal.h"
+#include "cutlass/gemm/kernel/trmm_universal.h"
 
-#include "cutlass/gemm/kernel/default_symm_universal.h"
+#include "cutlass/gemm/kernel/default_trmm_universal.h"
 #include "cutlass/gemm/device/default_gemm_configuration.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace device {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
+/*! Trmm device-level operator. This is an interface to efficient CUTLASS TRMM kernels that may
+  be invoked from host code.
+
+  The contributions of this class are:
+    
+    1. At compile time, it maps data types and high-level structural parameters onto 
+       specific CUTLASS components.
+
+    2. At runtime, it maps logical arguments to TRMM problems to kernel parameters.
+
+    3. At runtime, it launches kernels on the device.
+
+  The intent is to provide a convenient mechanism for interacting with most plausible TRMM
+  configurations for each supported architecture. Consequently, not all parameters are exposed
+  to the top-level interface. Rather, sensible defaults at each level of the CUTLASS hierarchy
+  are selected to tradeoff simplicity of the interface with flexibility. We expect 
+  most configurations to be specified at this level. Applications with more exotic requirements 
+  may construct their kernels of interest using CUTLASS components at the threadblock, warp, 
+  and thread levels of abstraction.
+
+  CUTLASS exposes computations using the functor design pattern in which objects compose some
+  internal state with an overloaded function call operator. This enables decoupling of
+  initialization from execution, possibly reducing overhead during steady state phases of
+  application execution.
+
+  CUTLASS device-level operators expose an Arguments structure encompassing each logical
+  input to the computation. This is distinct from the kernel-level Params structure pattern
+  which contains application-specific precomputed state needed by the device code.
+
+  Example of a CUTLASS TRMM operator implementing the functionality of cuBLAS's STRMM NN
+  is as follows:
+
+    //
+    // Instantiate the CUTLASS TRMM operator.
+    //
+
+    cutlass::gemm::device::Trmm<
+      float,
+      cutlass::layout::ColumnMajor,
+      cutlass::SideMode::kLeft,
+      cutlass::FillMode::kLower,
+      cutlass::DiagType::kNonUnit,
+      float,
+      cutlass::layout::ColumnMajor,
+      float,
+      cutlass::layout::ColumnMajor,
+    > trmm_op;
+
+    //
+    // Launch the TRMM operation on the device
+    //
+
+    cutlass::Status status = trmm_op({
+      cutlass::gemm::GemmUniversalMode,   // Trmm Problem Mode
+      {m, n, m/n},                        // GemmCoord problem_size (k is based on left- or right-side mode)
+      batch_count,
+      {alpha},                            // EpilogueOutputOp::Params epilogue_op_params
+      void const * ptr_A,
+      void const * ptr_B,
+      void const * ptr_C,
+      int64_t batch_stride_A,
+      int64_t batch_stride_B,
+      int64_t batch_stride_C,
+      int lda,
+      int ldb,
+      int ldc
+    });
+
+  A simplified view of the template is listed below.
+
+    template <
+      /// Element type for A matrix operand
+      typename ElementA,
+      
+      /// Layout type for A matrix operand
+      typename LayoutA,
+      
+      /// Side Mode for A (kLeft or kRight)
+      SideMode SideModeA,
+
+      /// Fill Mode for A (kLower or kUpper)
+      FillMode FillModeA,
+
+      /// DiagType for A (kNonUnit or kUnit)
+      DiagType DiagTypeA,
+
+      /// Element type for B matrix operand
+      typename ElementB,
+      
+      /// Layout type for B matrix operand
+      typename LayoutB,
+      
+      /// Element type for C and D matrix operands
+      typename ElementC,
+      
+      /// Layout type for C and D matrix operands
+      typename LayoutC,
+      
+      /// Element type for internal accumulation
+      typename ElementAccumulator,
+
+      /// Operator class tag
+      typename OperatorClass,
+      
+      /// Tag indicating architecture to tune for.  This is the minimum SM that
+      /// supports the intended feature. The device kernel can be built
+      /// targeting any SM larger than this number.
+      typename ArchTag,
+      
+      /// Threadblock-level tile size (concept: GemmShape)
+      typename ThreadblockShape,
+      
+      /// Warp-level tile size (concept: GemmShape)
+      typename WarpShape,
+      
+      /// Warp-level tile size (concept: GemmShape)
+      typename InstructionShape,
+      
+      /// Epilogue output operator
+      typename EpilogueOutputOp,
+      
+      /// Threadblock-level swizzling operator
+      typename ThreadblockSwizzle,
+      
+      /// Number of stages used in the pipelined mainloop
+      int Stages,
+
+      /// Access granularity of A matrix in units of elements
+      int AlignmentA,
+
+      /// Access granularity of B matrix in units of elements
+      int AlignmentB,
+
+      /// If true, kernel supports split-K with serial reduction
+      bool SplitKSerial,
+
+      /// Operation performed by TRMM
+      typename Operator,
+
+      /// Complex elementwise transformation on A operand
+      ComplexTransform TransformA
+    >
+    class Trmm;
+*/
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
     typename LayoutA_,
-    /// Side Mode for A (kLeft or kRight)
+    /// Side Mode for A 
     SideMode SideModeA,
-    /// Fill Mode for A (kLower or kUpper)
+    /// Fill Mode for A
     FillMode FillModeA,
+    /// DiagType for A
+    DiagType DiagTypeA,
     /// Element type for B matrix operand
     typename ElementB_,
     /// Layout type for B matrix operand
     typename LayoutB_,
     /// Element type for C and D matrix operands
     typename ElementC_,
     /// Layout type for C and D matrix operands
@@ -109,108 +255,120 @@
                                  ElementC_, ElementAccumulator_>::kAlignmentA,
     /// Access granularity of B matrix in units of elements
     int AlignmentB =
         DefaultGemmConfiguration<OperatorClass_, ArchTag_, ElementA_, ElementB_,
                                  ElementC_, ElementAccumulator_>::kAlignmentB,
     /// If true, kernel supports split-K with serial reduction
     bool SplitKSerial = false,
-    /// Operation performed by SYMM
+    /// Operation performed by TRMM
     typename Operator_ = typename DefaultGemmConfiguration<
         OperatorClass_, ArchTag_, ElementA_, ElementB_, ElementC_,
         ElementAccumulator_>::Operator,
-    /// Blas3 computation mode (symmetric/hermitian)
-    BlasMode BlasMode_ = BlasMode::kSymmetric>
-class Symm {
+    /// Complex elementwise transformation on A operand
+    ComplexTransform TransformA = ComplexTransform::kNone>
+class Trmm {
  public:
-
   using ElementA = ElementA_;
   using LayoutA = LayoutA_;
+  using TensorRefA = TensorRef<ElementA const, LayoutA>;
   using ElementAKernel = typename platform::conditional<(SideModeA == SideMode::kRight), ElementB_, ElementA_>::type;
   using LayoutAKernel = typename platform::conditional<(SideModeA == SideMode::kRight), LayoutB_, LayoutA_>::type;
   using ElementB = ElementB_;
   using LayoutB = LayoutB_;
+  using TensorRefB = TensorRef<ElementB const, LayoutB>;
   using ElementBKernel = typename platform::conditional<(SideModeA == SideMode::kRight), ElementA_, ElementB_>::type;
   using LayoutBKernel = typename platform::conditional<(SideModeA == SideMode::kRight), LayoutA_, LayoutB_>::type;
   using ElementC = ElementC_;
   using LayoutC = LayoutC_;
+  using TensorRefC = TensorRef<ElementC const, LayoutC>;
+  using TensorRefD = TensorRef<ElementC, LayoutC>;
   using ElementAccumulator = ElementAccumulator_;
   using OperatorClass = OperatorClass_;
   using ArchTag = ArchTag_;
   using ThreadblockShape = ThreadblockShape_;
   using WarpShape = WarpShape_;
   using InstructionShape = InstructionShape_;
   using EpilogueOutputOp = EpilogueOutputOp_;
   using ThreadblockSwizzle = ThreadblockSwizzle_;
   using Operator = Operator_;
-  static SideMode const kSideModeA = SideModeA;
-  static FillMode const kFillModeA = FillModeA;
+  static SideMode const kSideMode = SideModeA;
+  static FillMode const kFillMode = FillModeA;
+  static DiagType const kDiagType = DiagTypeA;
   static int const kStages = Stages;
   static int const kAlignmentA = AlignmentA;
   static int const kAlignmentAKernel = (SideModeA == SideMode::kRight) ? AlignmentB : AlignmentA;
   static int const kAlignmentB = AlignmentB;
   static int const kAlignmentBKernel = (SideModeA == SideMode::kRight) ? AlignmentA : AlignmentB;
   static int const kAlignmentC = EpilogueOutputOp::kCount;
   static bool const kSplitKSerial = SplitKSerial;
-  static BlasMode const kBlasMode = BlasMode_;
-
-  // static asserts for symm update kernel
-  static_assert(platform::is_same<LayoutA, LayoutB>::value,
-    "SYMM update operator support same layouts for operand A and B");
+  // Complex Transform don't appply to B
+  static ComplexTransform const kTransformA = TransformA; 
+  static ComplexTransform const kTransformB = ComplexTransform::kNone; 
+  static ComplexTransform const kTransformAKernel = (SideModeA == SideMode::kRight) ? 
+                                              ComplexTransform::kNone : TransformA;
+  static ComplexTransform const kTransformBKernel = (SideModeA == SideMode::kRight) ? 
+                                              TransformA : ComplexTransform::kNone;
 
   /// Define the kernel
-  using SymmKernel = typename kernel::DefaultSymmUniversal<
+  using TrmmKernel = typename kernel::DefaultTrmmUniversal<
     ElementAKernel,
     LayoutAKernel,
-    kSideModeA,
-    kFillModeA,
+    kTransformAKernel,
     kAlignmentAKernel,
     ElementBKernel,
     LayoutBKernel,
+    kTransformBKernel,
     kAlignmentBKernel,
+    kSideMode,
+    kFillMode,
+    kDiagType,
     ElementC,
     LayoutC,
     ElementAccumulator,
     OperatorClass,
     ArchTag,
     ThreadblockShape,
     WarpShape,
     InstructionShape,
     EpilogueOutputOp,
     ThreadblockSwizzle,
     kStages,
     kSplitKSerial,
-    Operator,
-    kBlasMode
-  >::SymmKernel;
+    Operator
+  >::TrmmKernel;
   
-  using Arguments = typename SymmKernel::Arguments;
+  using Arguments = typename TrmmKernel::Arguments;
 
 private:
 
   /// Kernel parameters object
-  typename SymmKernel::Params params_;
+  typename TrmmKernel::Params params_;
 public:
 
-  /// Constructs the SYMM.
-  Symm() { }
+  /// Constructs the TRMM.
+  Trmm() { }
 
-  /// Determines whether the SYMM can execute the given problem.
+  /// Determines whether the TRMM can execute the given problem.
   static Status can_implement(Arguments const &args) {
 
     if (!kSplitKSerial && args.batch_count > 1) {
       return Status::kErrorInvalidProblem;
     }
 
-    Status status = SymmKernel::can_implement(args);
-
+    Status status = TrmmKernel::can_implement(args);
+   
     if (SideModeA == SideMode::kInvalid) {
       return Status::kErrorInvalidProblem;
     }
-   
-    if (FillModeA != FillMode::kLower && FillModeA != FillMode::kUpper) {
+
+    if (FillModeA == FillMode::kInvalid) {
+      return Status::kErrorInvalidProblem;
+    }
+
+    if (DiagTypeA == DiagType::kInvalid) {
       return Status::kErrorInvalidProblem;
     }
 
     if (status != Status::kSuccess) {
       return status;
     }
 
@@ -234,17 +392,17 @@
 
       bytes += sizeof(int) * size_t(tiled_shape.m()) * size_t(tiled_shape.n());
     }
 
     return bytes;
   }
 
-  /// Initializes SYMM state from arguments.
+  /// Initializes TRMM state from arguments.
   Status initialize(Arguments const &args, void *workspace = nullptr, cudaStream_t stream = nullptr) {
-    
+ 
     // Determine grid shape
     ThreadblockSwizzle threadblock_swizzle;
 
     cutlass::gemm::GemmCoord grid_tiled_shape = threadblock_swizzle.get_tiled_shape(
       args.problem_size, 
       {ThreadblockShape::kM, ThreadblockShape::kN, ThreadblockShape::kK},
       args.batch_count);
@@ -270,28 +428,28 @@
         return Status::kErrorInvalidProblem;
       }
     }
     
     int gemm_k_size = args.problem_size.k();
 
    // Swapping argument for A and B, if A was on the right side (problem size doesn't need to change here).
-    if (kSideModeA == SideMode::kRight) {
+    if (kSideMode == SideMode::kRight) {
       // Initialize the Params structure
-      params_ = typename SymmKernel::Params{
+      params_ = typename TrmmKernel::Params{
         args.swapped_matrices(),
         grid_tiled_shape,
         gemm_k_size,
         static_cast<int *>(workspace)
       };
 
       return Status::kSuccess;
     }
 
     // Initialize the Params structure
-    params_ = typename SymmKernel::Params{
+    params_ = typename TrmmKernel::Params{
       args,
       grid_tiled_shape,
       gemm_k_size,
       static_cast<int *>(workspace)
     };
     
     return Status::kSuccess;
@@ -319,29 +477,29 @@
 
   /// Runs the kernel using initialized state.
   Status run(cudaStream_t stream = nullptr) {
 
     ThreadblockSwizzle threadblock_swizzle;
 
     dim3 grid = threadblock_swizzle.get_grid_shape(params_.grid_tiled_shape);
-    dim3 block(SymmKernel::kThreadCount, 1, 1);
-
-    int smem_size = int(sizeof(typename SymmKernel::SharedStorage));
+    dim3 block(TrmmKernel::kThreadCount, 1, 1);
 
+    int smem_size = int(sizeof(typename TrmmKernel::SharedStorage));
+    
     if (smem_size >= (48 << 10)) {
-      cudaError_t result = cudaFuncSetAttribute(Kernel<SymmKernel>,
+      cudaError_t result = cudaFuncSetAttribute(Kernel<TrmmKernel>,
                                     cudaFuncAttributeMaxDynamicSharedMemorySize,
                                     smem_size);
 
       if (result != cudaSuccess) {
         return Status::kErrorInternal;
       }
     }
 
-    cutlass::Kernel<SymmKernel><<<grid, block, smem_size, stream>>>(params_);
+    cutlass::Kernel<TrmmKernel><<<grid, block, smem_size, stream>>>(params_);
 
     cudaError_t result = cudaGetLastError();
 
     return result == cudaSuccess ? Status::kSuccess : Status::kErrorInternal;
   }
 
   /// Runs the kernel using initialized state.
@@ -360,30 +518,27 @@
     if (status == Status::kSuccess) {
       status = run(stream);
     }
 
     return status;
   }
 };
-////////////////////////////////////////////////////////////////////////////////
 
 /********************************************************************************************************
-  SYMM/HEMM has 4 combinations based on Layouts {RowMajor, ColumnMajor} x Side mode {LeftSide, RightSide}
-  In templates and arguments to cutlass kernel, `matrix A` is always symmetric/hermitian, and `matrix B` is rectangular. 
+  TRMM has 4 combinations based on Layouts {RowMajor, ColumnMajor} x Side mode {LeftSide, RightSide}
+  In templates and arguments to cutlass kernel, `matrix A` is always triangular, and `matrix B` is rectangular. 
   (adhering to the cuBLAS convention)
 
-  Although, cuBLAS SYMM/HEMM only supports ColumnMajor layouts for all matrices (A, B, C/D).
-
-  For the mainloop and symm kernel, `A` and `B` points to left-side and right-side matrices, respectively.
+For the mainloop and trmm kernel, `A` and `B` points to left-side and right-side matrices, respectively.
   
   Thus, for LeftSide mode `A` and `B` points to `matrix A` and `matrix B`, respectively. While for 
   the RightSide mode `A` and `B` points to `matrix B` and `matrix A`, respectively. 
   
   Additionally, CUTLASS GEMM epilogue is always RowMajor, and ColumnMajor output is achieved by 
-  transposing the GEMM problem. Thus, ColumnMajor output layout for SYMM/HEMM requires:
+  transposing the GEMM problem. Thus, ColumnMajor output layout for TRMM requires:
    - Transposing `matrix A` and `matrix B` layouts
    - Swapping problem size m and n values
    - Swapping LeftSide and RightSide mode
   
   RowMajor output:    D = matrix A x matrix B
   ColumnMajor output: D = matrix A x matrix B -> Transpose (D) = Transpose(matrix B) x Transpose(matrix A)
 
@@ -410,31 +565,31 @@
 
 /// Parital specialization for column-major output exchanges problem size and operand.
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
     typename LayoutA_,
-    /// Side Mode for A (kLeft or kRight)
+    /// Side Mode for A 
     SideMode SideModeA,
-    /// Fill Mode for A (kLower or kUpper)
+    /// Fill Mode for A
     FillMode FillModeA,
+    /// DiagType for A
+    DiagType DiagTypeA,
     /// Element type for B matrix operand
     typename ElementB_,
     /// Layout type for B matrix operand
     typename LayoutB_,
     /// Element type for C and D matrix operands
     typename ElementC_,
     /// Element type for internal accumulation
     typename ElementAccumulator_,
     /// Operator class tag
     typename OperatorClass_,
-    /// Tag indicating architecture to tune for.  This is the minimum SM that
-    /// supports the intended feature. The device kernel can be built
-    /// targeting any SM larger than this number.
+    /// Tag indicating architecture to tune for
     typename ArchTag_,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape_,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape_,
     /// Instruction-level tile size (concept: GemmShape)
     typename InstructionShape_,
@@ -444,59 +599,71 @@
     typename ThreadblockSwizzle_,
     /// Number of stages used in the pipelined mainloop
     int Stages,
     /// Access granularity of A matrix in units of elements
     int AlignmentA,
     /// Access granularity of B matrix in units of elements
     int AlignmentB,
-    /// If true, kernel supports split-K with serial reduction
+    /// If true, kernel supports split-K as a serial reduction
     bool SplitKSerial,
-    /// Operation performed by Symm update kernel
+    /// Operation performed by TRMM
     typename Operator_,
-    /// Blas3 computation mode (symmetric/hermitian)
-    BlasMode BlasMode_
-    >
-class Symm<ElementA_, LayoutA_, SideModeA, FillModeA, ElementB_, LayoutB_, ElementC_,
+    /// Complex elementwise transformation on A operand
+    ComplexTransform TransformA>
+class Trmm<ElementA_, LayoutA_, SideModeA, FillModeA, DiagTypeA,
+           ElementB_, LayoutB_, ElementC_,
            layout::ColumnMajor,  // partially specialized on LayoutC
            ElementAccumulator_, OperatorClass_, ArchTag_, ThreadblockShape_,
            WarpShape_, InstructionShape_, EpilogueOutputOp_,
-           ThreadblockSwizzle_, Stages, AlignmentA, AlignmentB,
-           SplitKSerial, Operator_, BlasMode_> {
+           ThreadblockSwizzle_, Stages, AlignmentA, AlignmentB, SplitKSerial,
+           Operator_, TransformA> {
  public:
 
   using ElementA = ElementA_;
-  using LayoutA = LayoutA_;
+  using LayoutA = LayoutA_; 
+  using TensorRefA = TensorRef<ElementA const, LayoutA>;
   using ElementB = ElementB_;
   using LayoutB = LayoutB_;
+  using TensorRefB = TensorRef<ElementB const, LayoutB>;
   using ElementC = ElementC_;
   using LayoutC = layout::ColumnMajor;
+  using TensorRefC = TensorRef<ElementC const, LayoutC>;
+  using TensorRefD = TensorRef<ElementC, LayoutC>;
   using ElementAccumulator = ElementAccumulator_;
   using OperatorClass = OperatorClass_;
   using ArchTag = ArchTag_;
   using ThreadblockShape = ThreadblockShape_;
   using WarpShape = WarpShape_;
   using InstructionShape = InstructionShape_;
   using EpilogueOutputOp = EpilogueOutputOp_;
   using ThreadblockSwizzle = ThreadblockSwizzle_;
   using Operator = Operator_;
-  static SideMode const kSideModeA = SideModeA;
-  static FillMode const kFillModeA = FillModeA;
+  static SideMode const kSideMode = SideModeA;
+  static FillMode const kFillMode = FillModeA;
+  static DiagType const kDiagType = DiagTypeA;
+  // Changing SideMode as we change the layout
+  static SideMode const kSideModeT = (SideModeA == SideMode::kLeft) ?
+                                      SideMode::kRight : SideMode::kLeft;
+  // Changing FillMode as we change the layout
+  static FillMode const kFillModeT = (FillModeA == FillMode::kLower) ? 
+                                      FillMode::kUpper : FillMode::kLower;
   static int const kStages = Stages;
   static int const kAlignmentA = AlignmentA;
   static int const kAlignmentB = AlignmentB;
-  static int const kAlignmentC = EpilogueOutputOp::kCount;
+  static ComplexTransform const kTransformA = TransformA;
+  // Complex Transform don't appply to B
+  static ComplexTransform const kTransformB = ComplexTransform::kNone; 
   static bool const kSplitKSerial = SplitKSerial;
-  static BlasMode const kBlasMode = BlasMode_;
-  
-  /// Define the kernel
-  using UnderlyingOperator = typename cutlass::gemm::device::Symm<
+
+  using UnderlyingOperator = Trmm<
     ElementA,
     typename layout::LayoutTranspose<LayoutA>::type,
-    InvertSideMode<kSideModeA>::mode,
-    InvertFillMode<kFillModeA>::mode,
+    kSideModeT,
+    kFillModeT,
+    kDiagType,
     ElementB,
     typename layout::LayoutTranspose<LayoutB>::type, 
     ElementC,
     layout::RowMajor,
     ElementAccumulator,
     OperatorClass,
     ArchTag,
@@ -506,59 +673,48 @@
     EpilogueOutputOp,
     ThreadblockSwizzle,
     kStages,
     kAlignmentA,
     kAlignmentB,
     kSplitKSerial,
     Operator,
-    kBlasMode
+    TransformA
   >;
-  
 
-  /// Argument structure
   using Arguments = typename UnderlyingOperator::Arguments;
-  using SymmKernel = typename UnderlyingOperator::SymmKernel;
+  using TrmmKernel = typename UnderlyingOperator::TrmmKernel;
+  static int const kAlignmentC = UnderlyingOperator::kAlignmentC;
 
 private:
 
   UnderlyingOperator underlying_operator_;
 
 public:
 
-  /// Constructs the Symm.
-  Symm() { }
+  /// Constructs the TRMM.
+  Trmm() { }
 
-  /// Helper to construct a transposed equivalent for the underying SYMM operator
+  /// Helper to construct a transposed equivalent for the underying TRMM operator which is identical
   static Arguments to_underlying_arguments(Arguments const &args) {
     return args.transposed_problem_size();
   }
 
-  /// Determines whether the Symm can execute the given problem.
+  /// Determines whether the TRMM can execute the given problem.
   static Status can_implement(Arguments const &args) {
 
     return UnderlyingOperator::can_implement(to_underlying_arguments(args));
   }
 
   /// Gets the workspace size
   static size_t get_workspace_size(Arguments const &args) {
     
     return UnderlyingOperator::get_workspace_size(to_underlying_arguments(args));
   }
 
-  /// Computes the grid shape
-  static dim3 get_grid_shape(Arguments const &args) { 
-    return UnderlyingOperator::get_grid_shape(to_underlying_arguments(args));
-  }
-
-  /// Computes the maximum number of active blocks per multiprocessor
-  static int maximum_active_blocks(int smem_capacity = -1) {
-    return UnderlyingOperator::maximum_active_blocks(smem_capacity);
-  }
-
-  /// Initializes Symm state from arguments.
+  /// Initializes TRMM state from arguments.
   Status initialize(Arguments const &args, void *workspace = nullptr, cudaStream_t stream = nullptr) {
 
     return underlying_operator_.initialize(to_underlying_arguments(args), workspace, stream);
   }
 
   /// Lightweight update given a subset of arguments
   Status update(Arguments const &args, void *workspace = nullptr) {
@@ -578,25 +734,25 @@
   }
 
   /// Runs the kernel using initialized state.
   Status operator()(
     Arguments const &args, 
     void *workspace = nullptr, 
     cudaStream_t stream = nullptr) {
-    
+   
     Status status = initialize(args, workspace, stream);
     
     if (status == Status::kSuccess) {
       status = run(stream);
     }
 
     return status;
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
 } // namespace device
-} // namespace Symm
+} // namespace gemm
 } // namespace cutlass
 
 ////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/gemm.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_ell_gemm.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_ell_gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped_softmax_mainloop_fusion.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_layernorm_mainloop_fusion.h`

 * *Files 9% similar despite different names*

```diff
@@ -27,67 +27,68 @@
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
     \brief 
-      Default kernel-level softmax-grouped-GEMM
+      Default kernel-level GEMM definitions combine threadblock-scoped matrix multiply-add with
+      the appropriate threadblock-scoped epilogue.
+  
+      Note, CUTLASS epilogues universally target row-major outputs. Column-major outputs are
+      accommodated by exchanging A and B operands and assuming transposed layouts. Partial
+      specializations here choose 'device::GemmTransposed' to implement this functionality.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 
-#include "cutlass/complex.h"
 #include "cutlass/layout/matrix.h"
 #include "cutlass/numeric_types.h"
+#include "cutlass/arch/wmma.h"
 
-#include "cutlass/gemm/kernel/gemm_grouped_softmax_mainloop_fusion.h"
-#include "cutlass/gemm/kernel/gemm_transpose_operands.h"
-#include "cutlass/gemm/kernel/default_gemm.h"
-#include "cutlass/gemm/kernel/default_gemm_complex.h"
-#include "cutlass/gemm/device/default_gemm_configuration.h"
-#include "cutlass/gemm/threadblock/default_mma_softmax_mainloop_fusion.h"
+#include "cutlass/epilogue/threadblock/epilogue.h"
+#include "cutlass/epilogue/thread/linear_combination.h"
 
-#include "cutlass/layout/permute.h"
+#include "cutlass/gemm/gemm.h"
+#include "cutlass/gemm/kernel/gemm_layernorm_mainloop_fusion.h"
+#include "cutlass/gemm/threadblock/default_mma_layernorm_mainloop_fusion.h"
+#include "cutlass/gemm/threadblock/threadblock_swizzle.h"
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+#include "cutlass/epilogue/threadblock/default_epilogue_tensor_op.h"
+#include "cutlass/transform/threadblock/predicated_tile_iterator.h"
 
 namespace cutlass {
 namespace gemm {
 namespace kernel {
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
 template <
     /// Element type for A matrix operand
-    typename ElementA_,
+    typename ElementA,
     /// Layout type for A matrix operand
-    typename LayoutA_,
-    /// Complex elementwise transformation on A operand
-    ComplexTransform TransformA,
+    typename LayoutA,
     /// Access granularity of A matrix in units of elements
     int kAlignmentA,
     /// Element type for B matrix operand
-    typename ElementB_,
+    typename ElementB,
     /// Layout type for B matrix operand
-    typename LayoutB_,
-    /// Complex elementwise transformation on B operand
-    ComplexTransform TransformB,
+    typename LayoutB,
     /// Access granularity of B matrix in units of elements
     int kAlignmentB,
     /// Element type for Scale/Bias vectors
-    typename ElementScaleBias_,
+    typename ElementScaleBias,
     /// Layout type for Scale/Bias vectors
-    typename LayoutScaleBias_,
+    typename LayoutScaleBias,
     /// Element type for C and D matrix operands
-    typename ElementC_,
+    typename ElementC,
     /// Layout type for C and D matrix operands
-    typename LayoutC_,
+    typename LayoutC,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Operator class tag
     typename OperatorClass,
     /// Tag indicating architecture to tune for
     typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
@@ -98,65 +99,37 @@
     typename InstructionShape,
     /// Epilogue output operator
     typename EpilogueOutputOp,
     /// Threadblock-level swizzling operator
     typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
-    /// Whether the schedule of problems to visit has been precomputed
-    GroupScheduleMode GroupScheduleMode_ = GroupScheduleMode::kDeviceOnly,
     /// Operation performed by GEMM
-    typename Operator = typename device::DefaultGemmConfiguration<
-        OperatorClass, ArchTag, ElementA_, ElementB_, ElementC_,
-        ElementAccumulator>::Operator,
+    typename Operator,
     /// Use zfill or predicate for out-of-bound cp.async
-    SharedMemoryClearOption SharedMemoryClear = SharedMemoryClearOption::kNone
-    >
-struct DefaultGemmGroupedSoftmaxMainloopFusion {
-  // If true, we must construct a 'transposed-and-exchanged' Mma operator.
-  static bool const kInternalTranspose = platform::is_same<LayoutC_, layout::ColumnMajor>::value;
-
-  using MapArguments = kernel::detail::MapArguments<
-    ElementA_,
-    LayoutA_,
-    ComplexTransform::kNone,
-    kAlignmentA,
-    ElementB_,
-    LayoutB_,
-    ComplexTransform::kNone,
-    kAlignmentB,
-    LayoutC_,
-    kInternalTranspose
-  >;
+    SharedMemoryClearOption SharedMemoryClear = SharedMemoryClearOption::kNone>
+struct DefaultGemmLayernormMainloopFusion {
 
-private:
   /// Define the threadblock-scoped matrix multiply-accumulate
-  using Mma = typename cutlass::gemm::threadblock::DefaultMmaSoftmaxMainloopFusion<
-      typename MapArguments::ElementA, typename MapArguments::LayoutA, MapArguments::kAlignmentA,
-      typename MapArguments::ElementB, typename MapArguments::LayoutB, MapArguments::kAlignmentB,
-      ElementScaleBias_, LayoutScaleBias_, ElementAccumulator, layout::RowMajor, OperatorClass, ArchTag,
-      ThreadblockShape, WarpShape, InstructionShape, Stages, kInternalTranspose,
+  using Mma = typename cutlass::gemm::threadblock::DefaultMmaLayernormMainloopFusion<
+      ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB,
+      ElementScaleBias, LayoutScaleBias, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm80,
+      ThreadblockShape, WarpShape, InstructionShape, Stages,
       Operator, false, SharedMemoryClear>::ThreadblockMma;
 
   static const int kPartitionsK = ThreadblockShape::kK / WarpShape::kK;
 
   /// Define the epilogue
   using Epilogue =
       typename cutlass::epilogue::threadblock::DefaultEpilogueTensorOp<
           ThreadblockShape, typename Mma::Operator, kPartitionsK, EpilogueOutputOp,
           EpilogueOutputOp::kCount>::Epilogue;
 
-public:
-  using GemmKernel = kernel::GemmGroupedSoftmaxMainloopFusion<
-    Mma,
-    Epilogue,
-    ThreadblockSwizzle,
-    GroupScheduleMode_,
-    kInternalTranspose
-  >;
+  /// Define the kernel-level GEMM operator.
+  using GemmKernel = kernel::GemmLayernormMainloopFusion<Mma, Epilogue, ThreadblockSwizzle>;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 }  // namespace kernel
 }  // namespace gemm
 }  // namespace cutlass
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_layernorm_mainloop_fusion.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_k_reduction.h`

 * *Files 12% similar despite different names*

```diff
@@ -47,52 +47,56 @@
 #include "cutlass/numeric_types.h"
 #include "cutlass/arch/wmma.h"
 
 #include "cutlass/epilogue/threadblock/epilogue.h"
 #include "cutlass/epilogue/thread/linear_combination.h"
 
 #include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/kernel/gemm_layernorm_mainloop_fusion.h"
-#include "cutlass/gemm/threadblock/default_mma_layernorm_mainloop_fusion.h"
+#include "cutlass/gemm/kernel/gemm_with_k_reduction.h"
+#include "cutlass/gemm/threadblock/default_mma_with_reduction.h"
+#include "cutlass/gemm/threadblock/default_mma_core_with_reduction.h"
 #include "cutlass/gemm/threadblock/threadblock_swizzle.h"
 
 #include "cutlass/epilogue/threadblock/default_epilogue_tensor_op.h"
+#include "cutlass/epilogue/threadblock/epilogue_gemm_k_reduction.h"
 #include "cutlass/transform/threadblock/predicated_tile_iterator.h"
 
 namespace cutlass {
 namespace gemm {
 namespace kernel {
 
 ////////////////////////////////////////////////////////////////////////////////
 
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
+    /// Complex elementwise transformation on A operand
+    ComplexTransform TransformA,
     /// Access granularity of A matrix in units of elements
     int kAlignmentA,
     /// Element type for B matrix operand
     typename ElementB,
     /// Layout type for B matrix operand
     typename LayoutB,
+    /// Complex elementwise transformation on B operand
+    ComplexTransform TransformB,
     /// Access granularity of B matrix in units of elements
     int kAlignmentB,
-    /// Element type for Scale/Bias vectors
-    typename ElementScaleBias,
-    /// Layout type for Scale/Bias vectors
-    typename LayoutScaleBias,
     /// Element type for C and D matrix operands
     typename ElementC,
     /// Layout type for C and D matrix operands
     typename LayoutC,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Operator class tag
     typename OperatorClass,
+    /// Reduce A or B along the K dimension
+    bool ReduceKForA_,
     /// Tag indicating architecture to tune for
     typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape,
     /// Warp-level tile size (concept: GemmShape)
@@ -102,34 +106,43 @@
     /// Threadblock-level swizzling operator
     typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
     /// Operation performed by GEMM
     typename Operator,
     /// Use zfill or predicate for out-of-bound cp.async
-    SharedMemoryClearOption SharedMemoryClear = SharedMemoryClearOption::kNone>
-struct DefaultGemmLayernormMainloopFusion {
+    SharedMemoryClearOption SharedMemoryClear = SharedMemoryClearOption::kNone,
+    ///
+    typename Enable = void>
+struct DefaultGemmWithKReduction {
+
+  static const bool kReduceKForA = (platform::is_same<LayoutC, cutlass::layout::RowMajor>::value) ? ReduceKForA_ : !ReduceKForA_;
 
   /// Define the threadblock-scoped matrix multiply-accumulate
-  using Mma = typename cutlass::gemm::threadblock::DefaultMmaLayernormMainloopFusion<
+  using Mma = typename cutlass::gemm::threadblock::DefaultMmaWithReduction<
       ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB,
-      ElementScaleBias, LayoutScaleBias, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm80,
+      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, kReduceKForA, arch::Sm80,
       ThreadblockShape, WarpShape, InstructionShape, Stages,
       Operator, false, SharedMemoryClear>::ThreadblockMma;
 
   static const int kPartitionsK = ThreadblockShape::kK / WarpShape::kK;
 
   /// Define the epilogue
   using Epilogue =
       typename cutlass::epilogue::threadblock::DefaultEpilogueTensorOp<
           ThreadblockShape, typename Mma::Operator, kPartitionsK, EpilogueOutputOp,
           EpilogueOutputOp::kCount>::Epilogue;
 
+  /// Define the epilogue of the reduction vector
+  using EpilogueGemmKReduction =
+      typename cutlass::epilogue::threadblock::EpilogueGemmKReduction<
+          ElementAccumulator, ElementC, ThreadblockShape, typename Mma::Operator, kReduceKForA>;
+
   /// Define the kernel-level GEMM operator.
-  using GemmKernel = kernel::GemmLayernormMainloopFusion<Mma, Epilogue, ThreadblockSwizzle>;
+  using GemmKernel = kernel::GemmWithKReduction<Mma, Epilogue, EpilogueGemmKReduction, ThreadblockSwizzle>;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 }  // namespace kernel
 }  // namespace gemm
 }  // namespace cutlass
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_planar_complex_universal.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_planar_complex_universal.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_sparse.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_sparse.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_splitk_parallel.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_splitk_parallel.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_universal.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_universal.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_broadcast.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_broadcast.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_k_reduction.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_softmax_mainloop_fusion.h`

 * *Files 14% similar despite different names*

```diff
@@ -24,127 +24,137 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
 /*! \file
-    \brief 
-      Default kernel-level GEMM definitions combine threadblock-scoped matrix multiply-add with
-      the appropriate threadblock-scoped epilogue.
-  
-      Note, CUTLASS epilogues universally target row-major outputs. Column-major outputs are
-      accommodated by exchanging A and B operands and assuming transposed layouts. Partial
-      specializations here choose 'device::GemmTransposed' to implement this functionality.
+    \brief Template for a pipelined softmax-GEMM kernel.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
-
-#include "cutlass/layout/matrix.h"
 #include "cutlass/numeric_types.h"
-#include "cutlass/arch/wmma.h"
-
-#include "cutlass/epilogue/threadblock/epilogue.h"
-#include "cutlass/epilogue/thread/linear_combination.h"
+#include "cutlass/arch/arch.h"
 
-#include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/kernel/gemm_with_k_reduction.h"
-#include "cutlass/gemm/threadblock/default_mma_with_reduction.h"
-#include "cutlass/gemm/threadblock/default_mma_core_with_reduction.h"
-#include "cutlass/gemm/threadblock/threadblock_swizzle.h"
-
-#include "cutlass/epilogue/threadblock/default_epilogue_tensor_op.h"
-#include "cutlass/epilogue/threadblock/epilogue_gemm_k_reduction.h"
+#include "cutlass/layout/matrix.h"
+#include "cutlass/gemm/threadblock/default_mma_core.h"
+#include "cutlass/gemm/threadblock/mma_softmax_mainloop_fusion_multistage.h"
+#include "cutlass/transform/threadblock/predicated_scale_bias_vector_iterator.h"
+#include "cutlass/transform/threadblock/predicated_scale_bias_vector_access_iterator.h"
+#include "cutlass/transform/threadblock/regular_scale_bias_vector_access_iterator.h"
+#include "cutlass/gemm/warp/scale_bias_tile_iterator.h"
 #include "cutlass/transform/threadblock/predicated_tile_iterator.h"
 
+////////////////////////////////////////////////////////////////////////////////
+
 namespace cutlass {
 namespace gemm {
-namespace kernel {
+namespace threadblock {
 
 ////////////////////////////////////////////////////////////////////////////////
 
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
-    /// Complex elementwise transformation on A operand
-    ComplexTransform TransformA,
     /// Access granularity of A matrix in units of elements
     int kAlignmentA,
     /// Element type for B matrix operand
     typename ElementB,
     /// Layout type for B matrix operand
     typename LayoutB,
-    /// Complex elementwise transformation on B operand
-    ComplexTransform TransformB,
     /// Access granularity of B matrix in units of elements
     int kAlignmentB,
-    /// Element type for C and D matrix operands
-    typename ElementC,
-    /// Layout type for C and D matrix operands
-    typename LayoutC,
+    /// Element type for Scale/Bias vectors
+    typename ElementScaleBias,
+    /// Layout type for Scale/Bias vectors
+    typename LayoutScaleBias,
     /// Element type for internal accumulation
     typename ElementAccumulator,
+    /// Layout type for C and D matrix operands
+    typename LayoutC,
     /// Operator class tag
     typename OperatorClass,
-    /// Reduce A or B along the K dimension
-    bool ReduceKForA_,
     /// Tag indicating architecture to tune for
     typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape,
-    /// Warp-level tile size (concept: GemmShape)
+    /// Instruction-level tile size (concept: GemmShape)
     typename InstructionShape,
-    /// Epilogue output operator
-    typename EpilogueOutputOp,
-    /// Threadblock-level swizzling operator
-    typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
-    /// Operation performed by GEMM
+    /// Whether problem has been transformed. This determines to which operand
+    /// the softmax is applied.
+    bool InternalTranspose,
+    /// Operation perfomed by GEMM
     typename Operator,
-    /// Use zfill or predicate for out-of-bound cp.async
-    SharedMemoryClearOption SharedMemoryClear = SharedMemoryClearOption::kNone,
-    ///
-    typename Enable = void>
-struct DefaultGemmWithKReduction {
-
-  static const bool kReduceKForA = (platform::is_same<LayoutC, cutlass::layout::RowMajor>::value) ? ReduceKForA_ : !ReduceKForA_;
-
-  /// Define the threadblock-scoped matrix multiply-accumulate
-  using Mma = typename cutlass::gemm::threadblock::DefaultMmaWithReduction<
-      ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB,
-      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, kReduceKForA, arch::Sm80,
-      ThreadblockShape, WarpShape, InstructionShape, Stages,
-      Operator, false, SharedMemoryClear>::ThreadblockMma;
-
-  static const int kPartitionsK = ThreadblockShape::kK / WarpShape::kK;
-
-  /// Define the epilogue
-  using Epilogue =
-      typename cutlass::epilogue::threadblock::DefaultEpilogueTensorOp<
-          ThreadblockShape, typename Mma::Operator, kPartitionsK, EpilogueOutputOp,
-          EpilogueOutputOp::kCount>::Epilogue;
-
-  /// Define the epilogue of the reduction vector
-  using EpilogueGemmKReduction =
-      typename cutlass::epilogue::threadblock::EpilogueGemmKReduction<
-          ElementAccumulator, ElementC, ThreadblockShape, typename Mma::Operator, kReduceKForA>;
-
-  /// Define the kernel-level GEMM operator.
-  using GemmKernel = kernel::GemmWithKReduction<Mma, Epilogue, EpilogueGemmKReduction, ThreadblockSwizzle>;
+    /// Store the accumulators in row major or column major.  Row major is used
+    /// when output layout is interleaved.
+    bool AccumulatorsInRowMajor = false,
+    /// Use zfill or predicate for SM80 out-of-bound cp.async 
+    SharedMemoryClearOption SharedMemoryClear = SharedMemoryClearOption::kNone
+    >
+struct DefaultMmaSoftmaxMainloopFusion {
+
+  static cutlass::arch::CacheOperation::Kind const CacheOpA =
+      ((sizeof_bits<ElementA>::value * kAlignmentA) == 128)
+          ? cutlass::arch::CacheOperation::Global
+          : cutlass::arch::CacheOperation::Always;
+
+  static cutlass::arch::CacheOperation::Kind const CacheOpB =
+      ((sizeof_bits<ElementB>::value * kAlignmentB) == 128)
+          ? cutlass::arch::CacheOperation::Global
+          : cutlass::arch::CacheOperation::Always;
+
+  static cutlass::arch::CacheOperation::Kind const CacheOpGammaBeta = CacheOpA;
+
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
+      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
+      Stages, Operator, false, CacheOpA, CacheOpB>;
+
+  // Define iterators over tiles from the A operand
+  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
+  using AccessTypeA = cutlass::Array<ElementA, kAlignmentA>;
+  using IteratorA =
+      cutlass::transform::threadblock::PredicatedTileAccessIterator<
+          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
+          ElementA, LayoutA, 1, ThreadMapA, AccessTypeA>;
+
+  // Define iterators over tiles from the B operand
+  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
+  using AccessTypeB = cutlass::Array<ElementB, kAlignmentB>;
+  using IteratorB =
+      cutlass::transform::threadblock::PredicatedTileAccessIterator<
+          cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
+          ElementB, LayoutB, 0, ThreadMapB, AccessTypeB>;
+
+  /// Define iterators over tiles from scale/bias vectors
+  using IteratorNormSum =
+      cutlass::transform::threadblock::PredicatedScaleBiasVectorIterator<
+          cutlass::MatrixShape<1, WarpShape::kN>,
+          ElementScaleBias,
+          LayoutScaleBias>;
+
+  // Define the threadblock-scoped multistage matrix multiply
+  using ThreadblockMma = cutlass::gemm::threadblock::MmaSoftmaxMainloopFusionMultistage<
+      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
+      MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
+      MmaCore::kCacheOpB, IteratorNormSum,
+      ElementAccumulator, layout::RowMajor,
+      typename MmaCore::MmaPolicy, Stages, InternalTranspose, SharedMemoryClear>;
 };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
-}  // namespace kernel
-}  // namespace gemm
-}  // namespace cutlass
+} // namespace threadblock
+} // namespace gemm
+} // namespace cutlass 
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_reduction.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_reduction.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemv.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemv.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_grouped.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_grouped.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_universal.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_universal.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k_complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k_universal.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k_universal.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm_complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm_universal.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm_universal.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm_complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm_universal.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm_universal.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/ell_gemm.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/ell_gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_array.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_array.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_batched.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_batched.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped_softmax_mainloop_fusion.h`

 * *Files 4% similar despite different names*

```diff
@@ -26,15 +26,15 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-    \brief Problem visitor for grouped GEMMs
+    \brief Problem visitor for grouped GEMMs with a softmax fused beforehand
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/fast_math.h"
 #include "cutlass/gemm/gemm.h"
@@ -58,15 +58,15 @@
 template <
   typename Mma_,                           ///! Threadblock-scoped matrix multiply-accumulate
   typename Epilogue_,                      ///! Epilogue
   typename ThreadblockSwizzle_,            ///! Threadblock swizzling function
   GroupScheduleMode GroupScheduleMode_,    ///! Type of scheduling to perform
   bool Transposed = false
 >
-struct GemmGrouped {
+struct GemmGroupedSoftmaxMainloopFusion {
 public:
 
   using Mma = Mma_;
   using Epilogue = Epilogue_;
   using EpilogueOutputOp = typename Epilogue::OutputOp;
   using ThreadblockSwizzle = ThreadblockSwizzle_;
   static GroupScheduleMode const kGroupScheduleMode = GroupScheduleMode_;
@@ -91,14 +91,16 @@
   using ElementA = typename MapArguments::ElementA;
   using LayoutA = typename MapArguments::LayoutA;
   using ElementB = typename MapArguments::ElementB;
   using LayoutB = typename MapArguments::LayoutB;
   using ElementC = typename Epilogue::OutputTileIterator::Element;
   using LayoutC = typename MapArguments::LayoutC;
 
+  using ElementScaleBias = typename Mma::IteratorNormSum::Element;
+
   static ComplexTransform const kTransformA = MapArguments::kTransformA;
   static ComplexTransform const kTransformB = MapArguments::kTransformB;
 
   // Type definitions about the mainloop.
   using Operator = typename Mma::Operator;
   using OperatorClass = typename Mma::Operator::OperatorClass;
   using ThreadblockShape = typename Mma::Shape;
@@ -139,14 +141,16 @@
 
     typename EpilogueOutputOp::Params output_op;
 
     ElementA ** ptr_A;
     ElementB ** ptr_B;
     ElementC ** ptr_C;
     ElementC ** ptr_D;
+    void ** ptr_norm;
+    void ** ptr_sum;
 
     typename LayoutA::Stride::LongIndex *lda;
     typename LayoutB::Stride::LongIndex *ldb;
     typename LayoutC::Stride::LongIndex *ldc;
     typename LayoutC::Stride::LongIndex *ldd;
 
     // Only used by device-level operator
@@ -154,55 +158,61 @@
 
     //
     // Methods
     //
 
     /// Default ctor
     CUTLASS_HOST_DEVICE
-    Arguments(): 
+    Arguments():
       problem_count(0),
-      threadblock_count(0), 
-      ptr_A(nullptr), 
-      ptr_B(nullptr), 
-      ptr_C(nullptr), 
-      ptr_D(nullptr), 
+      threadblock_count(0),
+      ptr_A(nullptr),
+      ptr_B(nullptr),
+      ptr_C(nullptr),
+      ptr_D(nullptr),
+      ptr_norm(nullptr),
+      ptr_sum(nullptr),
       lda(nullptr),
       ldb(nullptr),
       ldc(nullptr),
       ldd(nullptr),
       host_problem_sizes(nullptr)
     {
 
     }
 
     /// Ctor
     CUTLASS_HOST_DEVICE
-    Arguments(    
+    Arguments(
       GemmCoord *problem_sizes,
       int problem_count,
       int threadblock_count,
       typename EpilogueOutputOp::Params output_op,
       ElementA ** ptr_A,
       ElementB ** ptr_B,
       ElementC ** ptr_C,
       ElementC ** ptr_D,
+      void ** ptr_norm,
+      void ** ptr_sum,
       typename LayoutA::Stride::LongIndex *lda,
       typename LayoutB::Stride::LongIndex *ldb,
       typename LayoutC::Stride::LongIndex *ldc,
       typename LayoutC::Stride::LongIndex *ldd,
       GemmCoord *host_problem_sizes=nullptr
-    ): 
+    ):
       problem_sizes(problem_sizes),
       problem_count(problem_count),
       threadblock_count(threadblock_count),
       output_op(output_op),
       ptr_A(ptr_A),
       ptr_B(ptr_B),
       ptr_C(ptr_C),
       ptr_D(ptr_D),
+      ptr_norm(ptr_norm),
+      ptr_sum(ptr_sum),
       lda(lda),
       ldb(ldb),
       ldc(ldc),
       ldd(ldd),
       host_problem_sizes(host_problem_sizes)
     {
 
@@ -222,14 +232,17 @@
     typename EpilogueOutputOp::Params output_op;
 
     ElementA ** ptr_A;
     ElementB ** ptr_B;
     ElementC ** ptr_C;
     ElementC ** ptr_D;
 
+    void ** ptr_norm;
+    void ** ptr_sum;
+
     typename LayoutA::Stride::LongIndex *lda;
     typename LayoutB::Stride::LongIndex *ldb;
     typename LayoutC::Stride::LongIndex *ldc;
     typename LayoutC::Stride::LongIndex *ldd;
 
     //
     // Methods
@@ -237,14 +250,16 @@
 
     CUTLASS_HOST_DEVICE
     Params():
       ptr_A(nullptr),
       ptr_B(nullptr),
       ptr_C(nullptr),
       ptr_D(nullptr),
+      ptr_norm(nullptr),
+      ptr_sum(nullptr),
       lda(nullptr),
       ldb(nullptr),
       ldc(nullptr),
       ldd(nullptr)
     { }
 
     CUTLASS_HOST_DEVICE
@@ -254,19 +269,21 @@
       problem_visitor(args.problem_sizes, args.problem_count, workspace, tile_count),
       threadblock_count(args.threadblock_count),
       output_op(args.output_op),
       ptr_A(args.ptr_A),
       ptr_B(args.ptr_B),
       ptr_C(args.ptr_C),
       ptr_D(args.ptr_D),
+      ptr_norm(args.ptr_norm),
+      ptr_sum(args.ptr_sum),
       lda(args.lda),
       ldb(args.ldb),
       ldc(args.ldc),
       ldd(args.ldd)
-    { 
+    {
 
     }
 
     CUTLASS_HOST_DEVICE
     void update(
       Arguments const &args,
       void *workspace = nullptr,
@@ -276,14 +293,16 @@
                                                         workspace, tile_count);
       threadblock_count = args.threadblock_count;
       output_op = args.output_op;
       ptr_A = args.ptr_A;
       ptr_B = args.ptr_B;
       ptr_C = args.ptr_C;
       ptr_D = args.ptr_D;
+      ptr_norm = args.ptr_norm;
+      ptr_sum = args.ptr_sum;
       lda = args.lda;
       ldb = args.ldb;
       ldc = args.ldc;
       ldd = args.ldd;
     }
   };
 
@@ -301,25 +320,25 @@
 public:
 
   //
   // Methods
   //
 
   CUTLASS_DEVICE
-  GemmGrouped() { } 
+  GemmGroupedSoftmaxMainloopFusion() { }
 
   /// Determines whether kernel satisfies alignment
   static Status can_implement(cutlass::gemm::GemmCoord const & problem_size) {
     return Status::kSuccess;
   }
 
   static Status can_implement(Arguments const &args) {
     return Status::kSuccess;
   }
- 
+
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
     //
     // These types shadow the type-level definitions and support the ability to implement
     // a 'transposed' GEMM that computes the transposed problems.
@@ -385,18 +404,27 @@
       typename Mma::IteratorB iterator_B(
         LayoutB(ldm_B),
         ptr_B,
         {problem_size.k(), problem_size.n()},
         thread_idx,
         tb_offset_B);
 
+      // Construct iterator to the softmax norm/sum vector
+      typename Mma::IteratorNormSum iterator_norm_sum(
+        problem_size.m(),
+        static_cast<ElementScaleBias const *>(params.ptr_norm[problem_idx]),
+        static_cast<ElementScaleBias const *>(params.ptr_sum[problem_idx]),
+        thread_idx,
+        MatrixCoord(0, threadblock_offset.m())
+      );
+
       typename Mma::FragmentC accumulators;
 
       accumulators.clear();
-      
+
       // Broadcast the warp_id computed by lane 0 to ensure dependent code
       // is compiled as warp-uniform.
       int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
 
       int lane_idx = threadIdx.x % 32;
 
       //
@@ -410,18 +438,19 @@
       int gemm_k_iterations = (problem_size.k() + Mma::Shape::kK - 1) / Mma::Shape::kK;
 
       // Wait for all threads to finish their epilogue phases from the previous tile.
       __syncthreads();
 
       // Compute threadblock-scoped matrix multiply-add
       mma(
-        gemm_k_iterations, 
-        accumulators, 
-        iterator_A, 
-        iterator_B, 
+        gemm_k_iterations,
+        accumulators,
+        iterator_A,
+        iterator_B,
+        iterator_norm_sum,
         accumulators);
 
       //
       // Epilogue
       //
 
       EpilogueOutputOp output_op(params.output_op);
@@ -450,25 +479,25 @@
         ptr_D,
         problem_size.mn(),
         thread_idx,
         threadblock_offset.mn()
       );
 
       Epilogue epilogue(
-        shared_storage.kernel.epilogue, 
-        thread_idx, 
-        warp_idx, 
+        shared_storage.kernel.epilogue,
+        thread_idx,
+        warp_idx,
         lane_idx);
 
       // Execute the epilogue operator to update the destination tensor.
       epilogue(
-        output_op, 
-        iterator_D, 
-        accumulators, 
-        iterator_C); 
+        output_op,
+        iterator_D,
+        accumulators,
+        iterator_C);
 
       // Next tile
       problem_visitor.advance(gridDim.x);
     }
   }
 };
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped_problem_visitor.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped_problem_visitor.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped_softmax_mainloop_fusion.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_k_universal.h`

 * *Files 21% similar despite different names*

```diff
@@ -26,481 +26,536 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-    \brief Problem visitor for grouped GEMMs with a softmax fused beforehand
+    \brief 
+
 */
 
 #pragma once
 
-#include "cutlass/cutlass.h"
+#include "cutlass/blas3.h"
 #include "cutlass/fast_math.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/matrix_coord.h"
 #include "cutlass/complex.h"
 #include "cutlass/semaphore.h"
 
-#include "cutlass/layout/matrix.h"
-#include "cutlass/trace.h"
-#include "cutlass/gemm/kernel/gemm_transpose_operands.h"
-#include "cutlass/gemm/kernel/gemm_grouped_problem_visitor.h"
-
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace kernel {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
-  typename Mma_,                           ///! Threadblock-scoped matrix multiply-accumulate
-  typename Epilogue_,                      ///! Epilogue
-  typename ThreadblockSwizzle_,            ///! Threadblock swizzling function
-  GroupScheduleMode GroupScheduleMode_,    ///! Type of scheduling to perform
-  bool Transposed = false
+  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate 
+  typename Epilogue_,             ///! Epilogue
+  typename ThreadblockSwizzle_,   ///! Threadblock swizzling function
+  FillMode FillModeC_             ///! Fill Mode for C (kLower or kUpper)
 >
-struct GemmGroupedSoftmaxMainloopFusion {
+struct RankKUniversal {
 public:
 
   using Mma = Mma_;
   using Epilogue = Epilogue_;
   using EpilogueOutputOp = typename Epilogue::OutputOp;
   using ThreadblockSwizzle = ThreadblockSwizzle_;
-  static GroupScheduleMode const kGroupScheduleMode = GroupScheduleMode_;
-  static bool const kTransposed = Transposed;
 
-  // Optional transpose
-  using MapArguments = kernel::detail::MapArguments<
-    typename Mma::IteratorA::Element,
-    typename Mma::IteratorA::Layout,
-    Mma::kTransformA,
-    Mma::IteratorA::AccessType::kElements,
-    typename Mma::IteratorB::Element,
-    typename Mma::IteratorB::Layout,
-    Mma::kTransformB,
-    Mma::IteratorB::AccessType::kElements,
-    typename Mma::LayoutC,
-    kTransposed
-  >;
-
-  // Public-facing type definitions related to operand element type, layout, and complex conjugate
-  // operation. Must interact with the 'kTransposed' notion.
-  using ElementA = typename MapArguments::ElementA;
-  using LayoutA = typename MapArguments::LayoutA;
-  using ElementB = typename MapArguments::ElementB;
-  using LayoutB = typename MapArguments::LayoutB;
+  using ElementA = typename Mma::IteratorA::Element;
+  using LayoutA = typename Mma::IteratorA::Layout;
+  using ElementB = typename Mma::IteratorB::Element;
+  using LayoutB = typename Mma::IteratorB::Layout;
   using ElementC = typename Epilogue::OutputTileIterator::Element;
-  using LayoutC = typename MapArguments::LayoutC;
-
-  using ElementScaleBias = typename Mma::IteratorNormSum::Element;
+  using LayoutC = typename Epilogue::OutputTileIterator::Layout;
+  static FillMode const kFillModeC = FillModeC_;
 
-  static ComplexTransform const kTransformA = MapArguments::kTransformA;
-  static ComplexTransform const kTransformB = MapArguments::kTransformB;
-
-  // Type definitions about the mainloop.
+  static ComplexTransform const kTransformA = Mma::kTransformA;
+  static ComplexTransform const kTransformB = Mma::kTransformB;
   using Operator = typename Mma::Operator;
+
   using OperatorClass = typename Mma::Operator::OperatorClass;
   using ThreadblockShape = typename Mma::Shape;
   using WarpShape = typename Mma::Operator::Shape;
   using InstructionShape = typename Mma::Policy::Operator::InstructionShape;
   using ArchTag = typename Mma::ArchTag;
 
   static int const kStages = Mma::kStages;
-  static int const kAlignmentA = MapArguments::kAlignmentA;
-  static int const kAlignmentB = MapArguments::kAlignmentB;
+  static int const kAlignmentA = Mma::IteratorA::AccessType::kElements;
+  static int const kAlignmentB = Mma::IteratorB::AccessType::kElements;
   static int const kAlignmentC = Epilogue::OutputTileIterator::kElementsPerAccess;
 
   /// Warp count (concept: GemmShape)
   using WarpCount = typename Mma::WarpCount;
   static int const kThreadCount = 32 * WarpCount::kCount;
 
-  using ProblemVisitor = GemmGroupedProblemVisitor<
-                            ThreadblockShape,
-                            kGroupScheduleMode,
-                            kThreadCount,
-                            kThreadCount,
-                            kTransposed>;
+  /// Split-K preserves splits that are 128b aligned
+  static int const kSplitKAlignment = 128 / sizeof_bits<ElementA>::value;
 
   //
   // Structures
   //
 
   /// Argument structure
   struct Arguments {
 
     //
     // Data members
     //
 
-    GemmCoord *problem_sizes;
-    int problem_count;
-    int threadblock_count;
-
-    typename EpilogueOutputOp::Params output_op;
-
-    ElementA ** ptr_A;
-    ElementB ** ptr_B;
-    ElementC ** ptr_C;
-    ElementC ** ptr_D;
-    void ** ptr_norm;
-    void ** ptr_sum;
-
-    typename LayoutA::Stride::LongIndex *lda;
-    typename LayoutB::Stride::LongIndex *ldb;
-    typename LayoutC::Stride::LongIndex *ldc;
-    typename LayoutC::Stride::LongIndex *ldd;
-
-    // Only used by device-level operator
-    GemmCoord *host_problem_sizes;
+    GemmUniversalMode mode;
+    GemmCoord problem_size;
+    int batch_count;
+
+    typename EpilogueOutputOp::Params epilogue;
+
+    void const * ptr_A;
+    void const * ptr_C;
+    void * ptr_D;
+
+    int64_t batch_stride_A;
+    int64_t batch_stride_C;
+    int64_t batch_stride_D;
+
+    typename LayoutA::Stride::Index lda;
+    typename LayoutB::Stride::Index ldb;
+    typename LayoutC::Stride::Index ldc;
+    typename LayoutC::Stride::Index ldd;
 
     //
     // Methods
     //
+    
+    Arguments(): 
+      mode(GemmUniversalMode::kGemm), 
+      batch_count(1), 
+      ptr_A(nullptr), ptr_C(nullptr), ptr_D(nullptr) { }
 
-    /// Default ctor
-    CUTLASS_HOST_DEVICE
-    Arguments():
-      problem_count(0),
-      threadblock_count(0),
-      ptr_A(nullptr),
-      ptr_B(nullptr),
-      ptr_C(nullptr),
-      ptr_D(nullptr),
-      ptr_norm(nullptr),
-      ptr_sum(nullptr),
-      lda(nullptr),
-      ldb(nullptr),
-      ldc(nullptr),
-      ldd(nullptr),
-      host_problem_sizes(nullptr)
-    {
-
-    }
-
-    /// Ctor
-    CUTLASS_HOST_DEVICE
+    /// constructs an arguments structure
     Arguments(
-      GemmCoord *problem_sizes,
-      int problem_count,
-      int threadblock_count,
-      typename EpilogueOutputOp::Params output_op,
-      ElementA ** ptr_A,
-      ElementB ** ptr_B,
-      ElementC ** ptr_C,
-      ElementC ** ptr_D,
-      void ** ptr_norm,
-      void ** ptr_sum,
-      typename LayoutA::Stride::LongIndex *lda,
-      typename LayoutB::Stride::LongIndex *ldb,
-      typename LayoutC::Stride::LongIndex *ldc,
-      typename LayoutC::Stride::LongIndex *ldd,
-      GemmCoord *host_problem_sizes=nullptr
+      GemmUniversalMode mode,
+      GemmCoord problem_size,
+      int batch_count,
+      typename EpilogueOutputOp::Params epilogue,
+      void const * ptr_A,
+      void const * ptr_C,
+      void * ptr_D,
+      int64_t batch_stride_A,
+      int64_t batch_stride_C,
+      int64_t batch_stride_D,
+      typename LayoutA::Stride::Index lda,
+      typename LayoutC::Stride::Index ldc,
+      typename LayoutC::Stride::Index ldd
     ):
-      problem_sizes(problem_sizes),
-      problem_count(problem_count),
-      threadblock_count(threadblock_count),
-      output_op(output_op),
-      ptr_A(ptr_A),
-      ptr_B(ptr_B),
-      ptr_C(ptr_C),
-      ptr_D(ptr_D),
-      ptr_norm(ptr_norm),
-      ptr_sum(ptr_sum),
-      lda(lda),
-      ldb(ldb),
-      ldc(ldc),
-      ldd(ldd),
-      host_problem_sizes(host_problem_sizes)
-    {
+      mode(mode), 
+      problem_size(problem_size), 
+      batch_count(batch_count),
+      epilogue(epilogue), 
+      ptr_A(ptr_A), ptr_C(ptr_C), ptr_D(ptr_D), 
+      batch_stride_A(batch_stride_A), batch_stride_C(batch_stride_C), batch_stride_D(batch_stride_D), 
+      lda(lda), ldb(ldb), ldc(ldc), ldd(ldd) {
+
+      }
 
-    }
   };
 
   //
   // Structure for precomputing values in host memory and passing to kernels
   //
 
   /// Parameters structure
   struct Params {
 
-    typename ProblemVisitor::Params problem_visitor;
-    int threadblock_count;
-
+    cutlass::gemm::GemmCoord problem_size;
+    cutlass::gemm::GemmCoord grid_tiled_shape;
+    int swizzle_log_tile;
+   
+    typename Mma::IteratorA::Params params_A;
+    typename Mma::IteratorB::Params params_B;
+    typename Epilogue::OutputTileIterator::Params params_C;
+    typename Epilogue::OutputTileIterator::Params params_D;
+    
     typename EpilogueOutputOp::Params output_op;
 
-    ElementA ** ptr_A;
-    ElementB ** ptr_B;
-    ElementC ** ptr_C;
-    ElementC ** ptr_D;
-
-    void ** ptr_norm;
-    void ** ptr_sum;
-
-    typename LayoutA::Stride::LongIndex *lda;
-    typename LayoutB::Stride::LongIndex *ldb;
-    typename LayoutC::Stride::LongIndex *ldc;
-    typename LayoutC::Stride::LongIndex *ldd;
+    GemmUniversalMode mode;
+    int batch_count;
+    int gemm_k_size;
+
+    void * ptr_A;
+    void * ptr_B;
+    void * ptr_C;
+    void * ptr_D;
+
+    int64_t batch_stride_A;
+    int64_t batch_stride_B;
+    int64_t batch_stride_C;
+    int64_t batch_stride_D;
+
+    int *semaphore;
 
     //
     // Methods
     //
 
     CUTLASS_HOST_DEVICE
     Params():
+      swizzle_log_tile(0),
+      params_A(0),
+      params_B(0),
+      params_C(0),
+      params_D(0),
+      batch_count(0),
+      gemm_k_size(0),
+      mode(cutlass::gemm::GemmUniversalMode::kGemm),
       ptr_A(nullptr),
       ptr_B(nullptr),
       ptr_C(nullptr),
       ptr_D(nullptr),
-      ptr_norm(nullptr),
-      ptr_sum(nullptr),
-      lda(nullptr),
-      ldb(nullptr),
-      ldc(nullptr),
-      ldd(nullptr)
-    { }
+      batch_stride_A(0),
+      batch_stride_B(0),
+      batch_stride_C(0),
+      batch_stride_D(0),
+      semaphore(nullptr) { }
 
     CUTLASS_HOST_DEVICE
-    Params(Arguments const &args,
-          void *workspace = nullptr,
-          int tile_count = 0):
-      problem_visitor(args.problem_sizes, args.problem_count, workspace, tile_count),
-      threadblock_count(args.threadblock_count),
-      output_op(args.output_op),
-      ptr_A(args.ptr_A),
-      ptr_B(args.ptr_B),
-      ptr_C(args.ptr_C),
-      ptr_D(args.ptr_D),
-      ptr_norm(args.ptr_norm),
-      ptr_sum(args.ptr_sum),
-      lda(args.lda),
-      ldb(args.ldb),
-      ldc(args.ldc),
-      ldd(args.ldd)
-    {
-
+    Params(
+      Arguments const &args,
+      cutlass::gemm::GemmCoord const & grid_tiled_shape,
+      int gemm_k_size,
+      void *workspace = nullptr
+    ):
+      problem_size(args.problem_size),
+      grid_tiled_shape(grid_tiled_shape),
+      swizzle_log_tile(ThreadblockSwizzle().get_log_tile(grid_tiled_shape)),
+      params_A(args.lda),
+      params_B(args.lda),
+      params_C(args.ldc),
+      params_D(args.ldd),
+      output_op(args.epilogue),
+      mode(args.mode),
+      batch_count(args.batch_count),
+      gemm_k_size(gemm_k_size),
+      ptr_A(const_cast<void *>(args.ptr_A)),
+      ptr_B(const_cast<void *>(args.ptr_A)),
+      ptr_C(const_cast<void *>(args.ptr_C)),
+      ptr_D(const_cast<void *>(args.ptr_D)),
+      batch_stride_A(args.batch_stride_A),
+      batch_stride_B(args.batch_stride_A),
+      batch_stride_C(args.batch_stride_C),
+      batch_stride_D(args.batch_stride_D),
+      semaphore(static_cast<int *>(workspace)) {
     }
 
     CUTLASS_HOST_DEVICE
     void update(
       Arguments const &args,
-      void *workspace = nullptr,
-      int tile_count = 0) {
+      void *workspace = nullptr) {
 
-      problem_visitor = typename ProblemVisitor::Params(args.problem_sizes, args.problem_count,
-                                                        workspace, tile_count);
-      threadblock_count = args.threadblock_count;
-      output_op = args.output_op;
-      ptr_A = args.ptr_A;
-      ptr_B = args.ptr_B;
-      ptr_C = args.ptr_C;
+      ptr_A = const_cast<void *>(args.ptr_A);
+      ptr_B = const_cast<void *>(args.ptr_A);
+      ptr_C = const_cast<void *>(args.ptr_C);
       ptr_D = args.ptr_D;
-      ptr_norm = args.ptr_norm;
-      ptr_sum = args.ptr_sum;
-      lda = args.lda;
-      ldb = args.ldb;
-      ldc = args.ldc;
-      ldd = args.ldd;
+
+      output_op = args.epilogue;
+
+      semaphore = static_cast<int *>(workspace);
     }
+
   };
 
   /// Shared memory storage structure
-  struct SharedStorage {
-    union {
-      typename Mma::SharedStorage main_loop;
-      typename Epilogue::SharedStorage epilogue;
-    } kernel;
-
-    // ProblemVisitor shared storage can't be overlapped with others
-    typename ProblemVisitor::SharedStorage problem_visitor;
+  union SharedStorage {
+    typename Mma::SharedStorage main_loop;
+    typename Epilogue::SharedStorage epilogue;
   };
 
 public:
 
   //
   // Methods
   //
 
   CUTLASS_DEVICE
-  GemmGroupedSoftmaxMainloopFusion() { }
+  RankKUniversal() { } 
 
   /// Determines whether kernel satisfies alignment
-  static Status can_implement(cutlass::gemm::GemmCoord const & problem_size) {
+  static Status can_implement(
+    cutlass::gemm::GemmCoord const & problem_size) {
+
+    static int const kAlignmentA = Mma::IteratorA::AccessType::kElements;
+    static int const kAlignmentB = Mma::IteratorB::AccessType::kElements;
+    static int const kAlignmentC = Epilogue::OutputTileIterator::kElementsPerAccess;
+
+    if ((problem_size.m() % kAlignmentA) || (problem_size.k() % kAlignmentA) ||
+      (problem_size.n() % kAlignmentB) || (problem_size.k() % kAlignmentB) ||
+      (problem_size.m() % kAlignmentC) || (problem_size.n() % kAlignmentC)) {
+
+      return Status::kErrorMisalignedOperand;
+    }
+
     return Status::kSuccess;
   }
 
   static Status can_implement(Arguments const &args) {
-    return Status::kSuccess;
+    return can_implement(args.problem_size);
   }
 
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
+    // Compute threadblock location
+    ThreadblockSwizzle threadblock_swizzle;
+
+    cutlass::gemm::GemmCoord threadblock_tile_offset =
+        threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
+
+    // Early exit if CTA is out of range
+    if (params.grid_tiled_shape.m() <= threadblock_tile_offset.m() ||
+      params.grid_tiled_shape.n() <= threadblock_tile_offset.n()) {
+      return;
+    }
+   
+    // Early exit if Fill Mode is Lower and
+    // if the entire tile is above the main diagonal (bottom-left corner is at or above the diagonal)
+    if (kFillModeC == cutlass::FillMode::kLower &&
+        (threadblock_tile_offset.m() + 1) * Mma::Shape::kM <= threadblock_tile_offset.n() * Mma::Shape::kN) {
+      return;
+    }    
+    
+    // Early exit if Fill Mode is Upper and
+    // if the entire tile is below the main diagonal (top-right corner is at or below the diagonal)
+    if (kFillModeC == cutlass::FillMode::kUpper &&
+        threadblock_tile_offset.m() * Mma::Shape::kM >= (threadblock_tile_offset.n() + 1) * Mma::Shape::kN) {
+      return;
+    }    
+    
+    bool tile_on_diagonal = false;
+    // Mark tiles that are being crossed by the main diagonal
+    // (top-right and bottom-left corners are on either side of the diagonal)
+    if ((threadblock_tile_offset.m() + 1) * Mma::Shape::kM > threadblock_tile_offset.n() * Mma::Shape::kN
+        && threadblock_tile_offset.m() * Mma::Shape::kM < (threadblock_tile_offset.n() + 1) * Mma::Shape::kN) {
+      tile_on_diagonal = true;
+    }
+
+    int offset_k = 0;
+    int problem_size_k = params.problem_size.k();
+
+    ElementA *ptr_A = static_cast<ElementA *>(params.ptr_A); 
+    ElementB *ptr_B = static_cast<ElementB *>(params.ptr_B);
+
+    //
+    // Fetch pointers based on mode.
+    //
+    if (params.mode == GemmUniversalMode::kGemm || 
+      params.mode == GemmUniversalMode::kGemmSplitKParallel) {
+
+      if (threadblock_tile_offset.k() + 1 < params.grid_tiled_shape.k()) {
+
+        problem_size_k = (threadblock_tile_offset.k() + 1) * params.gemm_k_size; 
+      }
+
+      offset_k = threadblock_tile_offset.k() * params.gemm_k_size;
+    }
+    else if (params.mode == GemmUniversalMode::kBatched) {
+      ptr_A += threadblock_tile_offset.k() * params.batch_stride_A;
+      ptr_B += threadblock_tile_offset.k() * params.batch_stride_B;
+    }
+    else if (params.mode == GemmUniversalMode::kArray) {
+      ptr_A = static_cast<ElementA * const *>(params.ptr_A)[threadblock_tile_offset.k()];
+      ptr_B = static_cast<ElementB * const *>(params.ptr_B)[threadblock_tile_offset.k()];
+    }
+
+    __syncthreads();
+
+    // Compute initial location in logical coordinates
+    cutlass::MatrixCoord tb_offset_A{
+      threadblock_tile_offset.m() * Mma::Shape::kM,
+      offset_k,
+    };
+
+    cutlass::MatrixCoord tb_offset_B{
+      offset_k,
+      threadblock_tile_offset.n() * Mma::Shape::kN
+    };
+
+
+    // Compute position within threadblock
+    int thread_idx = threadIdx.x;
+
+    // Construct iterators to A and B operands
+    typename Mma::IteratorA iterator_A(
+      params.params_A,
+      ptr_A,
+      {params.problem_size.m(), problem_size_k},
+      thread_idx,
+      tb_offset_A);
+
+    typename Mma::IteratorB iterator_B(
+      params.params_B,
+      ptr_B,
+      {problem_size_k, params.problem_size.n()},
+      thread_idx,
+      tb_offset_B);
+
+    // Broadcast the warp_id computed by lane 0 to ensure dependent code
+    // is compiled as warp-uniform.
+    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
+
+    int lane_idx = threadIdx.x % 32;
+
+    //
+    // Main loop
+    //
+
+    // Construct thread-scoped matrix multiply
+    Mma mma(shared_storage.main_loop, thread_idx, warp_idx, lane_idx);
+
+    typename Mma::FragmentC accumulators;
+
+    accumulators.clear();
+
+    // Compute threadblock-scoped matrix multiply-add
+    int gemm_k_iterations = (problem_size_k - offset_k + Mma::Shape::kK - 1) / Mma::Shape::kK;
+
+    // Compute threadblock-scoped matrix multiply-add
+    mma(
+      gemm_k_iterations, 
+      accumulators, 
+      iterator_A, 
+      iterator_B, 
+      accumulators);
+
+    //
+    // Epilogue
+    //
+
+    EpilogueOutputOp output_op(params.output_op);
+
     //
-    // These types shadow the type-level definitions and support the ability to implement
-    // a 'transposed' GEMM that computes the transposed problems.
+    // Masked tile iterators constructed from members
     //
-    using ElementA = typename Mma::IteratorA::Element;
-    using LayoutA = typename Mma::IteratorA::Layout;
-    using ElementB = typename Mma::IteratorB::Element;
-    using LayoutB = typename Mma::IteratorB::Layout;
-    using ElementC = typename Epilogue::OutputTileIterator::Element;
-    using LayoutC = typename Epilogue::OutputTileIterator::Layout;
-
-    //
-    // Problem visitor.
-    //
-    ProblemVisitor problem_visitor(
-      params.problem_visitor,
-      shared_storage.problem_visitor,
-      blockIdx.x);
-
-    // Outer 'persistent' loop to iterate over tiles
-    while (problem_visitor.next_tile()) {
-
-      GemmCoord problem_size  = problem_visitor.problem_size();
-      int32_t problem_idx     = problem_visitor.problem_index();
-      int32_t threadblock_idx = int32_t(problem_visitor.threadblock_idx());
-
-      GemmCoord grid_shape = problem_visitor.grid_shape(problem_size);
-
-      cutlass::gemm::GemmCoord threadblock_offset(
-        int(threadblock_idx / grid_shape.n()) * Mma::Shape::kM,
-        int(threadblock_idx % grid_shape.n()) * Mma::Shape::kN,
-        0);
-
-      // Load element pointers. Exchange pointers and strides if working on the transpose
-      ElementA *ptr_A = reinterpret_cast<ElementA *>((kTransposed ? params.ptr_B[problem_idx] : params.ptr_A[problem_idx]));
-      typename LayoutA::LongIndex ldm_A = (kTransposed ? params.ldb[problem_idx] : params.lda[problem_idx]);
-
-      ElementB *ptr_B = reinterpret_cast<ElementB *>((kTransposed ? params.ptr_A[problem_idx] : params.ptr_B[problem_idx]));
-      typename LayoutB::LongIndex ldm_B = (kTransposed ? params.lda[problem_idx] : params.ldb[problem_idx]);
-
-      // Compute initial location in logical coordinates
-      cutlass::MatrixCoord tb_offset_A{
-        threadblock_offset.m(),
-        0,
-      };
-
-      cutlass::MatrixCoord tb_offset_B{
-        0,
-        threadblock_offset.n()
-      };
-
-      // Compute position within threadblock
-      int thread_idx = threadIdx.x;
-
-      // Construct iterators to A and B operands
-      typename Mma::IteratorA iterator_A(
-        LayoutA(ldm_A),
-        ptr_A,
-        {problem_size.m(), problem_size.k()},
-        thread_idx,
-        tb_offset_A);
-
-      typename Mma::IteratorB iterator_B(
-        LayoutB(ldm_B),
-        ptr_B,
-        {problem_size.k(), problem_size.n()},
-        thread_idx,
-        tb_offset_B);
-
-      // Construct iterator to the softmax norm/sum vector
-      typename Mma::IteratorNormSum iterator_norm_sum(
-        problem_size.m(),
-        static_cast<ElementScaleBias const *>(params.ptr_norm[problem_idx]),
-        static_cast<ElementScaleBias const *>(params.ptr_sum[problem_idx]),
-        thread_idx,
-        MatrixCoord(0, threadblock_offset.m())
-      );
-
-      typename Mma::FragmentC accumulators;
-
-      accumulators.clear();
-
-      // Broadcast the warp_id computed by lane 0 to ensure dependent code
-      // is compiled as warp-uniform.
-      int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
-
-      int lane_idx = threadIdx.x % 32;
-
-      //
-      // Matrix multiply phase
-      //
-
-      // Construct thread-scoped matrix multiply
-      Mma mma(shared_storage.kernel.main_loop, thread_idx, warp_idx, lane_idx);
-
-      // Compute threadblock-scoped matrix multiply-add
-      int gemm_k_iterations = (problem_size.k() + Mma::Shape::kK - 1) / Mma::Shape::kK;
-
-      // Wait for all threads to finish their epilogue phases from the previous tile.
-      __syncthreads();
-
-      // Compute threadblock-scoped matrix multiply-add
-      mma(
-        gemm_k_iterations,
-        accumulators,
-        iterator_A,
-        iterator_B,
-        iterator_norm_sum,
-        accumulators);
-
-      //
-      // Epilogue
-      //
-
-      EpilogueOutputOp output_op(params.output_op);
-
-      ElementC *ptr_C = params.ptr_C[problem_idx];
-      ElementC *ptr_D = params.ptr_D[problem_idx];
-
-      LayoutC layout_C(params.ldc[problem_idx]);
-      LayoutC layout_D(params.ldd[problem_idx]);
-
-      typename Epilogue::OutputTileIterator::Params params_C(layout_C);
-      typename Epilogue::OutputTileIterator::Params params_D(layout_D);
-
-      // Tile iterator loading from source tensor.
-      typename Epilogue::OutputTileIterator iterator_C(
-        params_C,
-        ptr_C,
-        problem_size.mn(),
-        thread_idx,
-        threadblock_offset.mn()
-      );
-
-      // Tile iterator writing to destination tensor.
-      typename Epilogue::OutputTileIterator iterator_D(
-        params_D,
-        ptr_D,
-        problem_size.mn(),
-        thread_idx,
-        threadblock_offset.mn()
-      );
-
-      Epilogue epilogue(
-        shared_storage.kernel.epilogue,
-        thread_idx,
-        warp_idx,
-        lane_idx);
-
-      // Execute the epilogue operator to update the destination tensor.
-      epilogue(
-        output_op,
-        iterator_D,
-        accumulators,
-        iterator_C);
 
-      // Next tile
-      problem_visitor.advance(gridDim.x);
+    threadblock_tile_offset =
+        threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
+
+    //assume identity swizzle
+    MatrixCoord threadblock_offset(
+      threadblock_tile_offset.m() * Mma::Shape::kM,
+      threadblock_tile_offset.n() * Mma::Shape::kN
+    );
+
+    int block_idx = threadblock_tile_offset.m() + threadblock_tile_offset.n() * params.grid_tiled_shape.m();
+
+    ElementC *ptr_C = static_cast<ElementC *>(params.ptr_C); 
+    ElementC *ptr_D = static_cast<ElementC *>(params.ptr_D);
+
+    //
+    // Fetch pointers based on mode.
+    //
+    
+    // Construct the semaphore.
+    Semaphore semaphore(params.semaphore + block_idx, thread_idx);
+
+    if (params.mode == GemmUniversalMode::kGemm) {
+
+      // If performing a reduction via split-K, fetch the initial synchronization
+      if (params.grid_tiled_shape.k() > 1) {
+        
+        // Fetch the synchronization lock initially but do not block.
+        semaphore.fetch();
+
+        // Indicate which position in a serial reduction the output operator is currently updating
+        output_op.set_k_partition(threadblock_tile_offset.k(), params.grid_tiled_shape.k());
+      }
+    }
+    else if (params.mode == GemmUniversalMode::kGemmSplitKParallel) {
+      ptr_D += threadblock_tile_offset.k() * params.batch_stride_D;
+    }
+    else if (params.mode == GemmUniversalMode::kBatched) {
+      ptr_C += threadblock_tile_offset.k() * params.batch_stride_C;
+      ptr_D += threadblock_tile_offset.k() * params.batch_stride_D;
+    }
+    else if (params.mode == GemmUniversalMode::kArray) {
+      ptr_C = static_cast<ElementC * const *>(params.ptr_C)[threadblock_tile_offset.k()];
+      ptr_D = static_cast<ElementC * const *>(params.ptr_D)[threadblock_tile_offset.k()];
+    }
+
+    
+    // If CTA not on diagonal, FillMode doesn't apply. 
+    FillMode kFillModeCTA = tile_on_diagonal ? kFillModeC : FillMode::kNone;
+
+    // Tile iterator loading from source tensor.
+    typename Epilogue::OutputTileIterator iterator_C(
+      params.params_C,
+      ptr_C,
+      params.problem_size.mn(),
+      thread_idx,
+      threadblock_offset,
+      kFillModeCTA
+    );
+
+    // Tile iterator writing to destination tensor.
+    typename Epilogue::OutputTileIterator iterator_D(
+      params.params_D,
+      ptr_D,
+      params.problem_size.mn(),
+      thread_idx,
+      threadblock_offset,
+      kFillModeCTA
+    );
+
+    Epilogue epilogue(
+      shared_storage.epilogue, 
+      thread_idx, 
+      warp_idx, 
+      lane_idx);
+
+    // Wait on the semaphore - this latency may have been covered by iterator construction
+    if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) {
+        
+      // For subsequent threadblocks, the source matrix is held in the 'D' tensor.
+      if (threadblock_tile_offset.k()) {
+        iterator_C = iterator_D;
+      }
+
+      semaphore.wait(threadblock_tile_offset.k());
+
+      __threadfence();
+    }
+
+    // Execute the epilogue operator to update the destination tensor.
+    epilogue(
+      output_op, 
+      iterator_D, 
+      accumulators, 
+      iterator_C); 
+    
+    //
+    // Release the semaphore
+    //
+
+    if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) { 
+
+      int lock = 0;
+      if (params.grid_tiled_shape.k() == threadblock_tile_offset.k() + 1) {
+
+        // The final threadblock resets the semaphore for subsequent grids.
+        lock = 0;
+      }
+      else {
+        // Otherwise, the semaphore is incremented
+        lock = threadblock_tile_offset.k() + 1;
+      }
+      
+      semaphore.release(lock);
     }
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace kernel
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_layernorm_mainloop_fusion.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_layernorm_mainloop_fusion.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_params.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_params.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_pipelined.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_pipelined.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex_array.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex_array.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_splitk_parallel.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_splitk_parallel.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_transpose_operands.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_transpose_operands.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_universal.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_universal.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_universal_streamk.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_universal_streamk.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_with_fused_epilogue.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_with_fused_epilogue.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_with_k_reduction.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_with_k_reduction.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemv.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemv.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemv_batched_strided.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemv_batched_strided.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/grouped_problem_visitor.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/grouped_problem_visitor.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/params_universal_base.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/params_universal_base.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped_problem_visitor.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped_problem_visitor.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_transpose_operands.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_transpose_operands.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_universal.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_universal.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_k_universal.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/trmm_universal.h`

 * *Files 18% similar despite different names*

```diff
@@ -38,44 +38,48 @@
 
 #include "cutlass/blas3.h"
 #include "cutlass/fast_math.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/matrix_coord.h"
 #include "cutlass/complex.h"
 #include "cutlass/semaphore.h"
-
+#include "cutlass/core_io.h"
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace kernel {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
   typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate 
   typename Epilogue_,             ///! Epilogue
   typename ThreadblockSwizzle_,   ///! Threadblock swizzling function
-  FillMode FillModeC_             ///! Fill Mode for C (kLower or kUpper)
+  SideMode SideMode_,             ///! Side Mode for the kernel (kLeft or kRight)
+  FillMode FillMode_,             ///! Fill Mode for triangular matrix (kLower or kUpper)
+  DiagType DiagType_              ///! Diag Type for triangular matrix (kNonUnit or kUnit)
 >
-struct RankKUniversal {
+struct TrmmUniversal {
 public:
 
   using Mma = Mma_;
   using Epilogue = Epilogue_;
   using EpilogueOutputOp = typename Epilogue::OutputOp;
   using ThreadblockSwizzle = ThreadblockSwizzle_;
 
   using ElementA = typename Mma::IteratorA::Element;
   using LayoutA = typename Mma::IteratorA::Layout;
   using ElementB = typename Mma::IteratorB::Element;
   using LayoutB = typename Mma::IteratorB::Layout;
   using ElementC = typename Epilogue::OutputTileIterator::Element;
   using LayoutC = typename Epilogue::OutputTileIterator::Layout;
-  static FillMode const kFillModeC = FillModeC_;
+  static SideMode const kSideMode = SideMode_;
+  static FillMode const kFillMode = FillMode_;
+  static DiagType const kDiagType = DiagType_;
 
   static ComplexTransform const kTransformA = Mma::kTransformA;
   static ComplexTransform const kTransformB = Mma::kTransformB;
   using Operator = typename Mma::Operator;
 
   using OperatorClass = typename Mma::Operator::OperatorClass;
   using ThreadblockShape = typename Mma::Shape;
@@ -89,15 +93,15 @@
   static int const kAlignmentC = Epilogue::OutputTileIterator::kElementsPerAccess;
 
   /// Warp count (concept: GemmShape)
   using WarpCount = typename Mma::WarpCount;
   static int const kThreadCount = 32 * WarpCount::kCount;
 
   /// Split-K preserves splits that are 128b aligned
-  static int const kSplitKAlignment = 128 / sizeof_bits<ElementA>::value;
+  static int const kSplitKAlignment = const_max(128 / sizeof_bits<ElementA>::value, 128 / sizeof_bits<ElementB>::value);
 
   //
   // Structures
   //
 
   /// Argument structure
   struct Arguments {
@@ -109,61 +113,78 @@
     GemmUniversalMode mode;
     GemmCoord problem_size;
     int batch_count;
 
     typename EpilogueOutputOp::Params epilogue;
 
     void const * ptr_A;
-    void const * ptr_C;
+    void const * ptr_B;
     void * ptr_D;
 
     int64_t batch_stride_A;
-    int64_t batch_stride_C;
+    int64_t batch_stride_B;
     int64_t batch_stride_D;
 
     typename LayoutA::Stride::Index lda;
     typename LayoutB::Stride::Index ldb;
-    typename LayoutC::Stride::Index ldc;
     typename LayoutC::Stride::Index ldd;
 
     //
     // Methods
     //
     
     Arguments(): 
       mode(GemmUniversalMode::kGemm), 
       batch_count(1), 
-      ptr_A(nullptr), ptr_C(nullptr), ptr_D(nullptr) { }
+      ptr_A(nullptr), ptr_B(nullptr), ptr_D(nullptr) { }
 
     /// constructs an arguments structure
     Arguments(
       GemmUniversalMode mode,
       GemmCoord problem_size,
       int batch_count,
       typename EpilogueOutputOp::Params epilogue,
       void const * ptr_A,
-      void const * ptr_C,
+      void const * ptr_B,
       void * ptr_D,
       int64_t batch_stride_A,
-      int64_t batch_stride_C,
+      int64_t batch_stride_B,
       int64_t batch_stride_D,
       typename LayoutA::Stride::Index lda,
-      typename LayoutC::Stride::Index ldc,
+      typename LayoutB::Stride::Index ldb,
       typename LayoutC::Stride::Index ldd
     ):
       mode(mode), 
-      problem_size(problem_size), 
+      problem_size(problem_size),
       batch_count(batch_count),
       epilogue(epilogue), 
-      ptr_A(ptr_A), ptr_C(ptr_C), ptr_D(ptr_D), 
-      batch_stride_A(batch_stride_A), batch_stride_C(batch_stride_C), batch_stride_D(batch_stride_D), 
-      lda(lda), ldb(ldb), ldc(ldc), ldd(ldd) {
-
+      ptr_A(ptr_A), ptr_B(ptr_B), ptr_D(ptr_D), 
+      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_D(batch_stride_D), 
+      lda(lda), ldb(ldb), ldd(ldd) {
       }
 
+    /// Returns arguments for the transposed problem sizes
+    Arguments transposed_problem_size() const {
+      Arguments args(*this);
+
+      std::swap(args.problem_size.m(), args.problem_size.n());
+
+      return args;
+    }
+
+    /// Returns arguments for the transposed matrices
+    Arguments swapped_matrices() const {
+      Arguments args(*this);
+
+      std::swap(args.ptr_A, args.ptr_B);
+      std::swap(args.lda, args.ldb);
+      std::swap(args.batch_stride_A, args.batch_stride_B);
+
+      return args;
+    }
   };
 
   //
   // Structure for precomputing values in host memory and passing to kernels
   //
 
   /// Parameters structure
@@ -171,98 +192,92 @@
 
     cutlass::gemm::GemmCoord problem_size;
     cutlass::gemm::GemmCoord grid_tiled_shape;
     int swizzle_log_tile;
    
     typename Mma::IteratorA::Params params_A;
     typename Mma::IteratorB::Params params_B;
-    typename Epilogue::OutputTileIterator::Params params_C;
     typename Epilogue::OutputTileIterator::Params params_D;
     
     typename EpilogueOutputOp::Params output_op;
 
     GemmUniversalMode mode;
     int batch_count;
     int gemm_k_size;
 
     void * ptr_A;
     void * ptr_B;
-    void * ptr_C;
     void * ptr_D;
 
     int64_t batch_stride_A;
     int64_t batch_stride_B;
-    int64_t batch_stride_C;
     int64_t batch_stride_D;
 
     int *semaphore;
 
     //
     // Methods
     //
 
     CUTLASS_HOST_DEVICE
     Params():
       swizzle_log_tile(0),
       params_A(0),
       params_B(0),
-      params_C(0),
       params_D(0),
       batch_count(0),
       gemm_k_size(0),
       mode(cutlass::gemm::GemmUniversalMode::kGemm),
       ptr_A(nullptr),
       ptr_B(nullptr),
-      ptr_C(nullptr),
       ptr_D(nullptr),
       batch_stride_A(0),
       batch_stride_B(0),
-      batch_stride_C(0),
       batch_stride_D(0),
       semaphore(nullptr) { }
 
     CUTLASS_HOST_DEVICE
     Params(
       Arguments const &args,
       cutlass::gemm::GemmCoord const & grid_tiled_shape,
       int gemm_k_size,
       void *workspace = nullptr
     ):
       problem_size(args.problem_size),
       grid_tiled_shape(grid_tiled_shape),
       swizzle_log_tile(ThreadblockSwizzle().get_log_tile(grid_tiled_shape)),
       params_A(args.lda),
-      params_B(args.lda),
-      params_C(args.ldc),
+      params_B(args.ldb),
       params_D(args.ldd),
       output_op(args.epilogue),
       mode(args.mode),
       batch_count(args.batch_count),
       gemm_k_size(gemm_k_size),
       ptr_A(const_cast<void *>(args.ptr_A)),
-      ptr_B(const_cast<void *>(args.ptr_A)),
-      ptr_C(const_cast<void *>(args.ptr_C)),
-      ptr_D(const_cast<void *>(args.ptr_D)),
+      ptr_B(const_cast<void *>(args.ptr_B)),
+      ptr_D(args.ptr_D),
       batch_stride_A(args.batch_stride_A),
-      batch_stride_B(args.batch_stride_A),
-      batch_stride_C(args.batch_stride_C),
+      batch_stride_B(args.batch_stride_B),
       batch_stride_D(args.batch_stride_D),
       semaphore(static_cast<int *>(workspace)) {
     }
 
     CUTLASS_HOST_DEVICE
     void update(
       Arguments const &args,
       void *workspace = nullptr) {
 
       ptr_A = const_cast<void *>(args.ptr_A);
-      ptr_B = const_cast<void *>(args.ptr_A);
-      ptr_C = const_cast<void *>(args.ptr_C);
+      ptr_B = const_cast<void *>(args.ptr_B);
       ptr_D = args.ptr_D;
 
+      batch_stride_A = args.batch_stride_A;
+      batch_stride_B = args.batch_stride_B;
+      batch_stride_D = args.batch_stride_D;
+
       output_op = args.epilogue;
 
       semaphore = static_cast<int *>(workspace);
     }
 
   };
 
@@ -275,15 +290,15 @@
 public:
 
   //
   // Methods
   //
 
   CUTLASS_DEVICE
-  RankKUniversal() { } 
+  TrmmUniversal() { } 
 
   /// Determines whether kernel satisfies alignment
   static Status can_implement(
     cutlass::gemm::GemmCoord const & problem_size) {
 
     static int const kAlignmentA = Mma::IteratorA::AccessType::kElements;
     static int const kAlignmentB = Mma::IteratorB::AccessType::kElements;
@@ -306,44 +321,22 @@
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
     // Compute threadblock location
     ThreadblockSwizzle threadblock_swizzle;
 
-    cutlass::gemm::GemmCoord threadblock_tile_offset =
-        threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
+    cutlass::gemm::GemmCoord threadblock_tile_offset = threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
 
     // Early exit if CTA is out of range
     if (params.grid_tiled_shape.m() <= threadblock_tile_offset.m() ||
       params.grid_tiled_shape.n() <= threadblock_tile_offset.n()) {
+
       return;
     }
-   
-    // Early exit if Fill Mode is Lower and
-    // if the entire tile is above the main diagonal (bottom-left corner is at or above the diagonal)
-    if (kFillModeC == cutlass::FillMode::kLower &&
-        (threadblock_tile_offset.m() + 1) * Mma::Shape::kM <= threadblock_tile_offset.n() * Mma::Shape::kN) {
-      return;
-    }    
-    
-    // Early exit if Fill Mode is Upper and
-    // if the entire tile is below the main diagonal (top-right corner is at or below the diagonal)
-    if (kFillModeC == cutlass::FillMode::kUpper &&
-        threadblock_tile_offset.m() * Mma::Shape::kM >= (threadblock_tile_offset.n() + 1) * Mma::Shape::kN) {
-      return;
-    }    
-    
-    bool tile_on_diagonal = false;
-    // Mark tiles that are being crossed by the main diagonal
-    // (top-right and bottom-left corners are on either side of the diagonal)
-    if ((threadblock_tile_offset.m() + 1) * Mma::Shape::kM > threadblock_tile_offset.n() * Mma::Shape::kN
-        && threadblock_tile_offset.m() * Mma::Shape::kM < (threadblock_tile_offset.n() + 1) * Mma::Shape::kN) {
-      tile_on_diagonal = true;
-    }
 
     int offset_k = 0;
     int problem_size_k = params.problem_size.k();
 
     ElementA *ptr_A = static_cast<ElementA *>(params.ptr_A); 
     ElementB *ptr_B = static_cast<ElementB *>(params.ptr_B);
 
@@ -378,33 +371,17 @@
     };
 
     cutlass::MatrixCoord tb_offset_B{
       offset_k,
       threadblock_tile_offset.n() * Mma::Shape::kN
     };
 
-
     // Compute position within threadblock
     int thread_idx = threadIdx.x;
 
-    // Construct iterators to A and B operands
-    typename Mma::IteratorA iterator_A(
-      params.params_A,
-      ptr_A,
-      {params.problem_size.m(), problem_size_k},
-      thread_idx,
-      tb_offset_A);
-
-    typename Mma::IteratorB iterator_B(
-      params.params_B,
-      ptr_B,
-      {problem_size_k, params.problem_size.n()},
-      thread_idx,
-      tb_offset_B);
-
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
     int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
 
     int lane_idx = threadIdx.x % 32;
 
     //
@@ -416,14 +393,79 @@
 
     typename Mma::FragmentC accumulators;
 
     accumulators.clear();
 
     // Compute threadblock-scoped matrix multiply-add
     int gemm_k_iterations = (problem_size_k - offset_k + Mma::Shape::kK - 1) / Mma::Shape::kK;
+    
+    /******************************************************************************************************
+      First two cases: (Left Side, Lower Fill) and (Right Side, Upper Fill) are transpose of each other
+        - (Left Side, Lower Fill): calculate bottom of the CTA tile,  then find the k-iterations 
+                                    needed to process all elements till that coordinate.
+        - (Right Side, Upper Fill): calculate right end of the CTA tile,  then find the k-iterations 
+                                    needed to process all elements till that coordinate.
+
+      Last two cases: (Left Side, Upper Fill) and (Right Side, Lower Fill) are transpose of each other
+        - (Left Side, Upper Fill): calculate the top of the CTA tile, then find k-iterations 
+                                   that can be skipped for all elements of this tile.
+        - (Right Side, Lower Fill): calculate the left start of the CTA tile, then find k-iterations 
+                                    that can be skipped for all elements of this tile.
+    ********************************************************************************************************/
+ 
+    if (kSideMode == SideMode::kLeft && kFillMode == FillMode::kLower) {
+
+      int k_iterations_till_diagonal = ((threadblock_tile_offset.m() + 1) * Mma::Shape::kM + Mma::Shape::kK - 1) / Mma::Shape::kK;
+      if (k_iterations_till_diagonal < gemm_k_iterations) {
+        gemm_k_iterations = k_iterations_till_diagonal;
+      }
+
+    } else if (kSideMode == SideMode::kRight && kFillMode == FillMode::kUpper) {
+
+      int k_iterations_till_diagonal = ((threadblock_tile_offset.n() + 1) * Mma::Shape::kN + Mma::Shape::kK - 1) / Mma::Shape::kK;
+      if (k_iterations_till_diagonal < gemm_k_iterations) {
+        gemm_k_iterations = k_iterations_till_diagonal;
+      }
+
+    } else if (kSideMode == SideMode::kLeft && kFillMode == FillMode::kUpper) {
+
+      int k_iterations_till_diagonal = ((threadblock_tile_offset.m()) * Mma::Shape::kM) / Mma::Shape::kK;
+
+      if (k_iterations_till_diagonal != 0) {
+        tb_offset_A += cutlass::MatrixCoord({0, k_iterations_till_diagonal * Mma::Shape::kK});
+        tb_offset_B += cutlass::MatrixCoord({k_iterations_till_diagonal * Mma::Shape::kK, 0});
+        gemm_k_iterations -= k_iterations_till_diagonal;
+      }
+
+    } else if (kSideMode == SideMode::kRight && kFillMode == FillMode::kLower) {
+
+      int k_iterations_till_diagonal = ((threadblock_tile_offset.n()) * Mma::Shape::kN) / Mma::Shape::kK;
+
+      if (k_iterations_till_diagonal != 0) {
+        tb_offset_A += cutlass::MatrixCoord({0, k_iterations_till_diagonal * Mma::Shape::kK});
+        tb_offset_B += cutlass::MatrixCoord({k_iterations_till_diagonal * Mma::Shape::kK, 0});
+        gemm_k_iterations -= k_iterations_till_diagonal;
+      }
+
+    }
+
+    // Construct iterators to A and B operands
+    typename Mma::IteratorA iterator_A(
+      params.params_A,
+      ptr_A,
+      {params.problem_size.m(), problem_size_k},
+      thread_idx,
+      tb_offset_A);
+
+    typename Mma::IteratorB iterator_B(
+      params.params_B,
+      ptr_B,
+      {problem_size_k, params.problem_size.n()},
+      thread_idx,
+      tb_offset_B);
 
     // Compute threadblock-scoped matrix multiply-add
     mma(
       gemm_k_iterations, 
       accumulators, 
       iterator_A, 
       iterator_B, 
@@ -435,26 +477,24 @@
 
     EpilogueOutputOp output_op(params.output_op);
 
     //
     // Masked tile iterators constructed from members
     //
 
-    threadblock_tile_offset =
-        threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
+    threadblock_tile_offset = threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
 
     //assume identity swizzle
     MatrixCoord threadblock_offset(
       threadblock_tile_offset.m() * Mma::Shape::kM,
       threadblock_tile_offset.n() * Mma::Shape::kN
     );
 
     int block_idx = threadblock_tile_offset.m() + threadblock_tile_offset.n() * params.grid_tiled_shape.m();
 
-    ElementC *ptr_C = static_cast<ElementC *>(params.ptr_C); 
     ElementC *ptr_D = static_cast<ElementC *>(params.ptr_D);
 
     //
     // Fetch pointers based on mode.
     //
     
     // Construct the semaphore.
@@ -472,44 +512,37 @@
         output_op.set_k_partition(threadblock_tile_offset.k(), params.grid_tiled_shape.k());
       }
     }
     else if (params.mode == GemmUniversalMode::kGemmSplitKParallel) {
       ptr_D += threadblock_tile_offset.k() * params.batch_stride_D;
     }
     else if (params.mode == GemmUniversalMode::kBatched) {
-      ptr_C += threadblock_tile_offset.k() * params.batch_stride_C;
       ptr_D += threadblock_tile_offset.k() * params.batch_stride_D;
     }
     else if (params.mode == GemmUniversalMode::kArray) {
-      ptr_C = static_cast<ElementC * const *>(params.ptr_C)[threadblock_tile_offset.k()];
       ptr_D = static_cast<ElementC * const *>(params.ptr_D)[threadblock_tile_offset.k()];
     }
 
     
-    // If CTA not on diagonal, FillMode doesn't apply. 
-    FillMode kFillModeCTA = tile_on_diagonal ? kFillModeC : FillMode::kNone;
-
-    // Tile iterator loading from source tensor.
+    // Tile iterator loading from source tensor (although irrelevant to this kernel as beta is zero).
     typename Epilogue::OutputTileIterator iterator_C(
-      params.params_C,
-      ptr_C,
+      params.params_D,
+      ptr_D,
       params.problem_size.mn(),
       thread_idx,
-      threadblock_offset,
-      kFillModeCTA
+      threadblock_offset
     );
 
     // Tile iterator writing to destination tensor.
     typename Epilogue::OutputTileIterator iterator_D(
       params.params_D,
       ptr_D,
       params.problem_size.mn(),
       thread_idx,
-      threadblock_offset,
-      kFillModeCTA
+      threadblock_offset
     );
 
     Epilogue epilogue(
       shared_storage.epilogue, 
       thread_idx, 
       warp_idx, 
       lane_idx);
@@ -523,14 +556,15 @@
       }
 
       semaphore.wait(threadblock_tile_offset.k());
 
       __threadfence();
     }
 
+
     // Execute the epilogue operator to update the destination tensor.
     epilogue(
       output_op, 
       iterator_D, 
       accumulators, 
       iterator_C);
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/sparse_gemm.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/sparse_gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/symm_universal.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/symm_universal.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/trmm_universal.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/gemm_universal_with_visitor.h`

 * *Files 20% similar despite different names*

```diff
@@ -27,294 +27,406 @@
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
     \brief 
-
 */
 
 #pragma once
 
-#include "cutlass/blas3.h"
+#include "cutlass/cutlass.h"
 #include "cutlass/fast_math.h"
 #include "cutlass/gemm/gemm.h"
+#include "cutlass/gemm/kernel/params_universal_base.h"
 #include "cutlass/matrix_coord.h"
 #include "cutlass/complex.h"
 #include "cutlass/semaphore.h"
-#include "cutlass/core_io.h"
+
+#include "cutlass/layout/matrix.h"
+
+#include "cutlass/trace.h"
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace kernel {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
-  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate 
+  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate
   typename Epilogue_,             ///! Epilogue
-  typename ThreadblockSwizzle_,   ///! Threadblock swizzling function
-  SideMode SideMode_,             ///! Side Mode for the kernel (kLeft or kRight)
-  FillMode FillMode_,             ///! Fill Mode for triangular matrix (kLower or kUpper)
-  DiagType DiagType_              ///! Diag Type for triangular matrix (kNonUnit or kUnit)
+  typename ThreadblockSwizzle_    ///! Threadblock swizzling function
 >
-struct TrmmUniversal {
+struct GemmUniversalwithEpilogueVisitor {
 public:
 
   using Mma = Mma_;
   using Epilogue = Epilogue_;
-  using EpilogueOutputOp = typename Epilogue::OutputOp;
+  using EpilogueVisitor = typename Epilogue::Visitor;
   using ThreadblockSwizzle = ThreadblockSwizzle_;
 
   using ElementA = typename Mma::IteratorA::Element;
   using LayoutA = typename Mma::IteratorA::Layout;
   using ElementB = typename Mma::IteratorB::Element;
   using LayoutB = typename Mma::IteratorB::Layout;
-  using ElementC = typename Epilogue::OutputTileIterator::Element;
-  using LayoutC = typename Epilogue::OutputTileIterator::Layout;
-  static SideMode const kSideMode = SideMode_;
-  static FillMode const kFillMode = FillMode_;
-  static DiagType const kDiagType = DiagType_;
+  using ElementC = typename EpilogueVisitor::ElementOutput;
+  using LayoutC = typename EpilogueVisitor::OutputTileIterator::Layout;
 
   static ComplexTransform const kTransformA = Mma::kTransformA;
   static ComplexTransform const kTransformB = Mma::kTransformB;
   using Operator = typename Mma::Operator;
 
   using OperatorClass = typename Mma::Operator::OperatorClass;
   using ThreadblockShape = typename Mma::Shape;
   using WarpShape = typename Mma::Operator::Shape;
   using InstructionShape = typename Mma::Policy::Operator::InstructionShape;
   using ArchTag = typename Mma::ArchTag;
 
   static int const kStages = Mma::kStages;
   static int const kAlignmentA = Mma::IteratorA::AccessType::kElements;
   static int const kAlignmentB = Mma::IteratorB::AccessType::kElements;
-  static int const kAlignmentC = Epilogue::OutputTileIterator::kElementsPerAccess;
+  static int const kAlignmentC = EpilogueVisitor::kElementsPerAccess;
 
   /// Warp count (concept: GemmShape)
   using WarpCount = typename Mma::WarpCount;
   static int const kThreadCount = 32 * WarpCount::kCount;
 
   /// Split-K preserves splits that are 128b aligned
-  static int const kSplitKAlignment = const_max(128 / sizeof_bits<ElementA>::value, 128 / sizeof_bits<ElementB>::value);
+  static int const kSplitKAlignment = const_max(
+    128 / sizeof_bits<ElementA>::value,
+    128 / sizeof_bits<ElementB>::value
+  );
 
   //
   // Structures
   //
 
   /// Argument structure
-  struct Arguments {
+  struct Arguments : UniversalArgumentsBase {
 
     //
     // Data members
     //
 
-    GemmUniversalMode mode;
-    GemmCoord problem_size;
-    int batch_count;
-
-    typename EpilogueOutputOp::Params epilogue;
+    typename EpilogueVisitor::Arguments epilogue_visitor;
 
     void const * ptr_A;
     void const * ptr_B;
+    void const * ptr_C;
     void * ptr_D;
 
     int64_t batch_stride_A;
     int64_t batch_stride_B;
-    int64_t batch_stride_D;
+    int64_t batch_stride_C;
 
-    typename LayoutA::Stride::Index lda;
-    typename LayoutB::Stride::Index ldb;
-    typename LayoutC::Stride::Index ldd;
+    typename LayoutA::Stride stride_a;
+    typename LayoutB::Stride stride_b;
+    typename LayoutC::Stride stride_c;
+    typename LayoutC::Stride stride_d;
+
+    typename LayoutA::Stride::LongIndex lda;
+    typename LayoutB::Stride::LongIndex ldb;
+    typename LayoutC::Stride::LongIndex ldc;
+    typename LayoutC::Stride::LongIndex ldd;
+
+    int const * ptr_gather_A_indices;
+    int const * ptr_gather_B_indices;
+    int const * ptr_scatter_D_indices;
 
     //
     // Methods
     //
     
     Arguments(): 
-      mode(GemmUniversalMode::kGemm), 
-      batch_count(1), 
-      ptr_A(nullptr), ptr_B(nullptr), ptr_D(nullptr) { }
+      ptr_A(nullptr), ptr_B(nullptr), ptr_C(nullptr), ptr_D(nullptr),
+      ptr_gather_A_indices(nullptr),
+      ptr_gather_B_indices(nullptr),
+      ptr_scatter_D_indices(nullptr) {}
 
     /// constructs an arguments structure
     Arguments(
       GemmUniversalMode mode,
       GemmCoord problem_size,
       int batch_count,
-      typename EpilogueOutputOp::Params epilogue,
+      typename EpilogueVisitor::Arguments epilogue_visitor,
       void const * ptr_A,
       void const * ptr_B,
+      void const * ptr_C,
       void * ptr_D,
       int64_t batch_stride_A,
       int64_t batch_stride_B,
+      int64_t batch_stride_C,
       int64_t batch_stride_D,
-      typename LayoutA::Stride::Index lda,
-      typename LayoutB::Stride::Index ldb,
-      typename LayoutC::Stride::Index ldd
+      typename LayoutA::Stride stride_a,
+      typename LayoutB::Stride stride_b,
+      typename LayoutC::Stride stride_c,
+      typename LayoutC::Stride stride_d,
+      int const *ptr_gather_A_indices = nullptr,
+      int const *ptr_gather_B_indices = nullptr,
+      int const *ptr_scatter_D_indices = nullptr
     ):
-      mode(mode), 
-      problem_size(problem_size),
-      batch_count(batch_count),
-      epilogue(epilogue), 
-      ptr_A(ptr_A), ptr_B(ptr_B), ptr_D(ptr_D), 
-      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_D(batch_stride_D), 
-      lda(lda), ldb(ldb), ldd(ldd) {
+      UniversalArgumentsBase(mode, problem_size, batch_count, batch_stride_D),
+      epilogue_visitor(epilogue_visitor), 
+      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D), 
+      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C),
+      stride_a(stride_a), stride_b(stride_b), stride_c(stride_c), stride_d(stride_d),
+      ptr_gather_A_indices(ptr_gather_A_indices), ptr_gather_B_indices(ptr_gather_B_indices),
+      ptr_scatter_D_indices(ptr_scatter_D_indices) {
+      lda = 0;
+      ldb = 0;
+      ldc = 0;
+      ldd = 0;
+      CUTLASS_TRACE_HOST("GemmUniversal::Arguments::Arguments() - problem_size: " << problem_size);
       }
 
-    /// Returns arguments for the transposed problem sizes
-    Arguments transposed_problem_size() const {
-      Arguments args(*this);
-
-      std::swap(args.problem_size.m(), args.problem_size.n());
-
-      return args;
-    }
+    /// constructs an arguments structure
+    Arguments(
+      GemmUniversalMode mode,
+      GemmCoord problem_size,
+      int batch_count,
+      typename EpilogueVisitor::Arguments epilogue_visitor,
+      void const * ptr_A,
+      void const * ptr_B,
+      void const * ptr_C,
+      void * ptr_D,
+      int64_t batch_stride_A,
+      int64_t batch_stride_B,
+      int64_t batch_stride_C,
+      int64_t batch_stride_D,
+      typename LayoutA::Stride::LongIndex lda,
+      typename LayoutB::Stride::LongIndex ldb,
+      typename LayoutC::Stride::LongIndex ldc,
+      typename LayoutC::Stride::LongIndex ldd,
+      int const *ptr_gather_A_indices = nullptr,
+      int const *ptr_gather_B_indices = nullptr,
+      int const *ptr_scatter_D_indices = nullptr
+    ):
+      UniversalArgumentsBase(mode, problem_size, batch_count, batch_stride_D),
+      epilogue_visitor(epilogue_visitor), 
+      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D), 
+      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C),
+      lda(lda), ldb(ldb), ldc(ldc), ldd(ldd),
+      ptr_gather_A_indices(ptr_gather_A_indices), ptr_gather_B_indices(ptr_gather_B_indices),
+      ptr_scatter_D_indices(ptr_scatter_D_indices) {
+      stride_a = make_Coord(lda);
+      stride_b = make_Coord(ldb);
+      stride_c = make_Coord(ldc);
+      stride_d = make_Coord(ldd);
+      CUTLASS_TRACE_HOST("GemmUniversal::Arguments::Arguments() - problem_size: " << problem_size);
+      }
 
-    /// Returns arguments for the transposed matrices
-    Arguments swapped_matrices() const {
+    /// Returns arguments for the transposed problem
+    Arguments transposed_problem() const {
       Arguments args(*this);
-
+      
+      std::swap(args.problem_size.m(), args.problem_size.n());
       std::swap(args.ptr_A, args.ptr_B);
       std::swap(args.lda, args.ldb);
+      std::swap(args.stride_a, args.stride_b);
       std::swap(args.batch_stride_A, args.batch_stride_B);
+      std::swap(args.ptr_gather_A_indices, args.ptr_gather_B_indices);
 
       return args;
     }
   };
 
   //
   // Structure for precomputing values in host memory and passing to kernels
   //
 
   /// Parameters structure
-  struct Params {
+  struct Params : UniversalParamsBase<
+    ThreadblockSwizzle,
+    ThreadblockShape,
+    ElementA,
+    ElementB,
+    ElementC> {
+
+    using ParamsBase = UniversalParamsBase<
+      ThreadblockSwizzle,
+      ThreadblockShape,
+      ElementA,
+      ElementB,
+      ElementC>;
 
-    cutlass::gemm::GemmCoord problem_size;
-    cutlass::gemm::GemmCoord grid_tiled_shape;
-    int swizzle_log_tile;
-   
     typename Mma::IteratorA::Params params_A;
     typename Mma::IteratorB::Params params_B;
-    typename Epilogue::OutputTileIterator::Params params_D;
+    typename EpilogueVisitor::OutputTileIterator::Params params_C;
+    typename EpilogueVisitor::OutputTileIterator::Params params_D;
     
-    typename EpilogueOutputOp::Params output_op;
-
-    GemmUniversalMode mode;
-    int batch_count;
-    int gemm_k_size;
+    typename EpilogueVisitor::Params epilogue_visitor;
 
     void * ptr_A;
     void * ptr_B;
+    void * ptr_C;
     void * ptr_D;
 
     int64_t batch_stride_A;
     int64_t batch_stride_B;
-    int64_t batch_stride_D;
+    int64_t batch_stride_C;
+
+    int * ptr_gather_A_indices;
+    int * ptr_gather_B_indices;
+    int * ptr_scatter_D_indices;
 
     int *semaphore;
 
     //
     // Methods
     //
 
-    CUTLASS_HOST_DEVICE
-    Params():
-      swizzle_log_tile(0),
-      params_A(0),
-      params_B(0),
-      params_D(0),
-      batch_count(0),
-      gemm_k_size(0),
-      mode(cutlass::gemm::GemmUniversalMode::kGemm),
-      ptr_A(nullptr),
-      ptr_B(nullptr),
-      ptr_D(nullptr),
-      batch_stride_A(0),
-      batch_stride_B(0),
-      batch_stride_D(0),
-      semaphore(nullptr) { }
+    /// Default constructor
+    Params() = default;
 
     CUTLASS_HOST_DEVICE
     Params(
       Arguments const &args,
-      cutlass::gemm::GemmCoord const & grid_tiled_shape,
-      int gemm_k_size,
-      void *workspace = nullptr
+      int device_sms,
+      int sm_occupancy
     ):
-      problem_size(args.problem_size),
-      grid_tiled_shape(grid_tiled_shape),
-      swizzle_log_tile(ThreadblockSwizzle().get_log_tile(grid_tiled_shape)),
-      params_A(args.lda),
-      params_B(args.ldb),
-      params_D(args.ldd),
-      output_op(args.epilogue),
-      mode(args.mode),
-      batch_count(args.batch_count),
-      gemm_k_size(gemm_k_size),
+      ParamsBase(args, device_sms, sm_occupancy),
+      params_A(args.lda ? make_Coord_with_padding<LayoutA::kStrideRank>(args.lda) : args.stride_a),
+      params_B(args.ldb ? make_Coord_with_padding<LayoutB::kStrideRank>(args.ldb) : args.stride_b),
+      params_C(args.ldc ? make_Coord_with_padding<LayoutC::kStrideRank>(args.ldc) : args.stride_c),
+      params_D(args.ldd ? make_Coord_with_padding<LayoutC::kStrideRank>(args.ldd) : args.stride_d),
+      epilogue_visitor(args.epilogue_visitor),
       ptr_A(const_cast<void *>(args.ptr_A)),
       ptr_B(const_cast<void *>(args.ptr_B)),
+      ptr_C(const_cast<void *>(args.ptr_C)),
       ptr_D(args.ptr_D),
       batch_stride_A(args.batch_stride_A),
       batch_stride_B(args.batch_stride_B),
-      batch_stride_D(args.batch_stride_D),
-      semaphore(static_cast<int *>(workspace)) {
+      batch_stride_C(args.batch_stride_C),
+      ptr_gather_A_indices(const_cast<int *>(args.ptr_gather_A_indices)),
+      ptr_gather_B_indices(const_cast<int *>(args.ptr_gather_B_indices)),
+      ptr_scatter_D_indices(const_cast<int *>(args.ptr_scatter_D_indices)) {
+
     }
 
     CUTLASS_HOST_DEVICE
     void update(
       Arguments const &args,
       void *workspace = nullptr) {
 
       ptr_A = const_cast<void *>(args.ptr_A);
       ptr_B = const_cast<void *>(args.ptr_B);
+      ptr_C = const_cast<void *>(args.ptr_C);
       ptr_D = args.ptr_D;
 
+      ptr_gather_A_indices = const_cast<int *>(args.ptr_gather_A_indices);
+      ptr_gather_B_indices = const_cast<int *>(args.ptr_gather_B_indices);
+      ptr_scatter_D_indices = const_cast<int *>(args.ptr_scatter_D_indices);
+
       batch_stride_A = args.batch_stride_A;
       batch_stride_B = args.batch_stride_B;
-      batch_stride_D = args.batch_stride_D;
-
-      output_op = args.epilogue;
+      batch_stride_C = args.batch_stride_C;
 
+      epilogue_visitor = args.epilogue_visitor;
+      
       semaphore = static_cast<int *>(workspace);
+      CUTLASS_TRACE_HOST("GemmUniversal::Params::update()");
     }
-
   };
 
   /// Shared memory storage structure
   union SharedStorage {
     typename Mma::SharedStorage main_loop;
     typename Epilogue::SharedStorage epilogue;
+    typename EpilogueVisitor::SharedStorage visitor;
   };
 
 public:
 
   //
   // Methods
   //
 
   CUTLASS_DEVICE
-  TrmmUniversal() { } 
+  GemmUniversalwithEpilogueVisitor() { } 
 
   /// Determines whether kernel satisfies alignment
   static Status can_implement(
     cutlass::gemm::GemmCoord const & problem_size) {
 
-    static int const kAlignmentA = Mma::IteratorA::AccessType::kElements;
-    static int const kAlignmentB = Mma::IteratorB::AccessType::kElements;
-    static int const kAlignmentC = Epilogue::OutputTileIterator::kElementsPerAccess;
-
-    if ((problem_size.m() % kAlignmentA) || (problem_size.k() % kAlignmentA) ||
-      (problem_size.n() % kAlignmentB) || (problem_size.k() % kAlignmentB) ||
-      (problem_size.m() % kAlignmentC) || (problem_size.n() % kAlignmentC)) {
+    CUTLASS_TRACE_HOST("GemmUniversalwithEpilogueVisitor::can_implement()");
+
+    static int const kAlignmentA = (platform::is_same<LayoutA,
+                                                      layout::ColumnMajorInterleaved<32>>::value)
+                                   ? 32
+                                   : (platform::is_same<LayoutA,
+                                                        layout::ColumnMajorInterleaved<64>>::value)
+                                     ? 64
+                                     : Mma::IteratorA::AccessType::kElements;
+    static int const kAlignmentB = (platform::is_same<LayoutB,
+                                                      layout::RowMajorInterleaved<32>>::value)
+                                   ? 32
+                                   : (platform::is_same<LayoutB,
+                                                        layout::RowMajorInterleaved<64>>::value)
+                                     ? 64
+                                     : Mma::IteratorB::AccessType::kElements;
+    static int const kAlignmentC = (platform::is_same<LayoutC,
+                                                      layout::ColumnMajorInterleaved<32>>::value)
+                                   ? 32
+                                   : (platform::is_same<LayoutC,
+                                                        layout::ColumnMajorInterleaved<64>>::value)
+                                     ? 64
+                                     : Epilogue::OutputTileIterator::kElementsPerAccess;
+
+    bool isAMisaligned = false;
+    bool isBMisaligned = false;
+    bool isCMisaligned = false;
+
+    if (platform::is_same<LayoutA, layout::RowMajor>::value) {
+      isAMisaligned = problem_size.k() % kAlignmentA;
+    } else if (platform::is_same<LayoutA, layout::ColumnMajor>::value) {
+      isAMisaligned = problem_size.m() % kAlignmentA;
+    } else if (platform::is_same<LayoutA, layout::ColumnMajorInterleaved<32>>::value
+            || platform::is_same<LayoutA, layout::ColumnMajorInterleaved<64>>::value) {
+      isAMisaligned = problem_size.k() % kAlignmentA;
+    }
+
+    if (platform::is_same<LayoutB, layout::RowMajor>::value) {
+      isBMisaligned = problem_size.n() % kAlignmentB;
+    } else if (platform::is_same<LayoutB, layout::ColumnMajor>::value) {
+      isBMisaligned = problem_size.k() % kAlignmentB;
+    } else if (platform::is_same<LayoutB, layout::RowMajorInterleaved<32>>::value
+            || platform::is_same<LayoutB, layout::RowMajorInterleaved<64>>::value) {
+      isBMisaligned = problem_size.k() % kAlignmentB;
+    }
+
+    if (platform::is_same<LayoutC, layout::RowMajor>::value) {
+      isCMisaligned = problem_size.n() % kAlignmentC;
+    } else if (platform::is_same<LayoutC, layout::ColumnMajor>::value) {
+      isCMisaligned = problem_size.m() % kAlignmentC;
+    } else if (platform::is_same<LayoutC, layout::ColumnMajorInterleaved<32>>::value
+            || platform::is_same<LayoutC, layout::ColumnMajorInterleaved<64>>::value) {
+      isCMisaligned = problem_size.n() % kAlignmentC;
+    }
+
+    if (isAMisaligned) {
+      CUTLASS_TRACE_HOST("  returning kErrorMisalignedOperand for A operand");
+      return Status::kErrorMisalignedOperand;
+    }
 
+    if (isBMisaligned) {
+      CUTLASS_TRACE_HOST("  returning kErrorMisalignedOperand for B operand");
       return Status::kErrorMisalignedOperand;
     }
 
+    if (isCMisaligned) {
+      CUTLASS_TRACE_HOST("  returning kErrorMisalignedOperand for C operand");
+      return Status::kErrorMisalignedOperand;
+    }
+
+    CUTLASS_TRACE_HOST("  returning kSuccess");
+
     return Status::kSuccess;
   }
 
   static Status can_implement(Arguments const &args) {
     return can_implement(args.problem_size);
   }
 
@@ -333,15 +445,15 @@
 
       return;
     }
 
     int offset_k = 0;
     int problem_size_k = params.problem_size.k();
 
-    ElementA *ptr_A = static_cast<ElementA *>(params.ptr_A); 
+    ElementA *ptr_A = static_cast<ElementA *>(params.ptr_A);
     ElementB *ptr_B = static_cast<ElementB *>(params.ptr_B);
 
     //
     // Fetch pointers based on mode.
     //
     if (params.mode == GemmUniversalMode::kGemm || 
       params.mode == GemmUniversalMode::kGemmSplitKParallel) {
@@ -374,14 +486,31 @@
       offset_k,
       threadblock_tile_offset.n() * Mma::Shape::kN
     };
 
     // Compute position within threadblock
     int thread_idx = threadIdx.x;
 
+    // Construct iterators to A and B operands
+    typename Mma::IteratorA iterator_A(
+      params.params_A,
+      ptr_A,
+      {params.problem_size.m(), problem_size_k},
+      thread_idx,
+      tb_offset_A,
+      params.ptr_gather_A_indices);
+
+    typename Mma::IteratorB iterator_B(
+      params.params_B,
+      ptr_B,
+      {problem_size_k, params.problem_size.n()},
+      thread_idx,
+      tb_offset_B,
+      params.ptr_gather_B_indices);
+
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
     int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
 
     int lane_idx = threadIdx.x % 32;
 
     //
@@ -393,93 +522,28 @@
 
     typename Mma::FragmentC accumulators;
 
     accumulators.clear();
 
     // Compute threadblock-scoped matrix multiply-add
     int gemm_k_iterations = (problem_size_k - offset_k + Mma::Shape::kK - 1) / Mma::Shape::kK;
-    
-    /******************************************************************************************************
-      First two cases: (Left Side, Lower Fill) and (Right Side, Upper Fill) are transpose of each other
-        - (Left Side, Lower Fill): calculate bottom of the CTA tile,  then find the k-iterations 
-                                    needed to process all elements till that coordinate.
-        - (Right Side, Upper Fill): calculate right end of the CTA tile,  then find the k-iterations 
-                                    needed to process all elements till that coordinate.
-
-      Last two cases: (Left Side, Upper Fill) and (Right Side, Lower Fill) are transpose of each other
-        - (Left Side, Upper Fill): calculate the top of the CTA tile, then find k-iterations 
-                                   that can be skipped for all elements of this tile.
-        - (Right Side, Lower Fill): calculate the left start of the CTA tile, then find k-iterations 
-                                    that can be skipped for all elements of this tile.
-    ********************************************************************************************************/
- 
-    if (kSideMode == SideMode::kLeft && kFillMode == FillMode::kLower) {
-
-      int k_iterations_till_diagonal = ((threadblock_tile_offset.m() + 1) * Mma::Shape::kM + Mma::Shape::kK - 1) / Mma::Shape::kK;
-      if (k_iterations_till_diagonal < gemm_k_iterations) {
-        gemm_k_iterations = k_iterations_till_diagonal;
-      }
-
-    } else if (kSideMode == SideMode::kRight && kFillMode == FillMode::kUpper) {
-
-      int k_iterations_till_diagonal = ((threadblock_tile_offset.n() + 1) * Mma::Shape::kN + Mma::Shape::kK - 1) / Mma::Shape::kK;
-      if (k_iterations_till_diagonal < gemm_k_iterations) {
-        gemm_k_iterations = k_iterations_till_diagonal;
-      }
-
-    } else if (kSideMode == SideMode::kLeft && kFillMode == FillMode::kUpper) {
-
-      int k_iterations_till_diagonal = ((threadblock_tile_offset.m()) * Mma::Shape::kM) / Mma::Shape::kK;
-
-      if (k_iterations_till_diagonal != 0) {
-        tb_offset_A += cutlass::MatrixCoord({0, k_iterations_till_diagonal * Mma::Shape::kK});
-        tb_offset_B += cutlass::MatrixCoord({k_iterations_till_diagonal * Mma::Shape::kK, 0});
-        gemm_k_iterations -= k_iterations_till_diagonal;
-      }
-
-    } else if (kSideMode == SideMode::kRight && kFillMode == FillMode::kLower) {
-
-      int k_iterations_till_diagonal = ((threadblock_tile_offset.n()) * Mma::Shape::kN) / Mma::Shape::kK;
-
-      if (k_iterations_till_diagonal != 0) {
-        tb_offset_A += cutlass::MatrixCoord({0, k_iterations_till_diagonal * Mma::Shape::kK});
-        tb_offset_B += cutlass::MatrixCoord({k_iterations_till_diagonal * Mma::Shape::kK, 0});
-        gemm_k_iterations -= k_iterations_till_diagonal;
-      }
-
-    }
-
-    // Construct iterators to A and B operands
-    typename Mma::IteratorA iterator_A(
-      params.params_A,
-      ptr_A,
-      {params.problem_size.m(), problem_size_k},
-      thread_idx,
-      tb_offset_A);
-
-    typename Mma::IteratorB iterator_B(
-      params.params_B,
-      ptr_B,
-      {problem_size_k, params.problem_size.n()},
-      thread_idx,
-      tb_offset_B);
 
     // Compute threadblock-scoped matrix multiply-add
     mma(
       gemm_k_iterations, 
       accumulators, 
       iterator_A, 
       iterator_B, 
       accumulators);
 
     //
     // Epilogue
     //
 
-    EpilogueOutputOp output_op(params.output_op);
+    // EpilogueOutputOp output_op(params.output_op);
 
     //
     // Masked tile iterators constructed from members
     //
 
     threadblock_tile_offset = threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
 
@@ -487,90 +551,77 @@
     MatrixCoord threadblock_offset(
       threadblock_tile_offset.m() * Mma::Shape::kM,
       threadblock_tile_offset.n() * Mma::Shape::kN
     );
 
     int block_idx = threadblock_tile_offset.m() + threadblock_tile_offset.n() * params.grid_tiled_shape.m();
 
+    ElementC *ptr_C = static_cast<ElementC *>(params.ptr_C); 
     ElementC *ptr_D = static_cast<ElementC *>(params.ptr_D);
 
     //
     // Fetch pointers based on mode.
     //
     
     // Construct the semaphore.
     Semaphore semaphore(params.semaphore + block_idx, thread_idx);
 
-    if (params.mode == GemmUniversalMode::kGemm) {
+    // if (params.mode == GemmUniversalMode::kGemm) {
 
-      // If performing a reduction via split-K, fetch the initial synchronization
-      if (params.grid_tiled_shape.k() > 1) {
+    //   // TODO: fix this order
+    //   // If performing a reduction via split-K, fetch the initial synchronization
+    //   if (params.grid_tiled_shape.k() > 1) {
         
-        // Fetch the synchronization lock initially but do not block.
-        semaphore.fetch();
-
-        // Indicate which position in a serial reduction the output operator is currently updating
-        output_op.set_k_partition(threadblock_tile_offset.k(), params.grid_tiled_shape.k());
-      }
-    }
-    else if (params.mode == GemmUniversalMode::kGemmSplitKParallel) {
-      ptr_D += threadblock_tile_offset.k() * params.batch_stride_D;
-    }
-    else if (params.mode == GemmUniversalMode::kBatched) {
-      ptr_D += threadblock_tile_offset.k() * params.batch_stride_D;
-    }
-    else if (params.mode == GemmUniversalMode::kArray) {
-      ptr_D = static_cast<ElementC * const *>(params.ptr_D)[threadblock_tile_offset.k()];
-    }
+    //     // Fetch the synchronization lock initially but do not block.
+    //     semaphore.fetch();
 
+    //     // Indicate which position in a serial reduction the output operator is currently updating
+    //     output_op.set_k_partition(threadblock_tile_offset.k(), params.grid_tiled_shape.k());
+    //   }
+    // }
     
-    // Tile iterator loading from source tensor (although irrelevant to this kernel as beta is zero).
-    typename Epilogue::OutputTileIterator iterator_C(
-      params.params_D,
-      ptr_D,
-      params.problem_size.mn(),
-      thread_idx,
-      threadblock_offset
-    );
+    // Tile iterator loading from source tensor.
 
-    // Tile iterator writing to destination tensor.
-    typename Epilogue::OutputTileIterator iterator_D(
-      params.params_D,
-      ptr_D,
-      params.problem_size.mn(),
-      thread_idx,
-      threadblock_offset
+    EpilogueVisitor epilogue_visitor(
+        params.epilogue_visitor,
+        shared_storage.visitor,
+        threadblock_offset,
+        threadblock_tile_offset,
+        thread_idx,
+        params.problem_size.mn()
     );
 
+    // if (params.mode == GemmUniversalMode::kGemmSplitKParallel) {
+    //   ptr_D += threadblock_tile_offset.k() * params.batch_stride_D;
+    // }
+    if (params.mode == GemmUniversalMode::kBatched || params.mode == GemmUniversalMode::kArray) {
+      epilogue_visitor.set_batch_index(threadblock_tile_offset.k());
+    }
+
     Epilogue epilogue(
-      shared_storage.epilogue, 
-      thread_idx, 
-      warp_idx, 
+      shared_storage.epilogue,
+      thread_idx,
+      warp_idx,
       lane_idx);
 
     // Wait on the semaphore - this latency may have been covered by iterator construction
     if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) {
         
       // For subsequent threadblocks, the source matrix is held in the 'D' tensor.
-      if (threadblock_tile_offset.k()) {
-        iterator_C = iterator_D;
-      }
+      // TODO: ???
+      // if (threadblock_tile_offset.k()) {
+      //   iterator_C = iterator_D;
+      // }
 
       semaphore.wait(threadblock_tile_offset.k());
-
-      __threadfence();
     }
 
 
     // Execute the epilogue operator to update the destination tensor.
-    epilogue(
-      output_op, 
-      iterator_D, 
-      accumulators, 
-      iterator_C); 
+    epilogue(epilogue_visitor, accumulators); 
     
     //
     // Release the semaphore
     //
 
     if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) {
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm50.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm50.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm60.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm60.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm61.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm61.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_ell_mma.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_ell_mma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_gemv_core.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_gemv_core.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_simt.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_simt.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm70.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm70.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm75.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm75.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm80.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm80.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_access_size.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_access_size.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_reduction.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_reduction.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_wmma.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_wmma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_layernorm_mainloop_fusion.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_layernorm_mainloop_fusion.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_multistage.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_pipelined.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_pipelined.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_softmax_mainloop_fusion.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_with_reduction.h`

 * *Files 11% similar despite different names*

```diff
@@ -25,31 +25,27 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Template for a pipelined softmax-GEMM kernel.
+    \brief Template for a pipelined GEMM kernel. Does not compute batching or support split-K.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/arch/arch.h"
 
 #include "cutlass/layout/matrix.h"
-#include "cutlass/gemm/threadblock/default_mma_core.h"
-#include "cutlass/gemm/threadblock/mma_softmax_mainloop_fusion_multistage.h"
-#include "cutlass/transform/threadblock/predicated_scale_bias_vector_iterator.h"
-#include "cutlass/transform/threadblock/predicated_scale_bias_vector_access_iterator.h"
-#include "cutlass/transform/threadblock/regular_scale_bias_vector_access_iterator.h"
-#include "cutlass/gemm/warp/scale_bias_tile_iterator.h"
 #include "cutlass/transform/threadblock/predicated_tile_iterator.h"
+#include "cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h"
+#include "cutlass/gemm/threadblock/default_mma_core_with_reduction.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace threadblock {
 
@@ -64,64 +60,57 @@
     int kAlignmentA,
     /// Element type for B matrix operand
     typename ElementB,
     /// Layout type for B matrix operand
     typename LayoutB,
     /// Access granularity of B matrix in units of elements
     int kAlignmentB,
-    /// Element type for Scale/Bias vectors
-    typename ElementScaleBias,
-    /// Layout type for Scale/Bias vectors
-    typename LayoutScaleBias,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Layout type for C and D matrix operands
     typename LayoutC,
     /// Operator class tag
     typename OperatorClass,
+    ///                                                                                               
+    bool ReduceKForA_,
     /// Tag indicating architecture to tune for
     typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape,
     /// Instruction-level tile size (concept: GemmShape)
     typename InstructionShape,
     /// Number of stages used in the pipelined mainloop
     int Stages,
-    /// Whether problem has been transformed. This determines to which operand
-    /// the softmax is applied.
-    bool InternalTranspose,
     /// Operation perfomed by GEMM
     typename Operator,
     /// Store the accumulators in row major or column major.  Row major is used
     /// when output layout is interleaved.
     bool AccumulatorsInRowMajor = false,
     /// Use zfill or predicate for SM80 out-of-bound cp.async 
     SharedMemoryClearOption SharedMemoryClear = SharedMemoryClearOption::kNone
     >
-struct DefaultMmaSoftmaxMainloopFusion {
+struct DefaultMmaWithReduction {
 
   static cutlass::arch::CacheOperation::Kind const CacheOpA =
       ((sizeof_bits<ElementA>::value * kAlignmentA) == 128)
           ? cutlass::arch::CacheOperation::Global
           : cutlass::arch::CacheOperation::Always;
 
   static cutlass::arch::CacheOperation::Kind const CacheOpB =
       ((sizeof_bits<ElementB>::value * kAlignmentB) == 128)
           ? cutlass::arch::CacheOperation::Global
           : cutlass::arch::CacheOperation::Always;
 
-  static cutlass::arch::CacheOperation::Kind const CacheOpGammaBeta = CacheOpA;
-
   // Define the MmaCore components
-  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaWithReductionCore<
       ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
       ElementB, LayoutB, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
-      Stages, Operator, false, CacheOpA, CacheOpB>;
+      ReduceKForA_,  Stages, Operator, false, CacheOpA, CacheOpB>;
 
   // Define iterators over tiles from the A operand
   using ThreadMapA = typename MmaCore::IteratorThreadMapA;
   using AccessTypeA = cutlass::Array<ElementA, kAlignmentA>;
   using IteratorA =
       cutlass::transform::threadblock::PredicatedTileAccessIterator<
           cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
@@ -131,28 +120,20 @@
   using ThreadMapB = typename MmaCore::IteratorThreadMapB;
   using AccessTypeB = cutlass::Array<ElementB, kAlignmentB>;
   using IteratorB =
       cutlass::transform::threadblock::PredicatedTileAccessIterator<
           cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
           ElementB, LayoutB, 0, ThreadMapB, AccessTypeB>;
 
-  /// Define iterators over tiles from scale/bias vectors
-  using IteratorNormSum =
-      cutlass::transform::threadblock::PredicatedScaleBiasVectorIterator<
-          cutlass::MatrixShape<1, WarpShape::kN>,
-          ElementScaleBias,
-          LayoutScaleBias>;
-
   // Define the threadblock-scoped multistage matrix multiply
-  using ThreadblockMma = cutlass::gemm::threadblock::MmaSoftmaxMainloopFusionMultistage<
+  using ThreadblockMma = cutlass::gemm::threadblock::MmaWithReductionMultistage<
       typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
       MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
-      MmaCore::kCacheOpB, IteratorNormSum,
-      ElementAccumulator, layout::RowMajor,
-      typename MmaCore::MmaPolicy, Stages, InternalTranspose, SharedMemoryClear>;
+      MmaCore::kCacheOpB, ElementAccumulator, layout::RowMajor,
+      typename MmaCore::MmaPolicy, Stages, SharedMemoryClear>;
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
 } // namespace threadblock
 } // namespace gemm
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_with_reduction.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex.h`

 * *Files 15% similar despite different names*

```diff
@@ -24,118 +24,136 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
+
 /*! \file
-    \brief Template for a pipelined GEMM kernel. Does not compute batching or support split-K.
+    \brief Template for a multistage GEMM kernel. Does not compute batching or support split-K.
 */
 
 #pragma once
 
+#include "cutlass/arch/arch.h"
 #include "cutlass/cutlass.h"
+#include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
 #include "cutlass/numeric_types.h"
-#include "cutlass/arch/arch.h"
-
-#include "cutlass/layout/matrix.h"
 #include "cutlass/transform/threadblock/predicated_tile_iterator.h"
-#include "cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h"
-#include "cutlass/gemm/threadblock/default_mma_core_with_reduction.h"
+#include "cutlass/gemm/threadblock/default_multistage_mma_complex_core_sm80.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace threadblock {
 
 ////////////////////////////////////////////////////////////////////////////////
 
 template <
     /// Element type for A matrix operand
+    typename ElementA_,
+    /// Layout type for A matrix operand
+    typename LayoutA_,
+    /// Element type for B matrix operand
+    typename ElementB_,
+    /// Layout type for B matrix operand
+    typename LayoutB_,
+    /// Element type for internal accumulation
+    typename ElementAccumulator_,
+    /// Layout type for C and D matrix operands
+    typename LayoutC_,
+    /// Operator class tag
+    typename OperatorClass_,
+    /// Tag indicating architecture to tune for
+    typename ArchTag_,
+    /// Threadblock-level tile size (concept: GemmShape)
+    typename ThreadblockShape_,
+    /// Warp-level tile size (concept: GemmShape)
+    typename WarpShape_,
+    /// Instruction-level tile size (concept: GemmShape)
+    typename InstructionShape_,
+    /// Number of stages used in the pipelined mainloop
+    int Stages,
+    /// Complex transformation on operand A
+    ComplexTransform TransformA = ComplexTransform::kNone,
+    /// Complex transformation on operand B
+    ComplexTransform TransformB = ComplexTransform::kNone,
+    /// Multiply-add operator (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
+    typename Operator = arch::OpMultiplyAddComplex,
+    /// Store the accumulators in row major or column major.  Row major is used
+    /// when output layout is interleaved.
+    bool AccumulatorsInRowMajor = false>
+struct DefaultMultistageMmaComplex;
+
+////////////////////////////////////////////////////////////////////////////////
+
+/// Specialization for row-major output
+template <
+    /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentA,
     /// Element type for B matrix operand
     typename ElementB,
     /// Layout type for B matrix operand
     typename LayoutB,
-    /// Access granularity of B matrix in units of elements
-    int kAlignmentB,
     /// Element type for internal accumulation
     typename ElementAccumulator,
-    /// Layout type for C and D matrix operands
-    typename LayoutC,
-    /// Operator class tag
+    /// Tag indicating architecture to tune for
     typename OperatorClass,
-    ///                                                                                               
-    bool ReduceKForA_,
     /// Tag indicating architecture to tune for
     typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape,
     /// Instruction-level tile size (concept: GemmShape)
     typename InstructionShape,
-    /// Number of stages used in the pipelined mainloop
+    /// Number of stages used in the multistage mainloop
     int Stages,
-    /// Operation perfomed by GEMM
-    typename Operator,
-    /// Store the accumulators in row major or column major.  Row major is used
-    /// when output layout is interleaved.
-    bool AccumulatorsInRowMajor = false,
-    /// Use zfill or predicate for SM80 out-of-bound cp.async 
-    SharedMemoryClearOption SharedMemoryClear = SharedMemoryClearOption::kNone
-    >
-struct DefaultMmaWithReduction {
-
-  static cutlass::arch::CacheOperation::Kind const CacheOpA =
-      ((sizeof_bits<ElementA>::value * kAlignmentA) == 128)
-          ? cutlass::arch::CacheOperation::Global
-          : cutlass::arch::CacheOperation::Always;
-
-  static cutlass::arch::CacheOperation::Kind const CacheOpB =
-      ((sizeof_bits<ElementB>::value * kAlignmentB) == 128)
-          ? cutlass::arch::CacheOperation::Global
-          : cutlass::arch::CacheOperation::Always;
-
+    /// Complex transformation on operand A
+    ComplexTransform TransformA,
+    /// Complex transformation on operand B
+    ComplexTransform TransformB,
+    /// Multiply-add operator (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
+    typename Operator>
+struct DefaultMultistageMmaComplex<ElementA, LayoutA, ElementB, LayoutB,
+                            ElementAccumulator, layout::RowMajor, OperatorClass,
+                            ArchTag, ThreadblockShape, WarpShape,
+                            InstructionShape, Stages, TransformA, TransformB, Operator> {
   // Define the MmaCore components
-  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaWithReductionCore<
-      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
-      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
-      ReduceKForA_,  Stages, Operator, false, CacheOpA, CacheOpB>;
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplexCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA, 
+      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, OperatorClass,
+      Stages, TransformA, TransformB, Operator>;
 
   // Define iterators over tiles from the A operand
   using ThreadMapA = typename MmaCore::IteratorThreadMapA;
-  using AccessTypeA = cutlass::Array<ElementA, kAlignmentA>;
+  using AccessTypeA = cutlass::Array<ElementA, ThreadMapA::kElementsPerAccess>;
   using IteratorA =
       cutlass::transform::threadblock::PredicatedTileAccessIterator<
           cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
           ElementA, LayoutA, 1, ThreadMapA, AccessTypeA>;
 
   // Define iterators over tiles from the B operand
   using ThreadMapB = typename MmaCore::IteratorThreadMapB;
-  using AccessTypeB = cutlass::Array<ElementB, kAlignmentB>;
+  using AccessTypeB = cutlass::Array<ElementB, ThreadMapB::kElementsPerAccess>;
   using IteratorB =
       cutlass::transform::threadblock::PredicatedTileAccessIterator<
           cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
           ElementB, LayoutB, 0, ThreadMapB, AccessTypeB>;
 
   // Define the threadblock-scoped multistage matrix multiply
-  using ThreadblockMma = cutlass::gemm::threadblock::MmaWithReductionMultistage<
+  using ThreadblockMma = cutlass::gemm::threadblock::MmaMultistage<
       typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
       MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
       MmaCore::kCacheOpB, ElementAccumulator, layout::RowMajor,
-      typename MmaCore::MmaPolicy, Stages, SharedMemoryClear>;
+      typename MmaCore::MmaPolicy, Stages>;
 };
 
-////////////////////////////////////////////////////////////////////////////////
-
-} // namespace threadblock
-} // namespace gemm
-} // namespace cutlass 
+}  // namespace threadblock
+}  // namespace gemm
+}  // namespace cutlass
 
 ////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_sparse_mma.h`

 * *Files 23% similar despite different names*

```diff
@@ -24,45 +24,57 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
 /*! \file
-    \brief Template for a multistage GEMM kernel. Does not compute batching or support split-K.
+    \brief Template for a pipelined GEMM kernel. Does not compute batching or support split-K.
 */
 
 #pragma once
 
-#include "cutlass/arch/arch.h"
 #include "cutlass/cutlass.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
 #include "cutlass/numeric_types.h"
+#include "cutlass/arch/arch.h"
+#include "cutlass/arch/wmma.h"
+
+#include "cutlass/layout/matrix.h"
 #include "cutlass/transform/threadblock/predicated_tile_iterator.h"
-#include "cutlass/gemm/threadblock/default_multistage_mma_complex_core_sm80.h"
+#include "cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h"
+#include "cutlass/gemm/threadblock/default_mma_core_sm70.h"
+#include "cutlass/gemm/threadblock/default_mma_core_sm75.h"
+#include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
+#include "cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h"
+#if defined(CUTLASS_ARCH_WMMA_ENABLED)
+#include "cutlass/gemm/threadblock/default_mma_core_wmma.h"
+#endif //CUTLASS_ARCH_WMMA_ENABLED
 
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace threadblock {
 
 ////////////////////////////////////////////////////////////////////////////////
 
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
     typename LayoutA_,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentA,
     /// Element type for B matrix operand
     typename ElementB_,
     /// Layout type for B matrix operand
     typename LayoutB_,
+    /// Access granularity of B matrix in units of elements
+    int kAlignmentB,
     /// Element type for internal accumulation
     typename ElementAccumulator_,
     /// Layout type for C and D matrix operands
     typename LayoutC_,
     /// Operator class tag
     typename OperatorClass_,
     /// Tag indicating architecture to tune for
@@ -71,89 +83,114 @@
     typename ThreadblockShape_,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape_,
     /// Instruction-level tile size (concept: GemmShape)
     typename InstructionShape_,
     /// Number of stages used in the pipelined mainloop
     int Stages,
-    /// Complex transformation on operand A
-    ComplexTransform TransformA = ComplexTransform::kNone,
-    /// Complex transformation on operand B
-    ComplexTransform TransformB = ComplexTransform::kNone,
-    /// Multiply-add operator (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
-    typename Operator = arch::OpMultiplyAddComplex,
+    /// Operation perfomed by GEMM
+    typename Operator,
     /// Store the accumulators in row major or column major.  Row major is used
     /// when output layout is interleaved.
-    bool AccumulatorsInRowMajor = false>
-struct DefaultMultistageMmaComplex;
+    bool AccumulatorsInRowMajor = false
+    >
+struct DefaultSparseMma;
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Specialization for row-major output
+/// Specialization for row-major output (OperatorClass TensorOp)
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentA,
     /// Element type for B matrix operand
     typename ElementB,
     /// Layout type for B matrix operand
     typename LayoutB,
+    /// Access granularity of B matrix in units of elements
+    int kAlignmentB,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Tag indicating architecture to tune for
-    typename OperatorClass,
-    /// Tag indicating architecture to tune for
     typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape,
     /// Instruction-level tile size (concept: GemmShape)
     typename InstructionShape,
     /// Number of stages used in the multistage mainloop
     int Stages,
-    /// Complex transformation on operand A
-    ComplexTransform TransformA,
-    /// Complex transformation on operand B
-    ComplexTransform TransformB,
-    /// Multiply-add operator (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
-    typename Operator>
-struct DefaultMultistageMmaComplex<ElementA, LayoutA, ElementB, LayoutB,
-                            ElementAccumulator, layout::RowMajor, OperatorClass,
-                            ArchTag, ThreadblockShape, WarpShape,
-                            InstructionShape, Stages, TransformA, TransformB, Operator> {
+    /// Operation perfomed by GEMM
+    typename Operator
+    >
+struct DefaultSparseMma<ElementA, LayoutA, kAlignmentA, ElementB, LayoutB,
+                  kAlignmentB, ElementAccumulator, layout::RowMajor,
+                  arch::OpClassTensorOp, ArchTag, ThreadblockShape, WarpShape,
+                  InstructionShape, Stages, Operator, false> {
+  static cutlass::arch::CacheOperation::Kind const CacheOpA =
+      ((sizeof_bits<ElementA>::value * kAlignmentA) == 128)
+          ? cutlass::arch::CacheOperation::Global
+          : cutlass::arch::CacheOperation::Always;
+
+  static cutlass::arch::CacheOperation::Kind const CacheOpB =
+      ((sizeof_bits<ElementB>::value * kAlignmentB) == 128)
+          ? cutlass::arch::CacheOperation::Global
+          : cutlass::arch::CacheOperation::Always;
+  
+
   // Define the MmaCore components
-  using MmaCore = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplexCore<
-      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA, 
-      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, OperatorClass,
-      Stages, TransformA, TransformB, Operator>;
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultSparseMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
+      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
+      Stages, Operator, false, CacheOpA, CacheOpB>;
+
+  static int const kSparse = MmaCore::kSparse;
 
   // Define iterators over tiles from the A operand
   using ThreadMapA = typename MmaCore::IteratorThreadMapA;
-  using AccessTypeA = cutlass::Array<ElementA, ThreadMapA::kElementsPerAccess>;
+  using AccessTypeA = cutlass::Array<ElementA, kAlignmentA>;
   using IteratorA =
       cutlass::transform::threadblock::PredicatedTileAccessIterator<
-          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
+          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK / kSparse>,
           ElementA, LayoutA, 1, ThreadMapA, AccessTypeA>;
 
   // Define iterators over tiles from the B operand
   using ThreadMapB = typename MmaCore::IteratorThreadMapB;
-  using AccessTypeB = cutlass::Array<ElementB, ThreadMapB::kElementsPerAccess>;
+  using AccessTypeB = cutlass::Array<ElementB, kAlignmentB>;
   using IteratorB =
       cutlass::transform::threadblock::PredicatedTileAccessIterator<
           cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
           ElementB, LayoutB, 0, ThreadMapB, AccessTypeB>;
 
+  // Define iterators over tiles from the E operand
+  using ElementE = typename MmaCore::ElementE;
+  using LayoutE = typename MmaCore::GmemLayoutE;
+  using ThreadMapE = typename MmaCore::IteratorThreadMapE;
+  using AccessTypeE =
+      cutlass::Array<ElementE, 128 / sizeof_bits<ElementE>::value>;
+  using IteratorE =
+      cutlass::transform::threadblock::PredicatedTileAccessIterator<
+          cutlass::MatrixShape<ThreadblockShape::kM,
+                               ThreadblockShape::kK / kSparse /
+                                   MmaCore::kElementsPerElementE>,
+          ElementE, LayoutE, 1, ThreadMapE, AccessTypeE>;
+
   // Define the threadblock-scoped multistage matrix multiply
-  using ThreadblockMma = cutlass::gemm::threadblock::MmaMultistage<
+  using ThreadblockMma = cutlass::gemm::threadblock::SparseMmaMultistage<
       typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
       MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
       MmaCore::kCacheOpB, ElementAccumulator, layout::RowMajor,
+      IteratorE, typename MmaCore::SmemIteratorE, MmaCore::kCacheOpE,
       typename MmaCore::MmaPolicy, Stages>;
 };
 
-}  // namespace threadblock
-}  // namespace gemm
-}  // namespace cutlass
+////////////////////////////////////////////////////////////////////////////////
+
+} // namespace threadblock
+} // namespace gemm
+} // namespace cutlass 
 
 ////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core_sm80.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core_sm80.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_trmm_complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_trmm_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_sparse_mma.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt.h`

 * *Files 21% similar despite different names*

```diff
@@ -25,172 +25,240 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Template for a pipelined GEMM kernel. Does not compute batching or support split-K.
+    \brief Templates implementing warp-level matrix multiply-accumulate operations.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
+#include "cutlass/array.h"
 #include "cutlass/numeric_types.h"
-#include "cutlass/arch/arch.h"
-#include "cutlass/arch/wmma.h"
+#include "cutlass/matrix_shape.h"
+#include "cutlass/gemm/gemm.h"
+#include "cutlass/gemm/warp/mma.h"
 
-#include "cutlass/layout/matrix.h"
-#include "cutlass/transform/threadblock/predicated_tile_iterator.h"
-#include "cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sm70.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sm75.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h"
-#if defined(CUTLASS_ARCH_WMMA_ENABLED)
-#include "cutlass/gemm/threadblock/default_mma_core_wmma.h"
-#endif //CUTLASS_ARCH_WMMA_ENABLED
+#include "cutlass/gemm/thread/mma.h"
 
-////////////////////////////////////////////////////////////////////////////////
+#include "cutlass/gemm/warp/mma_simt_tile_iterator.h"
+#include "cutlass/gemm/warp/mma_simt_policy.h"
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
-namespace threadblock {
+namespace warp {
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
+/// Structure to compute the matrix product targeting CUDA cores and SIMT math instructions.
 template <
-    /// Element type for A matrix operand
-    typename ElementA_,
-    /// Layout type for A matrix operand
-    typename LayoutA_,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentA,
-    /// Element type for B matrix operand
-    typename ElementB_,
-    /// Layout type for B matrix operand
-    typename LayoutB_,
-    /// Access granularity of B matrix in units of elements
-    int kAlignmentB,
-    /// Element type for internal accumulation
-    typename ElementAccumulator_,
-    /// Layout type for C and D matrix operands
-    typename LayoutC_,
-    /// Operator class tag
-    typename OperatorClass_,
-    /// Tag indicating architecture to tune for
-    typename ArchTag_,
-    /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape_,
-    /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape_,
-    /// Instruction-level tile size (concept: GemmShape)
-    typename InstructionShape_,
-    /// Number of stages used in the pipelined mainloop
-    int Stages,
-    /// Operation perfomed by GEMM
-    typename Operator,
-    /// Store the accumulators in row major or column major.  Row major is used
-    /// when output layout is interleaved.
-    bool AccumulatorsInRowMajor = false
-    >
-struct DefaultSparseMma;
+  /// Size of the Gemm problem - concept: gemm::GemmShape<>
+  typename Shape_,
+  /// Data type of A elements
+  typename ElementA_,
+  /// Layout of A matrix (concept: MatrixLayout)
+  typename LayoutA_,
+  /// Data type of B elements
+  typename ElementB_,
+  /// Layout of B matrix (concept: MatrixLayout)
+  typename LayoutB_,
+  /// Element type of C matrix
+  typename ElementC_,
+  /// Layout of C matrix (concept: MatrixLayout)
+  typename LayoutC_,
+  /// Shape of the warp in units of thread (concept: MmaSimtPolicy)
+  typename Policy_,
+  /// Number of partitions along K dimension
+  int PartitionsK = 1,
+  /// Complex transformation on operand A
+  ComplexTransform TransformA = ComplexTransform::kNone,
+  /// Complex transformation on operand B
+  ComplexTransform TransformB = ComplexTransform::kNone,
+  /// Used for partial specialization
+  typename Enable = bool
+>
+class MmaSimt {
+public:
+  /// Shape of warp-level matrix operation (concept: GemmShape)
+  using Shape = Shape_;
+
+  /// Data type of multiplicand A
+  using ElementA = ElementA_;
+
+  /// Layout of multiplicand A
+  using LayoutA = LayoutA_;
+
+  /// Data type of multiplicand B
+  using ElementB = ElementB_;
+
+  /// Layout of multiplicand B
+  using LayoutB = LayoutB_;
+
+  /// Data type of accumulator matrix C
+  using ElementC = ElementC_;
+
+  /// Layout of accumulator matrix C
+  using LayoutC = LayoutC_;
+
+  /// Shape of the warp in units of thread (concept: MmaLanePolicySimt)
+  using Policy = Policy_;
+
+  /// Indicates class of matrix operator
+  using OperatorClass = arch::OpClassSimt;
+
+  /// Hard-coded for now
+  using ArchTag = arch::Sm50;
+
+  /// Complex transform on A operand
+  static ComplexTransform const kTransformA = TransformA;
+
+  /// Complex transform on B operand
+  static ComplexTransform const kTransformB = TransformB;
+
+  /// Layout of threads
+  using ThreadLayoutA = typename platform::conditional< platform::is_same< layout::ColumnMajorInterleaved<4>, LayoutA >::value,
+                  layout::ColumnMajor,
+                  typename platform::conditional < platform::is_same< layout::RowMajorInterleaved<4>, LayoutA >::value,
+                      layout::RowMajor,
+                      LayoutA>::type
+                 >::type;
+  
+  using ThreadLayoutB = typename platform::conditional< platform::is_same< layout::ColumnMajorInterleaved<4>, LayoutB >::value,
+                  layout::ColumnMajor,
+                  typename platform::conditional < platform::is_same< layout::RowMajorInterleaved<4>, LayoutB >::value,
+                      layout::RowMajor,
+                      LayoutB>::type
+                 >::type;
+
+  static constexpr bool use_dp4a = (platform::is_same< layout::ColumnMajorInterleaved<4>, LayoutA>::value || 
+                                    platform::is_same< layout::RowMajorInterleaved<4>, LayoutA >::value) && 
+                                    platform::is_same< ElementA, int8_t >::value && 
+                                    platform::is_same< ElementB, int8_t >::value;
+
+  using dp4a_type = typename platform::conditional< use_dp4a , int8_t, bool >::type;
+
+  /// Thread-level matrix multiply accumulate operator
+  using ThreadMma = thread::Mma<
+    GemmShape<
+      Shape::kM / Policy::WarpShape::kRow,
+      Shape::kN / Policy::WarpShape::kColumn,
+      Policy::LaneMmaShape::kK>,
+    ElementA,
+    ThreadLayoutA,
+    ElementB,
+    ThreadLayoutB,
+    ElementC,
+    LayoutC,
+    arch::OpMultiplyAdd,
+    dp4a_type
+  >;
 
-////////////////////////////////////////////////////////////////////////////////
+  /// Underlying matrix multiply operator (concept: arch::Mma)
+  using ArchMmaOperator = typename ThreadMma::ArchMmaOperator;
 
-/// Specialization for row-major output (OperatorClass TensorOp)
-template <
-    /// Element type for A matrix operand
-    typename ElementA,
-    /// Layout type for A matrix operand
-    typename LayoutA,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentA,
-    /// Element type for B matrix operand
-    typename ElementB,
-    /// Layout type for B matrix operand
-    typename LayoutB,
-    /// Access granularity of B matrix in units of elements
-    int kAlignmentB,
-    /// Element type for internal accumulation
-    typename ElementAccumulator,
-    /// Tag indicating architecture to tune for
-    typename ArchTag,
-    /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape,
-    /// Instruction-level tile size (concept: GemmShape)
-    typename InstructionShape,
-    /// Number of stages used in the multistage mainloop
-    int Stages,
-    /// Operation perfomed by GEMM
-    typename Operator
-    >
-struct DefaultSparseMma<ElementA, LayoutA, kAlignmentA, ElementB, LayoutB,
-                  kAlignmentB, ElementAccumulator, layout::RowMajor,
-                  arch::OpClassTensorOp, ArchTag, ThreadblockShape, WarpShape,
-                  InstructionShape, Stages, Operator, false> {
-  static cutlass::arch::CacheOperation::Kind const CacheOpA =
-      ((sizeof_bits<ElementA>::value * kAlignmentA) == 128)
-          ? cutlass::arch::CacheOperation::Global
-          : cutlass::arch::CacheOperation::Always;
-
-  static cutlass::arch::CacheOperation::Kind const CacheOpB =
-      ((sizeof_bits<ElementB>::value * kAlignmentB) == 128)
-          ? cutlass::arch::CacheOperation::Global
-          : cutlass::arch::CacheOperation::Always;
+  /// Indicates math operator 
+  using MathOperator = typename ArchMmaOperator::Operator;
   
+  /// Shape of the underlying instruction
+  using InstructionShape = GemmShape<1,1,use_dp4a ? 4 : 1>;
 
-  // Define the MmaCore components
-  using MmaCore = typename cutlass::gemm::threadblock::DefaultSparseMmaCore<
-      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
-      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
-      Stages, Operator, false, CacheOpA, CacheOpB>;
-
-  static int const kSparse = MmaCore::kSparse;
-
-  // Define iterators over tiles from the A operand
-  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
-  using AccessTypeA = cutlass::Array<ElementA, kAlignmentA>;
-  using IteratorA =
-      cutlass::transform::threadblock::PredicatedTileAccessIterator<
-          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK / kSparse>,
-          ElementA, LayoutA, 1, ThreadMapA, AccessTypeA>;
-
-  // Define iterators over tiles from the B operand
-  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
-  using AccessTypeB = cutlass::Array<ElementB, kAlignmentB>;
-  using IteratorB =
-      cutlass::transform::threadblock::PredicatedTileAccessIterator<
-          cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
-          ElementB, LayoutB, 0, ThreadMapB, AccessTypeB>;
-
-  // Define iterators over tiles from the E operand
-  using ElementE = typename MmaCore::ElementE;
-  using LayoutE = typename MmaCore::GmemLayoutE;
-  using ThreadMapE = typename MmaCore::IteratorThreadMapE;
-  using AccessTypeE =
-      cutlass::Array<ElementE, 128 / sizeof_bits<ElementE>::value>;
-  using IteratorE =
-      cutlass::transform::threadblock::PredicatedTileAccessIterator<
-          cutlass::MatrixShape<ThreadblockShape::kM,
-                               ThreadblockShape::kK / kSparse /
-                                   MmaCore::kElementsPerElementE>,
-          ElementE, LayoutE, 1, ThreadMapE, AccessTypeE>;
-
-  // Define the threadblock-scoped multistage matrix multiply
-  using ThreadblockMma = cutlass::gemm::threadblock::SparseMmaMultistage<
-      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
-      MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
-      MmaCore::kCacheOpB, ElementAccumulator, layout::RowMajor,
-      IteratorE, typename MmaCore::SmemIteratorE, MmaCore::kCacheOpE,
-      typename MmaCore::MmaPolicy, Stages>;
+public:
+
+  /// Iterates over the A operand in memory
+  using IteratorA = MmaSimtTileIterator<
+    MatrixShape<Shape::kM, Policy::LaneMmaShape::kK>,
+    Operand::kA,
+    ElementA,
+    LayoutA,
+    Policy,
+    PartitionsK,
+    Shape::kK
+  >;
+
+  /// Storage for A tile
+  using FragmentA = typename IteratorA::Fragment;
+
+  /// Storage for transformed A tile
+  using TransformedFragmentA = FragmentA;
+
+  /// Iterates over the B operand in memory
+  using IteratorB = MmaSimtTileIterator<
+    MatrixShape<Policy::LaneMmaShape::kK, Shape::kN>,
+    Operand::kB,
+    ElementB,
+    LayoutB,
+    Policy,
+    PartitionsK,
+    Shape::kK
+  >;
+
+  /// Storage for B tile
+  using FragmentB = typename IteratorB::Fragment;
+
+  /// Storage for transformed A tile
+  using TransformedFragmentB = FragmentB;
+
+  /// Iterates over the C operand in memory
+  using IteratorC = MmaSimtTileIterator<
+    MatrixShape<Shape::kM, Shape::kN>,
+    Operand::kC,
+    ElementC,
+    LayoutC,
+    Policy
+  >;
+
+  /// Storage for C tile
+  using FragmentC = typename ThreadMma::FragmentC;
+
+public:
+
+  //
+  // Methods
+  //
+
+  /// Ctor
+  CUTLASS_DEVICE
+  MmaSimt() {}
+
+  /// Performs a warp-level matrix multiply-accumulate operation
+  CUTLASS_DEVICE
+  void operator()(
+    FragmentC &d, 
+    FragmentA a, 
+    FragmentB b, 
+    FragmentC const &c, int group_idx = 0) const {
+
+    ThreadMma mma;
+
+    if (kTransformA == ComplexTransform::kConjugate) {
+      conjugate<FragmentA> conj_a;
+      a = conj_a(a);
+    }
+
+    if (kTransformB == ComplexTransform::kConjugate) {
+      conjugate<FragmentB> conj_b;
+      b = conj_b(b);
+    }
+
+    mma(d, a, b, c);
+  }
+
+  /// Transform the mma operands to the required types
+  CUTLASS_DEVICE
+  void transform(TransformedFragmentA &dst_A, TransformedFragmentB &dst_B,
+                 FragmentA const &A, FragmentB const &B) const {
+    //TODO: Implement this
+    dst_A = A;
+    dst_B = B;
+  }
 };
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-} // namespace threadblock
+} // namespace warp
 } // namespace gemm
-} // namespace cutlass 
-
-////////////////////////////////////////////////////////////////////////////////
+} // namespace cutlass
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_trmm.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_trmm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/ell_mma_multistage.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/ell_mma_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/ell_mma_pipelined.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/ell_mma_pipelined.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/gemv.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/gemv.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/index_remat.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/index_remat.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_base.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_base.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_blas3_multistage.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_blas3_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_layernorm_mainloop_fusion_multistage.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_layernorm_mainloop_fusion_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_multistage.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_pipelined.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_pipelined.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_base.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_base.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_multistage.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_pipelined.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_pipelined.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_softmax_mainloop_fusion_multistage.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_softmax_mainloop_fusion_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_sparse_base.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_sparse_base.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_sparse_multistage.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_sparse_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_with_reduction_multistage.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_with_reduction_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle_streamk.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle_streamk.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_complex_tensor_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_complex_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_sparse_tensor_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_sparse_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op_sm80.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op_sm80.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_with_reduction_tensor_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_with_reduction_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_wmma_tensor_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_wmma_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/layernorm_scale_bias_transform.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/layernorm_scale_bias_transform.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reference/initialize_reference_operations.cu`

 * *Files 14% similar despite different names*

```diff
@@ -24,37 +24,40 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/*! \file
-    \brief Templates exposing architecture support for warp-level multiply-add operations
-*/
+/* \file
+   \brief
 
-#pragma once
+*/
 
 #include "cutlass/cutlass.h"
+#include "cutlass/library/library.h"
+#include "cutlass/library/manifest.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
-namespace gemm {
-namespace warp {
+namespace library {
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+void initialize_gemm_reference_operations(Manifest &manifest);
+void initialize_conv2d_reference_operations(Manifest &manifest);
+void initialize_conv3d_reference_operations(Manifest &manifest);
 
-/// Query the number of threads per warp
-template <typename OperatorClass>
-struct WarpSize {
-  static int const value = 32;
-};
+///////////////////////////////////////////////////////////////////////////////////////////////////
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+void initialize_reference_operations(Manifest &manifest) {
+  initialize_conv2d_reference_operations(manifest);
+  initialize_conv3d_reference_operations(manifest);
+  initialize_gemm_reference_operations(manifest);
+}
+
+///////////////////////////////////////////////////////////////////////////////////////////////////
 
-} // namespace warp
-} // namespace gemm
+} // namespace library
 } // namespace cutlass
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+///////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_fast_f32.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_fast_f32.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_tile_iterator_sm80.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_tile_iterator_sm80.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op_tile_iterator_sm80.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op_tile_iterator_sm80.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_planar_complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_planar_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_sm70.h`

 * *Files 19% similar despite different names*

```diff
@@ -25,30 +25,35 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Templates implementing warp-level matrix multiply-accumulate operations.
+    \brief Templates implementing warp-level matrix multiply-accumulate operations targeting
+      Tensor Cores.
+
+    This is a work in progress.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/array.h"
+
 #include "cutlass/numeric_types.h"
 #include "cutlass/matrix_shape.h"
+
+#include "cutlass/arch/mma.h"
+
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/gemm/warp/mma.h"
 
-#include "cutlass/gemm/thread/mma.h"
-
-#include "cutlass/gemm/warp/mma_simt_tile_iterator.h"
-#include "cutlass/gemm/warp/mma_simt_policy.h"
+#include "cutlass/gemm/warp/mma_tensor_op_policy.h"
+#include "cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm70.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace warp {
 
@@ -66,26 +71,20 @@
   typename ElementB_,
   /// Layout of B matrix (concept: MatrixLayout)
   typename LayoutB_,
   /// Element type of C matrix
   typename ElementC_,
   /// Layout of C matrix (concept: MatrixLayout)
   typename LayoutC_,
-  /// Shape of the warp in units of thread (concept: MmaSimtPolicy)
+  /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
   typename Policy_,
-  /// Number of partitions along K dimension
-  int PartitionsK = 1,
-  /// Complex transformation on operand A
-  ComplexTransform TransformA = ComplexTransform::kNone,
-  /// Complex transformation on operand B
-  ComplexTransform TransformB = ComplexTransform::kNone,
   /// Used for partial specialization
   typename Enable = bool
 >
-class MmaSimt {
+class MmaVoltaTensorOp {
 public:
   /// Shape of warp-level matrix operation (concept: GemmShape)
   using Shape = Shape_;
 
   /// Data type of multiplicand A
   using ElementA = ElementA_;
 
@@ -104,160 +103,177 @@
   /// Layout of accumulator matrix C
   using LayoutC = LayoutC_;
 
   /// Shape of the warp in units of thread (concept: MmaLanePolicySimt)
   using Policy = Policy_;
 
   /// Indicates class of matrix operator
-  using OperatorClass = arch::OpClassSimt;
+  using OperatorClass = arch::OpClassTensorOp;
 
-  /// Hard-coded for now
-  using ArchTag = arch::Sm50;
-
-  /// Complex transform on A operand
-  static ComplexTransform const kTransformA = TransformA;
-
-  /// Complex transform on B operand
-  static ComplexTransform const kTransformB = TransformB;
-
-  /// Layout of threads
-  using ThreadLayoutA = typename platform::conditional< platform::is_same< layout::ColumnMajorInterleaved<4>, LayoutA >::value,
-                  layout::ColumnMajor,
-                  typename platform::conditional < platform::is_same< layout::RowMajorInterleaved<4>, LayoutA >::value,
-                      layout::RowMajor,
-                      LayoutA>::type
-                 >::type;
-  
-  using ThreadLayoutB = typename platform::conditional< platform::is_same< layout::ColumnMajorInterleaved<4>, LayoutB >::value,
-                  layout::ColumnMajor,
-                  typename platform::conditional < platform::is_same< layout::RowMajorInterleaved<4>, LayoutB >::value,
-                      layout::RowMajor,
-                      LayoutB>::type
-                 >::type;
-
-  static constexpr bool use_dp4a = (platform::is_same< layout::ColumnMajorInterleaved<4>, LayoutA>::value || 
-                                    platform::is_same< layout::RowMajorInterleaved<4>, LayoutA >::value) && 
-                                    platform::is_same< ElementA, int8_t >::value && 
-                                    platform::is_same< ElementB, int8_t >::value;
-
-  using dp4a_type = typename platform::conditional< use_dp4a , int8_t, bool >::type;
-
-  /// Thread-level matrix multiply accumulate operator
-  using ThreadMma = thread::Mma<
-    GemmShape<
-      Shape::kM / Policy::WarpShape::kRow,
-      Shape::kN / Policy::WarpShape::kColumn,
-      Policy::LaneMmaShape::kK>,
-    ElementA,
-    ThreadLayoutA,
-    ElementB,
-    ThreadLayoutB,
-    ElementC,
-    LayoutC,
-    arch::OpMultiplyAdd,
-    dp4a_type
-  >;
+  /// Architecture tag
+  using ArchTag = arch::Sm70;
 
   /// Underlying matrix multiply operator (concept: arch::Mma)
-  using ArchMmaOperator = typename ThreadMma::ArchMmaOperator;
+  using ArchMmaOperator = typename Policy::Operator;
 
   /// Indicates math operator 
   using MathOperator = typename ArchMmaOperator::Operator;
   
-  /// Shape of the underlying instruction
-  using InstructionShape = GemmShape<1,1,use_dp4a ? 4 : 1>;
+  /// Underlying instruction shape
+  using InstructionShape = typename ArchMmaOperator::Shape;
+
+  /// Complex transform on A operand
+  static ComplexTransform const kTransformA = ComplexTransform::kNone;
+
+  /// Complex transform on B operand
+  static ComplexTransform const kTransformB = ComplexTransform::kNone;
+
+  /// Number of threads participating in warp-level matrix product
+  static int const kThreadCount = 32;
 
+  /// interleaved 32x32 tiles
+  using InterleavedTileShape = GemmShape<32, 32, 4>;
+
+  static_assert(!(Shape::kM % InterleavedTileShape::kM) &&
+                !(Shape::kN % InterleavedTileShape::kN),
+                "Shape must be a multiple of InterleavedTileShape.");
 public:
 
   /// Iterates over the A operand in memory
-  using IteratorA = MmaSimtTileIterator<
-    MatrixShape<Shape::kM, Policy::LaneMmaShape::kK>,
+  using IteratorA = MmaVoltaTensorOpMultiplicandTileIterator<
+    MatrixShape<Shape::kM, Shape::kK>,
     Operand::kA,
     ElementA,
     LayoutA,
-    Policy,
-    PartitionsK,
-    Shape::kK
+    MatrixShape<
+      ArchMmaOperator::Shape::kM,
+      ArchMmaOperator::Shape::kK
+    >,
+    Policy::OpDelta::kRow,
+    kThreadCount
   >;
 
   /// Storage for A tile
   using FragmentA = typename IteratorA::Fragment;
 
-  /// Storage for transformed A tile
-  using TransformedFragmentA = FragmentA;
-
   /// Iterates over the B operand in memory
-  using IteratorB = MmaSimtTileIterator<
-    MatrixShape<Policy::LaneMmaShape::kK, Shape::kN>,
+  using IteratorB = MmaVoltaTensorOpMultiplicandTileIterator<
+    MatrixShape<Shape::kK, Shape::kN>,
     Operand::kB,
     ElementB,
     LayoutB,
-    Policy,
-    PartitionsK,
-    Shape::kK
+    MatrixShape<
+      ArchMmaOperator::Shape::kK,
+      ArchMmaOperator::Shape::kN
+    >,
+    Policy::OpDelta::kRow,
+    kThreadCount
   >;
 
   /// Storage for B tile
   using FragmentB = typename IteratorB::Fragment;
 
-  /// Storage for transformed A tile
-  using TransformedFragmentB = FragmentB;
-
   /// Iterates over the C operand in memory
-  using IteratorC = MmaSimtTileIterator<
+  using IteratorC = MmaVoltaTensorOpAccumulatorTileIterator<
     MatrixShape<Shape::kM, Shape::kN>,
-    Operand::kC,
     ElementC,
     LayoutC,
-    Policy
+    typename ArchMmaOperator::Shape,
+    typename Policy::OpDelta
   >;
 
   /// Storage for C tile
-  using FragmentC = typename ThreadMma::FragmentC;
+  using FragmentC = typename IteratorC::Fragment;
+
+private:
+
+  static_assert(
+    !(Shape::kM % ArchMmaOperator::Shape::kM) && 
+    !(Shape::kN % ArchMmaOperator::Shape::kN),
+    "Shape of warp-level Mma must be divisible by operator shape.");
+
+  /// Number of mma operations performed
+  using MmaIterations = MatrixShape<
+    InterleavedTileShape::kM / ArchMmaOperator::Shape::kM,
+    InterleavedTileShape::kN / ArchMmaOperator::Shape::kN
+  >;
+  using TileIterations = MatrixShape<
+    Shape::kM / InterleavedTileShape::kM,
+    Shape::kN / InterleavedTileShape::kN
+  >;
+
+  // Whether matrix B is reordered
+  bool reorder_B_;
+
+public:
+
+  /// Underlying matrix multiply operator (concept: arch::Mma)
+  ArchMmaOperator mma;
 
 public:
 
   //
   // Methods
   //
-
+  
   /// Ctor
   CUTLASS_DEVICE
-  MmaSimt() {}
+  MmaVoltaTensorOp() {}
 
   /// Performs a warp-level matrix multiply-accumulate operation
   CUTLASS_DEVICE
   void operator()(
-    FragmentC &d, 
-    FragmentA a, 
-    FragmentB b, 
-    FragmentC const &c, int group_idx = 0) const {
-
-    ThreadMma mma;
-
-    if (kTransformA == ComplexTransform::kConjugate) {
-      conjugate<FragmentA> conj_a;
-      a = conj_a(a);
-    }
-
-    if (kTransformB == ComplexTransform::kConjugate) {
-      conjugate<FragmentB> conj_b;
-      b = conj_b(b);
+    FragmentC &D, 
+    FragmentA const &A, 
+    FragmentB const &B, 
+    FragmentC const &C)  {
+
+    using MmaOperandA = typename ArchMmaOperator::FragmentA;
+    using MmaOperandB = typename ArchMmaOperator::FragmentB;
+    using MmaOperandC = typename ArchMmaOperator::FragmentC;
+
+    D = C;
+
+    MmaOperandA const *ptr_A = reinterpret_cast<MmaOperandA const *>(&A);
+    MmaOperandB const *ptr_B = reinterpret_cast<MmaOperandB const *>(&B);
+    MmaOperandC *ptr_D = reinterpret_cast<MmaOperandC *>(&D);
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int outer_col = 0; outer_col < TileIterations::kColumn; ++outer_col) {
+      CUTLASS_PRAGMA_UNROLL
+      for (int inner_col = 0; inner_col < MmaIterations::kColumn; ++inner_col) {
+        CUTLASS_PRAGMA_UNROLL
+        for (int outer_row = 0; outer_row < TileIterations::kRow; ++outer_row) {
+          CUTLASS_PRAGMA_UNROLL
+
+          for (int inner_row = 0; inner_row < MmaIterations::kRow; ++inner_row) {
+      
+            int op_col = inner_col + MmaIterations::kColumn * outer_col;
+
+            // Column-major serpentine sequence to maximize reuse of A operand.
+            int inner_row_serp = inner_row;
+            int outer_row_serp = outer_row;
+            if (op_col & 1) {
+              inner_row_serp = MmaIterations::kRow - inner_row - 1;
+              outer_row_serp = TileIterations::kRow - outer_row - 1;
+            }
+            int op_row = inner_row_serp + MmaIterations::kRow * outer_row_serp;
+            int op_idx = inner_row_serp + MmaIterations::kRow * 
+                         (inner_col + MmaIterations::kColumn * 
+                          (outer_row_serp + TileIterations::kRow * outer_col));
+            mma(
+              ptr_D[op_idx],
+              ptr_A[op_row],
+              ptr_B[op_col],
+              ptr_D[op_idx]);
+
+          }
+        }
+      }
     }
-
-    mma(d, a, b, c);
-  }
-
-  /// Transform the mma operands to the required types
-  CUTLASS_DEVICE
-  void transform(TransformedFragmentA &dst_A, TransformedFragmentB &dst_B,
-                 FragmentA const &A, FragmentB const &B) const {
-    //TODO: Implement this
-    dst_A = A;
-    dst_B = B;
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace warp
 } // namespace gemm
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt_policy.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt_policy.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_sparse_tensor_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_sparse_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_fast_f32.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_fast_f32.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_fragment_iterator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_fragment_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_policy.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator.h`

 * *Files 18% similar despite different names*

```diff
@@ -25,41 +25,34 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Policy describing implementation details of warp-level GEMM targeting Tensor Cores.
+    \brief Templates implementing the address computation of storing of tiles
+   from pitch-linear rank=2 tensors.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
-#include "cutlass/matrix_shape.h"
-#include "cutlass/gemm/gemm.h"
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
-namespace gemm {
-namespace warp {
+namespace transform {
+namespace threadblock {
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
-/// Policy 
-template <
-  typename Operator_,        ///< hardware instruction(s) performing TensorOp (concept: arch::Mma)
-  typename OpDelta_          ///< distance between operations (concept: MatrixShape)
->
-struct MmaTensorOpPolicy {
-
-  using Operator = Operator_;    ///< hardware instruction(s) performing TensorOp (concept: arch::Mma)
-  using OpDelta = OpDelta_;      ///< distance between operations (concept: MatrixShape)
-  using MmaShape = typename Operator::Shape;
-};
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-} // namespace warp
-} // namespace gemm
-} // namespace cutlass
+template <typename Shape, typename Element, typename Layout, int AdvanceRank,
+          typename ThreadMap,
+          int Alignment =
+              sizeof_bits<Element>::value* ThreadMap::kElementsPerAccess / 8>
+class RegularTileAccessIterator;
+
+////////////////////////////////////////////////////////////////////////////////
+
+}  // namespace threadblock
+}  // namespace transform
+}  // namespace cutlass
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_sm70.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_wmma.h`

 * *Files 20% similar despite different names*

```diff
@@ -27,254 +27,197 @@
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
     \brief Templates implementing warp-level matrix multiply-accumulate operations targeting
       Tensor Cores.
-
-    This is a work in progress.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
-#include "cutlass/array.h"
+#include "cutlass/arch/wmma.h"
+
+#if defined(CUTLASS_ARCH_WMMA_ENABLED)
 
+#include "cutlass/wmma_array.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/matrix_shape.h"
 
-#include "cutlass/arch/mma.h"
+#include "cutlass/arch/memory_sm75.h"
+#include "cutlass/arch/mma_sm75.h"
+#include "cutlass/arch/mma_sm80.h"
 
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/gemm/warp/mma.h"
 
 #include "cutlass/gemm/warp/mma_tensor_op_policy.h"
-#include "cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm70.h"
+
+#include "cutlass/gemm/warp/mma_tensor_op_tile_iterator_wmma.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace warp {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Structure to compute the matrix product targeting CUDA cores and SIMT math instructions.
-template <
-  /// Size of the Gemm problem - concept: gemm::GemmShape<>
+///< Structure to compute the matrix product targeting CUDA cores via WMMA.
+template < 
+  ///< Size of the Gemm problem - concept: gemm::GemmShape<>
   typename Shape_,
-  /// Data type of A elements
+  ///< Data type of A elements
   typename ElementA_,
-  /// Layout of A matrix (concept: MatrixLayout)
+  ///< Layout of A matrix (concept: MatrixLayout)
   typename LayoutA_,
-  /// Data type of B elements
+  ///< Data type of B elements
   typename ElementB_,
   /// Layout of B matrix (concept: MatrixLayout)
   typename LayoutB_,
-  /// Element type of C matrix
+  ///< Element type of C matrix
   typename ElementC_,
-  /// Layout of C matrix (concept: MatrixLayout)
+  ///< Layout of C matrix (concept: MatrixLayout)
   typename LayoutC_,
-  /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
+  ///< Policy describing warp-level Wmma operation (concept: MmaTensorOpPolicy)
   typename Policy_,
-  /// Used for partial specialization
+  ///< Number of partitions along K dimension
+  int PartitionsK_ = 1,
+  ///< Used for partial specialization
   typename Enable = bool
 >
-class MmaVoltaTensorOp {
+class MmaTensorOpWmma {
 public:
-  /// Shape of warp-level matrix operation (concept: GemmShape)
+  ///< Shape of warp-level matrix operation (concept: GemmShape)
   using Shape = Shape_;
 
-  /// Data type of multiplicand A
+  ///< Data type of multiplicand A
   using ElementA = ElementA_;
 
-  /// Layout of multiplicand A
+  ///< Layout of multiplicand A
   using LayoutA = LayoutA_;
 
-  /// Data type of multiplicand B
+  ///< Data type of multiplicand B
   using ElementB = ElementB_;
 
-  /// Layout of multiplicand B
+  ///< Layout of multiplicand B
   using LayoutB = LayoutB_;
 
-  /// Data type of accumulator matrix C
+  ///< Data type of accumulator matrix C
   using ElementC = ElementC_;
 
-  /// Layout of accumulator matrix C
+  ///< Layout of accumulator matrix C
   using LayoutC = LayoutC_;
 
-  /// Shape of the warp in units of thread (concept: MmaLanePolicySimt)
+  /// Shape of the warp in units of thread (concept: MmaTensorOpPolicy)
   using Policy = Policy_;
 
-  /// Indicates class of matrix operator
-  using OperatorClass = arch::OpClassTensorOp;
-
-  /// Architecture tag
-  using ArchTag = arch::Sm70;
+  /// Underlying instruction shape
+  using InstructionShape = typename Policy::Operator::Shape;
 
   /// Underlying matrix multiply operator (concept: arch::Mma)
   using ArchMmaOperator = typename Policy::Operator;
 
   /// Indicates math operator 
   using MathOperator = typename ArchMmaOperator::Operator;
   
-  /// Underlying instruction shape
-  using InstructionShape = typename ArchMmaOperator::Shape;
+  /// Underlying architecture tag
+  using ArchTag = typename Policy::Operator::ArchTag;
 
   /// Complex transform on A operand
   static ComplexTransform const kTransformA = ComplexTransform::kNone;
 
   /// Complex transform on B operand
   static ComplexTransform const kTransformB = ComplexTransform::kNone;
 
+  /// Indicates class of matrix operator
+  using OperatorClass = arch::OpClassWmmaTensorOp;
+
   /// Number of threads participating in warp-level matrix product
   static int const kThreadCount = 32;
 
-  /// interleaved 32x32 tiles
-  using InterleavedTileShape = GemmShape<32, 32, 4>;
+  /// Number of partitions along K dimension
+  static int const kPartitionsK = PartitionsK_;
 
-  static_assert(!(Shape::kM % InterleavedTileShape::kM) &&
-                !(Shape::kN % InterleavedTileShape::kN),
-                "Shape must be a multiple of InterleavedTileShape.");
 public:
 
   /// Iterates over the A operand in memory
-  using IteratorA = MmaVoltaTensorOpMultiplicandTileIterator<
-    MatrixShape<Shape::kM, Shape::kK>,
-    Operand::kA,
-    ElementA,
-    LayoutA,
-    MatrixShape<
-      ArchMmaOperator::Shape::kM,
-      ArchMmaOperator::Shape::kK
-    >,
-    Policy::OpDelta::kRow,
-    kThreadCount
-  >;
+  using IteratorA = MmaTensorOpWmmaMultiplicandTileIterator<
+     MatrixShape<Shape::kM, Shape::kK>, Operand::kA, ElementA, LayoutA,
+     Policy::OpDelta::kRow, kThreadCount, Policy>;
 
   /// Storage for A tile
   using FragmentA = typename IteratorA::Fragment;
 
   /// Iterates over the B operand in memory
-  using IteratorB = MmaVoltaTensorOpMultiplicandTileIterator<
-    MatrixShape<Shape::kK, Shape::kN>,
-    Operand::kB,
-    ElementB,
-    LayoutB,
-    MatrixShape<
-      ArchMmaOperator::Shape::kK,
-      ArchMmaOperator::Shape::kN
-    >,
-    Policy::OpDelta::kRow,
-    kThreadCount
-  >;
+  using IteratorB = MmaTensorOpWmmaMultiplicandTileIterator<
+     MatrixShape<Shape::kK, Shape::kN>, Operand::kB, ElementB, LayoutB,
+     Policy::OpDelta::kRow, kThreadCount, Policy>;
 
   /// Storage for B tile
   using FragmentB = typename IteratorB::Fragment;
 
   /// Iterates over the C operand in memory
-  using IteratorC = MmaVoltaTensorOpAccumulatorTileIterator<
-    MatrixShape<Shape::kM, Shape::kN>,
-    ElementC,
-    LayoutC,
-    typename ArchMmaOperator::Shape,
-    typename Policy::OpDelta
-  >;
+  using IteratorC = MmaTensorOpWmmaAccumulatorTileIterator<
+     MatrixShape<Shape::kM, Shape::kN>, ElementC, LayoutC,
+    typename Policy::OpDelta, Policy>;
 
   /// Storage for C tile
   using FragmentC = typename IteratorC::Fragment;
 
 private:
 
   static_assert(
-    !(Shape::kM % ArchMmaOperator::Shape::kM) && 
-    !(Shape::kN % ArchMmaOperator::Shape::kN),
-    "Shape of warp-level Mma must be divisible by operator shape.");
-
-  /// Number of mma operations performed
-  using MmaIterations = MatrixShape<
-    InterleavedTileShape::kM / ArchMmaOperator::Shape::kM,
-    InterleavedTileShape::kN / ArchMmaOperator::Shape::kN
-  >;
-  using TileIterations = MatrixShape<
-    Shape::kM / InterleavedTileShape::kM,
-    Shape::kN / InterleavedTileShape::kN
+    !(Shape::kM % Policy::Operator::Shape::kM) && 
+    !(Shape::kN % Policy::Operator::Shape::kN),
+    "Shape of warp-level Wmma must be divisible by operator shape (wmma native size)");
+
+  /// Number of wmma operations performed
+  using WmmaIterations = MatrixShape<
+    Shape::kM / Policy::Operator::Shape::kM,
+    Shape::kN / Policy::Operator::Shape::kN 
   >;
 
-  // Whether matrix B is reordered
-  bool reorder_B_;
-
 public:
 
-  /// Underlying matrix multiply operator (concept: arch::Mma)
-  ArchMmaOperator mma;
+  /// Underlying matrix multiply operator (concept: cutlass::arch::Wmma)
+  typename Policy::Operator wmma;
 
 public:
 
   //
   // Methods
   //
-  
+
   /// Ctor
   CUTLASS_DEVICE
-  MmaVoltaTensorOp() {}
+  MmaTensorOpWmma() {}
 
   /// Performs a warp-level matrix multiply-accumulate operation
   CUTLASS_DEVICE
   void operator()(
     FragmentC &D, 
     FragmentA const &A, 
     FragmentB const &B, 
-    FragmentC const &C)  {
-
-    using MmaOperandA = typename ArchMmaOperator::FragmentA;
-    using MmaOperandB = typename ArchMmaOperator::FragmentB;
-    using MmaOperandC = typename ArchMmaOperator::FragmentC;
-
-    D = C;
-
-    MmaOperandA const *ptr_A = reinterpret_cast<MmaOperandA const *>(&A);
-    MmaOperandB const *ptr_B = reinterpret_cast<MmaOperandB const *>(&B);
-    MmaOperandC *ptr_D = reinterpret_cast<MmaOperandC *>(&D);
+    FragmentC const &C) const {
 
     CUTLASS_PRAGMA_UNROLL
-    for (int outer_col = 0; outer_col < TileIterations::kColumn; ++outer_col) {
+    for (int n = 0; n < WmmaIterations::kColumn; ++n) {
       CUTLASS_PRAGMA_UNROLL
-      for (int inner_col = 0; inner_col < MmaIterations::kColumn; ++inner_col) {
-        CUTLASS_PRAGMA_UNROLL
-        for (int outer_row = 0; outer_row < TileIterations::kRow; ++outer_row) {
-          CUTLASS_PRAGMA_UNROLL
-
-          for (int inner_row = 0; inner_row < MmaIterations::kRow; ++inner_row) {
-      
-            int op_col = inner_col + MmaIterations::kColumn * outer_col;
-
-            // Column-major serpentine sequence to maximize reuse of A operand.
-            int inner_row_serp = inner_row;
-            int outer_row_serp = outer_row;
-            if (op_col & 1) {
-              inner_row_serp = MmaIterations::kRow - inner_row - 1;
-              outer_row_serp = TileIterations::kRow - outer_row - 1;
-            }
-            int op_row = inner_row_serp + MmaIterations::kRow * outer_row_serp;
-            int op_idx = inner_row_serp + MmaIterations::kRow * 
-                         (inner_col + MmaIterations::kColumn * 
-                          (outer_row_serp + TileIterations::kRow * outer_col));
-            mma(
-              ptr_D[op_idx],
-              ptr_A[op_row],
-              ptr_B[op_col],
-              ptr_D[op_idx]);
+      for (int m = 0; m < WmmaIterations::kRow; ++m) {
 
-          }
-        }
+        // accumulate wmma mma
+        wmma(D[m * WmmaIterations::kColumn + n], A[m], B[n], C[m * WmmaIterations::kColumn + n]);
       }
-    }
+    }  
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace warp
 } // namespace gemm
 } // namespace cutlass
+
+#endif // if defined(CUTLASS_ARCH_WMMA_ENABLED)
+
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_access_iterator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_access_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm70.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm70.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm80.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm80.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sparse.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sparse.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_wmma.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_wmma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_with_reduction_tensor_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_with_reduction_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/scale_bias_tile_iterator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/scale_bias_tile_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/softmax_scale_bias_transform.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/softmax_scale_bias_transform.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/tile_iterator_planar_complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/tile_iterator_planar_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/half.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/half.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/integer_subbyte.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/integer_subbyte.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/kernel_launch.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/kernel_launch.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/layout/layout.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/layout.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/layout/matrix.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/matrix.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/layout/permute.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/permute.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/layout/pitch_linear.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/pitch_linear.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm70.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm70.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm75.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm75.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm80.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm80.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/layout/vector.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/vector.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/matrix.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/matrix.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/matrix_coord.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/matrix_coord.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/matrix_shape.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/matrix_shape.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/numeric_conversion.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/numeric_conversion.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/numeric_types.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/numeric_types.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/pitch_linear_coord.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/pitch_linear_coord.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/platform/platform.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/platform/platform.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/predicate_vector.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/predicate_vector.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/quaternion.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/quaternion.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/real.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/real.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/reduction/device/reduce_split_k.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/device/reduce_split_k.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_contiguous.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_contiguous.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_strided.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_strided.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/reduce_softmax_final.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/reduce_softmax_final.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/reduce_split_k.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/reduce_split_k.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_contiguous.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_contiguous.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_strided.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_strided.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/reduce.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/reduce.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/reduction_operators.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/reduction_operators.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/reduction/threadblock_swizzle.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/threadblock_swizzle.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/relatively_equal.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/relatively_equal.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/semaphore.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/semaphore.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/subbyte_reference.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/subbyte_reference.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/tensor_coord.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/tensor_coord.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/tensor_ref.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/tensor_ref.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/tensor_ref_planar_complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/tensor_ref_planar_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/tensor_view.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/tensor_view.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/tensor_view_planar_complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/tensor_view_planar_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/tfloat32.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/tfloat32.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/thread/matrix.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/thread/matrix.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/trace.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/trace.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/pitch_linear_thread_map.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/pitch_linear_thread_map.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/thread/transpose.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/thread/transpose.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/thread/unary_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/thread/unary_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_iterator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_access_iterator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_access_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_iterator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_access_iterator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_access_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_iterator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_2dthreadtile.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_2dthreadtile.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_params.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_params.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_triangular_matrix.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_triangular_matrix.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_triangular_matrix.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_triangular_matrix.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_vector_access_iterator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/symm_complex.h`

 * *Files 26% similar despite different names*

```diff
@@ -24,394 +24,296 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
 /*! \file
-    \brief Templates implementing computing the addresses of loading small
-    vectors from the global memory.
+    \brief Reference implementation for complex-valued SYMM update in host-side code.
+
+    
 */
 
 #pragma once
 
-#include "cutlass/cutlass.h"
-#include "cutlass/array.h"
-#include "cutlass/coord.h"
-#include "cutlass/layout/pitch_linear.h"
-#include "cutlass/layout/matrix.h"
-#include "cutlass/layout/tensor.h"
-#include "cutlass/matrix_coord.h"
-#include "cutlass/matrix_shape.h"
-#include "cutlass/tensor_ref.h"
-
-////////////////////////////////////////////////////////////////////////////////
+#include "cutlass/blas3.h"
+#include "cutlass/complex.h"
+#include "cutlass/numeric_conversion.h"
+#include "cutlass/tensor_view.h"
+#include "cutlass/gemm/gemm.h"
+#include <assert.h>
 
 namespace cutlass {
-namespace transform {
-namespace threadblock {
+namespace reference {
+namespace host {
 
-////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// PredicatedVectorAccessIterator
+/// Computes a general matrix product among matrices (tensors of rank=2) pointed to by TensorRef
+/// objects.
 ///
+/// Explicitly naming types needed by this template can be cumbersome, particularly for the
+/// accumulator type, so a function argument 'initial_accum' is exposed. Passing
+/// AccumulatorType(0) as the last function argument can be easier than naming all template
+/// arguments explicitly.
 template <
-    /// Shape of the vector accessed by the entire threadblock
-    typename Shape,
-    /// Shape of the vector accessed by the warp
-    typename WarpShape,
-    /// Type of Element
-    typename Element,
-    /// Layout of the vector
-    typename Layout,
-    /// Number of elements for each access
-    int ElementsPerAccess,
-    /// Support residual tile
-    bool EnableResidualAccess = false
+  typename ElementA,
+  typename LayoutA,
+  SideMode SideModeA,
+  FillMode FillModeA,
+  typename ElementB,
+  typename LayoutB,
+  typename ElementC,
+  typename LayoutC,
+  typename ScalarType,
+  typename ComputeType,
+  BlasMode BlasMode_ = BlasMode::kSymmetric,
+  typename InnerProductOp = multiply_add<ComputeType>,
+  typename ConvertOp = NumericConverter<ElementC, ScalarType>
 >
-class PredicatedVectorAccessIterator;
+void compute_symm_complex(
+  gemm::GemmCoord problem_size,
+  ScalarType alpha,
+  TensorRef<ElementA, LayoutA> tensor_a,
+  TensorRef<ElementB, LayoutB> tensor_b,
+  ScalarType beta,
+  TensorRef<ElementC, LayoutC> tensor_c,
+  TensorRef<ElementC, LayoutC> tensor_d,
+  ComputeType initial_accum,
+  int batch_count = 1,
+  int64_t batch_stride_A = 0,
+  int64_t batch_stride_B = 0,
+  int64_t batch_stride_C = 0,
+  int64_t batch_stride_D = 0) {
+  
+  static SideMode const kSideModeA = SideModeA;
+  static FillMode const kFillModeA = FillModeA;
+  static BlasMode const kBlasMode  = BlasMode_;
+
+  static_assert(
+    LayoutA::kRank == 2 &&
+    LayoutB::kRank == 2 &&
+    LayoutC::kRank == 2, "Tensors must be of rank 2");
+
+  static_assert(kSideModeA != SideMode::kInvalid
+                , "Side Mode can either be Left or Right.");
+
+  static_assert(
+    kFillModeA == FillMode::kLower || 
+    kFillModeA == FillMode::kUpper, 
+    "Fill Mode can either be Lower or Upper.");
+
+  using CompareOp_w_diag =  typename TrMatrixCompareOp<kFillModeA, DiagType::kNonUnit>::Type;
+  using CompareOp_wo_diag = typename TrMatrixCompareOp<kFillModeA, DiagType::kZero>::Type;
+
+  // Note: batch is ignored.
+  int const M = problem_size.m();
+  int const N = problem_size.n();
+  // Assuming correct k-dimension value is passed
+  int const K = problem_size.k();
+
+  // Blocking necessary to speedup reference implementation
+  int const Mblock = 16;
+  int const Nblock = 16;
+
+  ConvertOp convert_op;
+  InnerProductOp inner_product_op;
+  CompareOp_w_diag compare_op_1;
+  CompareOp_wo_diag compare_op_2;
+
+  for (int batch_idx = 0; batch_idx < batch_count; ++batch_idx) {
+
+    // Compute matrix product using blocks
+    for (int row_block = 0; row_block < M; row_block += Mblock) {
+      for (int col_block = 0; col_block < N; col_block += Nblock) {
+
+        ComputeType accum[Mblock][Nblock];
+
+        for (int j = 0; j < Nblock; j++) {
+          for (int i = 0; i < Mblock; i++) {
+            accum[i][j] = initial_accum;
+          }
+        }
+
+        for (int k_block = 0; k_block < K; ++k_block) {
+          for (int j = 0; j < Nblock; j++) {
+            for (int i = 0; i < Mblock; i++) {
+              int row = row_block + i;
+              int col = col_block + j;
+
+              if (row < M && col < N) 
+              {
+                ElementA a_1 = ElementA();
+                ElementB b_1 = ElementB();
+                ElementA a_2 = ElementA();
+                ElementB b_2 = ElementB();
+                
+                // A x B or B x A (with diagonal)
+                if (kSideModeA == SideMode::kLeft) {
+                  a_1 = (compare_op_1(row, k_block)) ? 
+                        (tensor_a.at(MatrixCoord(row, k_block))) : ElementA();
+                  b_1 = tensor_b.at(MatrixCoord(k_block, col));
+                } else if (kSideModeA == SideMode::kRight) {
+                  a_1 = tensor_b.at(MatrixCoord(row, k_block));
+                  b_1 = (compare_op_1(k_block, col)) ? 
+                        tensor_a.at(MatrixCoord(k_block, col)) : ElementA();
+                }
+                ComputeType compute_a_1 = ComputeType(a_1);
+                ComputeType compute_b_1 = ComputeType(b_1);
+
+                // The imaginary parts of the diagonal elements of 
+                // a complex data type are assumed and set to zero
+                if (kBlasMode == BlasMode::kHermitian && kSideModeA == SideMode::kLeft && row == k_block) {
+                  compute_a_1 = real(compute_a_1);
+                } else if (kBlasMode == BlasMode::kHermitian && kSideModeA == SideMode::kRight && k_block == col) {
+                  compute_b_1 = real(compute_b_1);
+                }
+
+                accum[i][j] = inner_product_op(compute_a_1, compute_b_1,  accum[i][j]);
+
+                // A^T x B or B x A^T (without diagonal)
+                if (kSideModeA == SideMode::kLeft) {
+                  a_2 = (compare_op_2(k_block, row)) ? 
+                        (tensor_a.at(MatrixCoord(k_block, row))) : ElementA();
+                  b_2 = tensor_b.at(MatrixCoord(k_block, col));
+                  if (kBlasMode == BlasMode::kHermitian)
+                    a_2 = conj(a_2);
+                } else if (kSideModeA == SideMode::kRight) {
+                  a_2 = tensor_b.at(MatrixCoord(row, k_block));
+                  b_2 = (compare_op_2(col, k_block)) ? 
+                        tensor_a.at(MatrixCoord(col, k_block)) : ElementA();
+                  if (kBlasMode == BlasMode::kHermitian)
+                    b_2 = conj(b_2);
+                }
+
+                ComputeType compute_a_2 = ComputeType(a_2);
+                ComputeType compute_b_2 = ComputeType(b_2);
+
+                accum[i][j] = inner_product_op(compute_a_2, compute_b_2, accum[i][j]);
+              }
+            }
+          }
+        }
+
+        for (int j = 0; j < Nblock; j++) {
+          for (int i = 0; i < Mblock; i++) {
+            int row = row_block + i;
+            int col = col_block + j;
+
+            MatrixCoord coord = MatrixCoord(row, col);
+
+            if (row < M && col < N) {
+
+              ScalarType c = tensor_c.at(coord);
+
+              tensor_d.at(coord) = convert_op(
+                alpha * ScalarType(accum[i][j]) + 
+                beta * c);
+            }
+          }
+        }
+
+      } // for (col_block)
+    } // for (row_block)
+
+    tensor_a.add_pointer_offset(batch_stride_A);
+    tensor_b.add_pointer_offset(batch_stride_B);
+    tensor_c.add_pointer_offset(batch_stride_C);
+    tensor_d.add_pointer_offset(batch_stride_D);
 
-////////////////////////////////////////////////////////////////////////////////
+  } // for (batch_idx)
+}
 
-/// Vector access iterator specialized for vectors, e.g. scale and bias
-/// Thread arrangements are for TensorOps
-///
-template <
-  typename Shape_, 
-  typename WarpShape_, 
-  typename Element_, 
-  int ElementsPerAccess, 
-  bool EnableResidualAccess
->
-class PredicatedVectorAccessIterator <
-  Shape_,
-  WarpShape_,
-  Element_,
-  layout::PitchLinear,
-  ElementsPerAccess,
-  EnableResidualAccess
-> {
-  public:
-
-  using Shape = Shape_;
-  using WarpShape = WarpShape_;
-  using Element = Element_;
-  using Layout = layout::PitchLinear;
-
-  using Index = typename Layout::Index;
-  using LongIndex = typename Layout::LongIndex;
-
-  using TensorRef = TensorRef<Element, Layout>;
-  using TensorView = TensorView<Element, Layout>;
-  using TensorCoord = typename Layout::TensorCoord;
-
-  using ConstPointer = const Element *;
-  using NonConstPointer = typename platform::remove_const<Element>::type *;
-
-//  static int const kElementsPerAccess = 128 / sizeof_bits<Element>::value;
-  static int const kElementsPerAccess = ElementsPerAccess;
-  static int const kThreads = 32;
-  static int const kRowsPerIteration = 8;
-  static int const kThreadsPerRow = kThreads / kRowsPerIteration;
-  static int const kThreadsPerRowMask = 0x3;
-  static int const kIterations = WarpShape::kContiguous / (kThreadsPerRow * kElementsPerAccess); 
-  static int const kWarpCountStrided = Shape::kStrided / WarpShape::kStrided;
-
-  using AccessType = AlignedArray<Element, kElementsPerAccess>;
-
- private:
-  /// Internal pointer type permits fast address arithmetic
-  using BytePointer = char *;
-
- private:
-  //
-  // Data members
-  //
-
-  /// Internal pointer to first access of tile
-  BytePointer pointer_;
-
-  /// Extent of tensor
-  TensorCoord extent_;
-
-  /// pointer offset of each thread
-  TensorCoord thread_offset_;
-
-  /// iteration index
-  LongIndex iteration_;
-
-  /// residual access
-  bool is_residual_;
-
-  /// residual offset of each thread
-  TensorCoord residual_offset_;
-
- public:
-  /// Constructs a vector access iterator
-  CUTLASS_HOST_DEVICE
-  PredicatedVectorAccessIterator(
-    /// Pointer to the start of the vector
-    ConstPointer pointer,
-    /// Extent of vector
-    TensorCoord extent,
-    /// ID of each participating thread
-    int thread_id,
-    /// ID of each participating warp
-    int warp_id,
-    /// Initial offset of threadblock
-    TensorCoord const &threadblock_offset)
-    : pointer_(reinterpret_cast<BytePointer>(
-                       const_cast<NonConstPointer>(pointer))),
-      extent_(extent),
-      is_residual_(false) {
-
-
-    int warp_offset = (warp_id / kWarpCountStrided) * WarpShape::kContiguous;
-
-    // Per-thread offset in logical coordinates of tensor
-
-    thread_offset_ = threadblock_offset + TensorCoord(warp_offset, 0) +
-        TensorCoord((thread_id & kThreadsPerRowMask) * kElementsPerAccess, 0);
-
-    set_iteration_index(0);
-
-    if(EnableResidualAccess) {
-      // compute residual offset
-      typename TensorCoord::Index residual_size = extent_.contiguous() % WarpShape::kContiguous;
-      if (residual_size) {
-        is_residual_ = true;
-        residual_offset_ = make_Coord(residual_size, 0);
-      }
-    }
-  }
-
-  /// Construct a PredicatedVectorAccessIterator with zero threadblock offset
-  CUTLASS_HOST_DEVICE
-  PredicatedVectorAccessIterator(
-    /// Pointer to start of vector
-    ConstPointer pointer,
-    /// Extent of vector
-    TensorCoord extent,
-    ///< ID of each participating thread
-    int thread_id,
-    /// ID of each participating warp
-    int warp_id)
-    : PredicatedVectorAccessIterator(pointer, extent, thread_id, warp_id,
-                                     make_Coord(0, 0)) {}
-
-
-  /// Overrides the internal iteration index
-  CUTLASS_HOST_DEVICE
-  void set_iteration_index(int index) {
-    iteration_ = index;
-  }
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
-  /// Advances an iterator along logical dimensions of matrix in units of whole tiles
-  CUTLASS_DEVICE
-  void add_tile_offset(
-      TensorCoord const &tile_offset) {
-
-    thread_offset_ =
-        thread_offset_ +
-        TensorCoord(WarpShape::kContiguous * tile_offset.contiguous(), 0);
-  }
-
-  /// Returns a pointer
-  CUTLASS_HOST_DEVICE
-  AccessType *get() const {
-
-    return reinterpret_cast<AccessType *>(
-        pointer_ +
-        ((thread_offset_.contiguous() + iteration_ * kThreadsPerRow * kElementsPerAccess) 
-        * sizeof_bits<Element>::value / 8));
-  }
-
-  /// Increment and return an instance to self.
-  CUTLASS_HOST_DEVICE
-  PredicatedVectorAccessIterator &operator++() {
-    ++iteration_;
-    if(iteration_ >= kIterations)
-      iteration_ = 0; 
-
-    return *this;
-  }
-
-  /// Increment and return an instance to self.
-  CUTLASS_HOST_DEVICE
-  void advance() {
-    if(EnableResidualAccess && is_residual_) {
-      is_residual_ = false;
-      thread_offset_ += residual_offset_; 
-    }
-    else
-      add_tile_offset(TensorCoord(1, 0));
-  }
-
-  /// Increment and return an instance to self.
-  CUTLASS_HOST_DEVICE
-  PredicatedVectorAccessIterator operator++(int) {
-    PredicatedVectorAccessIterator self(*this);
-    operator++();
-    return self;
-  }
-
-  /// Returns whether access is valid or not
-  CUTLASS_HOST_DEVICE
-  bool valid() {
-    return ((thread_offset_.contiguous() + 
-              iteration_ * kThreadsPerRow * kElementsPerAccess) < extent_.contiguous());
-  }
-};
-
-////////////////////////////////////////////////////////////////////////////////
-
-/// Specialization of PredicatedVectorAccessIterator for row-major data.
-///
 template <
-  typename Shape_,
-  typename WarpShape_,
-  typename Element_,
-  int ElementsPerAccess,
-  bool EnableResidualAccess
+  typename ElementA,
+  typename LayoutA,
+  SideMode SideModeA,
+  FillMode FillModeA,
+  typename ElementB,
+  typename LayoutB,
+  typename ElementC,
+  typename LayoutC,
+  typename ScalarType,
+  typename ComputeType,
+  BlasMode BlasMode_ = cutlass::BlasMode::kSymmetric,
+  typename InnerProductOp = cutlass::arch::OpMultiplyAddComplex
 >
-class PredicatedVectorAccessIterator<
-  Shape_,
-  WarpShape_,
-  Element_,
-  layout::RowMajor,
-  ElementsPerAccess,
-  EnableResidualAccess
-> {
- public:
-
-  using Shape = Shape_;
-  using WarpShape = WarpShape_;
-  using Element = Element_;
-  using Layout = layout::RowMajor;
-
-  using Index = typename Layout::Index;
-  using LongIndex = typename Layout::LongIndex;
-
-  using TensorRef = TensorRef<Element, Layout>;
-  using TensorView = TensorView<Element, Layout>;
-  using TensorCoord = typename Layout::TensorCoord;
-
-  using ConstPointer = const Element *;
-  using NonConstPointer = typename platform::remove_const<Element>::type *;
-
-  using UnderlyingIterator = PredicatedVectorAccessIterator<
-      layout::PitchLinearShape<Shape::kColumn, Shape::kRow>, 
-      layout::PitchLinearShape<WarpShape::kColumn, WarpShape::kRow>, 
-      Element,
-      layout::PitchLinear,
-      ElementsPerAccess,
-      EnableResidualAccess>;
-
-  using AccessType = typename UnderlyingIterator::AccessType;
-  static int const kElementsPerAccess = UnderlyingIterator::kElementsPerAccess;
-  static int const kRowsPerIteration = UnderlyingIterator::kRowsPerIteration;
-  static int const kThreads = UnderlyingIterator::kThreads;
-  static int const kIterations = UnderlyingIterator::kIterations;
-
- private:
-  //
-  // Data members
-  //
-
-  /// Underlying pitch-linear tile iterator
-  UnderlyingIterator iterator_;
-
- public:
-  /// Constructs a TileIterator from its precomputed state, threadblock offset,
-  /// and thread ID
-  CUTLASS_HOST_DEVICE
-  PredicatedVectorAccessIterator(
-      ///< Pointer to the start of the vector
-      ConstPointer pointer,
-      ///< Extent of tensor
-      TensorCoord extent,
-      ///< ID of each participating thread
-      int thread_id,
-      ///< ID of each participating warp
-      int warp_id,
-      ///< Initial offset of threadblock
-      TensorCoord const &threadblock_offset)
-      : iterator_(pointer, layout::PitchLinearCoord(extent.column(), extent.row()),
-                  thread_id, warp_id,
-                  layout::PitchLinearCoord(threadblock_offset.column(),
-                                           threadblock_offset.row())) {}
-
-  /// Construct a PredicatedVectorAccessIterator with zero threadblock offset
-  CUTLASS_HOST_DEVICE
-  PredicatedVectorAccessIterator(
-      ConstPointer pointer,   ///< Pointer to the start of the vector
-      TensorCoord extent,     ///< Extent of tensor
-      int thread_id,          ///< ID of each participating thread
-      int warp_id             ///< ID of each participating warp
-      )
-      : PredicatedVectorAccessIterator(pointer, extent, thread_id, warp_id, 
-                                        make_Coord(0, 0)) {}
-
-  /// Overrides the internal iteration index
-  CUTLASS_HOST_DEVICE
-  void set_iteration_index(int index) { iterator_.set_iteration_index(index); }
-
-  /// Advances an iterator along logical dimensions of matrix in units of whole
-  /// tiles
-  CUTLASS_HOST_DEVICE
-  void add_tile_offset(TensorCoord const &tile_offset) {
-    iterator_.add_tile_offset({tile_offset.column(), tile_offset.row()});
-  }
-
-  /// Returns a pointer
-  CUTLASS_HOST_DEVICE
-  AccessType *get() const {
-    return reinterpret_cast<AccessType *>(iterator_.get());
-  }
+struct SymmComplex;
 
-  /// Advances to the next tile in memory.
-  ///
-  /// The first time this method is called, predicates are updated, and the
-  /// iterator's internal pointer is reverted to the first "steady state" tile.
-  /// Subsequent calls are lightweight and must only update the internal
-  /// pointer.
-  CUTLASS_HOST_DEVICE
-  PredicatedVectorAccessIterator &operator++() {
-    ++iterator_;
-    return *this;
-  }
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
-  /// Advances to the next tile in memory.
-  ///
-  /// The first time this method is called, predicates are updated, and the
-  /// iterator's internal pointer is reverted to the first "steady state" tile.
-  /// Subsequent calls are lightweight and must only update the internal
-  /// pointer.
-  CUTLASS_HOST_DEVICE
-  PredicatedVectorAccessIterator operator++(int) {
-    PredicatedVectorAccessIterator self(*this);
-    operator++();
-    return self;
+/// Partial specialization for multiply-add
+template <typename ElementA, typename LayoutA,
+          SideMode SideModeA, FillMode FillModeA, 
+          typename ElementB, typename LayoutB,
+          typename ElementC, typename LayoutC,
+          typename ScalarType, typename ComputeType,
+          BlasMode BlasMode_>
+struct SymmComplex<ElementA, LayoutA, 
+                   SideModeA, FillModeA,
+                   ElementB, LayoutB,
+                   ElementC, LayoutC, ScalarType,
+                   ComputeType, BlasMode_,
+                   arch::OpMultiplyAddComplex> {
+
+  void operator()(gemm::GemmCoord problem_size, ScalarType alpha,
+                  TensorRef<ElementA, LayoutA> tensor_a,
+                  TensorRef<ElementB, LayoutB> tensor_b, ScalarType beta,
+                  TensorRef<ElementC, LayoutC> tensor_c,
+                  TensorRef<ElementC, LayoutC> tensor_d,
+                  ComputeType initial_accum = ComputeType(0)) {
+    static_assert(
+        LayoutA::kRank == 2 && LayoutC::kRank == 2,
+        "Tensors must be of rank 2");
+
+    compute_symm_complex<ElementA, LayoutA,
+                 SideModeA, FillModeA,
+                 ElementB, LayoutB,
+                 ElementC, LayoutC, 
+                 ScalarType, ComputeType, BlasMode_, multiply_add<ComputeType>>(
+                 problem_size, alpha, tensor_a, tensor_b, beta, tensor_c, tensor_d, initial_accum);
   }
+};
 
-  /// Increment and return an instance to self.
-  CUTLASS_HOST_DEVICE
-  void advance() {
-    iterator_.advance();
-  }
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
-  /// Returns whether access is valid or not
-  CUTLASS_HOST_DEVICE
-  bool valid() {
-    return iterator_.valid();
+/// Partial specialization for gaussian multiply-add 
+template <typename ElementA, typename LayoutA,
+          SideMode SideModeA, FillMode FillModeA,
+          typename ElementB, typename LayoutB,
+          typename ElementC, typename LayoutC,
+          typename ScalarType, typename ComputeType,
+          BlasMode BlasMode_>
+struct SymmComplex<ElementA, LayoutA, 
+                   SideModeA, FillModeA, 
+                   ElementB, LayoutB,
+                   ElementC, LayoutC, ScalarType,
+                   ComputeType, BlasMode_,
+                   arch::OpMultiplyAddGaussianComplex> {
+
+  void operator()(gemm::GemmCoord problem_size, ScalarType alpha,
+                  TensorRef<ElementA, LayoutA> tensor_a,
+                  TensorRef<ElementB, LayoutB> tensor_b, ScalarType beta,
+                  TensorRef<ElementC, LayoutC> tensor_c,
+                  TensorRef<ElementC, LayoutC> tensor_d,
+                  ComputeType initial_accum = ComputeType(0)) {
+    static_assert(
+        LayoutA::kRank == 2 && LayoutC::kRank == 2,
+        "Tensors must be of rank 2");
+
+    compute_symm_complex<ElementA, LayoutA,
+                 SideModeA, FillModeA,
+                 ElementB, LayoutB,
+                 ElementC, LayoutC, 
+                 ScalarType, ComputeType, BlasMode_, multiply_add<ComputeType>>(
+                 problem_size, alpha, tensor_a, tensor_b, beta, tensor_c, tensor_d, initial_accum);
   }
 };
 
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
-////////////////////////////////////////////////////////////////////////////////
-
-}  // namespace threadblock
-}  // namespace transform 
-}  // namespace cutlass
-
+} // namespace host
+} // namespace reference
+} // namespace cutlass
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_scale_bias_vector_access_iterator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_scale_bias_vector_access_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator.h`

 * *Files 7% similar despite different names*

```diff
@@ -25,34 +25,38 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Templates implementing the address computation of storing of tiles
-   from pitch-linear rank=2 tensors.
+    \brief Templates implementing storing of tiles from pitch-linear rank=2 tensors. 
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
+#include "cutlass/numeric_types.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace transform {
 namespace threadblock {
 
 ////////////////////////////////////////////////////////////////////////////////
 
-template <typename Shape, typename Element, typename Layout, int AdvanceRank,
-          typename ThreadMap,
-          int Alignment =
-              sizeof_bits<Element>::value* ThreadMap::kElementsPerAccess / 8>
-class RegularTileAccessIterator;
+template <
+  typename Shape,
+  typename Element,
+  typename Layout,
+  int AdvanceRank,
+  typename ThreadMap,
+  int Alignment = sizeof_bits<Element>::value * ThreadMap::kElementsPerAccess / 8
+>
+class RegularTileIterator;
 
 ////////////////////////////////////////////////////////////////////////////////
 
-}  // namespace threadblock
-}  // namespace transform
-}  // namespace cutlass
+} // namespace threadblock
+} // namespace transform
+} // namespace cutlass
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear_2dthreadtile.h`

 * *Files 18% similar despite different names*

```diff
@@ -25,384 +25,485 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Templates implementing computing the addresses of storing of tiles
-   from pitch-linear rank=2 tensors.
+    \brief Templates implementing loading of tiles from pitch-linear rank=2 tensors. 
+
+    This iterator uses masks to guard out-of-bounds accesses and visits the last "residue" tile
+    first, with the objective of minimizing predicate mask updates during steady-state operation.
+
+    A precomputed "Params" object minimizes the amount of state that must be stored in registers,
+    and integer addition is used to advance the pointer through memory.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
-#include "cutlass/array.h"
-#include "cutlass/layout/pitch_linear.h"
-#include "cutlass/layout/matrix.h"
-#include "cutlass/matrix_coord.h"
-#include "cutlass/matrix_shape.h"
 #include "cutlass/tensor_ref.h"
+#include "cutlass/layout/matrix.h"
+#include "cutlass/layout/pitch_linear.h"
 
-#include "cutlass/transform/threadblock/regular_tile_access_iterator.h"
+#include "regular_tile_iterator.h"
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace transform {
 namespace threadblock {
 
-////////////////////////////////////////////////////////////////////////////////
-
-/// Tile iterator specialized for congruous arrangements for TensorOps
-///
-///
-/// Satisfies: ForwardTileIteratorConcept |
-///            ReadableContiguousTileIteratorConcept |
-///            WriteableContiguousTileIteratorConcept
-///
-template <typename Shape_, typename Element_, int AdvanceRank,
-          typename ThreadMap_, int Alignment>
-class RegularTileAccessIterator<
-    Shape_, Element_,
-    layout::PitchLinear,
-    AdvanceRank, ThreadMap_, Alignment> {
- public:
-  static_assert(
-      AdvanceRank == 0 || AdvanceRank == 1,
-      "Specialization for pitch-linear iterator may along advance along the "
-      "contiguous(rank=0) or strided(rank=1) dimension.");
+/////////////////////////////////////////////////////////////////////////////////////////////////
+template <
+  typename Shape,
+  typename Element,
+  typename Layout,
+  int AdvanceRank,
+  typename ThreadMap,
+  int Alignment = sizeof_bits<Element>::value * ThreadMap::kElementsPerAccess / 8
+>
+class RegularTileIterator2dThreadTile;
+
+
+/// Regular tile iterator specialized for pitch-linear + 2d thread-tiled threadmapping
+template <
+  typename Shape_,
+  typename Element_,
+  int AdvanceRank,
+  typename ThreadMap_,
+  int Alignment
+>
+class RegularTileIterator2dThreadTile<Shape_, Element_, layout::PitchLinear, AdvanceRank, ThreadMap_, Alignment> {
+public:
 
   using Shape = Shape_;
   using Element = Element_;
   using Layout = layout::PitchLinear;
   static int const kAdvanceRank = AdvanceRank;
+  using ThreadMap = ThreadMap_;
   static int const kAlignment = Alignment;
 
   using Index = typename Layout::Index;
   using LongIndex = typename Layout::LongIndex;
   using StrideIndex = typename Layout::Stride::Index;
 
   using TensorRef = TensorRef<Element, Layout>;
   using TensorCoord = typename Layout::TensorCoord;
 
-  using ThreadMap = ThreadMap_;
+  using Fragment = Array<Element, ThreadMap::Iterations::kCount * ThreadMap::ThreadAccessShape::kCount>;
+
+  static_assert(kAdvanceRank == 0 || kAdvanceRank == 1, 
+    "Advance rank may only be along the contiguous or strided dimensions.");
+
+private:
 
-  /// Element type per access
-  using AccessType = Array<Element, ThreadMap::kElementsPerAccess>;
+  //
+  // Types
+  //
+  
+  using AccessType = AlignedArray<Element, ThreadMap::ThreadAccessShape::kCount, kAlignment>;
 
- private:
   //
   // Data members
   //
 
-  /// Stride value
+  /// Pointer to memory
+  uint8_t *pointer_;
+
+  /// Stride quantity
   StrideIndex stride_;
 
-  /// Internal pointer to first access of tile
-  AccessType *pointer_;
+  /// Amount to increment pointer along strided dimension
+  LongIndex increment_strided_;
 
-  /// Internal byte offset
-  Index byte_offset_;
+  /// Amount to advance pointer between tiles
+  LongIndex increment_advance_;
 
-  /// Iteration in the contiguous dimension
-  int iteration_contiguous_;
+public:
 
-  /// Iteration in the strided dimension
-  int iteration_strided_;
+  CUTLASS_DEVICE
+  RegularTileIterator2dThreadTile(): pointer_(nullptr), increment_strided_(0), increment_advance_(0) { }
 
- public:
-  /// Construct a TileIterator with zero threadblock offset
-  CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
-                            int thread_id   ///< ID of each participating thread
-                            )
-      : stride_(ref.stride(0) / ThreadMap::kElementsPerAccess),
-        byte_offset_(0) {
+  CUTLASS_DEVICE
+  RegularTileIterator2dThreadTile(
+    TensorRef const &ref, 
+    int thread_idx,
+    int interleave
+  ){ 
+    
+    TensorCoord t = ThreadMap::initial_offset(thread_idx);
+    long int offset = t[0] * interleave + t[1] * ref.stride()[0]/interleave;
+    pointer_ = reinterpret_cast<uint8_t *>(ref.data() + offset);
+
+    stride_ = ref.stride()[0] / interleave;
+    increment_strided_ = (ref.stride()[0] * sizeof_bits<Element>::value / 8) * ThreadMap::Delta::kStrided / interleave;
+
+    increment_advance_ = 
+      (kAdvanceRank == 0 ? 
+        Shape::kContiguous * sizeof_bits<Element>::value / 8 : 
+        Shape::kStrided * (ref.stride()[0] * sizeof_bits<Element>::value / 8) / interleave);
+  }
+
+  /// Loads a fragment
+  CUTLASS_DEVICE
+  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) {
+
+    AccessType *frag_ptr = reinterpret_cast<AccessType *>(&frag);
+    uint8_t const *byte_pointer = pointer_ + pointer_offset * sizeof_bits<Element>::value / 8;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int s = 0; s < ThreadMap::Iterations::kStrided; ++s) {
+
+      AccessType const *access_ptr = reinterpret_cast<AccessType const *>(byte_pointer);
 
-    layout::PitchLinearCoord thread_offset_base = ThreadMap::initial_offset(thread_id);
+      CUTLASS_PRAGMA_UNROLL
+      for (int c = 0; c < ThreadMap::Iterations::kContiguous; ++c) {
 
-    // initialize pointer
-    pointer_ = reinterpret_cast<AccessType *>(ref.data() + ref.offset(thread_offset_base));
+          int idx = c + s * ThreadMap::Iterations::kContiguous;
+           frag_ptr[idx] = access_ptr[c * ThreadMap::Delta::kContiguous / ThreadMap::ThreadAccessShape::kStrided];
+        }
 
-    set_iteration_index(0);
+      if (s + 1 < ThreadMap::Iterations::kStrided) {
+        byte_pointer += increment_strided_;
+      }
+    }
   }
 
-  /// Overrides the internal iteration index
+  /// Loads a fragment
   CUTLASS_HOST_DEVICE
-  void set_iteration_index(int index) {
-    iteration_contiguous_ = index % ThreadMap::Iterations::kContiguous;
-    iteration_strided_ = index / ThreadMap::Iterations::kContiguous;
+  void load(Fragment &frag, TensorCoord const & tile_offset) {
+    load_with_pointer_offset(
+      frag, 
+      tile_offset.contiguous() * Shape::kContiguous / ThreadMap::kElementsPerAccess + 
+        tile_offset.strided() * Shape::kStrided * stride_
+    );
   }
 
-  /// Adds a pointer offset in units of Element
+  /// Loads a fragment
   CUTLASS_HOST_DEVICE
-  void add_pointer_offset(LongIndex pointer_offset) {
-    byte_offset_ += pointer_offset * sizeof(Element);
+  void load(Fragment &frag) {
+    load_with_pointer_offset(frag, 0);
   }
 
-  /// Returns a pointer
-  CUTLASS_DEVICE
-  AccessType *get() const {
+  /// Stores a fragment
+  CUTLASS_HOST_DEVICE
+  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
 
-    AccessType *access_ptr = pointer_;
+    AccessType const *frag_ptr = reinterpret_cast<AccessType const*>(&frag);
+    uint8_t *byte_pointer = pointer_ + pointer_offset * sizeof_bits<Element>::value / 8;
 
-    int access_offset = iteration_strided_ * ThreadMap::Delta::kStrided * stride_ +
-                        iteration_contiguous_ * ThreadMap::Delta::kContiguous /
-                            ThreadMap::kElementsPerAccess;
+    CUTLASS_PRAGMA_UNROLL
+    for (int s = 0; s < ThreadMap::Iterations::kStrided; ++s) {
 
-    char *access_byte_ptr =
-        reinterpret_cast<char *>(access_ptr + access_offset);
+      AccessType *access_ptr = reinterpret_cast<AccessType *>(byte_pointer);
 
-    return reinterpret_cast<AccessType *>(access_byte_ptr + byte_offset_);
-  }
+      CUTLASS_PRAGMA_UNROLL
+      for (int c = 0; c < ThreadMap::Iterations::kContiguous; ++c) {
 
-  /// Advances to the next tile in memory.
-  CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator &operator++() {
-    ++iteration_contiguous_;
+          int idx = c + s * ThreadMap::Iterations::kContiguous;
+          access_ptr[c * ThreadMap::Delta::kContiguous / ThreadMap::ThreadAccessShape::kStrided] = frag_ptr[idx];
+      }
 
-    if (iteration_contiguous_ < ThreadMap::Iterations::kContiguous)
-      return *this;
+      if (s + 1 < ThreadMap::Iterations::kStrided) {
+        byte_pointer += increment_strided_;
+      }
+    }
+  }
 
-    // Enter here only if (iteration_contiguous_ ==
-    // ThreadMap::Iteration::kContiguous)
-    iteration_contiguous_ = 0;
-    ++iteration_strided_;
+  /// Stores a fragment
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag, TensorCoord const & tile_offset) {
+    store_with_pointer_offset(
+      frag,
+      tile_offset.contiguous() * Shape::kContiguous + tile_offset.strided() * Shape::kStrided * stride_
+    );
+  }
 
-    if (iteration_strided_ < ThreadMap::Iterations::kStrided) {
-      return *this;
-    }
+  /// Stores a fragment
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag) {
+    store_with_pointer_offset(frag, 0);
+  }
 
-    // Enter here only if (iteration_stride_ == ThreadMap::Iteration::kStrided)
-    // which means we enter the next tile.
-    iteration_strided_ = 0;
+  /// Advances the pointer
+  CUTLASS_HOST_DEVICE
+  RegularTileIterator2dThreadTile &operator++() {
+    pointer_ += increment_advance_;
+    return *this;
+  }
 
+  /// Advances the pointer
+  CUTLASS_HOST_DEVICE
+  RegularTileIterator2dThreadTile &operator--() {
+    pointer_ -= increment_advance_;
     return *this;
   }
 
-  /// Advances to the next tile in memory.
+  /// Adds a pointer offset in units of Element
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator operator++(int) {
-    RegularTileAccessIterator prev(*this);
-    this->operator++();
-
-    return prev;
-  }
-
-  /// Adds a tile offset in the unit of tile.
-  /// In GEMM/Conv implementation, this is used to move in the k dimension in the shared memory.
-  /// Below layouts are the shared memory layouts.  Current SM50 SIMT kernels only use col major A and row major B.
-  ///   For row major A operand, k dimension is contiguous dimension;
-  ///   For col major A operand, k dimension is strided dimension;
-  ///   For row major B operand, k dimension is strided dimension;
-  ///   For col major B operand, k dimension is contiguous dimension.
-  /// Below two classes map col/row major to the pitch linear coordinates used
-  /// in this base class.
+  void add_pointer_offset(LongIndex pointer_offset) {
+    pointer_ += pointer_offset;
+  }
+
+  /// Adds a tile offset
   CUTLASS_DEVICE
   void add_tile_offset(TensorCoord const &coord) {
-    add_pointer_offset(coord.contiguous() * Shape::kContiguous +
-                       coord.strided() * Shape::kStrided * stride_ *
-                           ThreadMap::kElementsPerAccess);
+    int offset = sizeof_bits<Element>::value *
+        (coord.contiguous() * Shape::kContiguous + coord.strided() * Shape::kStrided * stride_) / 8;
+    add_pointer_offset(offset);
   }
+
 };
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Tile iterator specialized for column major layouts
-///
-///
-/// Satisfies: ForwardTileIteratorConcept |
-///            ReadableContiguousTileIteratorConcept |
-///            WriteableContiguousTileIteratorConcept
-///
-template <typename Shape_, typename Element_, int AdvanceRank,
-          typename ThreadMap_, int Alignment>
-class RegularTileAccessIterator<
-    Shape_, Element_,
-    layout::ColumnMajor,
-    AdvanceRank, ThreadMap_, Alignment> {
- public:
-  static_assert(
-      AdvanceRank == 0 || AdvanceRank == 1,
-      "Specialization for pitch-linear iterator may along advance along the "
-      "contiguous(rank=0) or strided(rank=1) dimension.");
+/// Regular tile iterator specialized for interleaved layout + 2d thread-tiled threadmapping
+template <
+  typename Shape_,
+  typename Element_,
+  int AdvanceRank,
+  typename ThreadMap_,
+  int Alignment
+>
+class RegularTileIterator2dThreadTile<Shape_, Element_, layout::RowMajorInterleaved<4>, AdvanceRank, ThreadMap_, Alignment> {
+public:
 
   using Shape = Shape_;
   using Element = Element_;
-  using Layout = layout::ColumnMajor;
+  using Layout = layout::RowMajorInterleaved<4>;
   static int const kAdvanceRank = AdvanceRank;
+  using ThreadMap = ThreadMap_;
   static int const kAlignment = Alignment;
 
   using Index = typename Layout::Index;
   using LongIndex = typename Layout::LongIndex;
 
   using TensorRef = TensorRef<Element, Layout>;
   using TensorCoord = typename Layout::TensorCoord;
 
-  using ThreadMap = ThreadMap_;
+  using Fragment = Array<Element, ThreadMap::Iterations::kCount * ThreadMap::ThreadAccessShape::kCount>;
 
-  /// Underlying iterator type
-  using UnderlyingIterator = RegularTileAccessIterator<
-      layout::PitchLinearShape<Shape::kRow, Shape::kColumn>, Element,
-      layout::PitchLinear,
-      (kAdvanceRank == 0 ? 0 : 1), 
-      ThreadMap_>;
+  using Underlying = RegularTileIterator2dThreadTile<
+    layout::PitchLinearShape<Shape::kColumn, Shape::kRow>,
+    Element,
+    layout::PitchLinear,
+    (kAdvanceRank == 0 ? 1 : 0),
+    ThreadMap,
+    kAlignment
+  >;
+
+  static_assert(kAdvanceRank == 0 || kAdvanceRank == 1, 
+    "Advance rank may only be along the row or column dimensions.");
 
-  using AccessType = typename UnderlyingIterator::AccessType;
+private:
 
- private:
+  Underlying iterator_;
 
-  /// Underlying iterator
-  UnderlyingIterator iterator_;
+public:
+
+  CUTLASS_DEVICE
+  RegularTileIterator2dThreadTile() { }
 
- public:
-  /// Construct a TileIterator with zero threadblock offset
+  CUTLASS_DEVICE
+  RegularTileIterator2dThreadTile(
+    TensorRef const &ref, 
+    int thread_idx
+  ):
+    iterator_({ref.data(), ref.stride()}, thread_idx, 4) {
+
+  }
+
+  /// Loads a fragment
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
-                            int thread_id   ///< ID of each participating thread
-                            )
-      : iterator_({ref.data(), ref.stride()}, thread_id) {}
+  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) {
+    iterator_.load_with_pointer_offset(frag, pointer_offset);
+  }
 
-  /// Overrides the internal iteration index
+  /// Loads a fragment
   CUTLASS_HOST_DEVICE
-  void set_iteration_index(int index) { iterator_.set_iteration_index(index); }
+  void load(Fragment &frag, TensorCoord const & tile_offset) {
+    iterator_.load_with_pointer_offset(frag, {tile_offset.column(), tile_offset.row()});
+  }
 
-  /// Adds a pointer offset in units of Element
+  /// Loads a fragment
   CUTLASS_HOST_DEVICE
-  void add_pointer_offset(LongIndex pointer_offset) {
-    iterator_.add_pointer_offset(pointer_offset);
+  void load(Fragment &frag) {
+    iterator_.load_with_pointer_offset(frag, 0);
   }
 
-  /// Returns a pointer
+  /// Stores a fragment
   CUTLASS_HOST_DEVICE
-  AccessType *get() const {
-    return reinterpret_cast<AccessType *>(iterator_.get());
+  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
+    iterator_.store_with_pointer_offset(frag, pointer_offset);
   }
 
-  /// Adds a tile offset
-  CUTLASS_DEVICE
-  void add_tile_offset(TensorCoord const &coord) {
-    iterator_.add_tile_offset({coord.row(), coord.column()});
+  /// Stores a fragment
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag, TensorCoord const & tile_offset) {
+    iterator_.store_with_pointer_offset(frag, {tile_offset.column(), tile_offset.row()});
+  }
+
+  /// Stores a fragment
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag) {
+    iterator_.store_with_pointer_offset(frag, 0);
   }
 
-  /// Advances to the next tile in memory.
+  /// Advances the pointer
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator &operator++() {
+  RegularTileIterator2dThreadTile &operator++() {
     ++iterator_;
     return *this;
   }
 
-  /// Advances to the next tile in memory.
+  /// Advances the pointer
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator operator++(int) {
-    RegularTileAccessIterator prev(*this);
-    ++iterator_;
+  RegularTileIterator2dThreadTile &operator--() {
+    --iterator_;
+    return *this;
+  }
 
-    return prev;
+  /// Adds a pointer offset in units of Element
+  CUTLASS_HOST_DEVICE
+  void add_pointer_offset(LongIndex pointer_offset) {
+    iterator_.add_pointer_offset(pointer_offset);
   }
-};
 
+  /// Adds a tile offset
+  CUTLASS_DEVICE
+  void add_tile_offset(TensorCoord const &coord) {
+    iterator_.add_tile_offset({coord.column(), coord.row()});
+  }
 
-////////////////////////////////////////////////////////////////////////////////
+};
 
-/// Tile iterator specialized for row major layouts
-///
-///
-/// Satisfies: ForwardTileIteratorConcept |
-///            ReadableContiguousTileIteratorConcept |
-///            WriteableContiguousTileIteratorConcept
-///
-template <typename Shape_, typename Element_, int AdvanceRank,
-          typename ThreadMap_, int Alignment>
-class RegularTileAccessIterator<
-    Shape_, Element_,
-    layout::RowMajor,
-    AdvanceRank, ThreadMap_, Alignment> {
- public:
-  static_assert(
-      AdvanceRank == 0 || AdvanceRank == 1,
-      "Specialization for pitch-linear iterator may along advance along the "
-      "contiguous(rank=0) or strided(rank=1) dimension.");
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Regular tile iterator specialized for interleaved layout + 2d thread-tiled threadmapping
+template <
+  typename Shape_,
+  typename Element_,
+  int AdvanceRank,
+  typename ThreadMap_,
+  int Alignment
+>
+class RegularTileIterator2dThreadTile<Shape_, Element_, layout::ColumnMajorInterleaved<4>, AdvanceRank, ThreadMap_, Alignment> {
+public:
 
   using Shape = Shape_;
   using Element = Element_;
-  using Layout = layout::RowMajor;
+  using Layout = layout::ColumnMajorInterleaved<4>;
   static int const kAdvanceRank = AdvanceRank;
+  using ThreadMap = ThreadMap_;
   static int const kAlignment = Alignment;
 
   using Index = typename Layout::Index;
   using LongIndex = typename Layout::LongIndex;
 
   using TensorRef = TensorRef<Element, Layout>;
   using TensorCoord = typename Layout::TensorCoord;
 
-  using ThreadMap = ThreadMap_;
+  using Fragment = Array<Element, ThreadMap::Iterations::kCount * ThreadMap::ThreadAccessShape::kCount>;
+  using PitchLinearThreadMap = PitchLinearStripminedThreadMap< layout::PitchLinearShape<Shape::kRow, Shape::kColumn>, 
+                                  ThreadMap::kThreads, ThreadMap::ThreadAccessShape::kCount >;
+                        
+
+  using Underlying = RegularTileIterator2dThreadTile<
+    layout::PitchLinearShape<Shape::kRow, Shape::kColumn>,
+    Element,
+    layout::PitchLinear,
+    (kAdvanceRank == 0 ? 0 : 1),
+    ThreadMap
+  >;
 
-  /// Underlying iterator type
-  using UnderlyingIterator = RegularTileAccessIterator<
-      layout::PitchLinearShape<Shape::kColumn, Shape::kRow>, Element,
-      layout::PitchLinear,
-      (kAdvanceRank == 0 ? 1 : 0), 
-      ThreadMap_>;
+  static_assert(kAdvanceRank == 0 || kAdvanceRank == 1, 
+    "Advance rank may only be along the row or column dimensions.");
 
-  using AccessType = typename UnderlyingIterator::AccessType;
+private:
 
- private:
+  Underlying iterator_;
 
-  /// Underlying iterator
-  UnderlyingIterator iterator_;
+public:
 
- public:
-  /// Construct a TileIterator with zero threadblock offset
+  CUTLASS_DEVICE
+  RegularTileIterator2dThreadTile() { }
+
+  CUTLASS_DEVICE
+  RegularTileIterator2dThreadTile(
+    TensorRef const &ref, 
+    int thread_idx
+  ):
+    iterator_({ref.data(), ref.stride()}, thread_idx, 4) {
+
+  }
+
+  /// Loads a fragment
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
-                            int thread_id   ///< ID of each participating thread
-                            )
-      : iterator_({ref.data(), ref.stride()}, thread_id) {}
+  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) {
+    iterator_.load_with_pointer_offset(frag, pointer_offset);
+  }
 
-  /// Overrides the internal iteration index
+  /// Loads a fragment
   CUTLASS_HOST_DEVICE
-  void set_iteration_index(int index) { iterator_.set_iteration_index(index); }
+  void load(Fragment &frag, TensorCoord const & tile_offset) {
+    iterator_.load_with_pointer_offset(frag, {tile_offset.row(), tile_offset.column()});
+  }
 
-  /// Adds a pointer offset in units of Element
+  /// Loads a fragment
   CUTLASS_HOST_DEVICE
-  void add_pointer_offset(LongIndex pointer_offset) {
-    iterator_.add_pointer_offset(pointer_offset);
+  void load(Fragment &frag) {
+    iterator_.load_with_pointer_offset(frag, 0);
   }
 
-  /// Returns a pointer
+  /// Stores a fragment
   CUTLASS_HOST_DEVICE
-  AccessType *get() const {
-    return reinterpret_cast<AccessType *>(iterator_.get());
+  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
+    iterator_.store_with_pointer_offset(frag, pointer_offset);
   }
 
-  /// Adds a tile offset
-  CUTLASS_DEVICE
-  void add_tile_offset(TensorCoord const &coord) {
-    iterator_.add_tile_offset({coord.column(), coord.row()});
+  /// Stores a fragment
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag, TensorCoord const & tile_offset) {
+    iterator_.store_with_pointer_offset(frag, {tile_offset.row(), tile_offset.column()});
+  }
+
+  /// Stores a fragment
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag) {
+    iterator_.store_with_pointer_offset(frag, 0);
   }
 
-  /// Advances to the next tile in memory.
+  /// Advances the pointer
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator &operator++() {
+  RegularTileIterator2dThreadTile &operator++() {
     ++iterator_;
     return *this;
   }
 
-  /// Advances to the next tile in memory.
+  /// Advances the pointer
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator operator++(int) {
-    RegularTileAccessIterator prev(*this);
-    ++iterator_;
+  RegularTileIterator2dThreadTile &operator--() {
+    --iterator_;
+    return *this;
+  }
 
-    return prev;
+  /// Adds a pointer offset in units of Element
+  CUTLASS_HOST_DEVICE
+  void add_pointer_offset(LongIndex pointer_offset) {
+    iterator_.add_pointer_offset(pointer_offset);
   }
+
+  /// Adds a tile offset
+  CUTLASS_DEVICE
+  void add_tile_offset(TensorCoord const &coord) {
+    iterator_.add_tile_offset({coord.row(), coord.column()});
+  }
+
 };
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-}  // namespace threadblock
-}  // namespace transform
-}  // namespace cutlass
+} // namespace threadblock
+} // namespace transform
+} // namespace cutlass
 
-////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear_direct_conv.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear_direct_conv.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op_sm80.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op_sm80.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/performance_result.cu`

 * *Files 13% similar despite different names*

```diff
@@ -24,39 +24,38 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/*! \file
-    \brief Templates implementing storing of tiles from pitch-linear rank=2 tensors. 
+/* \file
+   \brief
 */
 
 #pragma once
 
+#include <vector>
+
 #include "cutlass/cutlass.h"
-#include "cutlass/numeric_types.h"
 
-////////////////////////////////////////////////////////////////////////////////
+// CUTLASS Profiler includes
+#include "enumerated_types.h"
+#include "performance_result.h"
+
+// CUTLASS Library includes
+#include "cutlass/library/library.h"
+#include "cutlass/library/util.h"
 
 namespace cutlass {
-namespace transform {
-namespace threadblock {
+namespace profiler {
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-template <
-  typename Shape,
-  typename Element,
-  typename Layout,
-  int AdvanceRank,
-  typename ThreadMap,
-  int Alignment = sizeof_bits<Element>::value * ThreadMap::kElementsPerAccess / 8
->
-class RegularTileIterator;
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-} // namespace threadblock
-} // namespace transform
+} // namespace profiler
 } // namespace cutlass
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op_sm70.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op_sm70.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/vector_iterator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/vector_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/transform/warp/vector_fragment_iterator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/warp/vector_fragment_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/uint128.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/uint128.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/include/cutlass/wmma_array.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/wmma_array.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/compiler.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/compiler.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/cutlass_bindings.cpp` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/cutlass.cpp`

 * *Files 2% similar despite different names*

```diff
@@ -25,17 +25,16 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /* \file
-   \brief binding CUTLASS C++ APIs to Python
+   \brief binding cutlass C++ APIs to python
 */
-
 #include <pybind11/pybind11.h>
 #include <pybind11/stl_bind.h>
 
 #include "builtin_types.h"
 #include "device_launch_parameters.h"
 #include "stddef.h"
 #include "cutlass/cutlass.h"
@@ -58,18 +57,18 @@
 // compiler
 #include "compiler.h"
 
 
 namespace py = pybind11;
 
 
-PYBIND11_MODULE(cutlass_bindings, m) {
+PYBIND11_MODULE(cutlass, m) {
 
     // module doc
-    m.doc() = "CUTLASS C++ binding";
+    m.doc() = "cutlass C++ binding";
 
     //
     // Bind data type
     //
     bind_cutlass_types(m);
 
     //
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/arch.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/arch.h`

 * *Files 2% similar despite different names*

```diff
@@ -47,13 +47,13 @@
 
 void bind_opcode(py::module &m) {
     py::enum_<cutlass::OpcodeClass>(m, "OpClass",
         R"pbdoc(classification of math operators)pbdoc")
         .value("Simt", cutlass::OpcodeClass::kSimt, 
             R"pbdoc(Tag classifying math operators as thread-level operations)pbdoc")
         .value("TensorOp", cutlass::OpcodeClass::kTensorOp, 
-            R"pbdoc(Tag classifying operators as Tensor Core operations)pbdoc")
+            R"pbdoc(Tag classifing operators as Tensor Core operations)pbdoc")
         .value("WmmaTensorOp", cutlass::OpcodeClass::kWmmaTensorOp, 
-            R"pbdoc(Tag classifying operators as WMMA Tensor Core operations)pbdoc")
+            R"pbdoc(Tag classifing operators as WMMA Tensor Core operations)pbdoc")
         .value("SparseTensorOp", cutlass::OpcodeClass::kSparseTensorOp, 
-            R"pbdoc(Tag classifying operators as sparseTensor Core operations)pbdoc");
+            R"pbdoc(Tag classifing operators as sparseTensor Core operations)pbdoc");
 }
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/conv/conv_problem_size.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/conv_problem_size.h`

 * *Files 0% similar despite different names*

```diff
@@ -38,15 +38,15 @@
 #include "cutlass/conv/conv2d_problem_size.h"
 
 namespace py = pybind11;
 
 void bind_conv_problem_size(py::module &m) {
     //
     // Conv2d Problem Size: 
-    // include/cutlass/conv/conv2d_problem_size.h
+    // include/cutlass/conv/conv2d_problem_sizd.h
     //
     py::class_<cutlass::conv::Conv2dProblemSize>(m, "Conv2dProblemSize")
          // constructors
         .def(py::init<int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, cutlass::conv::Mode, int, int>())
         .def(py::init<cutlass::Tensor4DCoord, cutlass::Tensor4DCoord, cutlass::Tensor4DCoord, cutlass::MatrixCoord, cutlass::MatrixCoord, cutlass::conv::Mode, int, int>())
         // attribute accessors
         .def_readwrite("N", &cutlass::conv::Conv2dProblemSize::N)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/conv/convolution.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/convolution.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/conv/host.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/host.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_generic.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_generic.h`

 * *Files 0% similar despite different names*

```diff
@@ -30,15 +30,14 @@
  **************************************************************************************************/
 
 /*! \file
 
   \brief A generic wrapper around an epilogue visitor operation
 */
 
-
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/arch/memory.h"
 #include "cutlass/arch/memory_sm75.h"
 #include "cutlass/gemm/kernel/gemm_transpose_operands.h"
 #include "cutlass/gemm/kernel/default_gemm.h"
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/binary_ops.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/host.h`

 * *Files 16% similar despite different names*

```diff
@@ -10,75 +10,38 @@
  *
  * 2. Redistributions in binary form must reproduce the above copyright notice,
  * this list of conditions and the following disclaimer in the documentation
  * and/or other materials provided with the distribution.
  *
  * 3. Neither the name of the copyright holder nor the names of its
  * contributors may be used to endorse or promote products derived from
- * this layernormware without specific prior written permission.
+ * this software without specific prior written permission.
  *
  * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
  * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
  * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
  * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
-/*! \file
-  
-  \brief A file contains the binary ops
+/* \file
+   \brief Bind gemm host helpers to python
 */
-
 #pragma once
-#include "cutlass/cutlass.h"
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-namespace cutlass {
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-
-/// Scalar multiplication
-template <typename T, int N>
-struct VectorAdd {
-
-    struct Arguments {
-        int tmp;
-
-        CUTLASS_HOST_DEVICE
-        Arguments():tmp(0){ }
-
-        CUTLASS_HOST_DEVICE
-        Arguments(int tmp): tmp(tmp) { }
-    };
-    
-    struct Params {
-
-        CUTLASS_HOST_DEVICE
-        Params(Arguments const &args) { }
-    };
-
-    CUTLASS_HOST_DEVICE
-    VectorAdd(
-        Params const &params
-    ) { }
-
-    CUTLASS_HOST_DEVICE
-    Array<T, N> operator()(Array<T, N> const &lhs, Array<T, N> const &rhs) const {
-        cutlass::plus<Array<T, N>> add_op;
-        return add_op(lhs, rhs);
-    }
+#include <pybind11/pybind11.h>
+#include <pybind11/stl_bind.h>
 
-};
+#include "cutlass/util/host_reorder.h"
+#include "cutlass/layout/tensor.h"
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+namespace py = pybind11;
 
-} // namespace cutlass
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+void bind_gemm_host_helper(py::module &m) {
+    m.def("reorder_column", &cutlass::reorder_column<32, int8_t, cutlass::layout::RowMajorInterleaved<32>>);
+    m.def("reorder_column", &cutlass::reorder_column<32, int8_t, cutlass::layout::ColumnMajorInterleaved<32>>);
+}
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/unary_ops.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/unary_ops.h`

 * *Files 2% similar despite different names*

```diff
@@ -26,16 +26,16 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-  
-  \brief A file contains the unary ops
+
+  \brief Unary operations to be used within the epilogue visitor model.
 */
 
 #pragma once
 #include "cutlass/cutlass.h"
 #include "cutlass/epilogue/thread/activation.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_accumulator.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_accumulator.h`

 * *Files 2% similar despite different names*

```diff
@@ -26,16 +26,16 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-  
-  \brief A file contains the epilogue visitor Op with accumulator
+
+  \brief Epilogue visitor operation that simply returns the accumulator
 */
 
 #pragma once
 #include "cutlass/cutlass.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_binary.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_binary.h`

 * *Files 2% similar despite different names*

```diff
@@ -26,16 +26,16 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-  
-  \brief A file contains the epilogue visitor Op with Binary op
+
+  \brief Epilogue visitor operator performing a binary operation between two visitor nodes
 */
 
 #pragma once
 #include "cutlass/cutlass.h"
 #include "binary_ops.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
@@ -80,18 +80,19 @@
 
     /// Fragment type returned by this visitor
     using VisitAccessType = Array<ElementCompute, kElementsPerAccess>; 
 
     /// Fragment type of accumulator
     using AccumulatorAccessType = Array<ElementAccumulator, kElementsPerAccess>;
 
+    /// Combination Op TODO: generalize this
     using BinaryOp = BinaryOp_<ElementCompute, kElementsPerAccess>;
 
     static_assert(kElementsPerAccess==VisitAccessTypeA::kElements, "kElementsPerAccess mismatches with Visitor A");
-    static_assert(kElementsPerAccess==VisitAccessTypeB::kElements, "kElementsPerAccess mismatches with Visitor B");
+    static_assert(kElementsPerAccess==VisitAccessTypeB::kElements, "kElementsPerAccess misnatches with Visitor B");
 
     /// SMEM buffer class required in the epilogue visitor
     struct SharedStorage {
         typename VisitorA::SharedStorage storage_a;
         typename VisitorB::SharedStorage storage_b;
 
         CUTLASS_HOST_DEVICE
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_broadcast.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_broadcast.h`

 * *Files 1% similar despite different names*

```diff
@@ -26,16 +26,16 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-  
-  \brief A file contains the epilogue visitor Op with broadcasting vector to all columns
+
+  \brief Epilogue visitor operation that broadcasts a vector to all columns
 */
 
 #pragma once
 #include "cutlass/cutlass.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_reduction.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_reduction.h`

 * *Files 1% similar despite different names*

```diff
@@ -26,16 +26,16 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-  
-  \brief A file contains the epilogue visitor Op with reduction over columns in CTA
+
+  \brief Epilogue visitor operation that performs a column-wise reduction within a threadblock
 */
 
 #pragma once
 #include "cutlass/cutlass.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
@@ -51,27 +51,28 @@
 ///
 template <
     typename ThreadblockShape_,             /// Threadblock shape
     typename ElementAccumulator_,           ///< Data type of the Accumulator
     typename ElementReduction_,             ///< Data type of the output reduction in device memory
     typename ElementReductionAccumulator_ , ///< Data type to accumulate reduction in smem and register
     typename OutputTileIterator_,           ///< Tile Iterator type
-    typename Visitor_                       ///< preceding visitor op
+    typename Visitor_                       ///< preceeding visitor op
 >
 class VisitorOpColumnReduction {
 public:
     using ElementAccumulator = ElementAccumulator_;
     using ElementReductionAccumulator = ElementReductionAccumulator_;
     using ElementReduction = ElementReduction_;
     using OutputTileIterator = OutputTileIterator_;
     using ThreadblockShape = ThreadblockShape_;
     using Visitor = Visitor_;
 
     static int const kElementsPerAccess = OutputTileIterator::kElementsPerAccess;
 
+    // TODO: generalize the reduction op
     using ReductionOp = cutlass::plus<Array<ElementReductionAccumulator, kElementsPerAccess>>;
     using ReductionOpScalar = cutlass::plus<ElementReductionAccumulator>;
     using ElementOutput = typename OutputTileIterator::Element;
 
     
 
     /// Fragment type returned from Visitor
@@ -79,15 +80,15 @@
     using ElementVisitor = typename VisitAccessTypeVisitor::Element;
 
     using VisitAccessType = VisitAccessTypeVisitor;
 
     /// Fragment type of accumulator
     using AccumulatorAccessType = Array<ElementAccumulator, kElementsPerAccess>;
 
-    /// Fragment type of reduction
+    /// Fragment type of redcution
     using ReductionAccumulatorAccessType = Array<ElementReductionAccumulator, kElementsPerAccess>;
 
     /// Thread map used by output tile iterators
     using ThreadMap = typename OutputTileIterator::ThreadMap;
     /// Used for the reduction
     struct ReductionDetail {
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_linear_combination.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_linear_combination.h`

 * *Files 2% similar despite different names*

```diff
@@ -26,16 +26,16 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-  
-  \brief A file contains the epilogue visitor Op with Linear Combination
+
+  \brief Epilogue visitor operation that performs a linear combination of two visitor nodes
 */
 
 #pragma once
 #include "cutlass/cutlass.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
@@ -78,19 +78,19 @@
 
     /// Fragment type returned by this visitor
     using VisitAccessType = Array<ElementCompute, kElementsPerAccess>; 
 
     /// Fragment type of accumulator
     using AccumulatorAccessType = Array<ElementAccumulator, kElementsPerAccess>;
 
-    /// Combination Op
+    /// Combination Op TODO: generalize this
     using CombinationOp = cutlass::plus<VisitAccessType>;
 
     static_assert(kElementsPerAccess==VisitAccessTypeA::kElements, "kElementsPerAccess mismatches with Visitor A");
-    static_assert(kElementsPerAccess==VisitAccessTypeB::kElements, "kElementsPerAccess mismatches with Visitor B");
+    static_assert(kElementsPerAccess==VisitAccessTypeB::kElements, "kElementsPerAccess misnatches with Visitor B");
 
     /// SMEM buffer class required in the epilogue visitor
     struct SharedStorage {
         typename VisitorA::SharedStorage storage_a;
         typename VisitorB::SharedStorage storage_b;
 
         CUTLASS_HOST_DEVICE
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_broadcast.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_broadcast.h`

 * *Files 2% similar despite different names*

```diff
@@ -26,16 +26,16 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-  
-  \brief A file contains the epilogue visitor Op with broadcasting vector to all rows
+
+  \brief Epilogue visitor operation that broadcasts a vector to all rows
 */
 
 #pragma once
 #include "cutlass/cutlass.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_reduction.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_reduction.h`

 * *Files 2% similar despite different names*

```diff
@@ -26,16 +26,16 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-  
-  \brief A file contains the epilogue visitor Op with reduction over rows in CTA
+
+  \brief Epilogue visitor operation that performs a column-wise reduction within a threadblock
 */
 
 #pragma once
 #include "cutlass/cutlass.h"
 #include "stdio.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
@@ -52,41 +52,42 @@
 ///
 template <
     typename ThreadblockShape_,             /// Threadblock shape
     typename ElementAccumulator_,           ///< Data type of the Accumulator
     typename ElementReduction_,             ///< Data type of the output reduction in device memory
     typename ElementReductionAccumulator_ , ///< Data type to accumulate reduction in smem and register
     typename OutputTileIterator_,           ///< Tile Iterator type
-    typename Visitor_                       ///< preceding visitor op
+    typename Visitor_                       ///< preceeding visitor op
 >
 class VisitorOpRowReduction {
 public:
     using ElementAccumulator = ElementAccumulator_;
     using ElementReductionAccumulator = ElementReductionAccumulator_;
     using ElementReduction = ElementReduction_;
     using OutputTileIterator = OutputTileIterator_;
     using ThreadblockShape = ThreadblockShape_;
     using Visitor = Visitor_;
 
     static int const kElementsPerAccess = OutputTileIterator::kElementsPerAccess;
 
+    // TODO: generalize the reduction op
     using ReductionOp = cutlass::plus<Array<ElementReductionAccumulator, kElementsPerAccess>>;
     using ReductionOpScalar = cutlass::plus<ElementReductionAccumulator>;
     using ElementOutput = typename OutputTileIterator::Element;
 
     /// Fragment type returned from Visitor
     using VisitAccessTypeVisitor = typename Visitor::VisitAccessType;
     using ElementVisitor = typename VisitAccessTypeVisitor::Element;
 
     using VisitAccessType = VisitAccessTypeVisitor;
 
     /// Fragment type of accumulator
     using AccumulatorAccessType = Array<ElementAccumulator, kElementsPerAccess>;
 
-    /// Fragment type of reduction
+    /// Fragment type of redcution
     using ReductionAccumulatorAccessType = Array<ElementReductionAccumulator, kElementsPerAccess>;
 
     /// Thread map used by output tile iterators
     using ThreadMap = typename OutputTileIterator::ThreadMap;
     /// Used for the reduction
     struct ReductionDetail {
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_input.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_input.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_output.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_output.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_with_layernorm.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_with_layernorm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/gemm/gemm.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/gemm.h`

 * *Files 0% similar despite different names*

```diff
@@ -48,15 +48,15 @@
     py::enum_<cutlass::gemm::GemmUniversalMode>(m, "Mode")
         .value("Gemm", cutlass::gemm::GemmUniversalMode::kGemm, "Ordinary GEMM & GEMM Split-K serial")
         .value("GemmSplitKParallel", cutlass::gemm::GemmUniversalMode::kGemmSplitKParallel, "GEMM Split-K parallel")
         .value("Batched", cutlass::gemm::GemmUniversalMode::kBatched, "Batched GEMM")
         .value("Array", cutlass::gemm::GemmUniversalMode::kArray)
         .value("Invalid", cutlass::gemm::GemmUniversalMode::kInvalid);
     
-    /// GemmCoord is a structure that specifies a location within the coordinate space of a GEMM problem
+    /// GemmCoord is a structure that specifies a location within the coordiate space of a GEMM problem
     py::class_<cutlass::gemm::GemmCoord>(m, "GemmCoord")
         .def(py::init<int, int, int>())
         .def("m", py::overload_cast<>(&cutlass::gemm::GemmCoord::m))
         .def("n", py::overload_cast<>(&cutlass::gemm::GemmCoord::n))
         .def("k", py::overload_cast<>(&cutlass::gemm::GemmCoord::k))
         // get tensor coords
         .def("mk",
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/gemm/host.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/layout.h`

 * *Files 8% similar despite different names*

```diff
@@ -25,23 +25,23 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /* \file
-   \brief Bind gemm host helpers to python
+   \brief Bind CUTLASS layouts to python
 */
 #pragma once
 #include <pybind11/pybind11.h>
 #include <pybind11/stl_bind.h>
 
-#include "cutlass/util/host_reorder.h"
-#include "cutlass/layout/tensor.h"
+#include "tensor.h"
+#include "matrix.h"
 
-namespace py = pybind11;
 
+namespace py = pybind11;
 
-void bind_gemm_host_helper(py::module &m) {
-    m.def("reorder_column", &cutlass::reorder_column<32, int8_t, cutlass::layout::RowMajorInterleaved<32>>);
-    m.def("reorder_column", &cutlass::reorder_column<32, int8_t, cutlass::layout::ColumnMajorInterleaved<32>>);
+void bind_layout(py::module &m) {
+    bind_tensor_layout(m);
+    bind_matrix_layout(m);
 }
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/layout/layout.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/gemm.h`

 * *Files 9% similar despite different names*

```diff
@@ -25,23 +25,21 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /* \file
-   \brief Bind CUTLASS layouts to python
+   \brief Bind gemm test to python
 */
 #pragma once
 #include <pybind11/pybind11.h>
 #include <pybind11/stl_bind.h>
 
-#include "tensor.h"
-#include "matrix.h"
-
+#include "host.h"
 
 namespace py = pybind11;
 
-void bind_layout(py::module &m) {
-    bind_tensor_layout(m);
-    bind_matrix_layout(m);
+void bind_gemm_test(py::module &m) {
+    py::module_ host_submodule = m.def_submodule("host");
+    bind_gemm_host_reference(host_submodule);
 }
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/layout/matrix.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/matrix.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/layout/tensor.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/tensor.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/swizzling.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/swizzling.h`

 * *Files 4% similar despite different names*

```diff
@@ -57,95 +57,87 @@
         R"pbdoc(Threadblock swizzling function for GEMMs)pbdoc")
         .def(py::init<>())
         .def("get_tiled_shape",
             py::overload_cast<cutlass::gemm::GemmCoord, cutlass::gemm::GemmCoord, int>(
                 &T::get_tiled_shape, py::const_
             ), py::arg("problem_size"), py::arg("tile_size"), py::arg("split_k_slices"),
             R"pbdoc(Returns the shape of the problem in units of logical tiles
-
+            
             :param problem_size: gemm(M, N, K)
             :type problem_size: :class:`cutlass.gemm.GemmCoord`
             )pbdoc")
         .def("get_tiled_shape",
             py::overload_cast<cutlass::conv::Operator, const cutlass::conv::Conv2dProblemSize&, cutlass::gemm::GemmCoord, int>(
                 &T::get_tiled_shape, py::const_
             ), py::arg("conv_operator"), py::arg("problem_size"), py::arg("tile_size"), py::arg("split_k_slices"),
             R"pbdoc(Returns the shape of the problem in units of logical tiles
-
+            
             :param problem_size: Implicit gemm problem size conv_operator(NPQK, NHWC, KRSC)
             :type problem_size: :class:`cutlass.gemm.GemmCoord`)
             )pbdoc")
         .def("get_tiled_shape",
             py::overload_cast<cutlass::conv::Operator, const cutlass::conv::Conv3dProblemSize&, cutlass::gemm::GemmCoord, int>(
                 &T::get_tiled_shape, py::const_
             ), py::arg("conv_operator"), py::arg("problem_size"), py::arg("tile_size"), py::arg("split_k_slices"),
             R"pbdoc(Returns the shape of the problem in units of logical tiles
-
+            
             :param problem_size: Implicit gemm problem size conv_operator(NZPQK, NDHWC, KTRSC)
             :type problem_size: :class:`cutlass.gemm.GemmCoord`)
             )pbdoc")
+        // TODO: the returned dim3 is not usable in python
         .def("get_grid_shape", &T::get_grid_shape,
             py::arg("tiled_shape"), 
             R"pbdoc(Computes CUDA grid dimensions given a size in units of logical tiles)pbdoc")
         .def("tag", [](const T & swizzle){
             return demangle(typeid(T).name());
-        }, R"pbdoc(Returns the c++ name of the swizzling for code emission)pbdoc");
+        }, R"pbdoc(Returns the c++ name of the swizzling for code emittion)pbdoc");
 }
 
 template<typename T>
 void bind_swizzle(py::module & m, std::string name, std::string doc) {
     py::class_<T>(m, name.c_str(), doc.c_str())
         .def(py::init<>())
         .def("get_tiled_shape",
             py::overload_cast<cutlass::gemm::GemmCoord, cutlass::gemm::GemmCoord, int>(
                 &T::get_tiled_shape, py::const_
             ), py::arg("problem_size"), py::arg("tile_size"), py::arg("split_k_slices"),
             R"pbdoc(Returns the shape of the problem in units of logical tiles
-
+            
             :param problem_size: gemm(M, N, K)
             :type problem_size: :class:`cutlass.gemm.GemmCoord`
             )pbdoc")
         .def("get_grid_shape", &T::get_grid_shape,
-            py::arg("tiled_shape"),
+            py::arg("tiled_shape"), 
             R"pbdoc(Computes CUDA grid dimensions given a size in units of logical tiles)pbdoc")
         .def("tag", [](const T & swizzle){
             return demangle(typeid(T).name());
-        }, R"pbdoc(Returns the c++ name of the swizzling for code emission)pbdoc");
-}
-
-template<typename T>
-void bind_swizzle_streamk(py::module & m, std::string name, std::string doc) {
-    py::class_<T>(m, name.c_str(), doc.c_str())
-        .def(py::init<>())
-        .def("tag", [](const T & swizzle){
-            return demangle(typeid(T).name());
-        }, R"pbdoc(Returns the c++ name of the swizzling for code emission)pbdoc");
+        }, R"pbdoc(Returns the c++ name of the swizzling for code emittion)pbdoc");
 }
 
 template<typename T>
 void bind_dgrad_swizzle(py::module & m, std::string name) {
     py::class_<T>(m, name.c_str(),
         R"pbdoc(Threadblock swizzling function for strided dgrad convolution)pbdoc")
         .def(py::init<>())
         .def("get_tiled_shape",
             py::overload_cast<cutlass::conv::Operator, const cutlass::conv::Conv2dProblemSize&, cutlass::gemm::GemmCoord, int>(
                 &T::get_tiled_shape, py::const_
             ), py::arg("conv_operator"), py::arg("problem_size"), py::arg("tile_size"), py::arg("split_k_slices"),
             R"pbdoc(Returns the shape of the problem in units of logical tiles
-
+            
             :param problem_size: Implicit gemm problem size conv_operator(NPQK, NHWC, KRSC)
             :type problem_size: :class:`cutlass.gemm.GemmCoord`)
             )pbdoc")
         .def("get_grid_shape", [](const T & swizzle, cutlass::gemm::GemmCoord tiled_shape) {
             return dim3(tiled_shape.m(), tiled_shape.n(), tiled_shape.k());
-        }, py::arg("tiled_shape"),
+        }, py::arg("tiled_shape"), 
             R"pbdoc(Computes CUDA grid dimensions given a size in units of logical tiles)pbdoc")
         .def("tag", [](const T & swizzle){
             return demangle(typeid(T).name());
-        }, R"pbdoc(Returns the c++ name of the swizzling for code emission)pbdoc");
+        }, R"pbdoc(Returns the c++ name of the swizzling for code emittion)pbdoc");
 }
 
 void bind_threadblock_swizzle(py::module &m) {
 
     py::class_<dim3>(m, "dim3",
         R"pbdoc(A int3 type xyz contains three integers)pbdoc")
         .def(py::init<int, int, int>(),
@@ -158,13 +150,11 @@
     bind_identity_swizzle<cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<2>>(m, "IdentitySwizzle2");
     bind_identity_swizzle<cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<4>>(m, "IdentitySwizzle4");
     bind_identity_swizzle<cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<8>>(m, "IdentitySwizzle8");
 
     bind_swizzle<cutlass::gemm::threadblock::GemmHorizontalThreadblockSwizzle>(m, "HorizontalSwizzle",  R"pbdoc(Threadblock swizzling function for GEMMs)pbdoc");
     bind_swizzle<cutlass::gemm::threadblock::GemmBatchedIdentityThreadblockSwizzle>(m, "BatchedIdentitySwizzle",  R"pbdoc(Threadblock swizzling function for batched GEMMs)pbdoc");
 
-    bind_swizzle_streamk<cutlass::gemm::threadblock::ThreadblockSwizzleStreamK>(m, "ThreadblockSwizzleStreamK", R"pbdoc(Threadblock swizzling function using Stream K feature)pbdoc");
-
     bind_dgrad_swizzle<cutlass::conv::threadblock::StridedDgradIdentityThreadblockSwizzle<1>>(m, "StridedDgradIdentitySwizzle1");
     bind_dgrad_swizzle<cutlass::conv::threadblock::StridedDgradIdentityThreadblockSwizzle<4>>(m, "StridedDgradIdentitySwizzle4");
     bind_dgrad_swizzle<cutlass::conv::threadblock::StridedDgradHorizontalThreadblockSwizzle>(m, "StridedDgradHorizontalSwizzle");
 }
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/tensor_coord.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_coord.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/tensor_ref_view.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_ref_view.h`

 * *Files 0% similar despite different names*

```diff
@@ -16,15 +16,15 @@
  * contributors may be used to endorse or promote products derived from
  * this software without specific prior written permission.
  *
  * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
  * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
  * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
- * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSE<cutlass::TensorRef<QUENTIAL
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/include/types.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/types.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/library.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/library.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/test/conv/conv_problems.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/conv_problems.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/test/conv/convolution.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/convolution.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/test/conv/host.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/host.h`

 * *Files 1% similar despite different names*

```diff
@@ -52,15 +52,15 @@
     m.def("CreateCachedConv2dTestKey", &test::conv::device::CreateCachedConv2dTestKey<Ta, La, Tb, Lb, Tc, Lc, Tacc, Te>);
 }
 
 template<typename Ta, typename La, typename Tb, typename Lb, typename Tc, typename Lc, typename Tacc, typename Te>
 void bind_conv2d_host_sat(py::module &m) {
     m.def("conv2d", \
         &cutlass::reference::host::Conv2d< \
-            Ta, La, Tb, Lb, Tc, Lc, Te, Tacc>);
+            Ta, La, Tb, Lb, Tc, Lc, Te, Tacc, cutlass::NumericConverterClamp<Tc, Te>>);
     
     m.def("CreateCachedConv2dTestKey", &test::conv::device::CreateCachedConv2dTestKey<Ta, La, Tb, Lb, Tc, Lc, Tacc, Te>);
 }
 
 template<typename Ta, typename Tb, typename Tc, typename Tacc, typename Te>
 void bind_conv2d_host_nhwc(py::module &m) {
     bind_conv2d_host<
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/test/gemm/gemm.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/gpu_timer.h`

 * *Files 24% similar despite different names*

```diff
@@ -25,21 +25,48 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /* \file
-   \brief Bind gemm test to python
+   \brief Defines a math function
 */
+
 #pragma once
-#include <pybind11/pybind11.h>
-#include <pybind11/stl_bind.h>
 
-#include "host.h"
+#include <cuda_runtime.h>
+#include "cutlass/cutlass.h"
+
+namespace cutlass {
+namespace profiler {
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+struct GpuTimer {
+
+  cudaEvent_t events[2];
+
+  //
+  // Methods
+  //
+  
+  GpuTimer();
+  ~GpuTimer();
+
+  /// Records a start event in the stream
+  void start(cudaStream_t stream = nullptr);
+
+  /// Records a stop event in the stream
+  void stop(cudaStream_t stream = nullptr);
+
+  /// Records a stop event in the stream and synchronizes on the stream
+  void stop_and_wait(cudaStream_t stream = nullptr);
+
+  /// Returns the duration in miliseconds
+  double duration(int iterations = 1) const;
+};
 
-namespace py = pybind11;
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-void bind_gemm_test(py::module &m) {
-    py::module_ host_submodule = m.def_submodule("host");
-    bind_gemm_host_reference(host_submodule);
-}
+} // namespace profiler
+} // namespace cutlass
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/python/cutlass/cpp/test/gemm/host.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/host.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/common/cutlass_unit_test.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/common/cutlass_unit_test.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/common/filter_architecture.cpp` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/common/filter_architecture.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/cache_testbed_output.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/cache_testbed_output.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_few_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_few_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_fixed_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_fixed_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_qf32nhwc_qf32nhwc_qf32nhwc_simt_f32_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_qf32nhwc_qf32nhwc_qf32nhwc_simt_f32_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_splitk.h`

 * *Files 25% similar despite different names*

```diff
@@ -25,103 +25,194 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide Implicit GEMM interface
+    \brief Tests for device-wide GEMM interface
 */
 
+#pragma once
+
+#include <iostream>
+#include <sstream>
+
 #include "../../common/cutlass_unit_test.h"
-#include "cutlass/cutlass.h"
-#include "cutlass/array.h"
-#include "cutlass/epilogue/thread/linear_combination_bias_elementwise.h"
-#include "cutlass/epilogue/thread/linear_combination_residual_block.h"
-#include "cutlass/epilogue/thread/activation.h"
-
-#include "cutlass/conv/kernel/default_conv2d_fprop_with_broadcast.h"
-#include "cutlass/conv/device/implicit_gemm_convolution.h"
-
-#include "conv2d_with_broadcast_testbed.h"
-
-#if defined(CUTLASS_ARCH_MMA_SM70_SUPPORTED)
-
-// Test residual block fusion: UnaryOp(BinaryOp(ActivationOp(Conv2d(X) + bias), residual))
-// LinearCombinationResidualBlock does not support the split-k mode unless ActivationOp is Identity.
-// This is because the activation needs to be applied to the fully accumulated output of the Conv2d op,
-// which only the last thread block would have an access to, before applying BinaryOp.
-// The epilogue functor in the last thread block would have to be given three inputs, namely
-// partial outputs, bias, and residual, but this is not supported in the current interface.
-// Set TestSplitK = false to skip split-k tests with non-trivial ActivationOp.
-template <
- typename ElementAccumulator,
- template<typename T> class ActivationOp,
- template<typename T> class BinaryOp,
- template<typename T> class UnaryOp,
- bool TestSplitK = false 
->
-void TestResidaulBlock() {
-  using ElementA = cutlass::half_t;
-  using ElementB = cutlass::half_t;
-  using ElementC = cutlass::half_t;
-  using ElementD = ElementC;
-  using ElementCompute = ElementAccumulator;
-
-  using EpilogueOutputOp = cutlass::epilogue::thread::LinearCombinationResidualBlock<
-    ElementD,
-    ElementAccumulator,
-    ElementCompute,
-    ElementC,
-    8,
-    ActivationOp,
-    BinaryOp,
-    UnaryOp
-  >;
-
-  using Conv2dFpropKernel = typename cutlass::conv::kernel::DefaultConv2dFpropWithBroadcast<
-    ElementA, cutlass::layout::TensorNHWC,
-    ElementB, cutlass::layout::TensorNHWC,
-    ElementC, cutlass::layout::TensorNHWC,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm70,
-    cutlass::gemm::GemmShape<128, 128, 32>,
-    cutlass::gemm::GemmShape<64, 64, 32>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
-    EpilogueOutputOp,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    2,
-    cutlass::arch::OpMultiplyAdd,
-    cutlass::conv::IteratorAlgorithm::kOptimized
-  >::Kernel;
-
-  using Conv2dFprop = cutlass::conv::device::ImplicitGemmConvolution<Conv2dFpropKernel>;
-
-  struct ReferenceOp {
-    using OutputOp = typename Conv2dFprop::EpilogueOutputOp;
-    using ElementZ = typename OutputOp::ElementZ;
-
-    ActivationOp<ElementCompute> activation;
-    BinaryOp<ElementCompute> binary_op;
-    UnaryOp<ElementCompute> unary_op;
 
-    void operator()(ElementZ &Z, ElementZ&, ElementCompute conv2d, ElementCompute residual) {
-      Z = ElementZ(unary_op(binary_op(activation(conv2d), residual)));
+#include "testbed.h"
+
+namespace test {
+namespace gemm {
+namespace device {
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+template <typename Gemm>
+struct TestbedSplitK : public Testbed<Gemm> {
+
+  using Base = Testbed<Gemm>;
+
+  using ElementCompute = typename Base::ElementCompute;
+
+  //
+  // Methods
+  //
+
+  TestbedSplitK(
+    cutlass::Distribution::Kind init_A_ = cutlass::Distribution::Uniform,
+    cutlass::Distribution::Kind init_B_ = cutlass::Distribution::Uniform,
+    cutlass::Distribution::Kind init_C_ = cutlass::Distribution::Uniform,
+    uint64_t seed_ = 2080
+  ):
+    Base(init_A_, init_B_, init_C_, seed_) { }
+
+  /// Returns true if the CUDA device is sufficient to execute the kernel.
+  bool sufficient() const {
+    //
+    // Determine SMEM requirements and waive if not satisfied
+    //
+
+    int smem_size = int(sizeof(typename Gemm::GemmKernel::SharedStorage));
+
+    cudaDeviceProp properties;
+    int device_idx;
+    cudaError_t result = cudaGetDevice(&device_idx);
+
+    if (result != cudaSuccess) {
+      throw std::runtime_error("cudaGetDevice() API call failed.");
+    }
+
+    result = cudaGetDeviceProperties(&properties, device_idx);
+
+    if (result != cudaSuccess) {
+      throw std::runtime_error("cudaGetDeviceProperties() failed");
+    }
+
+    if (properties.sharedMemPerBlockOptin < smem_size) {
+      return false;
     }
+
+    return true;
+  }
+  
+  /// Executes one test
+  bool run(
+    cutlass::gemm::GemmCoord problem_size, 
+    int split_k_slices,
+    ElementCompute alpha = ElementCompute(1), 
+    ElementCompute beta = ElementCompute(0)) {
+
+    // Waive test if insufficient CUDA device
+    if (!sufficient()) {
+      if (CUTLASS_TEST_UNIT_ENABLE_WARNINGS) {
+        std::cerr << "Test waived due to insufficient CUDA device." << std::endl;
+      }
+      return true;
+    }
+
+    this->initialize(problem_size);
+
+    //
+    // Initialize the GEMM operator
+    //
+
+    typename Gemm::Arguments arguments{
+      problem_size,
+      this->tensor_A.device_ref(),
+      this->tensor_B.device_ref(),
+      this->tensor_C.device_ref(),
+      this->tensor_D.device_ref(),
+      {alpha, beta},
+      split_k_slices
+    };
+
+    Gemm gemm_op;
+
+    size_t workspace_size = Gemm::get_workspace_size(arguments);
+
+    cutlass::device_memory::allocation<uint8_t> workspace(workspace_size);
+
+    cutlass::Status status = gemm_op.initialize(arguments, workspace.get());
+
+    EXPECT_TRUE(status == cutlass::Status::kSuccess);
+
+    //
+    // Run the GEMM
+    //
+
+    status = gemm_op();
+
+    EXPECT_TRUE(status == cutlass::Status::kSuccess);
+
+    //
+    // Verify
+    //
+
+    return this->verify(problem_size, alpha, beta);
+  }
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+template <typename Gemm>
+bool TestAllGemmSplitK() {
+  bool passed = true;
+
+  cutlass::gemm::GemmCoord problem_sizes[] = {
+    {8, 8, 2048},
+    {8, 8, 2056},
+    {264, 72, 520},
+    {264, 520,  120},
+    {264, 520,  264}
   };
 
-  bool passed = test::conv::device::TestAllConv2dWithBroadcast<Conv2dFprop, ReferenceOp, true, TestSplitK>();
+  int split_k_slices[] = {
+    1, 2, 4, 5, 7
+  };
+
+  double problem_alpha[] = {
+    0.5
+  };
+
+  double problem_beta[] = {
+    2.0
+  };
+
+  using Testbed = TestbedSplitK<Gemm>;
+  using ElementCompute = typename Testbed::ElementCompute;
+
+  Testbed testbed;
+
+  for (auto problem_size : problem_sizes) {
+    for (int split_k_count : split_k_slices) {
+      for (double alpha : problem_alpha) {
+        for (double beta : problem_beta) {
+
+          passed = testbed.run(
+            problem_size, 
+            split_k_count,
+            ElementCompute(alpha), 
+            ElementCompute(beta)
+          );
+
+          if (!passed) {
+            std::cout << "Failed on size " << problem_size << " with split_k_count " << split_k_count << std::endl;
+            return false;
+          }
+        }
+      }
+    }
+  }
+
   EXPECT_TRUE(passed);
-}
 
-TEST(SM70_Device_Conv2d_Fprop_With_Residual_Block_Plus_Optimized_ImplicitGemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32,
-     128x128_32x2_64x64x32) {
-  // Resnet
-  TestResidaulBlock<float, cutlass::epilogue::thread::ReLu, cutlass::plus, cutlass::epilogue::thread::Identity>();
+  return passed;
 }
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif  // CUTLASS_ARCH_MMA_SM70_SUPPORTED
+} // namespace device
+} // namespace gemm
+} // namespace test
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_reduction_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_reduction_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_problems.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_problems.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_testbed.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_testbed.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_testbed_interleaved.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_testbed_interleaved.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_with_broadcast_testbed.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_with_broadcast_testbed.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_with_reduction_testbed.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_with_reduction_testbed.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_problems.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_problems.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_testbed.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_testbed.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_direct_conv_testbed.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_direct_conv_testbed.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_fixed_stride_dilation_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_fixed_stride_dilation_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/conv/device/group_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/group_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/array.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/array.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/bfloat16.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/bfloat16.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/complex.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/complex.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/float8.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/float8.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/functional.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/functional.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/half.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/half.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/matrix.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/matrix.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/matrix_coord.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/matrix_coord.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/numeric_conversion.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/numeric_conversion.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/predicate_vector.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/predicate_vector.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/quaternion.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/quaternion.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/tensor_ref.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/tensor_ref.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/tensor_view.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/tensor_view.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/test_unit_core.cpp` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/test_unit_core.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/core/tfloat32.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/tfloat32.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/core/bitfield.cpp` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/exceptions.h`

 * *Files 26% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-/***************************************************************************************************
+/******************************************************************************
  * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
@@ -23,62 +23,47 @@
  * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
- **************************************************************************************************/
+ ******************************************************************************/
 
-#include "cutlass_unit_test.h"
+#pragma once
 
-#include <iostream>
-#include <iomanip>
-#include <utility>
-#include <type_traits>
-#include <vector>
-#include <numeric>
-
-#include <cute/tensor.hpp>
-#include <cute/container/bit_field.hpp>
-
-using namespace cute;
-
-TEST(CuTe_core, Bitfield)
-{
-  for_each(make_int_range<1,65>{}, [&](auto NumBits) {
-    for_each(make_int_range<0,129>{}, [&](auto BitStart) {
-
-      using BF = bit_field<decltype(BitStart)::value, decltype(NumBits)::value>;
-
-#if 0
-      printf("bit_field<%d,%d>:\n", decltype(BitStart)::value, decltype(NumBits)::value);
-      printf("  value_type_bits  : %d\n", BF::value_type_bits);
-      printf("  storage_type_bits: %d\n", BF::storage_type_bits);
-      printf("  N                : %d\n", BF::N);
-      printf("  idx              : %d\n", BF::idx);
-      printf("  bit_lo           : %d\n", BF::bit_lo);
-      printf("  bit_hi           : %d\n", BF::bit_hi);
-      printf("  mask             : 0x%lx\n", uint64_t(BF::mask));
-      printf("  mask_lo          : 0x%lx\n", uint64_t(BF::mask_lo));
-      printf("  mask_hi          : 0x%lx\n", uint64_t(BF::mask_hi));
-#endif
-
-      // Test
-      uint64_t v = decltype(NumBits)::value == 64 ? uint64_t(-1) : ((uint64_t(1) << NumBits) - 1);
-
-      BF bf{};
-      bf = v;
-      EXPECT_EQ(v, uint64_t(bf));
-    });
-  });
-
-  for_each(make_int_range<0,129>{}, [&](auto BitStart) {
-
-    using BF = bit_field<decltype(BitStart)::value, 32, float>;
-
-    BF bf{};
-    bf = 3.14f;
-    EXPECT_EQ(3.14f, float(bf));
-  });
+/**
+ * \file
+ * \brief C++ exception semantics for CUDA error codes
+ */
 
+#include <cuda_runtime.h>
+#include <iosfwd>
+#include <stdexcept>
+
+#include "cutlass/platform/platform.h"
+
+namespace cutlass {
+
+/// C++ exception wrapper for CUDA \p cudaError_t
+class cuda_exception : public std::exception {
+ public:
+  /// Constructor
+  cuda_exception(const char* msg = "", cudaError_t err = cudaErrorUnknown) : msg(msg), err(err) {}
+
+  /// Returns the underlying CUDA \p cudaError_t
+  cudaError_t cudaError() const { return err; }
+
+ protected:
+  /// Explanatory string
+  const char* msg;
+
+  /// Underlying CUDA \p cudaError_t
+  cudaError_t err;
+};
+
+/// Writes a cuda_exception instance to an output stream
+inline std::ostream& operator<<(std::ostream& out, cuda_exception const& e) {
+  return out << e.what() << ": " << cudaGetErrorString(e.cudaError());
 }
+
+}  // namespace cutlass
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/core/transform.cpp` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/test_unit.cpp`

 * *Files 12% similar despite different names*

```diff
@@ -24,26 +24,18 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
+/** \file
+    \brief Unit tests for CUTLASS core
+*/
 
-#include "cutlass_unit_test.h"
+#include "common/cutlass_unit_test.h"
 
-#include <cutlass/trace.h>
-#include <cute/tensor.hpp>
-#include <cute/numeric/complex.hpp>
-
-TEST(CuTe_core, Transform) {
-  using namespace cute;
-  complex<float> array[4] = {{0,0}, {1,0}, {0,1}, {1,1}};
-  complex<float> correct[4] = {{0,0}, {1,0}, {0,-1}, {1,-1}};
-  auto tensor = make_tensor(static_cast<complex<float>*>(array), make_layout(make_shape(4)));
-  conjugate conj;
-  transform(tensor, conj);
-  for (int i = 0; i < 4; ++i)
-  {
-    EXPECT_EQ(tensor(i), correct[i]);
-  }
+int main(int argc, char* arg[]) {
+  FilterArchitecture();
+  ::testing::InitGoogleTest(&argc, arg);
+  return RUN_ALL_TESTS();
 }
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/cute/layout/layout_operator.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf64_cf64_tensor_op_f64_sm90.cu`

 * *Files 24% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2023 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,113 +24,70 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
 /*! \file
-    \brief Unit tests Generic CuTe Layouts
+    \brief Tests for device-wide HERK interface
 */
 
-#include "../../common/cutlass_unit_test.h"
-#include "cutlass/cutlass.h"
-#include "cutlass/layout/layout.h"
-#include "cutlass/matrix_coord.h"
-
-// Cute includes
-#include <cute/layout.hpp>
-#include <cute/int_tuple.hpp>
-
-using namespace cutlass;
-using namespace cute;
-
-namespace test {
-namespace layout {
-
-template <typename GenericLayout, typename Layout> 
-  struct Testbed {
-
-
-    Testbed() {}
-
-    bool run() {
-      GenericLayout generic_layout;
-      Layout layout = Layout::packed({size<0>(generic_layout), size<1>(generic_layout)});
-
-      for (int m = 0; m < size<0>(generic_layout); m++) {
-        for (int n = 0; n < size<1>(generic_layout); n++) {
-          if (generic_layout(m, n) != layout({m, n})) return false;
-        }
-      }
-
-      return true;
-    }
-  };
-
-}
-}
-
-//////////////////////////////////////////////////////////////////////////
-//                      Test Generic CuTe Layouts
-//////////////////////////////////////////////////////////////////////////
-
-/// Canonical Layouts
-
-TEST(GenericLayout, ColumnMajor) {
-  using GenericLayout = cute::Layout<Shape<_8, _4>, Stride<_1, _8>>;
-  using Layout = cutlass::layout::ColumnMajor;
-
-  test::layout::Testbed<GenericLayout, Layout> testbed;
+#include <iostream>
 
-  EXPECT_TRUE(testbed.run());
-}
-//////////////////////////////////////////////////////////////////////////
-
-TEST(GenericLayout, RowMajor) {
-  using GenericLayout = cute::Layout<Shape<_8, _4>, Stride<_4, _1>>;
-  using Layout = cutlass::layout::RowMajor;
-
-  test::layout::Testbed<GenericLayout, Layout> testbed;
-
-  EXPECT_TRUE(testbed.run());
-}
-//////////////////////////////////////////////////////////////////////////
-
-
-/// Swizzle Shared Memory layouts
-
-TEST(GenericLayout, RowMajorTensorOpMultiplicandCrosswise) {
-
-  using GenericLayout = decltype(
-        composition(
-          Swizzle<3,3,3>{},
-          Layout<Shape<_128, _64>, Stride<_64, _1>>{})
-  );
-
-  using Layout = cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<
-      cutlass::sizeof_bits<cutlass::half_t>::value, 64>;
-
-  test::layout::Testbed<GenericLayout, Layout> testbed;
+#include "../../common/cutlass_unit_test.h"
+#include "cutlass/blas3.h"
+#include "cutlass/gemm/device/rank_k.h"
+#include "cutlass/util/host_tensor.h"
+#include "cutlass/util/reference/host/rank_k_complex.h"
+#include "cutlass/util/reference/host/tensor_compare.h"
+#include "cutlass/util/reference/host/tensor_copy.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/tensor_view_io.h"
+
+#include "testbed_rank_k_universal.h"
+
+#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+// HERK operator on CUBLAS_OP_C (row-major + conj) input layouts
+TEST(SM90_Device_Herk_cf64h_cf64n_l_tensor_op_f64, 64x64x16_32x32x16) {
+
+  using ElementA = cutlass::complex<double>;
+  using LayoutA = cutlass::layout::RowMajor;
+
+  using ElementC = cutlass::complex<double>;
+  using LayoutC = cutlass::layout::ColumnMajor;
+  using ElementAccumulator = cutlass::complex<double>;
+
+  using RankK = cutlass::gemm::device::RankK<
+    ElementA,
+    LayoutA,
+    ElementC,
+    LayoutC,
+    cutlass::FillMode::kLower,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm90,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
+    cutlass::gemm::GemmShape<16, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementC,
+      1,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    4,     // kStages 
+    1,     // AlignmentA
+    false, // SplitKSerial
+    cutlass::arch::OpMultiplyAddComplex,
+    cutlass::ComplexTransform::kConjugate,
+    cutlass::BlasMode::kHermitian
+  >;
 
-  EXPECT_TRUE(testbed.run());
+  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
-//////////////////////////////////////////////////////////////////////////
-
-TEST(GenericLayout, ColumnMajorTensorOpMultiplicandCongruous) {
-
-  using GenericLayout = decltype(
-        composition(
-          Swizzle<3,3,4>{},
-          Layout<Shape<_128, _64>>{})
-  );
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-  using Layout = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous<
-    cutlass::sizeof_bits<cutlass::half_t>::value, 64>;
-
-
-  test::layout::Testbed<GenericLayout, Layout> testbed;
-
-  EXPECT_TRUE(testbed.run());
-}
-//////////////////////////////////////////////////////////////////////////
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/thread/activation.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/thread/activation.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination_planar_complex.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination_planar_complex.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_planar_complex.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_planar_complex.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm60.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm60.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm61.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm61.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_tensor_op.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_tensor_op.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_volta_tensor_op.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_volta_tensor_op.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_tensor_op.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_tensor_op.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_testbed.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_testbed.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_wmma_tensor_op_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_wmma_tensor_op_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/output_tile_threadmap.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/output_tile_threadmap.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/predicated_tile_iterator.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/predicated_tile_iterator.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/testbed.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/testbed.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/testbed_planar_complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/testbed_planar_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_tensor_op.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_tensor_op.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_volta_tensor_op.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_volta_tensor_op.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_wmma_tensor_op.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_wmma_tensor_op.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_wmma_tensor_op_s32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_wmma_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_wmma_tensor_op_s32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_wmma_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_bf16n_bf16n_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_bf16n_bf16n_f32t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_bf16t_bf16t_bf16t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_bf16t_bf16t_bf16t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf32n_cf32t_cf32t_tensor_op_tf32_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf32n_cf32t_cf32t_tensor_op_tf32_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf32t_cf32n_cf32t_tensor_op_tf32_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf32t_cf32n_cf32t_tensor_op_tf32_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm90.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm90.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm90.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm90.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm90.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm90.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm90.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm90.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_direct_store_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_direct_store_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f16_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f16_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sparse_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_volta_tensor_op_f32_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_volta_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f16_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f16_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_volta_tensor_op_f32_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_volta_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f16_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f16_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -48,24 +48,24 @@
 
 #include "testbed.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM75_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM75_Device_Gemm_f16n_f16t_f16t_tensor_op_f16_sliced_k, 64x64x64_64x32x32) {
+TEST(SM75_Device_Gemm_f16t_f16n_f16t_tensor_op_f16_sliced_k, 64x64x64_64x32x32) {
 
   using ElementOutput = cutlass::half_t;
   using ElementAccumulator = cutlass::half_t;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::half_t,
-    cutlass::layout::ColumnMajor,
-    cutlass::half_t,
     cutlass::layout::RowMajor,
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor,
     ElementOutput,
     cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm75,
     cutlass::gemm::GemmShape<64, 64, 64>,
     cutlass::gemm::GemmShape<64, 32, 32>,
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sparse_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sparse_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_volta_tensor_op_f16_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_volta_tensor_op_f16_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f16_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f16_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32n_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_volta_tensor_op_f32_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_volta_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_singlestage_wmma_tensor_op_f16_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_singlestage_wmma_tensor_op_f16_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f16_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f16_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_singlestage_wmma_tensor_op_f16_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_singlestage_wmma_tensor_op_f16_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_broadcast_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_broadcast_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm80.cu`

 * *Files 5% similar despite different names*

```diff
@@ -26,14 +26,15 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
     \brief Tests for device-wide GEMM interface
+
 */
 
 #include <iostream>
 
 #include "cutlass/cutlass.h"
 #include "cutlass/gemm/device/gemm.h"
 
@@ -44,45 +45,45 @@
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/gemm.h"
 
 #include "testbed.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM75_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM75_Device_Gemm_f16t_f16n_f16t_tensor_op_f16_sliced_k, 64x64x64_64x32x32) {
+TEST(SM80_Device_Gemm_f16t_f16n_f16t_tensor_op_f16_sliced_k, 128x64x64_64x64x32) {
 
   using ElementOutput = cutlass::half_t;
   using ElementAccumulator = cutlass::half_t;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::half_t,
     cutlass::layout::RowMajor,
     cutlass::half_t,
     cutlass::layout::ColumnMajor,
     ElementOutput,
     cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm75,
-    cutlass::gemm::GemmShape<64, 64, 64>,
-    cutlass::gemm::GemmShape<64, 32, 32>,
-    cutlass::gemm::GemmShape<16, 8, 8>,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<128, 64, 64>,
+    cutlass::gemm::GemmShape<64, 64, 32>,
+    cutlass::gemm::GemmShape<16, 8, 16>,
     cutlass::epilogue::thread::LinearCombination<
       ElementOutput,
       64 / cutlass::sizeof_bits<ElementOutput>::value,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    2
+    3
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif  // if (CUTLASS_ARCH_MMA_SM75_SUPPORTED)
+#endif  // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_bf16_f32_sm80.cu`

 * *Files 16% similar despite different names*

```diff
@@ -25,21 +25,21 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide GEMM interface
-
+    \brief Tests for device-wide GEMM interface using BF16.
 */
 
 #include <iostream>
 
 #include "cutlass/cutlass.h"
+#include "cutlass/arch/mma.h"
 #include "cutlass/gemm/device/gemm.h"
 
 #include "../../common/cutlass_unit_test.h"
 
 #include "cutlass/util/host_tensor.h"
 #include "cutlass/util/tensor_view_io.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
@@ -49,41 +49,45 @@
 
 #include "testbed.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Gemm_f16t_f16n_f16t_tensor_op_f16_sliced_k, 128x64x64_64x64x32) {
+TEST(SM80_Device_Gemm_f32t_f32n_f32t_tensor_op_bf16_f32, 128x128x32_64x64x32) {
 
-  using ElementOutput = cutlass::half_t;
-  using ElementAccumulator = cutlass::half_t;
+  using ElementOutput = float;
+  using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
-    cutlass::half_t,
+    float,
     cutlass::layout::RowMajor,
-    cutlass::half_t,
+    float,
     cutlass::layout::ColumnMajor,
-    ElementOutput,
+    float,
     cutlass::layout::RowMajor,
-    ElementAccumulator,
+    float,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<128, 64, 64>,
+    cutlass::gemm::GemmShape<128, 128, 32>,
     cutlass::gemm::GemmShape<64, 64, 32>,
-    cutlass::gemm::GemmShape<16, 8, 16>,
+    cutlass::gemm::GemmShape<16, 8, 8>,
     cutlass::epilogue::thread::LinearCombination<
       ElementOutput,
-      64 / cutlass::sizeof_bits<ElementOutput>::value,
+      128 / cutlass::sizeof_bits<ElementOutput>::value,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3
+    3,
+    4,
+    4,
+    false,
+    cutlass::arch::OpMultiplyAddFastBF16
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif  // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sparse_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sparse_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_volta_tensor_op_f16_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_volta_tensor_op_f16_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f16_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f16_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32n_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_singlestage_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_singlestage_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_volta_tensor_op_f32_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_volta_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f16_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f16_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f16_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f16_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_volta_tensor_op_f32_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_volta_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_bf16_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sm80.cu`

 * *Files 7% similar despite different names*

```diff
@@ -25,21 +25,20 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide GEMM interface using BF16.
+    \brief Tests for device-wide GEMM interface
 */
 
 #include <iostream>
 
 #include "cutlass/cutlass.h"
-#include "cutlass/arch/mma.h"
 #include "cutlass/gemm/device/gemm.h"
 
 #include "../../common/cutlass_unit_test.h"
 
 #include "cutlass/util/host_tensor.h"
 #include "cutlass/util/tensor_view_io.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
@@ -49,22 +48,22 @@
 
 #include "testbed.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Gemm_f32t_f32n_f32t_tensor_op_bf16_f32, 128x128x32_64x64x32) {
+TEST(SM80_Device_Gemm_f32n_f32n_f32t_tensor_op_f32, 128x128x32_64x64x32) {
 
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     float,
-    cutlass::layout::RowMajor,
+    cutlass::layout::ColumnMajor,
     float,
     cutlass::layout::ColumnMajor,
     float,
     cutlass::layout::RowMajor,
     float,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
@@ -74,19 +73,15 @@
     cutlass::epilogue::thread::LinearCombination<
       ElementOutput,
       128 / cutlass::sizeof_bits<ElementOutput>::value,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3,
-    4,
-    4,
-    false,
-    cutlass::arch::OpMultiplyAddFastBF16
+    3
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu`

 * *Files 10% similar despite different names*

```diff
@@ -25,64 +25,71 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide GEMM interface
+    \brief Tests for device-wide SYRK interface
+  
 */
 
 #include <iostream>
 
-#include "cutlass/cutlass.h"
-#include "cutlass/gemm/device/gemm.h"
-
 #include "../../common/cutlass_unit_test.h"
-
+#include "cutlass/blas3.h"
+#include "cutlass/gemm/device/rank_k.h"
 #include "cutlass/util/host_tensor.h"
-#include "cutlass/util/tensor_view_io.h"
-#include "cutlass/util/reference/host/tensor_fill.h"
-#include "cutlass/util/reference/host/tensor_copy.h"
+#include "cutlass/util/reference/host/rank_k_complex.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
-#include "cutlass/util/reference/host/gemm.h"
+#include "cutlass/util/reference/host/tensor_copy.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/tensor_view_io.h"
 
-#include "testbed.h"
+#include "testbed_rank_k_universal.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Gemm_f32n_f32n_f32t_tensor_op_f32, 128x128x32_64x64x32) {
+TEST(SM80_Device_Syrk_cf64n_cf64t_l_tensor_op_f64_gaussian, 32x32x16_16x16x16) {
 
-  using ElementOutput = float;
-  using ElementAccumulator = float;
+  using ElementA = cutlass::complex<double>;
+  using LayoutA = cutlass::layout::ColumnMajor;
 
-  using Gemm = cutlass::gemm::device::Gemm<
-    float,
-    cutlass::layout::ColumnMajor,
-    float,
-    cutlass::layout::ColumnMajor,
-    float,
-    cutlass::layout::RowMajor,
-    float,
+  using ElementC = cutlass::complex<double>;
+  using LayoutC = cutlass::layout::RowMajor;
+  using ElementAccumulator = cutlass::complex<double>;
+
+  using RankK = cutlass::gemm::device::RankK<
+    ElementA,
+    LayoutA,
+    ElementC,
+    LayoutC,
+    cutlass::FillMode::kLower,
+    ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<128, 128, 32>,
-    cutlass::gemm::GemmShape<64, 64, 32>,
-    cutlass::gemm::GemmShape<16, 8, 8>,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
-      ElementOutput,
-      128 / cutlass::sizeof_bits<ElementOutput>::value,
+      ElementC,
+      1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3
+    4,     // kStages 
+    1,     // AlignmentA
+    false, // SplitKSerial
+    cutlass::arch::OpMultiplyAddGaussianComplex,
+    cutlass::ComplexTransform::kNone,
+    cutlass::BlasMode::kSymmetric
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 #endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32t_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32t_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32t_f32n_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32t_f32n_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32t_f32t_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32t_f32t_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm90.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm90.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm90.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm90.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_grouped_scheduler_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_grouped_scheduler_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_grouped_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_grouped_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_wmma_tensor_op_s32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_wmma_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sparse_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sparse_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_wmma_tensor_op_s32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_wmma_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_f16t_tensor_op_s32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_f8gemm_tn_sm50.cu`

 * *Files 13% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-/**************************************************************************************************
+/***************************************************************************************************
  * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
@@ -26,52 +26,62 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
     \brief Tests for device-wide GEMM interface
+    
 */
 
 #include <iostream>
 
 #include "../../common/cutlass_unit_test.h"
 #include "cutlass/cutlass.h"
 #include "cutlass/gemm/device/gemm.h"
-#include "multistage_testbed.h"
 #include "cutlass/util/host_tensor.h"
 #include "cutlass/util/reference/host/gemm.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
-#if (CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+#include "testbed.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Gemm_s8t_s8n_f16t_tensor_op_s32, 128x128x64_64x64x64) {
-  using ElementOutput = cutlass::half_t;
-  using ElementAccumulator = int32_t;
-  using ElementCompute = float;
+#if (__CUDACC_VER_MAJOR__ > 11) || ((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4))
 
-  using Gemm = cutlass::gemm::device::Gemm<
-      int8_t, cutlass::layout::RowMajor, int8_t,
-      cutlass::layout::ColumnMajor, ElementOutput, cutlass::layout::RowMajor,
-      ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
-      cutlass::gemm::GemmShape<128, 128, 64>,
-      cutlass::gemm::GemmShape<64, 64, 64>, cutlass::gemm::GemmShape<16, 8, 32>,
-      cutlass::epilogue::thread::LinearCombination<
-          ElementOutput,
-          128 / cutlass::sizeof_bits<ElementOutput>::value,
-          ElementAccumulator,
-          ElementCompute>,
-      cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3>;
+TEST(SM50_Device_Gemm_fe4m3t_fe4m3n_fe4m3t_simt_f32, 32x64x8_32x64x1) {
+  
+  using ElementA = cutlass::float_e4m3_t;
+  using ElementB = cutlass::float_e4m3_t;
+  using ElementC = cutlass::float_e4m3_t;
+  using ElementAccumulator = float;
 
-  test::gemm::device::MultistageTestbed<Gemm> testbed;
+  using Gemm = cutlass::gemm::device::Gemm<
+    ElementA, 
+    cutlass::layout::RowMajor,
+    ElementB, 
+    cutlass::layout::ColumnMajor,
+    ElementC,
+    cutlass::layout::RowMajor, 
+    ElementAccumulator,
+    cutlass::arch::OpClassSimt, 
+    cutlass::arch::Sm50,
+    cutlass::gemm::GemmShape<32, 64, 8>,
+    cutlass::gemm::GemmShape<32, 64, 8>, 
+    cutlass::gemm::GemmShape<1, 1, 1>,
+    cutlass::epilogue::thread::LinearCombination<
+        ElementC, 
+        1,
+        ElementAccumulator, 
+        ElementC>,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>
+  >;
 
-  EXPECT_TRUE(testbed.run_all());
+  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
 }
 
-////////////////////////////////////////////////////////////////////////////////
-#endif // #if (CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+#endif
 
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_wmma_tensor_op_s32_sm72.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_wmma_tensor_op_s32_sm72.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sparse_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sparse_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_wmma_tensor_op_s32_sm72.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_wmma_tensor_op_s32_sm72.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_wmma_tensor_op_s32_sm72.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_wmma_tensor_op_s32_sm72.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_wmma_tensor_op_s32_sm72.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_wmma_tensor_op_s32_sm72.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_serial_tensor_op_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_serial_tensor_op_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_simt_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_simt_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32n_tf32n_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32n_tf32n_f32t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32n_tf32t_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32n_tf32t_f32t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32t_tf32n_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32t_tf32n_f32t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32t_tf32t_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32t_tf32t_f32t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_u8t_u8n_s32t_wmma_tensor_op_s32_sm72.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_u8t_u8n_s32t_wmma_tensor_op_s32_sm72.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf32n_cf32n_cf32n_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf32n_cf32n_cf32n_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32n_tensor_op_f32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32n_tensor_op_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32t_tensor_op_f32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32t_tensor_op_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_broadcast_f16n_f16n_f16n_tensorop_f32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_broadcast_f16n_f16n_f16n_tensorop_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_reduction_f16n_f16n_f16n_tensorop_f32_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_reduction_f16n_f16n_f16n_tensorop_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_reduction_f16t_f16n_f16n_tensorop_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_reduction_f16t_f16n_f16n_tensorop_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/gemv.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemv.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_ls_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_rs_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_rs_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_ls_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_rs_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_rs_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64_cf64_cf64_tensor_op_f64_sm90.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64_cf64_cf64_tensor_op_f64_sm90.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_rs_f64_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_rs_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_fast_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64_cf64_tensor_op_f64_sm90.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64_cf64_tensor_op_f64_sm90.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64h_cf64n_tensor_op_f64_grouped_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64h_cf64n_tensor_op_f64_grouped_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64t_tensor_op_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_fast_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf64_cf64_tensor_op_f64_sm90.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_planar_complex_sm80.cu`

 * *Files 14% similar despite different names*

```diff
@@ -25,69 +25,55 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide HERK interface
+    \brief Unit tests for threadblock-level GEMM
 */
 
-#include <iostream>
+#include "cutlass/cutlass.h"
+#include "cutlass/gemm/threadblock/default_mma_planar_complex_multistage.h"
 
-#include "../../common/cutlass_unit_test.h"
-#include "cutlass/blas3.h"
-#include "cutlass/gemm/device/rank_k.h"
-#include "cutlass/util/host_tensor.h"
-#include "cutlass/util/reference/host/rank_k_complex.h"
-#include "cutlass/util/reference/host/tensor_compare.h"
-#include "cutlass/util/reference/host/tensor_copy.h"
-#include "cutlass/util/reference/host/tensor_fill.h"
-#include "cutlass/util/tensor_view_io.h"
-
-#include "testbed_rank_k_universal.h"
-
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-// HERK operator on CUBLAS_OP_C (row-major + conj) input layouts
-TEST(SM90_Device_Herk_cf64h_cf64n_l_tensor_op_f64, 64x64x16_32x32x16) {
-
-  using ElementA = cutlass::complex<double>;
-  using LayoutA = cutlass::layout::RowMajor;
-
-  using ElementC = cutlass::complex<double>;
-  using LayoutC = cutlass::layout::ColumnMajor;
-  using ElementAccumulator = cutlass::complex<double>;
-
-  using RankK = cutlass::gemm::device::RankK<
-    ElementA,
-    LayoutA,
-    ElementC,
-    LayoutC,
-    cutlass::FillMode::kLower,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm90,
-    cutlass::gemm::GemmShape<32, 32, 16>,
-    cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<16, 8, 4>,
-    cutlass::epilogue::thread::LinearCombination<
-      ElementC,
-      1,
-      ElementAccumulator,
-      ElementAccumulator
-    >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    4,     // kStages 
-    1,     // AlignmentA
-    false, // SplitKSerial
-    cutlass::arch::OpMultiplyAddComplex,
-    cutlass::ComplexTransform::kConjugate,
-    cutlass::BlasMode::kHermitian
-  >;
+#include "mma_planar_complex_testbed.h"
 
-  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
+#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+
+////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM80_gemm_threadblock_planar_complex_congruous, tensor_op_64x64x32_64x64x32_16x8x16_3stage) {
+
+  using ElementA = cutlass::half_t;
+  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementB = cutlass::half_t;
+  using LayoutB = cutlass::layout::RowMajor;
+  using ElementC = float;
+  using LayoutC = cutlass::layout::RowMajor;
+
+  cutlass::gemm::GemmCoord problem_size(64, 64, 8);
+
+  using ThreadblockShape = cutlass::gemm::GemmShape<64, 64, 32>;
+  using WarpShape = cutlass::gemm::GemmShape<32, 32, 32>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 16>;
+  int const Stages = 3;
+
+  // Define the MmaCore components
+  using Mma = typename cutlass::gemm::threadblock::DefaultMmaPlanarComplexMultistage<
+      ElementA, LayoutA, 8,
+      ElementB, LayoutB, 8,
+      ElementC, LayoutC, 
+      cutlass::arch::OpClassTensorOp,
+      cutlass::arch::Sm80,
+      ThreadblockShape, WarpShape, InstructionShape,
+      Stages>::ThreadblockMma;
+
+  dim3 grid(1, 1);
+  dim3 block(32, Mma::WarpCount::kCount, 1);
+
+  test::gemm::threadblock::TestbedPlanarComplex<Mma>(problem_size.m(), problem_size.n(),
+                                            problem_size.k())
+      .run(grid, block);
 }
-/////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+////////////////////////////////////////////////////////////////////////////////
+#endif  // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf64h_cf64n_tensor_op_f64_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf64h_cf64n_tensor_op_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/multistage_testbed.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/multistage_testbed.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/multistage_testbed_interleaved.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/multistage_testbed_interleaved.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/rank_2k_grouped_scheduler_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/rank_2k_grouped_scheduler_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nn_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tt_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tt_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_nn_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_nn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_nt_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_nt_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_tn_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_tn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_tt_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_tt_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_nn_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_nn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_nt_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_nt_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_tn_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_tn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_tt_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_tt_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_nn_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_nn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_nt_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_nt_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_tn_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_tn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_tt_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_tt_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_perf.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_perf.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_sliced_k.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_sliced_k.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_nn_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_nn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_nt_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_nt_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_tn_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_tn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_tt_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_tt_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nn_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tt_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tt_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_nn_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_nn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_nt_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_nt_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_tn_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_tn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_tt_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_tt_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm50_gemm_f32_f32_f32_simt.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64n_tensor_op_f64_sm80.cu`

 * *Files 26% similar despite different names*

```diff
@@ -25,111 +25,112 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide GEMM interface
+    \brief Tests for device-wide SYRK interface
+  
 */
 
 #include <iostream>
 
-#include "cutlass/cutlass.h"
-#include "cute/tensor.hpp"
-#include "cute/atom/mma_atom.hpp"
-
-#include "cutlass/numeric_types.h"
-
-#include "cutlass/gemm/device/gemm_universal_adapter.h"
-#include "default_gemm_configuration.hpp"
-
 #include "../../common/cutlass_unit_test.h"
+#include "cutlass/blas3.h"
+#include "cutlass/gemm/device/rank_k.h"
+#include "cutlass/util/host_tensor.h"
+#include "cutlass/util/reference/host/rank_k_complex.h"
+#include "cutlass/util/reference/host/tensor_compare.h"
+#include "cutlass/util/reference/host/tensor_copy.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/tensor_view_io.h"
 
-#include "gemm_testbed_3x.hpp"
+#include "testbed_rank_k_universal.h"
 
-using namespace cute;
+#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM50_Device_Gemm_f32n_f32n_f32n_simt_f32, 128x128x64_64x64x64) {
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassSimt, cutlass::arch::Sm50,
-    float, cutlass::layout::ColumnMajor,
-    float, cutlass::layout::ColumnMajor,
-    float, cutlass::layout::ColumnMajor,
-    float>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
+TEST(SM80_Device_Syrk_cf64n_cf64n_l_tensor_op_f64, 32x32x16_16x16x16) {
 
-TEST(SM50_Device_Gemm_f32n_f32t_f32n_simt_f32, 128x128x64_64x64x64) {
+  using ElementA = cutlass::complex<double>;
+  using LayoutA = cutlass::layout::ColumnMajor;
 
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassSimt, cutlass::arch::Sm50,
-    float, cutlass::layout::ColumnMajor,
-    float, cutlass::layout::RowMajor,
-    float, cutlass::layout::ColumnMajor,
-    float>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
+  using ElementC = cutlass::complex<double>;
+  using LayoutC = cutlass::layout::ColumnMajor;
+  using ElementAccumulator = cutlass::complex<double>;
+
+  using RankK = cutlass::gemm::device::RankK<
+    ElementA,
+    LayoutA,
+    ElementC,
+    LayoutC,
+    cutlass::FillMode::kLower,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementC,
+      1,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    4,     // kStages 
+    1,     // AlignmentA
+    false, // SplitKSerial
+    cutlass::arch::OpMultiplyAddComplex,
+    cutlass::ComplexTransform::kNone,
+    cutlass::BlasMode::kSymmetric
   >;
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM50_Device_Gemm_f32t_f32n_f32n_simt_f32, 128x128x64_64x64x64) {
+TEST(SM80_Device_Syrk_cf64n_cf64n_u_tensor_op_f64, 32x32x16_16x16x16) {
 
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassSimt, cutlass::arch::Sm50,
-    float, cutlass::layout::RowMajor,
-    float, cutlass::layout::ColumnMajor,
-    float, cutlass::layout::ColumnMajor,
-    float>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
+  using ElementA = cutlass::complex<double>;
+  using LayoutA = cutlass::layout::ColumnMajor;
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-TEST(SM50_Device_Gemm_f32t_f32t_f32n_simt_f32, 128x128x64_64x64x64) {
-
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassSimt, cutlass::arch::Sm50,
-    float, cutlass::layout::RowMajor,
-    float, cutlass::layout::RowMajor,
-    float, cutlass::layout::ColumnMajor,
-    float>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
+  using ElementC = cutlass::complex<double>;
+  using LayoutC = cutlass::layout::ColumnMajor;
+  using ElementAccumulator = cutlass::complex<double>;
+
+  using RankK = cutlass::gemm::device::RankK<
+    ElementA,
+    LayoutA,
+    ElementC,
+    LayoutC,
+    cutlass::FillMode::kUpper,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementC,
+      1,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    4,     // kStages 
+    1,     // AlignmentA
+    false, // SplitKSerial
+    cutlass::arch::OpMultiplyAddComplex,
+    cutlass::ComplexTransform::kNone,
+    cutlass::BlasMode::kSymmetric
   >;
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
+
+#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm50_gemm_f64_f64_f64_simt.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64_f64_tensor_op_f64_sm90.cu`

 * *Files 22% similar despite different names*

```diff
@@ -25,110 +25,110 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide GEMM interface
+    \brief Tests for device-wide SYRK interface
+  
 */
 
 #include <iostream>
 
-#include "cutlass/cutlass.h"
-#include "cute/tensor.hpp"
-#include "cute/atom/mma_atom.hpp"
-
-#include "cutlass/numeric_types.h"
-
-#include "cutlass/gemm/device/gemm_universal_adapter.h"
-#include "default_gemm_configuration.hpp"
-
 #include "../../common/cutlass_unit_test.h"
+#include "cutlass/blas3.h"
+#include "cutlass/gemm/device/rank_2k.h"
+#include "cutlass/util/host_tensor.h"
+#include "cutlass/util/reference/host/rank_2k.h"
+#include "cutlass/util/reference/host/tensor_compare.h"
+#include "cutlass/util/reference/host/tensor_copy.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/tensor_view_io.h"
 
-#include "gemm_testbed_3x.hpp"
+#include "testbed_rank2k_universal.h"
 
-using namespace cute;
+#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM50_Device_Gemm_f64n_f64n_f64n_simt_f64, 128x128x64_64x64x64) {
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassSimt, cutlass::arch::Sm50,
-    double, cutlass::layout::ColumnMajor,
-    double, cutlass::layout::ColumnMajor,
-    double, cutlass::layout::ColumnMajor,
-    double>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-TEST(SM50_Device_Gemm_f64n_f64t_f64n_simt_f64, 128x128x64_64x64x64) {
+TEST(SM90_Device_Syr2k_f64n_f64n_l_tensor_op_f64, 32x32x16_16x16x16) {
 
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassSimt, cutlass::arch::Sm50,
-    double, cutlass::layout::ColumnMajor,
-    double, cutlass::layout::RowMajor,
-    double, cutlass::layout::ColumnMajor,
-    double>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
+  using ElementA = double;
+  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementB = double;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = double;
+  using LayoutC = cutlass::layout::ColumnMajor;
+  using ElementAccumulator = double;
+
+  using Rank2K = cutlass::gemm::device::Rank2K<
+    ElementA,
+    LayoutA,
+    ElementB,
+    LayoutB,
+    ElementC,
+    LayoutC,
+    cutlass::FillMode::kLower,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm90,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
+    cutlass::gemm::GemmShape<16, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementC,
+      1,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    4
   >;
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-TEST(SM50_Device_Gemm_f64t_f64n_f64n_simt_f64, 128x128x64_64x64x64) {
+  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassSimt, cutlass::arch::Sm50,
-    double, cutlass::layout::RowMajor,
-    double, cutlass::layout::ColumnMajor,
-    double, cutlass::layout::ColumnMajor,
-    double>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM50_Device_Gemm_f64t_f64t_f64n_simt_f64, 128x128x64_64x64x64) {
+TEST(SM90_Device_Syr2k_f64t_f64n_l_tensor_op_f64, 128x128x16_32x64x16) {
 
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassSimt, cutlass::arch::Sm50,
-    double, cutlass::layout::RowMajor,
-    double, cutlass::layout::RowMajor,
-    double, cutlass::layout::ColumnMajor,
-    double>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
+  using ElementA = double;
+  using LayoutA = cutlass::layout::RowMajor;
+  using ElementB = double;
+  using LayoutB = cutlass::layout::RowMajor;
+  using ElementC = double;
+  using LayoutC = cutlass::layout::ColumnMajor;
+  using ElementAccumulator = double;
+
+  using Rank2K = cutlass::gemm::device::Rank2K<
+    ElementA,
+    LayoutA,
+    ElementB,
+    LayoutB,
+    ElementC,
+    LayoutC,
+    cutlass::FillMode::kLower,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm90,
+    cutlass::gemm::GemmShape<128, 128, 16>,
+    cutlass::gemm::GemmShape<32, 64, 16>,
+    cutlass::gemm::GemmShape<16, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementC,
+      1,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    3
   >;
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
+
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
+
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm61_gemm_s8_s8_s32_simt.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64_cf64_tensor_op_f64_sm90.cu`

 * *Files 21% similar despite different names*

```diff
@@ -25,112 +25,112 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide GEMM interface
+    \brief Tests for device-wide SYRK interface
+  
 */
 
 #include <iostream>
 
-#include "cutlass/cutlass.h"
-#include "cute/tensor.hpp"
-#include "cute/atom/mma_atom.hpp"
-
-#include "cutlass/numeric_types.h"
-
-#include "cutlass/gemm/device/gemm_universal_adapter.h"
-#include "default_gemm_configuration.hpp"
-
 #include "../../common/cutlass_unit_test.h"
+#include "cutlass/blas3.h"
+#include "cutlass/gemm/device/rank_k.h"
+#include "cutlass/util/host_tensor.h"
+#include "cutlass/util/reference/host/rank_k_complex.h"
+#include "cutlass/util/reference/host/tensor_compare.h"
+#include "cutlass/util/reference/host/tensor_copy.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/tensor_view_io.h"
 
-#include "gemm_testbed_3x.hpp"
+#include "testbed_rank_k_universal.h"
 
-using namespace cute;
-
-//#if defined(CUTLASS_ARCH_MMA_SM61_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM61_Device_Gemm_s8n_s8n_s32n_simt_s32, 128x128x64_64x64x64) {
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassSimt, cutlass::arch::Sm50,
-    int8_t, cutlass::layout::ColumnMajor,
-    int8_t, cutlass::layout::ColumnMajor,
-    int32_t, cutlass::layout::ColumnMajor,
-    int32_t>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
+TEST(SM90_Device_Syrk_cf64n_cf64n_l_tensor_op_f64, 32x32x16_16x16x16) {
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+  using ElementA = cutlass::complex<double>;
+  using LayoutA = cutlass::layout::ColumnMajor;
 
-TEST(SM61_Device_Gemm_s8n_s8t_s32n_simt_s32, 128x128x64_64x64x64) {
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassSimt, cutlass::arch::Sm50,
-    int8_t, cutlass::layout::ColumnMajor,
-    int8_t, cutlass::layout::RowMajor,
-    int32_t, cutlass::layout::ColumnMajor,
-    int32_t>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
+  using ElementC = cutlass::complex<double>;
+  using LayoutC = cutlass::layout::ColumnMajor;
+  using ElementAccumulator = cutlass::complex<double>;
+
+  using RankK = cutlass::gemm::device::RankK<
+    ElementA,
+    LayoutA,
+    ElementC,
+    LayoutC,
+    cutlass::FillMode::kLower,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm90,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
+    cutlass::gemm::GemmShape<16, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementC,
+      1,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    4,     // kStages 
+    1,     // AlignmentA
+    false, // SplitKSerial
+    cutlass::arch::OpMultiplyAddComplex,
+    cutlass::ComplexTransform::kNone,
+    cutlass::BlasMode::kSymmetric
   >;
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM61_Device_Gemm_s8t_s8n_s32n_simt_s32, 128x128x64_64x64x64) {
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassSimt, cutlass::arch::Sm50,
-    int8_t, cutlass::layout::RowMajor,
-    int8_t, cutlass::layout::ColumnMajor,
-    int32_t, cutlass::layout::ColumnMajor,
-    int32_t>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
+TEST(SM90_Device_Syrk_cf64n_cf64t_l_tensor_op_f64_gaussian, 32x32x16_16x16x16) {
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+  using ElementA = cutlass::complex<double>;
+  using LayoutA = cutlass::layout::ColumnMajor;
 
-TEST(SM61_Device_Gemm_s8t_s8t_s32n_simt_s32, 128x128x64_64x64x64) {
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassSimt, cutlass::arch::Sm50,
-    int8_t, cutlass::layout::RowMajor,
-    int8_t, cutlass::layout::RowMajor,
-    int32_t, cutlass::layout::ColumnMajor,
-    int32_t>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
+  using ElementC = cutlass::complex<double>;
+  using LayoutC = cutlass::layout::RowMajor;
+  using ElementAccumulator = cutlass::complex<double>;
+
+  using RankK = cutlass::gemm::device::RankK<
+    ElementA,
+    LayoutA,
+    ElementC,
+    LayoutC,
+    cutlass::FillMode::kLower,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm90,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
+    cutlass::gemm::GemmShape<16, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementC,
+      1,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    4,     // kStages 
+    1,     // AlignmentA
+    false, // SplitKSerial
+    cutlass::arch::OpMultiplyAddGaussianComplex,
+    cutlass::ComplexTransform::kNone,
+    cutlass::BlasMode::kSymmetric
   >;
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-//#endif // #if defined(CUTLASS_ARCH_MMA_SM61_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f16_f16_f32_tensor_op_f32.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64_cf64_cf64_tensor_op_f64_sm90.cu`

 * *Files 16% similar despite different names*

```diff
@@ -25,112 +25,109 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide GEMM interface
+    \brief Tests for device-wide SYMM interface
+
+  
 */
 
 #include <iostream>
 
-#include "cutlass/cutlass.h"
-#include "cute/tensor.hpp"
-#include "cute/atom/mma_atom.hpp"
-
-#include "cutlass/numeric_types.h"
-
-#include "cutlass/gemm/device/gemm_universal_adapter.h"
-#include "default_gemm_configuration.hpp"
-
 #include "../../common/cutlass_unit_test.h"
+#include "cutlass/blas3.h"
+#include "cutlass/gemm/device/symm.h"
+#include "cutlass/util/host_tensor.h"
+#include "cutlass/util/reference/host/symm_complex.h"
+#include "cutlass/util/reference/host/tensor_compare.h"
+#include "cutlass/util/reference/host/tensor_copy.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/tensor_view_io.h"
 
-#include "gemm_testbed_3x.hpp"
+#include "testbed_symm_universal.h"
 
-using namespace cute;
-
-//#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-#if 1
-TEST(SM80_Device_Gemm_f16t_f16n_f32t_tensor_op_f32_3x, 128x128x32_64x64x32) {
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
-    cutlass::half_t, cutlass::layout::RowMajor,
-    cutlass::half_t, cutlass::layout::ColumnMajor,
-    float, cutlass::layout::RowMajor,
-    float>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
-  >;
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-#endif
-/////////////////////////////////////////////////////////////////////////////////////////////////
-#if 1
-TEST(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32_3x, 128x128x32_64x64x32) {
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
-    cutlass::half_t, cutlass::layout::ColumnMajor,
-    cutlass::half_t, cutlass::layout::RowMajor,
-    float, cutlass::layout::RowMajor,
-    float>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
+TEST(SM90_Device_Symm_cf64n_cf64n_ls_l_tensor_op_f64_gaussian, 32x32x16_16x16x16) {
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+  using ElementOutput = cutlass::complex<double>;
+  using ElementAccumulator = cutlass::complex<double>;
 
-TEST(SM80_Device_Gemm_f16n_f16n_f32t_tensor_op_f32_3x, 128x128x32_64x64x32) {
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
-    cutlass::half_t, cutlass::layout::ColumnMajor,
-    cutlass::half_t, cutlass::layout::ColumnMajor,
-    float, cutlass::layout::RowMajor,
-    float>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
+  using Symm = cutlass::gemm::device::Symm<
+    cutlass::complex<double>,
+    cutlass::layout::ColumnMajor,
+    cutlass::SideMode::kLeft,
+    cutlass::FillMode::kLower,
+    cutlass::complex<double>,
+    cutlass::layout::ColumnMajor,
+    ElementOutput,
+    cutlass::layout::ColumnMajor,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm90,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
+    cutlass::gemm::GemmShape<16, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementOutput,
+      1,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    4,
+    1,
+    1,
+    false,
+    cutlass::arch::OpMultiplyAddGaussianComplex
   >;
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllSymmUniversal<Symm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Gemm_f16t_f16t_f32t_tensor_op_f32_3x, 128x128x32_64x64x32) {
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
-    cutlass::half_t, cutlass::layout::RowMajor,
-    cutlass::half_t, cutlass::layout::RowMajor,
-    float, cutlass::layout::RowMajor,
-    float>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
+TEST(SM90_Device_Symm_cf64n_cf64n_rs_u_tensor_op_f64, 64x64x16_32x32x16) {
+
+  using ElementOutput = cutlass::complex<double>;
+  using ElementAccumulator = cutlass::complex<double>;
+
+  using Symm = cutlass::gemm::device::Symm<
+    cutlass::complex<double>,
+    cutlass::layout::ColumnMajor,
+    cutlass::SideMode::kRight,
+    cutlass::FillMode::kUpper,
+    cutlass::complex<double>,
+    cutlass::layout::ColumnMajor,
+    ElementOutput,
+    cutlass::layout::ColumnMajor,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm90,
+    cutlass::gemm::GemmShape<64, 64, 16>,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementOutput,
+      1,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    4,
+    1,
+    1,
+    false,
+    cutlass::arch::OpMultiplyAddComplex
   >;
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllSymmUniversal<Symm>());
 }
-#endif
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-//#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f32_f32_f32_simt.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64_cf64_tensor_op_f64_sm90.cu`

 * *Files 21% similar despite different names*

```diff
@@ -25,111 +25,126 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide GEMM interface
+    \brief Tests for device-wide SYRK interface
+  
 */
 
 #include <iostream>
 
-#include "cutlass/cutlass.h"
-#include "cute/tensor.hpp"
-#include "cute/atom/mma_atom.hpp"
-
-#include "cutlass/numeric_types.h"
-
-#include "cutlass/gemm/device/gemm_universal_adapter.h"
-#include "default_gemm_configuration.hpp"
-
 #include "../../common/cutlass_unit_test.h"
+#include "cutlass/blas3.h"
+#include "cutlass/gemm/device/rank_2k.h"
+#include "cutlass/util/host_tensor.h"
+#include "cutlass/util/reference/host/rank_2k.h"
+#include "cutlass/util/reference/host/tensor_compare.h"
+#include "cutlass/util/reference/host/tensor_copy.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/tensor_view_io.h"
 
-#include "gemm_testbed_3x.hpp"
+#include "testbed_rank2k_universal.h"
 
-using namespace cute;
+#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Gemm_f32n_f32n_f32n_simt_f32, 128x128x64_64x64x64) {
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassSimt, cutlass::arch::Sm80,
-    float, cutlass::layout::ColumnMajor,
-    float, cutlass::layout::ColumnMajor,
-    float, cutlass::layout::ColumnMajor,
-    float>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
-  >;
+TEST(SM90_Device_Syr2k_cf64n_cf64n_l_tensor_op_f64, 32x32x16_16x16x16) {
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
+  using ElementA = cutlass::complex<double>;
+  using LayoutA = cutlass::layout::ColumnMajor;
 
-TEST(SM80_Device_Gemm_f32n_f32t_f32n_simt_f32, 128x128x64_64x64x64) {
+  using ElementB = cutlass::complex<double>;
+  using LayoutB = cutlass::layout::ColumnMajor;
 
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassSimt, cutlass::arch::Sm80,
-    float, cutlass::layout::ColumnMajor,
-    float, cutlass::layout::RowMajor,
-    float, cutlass::layout::ColumnMajor,
-    float>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
+  using ElementC = cutlass::complex<double>;
+  using LayoutC = cutlass::layout::ColumnMajor;
+  using ElementAccumulator = cutlass::complex<double>;
+
+  using Rank2K = cutlass::gemm::device::Rank2K<
+    ElementA,
+    LayoutA,
+    ElementB,
+    LayoutB,
+    ElementC,
+    LayoutC,
+    cutlass::FillMode::kLower,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm90,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
+    cutlass::gemm::GemmShape<16, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementC,
+      1,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    4,     // kStages 
+    1,     // AlignmentA
+    1,     // AlignmentB
+    false, // SplitKSerial
+    cutlass::arch::OpMultiplyAddComplex,
+    cutlass::ComplexTransform::kNone,
+    cutlass::ComplexTransform::kNone,
+    cutlass::BlasMode::kSymmetric
   >;
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Gemm_f32t_f32n_f32n_simt_f32, 128x128x64_64x64x64) {
+TEST(SM90_Device_Syr2k_cf64n_cf64t_u_tensor_op_f64, 32x32x16_16x16x16) {
 
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassSimt, cutlass::arch::Sm80,
-    float, cutlass::layout::RowMajor,
-    float, cutlass::layout::ColumnMajor,
-    float, cutlass::layout::ColumnMajor,
-    float>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
+  using ElementA = cutlass::complex<double>;
+  using LayoutA = cutlass::layout::ColumnMajor;
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-TEST(SM80_Device_Gemm_f32t_f32t_f32n_simt_f32, 128x128x64_64x64x64) {
+  using ElementB = cutlass::complex<double>;
+  using LayoutB = cutlass::layout::ColumnMajor;
 
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassSimt, cutlass::arch::Sm80,
-    float, cutlass::layout::RowMajor,
-    float, cutlass::layout::RowMajor,
-    float, cutlass::layout::ColumnMajor,
-    float>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
+  using ElementC = cutlass::complex<double>;
+  using LayoutC = cutlass::layout::RowMajor;
+  using ElementAccumulator = cutlass::complex<double>;
+
+  using Rank2K = cutlass::gemm::device::Rank2K<
+    ElementA,
+    LayoutA,
+    ElementB,
+    LayoutB,
+    ElementC,
+    LayoutC,
+    cutlass::FillMode::kUpper,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm90,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
+    cutlass::gemm::GemmShape<16, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementC,
+      1,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    4,     // kStages 
+    1,     // AlignmentA
+    1,     // AlignmentB
+    false, // SplitKSerial
+    cutlass::arch::OpMultiplyAddComplex,
+    cutlass::ComplexTransform::kNone,
+    cutlass::ComplexTransform::kNone,
+    cutlass::BlasMode::kSymmetric
   >;
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
+
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f64_f64_f64_simt.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f32n_f32n_tensor_op_fast_f32_sm80.cu`

 * *Files 22% similar despite different names*

```diff
@@ -25,110 +25,108 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide GEMM interface
+    \brief Tests for device-wide SYRK interface
+  
 */
 
 #include <iostream>
 
-#include "cutlass/cutlass.h"
-#include "cute/tensor.hpp"
-#include "cute/atom/mma_atom.hpp"
-
-#include "cutlass/numeric_types.h"
-
-#include "cutlass/gemm/device/gemm_universal_adapter.h"
-#include "default_gemm_configuration.hpp"
-
 #include "../../common/cutlass_unit_test.h"
+#include "cutlass/blas3.h"
+#include "cutlass/gemm/device/rank_2k.h"
+#include "cutlass/util/host_tensor.h"
+#include "cutlass/util/reference/host/rank_2k.h"
+#include "cutlass/util/reference/host/tensor_compare.h"
+#include "cutlass/util/reference/host/tensor_copy.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/tensor_view_io.h"
 
-#include "gemm_testbed_3x.hpp"
+#include "testbed_rank2k_universal.h"
 
-using namespace cute;
+#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Gemm_f64n_f64n_f64n_simt_f64, 128x128x64_64x64x64) {
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassSimt, cutlass::arch::Sm80,
-    double, cutlass::layout::ColumnMajor,
-    double, cutlass::layout::ColumnMajor,
-    double, cutlass::layout::ColumnMajor,
-    double>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-TEST(SM80_Device_Gemm_f64n_f64t_f64n_simt_f64, 128x128x64_64x64x64) {
+TEST(SM80_Device_Syr2k_f32n_f32n_l_tensor_op_fast_f32, 128x256x32_64x64x32) {
 
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassSimt, cutlass::arch::Sm80,
-    double, cutlass::layout::ColumnMajor,
-    double, cutlass::layout::RowMajor,
-    double, cutlass::layout::ColumnMajor,
-    double>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
+  using ElementA = cutlass::tfloat32_t;
+  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementB = cutlass::tfloat32_t;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = float;
+  using LayoutC = cutlass::layout::ColumnMajor;
+  using ElementAccumulator = float;
+
+  using Rank2K = cutlass::gemm::device::Rank2K<
+    ElementA,
+    LayoutA,
+    ElementB,
+    LayoutB,
+    ElementC,
+    LayoutC,
+    cutlass::FillMode::kLower,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<128, 256, 32>,
+    cutlass::gemm::GemmShape<64, 64, 32>,
+    cutlass::gemm::GemmShape<16, 8, 8>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementC,
+      1,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    3
   >;
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-TEST(SM80_Device_Gemm_f64t_f64n_f64n_simt_f64, 128x128x64_64x64x64) {
+  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassSimt, cutlass::arch::Sm80,
-    double, cutlass::layout::RowMajor,
-    double, cutlass::layout::ColumnMajor,
-    double, cutlass::layout::ColumnMajor,
-    double>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Gemm_f64t_f64t_f64n_simt_f64, 128x128x64_64x64x64) {
+TEST(SM80_Device_Syr2k_f32n_f32n_u_tensor_op_fast_f32, 128x256x32_64x64x32) {
 
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassSimt, cutlass::arch::Sm80,
-    double, cutlass::layout::RowMajor,
-    double, cutlass::layout::RowMajor,
-    double, cutlass::layout::ColumnMajor,
-    double>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
+  using ElementA = cutlass::tfloat32_t;
+  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementB = cutlass::tfloat32_t;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = float;
+  using LayoutC = cutlass::layout::ColumnMajor;
+  using ElementAccumulator = float;
+
+  using Rank2K = cutlass::gemm::device::Rank2K<
+    ElementA,
+    LayoutA,
+    ElementB,
+    LayoutB,
+    ElementC,
+    LayoutC,
+    cutlass::FillMode::kUpper,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<128, 256, 32>,
+    cutlass::gemm::GemmShape<64, 64, 32>,
+    cutlass::gemm::GemmShape<16, 8, 8>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementC,
+      1,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    3
   >;
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
+  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 
+}
 /////////////////////////////////////////////////////////////////////////////////////////////////
+#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f64_f64_f64_tensor_op_f64.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64_f64_tensor_op_f64_sm90.cu`

 * *Files 27% similar despite different names*

```diff
@@ -25,74 +25,102 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide GEMM interface
+    \brief Tests for device-wide SYRK interface
+  
 */
 
 #include <iostream>
 
-#include "cutlass/cutlass.h"
-#include "cute/tensor.hpp"
-#include "cute/atom/mma_atom.hpp"
-
-#include "cutlass/numeric_types.h"
-
-#include "cutlass/gemm/device/gemm_universal_adapter.h"
-#include "default_gemm_configuration.hpp"
-
 #include "../../common/cutlass_unit_test.h"
+#include "cutlass/blas3.h"
+#include "cutlass/gemm/device/rank_k.h"
+#include "cutlass/util/host_tensor.h"
+#include "cutlass/util/reference/host/rank_k_complex.h"
+#include "cutlass/util/reference/host/tensor_compare.h"
+#include "cutlass/util/reference/host/tensor_copy.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/tensor_view_io.h"
 
-#include "gemm_testbed_3x.hpp"
+#include "testbed_rank_k_universal.h"
 
-using namespace cute;
-
-//#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Gemm_f64n_f64t_f64n_tensor_op_f64, 128x128x64_64x64x64) {
+TEST(SM90_Device_Syrk_f64n_f64t_l_tensor_op_f64, 128x64x16_64x32x16) {
+
+  using ElementA = double;
+  using LayoutA = cutlass::layout::ColumnMajor;
 
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
-    double, cutlass::layout::ColumnMajor,
-    double, cutlass::layout::ColumnMajor,
-    double, cutlass::layout::ColumnMajor,
-    double>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
+  using ElementC = double;
+  using LayoutC = cutlass::layout::RowMajor;
+  using ElementAccumulator = double;
+
+  using RankK = cutlass::gemm::device::RankK<
+    ElementA,
+    LayoutA,
+    ElementC,
+    LayoutC,
+    cutlass::FillMode::kLower,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm90,
+    cutlass::gemm::GemmShape<128, 64, 16>,
+    cutlass::gemm::GemmShape<64, 32, 16>,
+    cutlass::gemm::GemmShape<16, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementC,
+      1,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    4
   >;
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
+
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Gemm_f64t_f64n_f64n_tensor_op_f64, 128x128x64_64x64x64) {
+TEST(SM90_Device_Syrk_f64t_f64n_l_tensor_op_f64, 32x32x16_16x16x16) {
 
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
-    double, cutlass::layout::RowMajor,
-    double, cutlass::layout::ColumnMajor,
-    double, cutlass::layout::ColumnMajor,
-    double>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
+  using ElementA = double;
+  using LayoutA = cutlass::layout::RowMajor;
+  using ElementC = double;
+  using LayoutC = cutlass::layout::ColumnMajor;
+  using ElementAccumulator = double;
+
+  using RankK = cutlass::gemm::device::RankK<
+    ElementA,
+    LayoutA,
+    ElementC,
+    LayoutC,
+    cutlass::FillMode::kLower,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm90,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
+    cutlass::gemm::GemmShape<16, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementC,
+      1,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    4
   >;
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-// #endif
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_s8_s8_s32_tensor_op.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/performance_result.h`

 * *Files 26% similar despite different names*

```diff
@@ -24,71 +24,105 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/*! \file
-    \brief Tests for device-wide GEMM interface
+/* \file
+   \brief Defines a math function
 */
 
-#include <iostream>
+#pragma once
 
-#include "cutlass/cutlass.h"
-#include "cute/tensor.hpp"
-#include "cute/atom/mma_atom.hpp"
-
-#include "cutlass/numeric_types.h"
-
-#include "cutlass/gemm/device/gemm_universal_adapter.h"
-#include "default_gemm_configuration.hpp"
+#include <vector>
 
-#include "../../common/cutlass_unit_test.h"
+#include "cutlass/cutlass.h"
 
-#include "gemm_testbed_3x.hpp"
+// CUTLASS Profiler includes
+#include "enumerated_types.h"
 
-using namespace cute;
+// CUTLASS Library includes
+#include "cutlass/library/library.h"
 
-//#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+namespace cutlass {
+namespace profiler {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(DISABLED_SM80_Device_Gemm_s8n_s8n_s32n_tensor_op_s32, 128x128x32_64x64x64) {
-
-}
+/// Performance result object
+struct PerformanceResult {
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+  /// Index of problem
+  size_t problem_index;
 
-TEST(DISABLED_SM80_Device_Gemm_s8n_s8t_s32n_tensor_op_s32, 128x128x32_64x64x64) {
+  /// library::Provider
+  library::Provider provider;
 
-}
+  /// Operation kind
+  library::OperationKind op_kind;
+
+  /// CUTLASS status result from kernels (success or failure)
+  // Status does information on verification
+  Status status;
+
+  /// Outcome of verification (worst case verification result)
+  Disposition disposition;
+  
+  /// Outcome of verification (all verification results)
+  DispositionMap verification_map;
+
+  /// Operation name
+  std::string operation_name;
+
+  /// Stringified vector of argument values
+  std::vector<std::pair<std::string, std::string> > arguments;
+
+  /// Number of bytes read or written
+  int64_t bytes;
+
+  /// Number of DL flops performed by the math function
+  int64_t flops;
+
+  /// Average runtime in ms
+  double runtime;
+
+  //
+  // Members
+  //
+
+  /// Ctor
+  PerformanceResult(): 
+    problem_index(0),
+    op_kind(library::OperationKind::kInvalid),
+    provider(library::Provider::kInvalid), 
+    disposition(Disposition::kNotRun),
+    status(Status::kInvalid),
+    bytes(0), 
+    flops(0), 
+    runtime(0)
+  { }
+
+  /// Returns true if the runtime is valid
+  bool good() const {
+    return runtime > 0;
+  }
+
+  /// Math throughput in units of GFLOP/s
+  double gflops_per_sec() const {
+    return double(flops) / runtime / 1.0e6;
+  }
+
+  /// memory bandwidth in units of GiB/s
+  double gbytes_per_sec() const {
+    return double(bytes) / double(1 << 30) / runtime * 1000.0;
+  }
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+};
 
-TEST(SM80_Device_Gemm_s8t_s8n_s32n_tensor_op_s32, 128x128x32_64x64x64) {
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
-    int8_t, cutlass::layout::RowMajor,
-    int8_t, cutlass::layout::ColumnMajor,
-    int32_t, cutlass::layout::ColumnMajor,
-    int32_t>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
+using PerformanceResultVector = std::vector<PerformanceResult>;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(DISABLED_SM80_Device_Gemm_s8t_s8t_s32n_tensor_op_s32, 128x128x32_64x64x64) {
-
-}
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
+} // namespace profiler
+} // namespace cutlass
 
-//#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_tf32_tf32_f32_tensor_op_f32.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_f32_sm80.cu`

 * *Files 19% similar despite different names*

```diff
@@ -25,111 +25,113 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide GEMM interface
+    \brief Tests for device-wide SYRK interface
+
+  
 */
 
 #include <iostream>
 
-#include "cutlass/cutlass.h"
-#include "cute/tensor.hpp"
-#include "cute/atom/mma_atom.hpp"
-
-#include "cutlass/numeric_types.h"
-
-#include "cutlass/gemm/device/gemm_universal_adapter.h"
-#include "default_gemm_configuration.hpp"
-
 #include "../../common/cutlass_unit_test.h"
+#include "cutlass/blas3.h"
+#include "cutlass/gemm/device/rank_k.h"
+#include "cutlass/util/host_tensor.h"
+#include "cutlass/util/reference/host/rank_k_complex.h"
+#include "cutlass/util/reference/host/tensor_compare.h"
+#include "cutlass/util/reference/host/tensor_copy.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/tensor_view_io.h"
 
-#include "gemm_testbed_3x.hpp"
-
-using namespace cute;
-
+#include "testbed_rank_k_universal.h"
 
-//#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Gemm_tf32n_tf32n_f32n_tensor_op_f32, 128x128x32_64x64x64) {
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
-    cutlass::tfloat32_t, cutlass::layout::ColumnMajor,
-    cutlass::tfloat32_t, cutlass::layout::ColumnMajor,
-    float, cutlass::layout::RowMajor,
-    float>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
+TEST(SM80_Device_Syrk_cf32n_cf32t_l_tensor_op_f32, 32x32x16_16x16x16) {
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+  using ElementA = cutlass::complex<float>;
+  using LayoutA = cutlass::layout::ColumnMajor;
 
-TEST(SM80_Device_Gemm_tf32n_tf32t_f32n_tensor_op_f32, 128x128x32_64x64x64) {
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
-    cutlass::tfloat32_t, cutlass::layout::ColumnMajor,
-    cutlass::tfloat32_t, cutlass::layout::RowMajor,
-    float, cutlass::layout::RowMajor,
-    float>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
+  using ElementC = cutlass::complex<float>;
+  using LayoutC = cutlass::layout::RowMajor;
+  using ElementAccumulator = cutlass::complex<float>;
+
+  using RankK = cutlass::gemm::device::RankK<
+    ElementA,
+    LayoutA,
+    ElementC,
+    LayoutC,
+    cutlass::FillMode::kLower,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
+    cutlass::gemm::GemmShape<16, 8, 8>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementC,
+      1,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    3,     // kStages 
+    1,     // AlignmentA
+    false, // SplitKSerial
+    cutlass::arch::OpMultiplyAddComplex,
+    cutlass::ComplexTransform::kNone,
+    cutlass::BlasMode::kSymmetric
   >;
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Gemm_tf32t_tf32n_f32n_tensor_op_f32, 128x128x32_64x64x64) {
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
-    cutlass::tfloat32_t, cutlass::layout::RowMajor,
-    cutlass::tfloat32_t, cutlass::layout::ColumnMajor,
-    float, cutlass::layout::RowMajor,
-    float>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
-  >;
+TEST(SM80_Device_Syrk_cf32n_cf32t_u_tensor_op_f32, 32x32x16_16x16x16) {
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
+  using ElementA = cutlass::complex<float>;
+  using LayoutA = cutlass::layout::ColumnMajor;
 
-TEST(SM80_Device_Gemm_tf32t_tf32t_f32n_tensor_op_f32, 128x128x32_64x64x64) {
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
-    cutlass::tfloat32_t, cutlass::layout::RowMajor,
-    cutlass::tfloat32_t, cutlass::layout::RowMajor,
-    float, cutlass::layout::RowMajor,
-    float>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
+  using ElementC = cutlass::complex<float>;
+  using LayoutC = cutlass::layout::RowMajor;
+  using ElementAccumulator = cutlass::complex<float>;
+
+  using RankK = cutlass::gemm::device::RankK<
+    ElementA,
+    LayoutA,
+    ElementC,
+    LayoutC,
+    cutlass::FillMode::kUpper,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
+    cutlass::gemm::GemmShape<16, 8, 8>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementC,
+      1,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    3,     // kStages 
+    1,     // AlignmentA
+    false, // SplitKSerial
+    cutlass::arch::OpMultiplyAddComplex,
+    cutlass::ComplexTransform::kNone,
+    cutlass::BlasMode::kSymmetric
   >;
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-//#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_bf16_bf16_bf16_alignx_tensor_op_f32.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_grouped_sm80.cu`

 * *Files 19% similar despite different names*

```diff
@@ -25,164 +25,144 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide GEMM interface
+    \brief Tests for grouped Rank2K interface
 */
 
 #include <iostream>
 
+#include "../../common/cutlass_unit_test.h"
 #include "cutlass/cutlass.h"
-#include "cute/tensor.hpp"
-#include "cute/atom/mma_atom.hpp"
-
-#include "cutlass/numeric_types.h"
 
+#include "cutlass/blas3.h"
 #include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/device/gemm_universal_adapter.h"
-#include "cutlass/gemm/kernel/gemm_universal.hpp"
-#include "cutlass/gemm/collective/collective_builder.hpp"
-#include "cutlass/epilogue/collective/default_epilogue.hpp"
-#include "cutlass/epilogue/thread/linear_combination.h"
+#include "cutlass/gemm/kernel/rank_2k_grouped.h"
+#include "cutlass/gemm/kernel/default_rank_2k_grouped.h"
+#include "cutlass/gemm/device/rank_2k_grouped.h"
 
-#include "../../common/cutlass_unit_test.h"
+#include "cutlass/util/host_tensor.h"
+#include "cutlass/util/reference/host/gemm.h"
+#include "cutlass/util/reference/host/tensor_compare.h"
+#include "cutlass/util/reference/host/tensor_copy.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/tensor_view_io.h"
 
-#include "gemm_testbed_3x.hpp"
+#include "testbed_grouped_rank_2k.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-using namespace cute;
+#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
-///////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Gemm_bf16t_bf16t_bf16n_align8_tensor_op_gmma_f32, 64x128x64) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::RowMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-    cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-    cutlass::bfloat16_t, LayoutA, 8,
-    cutlass::bfloat16_t, LayoutB, 8,
-    float,
-    Shape<_64,_128,_64>, Shape<_1,_1,_1>,
-    cutlass::gemm::collective::StageCountAuto,
-    cutlass::gemm::collective::KernelScheduleAuto
-  >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::bfloat16_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
+TEST(SM80_Device_Syr2kGrouped_cf64n_cf64t_l_tensor_op_f64, 32x32x16_16x16x16) {
 
-///////////////////////////////////////////////////////////////////////////////
-
-TEST(SM90_Device_Gemm_bf16t_bf16n_bf16n_align4_tensor_op_gmma_f32, 64x128x64) {
-  using LayoutA = cutlass::layout::RowMajor;
+  using ElementA = cutlass::complex<double>;
+  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementB = cutlass::complex<double>;
   using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::bfloat16_t, LayoutA, 4,
-      cutlass::bfloat16_t, LayoutB, 4,
-      float,
-      Shape<_64,_128,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::bfloat16_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+  using ElementC = cutlass::complex<double>;
+  using LayoutC = cutlass::layout::RowMajor;
+  using ElementAccumulator = cutlass::complex<double>;
+
+  using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
+    ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
+    ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
+    ElementC, LayoutC, cutlass::FillMode::kLower,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    3, // kStages
+    cutlass::arch::OpMultiplyAddComplex,
+    cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
+
+  using Rank2K = cutlass::gemm::device::Rank2KGrouped<Rank2Kkernel>;
+
+  test::gemm::device::TestbedGrouped<Rank2K> testbed;
+  bool passed = testbed.run(24);
+  EXPECT_TRUE(passed);
 }
 
-///////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Gemm_bf16n_bf16t_bf16n_align2_tensor_op_gmma_f32, 64x128x64) {
-  using LayoutA = cutlass::layout::ColumnMajor;
-  using LayoutB = cutlass::layout::RowMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::bfloat16_t, LayoutA, 2,
-      cutlass::bfloat16_t, LayoutB, 2,
-      float,
-      Shape<_64,_128,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::bfloat16_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
+TEST(SM80_Device_Syr2kGrouped_cf64n_cf64t_l_tensor_op_f64, 64x64x16_32x32x16) {
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+  using ElementA = cutlass::complex<double>;
+  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementB = cutlass::complex<double>;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = cutlass::complex<double>;
+  using LayoutC = cutlass::layout::RowMajor;
+  using ElementAccumulator = cutlass::complex<double>;
+
+  using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
+    ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
+    ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
+    ElementC, LayoutC, cutlass::FillMode::kLower,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<64, 64, 16>,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    3, // kStages
+    cutlass::arch::OpMultiplyAddComplex,
+    cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
+
+  using Rank2K = cutlass::gemm::device::Rank2KGrouped<Rank2Kkernel>;
+
+  test::gemm::device::TestbedGrouped<Rank2K> testbed;
+  bool passed = testbed.run(24);
+  EXPECT_TRUE(passed);
 }
 
-///////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Gemm_bf16n_bf16n_bf16n_align8_tensor_op_gmma_f32, 64x128x64) {
+TEST(SM80_Device_Syr2kGrouped_cf64n_cf64t_u_tensor_op_f64, 32x32x16_16x16x16) {
+
+  using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementB = cutlass::complex<double>;
   using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-    cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-    cutlass::bfloat16_t, LayoutA, 8,
-    cutlass::bfloat16_t, LayoutB, 8,
-    float,
-    Shape<_64,_128,_64>, Shape<_1,_1,_1>,
-    cutlass::gemm::collective::StageCountAuto,
-    cutlass::gemm::collective::KernelScheduleAuto
-  >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::bfloat16_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+  using ElementC = cutlass::complex<double>;
+  using LayoutC = cutlass::layout::RowMajor;
+  using ElementAccumulator = cutlass::complex<double>;
+
+  using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
+    ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
+    ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
+    ElementC, LayoutC, cutlass::FillMode::kUpper,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    3, // kStages
+    cutlass::arch::OpMultiplyAddComplex,
+    cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
+
+  using Rank2K = cutlass::gemm::device::Rank2KGrouped<Rank2Kkernel>;
+
+  test::gemm::device::TestbedGrouped<Rank2K> testbed;
+  bool passed = testbed.run(24);
+  EXPECT_TRUE(passed);
 }
 
-///////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
-#endif // defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_bf16_bf16_bf16_tensor_op_f32.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64n_tensor_op_f64_grouped_sm80.cu`

 * *Files 19% similar despite different names*

```diff
@@ -25,163 +25,144 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide GEMM interface
+    \brief Tests for grouped Rank2K interface
 */
 
 #include <iostream>
 
+#include "../../common/cutlass_unit_test.h"
 #include "cutlass/cutlass.h"
-#include "cute/tensor.hpp"
-#include "cute/atom/mma_atom.hpp"
 
-#include "cutlass/numeric_types.h"
+#include "cutlass/blas3.h"
+#include "cutlass/gemm/gemm.h"
+#include "cutlass/gemm/kernel/rank_2k_grouped.h"
+#include "cutlass/gemm/kernel/default_rank_2k_grouped.h"
+#include "cutlass/gemm/device/rank_2k_grouped.h"
 
-#include "cutlass/gemm/device/gemm_universal_adapter.h"
-#include "cutlass/gemm/kernel/gemm_universal.hpp"
-#include "cutlass/gemm/collective/collective_builder.hpp"
-#include "cutlass/epilogue/collective/default_epilogue.hpp"
-#include "cutlass/epilogue/thread/linear_combination.h"
+#include "cutlass/util/host_tensor.h"
+#include "cutlass/util/reference/host/gemm.h"
+#include "cutlass/util/reference/host/tensor_compare.h"
+#include "cutlass/util/reference/host/tensor_copy.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/tensor_view_io.h"
 
-#include "../../common/cutlass_unit_test.h"
+#include "testbed_grouped_rank_2k.h"
 
-#include "gemm_testbed_3x.hpp"
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
-using namespace cute;
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-///////////////////////////////////////////////////////////////////////////////
+TEST(SM80_Device_Syr2kGrouped_cf64n_cf64n_l_tensor_op_f64, 32x32x16_16x16x16) {
 
-TEST(SM90_Device_Gemm_bf16t_bf16t_bf16n_tensor_op_gmma_f32, 64x128x64) {
+  using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::RowMajor;
+  using ElementB = cutlass::complex<double>;
   using LayoutB = cutlass::layout::RowMajor;
+  using ElementC = cutlass::complex<double>;
   using LayoutC = cutlass::layout::ColumnMajor;
+  using ElementAccumulator = cutlass::complex<double>;
 
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::bfloat16_t, LayoutA, 8,
-      cutlass::bfloat16_t, LayoutB, 8,
-      float,
-      Shape<_64,_128,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::bfloat16_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+  using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
+    ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
+    ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
+    ElementC, LayoutC, cutlass::FillMode::kLower,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    3, // kStages
+    cutlass::arch::OpMultiplyAddComplex,
+    cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
+
+  using Rank2K = cutlass::gemm::device::Rank2KGrouped<Rank2Kkernel>;
+
+  test::gemm::device::TestbedGrouped<Rank2K> testbed;
+  bool passed = testbed.run(24);
+  EXPECT_TRUE(passed);
 }
 
-///////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM80_Device_Syr2kGrouped_cf64n_cf64n_l_tensor_op_f64, 64x64x16_32x32x16) {
 
-TEST(SM90_Device_Gemm_bf16t_bf16n_bf16n_tensor_op_gmma_f32, 64x128x64) {
+  using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementB = cutlass::complex<double>;
+  using LayoutB = cutlass::layout::RowMajor;
+  using ElementC = cutlass::complex<double>;
   using LayoutC = cutlass::layout::ColumnMajor;
+  using ElementAccumulator = cutlass::complex<double>;
 
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::bfloat16_t, LayoutA, 8,
-      cutlass::bfloat16_t, LayoutB, 8,
-      float,
-      Shape<_64,_128,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::bfloat16_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+  using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
+    ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
+    ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
+    ElementC, LayoutC, cutlass::FillMode::kLower,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<64, 64, 16>,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    3, // kStages
+    cutlass::arch::OpMultiplyAddComplex,
+    cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
+
+  using Rank2K = cutlass::gemm::device::Rank2KGrouped<Rank2Kkernel>;
+
+  test::gemm::device::TestbedGrouped<Rank2K> testbed;
+  bool passed = testbed.run(24);
+  EXPECT_TRUE(passed);
 }
 
-///////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM80_Device_Syr2kGrouped_cf64n_cf64n_u_tensor_op_f64, 32x32x16_16x16x16) {
 
-TEST(SM90_Device_Gemm_bf16n_bf16t_bf16n_tensor_op_gmma_f32, 64x128x64) {
-  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementA = cutlass::complex<double>;
+  using LayoutA = cutlass::layout::RowMajor;
+  using ElementB = cutlass::complex<double>;
   using LayoutB = cutlass::layout::RowMajor;
+  using ElementC = cutlass::complex<double>;
   using LayoutC = cutlass::layout::ColumnMajor;
+  using ElementAccumulator = cutlass::complex<double>;
 
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::bfloat16_t, LayoutA, 8,
-      cutlass::bfloat16_t, LayoutB, 8,
-      float,
-      Shape<_64,_128,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::bfloat16_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+  using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
+    ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
+    ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
+    ElementC, LayoutC, cutlass::FillMode::kUpper,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    3, // kStages
+    cutlass::arch::OpMultiplyAddComplex,
+    cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
+
+  using Rank2K = cutlass::gemm::device::Rank2KGrouped<Rank2Kkernel>;
+
+  test::gemm::device::TestbedGrouped<Rank2K> testbed;
+  bool passed = testbed.run(24);
+  EXPECT_TRUE(passed);
 }
 
-///////////////////////////////////////////////////////////////////////////////
-
-TEST(SM90_Device_Gemm_bf16n_bf16n_bf16n_tensor_op_gmma_f32, 64x128x64) {
-  using LayoutA = cutlass::layout::ColumnMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::bfloat16_t, LayoutA, 8,
-      cutlass::bfloat16_t, LayoutB, 8,
-      float,
-      Shape<_64,_128,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::bfloat16_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-///////////////////////////////////////////////////////////////////////////////
+#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
-#endif // defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/kernel/batched_gemv.cu`

 * *Files 26% similar despite different names*

```diff
@@ -24,1054 +24,1059 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/*! \file
-    \brief Tests for device-wide GEMM interface
-*/
 
-#include <iostream>
-
-#include "cutlass/cutlass.h"
-#include "cute/tensor.hpp"
-#include "cute/atom/mma_atom.hpp"
-
-#include "cutlass/numeric_types.h"
-
-#include "cutlass/gemm/device/gemm_universal_adapter.h"
-#include "cutlass/gemm/kernel/gemm_universal.hpp"
-#include "cutlass/gemm/collective/collective_builder.hpp"
-#include "cutlass/epilogue/collective/epilogue.hpp"
-#include "cutlass/epilogue/collective/default_epilogue.hpp"
-#include "cutlass/epilogue/thread/linear_combination.h"
-
-#include "../../common/cutlass_unit_test.h"
-
-#include "gemm_testbed_3x.hpp"
-
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
-
-using namespace cute;
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-TEST(SM90_Device_Gemm_f16t_f16t_f16n_tensor_op_gmma_f32, 64x128x64) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::RowMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_64,_128,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-TEST(SM90_Device_Gemm_f16t_f16t_f16n_tensor_op_gmma_f32, 128x128x32) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::RowMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_128,_128,_32>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-TEST(SM90_Device_Gemm_f16t_f16t_f16n_tensor_op_gmma_f32, 64x64x64) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::RowMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_64,_64,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-///////////////////////////////////////////////////////////////////////////////
-
-TEST(SM90_Device_Gemm_f16t_f16n_f16n_tensor_op_gmma_f32, 64x128x64) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_64,_128,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-TEST(SM90_Device_Gemm_f16t_f16n_f16n_tensor_op_gmma_f32, 128x128x32) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_128,_128,_32>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-TEST(SM90_Device_Gemm_f16t_f16n_f16n_tensor_op_gmma_f32, 64x64x64) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_64,_64,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-///////////////////////////////////////////////////////////////////////////////
-
-TEST(SM90_Device_Gemm_f16n_f16t_f16n_tensor_op_gmma_f32, 64x128x64) {
-  using LayoutA = cutlass::layout::ColumnMajor;
-  using LayoutB = cutlass::layout::RowMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_64,_128,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>>;
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-TEST(SM90_Device_Gemm_f16n_f16t_f16n_tensor_op_gmma_f32, 128x128x32) {
-  using LayoutA = cutlass::layout::ColumnMajor;
-  using LayoutB = cutlass::layout::RowMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_128,_128,_32>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-TEST(SM90_Device_Gemm_f16n_f16t_f16n_tensor_op_gmma_f32, 64x64x64) {
-  using LayoutA = cutlass::layout::ColumnMajor;
-  using LayoutB = cutlass::layout::RowMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_64,_64,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-///////////////////////////////////////////////////////////////////////////////
-
-TEST(SM90_Device_Gemm_f16n_f16n_f16n_tensor_op_gmma_f32, 64x128x64) {
-  using LayoutA = cutlass::layout::ColumnMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_64,_128,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-TEST(SM90_Device_Gemm_f16n_f16n_f16n_tensor_op_gmma_f32, 128x128x32) {
-  using LayoutA = cutlass::layout::ColumnMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_128,_128,_32>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-TEST(SM90_Device_Gemm_f16n_f16n_f16n_tensor_op_gmma_f32, 64x64x64) {
-  using LayoutA = cutlass::layout::ColumnMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_64,_64,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-///////////////////////////////////////////////////////////////////////////////
-///////////////////////////////////////////////////////////////////////////////
-///////////////////////////////////////////////////////////////////////////////
-
-TEST(SM90_Device_Gemm_f16t_f16t_f16n_tensor_op_gmma_f16, 64x128x64) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::RowMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      cutlass::half_t,
-      Shape<_64,_128,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, cutlass::half_t, cutlass::half_t>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-TEST(SM90_Device_Gemm_f16t_f16t_f16n_tensor_op_gmma_f16, 128x128x32) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::RowMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      cutlass::half_t,
-      Shape<_128,_128,_32>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, cutlass::half_t, cutlass::half_t>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-TEST(SM90_Device_Gemm_f16t_f16t_f16n_tensor_op_gmma_f16, 64x64x64) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::RowMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      cutlass::half_t,
-      Shape<_64,_64,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, cutlass::half_t, cutlass::half_t>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-///////////////////////////////////////////////////////////////////////////////
-
-TEST(SM90_Device_Gemm_f16t_f16n_f16n_tensor_op_gmma_f16, 64x128x64) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      cutlass::half_t,
-      Shape<_64,_128,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, cutlass::half_t, cutlass::half_t>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-TEST(SM90_Device_Gemm_f16t_f16n_f16n_tensor_op_gmma_f16, 128x128x32) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      cutlass::half_t,
-      Shape<_128,_128,_32>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, cutlass::half_t, cutlass::half_t>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-TEST(SM90_Device_Gemm_f16t_f16n_f16n_tensor_op_gmma_f16, 64x64x64) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      cutlass::half_t,
-      Shape<_64,_64,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, cutlass::half_t, cutlass::half_t>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-///////////////////////////////////////////////////////////////////////////////
-
-TEST(SM90_Device_Gemm_f16n_f16t_f16n_tensor_op_gmma_f16, 64x128x64) {
-  using LayoutA = cutlass::layout::ColumnMajor;
-  using LayoutB = cutlass::layout::RowMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      cutlass::half_t,
-      Shape<_64,_128,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, cutlass::half_t, cutlass::half_t>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-TEST(SM90_Device_Gemm_f16n_f16t_f16n_tensor_op_gmma_f16, 128x128x32) {
-  using LayoutA = cutlass::layout::ColumnMajor;
-  using LayoutB = cutlass::layout::RowMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      cutlass::half_t,
-      Shape<_128,_128,_32>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, cutlass::half_t, cutlass::half_t>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-TEST(SM90_Device_Gemm_f16n_f16t_f16n_tensor_op_gmma_f16, 64x64x64) {
-  using LayoutA = cutlass::layout::ColumnMajor;
-  using LayoutB = cutlass::layout::RowMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      cutlass::half_t,
-      Shape<_64,_64,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, cutlass::half_t, cutlass::half_t>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-///////////////////////////////////////////////////////////////////////////////
-
-TEST(SM90_Device_Gemm_f16n_f16n_f16n_tensor_op_gmma_f16, 64x128x64) {
-  using LayoutA = cutlass::layout::ColumnMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      cutlass::half_t,
-      Shape<_64,_128,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, cutlass::half_t, cutlass::half_t>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-TEST(SM90_Device_Gemm_f16n_f16n_f16n_tensor_op_gmma_f16, 128x128x32) {
-  using LayoutA = cutlass::layout::ColumnMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      cutlass::half_t,
-      Shape<_128,_128,_32>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, cutlass::half_t, cutlass::half_t>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-TEST(SM90_Device_Gemm_f16n_f16n_f16n_tensor_op_gmma_f16, 64x64x64) {
-  using LayoutA = cutlass::layout::ColumnMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      cutlass::half_t,
-      Shape<_64,_64,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, cutlass::half_t, cutlass::half_t>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
+#include "testbed_gemv.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Gemm_f16t_f16n_f16n_tensor_op_gmma_f16_Epilogue, 64x128x64) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      cutlass::half_t,
-      Shape<_64,_128,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::Epilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, cutlass::half_t, cutlass::half_t>,
-      ComposedLayout<Swizzle<3,4,3>, smem_ptr_flag_bits<sizeof_bits<cutlass::half_t>::value>, Layout<Shape<_64,_128>,Stride<_1,_64>>>,
-      Copy_Atom<SM90_U16x8_STSM_T, cutlass::half_t>,
-      TiledCopy<Copy_Atom<DefaultCopy, cutlass::half_t>,Layout<Shape<_128,_8>,Stride<_8,_1>>,Shape<_64,_16>>,
-      Copy_Atom<DefaultCopy, cutlass::half_t>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-TEST(SM90_Device_Gemm_f16t_f16n_f16n_tensor_op_gmma_f16_Epilogue, 128x64x64) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      cutlass::half_t,
-      Shape<_128,_64,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::Epilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, cutlass::half_t, cutlass::half_t>,
-      ComposedLayout<Swizzle<3,4,3>, smem_ptr_flag_bits<sizeof_bits<cutlass::half_t>::value>, Layout<Shape<Shape<_64,_2>,_64>,Stride<Stride<_1,_4096>,_64>>>,
-      Copy_Atom<SM90_U16x8_STSM_T, cutlass::half_t>,
-      TiledCopy<Copy_Atom<DefaultCopy, cutlass::half_t>,Layout<Shape<_128,_8>,Stride<_8,_1>>,Shape<_128,_8>>,
-      Copy_Atom<DefaultCopy, cutlass::half_t>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+TEST(SM50_batched_gemv, 1x64x64x1_1x64x4x1_1x4x4x1_rcr_fp32_fp32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 64, 1);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 1;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    float, float, float,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x64x64x4_1x64x4x2_1x4x4x2_rcr_fp32_fp32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 64, 4);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 2;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    float, float, float,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x256x256x64_1x64x4x8_1x4x4x8_rcr_fp32_fp32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 256, 256, 64);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 8;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    float, float, float,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x7x256x4096_1x8x4x64_1x1x4x64_rcr_fp32_fp32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 7, 256, 4096);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 8, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 1, 4>;
+  static int const kBatchTileSize = 64;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    float, float, float,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x64x27x4096_1x8x1x64_1x1x1x64_rcr_alpha_fp32_fp32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 27, 4096);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 8, 1>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 1, 1>;
+  static int const kBatchTileSize = 64;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    float, float, float,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    kBatchTileSize>(problem_size, -0.5f);
+}
+
+TEST(SM50_batched_gemv, 1x64x27x4096_1x8x1x64_1x1x1x64_rcr_alpha_beta_fp32_fp32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 27, 4096);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 8, 1>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 1, 1>;
+  static int const kBatchTileSize = 64;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    float, float, float,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    kBatchTileSize>(problem_size, 4.5f, -0.5f);
+}
+
+TEST(SM50_batched_gemv, 1x64x24x4096_1x8x4x64_1x1x4x64_rcr_alpha_beta_fp16_fp16)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 24, 4096);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 8, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 1, 4>;
+  static int const kBatchTileSize = 64;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    cutlass::half_t, float, cutlass::half_t,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    kBatchTileSize>(problem_size, cutlass::half_t(4.5f), cutlass::half_t(-0.5f));
+}
+
+///
+
+TEST(SM50_batched_gemv, 1x64x64x1_1x64x4x1_1x4x4x1_rcr_fp16_fp32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 64, 1);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 1;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    cutlass::half_t, float, float,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x64x64x4_1x64x4x2_1x4x4x2_rcr_fp16_fp32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 64, 4);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 2;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    cutlass::half_t, float, float,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x256x256x64_1x64x4x8_1x4x4x8_rcr_fp16_fp32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 256, 256, 64);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 8;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    cutlass::half_t, float, float,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x7x256x4096_1x8x4x64_1x1x4x64_rcr_fp16_fp32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 7, 256, 4096);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 8, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 1, 4>;
+  static int const kBatchTileSize = 64;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    cutlass::half_t, float, float,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+///
+
+TEST(SM50_batched_gemv, 1x64x64x1_1x64x4x1_1x4x4x1_rcr_fp16_fp16)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 64, 1);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 1;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    cutlass::half_t, float, cutlass::half_t,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x64x64x4_1x64x4x2_1x4x4x2_rcr_fp16_fp16)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 64, 4);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 2;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    cutlass::half_t, float, cutlass::half_t,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x256x256x64_1x64x4x8_1x4x4x8_rcr_fp16_fp16)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 256, 256, 64);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 8;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    cutlass::half_t, float, cutlass::half_t,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x7x256x4096_1x8x4x64_1x1x4x64_rcr_fp16_fp16)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 7, 256, 4096);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 8, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 1, 4>;
+  static int const kBatchTileSize = 64;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    cutlass::half_t, float, cutlass::half_t,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+///
+
+TEST(SM50_batched_gemv, 1x64x64x1_1x64x4x1_1x4x4x1_rcr_i8_i32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 64, 1);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 1;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    int8_t, int32_t, int32_t,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x64x64x4_1x64x4x2_1x4x4x2_rcr_i8_i32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 64, 4);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 2;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    int8_t, int32_t, int32_t,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x256x256x64_1x64x4x8_1x4x4x8_rcr_i8_i32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 256, 256, 64);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 8;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    int8_t, int32_t, int32_t,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x7x256x4096_1x8x4x64_1x1x4x64_rcr_i8_i32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 7, 256, 4096);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 8, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 1, 4>;
+  static int const kBatchTileSize = 64;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    int8_t, int32_t, int32_t,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+/////////////
+
+TEST(SM50_batched_gemv, 1x64x64x1_1x64x4x1_1x4x4x1_crc_fp32_fp32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 64, 1);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 1;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    float, float, float,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x64x64x4_1x64x4x2_1x4x4x2_crc_fp32_fp32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 64, 4);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 2;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    float, float, float,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x256x256x64_1x64x4x8_1x4x4x8_crc_fp32_fp32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 256, 256, 64);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 8;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    float, float, float,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x7x256x4096_1x8x4x64_1x1x4x64_crc_fp32_fp32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 7, 256, 4096);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 8, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 1, 4>;
+  static int const kBatchTileSize = 64;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    float, float, float,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+///
+
+TEST(SM50_batched_gemv, 1x64x64x1_1x64x4x1_1x4x4x1_crc_fp16_fp32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 64, 1);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 1;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    cutlass::half_t, float, float,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x64x64x4_1x64x4x2_1x4x4x2_crc_fp16_fp32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 64, 4);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 2;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    cutlass::half_t, float, float,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x256x256x64_1x64x4x8_1x4x4x8_crc_fp16_fp32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 256, 256, 64);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 8;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    cutlass::half_t, float, float,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x7x256x4096_1x8x4x64_1x1x4x64_crc_fp16_fp32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 7, 256, 4096);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 8, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 1, 4>;
+  static int const kBatchTileSize = 64;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    cutlass::half_t, float, float,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+///
+
+TEST(SM50_batched_gemv, 1x64x64x1_1x64x4x1_1x4x4x1_crc_fp16_fp16)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 64, 1);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 1;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    cutlass::half_t, float, cutlass::half_t,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x64x64x4_1x64x4x2_1x4x4x2_crc_fp16_fp16)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 64, 4);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 2;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    cutlass::half_t, float, cutlass::half_t,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x256x256x64_1x64x4x8_1x4x4x8_crc_fp16_fp16)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 256, 256, 64);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 8;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    cutlass::half_t, float,  cutlass::half_t,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x7x256x4096_1x8x4x64_1x1x4x64_crc_fp16_fp16)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 7, 256, 4096);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 8, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 1, 4>;
+  static int const kBatchTileSize = 64;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    cutlass::half_t, float, cutlass::half_t,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+///
+
+TEST(SM50_batched_gemv, 1x64x64x1_1x64x4x1_1x4x4x1_crc_i8_i32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 64, 1);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 1;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    int8_t, int32_t, int32_t,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x64x64x4_1x64x4x2_1x4x4x2_crc_i8_i32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 64, 4);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 2;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    int8_t, int32_t, int32_t,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x256x256x64_1x64x4x8_1x4x4x8_crc_i8_i32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 256, 256, 64);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 8;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    int8_t, int32_t, int32_t,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x7x256x4096_1x8x4x64_1x1x4x64_crc_i8_i32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 7, 256, 4096);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 8, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 1, 4>;
+  static int const kBatchTileSize = 64;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    int8_t, int32_t, int32_t,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x64x27x4096_1x8x1x64_1x1x1x64_crc_alpha_fp32_fp32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 27, 4096);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 8, 1>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 1, 1>;
+  static int const kBatchTileSize = 64;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    float, float, float,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size, -0.5f);
+}
+
+TEST(SM50_batched_gemv, 1x64x27x4096_1x8x1x64_1x1x1x64_crc_alpha_beta_fp32_fp32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 27, 4096);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 8, 1>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 1, 1>;
+  static int const kBatchTileSize = 64;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    float, float, float,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size, 4.5f, -0.5f);
+}
+
+TEST(SM50_batched_gemv, 1x64x24x4096_1x8x4x64_1x1x4x64_crc_alpha_beta_fp16_fp16)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 24, 4096);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 8, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 1, 4>;
+  static int const kBatchTileSize = 64;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    cutlass::half_t, float, cutlass::half_t,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size, cutlass::half_t(4.5f), cutlass::half_t(-0.5f));
+}
+
+/////////////
+
+TEST(SM50_batched_gemv, 1x64x64x1_1x64x4x1_1x4x4x1_rcc_fp32_fp32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 64, 1);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 1;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    float, float, float,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x64x64x4_1x64x4x2_1x4x4x2_rcc_fp32_fp32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 64, 4);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 2;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    float, float, float,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x256x256x64_1x64x4x8_1x4x4x8_rcc_fp32_fp32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 256, 256, 64);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 8;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    float, float, float,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x7x256x4096_1x8x4x64_1x1x4x64_rcc_fp32_fp32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 7, 256, 4096);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 8, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 1, 4>;
+  static int const kBatchTileSize = 64;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    float, float, float,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+///
+
+TEST(SM50_batched_gemv, 1x64x64x1_1x64x4x1_1x4x4x1_rcc_fp16_fp32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 64, 1);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 1;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    cutlass::half_t, float, float,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x64x64x4_1x64x4x2_1x4x4x2_rcc_fp16_fp32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 64, 4);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 2;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    cutlass::half_t, float, float,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x256x256x64_1x64x4x8_1x4x4x8_rcc_fp16_fp32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 256, 256, 64);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 8;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    cutlass::half_t, float, float,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x7x256x4096_1x8x4x64_1x1x4x64_rcc_fp16_fp32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 7, 256, 4096);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 8, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 1, 4>;
+  static int const kBatchTileSize = 64;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    cutlass::half_t, float, float,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+///
+
+TEST(SM50_batched_gemv, 1x64x64x1_1x64x4x1_1x4x4x1_rcc_fp16_fp16)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 64, 1);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 1;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    cutlass::half_t, float, cutlass::half_t,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x64x64x4_1x64x4x2_1x4x4x2_rcc_fp16_fp16)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 64, 4);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 2;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    cutlass::half_t, float, cutlass::half_t,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x256x256x64_1x64x4x8_1x4x4x8_rcc_fp16_fp16)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 256, 256, 64);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 8;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    cutlass::half_t, float,  cutlass::half_t,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x7x256x4096_1x8x4x64_1x1x4x64_rcc_fp16_fp16)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 7, 256, 4096);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 8, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 1, 4>;
+  static int const kBatchTileSize = 64;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    cutlass::half_t, float, cutlass::half_t,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+///
+
+TEST(SM50_batched_gemv, 1x64x64x1_1x64x4x1_1x4x4x1_rcc_i8_i32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 64, 1);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 1;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    int8_t, int32_t, int32_t,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x64x64x4_1x64x4x2_1x4x4x2_rcc_i8_i32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 64, 4);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 2;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    int8_t, int32_t, int32_t,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x256x256x64_1x64x4x8_1x4x4x8_rcc_i8_i32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 256, 256, 64);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 64, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 4, 4>;
+  static int const kBatchTileSize = 8;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    int8_t, int32_t, int32_t,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x7x256x4096_1x8x4x64_1x1x4x64_rcc_i8_i32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 7, 256, 4096);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 8, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 1, 4>;
+  static int const kBatchTileSize = 64;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    int8_t, int32_t, int32_t,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size);
+}
+
+TEST(SM50_batched_gemv, 1x64x27x4096_1x8x1x64_1x1x1x64_rcc_alpha_fp32_fp32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 27, 4096);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 8, 1>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 1, 1>;
+  static int const kBatchTileSize = 64;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    float, float, float,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size, -0.5f);
+}
+
+TEST(SM50_batched_gemv, 1x64x27x4096_1x8x1x64_1x1x1x64_rcc_alpha_beta_fp32_fp32)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 27, 4096);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 8, 1>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 1, 1>;
+  static int const kBatchTileSize = 64;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    float, float, float,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size, 4.5f, -0.5f);
+}
+
+TEST(SM50_batched_gemv, 1x64x24x4096_1x8x4x64_1x1x4x64_rcc_alpha_beta_fp16_fp16)
+{
+  cutlass::gemm::BatchedGemmCoord problem_size(1, 64, 24, 4096);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<1, 8, 4>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, 1, 4>;
+  static int const kBatchTileSize = 64;
+
+  test::gemm::kernel::batched_gemv_kernel_test<
+                                    ThreadBlockShape,
+                                    ThreadShape,
+                                    cutlass::half_t, float, cutlass::half_t,
+                                    cutlass::layout::RowMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    cutlass::layout::ColumnMajor,
+                                    kBatchTileSize>(problem_size, cutlass::half_t(4.5f), cutlass::half_t(-0.5f));
 }
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-TEST(SM90_Device_Gemm_f16t_f16n_f16t_tensor_op_gmma_f16_Epilogue, 64x128x64) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::RowMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      cutlass::half_t,
-      Shape<_64,_128,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::Epilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, cutlass::half_t, cutlass::half_t>,
-      ComposedLayout<Swizzle<3,4,3>, smem_ptr_flag_bits<sizeof_bits<cutlass::half_t>::value>, Layout<Shape<_64,Shape<_64,_2>>,Stride<_64,Stride<_1,_4096>>>>,
-      Copy_Atom<SM90_U32x4_STSM_N, cutlass::half_t>,
-      TiledCopy<Copy_Atom<DefaultCopy, cutlass::half_t>,Layout<Shape<_128,_8>,Stride<_8,_1>>,Shape<_8,_128>>,
-      Copy_Atom<DefaultCopy, cutlass::half_t>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-TEST(SM90_Device_Gemm_f16t_f16n_f16t_tensor_op_gmma_f16_Epilogue, 128x64x64) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::RowMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      cutlass::half_t,
-      Shape<_128,_64,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::Epilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, cutlass::half_t, cutlass::half_t>,
-      ComposedLayout<Swizzle<3,4,3>, smem_ptr_flag_bits<sizeof_bits<cutlass::half_t>::value>, Layout<Shape<_128,_64>,Stride<_64,_1>>>,
-      Copy_Atom<SM90_U32x4_STSM_N, cutlass::half_t>,
-      TiledCopy<Copy_Atom<DefaultCopy, cutlass::half_t>,Layout<Shape<_128,_8>,Stride<_8,_1>>,Shape<_16,_64>>,
-      Copy_Atom<DefaultCopy, cutlass::half_t>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-TEST(SM90_Device_Gemm_f16t_f16n_f16n_tensor_op_gmma_f32_Epilogue, 64x128x64) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_64,_128,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::Epilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>,
-      ComposedLayout<Swizzle<3,4,3>, smem_ptr_flag_bits<sizeof_bits<float>::value>, Layout<Shape<_64,_128>,Stride<_1,_64>>>,
-      Copy_Atom<DefaultCopy, float>,
-      TiledCopy<Copy_Atom<DefaultCopy, float>,Layout<Shape<_128,_8>,Stride<_8,_1>>,Shape<_64,_16>>,
-      Copy_Atom<DefaultCopy, cutlass::half_t>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-TEST(SM90_Device_Gemm_f16t_f16n_f16n_tensor_op_gmma_f32_Epilogue, 128x64x64) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_128,_64,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::Epilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>,
-      ComposedLayout<Swizzle<3,4,3>, smem_ptr_flag_bits<sizeof_bits<float>::value>, Layout<Shape<Shape<_64,_2>,_64>,Stride<Stride<_1,_4096>,_64>>>,
-      Copy_Atom<DefaultCopy, float>,
-      TiledCopy<Copy_Atom<DefaultCopy, float>,Layout<Shape<_128,_8>,Stride<_8,_1>>,Shape<_128,_8>>,
-      Copy_Atom<DefaultCopy, cutlass::half_t>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-TEST(SM90_Device_Gemm_f16t_f16n_f16t_tensor_op_gmma_f32_Epilogue, 64x128x64) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::RowMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_64,_128,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::Epilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>,
-      ComposedLayout<Swizzle<3,4,3>, smem_ptr_flag_bits<sizeof_bits<float>::value>, Layout<Shape<_64,Shape<_64,_2>>,Stride<_64,Stride<_1,_4096>>>>,
-      Copy_Atom<DefaultCopy, float>,
-      TiledCopy<Copy_Atom<DefaultCopy, float>,Layout<Shape<_128,_8>,Stride<_8,_1>>,Shape<_8,_128>>,
-      Copy_Atom<DefaultCopy, cutlass::half_t>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-TEST(SM90_Device_Gemm_f16t_f16n_f16t_tensor_op_gmma_f32_Epilogue, 128x64x64) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::RowMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_128,_64,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::Epilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>,
-      ComposedLayout<Swizzle<3,4,3>, smem_ptr_flag_bits<sizeof_bits<float>::value>, Layout<Shape<_128,_64>,Stride<_64,_1>>>,
-      Copy_Atom<DefaultCopy, float>,
-      TiledCopy<Copy_Atom<DefaultCopy, float>,Layout<Shape<_128,_8>,Stride<_8,_1>>,Shape<_16,_64>>,
-      Copy_Atom<DefaultCopy, cutlass::half_t>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-#endif // defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_unspecialized.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_complex_sm80.cu`

 * *Files 27% similar despite different names*

```diff
@@ -24,559 +24,675 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/*! \file
-    \brief Tests for device-wide GEMM interface
-*/
+/*! \file 
 
-#include <iostream>
+    \brief Unit tests for thread-level GEMM
+*/
 
 #include "cutlass/cutlass.h"
-#include "cute/tensor.hpp"
-#include "cute/atom/mma_atom.hpp"
-
-#include "cutlass/numeric_types.h"
-
-#include "cutlass/gemm/device/gemm_universal_adapter.h"
-#include "cutlass/gemm/kernel/gemm_universal.hpp"
-#include "cutlass/gemm/collective/collective_builder.hpp"
-#include "cutlass/epilogue/collective/default_epilogue.hpp"
-#include "cutlass/epilogue/thread/linear_combination.h"
-
 #include "../../common/cutlass_unit_test.h"
 
-#include "gemm_testbed_3x.hpp"
-
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#include "cutlass/aligned_buffer.h"
+#include "cutlass/half.h"
 
-using namespace cute;
+#include "cutlass/gemm/warp/default_mma_complex_tensor_op.h"
 
-///////////////////////////////////////////////////////////////////////////////
-/////////////////////////////// Cluster 2x2x1  ////////////////////////////////
-///////////////////////////////////////////////////////////////////////////////
+#include "cutlass/core_io.h"
+#include "cutlass/util/host_tensor.h"
+#include "cutlass/util/tensor_view_io.h"
+
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/reference/host/tensor_compare.h"
+#include "cutlass/util/reference/host/gemm.h"
+
+#include "testbed.h"
+
+#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+
+////////////////////////////////////////////////////////////////////////////////////////////////////
+// complex<double> * complex<double> => complex<double>
+// Input data type: complex<double>
+// Math instruction: mma.sync.aligned.m8n8k4.f64.f64.f64.f64
+// Output data type: complex<double>
+///////////////////////////////////////////////////////////////////////////////////////////////////
+TEST(SM80_warp_gemm_complex_tensor_op_f64, 8x8x4_8x8x4_nt) {
+
+  using Shape = cutlass::gemm::GemmShape<8, 8, 4>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
+  
+  using Element = cutlass::complex<double>;
+  using ElementC = cutlass::complex<double>;
+
+  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous128b;
+  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous128b;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor
+  >::Type;
+
+  test::gemm::warp::TestbedComplex<MmaTensorOp, cutlass::gemm::GemmShape<8, 8, 4> >().run();
+}
+
+TEST(SM80_warp_gemm_complex_tensor_op_f64, 16x16x4_8x8x4_nt) {
+
+  using Shape = cutlass::gemm::GemmShape<16, 16, 4>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
+  
+  using Element = cutlass::complex<double>;
+  using ElementC = cutlass::complex<double>;
+
+  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous128b;
+  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous128b;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor
+  >::Type;
+
+  test::gemm::warp::TestbedComplex<MmaTensorOp, cutlass::gemm::GemmShape<16, 16, 4> >().run();
+}
+
+TEST(SM80_warp_gemm_complex_tensor_op_f64, 16x32x4_8x8x4_nt) {
+
+  using Shape = cutlass::gemm::GemmShape<16, 32, 4>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
+  
+  using Element = cutlass::complex<double>;
+  using ElementC = cutlass::complex<double>;
+
+  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous128b;
+  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous128b;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor
+  >::Type;
+
+  test::gemm::warp::TestbedComplex<MmaTensorOp, cutlass::gemm::GemmShape<16, 32, 4> >().run();
+}
+
+TEST(SM80_warp_gemm_complex_tensor_op_f64, 32x16x4_8x8x4_nt) {
+
+  using Shape = cutlass::gemm::GemmShape<32, 16, 4>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
+  
+  using Element = cutlass::complex<double>;
+  using ElementC = cutlass::complex<double>;
+
+  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous128b;
+  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous128b;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor
+  >::Type;
+
+  test::gemm::warp::TestbedComplex<MmaTensorOp, cutlass::gemm::GemmShape<32, 16, 4> >().run();
+}
+
+TEST(SM80_warp_gemm_complex_tensor_op_f64, 32x32x4_8x8x4_nt) {
+
+  using Shape = cutlass::gemm::GemmShape<32, 32, 4>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
+  
+  using Element = cutlass::complex<double>;
+  using ElementC = cutlass::complex<double>;
+
+  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous128b;
+  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous128b;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor
+  >::Type;
+
+  test::gemm::warp::TestbedComplex<MmaTensorOp, cutlass::gemm::GemmShape<32, 32, 4> >().run();
+}
+
+TEST(SM80_warp_gemm_complex_tensor_op_f64, 32x32x4_8x8x4_nh) {
+
+  using Shape = cutlass::gemm::GemmShape<32, 32, 4>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
+  
+  using Element = cutlass::complex<double>;
+  using ElementC = cutlass::complex<double>;
+
+  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous128b;
+  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous128b;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor,
+    cutlass::ComplexTransform::kNone,
+    cutlass::ComplexTransform::kConjugate
+  >::Type;
+
+  test::gemm::warp::TestbedComplex<MmaTensorOp, cutlass::gemm::GemmShape<32, 32, 4> >().run();
+}
+
+TEST(SM80_warp_gemm_complex_tensor_op_f64, 32x32x4_8x8x4_ct) {
+
+  using Shape = cutlass::gemm::GemmShape<32, 32, 4>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
+  
+  using Element = cutlass::complex<double>;
+  using ElementC = cutlass::complex<double>;
+
+  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous128b;
+  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous128b;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor,
+    cutlass::ComplexTransform::kConjugate,
+    cutlass::ComplexTransform::kNone
+  >::Type;
+
+  test::gemm::warp::TestbedComplex<MmaTensorOp, cutlass::gemm::GemmShape<32, 32, 4> >().run();
+}
+
+TEST(SM80_warp_gemm_complex_tensor_op_f64, 8x8x4_8x8x4_tn) {
+
+  using Shape = cutlass::gemm::GemmShape<8, 8, 4>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
+  
+  using Element = cutlass::complex<double>;
+  using ElementC = cutlass::complex<double>;
+
+  using LayoutA = cutlass::layout::RowMajorTensorOpMultiplicandCrosswise128x4;
+  using LayoutB = cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise128x4;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor
+  >::Type;
+
+  test::gemm::warp::TestbedComplex<MmaTensorOp, cutlass::gemm::GemmShape<8, 8, 4> >().run();
+}
+
+TEST(SM80_warp_gemm_complex_tensor_op_f64, 16x16x4_8x8x4_tn) {
+
+  using Shape = cutlass::gemm::GemmShape<16, 16, 4>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
+  
+  using Element = cutlass::complex<double>;
+  using ElementC = cutlass::complex<double>;
+
+  using LayoutA = cutlass::layout::RowMajorTensorOpMultiplicandCrosswise128x4;
+  using LayoutB = cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise128x4;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor
+  >::Type;
+
+  test::gemm::warp::TestbedComplex<MmaTensorOp, cutlass::gemm::GemmShape<16, 16, 4> >().run();
+}
+
+///////////////////////////////////////////////////////////////////////////////////////////////////
+
+///////////////////////////////////////////////////////////////////////////////////////////////////
+// complex<float> * complex<float> => complex<float>
+// Input data type: complex<float>
+// Math instruction: mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32
+// Output data type: complex<float>
+// Shared memory layout: Congrous
+////////////////////////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM80_warp_gemm_complex_tensor_op_f32, 16x16x8_16x8x8_nt) {
+
+  using Shape = cutlass::gemm::GemmShape<16, 16, 8>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 8>;
+  
+  using Element = cutlass::complex<float>;
+  using ElementC = cutlass::complex<float>;
+
+  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous64b;
+  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous64b;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor
+  >::Type;
+
+  test::gemm::warp::TransformedTestbedComplex<
+      MmaTensorOp, cutlass::gemm::GemmShape<16, 16, 8> >()
+      .run();
+}
+
+TEST(SM80_warp_gemm_complex_tensor_op_f32, 16x16x16_16x8x8_nt) {
+
+  using Shape = cutlass::gemm::GemmShape<16, 16, 16>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 8>;
+  
+  using Element = cutlass::complex<float>;
+  using ElementC = cutlass::complex<float>;
+
+  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous64b;
+  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous64b;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor
+  >::Type;
+
+  test::gemm::warp::TransformedTestbedComplex<
+      MmaTensorOp, cutlass::gemm::GemmShape<16, 16, 16> >()
+      .run();
+}
+
+TEST(SM80_warp_gemm_complex_tensor_op_f32, 16x32x8_16x8x8_nt) {
+
+  using Shape = cutlass::gemm::GemmShape<16, 32, 8>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 8>;
+  
+  using Element = cutlass::complex<float>;
+  using ElementC = cutlass::complex<float>;
+
+  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous64b;
+  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous64b;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor
+  >::Type;
+
+  test::gemm::warp::TransformedTestbedComplex<
+      MmaTensorOp, cutlass::gemm::GemmShape<16, 32, 8> >()
+      .run();
+}
+
+TEST(SM80_warp_gemm_complex_tensor_op_f32, 32x16x8_16x16x8_nt) {
+
+  using Shape = cutlass::gemm::GemmShape<32, 16, 8>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 8>;
+  
+  using Element = cutlass::complex<float>;
+  using ElementC = cutlass::complex<float>;
+
+  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous64b;
+  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous64b;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor
+  >::Type;
+
+  test::gemm::warp::TransformedTestbedComplex<
+      MmaTensorOp, cutlass::gemm::GemmShape<32, 16, 8> >()
+      .run();
+}
+
+
+TEST(SM80_warp_gemm_complex_tensor_op_f32, 32x32x8_16x8x8_nt) {
+
+  using Shape = cutlass::gemm::GemmShape<32, 32, 8>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 8>;
+  
+  using Element = cutlass::complex<float>;
+  using ElementC = cutlass::complex<float>;
+
+  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous64b;
+  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous64b;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor
+  >::Type;
+
+  test::gemm::warp::TransformedTestbedComplex<
+      MmaTensorOp, cutlass::gemm::GemmShape<32, 32, 8> >()
+      .run();
+}
+
+TEST(SM80_warp_gemm_complex_tensor_op_f32, 32x32x8_16x8x8_nh) {
+
+  using Shape = cutlass::gemm::GemmShape<32, 32, 8>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 8>;
+  
+  using Element = cutlass::complex<float>;
+  using ElementC = cutlass::complex<float>;
+
+  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous64b;
+  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous64b;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor,
+    cutlass::ComplexTransform::kNone,
+    cutlass::ComplexTransform::kConjugate
+  >::Type;
+
+  test::gemm::warp::TransformedTestbedComplex<
+      MmaTensorOp, cutlass::gemm::GemmShape<32, 32, 8> >()
+      .run();
+}
+
+TEST(SM80_warp_gemm_complex_tensor_op_f32, 32x32x8_16x8x8_ct) {
+
+  using Shape = cutlass::gemm::GemmShape<32, 32, 8>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 8>;
+  
+  using Element = cutlass::complex<float>;
+  using ElementC = cutlass::complex<float>;
+
+  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous64b;
+  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous64b;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor,
+    cutlass::ComplexTransform::kConjugate,
+    cutlass::ComplexTransform::kNone
+  >::Type;
+
+  test::gemm::warp::TransformedTestbedComplex<
+      MmaTensorOp, cutlass::gemm::GemmShape<32, 32, 8> >()
+      .run();
+}
+
+///////////////////////////////////////////////////////////////////////////////////////////////////
+// complex<float> * complex<float> => complex<float>
+// Input data type: complex<float>
+// Math instruction: mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32
+// Output data type: complex<float>
+// Shared memory layout: Crosswise
+////////////////////////////////////////////////////////////////////////////////////////////////////
+TEST(SM80_warp_gemm_complex_tensor_op_f32, 16x16x8_16x8x8_tn) {
+
+  using Shape = cutlass::gemm::GemmShape<16, 16, 8>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 8>;
+  
+  using Element = cutlass::complex<float>;
+  using ElementC = cutlass::complex<float>;
+
+  using LayoutA = cutlass::layout::RowMajorTensorOpMultiplicand64bCrosswise;
+  using LayoutB = cutlass::layout::ColumnMajorTensorOpMultiplicand64bCrosswise;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor
+  >::Type;
+
+  test::gemm::warp::TransformedTestbedComplex<
+      MmaTensorOp, cutlass::gemm::GemmShape<16, 16, 8> >()
+      .run();
+}
+
+// TEST FAILS crosswise complex<float> TN mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32 test fails for k = 2*8 = 16
+TEST(SM80_warp_gemm_complex_tensor_op_f32, 16x16x16_16x8x8_tn) {
+
+  using Shape = cutlass::gemm::GemmShape<16, 16, 16>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 8>;
+  
+  using Element = cutlass::complex<float>;
+  using ElementC = cutlass::complex<float>;
+
+  using LayoutA = cutlass::layout::RowMajorTensorOpMultiplicand64bCrosswise;
+  using LayoutB = cutlass::layout::ColumnMajorTensorOpMultiplicand64bCrosswise;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor
+  >::Type;
+
+  test::gemm::warp::TransformedTestbedComplex<
+      MmaTensorOp, cutlass::gemm::GemmShape<16, 16, 16> >()
+      .run();
+}
+
+TEST(SM80_warp_gemm_complex_tensor_op_f32, 32x32x8_16x8x8_tn) {
+
+  using Shape = cutlass::gemm::GemmShape<32, 32, 8>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 8>;
+  
+  using Element = cutlass::complex<float>;
+  using ElementC = cutlass::complex<float>;
+
+  using LayoutA = cutlass::layout::RowMajorTensorOpMultiplicand64bCrosswise;
+  using LayoutB = cutlass::layout::ColumnMajorTensorOpMultiplicand64bCrosswise;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor
+  >::Type;
+
+  test::gemm::warp::TransformedTestbedComplex<
+      MmaTensorOp, cutlass::gemm::GemmShape<32, 32, 8> >()
+      .run();
+}
+
+TEST(SM80_warp_gemm_complex_tensor_op_f32, 32x64x8_16x8x8_tn) {
+
+  using Shape = cutlass::gemm::GemmShape<32, 64, 8>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 8>;
+  
+  using Element = cutlass::complex<float>;
+  using ElementC = cutlass::complex<float>;
+
+  using LayoutA = cutlass::layout::RowMajorTensorOpMultiplicand64bCrosswise;
+  using LayoutB = cutlass::layout::ColumnMajorTensorOpMultiplicand64bCrosswise;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor
+  >::Type;
+
+  test::gemm::warp::TransformedTestbedComplex<
+      MmaTensorOp, cutlass::gemm::GemmShape<32, 64, 8> >()
+      .run();
+}
+
+TEST(SM80_warp_gemm_complex_tensor_op_f32, 64x32x8_16x8x8_tn) {
+
+  using Shape = cutlass::gemm::GemmShape<64, 32, 8>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 8>;
+  
+  using Element = cutlass::complex<float>;
+  using ElementC = cutlass::complex<float>;
+
+  using LayoutA = cutlass::layout::RowMajorTensorOpMultiplicand64bCrosswise;
+  using LayoutB = cutlass::layout::ColumnMajorTensorOpMultiplicand64bCrosswise;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor
+  >::Type;
+
+  test::gemm::warp::TransformedTestbedComplex<
+      MmaTensorOp, cutlass::gemm::GemmShape<64, 32, 8> >()
+      .run();
+}
+
+////////////////////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM80_warp_gemm_complex_tensor_op_f64, 32x32x8_8x8x4_tn) {
+
+  using Shape = cutlass::gemm::GemmShape<32, 32, 4>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
+  
+  using Element = cutlass::complex<double>;
+  using ElementC = cutlass::complex<double>;
 
-TEST(SM90_Device_Gemm_f16t_f16t_f32n_tensor_op_gmma_f32_unspecialized, 64x128x64_2x2x1) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::RowMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_64,_128,_64>, Shape<_2,_2,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::KernelTma
-    >::CollectiveOp;
-
-  using CollectiveEpilogue = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveMainloop,
-      CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-///////////////////////////////////////////////////////////////////////////////
-
-TEST(SM90_Device_Gemm_f16t_f16n_f32n_tensor_op_gmma_f32_unspecialized, 64x128x64_2x2x1) {
   using LayoutA = cutlass::layout::RowMajor;
   using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_64,_128,_64>, Shape<_2,_2,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::KernelTma 
-    >::CollectiveOp;
-
-  using CollectiveEpilogue = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveMainloop,
-      CollectiveEpilogue
-  >;
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor
+  >::Type;
+
+  test::gemm::warp::TransformedTestbedComplex<
+      MmaTensorOp, cutlass::gemm::GemmShape<32, 32, 8> >()
+      .run();
+}
+
+////////////////////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM80_warp_gemm_complex_tensor_op_f64, 32x32x8_8x8x4_nt) {
+
+  using Shape = cutlass::gemm::GemmShape<32, 32, 4>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
+  
+  using Element = cutlass::complex<double>;
+  using ElementC = cutlass::complex<double>;
 
-///////////////////////////////////////////////////////////////////////////////
-
-TEST(SM90_Device_Gemm_f16n_f16t_f32n_tensor_op_gmma_f32_unspecialized, 64x128x64_2x2x1) {
-  using LayoutA = cutlass::layout::ColumnMajor;
-  using LayoutB = cutlass::layout::RowMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_64,_128,_64>, Shape<_2,_2,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::KernelTma
-    >::CollectiveOp;
-
-  using CollectiveEpilogue = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveMainloop,
-      CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-///////////////////////////////////////////////////////////////////////////////
-
-TEST(SM90_Device_Gemm_f16n_f16n_f32n_tensor_op_gmma_f32_unspecialized, 64x128x64_2x2x1) {
   using LayoutA = cutlass::layout::ColumnMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_64,_128,_64>, Shape<_2,_2,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::KernelTma
-    >::CollectiveOp;
-
-  using CollectiveEpilogue = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveMainloop,
-      CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-///////////////////////////////////////////////////////////////////////////////
-/////////////////////////////// Cluster 4x1x1  ////////////////////////////////
-///////////////////////////////////////////////////////////////////////////////
-
-
-TEST(SM90_Device_Gemm_f16t_f16t_f32n_tensor_op_gmma_f32_unspecialized, 64x128x64_4x1x1) {
-  using LayoutA = cutlass::layout::RowMajor;
   using LayoutB = cutlass::layout::RowMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_64,_128,_64>, Shape<_4,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::KernelTma
-    >::CollectiveOp;
-
-  using CollectiveEpilogue = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveMainloop,
-      CollectiveEpilogue
-  >;
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor
+  >::Type;
+
+  test::gemm::warp::TransformedTestbedComplex<
+      MmaTensorOp, cutlass::gemm::GemmShape<32, 32, 8> >()
+      .run();
 }
 
-///////////////////////////////////////////////////////////////////////////////
-
-TEST(SM90_Device_Gemm_f16t_f16n_f32n_tensor_op_gmma_f32_unspecialized, 64x128x64_4x1x1) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_64,_128,_64>, Shape<_4,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::KernelTma
-    >::CollectiveOp;
-
-  using CollectiveEpilogue = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveMainloop,
-      CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-///////////////////////////////////////////////////////////////////////////////
-
-TEST(SM90_Device_Gemm_f16n_f16t_f32n_tensor_op_gmma_f32_unspecialized, 64x128x64_4x1x1) {
-  using LayoutA = cutlass::layout::ColumnMajor;
-  using LayoutB = cutlass::layout::RowMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_64,_128,_64>, Shape<_4,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::KernelTma
-    >::CollectiveOp;
-
-  using CollectiveEpilogue = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveMainloop,
-      CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-///////////////////////////////////////////////////////////////////////////////
-
-TEST(SM90_Device_Gemm_f16n_f16n_f32n_tensor_op_gmma_f32_unspecialized, 64x128x64_4x1x1) {
-  using LayoutA = cutlass::layout::ColumnMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_64,_128,_64>, Shape<_4,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::KernelTma
-    >::CollectiveOp;
-
-  using CollectiveEpilogue = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveMainloop,
-      CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-
-///////////////////////////////////////////////////////////////////////////////
-/////////////////////////////// Cluster 1x4x1  ////////////////////////////////
-///////////////////////////////////////////////////////////////////////////////
-
-
-TEST(SM90_Device_Gemm_f16t_f16t_f32n_tensor_op_gmma_f32_unspecialized, 64x128x64_1x4x1) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::RowMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_64,_128,_64>, Shape<_1,_4,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::KernelTma
-    >::CollectiveOp;
-
-  using CollectiveEpilogue = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveMainloop,
-      CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-///////////////////////////////////////////////////////////////////////////////
-
-TEST(SM90_Device_Gemm_f16t_f16n_f32n_tensor_op_gmma_f32_unspecialized, 64x128x64_1x4x1) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_64,_128,_64>, Shape<_1,_4,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::KernelTma
-    >::CollectiveOp;
-
-  using CollectiveEpilogue = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveMainloop,
-      CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-///////////////////////////////////////////////////////////////////////////////
-
-TEST(SM90_Device_Gemm_f16n_f16t_f32n_tensor_op_gmma_f32_unspecialized, 64x128x64_1x4x1) {
-  using LayoutA = cutlass::layout::ColumnMajor;
-  using LayoutB = cutlass::layout::RowMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_64,_128,_64>, Shape<_1,_4,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::KernelTma
-    >::CollectiveOp;
-
-  using CollectiveEpilogue = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveMainloop,
-      CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-///////////////////////////////////////////////////////////////////////////////
-
-TEST(SM90_Device_Gemm_f16n_f16n_f32n_tensor_op_gmma_f32_unspecialized, 64x128x64_1x4x1) {
-  using LayoutA = cutlass::layout::ColumnMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_64,_128,_64>, Shape<_1,_4,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::KernelTma
-    >::CollectiveOp;
-
-  using CollectiveEpilogue = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveMainloop,
-      CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-
-///////////////////////////////////////////////////////////////////////////////
-/////////////////////////////// Cluster 2x4x1  ////////////////////////////////
-///////////////////////////////////////////////////////////////////////////////
-
-
-TEST(SM90_Device_Gemm_f16t_f16t_f32n_tensor_op_gmma_f32_unspecialized, 64x128x64_2x4x1) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::RowMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_64,_128,_64>, Shape<_2,_4,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::KernelTma
-    >::CollectiveOp;
-
-  using CollectiveEpilogue = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveMainloop,
-      CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-///////////////////////////////////////////////////////////////////////////////
-
-TEST(SM90_Device_Gemm_f16t_f16n_f32n_tensor_op_gmma_f32_unspecialized, 64x128x64_2x4x1) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_64,_128,_64>, Shape<_2,_4,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::KernelTma
-    >::CollectiveOp;
-
-  using CollectiveEpilogue = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveMainloop,
-      CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-///////////////////////////////////////////////////////////////////////////////
-
-TEST(SM90_Device_Gemm_f16n_f16t_f32n_tensor_op_gmma_f32_unspecialized, 64x128x64_2x4x1) {
-  using LayoutA = cutlass::layout::ColumnMajor;
-  using LayoutB = cutlass::layout::RowMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_64,_128,_64>, Shape<_2,_4,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::KernelTma
-    >::CollectiveOp;
-
-  using CollectiveEpilogue = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveMainloop,
-      CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-///////////////////////////////////////////////////////////////////////////////
-
-TEST(SM90_Device_Gemm_f16n_f16n_f32n_tensor_op_gmma_f32_unspecialized, 64x128x64_2x4x1) {
-  using LayoutA = cutlass::layout::ColumnMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_64,_128,_64>, Shape<_2,_4,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::KernelTma
-    >::CollectiveOp;
-
-  using CollectiveEpilogue = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<cutlass::half_t, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveMainloop,
-      CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
+////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_cooperative_bias_elementwise.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_contiguous.cu`

 * *Files 27% similar despite different names*

```diff
@@ -25,342 +25,452 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide GEMM interface with bias and elementwise epilogues.
+    \brief Tests for TensorReduce family of device-wide operators
 */
 
 #include <iostream>
 
+#include "../../common/cutlass_unit_test.h"
+
 #include "cutlass/cutlass.h"
-#include "cute/tensor.hpp"
-#include "cute/atom/mma_atom.hpp"
+#include "cutlass/complex.h"
+#include "cutlass/reduction/thread/reduction_operators.h"
+#include "cutlass/reduction/device/tensor_reduce.h"
+
+#include "cutlass/functional.h"
+#include "cutlass/layout/tensor.h"
+
+#include "cutlass/util/host_tensor.h"
+#include "cutlass/util/reference/host/gemm.h"
+#include "cutlass/util/reference/host/tensor_compare.h"
+#include "cutlass/util/reference/host/tensor_copy.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/reference/device/tensor_fill.h"
+#include "cutlass/util/reference/host/tensor_norm.h"
+#include "cutlass/util/tensor_view_io.h"
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// This reduces the C dimension, transforming an NHWC tensor into NHWC with C=1.
+template <typename TensorReduction, typename ElementCompute = typename TensorReduction::ElementCompute>
+bool TestAllReduction_NHWC_reduce_c(ElementCompute reduction_identity = ElementCompute()) {
+
+  using Layout = typename TensorReduction::Layout;
+  using ElementOutput = typename TensorReduction::ElementOutput;
+  using ElementSource = typename TensorReduction::ElementSource;
+
+  int const kV = TensorReduction::kVectorLength;
+
+  int const N_indices[] = {3, 13};
+  int const H_indices[] = {5, 17};
+  int const W_indices[] = {7, 19};
+  int const C_indices[] = {2049, 2048, 2047, 384, 64, 48, 32, 24, 16, 12, 8, 6, 4, 3, 2, 1};
+  
+  for (int N : N_indices) {
+    for (int H : H_indices) {
+      for (int W : W_indices) {
+        for (int Cx : C_indices) {
+
+          int C = Cx * kV;
+
+          cutlass::HostTensor<ElementSource, Layout> src_tensor({N, H, W, C});
+          cutlass::HostTensor<ElementOutput, Layout> dst_tensor({N, H, W, 1});
+
+          cutlass::reference::host::TensorFillRandomUniform(
+            src_tensor.host_view(), 17, 10, -10, 0);
+
+          dst_tensor.sync_device();
+          src_tensor.sync_device();
+
+          // Execute a tensor reduction over rank 3 (the 'C' dimension is reduced; NHWC => NHW)
+          TensorReduction reduction(src_tensor.extent(), 3);
+
+          cutlass::DeviceAllocation<uint8_t> device_workspace(reduction.workspace_size());
+
+          cutlass::Status status = reduction.reduce(
+            dst_tensor.device_ref(),
+            src_tensor.device_ref(),
+            device_workspace.get(),
+            reduction_identity
+          );
+
+          EXPECT_EQ(status, cutlass::Status::kSuccess);
+          EXPECT_EQ(cudaDeviceSynchronize(), cudaSuccess);
+          
+          dst_tensor.sync_host();
+
+          typename TensorReduction::ReductionOp reduction_op;
+
+          //
+          // Reference check
+          //
+          for (int n = 0; n < src_tensor.extent().n(); ++n) {
+            for (int h = 0; h < src_tensor.extent().h(); ++h) {
+              for (int w = 0; w < src_tensor.extent().w(); ++w) {
+
+                ElementCompute c_accum = reduction_identity;
+
+                for (int c = 0; c < src_tensor.extent().c(); ++c) {
+                  c_accum = reduction_op(c_accum, ElementCompute(src_tensor.at({n, h, w, c})));
+                }
+
+                ElementCompute got = ElementCompute(dst_tensor.at({n, h, w, 0}));
+
+                bool equal = (c_accum == got);
+
+                EXPECT_TRUE(equal);
+                if (!equal) {
+
+                  std::cerr 
+                    << "Error at location (" << n << ", " << h << ", " << w << ", 0)" << std::endl;
+
+                  std::cerr 
+                    << "  expected: " << c_accum << std::endl
+                    << "       got: " << got << std::endl;
+
+                  std::cerr 
+                    << "Problem: " << src_tensor.extent() << " -> " 
+                    << dst_tensor.extent() << std::endl;
+
+                  std::cerr 
+                    << "   Grid: " << reduction.reduction_strided.grid_shape 
+                    << "\n   Block: " << reduction.reduction_strided.threadblock_shape << std::endl
+                    << "  FInal: " << reduction.reduction_strided.grid_final 
+                    << "\n   Block: " << reduction.reduction_strided.threadblock_final << "\n";
+
+                  return false;
+                }
+
+              } //w
+            } // h
+          } // n
+          
+          //
+          // Next problem
+          //
+
+        } // C
+      } // W
+    } // H
+  } // N
 
-#include "cutlass/numeric_types.h"
+  return true;
+}
 
-#include "cutlass/gemm/device/gemm_universal_adapter.h"
-#include "cutlass/gemm/kernel/gemm_universal.hpp"
-#include "cutlass/epilogue/collective/collective_builder.hpp"
-#include "cutlass/gemm/collective/collective_builder.hpp"
-#include "cutlass/epilogue/collective/sm70_epilogue_vectorized.hpp"
-#include "cutlass/epilogue/collective/default_epilogue.hpp"
-#include "cutlass/epilogue/thread/linear_combination.h"
-#include "cutlass/epilogue/thread/linear_combination_bias_elementwise.h"
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-#include "../../common/cutlass_unit_test.h"
+/// Test tensor reduction from NHWC to NHW
+TEST(Reduction_TensorReduce, nhwc_reduce_c_f32x1) {
+
+  using Layout = cutlass::layout::TensorNHWC;
+  using ElementOutput = float;
+  using ElementSource = float;
+  using ElementCompute = float;
+  int const kV = 1;
+  
+  // Define the functor
+  using Functor = cutlass::plus<ElementCompute>;
+
+  using TensorReduction = cutlass::reduction::device::TensorReduction<
+    ElementOutput,
+    ElementSource,
+    Layout,
+    Functor,
+    kV,
+    ElementCompute
+  >;
+  
+  EXPECT_TRUE(TestAllReduction_NHWC_reduce_c<TensorReduction>());
+}
 
-#include "testing_elementwise.hpp"
-#include "gemm_testbed_3x.hpp"
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
+/// Test tensor reduction from NHWC to NHW
+TEST(Reduction_TensorReduce, nhwc_reduce_c_f32x1_f16x1) {
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+  using Layout = cutlass::layout::TensorNHWC;
+  using ElementOutput = float;
+  using ElementSource = cutlass::half_t;
+  using ElementCompute = float;
+  int const kV = 1;
+  
+  // Define the functor
+  using Functor = cutlass::plus<ElementCompute>;
+
+  using TensorReduction = cutlass::reduction::device::TensorReduction<
+    ElementOutput,
+    ElementSource,
+    Layout,
+    Functor,
+    kV,
+    ElementCompute
+  >;
+  
+  EXPECT_TRUE(TestAllReduction_NHWC_reduce_c<TensorReduction>());
+}
 
-using namespace cute;
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Gemm_f16t_f16n_f32t_tensor_op_gmma_f32_cooperative_epilogue, 256x128x64_2x2x1_ReLU) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::RowMajor;
-  using TileShape_MNK = Shape<_256,_128,_64>;
-  using ClusterShape_MNK = Shape<_2,_2,_1>;
-
-  using EpilogueSchedule = cutlass::epilogue::TmaWarpSpecializedCooperativeElementwise<
-        cutlass::epilogue::thread::ReLu>;
-
-  using CollectiveEpilogue = typename cutlass::epilogue::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      TileShape_MNK, ClusterShape_MNK,
-      cutlass::epilogue::collective::EpilogueTileAuto,
-      float, float,
-      cutlass::half_t, LayoutC, 8,
-      cutlass::half_t, LayoutC, 8,
-      EpilogueSchedule
-    >::CollectiveOp;
-
-  using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      TileShape_MNK, ClusterShape_MNK,
-      cutlass::gemm::collective::StageCountAutoCarveout<sizeof(typename CollectiveEpilogue::SharedStorage)>,
-      cutlass::gemm::KernelTmaWarpSpecializedCooperative
-    >::CollectiveOp;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveMainloop,
-      CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-
-  bool passed = test::gemm::device::TestAll<Gemm, cutlass::epilogue::thread::ReLu>();
-  EXPECT_TRUE(passed);
-}
-
-TEST(SM90_Device_Gemm_f16t_f16n_f32t_tensor_op_gmma_f32_cooperative_epilogue, 256x128x64_2x2x1_Bias_ReLU) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::RowMajor;
-  using TileShape_MNK = Shape<_256,_128,_64>;
-  using ClusterShape_MNK = Shape<_2,_2,_1>;
-
-  static constexpr bool StoreT = true;
-  using EpilogueSchedule = cutlass::epilogue::TmaWarpSpecializedCooperativeBiasElementwise<
-        cutlass::epilogue::thread::ReLu, cutlass::half_t, cutlass::plus, StoreT, float>;
-
-  using CollectiveEpilogue = typename cutlass::epilogue::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      TileShape_MNK, ClusterShape_MNK,
-      cutlass::epilogue::collective::EpilogueTileAuto,
-      float, float,
-      cutlass::half_t, LayoutC, 8,
-      cutlass::half_t, LayoutC, 8,
-      EpilogueSchedule
-    >::CollectiveOp;
-
-  using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      TileShape_MNK, ClusterShape_MNK,
-      cutlass::gemm::collective::StageCountAutoCarveout<sizeof(typename CollectiveEpilogue::SharedStorage)>,
-      cutlass::gemm::KernelTmaWarpSpecializedCooperative
-    >::CollectiveOp;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveMainloop,
-      CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-
-  bool passed = test::gemm::device::TestAllBiasElementwise<Gemm>();
-  EXPECT_TRUE(passed);
-}
-
-TEST(SM90_Device_Gemm_f16t_f16n_f32t_tensor_op_gmma_f32_cooperative_epilogue, 256x128x64_2x2x1_Bias_GELU) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::RowMajor;
-  using TileShape_MNK = Shape<_256,_128,_64>;
-  using ClusterShape_MNK = Shape<_2,_2,_1>;
-
-  static constexpr bool StoreT = true;
-  using EpilogueSchedule = cutlass::epilogue::TmaWarpSpecializedCooperativeBiasElementwise<
-        cutlass::epilogue::thread::GELU, cutlass::half_t, cutlass::plus, StoreT, float>;
-
-  using CollectiveEpilogue = typename cutlass::epilogue::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      TileShape_MNK, ClusterShape_MNK,
-      cutlass::epilogue::collective::EpilogueTileAuto,
-      float, float,
-      cutlass::half_t, LayoutC, 8,
-      cutlass::half_t, LayoutC, 8,
-      EpilogueSchedule
-    >::CollectiveOp;
-
-  using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      TileShape_MNK, ClusterShape_MNK,
-      cutlass::gemm::collective::StageCountAutoCarveout<sizeof(typename CollectiveEpilogue::SharedStorage)>,
-      cutlass::gemm::KernelTmaWarpSpecializedCooperative
-    >::CollectiveOp;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveMainloop,
-      CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-
-  bool check_relative_equality = true;
-  bool passed = test::gemm::device::TestAllBiasElementwise<Gemm>(check_relative_equality);
-  EXPECT_TRUE(passed);
-}
-
-TEST(SM90_Device_Gemm_f16t_f16n_f32t_tensor_op_gmma_f32_cooperative_epilogue, 256x128x64_2x2x1_Bias_ReLU_NoStoreT) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::RowMajor;
-  using TileShape_MNK = Shape<_256,_128,_64>;
-  using ClusterShape_MNK = Shape<_2,_2,_1>;
-
-  static constexpr bool StoreT = false;
-  using EpilogueSchedule = cutlass::epilogue::TmaWarpSpecializedCooperativeBiasElementwise<
-        cutlass::epilogue::thread::ReLu, cutlass::half_t, cutlass::plus, StoreT, float>;
-
-  using CollectiveEpilogue = typename cutlass::epilogue::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      TileShape_MNK, ClusterShape_MNK,
-      cutlass::epilogue::collective::EpilogueTileAuto,
-      float, float,
-      cutlass::half_t, LayoutC, 8,
-      cutlass::half_t, LayoutC, 8,
-      EpilogueSchedule
-    >::CollectiveOp;
-
-  using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      TileShape_MNK, ClusterShape_MNK,
-      cutlass::gemm::collective::StageCountAutoCarveout<sizeof(typename CollectiveEpilogue::SharedStorage)>,
-      cutlass::gemm::KernelTmaWarpSpecializedCooperative
-    >::CollectiveOp;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveMainloop,
-      CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-
-  bool passed = test::gemm::device::TestAllBiasElementwise<Gemm>();
-  EXPECT_TRUE(passed);
-}
-
-TEST(SM90_Device_Gemm_f16t_f16n_f32t_tensor_op_gmma_f32_cooperative_epilogue, 256x128x64_2x2x1_Bias_Negate) {
-
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::RowMajor;
-  using TileShape_MNK = Shape<_256,_128,_64>;
-  using ClusterShape_MNK = Shape<_2,_2,_1>;
-
-  static constexpr bool StoreT = true;
-  using EpilogueSchedule = cutlass::epilogue::TmaWarpSpecializedCooperativeBiasElementwise<
-        test::gemm::device::detail::Negate, cutlass::half_t, cutlass::plus, StoreT, float>;
-
-  using CollectiveEpilogue = typename cutlass::epilogue::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      TileShape_MNK, ClusterShape_MNK,
-      cutlass::epilogue::collective::EpilogueTileAuto,
-      float, float,
-      cutlass::half_t, LayoutC, 8,
-      cutlass::half_t, LayoutC, 8,
-      EpilogueSchedule
-    >::CollectiveOp;
-
-  using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      TileShape_MNK, ClusterShape_MNK,
-      cutlass::gemm::collective::StageCountAutoCarveout<sizeof(typename CollectiveEpilogue::SharedStorage)>,
-      cutlass::gemm::KernelTmaWarpSpecializedCooperative
-    >::CollectiveOp;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveMainloop,
-      CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-
-  bool passed = test::gemm::device::TestAllBiasElementwise<Gemm>();
-  EXPECT_TRUE(passed);
-}
-
-TEST(SM90_Device_Gemm_f16t_f16n_f32n_tensor_op_gmma_f32_cooperative_epilogue, 256x128x64_2x2x1_BiasMul_ReLU) {
-
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-  using TileShape_MNK = Shape<_256,_128,_64>;
-  using ClusterShape_MNK = Shape<_2,_2,_1>;
-
-  static constexpr bool StoreT = true;
-  using EpilogueSchedule = cutlass::epilogue::TmaWarpSpecializedCooperativeBiasElementwise<
-        cutlass::epilogue::thread::ReLu, cutlass::half_t, cutlass::multiplies, StoreT, float>;
-
-  using CollectiveEpilogue = typename cutlass::epilogue::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      TileShape_MNK, ClusterShape_MNK,
-      cutlass::epilogue::collective::EpilogueTileAuto,
-      float, float,
-      cutlass::half_t, LayoutC, 8,
-      cutlass::half_t, LayoutC, 8,
-      EpilogueSchedule
-    >::CollectiveOp;
-
-  using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      TileShape_MNK, ClusterShape_MNK,
-      cutlass::gemm::collective::StageCountAutoCarveout<sizeof(typename CollectiveEpilogue::SharedStorage)>,
-      cutlass::gemm::KernelTmaWarpSpecializedCooperative
-    >::CollectiveOp;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveMainloop,
-      CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-
-  bool passed = test::gemm::device::TestAllBiasElementwise<Gemm>();
-  EXPECT_TRUE(passed);
-}
-
-TEST(SM90_Device_Gemm_f16t_f16n_f32t_tensor_op_gmma_f32_cooperative_epilogue, 256x128x64_2x2x1_BiasMul_ReLU) {
-
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::RowMajor;
-  using TileShape_MNK = Shape<_256,_128,_64>;
-  using ClusterShape_MNK = Shape<_2,_2,_1>;
-
-  static constexpr bool StoreT = true;
-  using EpilogueSchedule = cutlass::epilogue::TmaWarpSpecializedCooperativeBiasElementwise<
-        cutlass::epilogue::thread::ReLu, cutlass::half_t, cutlass::multiplies, StoreT, float>;
-
-  using CollectiveEpilogue = typename cutlass::epilogue::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      TileShape_MNK, ClusterShape_MNK,
-      cutlass::epilogue::collective::EpilogueTileAuto,
-      float, float,
-      cutlass::half_t, LayoutC, 8,
-      cutlass::half_t, LayoutC, 8,
-      EpilogueSchedule
-    >::CollectiveOp;
-
-  using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      TileShape_MNK, ClusterShape_MNK,
-      cutlass::gemm::collective::StageCountAutoCarveout<sizeof(typename CollectiveEpilogue::SharedStorage)>,
-      cutlass::gemm::KernelTmaWarpSpecializedCooperative
-    >::CollectiveOp;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveMainloop,
-      CollectiveEpilogue
+/// Test tensor reduction from NHWC to NHW
+TEST(Reduction_TensorReduce, nhwc_reduce_c_f32x2) {
+
+  using Layout = cutlass::layout::TensorNHWC;
+  using ElementOutput = float;
+  using ElementSource = float;
+  using ElementCompute = float;
+  int const kV = 2;
+  
+  // Define the functor
+  using Functor = cutlass::plus<ElementCompute>;
+
+  using TensorReduction = cutlass::reduction::device::TensorReduction<
+    ElementOutput,
+    ElementSource,
+    Layout,
+    Functor,
+    kV,
+    ElementCompute
   >;
+  
+  EXPECT_TRUE(TestAllReduction_NHWC_reduce_c<TensorReduction>());
+}
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Test tensor reduction from NHWC to NHW
+TEST(Reduction_TensorReduce, nhwc_reduce_c_f32x2_f16x2) {
+
+  using Layout = cutlass::layout::TensorNHWC;
+  using ElementOutput = float;
+  using ElementSource = cutlass::half_t;
+  using ElementCompute = float;
+  int const kV = 2;
+  
+  // Define the functor
+  using Functor = cutlass::plus<ElementCompute>;
+
+  using TensorReduction = cutlass::reduction::device::TensorReduction<
+    ElementOutput,
+    ElementSource,
+    Layout,
+    Functor,
+    kV,
+    ElementCompute
+  >;
+  
+  EXPECT_TRUE(TestAllReduction_NHWC_reduce_c<TensorReduction>());
+}
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Test tensor reduction from NHWC to NHW
+TEST(Reduction_TensorReduce, nhwc_reduce_c_f32x4) {
+
+  using Layout = cutlass::layout::TensorNHWC;
+  using ElementOutput = float;
+  using ElementSource = float;
+  using ElementCompute = float;
+  int const kV = 4;
+  
+  // Define the functor
+  using Functor = cutlass::plus<ElementCompute>;
+
+  using TensorReduction = cutlass::reduction::device::TensorReduction<
+    ElementOutput,
+    ElementSource,
+    Layout,
+    Functor,
+    kV,
+    ElementCompute
+  >;
+  
+  EXPECT_TRUE(TestAllReduction_NHWC_reduce_c<TensorReduction>());
+}
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Test tensor reduction from NHWC to NHW
+TEST(Reduction_TensorReduce, nhwc_reduce_c_f32x4_f16x4) {
+
+  using Layout = cutlass::layout::TensorNHWC;
+  using ElementOutput = float;
+  using ElementSource = cutlass::half_t;
+  using ElementCompute = float;
+  int const kV = 4;
+  
+  // Define the functor
+  using Functor = cutlass::plus<ElementCompute>;
+
+  using TensorReduction = cutlass::reduction::device::TensorReduction<
+    ElementOutput,
+    ElementSource,
+    Layout,
+    Functor,
+    kV,
+    ElementCompute
+  >;
+  
+  EXPECT_TRUE(TestAllReduction_NHWC_reduce_c<TensorReduction>());
+}
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
+/// Test tensor reduction from NHWC to NHW
+TEST(Reduction_TensorReduce, nhwc_maximum_c_f32x4) {
 
-  bool passed = test::gemm::device::TestAllBiasElementwise<Gemm>();
-  EXPECT_TRUE(passed);
+  using Layout = cutlass::layout::TensorNHWC;
+  using ElementOutput = float;
+  using ElementSource = float;
+  using ElementCompute = float;
+  int const kV = 4;
+  
+  // Define the functor
+  using Functor = cutlass::maximum<ElementCompute>;
+
+  using TensorReduction = cutlass::reduction::device::TensorReduction<
+    ElementOutput,
+    ElementSource,
+    Layout,
+    Functor,
+    kV,
+    ElementCompute
+  >;
+  
+  EXPECT_TRUE(TestAllReduction_NHWC_reduce_c<TensorReduction>( -std::numeric_limits<float>::max() ));
+}
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Test tensor reduction from NHWC to NHW
+TEST(Reduction_TensorReduce, nhwc_minimum_c_f32x4) {
+
+  using Layout = cutlass::layout::TensorNHWC;
+  using ElementOutput = float;
+  using ElementSource = float;
+  using ElementCompute = float;
+  int const kV = 4;
+  
+  // Define the functor
+  using Functor = cutlass::minimum<ElementCompute>;
+
+  using TensorReduction = cutlass::reduction::device::TensorReduction<
+    ElementOutput,
+    ElementSource,
+    Layout,
+    Functor,
+    kV,
+    ElementCompute
+  >;
+  
+  EXPECT_TRUE(TestAllReduction_NHWC_reduce_c<TensorReduction>( std::numeric_limits<float>::max() ));
+}
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Test tensor reduction from NHWC to NHW
+TEST(Reduction_TensorReduce, nhwc_ANY_c_s32) {
+
+  using Layout = cutlass::layout::TensorNHWC;
+  using ElementOutput = int;
+  using ElementSource = int;
+  using ElementCompute = int;
+  int const kV = 1;
+  
+  // Define the functor
+  using Functor = cutlass::logical_or<ElementCompute>;
+
+  using TensorReduction = cutlass::reduction::device::TensorReduction<
+    ElementOutput,
+    ElementSource,
+    Layout,
+    Functor,
+    kV,
+    ElementCompute
+  >;
+  
+  EXPECT_TRUE(TestAllReduction_NHWC_reduce_c<TensorReduction>( ElementCompute(0) ));
+}
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Test tensor reduction from NHWC to NHW
+TEST(Reduction_TensorReduce, nhwc_ALL_c_s32) {
+
+  using Layout = cutlass::layout::TensorNHWC;
+  using ElementOutput = int;
+  using ElementSource = int;
+  using ElementCompute = int;
+  int const kV = 1;
+  
+  // Define the functor
+  using Functor = cutlass::logical_and<ElementCompute>;
+
+  using TensorReduction = cutlass::reduction::device::TensorReduction<
+    ElementOutput,
+    ElementSource,
+    Layout,
+    Functor,
+    kV,
+    ElementCompute
+  >;
+  
+  EXPECT_TRUE(TestAllReduction_NHWC_reduce_c<TensorReduction>( ElementCompute(1) ));
+}
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Test tensor reduction from NHWC to NHW
+TEST(Reduction_TensorReduce, nhwc_ANY_c_f32) {
+
+  using Layout = cutlass::layout::TensorNHWC;
+  using ElementOutput = float;
+  using ElementSource = float;
+  using ElementCompute = float;
+  int const kV = 1;
+  
+  // Define the functor
+  using Functor = cutlass::logical_or<ElementCompute>;
+
+  using TensorReduction = cutlass::reduction::device::TensorReduction<
+    ElementOutput,
+    ElementSource,
+    Layout,
+    Functor,
+    kV,
+    ElementCompute
+  >;
+  
+  EXPECT_TRUE(TestAllReduction_NHWC_reduce_c<TensorReduction>( ElementCompute(0) ));
+}
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Test tensor reduction from NHWC to NHW
+TEST(Reduction_TensorReduce, nhwc_ALL_c_f32) {
+
+  using Layout = cutlass::layout::TensorNHWC;
+  using ElementOutput = float;
+  using ElementSource = float;
+  using ElementCompute = float;
+  int const kV = 1;
+  
+  // Define the functor
+  using Functor = cutlass::logical_and<ElementCompute>;
+
+  using TensorReduction = cutlass::reduction::device::TensorReduction<
+    ElementOutput,
+    ElementSource,
+    Layout,
+    Functor,
+    kV,
+    ElementCompute
+  >;
+  
+  EXPECT_TRUE(TestAllReduction_NHWC_reduce_c<TensorReduction>( ElementCompute(1) ));
 }
 
-#endif // defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_tensor_broadcast.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_grouped_sm80.cu`

 * *Files 19% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2023 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,274 +25,249 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide GEMM interface with an elementwise tensor-tensor broadcast epilogue
+    \brief Tests for grouped Rank2K interface
 */
 
 #include <iostream>
 
+#include "../../common/cutlass_unit_test.h"
 #include "cutlass/cutlass.h"
-#include "cute/tensor.hpp"
-#include "cute/atom/mma_atom.hpp"
 
-#include "cutlass/numeric_types.h"
+#include "cutlass/blas3.h"
+#include "cutlass/gemm/gemm.h"
+#include "cutlass/gemm/kernel/rank_2k_grouped.h"
+#include "cutlass/gemm/kernel/default_rank_2k_grouped.h"
+#include "cutlass/gemm/device/rank_2k_grouped.h"
+
+#include "cutlass/util/host_tensor.h"
+#include "cutlass/util/reference/host/gemm.h"
+#include "cutlass/util/reference/host/tensor_compare.h"
+#include "cutlass/util/reference/host/tensor_copy.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/tensor_view_io.h"
 
-#include "cutlass/gemm/device/gemm_universal_adapter.h"
-#include "cutlass/gemm/kernel/gemm_universal.hpp"
-#include "cutlass/gemm/collective/collective_builder.hpp"
-#include "cutlass/epilogue/collective/epilogue_tensor_broadcast.hpp"
-#include "cutlass/epilogue/thread/linear_combination_tensor_broadcast.hpp"
+#include "testbed_grouped_rank_2k.h"
 
-#include "../../common/cutlass_unit_test.h"
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-#include "gemm_testbed_3x_tensor_broadcast.hpp"
-#include "testing_elementwise.hpp"
+#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-using namespace cute;
+TEST(SM80_Device_Syr2kGrouped_f64n_f64t_l_tensor_op_f64, 32x32x16_16x16x16) {
+
+  using ElementA = double;
+  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementB = double;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = double;
+  using LayoutC = cutlass::layout::RowMajor;
+  using ElementAccumulator = double;
+
+  using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
+    ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
+    ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
+    ElementC, LayoutC, cutlass::FillMode::kLower,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    3, // kStages
+    cutlass::arch::OpMultiplyAdd,
+    cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
+
+  using Rank2K = cutlass::gemm::device::Rank2KGrouped<Rank2Kkernel>;
+
+  test::gemm::device::TestbedGrouped<Rank2K> testbed;
+  bool passed = testbed.run(24);
+  EXPECT_TRUE(passed);
+}
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Gemm_f16t_f16t_f16n_tensor_op_gmma_f32_tensor_broadcast, 64x128x64_ActIdentity_Bin0Plus_Bin1NoOp_UnaryIdentity) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::RowMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using ElementOutput = float;
-  using ElementAccumulator = ElementOutput;
-  using ElementCompute = ElementOutput;
-  using ElementBias = ElementOutput;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      ElementOutput,
-      Shape<_64,_128,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::detail::Sm90TmaWarpSpecializedAdapter<
-    cutlass::epilogue::collective::EpilogueTensorBroadcast<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombinationTensorBroadcast<ElementOutput>,
-      cutlass::gemm::EpilogueDefault>>;
-
-  EXPECT_TRUE(EpilogueOp::IsBinaryOp0Enabled);
-  EXPECT_TRUE(!EpilogueOp::IsBinaryOp1Enabled);
-  EXPECT_TRUE(!EpilogueOp::IsUnaryOpEnabled);
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
+TEST(SM80_Device_Syr2kGrouped_f64n_f64t_l_tensor_op_f64, 64x64x16_32x32x16) {
+
+  using ElementA = double;
+  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementB = double;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = double;
+  using LayoutC = cutlass::layout::RowMajor;
+  using ElementAccumulator = double;
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAllTensorBroadcast<Gemm>());
+  using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
+    ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
+    ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
+    ElementC, LayoutC, cutlass::FillMode::kLower,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<64, 64, 16>,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    3, // kStages
+    cutlass::arch::OpMultiplyAdd,
+    cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
+
+  using Rank2K = cutlass::gemm::device::Rank2KGrouped<Rank2Kkernel>;
+
+  test::gemm::device::TestbedGrouped<Rank2K> testbed;
+  bool passed = testbed.run(24);
+  EXPECT_TRUE(passed);
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Gemm_f16t_f16t_f16n_tensor_op_gmma_f32_tensor_broadcast, 64x128x64_ActReLu_Bin0Plus_Bin1Plus_UnaryNegate) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::RowMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using ElementOutput = float;
-  using ElementAccumulator = ElementOutput;
-  using ElementCompute = ElementOutput;
-  using ElementBias = ElementOutput;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      ElementOutput,
-      Shape<_64,_128,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::detail::Sm90TmaWarpSpecializedAdapter<
-    cutlass::epilogue::collective::EpilogueTensorBroadcast<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombinationTensorBroadcast<
-        ElementOutput, ElementAccumulator, ElementCompute, ElementBias,
-        cutlass::epilogue::thread::ReLu,
-        cutlass::plus,
-        cutlass::plus,
-        test::gemm::device::detail::Negate
-        >,
-      cutlass::gemm::EpilogueDefault>>;
-
-  EXPECT_TRUE(EpilogueOp::IsBinaryOp0Enabled);
-  EXPECT_TRUE(EpilogueOp::IsBinaryOp1Enabled);
-  EXPECT_TRUE(EpilogueOp::IsUnaryOpEnabled);
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
+TEST(SM80_Device_Syr2kGrouped_f64n_f64t_l_tensor_op_f64, 64x32x16_32x32x16) {
+
+  using ElementA = double;
+  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementB = double;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = double;
+  using LayoutC = cutlass::layout::RowMajor;
+  using ElementAccumulator = double;
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAllTensorBroadcast<Gemm>());
+  using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
+    ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
+    ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
+    ElementC, LayoutC, cutlass::FillMode::kLower,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<64, 32, 16>,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    3, // kStages
+    cutlass::arch::OpMultiplyAdd,
+    cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
+
+  using Rank2K = cutlass::gemm::device::Rank2KGrouped<Rank2Kkernel>;
+
+  test::gemm::device::TestbedGrouped<Rank2K> testbed;
+  bool passed = testbed.run(24);
+  EXPECT_TRUE(passed);
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Gemm_f16n_f16t_f16t_tensor_op_gmma_f32_tensor_broadcast, 64x128x64_ActReLu_Bin0Mul_Bin1Plus_UnaryNegate) {
+TEST(SM80_Device_Syr2kGrouped_f64n_f64t_l_tensor_op_f64, 128x64x16_64x32x16) {
+
+  using ElementA = double;
   using LayoutA = cutlass::layout::ColumnMajor;
-  using LayoutB = cutlass::layout::RowMajor;
+  using ElementB = double;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = double;
   using LayoutC = cutlass::layout::RowMajor;
+  using ElementAccumulator = double;
 
-  using ElementOutput = float;
-  using ElementAccumulator = ElementOutput;
-  using ElementCompute = ElementOutput;
-  using ElementBias = ElementOutput;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      ElementOutput,
-      Shape<_64,_128,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::detail::Sm90TmaWarpSpecializedAdapter<
-    cutlass::epilogue::collective::EpilogueTensorBroadcast<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombinationTensorBroadcast<
-        ElementOutput, ElementAccumulator, ElementCompute, ElementBias,
-        cutlass::epilogue::thread::ReLu,
-        cutlass::multiplies,
-        cutlass::plus,
-        test::gemm::device::detail::Negate
-        >,
-      cutlass::gemm::EpilogueDefault>>;
-
-  EXPECT_TRUE(EpilogueOp::IsBinaryOp0Enabled);
-  EXPECT_TRUE(EpilogueOp::IsBinaryOp1Enabled);
-  EXPECT_TRUE(EpilogueOp::IsUnaryOpEnabled);
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAllTensorBroadcast<Gemm>());
+  using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
+    ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
+    ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
+    ElementC, LayoutC, cutlass::FillMode::kLower,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<128, 64, 16>,
+    cutlass::gemm::GemmShape<64, 32, 16>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    3, // kStages
+    cutlass::arch::OpMultiplyAdd,
+    cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
+
+  using Rank2K = cutlass::gemm::device::Rank2KGrouped<Rank2Kkernel>;
+
+  test::gemm::device::TestbedGrouped<Rank2K> testbed;
+  bool passed = testbed.run(24);
+  EXPECT_TRUE(passed);
 }
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Gemm_f16t_f16t_f16n_tensor_op_gmma_f32_tensor_broadcast, 128x128x64_ActReLu_Bin0NoOp_Bin1Plus_UnaryNegate) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::RowMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using ElementOutput = float;
-  using ElementAccumulator = ElementOutput;
-  using ElementCompute = ElementOutput;
-  using ElementBias = ElementOutput;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      ElementOutput,
-      Shape<_128,_128,_64>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::detail::Sm90TmaWarpSpecializedAdapter<
-    cutlass::epilogue::collective::EpilogueTensorBroadcast<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombinationTensorBroadcast<
-        ElementOutput, ElementAccumulator, ElementCompute, ElementBias,
-        cutlass::epilogue::thread::ReLu,
-        cutlass::epilogue::thread::detail::NoOp,
-        cutlass::plus,
-        test::gemm::device::detail::Negate
-        >,
-      cutlass::gemm::EpilogueDefault>>;
-
-  EXPECT_TRUE(!EpilogueOp::IsBinaryOp0Enabled);
-  EXPECT_TRUE(EpilogueOp::IsBinaryOp1Enabled);
-  EXPECT_TRUE(EpilogueOp::IsUnaryOpEnabled);
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
+TEST(SM80_Device_Syr2kGrouped_f64n_f64t_l_tensor_op_f64, 128x128x16_32x64x16) {
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAllTensorBroadcast<Gemm>());
+  using ElementA = double;
+  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementB = double;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = double;
+  using LayoutC = cutlass::layout::RowMajor;
+  using ElementAccumulator = double;
+
+  using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
+    ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
+    ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
+    ElementC, LayoutC, cutlass::FillMode::kLower,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<128, 128, 16>,
+    cutlass::gemm::GemmShape<32, 64, 16>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    3, // kStages
+    cutlass::arch::OpMultiplyAdd,
+    cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
+
+  using Rank2K = cutlass::gemm::device::Rank2KGrouped<Rank2Kkernel>;
+
+  test::gemm::device::TestbedGrouped<Rank2K> testbed;
+  bool passed = testbed.run(24);
+  EXPECT_TRUE(passed);
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Gemm_f16t_f16t_f32n_tensor_op_gmma_f32_warpspecialized_tensor_broadcast, 64x128x64_2x2x1_ActReLu_Bin0Mul_Bin1Plus_UnaryNegate) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::RowMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using ElementOutput = float;
-  using ElementAccumulator = ElementOutput;
-  using ElementCompute = ElementOutput;
-  using ElementBias = ElementOutput;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::half_t, LayoutA, 8,
-      cutlass::half_t, LayoutB, 8,
-      float,
-      Shape<_64,_128,_64>, Shape<_2,_2,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::KernelTmaWarpSpecialized
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::detail::Sm90TmaWarpSpecializedAdapter<
-    cutlass::epilogue::collective::EpilogueTensorBroadcast<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombinationTensorBroadcast<
-        ElementOutput, ElementAccumulator, ElementCompute, ElementBias,
-        cutlass::epilogue::thread::ReLu,
-        cutlass::multiplies,
-        cutlass::plus,
-        test::gemm::device::detail::Negate
-        >,
-      cutlass::gemm::EpilogueDefault>>;
-
-  EXPECT_TRUE(EpilogueOp::IsBinaryOp0Enabled);
-  EXPECT_TRUE(EpilogueOp::IsBinaryOp1Enabled);
-  EXPECT_TRUE(EpilogueOp::IsUnaryOpEnabled);
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
+TEST(SM80_Device_Syr2kGrouped_f64n_f64t_u_tensor_op_f64, 32x32x16_16x16x16) {
+
+  using ElementA = double;
+  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementB = double;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = double;
+  using LayoutC = cutlass::layout::RowMajor;
+  using ElementAccumulator = double;
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAllTensorBroadcast<Gemm>());
+  using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
+    ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
+    ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
+    ElementC, LayoutC, cutlass::FillMode::kUpper,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    3, // kStages
+    cutlass::arch::OpMultiplyAdd,
+    cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
+
+  using Rank2K = cutlass::gemm::device::Rank2KGrouped<Rank2Kkernel>;
+
+  test::gemm::device::TestbedGrouped<Rank2K> testbed;
+  bool passed = testbed.run(24);
+  EXPECT_TRUE(passed);
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f32_f32_f32_tensor_op_f32.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/cutlass_profiler.h`

 * *Files 26% similar despite different names*

```diff
@@ -1,86 +1,96 @@
 /***************************************************************************************************
- * Copyright (c) 2023, NVIDIA CORPORATION.  All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * SPDX-License-Identifier: BSD-3-Clause
  *
- * Redistribution and use in source and binary forms, with or without modification, are permitted
- * provided that the following conditions are met:
- *     * Redistributions of source code must retain the above copyright notice, this list of
- *       conditions and the following disclaimer.
- *     * Redistributions in binary form must reproduce the above copyright notice, this list of
- *       conditions and the following disclaimer in the documentation and/or other materials
- *       provided with the distribution.
- *     * Neither the name of the NVIDIA CORPORATION nor the names of its contributors may be used
- *       to endorse or promote products derived from this software without specific prior written
- *       permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR
- * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
- * FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE
- * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
- * BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
- * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
- * STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice, this
+ * list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ * 3. Neither the name of the copyright holder nor the names of its
+ * contributors may be used to endorse or promote products derived from
+ * this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+ * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/*! \file
-    \brief Tests for device-wide GEMM interface
+/* \file
+   \brief Execution environment
 */
 
-#include <iostream>
+#pragma once
+// CUTLASS Library includes
+#include "cutlass/library/library.h"
+#include "cutlass/library/manifest.h"
+#include "cutlass/library/singleton.h"
+
+#include "options.h"
+#include "operation_profiler.h"
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+namespace cutlass {
+namespace profiler {
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-#include "cutlass/cutlass.h"
-#include "cute/tensor.hpp"
-#include "cute/atom/mma_atom.hpp"
+/// CUTLASS Profiler application 
+class CutlassProfiler {
+private:
 
-#include "cutlass/numeric_types.h"
+  //
+  // Data members
+  //
 
-#include "cutlass/gemm/device/gemm_universal_adapter.h"
-#include "cutlass/gemm/kernel/gemm_universal.hpp"
-#include "cutlass/gemm/collective/collective_builder.hpp"
-#include "cutlass/epilogue/collective/default_epilogue.hpp"
-#include "cutlass/epilogue/collective/default_transposed_epilogue.hpp"
-#include "cutlass/epilogue/thread/linear_combination.h"
+  /// Performance testbench options
+  Options options_;
 
-#include "../../common/cutlass_unit_test.h"
+  /// Entry points for each operation
+  OperationProfilerVector operation_profilers_;
 
-#include "gemm_testbed_3x.hpp"
+private:
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+  /// Prints usage
+  void print_usage_(std::ostream &);
+  
+  /// Prints usage
+  void print_options_(std::ostream &);
 
-using namespace cute;
+  /// Initializes the device
+  void initialize_device_();
 
-///////////////////////////////////////////////////////////////////////////////
+  /// Enumerates all operations
+  void enumerate_();
 
-TEST(SM90_Device_Gemm_f32t_f32n_f32n_tensor_op_gmma_f32, 64x128x32_1x2x1) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
+  /// Profiles all operations
+  int profile_();
 
-  using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      float, LayoutA, 4,
-      float, LayoutB, 4,
-      float,
-      Shape<_64,_128,_128>, Shape<_1,_2,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
+public:
 
-  using CollectiveEpilogue = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<float, 1, float, float>>;
+  CutlassProfiler(Options const &options);
+  ~CutlassProfiler();
 
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveMainloop,
-      CollectiveEpilogue
-  >;
+  /// Invokes profiling operations
+  int operator()();
+};
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-///////////////////////////////////////////////////////////////////////////////
+} // namespace profiler
+} // namespace cutlass
 
-#endif // defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f32_f32_f32_tensor_op_f32_tensor_broadcast.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/epilogue_workspace.cu`

 * *Files 25% similar despite different names*

```diff
@@ -1,102 +1,130 @@
 /***************************************************************************************************
- * Copyright (c) 2023, NVIDIA CORPORATION.  All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * SPDX-License-Identifier: BSD-3-Clause
  *
- * Redistribution and use in source and binary forms, with or without modification, are permitted
- * provided that the following conditions are met:
- *     * Redistributions of source code must retain the above copyright notice, this list of
- *       conditions and the following disclaimer.
- *     * Redistributions in binary form must reproduce the above copyright notice, this list of
- *       conditions and the following disclaimer in the documentation and/or other materials
- *       provided with the distribution.
- *     * Neither the name of the NVIDIA CORPORATION nor the names of its contributors may be used
- *       to endorse or promote products derived from this software without specific prior written
- *       permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR
- * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
- * FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE
- * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
- * BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
- * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
- * STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice, this
+ * list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ * 3. Neither the name of the copyright holder nor the names of its
+ * contributors may be used to endorse or promote products derived from
+ * this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+ * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide GEMM interface with an elementwise tensor-tensor broadcast epilogue
+    \brief Unit tests for thread-level GEMM
 */
 
-#include <iostream>
+#include "../../common/cutlass_unit_test.h"
 
-#include "cutlass/cutlass.h"
-#include "cute/tensor.hpp"
-#include "cute/atom/mma_atom.hpp"
-
-#include "cutlass/numeric_types.h"
-
-#include "cutlass/gemm/device/gemm_universal_adapter.h"
-#include "cutlass/gemm/kernel/gemm_universal.hpp"
-#include "cutlass/gemm/collective/collective_builder.hpp"
-#include "cutlass/epilogue/collective/epilogue_tensor_broadcast.hpp"
-#include "cutlass/epilogue/thread/linear_combination_tensor_broadcast.hpp"
+#include "cutlass/epilogue/epilogue_workspace.h"
 
-#include "../../common/cutlass_unit_test.h"
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-#include "gemm_testbed_3x_tensor_broadcast.hpp"
+namespace test {
+namespace gemm {
+namespace threadblock {
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-using namespace cute;
+/// Kernel computes accumulator data and stores it out
+template <typename Epilogue>
+__global__ void kernel_epilogue_workspace(typename Epilogue::Params params) {
 
-///////////////////////////////////////////////////////////////////////////////
+  __shared__ typename Epilogue::SharedStorage shared_storage;
 
-TEST(SM90_Device_Gemm_f32t_f32n_f32n_tensor_op_gmma_f32_tensor_broadcast, 64x128x32_1x2x1_ActReLU_Bin0Mul_Bin1Plus_UnaryHardSwish) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using ElementOutput = float;
-  using ElementAccumulator = ElementOutput;
-  using ElementCompute = ElementOutput;
-  using ElementBias = ElementOutput;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      float, LayoutA, 4,
-      float, LayoutB, 4,
-      float,
-      Shape<_64,_128,_128>, Shape<_1,_2,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::detail::Sm90TmaWarpSpecializedAdapter<
-    cutlass::epilogue::collective::EpilogueTensorBroadcast<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombinationTensorBroadcast<
-        ElementOutput, ElementAccumulator, ElementCompute, ElementBias,
-        cutlass::epilogue::thread::ReLu,
-        cutlass::multiplies,
-        cutlass::plus,
-        cutlass::epilogue::thread::HardSwish
-        >,
-      cutlass::gemm::EpilogueDefault>>;
-
-  EXPECT_TRUE(EpilogueOp::IsBinaryOp0Enabled);
-  EXPECT_TRUE(EpilogueOp::IsBinaryOp1Enabled);
-  EXPECT_TRUE(EpilogueOp::IsUnaryOpEnabled);
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
+  int warp_id = threadIdx.y;
+  int lane_id = threadIdx.x;
+
+  Epilogue epilogue(params, shared_storage, warp_id, lane_id);
+
+  //
+  // Initialize accumulator tile
+  //
+  typename Epilogue::FragmentC accum;
+
+  CUTLASS_PRAGMA_UNROLL
+  for (int i = 0; i < Epilogue::FragmentC::kElements; ++i) {
+    accum[i] = Element(warp_id * blockDim.x + lane_id);
+  }
+
+  //
+  // Efficient epilogue
+  //
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAllTensorBroadcast<Gemm>());
+  cutlass::GemmCoord tb_tile_coord{blockIdx.x, blockIdx.y, 0};
+  
+  cutlass::GemmCoord problem_size = 
+    tb_tile_coord * 
+    cutlass::GemmCoord{Epilogue::Shape::kM, Epilogue::Shape::kN, 1};
+
+  // Store accumulators
+  epilogue(
+    problem_size, 
+    tb_tile_coord, 
+    accum);
 }
 
-///////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+} // namespace threadblock
+} // namespace gemm
+} // namespace test
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM75_gemm_threadblock_epilogue_workspace, tensor_op_128x128_64x64) {
+
+  //
+  // Define an instance of the epilogue and see if it works
+  //
+  static int const kWarpCount = 4;
+  static int const kWarpSize = 32;
+
+  using Shape = cutlass::MatrixShape<128, 128>;
+  using FragmentC = cutlass::Array<int, Shape::kCount / (kWarpCount * kWarpSize)>;
+
+  using Epilogue = cutlass::gemm::threadblock::EpilogueWorkspace<
+    Shape,
+    kWarpCount,
+    FragmentC
+  >;
+
+  typename Epilogue::Params params(
+    
+  );
+
+  // Launch the kernel
+  dim3 grid(1,1);
+  dim3 block(kWarpSize, kWarpCount);
+
+  test::gemm::threadblock::kernel_epilogue_workspace<Epilogue><<< grid, block >>>(
+    params
+  );
+
+  cudaError_t result = cudaDeviceSynchronize();
+  EXPECT_EQ(result, cudaSuccess) << "Kernel launch error - " << cudaGetErrorString(result);
+
+  //
+  // 
+  //
+}
 
-#endif // defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_s8_s8_s8_alignx_tensor_op_s32.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_f32_sm80.cu`

 * *Files 21% similar despite different names*

```diff
@@ -25,128 +25,126 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide GEMM interface
+    \brief Tests for device-wide SYRK interface
+  
 */
 
 #include <iostream>
 
-#include "cutlass/cutlass.h"
-#include "cute/tensor.hpp"
-#include "cute/atom/mma_atom.hpp"
-
-#include "cutlass/numeric_types.h"
-
-#include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/device/gemm_universal_adapter.h"
-#include "cutlass/gemm/kernel/gemm_universal.hpp"
-#include "cutlass/gemm/collective/collective_builder.hpp"
-#include "cutlass/epilogue/collective/default_epilogue.hpp"
-#include "cutlass/epilogue/thread/linear_combination.h"
-
 #include "../../common/cutlass_unit_test.h"
+#include "cutlass/blas3.h"
+#include "cutlass/gemm/device/rank_2k.h"
+#include "cutlass/util/host_tensor.h"
+#include "cutlass/util/reference/host/rank_2k.h"
+#include "cutlass/util/reference/host/tensor_compare.h"
+#include "cutlass/util/reference/host/tensor_copy.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/tensor_view_io.h"
+
+#include "testbed_rank2k_universal.h"
 
-#include "gemm_testbed_3x.hpp"
+#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-using namespace cute;
+TEST(SM80_Device_Syr2k_cf32n_cf32t_l_tensor_op_f32, 64x64x16_32x32x16) {
 
-///////////////////////////////////////////////////////////////////////////////
+  using ElementA = cutlass::complex<float>;
+  using LayoutA = cutlass::layout::ColumnMajor;
 
-TEST(SM90_Device_Gemm_s8t_s8n_s8n_align8_tensor_op_gmma_s32, 64x128x128) {
-  using LayoutA = cutlass::layout::RowMajor;
+  using ElementB = cutlass::complex<float>;
   using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
 
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      int8_t, LayoutA, 8,
-      int8_t, LayoutB, 8,
-      int32_t,
-      Shape<_64,_128,_128>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<int8_t, 1, int32_t, int32_t>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
+  using ElementC = cutlass::complex<float>;
+  using LayoutC = cutlass::layout::RowMajor;
+  using ElementAccumulator = cutlass::complex<float>;
+
+  using Rank2K = cutlass::gemm::device::Rank2K<
+    ElementA,
+    LayoutA,
+    ElementB,
+    LayoutB,
+    ElementC,
+    LayoutC,
+    cutlass::FillMode::kLower,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<64, 64, 16>,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 8, 8>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementC,
+      1,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    4,     // kStages 
+    1,     // AlignmentA
+    1,     // AlignmentB
+    false, // SplitKSerial
+    cutlass::arch::OpMultiplyAddComplex,
+    cutlass::ComplexTransform::kNone,
+    cutlass::ComplexTransform::kNone,
+    cutlass::BlasMode::kSymmetric
   >;
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 }
 
-TEST(SM90_Device_Gemm_s8t_s8n_s8n_align16_tensor_op_gmma_s32, 128x128x128) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      int8_t, LayoutA, 16,
-      int8_t, LayoutB, 16,
-      int32_t,
-      Shape<_128,_128,_128>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::KernelMultistage
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<int8_t, 1, int32_t, int32_t>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
+TEST(SM80_Device_Syr2k_cf32n_cf32t_u_tensor_op_f32, 64x64x16_32x32x16) {
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
+  using ElementA = cutlass::complex<float>;
+  using LayoutA = cutlass::layout::ColumnMajor;
 
-TEST(SM90_Device_Gemm_s8t_s8n_s8n_align4_tensor_op_gmma_s32, 128x64x128) {
-  using LayoutA = cutlass::layout::RowMajor;
+  using ElementB = cutlass::complex<float>;
   using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
 
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      int8_t, LayoutA, 4,
-      int8_t, LayoutB, 4,
-      int32_t,
-      Shape<_128,_64,_128>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<int8_t, 1, int32_t, int32_t>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
+  using ElementC = cutlass::complex<float>;
+  using LayoutC = cutlass::layout::RowMajor;
+  using ElementAccumulator = cutlass::complex<float>;
+
+  using Rank2K = cutlass::gemm::device::Rank2K<
+    ElementA,
+    LayoutA,
+    ElementB,
+    LayoutB,
+    ElementC,
+    LayoutC,
+    cutlass::FillMode::kUpper,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<64, 64, 16>,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 8, 8>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementC,
+      1,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    4,     // kStages 
+    1,     // AlignmentA
+    1,     // AlignmentB
+    false, // SplitKSerial
+    cutlass::arch::OpMultiplyAddComplex,
+    cutlass::ComplexTransform::kNone,
+    cutlass::ComplexTransform::kNone,
+    cutlass::BlasMode::kSymmetric
   >;
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 }
 
-///////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_s8_s8_s8_tensor_op_s32_tensor_broadcast.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_reorder.h`

 * *Files 26% similar despite different names*

```diff
@@ -1,102 +1,111 @@
 /***************************************************************************************************
- * Copyright (c) 2023, NVIDIA CORPORATION.  All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * SPDX-License-Identifier: BSD-3-Clause
  *
- * Redistribution and use in source and binary forms, with or without modification, are permitted
- * provided that the following conditions are met:
- *     * Redistributions of source code must retain the above copyright notice, this list of
- *       conditions and the following disclaimer.
- *     * Redistributions in binary form must reproduce the above copyright notice, this list of
- *       conditions and the following disclaimer in the documentation and/or other materials
- *       provided with the distribution.
- *     * Neither the name of the NVIDIA CORPORATION nor the names of its contributors may be used
- *       to endorse or promote products derived from this software without specific prior written
- *       permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR
- * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
- * FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE
- * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
- * BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
- * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
- * STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice, this
+ * list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ * 3. Neither the name of the copyright holder nor the names of its
+ * contributors may be used to endorse or promote products derived from
+ * this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+ * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
+
 /*! \file
-    \brief Tests for device-wide GEMM interface with an elementwise tensor-tensor broadcast epilogue
+    \brief reorder data from the host side 
 */
 
-#include <iostream>
-
-#include "cutlass/cutlass.h"
-#include "cute/tensor.hpp"
-#include "cute/atom/mma_atom.hpp"
-
-#include "cutlass/numeric_types.h"
-
-#include "cutlass/gemm/device/gemm_universal_adapter.h"
-#include "cutlass/gemm/kernel/gemm_universal.hpp"
-#include "cutlass/gemm/collective/collective_builder.hpp"
-#include "cutlass/epilogue/collective/epilogue_tensor_broadcast.hpp"
-#include "cutlass/epilogue/thread/linear_combination_tensor_broadcast.hpp"
-
-#include "../../common/cutlass_unit_test.h"
-
-#include "gemm_testbed_3x_tensor_broadcast.hpp"
-
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
-
-using namespace cute;
-
-///////////////////////////////////////////////////////////////////////////////
-
-TEST(SM90_Device_Gemm_s8t_s8n_s8n_tensor_op_gmma_s32_tensor_broadcast, 128x128x128_2x2x1_ActReLU_Bin0Mul_Bin1Plus_UnaryHardSwish) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using ElementOutput = int32_t;
-  using ElementAccumulator = ElementOutput;
-  using ElementCompute = ElementOutput;
-  using ElementBias = ElementOutput;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      int8_t, LayoutA, 16,
-      int8_t, LayoutB, 16,
-      int32_t,
-      Shape<_128,_128,_128>, Shape<_2,_2,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::detail::Sm90TmaWarpSpecializedAdapter<
-    cutlass::epilogue::collective::EpilogueTensorBroadcast<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombinationTensorBroadcast<
-        ElementOutput, ElementAccumulator, ElementCompute, ElementBias,
-        cutlass::epilogue::thread::ReLu,
-        cutlass::multiplies,
-        cutlass::plus,
-        cutlass::epilogue::thread::HardSwish
-        >,
-      cutlass::gemm::EpilogueDefault>>;
-
-  EXPECT_TRUE(EpilogueOp::IsBinaryOp0Enabled);
-  EXPECT_TRUE(EpilogueOp::IsBinaryOp1Enabled);
-  EXPECT_TRUE(EpilogueOp::IsUnaryOpEnabled);
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
+#pragma once
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAllTensorBroadcast<Gemm>());
+#include "cutlass/coord.h"
+#include "cutlass/util/host_tensor.h"
+#include "cutlass/tensor_view.h"
+#include "cutlass/util/tensor_view_io.h"
+#include "cutlass/util/reference/host/gemm.h"
+
+namespace cutlass {
+
+/// This is needed for the interleaved integer tensor core kernels.  The purpose
+/// is to use skip the shared memory part in the epilogue.
+template <int Interleaved, typename Element, typename Layout>
+void reorder_column(TensorRef<Element, Layout> dest,
+                    TensorRef<Element, Layout> src,
+                    cutlass::gemm::GemmCoord problem_size) {
+  const int InstructionShapeCol = 8;
+  // 4 threads per Quad
+  const int ElementsPerThread = InstructionShapeCol / 4;
+  // 4 threads per Quad
+  const int ReorderedElementsPerThread =
+      Interleaved / 4;
+
+  for (int n = 0; n < problem_size.n(); n++) {
+    for (int k = 0; k < problem_size.k(); k++) {
+      dest.at({k, (n / Interleaved) * Interleaved +
+                      ((n % ReorderedElementsPerThread) / ElementsPerThread) *
+                          InstructionShapeCol +
+                      ((n % Interleaved) / ReorderedElementsPerThread) *
+                          ElementsPerThread +
+                      (n % ElementsPerThread)}) = src.at({k, n});
+    }
+  }
 }
 
-///////////////////////////////////////////////////////////////////////////////
+template <int ColumnInterleaved, int LayoutInterleaved = ColumnInterleaved, typename Element, typename Layout>
+void reorder_convK(TensorRef<Element, Layout> dest,
+                    TensorRef<Element, Layout> src,
+                    cutlass::gemm::GemmCoord problem_size) {
+
+    TensorRef<Element, layout::RowMajorInterleaved<LayoutInterleaved>> mappedDest(dest.data(), dest.stride(0));
+    TensorRef<Element, layout::RowMajorInterleaved<LayoutInterleaved>> mappedSrc(src.data(), src.stride(0));
+    
+    reorder_column<ColumnInterleaved>(
+        mappedDest, mappedSrc, problem_size);
+}
 
-#endif // defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+/// This is needed for the sparse tensor core kernels.  The purpose
+/// is to use ldmatrix to load from shared memory to the register file.
+template <typename Element, typename LayoutDest, typename LayoutSrc>
+void reorder_meta(TensorRef<Element, LayoutDest> dest,
+                  TensorRef<Element, LayoutSrc> src,
+                  cutlass::gemm::GemmCoord problem_size) {
+  for (int m = 0; m < problem_size.m(); m++) {
+    for (int k = 0; k < problem_size.k(); k++) {
+      // First reorder the rows.
+      int group = (sizeof(Element) == 2) ? 32 : 16;
+      int interweave = (sizeof(Element) == 2) ? 4 : 2;
+
+      int dest_row = m / group * group + (m % 8) * interweave + (m % group) / 8;
+      int dest_col = k;
+
+      // Next swizzle the 2x2 blocks from Z to N.
+      if (((dest_row % 2) == 0) && ((dest_col % 2) == 1)) {
+        ++dest_row;
+        --dest_col;
+      } else if (((dest_row % 2) == 1) && ((dest_col % 2) == 0)) {
+        --dest_row;
+        ++dest_col;
+      }
+
+      dest.at({dest_row, dest_col}) = src.at({m, k});
+    }
+  }
+}
+} // namespace cutlass
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_alignx_tensor_op_f32.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_fast_f32_sm80.cu`

 * *Files 18% similar despite different names*

```diff
@@ -25,127 +25,126 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide GEMM interface
+    \brief Tests for device-wide SYRK interface
+  
 */
 
 #include <iostream>
 
-#include "cutlass/cutlass.h"
-#include "cute/tensor.hpp"
-#include "cute/atom/mma_atom.hpp"
-
-#include "cutlass/numeric_types.h"
-
-#include "cutlass/gemm/device/gemm_universal_adapter.h"
-#include "cutlass/gemm/kernel/gemm_universal.hpp"
-#include "cutlass/gemm/collective/collective_builder.hpp"
-#include "cutlass/epilogue/collective/default_epilogue.hpp"
-#include "cutlass/epilogue/thread/linear_combination.h"
-
 #include "../../common/cutlass_unit_test.h"
+#include "cutlass/blas3.h"
+#include "cutlass/gemm/device/rank_2k.h"
+#include "cutlass/util/host_tensor.h"
+#include "cutlass/util/reference/host/rank_2k.h"
+#include "cutlass/util/reference/host/tensor_compare.h"
+#include "cutlass/util/reference/host/tensor_copy.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/tensor_view_io.h"
+
+#include "testbed_rank2k_universal.h"
 
-#include "gemm_testbed_3x.hpp"
+#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-using namespace cute;
+TEST(SM80_Device_Syr2k_cf32n_cf32t_l_tensor_op_fast_f32, 64x64x16_32x32x16) {
 
-///////////////////////////////////////////////////////////////////////////////
+  using ElementA = cutlass::complex<float>;
+  using LayoutA = cutlass::layout::ColumnMajor;
 
-TEST(SM90_Device_Gemm_tf32t_tf32n_f32n_align4_tensor_op_gmma_f32, 64x128x32) {
-  using LayoutA = cutlass::layout::RowMajor;
+  using ElementB = cutlass::complex<float>;
   using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
 
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      tfloat32_t, LayoutA, 4,
-      tfloat32_t, LayoutB, 4,
-      float,
-      Shape<_64,_128,_32>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::KernelMultistage
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<float, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
+  using ElementC = cutlass::complex<float>;
+  using LayoutC = cutlass::layout::RowMajor;
+  using ElementAccumulator = cutlass::complex<float>;
+
+  using Rank2K = cutlass::gemm::device::Rank2K<
+    ElementA,
+    LayoutA,
+    ElementB,
+    LayoutB,
+    ElementC,
+    LayoutC,
+    cutlass::FillMode::kLower,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<64, 64, 16>,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 8, 8>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementC,
+      1,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    4,     // kStages 
+    1,     // AlignmentA
+    1,     // AlignmentB
+    false, // SplitKSerial
+    cutlass::arch::OpMultiplyAddComplexFastF32,
+    cutlass::ComplexTransform::kNone,
+    cutlass::ComplexTransform::kNone,
+    cutlass::BlasMode::kSymmetric
   >;
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 }
 
-TEST(SM90_Device_Gemm_tf32t_tf32n_f32n_align2_tensor_op_gmma_f32, 64x64x32) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::tfloat32_t, LayoutA, 2,
-      cutlass::tfloat32_t, LayoutB, 2,
-      float,
-      Shape<_64,_64,_32>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<float, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
+TEST(SM80_Device_Syr2k_cf32n_cf32t_u_tensor_op_fast_f32, 64x64x16_32x32x16) {
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
+  using ElementA = cutlass::complex<float>;
+  using LayoutA = cutlass::layout::ColumnMajor;
 
-TEST(SM90_Device_Gemm_tf32t_tf32n_f32n_align1_tensor_op_gmma_f32, 128x64x32) {
-  using LayoutA = cutlass::layout::RowMajor;
+  using ElementB = cutlass::complex<float>;
   using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
 
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::tfloat32_t, LayoutA, 1,
-      cutlass::tfloat32_t, LayoutB, 1,
-      float,
-      Shape<_128,_64,_32>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<float, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
+  using ElementC = cutlass::complex<float>;
+  using LayoutC = cutlass::layout::RowMajor;
+  using ElementAccumulator = cutlass::complex<float>;
+
+  using Rank2K = cutlass::gemm::device::Rank2K<
+    ElementA,
+    LayoutA,
+    ElementB,
+    LayoutB,
+    ElementC,
+    LayoutC,
+    cutlass::FillMode::kUpper,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<64, 64, 16>,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 8, 8>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementC,
+      1,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    4,     // kStages 
+    1,     // AlignmentA
+    1,     // AlignmentB
+    false, // SplitKSerial
+    cutlass::arch::OpMultiplyAddComplexFastF32,
+    cutlass::ComplexTransform::kNone,
+    cutlass::ComplexTransform::kNone,
+    cutlass::BlasMode::kSymmetric
   >;
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 }
 
-///////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_tensor_op_f32.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64t_tensor_op_f64_grouped_sm80.cu`

 * *Files 24% similar despite different names*

```diff
@@ -25,161 +25,144 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide GEMM interface
+    \brief Tests for grouped Rank2K interface
 */
 
 #include <iostream>
 
+#include "../../common/cutlass_unit_test.h"
 #include "cutlass/cutlass.h"
-#include "cute/tensor.hpp"
-#include "cute/atom/mma_atom.hpp"
-
-#include "cutlass/numeric_types.h"
 
-#include "cutlass/gemm/device/gemm_universal_adapter.h"
-#include "cutlass/gemm/kernel/gemm_universal.hpp"
-#include "cutlass/gemm/collective/collective_builder.hpp"
-#include "cutlass/epilogue/collective/default_epilogue.hpp"
-#include "cutlass/epilogue/thread/linear_combination.h"
+#include "cutlass/blas3.h"
+#include "cutlass/gemm/gemm.h"
+#include "cutlass/gemm/kernel/rank_2k_grouped.h"
+#include "cutlass/gemm/kernel/default_rank_2k_grouped.h"
+#include "cutlass/gemm/device/rank_2k_grouped.h"
 
-#include "../../common/cutlass_unit_test.h"
+#include "cutlass/util/host_tensor.h"
+#include "cutlass/util/reference/host/gemm.h"
+#include "cutlass/util/reference/host/tensor_compare.h"
+#include "cutlass/util/reference/host/tensor_copy.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/tensor_view_io.h"
 
-#include "gemm_testbed_3x.hpp"
+#include "testbed_grouped_rank_2k.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-using namespace cute;
+#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
-TEST(SM90_Device_Gemm_tf32t_tf32n_f32n_tensor_op_gmma_f32, 64x128x32) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::tfloat32_t, LayoutA, 4,
-      cutlass::tfloat32_t, LayoutB, 4,
-      float,
-      Shape<_64,_128,_32>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<float, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
+TEST(SM80_Device_Syr2kGrouped_cf64t_cf64t_l_tensor_op_f64, 32x32x16_16x16x16) {
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+  using ElementA = cutlass::complex<double>;
+  using LayoutA = cutlass::layout::RowMajor;
+  using ElementB = cutlass::complex<double>;
+  using LayoutB = cutlass::layout::RowMajor;
+  using ElementC = cutlass::complex<double>;
+  using LayoutC = cutlass::layout::RowMajor;
+  using ElementAccumulator = cutlass::complex<double>;
+
+  using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
+    ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
+    ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
+    ElementC, LayoutC, cutlass::FillMode::kLower,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    3, // kStages
+    cutlass::arch::OpMultiplyAddComplex,
+    cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
+
+  using Rank2K = cutlass::gemm::device::Rank2KGrouped<Rank2Kkernel>;
+
+  test::gemm::device::TestbedGrouped<Rank2K> testbed;
+  bool passed = testbed.run(24);
+  EXPECT_TRUE(passed);
 }
 
-///////////////////////////////////////////////////////////////////////////////
-
-TEST(SM90_Device_Gemm_tf32n_tf32n_f32n_tensor_op_gmma_f32, 64x128x32) {
-  using LayoutA = cutlass::layout::ColumnMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::tfloat32_t, LayoutA, 1,
-      cutlass::tfloat32_t, LayoutB, 4,
-      float,
-      Shape<_64,_128,_32>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<float, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-///////////////////////////////////////////////////////////////////////////////
+TEST(SM80_Device_Syr2kGrouped_cf64t_cf64t_l_tensor_op_f64, 64x64x16_32x32x16) {
 
-TEST(SM90_Device_Gemm_tf32n_tf32t_f32n_tensor_op_gmma_f32, 64x128x32) {
-  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementA = cutlass::complex<double>;
+  using LayoutA = cutlass::layout::RowMajor;
+  using ElementB = cutlass::complex<double>;
   using LayoutB = cutlass::layout::RowMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::tfloat32_t, LayoutA, 1,
-      cutlass::tfloat32_t, LayoutB, 1,
-      float,
-      Shape<_64,_128,_32>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<float, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+  using ElementC = cutlass::complex<double>;
+  using LayoutC = cutlass::layout::RowMajor;
+  using ElementAccumulator = cutlass::complex<double>;
+
+  using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
+    ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
+    ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
+    ElementC, LayoutC, cutlass::FillMode::kLower,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<64, 64, 16>,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    3, // kStages
+    cutlass::arch::OpMultiplyAddComplex,
+    cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
+
+  using Rank2K = cutlass::gemm::device::Rank2KGrouped<Rank2Kkernel>;
+
+  test::gemm::device::TestbedGrouped<Rank2K> testbed;
+  bool passed = testbed.run(24);
+  EXPECT_TRUE(passed);
 }
 
-///////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Gemm_tf32t_tf32t_f32n_tensor_op_gmma_f32, 64x128x32) {
+TEST(SM80_Device_Syr2kGrouped_cf64t_cf64t_u_tensor_op_f64, 32x32x16_16x16x16) {
+
+  using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::RowMajor;
+  using ElementB = cutlass::complex<double>;
   using LayoutB = cutlass::layout::RowMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
-
-  using CollectiveOp = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      cutlass::tfloat32_t, LayoutA, 4,
-      cutlass::tfloat32_t, LayoutB, 1,
-      float,
-      Shape<_64,_128,_32>, Shape<_1,_1,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
-
-  using EpilogueOp = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<float, 1, float, float>>;
-
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveOp,
-      EpilogueOp
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+  using ElementC = cutlass::complex<double>;
+  using LayoutC = cutlass::layout::RowMajor;
+  using ElementAccumulator = cutlass::complex<double>;
+
+  using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
+    ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
+    ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
+    ElementC, LayoutC, cutlass::FillMode::kUpper,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    3, // kStages
+    cutlass::arch::OpMultiplyAddComplex,
+    cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
+
+  using Rank2K = cutlass::gemm::device::Rank2KGrouped<Rank2Kkernel>;
+
+  test::gemm::device::TestbedGrouped<Rank2K> testbed;
+  bool passed = testbed.run(24);
+  EXPECT_TRUE(passed);
 }
 
-///////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
-#endif // defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_ls_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_rs_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_rs_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_ls_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_rs_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_rs_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64_cf64_cf64_tensor_op_f64_sm90.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_sm80.cu`

 * *Files 8% similar despite different names*

```diff
@@ -25,109 +25,112 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide SYMM interface
-
+    \brief Tests for device-wide SYRK interface
   
 */
 
 #include <iostream>
 
 #include "../../common/cutlass_unit_test.h"
 #include "cutlass/blas3.h"
-#include "cutlass/gemm/device/symm.h"
+#include "cutlass/gemm/device/rank_k.h"
 #include "cutlass/util/host_tensor.h"
-#include "cutlass/util/reference/host/symm_complex.h"
+#include "cutlass/util/reference/host/rank_k_complex.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
-#include "testbed_symm_universal.h"
+#include "testbed_rank_k_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Symm_cf64n_cf64n_ls_l_tensor_op_f64_gaussian, 32x32x16_16x16x16) {
+TEST(SM80_Device_Syrk_cf64n_cf64t_l_tensor_op_f64, 32x32x16_16x16x16) {
+
+  using ElementA = cutlass::complex<double>;
+  using LayoutA = cutlass::layout::ColumnMajor;
 
-  using ElementOutput = cutlass::complex<double>;
+  using ElementC = cutlass::complex<double>;
+  using LayoutC = cutlass::layout::RowMajor;
   using ElementAccumulator = cutlass::complex<double>;
 
-  using Symm = cutlass::gemm::device::Symm<
-    cutlass::complex<double>,
-    cutlass::layout::ColumnMajor,
-    cutlass::SideMode::kLeft,
+  using RankK = cutlass::gemm::device::RankK<
+    ElementA,
+    LayoutA,
+    ElementC,
+    LayoutC,
     cutlass::FillMode::kLower,
-    cutlass::complex<double>,
-    cutlass::layout::ColumnMajor,
-    ElementOutput,
-    cutlass::layout::ColumnMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm90,
+    cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<32, 32, 16>,
     cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<16, 8, 4>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
-      ElementOutput,
+      ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    4,
-    1,
-    1,
-    false,
-    cutlass::arch::OpMultiplyAddGaussianComplex
+    4,     // kStages 
+    1,     // AlignmentA
+    false, // SplitKSerial
+    cutlass::arch::OpMultiplyAddComplex,
+    cutlass::ComplexTransform::kNone,
+    cutlass::BlasMode::kSymmetric
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllSymmUniversal<Symm>());
+  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Symm_cf64n_cf64n_rs_u_tensor_op_f64, 64x64x16_32x32x16) {
+TEST(SM80_Device_Syrk_cf64n_cf64t_u_tensor_op_f64, 32x32x16_16x16x16) {
 
-  using ElementOutput = cutlass::complex<double>;
+  using ElementA = cutlass::complex<double>;
+  using LayoutA = cutlass::layout::ColumnMajor;
+
+  using ElementC = cutlass::complex<double>;
+  using LayoutC = cutlass::layout::RowMajor;
   using ElementAccumulator = cutlass::complex<double>;
 
-  using Symm = cutlass::gemm::device::Symm<
-    cutlass::complex<double>,
-    cutlass::layout::ColumnMajor,
-    cutlass::SideMode::kRight,
+  using RankK = cutlass::gemm::device::RankK<
+    ElementA,
+    LayoutA,
+    ElementC,
+    LayoutC,
     cutlass::FillMode::kUpper,
-    cutlass::complex<double>,
-    cutlass::layout::ColumnMajor,
-    ElementOutput,
-    cutlass::layout::ColumnMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm90,
-    cutlass::gemm::GemmShape<64, 64, 16>,
+    cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<32, 32, 16>,
-    cutlass::gemm::GemmShape<16, 8, 4>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
-      ElementOutput,
+      ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    4,
-    1,
-    1,
-    false,
-    cutlass::arch::OpMultiplyAddComplex
+    4,     // kStages 
+    1,     // AlignmentA
+    false, // SplitKSerial
+    cutlass::arch::OpMultiplyAddComplex,
+    cutlass::ComplexTransform::kNone,
+    cutlass::BlasMode::kSymmetric
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllSymmUniversal<Symm>());
+  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_rs_f64_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_rs_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_rs_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_rs_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64_f64_tensor_op_f64_sm90.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64_f64_tensor_op_f64_sm90.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_ls_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_rs_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_rs_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_ls_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_rs_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_rs_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_ls_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_rs_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_rs_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_ls_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_rs_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_rs_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_ls_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_rs_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_rs_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32t_f32t_tensor_op_f32_ls_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32t_f32t_tensor_op_f32_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_fast_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_sm80.cu`

 * *Files 6% similar despite different names*

```diff
@@ -47,40 +47,40 @@
 
 #include "testbed_rank2k_universal.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_cf32n_cf32t_l_tensor_op_f32, 64x64x16_32x32x16) {
+TEST(SM80_Device_Syr2k_cf64n_cf64n_l_tensor_op_f64, 32x32x16_16x16x16) {
 
-  using ElementA = cutlass::complex<float>;
+  using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::ColumnMajor;
 
-  using ElementB = cutlass::complex<float>;
+  using ElementB = cutlass::complex<double>;
   using LayoutB = cutlass::layout::ColumnMajor;
 
-  using ElementC = cutlass::complex<float>;
-  using LayoutC = cutlass::layout::RowMajor;
-  using ElementAccumulator = cutlass::complex<float>;
+  using ElementC = cutlass::complex<double>;
+  using LayoutC = cutlass::layout::ColumnMajor;
+  using ElementAccumulator = cutlass::complex<double>;
 
   using Rank2K = cutlass::gemm::device::Rank2K<
     ElementA,
     LayoutA,
     ElementB,
     LayoutB,
     ElementC,
     LayoutC,
     cutlass::FillMode::kLower,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<64, 64, 16>,
     cutlass::gemm::GemmShape<32, 32, 16>,
-    cutlass::gemm::GemmShape<16, 8, 8>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
@@ -95,40 +95,40 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_cf32n_cf32t_u_tensor_op_f32, 64x64x16_32x32x16) {
+TEST(SM80_Device_Syr2k_cf64n_cf64n_u_tensor_op_f64, 32x32x16_16x16x16) {
 
-  using ElementA = cutlass::complex<float>;
+  using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::ColumnMajor;
 
-  using ElementB = cutlass::complex<float>;
+  using ElementB = cutlass::complex<double>;
   using LayoutB = cutlass::layout::ColumnMajor;
 
-  using ElementC = cutlass::complex<float>;
-  using LayoutC = cutlass::layout::RowMajor;
-  using ElementAccumulator = cutlass::complex<float>;
+  using ElementC = cutlass::complex<double>;
+  using LayoutC = cutlass::layout::ColumnMajor;
+  using ElementAccumulator = cutlass::complex<double>;
 
   using Rank2K = cutlass::gemm::device::Rank2K<
     ElementA,
     LayoutA,
     ElementB,
     LayoutB,
     ElementC,
     LayoutC,
     cutlass::FillMode::kUpper,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<64, 64, 16>,
     cutlass::gemm::GemmShape<32, 32, 16>,
-    cutlass::gemm::GemmShape<16, 8, 8>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_f32_sm80.cu`

 * *Files 6% similar despite different names*

```diff
@@ -26,125 +26,112 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
     \brief Tests for device-wide SYRK interface
+
   
 */
 
 #include <iostream>
 
 #include "../../common/cutlass_unit_test.h"
 #include "cutlass/blas3.h"
-#include "cutlass/gemm/device/rank_2k.h"
+#include "cutlass/gemm/device/rank_k.h"
 #include "cutlass/util/host_tensor.h"
-#include "cutlass/util/reference/host/rank_2k.h"
+#include "cutlass/util/reference/host/rank_k_complex.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
-#include "testbed_rank2k_universal.h"
+#include "testbed_rank_k_universal.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_cf32n_cf32t_l_tensor_op_fast_f32, 64x64x16_32x32x16) {
+TEST(SM80_Device_Syrk_cf32n_cf32n_l_tensor_op_f32, 32x32x16_16x16x16) {
 
   using ElementA = cutlass::complex<float>;
   using LayoutA = cutlass::layout::ColumnMajor;
 
-  using ElementB = cutlass::complex<float>;
-  using LayoutB = cutlass::layout::ColumnMajor;
-
   using ElementC = cutlass::complex<float>;
-  using LayoutC = cutlass::layout::RowMajor;
+  using LayoutC = cutlass::layout::ColumnMajor;
   using ElementAccumulator = cutlass::complex<float>;
 
-  using Rank2K = cutlass::gemm::device::Rank2K<
+  using RankK = cutlass::gemm::device::RankK<
     ElementA,
     LayoutA,
-    ElementB,
-    LayoutB,
     ElementC,
     LayoutC,
     cutlass::FillMode::kLower,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<64, 64, 16>,
     cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
     cutlass::gemm::GemmShape<16, 8, 8>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    4,     // kStages 
+    3,     // kStages 
     1,     // AlignmentA
-    1,     // AlignmentB
     false, // SplitKSerial
-    cutlass::arch::OpMultiplyAddComplexFastF32,
-    cutlass::ComplexTransform::kNone,
+    cutlass::arch::OpMultiplyAddComplex,
     cutlass::ComplexTransform::kNone,
     cutlass::BlasMode::kSymmetric
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
+  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_cf32n_cf32t_u_tensor_op_fast_f32, 64x64x16_32x32x16) {
+TEST(SM80_Device_Syrk_cf32n_cf32n_u_tensor_op_f32, 32x32x16_16x16x16) {
 
   using ElementA = cutlass::complex<float>;
   using LayoutA = cutlass::layout::ColumnMajor;
 
-  using ElementB = cutlass::complex<float>;
-  using LayoutB = cutlass::layout::ColumnMajor;
-
   using ElementC = cutlass::complex<float>;
-  using LayoutC = cutlass::layout::RowMajor;
+  using LayoutC = cutlass::layout::ColumnMajor;
   using ElementAccumulator = cutlass::complex<float>;
 
-  using Rank2K = cutlass::gemm::device::Rank2K<
+  using RankK = cutlass::gemm::device::RankK<
     ElementA,
     LayoutA,
-    ElementB,
-    LayoutB,
     ElementC,
     LayoutC,
     cutlass::FillMode::kUpper,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<64, 64, 16>,
     cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
     cutlass::gemm::GemmShape<16, 8, 8>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    4,     // kStages 
+    3,     // kStages 
     1,     // AlignmentA
-    1,     // AlignmentB
     false, // SplitKSerial
-    cutlass::arch::OpMultiplyAddComplexFastF32,
-    cutlass::ComplexTransform::kNone,
+    cutlass::arch::OpMultiplyAddComplex,
     cutlass::ComplexTransform::kNone,
     cutlass::BlasMode::kSymmetric
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
+  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 #endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64_cf64_tensor_op_f64_sm90.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_sm80.cu`

 * *Files 6% similar despite different names*

```diff
@@ -43,44 +43,44 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_rank2k_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Syr2k_cf64n_cf64n_l_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM80_Device_Syr2k_cf64n_cf64t_l_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::ColumnMajor;
 
   using ElementB = cutlass::complex<double>;
   using LayoutB = cutlass::layout::ColumnMajor;
 
   using ElementC = cutlass::complex<double>;
-  using LayoutC = cutlass::layout::ColumnMajor;
+  using LayoutC = cutlass::layout::RowMajor;
   using ElementAccumulator = cutlass::complex<double>;
 
   using Rank2K = cutlass::gemm::device::Rank2K<
     ElementA,
     LayoutA,
     ElementB,
     LayoutB,
     ElementC,
     LayoutC,
     cutlass::FillMode::kLower,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm90,
+    cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<32, 32, 16>,
     cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<16, 8, 4>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
@@ -95,15 +95,15 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Syr2k_cf64n_cf64t_u_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM80_Device_Syr2k_cf64n_cf64t_u_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::ColumnMajor;
 
   using ElementB = cutlass::complex<double>;
   using LayoutB = cutlass::layout::ColumnMajor;
 
@@ -117,18 +117,18 @@
     ElementB,
     LayoutB,
     ElementC,
     LayoutC,
     cutlass::FillMode::kUpper,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm90,
+    cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<32, 32, 16>,
     cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<16, 8, 4>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
@@ -143,8 +143,8 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_fast_f32_sm80.cu`

 * *Files 6% similar despite different names*

```diff
@@ -26,125 +26,112 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
     \brief Tests for device-wide SYRK interface
+
   
 */
 
 #include <iostream>
 
 #include "../../common/cutlass_unit_test.h"
 #include "cutlass/blas3.h"
-#include "cutlass/gemm/device/rank_2k.h"
+#include "cutlass/gemm/device/rank_k.h"
 #include "cutlass/util/host_tensor.h"
-#include "cutlass/util/reference/host/rank_2k.h"
+#include "cutlass/util/reference/host/rank_k_complex.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
-#include "testbed_rank2k_universal.h"
+#include "testbed_rank_k_universal.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_cf64n_cf64n_l_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM80_Device_Syrk_cf32n_cf32t_l_tensor_op_fast_f32, 32x32x16_16x16x16) {
 
-  using ElementA = cutlass::complex<double>;
+  using ElementA = cutlass::complex<float>;
   using LayoutA = cutlass::layout::ColumnMajor;
 
-  using ElementB = cutlass::complex<double>;
-  using LayoutB = cutlass::layout::ColumnMajor;
-
-  using ElementC = cutlass::complex<double>;
-  using LayoutC = cutlass::layout::ColumnMajor;
-  using ElementAccumulator = cutlass::complex<double>;
+  using ElementC = cutlass::complex<float>;
+  using LayoutC = cutlass::layout::RowMajor;
+  using ElementAccumulator = cutlass::complex<float>;
 
-  using Rank2K = cutlass::gemm::device::Rank2K<
+  using RankK = cutlass::gemm::device::RankK<
     ElementA,
     LayoutA,
-    ElementB,
-    LayoutB,
     ElementC,
     LayoutC,
     cutlass::FillMode::kLower,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<32, 32, 16>,
     cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::gemm::GemmShape<16, 8, 8>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    4,     // kStages 
+    3,     // kStages 
     1,     // AlignmentA
-    1,     // AlignmentB
     false, // SplitKSerial
-    cutlass::arch::OpMultiplyAddComplex,
-    cutlass::ComplexTransform::kNone,
+    cutlass::arch::OpMultiplyAddComplexFastF32,
     cutlass::ComplexTransform::kNone,
     cutlass::BlasMode::kSymmetric
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
+  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_cf64n_cf64n_u_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM80_Device_Syrk_cf32n_cf32t_u_tensor_op_fast_f32, 32x32x16_16x16x16) {
 
-  using ElementA = cutlass::complex<double>;
+  using ElementA = cutlass::complex<float>;
   using LayoutA = cutlass::layout::ColumnMajor;
 
-  using ElementB = cutlass::complex<double>;
-  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = cutlass::complex<float>;
+  using LayoutC = cutlass::layout::RowMajor;
+  using ElementAccumulator = cutlass::complex<float>;
 
-  using ElementC = cutlass::complex<double>;
-  using LayoutC = cutlass::layout::ColumnMajor;
-  using ElementAccumulator = cutlass::complex<double>;
-
-  using Rank2K = cutlass::gemm::device::Rank2K<
+  using RankK = cutlass::gemm::device::RankK<
     ElementA,
     LayoutA,
-    ElementB,
-    LayoutB,
     ElementC,
     LayoutC,
     cutlass::FillMode::kUpper,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<32, 32, 16>,
     cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::gemm::GemmShape<16, 8, 8>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    4,     // kStages 
+    3,     // kStages 
     1,     // AlignmentA
-    1,     // AlignmentB
     false, // SplitKSerial
-    cutlass::arch::OpMultiplyAddComplex,
-    cutlass::ComplexTransform::kNone,
+    cutlass::arch::OpMultiplyAddComplexFastF32,
     cutlass::ComplexTransform::kNone,
     cutlass::BlasMode::kSymmetric
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
+  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 #endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_grouped_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_gaussian_complex_sm80.cu`

 * *Files 26% similar despite different names*

```diff
@@ -24,145 +24,264 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/*! \file
-    \brief Tests for grouped Rank2K interface
-*/
+/*! \file 
 
-#include <iostream>
+    \brief Unit tests for thread-level GEMM
+*/
 
-#include "../../common/cutlass_unit_test.h"
 #include "cutlass/cutlass.h"
+#include "../../common/cutlass_unit_test.h"
+
+#include "cutlass/aligned_buffer.h"
+#include "cutlass/half.h"
 
-#include "cutlass/blas3.h"
-#include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/kernel/rank_2k_grouped.h"
-#include "cutlass/gemm/kernel/default_rank_2k_grouped.h"
-#include "cutlass/gemm/device/rank_2k_grouped.h"
+#include "cutlass/gemm/warp/default_mma_complex_tensor_op.h"
 
+#include "cutlass/core_io.h"
 #include "cutlass/util/host_tensor.h"
-#include "cutlass/util/reference/host/gemm.h"
-#include "cutlass/util/reference/host/tensor_compare.h"
-#include "cutlass/util/reference/host/tensor_copy.h"
-#include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
-#include "testbed_grouped_rank_2k.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/reference/host/tensor_compare.h"
+#include "cutlass/util/reference/host/gemm.h"
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+#include "testbed.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+///////////////////////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM80_warp_gemm_gaussian_complex_tensor_op, 8x8x4_8x8x4_nt) {
+
+  using Shape = cutlass::gemm::GemmShape<8, 8, 4>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
+  
+  using Element = cutlass::complex<double>;
+  using ElementC = cutlass::complex<double>;
+
+  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous128b;
+  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous128b;
 
-TEST(SM80_Device_Syr2kGrouped_cf64n_cf64t_l_tensor_op_f64, 32x32x16_16x16x16) {
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor,
+    cutlass::ComplexTransform::kNone,
+    cutlass::ComplexTransform::kNone,
+    cutlass::arch::OpMultiplyAddGaussianComplex
+  >::Type;
 
-  using ElementA = cutlass::complex<double>;
-  using LayoutA = cutlass::layout::ColumnMajor;
-  using ElementB = cutlass::complex<double>;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using ElementC = cutlass::complex<double>;
-  using LayoutC = cutlass::layout::RowMajor;
-  using ElementAccumulator = cutlass::complex<double>;
-
-  using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
-    ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
-    ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
-    ElementC, LayoutC, cutlass::FillMode::kLower,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<32, 32, 16>,
-    cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
-    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3, // kStages
-    cutlass::arch::OpMultiplyAddComplex,
-    cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
-
-  using Rank2K = cutlass::gemm::device::Rank2KGrouped<Rank2Kkernel>;
-
-  test::gemm::device::TestbedGrouped<Rank2K> testbed;
-  bool passed = testbed.run(24);
-  EXPECT_TRUE(passed);
-}
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-TEST(SM80_Device_Syr2kGrouped_cf64n_cf64t_l_tensor_op_f64, 64x64x16_32x32x16) {
-
-  using ElementA = cutlass::complex<double>;
-  using LayoutA = cutlass::layout::ColumnMajor;
-  using ElementB = cutlass::complex<double>;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using ElementC = cutlass::complex<double>;
-  using LayoutC = cutlass::layout::RowMajor;
-  using ElementAccumulator = cutlass::complex<double>;
-
-  using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
-    ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
-    ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
-    ElementC, LayoutC, cutlass::FillMode::kLower,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<64, 64, 16>,
-    cutlass::gemm::GemmShape<32, 32, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
-    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3, // kStages
-    cutlass::arch::OpMultiplyAddComplex,
-    cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
-
-  using Rank2K = cutlass::gemm::device::Rank2KGrouped<Rank2Kkernel>;
-
-  test::gemm::device::TestbedGrouped<Rank2K> testbed;
-  bool passed = testbed.run(24);
-  EXPECT_TRUE(passed);
-}
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-TEST(SM80_Device_Syr2kGrouped_cf64n_cf64t_u_tensor_op_f64, 32x32x16_16x16x16) {
-
-  using ElementA = cutlass::complex<double>;
-  using LayoutA = cutlass::layout::ColumnMajor;
-  using ElementB = cutlass::complex<double>;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using ElementC = cutlass::complex<double>;
-  using LayoutC = cutlass::layout::RowMajor;
-  using ElementAccumulator = cutlass::complex<double>;
-
-  using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
-    ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
-    ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
-    ElementC, LayoutC, cutlass::FillMode::kUpper,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<32, 32, 16>,
-    cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
-    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3, // kStages
-    cutlass::arch::OpMultiplyAddComplex,
-    cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
-
-  using Rank2K = cutlass::gemm::device::Rank2KGrouped<Rank2Kkernel>;
-
-  test::gemm::device::TestbedGrouped<Rank2K> testbed;
-  bool passed = testbed.run(24);
-  EXPECT_TRUE(passed);
+  test::gemm::warp::TestbedComplex<MmaTensorOp, cutlass::gemm::GemmShape<8, 8, 4> >().run();
 }
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+TEST(SM80_warp_gemm_gaussian_complex_tensor_op, 16x16x4_8x8x4_nt) {
+
+  using Shape = cutlass::gemm::GemmShape<16, 16, 4>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
+  
+  using Element = cutlass::complex<double>;
+  using ElementC = cutlass::complex<double>;
+
+  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous128b;
+  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous128b;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor,
+    cutlass::ComplexTransform::kNone,
+    cutlass::ComplexTransform::kNone,
+    cutlass::arch::OpMultiplyAddGaussianComplex
+  >::Type;
+
+  test::gemm::warp::TestbedComplex<MmaTensorOp, cutlass::gemm::GemmShape<16, 16, 4> >().run();
+}
+
+
+TEST(SM80_warp_gemm_gaussian_complex_tensor_op, 16x32x4_8x8x4_nt) {
+
+  using Shape = cutlass::gemm::GemmShape<16, 32, 4>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
+  
+  using Element = cutlass::complex<double>;
+  using ElementC = cutlass::complex<double>;
+
+  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous128b;
+  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous128b;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor,
+    cutlass::ComplexTransform::kNone,
+    cutlass::ComplexTransform::kNone,
+    cutlass::arch::OpMultiplyAddGaussianComplex
+  >::Type;
+
+  test::gemm::warp::TestbedComplex<MmaTensorOp, cutlass::gemm::GemmShape<16, 32, 4> >().run();
+}
+
+TEST(SM80_warp_gemm_gaussian_complex_tensor_op, 32x16x4_8x8x4_nt) {
+
+  using Shape = cutlass::gemm::GemmShape<32, 16, 4>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
+  
+  using Element = cutlass::complex<double>;
+  using ElementC = cutlass::complex<double>;
+
+  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous128b;
+  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous128b;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor,
+    cutlass::ComplexTransform::kNone,
+    cutlass::ComplexTransform::kNone,
+    cutlass::arch::OpMultiplyAddGaussianComplex
+  >::Type;
+
+  test::gemm::warp::TestbedComplex<MmaTensorOp, cutlass::gemm::GemmShape<32, 16, 4> >().run();
+}
+
+TEST(SM80_warp_gemm_gaussian_complex_tensor_op, 32x32x4_8x8x4_nt) {
+
+  using Shape = cutlass::gemm::GemmShape<32, 32, 4>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
+  
+  using Element = cutlass::complex<double>;
+  using ElementC = cutlass::complex<double>;
+
+  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous128b;
+  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous128b;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor,
+    cutlass::ComplexTransform::kNone,
+    cutlass::ComplexTransform::kNone,
+    cutlass::arch::OpMultiplyAddGaussianComplex
+  >::Type;
+
+  test::gemm::warp::TestbedComplex<MmaTensorOp, cutlass::gemm::GemmShape<32, 32, 4> >().run();
+}
+
+TEST(SM80_warp_gemm_gaussian_complex_tensor_op, 32x32x4_8x8x4_nh) {
+
+  using Shape = cutlass::gemm::GemmShape<32, 32, 4>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
+  
+  using Element = cutlass::complex<double>;
+  using ElementC = cutlass::complex<double>;
+
+  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous128b;
+  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous128b;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor,
+    cutlass::ComplexTransform::kNone,
+    cutlass::ComplexTransform::kConjugate,
+    cutlass::arch::OpMultiplyAddGaussianComplex
+  >::Type;
+
+  test::gemm::warp::TestbedComplex<MmaTensorOp, cutlass::gemm::GemmShape<32, 32, 4> >().run();
+}
+
+TEST(SM80_warp_gemm_gaussian_complex_tensor_op, 32x32x4_8x8x4_ct) {
+
+  using Shape = cutlass::gemm::GemmShape<32, 32, 4>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
+  
+  using Element = cutlass::complex<double>;
+  using ElementC = cutlass::complex<double>;
+
+  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous128b;
+  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous128b;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor,
+    cutlass::ComplexTransform::kConjugate,
+    cutlass::ComplexTransform::kNone,
+    cutlass::arch::OpMultiplyAddGaussianComplex
+  >::Type;
+
+  test::gemm::warp::TestbedComplex<MmaTensorOp, cutlass::gemm::GemmShape<32, 32, 4> >().run();
+}
+
+///////////////////////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM80_warp_gemm_gaussian_complex_tensor_op, 16x16x4_8x8x4_tn) {
+
+  using Shape = cutlass::gemm::GemmShape<16, 16, 4>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
+  
+  using Element = cutlass::complex<double>;
+  using ElementC = cutlass::complex<double>;
+
+  using LayoutA = cutlass::layout::RowMajorTensorOpMultiplicandCrosswise128x4;
+  using LayoutB = cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise128x4;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
+    Shape, 
+    InstructionShape, 
+    Element, 
+    LayoutA, 
+    Element, 
+    LayoutB, 
+    ElementC,
+    cutlass::layout::RowMajor,
+    cutlass::ComplexTransform::kNone,
+    cutlass::ComplexTransform::kNone,
+    cutlass::arch::OpMultiplyAddGaussianComplex
+  >::Type;
+
+  test::gemm::warp::TestbedComplex<MmaTensorOp, cutlass::gemm::GemmShape<16, 16, 4> >().run();
+}
+///////////////////////////////////////////////////////////////////////////////////////////////////
 
 #endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_fast_f32_sm80.cu`

 * *Files 10% similar despite different names*

```diff
@@ -26,125 +26,112 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
     \brief Tests for device-wide SYRK interface
+
   
 */
 
 #include <iostream>
 
 #include "../../common/cutlass_unit_test.h"
 #include "cutlass/blas3.h"
-#include "cutlass/gemm/device/rank_2k.h"
+#include "cutlass/gemm/device/rank_k.h"
 #include "cutlass/util/host_tensor.h"
-#include "cutlass/util/reference/host/rank_2k.h"
+#include "cutlass/util/reference/host/rank_k_complex.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
-#include "testbed_rank2k_universal.h"
+#include "testbed_rank_k_universal.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_cf64n_cf64t_l_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM80_Device_Syrk_cf32n_cf32n_l_tensor_op_fast_f32, 32x32x16_16x16x16) {
 
-  using ElementA = cutlass::complex<double>;
+  using ElementA = cutlass::complex<float>;
   using LayoutA = cutlass::layout::ColumnMajor;
 
-  using ElementB = cutlass::complex<double>;
-  using LayoutB = cutlass::layout::ColumnMajor;
-
-  using ElementC = cutlass::complex<double>;
-  using LayoutC = cutlass::layout::RowMajor;
-  using ElementAccumulator = cutlass::complex<double>;
+  using ElementC = cutlass::complex<float>;
+  using LayoutC = cutlass::layout::ColumnMajor;
+  using ElementAccumulator = cutlass::complex<float>;
 
-  using Rank2K = cutlass::gemm::device::Rank2K<
+  using RankK = cutlass::gemm::device::RankK<
     ElementA,
     LayoutA,
-    ElementB,
-    LayoutB,
     ElementC,
     LayoutC,
     cutlass::FillMode::kLower,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<32, 32, 16>,
     cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::gemm::GemmShape<16, 8, 8>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    4,     // kStages 
+    3,     // kStages 
     1,     // AlignmentA
-    1,     // AlignmentB
     false, // SplitKSerial
-    cutlass::arch::OpMultiplyAddComplex,
-    cutlass::ComplexTransform::kNone,
+    cutlass::arch::OpMultiplyAddComplexFastF32,
     cutlass::ComplexTransform::kNone,
     cutlass::BlasMode::kSymmetric
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
+  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_cf64n_cf64t_u_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM80_Device_Syrk_cf32n_cf32n_u_tensor_op_fast_f32, 32x32x16_16x16x16) {
 
-  using ElementA = cutlass::complex<double>;
+  using ElementA = cutlass::complex<float>;
   using LayoutA = cutlass::layout::ColumnMajor;
 
-  using ElementB = cutlass::complex<double>;
-  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = cutlass::complex<float>;
+  using LayoutC = cutlass::layout::ColumnMajor;
+  using ElementAccumulator = cutlass::complex<float>;
 
-  using ElementC = cutlass::complex<double>;
-  using LayoutC = cutlass::layout::RowMajor;
-  using ElementAccumulator = cutlass::complex<double>;
-
-  using Rank2K = cutlass::gemm::device::Rank2K<
+  using RankK = cutlass::gemm::device::RankK<
     ElementA,
     LayoutA,
-    ElementB,
-    LayoutB,
     ElementC,
     LayoutC,
     cutlass::FillMode::kUpper,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<32, 32, 16>,
     cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::gemm::GemmShape<16, 8, 8>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    4,     // kStages 
+    3,     // kStages 
     1,     // AlignmentA
-    1,     // AlignmentB
     false, // SplitKSerial
-    cutlass::arch::OpMultiplyAddComplex,
-    cutlass::ComplexTransform::kNone,
+    cutlass::arch::OpMultiplyAddComplexFastF32,
     cutlass::ComplexTransform::kNone,
     cutlass::BlasMode::kSymmetric
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
+  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 #endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64n_tensor_op_f64_grouped_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_slicedk.cu`

 * *Files 26% similar despite different names*

```diff
@@ -24,145 +24,163 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
+
 /*! \file
-    \brief Tests for grouped Rank2K interface
+    \brief Unit tests for CTA-level GEMM specifically for sliced-k kernels (SM_61 and SM_75)
 */
 
-#include <iostream>
-
-#include "../../common/cutlass_unit_test.h"
-#include "cutlass/cutlass.h"
-
-#include "cutlass/blas3.h"
-#include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/kernel/rank_2k_grouped.h"
-#include "cutlass/gemm/kernel/default_rank_2k_grouped.h"
-#include "cutlass/gemm/device/rank_2k_grouped.h"
-
-#include "cutlass/util/host_tensor.h"
-#include "cutlass/util/reference/host/gemm.h"
-#include "cutlass/util/reference/host/tensor_compare.h"
-#include "cutlass/util/reference/host/tensor_copy.h"
-#include "cutlass/util/reference/host/tensor_fill.h"
-#include "cutlass/util/tensor_view_io.h"
-
-#include "testbed_grouped_rank_2k.h"
+#include "mma_pipelined_testbed_slicedk.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-
-#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
-
+// igemm_NT DP4A
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2kGrouped_cf64n_cf64n_l_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM61_igemm_sliced_k, igemm_int8_nt_32x32x128_32x32x4) {
+    using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+        cutlass::gemm::GemmShape<32, 32, 128>,   // ThreadblockShape,
+        cutlass::gemm::GemmShape<32, 32, 32>,    // WarpShape,
+        cutlass::gemm::GemmShape<1, 1, 4>,      // InstructionShape,
+        int8_t,                                 // ElementA,
+        cutlass::layout::ColumnMajor,           // LayoutA,
+        int8_t,                                 // ElementB,
+        cutlass::layout::RowMajor,              // LayoutB,
+        int,                                    // ElementC,
+        cutlass::layout::RowMajor,              // LayoutC,
+        cutlass::arch::OpClassSimt,             // OpClass
+        2>;                                     // Stages,
+
+    cutlass::gemm::GemmCoord problem_size(32, 32, 128);
+    float alpha = 1.f;
+    float beta = 0.0f;
+    dim3 grid(1, 1);
+    dim3 block(32, 4, 1);
+    test::gemm::threadblock::Testbed<MmaCore>(
+        problem_size.m(), problem_size.n(), problem_size.k(), alpha, beta)
+        .run(grid, block, cutlass::Distribution::Uniform, cutlass::Distribution::Uniform);
+}
+
+TEST(SM61_igemm_sliced_k_big, igemm_int8_nt_32x32x128_32x32x4_bigk) {
+    using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+        cutlass::gemm::GemmShape<32, 32, 128>,   // ThreadblockShape,
+        cutlass::gemm::GemmShape<32, 32, 32>,    // WarpShape,
+        cutlass::gemm::GemmShape<1, 1, 4>,      // InstructionShape,
+        int8_t,                                 // ElementA,
+        cutlass::layout::ColumnMajor,           // LayoutA,
+        int8_t,                                 // ElementB,
+        cutlass::layout::RowMajor,              // LayoutB,
+        int,                                    // ElementC,
+        cutlass::layout::RowMajor,              // LayoutC,
+        cutlass::arch::OpClassSimt,             // OpClass
+        2>;                                     // Stages,
+
+    cutlass::gemm::GemmCoord problem_size(32, 32, 1024);
+    float alpha = 1.f;
+    float beta = 0.0f;
+    dim3 grid(1, 1);
+    dim3 block(32, 4, 1);
+    test::gemm::threadblock::Testbed<MmaCore>(
+        problem_size.m(), problem_size.n(), problem_size.k(), alpha, beta)
+        .run(grid, block, cutlass::Distribution::Uniform, cutlass::Distribution::Uniform);
+}
 
-  using ElementA = cutlass::complex<double>;
-  using LayoutA = cutlass::layout::RowMajor;
-  using ElementB = cutlass::complex<double>;
-  using LayoutB = cutlass::layout::RowMajor;
-  using ElementC = cutlass::complex<double>;
-  using LayoutC = cutlass::layout::ColumnMajor;
-  using ElementAccumulator = cutlass::complex<double>;
 
-  using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
-    ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
-    ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
-    ElementC, LayoutC, cutlass::FillMode::kLower,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<32, 32, 16>,
-    cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
-    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3, // kStages
-    cutlass::arch::OpMultiplyAddComplex,
-    cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
-
-  using Rank2K = cutlass::gemm::device::Rank2KGrouped<Rank2Kkernel>;
-
-  test::gemm::device::TestbedGrouped<Rank2K> testbed;
-  bool passed = testbed.run(24);
-  EXPECT_TRUE(passed);
+TEST(SM61_igemm_sliced_k, igemm_int8_nt_32x64x128_32x32x4) {
+    using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+        cutlass::gemm::GemmShape<32, 64, 128>,   // ThreadblockShape,
+        cutlass::gemm::GemmShape<32, 32, 64>,    // WarpShape,
+        cutlass::gemm::GemmShape<1, 1, 4>,      // InstructionShape,
+        int8_t,                                 // ElementA,
+        cutlass::layout::ColumnMajor,           // LayoutA,
+        int8_t,                                 // ElementB,
+        cutlass::layout::RowMajor,              // LayoutB,
+        int,                                    // ElementC,
+        cutlass::layout::RowMajor,              // LayoutC,
+        cutlass::arch::OpClassSimt,             // OpClass
+        2>;                                     // Stages,
+
+    cutlass::gemm::GemmCoord problem_size(32, 64, 256);
+    float alpha = 1.f;
+    float beta = 0.0f;
+    dim3 grid(1, 1);
+    dim3 block(32, 4, 1);
+    test::gemm::threadblock::Testbed<MmaCore>(
+        problem_size.m(), problem_size.n(), problem_size.k(), alpha, beta)
+        .run(grid, block, cutlass::Distribution::Uniform, cutlass::Distribution::Uniform);
 }
 
+#if defined(CUTLASS_ARCH_MMA_SM75_SUPPORTED)
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+// Tensor Op GEMM for SM_75
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2kGrouped_cf64n_cf64n_l_tensor_op_f64, 64x64x16_32x32x16) {
+TEST(SM75_gemm_threadblock_congruous_sliced, tensor_op_64x64x256_tb64x64x64_warp64x32x32_16x8x8) {
 
-  using ElementA = cutlass::complex<double>;
-  using LayoutA = cutlass::layout::RowMajor;
-  using ElementB = cutlass::complex<double>;
+  using ElementA = cutlass::half_t;
+  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementB = cutlass::half_t;
   using LayoutB = cutlass::layout::RowMajor;
-  using ElementC = cutlass::complex<double>;
+  using ElementC = float;
   using LayoutC = cutlass::layout::ColumnMajor;
-  using ElementAccumulator = cutlass::complex<double>;
 
-  using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
-    ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
-    ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
-    ElementC, LayoutC, cutlass::FillMode::kLower,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<64, 64, 16>,
-    cutlass::gemm::GemmShape<32, 32, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
-    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3, // kStages
-    cutlass::arch::OpMultiplyAddComplex,
-    cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
-
-  using Rank2K = cutlass::gemm::device::Rank2KGrouped<Rank2Kkernel>;
-
-  test::gemm::device::TestbedGrouped<Rank2K> testbed;
-  bool passed = testbed.run(24);
-  EXPECT_TRUE(passed);
-}
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
+  cutlass::gemm::GemmCoord problem_size(64, 64, 256);
 
-TEST(SM80_Device_Syr2kGrouped_cf64n_cf64n_u_tensor_op_f64, 32x32x16_16x16x16) {
+  using ThreadblockShape = cutlass::gemm::GemmShape<64, 64, 64>;
+  using WarpShape = cutlass::gemm::GemmShape<64, 32, 32>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 8>;
+
+  float alpha = 1.f;
+  float beta = 0.0f;
+
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
+      ElementB, LayoutB, ElementC, LayoutC, cutlass::arch::OpClassTensorOp, 2,
+      cutlass::arch::OpMultiplyAdd>;
+
+  dim3 grid(1, 1);
+  dim3 block(32, 4, 1);
+
+  test::gemm::threadblock::Testbed<MmaCore>(problem_size.m(), problem_size.n(),
+                                            problem_size.k(), alpha, beta)
+      .run(grid, block);
+}
 
-  using ElementA = cutlass::complex<double>;
+TEST(SM75_gemm_threadblock_crosswise_sliced, tensor_op_64x64x256_tb64x64x64_warp64x32x32_16x8x8) {
+  using ElementA = cutlass::half_t;
   using LayoutA = cutlass::layout::RowMajor;
-  using ElementB = cutlass::complex<double>;
-  using LayoutB = cutlass::layout::RowMajor;
-  using ElementC = cutlass::complex<double>;
+  using ElementB = cutlass::half_t;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = float;
   using LayoutC = cutlass::layout::ColumnMajor;
-  using ElementAccumulator = cutlass::complex<double>;
 
-  using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
-    ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
-    ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
-    ElementC, LayoutC, cutlass::FillMode::kUpper,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<32, 32, 16>,
-    cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
-    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3, // kStages
-    cutlass::arch::OpMultiplyAddComplex,
-    cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
-
-  using Rank2K = cutlass::gemm::device::Rank2KGrouped<Rank2Kkernel>;
-
-  test::gemm::device::TestbedGrouped<Rank2K> testbed;
-  bool passed = testbed.run(24);
-  EXPECT_TRUE(passed);
-}
+  cutlass::gemm::GemmCoord problem_size(64, 64, 256);
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+  using ThreadblockShape = cutlass::gemm::GemmShape<64, 64, 64>;
+  using WarpShape = cutlass::gemm::GemmShape<64, 32, 32>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 8>;
+
+  float alpha = 1.f;
+  float beta = 0.0f;
+
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
+      ElementB, LayoutB, ElementC, LayoutC, cutlass::arch::OpClassTensorOp, 2,
+      cutlass::arch::OpMultiplyAdd>;
+
+  dim3 grid(1, 1);
+  dim3 block(32, 4, 1);
+
+  test::gemm::threadblock::Testbed<MmaCore>(problem_size.m(), problem_size.n(),
+                                            problem_size.k(), alpha, beta)
+      .run(grid, block);
+}
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+////////////////////////////////////////////////////////////////////////////////
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+#endif
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f32n_f32n_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_tf32n_f32n_tensor_op_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -47,15 +47,15 @@
 
 #include "testbed_rank2k_universal.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_f32n_f32n_l_tensor_op_fast_f32, 128x256x32_64x64x32) {
+TEST(SM80_Device_Syr2k_tf32n_f32n_l_tensor_op_f32, 128x256x32_64x64x32) {
 
   using ElementA = cutlass::tfloat32_t;
   using LayoutA = cutlass::layout::ColumnMajor;
   using ElementB = cutlass::tfloat32_t;
   using LayoutB = cutlass::layout::ColumnMajor;
   using ElementC = float;
   using LayoutC = cutlass::layout::ColumnMajor;
@@ -87,15 +87,15 @@
 
   EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_f32n_f32n_u_tensor_op_fast_f32, 128x256x32_64x64x32) {
+TEST(SM80_Device_Syr2k_tf32n_f32n_u_tensor_op_f32, 128x256x32_64x64x32) {
 
   using ElementA = cutlass::tfloat32_t;
   using LayoutA = cutlass::layout::ColumnMajor;
   using ElementB = cutlass::tfloat32_t;
   using LayoutB = cutlass::layout::ColumnMajor;
   using ElementC = float;
   using LayoutC = cutlass::layout::ColumnMajor;
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f32t_f32n_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f32t_f32n_tensor_op_fast_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64_f64_tensor_op_f64_sm90.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_tf32t_f32n_tensor_op_f32_sm80.cu`

 * *Files 4% similar despite different names*

```diff
@@ -43,82 +43,82 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_rank2k_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Syr2k_f64n_f64n_l_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM80_Device_Syr2k_tf32t_f32n_l_tensor_op_f32, 128x256x32_64x64x32) {
 
-  using ElementA = double;
-  using LayoutA = cutlass::layout::ColumnMajor;
-  using ElementB = double;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using ElementC = double;
+  using ElementA = cutlass::tfloat32_t;
+  using LayoutA = cutlass::layout::RowMajor;
+  using ElementB = cutlass::tfloat32_t;
+  using LayoutB = cutlass::layout::RowMajor;
+  using ElementC = float;
   using LayoutC = cutlass::layout::ColumnMajor;
-  using ElementAccumulator = double;
+  using ElementAccumulator = float;
 
   using Rank2K = cutlass::gemm::device::Rank2K<
     ElementA,
     LayoutA,
     ElementB,
     LayoutB,
     ElementC,
     LayoutC,
     cutlass::FillMode::kLower,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm90,
-    cutlass::gemm::GemmShape<32, 32, 16>,
-    cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<16, 8, 4>,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<128, 256, 32>,
+    cutlass::gemm::GemmShape<64, 64, 32>,
+    cutlass::gemm::GemmShape<16, 8, 8>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    4
+    3
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Syr2k_f64t_f64n_l_tensor_op_f64, 128x128x16_32x64x16) {
+TEST(SM80_Device_Syr2k_tf32t_f32n_u_tensor_op_f32, 128x256x32_64x64x32) {
 
-  using ElementA = double;
+  using ElementA = cutlass::tfloat32_t;
   using LayoutA = cutlass::layout::RowMajor;
-  using ElementB = double;
+  using ElementB = cutlass::tfloat32_t;
   using LayoutB = cutlass::layout::RowMajor;
-  using ElementC = double;
+  using ElementC = float;
   using LayoutC = cutlass::layout::ColumnMajor;
-  using ElementAccumulator = double;
+  using ElementAccumulator = float;
 
   using Rank2K = cutlass::gemm::device::Rank2K<
     ElementA,
     LayoutA,
     ElementB,
     LayoutB,
     ElementC,
     LayoutC,
-    cutlass::FillMode::kLower,
+    cutlass::FillMode::kUpper,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm90,
-    cutlass::gemm::GemmShape<128, 128, 16>,
-    cutlass::gemm::GemmShape<32, 64, 16>,
-    cutlass::gemm::GemmShape<16, 8, 4>,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<128, 256, 32>,
+    cutlass::gemm::GemmShape<64, 64, 32>,
+    cutlass::gemm::GemmShape<16, 8, 8>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
@@ -126,9 +126,8 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_grouped_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_grouped_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_grouped_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_grouped_sm80.cu`

 * *Files 5% similar despite different names*

```diff
@@ -54,22 +54,22 @@
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2kGrouped_f64n_f64t_l_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM80_Device_Syr2kGrouped_f64t_f64n_l_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = double;
-  using LayoutA = cutlass::layout::ColumnMajor;
+  using LayoutA = cutlass::layout::RowMajor;
   using ElementB = double;
-  using LayoutB = cutlass::layout::ColumnMajor;
+  using LayoutB = cutlass::layout::RowMajor;
   using ElementC = double;
-  using LayoutC = cutlass::layout::RowMajor;
+  using LayoutC = cutlass::layout::ColumnMajor;
   using ElementAccumulator = double;
 
   using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
     ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
     ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
     ElementC, LayoutC, cutlass::FillMode::kLower,
     ElementAccumulator,
@@ -89,22 +89,22 @@
   test::gemm::device::TestbedGrouped<Rank2K> testbed;
   bool passed = testbed.run(24);
   EXPECT_TRUE(passed);
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2kGrouped_f64n_f64t_l_tensor_op_f64, 64x64x16_32x32x16) {
+TEST(SM80_Device_Syr2kGrouped_f64t_f64n_l_tensor_op_f64, 64x64x16_32x32x16) {
 
   using ElementA = double;
-  using LayoutA = cutlass::layout::ColumnMajor;
+  using LayoutA = cutlass::layout::RowMajor;
   using ElementB = double;
-  using LayoutB = cutlass::layout::ColumnMajor;
+  using LayoutB = cutlass::layout::RowMajor;
   using ElementC = double;
-  using LayoutC = cutlass::layout::RowMajor;
+  using LayoutC = cutlass::layout::ColumnMajor;
   using ElementAccumulator = double;
 
   using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
     ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
     ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
     ElementC, LayoutC, cutlass::FillMode::kLower,
     ElementAccumulator,
@@ -124,22 +124,22 @@
   test::gemm::device::TestbedGrouped<Rank2K> testbed;
   bool passed = testbed.run(24);
   EXPECT_TRUE(passed);
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2kGrouped_f64n_f64t_l_tensor_op_f64, 64x32x16_32x32x16) {
+TEST(SM80_Device_Syr2kGrouped_f64t_f64n_l_tensor_op_f64, 64x32x16_32x32x16) {
 
   using ElementA = double;
-  using LayoutA = cutlass::layout::ColumnMajor;
+  using LayoutA = cutlass::layout::RowMajor;
   using ElementB = double;
-  using LayoutB = cutlass::layout::ColumnMajor;
+  using LayoutB = cutlass::layout::RowMajor;
   using ElementC = double;
-  using LayoutC = cutlass::layout::RowMajor;
+  using LayoutC = cutlass::layout::ColumnMajor;
   using ElementAccumulator = double;
 
   using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
     ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
     ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
     ElementC, LayoutC, cutlass::FillMode::kLower,
     ElementAccumulator,
@@ -159,22 +159,22 @@
   test::gemm::device::TestbedGrouped<Rank2K> testbed;
   bool passed = testbed.run(24);
   EXPECT_TRUE(passed);
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2kGrouped_f64n_f64t_l_tensor_op_f64, 128x64x16_64x32x16) {
+TEST(SM80_Device_Syr2kGrouped_f64t_f64n_l_tensor_op_f64, 128x64x16_64x32x16) {
 
   using ElementA = double;
-  using LayoutA = cutlass::layout::ColumnMajor;
+  using LayoutA = cutlass::layout::RowMajor;
   using ElementB = double;
-  using LayoutB = cutlass::layout::ColumnMajor;
+  using LayoutB = cutlass::layout::RowMajor;
   using ElementC = double;
-  using LayoutC = cutlass::layout::RowMajor;
+  using LayoutC = cutlass::layout::ColumnMajor;
   using ElementAccumulator = double;
 
   using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
     ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
     ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
     ElementC, LayoutC, cutlass::FillMode::kLower,
     ElementAccumulator,
@@ -194,22 +194,22 @@
   test::gemm::device::TestbedGrouped<Rank2K> testbed;
   bool passed = testbed.run(24);
   EXPECT_TRUE(passed);
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2kGrouped_f64n_f64t_l_tensor_op_f64, 128x128x16_32x64x16) {
+TEST(SM80_Device_Syr2kGrouped_f64t_f64n_l_tensor_op_f64, 128x128x16_32x64x16) {
 
   using ElementA = double;
-  using LayoutA = cutlass::layout::ColumnMajor;
+  using LayoutA = cutlass::layout::RowMajor;
   using ElementB = double;
-  using LayoutB = cutlass::layout::ColumnMajor;
+  using LayoutB = cutlass::layout::RowMajor;
   using ElementC = double;
-  using LayoutC = cutlass::layout::RowMajor;
+  using LayoutC = cutlass::layout::ColumnMajor;
   using ElementAccumulator = double;
 
   using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
     ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
     ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
     ElementC, LayoutC, cutlass::FillMode::kLower,
     ElementAccumulator,
@@ -229,22 +229,22 @@
   test::gemm::device::TestbedGrouped<Rank2K> testbed;
   bool passed = testbed.run(24);
   EXPECT_TRUE(passed);
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2kGrouped_f64n_f64t_u_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM80_Device_Syr2kGrouped_f64t_f64n_u_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = double;
-  using LayoutA = cutlass::layout::ColumnMajor;
+  using LayoutA = cutlass::layout::RowMajor;
   using ElementB = double;
-  using LayoutB = cutlass::layout::ColumnMajor;
+  using LayoutB = cutlass::layout::RowMajor;
   using ElementC = double;
-  using LayoutC = cutlass::layout::RowMajor;
+  using LayoutC = cutlass::layout::ColumnMajor;
   using ElementAccumulator = double;
 
   using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
     ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
     ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
     ElementC, LayoutC, cutlass::FillMode::kUpper,
     ElementAccumulator,
@@ -255,14 +255,49 @@
     cutlass::gemm::GemmShape<8, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     3, // kStages
     cutlass::arch::OpMultiplyAdd,
     cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
 
+  using Rank2K = cutlass::gemm::device::Rank2KGrouped<Rank2Kkernel>;
+
+  test::gemm::device::TestbedGrouped<Rank2K> testbed;
+  bool passed = testbed.run(24);
+  EXPECT_TRUE(passed);
+}
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM80_Device_Syr2kGrouped_f64t_f64n_u_tensor_op_f64, 64x32x16_32x32x16) {
+
+  using ElementA = double;
+  using LayoutA = cutlass::layout::RowMajor;
+  using ElementB = double;
+  using LayoutB = cutlass::layout::RowMajor;
+  using ElementC = double;
+  using LayoutC = cutlass::layout::ColumnMajor;
+  using ElementAccumulator = double;
+
+  using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
+    ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
+    ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
+    ElementC, LayoutC, cutlass::FillMode::kUpper,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<64, 32, 16>,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    3, // kStages
+    cutlass::arch::OpMultiplyAdd,
+    cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
+
   using Rank2K = cutlass::gemm::device::Rank2KGrouped<Rank2Kkernel>;
 
   test::gemm::device::TestbedGrouped<Rank2K> testbed;
   bool passed = testbed.run(24);
   EXPECT_TRUE(passed);
 }
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_grouped_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64t_tensor_op_f64_grouped_sm80.cu`

 * *Files 5% similar despite different names*

```diff
@@ -54,22 +54,22 @@
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2kGrouped_f64t_f64n_l_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM80_Device_Syr2kGrouped_f64t_f64t_l_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = double;
   using LayoutA = cutlass::layout::RowMajor;
   using ElementB = double;
   using LayoutB = cutlass::layout::RowMajor;
   using ElementC = double;
-  using LayoutC = cutlass::layout::ColumnMajor;
+  using LayoutC = cutlass::layout::RowMajor;
   using ElementAccumulator = double;
 
   using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
     ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
     ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
     ElementC, LayoutC, cutlass::FillMode::kLower,
     ElementAccumulator,
@@ -89,22 +89,22 @@
   test::gemm::device::TestbedGrouped<Rank2K> testbed;
   bool passed = testbed.run(24);
   EXPECT_TRUE(passed);
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2kGrouped_f64t_f64n_l_tensor_op_f64, 64x64x16_32x32x16) {
+TEST(SM80_Device_Syr2kGrouped_f64t_f64t_l_tensor_op_f64, 64x64x16_32x32x16) {
 
   using ElementA = double;
   using LayoutA = cutlass::layout::RowMajor;
   using ElementB = double;
   using LayoutB = cutlass::layout::RowMajor;
   using ElementC = double;
-  using LayoutC = cutlass::layout::ColumnMajor;
+  using LayoutC = cutlass::layout::RowMajor;
   using ElementAccumulator = double;
 
   using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
     ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
     ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
     ElementC, LayoutC, cutlass::FillMode::kLower,
     ElementAccumulator,
@@ -124,32 +124,32 @@
   test::gemm::device::TestbedGrouped<Rank2K> testbed;
   bool passed = testbed.run(24);
   EXPECT_TRUE(passed);
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2kGrouped_f64t_f64n_l_tensor_op_f64, 64x32x16_32x32x16) {
+TEST(SM80_Device_Syr2kGrouped_f64t_f64t_l_tensor_op_f64, 32x64x16_32x32x16) {
 
   using ElementA = double;
   using LayoutA = cutlass::layout::RowMajor;
   using ElementB = double;
   using LayoutB = cutlass::layout::RowMajor;
   using ElementC = double;
-  using LayoutC = cutlass::layout::ColumnMajor;
+  using LayoutC = cutlass::layout::RowMajor;
   using ElementAccumulator = double;
 
   using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
     ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
     ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
     ElementC, LayoutC, cutlass::FillMode::kLower,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<64, 32, 16>,
+    cutlass::gemm::GemmShape<32, 64, 16>,
     cutlass::gemm::GemmShape<32, 32, 16>,
     cutlass::gemm::GemmShape<8, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     3, // kStages
     cutlass::arch::OpMultiplyAdd,
     cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
@@ -159,22 +159,22 @@
   test::gemm::device::TestbedGrouped<Rank2K> testbed;
   bool passed = testbed.run(24);
   EXPECT_TRUE(passed);
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2kGrouped_f64t_f64n_l_tensor_op_f64, 128x64x16_64x32x16) {
+TEST(SM80_Device_Syr2kGrouped_f64t_f64t_l_tensor_op_f64, 128x64x16_64x32x16) {
 
   using ElementA = double;
   using LayoutA = cutlass::layout::RowMajor;
   using ElementB = double;
   using LayoutB = cutlass::layout::RowMajor;
   using ElementC = double;
-  using LayoutC = cutlass::layout::ColumnMajor;
+  using LayoutC = cutlass::layout::RowMajor;
   using ElementAccumulator = double;
 
   using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
     ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
     ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
     ElementC, LayoutC, cutlass::FillMode::kLower,
     ElementAccumulator,
@@ -194,22 +194,22 @@
   test::gemm::device::TestbedGrouped<Rank2K> testbed;
   bool passed = testbed.run(24);
   EXPECT_TRUE(passed);
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2kGrouped_f64t_f64n_l_tensor_op_f64, 128x128x16_32x64x16) {
+TEST(SM80_Device_Syr2kGrouped_f64t_f64t_l_tensor_op_f64, 128x128x16_32x64x16) {
 
   using ElementA = double;
   using LayoutA = cutlass::layout::RowMajor;
   using ElementB = double;
   using LayoutB = cutlass::layout::RowMajor;
   using ElementC = double;
-  using LayoutC = cutlass::layout::ColumnMajor;
+  using LayoutC = cutlass::layout::RowMajor;
   using ElementAccumulator = double;
 
   using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
     ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
     ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
     ElementC, LayoutC, cutlass::FillMode::kLower,
     ElementAccumulator,
@@ -229,22 +229,22 @@
   test::gemm::device::TestbedGrouped<Rank2K> testbed;
   bool passed = testbed.run(24);
   EXPECT_TRUE(passed);
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2kGrouped_f64t_f64n_u_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM80_Device_Syr2kGrouped_f64t_f64t_u_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = double;
   using LayoutA = cutlass::layout::RowMajor;
   using ElementB = double;
   using LayoutB = cutlass::layout::RowMajor;
   using ElementC = double;
-  using LayoutC = cutlass::layout::ColumnMajor;
+  using LayoutC = cutlass::layout::RowMajor;
   using ElementAccumulator = double;
 
   using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
     ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
     ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
     ElementC, LayoutC, cutlass::FillMode::kUpper,
     ElementAccumulator,
@@ -264,32 +264,32 @@
   test::gemm::device::TestbedGrouped<Rank2K> testbed;
   bool passed = testbed.run(24);
   EXPECT_TRUE(passed);
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2kGrouped_f64t_f64n_u_tensor_op_f64, 64x32x16_32x32x16) {
+TEST(SM80_Device_Syr2kGrouped_f64t_f64t_u_tensor_op_f64, 32x64x16_32x32x16) {
 
   using ElementA = double;
   using LayoutA = cutlass::layout::RowMajor;
   using ElementB = double;
   using LayoutB = cutlass::layout::RowMajor;
   using ElementC = double;
-  using LayoutC = cutlass::layout::ColumnMajor;
+  using LayoutC = cutlass::layout::RowMajor;
   using ElementAccumulator = double;
 
   using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
     ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
     ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
     ElementC, LayoutC, cutlass::FillMode::kUpper,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<64, 32, 16>,
+    cutlass::gemm::GemmShape<32, 64, 16>,
     cutlass::gemm::GemmShape<32, 32, 16>,
     cutlass::gemm::GemmShape<8, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     3, // kStages
     cutlass::arch::OpMultiplyAdd,
     cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64t_tensor_op_f64_grouped_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm75.cu`

 * *Files 26% similar despite different names*

```diff
@@ -25,284 +25,313 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for grouped Rank2K interface
+    \brief Unit tests for thread-level GEMM
 */
+#include "cutlass/arch/wmma.h"
 
-#include <iostream>
-
-#include "../../common/cutlass_unit_test.h"
-#include "cutlass/cutlass.h"
-
-#include "cutlass/blas3.h"
-#include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/kernel/rank_2k_grouped.h"
-#include "cutlass/gemm/kernel/default_rank_2k_grouped.h"
-#include "cutlass/gemm/device/rank_2k_grouped.h"
-
-#include "cutlass/util/host_tensor.h"
-#include "cutlass/util/reference/host/gemm.h"
-#include "cutlass/util/reference/host/tensor_compare.h"
-#include "cutlass/util/reference/host/tensor_copy.h"
-#include "cutlass/util/reference/host/tensor_fill.h"
-#include "cutlass/util/tensor_view_io.h"
-
-#include "testbed_grouped_rank_2k.h"
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-TEST(SM80_Device_Syr2kGrouped_f64t_f64t_l_tensor_op_f64, 32x32x16_16x16x16) {
-
-  using ElementA = double;
+#ifdef CUTLASS_ARCH_WMMA_SM75_ENABLED
+#include "mma_pipelined_testbed.h"
+#include "cutlass/gemm/threadblock/default_mma_core_wmma.h"
+
+/// All tests use double-buffered (kStages=2) mma pipeline for the gemm mainloop
+/// Test name format: SM[arch]_gemm_threadblock_wmma_tensor_op_[alayout]_[blayout]_[clayout]_[atype].[threadblock_shape]_[warp_shape]_[instruction_shape]
+
+/////////////////////////////////////////////////////////////////////////
+///       Integer (s8 and u8) WMMA threadblock level tests          /////
+/////////////////////////////////////////////////////////////////////////
+
+#if defined(CUTLASS_ARCH_INTEGER_MATRIX_MULTIPLY_ENABLED)
+TEST(SM75_gemm_threadblock_wmma_tensor_op_row_col_row_s8, 64x64x32_64x64x32_16x16x16) {
+ 
+  using ElementA = int8_t;
   using LayoutA = cutlass::layout::RowMajor;
-  using ElementB = double;
-  using LayoutB = cutlass::layout::RowMajor;
-  using ElementC = double;
+  using ElementB = int8_t;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = int32_t;
   using LayoutC = cutlass::layout::RowMajor;
-  using ElementAccumulator = double;
-
-  using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
-    ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
-    ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
-    ElementC, LayoutC, cutlass::FillMode::kLower,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<32, 32, 16>,
-    cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
-    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3, // kStages
-    cutlass::arch::OpMultiplyAdd,
-    cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
-
-  using Rank2K = cutlass::gemm::device::Rank2KGrouped<Rank2Kkernel>;
-
-  test::gemm::device::TestbedGrouped<Rank2K> testbed;
-  bool passed = testbed.run(24);
-  EXPECT_TRUE(passed);
-}
+  static const int kStages = 2; 
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+  cutlass::gemm::GemmCoord problem_size(64, 64, 128);
 
-TEST(SM80_Device_Syr2kGrouped_f64t_f64t_l_tensor_op_f64, 64x64x16_32x32x16) {
+  using ThreadblockShape = cutlass::gemm::GemmShape<64, 64, 32>;
+  using WarpShape = cutlass::gemm::GemmShape<64, 64, 32>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 16, 16>;
+
+  float alpha = 1.f;
+  float beta = 0.0f;
+
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, 
+      ElementA, LayoutA,
+      ElementB, LayoutB, 
+      ElementC, LayoutC,
+      cutlass::arch::OpClassWmmaTensorOp, kStages>;
+
+  dim3 grid(1, 1);
+  dim3 block(32, 1, 1);
+
+  test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
+                                            problem_size.k(), alpha, beta)
+      .run(grid, block);
+}
 
-  using ElementA = double;
+TEST(SM75_gemm_threadblock_wmma_tensor_op_row_col_row_s8, 64x64x64_64x64x64_16x16x16) {
+ 
+  using ElementA = int8_t;
   using LayoutA = cutlass::layout::RowMajor;
-  using ElementB = double;
-  using LayoutB = cutlass::layout::RowMajor;
-  using ElementC = double;
+  using ElementB = int8_t;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = int32_t;
   using LayoutC = cutlass::layout::RowMajor;
-  using ElementAccumulator = double;
+  static const int kStages = 2; 
 
-  using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
-    ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
-    ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
-    ElementC, LayoutC, cutlass::FillMode::kLower,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<64, 64, 16>,
-    cutlass::gemm::GemmShape<32, 32, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
-    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3, // kStages
-    cutlass::arch::OpMultiplyAdd,
-    cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
-
-  using Rank2K = cutlass::gemm::device::Rank2KGrouped<Rank2Kkernel>;
-
-  test::gemm::device::TestbedGrouped<Rank2K> testbed;
-  bool passed = testbed.run(24);
-  EXPECT_TRUE(passed);
-}
+  cutlass::gemm::GemmCoord problem_size(64, 64, 128);
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+  using ThreadblockShape = cutlass::gemm::GemmShape<64, 64, 64>;
+  using WarpShape = cutlass::gemm::GemmShape<64, 64, 64>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 16, 16>;
+
+  float alpha = 1.f;
+  float beta = 0.0f;
+
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, 
+      ElementA, LayoutA,
+      ElementB, LayoutB, 
+      ElementC, LayoutC,
+      cutlass::arch::OpClassWmmaTensorOp, kStages>;
+
+  dim3 grid(1, 1);
+  dim3 block(32, 1, 1);
+
+  test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
+                                            problem_size.k(), alpha, beta)
+      .run(grid, block);
+}
 
-TEST(SM80_Device_Syr2kGrouped_f64t_f64t_l_tensor_op_f64, 32x64x16_32x32x16) {
 
-  using ElementA = double;
-  using LayoutA = cutlass::layout::RowMajor;
-  using ElementB = double;
+TEST(SM75_gemm_threadblock_wmma_tensor_op_col_row_row_s8, 64x64x32_64x64x32_16x16x16) {
+ 
+  using ElementA = int8_t;
+  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementB = int8_t;
   using LayoutB = cutlass::layout::RowMajor;
-  using ElementC = double;
+  using ElementC = int32_t;
   using LayoutC = cutlass::layout::RowMajor;
-  using ElementAccumulator = double;
-
-  using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
-    ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
-    ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
-    ElementC, LayoutC, cutlass::FillMode::kLower,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<32, 64, 16>,
-    cutlass::gemm::GemmShape<32, 32, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
-    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3, // kStages
-    cutlass::arch::OpMultiplyAdd,
-    cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
-
-  using Rank2K = cutlass::gemm::device::Rank2KGrouped<Rank2Kkernel>;
-
-  test::gemm::device::TestbedGrouped<Rank2K> testbed;
-  bool passed = testbed.run(24);
-  EXPECT_TRUE(passed);
-}
+  static const int kStages = 2; 
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+  cutlass::gemm::GemmCoord problem_size(64, 64, 128);
 
-TEST(SM80_Device_Syr2kGrouped_f64t_f64t_l_tensor_op_f64, 128x64x16_64x32x16) {
+  using ThreadblockShape = cutlass::gemm::GemmShape<64, 64, 32>;
+  using WarpShape = cutlass::gemm::GemmShape<64, 64, 32>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 16, 16>;
+
+  float alpha = 1.f;
+  float beta = 0.0f;
+
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape,
+      ElementA, LayoutA,
+      ElementB, LayoutB, 
+      ElementC, LayoutC,
+      cutlass::arch::OpClassWmmaTensorOp, kStages>;
+
+  dim3 grid(1, 1);
+  dim3 block(32, 1, 1);
+
+  test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
+                                            problem_size.k(), alpha, beta)
+      .run(grid, block);
+}
 
-  using ElementA = double;
-  using LayoutA = cutlass::layout::RowMajor;
-  using ElementB = double;
+TEST(SM75_gemm_threadblock_wmma_tensor_op_col_row_row_s8, 64x64x64_64x64x64_16x16x16) {
+ 
+  using ElementA = int8_t;
+  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementB = int8_t;
   using LayoutB = cutlass::layout::RowMajor;
-  using ElementC = double;
+  using ElementC = int32_t;
   using LayoutC = cutlass::layout::RowMajor;
-  using ElementAccumulator = double;
+  static const int kStages = 2; 
 
-  using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
-    ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
-    ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
-    ElementC, LayoutC, cutlass::FillMode::kLower,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<128, 64, 16>,
-    cutlass::gemm::GemmShape<64, 32, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
-    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3, // kStages
-    cutlass::arch::OpMultiplyAdd,
-    cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
-
-  using Rank2K = cutlass::gemm::device::Rank2KGrouped<Rank2Kkernel>;
-
-  test::gemm::device::TestbedGrouped<Rank2K> testbed;
-  bool passed = testbed.run(24);
-  EXPECT_TRUE(passed);
+  cutlass::gemm::GemmCoord problem_size(64, 64, 128);
+
+  using ThreadblockShape = cutlass::gemm::GemmShape<64, 64, 64>;
+  using WarpShape = cutlass::gemm::GemmShape<64, 64, 64>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 16, 16>;
+
+  float alpha = 1.f;
+  float beta = 0.0f;
+
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, 
+      ElementA, LayoutA,
+      ElementB, LayoutB, 
+      ElementC, LayoutC,
+      cutlass::arch::OpClassWmmaTensorOp, kStages>;
+
+  dim3 grid(1, 1);
+  dim3 block(32, 1, 1);
+
+  test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
+                                            problem_size.k(), alpha, beta)
+      .run(grid, block);
 }
+#endif //CUTLASS_ARCH_INTEGER_MATRIX_MULTIPLY_ENABLED
+
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////
+///      SUBBYTE (s4 and b1) WMMA threadblock level tests          ////
+///////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2kGrouped_f64t_f64t_l_tensor_op_f64, 128x128x16_32x64x16) {
+#if defined(CUTLASS_SUBBYTE_INTEGER_MATRIX_MULTIPLY_ENABLED)
 
-  using ElementA = double;
+TEST(SM75_gemm_threadblock_wmma_tensor_op_row_col_row_s4, 64x64x128_64x64x128_8x8x32) {
+  using ElementA = cutlass::int4b_t;
   using LayoutA = cutlass::layout::RowMajor;
-  using ElementB = double;
-  using LayoutB = cutlass::layout::RowMajor;
-  using ElementC = double;
+  using ElementB = cutlass::int4b_t;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = int32_t;
   using LayoutC = cutlass::layout::RowMajor;
-  using ElementAccumulator = double;
+  static const int kStages = 2; 
 
-  using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
-    ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
-    ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
-    ElementC, LayoutC, cutlass::FillMode::kLower,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<128, 128, 16>,
-    cutlass::gemm::GemmShape<32, 64, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
-    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3, // kStages
-    cutlass::arch::OpMultiplyAdd,
-    cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
-
-  using Rank2K = cutlass::gemm::device::Rank2KGrouped<Rank2Kkernel>;
-
-  test::gemm::device::TestbedGrouped<Rank2K> testbed;
-  bool passed = testbed.run(24);
-  EXPECT_TRUE(passed);
-}
+  cutlass::gemm::GemmCoord problem_size(64, 64, 128);
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+  using ThreadBlockShape = cutlass::gemm::GemmShape<64, 64, 128>;
+  using WarpShape = cutlass::gemm::GemmShape<64, 64, 128>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 32>;
+
+  float alpha = 1.f;
+  float beta = 0.f;
+
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadBlockShape, WarpShape, InstructionShape, 
+      ElementA, LayoutA,
+      ElementB, LayoutB, 
+      ElementC, LayoutC, 
+      cutlass::arch::OpClassWmmaTensorOp, kStages>;
+
+  dim3 grid(1, 1);
+  dim3 block(32, 1, 1);
+
+  test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
+                                            problem_size.k(), alpha, beta)
+      .run(grid, block);
+}
 
-TEST(SM80_Device_Syr2kGrouped_f64t_f64t_u_tensor_op_f64, 32x32x16_16x16x16) {
 
-  using ElementA = double;
+TEST(SM75_gemm_threadblock_wmma_tensor_op_row_col_col_s4, 64x64x64_64x64x64_8x8x32) {
+  using ElementA = cutlass::int4b_t;
   using LayoutA = cutlass::layout::RowMajor;
-  using ElementB = double;
-  using LayoutB = cutlass::layout::RowMajor;
-  using ElementC = double;
-  using LayoutC = cutlass::layout::RowMajor;
-  using ElementAccumulator = double;
-
-  using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
-    ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
-    ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
-    ElementC, LayoutC, cutlass::FillMode::kUpper,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<32, 32, 16>,
-    cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
-    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3, // kStages
-    cutlass::arch::OpMultiplyAdd,
-    cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
-
-  using Rank2K = cutlass::gemm::device::Rank2KGrouped<Rank2Kkernel>;
-
-  test::gemm::device::TestbedGrouped<Rank2K> testbed;
-  bool passed = testbed.run(24);
-  EXPECT_TRUE(passed);
+  using ElementB = cutlass::int4b_t;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = int32_t;
+  using LayoutC = cutlass::layout::ColumnMajor;
+  static const int kStages = 2;
+
+  cutlass::gemm::GemmCoord problem_size(64, 64, 64);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<64, 64, 64>;
+  using WarpShape = cutlass::gemm::GemmShape<64, 64, 64>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 32>;
+
+  float alpha = 1.f;
+  float beta = 0.f;
+
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadBlockShape, WarpShape, InstructionShape, 
+      ElementA, LayoutA,
+      ElementB, LayoutB, 
+      ElementC, LayoutC, 
+      cutlass::arch::OpClassWmmaTensorOp, kStages>;
+
+  dim3 grid(1, 1);
+  dim3 block(32, 1, 1);
+
+  test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
+                                            problem_size.k(), alpha, beta)
+      .run(grid, block);
 }
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-TEST(SM80_Device_Syr2kGrouped_f64t_f64t_u_tensor_op_f64, 32x64x16_32x32x16) {
-
-  using ElementA = double;
+TEST(SM75_gemm_threadblock_wmma_tensor_op_row_col_row_b1, 64x64x512_64x64x512_8x8x128) {
+  using ElementA = cutlass::uint1b_t;
   using LayoutA = cutlass::layout::RowMajor;
-  using ElementB = double;
-  using LayoutB = cutlass::layout::RowMajor;
-  using ElementC = double;
+  using ElementB = cutlass::uint1b_t;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = int32_t;
   using LayoutC = cutlass::layout::RowMajor;
-  using ElementAccumulator = double;
+  static const int kStages = 2; 
 
-  using Rank2Kkernel = typename cutlass::gemm::kernel::DefaultRank2KGrouped<
-    ElementA, LayoutA, cutlass::ComplexTransform::kNone, 1,
-    ElementB, LayoutB, cutlass::ComplexTransform::kNone, 1,
-    ElementC, LayoutC, cutlass::FillMode::kUpper,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<32, 64, 16>,
-    cutlass::gemm::GemmShape<32, 32, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
-    cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementAccumulator>,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3, // kStages
-    cutlass::arch::OpMultiplyAdd,
-    cutlass::BlasMode::kSymmetric>::Rank2Kkernel;
-
-  using Rank2K = cutlass::gemm::device::Rank2KGrouped<Rank2Kkernel>;
-
-  test::gemm::device::TestbedGrouped<Rank2K> testbed;
-  bool passed = testbed.run(24);
-  EXPECT_TRUE(passed);
-}
+  cutlass::gemm::GemmCoord problem_size(64, 64, 2048);
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+  using ThreadBlockShape = cutlass::gemm::GemmShape<64, 64, 512>;
+  using WarpShape = cutlass::gemm::GemmShape<64, 64, 512>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 128>;
+
+  float alpha = 1.f;
+  float beta = 0.f;
+
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadBlockShape, WarpShape, InstructionShape, 
+      ElementA, LayoutA,
+      ElementB, LayoutB, 
+      ElementC, LayoutC, 
+      cutlass::arch::OpClassWmmaTensorOp, kStages,
+      cutlass::arch::OpXorPopc>;
+
+  dim3 grid(1, 1);
+  dim3 block(32, 1, 1);
+
+  test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
+                                            problem_size.k(), alpha, beta)
+      .run(grid, block);
+}
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+TEST(SM75_gemm_threadblock_wmma_tensor_op_row_col_col_b1, 64x64x512_64x64x512_8x8x128) {
+  using ElementA = cutlass::uint1b_t;
+  using LayoutA = cutlass::layout::RowMajor;
+  using ElementB = cutlass::uint1b_t;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = int32_t;
+  using LayoutC = cutlass::layout::ColumnMajor;
+  static const int kStages = 2; 
+
+  cutlass::gemm::GemmCoord problem_size(64, 64, 2048);
+
+  using ThreadBlockShape = cutlass::gemm::GemmShape<64, 64, 512>;
+  using WarpShape = cutlass::gemm::GemmShape<64, 64, 512>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 128>;
+
+  float alpha = 1.f;
+  float beta = 0.f;
+
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadBlockShape, WarpShape, InstructionShape, 
+      ElementA, LayoutA,
+      ElementB, LayoutB, 
+      ElementC, LayoutC, 
+      cutlass::arch::OpClassWmmaTensorOp, kStages,
+      cutlass::arch::OpXorPopc>;
+
+  dim3 grid(1, 1);
+  dim3 block(32, 1, 1);
+
+  test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
+                                            problem_size.k(), alpha, beta)
+      .run(grid, block);
+}
+#endif //CUTLASS_SUBBYTE_INTEGER_MATRIX_MULTIPLY_ENABLED
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+#endif //CUTLASS_ARCH_WMMA_SM75_ENABLED
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_tf32n_f32n_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64_f64_f64_tensor_op_f64_sm90.cu`

 * *Files 10% similar despite different names*

```diff
@@ -25,108 +25,103 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide SYRK interface
+    \brief Tests for device-wide TRMM interface
+
   
 */
 
 #include <iostream>
 
 #include "../../common/cutlass_unit_test.h"
 #include "cutlass/blas3.h"
-#include "cutlass/gemm/device/rank_2k.h"
+#include "cutlass/gemm/device/trmm.h"
 #include "cutlass/util/host_tensor.h"
-#include "cutlass/util/reference/host/rank_2k.h"
+#include "cutlass/util/reference/host/trmm.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
-#include "testbed_rank2k_universal.h"
+#include "testbed_trmm_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_tf32n_f32n_l_tensor_op_f32, 128x256x32_64x64x32) {
+TEST(SM90_Device_Trmm_f64n_f64n_f64t_rs_l_nu_tensor_op_f64, 32x32x16_16x16x16) {
+
+  using ElementOutput = double;
+  using ElementAccumulator = double;
 
-  using ElementA = cutlass::tfloat32_t;
-  using LayoutA = cutlass::layout::ColumnMajor;
-  using ElementB = cutlass::tfloat32_t;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using ElementC = float;
-  using LayoutC = cutlass::layout::ColumnMajor;
-  using ElementAccumulator = float;
-
-  using Rank2K = cutlass::gemm::device::Rank2K<
-    ElementA,
-    LayoutA,
-    ElementB,
-    LayoutB,
-    ElementC,
-    LayoutC,
+  using Trmm = cutlass::gemm::device::Trmm<
+    double,
+    cutlass::layout::ColumnMajor,
+    cutlass::SideMode::kRight,
     cutlass::FillMode::kLower,
+    cutlass::DiagType::kNonUnit,
+    double,
+    cutlass::layout::ColumnMajor,
+    ElementOutput,
+    cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<128, 256, 32>,
-    cutlass::gemm::GemmShape<64, 64, 32>,
-    cutlass::gemm::GemmShape<16, 8, 8>,
+    cutlass::arch::Sm90,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
+    cutlass::gemm::GemmShape<16, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
-      ElementC,
+      ElementOutput,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3
+    4
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
-
+  EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_tf32n_f32n_u_tensor_op_f32, 128x256x32_64x64x32) {
+TEST(SM90_Device_Trmm_f64t_f64t_f64n_rs_l_nu_tensor_op_f64, 64x64x16_32x32x16) {
+
+  using ElementOutput = double;
+  using ElementAccumulator = double;
 
-  using ElementA = cutlass::tfloat32_t;
-  using LayoutA = cutlass::layout::ColumnMajor;
-  using ElementB = cutlass::tfloat32_t;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using ElementC = float;
-  using LayoutC = cutlass::layout::ColumnMajor;
-  using ElementAccumulator = float;
-
-  using Rank2K = cutlass::gemm::device::Rank2K<
-    ElementA,
-    LayoutA,
-    ElementB,
-    LayoutB,
-    ElementC,
-    LayoutC,
-    cutlass::FillMode::kUpper,
+  using Trmm = cutlass::gemm::device::Trmm<
+    double,
+    cutlass::layout::RowMajor,
+    cutlass::SideMode::kRight,
+    cutlass::FillMode::kLower,
+    cutlass::DiagType::kNonUnit,
+    double,
+    cutlass::layout::RowMajor,
+    ElementOutput,
+    cutlass::layout::ColumnMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<128, 256, 32>,
-    cutlass::gemm::GemmShape<64, 64, 32>,
-    cutlass::gemm::GemmShape<16, 8, 8>,
+    cutlass::arch::Sm90,
+    cutlass::gemm::GemmShape<64, 64, 16>,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
-      ElementC,
+      ElementOutput,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3
+    4
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
-
+  EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
 }
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
-#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_tf32t_f32n_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/gemm_sm60_host.cu`

 * *Files 24% similar despite different names*

```diff
@@ -25,109 +25,152 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide SYRK interface
-  
+    \brief Unit tests for thread-level GEMM
 */
 
-#include <iostream>
+#include "../../../common/cutlass_unit_test.h"
 
-#include "../../common/cutlass_unit_test.h"
-#include "cutlass/blas3.h"
-#include "cutlass/gemm/device/rank_2k.h"
-#include "cutlass/util/host_tensor.h"
-#include "cutlass/util/reference/host/rank_2k.h"
-#include "cutlass/util/reference/host/tensor_compare.h"
-#include "cutlass/util/reference/host/tensor_copy.h"
-#include "cutlass/util/reference/host/tensor_fill.h"
-#include "cutlass/util/tensor_view_io.h"
+#include "cutlass/gemm/thread/mma.h"
 
-#include "testbed_rank2k_universal.h"
-
-#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+#include "testbed_host.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
+//
+// Compute capability SM60
+//
+
+TEST(SM60_host_Hgemm_thread, col_row_col_1x1x16) {
+
+  test::gemm::thread::Testbed<
+    cutlass::gemm::GemmShape<1, 1, 16>,
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor,
+    cutlass::half_t,
+    cutlass::layout::RowMajor,
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor
+  >().run();
+}
 
-TEST(SM80_Device_Syr2k_tf32t_f32n_l_tensor_op_f32, 128x256x32_64x64x32) {
+TEST(SM60_host_Hgemm_thread, row_col_row_1x1x16) {
 
-  using ElementA = cutlass::tfloat32_t;
-  using LayoutA = cutlass::layout::RowMajor;
-  using ElementB = cutlass::tfloat32_t;
-  using LayoutB = cutlass::layout::RowMajor;
-  using ElementC = float;
-  using LayoutC = cutlass::layout::ColumnMajor;
-  using ElementAccumulator = float;
-
-  using Rank2K = cutlass::gemm::device::Rank2K<
-    ElementA,
-    LayoutA,
-    ElementB,
-    LayoutB,
-    ElementC,
-    LayoutC,
-    cutlass::FillMode::kLower,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<128, 256, 32>,
-    cutlass::gemm::GemmShape<64, 64, 32>,
-    cutlass::gemm::GemmShape<16, 8, 8>,
-    cutlass::epilogue::thread::LinearCombination<
-      ElementC,
-      1,
-      ElementAccumulator,
-      ElementAccumulator
-    >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3
-  >;
+  test::gemm::thread::Testbed<
+    cutlass::gemm::GemmShape<1, 1, 16>,
+    cutlass::half_t,
+    cutlass::layout::RowMajor,
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor,
+    cutlass::half_t,
+    cutlass::layout::RowMajor
+  >().run();
+}
 
-  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
+TEST(SM60_host_Hgemm_thread, row_row_row_2x2x2) {
 
+  test::gemm::thread::Testbed<
+    cutlass::gemm::GemmShape<2, 2, 2>,
+    cutlass::half_t,
+    cutlass::layout::RowMajor,
+    cutlass::half_t,
+    cutlass::layout::RowMajor,
+    cutlass::half_t,
+    cutlass::layout::RowMajor
+  >().run();
 }
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+TEST(SM60_host_Hgemm_thread, row_row_col_2x2x2) {
+
+  test::gemm::thread::Testbed<
+    cutlass::gemm::GemmShape<2, 2, 2>,
+    cutlass::half_t,
+    cutlass::layout::RowMajor,
+    cutlass::half_t,
+    cutlass::layout::RowMajor,
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor
+  >().run();
+}
+
+TEST(SM60_host_Hgemm_thread, row_col_row_2x2x2) {
 
-TEST(SM80_Device_Syr2k_tf32t_f32n_u_tensor_op_f32, 128x256x32_64x64x32) {
+  test::gemm::thread::Testbed<
+    cutlass::gemm::GemmShape<2, 2, 2>,
+    cutlass::half_t,
+    cutlass::layout::RowMajor,
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor,
+    cutlass::half_t,
+    cutlass::layout::RowMajor
+  >().run();
+}
 
-  using ElementA = cutlass::tfloat32_t;
-  using LayoutA = cutlass::layout::RowMajor;
-  using ElementB = cutlass::tfloat32_t;
-  using LayoutB = cutlass::layout::RowMajor;
-  using ElementC = float;
-  using LayoutC = cutlass::layout::ColumnMajor;
-  using ElementAccumulator = float;
-
-  using Rank2K = cutlass::gemm::device::Rank2K<
-    ElementA,
-    LayoutA,
-    ElementB,
-    LayoutB,
-    ElementC,
-    LayoutC,
-    cutlass::FillMode::kUpper,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<128, 256, 32>,
-    cutlass::gemm::GemmShape<64, 64, 32>,
-    cutlass::gemm::GemmShape<16, 8, 8>,
-    cutlass::epilogue::thread::LinearCombination<
-      ElementC,
-      1,
-      ElementAccumulator,
-      ElementAccumulator
-    >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3
-  >;
+TEST(SM60_host_Hgemm_thread, row_col_col_2x2x2) {
+
+  test::gemm::thread::Testbed<
+    cutlass::gemm::GemmShape<2, 2, 2>,
+    cutlass::half_t,
+    cutlass::layout::RowMajor,
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor,
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor
+  >().run();
+}
+
+TEST(SM60_host_Hgemm_thread, col_row_row_2x2x2) {
+
+  test::gemm::thread::Testbed<
+    cutlass::gemm::GemmShape<2, 2, 2>,
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor,
+    cutlass::half_t,
+    cutlass::layout::RowMajor,
+    cutlass::half_t,
+    cutlass::layout::RowMajor
+  >().run();
+}
+
+TEST(SM60_host_Hgemm_thread, col_row_col_2x2x2) {
+
+  test::gemm::thread::Testbed<
+    cutlass::gemm::GemmShape<2, 2, 2>,
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor,
+    cutlass::half_t,
+    cutlass::layout::RowMajor,
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor
+  >().run();
+}
+
+TEST(SM60_host_Hgemm_thread, col_col_row_2x2x2) {
+
+  test::gemm::thread::Testbed<
+    cutlass::gemm::GemmShape<2, 2, 2>,
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor,
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor,
+    cutlass::half_t,
+    cutlass::layout::RowMajor
+  >().run();
+}
 
-  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
+TEST(SM60_host_Hgemm_thread, col_col_col_2x2x2) {
 
+  test::gemm::thread::Testbed<
+    cutlass::gemm::GemmShape<2, 2, 2>,
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor,
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor,
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor
+  >().run();
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64_cf64_cf64_tensor_op_f64_sm90.cu`

 * *Files 21% similar despite different names*

```diff
@@ -25,113 +25,113 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide SYRK interface
+    \brief Tests for device-wide TRMM interface
 
   
 */
 
 #include <iostream>
 
 #include "../../common/cutlass_unit_test.h"
 #include "cutlass/blas3.h"
-#include "cutlass/gemm/device/rank_k.h"
+#include "cutlass/gemm/device/trmm.h"
 #include "cutlass/util/host_tensor.h"
-#include "cutlass/util/reference/host/rank_k_complex.h"
+#include "cutlass/util/reference/host/trmm.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
-#include "testbed_rank_k_universal.h"
+#include "testbed_trmm_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syrk_cf32n_cf32n_l_tensor_op_f32, 32x32x16_16x16x16) {
+TEST(SM90_Device_Trmm_cf64n_cf64n_cf64t_ls_u_nu_tensor_op_f64_gaussian, 32x32x16_16x16x16) {
 
-  using ElementA = cutlass::complex<float>;
-  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementOutput = cutlass::complex<double>;
+  using ElementAccumulator = cutlass::complex<double>;
 
-  using ElementC = cutlass::complex<float>;
-  using LayoutC = cutlass::layout::ColumnMajor;
-  using ElementAccumulator = cutlass::complex<float>;
-
-  using RankK = cutlass::gemm::device::RankK<
-    ElementA,
-    LayoutA,
-    ElementC,
-    LayoutC,
-    cutlass::FillMode::kLower,
+  using Trmm = cutlass::gemm::device::Trmm<
+    cutlass::complex<double>,
+    cutlass::layout::ColumnMajor,
+    cutlass::SideMode::kLeft,
+    cutlass::FillMode::kUpper,
+    cutlass::DiagType::kNonUnit,
+    cutlass::complex<double>,
+    cutlass::layout::ColumnMajor,
+    ElementOutput,
+    cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
+    cutlass::arch::Sm90,
     cutlass::gemm::GemmShape<32, 32, 16>,
     cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<16, 8, 8>,
+    cutlass::gemm::GemmShape<16, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
-      ElementC,
+      ElementOutput,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3,     // kStages 
-    1,     // AlignmentA
-    false, // SplitKSerial
-    cutlass::arch::OpMultiplyAddComplex,
-    cutlass::ComplexTransform::kNone,
-    cutlass::BlasMode::kSymmetric
+    4,
+    1,
+    1,
+    false,
+    cutlass::arch::OpMultiplyAddGaussianComplex,
+    cutlass::ComplexTransform::kNone
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
+  EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syrk_cf32n_cf32n_u_tensor_op_f32, 32x32x16_16x16x16) {
+TEST(SM90_Device_Trmm_cf64h_cf64n_cf64t_ls_u_nu_tensor_op_f64, 64x64x16_32x32x16) {
 
-  using ElementA = cutlass::complex<float>;
-  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementOutput = cutlass::complex<double>;
+  using ElementAccumulator = cutlass::complex<double>;
 
-  using ElementC = cutlass::complex<float>;
-  using LayoutC = cutlass::layout::ColumnMajor;
-  using ElementAccumulator = cutlass::complex<float>;
-
-  using RankK = cutlass::gemm::device::RankK<
-    ElementA,
-    LayoutA,
-    ElementC,
-    LayoutC,
+  using Trmm = cutlass::gemm::device::Trmm<
+    cutlass::complex<double>,
+    cutlass::layout::ColumnMajor,
+    cutlass::SideMode::kLeft,
     cutlass::FillMode::kUpper,
+    cutlass::DiagType::kNonUnit,
+    cutlass::complex<double>,
+    cutlass::layout::ColumnMajor,
+    ElementOutput,
+    cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
+    cutlass::arch::Sm90,
+    cutlass::gemm::GemmShape<64, 64, 16>,
     cutlass::gemm::GemmShape<32, 32, 16>,
-    cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<16, 8, 8>,
+    cutlass::gemm::GemmShape<16, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
-      ElementC,
+      ElementOutput,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3,     // kStages 
-    1,     // AlignmentA
-    false, // SplitKSerial
+    4,
+    1,
+    1,
+    false,
     cutlass::arch::OpMultiplyAddComplex,
-    cutlass::ComplexTransform::kNone,
-    cutlass::BlasMode::kSymmetric
+    cutlass::ComplexTransform::kConjugate
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
+  EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu`

 * *Files 27% similar despite different names*

```diff
@@ -25,113 +25,113 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide SYRK interface
+    \brief Tests for device-wide TRMM interface
 
   
 */
 
 #include <iostream>
 
 #include "../../common/cutlass_unit_test.h"
 #include "cutlass/blas3.h"
-#include "cutlass/gemm/device/rank_k.h"
+#include "cutlass/gemm/device/trmm.h"
 #include "cutlass/util/host_tensor.h"
-#include "cutlass/util/reference/host/rank_k_complex.h"
+#include "cutlass/util/reference/host/trmm.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
-#include "testbed_rank_k_universal.h"
+#include "testbed_trmm_universal.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syrk_cf32n_cf32t_l_tensor_op_f32, 32x32x16_16x16x16) {
+TEST(SM80_Device_Trmm_cf64n_cf64n_cf64t_ls_u_nu_tensor_op_f64_gaussian, 32x32x16_16x16x16) {
 
-  using ElementA = cutlass::complex<float>;
-  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementOutput = cutlass::complex<double>;
+  using ElementAccumulator = cutlass::complex<double>;
 
-  using ElementC = cutlass::complex<float>;
-  using LayoutC = cutlass::layout::RowMajor;
-  using ElementAccumulator = cutlass::complex<float>;
-
-  using RankK = cutlass::gemm::device::RankK<
-    ElementA,
-    LayoutA,
-    ElementC,
-    LayoutC,
-    cutlass::FillMode::kLower,
+  using Trmm = cutlass::gemm::device::Trmm<
+    cutlass::complex<double>,
+    cutlass::layout::ColumnMajor,
+    cutlass::SideMode::kLeft,
+    cutlass::FillMode::kUpper,
+    cutlass::DiagType::kNonUnit,
+    cutlass::complex<double>,
+    cutlass::layout::ColumnMajor,
+    ElementOutput,
+    cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<32, 32, 16>,
     cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<16, 8, 8>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
-      ElementC,
+      ElementOutput,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3,     // kStages 
-    1,     // AlignmentA
-    false, // SplitKSerial
-    cutlass::arch::OpMultiplyAddComplex,
-    cutlass::ComplexTransform::kNone,
-    cutlass::BlasMode::kSymmetric
+    4,
+    1,
+    1,
+    false,
+    cutlass::arch::OpMultiplyAddGaussianComplex,
+    cutlass::ComplexTransform::kNone
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
+  EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syrk_cf32n_cf32t_u_tensor_op_f32, 32x32x16_16x16x16) {
+TEST(SM80_Device_Trmm_cf64h_cf64n_cf64t_ls_u_nu_tensor_op_f64_gaussian, 64x64x16_32x32x16) {
 
-  using ElementA = cutlass::complex<float>;
-  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementOutput = cutlass::complex<double>;
+  using ElementAccumulator = cutlass::complex<double>;
 
-  using ElementC = cutlass::complex<float>;
-  using LayoutC = cutlass::layout::RowMajor;
-  using ElementAccumulator = cutlass::complex<float>;
-
-  using RankK = cutlass::gemm::device::RankK<
-    ElementA,
-    LayoutA,
-    ElementC,
-    LayoutC,
+  using Trmm = cutlass::gemm::device::Trmm<
+    cutlass::complex<double>,
+    cutlass::layout::ColumnMajor,
+    cutlass::SideMode::kLeft,
     cutlass::FillMode::kUpper,
+    cutlass::DiagType::kNonUnit,
+    cutlass::complex<double>,
+    cutlass::layout::ColumnMajor,
+    ElementOutput,
+    cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<64, 64, 16>,
     cutlass::gemm::GemmShape<32, 32, 16>,
-    cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<16, 8, 8>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
-      ElementC,
+      ElementOutput,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3,     // kStages 
-    1,     // AlignmentA
-    false, // SplitKSerial
-    cutlass::arch::OpMultiplyAddComplex,
-    cutlass::ComplexTransform::kNone,
-    cutlass::BlasMode::kSymmetric
+    4,
+    1,
+    1,
+    false,
+    cutlass::arch::OpMultiplyAddGaussianComplex,
+    cutlass::ComplexTransform::kConjugate
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
+  EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 #endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/linear_to_coordinate.h`

 * *Files 26% similar despite different names*

```diff
@@ -25,71 +25,70 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide SYRK interface
-  
+    \brief Reference implementation for GEMM in host-side code.
 */
+#pragma once
 
-#include <iostream>
+#include "cutlass/cutlass.h"
+#include "cutlass/coord.h"
 
-#include "../../common/cutlass_unit_test.h"
-#include "cutlass/blas3.h"
-#include "cutlass/gemm/device/rank_k.h"
-#include "cutlass/util/host_tensor.h"
-#include "cutlass/util/reference/host/rank_k_complex.h"
-#include "cutlass/util/reference/host/tensor_compare.h"
-#include "cutlass/util/reference/host/tensor_copy.h"
-#include "cutlass/util/reference/host/tensor_fill.h"
-#include "cutlass/util/tensor_view_io.h"
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+namespace cutlass {
+namespace reference {
+namespace detail {
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-#include "testbed_rank_k_universal.h"
+template <int Rank, int Index>
+struct LinearToCoordinateHelper {
 
-#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+  CUTLASS_HOST_DEVICE
+  void operator()(Coord<Rank> &coord, int64_t idx, Coord<Rank> const &extent) const {
+
+    int64_t prod = 1;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = Rank - Index; i < Rank; ++i) {
+      prod *= int64_t(extent[i]);
+    }
+
+    coord[Rank - Index - 1] = int(idx / prod);
+
+    int64_t residual = idx % prod;
+    LinearToCoordinateHelper<Rank, Index - 1>()(coord, residual, extent);
+  }
+};
+
+template <int Rank>
+struct LinearToCoordinateHelper<Rank, 0> {
+
+  CUTLASS_HOST_DEVICE
+  void operator()(Coord<Rank> &coord, int64_t idx, Coord<Rank> const &extent) const {
+    coord[Rank - 1] = int(idx);
+  }
+};
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syrk_cf64n_cf64t_l_tensor_op_f64_gaussian, 32x32x16_16x16x16) {
+template <int Rank>
+struct LinearToCoordinate {
 
-  using ElementA = cutlass::complex<double>;
-  using LayoutA = cutlass::layout::ColumnMajor;
+  CUTLASS_HOST_DEVICE
+  void operator()(Coord<Rank> &coord, int64_t idx, Coord<Rank> const &extent) const {
+    LinearToCoordinateHelper<Rank, Rank - 1>()(coord, idx, extent);
+  }
+};
 
-  using ElementC = cutlass::complex<double>;
-  using LayoutC = cutlass::layout::RowMajor;
-  using ElementAccumulator = cutlass::complex<double>;
-
-  using RankK = cutlass::gemm::device::RankK<
-    ElementA,
-    LayoutA,
-    ElementC,
-    LayoutC,
-    cutlass::FillMode::kLower,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<32, 32, 16>,
-    cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
-    cutlass::epilogue::thread::LinearCombination<
-      ElementC,
-      1,
-      ElementAccumulator,
-      ElementAccumulator
-    >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    4,     // kStages 
-    1,     // AlignmentA
-    false, // SplitKSerial
-    cutlass::arch::OpMultiplyAddGaussianComplex,
-    cutlass::ComplexTransform::kNone,
-    cutlass::BlasMode::kSymmetric
-  >;
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
-}
+} // namespace detail
+} // namespace reference
+} // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f32n_f32t_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f32n_f32t_tensor_op_fast_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f32t_f32t_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f32t_f32t_tensor_op_fast_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64n_f64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64n_f64t_tensor_op_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64t_f64n_tensor_op_f64_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64t_f64n_tensor_op_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_tf32n_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_tf32n_f32t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_tf32t_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_tf32t_f32t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_gemm_with_reduction.h`

 * *Files 16% similar despite different names*

```diff
@@ -24,14 +24,15 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
+
 /*! \file
     \brief Tests for device-wide GEMM interface
 */
 
 #pragma once
 
 #include <iostream>
@@ -44,76 +45,88 @@
 #include "cutlass/util/tensor_view_io.h"
 #include "cutlass/util/distribution.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_norm.h"
 #include "cutlass/util/reference/host/gemm.h"
+#include "cutlass/util/reference/host/gemm_complex.h"
 
 #include "testbed_utils.h"
-#include "testbed_universal.h"
-
-#include "cutlass/layout/matrix.h"
-#include "cutlass/matrix_coord.h"
-#include "cutlass/gemm/device/gemm_universal_adapter.h"
 
 namespace test {
 namespace gemm {
 namespace device {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-template <typename Gemm, bool Relu = false>
-struct Testbed {
+template <typename Gemm, typename BinaryOp>
+struct GemmWithReductionReference {
+  using ElementAccumulator = typename Gemm::ElementAccumulator;
+  using ElementCompute = typename Gemm::GemmKernel::Epilogue::ElementCompute;
+  using ElementC = typename Gemm::ElementC;
+  using ElementT = typename Gemm::GemmKernel::Epilogue::ElementTensor;
+  //
+  // Data members
+  //
+
+  BinaryOp binary_op;
+
+  //
+  // Methods
+  //
+
+  GemmWithReductionReference() { }
+
+  ElementCompute operator()(
+    ElementAccumulator d_y, 
+    ElementT t) {
+    
+    return binary_op(ElementCompute(d_y), ElementCompute(t));
+  }
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+template <
+  typename Gemm,
+  typename ReferenceOp
+>
+struct TestbedGemmWithReduction {
 
   using ElementAccumulator = typename Gemm::ElementAccumulator;
-  using ElementCompute = typename Gemm::GemmKernel::Epilogue::OutputOp::ElementCompute;
+  using ElementT = typename Gemm::GemmKernel::Epilogue::ElementTensor;
 
   /// Initialization
-  typename Gemm::LayoutA::Stride stride_factor_A;
-  typename Gemm::LayoutB::Stride stride_factor_B;
-  typename Gemm::LayoutC::Stride stride_factor_C;
   cutlass::Distribution::Kind init_A;
   cutlass::Distribution::Kind init_B;
   cutlass::Distribution::Kind init_C;
   uint64_t seed;
 
   cutlass::HostTensor<typename Gemm::ElementA, typename Gemm::LayoutA> tensor_A;
   cutlass::HostTensor<typename Gemm::ElementB, typename Gemm::LayoutB> tensor_B;
   cutlass::HostTensor<typename Gemm::ElementC, typename Gemm::LayoutC> tensor_C;
   cutlass::HostTensor<typename Gemm::ElementC, typename Gemm::LayoutC> tensor_D;
+  cutlass::HostTensor<typename Gemm::ElementAccumulator, typename Gemm::LayoutC> tensor_Reduction;
+  cutlass::HostTensor<ElementT, typename Gemm::LayoutC> tensor_Tensor;
+  cutlass::HostTensor<ElementAccumulator, typename Gemm::LayoutC> tensor_C_ref;
+  cutlass::HostTensor<ElementAccumulator, typename Gemm::LayoutC> reference_d_Y;
   cutlass::HostTensor<typename Gemm::ElementC, typename Gemm::LayoutC> reference_D;
+  cutlass::HostTensor<typename Gemm::ElementAccumulator, typename Gemm::LayoutC> reference_Reduction;
 
   //
   // Methods
   //
 
-  Testbed(
-    cutlass::Distribution::Kind init_A_ = cutlass::Distribution::Uniform,
-    cutlass::Distribution::Kind init_B_ = cutlass::Distribution::Uniform,
-    cutlass::Distribution::Kind init_C_ = cutlass::Distribution::Uniform,
-    uint64_t seed_ = 2080
-  ):
-    stride_factor_A(typename Gemm::LayoutA::Stride()),
-    stride_factor_B(typename Gemm::LayoutB::Stride()),
-    stride_factor_C(typename Gemm::LayoutC::Stride()),
-    init_A(init_A_), init_B(init_B_), init_C(init_C_), seed(seed_) { }
-
-  Testbed(
-    typename Gemm::LayoutA::Stride stride_factor_A_,
-    typename Gemm::LayoutB::Stride stride_factor_B_,
-    typename Gemm::LayoutC::Stride stride_factor_C_,
+  TestbedGemmWithReduction(
     cutlass::Distribution::Kind init_A_ = cutlass::Distribution::Uniform,
     cutlass::Distribution::Kind init_B_ = cutlass::Distribution::Uniform,
     cutlass::Distribution::Kind init_C_ = cutlass::Distribution::Uniform,
     uint64_t seed_ = 2080
   ):
-    stride_factor_A(stride_factor_A_),
-    stride_factor_B(stride_factor_B_),
-    stride_factor_C(stride_factor_C_),
     init_A(init_A_), init_B(init_B_), init_C(init_C_), seed(seed_) { }
 
   /// Helper to initialize a tensor view
   template <typename Element, typename Layout>
   bool initialize_tensor(
     cutlass::TensorView<Element, Layout> view, 
     cutlass::Distribution::Kind dist_kind,
@@ -148,16 +161,21 @@
     } 
     else if (dist_kind == cutlass::Distribution::Gaussian) {
 
       cutlass::reference::host::TensorFillRandomGaussian(view, seed, 0, 0.5);
     }
     else if (dist_kind == cutlass::Distribution::Sequential) {
 
-      cutlass::reference::host::BlockFillSequential(
-        view.data(), view.capacity());
+      for (int m = 0; m < view.extent().row(); ++m) {
+        for (int n = 0; n < view.extent().column(); ++n) {
+          //view.at({m, n}) = Element(float(((idx ++) % 17) - 8));
+          view.at({m, n}) = (n == 0 ? Element(m) : Element());
+
+        }
+      }
     } 
     else {
       // TODO: Implement the rest
       EXPECT_TRUE(false) << "Not implemented";
       return false;
     }
 
@@ -166,135 +184,181 @@
 
   /// Initializes data structures
   void initialize(cutlass::gemm::GemmCoord problem_size) {
     //
     // Allocate the GEMM workspace
     //
 
-    tensor_A.resize(problem_size.mk(), cutlass::layout::Affine2Layout_Factory<typename Gemm::LayoutA>::layout_factory(problem_size.mk(), stride_factor_A));
-    tensor_B.resize(problem_size.kn(), cutlass::layout::Affine2Layout_Factory<typename Gemm::LayoutB>::layout_factory(problem_size.kn(), stride_factor_B));
-    tensor_C.resize(problem_size.mn(), cutlass::layout::Affine2Layout_Factory<typename Gemm::LayoutC>::layout_factory(problem_size.mn(), stride_factor_C));
-    tensor_D.resize(problem_size.mn(), cutlass::layout::Affine2Layout_Factory<typename Gemm::LayoutC>::layout_factory(problem_size.mn(), stride_factor_C));
-    reference_D.resize(problem_size.mn(), cutlass::layout::Affine2Layout_Factory<typename Gemm::LayoutC>::layout_factory(problem_size.mn(), stride_factor_C), false);
+    tensor_A.resize(problem_size.mk());
+    tensor_B.resize(problem_size.kn());
+    tensor_C.resize(problem_size.mn());
+    tensor_D.resize(problem_size.mn());
+
+    tensor_Reduction.resize({
+      problem_size.m(), 
+      (problem_size.n() - 1 + Gemm::ThreadblockShape::kN) / Gemm::ThreadblockShape::kN
+    });
+
+    tensor_Tensor.resize(problem_size.mn());
+    reference_D.resize(problem_size.mn(), false);
+    reference_d_Y.resize(problem_size.mn(), false);
+    tensor_C_ref.resize(problem_size.mn(), false);
+    reference_Reduction.resize({problem_size.m(), 1}, false);
 
     EXPECT_TRUE(initialize_tensor(tensor_A.host_view(), init_A, seed + 2019));
     EXPECT_TRUE(initialize_tensor(tensor_B.host_view(), init_B, seed + 2018));
     EXPECT_TRUE(initialize_tensor(tensor_C.host_view(), init_C, seed + 2017));
+    EXPECT_TRUE(initialize_tensor(tensor_Tensor.host_view(), init_C, seed + 2020));
 
     // It is possible to randomly initialize to all zeros, so override this with non-zeros
     // in the upper left corner of each operand.
     tensor_A.host_view().at({0, 0}) = typename Gemm::ElementA(1);
     tensor_B.host_view().at({0, 0}) = typename Gemm::ElementB(1);
-    tensor_C.host_view().at(cutlass::make_Coord(0, 0)) = typename Gemm::ElementC(1);
+    tensor_C.host_view().at({0, 0}) = typename Gemm::ElementC(1);
 
-    cutlass::reference::host::TensorCopy(reference_D.host_view(), tensor_C.host_view());
+    for (int m = 0; m < tensor_C_ref.extent().row(); ++m) {
+      for (int n = 0; n < tensor_C_ref.extent().column(); ++n) {
+        tensor_C_ref.at({m, n}) = ElementAccumulator(tensor_C.at({m, n}));
+      }
+    }
 
     tensor_A.sync_device();
     tensor_B.sync_device();
     tensor_C.sync_device();
     tensor_D.sync_device();
+    tensor_Reduction.sync_device();
+    tensor_Tensor.sync_device();
   }
 
   /// Compares computed reference with device reference and outputs to a file if incorrect
   bool compare_reference(
     cutlass::gemm::GemmCoord problem_size, 
-    ElementCompute alpha, 
-    ElementCompute beta) {
+    ElementAccumulator alpha, 
+    ElementAccumulator beta) {
 
+    tensor_Reduction.sync_host();
     tensor_D.sync_host();
 
     EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_A.host_view()), 0);
     EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_B.host_view()), 0);
     EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_C.host_view()), 0);
+    
+    EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_D.host_view()), 0);
+    EXPECT_GT(cutlass::reference::host::TensorNorm(reference_D.host_view()), 0);
+    EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_Reduction.host_view()), 0);
+
+    bool passed = true;
+    for (int m = 0; m < tensor_Reduction.extent().row(); ++m) {
+
+      ElementAccumulator reduced_value = ElementAccumulator();
+      for (int j = 0; j < tensor_Reduction.extent().column(); ++j) {
+        reduced_value += tensor_Reduction.at({m, j});
+      }
 
-    if (tensor_D.size() > 1)
-      EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_D.host_view()), 0);
-
-    if (reference_D.size() > 1)
-      EXPECT_GT(cutlass::reference::host::TensorNorm(reference_D.host_view()), 0);
-
-    bool passed = cutlass::reference::host::TensorEquals(reference_D.host_view(), tensor_D.host_view());
-
-    EXPECT_TRUE(passed);
+      if (reduced_value != reference_Reduction.at({m, 0})) {
+        std::cout << "Error in bias[" << m << "] - Expected: " << reference_Reduction.at({m, 0}) << ", got: " << reduced_value << std::endl;
+        passed = false;
+        break;
+      }
+    }
+    EXPECT_TRUE(passed) << "Reduction is incorect.";
 
+    if (!cutlass::reference::host::TensorEquals(reference_D.host_view(), tensor_D.host_view())) {
+      EXPECT_TRUE(false) << " mismatched reference";
+      passed = false;
+    }
+    
     if (!passed) {
 
+      /*
       std::stringstream fname;
 
-      fname << "error_Gemm_device_" 
+      fname << "error_Gemm_device_"
         << problem_size.m() << "x"
         << problem_size.n() << "x"
         << problem_size.k() << "_"
         << Gemm::ThreadblockShape::kM << "x"  
         << Gemm::ThreadblockShape::kN << "x"  
         << Gemm::ThreadblockShape::kK << "_"
         << Gemm::WarpShape::kM << "x"  
         << Gemm::WarpShape::kN << "x"  
         << Gemm::WarpShape::kK << ".txt";
 
       std::ofstream file(fname.str());
+      */
+
+      std::ofstream file("testbed_universal_errors_sm70.txt");
 
       file
         << "problem: " << problem_size 
         << ", alpha: " << alpha << ", beta: " << beta << "\n\n";
 
       file 
         << "A =\n" << tensor_A.host_view()
         << "\nB =\n" << tensor_B.host_view()
         << "\nC =\n" << tensor_C.host_view()
+        << "\nT = \n" << tensor_Tensor.host_view()
         << "\n\nReference =\n" << reference_D.host_view()
-        << "\nComputed =\n" << tensor_D.host_view();
+        << "\nComputed =\n" << tensor_D.host_view()
+        << "\n\nReduction =\n" << tensor_Reduction.host_view() << "\n"
+        << "\nReference reduction =\n" << reference_Reduction.host_view() << "\n";
     }
 
     return passed;
   }
 
   /// Verifies the result is a GEMM
   bool verify(
     cutlass::gemm::GemmCoord problem_size, 
-    ElementCompute alpha, 
-    ElementCompute beta) {
+    ElementAccumulator alpha, 
+    ElementAccumulator beta) {
 
     //
     // Verify
     //
-    
-    cutlass::reference::host::Gemm<
+
+    cutlass::reference::host::GemmComplex<
         typename Gemm::ElementA, typename Gemm::LayoutA,
         typename Gemm::ElementB, typename Gemm::LayoutB,
-        typename Gemm::ElementC, typename Gemm::LayoutC, ElementCompute,
-        ElementAccumulator, typename Gemm::Operator>
-        reference_gemm;
-
-    reference_gemm(
+        ElementAccumulator, typename Gemm::LayoutC, 
+        ElementAccumulator, ElementAccumulator
+    >(
       problem_size,
       alpha, 
-      tensor_A.host_ref(), 
-      tensor_B.host_ref(), 
+      tensor_A.host_ref(),
+      Gemm::kTransformA,
+      tensor_B.host_ref(),
+      Gemm::kTransformB,
       beta, 
-      reference_D.host_ref(), 
+      tensor_C_ref.host_ref(), 
+      reference_d_Y.host_ref(), 
       ElementAccumulator(0)
     );
 
-    if (Relu) {
-      for (int i = 0; i < problem_size.m(); ++i) {
-        for (int j = 0; j < problem_size.n(); ++j) {
-           reference_D.at(cutlass::MatrixCoord(i, j)) = 
-                  ((ElementCompute)reference_D.at(cutlass::MatrixCoord(i, j)) < (ElementCompute)0)
-                  ? (typename Gemm::ElementC)0
-                  : reference_D.at(cutlass::MatrixCoord(i, j));
-        }
+    using ElementC = typename Gemm::ElementC;
+
+    ReferenceOp reference_op;
+
+    // compute backwards 
+    for (int m = 0; m < problem_size.m(); ++m) {
+      ElementAccumulator reduced_value = ElementAccumulator();
+      for (int n = 0; n < problem_size.n(); ++n) {
+        ElementAccumulator d_full = reference_op(reference_d_Y.at({m, n}), tensor_Tensor.at({m, n}));
+        reduced_value += d_full;
+        reference_D.at({m, n}) = ElementC(d_full);
       }
+      reference_Reduction.at({m, 0}) = reduced_value;
     }
 
     return compare_reference(problem_size, alpha, beta);
   }
 
-	/// Determine if the CUDA device is sufficient to run the kernel
+  /// Returns true if the CUDA device is sufficient to execute the kernel.
   bool sufficient() const {
+
     //
     // Determine SMEM requirements and waive if not satisfied
     //
 
     int smem_size = int(sizeof(typename Gemm::GemmKernel::SharedStorage));
 
     cudaDeviceProp properties;
@@ -314,30 +378,21 @@
     if (properties.sharedMemPerBlockOptin < smem_size) {
       return false;
     }
 
     return true;
   }
 
-
   /// Executes one test
   bool run(
-    cutlass::gemm::GemmCoord problem_size,
-    int split_k_slices = 1,
-    ElementCompute alpha = ElementCompute(1),
-    ElementCompute beta = ElementCompute(0))
-  {
-/*
-    std::cout << "\n-----------------------\n";
-    std::cout << "problem size: " << problem_size << "\n";
-    std::cout << "split_k_slices: " << split_k_slices << "\n";
-    std::cout << "alpha: " << alpha << "\n";
-    std::cout << "beta: " << beta << "\n";
-    std::cout << "-----------------------\n\n";
-*/
+    cutlass::gemm::GemmUniversalMode mode,
+    cutlass::gemm::GemmCoord problem_size, 
+    int batch_count = 1,
+    ElementAccumulator alpha = ElementAccumulator(1), 
+    ElementAccumulator beta = ElementAccumulator(0)) {
 
     // Waive test if insufficient CUDA device
     if (!sufficient()) {
       if (CUTLASS_TEST_UNIT_ENABLE_WARNINGS) {
         std::cerr << "Test waived due to insufficient CUDA device." << std::endl;
       }
       return true;
@@ -346,36 +401,48 @@
     this->initialize(problem_size);
 
     //
     // Initialize the GEMM operator
     //
 
     typename Gemm::Arguments arguments{
+      mode,
       problem_size,
-      tensor_A.device_ref(),
-      tensor_B.device_ref(),
-      tensor_C.device_ref(),
-      tensor_D.device_ref(),
+      batch_count,
       {alpha, beta},
-      split_k_slices
+      tensor_A.device_data(),
+      tensor_B.device_data(),
+      tensor_C.device_data(),
+      tensor_D.device_data(),
+      tensor_Reduction.device_data(),
+      tensor_Tensor.device_data(),
+      problem_size.m() * problem_size.k(),
+      problem_size.n() * problem_size.k(),
+      problem_size.m() * problem_size.n(),
+      problem_size.m() * problem_size.n(),
+      problem_size.m(),
+      problem_size.m() * problem_size.n(),
+      tensor_A.layout().stride(0),
+      tensor_B.layout().stride(0),
+      tensor_C.layout().stride(0),
+      tensor_D.layout().stride(0),
+      tensor_Reduction.layout().stride(0),
+      tensor_Tensor.layout().stride(0),
     };
 
     Gemm gemm_op;
 
     size_t workspace_size = Gemm::get_workspace_size(arguments);
 
     cutlass::device_memory::allocation<uint8_t> workspace(workspace_size);
 
     cutlass::Status status = gemm_op.initialize(arguments, workspace.get());
 
-    if (status != cutlass::Status::kSuccess) {
-      cudaError_t error = cudaGetLastError();
-      std::cerr << "This test is not supported: " << cudaGetErrorString(error) << "\n";
-      return true;
-    }
+
+    EXPECT_TRUE(status == cutlass::Status::kSuccess) << to_string(status);
 
     //
     // Run the GEMM
     //
 
     status = gemm_op();
 
@@ -384,214 +451,135 @@
     //
     // Verify
     //
 
     bool passed = this->verify(problem_size, alpha, beta);
 
     if (!passed) {
-      std::cout << "Error with split_k_slices = " << split_k_slices << ", alpha: " << alpha << std::endl;
+      std::cout << "Failed with batch_count/split_k_slices = " << batch_count << std::endl;
     }
 
-    return passed;
-  }
-};
+    //
+    // Profile
+    //
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+    #if 0 // profiling disabled for now.
 
-template <typename Gemm, bool Relu=false>
-bool TestAllGemmBasic(
-    const typename Gemm::LayoutA::Stride& stride_factor_A = typename Gemm::LayoutA::Stride(),
-    const typename Gemm::LayoutB::Stride& stride_factor_B = typename Gemm::LayoutB::Stride(),
-    const typename Gemm::LayoutC::Stride& stride_factor_C = typename Gemm::LayoutC::Stride()) {
-  bool passed = true;
+    int const kWorkspaces = 100;
 
-  int const kMinimumOperandElementSize = 
-    std::min(
-      int(cutlass::sizeof_bits<typename Gemm::ElementA>::value), 
-      int(cutlass::sizeof_bits<typename Gemm::ElementB>::value));
-
-  int const kAlignment = cutlass::platform::is_same<
-                              typename Gemm::OperatorClass, 
-                              cutlass::arch::OpClassSimt>::value ? 1 : 128 / kMinimumOperandElementSize;
-
-  // int8_t gemm alignment constraints
-  int const kAlignmentM = cutlass::platform::is_same<typename Gemm::OperatorClass, cutlass::arch::OpClassSimt>::value &&
-                          cutlass::platform::is_same<typename Gemm::ElementA, int8_t>::value &&
-                          cutlass::platform::is_same<typename Gemm::LayoutA, cutlass::layout::ColumnMajor>::value ? 4 : kAlignment;
-
-  int const kAlignmentN = cutlass::platform::is_same<typename Gemm::OperatorClass, cutlass::arch::OpClassSimt>::value &&
-                          cutlass::platform::is_same<typename Gemm::ElementB, int8_t>::value &&
-                          cutlass::platform::is_same<typename Gemm::LayoutB, cutlass::layout::RowMajor>::value ? 4 : kAlignment;
-
-  int const kAlignmentK = cutlass::platform::is_same<typename Gemm::OperatorClass, cutlass::arch::OpClassSimt>::value &&
-                          cutlass::platform::is_same<typename Gemm::ElementA, int8_t>::value &&
-                          cutlass::platform::is_same<typename Gemm::ElementB, int8_t>::value &&
-                          (cutlass::platform::is_same<typename Gemm::LayoutA, cutlass::layout::RowMajor>::value ||
-                          cutlass::platform::is_same<typename Gemm::LayoutB, cutlass::layout::ColumnMajor>::value) ? 4 : kAlignment;
-
-  int problem_size_m[] = {kAlignmentM, 512 - 3 * kAlignmentM};
-
-  int problem_size_n[] = {kAlignmentN, 512 - 2 * kAlignmentN};
-
-  int problem_size_k[] = {
-      kAlignmentK, Gemm::ThreadblockShape::kK * (Gemm::kStages + 1) - kAlignmentK};
-
-  int split_k_slices[] = {
-    1, 2, 3
-  };
-
-  double problem_alpha[] = {
-    1
-  };
-
-  double problem_beta[] = {
-    2.0
-  };
-
-  Testbed<Gemm, Relu> testbed(stride_factor_A, stride_factor_B, stride_factor_C);
-
-  using ElementCompute = typename Gemm::EpilogueOutputOp::ElementCompute;
-
-  for (int m : problem_size_m) {
-    for (int n : problem_size_n) {
-      for (int k : problem_size_k) {
-        for (int split_k : split_k_slices) {
-
-          if (!Gemm::kSplitKSerial && split_k > 1) {
-            continue;
-          }
-
-          if (split_k > 1 && k / Gemm::ThreadblockShape::kK < split_k) {
-            continue;
-          }
-
-          for (auto alpha : problem_alpha) {
-            for (auto beta : problem_beta) {
-
-              cutlass::gemm::GemmCoord problem_size(m, n, k);
-              passed = testbed.run(
-                problem_size, 
-                split_k,
-                cutlass::from_real<ElementCompute>(alpha), 
-                cutlass::from_real<ElementCompute>(beta)
-              );
-
-              if (!passed) {
-                return false;
-              }
-            }
-          }
-        }
+    cutlass::DeviceAllocation<typename Gemm::ElementA> profiling_tensor_A(tensor_A.capacity() * kWorkspaces);
+    cutlass::DeviceAllocation<typename Gemm::ElementB> profiling_tensor_B(tensor_B.capacity() * kWorkspaces);
+    cutlass::DeviceAllocation<typename Gemm::ElementC> profiling_tensor_C(tensor_C.capacity() * kWorkspaces);
+    cutlass::DeviceAllocation<typename Gemm::ElementC> profiling_tensor_D(tensor_D.capacity() * kWorkspaces);
+    cutlass::DeviceAllocation<typename Gemm::ElementC> profiling_tensor_Reduction(tensor_Reduction.capacity() * kWorkspaces);
+    cutlass::DeviceAllocation<ElementT> profiling_tensor_Tensor(tensor_Tensor.capacity() * kWorkspaces);
+
+    cudaEvent_t events[2];
+    for (auto & event : events) {
+      cudaError_t result = cudaEventCreate(&event);
+      if (result != cudaSuccess) {
+        EXPECT_EQ(result, cudaSuccess) << " cudaEventCreate() failed with error " << cudaGetErrorString(result);
+        return false;
+        break;
       }
     }
-  }
-
-  return passed;
-}
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
 
-template <typename Gemm, bool Relu=false>
-bool TestAllGemm(
-    const typename Gemm::LayoutA::Stride& stride_factor_A,
-    const typename Gemm::LayoutB::Stride& stride_factor_B = typename Gemm::LayoutB::Stride(),
-    const typename Gemm::LayoutC::Stride& stride_factor_C = typename Gemm::LayoutC::Stride())
-{
-  // Test basic GEMM with non-default stride factors
-  return TestAllGemmBasic<Gemm, Relu>(stride_factor_A, stride_factor_B, stride_factor_C);
-}
-
-template <typename Gemm, bool Relu=false>
-bool TestAllGemm()
-{
-#ifdef NDEBUG
-  // Non-debug builds also test basic GEMM with default stride factors
-  if (!TestAllGemmBasic<Gemm, Relu>()) {
-    return false;
-  }
-#endif // NDEBUG
+    int const kWarmupIterations = 5;
+    int const kProfilingIterations = 100;
 
-  // Test universal GEMM
-#if 0
-  // Define the universal kernel
-  using UniversalKernel = cutlass::gemm::kernel::GemmUniversal<
-    typename Gemm::GemmKernel::Mma,                                 // Mma
-    typename Gemm::GemmKernel::Epilogue,                            // Epilogue
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>    // ThreadblockSwizzle
-  >;
-#else
-  // Define the streamk universal kernel
-  using UniversalKernel = cutlass::gemm::kernel::GemmUniversalStreamk<
-    typename Gemm::GemmKernel::Mma,                                 // Mma
-    typename Gemm::GemmKernel::Epilogue,                            // Epilogue
-    cutlass::gemm::threadblock::ThreadblockSwizzleStreamK           // ThreadblockSwizzle
-  >;
-#endif
-
-  // Define the universal adaptor
-  using UniversalGemm = cutlass::gemm::device::GemmUniversalAdapter<UniversalKernel>;
-
-  // Test universal GEMM
-  return TestAllGemmUniversal<UniversalGemm, Relu>();
-}
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-template <typename Gemm>
-bool TestGemmPerf(int iterations = 1) {
-  bool passed = true;
-
-  int problem_size_m[] = { 2048 };
+    for (int i = 0; i < kWarmupIterations; ++i) {
+      status = gemm_op();
+      EXPECT_TRUE(status == cutlass::Status::kSuccess) << to_string(status);
+    }
+    
 
-  int problem_size_n[] = { 4352 };
+    cudaError_t result = cudaEventRecord(events[0]);
+    EXPECT_EQ(result, cudaSuccess);
 
-  int problem_size_k[] = { 4096  };
+    for (int i = 0; i < kProfilingIterations; ++i) {
 
-  int split_k_slices[] = { 1 };
-  double problem_alpha[] = { 1 };
-  double problem_beta[] = { 0.0 };
+      typename Gemm::Arguments arguments{
+        mode,
+        problem_size,
+        batch_count,
+        {alpha, beta},
+        profiling_tensor_A.get() + tensor_A.capacity() * (i % kWorkspaces),
+        profiling_tensor_B.get() + tensor_B.capacity() * (i % kWorkspaces),
+        profiling_tensor_C.get() + tensor_C.capacity() * (i % kWorkspaces),
+        profiling_tensor_D.get() + tensor_D.capacity() * (i % kWorkspaces),
+        profiling_tensor_Reduction.get() + tensor_Reduction.capacity() * (i % kWorkspaces),
+        profiling_tensor_Tensor.get() + tensor_Tensor.capacity() * (i % kWorkspaces),
+        problem_size.m() * problem_size.k(),
+        problem_size.n() * problem_size.k(),
+        problem_size.m() * problem_size.n(),
+        problem_size.m() * problem_size.n(),
+        problem_size.m(),
+        problem_size.m() * problem_size.n(),
+        tensor_A.layout().stride(0),
+        tensor_B.layout().stride(0),
+        tensor_C.layout().stride(0),
+        tensor_D.layout().stride(0),
+        tensor_Reduction.layout().stride(0),
+        tensor_Tensor.layout().stride(0),
+      };
+
+      gemm_op.initialize(arguments, workspace.get());
+      status = gemm_op();
+      EXPECT_TRUE(status == cutlass::Status::kSuccess) << to_string(status);
+    }
+
+    result = cudaEventRecord(events[1]);
+    EXPECT_EQ(result, cudaSuccess);
+
+    result = cudaDeviceSynchronize();
+    EXPECT_EQ(result, cudaSuccess);
+
+    float elapsed_time = 0;
+    result = cudaEventElapsedTime(&elapsed_time, events[0], events[1]);
+    EXPECT_EQ(result, cudaSuccess);
 
-  Testbed<Gemm> testbed;
+    double average_time = double(elapsed_time) / double(kProfilingIterations);
 
-  using ElementCompute = typename Gemm::EpilogueOutputOp::ElementCompute;
+    std::cout << problem_size << ": " << average_time << " ms" << std::endl;
 
-  for (int m : problem_size_m) {
-    for (int n : problem_size_n) {
-      for (int k : problem_size_k) {
-        for (int split_k : split_k_slices) {
+    for (auto & event : events) {
+      cudaEventDestroy(event);
+    }
+    #endif
 
-          if (!Gemm::kSplitKSerial && split_k > 1) {
-            continue;
-          }
+    return passed;
+  }
+};
 
-          for (auto alpha : problem_alpha) {
-            for (auto beta : problem_beta) {
+/////////////////////////////////////////////////////////////////////////////////////////////////
+template <typename Gemm, typename ReferenceOp>
+bool TestGemmWithReduction(
+  cutlass::gemm::GemmCoord const & problem_size,
+  cutlass::gemm::GemmUniversalMode mode,
+  int batch_count = 1,
+  double alpha = 1.0, 
+  double beta = 2.0) {
 
-              cutlass::gemm::GemmCoord problem_size(m, n, k);
+  bool passed = true;
 
-              for (int i = 0; i < iterations; i++){
-                passed = testbed.run(
-                  problem_size, 
-                  split_k,
-                  cutlass::from_real<ElementCompute>(alpha), 
-                  cutlass::from_real<ElementCompute>(beta)
-                );
-              }
+  TestbedGemmWithReduction<Gemm, ReferenceOp> testbed;
+  
+  using ElementAccumulator = typename Gemm::ElementAccumulator;
 
-              if (!passed) {
-                return false;
-              }
-            }
-          }
-        }
-      }
-    }
-  }
+  passed = testbed.run(
+    mode,
+    problem_size, 
+    batch_count,
+    cutlass::from_real<ElementAccumulator>(alpha), 
+    cutlass::from_real<ElementAccumulator>(beta)
+  );
 
   return passed;
 }
 
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
 } // namespace device
 } // namespace gemm
 } // namespace test
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_gemm_with_broadcast.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_gemm_with_broadcast.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_gemm_with_reduction.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_sparse.h`

 * *Files 20% similar despite different names*

```diff
@@ -24,17 +24,18 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
 /*! \file
     \brief Tests for device-wide GEMM interface
+
+  Testbed for sparse operations not to be released for CUDA 11.0 GA. Expected release is 11.1.
 */
 
 #pragma once
 
 #include <iostream>
 #include <fstream>
 #include <sstream>
@@ -45,89 +46,71 @@
 #include "cutlass/util/tensor_view_io.h"
 #include "cutlass/util/distribution.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_norm.h"
 #include "cutlass/util/reference/host/gemm.h"
-#include "cutlass/util/reference/host/gemm_complex.h"
+#include "cutlass/util/host_reorder.h"
+#include "cutlass/util/host_uncompress.h"
 
 #include "testbed_utils.h"
 
 namespace test {
 namespace gemm {
 namespace device {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-template <typename Gemm, typename BinaryOp>
-struct GemmWithReductionReference {
-  using ElementAccumulator = typename Gemm::ElementAccumulator;
-  using ElementCompute = typename Gemm::GemmKernel::Epilogue::ElementCompute;
-  using ElementC = typename Gemm::ElementC;
-  using ElementT = typename Gemm::GemmKernel::Epilogue::ElementTensor;
-  //
-  // Data members
-  //
-
-  BinaryOp binary_op;
-
-  //
-  // Methods
-  //
-
-  GemmWithReductionReference() { }
-
-  ElementCompute operator()(
-    ElementAccumulator d_y, 
-    ElementT t) {
-    
-    return binary_op(ElementCompute(d_y), ElementCompute(t));
-  }
-};
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-template <
-  typename Gemm,
-  typename ReferenceOp
->
-struct TestbedGemmWithReduction {
+template <typename Gemm>
+struct SparseTestbed {
 
   using ElementAccumulator = typename Gemm::ElementAccumulator;
-  using ElementT = typename Gemm::GemmKernel::Epilogue::ElementTensor;
+  using ElementCompute = typename Gemm::GemmKernel::Epilogue::OutputOp::ElementCompute;
+
+  static int const kSparse = Gemm::GemmKernel::kSparse;
+  static int const kMetaSizeInBits = Gemm::GemmKernel::kMetaSizeInBits;
+  static int const kMaxID2 = Gemm::GemmKernel::kMaxID2;
+  static int const kElementsPerElementE = Gemm::GemmKernel::kElementsPerElementE;
+
+  using ElementE = typename Gemm::GemmKernel::ElementE;
+  using LayoutE = cutlass::layout::RowMajor;
+  using ReorderedLayoutE = typename Gemm::GemmKernel::LayoutE;
 
   /// Initialization
   cutlass::Distribution::Kind init_A;
   cutlass::Distribution::Kind init_B;
   cutlass::Distribution::Kind init_C;
+  cutlass::Distribution::Kind init_E;
   uint64_t seed;
 
   cutlass::HostTensor<typename Gemm::ElementA, typename Gemm::LayoutA> tensor_A;
+  cutlass::HostTensor<typename Gemm::ElementA, typename Gemm::LayoutA> tensor_A_uncompressed;
   cutlass::HostTensor<typename Gemm::ElementB, typename Gemm::LayoutB> tensor_B;
   cutlass::HostTensor<typename Gemm::ElementC, typename Gemm::LayoutC> tensor_C;
   cutlass::HostTensor<typename Gemm::ElementC, typename Gemm::LayoutC> tensor_D;
-  cutlass::HostTensor<typename Gemm::ElementAccumulator, typename Gemm::LayoutC> tensor_Reduction;
-  cutlass::HostTensor<ElementT, typename Gemm::LayoutC> tensor_Tensor;
-  cutlass::HostTensor<ElementAccumulator, typename Gemm::LayoutC> tensor_C_ref;
-  cutlass::HostTensor<ElementAccumulator, typename Gemm::LayoutC> reference_d_Y;
   cutlass::HostTensor<typename Gemm::ElementC, typename Gemm::LayoutC> reference_D;
-  cutlass::HostTensor<typename Gemm::ElementAccumulator, typename Gemm::LayoutC> reference_Reduction;
+  cutlass::HostTensor<ElementE, LayoutE> tensor_E;
+  cutlass::HostTensor<ElementE, ReorderedLayoutE> tensor_E_reordered;
 
   //
   // Methods
   //
 
-  TestbedGemmWithReduction(
-    cutlass::Distribution::Kind init_A_ = cutlass::Distribution::Uniform,
-    cutlass::Distribution::Kind init_B_ = cutlass::Distribution::Uniform,
-    cutlass::Distribution::Kind init_C_ = cutlass::Distribution::Uniform,
-    uint64_t seed_ = 2080
-  ):
-    init_A(init_A_), init_B(init_B_), init_C(init_C_), seed(seed_) { }
+  SparseTestbed(
+      cutlass::Distribution::Kind init_A_ = cutlass::Distribution::Uniform,
+      cutlass::Distribution::Kind init_B_ = cutlass::Distribution::Uniform,
+      cutlass::Distribution::Kind init_C_ = cutlass::Distribution::Uniform,
+      cutlass::Distribution::Kind init_E_ = cutlass::Distribution::Uniform,
+      uint64_t seed_ = 2080)
+      : init_A(init_A_),
+        init_B(init_B_),
+        init_C(init_C_),
+        init_E(init_E_),
+        seed(seed_) {}
 
   /// Helper to initialize a tensor view
   template <typename Element, typename Layout>
   bool initialize_tensor(
     cutlass::TensorView<Element, Layout> view, 
     cutlass::Distribution::Kind dist_kind,
     uint64_t seed) {
@@ -161,21 +144,16 @@
     } 
     else if (dist_kind == cutlass::Distribution::Gaussian) {
 
       cutlass::reference::host::TensorFillRandomGaussian(view, seed, 0, 0.5);
     }
     else if (dist_kind == cutlass::Distribution::Sequential) {
 
-      for (int m = 0; m < view.extent().row(); ++m) {
-        for (int n = 0; n < view.extent().column(); ++n) {
-          //view.at({m, n}) = Element(float(((idx ++) % 17) - 8));
-          view.at({m, n}) = (n == 0 ? Element(m) : Element());
-
-        }
-      }
+      cutlass::reference::host::BlockFillSequential(
+        view.data(), view.capacity());
     } 
     else {
       // TODO: Implement the rest
       EXPECT_TRUE(false) << "Not implemented";
       return false;
     }
 
@@ -183,182 +161,152 @@
   }
 
   /// Initializes data structures
   void initialize(cutlass::gemm::GemmCoord problem_size) {
     //
     // Allocate the GEMM workspace
     //
-
-    tensor_A.resize(problem_size.mk());
+    tensor_A.resize(cutlass::make_Coord(problem_size.m(), problem_size.k() / kSparse));
+    tensor_A_uncompressed.resize(problem_size.mk());
     tensor_B.resize(problem_size.kn());
     tensor_C.resize(problem_size.mn());
     tensor_D.resize(problem_size.mn());
-
-    tensor_Reduction.resize({
-      problem_size.m(), 
-      (problem_size.n() - 1 + Gemm::ThreadblockShape::kN) / Gemm::ThreadblockShape::kN
-    });
-
-    tensor_Tensor.resize(problem_size.mn());
     reference_D.resize(problem_size.mn(), false);
-    reference_d_Y.resize(problem_size.mn(), false);
-    tensor_C_ref.resize(problem_size.mn(), false);
-    reference_Reduction.resize({problem_size.m(), 1}, false);
+    tensor_E.resize(cutlass::make_Coord(
+        problem_size.m(), problem_size.k() / kSparse / kElementsPerElementE));
+    tensor_E_reordered.resize(cutlass::make_Coord(
+        problem_size.m(), problem_size.k() / kSparse / kElementsPerElementE));
 
     EXPECT_TRUE(initialize_tensor(tensor_A.host_view(), init_A, seed + 2019));
     EXPECT_TRUE(initialize_tensor(tensor_B.host_view(), init_B, seed + 2018));
     EXPECT_TRUE(initialize_tensor(tensor_C.host_view(), init_C, seed + 2017));
-    EXPECT_TRUE(initialize_tensor(tensor_Tensor.host_view(), init_C, seed + 2020));
+
+    if (init_E == cutlass::Distribution::Uniform) {
+      uint64_t seed = 7;
+      cutlass::reference::host::TensorFillRandomSparseMeta(
+          tensor_E.host_view(), seed, kMetaSizeInBits);
+    } else if (init_E == cutlass::Distribution::Identity) {
+      uint32_t content = (kMaxID2 == 1) ? 0x44444444 : 0x4444;
+      cutlass::reference::host::TensorFill(tensor_E.host_view(),
+                                           (ElementE)(content));
+    } else {
+      // TODO: Implement the rest
+      EXPECT_TRUE(false);
+    }
+
+    cutlass::reorder_meta(tensor_E_reordered.host_ref(), tensor_E.host_ref(),
+                          {problem_size.m(), problem_size.n(),
+                           problem_size.k() / kSparse / kElementsPerElementE});
 
     // It is possible to randomly initialize to all zeros, so override this with non-zeros
     // in the upper left corner of each operand.
     tensor_A.host_view().at({0, 0}) = typename Gemm::ElementA(1);
     tensor_B.host_view().at({0, 0}) = typename Gemm::ElementB(1);
     tensor_C.host_view().at({0, 0}) = typename Gemm::ElementC(1);
 
-    for (int m = 0; m < tensor_C_ref.extent().row(); ++m) {
-      for (int n = 0; n < tensor_C_ref.extent().column(); ++n) {
-        tensor_C_ref.at({m, n}) = ElementAccumulator(tensor_C.at({m, n}));
-      }
-    }
+    cutlass::reference::host::TensorCopy(reference_D.host_view(), tensor_C.host_view());
 
     tensor_A.sync_device();
     tensor_B.sync_device();
     tensor_C.sync_device();
     tensor_D.sync_device();
-    tensor_Reduction.sync_device();
-    tensor_Tensor.sync_device();
+    tensor_E_reordered.sync_device();
   }
 
   /// Compares computed reference with device reference and outputs to a file if incorrect
   bool compare_reference(
     cutlass::gemm::GemmCoord problem_size, 
-    ElementAccumulator alpha, 
-    ElementAccumulator beta) {
+    ElementCompute alpha, 
+    ElementCompute beta) {
 
-    tensor_Reduction.sync_host();
     tensor_D.sync_host();
 
     EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_A.host_view()), 0);
     EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_B.host_view()), 0);
     EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_C.host_view()), 0);
-    
-    EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_D.host_view()), 0);
-    EXPECT_GT(cutlass::reference::host::TensorNorm(reference_D.host_view()), 0);
-    EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_Reduction.host_view()), 0);
-
-    bool passed = true;
-    for (int m = 0; m < tensor_Reduction.extent().row(); ++m) {
-
-      ElementAccumulator reduced_value = ElementAccumulator();
-      for (int j = 0; j < tensor_Reduction.extent().column(); ++j) {
-        reduced_value += tensor_Reduction.at({m, j});
-      }
 
-      if (reduced_value != reference_Reduction.at({m, 0})) {
-        std::cout << "Error in bias[" << m << "] - Expected: " << reference_Reduction.at({m, 0}) << ", got: " << reduced_value << std::endl;
-        passed = false;
-        break;
-      }
-    }
-    EXPECT_TRUE(passed) << "Reduction is incorect.";
+    if (tensor_D.size() > 1)
+      EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_D.host_view()), 0);
+
+    if (reference_D.size() > 1)
+      EXPECT_GT(cutlass::reference::host::TensorNorm(reference_D.host_view()), 0);
+
+    bool passed = cutlass::reference::host::TensorEquals(reference_D.host_view(), tensor_D.host_view());
+
+    EXPECT_TRUE(passed);
 
-    if (!cutlass::reference::host::TensorEquals(reference_D.host_view(), tensor_D.host_view())) {
-      EXPECT_TRUE(false) << " mismatched reference";
-      passed = false;
-    }
-    
     if (!passed) {
 
-      /*
       std::stringstream fname;
 
-      fname << "error_Gemm_device_"
+      fname << "error_Gemm_device_" 
         << problem_size.m() << "x"
         << problem_size.n() << "x"
         << problem_size.k() << "_"
         << Gemm::ThreadblockShape::kM << "x"  
         << Gemm::ThreadblockShape::kN << "x"  
         << Gemm::ThreadblockShape::kK << "_"
         << Gemm::WarpShape::kM << "x"  
         << Gemm::WarpShape::kN << "x"  
         << Gemm::WarpShape::kK << ".txt";
 
       std::ofstream file(fname.str());
-      */
-
-      std::ofstream file("testbed_universal_errors_sm70.txt");
 
       file
         << "problem: " << problem_size 
         << ", alpha: " << alpha << ", beta: " << beta << "\n\n";
 
       file 
         << "A =\n" << tensor_A.host_view()
         << "\nB =\n" << tensor_B.host_view()
         << "\nC =\n" << tensor_C.host_view()
-        << "\nT = \n" << tensor_Tensor.host_view()
+        << "\nE =\n" << tensor_E.host_view()
         << "\n\nReference =\n" << reference_D.host_view()
-        << "\nComputed =\n" << tensor_D.host_view()
-        << "\n\nReduction =\n" << tensor_Reduction.host_view() << "\n"
-        << "\nReference reduction =\n" << reference_Reduction.host_view() << "\n";
+        << "\nComputed =\n" << tensor_D.host_view();
     }
 
     return passed;
   }
 
   /// Verifies the result is a GEMM
   bool verify(
     cutlass::gemm::GemmCoord problem_size, 
-    ElementAccumulator alpha, 
-    ElementAccumulator beta) {
+    ElementCompute alpha, 
+    ElementCompute beta) {
 
     //
     // Verify
     //
 
-    cutlass::reference::host::GemmComplex<
+    cutlass::uncompress(tensor_A_uncompressed.host_ref(), tensor_A.host_ref(),
+                        tensor_E.host_ref(), problem_size.m(), problem_size.k());
+
+    cutlass::reference::host::Gemm<
         typename Gemm::ElementA, typename Gemm::LayoutA,
         typename Gemm::ElementB, typename Gemm::LayoutB,
-        ElementAccumulator, typename Gemm::LayoutC, 
-        ElementAccumulator, ElementAccumulator
-    >(
+        typename Gemm::ElementC, typename Gemm::LayoutC, 
+        ElementCompute,
+        ElementAccumulator, typename Gemm::Operator>
+        reference_gemm;
+
+    reference_gemm(
       problem_size,
       alpha, 
-      tensor_A.host_ref(),
-      Gemm::kTransformA,
-      tensor_B.host_ref(),
-      Gemm::kTransformB,
+      tensor_A_uncompressed.host_ref(), 
+      tensor_B.host_ref(), 
       beta, 
-      tensor_C_ref.host_ref(), 
-      reference_d_Y.host_ref(), 
+      reference_D.host_ref(),
       ElementAccumulator(0)
     );
 
-    using ElementC = typename Gemm::ElementC;
-
-    ReferenceOp reference_op;
-
-    // compute backwards 
-    for (int m = 0; m < problem_size.m(); ++m) {
-      ElementAccumulator reduced_value = ElementAccumulator();
-      for (int n = 0; n < problem_size.n(); ++n) {
-        ElementAccumulator d_full = reference_op(reference_d_Y.at({m, n}), tensor_Tensor.at({m, n}));
-        reduced_value += d_full;
-        reference_D.at({m, n}) = ElementC(d_full);
-      }
-      reference_Reduction.at({m, 0}) = reduced_value;
-    }
-
     return compare_reference(problem_size, alpha, beta);
   }
 
   /// Returns true if the CUDA device is sufficient to execute the kernel.
   bool sufficient() const {
-
     //
     // Determine SMEM requirements and waive if not satisfied
     //
 
     int smem_size = int(sizeof(typename Gemm::GemmKernel::SharedStorage));
 
     cudaDeviceProp properties;
@@ -380,19 +328,18 @@
     }
 
     return true;
   }
 
   /// Executes one test
   bool run(
-    cutlass::gemm::GemmUniversalMode mode,
     cutlass::gemm::GemmCoord problem_size, 
-    int batch_count = 1,
-    ElementAccumulator alpha = ElementAccumulator(1), 
-    ElementAccumulator beta = ElementAccumulator(0)) {
+    int split_k_slices = 1,
+    ElementCompute alpha = ElementCompute(1), 
+    ElementCompute beta = ElementCompute(0)) {
 
     // Waive test if insufficient CUDA device
     if (!sufficient()) {
       if (CUTLASS_TEST_UNIT_ENABLE_WARNINGS) {
         std::cerr << "Test waived due to insufficient CUDA device." << std::endl;
       }
       return true;
@@ -401,48 +348,36 @@
     this->initialize(problem_size);
 
     //
     // Initialize the GEMM operator
     //
 
     typename Gemm::Arguments arguments{
-      mode,
       problem_size,
-      batch_count,
+      tensor_A.device_ref(),
+      tensor_B.device_ref(),
+      tensor_C.device_ref(),
+      tensor_D.device_ref(),
+      tensor_E_reordered.device_ref(),
       {alpha, beta},
-      tensor_A.device_data(),
-      tensor_B.device_data(),
-      tensor_C.device_data(),
-      tensor_D.device_data(),
-      tensor_Reduction.device_data(),
-      tensor_Tensor.device_data(),
-      problem_size.m() * problem_size.k(),
-      problem_size.n() * problem_size.k(),
-      problem_size.m() * problem_size.n(),
-      problem_size.m() * problem_size.n(),
-      problem_size.m(),
-      problem_size.m() * problem_size.n(),
-      tensor_A.layout().stride(0),
-      tensor_B.layout().stride(0),
-      tensor_C.layout().stride(0),
-      tensor_D.layout().stride(0),
-      tensor_Reduction.layout().stride(0),
-      tensor_Tensor.layout().stride(0),
+      split_k_slices
     };
 
     Gemm gemm_op;
 
     size_t workspace_size = Gemm::get_workspace_size(arguments);
 
     cutlass::device_memory::allocation<uint8_t> workspace(workspace_size);
 
     cutlass::Status status = gemm_op.initialize(arguments, workspace.get());
 
-
-    EXPECT_TRUE(status == cutlass::Status::kSuccess) << to_string(status);
+		// This failure is likely due to insufficient device capabilities. Waive the test.
+    if (status != cutlass::Status::kSuccess) {
+      return true;
+    }
 
     //
     // Run the GEMM
     //
 
     status = gemm_op();
 
@@ -451,131 +386,96 @@
     //
     // Verify
     //
 
     bool passed = this->verify(problem_size, alpha, beta);
 
     if (!passed) {
-      std::cout << "Failed with batch_count/split_k_slices = " << batch_count << std::endl;
+      std::cout << "Error with split_k_slices = " << split_k_slices << ", alpha: " << alpha << std::endl;
     }
 
-    //
-    // Profile
-    //
+    return passed;
+  }
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-    #if 0 // profiling disabled for now.
+template <typename Gemm>
+bool TestAllSparseGemm() {
+  bool passed = true;
 
-    int const kWorkspaces = 100;
+  int const kMinimumOperandElementSize = 
+    std::min(
+      int(cutlass::sizeof_bits<typename Gemm::ElementA>::value), 
+      int(cutlass::sizeof_bits<typename Gemm::ElementB>::value));
 
-    cutlass::DeviceAllocation<typename Gemm::ElementA> profiling_tensor_A(tensor_A.capacity() * kWorkspaces);
-    cutlass::DeviceAllocation<typename Gemm::ElementB> profiling_tensor_B(tensor_B.capacity() * kWorkspaces);
-    cutlass::DeviceAllocation<typename Gemm::ElementC> profiling_tensor_C(tensor_C.capacity() * kWorkspaces);
-    cutlass::DeviceAllocation<typename Gemm::ElementC> profiling_tensor_D(tensor_D.capacity() * kWorkspaces);
-    cutlass::DeviceAllocation<typename Gemm::ElementC> profiling_tensor_Reduction(tensor_Reduction.capacity() * kWorkspaces);
-    cutlass::DeviceAllocation<ElementT> profiling_tensor_Tensor(tensor_Tensor.capacity() * kWorkspaces);
+  // M dimension has to be multiple of 32 (sparse float) or 16 (sparse int)
+  // because of the reordering of operand E
+  int const kAlignmentM = std::max(((sizeof(typename Gemm::ElementE) == 2) ? 32 : 16),
+                                   kMinimumOperandElementSize);
 
-    cudaEvent_t events[2];
-    for (auto & event : events) {
-      cudaError_t result = cudaEventCreate(&event);
-      if (result != cudaSuccess) {
-        EXPECT_EQ(result, cudaSuccess) << " cudaEventCreate() failed with error " << cudaGetErrorString(result);
-        return false;
-        break;
-      }
-    }
+  int const kAlignmentN = 128 / kMinimumOperandElementSize;
 
-    int const kWarmupIterations = 5;
-    int const kProfilingIterations = 100;
+  int problem_size_m[] = {kAlignmentM, 512 - 3 * kAlignmentM};
 
-    for (int i = 0; i < kWarmupIterations; ++i) {
-      status = gemm_op();
-      EXPECT_TRUE(status == cutlass::Status::kSuccess) << to_string(status);
-    }
-    
-
-    cudaError_t result = cudaEventRecord(events[0]);
-    EXPECT_EQ(result, cudaSuccess);
-
-    for (int i = 0; i < kProfilingIterations; ++i) {
-
-      typename Gemm::Arguments arguments{
-        mode,
-        problem_size,
-        batch_count,
-        {alpha, beta},
-        profiling_tensor_A.get() + tensor_A.capacity() * (i % kWorkspaces),
-        profiling_tensor_B.get() + tensor_B.capacity() * (i % kWorkspaces),
-        profiling_tensor_C.get() + tensor_C.capacity() * (i % kWorkspaces),
-        profiling_tensor_D.get() + tensor_D.capacity() * (i % kWorkspaces),
-        profiling_tensor_Reduction.get() + tensor_Reduction.capacity() * (i % kWorkspaces),
-        profiling_tensor_Tensor.get() + tensor_Tensor.capacity() * (i % kWorkspaces),
-        problem_size.m() * problem_size.k(),
-        problem_size.n() * problem_size.k(),
-        problem_size.m() * problem_size.n(),
-        problem_size.m() * problem_size.n(),
-        problem_size.m(),
-        problem_size.m() * problem_size.n(),
-        tensor_A.layout().stride(0),
-        tensor_B.layout().stride(0),
-        tensor_C.layout().stride(0),
-        tensor_D.layout().stride(0),
-        tensor_Reduction.layout().stride(0),
-        tensor_Tensor.layout().stride(0),
-      };
-
-      gemm_op.initialize(arguments, workspace.get());
-      status = gemm_op();
-      EXPECT_TRUE(status == cutlass::Status::kSuccess) << to_string(status);
-    }
-
-    result = cudaEventRecord(events[1]);
-    EXPECT_EQ(result, cudaSuccess);
-
-    result = cudaDeviceSynchronize();
-    EXPECT_EQ(result, cudaSuccess);
-
-    float elapsed_time = 0;
-    result = cudaEventElapsedTime(&elapsed_time, events[0], events[1]);
-    EXPECT_EQ(result, cudaSuccess);
+  int problem_size_n[] = {kAlignmentN, 512 - 2 * kAlignmentN};
 
-    double average_time = double(elapsed_time) / double(kProfilingIterations);
+  int problem_size_k[] = {Gemm::ThreadblockShape::kK,
+                          Gemm::ThreadblockShape::kK * (Gemm::kStages + 1)};
 
-    std::cout << problem_size << ": " << average_time << " ms" << std::endl;
+  int split_k_slices[] = {
+    1, 2, 3
+  };
 
-    for (auto & event : events) {
-      cudaEventDestroy(event);
-    }
-    #endif
+  double problem_alpha[] = {
+    1
+  };
 
-    return passed;
-  }
-};
+  double problem_beta[] = {
+    2.0
+  };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-template <typename Gemm, typename ReferenceOp>
-bool TestGemmWithReduction(
-  cutlass::gemm::GemmCoord const & problem_size,
-  cutlass::gemm::GemmUniversalMode mode,
-  int batch_count = 1,
-  double alpha = 1.0, 
-  double beta = 2.0) {
+  SparseTestbed<Gemm> testbed;
 
-  bool passed = true;
+  using ElementCompute = typename Gemm::EpilogueOutputOp::ElementCompute;
 
-  TestbedGemmWithReduction<Gemm, ReferenceOp> testbed;
-  
-  using ElementAccumulator = typename Gemm::ElementAccumulator;
+  for (int m : problem_size_m) {
+    for (int n : problem_size_n) {
+      for (int k : problem_size_k) {
+        for (int split_k : split_k_slices) {
 
-  passed = testbed.run(
-    mode,
-    problem_size, 
-    batch_count,
-    cutlass::from_real<ElementAccumulator>(alpha), 
-    cutlass::from_real<ElementAccumulator>(beta)
-  );
+          if (!Gemm::kSplitKSerial && split_k > 1) {
+            continue;
+          }
+
+          if (split_k > 1 && k / Gemm::ThreadblockShape::kK < split_k) {
+            continue;
+          }
+
+          for (auto alpha : problem_alpha) {
+            for (auto beta : problem_beta) {
+
+              cutlass::gemm::GemmCoord problem_size(m, n, k);
+
+              passed = testbed.run(
+                problem_size, 
+                split_k,
+                cutlass::from_real<ElementCompute>(alpha), 
+                cutlass::from_real<ElementCompute>(beta)
+              );
+
+              if (!passed) {
+                return false;
+              }
+            }
+          }
+        }
+      }
+    }
+  }
 
   return passed;
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace device
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k_scheduler.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k_scheduler.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_scheduler.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_scheduler.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_interleaved.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_interleaved.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_planar_complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_planar_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_rank2k_universal.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_rank2k_universal.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_rank_k_universal.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_universal.h`

 * *Files 13% similar despite different names*

```diff
@@ -25,85 +25,85 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide Rank 2k update interface
-  
+    \brief Tests for device-wide GEMM interface
 */
 
 #pragma once
 
 #include <iostream>
 #include <fstream>
 #include <sstream>
 
 #include "../../common/cutlass_unit_test.h"
-#include "cutlass/blas3.h"
 
 #include "cutlass/util/host_tensor.h"
 #include "cutlass/util/tensor_view_io.h"
 #include "cutlass/util/distribution.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_norm.h"
-#include "cutlass/util/reference/host/error_metrics.h"
-#include "cutlass/util/reference/host/rank_k_complex.h"
+#include "cutlass/util/reference/host/gemm.h"
+#include "cutlass/util/reference/host/gemm_complex.h"
 
 #include "testbed_utils.h"
 
 namespace test {
 namespace gemm {
 namespace device {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-template <typename RankK>
-struct TestbedRank2KUniversal {
+template <typename Gemm, bool Relu = false>
+struct TestbedUniversal {
 
-  using ElementAccumulator = typename RankK::ElementAccumulator;
-  using ElementCompute = typename RankK::RankKkernel::Epilogue::OutputOp::ElementCompute;
+  using ElementAccumulator = typename Gemm::ElementAccumulator;
+  using ElementCompute = typename Gemm::GemmKernel::Epilogue::OutputOp::ElementCompute;
 
   /// Initialization
   cutlass::Distribution::Kind init_A;
+  cutlass::Distribution::Kind init_B;
   cutlass::Distribution::Kind init_C;
   uint64_t seed;
 
-  cutlass::HostTensor<typename RankK::ElementA, typename RankK::LayoutA> tensor_A;
-  cutlass::HostTensor<typename RankK::ElementC, typename RankK::LayoutC> tensor_C;
-  cutlass::HostTensor<typename RankK::ElementC, typename RankK::LayoutC> tensor_D;
-  cutlass::HostTensor<typename RankK::ElementC, typename RankK::LayoutC> reference_D;
+  cutlass::HostTensor<typename Gemm::ElementA, typename Gemm::LayoutA> tensor_A;
+  cutlass::HostTensor<typename Gemm::ElementB, typename Gemm::LayoutB> tensor_B;
+  cutlass::HostTensor<typename Gemm::ElementC, typename Gemm::LayoutC> tensor_C;
+  cutlass::HostTensor<typename Gemm::ElementC, typename Gemm::LayoutC> tensor_D;
+  cutlass::HostTensor<typename Gemm::ElementC, typename Gemm::LayoutC> reference_D;
 
   //
   // Methods
   //
 
-  TestbedRank2KUniversal(
+  TestbedUniversal(
     cutlass::Distribution::Kind init_A_ = cutlass::Distribution::Uniform,
+    cutlass::Distribution::Kind init_B_ = cutlass::Distribution::Uniform,
     cutlass::Distribution::Kind init_C_ = cutlass::Distribution::Uniform,
     uint64_t seed_ = 2080
   ):
-    init_A(init_A_), init_C(init_C_), seed(seed_) { }
+    init_A(init_A_), init_B(init_B_), init_C(init_C_), seed(seed_) { }
 
   /// Helper to initialize a tensor view
   template <typename Element, typename Layout>
   bool initialize_tensor(
-    cutlass::TensorView<Element, Layout> view, 
+    cutlass::TensorView<Element, Layout> view,
     cutlass::Distribution::Kind dist_kind,
-    uint64_t seed,
-    int mantissa_in_bits) {
+    uint64_t seed) {
 
     if (dist_kind == cutlass::Distribution::Uniform) {
 
       double scope_max, scope_min;
       int bits_input = cutlass::sizeof_bits<Element>::value;
-      int bits_output = cutlass::sizeof_bits<typename RankK::ElementC>::value;
+      int bits_output = cutlass::sizeof_bits<typename Gemm::ElementC>::value;
 
       if (bits_input == 1) {
         scope_max = 2;
         scope_min = 0;
       } else if (bits_input <= 8) {
         scope_max = 2;
         scope_min = -2;
@@ -112,169 +112,173 @@
         scope_min = -5;
       } else {
         scope_max = 8;
         scope_min = -8;
       }
 
       cutlass::reference::host::TensorFillRandomUniform(
-        view, seed, scope_max, scope_min, mantissa_in_bits);
-    } 
+        view, seed, scope_max, scope_min, 0);
+    }
     else if (dist_kind == cutlass::Distribution::Identity) {
 
       cutlass::reference::host::TensorFillIdentity(view);
-    } 
+    }
     else if (dist_kind == cutlass::Distribution::Gaussian) {
 
-      cutlass::reference::host::TensorFillRandomGaussian(view, seed, 0, 0.5, mantissa_in_bits);
+      cutlass::reference::host::TensorFillRandomGaussian(view, seed, 0, 0.5);
     }
     else if (dist_kind == cutlass::Distribution::Sequential) {
 
       cutlass::reference::host::BlockFillSequential(
         view.data(), view.capacity());
-    } 
-    else {
-
-      EXPECT_TRUE(false) << "Input distribution not implemented";
-      return false;
-    }
-
-    return true;
-  }
-
-
-  /// Helper to initialize a tensor view
-  template <typename Element, typename Layout>
-  bool initialize_symmetric_tensor(
-    cutlass::TensorView<Element, Layout> view, 
-    cutlass::Distribution::Kind dist_kind,
-    uint64_t seed,
-    int mantissa_in_bits) {
-
-    if (dist_kind == cutlass::Distribution::Uniform) {
-
-      double scope_max, scope_min;
-      int bits_input = cutlass::sizeof_bits<Element>::value;
-      int bits_output = cutlass::sizeof_bits<typename RankK::ElementC>::value;
-
-      if (bits_input == 1) {
-        scope_max = 2;
-        scope_min = 0;
-      } else if (bits_input <= 8) {
-        scope_max = 2;
-        scope_min = -2;
-      } else if (bits_output == 16) {
-        scope_max = 5;
-        scope_min = -5;
-      } else {
-        scope_max = 8;
-        scope_min = -8;
-      }
-
-      cutlass::reference::host::TensorFillSymmetricRandomUniform(
-        view, seed, RankK::kFillModeC, scope_max, scope_min, mantissa_in_bits);
-    } 
-    else if (dist_kind == cutlass::Distribution::Gaussian) {
-
-      cutlass::reference::host::TensorFillSymmetricRandomGaussian(
-        view, seed, RankK::kFillModeC, 0, 0.5, mantissa_in_bits);
     }
     else {
-
-      EXPECT_TRUE(false) << "Input distribution (symmetric tensor) not implemented";
+      // TODO: Implement the rest
+      EXPECT_TRUE(false) << "Not implemented";
       return false;
     }
 
     return true;
   }
+
   /// Initializes data structures
   void initialize(cutlass::gemm::GemmCoord problem_size) {
     //
-    // Allocate the RankK workspace
+    // Allocate the GEMM workspace
     //
 
     tensor_A.resize(problem_size.mk());
+    tensor_B.resize(problem_size.kn());
     tensor_C.resize(problem_size.mn());
     tensor_D.resize(problem_size.mn());
     reference_D.resize(problem_size.mn(), false);
 
-    EXPECT_TRUE(initialize_tensor(tensor_A.host_view(), init_A, seed + 2019, cutlass::MantissaInBits<typename RankK::ElementA>::bits));
-    EXPECT_TRUE(initialize_symmetric_tensor(tensor_C.host_view(), init_C, seed + 2017, cutlass::MantissaInBits<typename RankK::ElementC>::bits));
+    EXPECT_TRUE(initialize_tensor(tensor_A.host_view(), init_A, seed + 2019));
+    EXPECT_TRUE(initialize_tensor(tensor_B.host_view(), init_B, seed + 2018));
+    EXPECT_TRUE(initialize_tensor(tensor_C.host_view(), init_C, seed + 2017));
 
     // It is possible to randomly initialize to all zeros, so override this with non-zeros
     // in the upper left corner of each operand.
-    tensor_A.host_view().at({0, 0}) = typename RankK::ElementA(1);
-    tensor_C.host_view().at({0, 0}) = typename RankK::ElementC(1);
+    cutlass::Coord<2> origin(0);
+    tensor_A.host_view().at(origin) = typename Gemm::ElementA(1);
+    tensor_B.host_view().at(origin) = typename Gemm::ElementB(1);
+    tensor_C.host_view().at(origin) = typename Gemm::ElementC(1);
 
     cutlass::reference::host::TensorCopy(reference_D.host_view(), tensor_C.host_view());
 
     tensor_A.sync_device();
+    tensor_B.sync_device();
     tensor_C.sync_device();
     tensor_D.sync_device();
   }
 
   /// Compares computed reference with device reference and outputs to a file if incorrect
   bool compare_reference(
     cutlass::gemm::GemmCoord problem_size,
-    ElementCompute alpha, 
+    ElementCompute alpha,
     ElementCompute beta) {
 
     tensor_D.sync_host();
 
     EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_A.host_view()), 0);
+    EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_B.host_view()), 0);
     EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_C.host_view()), 0);
 
-    if (tensor_D.size() > 1)
-      EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_D.host_view()), 0);
+    EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_D.host_view()), 0);
+    EXPECT_GT(cutlass::reference::host::TensorNorm(reference_D.host_view()), 0);
 
-    if (reference_D.size() > 1)
-      EXPECT_GT(cutlass::reference::host::TensorNorm(reference_D.host_view()), 0);
+    bool passed = cutlass::reference::host::TensorEquals(reference_D.host_view(), tensor_D.host_view());
 
-    double l2_norm = cutlass::reference::host::TensorRelativeErrorMetric(reference_D.host_view(), tensor_D.host_view());
+    EXPECT_TRUE(passed) << " mismatched reference";
 
-    bool passed = l2_norm < cutlass::MantissaInBits<typename RankK::ElementA>::error;
+    if (!passed) {
+
+      /*
+      std::stringstream fname;
+
+      fname << "error_Gemm_device_"
+        << problem_size.m() << "x"
+        << problem_size.n() << "x"
+        << problem_size.k() << "_"
+        << Gemm::ThreadblockShape::kM << "x"
+        << Gemm::ThreadblockShape::kN << "x"
+        << Gemm::ThreadblockShape::kK << "_"
+        << Gemm::WarpShape::kM << "x"
+        << Gemm::WarpShape::kN << "x"
+        << Gemm::WarpShape::kK << ".txt";
+
+      std::ofstream file(fname.str());
+      */
+
+      std::ofstream file("testbed_universal_errors.txt");
+
+      file
+        << "problem: " << problem_size
+        << ", alpha: " << alpha << ", beta: " << beta << "\n\n";
+
+      file
+        << "A =\n" << tensor_A.host_view()
+        << "\nB =\n" << tensor_B.host_view()
+        << "\nC =\n" << tensor_C.host_view()
+        << "\n\nReference =\n" << reference_D.host_view()
+        << "\nComputed =\n" << tensor_D.host_view();
+    }
 
     return passed;
   }
 
-  /// Verifies the result is a RankK
+  /// Verifies the result is a GEMM
   bool verify(
-    cutlass::gemm::GemmCoord problem_size, 
-    ElementCompute alpha, 
+    cutlass::gemm::GemmCoord problem_size,
+    ElementCompute alpha,
     ElementCompute beta) {
 
     //
     // Verify
     //
-    cutlass::reference::host::Rank2KComplex<
-        typename RankK::ElementA, typename RankK::LayoutA,
-        typename RankK::ElementC, typename RankK::LayoutC, 
+
+    cutlass::reference::host::GemmComplex<
+        typename Gemm::ElementA, typename Gemm::LayoutA,
+        typename Gemm::ElementB, typename Gemm::LayoutB,
+        typename Gemm::ElementC, typename Gemm::LayoutC,
         ElementCompute, ElementAccumulator
     >(
       problem_size,
-      alpha, 
+      alpha,
       tensor_A.host_ref(),
-      RankK::kTransformA,
-      beta, 
-      tensor_C.host_ref(), 
+      Gemm::kTransformA,
+      tensor_B.host_ref(),
+      Gemm::kTransformB,
+      beta,
+      tensor_C.host_ref(),
       reference_D.host_ref(),
-      ElementAccumulator(0),
-      RankK::kFillModeC,
-      RankK::kBlasMode
+      ElementAccumulator(0)
     );
 
+    if (Relu) {
+      for (int i = 0; i < problem_size.m(); ++i) {
+        for (int j = 0; j < problem_size.n(); ++j) {
+           reference_D.at(cutlass::MatrixCoord(i, j)) =
+                  ((ElementCompute)reference_D.at(cutlass::MatrixCoord(i, j)) < (ElementCompute)0)
+                  ? (typename Gemm::ElementC)0
+                  : reference_D.at(cutlass::MatrixCoord(i, j));
+        }
+      }
+    }
+
     return compare_reference(problem_size, alpha, beta);
   }
 
   /// Returns true if the CUDA device is sufficient to execute the kernel.
   bool sufficient() const {
     //
     // Determine SMEM requirements and waive if not satisfied
     //
 
-    int smem_size = int(sizeof(typename RankK::RankKkernel::SharedStorage));
+    int smem_size = int(sizeof(typename Gemm::GemmKernel::SharedStorage));
 
     cudaDeviceProp properties;
     int device_idx;
     cudaError_t result = cudaGetDevice(&device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDevice() API call failed.");
@@ -294,216 +298,247 @@
   }
 
   /// Executes one test
   bool run(
     cutlass::gemm::GemmUniversalMode mode,
     cutlass::gemm::GemmCoord problem_size,
     int batch_count = 1,
-    ElementCompute alpha = ElementCompute(1), 
-    ElementCompute beta = ElementCompute(0)) {
+    ElementCompute alpha = ElementCompute(1),
+    ElementCompute beta = ElementCompute(0))
+  {
+/*
+    std::cout << "\n-----------------------\n";
+    std::cout << "mode: " << (int) mode << "\n";
+    std::cout << "problem size: " << problem_size << "\n";
+    std::cout << "batch_count: " << batch_count << "\n";
+    std::cout << "alpha: " << alpha << "\n";
+    std::cout << "beta: " << beta << "\n";
+    std::cout << "-----------------------\n\n";
+*/
 
     // Waive test if insufficient CUDA device
     if (!sufficient()) {
       if (CUTLASS_TEST_UNIT_ENABLE_WARNINGS) {
         std::cerr << "Test waived due to insufficient CUDA device." << std::endl;
       }
       return true;
     }
 
-#if 0
-    std::cout << "[TestbedRankKUniversal::run()] problem(m, n, k): " << problem_size
-              << " alpha: " << ElementCompute(alpha)
-              << " beta: " << ElementCompute(beta) << std::endl;
-#endif
-
     this->initialize(problem_size);
 
     //
-    // Initialize the RankK operator
+    // Initialize the GEMM operator
     //
 
-    typename RankK::Arguments arguments{
+    typename Gemm::Arguments arguments{
       mode,
       problem_size,
       batch_count,
       {alpha, beta},
       tensor_A.device_data(),
+      tensor_B.device_data(),
       tensor_C.device_data(),
       tensor_D.device_data(),
+      problem_size.m() * problem_size.k(),
       problem_size.n() * problem_size.k(),
       problem_size.m() * problem_size.n(),
       problem_size.m() * problem_size.n(),
       tensor_A.layout().stride(0),
+      tensor_B.layout().stride(0),
       tensor_C.layout().stride(0),
       tensor_D.layout().stride(0)
     };
 
-    RankK rank2k_op;
+    Gemm gemm_op;
 
-    size_t workspace_size = RankK::get_workspace_size(arguments);
+    size_t workspace_size = Gemm::get_workspace_size(arguments);
 
     cutlass::device_memory::allocation<uint8_t> workspace(workspace_size);
 
-    cutlass::Status status = rank2k_op.initialize(arguments, workspace.get());
+    cutlass::Status status = gemm_op.initialize(arguments, workspace.get());
 
     EXPECT_TRUE(status == cutlass::Status::kSuccess) << to_string(status);
 
     //
-    // Run the RankK
+    // Run the GEMM
     //
 
-    status = rank2k_op();
+    status = gemm_op();
 
     EXPECT_TRUE(status == cutlass::Status::kSuccess) << to_string(status);
 
     //
     // Verify
     //
 
     bool passed = this->verify(problem_size, alpha, beta);
 
-    //if (true) {
     if (!passed) {
-      std::stringstream fname;
-
-      fname << "error_RankK_device_"
-            << "fill_mode_c_"
-            << (RankK::kFillModeC == cutlass::FillMode::kLower ? "lower_" :
-                (RankK::kFillModeC == cutlass::FillMode::kUpper ? "upper_" : "invalid_"))
-            << "mnk_"
-            << problem_size.m() << "x"
-            << problem_size.n() << "x"
-            << problem_size.k() << "_"
-            << RankK::ThreadblockShape::kM << "x"  
-            << RankK::ThreadblockShape::kN << "x"  
-            << RankK::ThreadblockShape::kK << "_"
-            << RankK::WarpShape::kM << "x"  
-            << RankK::WarpShape::kN << "x"  
-            << RankK::WarpShape::kK << ".txt";
-
-      std::cout << fname.str() << std::endl;
-
-      std::ofstream results(fname.str());
-
-      results << problem_size << std::endl;
-
-      results
-        << "\nA:\n" << tensor_A.host_view() << "\n"
-        << "\nC:\n" << tensor_C.host_view() << "\n"
-        << "\nD reference:\n" << reference_D.host_view() << "\n"
-        << "\nD computed:\n" << tensor_D.host_view() << "\n";
-
+      std::cout << "Failed with batch_count/split_k_slices = " << batch_count << std::endl;
     }
 
     return passed;
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-template <typename RankK>
-bool TestRank2kUniversal(
+template <typename Gemm, bool Relu = false>
+bool TestGemmUniversal(
   cutlass::gemm::GemmCoord const & problem_size,
   cutlass::gemm::GemmUniversalMode mode,
   int batch_count,
-  double alpha = 1.0, 
+  double alpha = 1.0,
   double beta = 2.0) {
 
   bool passed = true;
 
-  TestbedRank2KUniversal<RankK> testbed;
-  
-  using ElementCompute = typename RankK::EpilogueOutputOp::ElementCompute;
+  TestbedUniversal<Gemm, Relu> testbed;
+
+  using ElementCompute = typename Gemm::EpilogueOutputOp::ElementCompute;
 
   passed = testbed.run(
     mode,
     problem_size,
     batch_count,
-    cutlass::from_real<ElementCompute>(alpha), 
+    cutlass::from_real<ElementCompute>(alpha),
     cutlass::from_real<ElementCompute>(beta)
   );
 
   return passed;
 }
 
-template <typename RankK>
-bool TestAllRankKUniversal() {
+template <typename Gemm, bool Relu = false>
+bool TestAllGemmUniversal() {
   bool passed = true;
 
 
-  int const kMinimumOperandElementSize = int(cutlass::sizeof_bits<typename RankK::ElementA>::value);
-  int const kAlignmentN = 128 / kMinimumOperandElementSize;
-  int const kAlignmentK = 128 / kMinimumOperandElementSize;
+  int const kMinimumOperandElementSize = 
+    std::min(
+      int(cutlass::sizeof_bits<typename Gemm::ElementA>::value), 
+      int(cutlass::sizeof_bits<typename Gemm::ElementB>::value));
+
+  int const kAlignment = cutlass::platform::is_same<
+                              typename Gemm::OperatorClass, 
+                              cutlass::arch::OpClassSimt>::value ? 1 : 128 / kMinimumOperandElementSize;
+
+  // int8_t gemm alignment constraints
+  int const kAlignmentM = cutlass::platform::is_same<typename Gemm::OperatorClass, cutlass::arch::OpClassSimt>::value &&
+                          cutlass::platform::is_same<typename Gemm::ElementA, int8_t>::value &&
+                          cutlass::platform::is_same<typename Gemm::LayoutA, cutlass::layout::ColumnMajor>::value ? 4 : kAlignment;
+
+  int const kAlignmentN = cutlass::platform::is_same<typename Gemm::OperatorClass, cutlass::arch::OpClassSimt>::value &&
+                          cutlass::platform::is_same<typename Gemm::ElementB, int8_t>::value &&
+                          cutlass::platform::is_same<typename Gemm::LayoutB, cutlass::layout::RowMajor>::value ? 4 : kAlignment;
+
+  int const kAlignmentK = cutlass::platform::is_same<typename Gemm::OperatorClass, cutlass::arch::OpClassSimt>::value &&
+                          cutlass::platform::is_same<typename Gemm::ElementA, int8_t>::value &&
+                          cutlass::platform::is_same<typename Gemm::ElementB, int8_t>::value &&
+                          (cutlass::platform::is_same<typename Gemm::LayoutA, cutlass::layout::RowMajor>::value ||
+                          cutlass::platform::is_same<typename Gemm::LayoutB, cutlass::layout::ColumnMajor>::value) ? 4 : kAlignment;
+
+
 
   cutlass::gemm::GemmUniversalMode modes[] = {
     cutlass::gemm::GemmUniversalMode::kGemm,
   };
 
+  int problem_size_m[] = {
+    kAlignmentM, 512 - 3*kAlignmentM
+  };
+
   int problem_size_n[] = {
     kAlignmentN, 512 - 2*kAlignmentN
   };
 
   int problem_size_k[] = {
-    kAlignmentK, 
-    RankK::ThreadblockShape::kK * RankK::kStages - kAlignmentK, 
-    RankK::ThreadblockShape::kK * RankK::kStages * 3 - kAlignmentK
+    kAlignmentK,
+    Gemm::ThreadblockShape::kK * Gemm::kStages - kAlignmentK,
+    Gemm::ThreadblockShape::kK * Gemm::kStages * 3 - kAlignmentK
   };
 
   int batch_counts[] = {      // may be interpretted as batch count or split-K slices
-    1                         // Just running one batch for now (removing 2, 3, 5, 7)
+    1, 2, 3, 5, 7
   };
 
   double problem_alpha[] = {
-    1.0
+    1
   };
 
   double problem_beta[] = {
     2.0
   };
 
 
-  using ElementCompute = typename RankK::EpilogueOutputOp::ElementCompute;
+  using ElementCompute = typename Gemm::EpilogueOutputOp::ElementCompute;
 
   for (cutlass::gemm::GemmUniversalMode mode : modes) {
-    for (int n : problem_size_n) {
-      for (int k : problem_size_k) {
-        for (int batch_count : batch_counts) {
-
-          for (auto alpha : problem_alpha) {
-            for (auto beta : problem_beta) {
-
-              if (mode == cutlass::gemm::GemmUniversalMode::kGemm ||
-                mode == cutlass::gemm::GemmUniversalMode::kGemmSplitKParallel) {
-              }
-
-              cutlass::gemm::GemmCoord problem_size(n, n, k);
-
-              TestbedRank2KUniversal<RankK> testbed;
-
-              passed = testbed.run(
-                mode,
-                problem_size,
-                batch_count,
-                cutlass::from_real<ElementCompute>(alpha), 
-                cutlass::from_real<ElementCompute>(beta)
-              );
-
-              if (!passed) {
-                return false;
+    for (int m : problem_size_m) {
+      for (int n : problem_size_n) {
+        for (int k : problem_size_k) {
+          for (int batch_count : batch_counts) {
+
+            for (auto alpha : problem_alpha) {
+              for (auto beta : problem_beta) {
+
+                if (mode == cutlass::gemm::GemmUniversalMode::kGemm ||
+                  mode == cutlass::gemm::GemmUniversalMode::kGemmSplitKParallel) {
+
+                  // skip very small K problems
+                  if (k / batch_count < 2 * Gemm::ThreadblockShape::kK) {
+                    continue;
+                  }
+                }
+
+                cutlass::gemm::GemmCoord problem_size(m, n, k);
+
+                TestbedUniversal<Gemm, Relu> testbed;
+
+                passed = testbed.run(
+                  mode,
+                  problem_size,
+                  batch_count,
+                  cutlass::from_real<ElementCompute>(alpha),
+                  cutlass::from_real<ElementCompute>(beta)
+                );
+
+                if (!passed) {
+                  return false;
+                }
               }
             }
           }
         }
       }
     }
   }
 
+  /*
+  // large problem with high coverage
+  for (int split_k_slices = 1; split_k_slices <= 3; ++split_k_slices) {
+    TestbedUniversal<Gemm> testbed;
+
+    cutlass::gemm::GemmCoord problem_size(72, 56, 8192);
+
+    passed = testbed.run(
+      cutlass::gemm::GemmUniversalMode::kGemm,
+      problem_size,
+      split_k_slices,
+      cutlass::from_real<ElementCompute>(1.0),
+      cutlass::from_real<ElementCompute>(2.0)
+    );
+
+    if (!passed) {
+      break;
+    }
+  }
+  */
+
   return passed;
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace device
 } // namespace gemm
 } // namespace test
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_sanity.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_sanity.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_sparse.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed_slicedk.h`

 * *Files 25% similar despite different names*

```diff
@@ -24,462 +24,366 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/*! \file
-    \brief Tests for device-wide GEMM interface
 
-  Testbed for sparse operations not to be released for CUDA 11.0 GA. Expected release is 11.1.
+/*! \file
+    \brief Unit testbed for kernel-level GEMM
 */
 
 #pragma once
 
-#include <iostream>
 #include <fstream>
-#include <sstream>
 
 #include "../../common/cutlass_unit_test.h"
 
+#include "cutlass/aligned_buffer.h"
+#include "cutlass/gemm/gemm.h"
+#include "cutlass/layout/matrix.h"
+#include "cutlass/layout/vector.h"
+#include "cutlass/numeric_types.h"
+
+#include "cutlass/core_io.h"
 #include "cutlass/util/host_tensor.h"
 #include "cutlass/util/tensor_view_io.h"
+
 #include "cutlass/util/distribution.h"
-#include "cutlass/util/reference/host/tensor_fill.h"
-#include "cutlass/util/reference/host/tensor_copy.h"
-#include "cutlass/util/reference/host/tensor_compare.h"
-#include "cutlass/util/reference/host/tensor_norm.h"
 #include "cutlass/util/reference/host/gemm.h"
-#include "cutlass/util/host_reorder.h"
-#include "cutlass/util/host_uncompress.h"
+#include "cutlass/util/reference/host/tensor_compare.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
 
-#include "testbed_utils.h"
+#include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
+#include "cutlass/transform/threadblock/predicated_tile_access_iterator.h"
+#include "cutlass/cutlass.h"
+#include "cutlass/platform/platform.h"
 
 namespace test {
 namespace gemm {
-namespace device {
+namespace threadblock {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-template <typename Gemm>
-struct SparseTestbed {
+template <typename Mma>
+__global__ void kernel_multistage_mma(cutlass::gemm::GemmCoord problem_size,
+                           typename Mma::IteratorA::Params params_A,
+                           typename Mma::IteratorA::TensorRef ref_A,
+                           typename Mma::IteratorB::Params params_B,
+                           typename Mma::IteratorB::TensorRef ref_B,
+                           typename Mma::ElementC **ptr_C,
+                           typename Mma::LayoutC::Stride::Index ldc) {
+  // Shared storage needed by threadblock-scoped matrix multiply-accumulate
 
-  using ElementAccumulator = typename Gemm::ElementAccumulator;
-  using ElementCompute = typename Gemm::GemmKernel::Epilogue::OutputOp::ElementCompute;
+  // Dynamic shared memory base pointer
+  extern __shared__ int GemmSharedStorageBase[];
 
-  static int const kSparse = Gemm::GemmKernel::kSparse;
-  static int const kMetaSizeInBits = Gemm::GemmKernel::kMetaSizeInBits;
-  static int const kMaxID2 = Gemm::GemmKernel::kMaxID2;
-  static int const kElementsPerElementE = Gemm::GemmKernel::kElementsPerElementE;
-
-  using ElementE = typename Gemm::GemmKernel::ElementE;
-  using LayoutE = cutlass::layout::RowMajor;
-  using ReorderedLayoutE = typename Gemm::GemmKernel::LayoutE;
-
-  /// Initialization
-  cutlass::Distribution::Kind init_A;
-  cutlass::Distribution::Kind init_B;
-  cutlass::Distribution::Kind init_C;
-  cutlass::Distribution::Kind init_E;
-  uint64_t seed;
-
-  cutlass::HostTensor<typename Gemm::ElementA, typename Gemm::LayoutA> tensor_A;
-  cutlass::HostTensor<typename Gemm::ElementA, typename Gemm::LayoutA> tensor_A_uncompressed;
-  cutlass::HostTensor<typename Gemm::ElementB, typename Gemm::LayoutB> tensor_B;
-  cutlass::HostTensor<typename Gemm::ElementC, typename Gemm::LayoutC> tensor_C;
-  cutlass::HostTensor<typename Gemm::ElementC, typename Gemm::LayoutC> tensor_D;
-  cutlass::HostTensor<typename Gemm::ElementC, typename Gemm::LayoutC> reference_D;
-  cutlass::HostTensor<ElementE, LayoutE> tensor_E;
-  cutlass::HostTensor<ElementE, ReorderedLayoutE> tensor_E_reordered;
+  // Declare pointer to dynamic shared memory.
+  typename Mma::SharedStorage *shared_storage =
+      reinterpret_cast<typename Mma::SharedStorage *>(GemmSharedStorageBase);
 
-  //
-  // Methods
-  //
+  // Compute threadblock location
+  cutlass::gemm::GemmCoord tb_tile_offset = {int(blockIdx.x), int(blockIdx.y),
+                                             0};
 
-  SparseTestbed(
-      cutlass::Distribution::Kind init_A_ = cutlass::Distribution::Uniform,
-      cutlass::Distribution::Kind init_B_ = cutlass::Distribution::Uniform,
-      cutlass::Distribution::Kind init_C_ = cutlass::Distribution::Uniform,
-      cutlass::Distribution::Kind init_E_ = cutlass::Distribution::Uniform,
-      uint64_t seed_ = 2080)
-      : init_A(init_A_),
-        init_B(init_B_),
-        init_C(init_C_),
-        init_E(init_E_),
-        seed(seed_) {}
-
-  /// Helper to initialize a tensor view
-  template <typename Element, typename Layout>
-  bool initialize_tensor(
-    cutlass::TensorView<Element, Layout> view, 
-    cutlass::Distribution::Kind dist_kind,
-    uint64_t seed) {
-
-    if (dist_kind == cutlass::Distribution::Uniform) {
-
-      double scope_max, scope_min;
-      int bits_input = cutlass::sizeof_bits<Element>::value;
-      int bits_output = cutlass::sizeof_bits<typename Gemm::ElementC>::value;
+  cutlass::MatrixCoord tb_offset_A{tb_tile_offset.m() * Mma::Shape::kM,
+                                   tb_tile_offset.k()};
 
-      if (bits_input == 1) {
-        scope_max = 2;
-        scope_min = 0;
-      } else if (bits_input <= 8) {
-        scope_max = 2;
-        scope_min = -2;
-      } else if (bits_output == 16) {
-        scope_max = 5;
-        scope_min = -5;
-      } else {
-        scope_max = 8;
-        scope_min = -8;
-      }
+  cutlass::MatrixCoord tb_offset_B{tb_tile_offset.k(),
+                                   tb_tile_offset.n() * Mma::Shape::kN};
 
-      cutlass::reference::host::TensorFillRandomUniform(
-        view, seed, scope_max, scope_min, 0);
-    } 
-    else if (dist_kind == cutlass::Distribution::Identity) {
-
-      cutlass::reference::host::TensorFillIdentity(view);
-    } 
-    else if (dist_kind == cutlass::Distribution::Gaussian) {
+  // Compute position within threadblock
+  int tb_thread_id = threadIdx.y * blockDim.x + threadIdx.x;
 
-      cutlass::reference::host::TensorFillRandomGaussian(view, seed, 0, 0.5);
-    }
-    else if (dist_kind == cutlass::Distribution::Sequential) {
+  // Construct iterators to A and B operands
+  typename Mma::IteratorA iterator_A(params_A, ref_A.data(),
+                                     {problem_size.m(), problem_size.k()},
+                                     tb_thread_id, tb_offset_A);
 
-      cutlass::reference::host::BlockFillSequential(
-        view.data(), view.capacity());
-    } 
-    else {
-      // TODO: Implement the rest
-      EXPECT_TRUE(false) << "Not implemented";
-      return false;
-    }
+  typename Mma::IteratorB iterator_B(params_B, ref_B.data(),
+                                     {problem_size.k(), problem_size.n()},
+                                     tb_thread_id, tb_offset_B);
 
-    return true;
-  }
+  int warp_id = __shfl_sync(0xffffffff, threadIdx.y, 0);
+  int lane_id = threadIdx.x;
 
-  /// Initializes data structures
-  void initialize(cutlass::gemm::GemmCoord problem_size) {
-    //
-    // Allocate the GEMM workspace
-    //
-    tensor_A.resize(cutlass::make_Coord(problem_size.m(), problem_size.k() / kSparse));
-    tensor_A_uncompressed.resize(problem_size.mk());
-    tensor_B.resize(problem_size.kn());
-    tensor_C.resize(problem_size.mn());
-    tensor_D.resize(problem_size.mn());
-    reference_D.resize(problem_size.mn(), false);
-    tensor_E.resize(cutlass::make_Coord(
-        problem_size.m(), problem_size.k() / kSparse / kElementsPerElementE));
-    tensor_E_reordered.resize(cutlass::make_Coord(
-        problem_size.m(), problem_size.k() / kSparse / kElementsPerElementE));
-
-    EXPECT_TRUE(initialize_tensor(tensor_A.host_view(), init_A, seed + 2019));
-    EXPECT_TRUE(initialize_tensor(tensor_B.host_view(), init_B, seed + 2018));
-    EXPECT_TRUE(initialize_tensor(tensor_C.host_view(), init_C, seed + 2017));
+  int partitionsK_idx = warp_id / (Mma::WarpCount::kM * Mma::WarpCount::kN);
 
-    if (init_E == cutlass::Distribution::Uniform) {
-      uint64_t seed = 7;
-      cutlass::reference::host::TensorFillRandomSparseMeta(
-          tensor_E.host_view(), seed, kMetaSizeInBits);
-    } else if (init_E == cutlass::Distribution::Identity) {
-      uint32_t content = (kMaxID2 == 1) ? 0x44444444 : 0x4444;
-      cutlass::reference::host::TensorFill(tensor_E.host_view(),
-                                           (ElementE)(content));
-    } else {
-      // TODO: Implement the rest
-      EXPECT_TRUE(false);
-    }
+  // Construct thread-scoped matrix multiply
+  Mma mma(*shared_storage, tb_thread_id, warp_id, threadIdx.x);
 
-    cutlass::reorder_meta(tensor_E_reordered.host_ref(), tensor_E.host_ref(),
-                          {problem_size.m(), problem_size.n(),
-                           problem_size.k() / kSparse / kElementsPerElementE});
-
-    // It is possible to randomly initialize to all zeros, so override this with non-zeros
-    // in the upper left corner of each operand.
-    tensor_A.host_view().at({0, 0}) = typename Gemm::ElementA(1);
-    tensor_B.host_view().at({0, 0}) = typename Gemm::ElementB(1);
-    tensor_C.host_view().at({0, 0}) = typename Gemm::ElementC(1);
-
-    cutlass::reference::host::TensorCopy(reference_D.host_view(), tensor_C.host_view());
-
-    tensor_A.sync_device();
-    tensor_B.sync_device();
-    tensor_C.sync_device();
-    tensor_D.sync_device();
-    tensor_E_reordered.sync_device();
-  }
+  typename Mma::FragmentC accum;
 
-  /// Compares computed reference with device reference and outputs to a file if incorrect
-  bool compare_reference(
-    cutlass::gemm::GemmCoord problem_size, 
-    ElementCompute alpha, 
-    ElementCompute beta) {
+  accum.clear();
 
-    tensor_D.sync_host();
+  int gemm_k_iterations = (problem_size.k() + Mma::Shape::kK - 1) / Mma::Shape::kK;
 
-    EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_A.host_view()), 0);
-    EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_B.host_view()), 0);
-    EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_C.host_view()), 0);
+  // Compute threadblock-scoped matrix multiply-add
+  mma(gemm_k_iterations, accum, iterator_A, iterator_B, accum);
 
-    if (tensor_D.size() > 1)
-      EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_D.host_view()), 0);
+  // Output results
+  typename Mma::Operator::IteratorC iterator_C({ptr_C[partitionsK_idx], ldc}, lane_id);
 
-    if (reference_D.size() > 1)
-      EXPECT_GT(cutlass::reference::host::TensorNorm(reference_D.host_view()), 0);
+  int warp_idx_mn = warp_id % (Mma::WarpCount::kM * Mma::WarpCount::kN);
+  iterator_C.add_tile_offset(
+      {(tb_tile_offset.m() * Mma::WarpCount::kM) +
+           (warp_idx_mn % Mma::WarpCount::kM),
+       (tb_tile_offset.n() * Mma::WarpCount::kN) +
+           (warp_idx_mn / Mma::WarpCount::kM)});
 
-    bool passed = cutlass::reference::host::TensorEquals(reference_D.host_view(), tensor_D.host_view());
-
-    EXPECT_TRUE(passed);
-
-    if (!passed) {
+  iterator_C.store(accum);
+}
 
-      std::stringstream fname;
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-      fname << "error_Gemm_device_" 
-        << problem_size.m() << "x"
-        << problem_size.n() << "x"
-        << problem_size.k() << "_"
-        << Gemm::ThreadblockShape::kM << "x"  
-        << Gemm::ThreadblockShape::kN << "x"  
-        << Gemm::ThreadblockShape::kK << "_"
-        << Gemm::WarpShape::kM << "x"  
-        << Gemm::WarpShape::kN << "x"  
-        << Gemm::WarpShape::kK << ".txt";
-
-      std::ofstream file(fname.str());
-
-      file
-        << "problem: " << problem_size 
-        << ", alpha: " << alpha << ", beta: " << beta << "\n\n";
-
-      file 
-        << "A =\n" << tensor_A.host_view()
-        << "\nB =\n" << tensor_B.host_view()
-        << "\nC =\n" << tensor_C.host_view()
-        << "\nE =\n" << tensor_E.host_view()
-        << "\n\nReference =\n" << reference_D.host_view()
-        << "\nComputed =\n" << tensor_D.host_view();
-    }
+/// Structure to compute the matrix product
+template <
+    /// Threadblock-level matrix multiply-accumulate
+    typename MmaCore_>
+struct Testbed {
+  /// Threadblock-level GEMM implementation
+  using MmaCore = MmaCore_;
+  using ThreadblockShape = typename MmaCore::Shape;
+  using WarpShape = typename MmaCore::WarpShape;
+  using InstructionShape = typename MmaCore::InstructionShape;
+  using ElementA = typename MmaCore::ElementA;
+  using LayoutA = typename MmaCore::LayoutA;
+  using ElementB = typename MmaCore::ElementB;
+  using LayoutB = typename MmaCore::LayoutB;
+  using ElementC = typename MmaCore::ElementC;
+  using LayoutC = typename MmaCore::LayoutC;
+  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
+  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
+  using AccessTypeA = cutlass::Array<ElementA, ThreadMapA::kElementsPerAccess>;
+  using AccessTypeB = cutlass::Array<ElementB, ThreadMapB::kElementsPerAccess>;
+  static int const Stages = MmaCore::kStages;
+  static cutlass::arch::CacheOperation::Kind const CacheOpA =
+      MmaCore::kCacheOpA;
+  static cutlass::arch::CacheOperation::Kind const CacheOpB =
+      MmaCore::kCacheOpB;
+
+  // Define iterators over tiles from the A operand
+  using IteratorA =
+      cutlass::transform::threadblock::PredicatedTileAccessIterator<
+          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
+          ElementA, LayoutA, 1, ThreadMapA, AccessTypeA>;
+
+  // Define iterators over tiles from the B operand
+  using IteratorB =
+      cutlass::transform::threadblock::PredicatedTileAccessIterator<
+          cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
+          ElementB, LayoutB, 0, ThreadMapB, AccessTypeB>;
+
+  // Define the threadblock-scoped pipelined matrix multiply
+  using Mma = cutlass::gemm::threadblock::MmaMultistage<
+      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA, CacheOpA,
+      IteratorB, typename MmaCore::SmemIteratorB, CacheOpB, ElementC, LayoutC,
+      typename MmaCore::MmaPolicy, Stages>;
 
-    return passed;
-  }
+  static int const kPartitionsK = MmaCore::MmaPolicy::kPartitionsK; 
 
-  /// Verifies the result is a GEMM
-  bool verify(
-    cutlass::gemm::GemmCoord problem_size, 
-    ElementCompute alpha, 
-    ElementCompute beta) {
+  //
+  // Data members
+  //
 
-    //
-    // Verify
-    //
+  cutlass::HostTensor<ElementA, LayoutA> matrix_A;
+  cutlass::HostTensor<ElementB, LayoutB> matrix_B;
+  cutlass::HostTensor<ElementC, LayoutC> matrix_C_computed[kPartitionsK];
+  cutlass::HostTensor<ElementC, LayoutC> matrix_C_reference;
+  cutlass::HostTensor<ElementC*, cutlass::layout::PackedVectorLayout> matrix_C_pointers;
 
-    cutlass::uncompress(tensor_A_uncompressed.host_ref(), tensor_A.host_ref(),
-                        tensor_E.host_ref(), problem_size.m(), problem_size.k());
+  cutlass::gemm::GemmCoord problem_size;
+  float alpha, beta;
 
-    cutlass::reference::host::Gemm<
-        typename Gemm::ElementA, typename Gemm::LayoutA,
-        typename Gemm::ElementB, typename Gemm::LayoutB,
-        typename Gemm::ElementC, typename Gemm::LayoutC, 
-        ElementCompute,
-        ElementAccumulator, typename Gemm::Operator>
-        reference_gemm;
+  //
+  // Methods
+  //
 
-    reference_gemm(
-      problem_size,
-      alpha, 
-      tensor_A_uncompressed.host_ref(), 
-      tensor_B.host_ref(), 
-      beta, 
-      reference_D.host_ref(),
-      ElementAccumulator(0)
-    );
+  /// Allocates workspace in device memory
+  Testbed(int m, int n, int k, float alpha_ = float(1), float beta_ = float(0))
+      : problem_size(m, n, k), alpha(alpha_), beta(beta_) {
+    matrix_A.reset(cutlass::make_Coord(m, k));
+    matrix_B.reset(cutlass::make_Coord(k, n));
+
+    CUTLASS_PRAGMA_UNROLL
+    for(int k = 0; k < kPartitionsK; k++)
+      matrix_C_computed[k].reset(cutlass::make_Coord(m, n));
 
-    return compare_reference(problem_size, alpha, beta);
+    matrix_C_reference.reset(cutlass::make_Coord(m, n), false);
+    matrix_C_pointers.reset(cutlass::Coord<1>(kPartitionsK));
   }
 
-  /// Returns true if the CUDA device is sufficient to execute the kernel.
-  bool sufficient() const {
+  /// Runs the test
+  bool run(
+      dim3 grid, dim3 block,
+      cutlass::Distribution::Kind init_A = cutlass::Distribution::Uniform,
+      cutlass::Distribution::Kind init_B = cutlass::Distribution::Uniform) {
     //
-    // Determine SMEM requirements and waive if not satisfied
+    // initialize device memory
     //
 
-    int smem_size = int(sizeof(typename Gemm::GemmKernel::SharedStorage));
-
-    cudaDeviceProp properties;
-    int device_idx;
-    cudaError_t result = cudaGetDevice(&device_idx);
-
-    if (result != cudaSuccess) {
-      throw std::runtime_error("cudaGetDevice() API call failed.");
-    }
+    if (init_A == cutlass::Distribution::Uniform) {
 
-    result = cudaGetDeviceProperties(&properties, device_idx);
+      int scope_max = 8;
+      int scope_min = -8;
 
-    if (result != cudaSuccess) {
-      throw std::runtime_error("cudaGetDeviceProperties() failed");
-    }
+      if (cutlass::sizeof_bits<ElementA>::value == 4) {
+        scope_max = 2;
+        scope_min = -2;
+      } else if (cutlass::sizeof_bits<ElementA>::value == 1) {
+        scope_max = 2;
+        scope_min = 0;
+      }
 
-    if (properties.sharedMemPerBlockOptin < smem_size) {
+      uint64_t seed = 7;
+      cutlass::reference::host::TensorFillRandomUniform(
+          matrix_A.host_view(), seed, scope_max, scope_min, 0);
+    } else if (init_A == cutlass::Distribution::Sequential) {
+      cutlass::reference::host::BlockFillSequential(matrix_A.host_data(),
+                                                    matrix_A.capacity());
+    } else if (init_A == cutlass::Distribution::Identity) {
+      cutlass::reference::host::TensorFillIdentity(matrix_A.host_view());
+    } else {
+      // TODO: Implement the rest
       return false;
     }
 
-    return true;
-  }
-
-  /// Executes one test
-  bool run(
-    cutlass::gemm::GemmCoord problem_size, 
-    int split_k_slices = 1,
-    ElementCompute alpha = ElementCompute(1), 
-    ElementCompute beta = ElementCompute(0)) {
-
-    // Waive test if insufficient CUDA device
-    if (!sufficient()) {
-      if (CUTLASS_TEST_UNIT_ENABLE_WARNINGS) {
-        std::cerr << "Test waived due to insufficient CUDA device." << std::endl;
-      }
-      return true;
-    }
+    if (init_B == cutlass::Distribution::Uniform) {
 
-    this->initialize(problem_size);
+      int scope_max = 8;
+      int scope_min = -8;
 
-    //
-    // Initialize the GEMM operator
-    //
+      if (cutlass::sizeof_bits<ElementB>::value == 4) {
+        scope_max = 2;
+        scope_min = -2;
+      } else if (cutlass::sizeof_bits<ElementB>::value == 1) {
+        scope_max = 2;
+        scope_min = 0;
+      }
 
-    typename Gemm::Arguments arguments{
-      problem_size,
-      tensor_A.device_ref(),
-      tensor_B.device_ref(),
-      tensor_C.device_ref(),
-      tensor_D.device_ref(),
-      tensor_E_reordered.device_ref(),
-      {alpha, beta},
-      split_k_slices
-    };
-
-    Gemm gemm_op;
-
-    size_t workspace_size = Gemm::get_workspace_size(arguments);
-
-    cutlass::device_memory::allocation<uint8_t> workspace(workspace_size);
-
-    cutlass::Status status = gemm_op.initialize(arguments, workspace.get());
-
-		// This failure is likely due to insufficient device capabilities. Waive the test.
-    if (status != cutlass::Status::kSuccess) {
-      return true;
+      uint64_t seed = 7;
+      cutlass::reference::host::TensorFillRandomUniform(
+          matrix_B.host_view(), seed + 16, scope_max, scope_min, 0);
+    } else if (init_B == cutlass::Distribution::Sequential) {
+      cutlass::reference::host::BlockFillSequential(matrix_B.host_data(),
+                                                    matrix_B.capacity());
+    } else if (init_B == cutlass::Distribution::Identity) {
+      cutlass::reference::host::TensorFillIdentity(matrix_B.host_view());
+    } else {
+      // TODO: Implement the rest
+      return false;
     }
 
-    //
-    // Run the GEMM
-    //
-
-    status = gemm_op();
-
-    EXPECT_TRUE(status == cutlass::Status::kSuccess) << to_string(status);
-
-    //
-    // Verify
-    //
-
-    bool passed = this->verify(problem_size, alpha, beta);
+    CUTLASS_PRAGMA_UNROLL
+    for(int k = 0; k < kPartitionsK; k++)
+      cutlass::reference::host::TensorFill(matrix_C_computed[k].host_view());
 
-    if (!passed) {
-      std::cout << "Error with split_k_slices = " << split_k_slices << ", alpha: " << alpha << std::endl;
-    }
+    cutlass::reference::host::TensorFill(matrix_C_reference.host_view());
 
-    return passed;
-  }
-};
+    matrix_A.sync_device();
+    matrix_B.sync_device();
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+    CUTLASS_PRAGMA_UNROLL
+    for(int k = 0; k < kPartitionsK; k++)
+      matrix_C_computed[k].sync_device();
 
-template <typename Gemm>
-bool TestAllSparseGemm() {
-  bool passed = true;
+    typename IteratorA::Params params_A(matrix_A.layout());
+    typename IteratorB::Params params_B(matrix_B.layout());
 
-  int const kMinimumOperandElementSize = 
-    std::min(
-      int(cutlass::sizeof_bits<typename Gemm::ElementA>::value), 
-      int(cutlass::sizeof_bits<typename Gemm::ElementB>::value));
+    CUTLASS_PRAGMA_UNROLL
+    for(int k = 0; k < kPartitionsK; k++)
+      matrix_C_pointers.at(cutlass::Coord<1>(k)) = matrix_C_computed[k].device_data();
 
-  // M dimension has to be multiple of 32 (sparse float) or 16 (sparse int)
-  // because of the reordering of operand E
-  int const kAlignmentM = std::max(((sizeof(typename Gemm::ElementE) == 2) ? 32 : 16),
-                                   kMinimumOperandElementSize);
+    matrix_C_pointers.sync_device();
 
-  int const kAlignmentN = 128 / kMinimumOperandElementSize;
+    cudaError_t result;
 
-  int problem_size_m[] = {kAlignmentM, 512 - 3 * kAlignmentM};
+    int smem_size = int(sizeof(typename Mma::SharedStorage));
+    if (smem_size >= (48 << 10)) {
+      result = cudaFuncSetAttribute(
+          test::gemm::threadblock::kernel_multistage_mma<Mma>,
+          cudaFuncAttributeMaxDynamicSharedMemorySize, smem_size);
 
-  int problem_size_n[] = {kAlignmentN, 512 - 2 * kAlignmentN};
+      EXPECT_EQ(result, cudaSuccess)
+          << " cudaFuncSetAttribute "
+             "cudaFuncAttributeMaxDynamicSharedMemorySize error: "
+          << cudaGetErrorString(result);
 
-  int problem_size_k[] = {Gemm::ThreadblockShape::kK,
-                          Gemm::ThreadblockShape::kK * (Gemm::kStages + 1)};
+      result = cudaFuncSetAttribute(
+          test::gemm::threadblock::kernel_multistage_mma<Mma>,
+          cudaFuncAttributePreferredSharedMemoryCarveout, 100);
 
-  int split_k_slices[] = {
-    1, 2, 3
-  };
+      EXPECT_EQ(result, cudaSuccess)
+          << " cudaFuncSetAttribute "
+             "cudaFuncAttributePreferredSharedMemoryCarveout error: "
+          << cudaGetErrorString(result);
+    }
 
-  double problem_alpha[] = {
-    1
-  };
+    test::gemm::threadblock::kernel_multistage_mma<Mma><<<grid, block, smem_size, 0>>>(
+        problem_size, params_A, matrix_A.device_ref(), params_B,
+        matrix_B.device_ref(), matrix_C_pointers.device_data(),
+        matrix_C_computed[0].layout().stride(0));
 
-  double problem_beta[] = {
-    2.0
-  };
+    //
+    // Check error code
+    //
 
-  SparseTestbed<Gemm> testbed;
+    result = cudaDeviceSynchronize();
+    EXPECT_EQ(result, cudaSuccess)
+        << " kernel error: " << cudaGetErrorString(result);
 
-  using ElementCompute = typename Gemm::EpilogueOutputOp::ElementCompute;
+    CUTLASS_PRAGMA_UNROLL
+    for(int k = 0; k < kPartitionsK; k++)
+      matrix_C_computed[k].sync_host();
 
-  for (int m : problem_size_m) {
-    for (int n : problem_size_n) {
-      for (int k : problem_size_k) {
-        for (int split_k : split_k_slices) {
+    // TODO: this is temporary. it will be removed after slicing can de
+    // reduction
+    //
+    // Reduce matrix_C_computed
+    //
+    CUTLASS_PRAGMA_UNROLL
+    for(int k = 1; k < kPartitionsK; k++) {
+      CUTLASS_PRAGMA_UNROLL
+      for(int m = 0; m < matrix_C_computed[0].extent().row(); m++){
+        CUTLASS_PRAGMA_UNROLL
+        for(int n = 0; n < matrix_C_computed[0].extent().column(); n++){
+          matrix_C_computed[0].at({m, n}) += matrix_C_computed[k].at({m, n});
+        }
+      }
+    }
 
-          if (!Gemm::kSplitKSerial && split_k > 1) {
-            continue;
-          }
+    cutlass::reference::host::Gemm<ElementA, LayoutA, ElementB, LayoutB,
+                                   ElementC, LayoutC, ElementC, ElementC,
+                                   typename MmaCore::Operator>
+        reference_gemm;
 
-          if (split_k > 1 && k / Gemm::ThreadblockShape::kK < split_k) {
-            continue;
-          }
+    reference_gemm(
+        problem_size, ElementC(alpha), matrix_A.host_view(),
+        matrix_B.host_view(), ElementC(beta), matrix_C_reference.host_view());
 
-          for (auto alpha : problem_alpha) {
-            for (auto beta : problem_beta) {
+    bool passed = cutlass::reference::host::TensorEquals(
+        matrix_C_computed[0].host_view(), matrix_C_reference.host_view());
 
-              cutlass::gemm::GemmCoord problem_size(m, n, k);
+    EXPECT_TRUE(passed);
 
-              passed = testbed.run(
-                problem_size, 
-                split_k,
-                cutlass::from_real<ElementCompute>(alpha), 
-                cutlass::from_real<ElementCompute>(beta)
-              );
+    if (!passed) {
+      std::ofstream output("mma_multistage_testbed_errors.txt");
 
-              if (!passed) {
-                return false;
-              }
-            }
-          }
-        }
-      }
+      output
+        << "A:\n" << matrix_A.host_view() << "\n"
+        << "B:\n" << matrix_B.host_view() << "\n"
+        << "Reference:\n"
+        << matrix_C_reference.host_view() << "\n"
+        << "Computed:\n"
+        << matrix_C_computed[0].host_view() << "\n";
     }
-  }
 
-  return passed;
-}
+    return passed;
+  }
+};
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-} // namespace device
-} // namespace gemm
-} // namespace test
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
+}  // namespace threadblock
+}  // namespace gemm
+}  // namespace test
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_splitk.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/thread/gemm.h`

 * *Files 24% similar despite different names*

```diff
@@ -25,194 +25,162 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide GEMM interface
+    \brief Reference implementation for GEMM in host-side code.
 */
 
 #pragma once
 
-#include <iostream>
-#include <sstream>
+#include "cutlass/coord.h"
+#include "cutlass/tensor_view.h"
+#include "cutlass/gemm/gemm.h"
 
-#include "../../common/cutlass_unit_test.h"
+namespace cutlass {
+namespace reference {
+namespace device {
+namespace thread {
 
-#include "testbed.h"
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
-namespace test {
-namespace gemm {
-namespace device {
+/// Thread-level blocked general matrix product.
+//
+// Note, this is a reference implementation. Performance is not expected to approach peak.
+//
+template <
+  typename TensorRefA,
+  typename TensorRefB,
+  typename TensorRefC,
+  typename ScalarType,
+  typename AccumulatorType,
+  typename OutputTile,
+  typename InnerProductOp = multiply_add<AccumulatorType>,
+  typename ConvertOp = NumericConverter<typename TensorRefC::Element, ScalarType>
+>
+struct Gemm {
+
+  using ElementA = typename TensorRefA::Element;
+  using ElementB = typename TensorRefB::Element;
+  using ElementC = typename TensorRefC::Element;
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+  //
+  // Data members
+  //
 
-template <typename Gemm>
-struct TestbedSplitK : public Testbed<Gemm> {
+  /// Tile for A operand
+  ElementA A_tile[OutputTile::kColumn];
 
-  using Base = Testbed<Gemm>;
+  /// Tile for B operand
+  ElementB B_tile[OutputTile::kRow];
 
-  using ElementCompute = typename Base::ElementCompute;
+  /// Tile for Accumulator
+  AccumulatorType accum[OutputTile::kColumn][OutputTile::kRow];
 
   //
   // Methods
   //
 
-  TestbedSplitK(
-    cutlass::Distribution::Kind init_A_ = cutlass::Distribution::Uniform,
-    cutlass::Distribution::Kind init_B_ = cutlass::Distribution::Uniform,
-    cutlass::Distribution::Kind init_C_ = cutlass::Distribution::Uniform,
-    uint64_t seed_ = 2080
-  ):
-    Base(init_A_, init_B_, init_C_, seed_) { }
-
-  /// Returns true if the CUDA device is sufficient to execute the kernel.
-  bool sufficient() const {
-    //
-    // Determine SMEM requirements and waive if not satisfied
-    //
-
-    int smem_size = int(sizeof(typename Gemm::GemmKernel::SharedStorage));
-
-    cudaDeviceProp properties;
-    int device_idx;
-    cudaError_t result = cudaGetDevice(&device_idx);
-
-    if (result != cudaSuccess) {
-      throw std::runtime_error("cudaGetDevice() API call failed.");
+  /// Constructor
+  CUTLASS_HOST_DEVICE
+  Gemm(AccumulatorType initial_accum = AccumulatorType(0)) {
+
+    // Clear fetch registers
+    for (int i = 0; i < OutputTile::kColumn; ++i) {
+      A_tile[i] = ElementA(0);
     }
 
-    result = cudaGetDeviceProperties(&properties, device_idx);
-
-    if (result != cudaSuccess) {
-      throw std::runtime_error("cudaGetDeviceProperties() failed");
-    }
-
-    if (properties.sharedMemPerBlockOptin < smem_size) {
-      return false;
+    for (int j = 0; j < OutputTile::kColumn; ++j) {
+      B_tile[j] = ElementB(0);
     }
 
-    return true;
-  }
-  
-  /// Executes one test
-  bool run(
-    cutlass::gemm::GemmCoord problem_size, 
-    int split_k_slices,
-    ElementCompute alpha = ElementCompute(1), 
-    ElementCompute beta = ElementCompute(0)) {
-
-    // Waive test if insufficient CUDA device
-    if (!sufficient()) {
-      if (CUTLASS_TEST_UNIT_ENABLE_WARNINGS) {
-        std::cerr << "Test waived due to insufficient CUDA device." << std::endl;
+    // Clear accumulators
+    CUTLASS_PRAGMA_UNROLL
+    for (int j = 0; j < OutputTile::kColumn; ++j) {
+      CUTLASS_PRAGMA_UNROLL
+      for (int i = 0; i < OutputTile::kRow; ++i) {
+        accum[j][i] = initial_accum;
       }
-      return true;
     }
+  }
 
-    this->initialize(problem_size);
-
-    //
-    // Initialize the GEMM operator
-    //
-
-    typename Gemm::Arguments arguments{
-      problem_size,
-      this->tensor_A.device_ref(),
-      this->tensor_B.device_ref(),
-      this->tensor_C.device_ref(),
-      this->tensor_D.device_ref(),
-      {alpha, beta},
-      split_k_slices
-    };
-
-    Gemm gemm_op;
-
-    size_t workspace_size = Gemm::get_workspace_size(arguments);
-
-    cutlass::device_memory::allocation<uint8_t> workspace(workspace_size);
-
-    cutlass::Status status = gemm_op.initialize(arguments, workspace.get());
-
-    EXPECT_TRUE(status == cutlass::Status::kSuccess);
-
-    //
-    // Run the GEMM
-    //
-
-    status = gemm_op();
+  /// Computes a matrix product
+  CUTLASS_HOST_DEVICE
+  Gemm & multiply_add(
+    gemm::GemmCoord problem_size,
+    TensorRefA tensor_a,
+    TensorRefB tensor_b,
+    MatrixCoord output_coord = MatrixCoord()) {
+
+    InnerProductOp inner_product_op;
+
+    // Loop over the GEMM K dimension
+    CUTLASS_PRAGMA_NO_UNROLL
+    for (int k = 0; k < problem_size.k(); ++k) {
+
+      // Fetch a slice of the A matrix
+      CUTLASS_PRAGMA_UNROLL
+      for (int i = 0; i < OutputTile::kColumn; ++i) {
+        if (output_coord.row() + i < problem_size.m()) {
+          A_tile[i] = tensor_a.at(make_Coord(output_coord.row() + i, k));
+        }
+      }
 
-    EXPECT_TRUE(status == cutlass::Status::kSuccess);
+      // Fetch a slice of the B matrix
+      CUTLASS_PRAGMA_UNROLL
+      for (int j = 0; j < OutputTile::kRow; ++j) {
+        if (output_coord.column() + j < problem_size.n()) {
+          B_tile[j] = tensor_b.at(make_Coord(k, output_coord.column() + j));
+        }
+      }
 
-    //
-    // Verify
-    //
+      // Compute an accumulated matrix product
+      CUTLASS_PRAGMA_UNROLL
+      for (int j = 0; j < OutputTile::kRow; ++j) {
+        CUTLASS_PRAGMA_UNROLL
+        for (int i = 0; i < OutputTile::kColumn; ++i) {
+          accum[j][i] = inner_product_op(A_tile[i], B_tile[j], accum[j][i]);
+        }
+      }
+    }
 
-    return this->verify(problem_size, alpha, beta);
+    return *this;
   }
-};
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
 
-template <typename Gemm>
-bool TestAllGemmSplitK() {
-  bool passed = true;
-
-  cutlass::gemm::GemmCoord problem_sizes[] = {
-    {8, 8, 2048},
-    {8, 8, 2056},
-    {264, 72, 520},
-    {264, 520,  120},
-    {264, 520,  264}
-  };
-
-  int split_k_slices[] = {
-    1, 2, 4, 5, 7
-  };
-
-  double problem_alpha[] = {
-    0.5
-  };
-
-  double problem_beta[] = {
-    2.0
-  };
-
-  using Testbed = TestbedSplitK<Gemm>;
-  using ElementCompute = typename Testbed::ElementCompute;
-
-  Testbed testbed;
-
-  for (auto problem_size : problem_sizes) {
-    for (int split_k_count : split_k_slices) {
-      for (double alpha : problem_alpha) {
-        for (double beta : problem_beta) {
-
-          passed = testbed.run(
-            problem_size, 
-            split_k_count,
-            ElementCompute(alpha), 
-            ElementCompute(beta)
+  /// Performs linear scaling of matrix product and updates output tensor
+  CUTLASS_HOST_DEVICE
+  Gemm & epilogue(
+    gemm::GemmCoord problem_size,
+    ScalarType alpha,
+    ScalarType beta,
+    TensorRefC tensor_c,
+    TensorRefC tensor_d,
+    MatrixCoord output_coord = MatrixCoord()) {
+
+    ConvertOp convert_op;
+    
+    // Update the output tensor
+    for (int j = 0; j < OutputTile::kRow; ++j) {
+      for (int i = 0; i < OutputTile::kColumn; ++i) {
+        MatrixCoord coord = output_coord + MatrixCoord(i, j);
+        if (coord.row() < problem_size.m() && coord.column() < problem_size.n()) {
+
+          tensor_d.at(coord) = convert_op(
+            alpha * ScalarType(accum[j][i]) +
+            beta * ScalarType(tensor_c.at(coord))
           );
-
-          if (!passed) {
-            std::cout << "Failed on size " << problem_size << " with split_k_count " << split_k_count << std::endl;
-            return false;
-          }
         }
       }
     }
-  }
 
-  EXPECT_TRUE(passed);
-
-  return passed;
-}
+    return *this;
+  }
+};
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
+} // namespace thread
 } // namespace device
-} // namespace gemm
-} // namespace test
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
+} // namespace reference
+} // namespace cutlass
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_symm_universal.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_symm_universal.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_trmm_universal.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_trmm_universal.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_universal.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse_testbed.h`

 * *Files 26% similar despite different names*

```diff
@@ -25,260 +25,227 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide GEMM interface
+    \brief Unit testbed for kernel-level GEMM
 */
 
 #pragma once
 
-#include <iostream>
-#include <fstream>
-#include <sstream>
-
 #include "../../common/cutlass_unit_test.h"
-
-#include "cutlass/util/host_tensor.h"
-#include "cutlass/util/tensor_view_io.h"
+#include "cutlass/aligned_buffer.h"
+#include "cutlass/array.h"
+#include "cutlass/core_io.h"
+#include "cutlass/gemm/gemm.h"
+#include "cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h"
+#include "cutlass/layout/matrix.h"
+#include "cutlass/numeric_types.h"
+#include "cutlass/transform/threadblock/predicated_tile_access_iterator.h"
 #include "cutlass/util/distribution.h"
-#include "cutlass/util/reference/host/tensor_fill.h"
-#include "cutlass/util/reference/host/tensor_copy.h"
+#include "cutlass/util/host_tensor.h"
+#include "cutlass/util/reference/host/gemm.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_norm.h"
-#include "cutlass/util/reference/host/gemm.h"
-#include "cutlass/util/reference/host/gemm_complex.h"
-
-#include "testbed_utils.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/tensor_view_io.h"
+#include "cutlass/util/host_reorder.h"
+#include "cutlass/util/host_uncompress.h"
 
 namespace test {
 namespace gemm {
-namespace device {
+namespace threadblock {
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
-template <typename Gemm, bool Relu = false>
-struct TestbedUniversal {
+template <typename Mma>
+__global__ void kernel_multistage_mma_sparse(cutlass::gemm::GemmCoord problem_size,
+                                      typename Mma::IteratorA::Params params_A,
+                                      typename Mma::IteratorA::TensorRef ref_A,
+                                      typename Mma::IteratorB::Params params_B,
+                                      typename Mma::IteratorB::TensorRef ref_B,
+                                      typename Mma::ElementC *ptr_C,
+                                      typename Mma::LayoutC::Stride::Index ldc,
+                                      typename Mma::IteratorE::Params params_E,
+                                      typename Mma::IteratorE::TensorRef ref_E) {
+  // Shared storage needed by threadblock-scoped matrix multiply-
+  // Dynamic shared memory base pointer
+  extern __shared__ int GemmSharedStorageBase[];
 
-  using ElementAccumulator = typename Gemm::ElementAccumulator;
-  using ElementCompute = typename Gemm::GemmKernel::Epilogue::OutputOp::ElementCompute;
-
-  /// Initialization
-  cutlass::Distribution::Kind init_A;
-  cutlass::Distribution::Kind init_B;
-  cutlass::Distribution::Kind init_C;
-  uint64_t seed;
-
-  cutlass::HostTensor<typename Gemm::ElementA, typename Gemm::LayoutA> tensor_A;
-  cutlass::HostTensor<typename Gemm::ElementB, typename Gemm::LayoutB> tensor_B;
-  cutlass::HostTensor<typename Gemm::ElementC, typename Gemm::LayoutC> tensor_C;
-  cutlass::HostTensor<typename Gemm::ElementC, typename Gemm::LayoutC> tensor_D;
-  cutlass::HostTensor<typename Gemm::ElementC, typename Gemm::LayoutC> reference_D;
+  // Declare pointer to dynamic shared memory.
+  typename Mma::SharedStorage *shared_storage =
+      reinterpret_cast<typename Mma::SharedStorage *>(GemmSharedStorageBase);
 
-  //
-  // Methods
-  //
+  // Compute threadblock location
+  cutlass::gemm::GemmCoord tb_tile_offset = {int(blockIdx.x), int(blockIdx.y),
+                                             0};
 
-  TestbedUniversal(
-    cutlass::Distribution::Kind init_A_ = cutlass::Distribution::Uniform,
-    cutlass::Distribution::Kind init_B_ = cutlass::Distribution::Uniform,
-    cutlass::Distribution::Kind init_C_ = cutlass::Distribution::Uniform,
-    uint64_t seed_ = 2080
-  ):
-    init_A(init_A_), init_B(init_B_), init_C(init_C_), seed(seed_) { }
-
-  /// Helper to initialize a tensor view
-  template <typename Element, typename Layout>
-  bool initialize_tensor(
-    cutlass::TensorView<Element, Layout> view,
-    cutlass::Distribution::Kind dist_kind,
-    uint64_t seed) {
-
-    if (dist_kind == cutlass::Distribution::Uniform) {
-
-      double scope_max, scope_min;
-      int bits_input = cutlass::sizeof_bits<Element>::value;
-      int bits_output = cutlass::sizeof_bits<typename Gemm::ElementC>::value;
+  cutlass::MatrixCoord tb_offset_A{tb_tile_offset.m() * Mma::Shape::kM,
+                                   tb_tile_offset.k() / Mma::kSparse};
 
-      if (bits_input == 1) {
-        scope_max = 2;
-        scope_min = 0;
-      } else if (bits_input <= 8) {
-        scope_max = 2;
-        scope_min = -2;
-      } else if (bits_output == 16) {
-        scope_max = 5;
-        scope_min = -5;
-      } else {
-        scope_max = 8;
-        scope_min = -8;
-      }
+  cutlass::MatrixCoord tb_offset_B{tb_tile_offset.k(),
+                                   tb_tile_offset.n() * Mma::Shape::kN};
 
-      cutlass::reference::host::TensorFillRandomUniform(
-        view, seed, scope_max, scope_min, 0);
-    }
-    else if (dist_kind == cutlass::Distribution::Identity) {
+  cutlass::MatrixCoord tb_offset_E{tb_tile_offset.m() * Mma::Shape::kM,
+                                   tb_tile_offset.k() / Mma::kSparse};
 
-      cutlass::reference::host::TensorFillIdentity(view);
-    }
-    else if (dist_kind == cutlass::Distribution::Gaussian) {
+  // Compute position within threadblock
+  int tb_thread_id = threadIdx.y * blockDim.x + threadIdx.x;
 
-      cutlass::reference::host::TensorFillRandomGaussian(view, seed, 0, 0.5);
-    }
-    else if (dist_kind == cutlass::Distribution::Sequential) {
+  // Construct iterators to A and B operands
+  typename Mma::IteratorA iterator_A(params_A, ref_A.data(),
+                                     {problem_size.m(), problem_size.k() / Mma::kSparse},
+                                     tb_thread_id, tb_offset_A);
 
-      cutlass::reference::host::BlockFillSequential(
-        view.data(), view.capacity());
-    }
-    else {
-      // TODO: Implement the rest
-      EXPECT_TRUE(false) << "Not implemented";
-      return false;
-    }
+  typename Mma::IteratorB iterator_B(params_B, ref_B.data(),
+                                     {problem_size.k(), problem_size.n()},
+                                     tb_thread_id, tb_offset_B);
 
-    return true;
-  }
+  typename Mma::IteratorE iterator_E(
+      params_E, ref_E.data(),
+      {problem_size.m(),
+       problem_size.k() / Mma::kSparse / Mma::kElementsPerElementE},
+      tb_thread_id, tb_offset_E);
 
-  /// Initializes data structures
-  void initialize(cutlass::gemm::GemmCoord problem_size) {
-    //
-    // Allocate the GEMM workspace
-    //
+  int warp_id = __shfl_sync(0xffffffff, threadIdx.y, 0);
 
-    tensor_A.resize(problem_size.mk());
-    tensor_B.resize(problem_size.kn());
-    tensor_C.resize(problem_size.mn());
-    tensor_D.resize(problem_size.mn());
-    reference_D.resize(problem_size.mn(), false);
-
-    EXPECT_TRUE(initialize_tensor(tensor_A.host_view(), init_A, seed + 2019));
-    EXPECT_TRUE(initialize_tensor(tensor_B.host_view(), init_B, seed + 2018));
-    EXPECT_TRUE(initialize_tensor(tensor_C.host_view(), init_C, seed + 2017));
-
-    // It is possible to randomly initialize to all zeros, so override this with non-zeros
-    // in the upper left corner of each operand.
-    cutlass::Coord<2> origin(0);
-    tensor_A.host_view().at(origin) = typename Gemm::ElementA(1);
-    tensor_B.host_view().at(origin) = typename Gemm::ElementB(1);
-    tensor_C.host_view().at(origin) = typename Gemm::ElementC(1);
-
-    cutlass::reference::host::TensorCopy(reference_D.host_view(), tensor_C.host_view());
-
-    tensor_A.sync_device();
-    tensor_B.sync_device();
-    tensor_C.sync_device();
-    tensor_D.sync_device();
-  }
+  // Construct thread-scoped matrix multiply
+  Mma mma(*shared_storage, tb_thread_id, warp_id, threadIdx.x);
 
-  /// Compares computed reference with device reference and outputs to a file if incorrect
-  bool compare_reference(
-    cutlass::gemm::GemmCoord problem_size,
-    ElementCompute alpha,
-    ElementCompute beta) {
-
-    tensor_D.sync_host();
-
-    EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_A.host_view()), 0);
-    EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_B.host_view()), 0);
-    EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_C.host_view()), 0);
-
-    EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_D.host_view()), 0);
-    EXPECT_GT(cutlass::reference::host::TensorNorm(reference_D.host_view()), 0);
-
-    bool passed = cutlass::reference::host::TensorEquals(reference_D.host_view(), tensor_D.host_view());
-
-    EXPECT_TRUE(passed) << " mismatched reference";
-
-    if (!passed) {
-
-      /*
-      std::stringstream fname;
-
-      fname << "error_Gemm_device_"
-        << problem_size.m() << "x"
-        << problem_size.n() << "x"
-        << problem_size.k() << "_"
-        << Gemm::ThreadblockShape::kM << "x"
-        << Gemm::ThreadblockShape::kN << "x"
-        << Gemm::ThreadblockShape::kK << "_"
-        << Gemm::WarpShape::kM << "x"
-        << Gemm::WarpShape::kN << "x"
-        << Gemm::WarpShape::kK << ".txt";
-
-      std::ofstream file(fname.str());
-      */
-
-      std::ofstream file("testbed_universal_errors.txt");
-
-      file
-        << "problem: " << problem_size
-        << ", alpha: " << alpha << ", beta: " << beta << "\n\n";
-
-      file
-        << "A =\n" << tensor_A.host_view()
-        << "\nB =\n" << tensor_B.host_view()
-        << "\nC =\n" << tensor_C.host_view()
-        << "\n\nReference =\n" << reference_D.host_view()
-        << "\nComputed =\n" << tensor_D.host_view();
-    }
+  typename Mma::FragmentC accum;
 
-    return passed;
-  }
+  accum.clear();
 
-  /// Verifies the result is a GEMM
-  bool verify(
-    cutlass::gemm::GemmCoord problem_size,
-    ElementCompute alpha,
-    ElementCompute beta) {
+  int gemm_k_iterations = (problem_size.k() + Mma::Shape::kK - 1) / Mma::Shape::kK;
 
-    //
-    // Verify
-    //
+  // Compute threadblock-scoped matrix multiply-add
+  mma(gemm_k_iterations, accum, iterator_A, iterator_B, iterator_E, accum);
 
-    cutlass::reference::host::GemmComplex<
-        typename Gemm::ElementA, typename Gemm::LayoutA,
-        typename Gemm::ElementB, typename Gemm::LayoutB,
-        typename Gemm::ElementC, typename Gemm::LayoutC,
-        ElementCompute, ElementAccumulator
-    >(
-      problem_size,
-      alpha,
-      tensor_A.host_ref(),
-      Gemm::kTransformA,
-      tensor_B.host_ref(),
-      Gemm::kTransformB,
-      beta,
-      tensor_C.host_ref(),
-      reference_D.host_ref(),
-      ElementAccumulator(0)
-    );
-
-    if (Relu) {
-      for (int i = 0; i < problem_size.m(); ++i) {
-        for (int j = 0; j < problem_size.n(); ++j) {
-           reference_D.at(cutlass::MatrixCoord(i, j)) =
-                  ((ElementCompute)reference_D.at(cutlass::MatrixCoord(i, j)) < (ElementCompute)0)
-                  ? (typename Gemm::ElementC)0
-                  : reference_D.at(cutlass::MatrixCoord(i, j));
-        }
-      }
-    }
+  // Output results
+  typename Mma::Operator::IteratorC iterator_C({ptr_C, ldc}, threadIdx.x);
 
-    return compare_reference(problem_size, alpha, beta);
+  iterator_C.add_tile_offset(
+      {(tb_tile_offset.m() * Mma::WarpCount::kM) +
+           (warp_id % Mma::WarpCount::kM),
+       (tb_tile_offset.n() * Mma::WarpCount::kN) +
+           (warp_id / Mma::WarpCount::kM)});
+
+  iterator_C.store(accum);
+}
+
+////////////////////////////////////////////////////////////////////////////////
+
+/// Structure to compute the matrix product
+template <
+    /// Threadblock-level matrix multiply-accumulate
+    typename MmaCore_>
+struct SparseTestbed {
+  /// Threadblock-level GEMM implementation
+  using MmaCore = MmaCore_;
+  using ThreadblockShape = typename MmaCore::Shape;
+  using WarpShape = typename MmaCore::WarpShape;
+  using InstructionShape = typename MmaCore::InstructionShape;
+  using ElementA = typename MmaCore::ElementA;
+  using LayoutA = typename MmaCore::LayoutA;
+  using ElementB = typename MmaCore::ElementB;
+  using LayoutB = typename MmaCore::LayoutB;
+  using ElementC = typename MmaCore::ElementC;
+  using LayoutC = typename MmaCore::LayoutC;
+  using ElementE = typename MmaCore::ElementE;
+  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
+  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
+  using ThreadMapE = typename MmaCore::IteratorThreadMapE;
+  using AccessTypeA = cutlass::Array<ElementA, ThreadMapA::kElementsPerAccess>;
+  using AccessTypeB = cutlass::Array<ElementB, ThreadMapB::kElementsPerAccess>;
+  using AccessTypeE = cutlass::Array<ElementE, ThreadMapE::kElementsPerAccess>;
+  static int const Stages = MmaCore::kStages;
+  static cutlass::arch::CacheOperation::Kind const CacheOpA =
+      MmaCore::kCacheOpA;
+  static cutlass::arch::CacheOperation::Kind const CacheOpB =
+      MmaCore::kCacheOpB;
+  static cutlass::arch::CacheOperation::Kind const CacheOpE =
+      MmaCore::kCacheOpE;
+
+  static int const Sparse = MmaCore::kSparse;
+  static int const MetaSizeInBits = MmaCore::kMetaSizeInBits;
+  static int const MaxID2 = MmaCore::kMaxID2;
+
+  using LayoutE = cutlass::layout::RowMajor;
+  using ReorderedLayoutE = typename MmaCore::GmemLayoutE;
+
+  static int const ElementsPerElementE = MmaCore::kElementsPerElementE;
+
+  // Define iterators over tiles from the A operand
+  using IteratorA =
+      cutlass::transform::threadblock::PredicatedTileAccessIterator<
+          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK / Sparse>,
+          ElementA, LayoutA, 1, ThreadMapA, AccessTypeA>;
+
+  // Define iterators over tiles from the B operand
+  using IteratorB =
+      cutlass::transform::threadblock::PredicatedTileAccessIterator<
+          cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
+          ElementB, LayoutB, 0, ThreadMapB, AccessTypeB>;
+
+  // Define iterators over tiles from the E operand
+  using IteratorE =
+      cutlass::transform::threadblock::PredicatedTileAccessIterator<
+          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK /
+                                                         Sparse /
+                                                         ElementsPerElementE>,
+          ElementE, ReorderedLayoutE, 1, ThreadMapE, AccessTypeE>;
+
+  // Define the threadblock-scoped pipelined matrix multiply
+  using Mma = cutlass::gemm::threadblock::SparseMmaMultistage<
+      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
+      CacheOpA, IteratorB, typename MmaCore::SmemIteratorB, CacheOpB, ElementC,
+      LayoutC, IteratorE, typename MmaCore::SmemIteratorE, CacheOpE,
+      typename MmaCore::MmaPolicy, Stages>;
+
+  //
+  // Data members
+  //
+
+  cutlass::HostTensor<ElementA, LayoutA> matrix_A;
+  cutlass::HostTensor<ElementA, LayoutA> matrix_A_uncompressed;
+  cutlass::HostTensor<ElementB, LayoutB> matrix_B;
+  cutlass::HostTensor<ElementC, LayoutC> matrix_C_computed;
+  cutlass::HostTensor<ElementC, LayoutC> matrix_C_reference;
+  cutlass::HostTensor<ElementE, LayoutE> matrix_E;
+  cutlass::HostTensor<ElementE, ReorderedLayoutE> matrix_E_reordered;
+
+  cutlass::gemm::GemmCoord problem_size;
+  float alpha, beta;
+
+  //
+  // Methods
+  //
+
+  /// Allocates workspace in device memory
+  SparseTestbed(int m, int n, int k, float alpha_ = float(1), float beta_ = float(0))
+      : problem_size(m, n, k), alpha(alpha_), beta(beta_) {
+    matrix_A.reset(cutlass::make_Coord(m, k / Sparse));
+    matrix_A_uncompressed.reset(cutlass::make_Coord(m, k));
+    matrix_B.reset(cutlass::make_Coord(k, n));
+    matrix_C_computed.reset(cutlass::make_Coord(m, n));
+    matrix_C_reference.reset(cutlass::make_Coord(m, n), false);
+    matrix_E.reset(cutlass::make_Coord(m, k / Sparse / ElementsPerElementE));
+    matrix_E_reordered.reset(
+        cutlass::make_Coord(m, k / Sparse / ElementsPerElementE));
   }
 
   /// Returns true if the CUDA device is sufficient to execute the kernel.
   bool sufficient() const {
     //
     // Determine SMEM requirements and waive if not satisfied
     //
 
-    int smem_size = int(sizeof(typename Gemm::GemmKernel::SharedStorage));
+    int smem_size = int(sizeof(typename Mma::SharedStorage));
 
     cudaDeviceProp properties;
     int device_idx;
     cudaError_t result = cudaGetDevice(&device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDevice() API call failed.");
@@ -293,252 +260,179 @@
     if (properties.sharedMemPerBlockOptin < smem_size) {
       return false;
     }
 
     return true;
   }
 
-  /// Executes one test
+  /// Runs the test
   bool run(
-    cutlass::gemm::GemmUniversalMode mode,
-    cutlass::gemm::GemmCoord problem_size,
-    int batch_count = 1,
-    ElementCompute alpha = ElementCompute(1),
-    ElementCompute beta = ElementCompute(0))
-  {
-/*
-    std::cout << "\n-----------------------\n";
-    std::cout << "mode: " << (int) mode << "\n";
-    std::cout << "problem size: " << problem_size << "\n";
-    std::cout << "batch_count: " << batch_count << "\n";
-    std::cout << "alpha: " << alpha << "\n";
-    std::cout << "beta: " << beta << "\n";
-    std::cout << "-----------------------\n\n";
-*/
+      dim3 grid, dim3 block,
+      cutlass::Distribution::Kind init_A = cutlass::Distribution::Uniform,
+      cutlass::Distribution::Kind init_B = cutlass::Distribution::Uniform,
+      cutlass::Distribution::Kind init_E = cutlass::Distribution::Uniform) {
 
-    // Waive test if insufficient CUDA device
+    // Waive the test
     if (!sufficient()) {
-      if (CUTLASS_TEST_UNIT_ENABLE_WARNINGS) {
-        std::cerr << "Test waived due to insufficient CUDA device." << std::endl;
-      }
       return true;
     }
 
-    this->initialize(problem_size);
-
     //
-    // Initialize the GEMM operator
+    // initialize device memory
     //
 
-    typename Gemm::Arguments arguments{
-      mode,
-      problem_size,
-      batch_count,
-      {alpha, beta},
-      tensor_A.device_data(),
-      tensor_B.device_data(),
-      tensor_C.device_data(),
-      tensor_D.device_data(),
-      problem_size.m() * problem_size.k(),
-      problem_size.n() * problem_size.k(),
-      problem_size.m() * problem_size.n(),
-      problem_size.m() * problem_size.n(),
-      tensor_A.layout().stride(0),
-      tensor_B.layout().stride(0),
-      tensor_C.layout().stride(0),
-      tensor_D.layout().stride(0)
-    };
-
-    Gemm gemm_op;
+    if (init_A == cutlass::Distribution::Uniform) {
 
-    size_t workspace_size = Gemm::get_workspace_size(arguments);
+      int scope_max = 8;
+      int scope_min = -8;
 
-    cutlass::device_memory::allocation<uint8_t> workspace(workspace_size);
+      if (cutlass::sizeof_bits<ElementA>::value == 4) {
+        scope_max = 2;
+        scope_min = -2;
+      } else if (cutlass::sizeof_bits<ElementA>::value == 1) {
+        scope_max = 2;
+        scope_min = 0;
+      }
 
-    cutlass::Status status = gemm_op.initialize(arguments, workspace.get());
+      uint64_t seed = 7;
+      cutlass::reference::host::TensorFillRandomUniform(
+          matrix_A.host_view(), seed, scope_max, scope_min, 0);
+    } else if (init_A == cutlass::Distribution::Sequential) {
+      cutlass::reference::host::BlockFillSequential(matrix_A.host_data(),
+                                                    matrix_A.capacity());
+    } else if (init_A == cutlass::Distribution::Identity) {
+      cutlass::reference::host::TensorFillIdentity(matrix_A.host_view());
+    } else {
+      // TODO: Implement the rest
+      return false;
+    }
 
-    EXPECT_TRUE(status == cutlass::Status::kSuccess) << to_string(status);
+    if (init_B == cutlass::Distribution::Uniform) {
 
-    //
-    // Run the GEMM
-    //
+      int scope_max = 8;
+      int scope_min = -8;
 
-    status = gemm_op();
+      if (cutlass::sizeof_bits<ElementB>::value == 4) {
+        scope_max = 2;
+        scope_min = -2;
+      } else if (cutlass::sizeof_bits<ElementB>::value == 1) {
+        scope_max = 2;
+        scope_min = 0;
+      }
 
-    EXPECT_TRUE(status == cutlass::Status::kSuccess) << to_string(status);
+      uint64_t seed = 7;
+      cutlass::reference::host::TensorFillRandomUniform(
+          matrix_B.host_view(), seed + 16, scope_max, scope_min, 0);
+    } else if (init_B == cutlass::Distribution::Sequential) {
+      cutlass::reference::host::BlockFillSequential(matrix_B.host_data(),
+                                                    matrix_B.capacity());
+    } else if (init_B == cutlass::Distribution::Identity) {
+      cutlass::reference::host::TensorFillIdentity(matrix_B.host_view());
+    } else {
+      // TODO: Implement the rest
+      return false;
+    }
 
-    //
-    // Verify
-    //
+    cutlass::reference::host::TensorFill(matrix_C_computed.host_view());
 
-    bool passed = this->verify(problem_size, alpha, beta);
+    cutlass::reference::host::TensorFill(matrix_C_reference.host_view());
 
-    if (!passed) {
-      std::cout << "Failed with batch_count/split_k_slices = " << batch_count << std::endl;
+    if (init_E == cutlass::Distribution::Uniform) {
+      uint64_t seed = 7;
+      cutlass::reference::host::TensorFillRandomSparseMeta(
+          matrix_E.host_view(), seed, MetaSizeInBits);
+    } else if (init_E == cutlass::Distribution::Identity) {
+      uint32_t content = (MaxID2 == 1) ? 0x44444444 : 0x4444;
+      cutlass::reference::host::TensorFill(matrix_E.host_view(),
+                                           (ElementE)(content));
+    } else {
+      // TODO: Implement the rest
+      return false;
     }
 
-    return passed;
-  }
-};
+    cutlass::reorder_meta(matrix_E_reordered.host_ref(), matrix_E.host_ref(),
+                          {problem_size.m(), problem_size.n(),
+                           problem_size.k() / Sparse / ElementsPerElementE});
+
+    matrix_A.sync_device();
+    matrix_B.sync_device();
+    matrix_C_computed.sync_device();
+    matrix_E_reordered.sync_device();
+
+    typename IteratorA::Params params_A(matrix_A.layout());
+    typename IteratorB::Params params_B(matrix_B.layout());
+    typename IteratorE::Params params_E(matrix_E_reordered.layout());
+
+    cudaError_t result;
+
+    int smem_size = int(sizeof(typename Mma::SharedStorage));
+    if (smem_size >= (48 << 10)) {
+      result = cudaFuncSetAttribute(
+          test::gemm::threadblock::kernel_multistage_mma_sparse<Mma>,
+          cudaFuncAttributeMaxDynamicSharedMemorySize, smem_size);
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-template <typename Gemm, bool Relu = false>
-bool TestGemmUniversal(
-  cutlass::gemm::GemmCoord const & problem_size,
-  cutlass::gemm::GemmUniversalMode mode,
-  int batch_count,
-  double alpha = 1.0,
-  double beta = 2.0) {
-
-  bool passed = true;
-
-  TestbedUniversal<Gemm, Relu> testbed;
-
-  using ElementCompute = typename Gemm::EpilogueOutputOp::ElementCompute;
-
-  passed = testbed.run(
-    mode,
-    problem_size,
-    batch_count,
-    cutlass::from_real<ElementCompute>(alpha),
-    cutlass::from_real<ElementCompute>(beta)
-  );
+      if (result != cudaSuccess) {
+          return true;
+      }
 
-  return passed;
-}
+      result = cudaFuncSetAttribute(
+          test::gemm::threadblock::kernel_multistage_mma_sparse<Mma>,
+          cudaFuncAttributePreferredSharedMemoryCarveout, 100);
 
-template <typename Gemm, bool Relu = false>
-bool TestAllGemmUniversal() {
-  bool passed = true;
-
-
-  int const kMinimumOperandElementSize = 
-    std::min(
-      int(cutlass::sizeof_bits<typename Gemm::ElementA>::value), 
-      int(cutlass::sizeof_bits<typename Gemm::ElementB>::value));
-
-  int const kAlignment = cutlass::platform::is_same<
-                              typename Gemm::OperatorClass, 
-                              cutlass::arch::OpClassSimt>::value ? 1 : 128 / kMinimumOperandElementSize;
-
-  // int8_t gemm alignment constraints
-  int const kAlignmentM = cutlass::platform::is_same<typename Gemm::OperatorClass, cutlass::arch::OpClassSimt>::value &&
-                          cutlass::platform::is_same<typename Gemm::ElementA, int8_t>::value &&
-                          cutlass::platform::is_same<typename Gemm::LayoutA, cutlass::layout::ColumnMajor>::value ? 4 : kAlignment;
-
-  int const kAlignmentN = cutlass::platform::is_same<typename Gemm::OperatorClass, cutlass::arch::OpClassSimt>::value &&
-                          cutlass::platform::is_same<typename Gemm::ElementB, int8_t>::value &&
-                          cutlass::platform::is_same<typename Gemm::LayoutB, cutlass::layout::RowMajor>::value ? 4 : kAlignment;
-
-  int const kAlignmentK = cutlass::platform::is_same<typename Gemm::OperatorClass, cutlass::arch::OpClassSimt>::value &&
-                          cutlass::platform::is_same<typename Gemm::ElementA, int8_t>::value &&
-                          cutlass::platform::is_same<typename Gemm::ElementB, int8_t>::value &&
-                          (cutlass::platform::is_same<typename Gemm::LayoutA, cutlass::layout::RowMajor>::value ||
-                          cutlass::platform::is_same<typename Gemm::LayoutB, cutlass::layout::ColumnMajor>::value) ? 4 : kAlignment;
-
-
-
-  cutlass::gemm::GemmUniversalMode modes[] = {
-    cutlass::gemm::GemmUniversalMode::kGemm,
-  };
-
-  int problem_size_m[] = {
-    kAlignmentM, 512 - 3*kAlignmentM
-  };
-
-  int problem_size_n[] = {
-    kAlignmentN, 512 - 2*kAlignmentN
-  };
-
-  int problem_size_k[] = {
-    kAlignmentK,
-    Gemm::ThreadblockShape::kK * Gemm::kStages - kAlignmentK,
-    Gemm::ThreadblockShape::kK * Gemm::kStages * 3 - kAlignmentK
-  };
-
-  int batch_counts[] = {      // may be interpretted as batch count or split-K slices
-    1, 2, 3, 5, 7
-  };
-
-  double problem_alpha[] = {
-    1
-  };
-
-  double problem_beta[] = {
-    2.0
-  };
-
-
-  using ElementCompute = typename Gemm::EpilogueOutputOp::ElementCompute;
-
-  for (cutlass::gemm::GemmUniversalMode mode : modes) {
-    for (int m : problem_size_m) {
-      for (int n : problem_size_n) {
-        for (int k : problem_size_k) {
-          for (int batch_count : batch_counts) {
-
-            for (auto alpha : problem_alpha) {
-              for (auto beta : problem_beta) {
-
-                if (mode == cutlass::gemm::GemmUniversalMode::kGemm ||
-                  mode == cutlass::gemm::GemmUniversalMode::kGemmSplitKParallel) {
-
-                  // skip very small K problems
-                  if (k / batch_count < 2 * Gemm::ThreadblockShape::kK) {
-                    continue;
-                  }
-                }
-
-                cutlass::gemm::GemmCoord problem_size(m, n, k);
-
-                TestbedUniversal<Gemm, Relu> testbed;
-
-                passed = testbed.run(
-                  mode,
-                  problem_size,
-                  batch_count,
-                  cutlass::from_real<ElementCompute>(alpha),
-                  cutlass::from_real<ElementCompute>(beta)
-                );
-
-                if (!passed) {
-                  return false;
-                }
-              }
-            }
-          }
-        }
+      if (result != cudaSuccess) {
+          return true;
       }
     }
-  }
 
-  /*
-  // large problem with high coverage
-  for (int split_k_slices = 1; split_k_slices <= 3; ++split_k_slices) {
-    TestbedUniversal<Gemm> testbed;
-
-    cutlass::gemm::GemmCoord problem_size(72, 56, 8192);
-
-    passed = testbed.run(
-      cutlass::gemm::GemmUniversalMode::kGemm,
-      problem_size,
-      split_k_slices,
-      cutlass::from_real<ElementCompute>(1.0),
-      cutlass::from_real<ElementCompute>(2.0)
-    );
+    test::gemm::threadblock::kernel_multistage_mma_sparse<Mma>
+        <<<grid, block, smem_size, 0>>>(
+            problem_size, params_A, matrix_A.device_ref(), params_B,
+            matrix_B.device_ref(), matrix_C_computed.device_data(),
+            matrix_C_computed.layout().stride(0), params_E,
+            matrix_E_reordered.device_ref());
 
-    if (!passed) {
-      break;
-    }
-  }
-  */
+    //
+    // Check error code
+    //
 
-  return passed;
-}
+    result = cudaDeviceSynchronize();
+    EXPECT_EQ(result, cudaSuccess)
+        << " kernel error: " << cudaGetErrorString(result);
+
+    matrix_C_computed.sync_host();
+
+    cutlass::uncompress(matrix_A_uncompressed.host_ref(), matrix_A.host_ref(),
+                        matrix_E.host_ref(), problem_size.m(),
+                        problem_size.k());
+
+    cutlass::reference::host::Gemm<ElementA, LayoutA, ElementB, LayoutB,
+                                   ElementC, LayoutC, ElementC, ElementC>
+        reference_gemm;
+
+    reference_gemm(problem_size, ElementC(alpha),
+                   matrix_A_uncompressed.host_view(), matrix_B.host_view(),
+                   ElementC(beta), matrix_C_reference.host_view());
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+    bool passed = cutlass::reference::host::TensorEquals(
+        matrix_C_computed.host_view(), matrix_C_reference.host_view());
+
+    EXPECT_TRUE(passed)
+        << "A:\n" << matrix_A.host_view() << "\n"
+        << "B:\n" << matrix_B.host_view() << "\n"
+        << "E:\n" << matrix_E.host_view() << "\n"
+        << "Reference:\n"
+        << matrix_C_reference.host_view() << "\n"
+        << "Computed:\n"
+        << matrix_C_computed.host_view() << "\n";
+
+    EXPECT_GT(cutlass::reference::host::TensorNorm(matrix_C_reference.host_view()), 0);
+    EXPECT_GT(cutlass::reference::host::TensorNorm(matrix_C_computed.host_view()), 0);
+
+    return passed;
+  }
+};
 
-} // namespace device
-} // namespace gemm
-} // namespace test
+////////////////////////////////////////////////////////////////////////////////
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+}  // namespace threadblock
+}  // namespace gemm
+}  // namespace test
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_utils.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_utils.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_fast_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm60.cu`

 * *Files 26% similar despite different names*

```diff
@@ -25,113 +25,116 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide TRMM interface
-
-  
+    \brief Unit tests for thread-level GEMM
 */
 
-#include <iostream>
-
 #include "../../common/cutlass_unit_test.h"
-#include "cutlass/blas3.h"
-#include "cutlass/gemm/device/trmm.h"
-#include "cutlass/util/host_tensor.h"
-#include "cutlass/util/reference/host/trmm.h"
-#include "cutlass/util/reference/host/tensor_compare.h"
-#include "cutlass/util/reference/host/tensor_copy.h"
-#include "cutlass/util/reference/host/tensor_fill.h"
-#include "cutlass/util/tensor_view_io.h"
 
-#include "testbed_trmm_universal.h"
+#include "cutlass/gemm/gemm.h"
+#include "cutlass/gemm/warp/mma_simt.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+#include "testbed.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Trmm_cf64n_cf64n_cf64t_ls_u_nu_tensor_op_f64_gaussian, 32x32x16_16x16x16) {
+TEST(SM60_warp_gemm_f16_col_row, 8x4x1_1x1x1) {
 
-  using ElementOutput = cutlass::complex<double>;
-  using ElementAccumulator = cutlass::complex<double>;
+  using Policy = cutlass::gemm::warp::MmaSimtPolicy<
+    cutlass::MatrixShape<8, 4>,
+    cutlass::layout::ColumnMajorInterleaved<2>,
+    cutlass::gemm::GemmShape<1, 1, 1>
+  >;
 
-  using Trmm = cutlass::gemm::device::Trmm<
-    cutlass::complex<double>,
-    cutlass::layout::ColumnMajor,
-    cutlass::SideMode::kLeft,
-    cutlass::FillMode::kUpper,
-    cutlass::DiagType::kNonUnit,
-    cutlass::complex<double>,
+  using Mma = cutlass::gemm::warp::MmaSimt<
+    cutlass::gemm::GemmShape<8, 4, 8>,
+    cutlass::half_t,
     cutlass::layout::ColumnMajor,
-    ElementOutput,
+    cutlass::half_t,
     cutlass::layout::RowMajor,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<32, 32, 16>,
-    cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
-    cutlass::epilogue::thread::LinearCombination<
-      ElementOutput,
-      1,
-      ElementAccumulator,
-      ElementAccumulator
-    >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    4,
-    1,
-    1,
-    false,
-    cutlass::arch::OpMultiplyAddGaussianComplex,
-    cutlass::ComplexTransform::kNone
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor,
+    Policy
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
+  test::gemm::warp::Testbed<Mma, cutlass::gemm::GemmShape<128, 128, 8> >().run();
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Trmm_cf64h_cf64n_cf64t_ls_u_nu_tensor_op_f64_gaussian, 64x64x16_32x32x16) {
+TEST(SM60_warp_gemm_f16_col_row, 16x8x1_2x2x1) {
 
-  using ElementOutput = cutlass::complex<double>;
-  using ElementAccumulator = cutlass::complex<double>;
+  using Policy = cutlass::gemm::warp::MmaSimtPolicy<
+    cutlass::MatrixShape<8, 4>,
+    cutlass::layout::ColumnMajorInterleaved<2>,
+    cutlass::gemm::GemmShape<2, 2, 1>
+  >;
 
-  using Trmm = cutlass::gemm::device::Trmm<
-    cutlass::complex<double>,
+  using Mma = cutlass::gemm::warp::MmaSimt<
+    cutlass::gemm::GemmShape<16, 8, 8>,
+    cutlass::half_t,
     cutlass::layout::ColumnMajor,
-    cutlass::SideMode::kLeft,
-    cutlass::FillMode::kUpper,
-    cutlass::DiagType::kNonUnit,
-    cutlass::complex<double>,
+    cutlass::half_t,
+    cutlass::layout::RowMajor,
+    cutlass::half_t,
     cutlass::layout::ColumnMajor,
-    ElementOutput,
+    Policy
+  >;
+
+  test::gemm::warp::Testbed<Mma, cutlass::gemm::GemmShape<128, 128, 8> >().run();
+}
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM60_warp_gemm_f16_col_row, 32x16x1_4x4x1) {
+
+  using Policy = cutlass::gemm::warp::MmaSimtPolicy<
+    cutlass::MatrixShape<8, 4>,
+    cutlass::layout::ColumnMajorInterleaved<2>,
+    cutlass::gemm::GemmShape<4, 4, 1>
+  >;
+
+  using Mma = cutlass::gemm::warp::MmaSimt<
+    cutlass::gemm::GemmShape<32, 16, 8>,
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor,
+    cutlass::half_t,
     cutlass::layout::RowMajor,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<64, 64, 16>,
-    cutlass::gemm::GemmShape<32, 32, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
-    cutlass::epilogue::thread::LinearCombination<
-      ElementOutput,
-      1,
-      ElementAccumulator,
-      ElementAccumulator
-    >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    4,
-    1,
-    1,
-    false,
-    cutlass::arch::OpMultiplyAddGaussianComplex,
-    cutlass::ComplexTransform::kConjugate
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor,
+    Policy
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
+  test::gemm::warp::Testbed<Mma, cutlass::gemm::GemmShape<128, 128, 8> >().run();
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM60_warp_gemm_f16_col_row, 64x16x1_8x4x1) {
+
+  using Policy = cutlass::gemm::warp::MmaSimtPolicy<
+    cutlass::MatrixShape<8, 4>,
+    cutlass::layout::ColumnMajorInterleaved<2>,
+    cutlass::gemm::GemmShape<8, 8, 1>
+  >;
+
+  using Mma = cutlass::gemm::warp::MmaSimt<
+    cutlass::gemm::GemmShape<64, 32, 8>,
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor,
+    cutlass::half_t,
+    cutlass::layout::RowMajor,
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor,
+    Policy
+  >;
+
+  test::gemm::warp::Testbed<Mma, cutlass::gemm::GemmShape<128, 128, 8> >().run();
+}
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_rs_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_rs_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32t_tensor_op_fast_f32_ls_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32t_tensor_op_fast_f32_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_ls_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_rs_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_rs_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64t_f64t_tensor_op_f64_rs_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64t_f64t_tensor_op_f64_rs_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_ls_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_rs_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_rs_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_ls_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_rs_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_rs_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32n_tensor_op_f32_ls_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32n_tensor_op_f32_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32t_tensor_op_f32_ls_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32t_tensor_op_f32_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/kernel/testbed_gemv.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/kernel/testbed_gemv.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm60.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm60.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm61.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm61.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/testbed_host.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/testbed_host.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/thread/testbed.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/thread/testbed.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/batched_gemv.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/batched_gemv.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_slicedk.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_slicedk.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse_testbed.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_testbed_slicedk.h`

 * *Files 16% similar despite different names*

```diff
@@ -24,262 +24,210 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
+
 /*! \file
     \brief Unit testbed for kernel-level GEMM
 */
 
 #pragma once
 
+#include <fstream>
+
 #include "../../common/cutlass_unit_test.h"
+
 #include "cutlass/aligned_buffer.h"
-#include "cutlass/array.h"
-#include "cutlass/core_io.h"
 #include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h"
 #include "cutlass/layout/matrix.h"
+#include "cutlass/layout/vector.h"
 #include "cutlass/numeric_types.h"
-#include "cutlass/transform/threadblock/predicated_tile_access_iterator.h"
-#include "cutlass/util/distribution.h"
+
+#include "cutlass/core_io.h"
 #include "cutlass/util/host_tensor.h"
+#include "cutlass/util/tensor_view_io.h"
+
+#include "cutlass/util/distribution.h"
 #include "cutlass/util/reference/host/gemm.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
-#include "cutlass/util/reference/host/tensor_norm.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
-#include "cutlass/util/tensor_view_io.h"
-#include "cutlass/util/host_reorder.h"
-#include "cutlass/util/host_uncompress.h"
+
+#include "cutlass/gemm/threadblock/default_mma_core_simt.h"
+#include "cutlass/gemm/threadblock/default_mma_core_sm75.h"
+#include "cutlass/gemm/threadblock/default_mma_core_sm70.h"
+#include "cutlass/transform/threadblock/predicated_tile_iterator.h"
+#include "cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h"
+#include "cutlass/cutlass.h"
+#include "cutlass/platform/platform.h"
 
 namespace test {
 namespace gemm {
 namespace threadblock {
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename Mma>
-__global__ void kernel_multistage_mma_sparse(cutlass::gemm::GemmCoord problem_size,
-                                      typename Mma::IteratorA::Params params_A,
-                                      typename Mma::IteratorA::TensorRef ref_A,
-                                      typename Mma::IteratorB::Params params_B,
-                                      typename Mma::IteratorB::TensorRef ref_B,
-                                      typename Mma::ElementC *ptr_C,
-                                      typename Mma::LayoutC::Stride::Index ldc,
-                                      typename Mma::IteratorE::Params params_E,
-                                      typename Mma::IteratorE::TensorRef ref_E) {
-  // Shared storage needed by threadblock-scoped matrix multiply-
-  // Dynamic shared memory base pointer
-  extern __shared__ int GemmSharedStorageBase[];
-
-  // Declare pointer to dynamic shared memory.
-  typename Mma::SharedStorage *shared_storage =
-      reinterpret_cast<typename Mma::SharedStorage *>(GemmSharedStorageBase);
+__global__ void kernel_mma(cutlass::gemm::GemmCoord problem_size,
+                           typename Mma::IteratorA::Params params_A,
+                           typename Mma::IteratorA::TensorRef ref_A,
+                           typename Mma::IteratorB::Params params_B,
+                           typename Mma::IteratorB::TensorRef ref_B,
+                           typename Mma::ElementC **ptr_C,
+                           typename Mma::LayoutC::Stride::Index ldc) {
+  // Shared storage needed by threadblock-scoped matrix multiply-accumulate
+  __shared__ typename Mma::SharedStorage shared_storage;
 
   // Compute threadblock location
   cutlass::gemm::GemmCoord tb_tile_offset = {int(blockIdx.x), int(blockIdx.y),
                                              0};
 
   cutlass::MatrixCoord tb_offset_A{tb_tile_offset.m() * Mma::Shape::kM,
-                                   tb_tile_offset.k() / Mma::kSparse};
+                                   tb_tile_offset.k()};
 
   cutlass::MatrixCoord tb_offset_B{tb_tile_offset.k(),
                                    tb_tile_offset.n() * Mma::Shape::kN};
 
-  cutlass::MatrixCoord tb_offset_E{tb_tile_offset.m() * Mma::Shape::kM,
-                                   tb_tile_offset.k() / Mma::kSparse};
-
   // Compute position within threadblock
   int tb_thread_id = threadIdx.y * blockDim.x + threadIdx.x;
 
   // Construct iterators to A and B operands
   typename Mma::IteratorA iterator_A(params_A, ref_A.data(),
-                                     {problem_size.m(), problem_size.k() / Mma::kSparse},
+                                     {problem_size.m(), problem_size.k()},
                                      tb_thread_id, tb_offset_A);
 
   typename Mma::IteratorB iterator_B(params_B, ref_B.data(),
                                      {problem_size.k(), problem_size.n()},
                                      tb_thread_id, tb_offset_B);
 
-  typename Mma::IteratorE iterator_E(
-      params_E, ref_E.data(),
-      {problem_size.m(),
-       problem_size.k() / Mma::kSparse / Mma::kElementsPerElementE},
-      tb_thread_id, tb_offset_E);
+  int warp_id = threadIdx.y;
+  int lane_id = threadIdx.x;
 
-  int warp_id = __shfl_sync(0xffffffff, threadIdx.y, 0);
+  int partitionsK_idx = warp_id / (Mma::WarpCount::kM * Mma::WarpCount::kN);
 
   // Construct thread-scoped matrix multiply
-  Mma mma(*shared_storage, tb_thread_id, warp_id, threadIdx.x);
+  Mma mma(shared_storage, tb_thread_id, warp_id, threadIdx.x);
 
   typename Mma::FragmentC accum;
 
   accum.clear();
 
   int gemm_k_iterations = (problem_size.k() + Mma::Shape::kK - 1) / Mma::Shape::kK;
 
   // Compute threadblock-scoped matrix multiply-add
-  mma(gemm_k_iterations, accum, iterator_A, iterator_B, iterator_E, accum);
+  mma(gemm_k_iterations, accum, iterator_A, iterator_B, accum);
 
   // Output results
-  typename Mma::Operator::IteratorC iterator_C({ptr_C, ldc}, threadIdx.x);
+  typename Mma::Operator::IteratorC iterator_C({ptr_C[partitionsK_idx], ldc}, lane_id);
+
 
+  int warp_idx_mn = warp_id % (Mma::WarpCount::kM * Mma::WarpCount::kN);
   iterator_C.add_tile_offset(
       {(tb_tile_offset.m() * Mma::WarpCount::kM) +
-           (warp_id % Mma::WarpCount::kM),
+           (warp_idx_mn % Mma::WarpCount::kM),
        (tb_tile_offset.n() * Mma::WarpCount::kN) +
-           (warp_id / Mma::WarpCount::kM)});
+           (warp_idx_mn / Mma::WarpCount::kM)});
 
   iterator_C.store(accum);
 }
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Structure to compute the matrix product
 template <
     /// Threadblock-level matrix multiply-accumulate
     typename MmaCore_>
-struct SparseTestbed {
+struct Testbed {
   /// Threadblock-level GEMM implementation
   using MmaCore = MmaCore_;
   using ThreadblockShape = typename MmaCore::Shape;
   using WarpShape = typename MmaCore::WarpShape;
   using InstructionShape = typename MmaCore::InstructionShape;
   using ElementA = typename MmaCore::ElementA;
   using LayoutA = typename MmaCore::LayoutA;
   using ElementB = typename MmaCore::ElementB;
   using LayoutB = typename MmaCore::LayoutB;
   using ElementC = typename MmaCore::ElementC;
   using LayoutC = typename MmaCore::LayoutC;
-  using ElementE = typename MmaCore::ElementE;
-  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
-  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
-  using ThreadMapE = typename MmaCore::IteratorThreadMapE;
-  using AccessTypeA = cutlass::Array<ElementA, ThreadMapA::kElementsPerAccess>;
-  using AccessTypeB = cutlass::Array<ElementB, ThreadMapB::kElementsPerAccess>;
-  using AccessTypeE = cutlass::Array<ElementE, ThreadMapE::kElementsPerAccess>;
-  static int const Stages = MmaCore::kStages;
-  static cutlass::arch::CacheOperation::Kind const CacheOpA =
-      MmaCore::kCacheOpA;
-  static cutlass::arch::CacheOperation::Kind const CacheOpB =
-      MmaCore::kCacheOpB;
-  static cutlass::arch::CacheOperation::Kind const CacheOpE =
-      MmaCore::kCacheOpE;
-
-  static int const Sparse = MmaCore::kSparse;
-  static int const MetaSizeInBits = MmaCore::kMetaSizeInBits;
-  static int const MaxID2 = MmaCore::kMaxID2;
-
-  using LayoutE = cutlass::layout::RowMajor;
-  using ReorderedLayoutE = typename MmaCore::GmemLayoutE;
-
-  static int const ElementsPerElementE = MmaCore::kElementsPerElementE;
 
   // Define iterators over tiles from the A operand
-  using IteratorA =
-      cutlass::transform::threadblock::PredicatedTileAccessIterator<
-          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK / Sparse>,
-          ElementA, LayoutA, 1, ThreadMapA, AccessTypeA>;
+  static const bool use_idp4a = cutlass::platform::is_same<ElementA, int8_t>::value && 
+                                cutlass::platform::is_same<ElementB, int8_t>::value && 
+                                cutlass::platform::is_same<typename MmaCore::OperatorClass, cutlass::arch::OpClassSimt>::value;
+
+  static const bool transposeA =  cutlass::platform::is_same< LayoutA, cutlass::layout::ColumnMajor >::value;
+  static const bool transposeB =  cutlass::platform::is_same< LayoutB, cutlass::layout::RowMajor >::value;
+
+  using IteratorA = typename cutlass::platform::conditional< use_idp4a,
+      cutlass::transform::threadblock::PredicatedTileIterator2dThreadTile<
+          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
+          ElementA, LayoutA, 1, typename MmaCore::IteratorThreadMapA, transposeA> ,
+        
+      cutlass::transform::threadblock::PredicatedTileIterator<
+          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
+          ElementA, LayoutA, 1, typename MmaCore::IteratorThreadMapA>
+      >::type;
 
   // Define iterators over tiles from the B operand
-  using IteratorB =
-      cutlass::transform::threadblock::PredicatedTileAccessIterator<
+  using IteratorB = typename cutlass::platform::conditional< use_idp4a,
+      cutlass::transform::threadblock::PredicatedTileIterator2dThreadTile<
           cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
-          ElementB, LayoutB, 0, ThreadMapB, AccessTypeB>;
+          ElementB, LayoutB, 0, typename MmaCore::IteratorThreadMapB, transposeB> ,
 
-  // Define iterators over tiles from the E operand
-  using IteratorE =
-      cutlass::transform::threadblock::PredicatedTileAccessIterator<
-          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK /
-                                                         Sparse /
-                                                         ElementsPerElementE>,
-          ElementE, ReorderedLayoutE, 1, ThreadMapE, AccessTypeE>;
+      cutlass::transform::threadblock::PredicatedTileIterator<
+          cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
+          ElementB, LayoutB, 0, typename MmaCore::IteratorThreadMapB>
+      >::type;
 
   // Define the threadblock-scoped pipelined matrix multiply
-  using Mma = cutlass::gemm::threadblock::SparseMmaMultistage<
+  using Mma = cutlass::gemm::threadblock::MmaPipelined<
       typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
-      CacheOpA, IteratorB, typename MmaCore::SmemIteratorB, CacheOpB, ElementC,
-      LayoutC, IteratorE, typename MmaCore::SmemIteratorE, CacheOpE,
-      typename MmaCore::MmaPolicy, Stages>;
+      IteratorB, typename MmaCore::SmemIteratorB, ElementC, LayoutC,
+      typename MmaCore::MmaPolicy>;
+
+  static int const kPartitionsK = MmaCore::MmaPolicy::kPartitionsK; 
 
   //
   // Data members
   //
 
   cutlass::HostTensor<ElementA, LayoutA> matrix_A;
-  cutlass::HostTensor<ElementA, LayoutA> matrix_A_uncompressed;
   cutlass::HostTensor<ElementB, LayoutB> matrix_B;
-  cutlass::HostTensor<ElementC, LayoutC> matrix_C_computed;
+  cutlass::HostTensor<ElementC, LayoutC> matrix_C_computed[kPartitionsK];
   cutlass::HostTensor<ElementC, LayoutC> matrix_C_reference;
-  cutlass::HostTensor<ElementE, LayoutE> matrix_E;
-  cutlass::HostTensor<ElementE, ReorderedLayoutE> matrix_E_reordered;
+  cutlass::HostTensor<ElementC*, cutlass::layout::PackedVectorLayout> matrix_C_pointers;
 
   cutlass::gemm::GemmCoord problem_size;
   float alpha, beta;
 
   //
   // Methods
   //
 
   /// Allocates workspace in device memory
-  SparseTestbed(int m, int n, int k, float alpha_ = float(1), float beta_ = float(0))
+  Testbed(int m, int n, int k, float alpha_, float beta_)
       : problem_size(m, n, k), alpha(alpha_), beta(beta_) {
-    matrix_A.reset(cutlass::make_Coord(m, k / Sparse));
-    matrix_A_uncompressed.reset(cutlass::make_Coord(m, k));
+    matrix_A.reset(cutlass::make_Coord(m, k));
     matrix_B.reset(cutlass::make_Coord(k, n));
-    matrix_C_computed.reset(cutlass::make_Coord(m, n));
-    matrix_C_reference.reset(cutlass::make_Coord(m, n), false);
-    matrix_E.reset(cutlass::make_Coord(m, k / Sparse / ElementsPerElementE));
-    matrix_E_reordered.reset(
-        cutlass::make_Coord(m, k / Sparse / ElementsPerElementE));
-  }
-
-  /// Returns true if the CUDA device is sufficient to execute the kernel.
-  bool sufficient() const {
-    //
-    // Determine SMEM requirements and waive if not satisfied
-    //
-
-    int smem_size = int(sizeof(typename Mma::SharedStorage));
-
-    cudaDeviceProp properties;
-    int device_idx;
-    cudaError_t result = cudaGetDevice(&device_idx);
-
-    if (result != cudaSuccess) {
-      throw std::runtime_error("cudaGetDevice() API call failed.");
-    }
-
-    result = cudaGetDeviceProperties(&properties, device_idx);
-
-    if (result != cudaSuccess) {
-      throw std::runtime_error("cudaGetDeviceProperties() failed");
-    }
 
-    if (properties.sharedMemPerBlockOptin < smem_size) {
-      return false;
-    }
+    CUTLASS_PRAGMA_UNROLL
+    for(int k = 0; k < kPartitionsK; k++)
+      matrix_C_computed[k].reset(cutlass::make_Coord(m, n));
 
-    return true;
+    matrix_C_reference.reset(cutlass::make_Coord(m, n), false);
+    matrix_C_pointers.reset(cutlass::Coord<1>(kPartitionsK));
   }
 
   /// Runs the test
   bool run(
       dim3 grid, dim3 block,
       cutlass::Distribution::Kind init_A = cutlass::Distribution::Uniform,
-      cutlass::Distribution::Kind init_B = cutlass::Distribution::Uniform,
-      cutlass::Distribution::Kind init_E = cutlass::Distribution::Uniform) {
-
-    // Waive the test
-    if (!sufficient()) {
-      return true;
-    }
-
+      cutlass::Distribution::Kind init_B = cutlass::Distribution::Uniform) {
     //
     // initialize device memory
     //
 
     if (init_A == cutlass::Distribution::Uniform) {
 
       int scope_max = 8;
@@ -328,111 +276,97 @@
     } else if (init_B == cutlass::Distribution::Identity) {
       cutlass::reference::host::TensorFillIdentity(matrix_B.host_view());
     } else {
       // TODO: Implement the rest
       return false;
     }
 
-    cutlass::reference::host::TensorFill(matrix_C_computed.host_view());
+    CUTLASS_PRAGMA_UNROLL
+    for(int k = 0; k < kPartitionsK; k++)
+      cutlass::reference::host::TensorFill(matrix_C_computed[k].host_view());
 
     cutlass::reference::host::TensorFill(matrix_C_reference.host_view());
 
-    if (init_E == cutlass::Distribution::Uniform) {
-      uint64_t seed = 7;
-      cutlass::reference::host::TensorFillRandomSparseMeta(
-          matrix_E.host_view(), seed, MetaSizeInBits);
-    } else if (init_E == cutlass::Distribution::Identity) {
-      uint32_t content = (MaxID2 == 1) ? 0x44444444 : 0x4444;
-      cutlass::reference::host::TensorFill(matrix_E.host_view(),
-                                           (ElementE)(content));
-    } else {
-      // TODO: Implement the rest
-      return false;
-    }
-
-    cutlass::reorder_meta(matrix_E_reordered.host_ref(), matrix_E.host_ref(),
-                          {problem_size.m(), problem_size.n(),
-                           problem_size.k() / Sparse / ElementsPerElementE});
-
     matrix_A.sync_device();
     matrix_B.sync_device();
-    matrix_C_computed.sync_device();
-    matrix_E_reordered.sync_device();
+
+    CUTLASS_PRAGMA_UNROLL
+    for(int k = 0; k < kPartitionsK; k++)
+      matrix_C_computed[k].sync_device();
 
     typename IteratorA::Params params_A(matrix_A.layout());
     typename IteratorB::Params params_B(matrix_B.layout());
-    typename IteratorE::Params params_E(matrix_E_reordered.layout());
-
-    cudaError_t result;
 
-    int smem_size = int(sizeof(typename Mma::SharedStorage));
-    if (smem_size >= (48 << 10)) {
-      result = cudaFuncSetAttribute(
-          test::gemm::threadblock::kernel_multistage_mma_sparse<Mma>,
-          cudaFuncAttributeMaxDynamicSharedMemorySize, smem_size);
-
-      if (result != cudaSuccess) {
-          return true;
-      }
-
-      result = cudaFuncSetAttribute(
-          test::gemm::threadblock::kernel_multistage_mma_sparse<Mma>,
-          cudaFuncAttributePreferredSharedMemoryCarveout, 100);
-
-      if (result != cudaSuccess) {
-          return true;
-      }
-    }
-
-    test::gemm::threadblock::kernel_multistage_mma_sparse<Mma>
-        <<<grid, block, smem_size, 0>>>(
-            problem_size, params_A, matrix_A.device_ref(), params_B,
-            matrix_B.device_ref(), matrix_C_computed.device_data(),
-            matrix_C_computed.layout().stride(0), params_E,
-            matrix_E_reordered.device_ref());
+    CUTLASS_PRAGMA_UNROLL
+    for(int k = 0; k < kPartitionsK; k++)
+      matrix_C_pointers.at(cutlass::Coord<1>(k)) = matrix_C_computed[k].device_data();
+
+    matrix_C_pointers.sync_device();
+
+    test::gemm::threadblock::kernel_mma<Mma><<<grid, block>>>(
+        problem_size, params_A, matrix_A.device_ref(), params_B,
+        matrix_B.device_ref(), matrix_C_pointers.device_data(),
+        matrix_C_computed[0].layout().stride(0));
 
     //
     // Check error code
     //
 
-    result = cudaDeviceSynchronize();
+    cudaError_t result = cudaDeviceSynchronize();
     EXPECT_EQ(result, cudaSuccess)
         << " kernel error: " << cudaGetErrorString(result);
 
-    matrix_C_computed.sync_host();
+    CUTLASS_PRAGMA_UNROLL
+    for(int k = 0; k < kPartitionsK; k++)
+      matrix_C_computed[k].sync_host();
 
-    cutlass::uncompress(matrix_A_uncompressed.host_ref(), matrix_A.host_ref(),
-                        matrix_E.host_ref(), problem_size.m(),
-                        problem_size.k());
+    // TODO: this is temporary. it will be removed after slicing can de
+    // reduction
+    //
+    // Reduce matrix_C_computed
+    //
+    CUTLASS_PRAGMA_UNROLL
+    for(int k = 1; k < kPartitionsK; k++) {
+      CUTLASS_PRAGMA_UNROLL
+      for(int m = 0; m < matrix_C_computed[0].extent().row(); m++){
+        CUTLASS_PRAGMA_UNROLL
+        for(int n = 0; n < matrix_C_computed[0].extent().column(); n++){
+          matrix_C_computed[0].at({m, n}) += matrix_C_computed[k].at({m, n});
+        }
+      }
+    }
 
     cutlass::reference::host::Gemm<ElementA, LayoutA, ElementB, LayoutB,
-                                   ElementC, LayoutC, ElementC, ElementC>
+                                   ElementC, LayoutC, ElementC, ElementC,
+                                   typename MmaCore::Operator>
         reference_gemm;
 
-    reference_gemm(problem_size, ElementC(alpha),
-                   matrix_A_uncompressed.host_view(), matrix_B.host_view(),
-                   ElementC(beta), matrix_C_reference.host_view());
+    reference_gemm(
+        problem_size, ElementC(alpha), matrix_A.host_view(),
+        matrix_B.host_view(), ElementC(beta), matrix_C_reference.host_view());
 
     bool passed = cutlass::reference::host::TensorEquals(
-        matrix_C_computed.host_view(), matrix_C_reference.host_view());
+        matrix_C_computed[0].host_view(), matrix_C_reference.host_view());
+
+    EXPECT_TRUE(passed);
 
-    EXPECT_TRUE(passed)
+    if (!passed) {
+      std::ofstream output("mma_pipelined_testbed_errors.txt");
+
+      output
         << "A:\n" << matrix_A.host_view() << "\n"
         << "B:\n" << matrix_B.host_view() << "\n"
-        << "E:\n" << matrix_E.host_view() << "\n"
         << "Reference:\n"
         << matrix_C_reference.host_view() << "\n"
         << "Computed:\n"
-        << matrix_C_computed.host_view() << "\n";
-
-    EXPECT_GT(cutlass::reference::host::TensorNorm(matrix_C_reference.host_view()), 0);
-    EXPECT_GT(cutlass::reference::host::TensorNorm(matrix_C_computed.host_view()), 0);
+        << matrix_C_computed[0].host_view() << "\n";
+    }
 
     return passed;
   }
 };
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 }  // namespace threadblock
 }  // namespace gemm
 }  // namespace test
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed_slicedk.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_testbed.h`

 * *Files 9% similar despite different names*

```diff
@@ -24,15 +24,14 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
 /*! \file
     \brief Unit testbed for kernel-level GEMM
 */
 
 #pragma once
 
 #include <fstream>
@@ -50,41 +49,38 @@
 #include "cutlass/util/tensor_view_io.h"
 
 #include "cutlass/util/distribution.h"
 #include "cutlass/util/reference/host/gemm.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 
-#include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
-#include "cutlass/transform/threadblock/predicated_tile_access_iterator.h"
+#include "cutlass/gemm/threadblock/default_mma_core_simt.h"
+#include "cutlass/gemm/threadblock/default_mma_core_sm75.h"
+#include "cutlass/gemm/threadblock/default_mma_core_sm70.h"
+#include "cutlass/transform/threadblock/predicated_tile_iterator.h"
+#include "cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h"
 #include "cutlass/cutlass.h"
 #include "cutlass/platform/platform.h"
 
 namespace test {
 namespace gemm {
 namespace threadblock {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename Mma>
-__global__ void kernel_multistage_mma(cutlass::gemm::GemmCoord problem_size,
+__global__ void kernel_mma(cutlass::gemm::GemmCoord problem_size,
                            typename Mma::IteratorA::Params params_A,
                            typename Mma::IteratorA::TensorRef ref_A,
                            typename Mma::IteratorB::Params params_B,
                            typename Mma::IteratorB::TensorRef ref_B,
-                           typename Mma::ElementC **ptr_C,
+                           typename Mma::ElementC *ptr_C,
                            typename Mma::LayoutC::Stride::Index ldc) {
   // Shared storage needed by threadblock-scoped matrix multiply-accumulate
-
-  // Dynamic shared memory base pointer
-  extern __shared__ int GemmSharedStorageBase[];
-
-  // Declare pointer to dynamic shared memory.
-  typename Mma::SharedStorage *shared_storage =
-      reinterpret_cast<typename Mma::SharedStorage *>(GemmSharedStorageBase);
+  __shared__ typename Mma::SharedStorage shared_storage;
 
   // Compute threadblock location
   cutlass::gemm::GemmCoord tb_tile_offset = {int(blockIdx.x), int(blockIdx.y),
                                              0};
 
   cutlass::MatrixCoord tb_offset_A{tb_tile_offset.m() * Mma::Shape::kM,
                                    tb_tile_offset.k()};
@@ -100,121 +96,129 @@
                                      {problem_size.m(), problem_size.k()},
                                      tb_thread_id, tb_offset_A);
 
   typename Mma::IteratorB iterator_B(params_B, ref_B.data(),
                                      {problem_size.k(), problem_size.n()},
                                      tb_thread_id, tb_offset_B);
 
-  int warp_id = __shfl_sync(0xffffffff, threadIdx.y, 0);
+  int warp_id = threadIdx.y;
   int lane_id = threadIdx.x;
 
-  int partitionsK_idx = warp_id / (Mma::WarpCount::kM * Mma::WarpCount::kN);
-
   // Construct thread-scoped matrix multiply
-  Mma mma(*shared_storage, tb_thread_id, warp_id, threadIdx.x);
+  Mma mma(shared_storage, tb_thread_id, warp_id, threadIdx.x);
 
   typename Mma::FragmentC accum;
 
   accum.clear();
 
   int gemm_k_iterations = (problem_size.k() + Mma::Shape::kK - 1) / Mma::Shape::kK;
 
   // Compute threadblock-scoped matrix multiply-add
   mma(gemm_k_iterations, accum, iterator_A, iterator_B, accum);
 
   // Output results
-  typename Mma::Operator::IteratorC iterator_C({ptr_C[partitionsK_idx], ldc}, lane_id);
+  typename Mma::Operator::IteratorC iterator_C({ptr_C, ldc}, lane_id);
 
-  int warp_idx_mn = warp_id % (Mma::WarpCount::kM * Mma::WarpCount::kN);
   iterator_C.add_tile_offset(
       {(tb_tile_offset.m() * Mma::WarpCount::kM) +
-           (warp_idx_mn % Mma::WarpCount::kM),
+           (warp_id % Mma::WarpCount::kM),
        (tb_tile_offset.n() * Mma::WarpCount::kN) +
-           (warp_idx_mn / Mma::WarpCount::kM)});
+           (warp_id / Mma::WarpCount::kM)});
 
   iterator_C.store(accum);
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Structure to compute the matrix product
 template <
     /// Threadblock-level matrix multiply-accumulate
-    typename MmaCore_>
+    typename MmaCore_,
+    /// Number of stages
+    int Stages = 2>
 struct Testbed {
   /// Threadblock-level GEMM implementation
   using MmaCore = MmaCore_;
   using ThreadblockShape = typename MmaCore::Shape;
   using WarpShape = typename MmaCore::WarpShape;
   using InstructionShape = typename MmaCore::InstructionShape;
   using ElementA = typename MmaCore::ElementA;
   using LayoutA = typename MmaCore::LayoutA;
   using ElementB = typename MmaCore::ElementB;
   using LayoutB = typename MmaCore::LayoutB;
   using ElementC = typename MmaCore::ElementC;
   using LayoutC = typename MmaCore::LayoutC;
-  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
-  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
-  using AccessTypeA = cutlass::Array<ElementA, ThreadMapA::kElementsPerAccess>;
-  using AccessTypeB = cutlass::Array<ElementB, ThreadMapB::kElementsPerAccess>;
-  static int const Stages = MmaCore::kStages;
-  static cutlass::arch::CacheOperation::Kind const CacheOpA =
-      MmaCore::kCacheOpA;
-  static cutlass::arch::CacheOperation::Kind const CacheOpB =
-      MmaCore::kCacheOpB;
+  static const int kStages = Stages;
 
   // Define iterators over tiles from the A operand
-  using IteratorA =
-      cutlass::transform::threadblock::PredicatedTileAccessIterator<
+  static const bool use_idp4a = cutlass::platform::is_same<ElementA, int8_t>::value && 
+                                cutlass::platform::is_same<ElementB, int8_t>::value && 
+                                cutlass::platform::is_same<typename MmaCore::OperatorClass, cutlass::arch::OpClassSimt>::value;
+
+  static const bool transposeA =  cutlass::platform::is_same< LayoutA, cutlass::layout::ColumnMajor >::value;
+  static const bool transposeB =  cutlass::platform::is_same< LayoutB, cutlass::layout::RowMajor >::value;
+
+  using IteratorA = typename cutlass::platform::conditional< use_idp4a,
+      cutlass::transform::threadblock::PredicatedTileIterator2dThreadTile<
+          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
+          ElementA, LayoutA, 1, typename MmaCore::IteratorThreadMapA, transposeA> ,
+        
+      cutlass::transform::threadblock::PredicatedTileIterator<
           cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
-          ElementA, LayoutA, 1, ThreadMapA, AccessTypeA>;
+          ElementA, LayoutA, 1, typename MmaCore::IteratorThreadMapA>
+      >::type;
 
   // Define iterators over tiles from the B operand
-  using IteratorB =
-      cutlass::transform::threadblock::PredicatedTileAccessIterator<
+  using IteratorB = typename cutlass::platform::conditional< use_idp4a,
+      cutlass::transform::threadblock::PredicatedTileIterator2dThreadTile<
           cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
-          ElementB, LayoutB, 0, ThreadMapB, AccessTypeB>;
-
-  // Define the threadblock-scoped pipelined matrix multiply
-  using Mma = cutlass::gemm::threadblock::MmaMultistage<
-      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA, CacheOpA,
-      IteratorB, typename MmaCore::SmemIteratorB, CacheOpB, ElementC, LayoutC,
-      typename MmaCore::MmaPolicy, Stages>;
+          ElementB, LayoutB, 0, typename MmaCore::IteratorThreadMapB, transposeB> ,
 
-  static int const kPartitionsK = MmaCore::MmaPolicy::kPartitionsK; 
+      cutlass::transform::threadblock::PredicatedTileIterator<
+          cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
+          ElementB, LayoutB, 0, typename MmaCore::IteratorThreadMapB>
+      >::type;
 
+  // Define MmaPipeline Single Stage
+  using MmaPipelineSingleStage =  cutlass::gemm::threadblock::MmaSingleStage<
+      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
+      IteratorB, typename MmaCore::SmemIteratorB, ElementC, LayoutC,
+      typename MmaCore::MmaPolicy>;
+
+  // Define MmaPipeline Two Stages
+  using MmaPipelineTwoStages =  cutlass::gemm::threadblock::MmaPipelined<
+      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
+      IteratorB, typename MmaCore::SmemIteratorB, ElementC, LayoutC,
+      typename MmaCore::MmaPolicy>;
+  
+  // Define the threadblock-scoped pipelined matrix multiply (Select between Single vs. Two stages)
+  using Mma = typename cutlass::platform::conditional<(kStages==1), MmaPipelineSingleStage, MmaPipelineTwoStages>::type;
   //
   // Data members
   //
 
   cutlass::HostTensor<ElementA, LayoutA> matrix_A;
   cutlass::HostTensor<ElementB, LayoutB> matrix_B;
-  cutlass::HostTensor<ElementC, LayoutC> matrix_C_computed[kPartitionsK];
+  cutlass::HostTensor<ElementC, LayoutC> matrix_C_computed;
   cutlass::HostTensor<ElementC, LayoutC> matrix_C_reference;
-  cutlass::HostTensor<ElementC*, cutlass::layout::PackedVectorLayout> matrix_C_pointers;
 
   cutlass::gemm::GemmCoord problem_size;
   float alpha, beta;
 
   //
   // Methods
   //
 
   /// Allocates workspace in device memory
-  Testbed(int m, int n, int k, float alpha_ = float(1), float beta_ = float(0))
+  Testbed(int m, int n, int k, float alpha_, float beta_)
       : problem_size(m, n, k), alpha(alpha_), beta(beta_) {
     matrix_A.reset(cutlass::make_Coord(m, k));
     matrix_B.reset(cutlass::make_Coord(k, n));
-
-    CUTLASS_PRAGMA_UNROLL
-    for(int k = 0; k < kPartitionsK; k++)
-      matrix_C_computed[k].reset(cutlass::make_Coord(m, n));
-
+    matrix_C_computed.reset(cutlass::make_Coord(m, n));
     matrix_C_reference.reset(cutlass::make_Coord(m, n), false);
-    matrix_C_pointers.reset(cutlass::Coord<1>(kPartitionsK));
   }
 
   /// Runs the test
   bool run(
       dim3 grid, dim3 block,
       cutlass::Distribution::Kind init_A = cutlass::Distribution::Uniform,
       cutlass::Distribution::Kind init_B = cutlass::Distribution::Uniform) {
@@ -270,116 +274,64 @@
     } else if (init_B == cutlass::Distribution::Identity) {
       cutlass::reference::host::TensorFillIdentity(matrix_B.host_view());
     } else {
       // TODO: Implement the rest
       return false;
     }
 
-    CUTLASS_PRAGMA_UNROLL
-    for(int k = 0; k < kPartitionsK; k++)
-      cutlass::reference::host::TensorFill(matrix_C_computed[k].host_view());
+    cutlass::reference::host::TensorFill(matrix_C_computed.host_view());
 
     cutlass::reference::host::TensorFill(matrix_C_reference.host_view());
 
     matrix_A.sync_device();
     matrix_B.sync_device();
-
-    CUTLASS_PRAGMA_UNROLL
-    for(int k = 0; k < kPartitionsK; k++)
-      matrix_C_computed[k].sync_device();
+    matrix_C_computed.sync_device();
 
     typename IteratorA::Params params_A(matrix_A.layout());
     typename IteratorB::Params params_B(matrix_B.layout());
 
-    CUTLASS_PRAGMA_UNROLL
-    for(int k = 0; k < kPartitionsK; k++)
-      matrix_C_pointers.at(cutlass::Coord<1>(k)) = matrix_C_computed[k].device_data();
-
-    matrix_C_pointers.sync_device();
-
-    cudaError_t result;
-
-    int smem_size = int(sizeof(typename Mma::SharedStorage));
-    if (smem_size >= (48 << 10)) {
-      result = cudaFuncSetAttribute(
-          test::gemm::threadblock::kernel_multistage_mma<Mma>,
-          cudaFuncAttributeMaxDynamicSharedMemorySize, smem_size);
-
-      EXPECT_EQ(result, cudaSuccess)
-          << " cudaFuncSetAttribute "
-             "cudaFuncAttributeMaxDynamicSharedMemorySize error: "
-          << cudaGetErrorString(result);
-
-      result = cudaFuncSetAttribute(
-          test::gemm::threadblock::kernel_multistage_mma<Mma>,
-          cudaFuncAttributePreferredSharedMemoryCarveout, 100);
-
-      EXPECT_EQ(result, cudaSuccess)
-          << " cudaFuncSetAttribute "
-             "cudaFuncAttributePreferredSharedMemoryCarveout error: "
-          << cudaGetErrorString(result);
-    }
-
-    test::gemm::threadblock::kernel_multistage_mma<Mma><<<grid, block, smem_size, 0>>>(
+    test::gemm::threadblock::kernel_mma<Mma><<<grid, block>>>(
         problem_size, params_A, matrix_A.device_ref(), params_B,
-        matrix_B.device_ref(), matrix_C_pointers.device_data(),
-        matrix_C_computed[0].layout().stride(0));
+        matrix_B.device_ref(), matrix_C_computed.device_data(),
+        matrix_C_computed.layout().stride(0));
 
     //
     // Check error code
     //
 
-    result = cudaDeviceSynchronize();
+    cudaError_t result = cudaDeviceSynchronize();
     EXPECT_EQ(result, cudaSuccess)
         << " kernel error: " << cudaGetErrorString(result);
 
-    CUTLASS_PRAGMA_UNROLL
-    for(int k = 0; k < kPartitionsK; k++)
-      matrix_C_computed[k].sync_host();
-
-    // TODO: this is temporary. it will be removed after slicing can de
-    // reduction
-    //
-    // Reduce matrix_C_computed
-    //
-    CUTLASS_PRAGMA_UNROLL
-    for(int k = 1; k < kPartitionsK; k++) {
-      CUTLASS_PRAGMA_UNROLL
-      for(int m = 0; m < matrix_C_computed[0].extent().row(); m++){
-        CUTLASS_PRAGMA_UNROLL
-        for(int n = 0; n < matrix_C_computed[0].extent().column(); n++){
-          matrix_C_computed[0].at({m, n}) += matrix_C_computed[k].at({m, n});
-        }
-      }
-    }
+    matrix_C_computed.sync_host();
 
     cutlass::reference::host::Gemm<ElementA, LayoutA, ElementB, LayoutB,
                                    ElementC, LayoutC, ElementC, ElementC,
                                    typename MmaCore::Operator>
         reference_gemm;
 
     reference_gemm(
         problem_size, ElementC(alpha), matrix_A.host_view(),
         matrix_B.host_view(), ElementC(beta), matrix_C_reference.host_view());
 
     bool passed = cutlass::reference::host::TensorEquals(
-        matrix_C_computed[0].host_view(), matrix_C_reference.host_view());
+        matrix_C_computed.host_view(), matrix_C_reference.host_view());
 
     EXPECT_TRUE(passed);
 
     if (!passed) {
-      std::ofstream output("mma_multistage_testbed_errors.txt");
+      std::ofstream output("mma_pipelined_testbed_errors.txt");
 
       output
         << "A:\n" << matrix_A.host_view() << "\n"
         << "B:\n" << matrix_B.host_view() << "\n"
         << "Reference:\n"
         << matrix_C_reference.host_view() << "\n"
         << "Computed:\n"
-        << matrix_C_computed[0].host_view() << "\n";
+        << matrix_C_computed.host_view() << "\n";
     }
 
     return passed;
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_simt.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_simt.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_singlestage_wmma_sm75.cu`

 * *Files 4% similar despite different names*

```diff
@@ -33,31 +33,31 @@
 */
 #include "cutlass/arch/wmma.h"
 
 #ifdef CUTLASS_ARCH_WMMA_SM75_ENABLED
 #include "mma_pipelined_testbed.h"
 #include "cutlass/gemm/threadblock/default_mma_core_wmma.h"
 
-/// All tests use double-buffered (kStages=2) mma pipeline for the gemm mainloop
-/// Test name format: SM[arch]_gemm_threadblock_wmma_tensor_op_[alayout]_[blayout]_[clayout]_[atype].[threadblock_shape]_[warp_shape]_[instruction_shape]
+/// All tests use single staged (kStages=1) mma pipeline for the gemm mainloop
+/// Test name format: SM[arch]_gemm_threadblock_singlestage_wmma_tensor_op_[alayout]_[blayout]_[clayout]_[atype].[threadblock_shape]_[warp_shape]_[instruction_shape]
 
 /////////////////////////////////////////////////////////////////////////
-///       Integer (s8 and u8) WMMA threadblock level tests          /////
+///       Integer (s8 and u8) WMMA threadblock level tests          ////
 /////////////////////////////////////////////////////////////////////////
 
 #if defined(CUTLASS_ARCH_INTEGER_MATRIX_MULTIPLY_ENABLED)
-TEST(SM75_gemm_threadblock_wmma_tensor_op_row_col_row_s8, 64x64x32_64x64x32_16x16x16) {
+TEST(SM75_gemm_threadblock_singlestage_wmma_tensor_op_row_col_row_s8, 64x64x32_64x64x32_16x16x16) {
  
   using ElementA = int8_t;
   using LayoutA = cutlass::layout::RowMajor;
   using ElementB = int8_t;
   using LayoutB = cutlass::layout::ColumnMajor;
   using ElementC = int32_t;
   using LayoutC = cutlass::layout::RowMajor;
-  static const int kStages = 2; 
+  static const int kStages = 1; 
 
   cutlass::gemm::GemmCoord problem_size(64, 64, 128);
 
   using ThreadblockShape = cutlass::gemm::GemmShape<64, 64, 32>;
   using WarpShape = cutlass::gemm::GemmShape<64, 64, 32>;
   using InstructionShape = cutlass::gemm::GemmShape<16, 16, 16>;
 
@@ -76,23 +76,23 @@
   dim3 block(32, 1, 1);
 
   test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
                                             problem_size.k(), alpha, beta)
       .run(grid, block);
 }
 
-TEST(SM75_gemm_threadblock_wmma_tensor_op_row_col_row_s8, 64x64x64_64x64x64_16x16x16) {
+TEST(SM75_gemm_threadblock_singlestage_wmma_tensor_op_row_col_row_s8, 64x64x64_64x64x64_16x16x16) {
  
   using ElementA = int8_t;
   using LayoutA = cutlass::layout::RowMajor;
   using ElementB = int8_t;
   using LayoutB = cutlass::layout::ColumnMajor;
   using ElementC = int32_t;
   using LayoutC = cutlass::layout::RowMajor;
-  static const int kStages = 2; 
+  static const int kStages = 1; 
 
   cutlass::gemm::GemmCoord problem_size(64, 64, 128);
 
   using ThreadblockShape = cutlass::gemm::GemmShape<64, 64, 64>;
   using WarpShape = cutlass::gemm::GemmShape<64, 64, 64>;
   using InstructionShape = cutlass::gemm::GemmShape<16, 16, 16>;
 
@@ -112,58 +112,58 @@
 
   test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
                                             problem_size.k(), alpha, beta)
       .run(grid, block);
 }
 
 
-TEST(SM75_gemm_threadblock_wmma_tensor_op_col_row_row_s8, 64x64x32_64x64x32_16x16x16) {
+TEST(SM75_gemm_threadblock_singlestage_wmma_tensor_op_col_row_row_s8, 64x64x32_64x64x32_16x16x16) {
  
   using ElementA = int8_t;
   using LayoutA = cutlass::layout::ColumnMajor;
   using ElementB = int8_t;
   using LayoutB = cutlass::layout::RowMajor;
   using ElementC = int32_t;
   using LayoutC = cutlass::layout::RowMajor;
-  static const int kStages = 2; 
+  static const int kStages = 1; 
 
   cutlass::gemm::GemmCoord problem_size(64, 64, 128);
 
   using ThreadblockShape = cutlass::gemm::GemmShape<64, 64, 32>;
   using WarpShape = cutlass::gemm::GemmShape<64, 64, 32>;
   using InstructionShape = cutlass::gemm::GemmShape<16, 16, 16>;
 
   float alpha = 1.f;
   float beta = 0.0f;
 
   // Define the MmaCore components
   using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
-      ThreadblockShape, WarpShape, InstructionShape,
+      ThreadblockShape, WarpShape, InstructionShape, 
       ElementA, LayoutA,
       ElementB, LayoutB, 
       ElementC, LayoutC,
       cutlass::arch::OpClassWmmaTensorOp, kStages>;
 
   dim3 grid(1, 1);
   dim3 block(32, 1, 1);
 
   test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
                                             problem_size.k(), alpha, beta)
       .run(grid, block);
 }
 
-TEST(SM75_gemm_threadblock_wmma_tensor_op_col_row_row_s8, 64x64x64_64x64x64_16x16x16) {
+TEST(SM75_gemm_threadblock_singlestage_wmma_tensor_op_col_row_row_s8, 64x64x64_64x64x64_16x16x16) {
  
   using ElementA = int8_t;
   using LayoutA = cutlass::layout::ColumnMajor;
   using ElementB = int8_t;
   using LayoutB = cutlass::layout::RowMajor;
   using ElementC = int32_t;
   using LayoutC = cutlass::layout::RowMajor;
-  static const int kStages = 2; 
+  static const int kStages = 1; 
 
   cutlass::gemm::GemmCoord problem_size(64, 64, 128);
 
   using ThreadblockShape = cutlass::gemm::GemmShape<64, 64, 64>;
   using WarpShape = cutlass::gemm::GemmShape<64, 64, 64>;
   using InstructionShape = cutlass::gemm::GemmShape<16, 16, 16>;
 
@@ -190,22 +190,22 @@
 
 ////////////////////////////////////////////////////////////////////////
 ///      SUBBYTE (s4 and b1) WMMA threadblock level tests          ////
 ///////////////////////////////////////////////////////////////////////
 
 #if defined(CUTLASS_SUBBYTE_INTEGER_MATRIX_MULTIPLY_ENABLED)
 
-TEST(SM75_gemm_threadblock_wmma_tensor_op_row_col_row_s4, 64x64x128_64x64x128_8x8x32) {
+TEST(SM75_gemm_threadblock_singlestage_wmma_tensor_op_row_col_row_s4, 64x64x128_64x64x128_8x8x32) {
   using ElementA = cutlass::int4b_t;
   using LayoutA = cutlass::layout::RowMajor;
   using ElementB = cutlass::int4b_t;
   using LayoutB = cutlass::layout::ColumnMajor;
   using ElementC = int32_t;
   using LayoutC = cutlass::layout::RowMajor;
-  static const int kStages = 2; 
+  static const int kStages = 1; 
 
   cutlass::gemm::GemmCoord problem_size(64, 64, 128);
 
   using ThreadBlockShape = cutlass::gemm::GemmShape<64, 64, 128>;
   using WarpShape = cutlass::gemm::GemmShape<64, 64, 128>;
   using InstructionShape = cutlass::gemm::GemmShape<8, 8, 32>;
 
@@ -225,22 +225,22 @@
 
   test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
                                             problem_size.k(), alpha, beta)
       .run(grid, block);
 }
 
 
-TEST(SM75_gemm_threadblock_wmma_tensor_op_row_col_col_s4, 64x64x64_64x64x64_8x8x32) {
+TEST(SM75_gemm_threadblock_singlestage_wmma_tensor_op_row_col_col_s4, 64x64x64_64x64x64_8x8x32) {
   using ElementA = cutlass::int4b_t;
   using LayoutA = cutlass::layout::RowMajor;
   using ElementB = cutlass::int4b_t;
   using LayoutB = cutlass::layout::ColumnMajor;
   using ElementC = int32_t;
   using LayoutC = cutlass::layout::ColumnMajor;
-  static const int kStages = 2;
+  static const int kStages = 1; 
 
   cutlass::gemm::GemmCoord problem_size(64, 64, 64);
 
   using ThreadBlockShape = cutlass::gemm::GemmShape<64, 64, 64>;
   using WarpShape = cutlass::gemm::GemmShape<64, 64, 64>;
   using InstructionShape = cutlass::gemm::GemmShape<8, 8, 32>;
 
@@ -259,22 +259,22 @@
   dim3 block(32, 1, 1);
 
   test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
                                             problem_size.k(), alpha, beta)
       .run(grid, block);
 }
 
-TEST(SM75_gemm_threadblock_wmma_tensor_op_row_col_row_b1, 64x64x512_64x64x512_8x8x128) {
+TEST(SM75_gemm_threadblock_singlestage_wmma_tensor_op_row_col_row_b1, 64x64x512_64x64x512_8x8x128) {
   using ElementA = cutlass::uint1b_t;
   using LayoutA = cutlass::layout::RowMajor;
   using ElementB = cutlass::uint1b_t;
   using LayoutB = cutlass::layout::ColumnMajor;
   using ElementC = int32_t;
   using LayoutC = cutlass::layout::RowMajor;
-  static const int kStages = 2; 
+  static const int kStages = 1; 
 
   cutlass::gemm::GemmCoord problem_size(64, 64, 2048);
 
   using ThreadBlockShape = cutlass::gemm::GemmShape<64, 64, 512>;
   using WarpShape = cutlass::gemm::GemmShape<64, 64, 512>;
   using InstructionShape = cutlass::gemm::GemmShape<8, 8, 128>;
 
@@ -294,22 +294,22 @@
   dim3 block(32, 1, 1);
 
   test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
                                             problem_size.k(), alpha, beta)
       .run(grid, block);
 }
 
-TEST(SM75_gemm_threadblock_wmma_tensor_op_row_col_col_b1, 64x64x512_64x64x512_8x8x128) {
+TEST(SM75_gemm_threadblock_singlestage_wmma_tensor_op_row_col_col_b1, 64x64x512_64x64x512_8x8x128) {
   using ElementA = cutlass::uint1b_t;
   using LayoutA = cutlass::layout::RowMajor;
   using ElementB = cutlass::uint1b_t;
   using LayoutB = cutlass::layout::ColumnMajor;
   using ElementC = int32_t;
   using LayoutC = cutlass::layout::ColumnMajor;
-  static const int kStages = 2; 
+  static const int kStages = 1; 
 
   cutlass::gemm::GemmCoord problem_size(64, 64, 2048);
 
   using ThreadBlockShape = cutlass::gemm::GemmShape<64, 64, 512>;
   using WarpShape = cutlass::gemm::GemmShape<64, 64, 512>;
   using InstructionShape = cutlass::gemm::GemmShape<8, 8, 128>;
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_singlestage_wmma_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_singlestage_wmma_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_complex_sm90.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_complex_sm90.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_gaussian_complex_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm90.cu`

 * *Files 27% similar despite different names*

```diff
@@ -26,262 +26,181 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file 
 
-    \brief Unit tests for thread-level GEMM
+    \brief Unit tests for thread-level GEMM with Hopper FP64
 */
 
-#include "cutlass/cutlass.h"
 #include "../../common/cutlass_unit_test.h"
 
 #include "cutlass/aligned_buffer.h"
 #include "cutlass/half.h"
 
-#include "cutlass/gemm/warp/default_mma_complex_tensor_op.h"
+#include "cutlass/gemm/warp/default_mma_tensor_op.h"
 
 #include "cutlass/core_io.h"
 #include "cutlass/util/host_tensor.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/gemm.h"
 
 #include "testbed.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
-///////////////////////////////////////////////////////////////////////////////////////////////////
-
-TEST(SM80_warp_gemm_gaussian_complex_tensor_op, 8x8x4_8x8x4_nt) {
+TEST(SM90_warp_gemm_tensor_op_congruous_f64, 16x16x4_16x16x4_16x8x4) {
+  using Shape = cutlass::gemm::GemmShape<16, 16, 4>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 4>;
+  using Element = double;
+  using ElementC = double;
+  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous64b;
+  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous64b;
 
-  using Shape = cutlass::gemm::GemmShape<8, 8, 4>;
-  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
-  
-  using Element = cutlass::complex<double>;
-  using ElementC = cutlass::complex<double>;
-
-  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous128b;
-  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous128b;
-
-  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
-    Shape, 
-    InstructionShape, 
-    Element, 
-    LayoutA, 
-    Element, 
-    LayoutB, 
-    ElementC,
-    cutlass::layout::RowMajor,
-    cutlass::ComplexTransform::kNone,
-    cutlass::ComplexTransform::kNone,
-    cutlass::arch::OpMultiplyAddGaussianComplex
-  >::Type;
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaTensorOp<
+      Shape, InstructionShape, Element, LayoutA, Element, LayoutB, ElementC,
+      cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>::Type;
 
-  test::gemm::warp::TestbedComplex<MmaTensorOp, cutlass::gemm::GemmShape<8, 8, 4> >().run();
+  test::gemm::warp::Testbed<MmaTensorOp,
+                            cutlass::gemm::GemmShape<16, 16, 4> >()
+      .run();
 }
 
-TEST(SM80_warp_gemm_gaussian_complex_tensor_op, 16x16x4_8x8x4_nt) {
+////////////////////////////////////////////////////////////////////////////////
 
-  using Shape = cutlass::gemm::GemmShape<16, 16, 4>;
-  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
-  
-  using Element = cutlass::complex<double>;
-  using ElementC = cutlass::complex<double>;
-
-  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous128b;
-  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous128b;
-
-  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
-    Shape, 
-    InstructionShape, 
-    Element, 
-    LayoutA, 
-    Element, 
-    LayoutB, 
-    ElementC,
-    cutlass::layout::RowMajor,
-    cutlass::ComplexTransform::kNone,
-    cutlass::ComplexTransform::kNone,
-    cutlass::arch::OpMultiplyAddGaussianComplex
-  >::Type;
+TEST(SM90_warp_gemm_tensor_op_congruous_f64, 32x16x4_32x16x4_16x8x4) {
+  using Shape = cutlass::gemm::GemmShape<32, 16, 4>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 4>;
+  using Element = double;
+  using ElementC = double;
+  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous64b;
+  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous64b;
 
-  test::gemm::warp::TestbedComplex<MmaTensorOp, cutlass::gemm::GemmShape<16, 16, 4> >().run();
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaTensorOp<
+      Shape, InstructionShape, Element, LayoutA, Element, LayoutB, ElementC,
+      cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>::Type;
+
+  test::gemm::warp::Testbed<MmaTensorOp,
+                            cutlass::gemm::GemmShape<32, 16, 4> >()
+      .run();
 }
 
+////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_warp_gemm_gaussian_complex_tensor_op, 16x32x4_8x8x4_nt) {
+TEST(SM90_warp_gemm_tensor_op_congruous_f64, 32x32x4_32x32x4_16x8x4) {
+  using Shape = cutlass::gemm::GemmShape<32, 32, 4>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 4>;
+  using Element = double;
+  using ElementC = double;
+  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous64b;
+  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous64b;
 
-  using Shape = cutlass::gemm::GemmShape<16, 32, 4>;
-  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
-  
-  using Element = cutlass::complex<double>;
-  using ElementC = cutlass::complex<double>;
-
-  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous128b;
-  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous128b;
-
-  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
-    Shape, 
-    InstructionShape, 
-    Element, 
-    LayoutA, 
-    Element, 
-    LayoutB, 
-    ElementC,
-    cutlass::layout::RowMajor,
-    cutlass::ComplexTransform::kNone,
-    cutlass::ComplexTransform::kNone,
-    cutlass::arch::OpMultiplyAddGaussianComplex
-  >::Type;
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaTensorOp<
+      Shape, InstructionShape, Element, LayoutA, Element, LayoutB, ElementC,
+      cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>::Type;
 
-  test::gemm::warp::TestbedComplex<MmaTensorOp, cutlass::gemm::GemmShape<16, 32, 4> >().run();
+  test::gemm::warp::Testbed<MmaTensorOp,
+                            cutlass::gemm::GemmShape<32, 32, 4> >()
+      .run();
 }
 
-TEST(SM80_warp_gemm_gaussian_complex_tensor_op, 32x16x4_8x8x4_nt) {
+////////////////////////////////////////////////////////////////////////////////
 
-  using Shape = cutlass::gemm::GemmShape<32, 16, 4>;
-  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
-  
-  using Element = cutlass::complex<double>;
-  using ElementC = cutlass::complex<double>;
-
-  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous128b;
-  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous128b;
-
-  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
-    Shape, 
-    InstructionShape, 
-    Element, 
-    LayoutA, 
-    Element, 
-    LayoutB, 
-    ElementC,
-    cutlass::layout::RowMajor,
-    cutlass::ComplexTransform::kNone,
-    cutlass::ComplexTransform::kNone,
-    cutlass::arch::OpMultiplyAddGaussianComplex
-  >::Type;
+TEST(SM90_warp_gemm_tensor_op_congruous_f64, 32x64x4_32x64x4_16x8x4) {
+  using Shape = cutlass::gemm::GemmShape<32, 64, 4>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 4>;
+  using Element = double;
+  using ElementC = double;
+  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous64b;
+  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous64b;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaTensorOp<
+      Shape, InstructionShape, Element, LayoutA, Element, LayoutB, ElementC,
+      cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>::Type;
 
-  test::gemm::warp::TestbedComplex<MmaTensorOp, cutlass::gemm::GemmShape<32, 16, 4> >().run();
+  test::gemm::warp::Testbed<MmaTensorOp,
+                            cutlass::gemm::GemmShape<32, 64, 4> >()
+      .run();
 }
 
-TEST(SM80_warp_gemm_gaussian_complex_tensor_op, 32x32x4_8x8x4_nt) {
+////////////////////////////////////////////////////////////////////////////////
 
-  using Shape = cutlass::gemm::GemmShape<32, 32, 4>;
-  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
-  
-  using Element = cutlass::complex<double>;
-  using ElementC = cutlass::complex<double>;
-
-  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous128b;
-  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous128b;
-
-  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
-    Shape, 
-    InstructionShape, 
-    Element, 
-    LayoutA, 
-    Element, 
-    LayoutB, 
-    ElementC,
-    cutlass::layout::RowMajor,
-    cutlass::ComplexTransform::kNone,
-    cutlass::ComplexTransform::kNone,
-    cutlass::arch::OpMultiplyAddGaussianComplex
-  >::Type;
+TEST(SM90_warp_gemm_tensor_op_crosswise_f64, 16x16x16_16x16x16_16x8x4) {
+  using Shape = cutlass::gemm::GemmShape<16, 16, 16>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 4>;
+  using Element = double;
+  using ElementC = double;
+  using LayoutA = cutlass::layout::RowMajorTensorOpMultiplicand64bCrosswise;
+  using LayoutB = cutlass::layout::ColumnMajorTensorOpMultiplicand64bCrosswise;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaTensorOp<
+      Shape, InstructionShape, Element, LayoutA, Element, LayoutB, ElementC,
+      cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>::Type;
 
-  test::gemm::warp::TestbedComplex<MmaTensorOp, cutlass::gemm::GemmShape<32, 32, 4> >().run();
+  test::gemm::warp::Testbed<MmaTensorOp,
+                            cutlass::gemm::GemmShape<16, 16, 16> >()
+      .run();
 }
 
-TEST(SM80_warp_gemm_gaussian_complex_tensor_op, 32x32x4_8x8x4_nh) {
+////////////////////////////////////////////////////////////////////////////////
 
-  using Shape = cutlass::gemm::GemmShape<32, 32, 4>;
-  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
-  
-  using Element = cutlass::complex<double>;
-  using ElementC = cutlass::complex<double>;
-
-  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous128b;
-  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous128b;
-
-  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
-    Shape, 
-    InstructionShape, 
-    Element, 
-    LayoutA, 
-    Element, 
-    LayoutB, 
-    ElementC,
-    cutlass::layout::RowMajor,
-    cutlass::ComplexTransform::kNone,
-    cutlass::ComplexTransform::kConjugate,
-    cutlass::arch::OpMultiplyAddGaussianComplex
-  >::Type;
+TEST(SM90_warp_gemm_tensor_op_crosswise_f64, 32x32x16_32x32x16_16x8x4) {
+  using Shape = cutlass::gemm::GemmShape<32, 32, 16>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 4>;
+  using Element = double;
+  using ElementC = double;
+  using LayoutA = cutlass::layout::RowMajorTensorOpMultiplicand64bCrosswise;
+  using LayoutB = cutlass::layout::ColumnMajorTensorOpMultiplicand64bCrosswise;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaTensorOp<
+      Shape, InstructionShape, Element, LayoutA, Element, LayoutB, ElementC,
+      cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>::Type;
 
-  test::gemm::warp::TestbedComplex<MmaTensorOp, cutlass::gemm::GemmShape<32, 32, 4> >().run();
+  test::gemm::warp::Testbed<MmaTensorOp,
+                            cutlass::gemm::GemmShape<32, 32, 16> >()
+      .run();
 }
 
-TEST(SM80_warp_gemm_gaussian_complex_tensor_op, 32x32x4_8x8x4_ct) {
+////////////////////////////////////////////////////////////////////////////////
 
-  using Shape = cutlass::gemm::GemmShape<32, 32, 4>;
-  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
-  
-  using Element = cutlass::complex<double>;
-  using ElementC = cutlass::complex<double>;
-
-  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous128b;
-  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous128b;
-
-  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
-    Shape, 
-    InstructionShape, 
-    Element, 
-    LayoutA, 
-    Element, 
-    LayoutB, 
-    ElementC,
-    cutlass::layout::RowMajor,
-    cutlass::ComplexTransform::kConjugate,
-    cutlass::ComplexTransform::kNone,
-    cutlass::arch::OpMultiplyAddGaussianComplex
-  >::Type;
+TEST(SM90_warp_gemm_tensor_op_crosswise_f64, 64x32x16_64x32x16_16x8x4) {
+  using Shape = cutlass::gemm::GemmShape<64, 32, 16>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 4>;
+  using Element = double;
+  using ElementC = double;
+  using LayoutA = cutlass::layout::RowMajorTensorOpMultiplicand64bCrosswise;
+  using LayoutB = cutlass::layout::ColumnMajorTensorOpMultiplicand64bCrosswise;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaTensorOp<
+      Shape, InstructionShape, Element, LayoutA, Element, LayoutB, ElementC,
+      cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>::Type;
 
-  test::gemm::warp::TestbedComplex<MmaTensorOp, cutlass::gemm::GemmShape<32, 32, 4> >().run();
+  test::gemm::warp::Testbed<MmaTensorOp,
+                            cutlass::gemm::GemmShape<64, 32, 16> >()
+      .run();
 }
 
-///////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_warp_gemm_gaussian_complex_tensor_op, 16x16x4_8x8x4_tn) {
+TEST(SM90_warp_gemm_tensor_op_crosswise_f64, 32x64x16_32x64x16_16x8x4) {
+  using Shape = cutlass::gemm::GemmShape<32, 64, 16>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 4>;
+  using Element = double;
+  using ElementC = double;
+  using LayoutA = cutlass::layout::RowMajorTensorOpMultiplicand64bCrosswise;
+  using LayoutB = cutlass::layout::ColumnMajorTensorOpMultiplicand64bCrosswise;
 
-  using Shape = cutlass::gemm::GemmShape<16, 16, 4>;
-  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
-  
-  using Element = cutlass::complex<double>;
-  using ElementC = cutlass::complex<double>;
-
-  using LayoutA = cutlass::layout::RowMajorTensorOpMultiplicandCrosswise128x4;
-  using LayoutB = cutlass::layout::ColumnMajorTensorOpMultiplicandCrosswise128x4;
-
-  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaComplexTensorOp<
-    Shape, 
-    InstructionShape, 
-    Element, 
-    LayoutA, 
-    Element, 
-    LayoutB, 
-    ElementC,
-    cutlass::layout::RowMajor,
-    cutlass::ComplexTransform::kNone,
-    cutlass::ComplexTransform::kNone,
-    cutlass::arch::OpMultiplyAddGaussianComplex
-  >::Type;
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaTensorOp<
+      Shape, InstructionShape, Element, LayoutA, Element, LayoutB, ElementC,
+      cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>::Type;
 
-  test::gemm::warp::TestbedComplex<MmaTensorOp, cutlass::gemm::GemmShape<16, 16, 4> >().run();
+  test::gemm::warp::Testbed<MmaTensorOp,
+                            cutlass::gemm::GemmShape<32, 64, 16> >()
+      .run();
 }
-///////////////////////////////////////////////////////////////////////////////////////////////////
-
-#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+////////////////////////////////////////////////////////////////////////////////
 
+#endif // if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm50.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm60.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm61.cu`

 * *Files 21% similar despite different names*

```diff
@@ -36,105 +36,163 @@
 
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/gemm/warp/mma_simt.h"
 
 #include "testbed.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-
-TEST(SM60_warp_gemm_f16_col_row, 8x4x1_1x1x1) {
+TEST(SM61_warp_gemm_int8_col_row, col_row_8x4x8_1x1x4) {
 
   using Policy = cutlass::gemm::warp::MmaSimtPolicy<
     cutlass::MatrixShape<8, 4>,
     cutlass::layout::ColumnMajorInterleaved<2>,
-    cutlass::gemm::GemmShape<1, 1, 1>
+    cutlass::gemm::GemmShape<1, 1, 4>
   >;
 
   using Mma = cutlass::gemm::warp::MmaSimt<
     cutlass::gemm::GemmShape<8, 4, 8>,
-    cutlass::half_t,
-    cutlass::layout::ColumnMajor,
-    cutlass::half_t,
-    cutlass::layout::RowMajor,
-    cutlass::half_t,
+    int8_t,
+    cutlass::layout::ColumnMajorInterleaved<4>,
+    int8_t,
+    cutlass::layout::RowMajorInterleaved<4>,
+    int,
     cutlass::layout::ColumnMajor,
     Policy
   >;
 
-  test::gemm::warp::Testbed<Mma, cutlass::gemm::GemmShape<128, 128, 8> >().run();
+  test::gemm::warp::Testbed<Mma, cutlass::gemm::GemmShape<8, 4, 8> >().run();
 }
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-TEST(SM60_warp_gemm_f16_col_row, 16x8x1_2x2x1) {
+TEST(SM61_warp_gemm_int8_col_row, col_row_8x4x4_1x1x4) {
 
   using Policy = cutlass::gemm::warp::MmaSimtPolicy<
     cutlass::MatrixShape<8, 4>,
     cutlass::layout::ColumnMajorInterleaved<2>,
-    cutlass::gemm::GemmShape<2, 2, 1>
+    cutlass::gemm::GemmShape<1, 1, 4>
   >;
 
   using Mma = cutlass::gemm::warp::MmaSimt<
-    cutlass::gemm::GemmShape<16, 8, 8>,
-    cutlass::half_t,
-    cutlass::layout::ColumnMajor,
-    cutlass::half_t,
-    cutlass::layout::RowMajor,
-    cutlass::half_t,
+    cutlass::gemm::GemmShape<8, 4, 8>,
+    int8_t,
+    cutlass::layout::ColumnMajorInterleaved<4>,
+    int8_t,
+    cutlass::layout::RowMajorInterleaved<4>,
+    int,
     cutlass::layout::ColumnMajor,
     Policy
   >;
 
-  test::gemm::warp::Testbed<Mma, cutlass::gemm::GemmShape<128, 128, 8> >().run();
+  test::gemm::warp::Testbed<Mma, cutlass::gemm::GemmShape<128, 64, 8> >().run();
 }
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-TEST(SM60_warp_gemm_f16_col_row, 32x16x1_4x4x1) {
+TEST(SM61_warp_gemm_int8_col_row, col_row_16x4x4_2x1x4) {
 
   using Policy = cutlass::gemm::warp::MmaSimtPolicy<
     cutlass::MatrixShape<8, 4>,
     cutlass::layout::ColumnMajorInterleaved<2>,
-    cutlass::gemm::GemmShape<4, 4, 1>
+    cutlass::gemm::GemmShape<2, 1, 4>
   >;
 
   using Mma = cutlass::gemm::warp::MmaSimt<
-    cutlass::gemm::GemmShape<32, 16, 8>,
-    cutlass::half_t,
+    cutlass::gemm::GemmShape<16, 4, 4>,
+    int8_t,
+    cutlass::layout::ColumnMajorInterleaved<4>,
+    int8_t,
+    cutlass::layout::RowMajorInterleaved<4>,
+    int,
     cutlass::layout::ColumnMajor,
-    cutlass::half_t,
-    cutlass::layout::RowMajor,
-    cutlass::half_t,
+    Policy
+  >;
+
+  test::gemm::warp::Testbed<Mma, cutlass::gemm::GemmShape<16, 4, 4> >().run();
+}
+
+TEST(SM61_warp_gemm_int8_col_row, col_row_16x4x4_2x2x4) {
+
+  using Policy = cutlass::gemm::warp::MmaSimtPolicy<
+    cutlass::MatrixShape<8, 4>,
+    cutlass::layout::ColumnMajorInterleaved<2>,
+    cutlass::gemm::GemmShape<2, 2, 4>
+  >;
+
+  using Mma = cutlass::gemm::warp::MmaSimt<
+    cutlass::gemm::GemmShape<16, 8, 4>,
+    int8_t,
+    cutlass::layout::ColumnMajorInterleaved<4>,
+    int8_t,
+    cutlass::layout::RowMajorInterleaved<4>,
+    int,
     cutlass::layout::ColumnMajor,
     Policy
   >;
 
-  test::gemm::warp::Testbed<Mma, cutlass::gemm::GemmShape<128, 128, 8> >().run();
+  test::gemm::warp::Testbed<Mma, cutlass::gemm::GemmShape<16, 8, 4> >().run();
 }
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+TEST(SM61_warp_gemm_int8_col_row, col_row_32x16x4_4x4x4) {
+
+  using Policy = cutlass::gemm::warp::MmaSimtPolicy<
+    cutlass::MatrixShape<8, 4>,
+    cutlass::layout::ColumnMajorInterleaved<2>,
+    cutlass::gemm::GemmShape<4, 4, 4>
+  >;
+
+  using Mma = cutlass::gemm::warp::MmaSimt<
+    cutlass::gemm::GemmShape<32, 16, 16>,
+    int8_t,
+    cutlass::layout::ColumnMajorInterleaved<4>,
+    int8_t,
+    cutlass::layout::RowMajorInterleaved<4>,
+    int,
+    cutlass::layout::ColumnMajor,
+    Policy
+  >;
+
+  test::gemm::warp::Testbed<Mma, cutlass::gemm::GemmShape<128, 64, 16> >().run();
+}
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM60_warp_gemm_f16_col_row, 64x16x1_8x4x1) {
+TEST(SM61_warp_gemm_int8_col_row, col_row_128x64x4_16x16x4) {
 
   using Policy = cutlass::gemm::warp::MmaSimtPolicy<
     cutlass::MatrixShape<8, 4>,
     cutlass::layout::ColumnMajorInterleaved<2>,
-    cutlass::gemm::GemmShape<8, 8, 1>
+    cutlass::gemm::GemmShape<16, 16, 4>
   >;
 
   using Mma = cutlass::gemm::warp::MmaSimt<
-    cutlass::gemm::GemmShape<64, 32, 8>,
-    cutlass::half_t,
+    cutlass::gemm::GemmShape<128, 64, 4>,
+    int8_t,
+    cutlass::layout::ColumnMajorInterleaved<4>,
+    int8_t,
+    cutlass::layout::RowMajorInterleaved<4>,
+    int,
     cutlass::layout::ColumnMajor,
-    cutlass::half_t,
-    cutlass::layout::RowMajor,
-    cutlass::half_t,
+    Policy
+  >;
+
+  test::gemm::warp::Testbed<Mma, cutlass::gemm::GemmShape<128, 64, 4> >().run();
+}
+
+TEST(SM61_warp_gemm_int8_col_row, col_row_64x64x4_4x4x4) {
+
+  using Policy = cutlass::gemm::warp::MmaSimtPolicy<
+    cutlass::MatrixShape<8, 4>,
+    cutlass::layout::ColumnMajorInterleaved<2>,
+    cutlass::gemm::GemmShape<4, 4, 4>
+  >;
+
+  using Mma = cutlass::gemm::warp::MmaSimt<
+    cutlass::gemm::GemmShape<64, 64, 8>,
+    int8_t,
+    cutlass::layout::ColumnMajorInterleaved<4>,
+    int8_t,
+    cutlass::layout::RowMajorInterleaved<4>,
+    int,
     cutlass::layout::ColumnMajor,
     Policy
   >;
 
-  test::gemm::warp::Testbed<Mma, cutlass::gemm::GemmShape<128, 128, 8> >().run();
+  test::gemm::warp::Testbed<Mma, cutlass::gemm::GemmShape<64, 64, 8> >().run();
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm61.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/gemm_nvrtc.cu`

 * *Files 26% similar despite different names*

```diff
@@ -30,169 +30,174 @@
  **************************************************************************************************/
 /*! \file
     \brief Unit tests for thread-level GEMM
 */
 
 #include "../../common/cutlass_unit_test.h"
 
-#include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/warp/mma_simt.h"
+#include "cutlass/gemm/thread/mma.h"
 
 #include "testbed.h"
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-TEST(SM61_warp_gemm_int8_col_row, col_row_8x4x8_1x1x4) {
+#if 0
+int main() {
+  nvrtc::thread::Testbed<
+    cutlass::gemm::GemmShape<3, 4, 2>,
+    float,
+    cutlass::layout::ColumnMajor,
+    float,
+    cutlass::layout::RowMajor,
+    float,
+    cutlass::layout::ColumnMajor
+  >().run("cutlass::gemm::thread::Mma<cutlass::gemm::GemmShape<3, 4, 2>, float, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, float, cutlass::layout::ColumnMajor >");
+  return 0;
+}
+#endif
+
+TEST(SM50_Sgemm_thread_nvrtc, DISABLED_col_row_3x4x2) {
+
+  test::nvrtc::thread::Testbed<
+    cutlass::gemm::GemmShape<3, 4, 2>,
+    float,
+    cutlass::layout::ColumnMajor,
+    float,
+    cutlass::layout::RowMajor,
+    float,
+    cutlass::layout::ColumnMajor
+  >().run("cutlass::gemm::thread::Mma<cutlass::gemm::GemmShape<3, 4, 2>, float, cutlass::layout::ColumnMajor, float, cutlass::layout::RowMajor, float, cutlass::layout::ColumnMajor >");
+}
 
-  using Policy = cutlass::gemm::warp::MmaSimtPolicy<
-    cutlass::MatrixShape<8, 4>,
-    cutlass::layout::ColumnMajorInterleaved<2>,
-    cutlass::gemm::GemmShape<1, 1, 4>
-  >;
+/////////////////////////////////////////////////////////////////////////////////////////////////
+#if 0
+TEST(SM50_Sgemm_thread, col_row_3x4x2) {
 
-  using Mma = cutlass::gemm::warp::MmaSimt<
-    cutlass::gemm::GemmShape<8, 4, 8>,
-    int8_t,
-    cutlass::layout::ColumnMajorInterleaved<4>,
-    int8_t,
-    cutlass::layout::RowMajorInterleaved<4>,
-    int,
+  test::gemm::thread::Testbed<
+    cutlass::gemm::GemmShape<3, 4, 2>,
+    float,
     cutlass::layout::ColumnMajor,
-    Policy
-  >;
-
-  test::gemm::warp::Testbed<Mma, cutlass::gemm::GemmShape<8, 4, 8> >().run();
+    float,
+    cutlass::layout::RowMajor,
+    float,
+    cutlass::layout::ColumnMajor
+  >().run();
 }
 
-TEST(SM61_warp_gemm_int8_col_row, col_row_8x4x4_1x1x4) {
-
-  using Policy = cutlass::gemm::warp::MmaSimtPolicy<
-    cutlass::MatrixShape<8, 4>,
-    cutlass::layout::ColumnMajorInterleaved<2>,
-    cutlass::gemm::GemmShape<1, 1, 4>
-  >;
+TEST(SM50_Sgemm_thread, col_row_4x4x2) {
 
-  using Mma = cutlass::gemm::warp::MmaSimt<
-    cutlass::gemm::GemmShape<8, 4, 8>,
-    int8_t,
-    cutlass::layout::ColumnMajorInterleaved<4>,
-    int8_t,
-    cutlass::layout::RowMajorInterleaved<4>,
-    int,
+  test::gemm::thread::Testbed<
+    cutlass::gemm::GemmShape<4, 4, 2>,
+    float,
     cutlass::layout::ColumnMajor,
-    Policy
-  >;
-
-  test::gemm::warp::Testbed<Mma, cutlass::gemm::GemmShape<128, 64, 8> >().run();
+    float,
+    cutlass::layout::RowMajor,
+    float,
+    cutlass::layout::ColumnMajor
+  >().run();
 }
 
-TEST(SM61_warp_gemm_int8_col_row, col_row_16x4x4_2x1x4) {
+TEST(SM50_Sgemm_thread, row_col_4x4x2) {
 
-  using Policy = cutlass::gemm::warp::MmaSimtPolicy<
-    cutlass::MatrixShape<8, 4>,
-    cutlass::layout::ColumnMajorInterleaved<2>,
-    cutlass::gemm::GemmShape<2, 1, 4>
-  >;
-
-  using Mma = cutlass::gemm::warp::MmaSimt<
-    cutlass::gemm::GemmShape<16, 4, 4>,
-    int8_t,
-    cutlass::layout::ColumnMajorInterleaved<4>,
-    int8_t,
-    cutlass::layout::RowMajorInterleaved<4>,
-    int,
+  test::gemm::thread::Testbed<
+    cutlass::gemm::GemmShape<4, 4, 2>,
+    float,
+    cutlass::layout::RowMajor,
+    float,
     cutlass::layout::ColumnMajor,
-    Policy
-  >;
-
-  test::gemm::warp::Testbed<Mma, cutlass::gemm::GemmShape<16, 4, 4> >().run();
+    float,
+    cutlass::layout::ColumnMajor
+  >().run();
 }
 
-TEST(SM61_warp_gemm_int8_col_row, col_row_16x4x4_2x2x4) {
-
-  using Policy = cutlass::gemm::warp::MmaSimtPolicy<
-    cutlass::MatrixShape<8, 4>,
-    cutlass::layout::ColumnMajorInterleaved<2>,
-    cutlass::gemm::GemmShape<2, 2, 4>
-  >;
+TEST(SM50_Sgemm_thread, col_row_4x5x3) {
 
-  using Mma = cutlass::gemm::warp::MmaSimt<
-    cutlass::gemm::GemmShape<16, 8, 4>,
-    int8_t,
-    cutlass::layout::ColumnMajorInterleaved<4>,
-    int8_t,
-    cutlass::layout::RowMajorInterleaved<4>,
-    int,
+  test::gemm::thread::Testbed<
+    cutlass::gemm::GemmShape<4, 5, 3>,
+    float,
     cutlass::layout::ColumnMajor,
-    Policy
-  >;
-
-  test::gemm::warp::Testbed<Mma, cutlass::gemm::GemmShape<16, 8, 4> >().run();
+    float,
+    cutlass::layout::RowMajor,
+    float,
+    cutlass::layout::ColumnMajor
+  >().run();
 }
 
-TEST(SM61_warp_gemm_int8_col_row, col_row_32x16x4_4x4x4) {
-
-  using Policy = cutlass::gemm::warp::MmaSimtPolicy<
-    cutlass::MatrixShape<8, 4>,
-    cutlass::layout::ColumnMajorInterleaved<2>,
-    cutlass::gemm::GemmShape<4, 4, 4>
-  >;
+TEST(SM50_Sgemm_thread, col_row) {
 
-  using Mma = cutlass::gemm::warp::MmaSimt<
-    cutlass::gemm::GemmShape<32, 16, 16>,
-    int8_t,
-    cutlass::layout::ColumnMajorInterleaved<4>,
-    int8_t,
-    cutlass::layout::RowMajorInterleaved<4>,
-    int,
+  test::gemm::thread::Testbed<
+    cutlass::gemm::GemmShape<8, 8, 1>,
+    float,
     cutlass::layout::ColumnMajor,
-    Policy
-  >;
-
-  test::gemm::warp::Testbed<Mma, cutlass::gemm::GemmShape<128, 64, 16> >().run();
+    float,
+    cutlass::layout::RowMajor,
+    float,
+    cutlass::layout::ColumnMajor
+  >().run();
 }
 
+TEST(SM50_Sgemm_thread, row_col) {
 
-TEST(SM61_warp_gemm_int8_col_row, col_row_128x64x4_16x16x4) {
+  test::gemm::thread::Testbed<
+    cutlass::gemm::GemmShape<8, 8, 1>,
+    float,
+    cutlass::layout::RowMajor,
+    float,
+    cutlass::layout::ColumnMajor,
+    float,
+    cutlass::layout::ColumnMajor
+  >().run();
+}
 
-  using Policy = cutlass::gemm::warp::MmaSimtPolicy<
-    cutlass::MatrixShape<8, 4>,
-    cutlass::layout::ColumnMajorInterleaved<2>,
-    cutlass::gemm::GemmShape<16, 16, 4>
-  >;
+TEST(SM50_Sgemm_thread, col_col) {
 
-  using Mma = cutlass::gemm::warp::MmaSimt<
-    cutlass::gemm::GemmShape<128, 64, 4>,
-    int8_t,
-    cutlass::layout::ColumnMajorInterleaved<4>,
-    int8_t,
-    cutlass::layout::RowMajorInterleaved<4>,
-    int,
+  test::gemm::thread::Testbed<
+    cutlass::gemm::GemmShape<8, 8, 1>,
+    float,
     cutlass::layout::ColumnMajor,
-    Policy
-  >;
+    float,
+    cutlass::layout::ColumnMajor,
+    float,
+    cutlass::layout::ColumnMajor
+  >().run();
+}
+
+TEST(SM50_Sgemm_thread, row_row) {
 
-  test::gemm::warp::Testbed<Mma, cutlass::gemm::GemmShape<128, 64, 4> >().run();
+  test::gemm::thread::Testbed<
+    cutlass::gemm::GemmShape<8, 8, 1>,
+    float,
+    cutlass::layout::RowMajor,
+    float,
+    cutlass::layout::RowMajor,
+    float,
+    cutlass::layout::ColumnMajor
+  >().run();
 }
 
-TEST(SM61_warp_gemm_int8_col_row, col_row_64x64x4_4x4x4) {
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-  using Policy = cutlass::gemm::warp::MmaSimtPolicy<
-    cutlass::MatrixShape<8, 4>,
-    cutlass::layout::ColumnMajorInterleaved<2>,
-    cutlass::gemm::GemmShape<4, 4, 4>
-  >;
+TEST(SM50_Dgemm_thread, col_row) {
 
-  using Mma = cutlass::gemm::warp::MmaSimt<
-    cutlass::gemm::GemmShape<64, 64, 8>,
-    int8_t,
-    cutlass::layout::ColumnMajorInterleaved<4>,
-    int8_t,
-    cutlass::layout::RowMajorInterleaved<4>,
-    int,
+  test::gemm::thread::Testbed<
+    cutlass::gemm::GemmShape<8, 8, 1>,
+    double,
     cutlass::layout::ColumnMajor,
-    Policy
-  >;
-
-  test::gemm::warp::Testbed<Mma, cutlass::gemm::GemmShape<64, 64, 8> >().run();
+    double,
+    cutlass::layout::RowMajor,
+    double,
+    cutlass::layout::ColumnMajor
+  >().run();
 }
 
+TEST(SM50_Dgemm_thread, row_col) {
+
+  test::gemm::thread::Testbed<
+    cutlass::gemm::GemmShape<8, 8, 1>,
+    double,
+    cutlass::layout::RowMajor,
+    double,
+    cutlass::layout::ColumnMajor,
+    double,
+    cutlass::layout::ColumnMajor
+  >().run();
+}
+#endif
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sparse_sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sparse_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/warp/testbed.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/testbed.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm70.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm72.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm72.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm75.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/layout/matrix.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/layout/matrix.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/layout/tensor.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/layout/tensor.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/layout/tensor_nhwc.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/layout/tensor_nhwc.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/nvrtc/cutlass/nvrtc/environment.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/cutlass/nvrtc/environment.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/nvrtc/kernel/thread/testbed_kernel.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/kernel/thread/testbed_kernel.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/nvrtc/stdlib/stdint.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/stdlib/stdint.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/testbed.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/testbed.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/pipeline/testbed.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_utils.h`

 * *Files 26% similar despite different names*

```diff
@@ -26,120 +26,102 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-    \brief Common Testbed file shared by Pipeline unit tests
+    \brief utils code for device cutlass code
 */
 
-#include <cstdlib>
-#include <cstdio>
-#include <cassert>
-#include <cutlass/gemm/gemm.h>
-
-#include "cutlass/util/command_line.h"
-#include "../common/cutlass_unit_test.h"
-
-#if CUDA_12_0_SM90_FEATURES_SUPPORTED
-  #define CUTLASS_UNIT_TEST_PIPELINE true
-#else
-  #define CUTLASS_UNIT_TEST_PIPELINE false
-#endif
-
-// Command line test options
-struct Options {
-  //
-  // Data Members
-  // 
-  bool help;
-  bool verification_enabled;
-  int SM_count;
-  int clock_MHz;
-
-  //
-  // Methods
-  // 
-  Options():
-    help(false),
-    verification_enabled(true),
-    SM_count(116),
-    clock_MHz(1477)
-  { }
-
-  void parse(int argc, char const **args) {
-    cutlass::CommandLine cmd(argc, args);
-
-    if (cmd.check_cmd_line_flag("help")) {
-      help = true;
-    }
-
-    cmd.get_cmd_line_argument("verification-enabled", verification_enabled, true);
-    cmd.get_cmd_line_argument("sm-count", SM_count, 116);
-    cmd.get_cmd_line_argument("clock", clock_MHz, 1477);
-  }
-
-  /// Prints the usage statement.
-  std::ostream & print_usage(std::ostream &out) const {
-
-    out << "Options:\n\n"
-      << "  --help                          If specified, displays this usage statement.\n\n"
-      << "  --verification-enabled=<bool>   Enable/Disable verification\n"
-      << "  --sm-count=<int>                Number of SMs on the chip\n"
-      << "  --clock=<int>                   Locked clock value in Mhz\n";
+#pragma once
 
-    return out;
-  }
-};
-
-//
-// Testbed
-//
-
-template<typename Pipeline>
-struct Testbed {
-private:
-  // Commandline options
-  Options options;
+#include <cuda_fp16.h>
+#include <float.h>
+#define FINAL_MASK 0xffffffff
 
-  void run_test(uint32_t const kNumIters) {
-
-    // Run CuTe Gemm 
-    Pipeline pipeline;
+struct half4 {
+    half x, y, z, w;
+};
 
-    cudaError_t result = pipeline.run(kNumIters);
+template<typename T, int NUM>
+__inline__ __device__ T warpReduceSum(T* val)
+{
+#pragma unroll
+    for (int i = 0; i < NUM; i++) {
+#pragma unroll
+        for (int mask = 16; mask > 0; mask >>= 1)
+            val[i] += __shfl_xor_sync(FINAL_MASK, val[i], mask, 32);
+    }
+    return (T)(0.0f);
+}
 
-    CUTE_CHECK_LAST();
-  }
+template<typename T, int NUM>
+__inline__ __device__ T blockReduceSum(T* val)
+{
+    __shared__ T shared[NUM][33];
+    int lane = threadIdx.x & 0x1f;
+    int wid = threadIdx.x >> 5;
+
+    warpReduceSum<T, NUM>(val);
+
+    if (lane == 0) {
+#pragma unroll
+        for (int i = 0; i < NUM; i++) {
+            shared[i][wid] = val[i];
+        }
+    }
 
+    __syncthreads();
 
-public:
-  Testbed(Options const &options_) : options(options_) {
-    int device_id = 0;
-    cudaDeviceProp device_prop;
-    CUTE_CHECK_ERROR(cudaSetDevice(device_id));
-    CUTE_CHECK_ERROR(cudaGetDeviceProperties(&device_prop, device_id));
-  
-    if (device_prop.major < 1) {
-      fprintf(stderr, "Device does not support CUDA.\n");
-      exit(1);
+    bool is_mask = threadIdx.x < (blockDim.x / 32.f);
+#pragma unroll
+    for (int i = 0; i < NUM; i++) {
+        val[i] = is_mask ? shared[i][lane] : (T)(0.0f);
+    }
+    warpReduceSum<T, NUM>(val);
+    return (T)0.0f;
+}
+
+template<typename T, int NUM>
+__inline__ __device__ T warpReduceMax(T* val)
+{
+#pragma unroll
+    for (int i = 0; i < NUM; i++) {
+#pragma unroll
+        for (int mask = 16; mask > 0; mask >>= 1)
+            val[i] = max(val[i], __shfl_xor_sync(FINAL_MASK, val[i], mask, 32));
     }
-  }
+    return (T)(0.0f);
+}
 
-  /// Run verification Gemm problem sizes
-  bool verification() {
+template<typename T, int NUM>
+__inline__ __device__ T blockReduceMax(T* val)
+{
+    static __shared__ T shared[32][NUM];
+    int lane = threadIdx.x & 0x1f;  // in-warp idx
+    int wid = threadIdx.x >> 5;     // warp idx
+
+    warpReduceMax<T, NUM>(val);  // get maxx in each warp
+
+    if (lane == 0)  // record in-warp maxx by warp Idx
+    {
+#pragma unroll
+        for (int i = 0; i < NUM; i++) {
+            shared[wid][i] = val[i];
+        }
+    }
 
-    std::array<uint32_t, 5> kNumIters;
+    __syncthreads();
 
-    for (int i = 0; i < kNumIters.size(); ++i) {
-      kNumIters[i] = (rand() % 1000) + 1;
+    // Modify from blockDim.x << 5 to blockDim.x / 32. to prevent
+    // blockDim.x is not divided by 32
+    bool is_mask = threadIdx.x < (blockDim.x / 32.f);
+#pragma unroll
+    for (int i = 0; i < NUM; i++) {
+        val[i] = is_mask ? shared[lane][i] : (T)(-FLT_MAX);
     }
+    warpReduceMax<T, NUM>(val);
 
-    for (int n : kNumIters) {
-      std::cout << "Stages = " << Pipeline::Stages << " kNumIters = " << n << "\n";
-      run_test(n);
-    }
+    return (T)0.0f;
+}
 
-    return true;
-  }
-};
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_contiguous.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_strided.cu`

 * *Files 5% similar despite different names*

```diff
@@ -29,14 +29,15 @@
  *
  **************************************************************************************************/
 /*! \file
     \brief Tests for TensorReduce family of device-wide operators
 */
 
 #include <iostream>
+#include <limits>
 
 #include "../../common/cutlass_unit_test.h"
 
 #include "cutlass/cutlass.h"
 #include "cutlass/complex.h"
 #include "cutlass/reduction/thread/reduction_operators.h"
 #include "cutlass/reduction/device/tensor_reduce.h"
@@ -51,426 +52,472 @@
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/reference/device/tensor_fill.h"
 #include "cutlass/util/reference/host/tensor_norm.h"
 #include "cutlass/util/tensor_view_io.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// This reduces the C dimension, transforming an NHWC tensor into NHWC with C=1.
-template <typename TensorReduction, typename ElementCompute = typename TensorReduction::ElementCompute>
-bool TestAllReduction_NHWC_reduce_c(ElementCompute reduction_identity = ElementCompute()) {
+/// This reduces the W dimension, transforming an NHWC tensor into NHWC with W=1.
+template <
+  typename TensorReduction, 
+  typename ElementCompute = typename TensorReduction::ElementCompute
+>
+bool TestAllReduction_NHWC_reduce_w(ElementCompute reduction_identity = ElementCompute()) {
 
   using Layout = typename TensorReduction::Layout;
   using ElementOutput = typename TensorReduction::ElementOutput;
   using ElementSource = typename TensorReduction::ElementSource;
 
   int const kV = TensorReduction::kVectorLength;
 
-  int const N_indices[] = {3, 13};
-  int const H_indices[] = {5, 17};
-  int const W_indices[] = {7, 19};
-  int const C_indices[] = {2049, 2048, 2047, 384, 64, 48, 32, 24, 16, 12, 8, 6, 4, 3, 2, 1};
-  
+  int const N_indices[] = {1, 2, 5, 10};
+  int const H_indices[] = {1, 3, 9 };
+  int const W_indices[] = {1, 5, 19, 40, 224};
+  int const C_indices[] = {
+    kV, 
+    2 * kV, 
+    5 * kV, 
+    9 * kV, 
+    17 * kV, 
+    39 * kV, 
+    257 * kV, 
+    kV * 760
+  };
+
+  using Element = int;
+
   for (int N : N_indices) {
     for (int H : H_indices) {
       for (int W : W_indices) {
-        for (int Cx : C_indices) {
-
-          int C = Cx * kV;
+        for (int C : C_indices) {
 
           cutlass::HostTensor<ElementSource, Layout> src_tensor({N, H, W, C});
-          cutlass::HostTensor<ElementOutput, Layout> dst_tensor({N, H, W, 1});
+          cutlass::HostTensor<ElementOutput, Layout> dst_tensor({N, H, 1, C});
 
           cutlass::reference::host::TensorFillRandomUniform(
             src_tensor.host_view(), 17, 10, -10, 0);
 
+          cutlass::reference::host::BlockFillSequential(
+            dst_tensor.host_data(), dst_tensor.capacity());
+
           dst_tensor.sync_device();
           src_tensor.sync_device();
 
-          // Execute a tensor reduction over rank 3 (the 'C' dimension is reduced; NHWC => NHW)
-          TensorReduction reduction(src_tensor.extent(), 3);
+          // Execute a tensor reduction over rank 2 (the 'W' dimension is reduced; NHWC => NHC)
+          TensorReduction reduction(src_tensor.extent(), 2);
 
           cutlass::DeviceAllocation<uint8_t> device_workspace(reduction.workspace_size());
 
           cutlass::Status status = reduction.reduce(
             dst_tensor.device_ref(),
             src_tensor.device_ref(),
             device_workspace.get(),
             reduction_identity
           );
 
           EXPECT_EQ(status, cutlass::Status::kSuccess);
           EXPECT_EQ(cudaDeviceSynchronize(), cudaSuccess);
-          
+          // Reference check
           dst_tensor.sync_host();
 
           typename TensorReduction::ReductionOp reduction_op;
 
-          //
-          // Reference check
-          //
           for (int n = 0; n < src_tensor.extent().n(); ++n) {
             for (int h = 0; h < src_tensor.extent().h(); ++h) {
-              for (int w = 0; w < src_tensor.extent().w(); ++w) {
+              for (int c = 0; c < src_tensor.extent().c(); ++c) {
 
-                ElementCompute c_accum = reduction_identity;
+                ElementCompute w_accum = reduction_identity;
 
-                for (int c = 0; c < src_tensor.extent().c(); ++c) {
-                  c_accum = reduction_op(c_accum, ElementCompute(src_tensor.at({n, h, w, c})));
+                for (int w = 0; w < src_tensor.extent().w(); ++w) {
+                  w_accum = reduction_op(w_accum, ElementCompute(src_tensor.at({n, h, w, c})));
                 }
 
-                ElementCompute got = ElementCompute(dst_tensor.at({n, h, w, 0}));
+                ElementCompute got = ElementCompute(dst_tensor.at({n, h, 0, c}));
 
-                bool equal = (c_accum == got);
+                bool equal = (w_accum == got);
 
                 EXPECT_TRUE(equal);
                 if (!equal) {
 
                   std::cerr 
-                    << "Error at location (" << n << ", " << h << ", " << w << ", 0)" << std::endl;
+                    << "Error at location (" << n << ", " << h << ", 0, " << c << ")" << std::endl;
 
                   std::cerr 
-                    << "  expected: " << c_accum << std::endl
+                    << "  expected: " << w_accum << std::endl
                     << "       got: " << got << std::endl;
 
                   std::cerr 
                     << "Problem: " << src_tensor.extent() << " -> " 
                     << dst_tensor.extent() << std::endl;
 
                   std::cerr 
                     << "   Grid: " << reduction.reduction_strided.grid_shape 
-                    << "\n   Block: " << reduction.reduction_strided.threadblock_shape << std::endl
-                    << "  FInal: " << reduction.reduction_strided.grid_final 
-                    << "\n   Block: " << reduction.reduction_strided.threadblock_final << "\n";
+                    << "\n  Block: " << reduction.reduction_strided.threadblock_shape << std::endl
+                    << "  Final: " << reduction.reduction_strided.grid_final 
+                    << "\n  Block: " << reduction.reduction_strided.threadblock_final << "\n";
 
                   return false;
                 }
-
-              } //w
-            } // h
-          } // n
-          
-          //
-          // Next problem
-          //
-
-        } // C
-      } // W
-    } // H
-  } // N
+              }
+            }
+          }
+        }
+      }
+    }
+  }
 
   return true;
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Test tensor reduction from NHWC to NHW
-TEST(Reduction_TensorReduce, nhwc_reduce_c_f32x1) {
+/// Test tensor reduction from NHWC to NHC
+TEST(Reduction_TensorReduce, nhwc_reduce_w_f32x8_f16x8) {
 
-  using Layout = cutlass::layout::TensorNHWC;
+  int const kV = 8;
   using ElementOutput = float;
-  using ElementSource = float;
+  using ElementSource = cutlass::half_t;
   using ElementCompute = float;
-  int const kV = 1;
-  
+  using Layout = cutlass::layout::TensorNHWC;
+
   // Define the functor
   using Functor = cutlass::plus<ElementCompute>;
 
   using TensorReduction = cutlass::reduction::device::TensorReduction<
     ElementOutput,
     ElementSource,
     Layout,
     Functor,
     kV,
     ElementCompute
   >;
-  
-  EXPECT_TRUE(TestAllReduction_NHWC_reduce_c<TensorReduction>());
-}
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+  EXPECT_TRUE(TestAllReduction_NHWC_reduce_w<TensorReduction>());          
+}
 
-/// Test tensor reduction from NHWC to NHW
-TEST(Reduction_TensorReduce, nhwc_reduce_c_f32x1_f16x1) {
+/// Test tensor reduction from NHWC to NHC
+TEST(Reduction_TensorReduce, nhwc_reduce_w_f32x2_f16x2) {
 
-  using Layout = cutlass::layout::TensorNHWC;
+  int const kV = 2;
   using ElementOutput = float;
   using ElementSource = cutlass::half_t;
   using ElementCompute = float;
-  int const kV = 1;
-  
+  using Layout = cutlass::layout::TensorNHWC;
+
   // Define the functor
   using Functor = cutlass::plus<ElementCompute>;
 
   using TensorReduction = cutlass::reduction::device::TensorReduction<
     ElementOutput,
     ElementSource,
     Layout,
     Functor,
     kV,
     ElementCompute
   >;
-  
-  EXPECT_TRUE(TestAllReduction_NHWC_reduce_c<TensorReduction>());
-}
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+  EXPECT_TRUE(TestAllReduction_NHWC_reduce_w<TensorReduction>());          
+}
 
-/// Test tensor reduction from NHWC to NHW
-TEST(Reduction_TensorReduce, nhwc_reduce_c_f32x2) {
+/// Test tensor reduction from NHWC to NHC
+TEST(Reduction_TensorReduce, nhwc_reduce_w_f32x1_f16x1) {
 
-  using Layout = cutlass::layout::TensorNHWC;
+  int const kV = 1;
   using ElementOutput = float;
-  using ElementSource = float;
+  using ElementSource = cutlass::half_t;
   using ElementCompute = float;
-  int const kV = 2;
-  
+  using Layout = cutlass::layout::TensorNHWC;
+
   // Define the functor
   using Functor = cutlass::plus<ElementCompute>;
 
   using TensorReduction = cutlass::reduction::device::TensorReduction<
     ElementOutput,
     ElementSource,
     Layout,
     Functor,
     kV,
     ElementCompute
   >;
-  
-  EXPECT_TRUE(TestAllReduction_NHWC_reduce_c<TensorReduction>());
+
+  EXPECT_TRUE(TestAllReduction_NHWC_reduce_w<TensorReduction>());          
 }
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+/// Test tensor reduction from NHWC to NHC
+TEST(Reduction_TensorReduce, nhwc_reduce_w_s32x4) {
 
-/// Test tensor reduction from NHWC to NHW
-TEST(Reduction_TensorReduce, nhwc_reduce_c_f32x2_f16x2) {
+  int const kV = 4;
+  using Element = int;
+  using Layout = cutlass::layout::TensorNHWC;
+
+  // Define the functor
+  using Functor = cutlass::plus<Element>;
+
+  using TensorReduction = cutlass::reduction::device::TensorReduction<
+    Element,
+    Element,
+    Layout,
+    Functor,
+    kV,
+    Element
+  >;
 
+  EXPECT_TRUE(TestAllReduction_NHWC_reduce_w<TensorReduction>());          
+}
+
+/// Test tensor reduction from NHWC to NHC
+TEST(Reduction_TensorReduce, nhwc_reduce_w_cf32) {
+
+  int const kV = 1;
+  using ElementOutput = cutlass::complex<float>;
+  using ElementSource = cutlass::complex<float>;
+  using ElementCompute = cutlass::complex<float>;
   using Layout = cutlass::layout::TensorNHWC;
-  using ElementOutput = float;
-  using ElementSource = cutlass::half_t;
-  using ElementCompute = float;
-  int const kV = 2;
-  
+
   // Define the functor
   using Functor = cutlass::plus<ElementCompute>;
 
   using TensorReduction = cutlass::reduction::device::TensorReduction<
     ElementOutput,
     ElementSource,
     Layout,
     Functor,
     kV,
     ElementCompute
   >;
-  
-  EXPECT_TRUE(TestAllReduction_NHWC_reduce_c<TensorReduction>());
+
+  EXPECT_TRUE(TestAllReduction_NHWC_reduce_w<TensorReduction>());          
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Test tensor reduction from NHWC to NHW
-TEST(Reduction_TensorReduce, nhwc_reduce_c_f32x4) {
+/// Test tensor reduction from NHWC to NHC
+TEST(Reduction_TensorReduce, nhwc_maximum_w_cf32) {
 
+  int const kV = 1;
+  using ElementOutput = float;
+  using ElementSource = float;
+  using ElementCompute = float;
   using Layout = cutlass::layout::TensorNHWC;
+
+  // Define the functor
+  using Functor = cutlass::maximum<ElementCompute>;
+
+  using TensorReduction = cutlass::reduction::device::TensorReduction<
+    ElementOutput,
+    ElementSource,
+    Layout,
+    Functor,
+    kV,
+    ElementCompute
+  >;
+
+  EXPECT_TRUE(TestAllReduction_NHWC_reduce_w<TensorReduction>( -std::numeric_limits<float>::max() ));          
+}
+
+/// Test tensor reduction from NHWC to NHC
+TEST(Reduction_TensorReduce, nhwc_minimum_w_cf32) {
+
+  int const kV = 1;
   using ElementOutput = float;
   using ElementSource = float;
   using ElementCompute = float;
-  int const kV = 4;
-  
+  using Layout = cutlass::layout::TensorNHWC;
+
   // Define the functor
-  using Functor = cutlass::plus<ElementCompute>;
+  using Functor = cutlass::minimum<ElementCompute>;
 
   using TensorReduction = cutlass::reduction::device::TensorReduction<
     ElementOutput,
     ElementSource,
     Layout,
     Functor,
     kV,
     ElementCompute
   >;
-  
-  EXPECT_TRUE(TestAllReduction_NHWC_reduce_c<TensorReduction>());
+
+  EXPECT_TRUE(TestAllReduction_NHWC_reduce_w<TensorReduction>(std::numeric_limits<float>::max()));          
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Test tensor reduction from NHWC to NHW
-TEST(Reduction_TensorReduce, nhwc_reduce_c_f32x4_f16x4) {
+/// Test tensor reduction from NHWC to NHC
+TEST(Reduction_TensorReduce, nhwc_XOR_w_u32) {
 
+  int const kV = 1;
+  using ElementOutput = int;
+  using ElementSource = int;
+  using ElementCompute = int;
   using Layout = cutlass::layout::TensorNHWC;
-  using ElementOutput = float;
-  using ElementSource = cutlass::half_t;
-  using ElementCompute = float;
-  int const kV = 4;
-  
+
   // Define the functor
-  using Functor = cutlass::plus<ElementCompute>;
+  using Functor = cutlass::bit_xor<ElementCompute>;
 
   using TensorReduction = cutlass::reduction::device::TensorReduction<
     ElementOutput,
     ElementSource,
     Layout,
     Functor,
     kV,
     ElementCompute
   >;
-  
-  EXPECT_TRUE(TestAllReduction_NHWC_reduce_c<TensorReduction>());
+
+  EXPECT_TRUE(TestAllReduction_NHWC_reduce_w<TensorReduction>());          
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Test tensor reduction from NHWC to NHW
-TEST(Reduction_TensorReduce, nhwc_maximum_c_f32x4) {
+/// Test tensor reduction from NHWC to NHC
+TEST(Reduction_TensorReduce, nhwc_AND_w_s32) {
 
+  int const kV = 1;
+  using ElementOutput = unsigned;
+  using ElementSource = unsigned;
+  using ElementCompute = unsigned;
   using Layout = cutlass::layout::TensorNHWC;
-  using ElementOutput = float;
-  using ElementSource = float;
-  using ElementCompute = float;
-  int const kV = 4;
-  
+
   // Define the functor
-  using Functor = cutlass::maximum<ElementCompute>;
+  using Functor = cutlass::bit_and<ElementCompute>;
 
   using TensorReduction = cutlass::reduction::device::TensorReduction<
     ElementOutput,
     ElementSource,
     Layout,
     Functor,
     kV,
     ElementCompute
   >;
-  
-  EXPECT_TRUE(TestAllReduction_NHWC_reduce_c<TensorReduction>( -std::numeric_limits<float>::max() ));
+
+  EXPECT_TRUE(TestAllReduction_NHWC_reduce_w<TensorReduction>(0xffffffff));
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Test tensor reduction from NHWC to NHW
-TEST(Reduction_TensorReduce, nhwc_minimum_c_f32x4) {
+/// Test tensor reduction from NHWC to NHC
+TEST(Reduction_TensorReduce, nhwc_OR_w_u32) {
 
+  int const kV = 1;
+  using ElementOutput = int;
+  using ElementSource = int;
+  using ElementCompute = int;
   using Layout = cutlass::layout::TensorNHWC;
-  using ElementOutput = float;
-  using ElementSource = float;
-  using ElementCompute = float;
-  int const kV = 4;
-  
+
   // Define the functor
-  using Functor = cutlass::minimum<ElementCompute>;
+  using Functor = cutlass::bit_or<ElementCompute>;
 
   using TensorReduction = cutlass::reduction::device::TensorReduction<
     ElementOutput,
     ElementSource,
     Layout,
     Functor,
     kV,
     ElementCompute
   >;
-  
-  EXPECT_TRUE(TestAllReduction_NHWC_reduce_c<TensorReduction>( std::numeric_limits<float>::max() ));
+
+  EXPECT_TRUE(TestAllReduction_NHWC_reduce_w<TensorReduction>());          
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Test tensor reduction from NHWC to NHW
-TEST(Reduction_TensorReduce, nhwc_ANY_c_s32) {
+/// Test tensor reduction from NHWC to NHC
+TEST(Reduction_TensorReduce, nhwc_ANY_w_s32) {
 
-  using Layout = cutlass::layout::TensorNHWC;
+  int const kV = 1;
   using ElementOutput = int;
   using ElementSource = int;
   using ElementCompute = int;
-  int const kV = 1;
-  
+  using Layout = cutlass::layout::TensorNHWC;
+
   // Define the functor
   using Functor = cutlass::logical_or<ElementCompute>;
 
   using TensorReduction = cutlass::reduction::device::TensorReduction<
     ElementOutput,
     ElementSource,
     Layout,
     Functor,
     kV,
     ElementCompute
   >;
-  
-  EXPECT_TRUE(TestAllReduction_NHWC_reduce_c<TensorReduction>( ElementCompute(0) ));
+
+  EXPECT_TRUE(TestAllReduction_NHWC_reduce_w<TensorReduction>(ElementCompute(0)));          
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Test tensor reduction from NHWC to NHW
-TEST(Reduction_TensorReduce, nhwc_ALL_c_s32) {
+/// Test tensor reduction from NHWC to NHC
+TEST(Reduction_TensorReduce, nhwc_ALL_w_s32) {
 
-  using Layout = cutlass::layout::TensorNHWC;
+  int const kV = 1;
   using ElementOutput = int;
   using ElementSource = int;
   using ElementCompute = int;
-  int const kV = 1;
-  
+  using Layout = cutlass::layout::TensorNHWC;
+
   // Define the functor
   using Functor = cutlass::logical_and<ElementCompute>;
 
   using TensorReduction = cutlass::reduction::device::TensorReduction<
     ElementOutput,
     ElementSource,
     Layout,
     Functor,
     kV,
     ElementCompute
   >;
-  
-  EXPECT_TRUE(TestAllReduction_NHWC_reduce_c<TensorReduction>( ElementCompute(1) ));
+
+  EXPECT_TRUE(TestAllReduction_NHWC_reduce_w<TensorReduction>(ElementCompute(1)));          
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Test tensor reduction from NHWC to NHW
-TEST(Reduction_TensorReduce, nhwc_ANY_c_f32) {
+/// Test tensor reduction from NHWC to NHC
+TEST(Reduction_TensorReduce, nhwc_ANY_w_f32) {
 
-  using Layout = cutlass::layout::TensorNHWC;
+  int const kV = 1;
   using ElementOutput = float;
   using ElementSource = float;
   using ElementCompute = float;
-  int const kV = 1;
-  
+  using Layout = cutlass::layout::TensorNHWC;
+
   // Define the functor
   using Functor = cutlass::logical_or<ElementCompute>;
 
   using TensorReduction = cutlass::reduction::device::TensorReduction<
     ElementOutput,
     ElementSource,
     Layout,
     Functor,
     kV,
     ElementCompute
   >;
-  
-  EXPECT_TRUE(TestAllReduction_NHWC_reduce_c<TensorReduction>( ElementCompute(0) ));
+
+  EXPECT_TRUE(TestAllReduction_NHWC_reduce_w<TensorReduction>(ElementCompute(0)));          
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Test tensor reduction from NHWC to NHW
-TEST(Reduction_TensorReduce, nhwc_ALL_c_f32) {
+/// Test tensor reduction from NHWC to NHC
+TEST(Reduction_TensorReduce, nhwc_ALL_w_f32) {
 
-  using Layout = cutlass::layout::TensorNHWC;
+  int const kV = 1;
   using ElementOutput = float;
   using ElementSource = float;
   using ElementCompute = float;
-  int const kV = 1;
-  
+  using Layout = cutlass::layout::TensorNHWC;
+
   // Define the functor
   using Functor = cutlass::logical_and<ElementCompute>;
 
   using TensorReduction = cutlass::reduction::device::TensorReduction<
     ElementOutput,
     ElementSource,
     Layout,
     Functor,
     kV,
     ElementCompute
   >;
-  
-  EXPECT_TRUE(TestAllReduction_NHWC_reduce_c<TensorReduction>( ElementCompute(1) ));
+
+  EXPECT_TRUE(TestAllReduction_NHWC_reduce_w<TensorReduction>(ElementCompute(1)));          
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk_testbed.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk_testbed.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/reduction/thread/reduction_thread.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/thread/reduction_thread.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/test_unit.cpp` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/main.cpp`

 * *Files 10% similar despite different names*

```diff
@@ -24,18 +24,30 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/** \file
-    \brief Unit tests for CUTLASS core
+/* \file
+   \brief 
 */
 
-#include "common/cutlass_unit_test.h"
+#include <iostream>
 
-int main(int argc, char* arg[]) {
-  FilterArchitecture();
-  ::testing::InitGoogleTest(&argc, arg);
-  return RUN_ALL_TESTS();
+#include "options.h"
+
+#include "cutlass_profiler.h"
+
+///////////////////////////////////////////////////////////////////////////////////////////////////
+
+int main(int argc, char const *arg[]) {
+
+  cutlass::CommandLine cmdline(argc, arg);
+  cutlass::profiler::Options options(cmdline);
+
+  cutlass::profiler::CutlassProfiler profiler(options);
+
+  return profiler();
 }
+
+///////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/transform/threadblock/predicated_tile_iterator.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/transform/threadblock/predicated_tile_iterator.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/transform/threadblock/regular_tile_iterator_tensor_op.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/transform/threadblock/regular_tile_iterator_tensor_op.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/util/cutlass_test_levels.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/util/cutlass_test_levels.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/test/unit/util/tensor_reduce.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/util/tensor_reduce.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/arch_mappings.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/arch_mappings.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/handle.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/handle.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/library.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/library.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/manifest.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/manifest.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/operation_table.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/operation_table.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/singleton.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/singleton.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/util.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/util.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/compiler.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/debug.h`

 * *Files 19% similar despite different names*

```diff
@@ -25,51 +25,32 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /* \file
-   \brief In-memory compiled artifact cache
+   \brief
 */
 
-#include <pybind11/pybind11.h>
-#include <string>
-#include <unordered_map>
+#pragma once
 
+#include <iostream>
 
-namespace py = pybind11;
+//#define report(x) { std::cout << "\033[31m" << __FILE__ << ":" << __LINE__ << "  " << x << "\033[0m" << std::endl; }
+//#define report(x) {}
 
-namespace cutlass {
-
-struct CompileCache {
-public:
-    CompileCache() = default;
-    ~CompileCache() = default;
-
-    using Cache = std::unordered_map<std::string, py::object>;
-
-    /// Check if the kernel has already been compiled
-    py::object at(const std::string &kernel) {
-        auto item = cache_.find(kernel);
-
-        if (item != cache_.end()) {
-            return item->second;
-        }
-        return py::none();
-    }
-
-    /// Insert a new compiled kernel for new configuration
-    void insert(const std::string &kernel, const py::object &compiled_kernel){
-        cache_.emplace(kernel, compiled_kernel);
-    }
-
-    const int64_t size() const { return cache_.size(); }
-
-    /// Clear the cache
-    void clear() { cache_.clear(); }
-
-private:
-    Cache cache_;
-};
-
-} // namespace cutlass
+// Enable/Disble Profiler debug prints
+//#define DEBUG_PROFILER 
+
+//RED    31m   // profiler prints debug messages in red
+//YELLOW 33m   // ir prints debug messages in yellow
+
+#ifndef DEBUG_PROFILER
+#define debugprof(...)
+#else
+#define debugprof(...) do { \
+          printf("\033[33m[DEBUG PROF]  %s:%d | ", __FILE__, __LINE__); \
+          printf(__VA_ARGS__); \
+          printf("\033[0m\n"); \
+      } while (0)
+#endif
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/cute.cpp` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/binary_ops.h`

 * *Files 18% similar despite different names*

```diff
@@ -10,45 +10,75 @@
  *
  * 2. Redistributions in binary form must reproduce the above copyright notice,
  * this list of conditions and the following disclaimer in the documentation
  * and/or other materials provided with the distribution.
  *
  * 3. Neither the name of the copyright holder nor the names of its
  * contributors may be used to endorse or promote products derived from
- * this software without specific prior written permission.
+ * this layernormware without specific prior written permission.
  *
  * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
  * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
  * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
  * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/* \file
-   \brief binding CuTe C++ APIs to Python
+
+/*! \file
+
+  \brief Binary operations to be used within the epilogue visitor model.
 */
 
-#include <pybind11/pybind11.h>
-#include <pybind11/stl_bind.h>
+#pragma once
+#include "cutlass/cutlass.h"
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+namespace cutlass {
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+
+/// Elementwise addition of two arrays
+template <typename T, int N>
+struct VectorAdd {
+
+    struct Arguments {
+        int tmp;
+
+        CUTLASS_HOST_DEVICE
+        Arguments():tmp(0){ }
+
+        CUTLASS_HOST_DEVICE
+        Arguments(int tmp): tmp(tmp) { }
+    };
+    
+    struct Params {
+
+        CUTLASS_HOST_DEVICE
+        Params(Arguments const &args) { }
+    };
 
-#include "cute/arch/mma_sm90_gmma.hpp"
+    CUTLASS_HOST_DEVICE
+    VectorAdd(
+        Params const &params
+    ) { }
 
-namespace py = pybind11;
+    CUTLASS_HOST_DEVICE
+    Array<T, N> operator()(Array<T, N> const &lhs, Array<T, N> const &rhs) const {
+        cutlass::plus<Array<T, N>> add_op;
+        return add_op(lhs, rhs);
+    }
 
+};
 
-PYBIND11_MODULE(cute, m) {
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-    // module doc
-    m.doc() = "CuTe C++ bindings";
+} // namespace cutlass
 
-    py::enum_<cute::GMMA::Major>(m, "GMMAMajor",
-        R"pbdoc(classification of CuTe GMMA tensor major specification)pbdoc")
-        .value("K", cute::GMMA::Major::K,
-            R"pbdoc(Tensor is contiguous in reduction dimension)pbdoc")
-        .value("MN", cute::GMMA::Major::MN,
-            R"pbdoc(Tensor is contiguous in non-reduction dimension)pbdoc");
-}
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/arch.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/singleton.cu`

 * *Files 24% similar despite different names*

```diff
@@ -24,36 +24,39 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/* \file
-   \brief Bind opcode classes to python
-*/
-#pragma once
-#include <pybind11/pybind11.h>
-#include <pybind11/stl_bind.h>
 
-#include "cutlass/arch/mma.h"
+#include <memory>
+#include "cutlass/library/library.h"
+#include "cutlass/library/manifest.h"
+#include "cutlass/library/operation_table.h"
+#include "cutlass/library/singleton.h"
 
-namespace py = pybind11;
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
-enum class OpcodeClass {
-    kSimt, kTensorOp, kWmmaTensorOp, kSparseTensorOp
-};
+namespace library {
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+Singleton::Singleton() {
+
+  manifest.initialize();
+
+  operation_table.append(manifest);
 }
 
-void bind_opcode(py::module &m) {
-    py::enum_<cutlass::OpcodeClass>(m, "OpClass",
-        R"pbdoc(classification of math operators)pbdoc")
-        .value("Simt", cutlass::OpcodeClass::kSimt, 
-            R"pbdoc(Tag classifying math operators as thread-level operations)pbdoc")
-        .value("TensorOp", cutlass::OpcodeClass::kTensorOp, 
-            R"pbdoc(Tag classifing operators as Tensor Core operations)pbdoc")
-        .value("WmmaTensorOp", cutlass::OpcodeClass::kWmmaTensorOp, 
-            R"pbdoc(Tag classifing operators as WMMA Tensor Core operations)pbdoc")
-        .value("SparseTensorOp", cutlass::OpcodeClass::kSparseTensorOp, 
-            R"pbdoc(Tag classifing operators as sparseTensor Core operations)pbdoc");
+Singleton const & Singleton::get() {
+  static Singleton instance;
+  return instance;
 }
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+} // namespace library
+} // namespace cutlass
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/convolution.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/reduction_operation_profiler.h`

 * *Files 26% similar despite different names*

```diff
@@ -25,67 +25,149 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /* \file
-   \brief Bind convolution related enum types to python
+   \brief Defines profiling functionality for reduction operation
+
 */
+
 #pragma once
-#include <pybind11/pybind11.h>
-#include <pybind11/stl_bind.h>
 
-#include "conv_problem_size.h"
-#include "host.h"
-#include "cutlass/conv/convolution.h"
+#include <vector>
+#include <string>
+#include <memory>
+#include <algorithm>
+#include <unordered_map>
+
+// CUTLASS Library includes
+#include "cutlass/library/library.h"
+#include "cutlass/library/util.h"
+#include "cutlass/library/manifest.h"
+
+// Profiler includes
+#include "options.h"
+#include "device_context.h"
+#include "operation_profiler.h"
+#include "performance_result.h"
+#include "problem_space.h"
+#if CUTLASS_ENABLE_CUDNN
+#include "cudnn_helpers.h"
+#endif //#if CUTLASS_ENABLE_CUDNN
+#include "debug.h"
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+namespace cutlass {
+namespace profiler {
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Abstract base class for each math function
+class ReductionOperationProfiler : public OperationProfiler {
+public:
+
+
+  /// Workspace used 
+  struct ReductionWorkspace {
+
+    /// Conv device allocations
+    DeviceAllocation *Workspace;
+    DeviceAllocation *Source;
+    DeviceAllocation *Destination;
+    DeviceAllocation *Reference;
+    
+    /// Library configuration and arguments
+    library::ReductionConfiguration configuration;
+    library::ReductionArguments arguments;
+
+    /// Buffer used for the cutlass operations' host workspace
+    std::vector<uint8_t> host_workspace;
 
-namespace py = pybind11;
+    /// Buffer used for the cutlass operations' device workspace
+    DeviceAllocation device_workspace;
 
-void bind_convolution(py::module &m) {
     //
-    // Enumerate types
-    // cutlass/include/cutlass/conv/convolution.h
+    // Methods
     //
 
-    /// Convolutional operator
-    py::enum_<cutlass::conv::Operator>(m, "Operator", R"pbdoc(Convolutional operator)pbdoc")
-        .value("fprop", cutlass::conv::Operator::kFprop, "Forward propagation")
-        .value("dgrad", cutlass::conv::Operator::kDgrad, "Activation grad")
-        .value("wgrad", cutlass::conv::Operator::kWgrad, "Weight grad");
-
-    /// Distinguishes convolution  from cross correlation
-    py::enum_<cutlass::conv::Mode>(m, "Mode")
-        .value("cross_correlation", cutlass::conv::Mode::kCrossCorrelation)
-        .value("convolution", cutlass::conv::Mode::kConvolution);
-    
-    /// Selects among several implementation variants trading off performance with simplicity
-    py::enum_<cutlass::conv::IteratorAlgorithm>(m, "IteratorAlgorithm",
-        R"pbdoc(Selects among several implementation variants trading off performance with simplicity)pbdoc")
-        .value("analytic", cutlass::conv::IteratorAlgorithm::kAnalytic, R"pbdoc(functionally correct in all cases but lower performance)pbdoc")
-        .value("optimized", cutlass::conv::IteratorAlgorithm::kOptimized, R"pbdoc(optimized for R <= 32, S <= 32 and unity-stride dgrad)pbdoc")
-        .value("fixed_channels", cutlass::conv::IteratorAlgorithm::kFixedChannels, R"pbdoc(Analytic algorithm optimized for fixed channel count (C == AccessSize))pbdoc")
-        .value("few_channels", cutlass::conv::IteratorAlgorithm::kFewChannels, R"pbdoc(Analytic algorithm optimized for few channels (C divisible by AccessSize))pbdoc");
-    
-    /// Distinguishes among partial specializations that accelerate certain problems where convolution
-    /// stride is unit.
-    py::enum_<cutlass::conv::StrideSupport>(m, "StrideSupport",
-        R"pbdoc(Distinguishes among partial specializations that accelerate certain problems where convolution
-        stride is unit.)pbdoc")
-        .value("strided", cutlass::conv::StrideSupport::kStrided, R"pbdoc(arbitrary convolution stride)pbdoc")
-        .value("unity", cutlass::conv::StrideSupport::kUnity, R"pbdoc(unit convolution stride)pbdoc");
-    
-    /// Identifies split-K mode
-    py::enum_<cutlass::conv::SplitKMode>(m, "SplitKMode")
-        .value("None", cutlass::conv::SplitKMode::kNone)
-        .value("Serial", cutlass::conv::SplitKMode::kSerial)
-        .value("Parallel", cutlass::conv::SplitKMode::kParallel);
-    
-    // Conv problem sizes
-    bind_conv_problem_size(m);
+    ReductionWorkspace(): 
+      Workspace(nullptr), Source(nullptr), Destination(nullptr), Reference(nullptr) { }
+  };
+
+protected:
+
+  //
+  // Data members
+  //
+
+  /// Reduction problem obtained from problem space
+  MatrixCoord problem_;
+
+  /// Device memory allocations 
+  ReductionWorkspace conv_workspace_;
+
+
+public:
+  //
+  // Methods
+  //
+
+  /// Ctor
+  ReductionOperationProfiler(Options const &options);
+
+  /// Destructor
+  virtual ~ReductionOperationProfiler();
+
+  /// Prints usage statement for the math function
+  virtual void print_usage(std::ostream &out) const;
+
+  /// Prints examples
+  virtual void print_examples(std::ostream &out) const;
+
+  /// Extracts the problem dimensions
+  virtual Status initialize_configuration(
+    Options const &options, 
+    PerformanceReport &report, 
+    DeviceContext &device_context,
+    library::Operation const *operation,
+    ProblemSpace const &problem_space,
+    ProblemSpace::Problem const &problem);
+
+  /// Initializes workspace
+  virtual Status initialize_workspace(
+    Options const &options, 
+    PerformanceReport &report, 
+    DeviceContext &device_context,
+    library::Operation const *operation,
+    ProblemSpace const &problem_space,
+    ProblemSpace::Problem const &problem);
+
+  /// Verifies CUTLASS against references
+  virtual bool verify_cutlass(
+    Options const &options,  
+    PerformanceReport &report,
+    DeviceContext &device_context,
+    library::Operation const *operation,
+    ProblemSpace const &problem_space,
+    ProblemSpace::Problem const &problem);
+
+  /// Measures performance results
+  virtual bool profile(
+    Options const &options, 
+    PerformanceReport &report, 
+    DeviceContext &device_context,
+    library::Operation const *operation,
+    ProblemSpace const &problem_space,
+    ProblemSpace::Problem const &problem);
+  
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+} // namespace profiler
+} // namespace cutlass
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-    //
-    // host helper functions
-    //
-    py::module_ host_submodule = m.def_submodule("host");
-    bind_conv_host_helper(host_submodule);
-}
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/host.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_norm.h`

 * *Files 27% similar despite different names*

```diff
@@ -24,31 +24,19 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/* \file
-   \brief Bind conv host helpers to python
-*/
 #pragma once
-#include <pybind11/pybind11.h>
-#include <pybind11/stl_bind.h>
 
-#include "cutlass/util/host_reorder.h"
-#include "cutlass/layout/tensor.h"
 
-namespace py = pybind11;
+#include "cutlass/cutlass.h"
+
+// The contents of this file have been moved  to 'tensor_reduce' to cover other types of reductions.
+
+#include "cutlass/util/reference/host/tensor_reduce.h"
+
+///////////////////////////////////////////////////////////////////////////////////////////////////
 
 
-void bind_conv_host_helper(py::module &m) {
-
-    /// reorder operand B for interleaved layout
-    m.def("reorder_convK", [](
-        cutlass::TensorRef<int8_t, cutlass::layout::TensorCxRSKx<32>> dest,
-        cutlass::TensorRef<int8_t, cutlass::layout::TensorCxRSKx<32>> src,
-        cutlass::conv::Operator conv_op, const cutlass::conv::Conv2dProblemSize & problem_size) {
-            cutlass::gemm::GemmCoord implicit_problem_size = cutlass::conv::implicit_gemm_problem_size(conv_op, problem_size);
-            cutlass::reorder_convK<32>(dest, src, implicit_problem_size);
-        });
-}
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/binary_ops.h` & `flash_attn-1.0.9/csrc/flash_attn/src/fmha_blockmask.h`

 * *Files 21% similar despite different names*

```diff
@@ -1,84 +1,57 @@
-/***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
- * SPDX-License-Identifier: BSD-3-Clause
- *
+/******************************************************************************
+ * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
+ * 
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of the NVIDIA CORPORATION nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ * 
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
- * 1. Redistributions of source code must retain the above copyright notice, this
- * list of conditions and the following disclaimer.
- *
- * 2. Redistributions in binary form must reproduce the above copyright notice,
- * this list of conditions and the following disclaimer in the documentation
- * and/or other materials provided with the distribution.
- *
- * 3. Neither the name of the copyright holder nor the names of its
- * contributors may be used to endorse or promote products derived from
- * this layernormware without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
- * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
- * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
- * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
- * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
- * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
- * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- *
- **************************************************************************************************/
-
-/*! \file
-
-  \brief Binary operations to be used within the epilogue visitor model.
-*/
+ ******************************************************************************/
 
 #pragma once
-#include "cutlass/cutlass.h"
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-namespace cutlass {
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-
-/// Elementwise addition of two arrays
-template <typename T, int N>
-struct VectorAdd {
-
-    struct Arguments {
-        int tmp;
-
-        CUTLASS_HOST_DEVICE
-        Arguments():tmp(0){ }
-
-        CUTLASS_HOST_DEVICE
-        Arguments(int tmp): tmp(tmp) { }
-    };
-    
-    struct Params {
-
-        CUTLASS_HOST_DEVICE
-        Params(Arguments const &args) { }
-    };
-
-    CUTLASS_HOST_DEVICE
-    VectorAdd(
-        Params const &params
-    ) { }
+#include <fmha.h>
+#include <fmha/utils.h>
+#include <fmha/smem_tile.h>
+#include <fmha/gmem_tile.h>
+#include <fmha/mask.h>
+#include <fmha/softmax.h>
+
+namespace fmha {
+
+////////////////////////////////////////////////////////////////////////////////////////////////////
+
+struct Blockmask {
+
+    template<typename Params>
+    __device__ Blockmask(const Params &params, int loop_step_idx) :
+        blockmask_ptr(params.blockmask + loop_step_idx * params.seqlen_q / 16) {
+    }
 
-    CUTLASS_HOST_DEVICE
-    Array<T, N> operator()(Array<T, N> const &lhs, Array<T, N> const &rhs) const {
-        cutlass::plus<Array<T, N>> add_op;
-        return add_op(lhs, rhs);
+    __device__ int mask_val(int block_row_idx) const {
+        return blockmask_ptr[block_row_idx];
     }
 
+    const int *blockmask_ptr;
 };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-} // namespace cutlass
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+}  // namespace fmha
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/layout.h` & `flash_attn-1.0.9/csrc/flash_attn/src/fmha_kernel.h`

 * *Files 24% similar despite different names*

```diff
@@ -1,47 +1,78 @@
-/***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
- * SPDX-License-Identifier: BSD-3-Clause
- *
+/******************************************************************************
+ * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
+ * 
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of the NVIDIA CORPORATION nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ * 
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
- * 1. Redistributions of source code must retain the above copyright notice, this
- * list of conditions and the following disclaimer.
- *
- * 2. Redistributions in binary form must reproduce the above copyright notice,
- * this list of conditions and the following disclaimer in the documentation
- * and/or other materials provided with the distribution.
- *
- * 3. Neither the name of the copyright holder nor the names of its
- * contributors may be used to endorse or promote products derived from
- * this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
- * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
- * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
- * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
- * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
- * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
- * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- *
- **************************************************************************************************/
-/* \file
-   \brief Bind CUTLASS layouts to python
-*/
+ ******************************************************************************/
+
 #pragma once
-#include <pybind11/pybind11.h>
-#include <pybind11/stl_bind.h>
 
-#include "tensor.h"
-#include "matrix.h"
+#include <philox.cuh>
+
+#include <fmha.h>
+#include <fmha/utils.h>
+#include <fmha/smem_tile.h>
+#include <fmha/gmem_tile.h>
+#include <fmha/mask.h>
+#include <fmha/softmax.h>
+
+namespace fmha {
+
+////////////////////////////////////////////////////////////////////////////////////////////////////
+
+template<int THREADS_PER_CTA>
+struct BlockInfoPadded {
+
+    template<typename Params>
+    __device__ BlockInfoPadded(const Params &params,
+                               const int bidb,
+                               const int bidh,
+                               const int tidx)
+        : bidb(bidb), bidh(bidh), h(params.h) {
+
+        // The block index.
+        sum_s_k = params.cu_seqlens_k[bidb];
+        actual_seqlen_k = params.cu_seqlens_k[bidb + 1] - sum_s_k;
+        sum_s_q = params.cu_seqlens_q[bidb];
+        actual_seqlen_q = params.cu_seqlens_q[bidb + 1] - sum_s_q;
+
+        tidx_global = (bidb * params.h + bidh) * THREADS_PER_CTA + tidx;
+    }
+
+    __device__ bool stop_early(const int start_col = 0) const {
+        return actual_seqlen_k <= start_col;
+    }
 
+    int actual_seqlen_q;
+    int actual_seqlen_k;
+    int sum_s_q;
+    int sum_s_k;
+    int bidh;
+    int bidb;
+    int tidx_global;
+    int h;
+};
 
-namespace py = pybind11;
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
-void bind_layout(py::module &m) {
-    bind_tensor_layout(m);
-    bind_matrix_layout(m);
-}
+}  // namespace fmha
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/gemm.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reduction/init_reduction_operations.cu`

 * *Files 27% similar despite different names*

```diff
@@ -25,21 +25,41 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /* \file
-   \brief Bind gemm test to python
+   \brief Initialize operations for reduction operation in CUTLASS Library.
+
 */
-#pragma once
-#include <pybind11/pybind11.h>
-#include <pybind11/stl_bind.h>
 
-#include "host.h"
+#include "cutlass/cutlass.h"
+#include "cutlass/library/library.h"
+#include "cutlass/library/manifest.h"
+
+namespace cutlass {
+namespace library {
+///////////////////////////////////////////////////////////////////////////////////////////////
+//                             CUTLASS Reduction Instances                                   //
+///////////////////////////////////////////////////////////////////////////////////////////////
+void initialize_reduce_add_linear_combination_f32_f32_f16(Manifest &manifest);
+void initialize_reduce_add_linear_combination_f32_f32_f32(Manifest &manifest);
+void initialize_reduce_add_linear_combination_f64_f64_f64(Manifest &manifest);
+void initialize_reduce_add_linear_combination_cf32_cf32_cf32(Manifest &manifest);
+
+//
+// Entry point to construct operations
+//
+void initialize_all_reduction_op(Manifest &manifest) {
 
-namespace py = pybind11;
+  initialize_reduce_add_linear_combination_f32_f32_f16(manifest);
+  initialize_reduce_add_linear_combination_f32_f32_f32(manifest);
+  initialize_reduce_add_linear_combination_f64_f64_f64(manifest);
+  initialize_reduce_add_linear_combination_cf32_cf32_cf32(manifest);
 
-void bind_gemm_test(py::module &m) {
-    py::module_ host_submodule = m.def_submodule("host");
-    bind_gemm_host_reference(host_submodule);
 }
+
+///////////////////////////////////////////////////////////////////////////////////////////////////
+
+} // namespace library
+} // namespace cutlass
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/conv2d_operation.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/conv2d_operation.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/conv3d_operation.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/conv3d_operation.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/gemm_operation.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/gemm_operation.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/library_internal.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/library_internal.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/manifest.cpp` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/manifest.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/operation_table.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/operation_table.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/rank_2k_operation.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/rank_2k_operation.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/rank_k_operation.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/rank_k_operation.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/reduction/reduction_device.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reduction/reduction_device.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/reduction/reduction_operation.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reduction/reduction_operation.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/reference/conv2d.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reference/conv2d.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/reference/conv3d.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reference/conv3d.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/reference/conv_reference_operation.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reference/conv_reference_operation.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/reference/gemm.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reference/gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/reference/gemm_reference_operation.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reference/gemm_reference_operation.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/symm_operation.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/symm_operation.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/trmm_operation.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/trmm_operation.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/library/src/util.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/util.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/conv2d_operation_profiler.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/conv2d_operation_profiler.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/conv2d_operation_profiler.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/conv2d_operation_profiler.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/conv3d_operation_profiler.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/conv3d_operation_profiler.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/conv3d_operation_profiler.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/conv3d_operation_profiler.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/cublas_helpers.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/cublas_helpers.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/cublas_helpers.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/cublas_helpers.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/cudnn_helpers.cpp` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/cudnn_helpers.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/cudnn_helpers.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/cudnn_helpers.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/cutlass_profiler.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/cutlass_profiler.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/device_allocation.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/device_allocation.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/device_allocation.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/device_allocation.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/device_context.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/device_context.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/device_context.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/device_context.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/enumerated_types.cpp` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/enumerated_types.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/enumerated_types.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/enumerated_types.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/gemm_operation_profiler.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/gemm_operation_profiler.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/gemm_operation_profiler.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/gemm_operation_profiler.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/gpu_timer.cpp` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/gpu_timer.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/main.cpp` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/error_metrics.h`

 * *Files 27% similar despite different names*

```diff
@@ -1,7 +1,8 @@
+
 /***************************************************************************************************
  * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
@@ -24,30 +25,42 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/* \file
-   \brief 
-*/
-
-#include <iostream>
-
-#include "options.h"
+#pragma once
 
-#include "cutlass_profiler.h"
-
-///////////////////////////////////////////////////////////////////////////////////////////////////
+#include <cmath>
 
-int main(int argc, char const *arg[]) {
+#include "cutlass/cutlass.h"
+#include "cutlass/complex.h"
+#include "cutlass/util/reference/host/tensor_reduce.h"
+#include "cutlass/core_io.h"
+
+namespace cutlass  {
+namespace reference {
+namespace host {
+
+/// Helper to compute the relative error metric for tensor A_computed  w.r.t. to tensor A_reference
+template <
+  typename Element,
+  typename Layout,
+  typename ComputeType = double
+>
+ComputeType TensorRelativeErrorMetric(
+  TensorView<Element, Layout> view_A_computed,
+  TensorView<Element, Layout> view_B_reference,
+  ComputeType identity = ComputeType()
+) {
 
-  cutlass::CommandLine cmdline(argc, arg);
-  cutlass::profiler::Options options(cmdline);
-
-  cutlass::profiler::CutlassProfiler profiler(options);
-
-  return profiler();
+  return cutlass::reference::host::TensorNormDiff(view_A_computed, view_B_reference, identity) /
+   cutlass::reference::host::TensorNorm(view_B_reference, identity);
 }
 
+
 ///////////////////////////////////////////////////////////////////////////////////////////////////
+
+} // namespace host
+} // namespace reference
+} // namespace cutlass
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/operation_profiler.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/operation_profiler.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/operation_profiler.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/operation_profiler.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/options.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/options.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/options.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/options.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/performance_report.cpp` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/performance_report.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/performance_report.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/performance_report.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/problem_space.cpp` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/problem_space.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/problem_space.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/problem_space.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/rank_2k_operation_profiler.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/rank_2k_operation_profiler.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/rank_2k_operation_profiler.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/rank_2k_operation_profiler.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/rank_k_operation_profiler.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/rank_k_operation_profiler.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/rank_k_operation_profiler.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/rank_k_operation_profiler.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/symm_operation_profiler.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/symm_operation_profiler.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/symm_operation_profiler.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/symm_operation_profiler.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/trmm_operation_profiler.cu` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/trmm_operation_profiler.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/profiler/src/trmm_operation_profiler.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/trmm_operation_profiler.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/command_line.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/command_line.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/debug.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/debug.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_dump.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_dump.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_groupnorm.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_groupnorm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_layernorm.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_layernorm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_memory.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_memory.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nchw_to_nhwc.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nchw_to_nhwc.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_padding.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_padding.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_pooling.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_pooling.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_to_nchw.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_to_nchw.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_utils.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/inner_product.h`

 * *Files 27% similar despite different names*

```diff
@@ -24,104 +24,112 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
 /*! \file
-    \brief utils code for device cutlass code
+    \brief Reference implementation for GEMM in host-side code.
 */
-
 #pragma once
 
-#include <cuda_fp16.h>
-#include <float.h>
-#define FINAL_MASK 0xffffffff
+#include "cutlass/cutlass.h"
+#include "cutlass/array.h"
 
-struct half4 {
-    half x, y, z, w;
-};
+namespace cutlass {
+namespace reference {
+namespace detail {
+
+////////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Template function to compute an inner product.
+#pragma hd_warning_disable  // Suppresses warnings when attempting to instantiate with a
+                            // host-only type
+template <typename Atype, typename Btype, typename Ctype>
+CUTLASS_HOST_DEVICE
+Ctype inner_product(Atype a, Btype b, Ctype c) {
+  return Ctype(a) * Ctype(b) + c;
+}
 
-template<typename T, int NUM>
-__inline__ __device__ T warpReduceSum(T* val)
-{
-#pragma unroll
-    for (int i = 0; i < NUM; i++) {
-#pragma unroll
-        for (int mask = 16; mask > 0; mask >>= 1)
-            val[i] += __shfl_xor_sync(FINAL_MASK, val[i], mask, 32);
-    }
-    return (T)(0.0f);
+/// Specialization for matrix multiplication with binary operands
+template <>
+CUTLASS_HOST_DEVICE
+int inner_product<Array<bin1_t, 32>, Array<bin1_t, 32>, int>(
+    Array<bin1_t, 32> a,
+    Array<bin1_t, 32> b,
+    int c) {
+
+  int accum = 0;
+  for (int bit = 0; bit < 32; bit++) {
+    accum += a[bit] ^ b[bit];
+  }
+  return accum + c;
 }
 
-template<typename T, int NUM>
-__inline__ __device__ T blockReduceSum(T* val)
-{
-    __shared__ T shared[NUM][33];
-    int lane = threadIdx.x & 0x1f;
-    int wid = threadIdx.x >> 5;
-
-    warpReduceSum<T, NUM>(val);
-
-    if (lane == 0) {
-#pragma unroll
-        for (int i = 0; i < NUM; i++) {
-            shared[i][wid] = val[i];
-        }
-    }
-
-    __syncthreads();
-
-    bool is_mask = threadIdx.x < (blockDim.x / 32.f);
-#pragma unroll
-    for (int i = 0; i < NUM; i++) {
-        val[i] = is_mask ? shared[i][lane] : (T)(0.0f);
-    }
-    warpReduceSum<T, NUM>(val);
-    return (T)0.0f;
+/*
+/// Specialization for matrix multiplication with signed 4-bit integer operands
+template <>
+CUTLASS_HOST_DEVICE
+int inner_product<Array<int4b_t, 8>, Array<int4b_t, 8>, int>(
+    Array<int4b_t, 8> a,
+    Array<int4b_t, 8> b,
+    int c) {
+
+  int accum = 0;
+  for (int k = 0; k < 8; k++) {
+    accum += a[k] * b[k];
+  }
+  return accum + c;
 }
 
-template<typename T, int NUM>
-__inline__ __device__ T warpReduceMax(T* val)
-{
-#pragma unroll
-    for (int i = 0; i < NUM; i++) {
-#pragma unroll
-        for (int mask = 16; mask > 0; mask >>= 1)
-            val[i] = max(val[i], __shfl_xor_sync(FINAL_MASK, val[i], mask, 32));
-    }
-    return (T)(0.0f);
+/// Specialization for matrix multiplication with unsigned 4-bit integer operands
+template <>
+CUTLASS_HOST_DEVICE
+int inner_product<Array<uint4b_t, 8>, Array<uint4b_t, 8>, int>(
+    Array<uint4b_t, 8> a,
+    Array<uint4b_t, 8> b,
+    int c) {
+
+  int accum = 0;
+  for (int k = 0; k < 8; k++) {
+    accum += a[k] * b[k];
+  }
+  return accum + c;
 }
+*/
 
-template<typename T, int NUM>
-__inline__ __device__ T blockReduceMax(T* val)
-{
-    static __shared__ T shared[32][NUM];
-    int lane = threadIdx.x & 0x1f;  // in-warp idx
-    int wid = threadIdx.x >> 5;     // warp idx
-
-    warpReduceMax<T, NUM>(val);  // get maxx in each warp
-
-    if (lane == 0)  // record in-warp maxx by warp Idx
-    {
-#pragma unroll
-        for (int i = 0; i < NUM; i++) {
-            shared[wid][i] = val[i];
-        }
-    }
-
-    __syncthreads();
-
-    // Modify from blockDim.x << 5 to blockDim.x / 32. to prevent
-    // blockDim.x is not divided by 32
-    bool is_mask = threadIdx.x < (blockDim.x / 32.f);
-#pragma unroll
-    for (int i = 0; i < NUM; i++) {
-        val[i] = is_mask ? shared[lane][i] : (T)(-FLT_MAX);
-    }
-    warpReduceMax<T, NUM>(val);
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
-    return (T)0.0f;
-}
+template <typename SrcType, typename DstType>
+struct Cast {
+  // Default behavior: convert to the destination type
+#pragma hd_warning_disable  // Suppresses warnings when attempting to instantiate complex<T> with a
+                            // host-only type
+  CUTLASS_HOST_DEVICE
+  static DstType apply(SrcType src) { return static_cast<DstType>(src); };
+};
+
+template <>
+struct Cast<float, int8_t> {
+  CUTLASS_HOST_DEVICE
+  static int8_t apply(float src) {
+    // Clamp to the range of signed 8-bit integers.
+    return static_cast<int8_t>(fmaxf(-128.f, fminf(127.f, src)));
+  };
+};
+
+template <>
+struct Cast<float, uint8_t> {
+  CUTLASS_HOST_DEVICE
+  static uint8_t apply(float src) {
+    // Clamp to the range of signed 8-bit integers.
+    return static_cast<uint8_t>(fmaxf(0.f, fminf(255.f, src)));
+  };
+};
+
+////////////////////////////////////////////////////////////////////////////////////////////////////
+
+} // namespace detail
+} // namespace reference
+} // namespace cutlass
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/distribution.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/distribution.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_tensor.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_tensor.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_tensor_planar_complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_tensor_planar_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_uncompress.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_uncompress.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/convolution.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/convolution.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm_complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm_planar_complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm_planar_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/gemm.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_elementwise.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_elementwise.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_foreach.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_foreach.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/rank_2k_complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/rank_2k_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_compare.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_compare.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_fill.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_fill.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_foreach.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_foreach.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_reduce.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_reduce.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_relu.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_relu.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/convolution.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/convolution.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm_complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm_planar_complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm_planar_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k_complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_k_complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_k_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/symm.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/symm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/symm_complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/trmm.h`

 * *Files 26% similar despite different names*

```diff
@@ -25,294 +25,190 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Reference implementation for complex-valued SYMM update in host-side code.
+    \brief Reference implementation for TRMM in host-side code.
 
-    
+  
 */
 
 #pragma once
 
 #include "cutlass/blas3.h"
-#include "cutlass/complex.h"
 #include "cutlass/numeric_conversion.h"
 #include "cutlass/tensor_view.h"
 #include "cutlass/gemm/gemm.h"
-#include <assert.h>
+#include "cutlass/arch/mma.h"
+#include "cutlass/util/host_tensor.h"
+
+#include "cutlass/util/reference/host/gemm.h"
 
 namespace cutlass {
 namespace reference {
 namespace host {
 
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Computes a general matrix product among matrices (tensors of rank=2) pointed to by TensorRef
+/// Computes a Triangular Matrix Multiplication (tensors of rank=2) pointed to by TensorRef
 /// objects.
-///
-/// Explicitly naming types needed by this template can be cumbersome, particularly for the
-/// accumulator type, so a function argument 'initial_accum' is exposed. Passing
-/// AccumulatorType(0) as the last function argument can be easier than naming all template
-/// arguments explicitly.
 template <
   typename ElementA,
   typename LayoutA,
   SideMode SideModeA,
   FillMode FillModeA,
+  DiagType DiagTypeA,
   typename ElementB,
   typename LayoutB,
   typename ElementC,
   typename LayoutC,
   typename ScalarType,
   typename ComputeType,
-  BlasMode BlasMode_ = BlasMode::kSymmetric,
   typename InnerProductOp = multiply_add<ComputeType>,
   typename ConvertOp = NumericConverter<ElementC, ScalarType>
 >
-void compute_symm_complex(
+void compute_trmm(
   gemm::GemmCoord problem_size,
   ScalarType alpha,
   TensorRef<ElementA, LayoutA> tensor_a,
   TensorRef<ElementB, LayoutB> tensor_b,
-  ScalarType beta,
-  TensorRef<ElementC, LayoutC> tensor_c,
   TensorRef<ElementC, LayoutC> tensor_d,
-  ComputeType initial_accum,
-  int batch_count = 1,
-  int64_t batch_stride_A = 0,
-  int64_t batch_stride_B = 0,
-  int64_t batch_stride_C = 0,
-  int64_t batch_stride_D = 0) {
-  
-  static SideMode const kSideModeA = SideModeA;
-  static FillMode const kFillModeA = FillModeA;
-  static BlasMode const kBlasMode  = BlasMode_;
+  ComputeType initial_accum) {
 
   static_assert(
     LayoutA::kRank == 2 &&
-    LayoutB::kRank == 2 &&
     LayoutC::kRank == 2, "Tensors must be of rank 2");
 
-  static_assert(kSideModeA != SideMode::kInvalid
+  static_assert(SideModeA != SideMode::kInvalid
                 , "Side Mode can either be Left or Right.");
 
-  static_assert(
-    kFillModeA == FillMode::kLower || 
-    kFillModeA == FillMode::kUpper, 
-    "Fill Mode can either be Lower or Upper.");
+  static_assert(FillModeA == FillMode::kLower || FillModeA == FillMode::kUpper
+                , "Fill Mode can either be Lower or Upper.");
 
-  using CompareOp_w_diag =  typename TrMatrixCompareOp<kFillModeA, DiagType::kNonUnit>::Type;
-  using CompareOp_wo_diag = typename TrMatrixCompareOp<kFillModeA, DiagType::kZero>::Type;
+  using CompareOp = typename TrMatrixCompareOp<FillModeA, DiagTypeA>::Type;
 
   // Note: batch is ignored.
   int const M = problem_size.m();
   int const N = problem_size.n();
   // Assuming correct k-dimension value is passed
   int const K = problem_size.k();
-
+ 
   // Blocking necessary to speedup reference implementation
   int const Mblock = 16;
   int const Nblock = 16;
 
   ConvertOp convert_op;
   InnerProductOp inner_product_op;
-  CompareOp_w_diag compare_op_1;
-  CompareOp_wo_diag compare_op_2;
-
-  for (int batch_idx = 0; batch_idx < batch_count; ++batch_idx) {
-
-    // Compute matrix product using blocks
-    for (int row_block = 0; row_block < M; row_block += Mblock) {
-      for (int col_block = 0; col_block < N; col_block += Nblock) {
-
-        ComputeType accum[Mblock][Nblock];
-
-        for (int j = 0; j < Nblock; j++) {
-          for (int i = 0; i < Mblock; i++) {
-            accum[i][j] = initial_accum;
-          }
-        }
-
-        for (int k_block = 0; k_block < K; ++k_block) {
-          for (int j = 0; j < Nblock; j++) {
-            for (int i = 0; i < Mblock; i++) {
-              int row = row_block + i;
-              int col = col_block + j;
-
-              if (row < M && col < N) 
-              {
-                ElementA a_1 = ElementA();
-                ElementB b_1 = ElementB();
-                ElementA a_2 = ElementA();
-                ElementB b_2 = ElementB();
-                
-                // A x B or B x A (with diagonal)
-                if (kSideModeA == SideMode::kLeft) {
-                  a_1 = (compare_op_1(row, k_block)) ? 
-                        (tensor_a.at(MatrixCoord(row, k_block))) : ElementA();
-                  b_1 = tensor_b.at(MatrixCoord(k_block, col));
-                } else if (kSideModeA == SideMode::kRight) {
-                  a_1 = tensor_b.at(MatrixCoord(row, k_block));
-                  b_1 = (compare_op_1(k_block, col)) ? 
-                        tensor_a.at(MatrixCoord(k_block, col)) : ElementA();
-                }
-                ComputeType compute_a_1 = ComputeType(a_1);
-                ComputeType compute_b_1 = ComputeType(b_1);
-
-                // The imaginary parts of the diagonal elements of 
-                // a complex data type are assumed and set to zero
-                if (kBlasMode == BlasMode::kHermitian && kSideModeA == SideMode::kLeft && row == k_block) {
-                  compute_a_1 = real(compute_a_1);
-                } else if (kBlasMode == BlasMode::kHermitian && kSideModeA == SideMode::kRight && k_block == col) {
-                  compute_b_1 = real(compute_b_1);
-                }
-
-                accum[i][j] = inner_product_op(compute_a_1, compute_b_1,  accum[i][j]);
+  CompareOp compare_op;
 
-                // A^T x B or B x A^T (without diagonal)
-                if (kSideModeA == SideMode::kLeft) {
-                  a_2 = (compare_op_2(k_block, row)) ? 
-                        (tensor_a.at(MatrixCoord(k_block, row))) : ElementA();
-                  b_2 = tensor_b.at(MatrixCoord(k_block, col));
-                  if (kBlasMode == BlasMode::kHermitian)
-                    a_2 = conj(a_2);
-                } else if (kSideModeA == SideMode::kRight) {
-                  a_2 = tensor_b.at(MatrixCoord(row, k_block));
-                  b_2 = (compare_op_2(col, k_block)) ? 
-                        tensor_a.at(MatrixCoord(col, k_block)) : ElementA();
-                  if (kBlasMode == BlasMode::kHermitian)
-                    b_2 = conj(b_2);
-                }
+  for (int row_block = 0; row_block < M; row_block += Mblock) {
+    for (int col_block = 0; col_block < N; col_block += Nblock) {
 
-                ComputeType compute_a_2 = ComputeType(a_2);
-                ComputeType compute_b_2 = ComputeType(b_2);
+      ComputeType accum[Mblock][Nblock];
 
-                accum[i][j] = inner_product_op(compute_a_2, compute_b_2, accum[i][j]);
-              }
-            }
-          }
+      for (int j = 0; j < Nblock; j++) {
+        for (int i = 0; i < Mblock; i++) {
+          accum[i][j] = initial_accum;
         }
+      }
 
+      for (int k_block = 0; k_block < K; ++k_block) {
         for (int j = 0; j < Nblock; j++) {
           for (int i = 0; i < Mblock; i++) {
             int row = row_block + i;
             int col = col_block + j;
 
-            MatrixCoord coord = MatrixCoord(row, col);
-
             if (row < M && col < N) {
+              ElementA a = ElementA();
+              ElementB b = ElementB();
 
-              ScalarType c = tensor_c.at(coord);
+              if (SideModeA == SideMode::kLeft) {
+                a = (compare_op(row, k_block)) ? 
+                            (tensor_a.at(MatrixCoord(row, k_block))) : ElementA(0);
+                if (row == k_block && DiagTypeA == DiagType::kUnit) {
+                  a = ElementA(1);
+                }
+                b = tensor_b.at(MatrixCoord(k_block, col));
+              } else if (SideModeA == SideMode::kRight) {
+                a = tensor_b.at(MatrixCoord(row, k_block));
+                b = (compare_op(k_block, col)) ? 
+                      tensor_a.at(MatrixCoord(k_block, col)) : ElementA(0);
+                if (k_block == col && DiagTypeA == DiagType::kUnit) {
+                  b = ElementA(1);
+                }
+              }
+                            
+              ComputeType compute_a(cast_if_scalar<ComputeType>(a));
+              ComputeType compute_b(cast_if_scalar<ComputeType>(b));
 
-              tensor_d.at(coord) = convert_op(
-                alpha * ScalarType(accum[i][j]) + 
-                beta * c);
+              accum[i][j] = inner_product_op(compute_a, compute_b, accum[i][j]);
             }
           }
         }
+      }
 
-      } // for (col_block)
-    } // for (row_block)
-
-    tensor_a.add_pointer_offset(batch_stride_A);
-    tensor_b.add_pointer_offset(batch_stride_B);
-    tensor_c.add_pointer_offset(batch_stride_C);
-    tensor_d.add_pointer_offset(batch_stride_D);
-
-  } // for (batch_idx)
+      for (int j = 0; j < Nblock; j++) {
+        for (int i = 0; i < Mblock; i++) {
+          int row = row_block + i;
+          int col = col_block + j;
+
+          MatrixCoord coord = MatrixCoord(row, col);
+
+          if (row < M && col < N) {
+            tensor_d.at(coord) = convert_op(
+              alpha * ScalarType(accum[i][j]));
+          }
+        }
+      }
+    }
+  }
 }
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
   typename ElementA,
   typename LayoutA,
   SideMode SideModeA,
   FillMode FillModeA,
+  DiagType DiagTypeA,
   typename ElementB,
   typename LayoutB,
   typename ElementC,
   typename LayoutC,
   typename ScalarType,
   typename ComputeType,
-  BlasMode BlasMode_ = cutlass::BlasMode::kSymmetric,
-  typename InnerProductOp = cutlass::arch::OpMultiplyAddComplex
+  typename InnerProductOp = cutlass::arch::OpMultiplyAdd
 >
-struct SymmComplex;
+struct Trmm;
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for multiply-add
-template <typename ElementA, typename LayoutA,
-          SideMode SideModeA, FillMode FillModeA, 
-          typename ElementB, typename LayoutB,
-          typename ElementC, typename LayoutC,
-          typename ScalarType, typename ComputeType,
-          BlasMode BlasMode_>
-struct SymmComplex<ElementA, LayoutA, 
-                   SideModeA, FillModeA,
-                   ElementB, LayoutB,
-                   ElementC, LayoutC, ScalarType,
-                   ComputeType, BlasMode_,
-                   arch::OpMultiplyAddComplex> {
-
-  void operator()(gemm::GemmCoord problem_size, ScalarType alpha,
-                  TensorRef<ElementA, LayoutA> tensor_a,
-                  TensorRef<ElementB, LayoutB> tensor_b, ScalarType beta,
-                  TensorRef<ElementC, LayoutC> tensor_c,
-                  TensorRef<ElementC, LayoutC> tensor_d,
-                  ComputeType initial_accum = ComputeType(0)) {
-    static_assert(
-        LayoutA::kRank == 2 && LayoutC::kRank == 2,
-        "Tensors must be of rank 2");
-
-    compute_symm_complex<ElementA, LayoutA,
-                 SideModeA, FillModeA,
-                 ElementB, LayoutB,
-                 ElementC, LayoutC, 
-                 ScalarType, ComputeType, BlasMode_, multiply_add<ComputeType>>(
-                 problem_size, alpha, tensor_a, tensor_b, beta, tensor_c, tensor_d, initial_accum);
-  }
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Partial specialization for gaussian multiply-add 
-template <typename ElementA, typename LayoutA,
-          SideMode SideModeA, FillMode FillModeA,
-          typename ElementB, typename LayoutB,
-          typename ElementC, typename LayoutC,
-          typename ScalarType, typename ComputeType,
-          BlasMode BlasMode_>
-struct SymmComplex<ElementA, LayoutA, 
-                   SideModeA, FillModeA, 
-                   ElementB, LayoutB,
-                   ElementC, LayoutC, ScalarType,
-                   ComputeType, BlasMode_,
-                   arch::OpMultiplyAddGaussianComplex> {
+template <typename ElementA, typename LayoutA, SideMode SideModeA,
+           FillMode FillModeA, DiagType DiagTypeA, 
+           typename ElementB, typename LayoutB,
+           typename ElementC, typename LayoutC,
+          typename ScalarType, typename ComputeType>
+struct Trmm<ElementA, LayoutA, SideModeA, FillModeA, DiagTypeA, ElementB, LayoutB,
+            ElementC, LayoutC, ScalarType,
+            ComputeType, arch::OpMultiplyAdd> {
 
   void operator()(gemm::GemmCoord problem_size, ScalarType alpha,
                   TensorRef<ElementA, LayoutA> tensor_a,
-                  TensorRef<ElementB, LayoutB> tensor_b, ScalarType beta,
-                  TensorRef<ElementC, LayoutC> tensor_c,
+                  TensorRef<ElementB, LayoutB> tensor_b,
                   TensorRef<ElementC, LayoutC> tensor_d,
                   ComputeType initial_accum = ComputeType(0)) {
     static_assert(
         LayoutA::kRank == 2 && LayoutC::kRank == 2,
         "Tensors must be of rank 2");
 
-    compute_symm_complex<ElementA, LayoutA,
-                 SideModeA, FillModeA,
-                 ElementB, LayoutB,
-                 ElementC, LayoutC, 
-                 ScalarType, ComputeType, BlasMode_, multiply_add<ComputeType>>(
-                 problem_size, alpha, tensor_a, tensor_b, beta, tensor_c, tensor_d, initial_accum);
+    compute_trmm<ElementA, LayoutA, SideModeA, FillModeA, DiagTypeA, ElementB, LayoutB,
+                 ElementC, LayoutC, ScalarType, ComputeType, multiply_add<ComputeType>>(
+                 problem_size, alpha, tensor_a, tensor_b, tensor_d, initial_accum);
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace host
 } // namespace reference
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_compare.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_compare.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_copy.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_copy.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_elementwise.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_elementwise.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_fill.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_fill.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_foreach.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_foreach.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_reduce.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_reduce.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/trmm_complex.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/trmm_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/tensor_view_io.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/tensor_view_io.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/type_traits.h` & `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/type_traits.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/flash_api.cpp` & `flash_attn-1.0.9/csrc/flash_attn/fmha_api.cpp`

 * *Files 11% similar despite different names*

```diff
@@ -1,348 +1,224 @@
 /******************************************************************************
- * Copyright (c) 2023, Tri Dao.
+ * Copyright (c) 2022, Tri Dao.
+ * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
+ * 
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of the NVIDIA CORPORATION nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ * 
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
  ******************************************************************************/
 
 #include <torch/extension.h>
 #include <ATen/cuda/CUDAContext.h>
 #include <c10/cuda/CUDAGuard.h>
 
-#include <cutlass/numeric_types.h>
-
-#include "flash.h"
-#include "static_switch.h"
+#include "fmha.h"
 
 #define CHECK_SHAPE(x, ...) TORCH_CHECK(x.sizes() == torch::IntArrayRef({__VA_ARGS__}), #x " must have shape (" #__VA_ARGS__ ")")
 
 
-void set_params_fprop(Flash_fwd_params &params,
+void set_params_fprop(FMHA_fprop_params &params,
                       // sizes
                       const size_t b,
                       const size_t seqlen_q,
                       const size_t seqlen_k,
-                      const size_t seqlen_q_rounded,
-                      const size_t seqlen_k_rounded,
                       const size_t h,
                       const size_t d,
-                      const size_t d_rounded,
                       // device pointers
                       const at::Tensor q,
                       const at::Tensor k,
                       const at::Tensor v,
                       at::Tensor out,
                       void *cu_seqlens_q_d,
                       void *cu_seqlens_k_d,
-                      void *p_d,
+                      void *o_tmp_d,
+                      void *s_d,
                       void *softmax_lse_d,
                       float p_dropout,
                       float softmax_scale,
-                      bool is_causal) {
+                      bool is_causal,
+                      int num_splits) {
+
+    Data_type acc_type = DATA_TYPE_FP32;
+    Data_type data_type = !(q.dtype() == torch::kBFloat16) ? DATA_TYPE_FP16 : DATA_TYPE_BF16;
 
     // Reset the parameters
     memset(&params, 0, sizeof(params));
 
     params.is_bf16 = q.dtype() == torch::kBFloat16;
 
     // Set the pointers and strides.
     params.q_ptr = q.data_ptr();
     params.k_ptr = k.data_ptr();
     params.v_ptr = v.data_ptr();
-    // All stride are in elements, not bytes.
-    params.q_row_stride = q.stride(-3);
-    params.k_row_stride = k.stride(-3);
-    params.v_row_stride = v.stride(-3);
-    params.q_head_stride = q.stride(-2);
-    params.k_head_stride = k.stride(-2);
-    params.v_head_stride = v.stride(-2);
+    params.q_row_stride_in_elts = q.stride(0);
+    params.k_row_stride_in_elts = k.stride(0);
+    params.v_row_stride_in_elts = v.stride(0);
+    params.q_head_stride_in_elts = q.stride(1);
+    params.k_head_stride_in_elts = k.stride(1);
+    params.v_head_stride_in_elts = v.stride(1);
     params.o_ptr = out.data_ptr();
-    params.o_row_stride = out.stride(-3);
-    params.o_head_stride = out.stride(-2);
-
-    if (cu_seqlens_q_d == nullptr) {
-        params.q_batch_stride = q.stride(0);
-        params.k_batch_stride = k.stride(0);
-        params.v_batch_stride = v.stride(0);
-        params.o_batch_stride = out.stride(0);
-    }
+    params.o_row_stride_in_elts = out.stride(0);
+    params.o_head_stride_in_elts = out.stride(1);
+    params.o_tmp_ptr = o_tmp_d;
+    params.o_tmp_row_stride_in_elts = h * d;
+    params.o_tmp_head_stride_in_elts = d;
 
     params.cu_seqlens_q = static_cast<int *>(cu_seqlens_q_d);
     params.cu_seqlens_k = static_cast<int *>(cu_seqlens_k_d);
 
-    // P = softmax(QK^T)
-    params.p_ptr = p_d;
+    // S = softmax(P)
+    params.s_ptr = s_d;
+    params.s_stride_in_bytes = get_size_in_bytes(b * h * seqlen_k, data_type);
 
     // Softmax sum
     params.softmax_lse_ptr = softmax_lse_d;
 
     // Set the dimensions.
     params.b = b;
     params.h = h;
     params.seqlen_q = seqlen_q;
     params.seqlen_k = seqlen_k;
-    params.seqlen_q_rounded = seqlen_q_rounded;
-    params.seqlen_k_rounded = seqlen_k_rounded;
     params.d = d;
-    params.d_rounded = d_rounded;
 
     // Set the different scale values.
-    params.scale_softmax = softmax_scale;
-    params.scale_softmax_log2 = softmax_scale * M_LOG2E;
+    // const float scale_bmm1 = 1.f / sqrtf(d);
+    const float scale_bmm1 = softmax_scale;
+
+    params.scale_bmm1f = scale_bmm1;
+    set_alpha(params.scale_bmm1, scale_bmm1, data_type);
 
     // Set this to probability of keeping an element to simplify things.
     params.p_dropout = 1.f - p_dropout;
     // Convert p from float to int so we don't have to convert the random uint to float to compare.
     // [Minor] We want to round down since when we do the comparison we use <= instead of <
-    // params.p_dropout_in_uint = uint32_t(std::floor(params.p_dropout * 4294967295.0));
-    // params.p_dropout_in_uint16_t = uint16_t(std::floor(params.p_dropout * 65535.0));
-    params.p_dropout_in_uint8_t = uint8_t(std::floor(params.p_dropout * 255.0));
+    params.p_dropout_in_uint = uint32_t(std::floor(params.p_dropout * 4294967295.0));
+    params.p_dropout_in_uint16_t = uint16_t(std::floor(params.p_dropout * 65535.0));
     params.rp_dropout = 1.f / params.p_dropout;
-    params.scale_softmax_rp_dropout = params.rp_dropout * params.scale_softmax;
+    params.scale_bmm1_rp_dropout = params.rp_dropout * params.scale_bmm1f;
     TORCH_CHECK(p_dropout < 1.f);
+    set_alpha(params.scale_dropout, params.rp_dropout, data_type);
 
     params.is_causal = is_causal;
+    params.num_splits = num_splits;
 }
 
-void set_params_dgrad(Flash_bwd_params &params,
+void set_params_dgrad(FMHA_dgrad_params &params,
                       // sizes
                       const size_t b,
                       const size_t seqlen_q,
                       const size_t seqlen_k,
-                      const size_t seqlen_q_rounded,
-                      const size_t seqlen_k_rounded,
                       const size_t h,
                       const size_t d,
-                      const size_t d_rounded,
                       // device pointers
                       const at::Tensor q,
                       const at::Tensor k,
                       const at::Tensor v,
                       const at::Tensor out,
-                      const at::Tensor dout,
                       at::Tensor dq,
                       at::Tensor dk,
                       at::Tensor dv,
                       void *cu_seqlens_q_d,
                       void *cu_seqlens_k_d,
-                      void *dq_accum_d,
-                      void *dk_accum_d,
-                      void *dv_accum_d,
+                      void *dq_tmp_d,
+                      void *do_packed_d,
                       void *softmax_lse_d,
                       void *dsoftmax_sum_d,
                       float p_dropout,
                       float softmax_scale,
-                      bool is_causal) {
+                      bool is_causal,
+                      int num_splits) {
 
     set_params_fprop(params,
-                     b, seqlen_q, seqlen_k, seqlen_q_rounded, seqlen_k_rounded, h, d, d_rounded,
+                     b, seqlen_q, seqlen_k, h, d,
                      q, k, v, out,
                      cu_seqlens_q_d,
                      cu_seqlens_k_d,
+                     dq_tmp_d,  // Reusing the o_tmp_ptr variable to store dq_tmp
                      nullptr,
                      softmax_lse_d,
                      p_dropout,
                      softmax_scale,
-                     is_causal);
+                     is_causal,
+                     num_splits);
 
     // Set the pointers and strides.
-    params.do_ptr = dout.data_ptr();
-    params.do_row_stride = dout.stride(-3);
-    params.do_head_stride = dout.stride(-2);
     params.dq_ptr = dq.data_ptr();
     params.dk_ptr = dk.data_ptr();
     params.dv_ptr = dv.data_ptr();
-    params.dq_row_stride = dq.stride(-3);
-    params.dk_row_stride = dk.stride(-3);
-    params.dv_row_stride = dv.stride(-3);
-    params.dq_head_stride = dq.stride(-2);
-    params.dk_head_stride = dk.stride(-2);
-    params.dv_head_stride = dv.stride(-2);
-
-    if (cu_seqlens_q_d == nullptr) {
-        params.do_batch_stride = dout.stride(0);
-        params.dq_batch_stride = dq.stride(0);
-        params.dk_batch_stride = dk.stride(0);
-        params.dv_batch_stride = dv.stride(0);
-    }
-
-    params.dq_accum_ptr = dq_accum_d;
-    params.dk_accum_ptr = dk_accum_d;
-    params.dv_accum_ptr = dv_accum_d;
+    params.dq_row_stride_in_elts = dq.stride(0);
+    params.dk_row_stride_in_elts = dk.stride(0);
+    params.dv_row_stride_in_elts = dv.stride(0);
+    params.dq_head_stride_in_elts = dq.stride(1);
+    params.dk_head_stride_in_elts = dk.stride(1);
+    params.dv_head_stride_in_elts = dv.stride(1);
+    params.do_ptr = do_packed_d;
 
     // Softmax sum
     params.dsoftmax_sum = dsoftmax_sum_d;
 }
 
-void run_mha_fwd(Flash_fwd_params &params, cudaStream_t stream) {
-    FP16_SWITCH(!params.is_bf16, [&] {
-        if (params.d <= 32) {
-            run_mha_fwd_<elem_type, 32>(params, stream);
-        } else if (params.d <= 64) {
-            run_mha_fwd_<elem_type, 64>(params, stream);
-        } else if (params.d <= 96) {
-            run_mha_fwd_<elem_type, 96>(params, stream);
-        } else if (params.d <= 128) {
-            run_mha_fwd_<elem_type, 128>(params, stream);
-        } else if (params.d <= 160) {
-            run_mha_fwd_<elem_type, 160>(params, stream);
-        } else if (params.d <= 192) {
-            run_mha_fwd_<elem_type, 192>(params, stream);
-        }
-    });
+void run_fmha_fwd(Launch_params<FMHA_fprop_params> &launch_params) {
+    if (launch_params.params.d <= 32) {
+        run_fmha_fwd_hdim32(launch_params);
+    } else if (launch_params.params.d <= 64) {
+        run_fmha_fwd_hdim64(launch_params);
+    } else if (launch_params.params.d <= 128) {
+        run_fmha_fwd_hdim128(launch_params);
+    }
 }
 
 std::vector<at::Tensor>
-mha_fwd(const at::Tensor &q,         // batch_size x seqlen_q x num_heads x head_size
-        const at::Tensor &k,         // batch_size x seqlen_k x num_heads x head_size
-        const at::Tensor &v,         // batch_size x seqlen_k x num_heads x head_size
-        c10::optional<at::Tensor> &out_,             // batch_size x seqlen_q x num_heads x head_size
+mha_fwd(const at::Tensor &q,         // total_q x num_heads x head_size, total_q := \sum_{i=0}^{b} s_i
+        const at::Tensor &k,         // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
+        const at::Tensor &v,         // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
+        at::Tensor &out,             // total_q x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
+        const at::Tensor &cu_seqlens_q,  // b+1
+        const at::Tensor &cu_seqlens_k,  // b+1
+        const int max_seqlen_q_,
+        const int max_seqlen_k_,
         const float p_dropout,
         const float softmax_scale,
+        const bool zero_tensors,
         const bool is_causal,
         const bool return_softmax,
+        const int num_splits,
         c10::optional<at::Generator> gen_) {
 
     auto dprops = at::cuda::getCurrentDeviceProperties();
     bool is_sm75 = dprops->major == 7 && dprops->minor == 5;
+    bool is_sm80 = dprops->major == 8 && dprops->minor == 0;
     bool is_sm8x = dprops->major == 8 && dprops->minor >= 0;
     bool is_sm90 = dprops->major == 9 && dprops->minor == 0;
     TORCH_CHECK(is_sm90 || is_sm8x || is_sm75);
-
-    auto q_dtype = q.dtype();
-    TORCH_CHECK(q_dtype == torch::kFloat16 || ((is_sm90 || is_sm8x) && q_dtype == torch::kBFloat16));
-    TORCH_CHECK(k.dtype() == q_dtype);
-    TORCH_CHECK(v.dtype() == q_dtype);
-
-    TORCH_CHECK(q.is_cuda());
-    TORCH_CHECK(k.is_cuda());
-    TORCH_CHECK(v.is_cuda());
-
-    TORCH_CHECK(q.stride(-1) == 1);
-    TORCH_CHECK(k.stride(-1) == 1);
-    TORCH_CHECK(v.stride(-1) == 1);
-
-    const auto sizes = q.sizes();
-
-    const int batch_size = sizes[0];
-    const int seqlen_q = sizes[1];
-    const int num_heads = sizes[2];
-    const int head_size_og = sizes[3];
-    const int seqlen_k = k.size(1);
-    TORCH_CHECK(head_size_og <= 192);
-
-    CHECK_SHAPE(q, batch_size, seqlen_q, num_heads, head_size_og);
-    CHECK_SHAPE(k, batch_size, seqlen_k, num_heads, head_size_og);
-    CHECK_SHAPE(v, batch_size, seqlen_k, num_heads, head_size_og);
-
-    at::Tensor q_padded, k_padded, v_padded;
-    if (head_size_og % 8 != 0) {
-        q_padded = torch::nn::functional::pad(q, torch::nn::functional::PadFuncOptions({0, 8 - head_size_og % 8}));
-        k_padded = torch::nn::functional::pad(k, torch::nn::functional::PadFuncOptions({0, 8 - head_size_og % 8}));
-        v_padded = torch::nn::functional::pad(v, torch::nn::functional::PadFuncOptions({0, 8 - head_size_og % 8}));
-    } else {
-        q_padded = q;
-        k_padded = k;
-        v_padded = v;
-    }
-
-    at::Tensor out;
-    if (out_.has_value()) {
-        out = out_.value();
-        TORCH_CHECK(out.dtype() == q_dtype);
-        TORCH_CHECK(out.is_cuda());
-        TORCH_CHECK(out.stride(-1) == 1);
-        CHECK_SHAPE(out, batch_size, seqlen_q, num_heads, head_size_og);
-        if (head_size_og % 8 != 0) { out = torch::empty_like(q_padded); }
-    } else {
-        out = torch::empty_like(q_padded);
-    }
-
-    auto round_multiple = [](int x, int m) { return (x + m - 1) / m * m; };
-    const int head_size = round_multiple(head_size_og, 8);
-    const int head_size_rounded = round_multiple(head_size, 32);
-    const int seqlen_q_rounded = round_multiple(seqlen_q, 128);
-    const int seqlen_k_rounded = round_multiple(seqlen_k, 128);
-
-    // Otherwise the kernel will be launched from cuda:0 device
-    // Cast to char to avoid compiler warning about narrowing
-    at::cuda::CUDAGuard device_guard{(char)q.get_device()};
-
-    auto opts = q.options();
-
-    auto softmax_lse = torch::empty({batch_size, num_heads, seqlen_q}, opts.dtype(at::kFloat));
-    at::Tensor p;
-    // Only return softmax if there's dropout to reduce compilation time
-    if (return_softmax) {
-        TORCH_CHECK(p_dropout > 0.0f, "return_softmax is only supported when p_dropout > 0.0");
-        p = torch::empty({ batch_size, num_heads, seqlen_q_rounded, seqlen_k_rounded }, opts);
-    }
-
-    Flash_fwd_params params;
-    set_params_fprop(params,
-                     batch_size,
-                     seqlen_q, seqlen_k,
-                     seqlen_q_rounded, seqlen_k_rounded,
-                     num_heads,
-                     head_size, head_size_rounded,
-                     q_padded, k_padded, v_padded, out,
-                     /*cu_seqlens_q_d=*/nullptr,
-                     /*cu_seqlens_k_d=*/nullptr,
-                     return_softmax ? p.data_ptr() : nullptr,
-                     softmax_lse.data_ptr(),
-                     p_dropout,
-                     softmax_scale,
-                     is_causal);
-
-    if (p_dropout > 0.0)  {
-        // number of times random will be generated per thread, to offset philox counter in thc random
-        // state
-        // We use a custom RNG that increases the offset by batch_size * nheads * 32.
-        int64_t counter_offset = params.b * params.h * 32;
-        auto gen = at::get_generator_or_default<at::CUDAGeneratorImpl>(
-            gen_, at::cuda::detail::getDefaultCUDAGenerator());
-        // See Note [Acquire lock when using random generators]
-        std::lock_guard<std::mutex> lock(gen->mutex_);
-        params.philox_args = gen->philox_cuda_state(counter_offset);
-    }
-
     auto stream = at::cuda::getCurrentCUDAStream().stream();
-    run_mha_fwd(params, stream);
-
-    at::Tensor out_padded = out;
-    if (head_size_og % 8 != 0) {
-        out = out.index({"...", torch::indexing::Slice(torch::indexing::None, head_size_og)});
-        if (out_.has_value()) { out_.value().copy_(out); }
-    }
-
-    return {out, q_padded, k_padded, v_padded, out_padded, softmax_lse, p};
-}
-
-std::vector<at::Tensor>
-mha_varlen_fwd(const at::Tensor &q,  // total_q x num_heads x head_size, total_q := \sum_{i=0}^{b} s_i
-               const at::Tensor &k,  // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
-               const at::Tensor &v,  // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
-               at::Tensor &out,      // total_q x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
-               const at::Tensor &cu_seqlens_q,  // b+1
-               const at::Tensor &cu_seqlens_k,  // b+1
-               const int max_seqlen_q,
-               const int max_seqlen_k,
-               const float p_dropout,
-               const float softmax_scale,
-               const bool zero_tensors,
-               const bool is_causal,
-               const bool return_softmax,
-               c10::optional<at::Generator> gen_) {
-
-    auto dprops = at::cuda::getCurrentDeviceProperties();
-    bool is_sm75 = dprops->major == 7 && dprops->minor == 5;
-    bool is_sm8x = dprops->major == 8 && dprops->minor >= 0;
-    bool is_sm90 = dprops->major == 9 && dprops->minor == 0;
-    TORCH_CHECK(is_sm90 || is_sm8x || is_sm75);
+    bool is_dropout = p_dropout > 0.0;
+    Launch_params<FMHA_fprop_params> launch_params(dprops, stream, is_dropout, return_softmax);
 
     auto q_dtype = q.dtype();
-    TORCH_CHECK(q_dtype == torch::kFloat16 || ((is_sm90 || is_sm8x) && q_dtype == torch::kBFloat16));
+    TORCH_CHECK(q_dtype == torch::kFloat16 || ((is_sm8x || is_sm90) && q_dtype == torch::kBFloat16));
     TORCH_CHECK(k.dtype() == q_dtype);
     TORCH_CHECK(v.dtype() == q_dtype);
     TORCH_CHECK(out.dtype() == q_dtype);
     TORCH_CHECK(cu_seqlens_q.dtype() == torch::kInt32);
     TORCH_CHECK(cu_seqlens_k.dtype() == torch::kInt32);
 
     TORCH_CHECK(q.is_cuda());
@@ -357,450 +233,564 @@
     TORCH_CHECK(v.stride(-1) == 1);
     TORCH_CHECK(out.stride(-1) == 1);
     TORCH_CHECK(cu_seqlens_q.is_contiguous());
     TORCH_CHECK(cu_seqlens_k.is_contiguous());
 
     const auto sizes = q.sizes();
 
-    const int total_q = sizes[0];
     const int batch_size = cu_seqlens_q.numel() - 1;
-    const int num_heads = sizes[1];
-    const int head_size = sizes[2];
-    const int total_k = k.size(0);
+    const int total_q = sizes[TOTAL_DIM];
+    const int num_heads = sizes[H_DIM];
+    const int head_size = sizes[D_DIM];
+    const int total_k = k.size(TOTAL_DIM);
     TORCH_CHECK(batch_size > 0);
-    TORCH_CHECK((head_size % 8 == 0) && (head_size <= 192));
-    // const int head_size_rounded = head_size <= 64 ? 64 : 128;
-    const int head_size_rounded = head_size <= 32 ? 32 : (head_size <= 64 ? 64 : 128);
+    TORCH_CHECK((head_size % 8 == 0) && (head_size <= 128));
 
     CHECK_SHAPE(q, total_q, num_heads, head_size);
     CHECK_SHAPE(k, total_k, num_heads, head_size);
     CHECK_SHAPE(v, total_k, num_heads, head_size);
     CHECK_SHAPE(out, total_q, num_heads, head_size);
     CHECK_SHAPE(cu_seqlens_q, batch_size + 1);
     CHECK_SHAPE(cu_seqlens_k, batch_size + 1);
 
-    // int max_seqlen_q = ((max_seqlen_q + 16 - 1) / 16) * 16;
+    int blocksize_c = head_size > 64 ? 128 : 256;
+    // Need to round max_seqlen_k to multiples of blocksize_c
+    int max_seqlen_k = ((max_seqlen_k_ + blocksize_c - 1) / blocksize_c) * blocksize_c;
+    if( max_seqlen_k_ <= 128 ) {
+        max_seqlen_k = 128;
+    } else if( max_seqlen_k_ <= 256 ) {
+        max_seqlen_k = 256;
+    }
+    int max_seqlen_q = ((max_seqlen_q_ + 16 - 1) / 16) * 16;
+    bool loop = max_seqlen_k > blocksize_c;
 
     // Otherwise the kernel will be launched from cuda:0 device
     // Cast to char to avoid compiler warning about narrowing
     at::cuda::CUDAGuard device_guard{(char)q.get_device()};
 
     auto opts = q.options();
 
+    // auto o = torch::empty({ total_q, num_heads, head_size }, opts);
+
+    at::Tensor o_tmp;
+    if (loop) { o_tmp = torch::empty({total_q, num_heads, head_size}, opts.dtype(at::kFloat)); }
+
     auto softmax_lse = torch::empty({batch_size, num_heads, max_seqlen_q}, opts.dtype(at::kFloat));
+    // auto softmax_lse = torch::full({batch_size, num_heads, max_seqlen_k}, -std::numeric_limits<float>::infinity(), opts.dtype(at::kFloat));
+
     at::Tensor s;
     if (return_softmax) { s = torch::empty({ batch_size, num_heads, max_seqlen_q, max_seqlen_k }, opts); }
 
     if( zero_tensors ) {
         out.zero_();
         softmax_lse.fill_(-std::numeric_limits<float>::infinity());
         if (return_softmax) {s.zero_();}
     }
 
-    Flash_fwd_params params;
-    set_params_fprop(params,
+    auto gen = at::get_generator_or_default<at::CUDAGeneratorImpl>(
+        gen_, at::cuda::detail::getDefaultCUDAGenerator());
+
+    set_params_fprop(launch_params.params,
                      batch_size,
-                     max_seqlen_q, max_seqlen_k,
-                     max_seqlen_q, max_seqlen_k, // TODO: round these
+                     max_seqlen_q,
+                     max_seqlen_k,
                      num_heads,
-                     head_size, head_size_rounded,
+                     head_size,
                      q, k, v, out,
                      cu_seqlens_q.data_ptr(),
                      cu_seqlens_k.data_ptr(),
+                     loop ? o_tmp.data_ptr() : nullptr,
                      return_softmax ? s.data_ptr() : nullptr,
                      softmax_lse.data_ptr(),
                      p_dropout,
                      softmax_scale,
-                     is_causal);
+                     is_causal,
+                     num_splits);
 
-    if (p_dropout > 0.0)  {
-        // number of times random will be generated per thread, to offset philox counter in thc random
-        // state
-        // We use a custom RNG that increases the offset by batch_size * nheads * 32.
-        int64_t counter_offset = params.b * params.h * 32;
-        auto gen = at::get_generator_or_default<at::CUDAGeneratorImpl>(
-            gen_, at::cuda::detail::getDefaultCUDAGenerator());
+    // number of times random will be generated per thread, to offset philox counter in thc random
+    // state
+    // We use a custom RNG that increases the offset by batch_size * nheads * 32.
+    int64_t counter_offset = launch_params.params.b * launch_params.params.h * 32;
+    auto options = torch::TensorOptions().dtype(torch::kFloat32).device(torch::kCUDA);
+    auto rng_state = torch::empty({2}, options.dtype(torch::kInt64));
+    // Forward kernel will populate memory with the seed and offset.
+    launch_params.params.rng_state = reinterpret_cast<uint64_t*>(rng_state.data_ptr());
+
+    if( is_dropout ) {
         // See Note [Acquire lock when using random generators]
         std::lock_guard<std::mutex> lock(gen->mutex_);
-        params.philox_args = gen->philox_cuda_state(counter_offset);
+        launch_params.params.philox_args = gen->philox_cuda_state(counter_offset);
     }
 
-    auto stream = at::cuda::getCurrentCUDAStream().stream();
-    run_mha_fwd(params, stream);
+    run_fmha_fwd(launch_params);
 
     std::vector<at::Tensor> result = {softmax_lse};
-    if (return_softmax) { result.push_back(s); }
+    result.push_back(rng_state);
+    if (return_softmax) {result.push_back(s);}
     return result;
 }
 
-void run_mha_bwd(Flash_bwd_params &params, cudaStream_t stream, const bool configure) {
-    FP16_SWITCH(!params.is_bf16, [&] {
-        if (params.d <= 32) {
-            run_mha_bwd_<elem_type, 32>(params, stream, configure);
-        } else if (params.d <= 64) {
-            run_mha_bwd_<elem_type, 64>(params, stream, configure);
-        } else if (params.d <= 96) {
-            run_mha_bwd_<elem_type, 96>(params, stream, configure);
-        } else if (params.d <= 128) {
-            run_mha_bwd_<elem_type, 128>(params, stream, configure);
-        }
-    });
+void run_fmha_bwd(FMHA_dgrad_params &params, cudaStream_t stream, const bool configure) {
+  if (params.d <= 32) {
+      run_fmha_bwd_hdim32(params, stream, configure);
+  } else if (params.d <= 64) {
+      run_fmha_bwd_hdim64(params, stream, configure);
+  } else if (params.d <= 128) {
+      run_fmha_bwd_hdim128(params, stream, configure);
+  }
 }
 
 std::vector<at::Tensor>
-mha_bwd(const at::Tensor &dout,  // batch_size x seqlen_q x num_heads, x head_size_og
-        const at::Tensor &q,   // batch_size x seqlen_q x num_heads x head_size
-        const at::Tensor &k,   // batch_size x seqlen_k x num_heads x head_size
-        const at::Tensor &v,   // batch_size x seqlen_k x num_heads x head_size
-        const at::Tensor &out,   // batch_size x seqlen_q x num_heads x head_size
-        const at::Tensor &softmax_lse,     // b x h x seqlen_q
-        c10::optional<at::Tensor> &dq_,   // batch_size x seqlen_q x num_heads x head_size
-        c10::optional<at::Tensor> &dk_,   // batch_size x seqlen_k x num_heads x head_size
-        c10::optional<at::Tensor> &dv_,   // batch_size x seqlen_k x num_heads x head_size
+mha_bwd(const at::Tensor &dout,  // total_q x num_heads, x head_size
+        const at::Tensor &q,   // total_q x num_heads x head_size, total_q := \sum_{i=0}^{b} s_i
+        const at::Tensor &k,   // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
+        const at::Tensor &v,   // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
+        const at::Tensor &out,   // total_q x num_heads x head_size
+        const at::Tensor &softmax_lse_,     // b x h x s softmax logsumexp
+        at::Tensor &dq,   // total_q x num_heads x head_size, total_q := \sum_{i=0}^{b} s_i
+        at::Tensor &dk,   // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
+        at::Tensor &dv,   // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
+        const at::Tensor &cu_seqlens_q,  // b+1
+        const at::Tensor &cu_seqlens_k,  // b+1
+        const int max_seqlen_q_,
+        const int max_seqlen_k_,          // max sequence length to choose the kernel
         const float p_dropout,         // probability to drop
         const float softmax_scale,
+        const bool zero_tensors,
         const bool is_causal,
-        c10::optional<at::Generator> gen_) {
+        const int num_splits,
+        c10::optional<at::Generator> gen_,
+        c10::optional<at::Tensor> &rng_state
+) {
     auto dprops = at::cuda::getCurrentDeviceProperties();
     bool is_sm75 = dprops->major == 7 && dprops->minor == 5;
+    bool is_sm80 = dprops->major == 8 && dprops->minor == 0;
     bool is_sm8x = dprops->major == 8 && dprops->minor >= 0;
     bool is_sm90 = dprops->major == 9 && dprops->minor == 0;
     TORCH_CHECK(is_sm90 || is_sm8x || is_sm75);
-    auto launch = &run_mha_bwd;
+    auto launch = &run_fmha_bwd;
 
     bool is_dropout = p_dropout > 0.0;
     auto stream = at::cuda::getCurrentCUDAStream().stream();
 
     auto q_dtype = q.dtype();
-    TORCH_CHECK(q_dtype == torch::kFloat16 || ((is_sm90 || is_sm8x) && q_dtype == torch::kBFloat16));
+    TORCH_CHECK(q_dtype == torch::kFloat16 || ((is_sm8x || is_sm90) && q_dtype == torch::kBFloat16));
     TORCH_CHECK(k.dtype() == q_dtype);
     TORCH_CHECK(v.dtype() == q_dtype);
     TORCH_CHECK(out.dtype() == q_dtype);
     TORCH_CHECK(dout.dtype() == q_dtype);
+    TORCH_CHECK(dq.dtype() == q_dtype);
+    TORCH_CHECK(dk.dtype() == q_dtype);
+    TORCH_CHECK(dv.dtype() == q_dtype);
+    TORCH_CHECK(cu_seqlens_q.dtype() == torch::kInt32);
+    TORCH_CHECK(cu_seqlens_k.dtype() == torch::kInt32);
 
     TORCH_CHECK(q.is_cuda());
     TORCH_CHECK(k.is_cuda());
     TORCH_CHECK(v.is_cuda());
     TORCH_CHECK(out.is_cuda());
     TORCH_CHECK(dout.is_cuda());
-    TORCH_CHECK(softmax_lse.is_cuda());
+    TORCH_CHECK(softmax_lse_.is_cuda());
+    TORCH_CHECK(cu_seqlens_q.is_cuda());
+    TORCH_CHECK(cu_seqlens_k.is_cuda());
 
     TORCH_CHECK(q.stride(-1) == 1);
     TORCH_CHECK(k.stride(-1) == 1);
     TORCH_CHECK(v.stride(-1) == 1);
-    TORCH_CHECK(out.stride(-1) == 1);
-    TORCH_CHECK(dout.stride(-1) == 1);
+    TORCH_CHECK(out.is_contiguous());
+    TORCH_CHECK(dout.is_contiguous());
+    TORCH_CHECK(dq.stride(-1) == 1);
+    TORCH_CHECK(dk.stride(-1) == 1);
+    TORCH_CHECK(dv.stride(-1) == 1);
+    TORCH_CHECK(cu_seqlens_q.is_contiguous());
+    TORCH_CHECK(cu_seqlens_k.is_contiguous());
 
     const auto sizes = q.sizes();
 
-    const int batch_size = sizes[0];
-    const int seqlen_q = sizes[1];
-    const int num_heads = sizes[2];
-    const int head_size_og = dout.size(3);
-    const int head_size = sizes[3];
-    const int seqlen_k = k.size(1);
+    const int batch_size = cu_seqlens_q.numel() - 1;
+    const int total_q = sizes[TOTAL_DIM];
+    const int num_heads = sizes[H_DIM];
+    const int head_size = sizes[D_DIM];
+    const int total_k = k.size(TOTAL_DIM);
     TORCH_CHECK(batch_size > 0);
     TORCH_CHECK((head_size % 8 == 0) && (head_size <= 128));
+    if (head_size > 64) {
+        TORCH_CHECK(is_sm80 || is_sm90, "FlashAttention backward for head dim > 64 requires A100 or H100 GPUs as the implementation needs a large amount of shared memory.");
+    }
+
+    CHECK_SHAPE(q, total_q, num_heads, head_size);
+    CHECK_SHAPE(k, total_k, num_heads, head_size);
+    CHECK_SHAPE(v, total_k, num_heads, head_size);
+    CHECK_SHAPE(out, total_q, num_heads, head_size);
+    CHECK_SHAPE(dout, total_q, num_heads, head_size);
+    CHECK_SHAPE(dq, total_q, num_heads, head_size);
+    CHECK_SHAPE(dk, total_k, num_heads, head_size);
+    CHECK_SHAPE(dv, total_k, num_heads, head_size);
+    CHECK_SHAPE(cu_seqlens_q, batch_size + 1);
+    CHECK_SHAPE(cu_seqlens_k, batch_size + 1);
 
-    auto round_multiple = [](int x, int m) { return (x + m - 1) / m * m; };
-    const int head_size_rounded = round_multiple(head_size, 32);
-    const int seqlen_q_rounded = round_multiple(seqlen_q, 128);
-    const int seqlen_k_rounded = round_multiple(seqlen_k, 128);
-
-    TORCH_CHECK(head_size == round_multiple(head_size_og, 8));
-
-    CHECK_SHAPE(q, batch_size, seqlen_q, num_heads, head_size);
-    CHECK_SHAPE(k, batch_size, seqlen_k, num_heads, head_size);
-    CHECK_SHAPE(v, batch_size, seqlen_k, num_heads, head_size);
-    CHECK_SHAPE(out, batch_size, seqlen_q, num_heads, head_size);
-    CHECK_SHAPE(dout, batch_size, seqlen_q, num_heads, head_size_og);
-
-    at::Tensor dq, dk, dv;
-    if (dq_.has_value()) {
-        dq = dq_.value();
-        TORCH_CHECK(dq.dtype() == q_dtype);
-        TORCH_CHECK(dq.is_cuda());
-        TORCH_CHECK(dq.stride(-1) == 1);
-        CHECK_SHAPE(dq, batch_size, seqlen_q, num_heads, head_size);
-    } else {
-        dq = torch::empty_like(q);
-    }
-    if (dk_.has_value()) {
-        dk = dk_.value();
-        TORCH_CHECK(dk.dtype() == q_dtype);
-        TORCH_CHECK(dk.is_cuda());
-        TORCH_CHECK(dk.stride(-1) == 1);
-        CHECK_SHAPE(dk, batch_size, seqlen_k, num_heads, head_size);
-    } else {
-        dk = torch::empty_like(k);
-    }
-    if (dv_.has_value()) {
-        dv = dv_.value();
-        TORCH_CHECK(dv.dtype() == q_dtype);
-        TORCH_CHECK(dv.is_cuda());
-        TORCH_CHECK(dv.stride(-1) == 1);
-        CHECK_SHAPE(dv, batch_size, seqlen_k, num_heads, head_size);
-    } else {
-        dv = torch::empty_like(k);
-    }
-
-    at::Tensor dout_padded;
-    if (head_size_og % 8 != 0) {
-        dout_padded = torch::nn::functional::pad(dout, torch::nn::functional::PadFuncOptions({0, 8 - head_size_og % 8}));
-    } else {
-        dout_padded = dout;
-    }
-
-    // TODO: will need to round depending on kBlockM
-    // int seqlen_q = ((max_seqlen_q_ + 16 - 1) / 16) * 16;
-    // bool loop = seqlen_k > blocksize_c;
-    // TODO: change later, for now set to true for simplicity
-    bool loop = true;
+    int blocksize_c = (head_size > 64 || (is_sm75 && head_size > 32)) ? 128 : 256;
+    int max_seqlen_k = ((max_seqlen_k_ + blocksize_c - 1) / blocksize_c) * blocksize_c;
+    if( max_seqlen_k_ <= 128 ) {
+        max_seqlen_k = 128;
+    } else if( max_seqlen_k_ <= 256 ) {
+        max_seqlen_k = 256;
+    }
+    int max_seqlen_q = ((max_seqlen_q_ + 16 - 1) / 16) * 16;
+    bool loop = max_seqlen_k > blocksize_c;
 
     // Otherwise the kernel will be launched from cuda:0 device
     // Cast to char to avoid compiler warning about narrowing
     at::cuda::CUDAGuard device_guard{(char)q.get_device()};
 
+    // It's possible the softmax_lse_ from the fwd has a different length since blocksize_c could be different.
+    auto softmax_lse = softmax_lse_.index({torch::indexing::Slice(), torch::indexing::Slice(), torch::indexing::Slice(torch::indexing::None, max_seqlen_q)}).contiguous();
+
     auto opts = q.options();
-    auto softmax_d = torch::empty({batch_size, num_heads, seqlen_q_rounded}, opts.dtype(at::kFloat));
-    at::Tensor dq_accum;
-    at::Tensor dk_accum, dv_accum;
-    if (loop) {
-        dq_accum = torch::empty({batch_size, num_heads, seqlen_q_rounded, head_size_rounded}, opts.dtype(at::kFloat));
-        dk_accum = torch::empty({batch_size, num_heads, seqlen_k_rounded, head_size_rounded}, opts.dtype(at::kFloat));
-        dv_accum = torch::empty({batch_size, num_heads, seqlen_k_rounded, head_size_rounded}, opts.dtype(at::kFloat));
+    auto softmax_d = torch::empty({batch_size, num_heads, max_seqlen_q}, opts.dtype(at::kFloat));
+    at::Tensor dq_tmp;
+    if (loop) { dq_tmp = torch::empty({total_q, num_heads, head_size}, opts.dtype(at::kFloat)); }
+
+    if( zero_tensors ) {
+        dq.zero_();
+        dk.zero_();
+        dv.zero_();
+        softmax_d.zero_();
     }
 
-    Flash_bwd_params params;
+    FMHA_dgrad_params params;
 
     set_params_dgrad(params,
                      batch_size,
-                     seqlen_q, seqlen_k,
-                     seqlen_q_rounded, seqlen_k_rounded,
+                     max_seqlen_q,
+                     max_seqlen_k,
                      num_heads,
-                     head_size, head_size_rounded,
+                     head_size,
                      q, k, v, out,
-                     dout_padded, dq, dk, dv,
-                     nullptr,
-                     nullptr,
-                     loop ? dq_accum.data_ptr() : nullptr,
-                     loop ? dk_accum.data_ptr() : nullptr,
-                     loop ? dv_accum.data_ptr() : nullptr,
+                     dq, dk, dv,
+                     cu_seqlens_q.data_ptr(),
+                     cu_seqlens_k.data_ptr(),
+                     loop ? dq_tmp.data_ptr() : nullptr,
+                     dout.data_ptr(),
                      softmax_lse.data_ptr(),
                      softmax_d.data_ptr(),
                      p_dropout,
                      softmax_scale,
-                     is_causal);
+                     is_causal,
+                     num_splits);
 
-    // launch(params, stream, /*configure=*/true);
+    launch(params, stream, /*configure=*/true);
 
-    // if (params.num_splits > 1) {
-    //     if (!dq_accum.defined()) {
-    //         dq_accum = torch::zeros({total_q, num_heads, head_size}, opts.dtype(at::kFloat));
-    //         params.o_tmp_ptr = dq_accum.data_ptr();  // o_tmp stores dq_accum in the backward pass
-    //     } else {
-    //         dq_accum.zero_();
-    //     }
-    // }
-    // TODO: zero out only if seqparallel
-    // dq_accum.zero_();
+    if (params.num_splits > 1) {
+        if (!dq_tmp.defined()) {
+            dq_tmp = torch::zeros({total_q, num_heads, head_size}, opts.dtype(at::kFloat));
+            params.o_tmp_ptr = dq_tmp.data_ptr();  // o_tmp stores dq_tmp in the backward pass
+        } else {
+            dq_tmp.zero_();
+        }
+    }
 
     auto gen = at::get_generator_or_default<at::CUDAGeneratorImpl>(
         gen_, at::cuda::detail::getDefaultCUDAGenerator());
 
     // We use a custom RNG that increases the offset by batch_size * nheads * 32.
     int64_t counter_offset = params.b * params.h * 32;
-
-    if (is_dropout) {
+    if ( rng_state.has_value() ) {
+        params.rng_state = reinterpret_cast<uint64_t*>(rng_state.value().data_ptr());
+    } else if( is_dropout ) {
         // See Note [Acquire lock when using random generators]
         std::lock_guard<std::mutex> lock(gen->mutex_);
         params.philox_args = gen->philox_cuda_state(counter_offset);
+        auto seeds = at::cuda::philox::unpack(params.philox_args);
+        params.rng_state[0] = std::get<0>(seeds);
+        params.rng_state[1] = std::get<1>(seeds);
     }
 
     launch(params, stream, /*configure=*/false);
 
-    if (head_size_og % 8 != 0) {
-        dq = dq.index({"...", torch::indexing::Slice(torch::indexing::None, head_size_og)});
-        dk = dk.index({"...", torch::indexing::Slice(torch::indexing::None, head_size_og)});
-        dv = dv.index({"...", torch::indexing::Slice(torch::indexing::None, head_size_og)});
+    if (params.num_splits > 1) {
+        dq.copy_(dq_tmp);
     }
 
     return { dq, dk, dv, softmax_d };
 }
 
 std::vector<at::Tensor>
-mha_varlen_bwd(const at::Tensor &dout,  // total_q x num_heads, x head_size
-               const at::Tensor &q,   // total_q x num_heads x head_size, total_q := \sum_{i=0}^{b} s_i
-               const at::Tensor &k,   // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
-               const at::Tensor &v,   // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
-               const at::Tensor &out,   // total_q x num_heads x head_size
-               const at::Tensor &softmax_lse_,     // b x h x s softmax logsumexp
-               at::Tensor &dq,   // total_q x num_heads x head_size, total_q := \sum_{i=0}^{b} s_i
-               at::Tensor &dk,   // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
-               at::Tensor &dv,   // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
-               const at::Tensor &cu_seqlens_q,  // b+1
-               const at::Tensor &cu_seqlens_k,  // b+1
-               const int max_seqlen_q_,
-               const int max_seqlen_k_,          // max sequence length to choose the kernel
-               const float p_dropout,         // probability to drop
-               const float softmax_scale,
-               const bool zero_tensors,
-               const bool is_causal,
-               c10::optional<at::Generator> gen_
+mha_fwd_block(const at::Tensor &q,         // total_q x num_heads x head_size, total := \sum_{i=0}^{b} s_i
+              const at::Tensor &k,         // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
+              const at::Tensor &v,         // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
+              const at::Tensor &cu_seqlens_q,  // b+1
+              const at::Tensor &cu_seqlens_k,  // b+1
+              const at::Tensor &blockmask,   // (seqlen / 256, seqlen / 16)
+              const int max_seqlen_q_,
+              const int max_seqlen_k_,
+              const float p_dropout,
+              const float softmax_scale,
+              const bool is_causal,
+              const bool return_softmax,
+              c10::optional<at::Generator> gen_) {
+
+    auto dprops = at::cuda::getCurrentDeviceProperties();
+    bool is_sm80 = dprops->major == 8 && dprops->minor == 0;
+    bool is_sm8x = dprops->major == 8 && dprops->minor >= 0;
+    bool is_sm90 = dprops->major == 9 && dprops->minor == 0;
+    TORCH_CHECK(is_sm8x || is_sm90);
+    auto stream = at::cuda::getCurrentCUDAStream().stream();
+    bool is_dropout = p_dropout > 0.0;
+    Launch_params<FMHA_fprop_params> launch_params(dprops, stream, is_dropout, return_softmax);
+
+    TORCH_CHECK(q.dtype() == torch::kFloat16);
+    TORCH_CHECK(k.dtype() == torch::kFloat16);
+    TORCH_CHECK(v.dtype() == torch::kFloat16);
+    TORCH_CHECK(cu_seqlens_q.dtype() == torch::kInt32);
+    TORCH_CHECK(cu_seqlens_k.dtype() == torch::kInt32);
+    TORCH_CHECK(blockmask.dtype() == torch::kInt32);
+
+    TORCH_CHECK(q.is_cuda());
+    TORCH_CHECK(k.is_cuda());
+    TORCH_CHECK(v.is_cuda());
+    TORCH_CHECK(cu_seqlens_q.is_cuda());
+    TORCH_CHECK(cu_seqlens_k.is_cuda());
+    TORCH_CHECK(blockmask.is_cuda())
+
+    TORCH_CHECK(q.stride(-1) == 1);
+    TORCH_CHECK(k.stride(-1) == 1);
+    TORCH_CHECK(v.stride(-1) == 1);
+    TORCH_CHECK(cu_seqlens_k.is_contiguous());
+    TORCH_CHECK(cu_seqlens_k.is_contiguous());
+    TORCH_CHECK(blockmask.is_contiguous())
+
+    const auto sizes = q.sizes();
+
+    const int batch_size = cu_seqlens_q.numel() - 1;
+    const int total_q = sizes[TOTAL_DIM];
+    const int num_heads = sizes[H_DIM];
+    const int head_size = sizes[D_DIM];
+    const int total_k = k.size(TOTAL_DIM);
+    TORCH_CHECK(batch_size > 0);
+    TORCH_CHECK(head_size == 16 || head_size == 32 || head_size == 64 || head_size == 128);
+
+    CHECK_SHAPE(q, total_q, num_heads, head_size);
+    CHECK_SHAPE(k, total_k, num_heads, head_size);
+    CHECK_SHAPE(v, total_k, num_heads, head_size);
+    CHECK_SHAPE(cu_seqlens_q, batch_size + 1);
+    CHECK_SHAPE(cu_seqlens_k, batch_size + 1);
+
+    int max_seqlen_k = ((max_seqlen_k_ + 256 - 1) / 256) * 256;
+    if( max_seqlen_k <= 256 ) {
+        max_seqlen_k = 256;
+    }
+    int max_seqlen_q = ((max_seqlen_q_ + 16 - 1) / 16) * 16;
+    bool loop = max_seqlen_k > 256;
+    CHECK_SHAPE(blockmask, max_seqlen_k / 256, max_seqlen_q / 16);
+
+    auto opts = q.options();
+
+    auto o = torch::zeros({ total_q, num_heads, head_size }, opts);
+
+    at::Tensor o_tmp;
+    if (loop) {
+        // o_tmp = torch::zeros({total, num_heads, head_size}, opts.dtype(at::kFloat));
+        o_tmp = torch::empty({total_q, num_heads, head_size}, opts.dtype(at::kFloat));
+    }
+
+    // auto softmax_lse = torch::full({batch_size, num_heads, max_seqlen_k}, -std::numeric_limits<float>::infinity(), opts.dtype(at::kFloat));
+    auto softmax_lse = torch::empty({batch_size, num_heads, max_seqlen_q}, opts.dtype(at::kFloat));
+
+    at::Tensor s;
+    if (return_softmax) {
+        s = torch::zeros({ batch_size, num_heads, max_seqlen_q, max_seqlen_k }, opts);
+    }
+
+    auto gen = at::get_generator_or_default<at::CUDAGeneratorImpl>(
+        gen_, at::cuda::detail::getDefaultCUDAGenerator());
+
+    set_params_fprop(launch_params.params,
+                     batch_size,
+                     max_seqlen_q,
+                     max_seqlen_k,
+                     num_heads,
+                     head_size,
+                     q, k, v, o,
+                     cu_seqlens_q.data_ptr(),
+                     cu_seqlens_k.data_ptr(),
+                     loop ? o_tmp.data_ptr() : nullptr,
+                     return_softmax ? s.data_ptr() : nullptr,
+                     softmax_lse.data_ptr(),
+                     p_dropout,
+                     softmax_scale,
+                     is_causal,
+                     /*num_splits=*/1);
+    launch_params.params.blockmask = static_cast<int *>(blockmask.data_ptr());
+
+    run_fmha_block_fp16_sm80(launch_params, /*configure=*/ true);
+    // number of times random will be generated per thread, to offset philox counter in thc random
+    // state
+    int64_t counter_offset = launch_params.elts_per_thread;
+
+    if( is_dropout ) {
+        // See Note [Acquire lock when using random generators]
+        std::lock_guard<std::mutex> lock(gen->mutex_);
+        launch_params.params.philox_args = gen->philox_cuda_state(counter_offset);
+    }
+
+    run_fmha_block_fp16_sm80(launch_params, /*configure=*/false);
+
+    std::vector<at::Tensor> result = {o, softmax_lse};
+    if (return_softmax) {result.push_back(s);}
+    return result;
+}
+
+std::vector<at::Tensor>
+mha_bwd_block(const at::Tensor &dout,  // total x num_heads, x head_size
+              const at::Tensor &q,   // total_q x num_heads x head_size, total_q := \sum_{i=0}^{b} s_i
+              const at::Tensor &k,   // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
+              const at::Tensor &v,   // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
+              const at::Tensor &out,   // total_q x num_heads x head_size
+              const at::Tensor &softmax_lse_,     // b x h x s softmax logsumexp
+              at::Tensor &dq,   // total_q x num_heads x head_size, total_q := \sum_{i=0}^{b} s_i
+              at::Tensor &dk,   // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
+              at::Tensor &dv,   // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
+              const at::Tensor &cu_seqlens_q,  // b+1
+              const at::Tensor &cu_seqlens_k,  // b+1
+              const at::Tensor &blockmask,   // (seqlen / 256, seqlen / 16)
+              const int max_seqlen_q_,
+              const int max_seqlen_k_,          // max sequence length to choose the kernel
+              const float p_dropout,         // probability to drop
+              const float softmax_scale,
+              const bool is_causal,
+              c10::optional<at::Generator> gen_
 ) {
     auto dprops = at::cuda::getCurrentDeviceProperties();
-    bool is_sm75 = dprops->major == 7 && dprops->minor == 5;
+    bool is_sm80 = dprops->major == 8 && dprops->minor == 0;
     bool is_sm8x = dprops->major == 8 && dprops->minor >= 0;
     bool is_sm90 = dprops->major == 9 && dprops->minor == 0;
-    TORCH_CHECK(is_sm90 || is_sm8x || is_sm75);
-    auto launch = &run_mha_bwd;
+    TORCH_CHECK(is_sm8x || is_sm90);
+    auto launch = &run_fmha_block_dgrad_fp16_sm80;
 
     bool is_dropout = p_dropout > 0.0;
     auto stream = at::cuda::getCurrentCUDAStream().stream();
 
-    auto q_dtype = q.dtype();
-    TORCH_CHECK(q_dtype == torch::kFloat16 || ((is_sm90 || is_sm8x) && q_dtype == torch::kBFloat16));
-    TORCH_CHECK(k.dtype() == q_dtype);
-    TORCH_CHECK(v.dtype() == q_dtype);
-    TORCH_CHECK(out.dtype() == q_dtype);
-    TORCH_CHECK(dout.dtype() == q_dtype);
-    TORCH_CHECK(dq.dtype() == q_dtype);
-    TORCH_CHECK(dk.dtype() == q_dtype);
-    TORCH_CHECK(dv.dtype() == q_dtype);
+    TORCH_CHECK(q.dtype() == torch::kFloat16);
+    TORCH_CHECK(k.dtype() == torch::kFloat16);
+    TORCH_CHECK(v.dtype() == torch::kFloat16);
+    TORCH_CHECK(out.dtype() == torch::kFloat16);
+    TORCH_CHECK(dout.dtype() == torch::kFloat16);
+    TORCH_CHECK(dq.dtype() == torch::kFloat16);
+    TORCH_CHECK(dk.dtype() == torch::kFloat16);
+    TORCH_CHECK(dv.dtype() == torch::kFloat16);
     TORCH_CHECK(cu_seqlens_q.dtype() == torch::kInt32);
     TORCH_CHECK(cu_seqlens_k.dtype() == torch::kInt32);
+    TORCH_CHECK(blockmask.dtype() == torch::kInt32);
 
     TORCH_CHECK(q.is_cuda());
     TORCH_CHECK(k.is_cuda());
     TORCH_CHECK(v.is_cuda());
     TORCH_CHECK(out.is_cuda());
     TORCH_CHECK(dout.is_cuda());
     TORCH_CHECK(softmax_lse_.is_cuda());
     TORCH_CHECK(cu_seqlens_q.is_cuda());
     TORCH_CHECK(cu_seqlens_k.is_cuda());
+    TORCH_CHECK(blockmask.is_cuda());
 
     TORCH_CHECK(q.stride(-1) == 1);
     TORCH_CHECK(k.stride(-1) == 1);
     TORCH_CHECK(v.stride(-1) == 1);
     TORCH_CHECK(out.is_contiguous());
     TORCH_CHECK(dout.is_contiguous());
     TORCH_CHECK(dq.stride(-1) == 1);
     TORCH_CHECK(dk.stride(-1) == 1);
     TORCH_CHECK(dv.stride(-1) == 1);
     TORCH_CHECK(cu_seqlens_q.is_contiguous());
     TORCH_CHECK(cu_seqlens_k.is_contiguous());
+    TORCH_CHECK(blockmask.is_contiguous());
 
     const auto sizes = q.sizes();
 
-    const int total_q = sizes[0];
     const int batch_size = cu_seqlens_q.numel() - 1;
-    const int num_heads = sizes[1];
-    const int head_size = sizes[2];
-    const int total_k = k.size(0);
+    const int total_q = sizes[TOTAL_DIM];
+    const int num_heads = sizes[H_DIM];
+    const int head_size = sizes[D_DIM];
+    const int total_k = k.size(TOTAL_DIM);
     TORCH_CHECK(batch_size > 0);
-    TORCH_CHECK((head_size % 8 == 0) && (head_size <= 128));
+    TORCH_CHECK(head_size == 16 || head_size == 32 || head_size == 64 || head_size == 128);
+    if (head_size == 128) {  // TODO: eventually we should support SM86 and SM70 with d=128 as well
+        TORCH_CHECK(is_sm80 || is_sm90);
+    }
 
     CHECK_SHAPE(q, total_q, num_heads, head_size);
     CHECK_SHAPE(k, total_k, num_heads, head_size);
     CHECK_SHAPE(v, total_k, num_heads, head_size);
     CHECK_SHAPE(out, total_q, num_heads, head_size);
     CHECK_SHAPE(dout, total_q, num_heads, head_size);
     CHECK_SHAPE(dq, total_q, num_heads, head_size);
     CHECK_SHAPE(dk, total_k, num_heads, head_size);
     CHECK_SHAPE(dv, total_k, num_heads, head_size);
     CHECK_SHAPE(cu_seqlens_q, batch_size + 1);
     CHECK_SHAPE(cu_seqlens_k, batch_size + 1);
 
-    int blocksize_c = (head_size > 64 || (is_sm75 && head_size > 32)) ? 128 : 256;
-    int max_seqlen_k = ((max_seqlen_k_ + blocksize_c - 1) / blocksize_c) * blocksize_c;
-    if( max_seqlen_k_ <= 128 ) {
-        max_seqlen_k = 128;
-    } else if( max_seqlen_k_ <= 256 ) {
+    int max_seqlen_k = ((max_seqlen_k_ + 256 - 1) / 256) * 256;
+    if( max_seqlen_k <= 256 ) {
         max_seqlen_k = 256;
     }
-    // TODO: will need to round depending on kBlockM
-    // int max_seqlen_q = ((max_seqlen_q_ + 16 - 1) / 16) * 16;
-    int max_seqlen_q = max_seqlen_q_;
-    // bool loop = max_seqlen_k > blocksize_c;
-    // TODO: change later, for now set to true for simplicity
-    bool loop = true;
-
-    // Otherwise the kernel will be launched from cuda:0 device
-    // Cast to char to avoid compiler warning about narrowing
-    at::cuda::CUDAGuard device_guard{(char)q.get_device()};
+    int max_seqlen_q = ((max_seqlen_q_ + 16 - 1) / 16) * 16;
+    bool loop = max_seqlen_k > 256;
+    CHECK_SHAPE(blockmask, max_seqlen_k / 256, max_seqlen_q / 16);
 
     // It's possible the softmax_lse_ from the fwd has a different length since blocksize_c could be different.
     auto softmax_lse = softmax_lse_.index({torch::indexing::Slice(), torch::indexing::Slice(), torch::indexing::Slice(torch::indexing::None, max_seqlen_q)}).contiguous();
 
     auto opts = q.options();
     auto softmax_d = torch::empty({batch_size, num_heads, max_seqlen_q}, opts.dtype(at::kFloat));
-    at::Tensor dq_accum;
+    at::Tensor dq_tmp;
     if (loop) {
-        // TODO: this needs to be seqlen_q_rounded and head_size_rounded
-        dq_accum = torch::empty({batch_size, num_heads, max_seqlen_q, head_size}, opts.dtype(at::kFloat));
+        // dq_tmp = torch::zeros({total, num_heads, head_size}, opts.dtype(at::kFloat));
+        dq_tmp = torch::empty({total_q, num_heads, head_size}, opts.dtype(at::kFloat));
     }
 
-    if( zero_tensors ) {
-        dq.zero_();
-        dk.zero_();
-        dv.zero_();
-        softmax_d.zero_();
-    }
-
-    Flash_bwd_params params;
+    FMHA_dgrad_params params;
 
     set_params_dgrad(params,
                      batch_size,
-                     max_seqlen_q, max_seqlen_k,
-                     max_seqlen_q, max_seqlen_k, // TODO: round these
+                     max_seqlen_q,
+                     max_seqlen_k,
                      num_heads,
-                     head_size, head_size,  // TODO: round this
+                     head_size,
                      q, k, v, out,
-                     dout, dq, dk, dv,
+                     dq, dk, dv,
                      cu_seqlens_q.data_ptr(),
                      cu_seqlens_k.data_ptr(),
-                     loop ? dq_accum.data_ptr() : nullptr,
-                     nullptr,
-                     nullptr,
+                     loop ? dq_tmp.data_ptr() : nullptr,
+                     dout.data_ptr(),
                      softmax_lse.data_ptr(),
                      softmax_d.data_ptr(),
                      p_dropout,
                      softmax_scale,
-                     is_causal);
-
-    // launch(params, stream, /*configure=*/true);
-
-    // if (params.num_splits > 1) {
-    //     if (!dq_accum.defined()) {
-    //         dq_accum = torch::zeros({total_q, num_heads, head_size}, opts.dtype(at::kFloat));
-    //         params.o_tmp_ptr = dq_accum.data_ptr();  // o_tmp stores dq_accum in the backward pass
-    //     } else {
-    //         dq_accum.zero_();
-    //     }
-    // }
-    // TODO: zero out only if seqparallel
-    // dq_accum.zero_();
+                     is_causal,
+                     /*num_splits=*/1);
+    params.blockmask = static_cast<int *>(blockmask.data_ptr());
 
     auto gen = at::get_generator_or_default<at::CUDAGeneratorImpl>(
         gen_, at::cuda::detail::getDefaultCUDAGenerator());
 
-    // We use a custom RNG that increases the offset by batch_size * nheads * 32.
-    int64_t counter_offset = params.b * params.h * 32;
+    // We're gonna reset the rng state in Python after this kernel, so the counter offset
+    // here doesn't matter at all. We just choose an arbitrary number;
+    int64_t counter_offset = 4;
 
-    if (is_dropout) {
+    if( is_dropout ) {
         // See Note [Acquire lock when using random generators]
         std::lock_guard<std::mutex> lock(gen->mutex_);
         params.philox_args = gen->philox_cuda_state(counter_offset);
     }
 
-    launch(params, stream, /*configure=*/false);
-
-    return { softmax_d };
+    launch(params, stream);
+    return { dq, dk, dv, softmax_d };
 }
 
+
 PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
-    m.doc() = "FlashAttention";
+    m.doc() = "Fused Multi-head Self-attention";
     m.def("fwd", &mha_fwd, "Forward pass");
-    m.def("varlen_fwd", &mha_varlen_fwd, "Forward pass (variable length)");
     m.def("bwd", &mha_bwd, "Backward pass");
-    m.def("varlen_bwd", &mha_varlen_bwd, "Backward pass (variable length)");
+    m.def("fwd_block", &mha_fwd_block, "Forward pass (blocksparse)");
+    m.def("bwd_block", &mha_bwd_block, "Backward pass (blocksparse)");
 }
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/src/flash_fwd_kernel_old.h` & `flash_attn-1.0.9/csrc/flash_attn/src/fmha/softmax.h`

 * *Files 27% similar despite different names*

```diff
@@ -1,530 +1,607 @@
 /******************************************************************************
- * Copyright (c) 2023, Tri Dao.
+ * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
+ * 
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of the NVIDIA CORPORATION nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ * 
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
  ******************************************************************************/
 
 #pragma once
 
-#include <cute/algorithm/copy.hpp>
-#include <cute/algorithm/gemm.hpp>
+#include <cmath>
+#include <cuda_fp16.h>
 
-#include <cutlass/cutlass.h>
-#include <cutlass/array.h>
-#include <cutlass/numeric_types.h>
-#include <cutlass/numeric_conversion.h>
-
-#include "block_info.h"
-#include "kernel_traits.h"
-#include "softmax.h"
-#include "philox.cuh"
+namespace fmha {
 
-namespace flash {
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
-using namespace cute;
+inline __device__ float apply_exp_(float x, float max) {
+    return __expf(x - max);
+}
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
-template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Return_softmax, bool Is_even_M, typename Params, typename Prng>
-inline __device__ void compute_attn_1Mblock(const Params &params, const int bidb, const int bidh, Prng &ph) {
+inline __device__ float apply_exp2_(float x, float max) {
+    return exp2f(x - max);
+    // With fast-math, this produces the same PTX instruction as the assembly below
+    // float diff = x - max;
+    // float res;
+    // asm ("ex2.approx.ftz.f32 %0, %1;\n\t" : "=f"(res) : "f"(diff));
+    // return res;
+}
 
-    using Element = typename Kernel_traits::Element;
-    using ElementAccum = typename Kernel_traits::ElementAccum;
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
-    // Shared memory.
-    extern __shared__ char smem_[];
+template<int COLS> struct ReadType {};
+template<> struct ReadType<4> { using T = float;};
+template<> struct ReadType<8> { using T = float2;};
 
-    // The thread index.
-    const int tidx = threadIdx.x;
-
-    using X = Underscore;
-
-    constexpr int kBlockM = Kernel_traits::kBlockM;
-    constexpr int kBlockN = Kernel_traits::kBlockN;
-    constexpr int kHeadDim = Kernel_traits::kHeadDim;
-
-    const BlockInfo binfo(params, bidb);
-    // if( binfo.stop_early(loop_step_idx * Cta_tile_p::N) ) return;
-    if (blockIdx.x * kBlockM >= binfo.actual_seqlen_q) return;
-
-    const uint32_t row_offset_q = binfo.sum_s_q * params.q_row_stride + bidh * params.q_head_stride;
-    const uint32_t row_offset_k = binfo.sum_s_k * params.k_row_stride + bidh * params.k_head_stride;
-    const uint32_t row_offset_v = binfo.sum_s_k * params.v_row_stride + bidh * params.v_head_stride;
-
-    // We assume that params.d == kHeadDim for now
-    Tensor mQ = make_tensor(make_gmem_ptr(reinterpret_cast<Element *>(params.q_ptr) + row_offset_q),
-                            // make_shape(binfo.actual_seqlen_q, params.d),
-                            // Need static shape in the k dim here
-                            make_shape(binfo.actual_seqlen_q, Int<kHeadDim>{}),
-                            make_stride(params.q_row_stride, _1{}));
-    // if (cute::thread0()) { print(mQ.layout()); printf("\n"); }
-    // Tensor mK = make_tensor(make_gmem_ptr(reinterpret_cast<Element *>(params.k_ptr) + row_offset_k),
-    //                         make_shape(binfo.actual_seqlen_k, Int<kHeadDim>{}),
-    //                         make_stride(params.k_row_stride, _1{}));
-
-    Tensor gQ = local_tile(mQ, Shape<Int<kBlockM>>{}, make_coord(blockIdx.x));  // (kBlockM, kHeadDim)
-    // Tensor gK = local_tile(mK, Shape<Int<kBlockN>, Int<kHeadDim>>{}, make_coord(_, _0{}));  // (kBlockN, kHeadDim, n)
-    Tensor gK = make_tensor(make_gmem_ptr(reinterpret_cast<Element *>(params.k_ptr) + row_offset_k),
-                            Shape<Int<kBlockN>, Int<kHeadDim>>{}, make_stride(params.k_row_stride, _1{}));
-    Tensor gV = make_tensor(make_gmem_ptr(reinterpret_cast<Element *>(params.v_ptr) + row_offset_v),
-                            Shape<Int<kBlockN>, Int<kHeadDim>>{}, make_stride(params.v_row_stride, _1{}));
-
-    Tensor sQ = make_tensor(make_smem_ptr(reinterpret_cast<Element *>(smem_)),
-                            typename Kernel_traits::SmemLayoutQ{});
-    // Careful we're using the same smem for sQ and sK | sV if Is_Q_in_regs
-    Tensor sK = make_tensor(sQ.data() + (Kernel_traits::Is_Q_in_regs ? 0 : size(sQ)),
-                            typename Kernel_traits::SmemLayoutKV{});
-    Tensor sV = make_tensor(sK.data() + size(sK), typename Kernel_traits::SmemLayoutKV{});
-    Tensor sVtransposed = make_tensor(sV.data(), typename Kernel_traits::SmemLayoutVtransposed{});
-    Tensor sVtransposedNoSwizzle = make_tensor(sV.data(),
-                                               typename Kernel_traits::SmemLayoutVtransposedNoSwizzle{});
-
-    auto gmem_thr_copy_Q = typename Kernel_traits::GmemTiledCopyQ{}.get_thread_slice(tidx);
-    auto gmem_thr_copy_KV = typename Kernel_traits::GmemTiledCopyKV{}.get_thread_slice(tidx);
-    Tensor tQgQ = gmem_thr_copy_Q.partition_S(gQ);
-    Tensor tQsQ = gmem_thr_copy_Q.partition_D(sQ);
-    Tensor tKgK = gmem_thr_copy_KV.partition_S(gK);  // (KCPY, KCPY_N, KCPY_K)
-    Tensor tKsK = gmem_thr_copy_KV.partition_D(sK);
-    Tensor tVgV = gmem_thr_copy_KV.partition_S(gV);  // (VCPY, VCPY_N, VCPY_K)
-    Tensor tVsV = gmem_thr_copy_KV.partition_D(sV);
-
-    typename Kernel_traits::TiledMma tiled_mma;
-    auto thr_mma = tiled_mma.get_thread_slice(tidx);
-    Tensor tSrQ  = thr_mma.partition_fragment_A(sQ);                           // (MMA,MMA_M,MMA_K)
-    Tensor tSrK  = thr_mma.partition_fragment_B(sK);                           // (MMA,MMA_N,MMA_K)
-    CUTE_STATIC_ASSERT_V(size<2>(tSrQ) == size<2>(tSrK));                      // MMA_K
-
-    Tensor acc_o = partition_fragment_C(tiled_mma, Shape<Int<kBlockM>, Int<kHeadDim>>{});  // MMA, MMA_M, MMA_K
-    // Tensor tOrV  = thr_mma.partition_fragment_B(sVtransposed);                           // (MMA, MMA_K,MMA_N)
-    Tensor tOrV  = thr_mma.partition_fragment_B(sVtransposedNoSwizzle);                           // (MMA, MMA_K,MMA_N)
-    CUTE_STATIC_ASSERT_V(size<1>(tOrV) == size<2>(acc_o));                     // MMA_N
-
-    //
-    // Copy Atom retiling
-    //
-
-    auto smem_thr_copy_Q = make_tiled_copy_A(typename Kernel_traits::SmemCopyAtom{}, tiled_mma).get_thread_slice(tidx);
-    Tensor tSsQ = smem_thr_copy_Q.partition_S(sQ);
-    Tensor tSrQ_copy_view = smem_thr_copy_Q.retile_D(tSrQ);
-    CUTE_STATIC_ASSERT_V(size<1>(tSsQ) == size<1>(tSrQ_copy_view));            // M
-
-    auto smem_thr_copy_K = make_tiled_copy_B(typename Kernel_traits::SmemCopyAtom{}, tiled_mma).get_thread_slice(tidx);
-    Tensor tSsK = smem_thr_copy_K.partition_S(sK);
-    Tensor tSrK_copy_view = smem_thr_copy_K.retile_D(tSrK);
-    CUTE_STATIC_ASSERT_V(size<1>(tSsK) == size<1>(tSrK_copy_view));            // N
-
-    auto smem_thr_copy_V = make_tiled_copy_B(typename Kernel_traits::SmemCopyAtomTransposed{}, tiled_mma).get_thread_slice(tidx);
-    Tensor tOsV = smem_thr_copy_V.partition_S(sVtransposed);
-    Tensor tOrV_copy_view = smem_thr_copy_V.retile_D(tOrV);
-    CUTE_STATIC_ASSERT_V(size<1>(tOsV) == size<1>(tOrV_copy_view));            // N
-
-    // TODO: this might need to change if we change the mma instruction in SM70
-    Tensor scores_max = make_tensor<ElementAccum>(Shape<Int<2 * size<1>(acc_o)>>{});
-    Tensor scores_sum = make_fragment_like(scores_max);
-
-    //
-    // PREDICATES
-    //
-
-    // // Allocate predicate tensors for m and n
-    // Tensor tQpQ = make_tensor<bool>(make_shape(size<1>(tQsQ), size<2>(tQsQ)), Stride<_1,_0>{});
-    // Tensor tKVpKV = make_tensor<bool>(make_shape(size<1>(tKsK), size<2>(tKsK)), Stride<_1,_0>{});
-
-    // Construct identity layout for sQ and sK
-    Tensor cQ = make_identity_tensor(make_shape(size<0>(sQ), size<1>(sQ)));    // (BLK_M,BLK_K) -> (blk_m,blk_k)
-    Tensor cKV = make_identity_tensor(make_shape(size<0>(sK), size<1>(sK)));    // (BLK_N,BLK_K) -> (blk_n,blk_k)
-
-    // Repeat the partitioning with identity layouts
-    Tensor tQcQ = gmem_thr_copy_Q.partition_S(cQ);                             // (ACPY,ACPY_M,ACPY_K) -> (blk_m,blk_k)
-    Tensor tKVcKV = gmem_thr_copy_KV.partition_S(cKV);                             // (BCPY,BCPY_N,BCPY_K) -> (blk_n,blk_k)
-
-    // // Set predicates for m bounds
-    // CUTLASS_PRAGMA_UNROLL
-    // for (int m = 0; m < size<0>(tQpQ); ++m) {
-    //     // if (cute::thread0()) {printf("m = %d\n", get<0>(tQcQ(0, m, 0))); }
-    //     tQpQ(m, 0) = get<0>(tQcQ(0, m, 0)) < binfo.actual_seqlen_q - blockIdx.x * kBlockM;  // blk_m coord < residue_m
-    // }
-    // // if (cute::thread0()) { print(tQpQ); }
-    // // Set predicates for n bounds
-    // CUTLASS_PRAGMA_UNROLL
-    // for (int n = 0; n < size<0>(tKVpKV); ++n) {
-    //     tKVpKV(n, 0) = get<0>(tKVcKV(0, n, 0)) < binfo.actual_seqlen_k;  // blk_n coord < residue_n
-    // }
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
-    // Prologue
+template <typename Cta_tile, typename Kernel_traits>
+struct Smem_tile_reduce {
+    // Helper class to distribute MMA tiles reduced over rows per warp over quads.
+
+    // The Mma tile.
+    using Mma_tile = fmha::Hmma_tile<Cta_tile>;
+
+    // The number of MMAs in M/N dimensions.
+    static constexpr int MMAS_M = Mma_tile::MMAS_M;
+    static constexpr int MMAS_N = Mma_tile::MMAS_N;
+
+    static constexpr int WARPS_M = Cta_tile::WARPS_M;
+    static constexpr int WARPS_N = Cta_tile::WARPS_N;
+
+
+    static constexpr int ROWS = WARPS_M * MMAS_M * 16;
+    static constexpr int COLS = WARPS_N;
+    static_assert(COLS == 4 || COLS == 8);
+    static constexpr int ROWS_PER_XOR_PATTERN = (COLS == 8) ? 4 : 8;
+    static constexpr int BYTES_PER_TILE = ROWS * COLS * sizeof(float);
+    static constexpr int ELTS_PER_TILE = ROWS * COLS;
+
+    static constexpr int THREADS_PER_GROUP = Kernel_traits::Gmem_tile_o::THREADS_PER_ROW;
+    // TD [2022-05-02]: No longer true if head_dim != 64
+    // static_assert(THREADS_PER_GROUP == 16); // DEBUG
+    static constexpr int ROWS_PER_WARP = 32 / THREADS_PER_GROUP;
+    static constexpr int LOOPS = Kernel_traits::Gmem_tile_o::LOOPS;
+    static_assert(LOOPS == 1);
+
+    using read_t = typename ReadType<COLS>::T;
+
+    __device__ inline Smem_tile_reduce(float *smem_, const int tidx) {
+
+        int lane = tidx % 32;
+        int warp = tidx / 32;
+
+        int warp_m = warp % WARPS_M;
+        int warp_n = warp / WARPS_M;
+
+        qid_ = lane % 4;
+        int qp = lane / 4;
+
+        // Swizzle the column to avoid 2-fold bank conflicts when we have 8 warps.
+        // This won't affect reading as we assume commutative reduction ops.
+        const int col = warp_n ^ (qp / ROWS_PER_XOR_PATTERN);
+        smem_write_ = &smem_[warp_m * 16 * MMAS_M * WARPS_N + qp * WARPS_N + col];
+        smem_read_ = &reinterpret_cast<read_t *>(smem_)[warp_m * 16 * MMAS_M * 4 + qp * 4 + qid_];
+        smem_read_row_ = &reinterpret_cast<read_t *>(smem_)[warp_m * 16 * MMAS_M * 4 + qid_];
 
-    Tensor tQrQ = make_fragment_like(tQgQ);
-    // copy(gmem_thr_copy_Q, tQgQ, tQsQ);
-    // We don't need to clear the sQ smem tiles since we'll only write out the valid outputs
-    // copy_if(gmem_thr_copy_Q, tQpQ, tQgQ, tQsQ);
-    #pragma unroll
-    for (int m = 0; m < size<1>(tQgQ); ++m) {
-        if (get<0>(tQcQ(0, m, 0)) < binfo.actual_seqlen_q - blockIdx.x * kBlockM) {
-            copy(gmem_thr_copy_Q, tQgQ(_, m, _), tQsQ(_, m, _));
-        }
     }
 
-    // // Copy rmem to smem
-    // // copy(tQrQ, tQsQ);
-    // cute::cp_async_wait<0>();
-    // __syncthreads();
-    // // if (cute::thread(1, 0)) { print(tQsQ); }
-    // // Tensor sQNoSwizzle = make_tensor(make_smem_ptr(reinterpret_cast<Element *>(smem_)), typename Kernel_traits::SmemLayoutQNoSwizzle{});
-    // // if (cute::thread0()) { print(sQNoSwizzle); }
+    __device__ inline void store(float (&frag)[2 * MMAS_M]) {
+        if( qid_ == 0 ) {
+            #pragma unroll
+            for( int mi = 0; mi < MMAS_M; mi++ ) {
+                int offset = mi * 16 * WARPS_N;
+                smem_write_[offset + 0 * 8 * WARPS_N] = frag[mi * 2 + 0];
+                smem_write_[offset + 1 * 8 * WARPS_N] = frag[mi * 2 + 1];
+            }
+        }
+    }
 
-    if (Kernel_traits::Is_Q_in_regs) {
-        cute::cp_async_wait<0>();
-        __syncthreads();
-        copy(smem_thr_copy_Q, tSsQ, tSrQ_copy_view);
-        __syncthreads();
+    __device__ inline void load(read_t (&frag)[2 * MMAS_M]) {
+        #pragma unroll
+        for( int mi = 0; mi < MMAS_M; mi++ ) {
+            int offset = mi * 16 * 4;
+            frag[mi * 2 + 0] = smem_read_[offset + 0 * 8 * 4];
+            frag[mi * 2 + 1] = smem_read_[offset + 1 * 8 * 4];
+        }
     }
 
-    uint32_t n_block_max = cute::ceil_div(binfo.actual_seqlen_k, kBlockN);
-    if (Is_causal) {
-        n_block_max = std::min(n_block_max, cute::ceil_div(blockIdx.x * kBlockM, kBlockN) + 1);
-        // if (threadIdx.x == 0 && blockIdx.y == 0 && blockIdx.z == 0) {
-        //     printf("blockIdx.x = %d, n_block_max = %d\n", blockIdx.x, n_block_max);
-        // }
-    }
-
-    // // Copy rmem to smem
-    Tensor tKrK = make_fragment_like(tKsK);
-    // copy(gmem_thr_copy_KV, tKgK, tKrK);
-    // copy(tKrK, tKsK);
-    if (Is_even_M || n_block_max > 1) {
-        copy(gmem_thr_copy_KV, tKgK, tKsK);
-    } else {
-        // We don't need to clear the sK smem tiles since we'll mask out the scores anyway.
-        // copy_if(gmem_thr_copy_KV, tKVpKV, tKgK, tKsK);
-        #pragma unroll
-        for (int n = 0; n < size<1>(tKgK); ++n) {
-            if (get<0>(tKVcKV(0, n, 0)) < binfo.actual_seqlen_k) {
-                copy(gmem_thr_copy_KV, tKgK(_, n, _), tKsK(_, n, _));
-            }
+    __device__ inline void load_row(read_t (&frag)[MMAS_M], int row) {
+        #pragma unroll
+        for( int mi = 0; mi < MMAS_M; mi++ ) {
+            int offset = mi * 16 * 4;
+            frag[mi] = smem_read_row_[offset + 0 * 8 * 4 + row * 4];
         }
     }
-    cute::cp_async_fence();
-    // Advance gK
-    tKgK.data() = tKgK.data() + kBlockN * params.k_row_stride;
-
-    clear(acc_o);
-
-    auto seeds = at::cuda::philox::unpack(params.philox_args);
-    unsigned long long seed = std::get<0>(seeds);
-    unsigned long long offset = std::get<1>(seeds) + (bidb * params.h + bidh) * 32 + tidx % 32;
-    // Unrolling seems to help even though it says there's more register spilling
-    #pragma unroll 2
-    for (uint32_t n_block = 0; n_block < n_block_max; ++n_block) {
-        Tensor acc_s = partition_fragment_C(tiled_mma, Shape<Int<kBlockM>, Int<kBlockN>>{});  // (MMA=4, MMA_M, MMA_N)
-        // if (cute::thread0()) { print(acc_s.layout()); printf("\n"); }
-        CUTE_STATIC_ASSERT_V(size<1>(tSrQ) == size<1>(acc_s));                     // MMA_M
-        CUTE_STATIC_ASSERT_V(size<1>(tSrK) == size<2>(acc_s));                     // MMA_N
 
-        clear(acc_s);
-        cute::cp_async_wait<0>();
-        __syncthreads();
+    int qid_;
+    float *smem_write_;
+    read_t *smem_read_;
+    read_t *smem_read_row_;
+
+};
+
+
+template<typename Cta_tile, typename Kernel_traits>
+struct Softmax_base {
+
+    // The Mma tile.
+    using Mma_tile = fmha::Hmma_tile<Cta_tile>;
+
+    // The number of MMAs in M/N dimensions.
+    static constexpr int MMAS_M = Mma_tile::MMAS_M;
+    static constexpr int MMAS_N = Mma_tile::MMAS_N;
+
+    // The number of groups of warp such that we have at most 4 warps writing consecutive elements.
+    static constexpr int GROUPS = fmha::DivUpConstexpr(Cta_tile::WARPS_N, 4);
+    // The number of elements that we are going to store per row.
+    static constexpr int ELEMENTS_PER_ROW = Cta_tile::WARPS_N / GROUPS;
+    // The number of rows.
+    static constexpr int ROWS = Cta_tile::M * GROUPS;
+    // The total number of elements.
+    static constexpr int ELEMENTS = ROWS * ELEMENTS_PER_ROW;
+
+    // Ctor.
+    template<typename Params>
+    inline __device__ Softmax_base(const Params &params, void *smem, int tidx)
+        :  // packed_mask_ptr_(reinterpret_cast<const char*>(params.packed_mask_ptr)),
+          smem_(reinterpret_cast<float *>(smem)), tidx_(tidx) {
+
+        // Move to the 1st mask loaded by the thread+ tidx;
+        // packed_mask_ptr_ += bidb * params.packed_mask_stride_in_bytes + tidx * sizeof(uint32_t);
+
+        // Extract the position in the warp.
+        int warp = tidx / Cta_tile::THREADS_PER_WARP;
+        int lane = tidx % Cta_tile::THREADS_PER_WARP;
+
+        // Decompose the warp index into M and N.
+        int warp_m = warp % Cta_tile::WARPS_M;
+        int warp_n = warp / Cta_tile::WARPS_M;
+
+        // Decompose the warp-n index into group/position-inside-the-group.
+        int warp_g = warp_n / ELEMENTS_PER_ROW;
+        int warp_i = warp_n % ELEMENTS_PER_ROW;
+
+        // The location written by the threads.
+        int write_row = warp_g * (ROWS / GROUPS) + warp_m * Mma_tile::M_PER_MMA + lane / 4;
+        int write_col = warp_i;
+
+        // Assemble the write pointer.
+        smem_write_ = &smem_[write_row * ELEMENTS_PER_ROW + write_col];
+
+        // Assemble the read pointer.
+        smem_read_ = &smem_[warp_m * Mma_tile::M_PER_MMA + lane / 4];
+    }
 
-        Tensor tVrV = make_fragment_like(tVsV);
-        // copy(gmem_thr_copy_KV, tVgV, tVrV);
-        if (Is_even_M || n_block < n_block_max - 1) {
-            copy(gmem_thr_copy_KV, tVgV, tVsV);
-        } else {
-            // Clear the smem tiles to account for predicated off loads
-            // clear(tVsV);
-            // copy_if(gmem_thr_copy_KV, tKVpKV, tVgV, tVsV);
-            #pragma unroll
-            for (int n = 0; n < size<1>(tVgV); ++n) {
-                if (get<0>(tKVcKV(0, n, 0)) < binfo.actual_seqlen_k - n_block * kBlockN) {
-                    copy(gmem_thr_copy_KV, tVgV(_, n, _), tVsV(_, n, _));
-                } else {
-                    clear(tVsV(_, n, _));
+    template<bool zero=false, typename Mask>
+    inline __device__ void apply_mask(const Mask &mask) {
+        #pragma unroll
+        for( int mi = 0; mi < MMAS_M; ++mi ) {
+            #pragma unroll
+            for( int ii = 0; ii < 2; ++ii ) {
+                #pragma unroll
+                for( int ni = 0; ni < MMAS_N; ++ni ) {
+                    #pragma unroll
+                    for( int jj = 0; jj < 4; ++jj ) {
+                        if( !mask.is_valid(mi, ni, ii, jj) ) {
+                            elt_[2 * mi + ii][4 * ni + jj] = zero ? 0.f : -INFINITY;
+                        }
+                    }
                 }
             }
         }
-        cute::cp_async_fence();
-        // Advance gV
-        tVgV.data() = tVgV.data() + kBlockN * params.v_row_stride;
-
-        if (!Kernel_traits::Is_Q_in_regs) {
-            copy(smem_thr_copy_Q, tSsQ(_, _, _0{}), tSrQ_copy_view(_, _, _0{}));
-        }
-        copy(smem_thr_copy_K, tSsK(_, _, _0{}), tSrK_copy_view(_, _, _0{}));
-        #pragma unroll
-        for (int i = 0; i < size<2>(tSrQ); ++i) {
-            if (i < size<2>(tSrQ) - 1) {
-                if (!Kernel_traits::Is_Q_in_regs) {
-                    copy(smem_thr_copy_Q, tSsQ(_, _, i + 1), tSrQ_copy_view(_, _, i + 1));
-                }
-                copy(smem_thr_copy_K, tSsK(_, _, i + 1), tSrK_copy_view(_, _, i + 1));
+    }
+
+    // Apply the exp to all the elements.
+    template <bool max_in_base2=false, bool elt_in_base2=false>
+    inline __device__ void apply_exp(const float (&max)[MMAS_M * 2]) {
+        #pragma unroll
+        for( int mi = 0; mi < MMAS_M * 2; ++mi ) {
+            // Instead of computing exp(x - max), we compute exp2(x * log_2(e) -
+            // max * log_2(e)) This allows the compiler to use the ffma
+            // instruction instead of fadd and fmul separately.
+            constexpr float kLog2e = M_LOG2E;
+            const float max_base2 = max_in_base2 ? max[mi] : max[mi] * kLog2e;
+            #pragma unroll
+            for( int ni = 0; ni < MMAS_N * 4; ++ni ) {
+                // elt_[mi][ni] = apply_exp_(elt_[mi][ni], max[mi]);
+                elt_[mi][ni] = apply_exp2_(elt_in_base2 ? elt_[mi][ni] : elt_[mi][ni] * kLog2e,
+                                           max_base2);
             }
-            cute::gemm(tiled_mma, tSrQ(_, _, i), tSrK(_, _, i), acc_s);
         }
+    }
 
-        // if (cute::thread0() && n_block == 0) { print(acc_s.layout()); printf("\n"); }
-        // if (cute::thread0() && n_block == 0) { print(acc_s); }
+    // Apply the exp to all the elements.
+    template <bool scale_max=true>
+    inline __device__ void scale_apply_exp(const float (&max)[MMAS_M * 2], const float scale_) {
+        const float max_scale = scale_max ? scale_ * M_LOG2E : M_LOG2E;
+        const float scale = scale_ * M_LOG2E;
+        #pragma unroll
+        for( int mi = 0; mi < MMAS_M * 2; ++mi ) {
+            // Instead of computing exp(x - max), we compute exp2(x * log_2(e) -
+            // max * log_2(e)) This allows the compiler to use the ffma
+            // instruction instead of fadd and fmul separately.
+            const float max_scaled = max[mi] * max_scale;
+            #pragma unroll
+            for( int ni = 0; ni < MMAS_N * 4; ++ni ) {
+                elt_[mi][ni] = apply_exp2_(elt_[mi][ni] * scale, max_scaled);
+            }
+        }
+    }
 
-        // Reshape acc_s from (MMA=4, MMA_M, MMA_N) to (nrow=(2, MMA_M), ncol=(2, MMA_N))
-        Layout s_l = logical_divide(acc_s.layout(), Shape<_2>{});  // ((2, 2), MMA_M, MMA_N)
-        Tensor scores = make_tensor(acc_s.data(),
-                                    make_layout(make_layout(get<0, 1>(s_l), get<1>(s_l)),
-                                                make_layout(get<0, 0>(s_l), get<2>(s_l))));
-        // if (cute::thread(0, 0)) { print(scores); }
-        if (!Is_causal) {
-            if (!Is_even_M && n_block == n_block_max - 1) {
-                flash::apply_mask(scores, binfo.actual_seqlen_k - n_block * kBlockN);
-            }
-        } else {
-            if (n_block >= n_block_max - cute::ceil_div(kBlockM, kBlockN)) {
-                flash::apply_mask_causal(scores, n_block * kBlockN, binfo.actual_seqlen_k,
-                                         blockIdx.x * kBlockM);
-            }
-        }
-        // if (cute::thread0()) { print(scores); }
-
-        // // Copy rmem to smem
-        // copy(tVrV, tVsV);
-
-        if (n_block == 0) {
-            flash::template reduce_max</*zero_init=*/true>(scores, scores_max);
-        } else {
-            Tensor scores_max_prev = make_fragment_like(scores_max);
-            copy(scores_max, scores_max_prev);
-            flash::template reduce_max</*zero_init=*/false>(scores, scores_max);
-            // Reshape acc_o from (MMA=4, MMA_M, MMA_K) to (nrow=(2, MMA_M), ncol=(2, MMA_K))
-            Layout o_l = logical_divide(acc_o.layout(), Shape<_2>{});  // ((2, 2), MMA_M, MMA_N)
-            Tensor acc_o_reshaped = make_tensor(acc_o.data(),
-                                                make_layout(make_layout(get<0, 1>(o_l), get<1>(o_l)),
-                                                            make_layout(get<0, 0>(o_l), get<2>(o_l))));
-            #pragma unroll
-            for (int mi = 0; mi < size(scores_max); ++mi) {
-                // TODO: we might need to deal with scores_max == -inf here?
-                float scores_scale = exp2f((scores_max_prev(mi) - scores_max(mi)) * params.scale_softmax_log2);
-                scores_sum(mi) *= scores_scale;
-                #pragma unroll
-                for (int ni = 0; ni < size<1>(acc_o_reshaped); ++ni) {
-                    acc_o_reshaped(mi, ni) *= scores_scale;
-                }
+    // Apply the exp to all the elements.
+    template <bool max_in_base2=false>
+    inline __device__ void apply_exp_col(const float (&max)[MMAS_N * 4]) {
+        #pragma unroll
+        for( int ni = 0; ni < MMAS_N * 4; ++ni ) {
+            constexpr float kLog2e = M_LOG2E;
+            const float max_base2 = max_in_base2 ? max[ni] : max[ni] * kLog2e;
+            #pragma unroll
+            for( int mi = 0; mi < MMAS_M * 2; ++mi ) {
+                elt_[mi][ni] = apply_exp2_(elt_[mi][ni] * kLog2e, max_base2);
             }
-            // if (cute::thread(0, 0)) { print(scores_max_prev); print(scores_sum); }
         }
-        // if (cute::thread(0, 0)) { print(scores_max); }
+    }
+    // inline __device__ void apply_exp_col(const float (&max)[MMAS_N]) {
+    //     constexpr float kLog2e = M_LOG2E;
+    //     #pragma unroll
+    //     for( int ni = 0; ni < MMAS_N * 4; ++ni ) {
+    //         float max_base2 = max_in_base2 ? max[ni / 4] : max[ni / 4] * kLog2e;
+    //         max_base2 = __shfl_sync(0xffffffff, max_base2, (ni % 4) * 8 + threadIdx.x % 8);
+    //         #pragma unroll
+    //         for( int mi = 0; mi < MMAS_M * 2; ++mi ) {
+    //             elt_[mi][ni] = apply_exp2_(elt_[mi][ni] * kLog2e, max_base2);
+    //         }
+    //     }
+    // }
 
-        // Compute the exponential value.
-        // flash::scale_apply_exp(scores, scores_max, params.scale_softmax);
-        flash::scale_apply_exp2(scores, scores_max, params.scale_softmax_log2);
-        // if (cute::thread(1, 0)) { print(scores); }
-
-        Tensor scores_sum_prev = make_fragment_like(scores_sum);
-        if (n_block == 0) {
-            flash::reduce_sum(scores, scores_sum);
-        } else {
-            copy(scores_sum, scores_sum_prev);
-            flash::reduce_sum(scores, scores_sum);
-            #pragma unroll
-            for (int mi = 0; mi < size(scores_sum); ++mi) {
-                scores_sum(mi) += scores_sum_prev(mi);
+    template <bool encode_dropout_in_sign_bit=false>
+    inline __device__ void apply_dropout_16bits(Philox &ph, uint16_t p_dropout_in_uint16_t) {
+        // We encode the dropout pattern in the sign bit of the non-negative
+        // softmax to distinguish from pre-existing zeros
+        auto encode_dropout = [](bool keep, float val) {
+            return keep ? val : (encode_dropout_in_sign_bit ? -val : float(0));
+        };
+        #pragma unroll
+        for( int mi = 0; mi < MMAS_M; mi++ ) {
+            #pragma unroll
+            for( int ni = 0; ni < MMAS_N; ni++ ) {
+                uint16_t tmp[8];
+                // fmha::uint4_to_ushort8(ph(), tmp);
+                uint4 tmp_32 = ph();
+                fmha::uint4_to_ushort8(tmp_32, tmp);
+                // if ((threadIdx.x % 32 == 0) && (blockIdx.x == 0) && (blockIdx.y == 0)) {
+                //     printf("tidx = %d, ni = %d, ph  Philox: %u, %u, %u, %u\n", threadIdx.x, ni, tmp_32.x, tmp_32.y, tmp_32.z, tmp_32.w);
+                // }
+                #pragma unroll
+                for (int ii = 0; ii < 2; ++ii) {
+                    #pragma unroll
+                    for (int jj = 0; jj < 4; ++jj) {
+                        elt_[mi * 2 + ii][4 * ni + jj] =
+                            encode_dropout(tmp[ii * 4 + jj] <= p_dropout_in_uint16_t, elt_[mi * 2 + ii][4 * ni + jj]);
+                    }
+                }
             }
         }
+    }
 
-        // if (cute::thread(0, 0)) { print(scores_sum); }
-
-        // if (Is_dropout) {
-        //     flash::apply_dropout(scores, ph, params.p_dropout_in_uint8_t);
-        // }
-
-        // Convert acc_s from fp32 to fp16/bf16
-        cutlass::NumericArrayConverter<Element, ElementAccum, size(scores)> convert_p;
-        auto frag_p = convert_p(*reinterpret_cast<const cutlass::Array<float, size(scores)>*>(scores.data()));
-        Tensor rP = make_tensor(make_rmem_ptr<Element>(&frag_p), scores.layout());
-        // if (cute::thread(1, 0)) { print(rP); }
-        // Reshape rP from (nrow=(2, MMA_M), ncol=(2, MMA_N)) to ((2, 2, 2), MMA_M, MMA_N / 2) if using m16n8k16
-        // Reshape rP from (nrow=(2, MMA_M), ncol=(2, MMA_N)) to ((2, 2), MMA_M, MMA_N) if using m16n8k8
-        using MMA_N_divisor = typename std::conditional<std::is_same<typename Kernel_traits::TiledMma::Shape_MNK, Shape<_16, _8, _8>>::value,
-            _1, _2>::type;
-        // Layout p_l = logical_divide(rP.layout(), Shape<X, Shape<X, _2>>{});  // ((2, MMA_M), (2, (2, MMA_N / 2)))
-        Layout p_l = logical_divide(rP.layout(), Shape<X, Shape<X, MMA_N_divisor>>{});  // ((2, MMA_M), (2, (2, MMA_N / 2)))
-        Tensor tOrP = make_tensor(rP.data(),
-                                  make_layout(make_layout(get<1, 0>(p_l), get<0, 0>(p_l), get<1, 1, 0>(p_l)),
-                                              get<0, 1>(p_l),
-                                              get<1, 1, 1>(p_l)));
-        CUTE_STATIC_ASSERT_V(size<1>(tOrP) == size<1>(acc_o));                     // MMA_M
-        CUTE_STATIC_ASSERT_V(size<2>(tOrP) == size<2>(tOrV));                     // MMA_K
-
-        if (Is_dropout) {
-            // if (cute::thread0()) {print(tOrP);}
-            // flash::apply_dropout(tOrP, ph, params.p_dropout_in_uint8_t);
-            // flash::apply_dropout(tOrP, ph, params.p_dropout_in_uint16_t);
-            auto seeds = at::cuda::philox::unpack(params.philox_args);
-            // TODO: we might switch from 16x32 block to 32x16 block.
-            uint32_t warp_id = tidx / 32;
-            uint32_t block_row_idx = (blockIdx.x * Kernel_traits::kNWarps + warp_id) * (kBlockM / Kernel_traits::kNWarps / 16);
-            uint32_t block_col_idx = n_block * kBlockN / 32;
-            uint32_t num_block_cols = cute::ceil_div(binfo.actual_seqlen_k, 32);
-            uint32_t subsequence_start = block_row_idx * num_block_cols + block_col_idx;
-            flash::apply_dropout_philox(tOrP, params.p_dropout_in_uint8_t,
-                                        seed, offset, subsequence_start, num_block_cols);
-            // if (cute::thread0()) {print(tOrP);}
+    template <bool encode_dropout_in_sign_bit=false>
+    inline __device__ void apply_dropout_16bits(Philox &ph, uint16_t p_dropout_in_uint16_t,
+                                                unsigned long long philox_subsequence) {
+        // We encode the dropout pattern in the sign bit of the non-negative
+        // softmax to distinguish from pre-existing zeros
+        auto encode_dropout = [](bool keep, float val) {
+            return keep ? val : (encode_dropout_in_sign_bit ? -val : float(0));
+        };
+        static_assert(MMAS_M == 1);  // We're assuming 16x16 blocks.
+        #pragma unroll
+        for( int mi = 0; mi < MMAS_M; mi++ ) {
+            #pragma unroll
+            for( int ni = 0; ni < MMAS_N; ni++ ) {
+                uint16_t tmp[8];
+                // fmha::uint4_to_ushort8(ph(), tmp);
+                fmha::uint4_to_ushort8(ph(philox_subsequence + ni * Cta_tile::WARPS_N), tmp);
+                // uint4 tmp_32 = ph(philox_subsequence + ni * Cta_tile::WARPS_N);
+                // fmha::uint4_to_ushort8(tmp_32, tmp);
+                // if ((threadIdx.x == 0) && (blockIdx.x == 0) && (blockIdx.y == 0)) {
+                //     printf("ni = %d, ph  Philox: %u, %u, %u, %u\n", ni, tmp_32.x, tmp_32.y, tmp_32.z, tmp_32.w);
+                // }
+                #pragma unroll
+                for (int ii = 0; ii < 2; ++ii) {
+                    #pragma unroll
+                    for (int jj = 0; jj < 4; ++jj) {
+                        elt_[mi * 2 + ii][4 * ni + jj] =
+                            encode_dropout(tmp[ii * 4 + jj] <= p_dropout_in_uint16_t, elt_[mi * 2 + ii][4 * ni + jj]);
+                    }
+                }
+            }
         }
-        // if (cute::thread0() && n_block == 0) { print(tOrP.layout()); printf("\n"); }
+    }
 
-        cute::cp_async_wait<0>();
-        __syncthreads();
-        // if (cute::thread(0, 0)) { print(sV); }
-        // if (cute::thread(0, 0)) { print(tVsV); }
-        // Tensor sVNoSwizzle = make_tensor(make_smem_ptr(reinterpret_cast<Element *>(smem_) + 128 * 64 + 64 * 64), typename Kernel_traits::SmemLayoutVNoSwizzle{});
-        // if (cute::thread0()) { print(sVNoSwizzle); }
-
-        if (n_block < n_block_max - 1) {
-            // copy(gmem_thr_copy_KV, tKgK, tKrK);
-            if (Is_even_M || n_block + 1 < n_block_max - 1) {
-                copy(gmem_thr_copy_KV, tKgK, tKsK);
-            } else {
-                // Set predicates for n bounds
-                // CUTLASS_PRAGMA_UNROLL
-                // for (int n = 0; n < size<0>(tKVpKV); ++n) {
-                //     tKVpKV(n, 0) = get<0>(tKVcKV(0, n, 0)) < binfo.actual_seqlen_k - (n_block + 1) * kBlockN;  // blk_n coord < residue_n
+    template <bool encode_dropout_in_sign_bit=false>
+    inline __device__ void apply_dropout_16bits(Philox &ph0, Philox &ph1, uint16_t p_dropout_in_uint16_t) {
+        // We encode the dropout pattern in the sign bit of the non-negative
+        // softmax to distinguish from pre-existing zeros
+        auto encode_dropout = [](bool keep, float val) {
+            return keep ? val : (encode_dropout_in_sign_bit ? -val : float(0));
+        };
+        #pragma unroll
+        for( int mi = 0; mi < MMAS_M; mi++ ) {
+            static_assert(MMAS_N % 2 == 0);
+            #pragma unroll
+            for( int ni = 0; ni < MMAS_N; ni += 2 ) {
+                uint16_t tmp[8];
+                fmha::uint4_to_ushort8(ph0(), tmp);
+                // if ((threadIdx.x == 0) && (blockIdx.x == 0) && (blockIdx.y == 0)) {
+                //     printf("ni = %d, ph  Philox: %u, %u, %u, %u\n", ni, tmp.x, tmp.y, tmp.z, tmp.w);
+                // }
+                #pragma unroll
+                for (int ii = 0; ii < 2; ++ii) {
+                    #pragma unroll
+                    for (int jj = 0; jj < 4; ++jj) {
+                        elt_[mi * 2 + ii][4 * ni + jj] =
+                            encode_dropout(tmp[ii * 4 + jj] <= p_dropout_in_uint16_t, elt_[mi * 2 + ii][4 * ni + jj]);
+                    }
+                }
+                fmha::uint4_to_ushort8(ph1(), tmp);
+                // if ((threadIdx.x == 0) && (blockIdx.x == 0) && (blockIdx.y == 0)) {
+                //     printf("ni = %d, ph  Philox: %u, %u, %u, %u\n", ni, tmp.x, tmp.y, tmp.z, tmp.w);
                 // }
-                // copy_if(gmem_thr_copy_KV, tKVpKV, tKgK, tKsK);
                 #pragma unroll
-                for (int n = 0; n < size<1>(tKgK); ++n) {
-                    if (get<0>(tKVcKV(0, n, 0)) < binfo.actual_seqlen_k - (n_block + 1) * kBlockN) {
-                        copy(gmem_thr_copy_KV, tKgK(_, n, _), tKsK(_, n, _));
+                for (int ii = 0; ii < 2; ++ii) {
+                    #pragma unroll
+                    for (int jj = 0; jj < 4; ++jj) {
+                        elt_[mi * 2 + ii][4 * (ni + 1) + jj] =
+                            encode_dropout(tmp[ii * 4 + jj] <= p_dropout_in_uint16_t, elt_[mi * 2 + ii][4 * (ni + 1) + jj]);
                     }
                 }
             }
-            cute::cp_async_fence();
-            // Advance gK
-            tKgK.data() = tKgK.data() + kBlockN * params.k_row_stride;
-        }
-
-        copy(smem_thr_copy_V, tOsV(_, _, _0{}), tOrV_copy_view(_, _, _0{}));
-        #pragma unroll
-        for (int i = 0; i < size<2>(tOrP); ++i) {
-            if (i < size<2>(tOrP) - 1) { copy(smem_thr_copy_V, tOsV(_, _, i + 1), tOrV_copy_view(_, _, i + 1)); }
-            cute::gemm(tiled_mma, tOrP(_, _, i), tOrV(_, _, i), acc_o);
-        }
-
-        // if (cute::thread0()) { print(acc_o); }
-        // // Copy rmem to smem
-        // if (n_block < n_block_max - 1) {
-        //     copy(tKrK, tKsK);
-        // }
-
-    }
-
-    // Epilogue
-
-    // Reshape acc_o from (MMA=4, MMA_M, MMA_K) to (nrow=(2, MMA_M), ncol=(2, MMA_K))
-    Layout o_l = logical_divide(acc_o.layout(), Shape<_2>{});  // ((2, 2), MMA_M, MMA_N)
-    Tensor acc_o_reshaped = make_tensor(acc_o.data(),
-                                        make_layout(make_layout(get<0, 1>(o_l), get<1>(o_l)),
-                                                    make_layout(get<0, 0>(o_l), get<2>(o_l))));
-    Tensor lse = make_fragment_like(scores_sum);
-    #pragma unroll
-    for (int mi = 0; mi < size<0>(acc_o_reshaped); ++mi) {
-        float sum = scores_sum(mi);
-        float inv_sum = (sum == 0.f || sum != sum) ? 1.f : 1.f / sum;
-        lse(mi) = (sum == 0.f || sum != sum) ? INFINITY : scores_max(mi) * params.scale_softmax + __logf(sum);
-        #pragma unroll
-        for (int ni = 0; ni < size<1>(acc_o_reshaped); ++ni) {
-            acc_o_reshaped(mi, ni) *= inv_sum;
-        }
-    }
-
-    // if (cute::thread0()) { print(acc_o_reshaped); }
-
-    // Convert acc_o from fp32 to fp16/bf16
-    cutlass::NumericArrayConverter<Element, ElementAccum, size(acc_o)> convert_o;
-    auto frag_o = convert_o(*reinterpret_cast<const cutlass::Array<float, size(acc_o)>*>(acc_o.data()));
-    Tensor rO = make_tensor(make_rmem_ptr<Element>(&frag_o), acc_o.layout());
-
-    Tensor sO = make_tensor(sQ.data(), typename Kernel_traits::SmemLayoutO{});    // (SMEM_M,SMEM_N)
-    // Partition sO to match the accumulator partitioning
-    auto smem_thr_copy_O = make_tiled_copy_C(typename Kernel_traits::SmemCopyAtomO{}, tiled_mma).get_thread_slice(tidx);
-    Tensor taccOrO = smem_thr_copy_O.retile_S(rO);        // ((Atom,AtomNum), MMA_M, MMA_N)
-    Tensor taccOsO = smem_thr_copy_O.partition_D(sO);     // ((Atom,AtomNum),PIPE_M,PIPE_N)
-
-    // sO has the same size as sQ, so we don't need to sync here.
-    if (Kernel_traits::Is_Q_in_regs) { __syncthreads(); }
-
-    copy(smem_thr_copy_O, taccOrO, taccOsO);
-
-    const uint32_t row_offset_o = binfo.sum_s_q * params.o_row_stride + bidh * params.o_head_stride;
-    const uint32_t row_offset_lse = (bidb * params.h + bidh) * params.seqlen_q;
-    Tensor mO = make_tensor(make_gmem_ptr(reinterpret_cast<Element *>(params.o_ptr) + row_offset_o),
-                            make_shape(binfo.actual_seqlen_q, Int<kHeadDim>{}),
-                            make_stride(params.o_row_stride, _1{}));
-    Tensor gO = local_tile(mO, Shape<Int<kBlockM>>{}, make_coord(blockIdx.x));  // (kBlockM, kHeadDim)
-    Tensor mLSE = make_tensor(make_gmem_ptr(reinterpret_cast<ElementAccum *>(params.softmax_lse_ptr) + row_offset_lse),
-                              make_shape(binfo.actual_seqlen_q), Stride<_1>{});
-    Tensor gLSE = local_tile(mLSE, Shape<Int<kBlockM>>{}, make_coord(blockIdx.x));  // (kBlockM)
-
-    auto gmem_thr_copy_O = typename Kernel_traits::GmemTiledCopyO{}.get_thread_slice(tidx);
-    Tensor tOsO = gmem_thr_copy_O.partition_S(sO);                                   //               ((Atom,AtomNum),ATOM_M,ATOM_N)
-    Tensor tOgO = gmem_thr_copy_O.partition_D(gO);
-
-    // Tensor cO = local_tile(make_identity_tensor(make_shape(size<0>(mO), size<1>(mO))),
-    //                        Shape<Int<kBlockM>>{}, make_coord(blockIdx.x));
-    // Tensor tOcQ  = thr_mma.partition_fragment_A(cO);                           // (MMA,MMA_M,MMA_K)
-    // if (cute::thread0()) { print(tOcQ); }
-    // if (cute::thread0()) { print(lse); }
-    __syncthreads();
-
-    Tensor tOrO = make_tensor<Element>(shape(tOgO));
-    copy(gmem_thr_copy_O, tOsO, tOrO);
-
-    static_assert(decltype(size(lse))::value % 2 == 0);
-    if (tidx % 4 == 0) {
-        #pragma unroll
-        for (int mi = 0; mi < size(lse) / 2; ++mi) {
-            // printf("tidx = %d, row0 = %d, row1 = %d\n", tidx, tidx / 32 * 16 + (tidx % 32) / 4 + 0, tidx / 32 * 16 + (tidx % 32) / 4 + 8);
-            const uint32_t row0 = (mi * Kernel_traits::kNWarps + tidx / 32) * 16 + (tidx % 32) / 4 + 0;
-            const uint32_t row1 = (mi * Kernel_traits::kNWarps + tidx / 32) * 16 + (tidx % 32) / 4 + 8;
-            if (row0 < binfo.actual_seqlen_q - blockIdx.x * kBlockM) { gLSE(row0) = lse(mi * 2); }
-            if (row1 < binfo.actual_seqlen_q - blockIdx.x * kBlockM) { gLSE(row1) = lse(mi * 2 + 1); }
-        }
-    }
-
-    // Construct identity layout for sO and sK
-    Tensor cO = make_identity_tensor(make_shape(size<0>(sO), size<1>(sO)));    // (BLK_M,BLK_K) -> (blk_m,blk_k)
-    // Repeat the partitioning with identity layouts
-    Tensor tOcO = gmem_thr_copy_O.partition_D(cO);                             // (ACPY,ACPY_M,ACPY_K) -> (blk_m,blk_k)
-    // For some reason this calls global store with size=16 (instead of 128) so it's much slower.
-    // By calling copy on each slice indexed by m, it calls global store with size=128.
-    // copy(gmem_thr_copy_O, tOrO, tOgO);
-    #pragma unroll
-    for (int m = 0; m < size<1>(tOgO); ++m) {
-        // if (cute::thread0()) {printf("m = %d\n", get<0>(tQcQ(0, m, 0))); }
-        if (get<0>(tOcO(0, m, 0)) < binfo.actual_seqlen_q - blockIdx.x * kBlockM) {
-            copy(gmem_thr_copy_O, tOrO(_, m, _), tOgO(_, m, _));
         }
     }
-}
+
+    // Scale all the elements.
+    inline __device__ void scale(const float (&sum)[MMAS_M * 2]) {
+        // Precompute the inverse sum to normalize. Without -use_fast_math, it makes a huge deal.
+        float inv_sum[MMAS_M * 2];
+        #pragma unroll
+        for( int mi = 0; mi < MMAS_M * 2; ++mi ) {
+            inv_sum[mi] = (sum[mi] == 0.f || sum[mi] != sum[mi]) ? 1.f : 1.f / sum[mi];
+        }
+
+        // Update the values.
+        #pragma unroll
+        for( int mi = 0; mi < MMAS_M * 2; ++mi ) {
+            #pragma unroll
+            for( int ni = 0; ni < MMAS_N * 4; ++ni ) {
+                elt_[mi][ni] *= inv_sum[mi];
+            }
+        }
+    }
+
+    // Subtract all elements by dp_sum
+    inline __device__ void subtract_dp_sum(const float (&dp_sum)[MMAS_M * 2]) {
+        #pragma unroll
+        for( int mi = 0; mi < MMAS_M * 2; ++mi ) {
+            #pragma unroll
+            for( int ni = 0; ni < MMAS_N * 4; ++ni ) {
+                elt_[mi][ni] -= dp_sum[mi];
+            }
+        }
+    }
+
+    // The pointer to the mask.
+    const char *packed_mask_ptr_;
+    // Shared memory for the CTA-wide reduction.
+    float *smem_, *smem_write_, *smem_read_;
+    // The current thread index.
+    int tidx_;
+    // The elements.
+    float elt_[MMAS_M * 2][MMAS_N * 4];
+};
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
-template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Return_softmax, bool Is_even_M, typename Params>
-inline __device__ void compute_attn(const Params &params) {
-    // The block index for the batch.
-    // const int bidb = blockIdx.x;
-    const int bidb = blockIdx.y;
-    // The block index for the head.
-    // const int bidh = blockIdx.y;
-    const int bidh = blockIdx.z;
-    // The thread index.
-    const int tidx = threadIdx.x;
-
-    // We want the fwd and bwd to generate the same dropout pattern (RNG), without restricting
-    // them to have the same number of threads or have to traverse the attention matrix
-    // in the same order.
-    // In the Philox RNG, we use the offset to store the batch, head, and the lane id
-    // (within a warp). We use the subsequence to store the location of the 16 x 16 blocks within
-    // the attention matrix. This way, as long as we have the batch, head, and the location of
-    // the 16 x 16 block within the attention matrix, we can generate the exact same dropout pattern.
-    auto seeds = at::cuda::philox::unpack(params.philox_args);
-    Philox ph(std::get<0>(seeds), 0, std::get<1>(seeds) + (bidb * params.h + bidh) * 32 + tidx % 32);
+template<typename Cta_tile, typename Kernel_traits>
+struct Softmax : public Softmax_base<Cta_tile, Kernel_traits> {
 
-    flash::compute_attn_1Mblock<Kernel_traits, Is_dropout, Is_causal, Return_softmax, Is_even_M>(params, bidb, bidh, ph);
-}
+    // The base class.
+    using Base = Softmax_base<Cta_tile, Kernel_traits>;
+    // The fragment.
+    using Fragment_a = fmha::Fragment_a<fmha::Row>;
+
+    static_assert(Fragment_a::NUM_REGS == 4);
+
+    static constexpr int WARPS_M = Cta_tile::WARPS_M;
+    static constexpr int WARPS_N = Cta_tile::WARPS_N;
+    // The MMAs.
+    static constexpr int MMAS_M = Base::MMAS_M;
+    static constexpr int MMAS_N = Base::MMAS_N;
+
+    // The accumulators.
+    using Accumulator = fmha::Fragment_accumulator;
+    using Accumulator_out = Fragment<uint16_t, 8>;
+    static_assert(Accumulator_out::NUM_REGS == 4);
+
+    static_assert(std::is_same<Accumulator::Data_type, float>::value);
+
+    using Smem_tile_red = Smem_tile_reduce<Cta_tile, Kernel_traits>;
+    static_assert(Smem_tile_red::ELTS_PER_TILE == Cta_tile::M * WARPS_N);
+    // Ctor.
+    template<typename Params>
+    inline __device__ Softmax(const Params &params, void *smem, int tidx)
+        : Base(params, smem, tidx)
+        , params_scale_bmm1_(params.scale_bmm1) 
+        , smem_sum_(static_cast<float*>(smem), tidx)
+        , smem_max_(static_cast<float*>(smem) + Smem_tile_red::ELTS_PER_TILE, tidx) {
+    }
+
+    // Pack the data to a fragment for the next GEMM.
+    template<typename elem_type=__half, int K, int M>
+    inline __device__ void pack(Fragment_a (&dst)[K][M]) const {
+        #pragma unroll
+        for( int mi = 0; mi < M; ++mi ) {
+            #pragma unroll
+            for( int ki = 0; ki < K; ++ki ) {
+
+                // 1st row - 4 elements per row.
+                float tmp_00 = this->elt_[2 * mi + 0][4 * ki + 0];
+                float tmp_01 = this->elt_[2 * mi + 0][4 * ki + 1];
+                float tmp_02 = this->elt_[2 * mi + 0][4 * ki + 2];
+                float tmp_03 = this->elt_[2 * mi + 0][4 * ki + 3];
+
+                // 2nd row - 4 elements per row.
+                float tmp_10 = this->elt_[2 * mi + 1][4 * ki + 0];
+                float tmp_11 = this->elt_[2 * mi + 1][4 * ki + 1];
+                float tmp_12 = this->elt_[2 * mi + 1][4 * ki + 2];
+                float tmp_13 = this->elt_[2 * mi + 1][4 * ki + 3];
+
+                // Pack to 4 registers.
+                dst[ki][mi].reg(0) = fmha::float2_pack<elem_type>(tmp_00, tmp_01);
+                dst[ki][mi].reg(1) = fmha::float2_pack<elem_type>(tmp_10, tmp_11);
+                dst[ki][mi].reg(2) = fmha::float2_pack<elem_type>(tmp_02, tmp_03);
+                dst[ki][mi].reg(3) = fmha::float2_pack<elem_type>(tmp_12, tmp_13);
+            }
+        }
+    }
+
+    // Scale FP32 fragments
+    inline __device__ void unpack(const Accumulator (&acc)[MMAS_M][MMAS_N]) {
+        const float scalef = reinterpret_cast<const float &>(this->params_scale_bmm1_);
+
+        #pragma unroll
+        for( int mi = 0; mi < MMAS_M; ++mi ) {
+            #pragma unroll
+            for( int ni = 0; ni < MMAS_N; ++ni ) {
+                // 1st row - 4 elements per row.
+                this->elt_[2 * mi + 0][4 * ni + 0] = acc[mi][ni].elt(0) * scalef;
+                this->elt_[2 * mi + 0][4 * ni + 1] = acc[mi][ni].elt(1) * scalef;
+                this->elt_[2 * mi + 0][4 * ni + 2] = acc[mi][ni].elt(4) * scalef;
+                this->elt_[2 * mi + 0][4 * ni + 3] = acc[mi][ni].elt(5) * scalef;
+                // 2nd row - 4 elements per row.
+                this->elt_[2 * mi + 1][4 * ni + 0] = acc[mi][ni].elt(2) * scalef;
+                this->elt_[2 * mi + 1][4 * ni + 1] = acc[mi][ni].elt(3) * scalef;
+                this->elt_[2 * mi + 1][4 * ni + 2] = acc[mi][ni].elt(6) * scalef;
+                this->elt_[2 * mi + 1][4 * ni + 3] = acc[mi][ni].elt(7) * scalef;
+            }
+        }
+    }
+
+    // Scale FP32 fragments
+    inline __device__ void unpack_noscale(const Accumulator (&acc)[MMAS_M][MMAS_N]) {
+
+        #pragma unroll
+        for( int mi = 0; mi < MMAS_M; ++mi ) {
+            #pragma unroll
+            for( int ni = 0; ni < MMAS_N; ++ni ) {
+                // 1st row - 4 elements per row.
+                this->elt_[2 * mi + 0][4 * ni + 0] = acc[mi][ni].elt(0);
+                this->elt_[2 * mi + 0][4 * ni + 1] = acc[mi][ni].elt(1);
+                this->elt_[2 * mi + 0][4 * ni + 2] = acc[mi][ni].elt(4);
+                this->elt_[2 * mi + 0][4 * ni + 3] = acc[mi][ni].elt(5);
+                // 2nd row - 4 elements per row.
+                this->elt_[2 * mi + 1][4 * ni + 0] = acc[mi][ni].elt(2);
+                this->elt_[2 * mi + 1][4 * ni + 1] = acc[mi][ni].elt(3);
+                this->elt_[2 * mi + 1][4 * ni + 2] = acc[mi][ni].elt(6);
+                this->elt_[2 * mi + 1][4 * ni + 3] = acc[mi][ni].elt(7);
+            }
+        }
+    }
+
+    template<bool zero_init=true, typename Operator>
+    __device__ inline void thread_reduce_(float (&frag)[2 * MMAS_M], Operator &op) {
+        #pragma unroll
+        for( int mi = 0; mi < 2 * MMAS_M; mi++ ) {
+            frag[mi] = zero_init ? this->elt_[mi][0] : op(frag[mi], this->elt_[mi][0]);
+            #pragma unroll
+            for( int ni = 1; ni < 4 * MMAS_N; ni++ ) {
+                frag[mi] = op(frag[mi], this->elt_[mi][ni]);
+            }
+        }
+    }
+
+    template<bool zero_init=true, typename Operator>
+    __device__ inline void reduce_(float (&frag)[2 * MMAS_M], Operator &op, Smem_tile_red & smem_red) {
+        thread_reduce_<zero_init>(frag, op);
+        quad_reduce(frag, frag, op);
+        smem_red.store(frag);
+        __syncthreads();
+        typename Smem_tile_red::read_t tmp[2 * MMAS_M];
+        smem_red.load(tmp);
+        quad_allreduce(frag, tmp, op);
+    }
+
+    template<bool zero_init=true>
+    __device__ inline void reduce_max(float (&frag)[2 * MMAS_M]){ 
+        MaxOp<float> max;
+        reduce_<zero_init>(frag, max, smem_max_);
+    }
+
+    __device__ inline void reduce_sum(float (&frag)[2 * MMAS_M]){
+        SumOp<float> sum;
+        reduce_(frag, sum, smem_sum_);
+    }
+
+    template<bool zero_init=true>
+    __device__ inline void reduce_sum_before_sync_(float (&frag)[2 * MMAS_M]){
+        SumOp<float> sum;
+        thread_reduce_<zero_init>(frag, sum);
+        quad_reduce(frag, frag, sum);
+        smem_sum_.store(frag);
+    }
+
+    template<int NROWS, typename Operator>
+    __device__ inline void reduce_after_sync_(float (&frag)[NROWS][MMAS_M],
+                                              const int (&rows)[NROWS],
+                                              Operator &op, Smem_tile_red & smem_red) {
+        #pragma unroll
+        for (int ii = 0; ii < NROWS; ii++) {
+            typename Smem_tile_red::read_t tmp[MMAS_M];
+            smem_red.load_row(tmp, rows[ii]);
+            quad_allreduce(frag[ii], tmp, op);
+        }
+    }
+
+    template<int NROWS>
+    __device__ inline void reduce_sum_after_sync_(float (&frag)[NROWS][MMAS_M],
+                                                  const int (&rows)[NROWS]){
+        SumOp<float> sum;
+        reduce_after_sync_(frag, rows, sum, smem_sum_);
+    }
+
+    template<int NROWS>
+    __device__ inline void reduce_max_after_sync_(float (&frag)[NROWS][MMAS_M],
+                                                  const int (&rows)[NROWS]){
+        MaxOp<float> max;
+        reduce_after_sync_(frag, rows, max, smem_max_);
+    }
+
+    const uint32_t params_scale_bmm1_;
+    Smem_tile_red smem_max_;
+    Smem_tile_red smem_sum_;
+};
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
-} // namespace flash
+}  // namespace fmha
```

### Comparing `flash_attn-1.0.8/csrc/flash_attn/src/fmha/gemm.h` & `flash_attn-1.0.9/csrc/flash_attn/src/fmha/gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/src/fmha/gmem_tile.h` & `flash_attn-1.0.9/csrc/flash_attn/src/fmha/gmem_tile.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/src/fmha/kernel_traits.h` & `flash_attn-1.0.9/csrc/flash_attn/src/fmha/kernel_traits.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/src/fmha/mask.h` & `flash_attn-1.0.9/csrc/flash_attn/src/fmha/mask.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/src/fmha/smem_tile.h` & `flash_attn-1.0.9/csrc/flash_attn/src/fmha/smem_tile.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/src/fmha/utils.h` & `flash_attn-1.0.9/csrc/flash_attn/src/fmha/utils.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/src/fmha.h` & `flash_attn-1.0.9/csrc/flash_attn/src/fmha.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/src/fmha_block_dgrad_fp16_kernel_loop.sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/src/fmha_block_dgrad_fp16_kernel_loop.sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/src/fmha_block_dgrad_kernel_1xN_loop.h` & `flash_attn-1.0.9/csrc/flash_attn/src/fmha_block_dgrad_kernel_1xN_loop.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/src/fmha_block_fprop_fp16_kernel.sm80.cu` & `flash_attn-1.0.9/csrc/flash_attn/src/fmha_block_fprop_fp16_kernel.sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/src/fmha_block_fprop_kernel_1xN.h` & `flash_attn-1.0.9/csrc/flash_attn/src/fmha_block_fprop_kernel_1xN.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/src/fmha_bwd_hdim32.cu` & `flash_attn-1.0.9/csrc/flash_attn/src/fmha_bwd_hdim32.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/src/fmha_bwd_hdim64.cu` & `flash_attn-1.0.9/csrc/flash_attn/src/fmha_bwd_hdim64.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/src/fmha_bwd_launch_template.h` & `flash_attn-1.0.9/csrc/flash_attn/src/fmha_bwd_launch_template.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/src/fmha_dgrad_kernel_1xN_loop.h` & `flash_attn-1.0.9/csrc/flash_attn/src/fmha_dgrad_kernel_1xN_loop.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/src/fmha_fprop_kernel_1xN.h` & `flash_attn-1.0.9/csrc/flash_attn/src/fmha_fprop_kernel_1xN.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/src/fmha_fwd_hdim32.cu` & `flash_attn-1.0.9/csrc/flash_attn/src/fmha_fwd_hdim32.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/src/fmha_fwd_hdim64.cu` & `flash_attn-1.0.9/csrc/flash_attn/src/fmha_fwd_hdim64.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/src/fmha_fwd_launch_template.h` & `flash_attn-1.0.9/csrc/flash_attn/src/fmha_fwd_launch_template.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/src/fmha_utils.h` & `flash_attn-1.0.9/csrc/flash_attn/src/fmha_utils.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/src/philox.cuh` & `flash_attn-1.0.9/csrc/flash_attn/src/philox.cuh`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_attn/src/static_switch.h` & `flash_attn-1.0.9/csrc/flash_attn/src/static_switch.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/flash_gen/decoder_masked_multihead_attention.cu` & `flash_attn-1.0.9/csrc/flash_gen/decoder_masked_multihead_attention.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/ft_attention/cuda_bf16_fallbacks.cuh` & `flash_attn-1.0.9/csrc/ft_attention/cuda_bf16_fallbacks.cuh`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/ft_attention/cuda_bf16_wrapper.h` & `flash_attn-1.0.9/csrc/ft_attention/cuda_bf16_wrapper.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/ft_attention/decoder_masked_multihead_attention.cu` & `flash_attn-1.0.9/csrc/ft_attention/decoder_masked_multihead_attention.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/ft_attention/decoder_masked_multihead_attention.h` & `flash_attn-1.0.9/csrc/ft_attention/decoder_masked_multihead_attention.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/ft_attention/decoder_masked_multihead_attention_utils.h` & `flash_attn-1.0.9/csrc/ft_attention/decoder_masked_multihead_attention_utils.h`

 * *Files 7% similar despite different names*

```diff
@@ -1545,208 +1545,208 @@
 }
 
 inline __device__ void apply_rotary_embedding(float2& q, int tid, int rot_embed_dim, int t_step, const float* rotary_cos, const float* rotary_sin)
 {
     if (2 * tid >= rot_embed_dim) {
         return;
     }
-    const auto coef = rotary_embedding_coefficient(2 * tid, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef = rotary_embedding_coefficient(2 * tid, t_step, rotary_cos, rotary_sin);
     q               = rotary_embedding_transform(q, coef);
 }
 
 inline __device__ void apply_rotary_embedding(float2& q, float2& k, int tid, int rot_embed_dim, int t_step, const float* rotary_cos, const float* rotary_sin)
 {
     if (2 * tid >= rot_embed_dim) {
         return;
     }
-    const auto coef = rotary_embedding_coefficient(2 * tid, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef = rotary_embedding_coefficient(2 * tid, t_step, rotary_cos, rotary_sin);
     q               = rotary_embedding_transform(q, coef);
     k               = rotary_embedding_transform(k, coef);
 }
 
 inline __device__ void apply_rotary_embedding(float4& q, int tid, int rot_embed_dim, int t_step, const float* rotary_cos, const float* rotary_sin)
 {
     if (4 * tid >= rot_embed_dim) {
         return;
     }
 
     Float4_&   q_    = *reinterpret_cast<Float4_*>(&q);
-    const auto coef0 = rotary_embedding_coefficient(4 * tid, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef0 = rotary_embedding_coefficient(4 * tid, t_step, rotary_cos, rotary_sin);
     q_.x             = rotary_embedding_transform(q_.x, coef0);
-    const auto coef1 = rotary_embedding_coefficient(4 * tid + 2, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef1 = rotary_embedding_coefficient(4 * tid + 2, t_step, rotary_cos, rotary_sin);
     q_.y             = rotary_embedding_transform(q_.y, coef1);
 }
 
 inline __device__ void apply_rotary_embedding(float4& q, float4& k, int tid, int rot_embed_dim, int t_step, const float* rotary_cos, const float* rotary_sin)
 {
     if (4 * tid >= rot_embed_dim) {
         return;
     }
 
     Float4_&   q_    = *reinterpret_cast<Float4_*>(&q);
     Float4_&   k_    = *reinterpret_cast<Float4_*>(&k);
-    const auto coef0 = rotary_embedding_coefficient(4 * tid, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef0 = rotary_embedding_coefficient(4 * tid, t_step, rotary_cos, rotary_sin);
     q_.x             = rotary_embedding_transform(q_.x, coef0);
     k_.x             = rotary_embedding_transform(k_.x, coef0);
-    const auto coef1 = rotary_embedding_coefficient(4 * tid + 2, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef1 = rotary_embedding_coefficient(4 * tid + 2, t_step, rotary_cos, rotary_sin);
     q_.y             = rotary_embedding_transform(q_.y, coef1);
     k_.y             = rotary_embedding_transform(k_.y, coef1);
 }
 
 inline __device__ void apply_rotary_embedding(uint32_t& q, int tid, int rot_embed_dim, int t_step, const uint16_t* rotary_cos, const uint16_t* rotary_sin)
 {
     if (2 * tid >= rot_embed_dim) {
         return;
     }
-    const auto coef = rotary_embedding_coefficient(2 * tid, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef = rotary_embedding_coefficient(2 * tid, t_step, rotary_cos, rotary_sin);
     q               = rotary_embedding_transform(q, coef);
 }
 
 inline __device__ void apply_rotary_embedding(uint32_t& q, uint32_t& k, int tid, int rot_embed_dim, int t_step, const uint16_t* rotary_cos, const uint16_t* rotary_sin)
 {
     if (2 * tid >= rot_embed_dim) {
         return;
     }
-    const auto coef = rotary_embedding_coefficient(2 * tid, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef = rotary_embedding_coefficient(2 * tid, t_step, rotary_cos, rotary_sin);
     q               = rotary_embedding_transform(q, coef);
     k               = rotary_embedding_transform(k, coef);
 }
 
 inline __device__ void apply_rotary_embedding(uint2& q, int tid, int rot_embed_dim, int t_step, const uint16_t* rotary_cos, const uint16_t* rotary_sin)
 {
     if (4 * tid >= rot_embed_dim) {
         return;
     }
-    const auto coef0 = rotary_embedding_coefficient(4 * tid, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef0 = rotary_embedding_coefficient(4 * tid, t_step, rotary_cos, rotary_sin);
     q.x              = rotary_embedding_transform(q.x, coef0);
-    const auto coef1 = rotary_embedding_coefficient(4 * tid + 2, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef1 = rotary_embedding_coefficient(4 * tid + 2, t_step, rotary_cos, rotary_sin);
     q.y              = rotary_embedding_transform(q.y, coef1);
 }
 
 inline __device__ void apply_rotary_embedding(uint2& q, uint2& k, int tid, int rot_embed_dim, int t_step, const uint16_t* rotary_cos, const uint16_t* rotary_sin)
 {
     if (4 * tid >= rot_embed_dim) {
         return;
     }
-    const auto coef0 = rotary_embedding_coefficient(4 * tid, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef0 = rotary_embedding_coefficient(4 * tid, t_step, rotary_cos, rotary_sin);
     q.x              = rotary_embedding_transform(q.x, coef0);
     k.x              = rotary_embedding_transform(k.x, coef0);
-    const auto coef1 = rotary_embedding_coefficient(4 * tid + 2, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef1 = rotary_embedding_coefficient(4 * tid + 2, t_step, rotary_cos, rotary_sin);
     q.y              = rotary_embedding_transform(q.y, coef1);
     k.y              = rotary_embedding_transform(k.y, coef1);
 }
 
 inline __device__ void apply_rotary_embedding(uint4& q, int tid, int rot_embed_dim, int t_step, const uint16_t* rotary_cos, const uint16_t* rotary_sin)
 {
     if (8 * tid >= rot_embed_dim) {
         return;
     }
-    const auto coef0 = rotary_embedding_coefficient(8 * tid, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef0 = rotary_embedding_coefficient(8 * tid, t_step, rotary_cos, rotary_sin);
     q.x              = rotary_embedding_transform(q.x, coef0);
-    const auto coef1 = rotary_embedding_coefficient(8 * tid + 2, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef1 = rotary_embedding_coefficient(8 * tid + 2, t_step, rotary_cos, rotary_sin);
     q.y              = rotary_embedding_transform(q.y, coef1);
-    const auto coef2 = rotary_embedding_coefficient(8 * tid + 4, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef2 = rotary_embedding_coefficient(8 * tid + 4, t_step, rotary_cos, rotary_sin);
     q.z              = rotary_embedding_transform(q.z, coef2);
-    const auto coef3 = rotary_embedding_coefficient(8 * tid + 6, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef3 = rotary_embedding_coefficient(8 * tid + 6, t_step, rotary_cos, rotary_sin);
     q.w              = rotary_embedding_transform(q.w, coef3);
 }
 
 inline __device__ void apply_rotary_embedding(uint4& q, uint4& k, int tid, int rot_embed_dim, int t_step, const uint16_t* rotary_cos, const uint16_t* rotary_sin)
 {
     if (8 * tid >= rot_embed_dim) {
         return;
     }
-    const auto coef0 = rotary_embedding_coefficient(8 * tid, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef0 = rotary_embedding_coefficient(8 * tid, t_step, rotary_cos, rotary_sin);
     q.x              = rotary_embedding_transform(q.x, coef0);
     k.x              = rotary_embedding_transform(k.x, coef0);
-    const auto coef1 = rotary_embedding_coefficient(8 * tid + 2, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef1 = rotary_embedding_coefficient(8 * tid + 2, t_step, rotary_cos, rotary_sin);
     q.y              = rotary_embedding_transform(q.y, coef1);
     k.y              = rotary_embedding_transform(k.y, coef1);
-    const auto coef2 = rotary_embedding_coefficient(8 * tid + 4, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef2 = rotary_embedding_coefficient(8 * tid + 4, t_step, rotary_cos, rotary_sin);
     q.z              = rotary_embedding_transform(q.z, coef2);
     k.z              = rotary_embedding_transform(k.z, coef2);
-    const auto coef3 = rotary_embedding_coefficient(8 * tid + 6, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef3 = rotary_embedding_coefficient(8 * tid + 6, t_step, rotary_cos, rotary_sin);
     q.w              = rotary_embedding_transform(q.w, coef3);
     k.w              = rotary_embedding_transform(k.w, coef3);
 }
 
 #ifdef ENABLE_BF16
 inline __device__ void apply_rotary_embedding(__nv_bfloat162& q, int tid, int rot_embed_dim, int t_step, const __nv_bfloat16* rotary_cos, const __nv_bfloat16* rotary_sin)
 {
     if (2 * tid >= rot_embed_dim) {
         return;
     }
-    const auto coef = rotary_embedding_coefficient(2 * tid, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef = rotary_embedding_coefficient(2 * tid, t_step, rotary_cos, rotary_sin);
     q               = rotary_embedding_transform(q, coef);
 }
 
 inline __device__ void apply_rotary_embedding(__nv_bfloat162& q, __nv_bfloat162& k, int tid, int rot_embed_dim, int t_step, const __nv_bfloat16* rotary_cos, const __nv_bfloat16* rotary_sin)
 {
     if (2 * tid >= rot_embed_dim) {
         return;
     }
-    const auto coef = rotary_embedding_coefficient(2 * tid, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef = rotary_embedding_coefficient(2 * tid, t_step, rotary_cos, rotary_sin);
     q               = rotary_embedding_transform(q, coef);
     k               = rotary_embedding_transform(k, coef);
 }
 
 inline __device__ void apply_rotary_embedding(bf16_4_t& q, int tid, int rot_embed_dim, int t_step, const __nv_bfloat16* rotary_cos, const __nv_bfloat16* rotary_sin)
 {
     if (4 * tid >= rot_embed_dim) {
         return;
     }
-    const auto coef0 = rotary_embedding_coefficient(4 * tid, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef0 = rotary_embedding_coefficient(4 * tid, t_step, rotary_cos, rotary_sin);
     q.x              = rotary_embedding_transform(q.x, coef0);
-    const auto coef1 = rotary_embedding_coefficient(4 * tid + 2, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef1 = rotary_embedding_coefficient(4 * tid + 2, t_step, rotary_cos, rotary_sin);
     q.y              = rotary_embedding_transform(q.y, coef1);
 }
 
 inline __device__ void apply_rotary_embedding(bf16_4_t& q, bf16_4_t& k, int tid, int rot_embed_dim, int t_step, const __nv_bfloat16* rotary_cos, const __nv_bfloat16* rotary_sin)
 {
     if (4 * tid >= rot_embed_dim) {
         return;
     }
-    const auto coef0 = rotary_embedding_coefficient(4 * tid, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef0 = rotary_embedding_coefficient(4 * tid, t_step, rotary_cos, rotary_sin);
     q.x              = rotary_embedding_transform(q.x, coef0);
     k.x              = rotary_embedding_transform(k.x, coef0);
-    const auto coef1 = rotary_embedding_coefficient(4 * tid + 2, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef1 = rotary_embedding_coefficient(4 * tid + 2, t_step, rotary_cos, rotary_sin);
     q.y              = rotary_embedding_transform(q.y, coef1);
     k.y              = rotary_embedding_transform(k.y, coef1);
 }
 
 inline __device__ void apply_rotary_embedding(bf16_8_t& q, int tid, int rot_embed_dim, int t_step, const __nv_bfloat16* rotary_cos, const __nv_bfloat16* rotary_sin)
 {
     if (8 * tid >= rot_embed_dim) {
         return;
     }
-    const auto coef0 = rotary_embedding_coefficient(8 * tid, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef0 = rotary_embedding_coefficient(8 * tid, t_step, rotary_cos, rotary_sin);
     q.x              = rotary_embedding_transform(q.x, coef0);
-    const auto coef1 = rotary_embedding_coefficient(8 * tid + 2, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef1 = rotary_embedding_coefficient(8 * tid + 2, t_step, rotary_cos, rotary_sin);
     q.y              = rotary_embedding_transform(q.y, coef1);
-    const auto coef2 = rotary_embedding_coefficient(8 * tid + 4, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef2 = rotary_embedding_coefficient(8 * tid + 4, t_step, rotary_cos, rotary_sin);
     q.z              = rotary_embedding_transform(q.z, coef2);
-    const auto coef3 = rotary_embedding_coefficient(8 * tid + 6, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef3 = rotary_embedding_coefficient(8 * tid + 6, t_step, rotary_cos, rotary_sin);
     q.w              = rotary_embedding_transform(q.w, coef3);
 }
 
 inline __device__ void apply_rotary_embedding(bf16_8_t& q, bf16_8_t& k, int tid, int rot_embed_dim, int t_step, const __nv_bfloat16* rotary_cos, const __nv_bfloat16* rotary_sin)
 {
     if (8 * tid >= rot_embed_dim) {
         return;
     }
-    const auto coef0 = rotary_embedding_coefficient(8 * tid, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef0 = rotary_embedding_coefficient(8 * tid, t_step, rotary_cos, rotary_sin);
     q.x              = rotary_embedding_transform(q.x, coef0);
     k.x              = rotary_embedding_transform(k.x, coef0);
-    const auto coef1 = rotary_embedding_coefficient(8 * tid + 2, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef1 = rotary_embedding_coefficient(8 * tid + 2, t_step, rotary_cos, rotary_sin);
     q.y              = rotary_embedding_transform(q.y, coef1);
     k.y              = rotary_embedding_transform(k.y, coef1);
-    const auto coef2 = rotary_embedding_coefficient(8 * tid + 4, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef2 = rotary_embedding_coefficient(8 * tid + 4, t_step, rotary_cos, rotary_sin);
     q.z              = rotary_embedding_transform(q.z, coef2);
     k.z              = rotary_embedding_transform(k.z, coef2);
-    const auto coef3 = rotary_embedding_coefficient(8 * tid + 6, t_step, rotary_cos + t_step * rot_embed_dim / 2, rotary_sin + t_step * rot_embed_dim / 2);
+    const auto coef3 = rotary_embedding_coefficient(8 * tid + 6, t_step, rotary_cos, rotary_sin);
     q.w              = rotary_embedding_transform(q.w, coef3);
     k.w              = rotary_embedding_transform(k.w, coef3);
 }
 #endif  // ENABLE_BF16
 
 template<typename Vec_T, typename T>
 __device__ __inline__ void vec_from_smem_transpose(Vec_T& vec, T* smem, int transpose_idx, int smem_pitch);
```

### Comparing `flash_attn-1.0.8/csrc/ft_attention/ft_attention.cpp` & `flash_attn-1.0.9/csrc/ft_attention/ft_attention.cpp`

 * *Files 2% similar despite different names*

```diff
@@ -156,24 +156,23 @@
         CHECK_CONTIGUOUS(length_per_sample);
         TORCH_CHECK(length_per_sample.dtype() == torch::kInt32);
     }
 
     if (rotary_cos_.has_value()) {
         auto rotary_cos = rotary_cos_.value();
         CHECK_DEVICE(rotary_cos);
-        int rotary_seqlen = rotary_cos.size(0);
-        rotary_embedding_dim = rotary_cos.size(1) * 2;
-        CHECK_SHAPE(rotary_cos, rotary_seqlen, rotary_embedding_dim / 2);
+        rotary_embedding_dim = rotary_cos.size(-1) * 2;
+        CHECK_SHAPE(rotary_cos, batch_size, rotary_embedding_dim / 2);
         CHECK_CONTIGUOUS(rotary_cos);
         TORCH_CHECK(rotary_cos.scalar_type() == input_type);
 
         TORCH_CHECK(rotary_sin_.has_value());
         auto rotary_sin = rotary_sin_.value();
         CHECK_DEVICE(rotary_sin);
-        CHECK_SHAPE(rotary_cos, rotary_seqlen, rotary_embedding_dim / 2);
+        CHECK_SHAPE(rotary_cos, batch_size, rotary_embedding_dim / 2);
         CHECK_CONTIGUOUS(rotary_sin);
         TORCH_CHECK(rotary_sin.scalar_type() == input_type);
     }
 
     if (nnz_head_idx_.has_value()) {
         auto nnz_head_idx = nnz_head_idx_.value();
         CHECK_DEVICE(nnz_head_idx);
```

### Comparing `flash_attn-1.0.8/csrc/fused_dense_lib/fused_dense.cpp` & `flash_attn-1.0.9/csrc/fused_dense_lib/fused_dense.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/fused_dense_lib/fused_dense_cuda.cu` & `flash_attn-1.0.9/csrc/fused_dense_lib/fused_dense_cuda.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/fused_softmax/fused_softmax.cpp` & `flash_attn-1.0.9/csrc/fused_softmax/fused_softmax.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/fused_softmax/scaled_masked_softmax.h` & `flash_attn-1.0.9/csrc/fused_softmax/scaled_masked_softmax.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/fused_softmax/scaled_masked_softmax_cuda.cu` & `flash_attn-1.0.9/csrc/fused_softmax/scaled_masked_softmax_cuda.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/fused_softmax/scaled_upper_triang_masked_softmax.h` & `flash_attn-1.0.9/csrc/fused_softmax/scaled_upper_triang_masked_softmax.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/fused_softmax/scaled_upper_triang_masked_softmax_cuda.cu` & `flash_attn-1.0.9/csrc/fused_softmax/scaled_upper_triang_masked_softmax_cuda.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/fused_softmax/type_shim.h` & `flash_attn-1.0.9/csrc/fused_softmax/type_shim.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln.h` & `flash_attn-1.0.9/csrc/layer_norm/ln.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_api.cpp` & `flash_attn-1.0.9/csrc/layer_norm/ln_api.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_bwd_1024.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_1024.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_bwd_1280.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_1280.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_bwd_1536.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_1536.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_bwd_2048.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_2048.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_bwd_256.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_256.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_bwd_2560.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_2560.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_bwd_3072.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_3072.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_bwd_4096.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_4096.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_bwd_512.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_512.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_bwd_5120.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_5120.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_bwd_6144.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_6144.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_bwd_7168.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_7168.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_bwd_768.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_768.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_bwd_8192.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_8192.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_bwd_kernels.cuh` & `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_kernels.cuh`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_bwd_semi_cuda_kernel_old.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_semi_cuda_kernel_old.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_fwd_1024.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_1024.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_fwd_10240.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_10240.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_fwd_12288.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_12288.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_fwd_128.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_128.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_fwd_1280.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_1280.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_fwd_1536.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_1536.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_fwd_2048.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_2048.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_fwd_256.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_256.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_fwd_2560.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_2560.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_fwd_3072.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_3072.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_fwd_384.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_384.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_fwd_4096.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_4096.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_fwd_512.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_512.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_fwd_5120.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_5120.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_fwd_6144.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_6144.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_fwd_7168.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_7168.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_fwd_768.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_768.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_fwd_8192.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_8192.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_fwd_9216.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_9216.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_fwd_cuda_kernel_old.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_cuda_kernel_old.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_fwd_kernels.cuh` & `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_kernels.cuh`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_kernel_traits.h` & `flash_attn-1.0.9/csrc/layer_norm/ln_kernel_traits.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_parallel_bwd_1024.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_1024.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_parallel_bwd_1280.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_1280.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_parallel_bwd_1536.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_1536.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_parallel_bwd_2048.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_2048.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_parallel_bwd_256.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_256.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_parallel_bwd_2560.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_2560.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_parallel_bwd_3072.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_3072.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_parallel_bwd_4096.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_4096.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_parallel_bwd_512.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_512.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_parallel_bwd_5120.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_5120.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_parallel_bwd_6144.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_6144.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_parallel_bwd_7168.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_7168.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_parallel_bwd_768.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_768.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_parallel_bwd_8192.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_8192.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_parallel_fwd_1024.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_1024.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_parallel_fwd_128.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_2048.cu`

 * *Files 24% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 #include "ln_parallel_residual_fwd_kernels.cuh"
 
 // Create forward launch function and register. Macro signature:
 //  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
 
-REGISTER_FWD_PARALLEL_LAUNCHER(  128, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_FWD_PARALLEL_LAUNCHER(  128, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_FWD_PARALLEL_LAUNCHER(  128, fp32, fp16, fp32, fp16, fp32, 1, 4, 1,  8);
-REGISTER_FWD_PARALLEL_LAUNCHER(  128, fp16, fp16, fp32, fp16, fp32, 1, 4, 1,  8);
-REGISTER_FWD_PARALLEL_LAUNCHER(  128, fp32, fp16, fp16, fp16, fp32, 1, 4, 1,  8);
-REGISTER_FWD_PARALLEL_LAUNCHER(  128, fp32, bf16, fp32, bf16, fp32, 1, 4, 1,  8);
-REGISTER_FWD_PARALLEL_LAUNCHER(  128, bf16, bf16, fp32, bf16, fp32, 1, 4, 1,  8);
-REGISTER_FWD_PARALLEL_LAUNCHER(  128, fp32, bf16, bf16, bf16, fp32, 1, 4, 1,  8);
-REGISTER_FWD_PARALLEL_LAUNCHER(  128, fp16, fp16, fp16, fp16, fp32, 1, 4, 1,  8);
-REGISTER_FWD_PARALLEL_LAUNCHER(  128, bf16, bf16, bf16, bf16, fp32, 1, 4, 1,  8);
+REGISTER_PARALLEL_FWD_LAUNCHER( 2048, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 2048, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 2048, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 2048, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 2048, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 2048, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 2048, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 2048, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 2048, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 2048, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
```

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_parallel_fwd_1280.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_1280.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_parallel_fwd_1536.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_1536.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_parallel_fwd_2048.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_2560.cu`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 #include "ln_parallel_residual_fwd_kernels.cuh"
 
 // Create forward launch function and register. Macro signature:
 //  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
 
-REGISTER_PARALLEL_FWD_LAUNCHER( 2048, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2048, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2048, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2048, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2048, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2048, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2048, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2048, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2048, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2048, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 2560, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 2560, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 2560, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 2560, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 2560, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 2560, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 2560, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 2560, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 2560, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 2560, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
```

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_parallel_fwd_256.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_256.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_parallel_fwd_2560.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_5120.cu`

 * *Files 16% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 #include "ln_parallel_residual_fwd_kernels.cuh"
 
 // Create forward launch function and register. Macro signature:
 //  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
 
-REGISTER_PARALLEL_FWD_LAUNCHER( 2560, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2560, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2560, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2560, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2560, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2560, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2560, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2560, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2560, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 2560, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 5120, fp32, fp32, fp32, fp32, fp32, 1, 1, 4, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 5120, fp16, fp32, fp32, fp32, fp32, 1, 1, 4, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 5120, fp32, fp16, fp32, fp16, fp32, 1, 1, 4, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 5120, fp16, fp16, fp32, fp16, fp32, 1, 1, 4, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 5120, fp32, fp16, fp16, fp16, fp32, 1, 1, 4, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 5120, fp32, bf16, fp32, bf16, fp32, 1, 1, 4, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 5120, bf16, bf16, fp32, bf16, fp32, 1, 1, 4, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 5120, fp32, bf16, bf16, bf16, fp32, 1, 1, 4, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 5120, fp16, fp16, fp16, fp16, fp32, 1, 1, 4, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 5120, bf16, bf16, bf16, bf16, fp32, 1, 1, 4, 16);
```

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_parallel_fwd_3072.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_3072.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_parallel_fwd_4096.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_4096.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_parallel_fwd_512.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_512.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_parallel_fwd_5120.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_768.cu`

 * *Files 21% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 #include "ln_parallel_residual_fwd_kernels.cuh"
 
 // Create forward launch function and register. Macro signature:
 //  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
 
-REGISTER_PARALLEL_FWD_LAUNCHER( 5120, fp32, fp32, fp32, fp32, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 5120, fp16, fp32, fp32, fp32, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 5120, fp32, fp16, fp32, fp16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 5120, fp16, fp16, fp32, fp16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 5120, fp32, fp16, fp16, fp16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 5120, fp32, bf16, fp32, bf16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 5120, bf16, bf16, fp32, bf16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 5120, fp32, bf16, bf16, bf16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 5120, fp16, fp16, fp16, fp16, fp32, 1, 1, 4, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER( 5120, bf16, bf16, bf16, bf16, fp32, 1, 1, 4, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER(  768, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER(  768, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER(  768, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER(  768, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER(  768, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER(  768, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER(  768, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER(  768, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER(  768, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER(  768, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
```

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_parallel_fwd_6144.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_6144.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_parallel_fwd_7168.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_7168.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_parallel_fwd_768.cu` & `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_8192.cu`

 * *Files 25% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 #include "ln_parallel_residual_fwd_kernels.cuh"
 
 // Create forward launch function and register. Macro signature:
 //  HIDDEN_SIZE, WTYPE, ITYPE, RYTPE, OTYPE, CTYPE, CTAS_PER_ROW, WARPS_M, WARPS_N, BYTES_PER_LDG
 
-REGISTER_PARALLEL_FWD_LAUNCHER(  768, fp32, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  768, fp16, fp32, fp32, fp32, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  768, fp32, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  768, fp16, fp16, fp32, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  768, fp32, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  768, fp32, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  768, bf16, bf16, fp32, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  768, fp32, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  768, fp16, fp16, fp16, fp16, fp32, 1, 4, 1, 16);
-REGISTER_PARALLEL_FWD_LAUNCHER(  768, bf16, bf16, bf16, bf16, fp32, 1, 4, 1, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 8192, fp32, fp32, fp32, fp32, fp32, 1, 1, 8, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 8192, fp16, fp32, fp32, fp32, fp32, 1, 1, 8, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 8192, fp32, fp16, fp32, fp16, fp32, 1, 1, 8, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 8192, fp16, fp16, fp32, fp16, fp32, 1, 1, 8, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 8192, fp32, fp16, fp16, fp16, fp32, 1, 1, 8, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 8192, fp32, bf16, fp32, bf16, fp32, 1, 1, 8, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 8192, bf16, bf16, fp32, bf16, fp32, 1, 1, 8, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 8192, fp32, bf16, bf16, bf16, fp32, 1, 1, 8, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 8192, fp16, fp16, fp16, fp16, fp32, 1, 1, 8, 16);
+REGISTER_PARALLEL_FWD_LAUNCHER( 8192, bf16, bf16, bf16, bf16, fp32, 1, 1, 8, 16);
```

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_parallel_res_fwd_kernel.cuh` & `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_residual_fwd_kernels.cuh`

 * *Files 18% similar despite different names*

```diff
@@ -43,15 +43,16 @@
     using Cvec = typename Ktraits::Cvec;
     using Mvec = typename Ktraits::Mvec;
 
     using Stats = typename Ktraits::Stats;
     using stats_t = typename Stats::stats_t;
 
     const bool has_residual = params.residual != nullptr;
-    const bool save_x = has_residual || Is_dropout || !(std::is_same<input_t, residual_t>::value);
+    const bool has_x1 = params.x1 != nullptr;
+    const bool save_x = has_residual || has_x1 || Is_dropout || !(std::is_same<input_t, residual_t>::value);
 
     extern __shared__ char smem_[];
 
     const index_t tidx = threadIdx.x;
     const index_t bidn = blockIdx.x % CTAS_PER_ROW;
     const index_t bidm = blockIdx.x / CTAS_PER_ROW;
     const index_t lane = tidx % THREADS_PER_WARP;
@@ -73,67 +74,82 @@
         auto seeds = at::cuda::philox::unpack(params.philox_args);
         const index_t tidx_global = blockIdx.x * blockDim.x + threadIdx.x;
         curand_init(std::get<0>(seeds), tidx_global, std::get<1>(seeds), &state);
     }
 
     const index_t num_valid_ldgs = ((params.cols / Ktraits::ELTS_PER_LDG) - 1 - c + VEC_COLS_PER_LDG) / VEC_COLS_PER_LDG;
 
-    Wvec gamma[LDGS];
-    Wvec beta[LDGS];
+    Wvec gamma0[LDGS];
+    Wvec beta0[LDGS];
+    Wvec gamma1[LDGS];
+    Wvec beta1[LDGS];
     index_t idx = c;
     #pragma unroll
     for( int it = 0; it < LDGS; it++ ) {
         if (Is_even_cols || (it < num_valid_ldgs)) {
-            gamma[it].load_from(params.gamma, idx);
+            gamma0[it].load_from(params.gamma, idx);
             if (params.beta != nullptr) {
-                beta[it].load_from(params.beta, idx);
+                beta0[it].load_from(params.beta, idx);
             } else {
-                beta[it].zero_();
+                beta0[it].zero_();
+            }
+            if (!Tied_norm) {
+                gamma1[it].load_from(params.gamma1, idx);
+                if (params.beta1 != nullptr) {
+                    beta1[it].load_from(params.beta1, idx);
+                } else {
+                    beta1[it].zero_();
+                }
             }
             idx += VEC_COLS_PER_LDG;
         }
     }
 
     for( int row = r; row < params.rows; row += params.ctas_per_col * ROWS_PER_CTA ) {
-        const int row_x0 = !Has_subset ? row + 1 : x0_subset[row];
-        const int row_z = !Has_subset ? row + 1 : z_subset[row];
-        const bool load_x0 = !Has_subset || row_x0 > 0;
-        index_t idx_x = row * params.cols / Ktraits::ELTS_PER_LDG + c;
-        index_t idx_x0 = !Has_subset ? idx_x : (load_x0 ? (row_x0 - 1) * params.cols / Ktraits::ELTS_PER_LDG + c : 0);
+        index_t idx = row * params.cols / Ktraits::ELTS_PER_LDG + c;
         compute_t xf[LDGS * NUM_ELTS];
         #pragma unroll
         for( int it = 0; it < LDGS; it++ ) {
             if (Is_even_cols || (it < num_valid_ldgs)) {
                 Ivec x0;
+                Ivec x1;
                 Rvec residual;
                 Rvec x;
-                Mvec dmask;
-                if (load_x0) { x0.load_from(params.x0, !Has_subset ? idx_x : idx_x0); }
-                if (has_residual) { residual.load_from(params.residual, idx_x); }
+                Mvec dmask0;
+                Mvec dmask1;
+                x0.load_from(params.x0, idx);
+                if (has_x1) { x1.load_from(params.x1, idx); }
+                if (has_residual) { residual.load_from(params.residual, idx); }
                 #pragma unroll
                 for( int jt = 0; jt < NUM_ELTS; jt++ ) {
                     // TD [2022-04-22]: We're memory bound, not compute bound, so we don't need to use
                     // the more efficient curand_uniform4.
                     compute_t x_ij;
-                    if (load_x0) {
-                        mask_t keep = !Is_dropout ? true : curand_uniform(&state) <= params.dropout_keep_p;
-                        if (Is_dropout) { dmask.data.elt[jt] = keep; }
-                        compute_t x0_ij = compute_t(x0.data.elt[jt]);
-                        x0_ij = keep ? (Is_dropout ? x0_ij * params.dropout_scale : x0_ij) : 0.0f;
-                        x_ij = has_residual ? x0_ij + compute_t(residual.data.elt[jt]) : x0_ij;
+                    mask_t keep0 = !Is_dropout ? true : curand_uniform(&state) <= params.dropout_keep_p;
+                    if (Is_dropout) { dmask0.data.elt[jt] = keep0; }
+                    compute_t x0_ij = compute_t(x0.data.elt[jt]);
+                    x0_ij = keep0 ? (Is_dropout ? x0_ij * params.dropout_scale : x0_ij) : 0.0f;
+                    if (has_x1) {
+                        mask_t keep1 = !Is_dropout ? true : curand_uniform(&state) <= params.dropout_keep_p;
+                        if (Is_dropout) { dmask1.data.elt[jt] = keep1; }
+                        compute_t x1_ij = compute_t(x1.data.elt[jt]);
+                        x1_ij = keep1 ? (Is_dropout ? x1_ij * params.dropout_scale : x1_ij) : 0.0f;
+                        x_ij = has_residual ? x0_ij + x1_ij + compute_t(residual.data.elt[jt]) : x0_ij + x1_ij;
                     } else {
-                        x_ij = has_residual ? compute_t(residual.data.elt[jt]) : 0.f;
+                        x_ij = has_residual ? x0_ij + compute_t(residual.data.elt[jt]) : x0_ij;
                     }
                     if (save_x) { x.data.elt[jt] = x_ij; }
                     xf[it * NUM_ELTS + jt] = x_ij;
                 }
-                if (save_x) { x.store_to(params.x, idx_x); }
-                if (Is_dropout && load_x0) { dmask.store_to(params.dmask, !Has_subset ? idx_x : idx_x0); }
-                idx_x += VEC_COLS_PER_LDG;
-                idx_x0 += VEC_COLS_PER_LDG;
+                if (save_x) { x.store_to(params.x, idx); }
+                if (Is_dropout) {
+                    dmask0.store_to(params.dmask, idx);
+                    if (has_x1) { dmask1.store_to(params.dmask1, idx); }
+                }
+                idx += VEC_COLS_PER_LDG;
             }
         }
 
         static_assert(CTAS_PER_ROW == 1, "Don't support multiple CTAs per row for now");
         const index_t num_vecs = params.cols / Ktraits::ELTS_PER_LDG;
         const index_t num_full_ldgs = num_vecs / Ktraits::VEC_COLS_PER_LDG;
         const index_t remaining_vecs = num_vecs % Ktraits::VEC_COLS_PER_LDG;
@@ -157,31 +173,35 @@
 
         compute_t rs = rsqrtf(m2 * params.inverse_cols + params.epsilon + (!params.is_rms_norm ? 0.f : mu * mu));
 
         if( bidn == 0 && warp_n == 0 && lane == 0 ) {
             rs_ptr[row] = rs;
         }
 
-        const bool save_z = !Has_subset || row_z > 0;
-        if (save_z) {
-            index_t idx_z = (!Has_subset ? row : (row_z - 1)) * params.cols / Ktraits::ELTS_PER_LDG + c;
-            #pragma unroll
-            for( int it = 0; it < LDGS; it++ ) {
-                if (Is_even_cols || (it < num_valid_ldgs)) {
-                    Ovec z;
-                    #pragma unroll
-                    for( int jt = 0; jt < NUM_ELTS; jt++ ) {
-                        compute_t y_ij = compute_t(rs * (xf[it * NUM_ELTS + jt] - (!params.is_rms_norm ? mu : 0.f)));
-                        compute_t g_ij = gamma[it].data.elt[jt];
-                        compute_t b_ij = beta[it].data.elt[jt];
-                        z.data.elt[jt] = output_t(g_ij * y_ij + b_ij);
+        idx = row * params.cols / Ktraits::ELTS_PER_LDG + c;
+        #pragma unroll
+        for( int it = 0; it < LDGS; it++ ) {
+            if (Is_even_cols || (it < num_valid_ldgs)) {
+                Ovec z0;
+                Ovec z1;
+                #pragma unroll
+                for( int jt = 0; jt < NUM_ELTS; jt++ ) {
+                    compute_t y_ij = compute_t(rs * (xf[it * NUM_ELTS + jt] - (!params.is_rms_norm ? mu : 0.f)));
+                    compute_t g0_ij = gamma0[it].data.elt[jt];
+                    compute_t b0_ij = beta0[it].data.elt[jt];
+                    z0.data.elt[jt] = output_t(g0_ij * y_ij + b0_ij);
+                    if (!Tied_norm) {
+                        compute_t g1_ij = gamma1[it].data.elt[jt];
+                        compute_t b1_ij = beta1[it].data.elt[jt];
+                        z1.data.elt[jt] = output_t(g1_ij * y_ij + b1_ij);
                     }
-                    z.store_to(params.z, idx_z);
-                    idx_z += VEC_COLS_PER_LDG;
                 }
+                z0.store_to(params.z, idx);
+                if (!Tied_norm) { z1.store_to(params.z1, idx); }
+                idx += VEC_COLS_PER_LDG;
             }
         }
 
     }
 }
 
 }  // namespace layer_norm
@@ -197,33 +217,34 @@
     typename index_t,
     int HIDDEN_SIZE,
     int CTAS_PER_ROW,
     int WARPS_M,
     int WARPS_N,
     int BYTES_PER_LDG
 >
-void launch_(LaunchParams<FwdParams> &launch_params, const bool configure_params){
+void launch_parallel_residual_(LaunchParams<FwdParams> &launch_params, const bool configure_params){
 
     using Kernel_traits = Kernel_traits<weight_t,
                                         input_t,
                                         residual_t,
                                         output_t,
                                         compute_t,
                                         index_t,
                                         HIDDEN_SIZE,
                                         CTAS_PER_ROW,
                                         WARPS_M,
                                         WARPS_N,
                                         BYTES_PER_LDG
                                         >;
     bool is_even_cols = launch_params.params.cols == HIDDEN_SIZE;
+    bool tied_norm = launch_params.params.gamma1 == nullptr;
     BOOL_SWITCH(launch_params.params.dropout_keep_p < 1.f, IsDropoutConst, [&] {
-        BOOL_SWITCH(has_subset, HasSubsetConst, [&] {
-                BOOL_SWITCH(is_even_cols, IsEvenColsConst, [&] {
-                    auto kernel = &ln_fwd_kernel<Kernel_traits, IsDropoutConst, HasSubsetConst, IsEvenColsConst>;
+        BOOL_SWITCH(tied_norm, TiedNormConst, [&] {
+            BOOL_SWITCH(is_even_cols, IsEvenColsConst, [&] {
+                auto kernel = &ln_parallel_residual_fwd_kernel<Kernel_traits, IsDropoutConst, TiedNormConst, IsEvenColsConst>;
                 if( configure_params ) {
                     int ctas_per_sm;
                     CHECK_CUDA(cudaOccupancyMaxActiveBlocksPerMultiprocessor(
                         &ctas_per_sm, kernel, Kernel_traits::THREADS_PER_CTA, Kernel_traits::SMEM_BYTES_FWD));
                     launch_params.params.ctas_per_col = launch_params.props->multiProcessorCount * ctas_per_sm / Kernel_traits::CTAS_PER_ROW;
                     const size_t rows_per_loop = launch_params.params.ctas_per_col * Kernel_traits::ROWS_PER_CTA;
                     launch_params.elts_per_thread = (launch_params.params.rows + rows_per_loop - 1) / rows_per_loop * Kernel_traits::LDGS * Kernel_traits::NUM_ELTS;
```

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_parallel_residual_bwd_kernels.cuh` & `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_residual_bwd_kernels.cuh`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/ln_utils.cuh` & `flash_attn-1.0.9/csrc/layer_norm/ln_utils.cuh`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/layer_norm/static_switch.h` & `flash_attn-1.0.9/csrc/layer_norm/static_switch.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/rotary/rotary.cpp` & `flash_attn-1.0.9/csrc/rotary/rotary.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/rotary/rotary_cuda.cu` & `flash_attn-1.0.9/csrc/rotary/rotary_cuda.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/xentropy/interface.cpp` & `flash_attn-1.0.9/csrc/xentropy/interface.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/csrc/xentropy/xentropy_kernel.cu` & `flash_attn-1.0.9/csrc/xentropy/xentropy_kernel.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn/attention_kernl.py` & `flash_attn-1.0.9/flash_attn/attention_kernl.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn/bert_padding.py` & `flash_attn-1.0.9/flash_attn/bert_padding.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn/flash_attention.py` & `flash_attn-1.0.9/flash_attn/flash_attention.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn/flash_attn_interface.py` & `flash_attn-1.0.9/flash_attn/flash_attn_interface.py`

 * *Files 0% similar despite different names*

```diff
@@ -275,15 +275,15 @@
             Default to 1 / sqrt(headdim).
         causal: bool. Whether to apply causal attention mask (e.g., for auto-regressive modeling).
         return_attn_probs: bool. Whether to return the attention probabilities. This option is for
            testing only. The returned probabilities are not guaranteed to be correct
            (they might not have the right scaling).
         deterministic: bool. Whether or not to ensure deterministic execution.
     Return:
-        out: (total, nheads, headdim).
+        out: (total_q, nheads, headdim).
         softmax_lse [optional, if return_attn_probs=True]: (batch_size, nheads, seqlen). The
             logsumexp of each row of the matrix QK^T * scaling (e.g., log of the softmax
             normalization factor).
         S_dmask [optional, if return_attn_probs=True]: (batch_size, nheads, seqlen, seqlen).
             The output of softmax (possibly with different scaling). It also encodes the dropout
             pattern (negative means that location was dropped, nonnegative means it was kept).
     """
@@ -311,15 +311,15 @@
             Default to 1 / sqrt(headdim).
         causal: bool. Whether to apply causal attention mask (e.g., for auto-regressive modeling).
         return_attn_probs: bool. Whether to return the attention probabilities. This option is for
            testing only. The returned probabilities are not guaranteed to be correct
            (they might not have the right scaling).
         deterministic: bool. Whether or not to ensure deterministic execution.
     Return:
-        out: (total, nheads, headdim).
+        out: (total_q, nheads, headdim).
         softmax_lse [optional, if return_attn_probs=True]: (batch_size, nheads, seqlen). The
             logsumexp of each row of the matrix QK^T * scaling (e.g., log of the softmax
             normalization factor).
         S_dmask [optional, if return_attn_probs=True]: (batch_size, nheads, seqlen, seqlen).
             The output of softmax (possibly with different scaling). It also encodes the dropout
             pattern (negative means that location was dropped, nonnegative means it was kept).
     """
```

### Comparing `flash_attn-1.0.8/flash_attn/flash_attn_triton.py` & `flash_attn-1.0.9/flash_attn/flash_attn_triton.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn/flash_attn_triton_og.py` & `flash_attn-1.0.9/flash_attn/flash_attn_triton_og.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn/flash_attn_triton_single_query.py` & `flash_attn-1.0.9/flash_attn/flash_attn_triton_single_query.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn/flash_attn_triton_tmp.py` & `flash_attn-1.0.9/flash_attn/flash_attn_triton_tmp.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn/flash_attn_triton_tmp_og.py` & `flash_attn-1.0.9/flash_attn/flash_attn_triton_tmp_og.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn/flash_blocksparse_attention.py` & `flash_attn-1.0.9/flash_attn/flash_blocksparse_attention.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn/flash_blocksparse_attn_interface.py` & `flash_attn-1.0.9/flash_attn/flash_blocksparse_attn_interface.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn/fused_softmax.py` & `flash_attn-1.0.9/flash_attn/fused_softmax.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn/layers/patch_embed.py` & `flash_attn-1.0.9/flash_attn/layers/patch_embed.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn/layers/rotary.py` & `flash_attn-1.0.9/flash_attn/layers/rotary.py`

 * *Files 2% similar despite different names*

```diff
@@ -207,17 +207,19 @@
     def _compute_inv_freq(self, device=None):
         return 1.0 / (self.base ** (torch.arange(0, self.dim, 2, device=device,
                                                  dtype=torch.float32) / self.dim))
 
 
     def _update_cos_sin_cache(self, seqlen, device=None, dtype=None):
         # Reset the tables if the sequence length has changed,
-        # or if we're on a new device (possibly due to tracing for instance)
+        # if we're on a new device (possibly due to tracing for instance),
+        # or if we're switching from inference mode to training
         if (seqlen > self._seq_len_cached or self._cos_cached.device != device
-            or self._cos_cached.dtype != dtype):
+            or self._cos_cached.dtype != dtype
+            or (self.training and self._cos_cached.is_inference())):
             self._seq_len_cached = seqlen
             # We want fp32 here, not self.inv_freq.dtype, since the model could be loaded in bf16
             # And the output of arange can be quite large, so bf16 would lose a lot of precision.
             # However, for compatibility reason, we add an option to use the dtype of self.inv_freq.
             if self.pos_idx_in_fp32:
                 t = torch.arange(seqlen, device=device, dtype=torch.float32)
                 # We want fp32 here as well since inv_freq will be multiplied with t, and the output
```

### Comparing `flash_attn-1.0.8/flash_attn/losses/cross_entropy.py` & `flash_attn-1.0.9/flash_attn/losses/cross_entropy.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn/models/bert.py` & `flash_attn-1.0.9/flash_attn/models/bert.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn/models/gpt.py` & `flash_attn-1.0.9/flash_attn/models/gpt.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn/models/gpt_j.py` & `flash_attn-1.0.9/flash_attn/models/opt.py`

 * *Files 9% similar despite different names*

```diff
@@ -4,21 +4,22 @@
 import re
 
 from collections import OrderedDict
 
 import torch
 import torch.nn.functional as F
 
-from transformers import GPT2Config, GPTJConfig
+from transformers import GPT2Config, OPTConfig
 
 
-def remap_state_dict_hf_gptj(state_dict, config):
-    breakpoint()
+def remap_state_dict_hf_opt(state_dict, config):
     def key_mapping_model(key):
         key = re.sub(r'^model.decoder.', 'transformer.', key)
+        # The OPT-350m model uses '^decoder' instead of '^model.decoder'
+        key = re.sub(r'^decoder.', 'transformer.', key)
         return key
     state_dict = OrderedDict((key_mapping_model(k), v) for k, v in state_dict.items())
     # Word embedding and position embedding
     def key_mapping_emb(key):
         key = re.sub(r'^transformer.embed_tokens.', 'transformer.embeddings.word_embeddings.', key)
         # The OPT-350m model uses has project_in and project_out
         key = re.sub(r'^transformer.project_in.', 'transformer.embeddings.project_in.', key)
@@ -61,43 +62,41 @@
     for l in range(config.n_layer):
         Wq = state_dict.pop(f'transformer.layers.{l}.self_attn.q_proj.weight')
         Wk = state_dict.pop(f'transformer.layers.{l}.self_attn.k_proj.weight')
         Wv = state_dict.pop(f'transformer.layers.{l}.self_attn.v_proj.weight')
         bq = state_dict.pop(f'transformer.layers.{l}.self_attn.q_proj.bias')
         bk = state_dict.pop(f'transformer.layers.{l}.self_attn.k_proj.bias')
         bv = state_dict.pop(f'transformer.layers.{l}.self_attn.v_proj.bias')
-        state_dict[f'transformer.layers.{l}.mixer.Wqkv.weight'] = torch.cat(
-            [Wq, Wk, Wv], dim=0
-        )
-        state_dict[f'transformer.layers.{l}.mixer.Wqkv.bias'] = torch.cat(
-            [bq, bk, bv], dim=0
-        )
+        state_dict[f'transformer.layers.{l}.mixer.Wqkv.weight'] = torch.cat([Wq, Wk, Wv], dim=0)
+        state_dict[f'transformer.layers.{l}.mixer.Wqkv.bias'] = torch.cat([bq, bk, bv], dim=0)
     def key_mapping_attn(key):
         return re.sub(r'^transformer.layers.(\d+).self_attn.out_proj.',
                       r'transformer.layers.\1.mixer.out_proj.', key)
     state_dict = OrderedDict((key_mapping_attn(k), v) for k, v in state_dict.items())
 
     return state_dict
 
 
-def gptj_config_to_gpt2_config(gptj_config: GPTJConfig) -> GPT2Config:
-    headdim = gptj_config.n_embd // gptj_config.n_head
+def opt_config_to_gpt2_config(opt_config: OPTConfig) -> GPT2Config:
+    assert opt_config.layerdrop == 0.0
+    assert opt_config.layer_norm_elementwise_affine
+    word_embed_proj_dim = (None if opt_config.word_embed_proj_dim == opt_config.hidden_size
+                           else opt_config.word_embed_proj_dim)
     return GPT2Config(
-        vocab_size=gptj_config.vocab_size,
-        n_positions=gptj_config.n_positions,
-        n_embd=gptj_config.n_embd,
-        n_layer=gptj_config.n_layer,
-        n_head=gptj_config.n_head,
-        n_inner=gptj_config.n_inner,
-        activation_function=gptj_config.activation_function,
-        resid_pdrop=gptj_config.resid_pdrop,
-        embd_pdrop=gptj_config.embd_pdrop,
-        attn_pdrop=gptj_config.attn_pdrop,
-        layer_norm_epsilon=gptj_config.layer_norm_epsilon,
-        initializer_range=gptj_config.initializer_range,
-        bos_token_id=gptj_config.bos_token_id,
-        eos_token_id=gptj_config.eos_token_id,
+        vocab_size=opt_config.vocab_size,
+        n_positions=opt_config.max_position_embeddings,
+        n_embd=opt_config.hidden_size,
+        n_layer=opt_config.num_hidden_layers,
+        n_head=opt_config.num_attention_heads,
+        n_inner=opt_config.ffn_dim,
+        activation_function=opt_config.activation_function,
+        resid_pdrop=opt_config.dropout,
+        # HF's implementation of OPT doesn't seem to have embedding dropout
+        embd_pdrop=opt_config.dropout,
+        attn_pdrop=opt_config.attention_dropout,
+        initializer_range=opt_config.init_std,
+        bos_token_id=opt_config.bos_token_id,
+        eos_token_id=opt_config.eos_token_id,
         # These are new arguments not in the original GPT2Config
-        prenorm=True,
-        rotary_emb_fraction=gptj_config.rotary_dim / headdim,
-        rotary_emb_interleaved=True
+        prenorm=opt_config.do_layer_norm_before,
+        word_embed_proj_dim=word_embed_proj_dim
     )
```

### Comparing `flash_attn-1.0.8/flash_attn/models/gpt_neox.py` & `flash_attn-1.0.9/flash_attn/models/gpt_neox.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn/models/gptj.py` & `flash_attn-1.0.9/flash_attn/models/gptj.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn/models/llama.py` & `flash_attn-1.0.9/flash_attn/models/llama.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn/models/vit.py` & `flash_attn-1.0.9/flash_attn/models/vit.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn/modules/block.py` & `flash_attn-1.0.9/flash_attn/modules/block.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn/modules/embedding.py` & `flash_attn-1.0.9/flash_attn/modules/embedding.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn/modules/mha.py` & `flash_attn-1.0.9/flash_attn/modules/mha.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn/modules/mlp.py` & `flash_attn-1.0.9/flash_attn/modules/mlp.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn/ops/activations.py` & `flash_attn-1.0.9/flash_attn/ops/activations.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn/ops/fused_dense.py` & `flash_attn-1.0.9/flash_attn/ops/fused_dense.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn/ops/layer_norm.py` & `flash_attn-1.0.9/flash_attn/ops/layer_norm.py`

 * *Files 16% similar despite different names*

```diff
@@ -3,17 +3,25 @@
 
 import torch
 from torch.nn import init
 
 import dropout_layer_norm
 
 
+def maybe_align(x, alignment_in_bytes=16):
+    """Assume that x already has last dim divisible by alignment_in_bytes
+    """
+    # TD [2023-07-04] I'm not 100% sure that clone will align the memory
+    # https://discuss.pytorch.org/t/how-to-ensure-that-tensor-data-ptr-is-aligned-to-16-bytes/183440
+    return x if x.data_ptr() % alignment_in_bytes == 0 else x.clone()
+
+
 def _dropout_add_layer_norm_forward(x0, residual, gamma, beta, rowscale, colscale, dropout_p,
                                     epsilon, residual_in_fp32=False, is_rms_norm=False):
-    """ Assume that arguments are contiguous
+    """ Assume that arguments are contiguous and aligned to 16 bytes
     """
     hidden_size = gamma.numel()
     x0mat = x0.view((-1, hidden_size))
     residualmat = residual.view((-1, hidden_size)) if residual is not None else None
     rowscale = rowscale.view(-1) if rowscale is not None else None
     zmat, xmat, dmask, mu, rsigma = dropout_layer_norm.dropout_add_ln_fwd(
         x0mat, residualmat, gamma, beta, rowscale, colscale, None, None, dropout_p, epsilon,
@@ -22,15 +30,15 @@
     # dmask is None if dropout_p == 0.0
     # xmat is None if dropout_p == 0.0 and residual is None and residual_dtype != input_dtype
     return zmat, xmat if xmat is not None else x0mat, dmask, mu, rsigma
 
 
 def _dropout_add_layer_norm_backward(dz, dx, x, x0, dmask, mu, rsigma, gamma, rowscale, colscale,
                                      dropout_p, has_residual, is_rms_norm=False):
-    """ Assume that arguments are contiguous
+    """ Assume that arguments are contiguous and aligned to 16 bytes
     dx == None means that it was a post-norm architecture
     (x = drop(x0) + residual was not returned in the fwd).
     x0 must not be None if we have colscale.
     """
     hidden_size = gamma.numel()
     xmat = x.view((-1, hidden_size))
     dzmat = dz.view(xmat.shape)
@@ -50,15 +58,15 @@
         dcolscale = rest[0]
         return dx0mat, dresidualmat, dgamma, dbeta, dcolscale
 
 
 def _dropout_add_layer_norm_subset_forward(x0, residual, gamma, beta, colscale, x0_subset,
                                            out_subset, dropout_p, epsilon, rowscale_const,
                                            out_numrows, residual_in_fp32=False, is_rms_norm=False):
-    """ Assume that arguments are contiguous
+    """ Assume that arguments are contiguous and aligned to 16 bytes
     """
     hidden_size = gamma.numel()
     x0mat = x0.view((-1, hidden_size))
     residualmat = residual.view((-1, hidden_size)) if residual is not None else None
     x0_subset = x0_subset.view(-1) if x0_subset is not None else None
     out_subset = out_subset.view(-1) if out_subset is not None else None
     zmat, xmat, dmask, mu, rsigma = dropout_layer_norm.dropout_add_ln_fwd(
@@ -69,15 +77,15 @@
     # xmat is None if dropout_p == 0.0 and residual is None and residual_dtype != input_dtype
     return zmat, xmat if xmat is not None else x0mat, dmask, mu, rsigma
 
 
 def _dropout_add_layer_norm_subset_backward(dz, dx, x, x0, dmask, mu, rsigma, gamma, colscale,
                                             x0_subset, out_subset, dropout_p, rowscale_const,
                                             x0_numrows, has_residual, is_rms_norm=False):
-    """ Assume that arguments are contiguous
+    """ Assume that arguments are contiguous and aligned to 16 bytes
     dx == None means that it was a post-norm architecture
     (x = drop(x0) + residual was not returned in the fwd).
     x0 must not be None if we have colscale.
     """
     hidden_size = gamma.numel()
     xmat = x.view((-1, hidden_size))
     dzmat = dz.view(-1, hidden_size)
@@ -99,15 +107,15 @@
         return dx0mat, dresidualmat, dgamma, dbeta, dcolscale
 
 
 def _dropout_add_layer_norm_parallel_residual_forward(
     x0, x1, residual, gamma0, beta0, gamma1, beta1, dropout_p,
     epsilon, residual_in_fp32=False, is_rms_norm=False
 ):
-    """ Assume that arguments are contiguous
+    """ Assume that arguments are contiguous and aligned to 16 bytes
     """
     hidden_size = gamma0.numel()
     x0mat = x0.view((-1, hidden_size))
     x1mat = x1.view((-1, hidden_size)) if x1 is not None else None
     residualmat = residual.view((-1, hidden_size)) if residual is not None else None
     z0mat, z1mat, xmat, dmask0, dmask1, mu, rsigma = dropout_layer_norm.dropout_add_ln_parallel_residual_fwd(
         x0mat, x1mat, residualmat, gamma0, beta0, gamma1, beta1, dropout_p, epsilon,
@@ -118,15 +126,15 @@
     return z0mat, z1mat, xmat if xmat is not None else x0mat, dmask0, dmask1, mu, rsigma
 
 
 def _dropout_add_layer_norm_parallel_residual_backward(
     dz0, dz1, dx, x, dmask0, dmask1, mu, rsigma, gamma0, gamma1,
     dropout_p, has_x1, has_residual, is_rms_norm=False
 ):
-    """ Assume that arguments are contiguous
+    """ Assume that arguments are contiguous and aligned to 16 bytes
     dx == None means that it was a post-norm architecture
     (x = drop(x0) + residual was not returned in the fwd).
     """
     hidden_size = gamma0.numel()
     xmat = x.view((-1, hidden_size))
     dz0mat = dz0.view(xmat.shape)
     dz1mat = dz1.view(xmat.shape) if dz1 is not None else None
@@ -139,20 +147,20 @@
     return dx0mat, dx1mat, dresidualmat, dgamma0, dbeta0, dgamma1, dbeta1
 
 
 class DropoutAddLayerNormFn(torch.autograd.Function):
     @staticmethod
     def forward(ctx, x0, residual, gamma, beta, rowscale, colscale, dropout_p, epsilon,
                 residual_in_fp32=False, prenorm=False, is_rms_norm=False, return_dmask=False):
-        x0 = x0.contiguous()
-        residual = residual.contiguous() if residual is not None else None
-        gamma = gamma.contiguous()
-        beta = beta.contiguous() if beta is not None else None
-        rowscale = rowscale.contiguous() if rowscale is not None else None
-        colscale = colscale.contiguous() if colscale is not None else None
+        x0 = maybe_align(x0.contiguous(), 16)
+        residual = maybe_align(residual.contiguous(), 16) if residual is not None else None
+        gamma = maybe_align(gamma.contiguous(), 16)
+        beta = maybe_align(beta.contiguous(), 16) if beta is not None else None
+        rowscale = maybe_align(rowscale.contiguous(), 16) if rowscale is not None else None
+        colscale = maybe_align(colscale.contiguous(), 16) if colscale is not None else None
         zmat, xmat, dmask, mu, rsigma = _dropout_add_layer_norm_forward(
             x0, residual, gamma, beta, rowscale, colscale, dropout_p, epsilon,
             residual_in_fp32, is_rms_norm
         )
         # Only need to save x0 if we need to compute gradient wrt colscale
         x0_saved = x0 if colscale is not None else None
         ctx.save_for_backward(xmat.view(x0.shape), x0_saved, dmask, gamma, mu, rsigma, rowscale, colscale)
@@ -170,16 +178,16 @@
             ctx.mark_non_differentiable(dmask)
             return ((zmat.view(x0.shape), dmask) if not prenorm
                     else (zmat.view(x0.shape), xmat.view(x0.shape), dmask))
 
     @staticmethod
     def backward(ctx, dz, *args):
         # assert dz.is_contiguous()
-        dz = dz.contiguous()  # this happens!
-        dx = args[0].contiguous() if ctx.prenorm else None
+        dz = maybe_align(dz.contiguous(), 16)  # this happens!
+        dx = maybe_align(args[0].contiguous(), 16) if ctx.prenorm else None
         x, x0, dmask, gamma, mu, rsigma, rowscale, colscale = ctx.saved_tensors
         # x0 is None if colscale is None
         dropout_p = ctx.dropout_p
         has_residual = ctx.has_residual
         dx0mat, dresidualmat, dgamma, dbeta, *rest = _dropout_add_layer_norm_backward(
             dz, dx, x, x0, dmask, mu, rsigma, gamma, rowscale, colscale, dropout_p, has_residual,
             ctx.is_rms_norm
@@ -192,19 +200,19 @@
 
 
 class DropoutAddLayerNormSubsetFn(torch.autograd.Function):
     @staticmethod
     def forward(ctx, x0, residual, gamma, beta, colscale, x0_subset, out_subset, dropout_p, epsilon,
                 rowscale_const, out_numrows, residual_in_fp32=False,
                 prenorm=False, is_rms_norm=False, return_dmask=False):
-        x0 = x0.contiguous()
-        residual = residual.contiguous() if residual is not None else None
-        gamma = gamma.contiguous()
-        beta = beta.contiguous() if beta is not None else None
-        colscale = colscale.contiguous() if colscale is not None else None
+        x0 = maybe_align(x0.contiguous(), 16)
+        residual = maybe_align(residual.contiguous(), 16) if residual is not None else None
+        gamma = maybe_align(gamma.contiguous(), 16)
+        beta = maybe_align(beta.contiguous(), 16) if beta is not None else None
+        colscale = maybe_align(colscale.contiguous(), 16) if colscale is not None else None
         zmat, xmat, dmask, mu, rsigma = _dropout_add_layer_norm_subset_forward(
             x0, residual, gamma, beta, colscale, x0_subset, out_subset, dropout_p, epsilon,
             rowscale_const, out_numrows, residual_in_fp32, is_rms_norm
         )
         # Only need to save x0 if we need to compute gradient wrt colscale
         x0_saved = x0 if colscale is not None else None
         x_shape = (-1, *x0.shape[1:])
@@ -227,16 +235,16 @@
                      else torch.ones(x0.shape, dtype=torch.uint8, device=x0.device))
             ctx.mark_non_differentiable(dmask)
             return ((z, dmask) if not prenorm else (z, xmat.view(x_shape), dmask))
 
     @staticmethod
     def backward(ctx, dz, *args):
         # assert dz.is_contiguous()
-        dz = dz.contiguous()  # this happens!
-        dx = args[0].contiguous() if ctx.prenorm else None
+        dz = maybe_align(dz.contiguous(), 16)  # this happens!
+        dx = maybe_align(args[0].contiguous(), 16) if ctx.prenorm else None
         x, x0, dmask, gamma, mu, rsigma, colscale, x0_subset, out_subset = ctx.saved_tensors
         # x0 is None if colscale is None
         dropout_p = ctx.dropout_p
         has_residual = ctx.has_residual
         dx0mat, dresidualmat, dgamma, dbeta, *rest = _dropout_add_layer_norm_subset_backward(
             dz, dx, x, x0, dmask, mu, rsigma, gamma, colscale, x0_subset, out_subset, dropout_p,
             ctx.rowscale_const, ctx.x0_numrows, has_residual, ctx.is_rms_norm
@@ -248,21 +256,21 @@
                 None, None, None, None, None, None, None, None)
 
 
 class DropoutAddLayerNormParallelResidualFn(torch.autograd.Function):
     @staticmethod
     def forward(ctx, x0, x1, residual, gamma0, beta0, gamma1, beta1, dropout_p, epsilon,
                 residual_in_fp32=False, prenorm=False, is_rms_norm=False, return_dmask=False):
-        x0 = x0.contiguous()
-        x1 = x1.contiguous() if x1 is not None else None
-        residual = residual.contiguous() if residual is not None else None
-        gamma0 = gamma0.contiguous()
-        beta0 = beta0.contiguous() if beta0 is not None else None
-        gamma1 = gamma1.contiguous() if gamma1 is not None else None
-        beta1 = beta1.contiguous() if beta1 is not None else None
+        x0 = maybe_align(x0.contiguous(), 16)
+        x1 = maybe_align(x1.contiguous(), 16) if x1 is not None else None
+        residual = maybe_align(residual.contiguous(), 16) if residual is not None else None
+        gamma0 = maybe_align(gamma0.contiguous(), 16)
+        beta0 = maybe_align(beta0.contiguous(), 16) if beta0 is not None else None
+        gamma1 = maybe_align(gamma1.contiguous(), 16) if gamma1 is not None else None
+        beta1 = maybe_align(beta1.contiguous(), 16) if beta1 is not None else None
         z0mat, z1mat, xmat, dmask0, dmask1, mu, rsigma = _dropout_add_layer_norm_parallel_residual_forward(
             x0, x1, residual, gamma0, beta0, gamma1, beta1, dropout_p, epsilon,
             residual_in_fp32, is_rms_norm
         )
         ctx.save_for_backward(xmat.view(x0.shape), dmask0, dmask1, gamma0, gamma1, mu, rsigma)
         ctx.prenorm = prenorm
         ctx.dropout_p = dropout_p
@@ -280,17 +288,17 @@
                       else torch.ones(x0.shape, dtype=torch.uint8, device=x0.device))
             ctx.mark_non_differentiable(dmask0)
             ctx.mark_non_differentiable(dmask1)
             return (*z, dmask0, dmask1) if not prenorm else (*z, xmat.view(x0.shape), dmask0, dmask1)
 
     @staticmethod
     def backward(ctx, dz0, dz1, *args):
-        dz0 = dz0.contiguous()  # this happens!
-        dz1 = dz1.contiguous() if dz1 is not None else None
-        dx = args[0].contiguous() if ctx.prenorm else None
+        dz0 = maybe_align(dz0.contiguous(), 16)  # this happens!
+        dz1 = maybe_align(dz1.contiguous(), 16) if dz1 is not None else None
+        dx = maybe_align(args[0].contiguous(), 16) if ctx.prenorm else None
         x, dmask0, dmask1, gamma0, gamma1, mu, rsigma = ctx.saved_tensors
         dropout_p = ctx.dropout_p
         has_x1 = ctx.has_x1
         has_residual = ctx.has_residual
         dx0mat, dx1mat, dresidualmat, dgamma0, dbeta0, dgamma1, dbeta1 = _dropout_add_layer_norm_parallel_residual_backward(
             dz0, dz1, dx, x, dmask0, dmask1, mu, rsigma, gamma0, gamma1, dropout_p, has_x1,
             has_residual, ctx.is_rms_norm
```

### Comparing `flash_attn-1.0.8/flash_attn/ops/rms_norm.py` & `flash_attn-1.0.9/flash_attn/ops/rms_norm.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn/utils/benchmark.py` & `flash_attn-1.0.9/flash_attn/utils/benchmark.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn/utils/distributed.py` & `flash_attn-1.0.9/flash_attn/utils/distributed.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn/utils/generation.py` & `flash_attn-1.0.9/flash_attn/utils/generation.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn/utils/pretrained.py` & `flash_attn-1.0.9/flash_attn/utils/pretrained.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.8/flash_attn.egg-info/PKG-INFO` & `flash_attn-1.0.9/flash_attn.egg-info/PKG-INFO`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: flash-attn
-Version: 1.0.8
+Version: 1.0.9
 Summary: Flash Attention: Fast and Memory-Efficient Exact Attention
 Home-page: https://github.com/HazyResearch/flash-attention
 Author: Tri Dao
 Author-email: trid@stanford.edu
 License: UNKNOWN
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
```

### Comparing `flash_attn-1.0.8/flash_attn.egg-info/SOURCES.txt` & `flash_attn-1.0.9/flash_attn.egg-info/SOURCES.txt`

 * *Files 2% similar despite different names*

```diff
@@ -1,14 +1,12 @@
 AUTHORS
 LICENSE
 MANIFEST.in
 README.md
-pyproject.toml
 setup.py
-csrc/flash_attn/flash_api.cpp
 csrc/flash_attn/fmha_api.cpp
 csrc/flash_attn/cutlass/cmake/nop.cu
 csrc/flash_attn/cutlass/examples/00_basic_gemm/basic_gemm.cu
 csrc/flash_attn/cutlass/examples/01_cutlass_utilities/cutlass_utilities.cu
 csrc/flash_attn/cutlass/examples/02_dump_reg_shmem/dump_reg_shmem.cu
 csrc/flash_attn/cutlass/examples/03_visualize_layout/options.h
 csrc/flash_attn/cutlass/examples/03_visualize_layout/register_layout.cu
@@ -81,15 +79,14 @@
 csrc/flash_attn/cutlass/examples/23_ampere_gemm_operand_reduction_fusion/ampere_gemm_operand_reduction_fusion.cu
 csrc/flash_attn/cutlass/examples/24_gemm_grouped/gemm_grouped.cu
 csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_3d_fprop_mainloop_fusion.cu
 csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_fprop_mainloop_fusion.cu
 csrc/flash_attn/cutlass/examples/26_ampere_wgrad_mainloop_fusion/ampere_wgrad_mainloop_fusion.cu
 csrc/flash_attn/cutlass/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu
 csrc/flash_attn/cutlass/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/ampere_3xtf32_fast_accurate_tensorop_fprop.cu
-csrc/flash_attn/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_3xtf32_complex_gemm.cu
 csrc/flash_attn/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm.cu
 csrc/flash_attn/cutlass/examples/30_wgrad_split_k/30_wgrad_split_k.cu
 csrc/flash_attn/cutlass/examples/31_basic_syrk/basic_syrk.cu
 csrc/flash_attn/cutlass/examples/32_basic_trmm/basic_trmm.cu
 csrc/flash_attn/cutlass/examples/33_ampere_3xtf32_tensorop_symm/ampere_3xtf32_tensorop_symm.cu
 csrc/flash_attn/cutlass/examples/34_transposed_conv2d/34_transposed_conv2d.cu
 csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_softmax.cu
@@ -97,83 +94,59 @@
 csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_with_softmax.h
 csrc/flash_attn/cutlass/examples/36_gather_scatter_fusion/gather_scatter_fusion.cu
 csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu
 csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h
 csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h
 csrc/flash_attn/cutlass/examples/38_syr2k_grouped/syr2k_grouped.cu
 csrc/flash_attn/cutlass/examples/39_gemm_permute/gemm_permute.cu
-csrc/flash_attn/cutlass/examples/39_gemm_permute/layouts.h
-csrc/flash_attn/cutlass/examples/39_gemm_permute/permute_info.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/attention_scaling_coefs_updater.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/debug_utils.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/default_fmha_grouped.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_pipelined.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_rescale_output.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_thread_apply_logsumexp.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/find_default_mma.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped_problem_visitor.h
-csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multi_head_attention_backward.cu
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm_kernel_utils.h
-csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/kernel_backward.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h
-csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_pipelined.h
-csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_rescale_output.h
-csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_thread_apply_logsumexp.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_base.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_multistage.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_pipelined.h
-csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/find_default_mma.h
-csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/mma_accum_lambda_iterator.h
-csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/mma_from_smem.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/epilogue_predicated_tile_iterator.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/make_residual_last.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_access_iterator_residual_last.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_iterator_residual_last.h
-csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/transpose_warp_iterator.h
-csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/warp_iterator_from_smem.h
-csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/transform/tile_smem_loader.h
-csrc/flash_attn/cutlass/examples/41_multi_head_attention/fused_multihead_attention.cu
-csrc/flash_attn/cutlass/examples/41_multi_head_attention/gemm_attention.h
-csrc/flash_attn/cutlass/examples/41_multi_head_attention/gemm_grouped_with_softmax_visitor.h
 csrc/flash_attn/cutlass/examples/42_ampere_tensorop_group_conv/ampere_tensorop_group_conv.cu
 csrc/flash_attn/cutlass/examples/43_ell_block_sparse_gemm/ell_block_sparse_gemm.cu
 csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/leaky_bias.h
 csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/utils.h
 csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_bias_act_epilogue_tensor_op.h
 csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_thread_map_tensor_op_for_fused_bias.h
 csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/fused_bias_act_epilogue.h
 csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/output_tile_thread_map_for_fused_bias.h
 csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/warp/fused_bias_act_fragment_iterator_tensor_op.h
 csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/warp/mma_tensor_op_fragment_iterator_without_output_op.h
 csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm.cu
-csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm_common.h
 csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm_run.h
 csrc/flash_attn/cutlass/examples/45_dual_gemm/test_run.h
 csrc/flash_attn/cutlass/examples/45_dual_gemm/device/dual_gemm.h
 csrc/flash_attn/cutlass/examples/45_dual_gemm/kernel/dual_gemm.h
 csrc/flash_attn/cutlass/examples/45_dual_gemm/thread/left_silu_and_mul.h
 csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_epilogue.h
 csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_base.h
 csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_multistage.h
 csrc/flash_attn/cutlass/examples/46_depthwise_simt_conv2dfprop/depthwise_simt_conv2dfprop.cu
 csrc/flash_attn/cutlass/examples/47_ampere_gemm_universal_streamk/ampere_gemm_universal_streamk.cu
-csrc/flash_attn/cutlass/examples/48_hopper_warp_specialized_gemm/48_hopper_warp_specialized_gemm.cu
-csrc/flash_attn/cutlass/examples/49_hopper_gemm_schedules_with_collective_builder/49_hopper_gemm_schedules_with_collective_builder.cu
-csrc/flash_attn/cutlass/examples/49_hopper_gemm_with_collective_builder/49_collective_builder.cu
-csrc/flash_attn/cutlass/examples/50_hopper_gemm_with_epilogue_swizzle/50_hopper_gemm_with_epilogue_swizzle.cu
-csrc/flash_attn/cutlass/examples/51_hopper_gett/51_hopper_gett.cu
-csrc/flash_attn/cutlass/examples/51_hopper_gett/gett_kernel.cuh
 csrc/flash_attn/cutlass/examples/60_cutlass_import/main.cpp
 csrc/flash_attn/cutlass/examples/common/helper.h
-csrc/flash_attn/cutlass/examples/cute/tutorial/sgemm_nt_1.cu
 csrc/flash_attn/cutlass/include/cutlass/aligned_buffer.h
 csrc/flash_attn/cutlass/include/cutlass/array.h
 csrc/flash_attn/cutlass/include/cutlass/array_planar_complex.h
 csrc/flash_attn/cutlass/include/cutlass/array_subbyte.h
 csrc/flash_attn/cutlass/include/cutlass/barrier.h
 csrc/flash_attn/cutlass/include/cutlass/bfloat16.h
 csrc/flash_attn/cutlass/include/cutlass/blas3.h
@@ -209,29 +182,27 @@
 csrc/flash_attn/cutlass/include/cutlass/tensor_view.h
 csrc/flash_attn/cutlass/include/cutlass/tensor_view_planar_complex.h
 csrc/flash_attn/cutlass/include/cutlass/tfloat32.h
 csrc/flash_attn/cutlass/include/cutlass/trace.h
 csrc/flash_attn/cutlass/include/cutlass/uint128.h
 csrc/flash_attn/cutlass/include/cutlass/wmma_array.h
 csrc/flash_attn/cutlass/include/cutlass/arch/arch.h
-csrc/flash_attn/cutlass/include/cutlass/arch/barrier.h
 csrc/flash_attn/cutlass/include/cutlass/arch/cache_operation.h
 csrc/flash_attn/cutlass/include/cutlass/arch/memory.h
 csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm75.h
 csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm80.h
 csrc/flash_attn/cutlass/include/cutlass/arch/mma.h
 csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm50.h
 csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm60.h
 csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm61.h
 csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm70.h
 csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm75.h
 csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm80.h
 csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm90.h
 csrc/flash_attn/cutlass/include/cutlass/arch/mma_sparse_sm80.h
-csrc/flash_attn/cutlass/include/cutlass/arch/reg_reconfig.h
 csrc/flash_attn/cutlass/include/cutlass/arch/simd.h
 csrc/flash_attn/cutlass/include/cutlass/arch/simd_sm60.h
 csrc/flash_attn/cutlass/include/cutlass/arch/simd_sm61.h
 csrc/flash_attn/cutlass/include/cutlass/arch/wmma.h
 csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm70.h
 csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm72.h
 csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm75.h
@@ -597,52 +568,15 @@
 csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator.h
 csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear.h
 csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear_2dthreadtile.h
 csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op.h
 csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op_sm70.h
 csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/vector_iterator.h
 csrc/flash_attn/cutlass/include/cutlass/transform/warp/vector_fragment_iterator.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/compiler.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/cutlass_bindings.cpp
-csrc/flash_attn/cutlass/python/cutlass/cpp/library.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/include/arch.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/include/swizzling.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/include/tensor_coord.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/include/tensor_ref_view.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/include/types.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/include/conv/conv_problem_size.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/include/conv/convolution.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/include/conv/host.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_generic.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_with_layernorm.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/binary_ops.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/unary_ops.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_accumulator.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_binary.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_broadcast.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_reduction.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_linear_combination.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_broadcast.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_reduction.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_input.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_output.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/include/epilogue/epilogue_visitor_op/visitor_op_unary.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/include/gemm/gemm.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/include/gemm/gemm_universal_with_visitor.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/include/gemm/host.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/include/layout/layout.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/include/layout/matrix.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/include/layout/tensor.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/test/conv/conv_problems.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/test/conv/convolution.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/test/conv/host.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/test/gemm/gemm.h
-csrc/flash_attn/cutlass/python/cutlass/cpp/test/gemm/host.h
 csrc/flash_attn/cutlass/test/unit/test_unit.cpp
-csrc/flash_attn/cutlass/test/unit/cluster_launch/cluster_launch.cu
 csrc/flash_attn/cutlass/test/unit/common/cutlass_unit_test.h
 csrc/flash_attn/cutlass/test/unit/common/filter_architecture.cpp
 csrc/flash_attn/cutlass/test/unit/conv/device/cache_testbed_output.h
 csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu
 csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu
 csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu
 csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu
@@ -700,15 +634,14 @@
 csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu
 csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
 csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
 csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_direct_conv_testbed.h
 csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
 csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_fixed_stride_dilation_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
 csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
-csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
 csrc/flash_attn/cutlass/test/unit/conv/device/group_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
 csrc/flash_attn/cutlass/test/unit/core/array.cu
 csrc/flash_attn/cutlass/test/unit/core/bfloat16.cu
 csrc/flash_attn/cutlass/test/unit/core/complex.cu
 csrc/flash_attn/cutlass/test/unit/core/float8.cu
 csrc/flash_attn/cutlass/test/unit/core/functional.cu
 csrc/flash_attn/cutlass/test/unit/core/half.cu
@@ -717,37 +650,14 @@
 csrc/flash_attn/cutlass/test/unit/core/numeric_conversion.cu
 csrc/flash_attn/cutlass/test/unit/core/predicate_vector.cu
 csrc/flash_attn/cutlass/test/unit/core/quaternion.cu
 csrc/flash_attn/cutlass/test/unit/core/tensor_ref.cu
 csrc/flash_attn/cutlass/test/unit/core/tensor_view.cu
 csrc/flash_attn/cutlass/test/unit/core/test_unit_core.cpp
 csrc/flash_attn/cutlass/test/unit/core/tfloat32.cu
-csrc/flash_attn/cutlass/test/unit/cute/ampere/cp_async.cu
-csrc/flash_attn/cutlass/test/unit/cute/ampere/ldsm.cu
-csrc/flash_attn/cutlass/test/unit/cute/core/array_subbyte.cpp
-csrc/flash_attn/cutlass/test/unit/cute/core/bitfield.cpp
-csrc/flash_attn/cutlass/test/unit/cute/core/coalesce.cpp
-csrc/flash_attn/cutlass/test/unit/cute/core/compact_xmajor.cpp
-csrc/flash_attn/cutlass/test/unit/cute/core/compare.cpp
-csrc/flash_attn/cutlass/test/unit/cute/core/complement.cpp
-csrc/flash_attn/cutlass/test/unit/cute/core/composition.cpp
-csrc/flash_attn/cutlass/test/unit/cute/core/inverse_left.cpp
-csrc/flash_attn/cutlass/test/unit/cute/core/inverse_right.cpp
-csrc/flash_attn/cutlass/test/unit/cute/core/logical_divide.cpp
-csrc/flash_attn/cutlass/test/unit/cute/core/logical_product.cpp
-csrc/flash_attn/cutlass/test/unit/cute/core/mixedbits.cpp
-csrc/flash_attn/cutlass/test/unit/cute/core/transform.cpp
-csrc/flash_attn/cutlass/test/unit/cute/core/tuple.cpp
-csrc/flash_attn/cutlass/test/unit/cute/hopper/bulk_load.cu
-csrc/flash_attn/cutlass/test/unit/cute/hopper/bulk_store.cu
-csrc/flash_attn/cutlass/test/unit/cute/hopper/stsm.cu
-csrc/flash_attn/cutlass/test/unit/cute/hopper/tma_load.cu
-csrc/flash_attn/cutlass/test/unit/cute/hopper/tma_store.cu
-csrc/flash_attn/cutlass/test/unit/cute/layout/layout_operator.cu
-csrc/flash_attn/cutlass/test/unit/cute/msvc_compilation/tuple.cpp
 csrc/flash_attn/cutlass/test/unit/epilogue/thread/activation.cu
 csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination.cu
 csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination_planar_complex.cu
 csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_planar_complex.cu
 csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt.cu
 csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm60.cu
 csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm61.cu
@@ -873,15 +783,14 @@
 csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_wmma_tensor_op_s32_sm75.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm75.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm80.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm75.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm80.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm75.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm80.cu
-csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_f16t_tensor_op_s32_sm80.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm75.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm80.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_wmma_tensor_op_s32_sm72.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm75.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm80.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sparse_sm80.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_wmma_tensor_op_s32_sm72.cu
@@ -963,43 +872,14 @@
 csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm50.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm80.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tt_sm50.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_nn_sm50.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_nt_sm50.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_tn_sm50.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_tt_sm50.cu
-csrc/flash_attn/cutlass/test/unit/gemm/device/sm50_gemm_f32_f32_f32_simt.cu
-csrc/flash_attn/cutlass/test/unit/gemm/device/sm50_gemm_f64_f64_f64_simt.cu
-csrc/flash_attn/cutlass/test/unit/gemm/device/sm61_gemm_s8_s8_s32_simt.cu
-csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f16_f16_f32_tensor_op_f32.cu
-csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f32_f32_f32_simt.cu
-csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f64_f64_f64_simt.cu
-csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f64_f64_f64_tensor_op_f64.cu
-csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_s8_s8_s32_tensor_op.cu
-csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_tf32_tf32_f32_tensor_op_f32.cu
-csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_bf16_bf16_bf16_alignx_tensor_op_f32.cu
-csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_bf16_bf16_bf16_tensor_op_f32.cu
-csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_alignx_tensor_op.cu
-csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op.cu
-csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_unspecialized.cu
-csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized.cu
-csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_cooperative.cu
-csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_cooperative_bias_elementwise.cu
-csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_persistent.cu
-csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_pingpong.cu
-csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_pingpong_bias_elementwise.cu
-csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_tensor_broadcast.cu
-csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f32_f32_f32_tensor_op_f32.cu
-csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f32_f32_f32_tensor_op_f32_tensor_broadcast.cu
-csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_s8_s8_s8_alignx_tensor_op_s32.cu
-csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_s8_s8_s8_tensor_op_s32.cu
-csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_s8_s8_s8_tensor_op_s32_tensor_broadcast.cu
-csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_alignx_tensor_op_f32.cu
-csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_tensor_op_f32.cu
-csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_tensor_op_f32_gmma_rs_cluster_warpspecialized.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_ls_sm80.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_rs_sm80.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_ls_sm80.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_rs_sm80.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64_cf64_cf64_tensor_op_f64_sm90.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu
 csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_sm80.cu
@@ -1144,40 +1024,32 @@
 csrc/flash_attn/cutlass/test/unit/layout/tensor_nhwc.cu
 csrc/flash_attn/cutlass/test/unit/nvrtc/cutlass/nvrtc/environment.h
 csrc/flash_attn/cutlass/test/unit/nvrtc/kernel/thread/testbed_kernel.h
 csrc/flash_attn/cutlass/test/unit/nvrtc/stdlib/assert.h
 csrc/flash_attn/cutlass/test/unit/nvrtc/stdlib/stdint.h
 csrc/flash_attn/cutlass/test/unit/nvrtc/thread/gemm_nvrtc.cu
 csrc/flash_attn/cutlass/test/unit/nvrtc/thread/testbed.h
-csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_async.cu
-csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_tma_async.cu
-csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_tma_async_warp_specialized.cu
-csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_tma_async_warp_specialized_persistent.cu
-csrc/flash_attn/cutlass/test/unit/pipeline/sequence_barrier.cu
-csrc/flash_attn/cutlass/test/unit/pipeline/testbed.h
 csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_contiguous.cu
 csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_strided.cu
 csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk.cu
 csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk_testbed.h
 csrc/flash_attn/cutlass/test/unit/reduction/thread/reduction_thread.cu
 csrc/flash_attn/cutlass/test/unit/reduction/thread/testbed.h
-csrc/flash_attn/cutlass/test/unit/substrate/dependent_false.cpp
 csrc/flash_attn/cutlass/test/unit/transform/threadblock/predicated_tile_iterator.cu
 csrc/flash_attn/cutlass/test/unit/transform/threadblock/regular_tile_iterator_tensor_op.cu
 csrc/flash_attn/cutlass/test/unit/util/cutlass_test_levels.cu
 csrc/flash_attn/cutlass/test/unit/util/tensor_reduce.cu
 csrc/flash_attn/cutlass/tools/library/include/cutlass/library/arch_mappings.h
 csrc/flash_attn/cutlass/tools/library/include/cutlass/library/handle.h
 csrc/flash_attn/cutlass/tools/library/include/cutlass/library/library.h
 csrc/flash_attn/cutlass/tools/library/include/cutlass/library/manifest.h
 csrc/flash_attn/cutlass/tools/library/include/cutlass/library/operation_table.h
 csrc/flash_attn/cutlass/tools/library/include/cutlass/library/singleton.h
 csrc/flash_attn/cutlass/tools/library/include/cutlass/library/util.h
 csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/compiler.h
-csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/cute.cpp
 csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/cutlass.cpp
 csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/library.h
 csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/arch.h
 csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/swizzling.h
 csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_coord.h
 csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_ref_view.h
 csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/types.h
@@ -1325,64 +1197,14 @@
 csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_elementwise.h
 csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_fill.h
 csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_foreach.h
 csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_norm.h
 csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_reduce.h
 csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/trmm.h
 csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/trmm_complex.h
-csrc/flash_attn/src/block_info.h
-csrc/flash_attn/src/flash.h
-csrc/flash_attn/src/flash_bwd_hdim128.cu
-csrc/flash_attn/src/flash_bwd_hdim128_bf16_sm80.cu
-csrc/flash_attn/src/flash_bwd_hdim128_fp16_sm80.cu
-csrc/flash_attn/src/flash_bwd_hdim128_sm80_fp16.cu
-csrc/flash_attn/src/flash_bwd_hdim32.cu
-csrc/flash_attn/src/flash_bwd_hdim32_bf16_sm80.cu
-csrc/flash_attn/src/flash_bwd_hdim32_fp16_sm80.cu
-csrc/flash_attn/src/flash_bwd_hdim32_sm80_fp16.cu
-csrc/flash_attn/src/flash_bwd_hdim64.cu
-csrc/flash_attn/src/flash_bwd_hdim64_bf16_sm80.cu
-csrc/flash_attn/src/flash_bwd_hdim64_fp16_sm80.cu
-csrc/flash_attn/src/flash_bwd_hdim64_sm80_fp16.cu
-csrc/flash_attn/src/flash_bwd_hdim96.cu
-csrc/flash_attn/src/flash_bwd_hdim96_bf16_sm80.cu
-csrc/flash_attn/src/flash_bwd_hdim96_fp16_sm80.cu
-csrc/flash_attn/src/flash_bwd_hdim96_sm80_fp16.cu
-csrc/flash_attn/src/flash_bwd_kernel.h
-csrc/flash_attn/src/flash_bwd_kernel_bak.h
-csrc/flash_attn/src/flash_bwd_kernel_new.h
-csrc/flash_attn/src/flash_bwd_kernel_reverse.h
-csrc/flash_attn/src/flash_bwd_launch_template.h
-csrc/flash_attn/src/flash_fwd_hdim128.cu
-csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu
-csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu
-csrc/flash_attn/src/flash_fwd_hdim128_sm80_fp16.cu
-csrc/flash_attn/src/flash_fwd_hdim160.cu
-csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu
-csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu
-csrc/flash_attn/src/flash_fwd_hdim160_sm80_fp16.cu
-csrc/flash_attn/src/flash_fwd_hdim192.cu
-csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu
-csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu
-csrc/flash_attn/src/flash_fwd_hdim192_sm80_fp16.cu
-csrc/flash_attn/src/flash_fwd_hdim32.cu
-csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu
-csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu
-csrc/flash_attn/src/flash_fwd_hdim32_sm80_fp16.cu
-csrc/flash_attn/src/flash_fwd_hdim64.cu
-csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu
-csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu
-csrc/flash_attn/src/flash_fwd_hdim64_sm80_fp16.cu
-csrc/flash_attn/src/flash_fwd_hdim96.cu
-csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu
-csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu
-csrc/flash_attn/src/flash_fwd_hdim96_sm80_fp16.cu
-csrc/flash_attn/src/flash_fwd_kernel.h
-csrc/flash_attn/src/flash_fwd_kernel_old.h
-csrc/flash_attn/src/flash_fwd_launch_template.h
 csrc/flash_attn/src/fmha.h
 csrc/flash_attn/src/fmha_block_dgrad_fp16_kernel_loop.sm80.cu
 csrc/flash_attn/src/fmha_block_dgrad_kernel_1xN_loop.h
 csrc/flash_attn/src/fmha_block_fprop_fp16_kernel.sm80.cu
 csrc/flash_attn/src/fmha_block_fprop_kernel_1xN.h
 csrc/flash_attn/src/fmha_blockmask.h
 csrc/flash_attn/src/fmha_bwd_hdim128.cu
@@ -1393,20 +1215,16 @@
 csrc/flash_attn/src/fmha_fprop_kernel_1xN.h
 csrc/flash_attn/src/fmha_fwd_hdim128.cu
 csrc/flash_attn/src/fmha_fwd_hdim32.cu
 csrc/flash_attn/src/fmha_fwd_hdim64.cu
 csrc/flash_attn/src/fmha_fwd_launch_template.h
 csrc/flash_attn/src/fmha_kernel.h
 csrc/flash_attn/src/fmha_utils.h
-csrc/flash_attn/src/kernel_traits.h
-csrc/flash_attn/src/mask.h
 csrc/flash_attn/src/philox.cuh
-csrc/flash_attn/src/softmax.h
 csrc/flash_attn/src/static_switch.h
-csrc/flash_attn/src/utils.h
 csrc/flash_attn/src/fmha/gemm.h
 csrc/flash_attn/src/fmha/gmem_tile.h
 csrc/flash_attn/src/fmha/kernel_traits.h
 csrc/flash_attn/src/fmha/mask.h
 csrc/flash_attn/src/fmha/smem_tile.h
 csrc/flash_attn/src/fmha/softmax.h
 csrc/flash_attn/src/fmha/utils.h
@@ -1476,32 +1294,28 @@
 csrc/layer_norm/ln_parallel_bwd_512.cu
 csrc/layer_norm/ln_parallel_bwd_5120.cu
 csrc/layer_norm/ln_parallel_bwd_6144.cu
 csrc/layer_norm/ln_parallel_bwd_7168.cu
 csrc/layer_norm/ln_parallel_bwd_768.cu
 csrc/layer_norm/ln_parallel_bwd_8192.cu
 csrc/layer_norm/ln_parallel_fwd_1024.cu
-csrc/layer_norm/ln_parallel_fwd_128.cu
 csrc/layer_norm/ln_parallel_fwd_1280.cu
 csrc/layer_norm/ln_parallel_fwd_1536.cu
 csrc/layer_norm/ln_parallel_fwd_2048.cu
 csrc/layer_norm/ln_parallel_fwd_256.cu
 csrc/layer_norm/ln_parallel_fwd_2560.cu
 csrc/layer_norm/ln_parallel_fwd_3072.cu
 csrc/layer_norm/ln_parallel_fwd_4096.cu
 csrc/layer_norm/ln_parallel_fwd_512.cu
 csrc/layer_norm/ln_parallel_fwd_5120.cu
 csrc/layer_norm/ln_parallel_fwd_6144.cu
 csrc/layer_norm/ln_parallel_fwd_7168.cu
 csrc/layer_norm/ln_parallel_fwd_768.cu
 csrc/layer_norm/ln_parallel_fwd_8192.cu
-csrc/layer_norm/ln_parallel_res_fwd_kernel.cuh
-csrc/layer_norm/ln_parallel_residual_bwd_512.cu
 csrc/layer_norm/ln_parallel_residual_bwd_kernels.cuh
-csrc/layer_norm/ln_parallel_residual_fwd_kernel.cuh
 csrc/layer_norm/ln_parallel_residual_fwd_kernels.cuh
 csrc/layer_norm/ln_utils.cuh
 csrc/layer_norm/static_switch.h
 csrc/rotary/rotary.cpp
 csrc/rotary/rotary_cuda.cu
 csrc/xentropy/interface.cpp
 csrc/xentropy/xentropy_kernel.cu
@@ -1511,51 +1325,43 @@
 flash_attn/flash_attention.py
 flash_attn/flash_attn_interface.py
 flash_attn/flash_attn_triton.py
 flash_attn/flash_attn_triton_og.py
 flash_attn/flash_attn_triton_single_query.py
 flash_attn/flash_attn_triton_tmp.py
 flash_attn/flash_attn_triton_tmp_og.py
-flash_attn/flash_attn_triton_varlen.py
 flash_attn/flash_blocksparse_attention.py
 flash_attn/flash_blocksparse_attn_interface.py
 flash_attn/fused_softmax.py
-flash_attn/rotary.py
 flash_attn.egg-info/PKG-INFO
 flash_attn.egg-info/SOURCES.txt
 flash_attn.egg-info/dependency_links.txt
 flash_attn.egg-info/requires.txt
 flash_attn.egg-info/top_level.txt
 flash_attn/layers/__init__.py
 flash_attn/layers/patch_embed.py
 flash_attn/layers/rotary.py
 flash_attn/losses/__init__.py
 flash_attn/losses/cross_entropy.py
-flash_attn/losses/cross_entropy_apex.py
-flash_attn/losses/cross_entropy_parallel.py
 flash_attn/models/__init__.py
 flash_attn/models/bert.py
 flash_attn/models/gpt.py
-flash_attn/models/gpt_j.py
 flash_attn/models/gpt_neox.py
 flash_attn/models/gptj.py
 flash_attn/models/llama.py
 flash_attn/models/opt.py
 flash_attn/models/vit.py
 flash_attn/modules/__init__.py
 flash_attn/modules/block.py
 flash_attn/modules/embedding.py
 flash_attn/modules/mha.py
 flash_attn/modules/mlp.py
 flash_attn/ops/__init__.py
 flash_attn/ops/activations.py
 flash_attn/ops/fused_dense.py
-flash_attn/ops/gelu_activation.py
 flash_attn/ops/layer_norm.py
 flash_attn/ops/rms_norm.py
-flash_attn/triton/__init__.py
-flash_attn/triton/fused_attention.py
 flash_attn/utils/__init__.py
 flash_attn/utils/benchmark.py
 flash_attn/utils/distributed.py
 flash_attn/utils/generation.py
 flash_attn/utils/pretrained.py
```

### Comparing `flash_attn-1.0.8/setup.py` & `flash_attn-1.0.9/setup.py`

 * *Files identical despite different names*

