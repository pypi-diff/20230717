# Comparing `tmp/flash_attn-1.0.9.tar.gz` & `tmp/flash_attn-2.0.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "flash_attn-1.0.9.tar", last modified: Mon Jul 17 10:19:08 2023, max compression
+gzip compressed data, was "flash_attn-2.0.0.tar", last modified: Mon Jul 17 12:48:05 2023, max compression
```

## Comparing `flash_attn-1.0.9.tar` & `flash_attn-2.0.0.tar`

### file list

```diff
@@ -1,1553 +1,1639 @@
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:08.030051 flash_attn-1.0.9/
--rw-r--r--   0 root         (0) root         (0)       56 2022-11-17 23:40:55.000000 flash_attn-1.0.9/AUTHORS
--rw-r--r--   0 root         (0) root         (0)     1558 2022-09-09 19:08:03.000000 flash_attn-1.0.9/LICENSE
--rw-r--r--   0 root         (0) root         (0)      251 2023-04-16 00:48:36.000000 flash_attn-1.0.9/MANIFEST.in
--rw-rw-r--   0 root         (0) root         (0)    10197 2023-07-17 10:19:08.024764 flash_attn-1.0.9/PKG-INFO
--rw-r--r--   0 root         (0) root         (0)     9689 2023-05-25 23:52:21.000000 flash_attn-1.0.9/README.md
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.760420 flash_attn-1.0.9/csrc/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.869229 flash_attn-1.0.9/csrc/flash_attn/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.560240 flash_attn-1.0.9/csrc/flash_attn/cutlass/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.890489 flash_attn-1.0.9/csrc/flash_attn/cutlass/cmake/
--rw-r--r--   0 root         (0) root         (0)     2023 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/cmake/nop.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.254948 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.912239 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/00_basic_gemm/
--rw-r--r--   0 root         (0) root         (0)    14698 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/00_basic_gemm/basic_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.932576 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/01_cutlass_utilities/
--rw-r--r--   0 root         (0) root         (0)    13255 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/01_cutlass_utilities/cutlass_utilities.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.954599 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/02_dump_reg_shmem/
--rw-r--r--   0 root         (0) root         (0)     7157 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/02_dump_reg_shmem/dump_reg_shmem.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.034545 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/03_visualize_layout/
--rw-r--r--   0 root         (0) root         (0)     4478 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/03_visualize_layout/options.h
--rw-r--r--   0 root         (0) root         (0)     7081 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/03_visualize_layout/register_layout.cu
--rw-r--r--   0 root         (0) root         (0)     2691 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/03_visualize_layout/register_layout.h
--rw-r--r--   0 root         (0) root         (0)     5819 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/03_visualize_layout/visualize_layout.cpp
--rw-r--r--   0 root         (0) root         (0)    11415 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/03_visualize_layout/visualize_layout.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.055167 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/04_tile_iterator/
--rw-r--r--   0 root         (0) root         (0)     8226 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/04_tile_iterator/tile_iterator.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.075244 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/05_batched_gemm/
--rw-r--r--   0 root         (0) root         (0)    15161 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/05_batched_gemm/batched_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.095573 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/06_splitK_gemm/
--rw-r--r--   0 root         (0) root         (0)    17570 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/06_splitK_gemm/splitk_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.115767 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/07_volta_tensorop_gemm/
--rw-r--r--   0 root         (0) root         (0)    18280 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/07_volta_tensorop_gemm/volta_tensorop_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.136182 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/08_turing_tensorop_gemm/
--rw-r--r--   0 root         (0) root         (0)    18226 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/08_turing_tensorop_gemm/turing_tensorop_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.156347 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/09_turing_tensorop_conv2dfprop/
--rw-r--r--   0 root         (0) root         (0)    28124 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/09_turing_tensorop_conv2dfprop/turing_tensorop_conv2dfprop.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.177762 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/10_planar_complex/
--rw-r--r--   0 root         (0) root         (0)    21947 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/10_planar_complex/planar_complex.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.197916 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/11_planar_complex_array/
--rw-r--r--   0 root         (0) root         (0)    23244 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/11_planar_complex_array/planar_complex_array.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.218608 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/12_gemm_bias_relu/
--rw-r--r--   0 root         (0) root         (0)    13151 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/12_gemm_bias_relu/gemm_bias_relu.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.546240 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/
--rw-r--r--   0 root         (0) root         (0)    26102 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_conv2d_run.h
--rw-r--r--   0 root         (0) root         (0)    22877 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_gemm_run.h
--rw-r--r--   0 root         (0) root         (0)    28268 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_conv2d_run.h
--rw-r--r--   0 root         (0) root         (0)    24493 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_gemm_run.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.581635 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/
--rw-r--r--   0 root         (0) root         (0)    15552 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/b2b_gemm.h
--rw-r--r--   0 root         (0) root         (0)    11520 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/b2b_implicit_gemm_convolution.h
--rw-r--r--   0 root         (0) root         (0)     8756 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_rf.cu
--rw-r--r--   0 root         (0) root         (0)     8759 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_shmem.cu
--rw-r--r--   0 root         (0) root         (0)     8712 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_rf.cu
--rw-r--r--   0 root         (0) root         (0)     8762 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_shmem.cu
--rw-r--r--   0 root         (0) root         (0)     8787 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_rf.cu
--rw-r--r--   0 root         (0) root         (0)     8793 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_shmem.cu
--rw-r--r--   0 root         (0) root         (0)     8711 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_rf.cu
--rw-r--r--   0 root         (0) root         (0)     8775 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_shmem.cu
--rw-r--r--   0 root         (0) root         (0)     7269 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_rf.cu
--rw-r--r--   0 root         (0) root         (0)     7338 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_shmem.cu
--rw-r--r--   0 root         (0) root         (0)     7294 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_rf.cu
--rw-r--r--   0 root         (0) root         (0)     7359 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_shmem.cu
--rw-r--r--   0 root         (0) root         (0)     7362 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_rf.cu
--rw-r--r--   0 root         (0) root         (0)     7430 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_shmem.cu
--rw-r--r--   0 root         (0) root         (0)     7627 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_rf.cu
--rw-r--r--   0 root         (0) root         (0)     7634 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_shmem.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.722921 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/
--rw-r--r--   0 root         (0) root         (0)    16152 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_gemm.h
--rw-r--r--   0 root         (0) root         (0)    18151 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_implicit_gemm_convolution.h
--rw-r--r--   0 root         (0) root         (0)     3973 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop.h
--rw-r--r--   0 root         (0) root         (0)    26762 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm75.h
--rw-r--r--   0 root         (0) root         (0)    26775 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm80.h
--rw-r--r--   0 root         (0) root         (0)    28422 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm75.h
--rw-r--r--   0 root         (0) root         (0)    28073 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm80.h
--rw-r--r--   0 root         (0) root         (0)    17111 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm.h
--rw-r--r--   0 root         (0) root         (0)    15658 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm_smem_accumulator.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.035203 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/reference/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.746531 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/reference/device/
--rw-r--r--   0 root         (0) root         (0)    10368 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/reference/device/tensor_scale_bias.h
--rw-r--r--   0 root         (0) root         (0)     3577 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/test_run.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.949405 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/
--rw-r--r--   0 root         (0) root         (0)    31616 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage.h
--rw-r--r--   0 root         (0) root         (0)    31443 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage_smem_accumulator.h
--rw-r--r--   0 root         (0) root         (0)    21010 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined.h
--rw-r--r--   0 root         (0) root         (0)    20493 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined_smem_accumulator.h
--rw-r--r--   0 root         (0) root         (0)     7983 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base.h
--rw-r--r--   0 root         (0) root         (0)     6047 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base_smem_accumulator.h
--rw-r--r--   0 root         (0) root         (0)    33788 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage.h
--rw-r--r--   0 root         (0) root         (0)    33506 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage_smem_accumulator.h
--rw-r--r--   0 root         (0) root         (0)    21451 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined.h
--rw-r--r--   0 root         (0) root         (0)    21065 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined_smem_accumulator.h
--rw-r--r--   0 root         (0) root         (0)    27144 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma.h
--rw-r--r--   0 root         (0) root         (0)    27400 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma_smem_accumulator.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.972803 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/14_ampere_tf32_tensorop_gemm/
--rw-r--r--   0 root         (0) root         (0)    18020 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/14_ampere_tf32_tensorop_gemm/ampere_tf32_tensorop_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:45.995189 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/15_ampere_sparse_tensorop_gemm/
--rw-r--r--   0 root         (0) root         (0)    15042 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/15_ampere_sparse_tensorop_gemm/ampere_sparse_tensorop_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.015483 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/16_ampere_tensorop_conv2dfprop/
--rw-r--r--   0 root         (0) root         (0)    27755 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/16_ampere_tensorop_conv2dfprop/ampere_tensorop_conv2dfprop.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.037174 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/17_fprop_per_channel_bias/
--rw-r--r--   0 root         (0) root         (0)    12580 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/17_fprop_per_channel_bias/fprop_per_channel_bias.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.057270 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/18_ampere_fp64_tensorop_affine2_gemm/
--rw-r--r--   0 root         (0) root         (0)    14007 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/18_ampere_fp64_tensorop_affine2_gemm/ampere_fp64_tensorop_affine2_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.078023 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/19_tensorop_canonical/
--rw-r--r--   0 root         (0) root         (0)    13401 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/19_tensorop_canonical/tensorop_canonical.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.098693 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/20_simt_canonical/
--rw-r--r--   0 root         (0) root         (0)    12556 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/20_simt_canonical/simt_canonical.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.119272 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/21_quaternion_gemm/
--rw-r--r--   0 root         (0) root         (0)    17319 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/21_quaternion_gemm/quaternion_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.139926 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/22_quaternion_conv/
--rw-r--r--   0 root         (0) root         (0)    21495 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/22_quaternion_conv/quaternion_conv.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.160615 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/23_ampere_gemm_operand_reduction_fusion/
--rw-r--r--   0 root         (0) root         (0)    27530 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/23_ampere_gemm_operand_reduction_fusion/ampere_gemm_operand_reduction_fusion.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.180930 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/24_gemm_grouped/
--rw-r--r--   0 root         (0) root         (0)    50996 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/24_gemm_grouped/gemm_grouped.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.217414 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/
--rw-r--r--   0 root         (0) root         (0)    26547 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_3d_fprop_mainloop_fusion.cu
--rw-r--r--   0 root         (0) root         (0)    25628 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_fprop_mainloop_fusion.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.237774 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/26_ampere_wgrad_mainloop_fusion/
--rw-r--r--   0 root         (0) root         (0)    25538 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/26_ampere_wgrad_mainloop_fusion/ampere_wgrad_mainloop_fusion.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.258229 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/
--rw-r--r--   0 root         (0) root         (0)    30446 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.279356 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/
--rw-r--r--   0 root         (0) root         (0)    28159 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/ampere_3xtf32_fast_accurate_tensorop_fprop.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.299724 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/
--rw-r--r--   0 root         (0) root         (0)    28403 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.320169 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/30_wgrad_split_k/
--rw-r--r--   0 root         (0) root         (0)    27329 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/30_wgrad_split_k/30_wgrad_split_k.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.340319 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/31_basic_syrk/
--rw-r--r--   0 root         (0) root         (0)    15206 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/31_basic_syrk/basic_syrk.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.360436 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/32_basic_trmm/
--rw-r--r--   0 root         (0) root         (0)    15907 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/32_basic_trmm/basic_trmm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.380552 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/33_ampere_3xtf32_tensorop_symm/
--rw-r--r--   0 root         (0) root         (0)    31803 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/33_ampere_3xtf32_tensorop_symm/ampere_3xtf32_tensorop_symm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.401820 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/34_transposed_conv2d/
--rw-r--r--   0 root         (0) root         (0)    22378 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/34_transposed_conv2d/34_transposed_conv2d.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.454304 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/35_gemm_softmax/
--rw-r--r--   0 root         (0) root         (0)    23114 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_softmax.cu
--rw-r--r--   0 root         (0) root         (0)    16723 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_with_epilogue_visitor.h
--rw-r--r--   0 root         (0) root         (0)    18713 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_with_softmax.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.474489 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/36_gather_scatter_fusion/
--rw-r--r--   0 root         (0) root         (0)    20795 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/36_gather_scatter_fusion/gather_scatter_fusion.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.526117 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/
--rw-r--r--   0 root         (0) root         (0)    31111 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu
--rw-r--r--   0 root         (0) root         (0)    13982 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h
--rw-r--r--   0 root         (0) root         (0)    33916 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.546987 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/38_syr2k_grouped/
--rw-r--r--   0 root         (0) root         (0)    47455 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/38_syr2k_grouped/syr2k_grouped.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.568570 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/39_gemm_permute/
--rw-r--r--   0 root         (0) root         (0)    37896 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/39_gemm_permute/gemm_permute.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.791070 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/
--rw-r--r--   0 root         (0) root         (0)    18389 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/attention_scaling_coefs_updater.h
--rw-r--r--   0 root         (0) root         (0)     8286 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/debug_utils.h
--rw-r--r--   0 root         (0) root         (0)     9888 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/default_fmha_grouped.h
--rw-r--r--   0 root         (0) root         (0)    22349 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_pipelined.h
--rw-r--r--   0 root         (0) root         (0)     9162 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_rescale_output.h
--rw-r--r--   0 root         (0) root         (0)     6111 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_thread_apply_logsumexp.h
--rw-r--r--   0 root         (0) root         (0)     6768 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/find_default_mma.h
--rw-r--r--   0 root         (0) root         (0)    29972 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h
--rw-r--r--   0 root         (0) root         (0)     6666 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped_problem_visitor.h
--rw-r--r--   0 root         (0) root         (0)    37104 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu
--rw-r--r--   0 root         (0) root         (0)    39975 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.857765 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/
--rw-r--r--   0 root         (0) root         (0)     3994 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma.h
--rw-r--r--   0 root         (0) root         (0)     6241 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_base.h
--rw-r--r--   0 root         (0) root         (0)    27198 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_multistage.h
--rw-r--r--   0 root         (0) root         (0)    14090 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_pipelined.h
--rw-r--r--   0 root         (0) root         (0)    12089 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm_kernel_utils.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.932241 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/
--rw-r--r--   0 root         (0) root         (0)    23805 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/epilogue_predicated_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)     3142 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/make_residual_last.h
--rw-r--r--   0 root         (0) root         (0)    64480 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_access_iterator_residual_last.h
--rw-r--r--   0 root         (0) root         (0)    64500 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_iterator_residual_last.h
--rw-r--r--   0 root         (0) root         (0)    37905 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h
--rw-r--r--   0 root         (0) root         (0)    61195 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.957981 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/42_ampere_tensorop_group_conv/
--rw-r--r--   0 root         (0) root         (0)    23901 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/42_ampere_tensorop_group_conv/ampere_tensorop_group_conv.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:46.979523 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/43_ell_block_sparse_gemm/
--rw-r--r--   0 root         (0) root         (0)    23867 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/43_ell_block_sparse_gemm/ell_block_sparse_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:47.013797 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.209645 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.199160 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:47.087340 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/
--rw-r--r--   0 root         (0) root         (0)     6370 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_bias_act_epilogue_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     4099 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_thread_map_tensor_op_for_fused_bias.h
--rw-r--r--   0 root         (0) root         (0)     8285 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/fused_bias_act_epilogue.h
--rw-r--r--   0 root         (0) root         (0)    10439 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/output_tile_thread_map_for_fused_bias.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:47.107500 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/warp/
--rw-r--r--   0 root         (0) root         (0)     6848 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/warp/fused_bias_act_fragment_iterator_tensor_op.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.214256 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:47.130835 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/warp/
--rw-r--r--   0 root         (0) root         (0)    14747 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/warp/mma_tensor_op_fragment_iterator_without_output_op.h
--rw-r--r--   0 root         (0) root         (0)    10231 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/leaky_bias.h
--rw-r--r--   0 root         (0) root         (0)     3745 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/utils.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:47.181167 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:47.201437 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/device/
--rw-r--r--   0 root         (0) root         (0)    15362 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/device/dual_gemm.h
--rw-r--r--   0 root         (0) root         (0)     8109 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm.cu
--rw-r--r--   0 root         (0) root         (0)    26478 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm_run.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:47.222190 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/kernel/
--rw-r--r--   0 root         (0) root         (0)    16424 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/kernel/dual_gemm.h
--rw-r--r--   0 root         (0) root         (0)     3577 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/test_run.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:47.242488 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/thread/
--rw-r--r--   0 root         (0) root         (0)     5818 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/thread/left_silu_and_mul.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:47.292738 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/
--rw-r--r--   0 root         (0) root         (0)    15613 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_epilogue.h
--rw-r--r--   0 root         (0) root         (0)     7264 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_base.h
--rw-r--r--   0 root         (0) root         (0)    28984 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_multistage.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:47.314089 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/46_depthwise_simt_conv2dfprop/
--rw-r--r--   0 root         (0) root         (0)    24464 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/46_depthwise_simt_conv2dfprop/depthwise_simt_conv2dfprop.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:47.334599 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/47_ampere_gemm_universal_streamk/
--rw-r--r--   0 root         (0) root         (0)    22677 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/47_ampere_gemm_universal_streamk/ampere_gemm_universal_streamk.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:47.355425 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/60_cutlass_import/
--rw-r--r--   0 root         (0) root         (0)     2849 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/60_cutlass_import/main.cpp
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:47.376262 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/common/
--rw-r--r--   0 root         (0) root         (0)     2621 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/common/helper.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.266734 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:48.037540 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/
--rw-r--r--   0 root         (0) root         (0)     3793 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/aligned_buffer.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:48.364668 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/
--rw-r--r--   0 root         (0) root         (0)     3538 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/arch.h
--rw-r--r--   0 root         (0) root         (0)     2691 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/cache_operation.h
--rw-r--r--   0 root         (0) root         (0)    14313 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/memory.h
--rw-r--r--   0 root         (0) root         (0)    10490 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm75.h
--rw-r--r--   0 root         (0) root         (0)    15166 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm80.h
--rw-r--r--   0 root         (0) root         (0)     8073 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma.h
--rw-r--r--   0 root         (0) root         (0)    11096 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm50.h
--rw-r--r--   0 root         (0) root         (0)     7040 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm60.h
--rw-r--r--   0 root         (0) root         (0)     4193 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm61.h
--rw-r--r--   0 root         (0) root         (0)    16554 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm70.h
--rw-r--r--   0 root         (0) root         (0)    31682 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm75.h
--rw-r--r--   0 root         (0) root         (0)    55573 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm80.h
--rw-r--r--   0 root         (0) root         (0)     4430 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm90.h
--rw-r--r--   0 root         (0) root         (0)    43978 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sparse_sm80.h
--rw-r--r--   0 root         (0) root         (0)     3998 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/simd.h
--rw-r--r--   0 root         (0) root         (0)     3656 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/simd_sm60.h
--rw-r--r--   0 root         (0) root         (0)     5102 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/simd_sm61.h
--rw-r--r--   0 root         (0) root         (0)     8473 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/wmma.h
--rw-r--r--   0 root         (0) root         (0)     5286 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm70.h
--rw-r--r--   0 root         (0) root         (0)     7746 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm72.h
--rw-r--r--   0 root         (0) root         (0)     7616 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm75.h
--rw-r--r--   0 root         (0) root         (0)    62709 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/array.h
--rw-r--r--   0 root         (0) root         (0)     3662 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/array_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)    13154 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/array_subbyte.h
--rw-r--r--   0 root         (0) root         (0)     6371 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/barrier.h
--rw-r--r--   0 root         (0) root         (0)    13371 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/bfloat16.h
--rw-r--r--   0 root         (0) root         (0)     6338 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/blas3.h
--rw-r--r--   0 root         (0) root         (0)     9372 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/block_striped.h
--rw-r--r--   0 root         (0) root         (0)    19422 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/complex.h
--rw-r--r--   0 root         (0) root         (0)    47943 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/constants.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:48.418141 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/
--rw-r--r--   0 root         (0) root         (0)    22725 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/conv2d_problem_size.h
--rw-r--r--   0 root         (0) root         (0)    16292 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/conv3d_problem_size.h
--rw-r--r--   0 root         (0) root         (0)     6664 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/convolution.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:48.468593 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/device/
--rw-r--r--   0 root         (0) root         (0)     9744 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/device/direct_convolution.h
--rw-r--r--   0 root         (0) root         (0)    12078 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/device/implicit_gemm_convolution.h
--rw-r--r--   0 root         (0) root         (0)    10044 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/device/implicit_gemm_convolution_fusion.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:48.769540 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/
--rw-r--r--   0 root         (0) root         (0)     7671 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d.h
--rw-r--r--   0 root         (0) root         (0)    53546 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_dgrad.h
--rw-r--r--   0 root         (0) root         (0)    56838 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop.h
--rw-r--r--   0 root         (0) root         (0)    11953 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_fusion.h
--rw-r--r--   0 root         (0) root         (0)     4658 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_broadcast.h
--rw-r--r--   0 root         (0) root         (0)     4660 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_reduction.h
--rw-r--r--   0 root         (0) root         (0)    15891 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_group_fprop.h
--rw-r--r--   0 root         (0) root         (0)    28745 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad.h
--rw-r--r--   0 root         (0) root         (0)    10459 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad_fusion.h
--rw-r--r--   0 root         (0) root         (0)     9324 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_dgrad.h
--rw-r--r--   0 root         (0) root         (0)    14864 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop.h
--rw-r--r--   0 root         (0) root         (0)    11980 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop_fusion.h
--rw-r--r--   0 root         (0) root         (0)    14883 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_wgrad.h
--rw-r--r--   0 root         (0) root         (0)    19294 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_depthwise_fprop.h
--rw-r--r--   0 root         (0) root         (0)    18048 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/direct_convolution.h
--rw-r--r--   0 root         (0) root         (0)    15454 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution.h
--rw-r--r--   0 root         (0) root         (0)    15709 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_fusion.h
--rw-r--r--   0 root         (0) root         (0)    17131 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_strided_dgrad.h
--rw-r--r--   0 root         (0) root         (0)    16749 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_with_fused_epilogue.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:48.789924 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/thread/
--rw-r--r--   0 root         (0) root         (0)     9689 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/thread/depthwise_mma.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:49.515409 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/
--rw-r--r--   0 root         (0) root         (0)    15306 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    19735 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)    18940 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    26137 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)    10953 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    11529 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_few_channels.h
--rw-r--r--   0 root         (0) root         (0)    11333 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_fixed_channels.h
--rw-r--r--   0 root         (0) root         (0)    13664 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)    10627 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)     9314 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_few_channels.h
--rw-r--r--   0 root         (0) root         (0)     9018 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_fixed_channels.h
--rw-r--r--   0 root         (0) root         (0)    10387 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)    30197 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_params.h
--rw-r--r--   0 root         (0) root         (0)    11202 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)    10350 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    11520 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)     9043 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    10832 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)     8450 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)     9569 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)    11020 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    15014 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)     9634 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    15132 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)     7945 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)     8891 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)    18249 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_params.h
--rw-r--r--   0 root         (0) root         (0)     9971 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    12024 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)     8821 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    10744 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)     8871 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_direct_conv_params.h
--rw-r--r--   0 root         (0) root         (0)    10747 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_fixed_stride_dilation.h
--rw-r--r--   0 root         (0) root         (0)     9899 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_optimized.h
--rw-r--r--   0 root         (0) root         (0)    20899 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_direct_conv_multistage.h
--rw-r--r--   0 root         (0) root         (0)     8921 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_filter_tile_access_iterator_direct_conv_optimized.h
--rw-r--r--   0 root         (0) root         (0)    12744 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_pipelined.h
--rw-r--r--   0 root         (0) root         (0)     8097 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_mma_base.h
--rw-r--r--   0 root         (0) root         (0)    36697 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_mma_core_with_lane_access_size.h
--rw-r--r--   0 root         (0) root         (0)    30106 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_fprop_fusion_multistage.h
--rw-r--r--   0 root         (0) root         (0)    20086 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_multistage.h
--rw-r--r--   0 root         (0) root         (0)    12174 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_pipelined.h
--rw-r--r--   0 root         (0) root         (0)    26320 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_wgrad_fusion_multistage.h
--rw-r--r--   0 root         (0) root         (0)    16915 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)    12476 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_iterator.h
--rw-r--r--   0 root         (0) root         (0)     8050 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/threadblock_swizzle.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:49.567108 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/warp/
--rw-r--r--   0 root         (0) root         (0)    12419 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/warp/mma_depthwise_simt.h
--rw-r--r--   0 root         (0) root         (0)    30655 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/warp/mma_depthwise_simt_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)     8772 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/warp/scale_bias_relu_transform.h
--rw-r--r--   0 root         (0) root         (0)    11827 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/coord.h
--rw-r--r--   0 root         (0) root         (0)    11077 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/core_io.h
--rw-r--r--   0 root         (0) root         (0)     7838 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/cutlass.h
--rw-r--r--   0 root         (0) root         (0)     3108 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/device_kernel.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.318316 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:49.922597 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/
--rw-r--r--   0 root         (0) root         (0)    18909 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/activation.h
--rw-r--r--   0 root         (0) root         (0)     4691 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/conversion_op.h
--rw-r--r--   0 root         (0) root         (0)     9349 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination.h
--rw-r--r--   0 root         (0) root         (0)     8344 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_elementwise.h
--rw-r--r--   0 root         (0) root         (0)    13490 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_relu.h
--rw-r--r--   0 root         (0) root         (0)    23649 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_clamp.h
--rw-r--r--   0 root         (0) root         (0)     9067 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_dgelu.h
--rw-r--r--   0 root         (0) root         (0)    15195 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_drelu.h
--rw-r--r--   0 root         (0) root         (0)     3669 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_gelu.h
--rw-r--r--   0 root         (0) root         (0)     8065 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_generic.h
--rw-r--r--   0 root         (0) root         (0)     3693 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_hardswish.h
--rw-r--r--   0 root         (0) root         (0)     8344 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_leaky_relu.h
--rw-r--r--   0 root         (0) root         (0)     3058 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_params.h
--rw-r--r--   0 root         (0) root         (0)     9351 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)    20486 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_relu.h
--rw-r--r--   0 root         (0) root         (0)    19348 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_relu0.h
--rw-r--r--   0 root         (0) root         (0)    11855 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_residual_block.h
--rw-r--r--   0 root         (0) root         (0)     3688 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_sigmoid.h
--rw-r--r--   0 root         (0) root         (0)     3669 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_silu.h
--rw-r--r--   0 root         (0) root         (0)     8662 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_with_elementwise.h
--rw-r--r--   0 root         (0) root         (0)     3416 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/reduction_op.h
--rw-r--r--   0 root         (0) root         (0)     2656 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/scale_type.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:50.606997 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/
--rw-r--r--   0 root         (0) root         (0)     9142 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     9441 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op_blas3.h
--rw-r--r--   0 root         (0) root         (0)     3234 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_direct_store.h
--rw-r--r--   0 root         (0) root         (0)     7209 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)    13385 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_simt.h
--rw-r--r--   0 root         (0) root         (0)    27150 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     7129 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op_blas3.h
--rw-r--r--   0 root         (0) root         (0)    10846 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     5817 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_broadcast.h
--rw-r--r--   0 root         (0) root         (0)     5763 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_reduction.h
--rw-r--r--   0 root         (0) root         (0)     5947 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     4409 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_simt.h
--rw-r--r--   0 root         (0) root         (0)     7398 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     7303 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_volta_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     4098 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_wmma_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     4678 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/direct_store_epilogue_iterator.h
--rw-r--r--   0 root         (0) root         (0)    19214 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue.h
--rw-r--r--   0 root         (0) root         (0)     8279 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_base.h
--rw-r--r--   0 root         (0) root         (0)     7455 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_base_streamk.h
--rw-r--r--   0 root         (0) root         (0)    13424 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_depthwise.h
--rw-r--r--   0 root         (0) root         (0)    13933 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_direct_store.h
--rw-r--r--   0 root         (0) root         (0)     7401 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_gemm_k_reduction.h
--rw-r--r--   0 root         (0) root         (0)    14610 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)     9073 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_smem_accumulator.h
--rw-r--r--   0 root         (0) root         (0)    16804 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_visitor_with_softmax.h
--rw-r--r--   0 root         (0) root         (0)    52430 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_broadcast.h
--rw-r--r--   0 root         (0) root         (0)    29199 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_reduction.h
--rw-r--r--   0 root         (0) root         (0)    13454 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h
--rw-r--r--   0 root         (0) root         (0)     7308 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_workspace.h
--rw-r--r--   0 root         (0) root         (0)    14359 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/interleaved_epilogue.h
--rw-rw-r--   0 root         (0) root         (0)     2912 2022-06-02 16:47:41.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/output_iterator_parameter.h
--rw-r--r--   0 root         (0) root         (0)    19750 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/output_tile_thread_map.h
--rw-r--r--   0 root         (0) root         (0)    40870 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)    18821 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine.h
--rw-r--r--   0 root         (0) root         (0)     5636 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine_layout_params.h
--rw-r--r--   0 root         (0) root         (0)    21249 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_blas3.h
--rw-r--r--   0 root         (0) root         (0)    13872 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_direct_conv.h
--rw-r--r--   0 root         (0) root         (0)    14496 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_params.h
--rw-r--r--   0 root         (0) root         (0)     9146 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_predicates.h
--rw-r--r--   0 root         (0) root         (0)    15536 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_strided_dgrad.h
--rw-r--r--   0 root         (0) root         (0)     7487 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator.h
--rw-r--r--   0 root         (0) root         (0)    17683 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_mixed.h
--rw-r--r--   0 root         (0) root         (0)     7394 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_pitch_liner.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:50.855581 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/
--rw-r--r--   0 root         (0) root         (0)     7055 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_complex_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     7736 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_gaussian_complex_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     5880 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_simt.h
--rw-r--r--   0 root         (0) root         (0)     9883 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     8924 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_volta_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     6045 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_wmma_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     4864 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/simt_policy.h
--rw-r--r--   0 root         (0) root         (0)     5979 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tensor_op_policy.h
--rw-r--r--   0 root         (0) root         (0)    25658 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_simt.h
--rw-r--r--   0 root         (0) root         (0)    20290 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    22857 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op_mixed.h
--rw-r--r--   0 root         (0) root         (0)    14258 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_volta_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     7704 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_wmma_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     7485 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/volta_tensor_op_policy.h
--rw-r--r--   0 root         (0) root         (0)     3916 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/wmma_tensor_op_policy.h
--rw-r--r--   0 root         (0) root         (0)    26026 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/fast_math.h
--rw-r--r--   0 root         (0) root         (0)    35325 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/float8.h
--rw-r--r--   0 root         (0) root         (0)     2645 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/floating_point_nvrtc.h
--rw-r--r--   0 root         (0) root         (0)    11242 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/functional.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:50.875664 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:51.220565 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/
--rw-r--r--   0 root         (0) root         (0)    17023 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/base_grouped.h
--rw-r--r--   0 root         (0) root         (0)    24413 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/default_gemm_configuration.h
--rw-r--r--   0 root         (0) root         (0)    27616 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/ell_gemm.h
--rw-r--r--   0 root         (0) root         (0)    25202 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm.h
--rw-r--r--   0 root         (0) root         (0)    22367 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_array.h
--rw-r--r--   0 root         (0) root         (0)    22375 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_batched.h
--rw-r--r--   0 root         (0) root         (0)    22725 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_complex.h
--rw-r--r--   0 root         (0) root         (0)     2591 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_grouped.h
--rw-r--r--   0 root         (0) root         (0)    13736 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_layernorm_mainloop_fusion.h
--rw-r--r--   0 root         (0) root         (0)    17329 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_sparse.h
--rw-r--r--   0 root         (0) root         (0)    20450 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_splitk_parallel.h
--rw-r--r--   0 root         (0) root         (0)    14902 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal.h
--rw-r--r--   0 root         (0) root         (0)     7444 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_adapter.h
--rw-r--r--   0 root         (0) root         (0)    13352 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_base.h
--rw-r--r--   0 root         (0) root         (0)    13968 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_with_broadcast.h
--rw-r--r--   0 root         (0) root         (0)    14853 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_with_k_reduction.h
--rw-r--r--   0 root         (0) root         (0)     5690 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemv.h
--rw-r--r--   0 root         (0) root         (0)    18127 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_2k.h
--rw-r--r--   0 root         (0) root         (0)     2747 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_2k_grouped.h
--rw-r--r--   0 root         (0) root         (0)    16719 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_k.h
--rwxr-xr-x   0 root         (0) root         (0)    21050 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/symm.h
--rw-r--r--   0 root         (0) root         (0)    26464 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/trmm.h
--rw-r--r--   0 root         (0) root         (0)    11570 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/gemm.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:52.115850 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/
--rw-r--r--   0 root         (0) root         (0)    29360 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_ell_gemm.h
--rw-r--r--   0 root         (0) root         (0)    37752 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm.h
--rw-r--r--   0 root         (0) root         (0)    16130 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_complex.h
--rw-r--r--   0 root         (0) root         (0)    12385 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped.h
--rw-r--r--   0 root         (0) root         (0)     6592 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped_softmax_mainloop_fusion.h
--rw-r--r--   0 root         (0) root         (0)     5848 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_layernorm_mainloop_fusion.h
--rw-r--r--   0 root         (0) root         (0)    11104 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_planar_complex_universal.h
--rw-r--r--   0 root         (0) root         (0)     7983 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_sparse.h
--rw-r--r--   0 root         (0) root         (0)     4932 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_splitk_parallel.h
--rw-r--r--   0 root         (0) root         (0)    11951 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_universal.h
--rw-r--r--   0 root         (0) root         (0)     8063 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_broadcast.h
--rw-r--r--   0 root         (0) root         (0)     6457 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_k_reduction.h
--rw-r--r--   0 root         (0) root         (0)     8086 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_reduction.h
--rwxr-xr-x   0 root         (0) root         (0)     5349 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemv.h
--rw-r--r--   0 root         (0) root         (0)    11560 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k.h
--rw-r--r--   0 root         (0) root         (0)    20509 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_complex.h
--rw-r--r--   0 root         (0) root         (0)    12470 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_grouped.h
--rw-r--r--   0 root         (0) root         (0)    10620 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_universal.h
--rw-r--r--   0 root         (0) root         (0)     9872 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k.h
--rw-r--r--   0 root         (0) root         (0)    16990 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k_complex.h
--rw-r--r--   0 root         (0) root         (0)     9444 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k_universal.h
--rwxr-xr-x   0 root         (0) root         (0)    13375 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm.h
--rwxr-xr-x   0 root         (0) root         (0)    21830 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm_complex.h
--rwxr-xr-x   0 root         (0) root         (0)    10315 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm_universal.h
--rw-r--r--   0 root         (0) root         (0)    10873 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm.h
--rw-r--r--   0 root         (0) root         (0)    10730 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm_complex.h
--rw-r--r--   0 root         (0) root         (0)    10850 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm_universal.h
--rw-r--r--   0 root         (0) root         (0)    28916 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/ell_gemm.h
--rw-r--r--   0 root         (0) root         (0)    13381 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm.h
--rw-r--r--   0 root         (0) root         (0)     8717 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_array.h
--rw-r--r--   0 root         (0) root         (0)     8785 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_batched.h
--rw-r--r--   0 root         (0) root         (0)    14711 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped.h
--rw-r--r--   0 root         (0) root         (0)     4691 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped_problem_visitor.h
--rw-r--r--   0 root         (0) root         (0)    15623 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped_softmax_mainloop_fusion.h
--rw-r--r--   0 root         (0) root         (0)    27281 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_layernorm_mainloop_fusion.h
--rwxr-xr-x   0 root         (0) root         (0)     6144 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_params.h
--rw-r--r--   0 root         (0) root         (0)     5165 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_pipelined.h
--rw-r--r--   0 root         (0) root         (0)    22973 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)    18961 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex_array.h
--rw-r--r--   0 root         (0) root         (0)     8142 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_splitk_parallel.h
--rw-r--r--   0 root         (0) root         (0)     4291 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_transpose_operands.h
--rw-r--r--   0 root         (0) root         (0)    22913 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_universal.h
--rw-r--r--   0 root         (0) root         (0)    41469 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_universal_streamk.h
--rw-r--r--   0 root         (0) root         (0)    47222 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_with_fused_epilogue.h
--rw-r--r--   0 root         (0) root         (0)    23629 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_with_k_reduction.h
--rw-r--r--   0 root         (0) root         (0)     8090 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemv.h
--rwxr-xr-x   0 root         (0) root         (0)     8979 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemv_batched_strided.h
--rw-r--r--   0 root         (0) root         (0)    16849 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/grouped_problem_visitor.h
--rw-r--r--   0 root         (0) root         (0)     7148 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/params_universal_base.h
--rw-r--r--   0 root         (0) root         (0)    22962 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped.h
--rw-r--r--   0 root         (0) root         (0)    16100 2023-04-15 15:41:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped_problem_visitor.h
--rw-r--r--   0 root         (0) root         (0)     4334 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_transpose_operands.h
--rw-r--r--   0 root         (0) root         (0)    24162 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_universal.h
--rw-r--r--   0 root         (0) root         (0)    17567 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_k_universal.h
--rw-r--r--   0 root         (0) root         (0)    13610 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/sparse_gemm.h
--rwxr-xr-x   0 root         (0) root         (0)    23900 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/symm_universal.h
--rw-r--r--   0 root         (0) root         (0)    19537 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/trmm_universal.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:52.182437 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/
--rw-r--r--   0 root         (0) root         (0)     3567 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma.h
--rw-r--r--   0 root         (0) root         (0)    15373 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm50.h
--rw-r--r--   0 root         (0) root         (0)    29987 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm60.h
--rw-r--r--   0 root         (0) root         (0)     8142 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm61.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:52.842889 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/
--rw-r--r--   0 root         (0) root         (0)    31930 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_ell_mma.h
--rwxr-xr-x   0 root         (0) root         (0)     6979 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_gemv_core.h
--rw-r--r--   0 root         (0) root         (0)    34241 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma.h
--rw-r--r--   0 root         (0) root         (0)     5123 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core.h
--rw-r--r--   0 root         (0) root         (0)    57426 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_simt.h
--rw-r--r--   0 root         (0) root         (0)    19257 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm70.h
--rw-r--r--   0 root         (0) root         (0)    42310 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm75.h
--rw-r--r--   0 root         (0) root         (0)   103000 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm80.h
--rw-r--r--   0 root         (0) root         (0)    32106 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h
--rw-r--r--   0 root         (0) root         (0)    12645 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_access_size.h
--rw-r--r--   0 root         (0) root         (0)     7387 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_reduction.h
--rw-r--r--   0 root         (0) root         (0)    20975 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_wmma.h
--rw-r--r--   0 root         (0) root         (0)     7998 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_layernorm_mainloop_fusion.h
--rw-r--r--   0 root         (0) root         (0)     5110 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_multistage.h
--rw-r--r--   0 root         (0) root         (0)     4627 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_pipelined.h
--rw-r--r--   0 root         (0) root         (0)     7113 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_softmax_mainloop_fusion.h
--rw-r--r--   0 root         (0) root         (0)     6323 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_with_reduction.h
--rw-r--r--   0 root         (0) root         (0)     7121 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex.h
--rw-r--r--   0 root         (0) root         (0)     4959 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core.h
--rw-r--r--   0 root         (0) root         (0)    65201 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core_sm80.h
--rw-r--r--   0 root         (0) root         (0)    25495 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_trmm_complex.h
--rw-r--r--   0 root         (0) root         (0)     8509 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_sparse_mma.h
--rw-r--r--   0 root         (0) root         (0)    19515 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_trmm.h
--rw-r--r--   0 root         (0) root         (0)    24047 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/ell_mma_multistage.h
--rw-r--r--   0 root         (0) root         (0)    13836 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/ell_mma_pipelined.h
--rwxr-xr-x   0 root         (0) root         (0)     4726 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/gemv.h
--rw-r--r--   0 root         (0) root         (0)     3652 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/index_remat.h
--rw-r--r--   0 root         (0) root         (0)     7823 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_base.h
--rw-r--r--   0 root         (0) root         (0)    27415 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_blas3_multistage.h
--rw-r--r--   0 root         (0) root         (0)    32894 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_layernorm_mainloop_fusion_multistage.h
--rw-r--r--   0 root         (0) root         (0)    28015 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_multistage.h
--rw-r--r--   0 root         (0) root         (0)    15995 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_pipelined.h
--rw-r--r--   0 root         (0) root         (0)     6901 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_base.h
--rw-r--r--   0 root         (0) root         (0)    22653 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_multistage.h
--rw-r--r--   0 root         (0) root         (0)    14746 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_pipelined.h
--rw-r--r--   0 root         (0) root         (0)     9864 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_singlestage.h
--rw-r--r--   0 root         (0) root         (0)    27061 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_softmax_mainloop_fusion_multistage.h
--rw-r--r--   0 root         (0) root         (0)     9210 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_sparse_base.h
--rw-r--r--   0 root         (0) root         (0)    25333 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_sparse_multistage.h
--rw-r--r--   0 root         (0) root         (0)    20473 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_with_reduction_multistage.h
--rw-r--r--   0 root         (0) root         (0)    15007 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle.h
--rw-r--r--   0 root         (0) root         (0)    26621 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle_streamk.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:53.397215 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/
--rw-r--r--   0 root         (0) root         (0)    20553 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_complex_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     6684 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_sparse_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     5160 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     9026 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op_sm80.h
--rw-r--r--   0 root         (0) root         (0)     4053 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_with_reduction_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     4685 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_wmma_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     5725 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/layernorm_scale_bias_transform.h
--rw-r--r--   0 root         (0) root         (0)     2619 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma.h
--rw-r--r--   0 root         (0) root         (0)    37705 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    23132 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_fast_f32.h
--rw-r--r--   0 root         (0) root         (0)    78615 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_tile_iterator_sm80.h
--rw-r--r--   0 root         (0) root         (0)    21205 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    14589 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op_tile_iterator_sm80.h
--rw-r--r--   0 root         (0) root         (0)     6144 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)     8446 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt.h
--rw-r--r--   0 root         (0) root         (0)     3079 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt_policy.h
--rw-r--r--   0 root         (0) root         (0)    59793 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)    11758 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_sparse_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    14407 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    15721 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_fast_f32.h
--rw-rw-r--   0 root         (0) root         (0)    18643 2022-06-02 16:47:41.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_fragment_iterator.h
--rw-r--r--   0 root         (0) root         (0)     2939 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_policy.h
--rw-r--r--   0 root         (0) root         (0)     8966 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_sm70.h
--rw-r--r--   0 root         (0) root         (0)    11017 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)   136033 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)    99649 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm70.h
--rw-r--r--   0 root         (0) root         (0)    75179 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm80.h
--rw-r--r--   0 root         (0) root         (0)    13151 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sparse.h
--rw-r--r--   0 root         (0) root         (0)    27101 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_wmma.h
--rw-r--r--   0 root         (0) root         (0)     7241 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_wmma.h
--rw-r--r--   0 root         (0) root         (0)    17271 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_with_reduction_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    19125 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/scale_bias_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)     4610 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/softmax_scale_bias_transform.h
--rw-r--r--   0 root         (0) root         (0)     8728 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/tile_iterator_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)    23615 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/half.h
--rw-r--r--   0 root         (0) root         (0)     6893 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/integer_subbyte.h
--rw-r--r--   0 root         (0) root         (0)     2801 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/kernel_launch.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:53.549120 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/
--rw-r--r--   0 root         (0) root         (0)     3020 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/layout.h
--rw-r--r--   0 root         (0) root         (0)    34712 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/matrix.h
--rw-r--r--   0 root         (0) root         (0)     9133 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/permute.h
--rw-r--r--   0 root         (0) root         (0)     4696 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/pitch_linear.h
--rw-r--r--   0 root         (0) root         (0)    18295 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/tensor.h
--rw-r--r--   0 root         (0) root         (0)    29599 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm70.h
--rw-r--r--   0 root         (0) root         (0)    33137 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm75.h
--rw-r--r--   0 root         (0) root         (0)    29336 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm80.h
--rw-r--r--   0 root         (0) root         (0)     3328 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/vector.h
--rw-r--r--   0 root         (0) root         (0)   364115 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/matrix.h
--rw-r--r--   0 root         (0) root         (0)     4991 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/matrix_coord.h
--rw-r--r--   0 root         (0) root         (0)     2726 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/matrix_shape.h
--rw-r--r--   0 root         (0) root         (0)    71278 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/numeric_conversion.h
--rw-r--r--   0 root         (0) root         (0)     3505 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/numeric_types.h
--rw-r--r--   0 root         (0) root         (0)     5492 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/pitch_linear_coord.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:53.569616 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/platform/
--rw-r--r--   0 root         (0) root         (0)    26097 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/platform/platform.h
--rw-r--r--   0 root         (0) root         (0)    15565 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/predicate_vector.h
--rw-r--r--   0 root         (0) root         (0)    20901 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/quaternion.h
--rw-r--r--   0 root         (0) root         (0)     2369 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/real.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:53.589365 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:53.659793 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/device/
--rw-r--r--   0 root         (0) root         (0)     6823 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/device/reduce_split_k.h
--rw-r--r--   0 root         (0) root         (0)     8152 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce.h
--rw-r--r--   0 root         (0) root         (0)    11579 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_contiguous.h
--rw-r--r--   0 root         (0) root         (0)    11448 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_strided.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:53.729681 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/
--rw-r--r--   0 root         (0) root         (0)     8762 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/reduce_softmax_final.h
--rw-r--r--   0 root         (0) root         (0)     7897 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/reduce_split_k.h
--rw-r--r--   0 root         (0) root         (0)    20685 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_contiguous.h
--rw-r--r--   0 root         (0) root         (0)    21662 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_strided.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:53.765345 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/
--rw-r--r--   0 root         (0) root         (0)     7208 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/reduce.h
--rw-r--r--   0 root         (0) root         (0)     6790 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/reduction_operators.h
--rw-r--r--   0 root         (0) root         (0)     2936 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/threadblock_swizzle.h
--rw-r--r--   0 root         (0) root         (0)     5929 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/relatively_equal.h
--rw-r--r--   0 root         (0) root         (0)     4186 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/semaphore.h
--rw-r--r--   0 root         (0) root         (0)    17243 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/subbyte_reference.h
--rw-r--r--   0 root         (0) root         (0)     8964 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/tensor_coord.h
--rw-r--r--   0 root         (0) root         (0)    12207 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/tensor_ref.h
--rw-r--r--   0 root         (0) root         (0)    11201 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/tensor_ref_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)     9509 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/tensor_view.h
--rw-r--r--   0 root         (0) root         (0)    10250 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/tensor_view_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)    13017 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/tfloat32.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:53.786094 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/thread/
--rw-r--r--   0 root         (0) root         (0)     5931 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/thread/matrix.h
--rw-r--r--   0 root         (0) root         (0)     2581 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/trace.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:53.804532 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/
--rw-r--r--   0 root         (0) root         (0)    33392 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/pitch_linear_thread_map.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:53.840282 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/thread/
--rw-r--r--   0 root         (0) root         (0)     3835 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/thread/transpose.h
--rw-r--r--   0 root         (0) root         (0)     4309 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/thread/unary_op.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:54.240776 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/
--rw-r--r--   0 root         (0) root         (0)     6181 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_iterator.h
--rw-r--r--   0 root         (0) root         (0)    44443 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)    44309 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)    12890 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)    11097 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_iterator.h
--rw-r--r--   0 root         (0) root         (0)    70684 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)    28232 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_2dthreadtile.h
--rwxr-xr-x   0 root         (0) root         (0)    10243 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_params.h
--rw-r--r--   0 root         (0) root         (0)    31412 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_triangular_matrix.h
--rw-r--r--   0 root         (0) root         (0)    62672 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)    27175 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h
--rw-r--r--   0 root         (0) root         (0)    28064 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_triangular_matrix.h
--rw-r--r--   0 root         (0) root         (0)    13088 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_vector_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)     8232 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_scale_bias_vector_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)     2638 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)    13283 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear.h
--rw-r--r--   0 root         (0) root         (0)    18623 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear_direct_conv.h
--rw-r--r--   0 root         (0) root         (0)    27922 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    47789 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op_sm80.h
--rw-r--r--   0 root         (0) root         (0)     2616 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)    16510 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear.h
--rw-r--r--   0 root         (0) root         (0)    15486 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear_2dthreadtile.h
--rw-r--r--   0 root         (0) root         (0)    36050 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    43663 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op_sm70.h
--rw-r--r--   0 root         (0) root         (0)     5226 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/vector_iterator.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:54.260987 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/warp/
--rw-r--r--   0 root         (0) root         (0)     8828 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/warp/vector_fragment_iterator.h
--rw-r--r--   0 root         (0) root         (0)     8166 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/uint128.h
--rw-r--r--   0 root         (0) root         (0)     3359 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/wmma_array.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.407931 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:54.282603 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:54.317731 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/common/
--rw-r--r--   0 root         (0) root         (0)     4273 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/common/cutlass_unit_test.h
--rw-r--r--   0 root         (0) root         (0)     4341 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/common/filter_architecture.cpp
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.423378 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:55.312395 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/
--rw-r--r--   0 root         (0) root         (0)    21797 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/cache_testbed_output.h
--rw-r--r--   0 root         (0) root         (0)     5344 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     5443 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    11470 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5239 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     9110 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     8485 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5243 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5378 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12054 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_few_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9603 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_fixed_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5267 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     5357 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5089 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
--rw-r--r--   0 root         (0) root         (0)    13690 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5390 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5191 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    11136 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     5291 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3551 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     5157 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu
--rwxr-xr-x   0 root         (0) root         (0)     8278 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_qf32nhwc_qf32nhwc_qf32nhwc_simt_f32_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    20555 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    20647 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5155 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     5239 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    26114 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    26210 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5111 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     5194 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5738 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5439 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7363 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     3984 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_reduction_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    39452 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_problems.h
--rw-r--r--   0 root         (0) root         (0)    14471 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4662 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    26224 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_testbed.h
--rw-r--r--   0 root         (0) root         (0)    21174 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_testbed_interleaved.h
--rw-r--r--   0 root         (0) root         (0)     5179 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     5358 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5264 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3615 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7591 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    10514 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5157 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5772 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    23460 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_with_broadcast_testbed.h
--rw-r--r--   0 root         (0) root         (0)    21512 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_with_reduction_testbed.h
--rw-r--r--   0 root         (0) root         (0)     5135 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5347 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3736 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     6560 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5257 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12276 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_problems.h
--rw-r--r--   0 root         (0) root         (0)    21643 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_testbed.h
--rw-r--r--   0 root         (0) root         (0)     3622 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     6560 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5256 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    17700 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_direct_conv_testbed.h
--rw-r--r--   0 root         (0) root         (0)    18437 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
--rw-r--r--   0 root         (0) root         (0)    22194 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_fixed_stride_dilation_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
--rw-r--r--   0 root         (0) root         (0)     9383 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
--rw-r--r--   0 root         (0) root         (0)    16100 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/group_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:55.548003 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/
--rw-r--r--   0 root         (0) root         (0)     7365 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/array.cu
--rw-r--r--   0 root         (0) root         (0)     7353 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/bfloat16.cu
--rw-r--r--   0 root         (0) root         (0)     6981 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/complex.cu
--rw-r--r--   0 root         (0) root         (0)     4009 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/float8.cu
--rw-r--r--   0 root         (0) root         (0)    13001 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/functional.cu
--rw-r--r--   0 root         (0) root         (0)     3553 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/half.cu
--rw-r--r--   0 root         (0) root         (0)     5295 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/matrix.cu
--rw-r--r--   0 root         (0) root         (0)     8592 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/matrix_coord.cu
--rw-r--r--   0 root         (0) root         (0)    11508 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/numeric_conversion.cu
--rw-r--r--   0 root         (0) root         (0)     8148 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/predicate_vector.cu
--rw-r--r--   0 root         (0) root         (0)     5777 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/quaternion.cu
--rw-r--r--   0 root         (0) root         (0)     6746 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/tensor_ref.cu
--rw-r--r--   0 root         (0) root         (0)     8885 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/tensor_view.cu
--rw-r--r--   0 root         (0) root         (0)     2050 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/test_unit_core.cpp
--rw-r--r--   0 root         (0) root         (0)     7088 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/tfloat32.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.446234 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:55.601470 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/thread/
--rw-r--r--   0 root         (0) root         (0)    15818 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/thread/activation.cu
--rw-r--r--   0 root         (0) root         (0)     6534 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination.cu
--rw-r--r--   0 root         (0) root         (0)     9964 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination_planar_complex.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:55.811010 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/
--rw-r--r--   0 root         (0) root         (0)    13824 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_planar_complex.cu
--rw-r--r--   0 root         (0) root         (0)    27176 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt.cu
--rw-r--r--   0 root         (0) root         (0)    12061 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm60.cu
--rw-r--r--   0 root         (0) root         (0)    25275 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm61.cu
--rw-r--r--   0 root         (0) root         (0)    84612 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_tensor_op.cu
--rw-r--r--   0 root         (0) root         (0)    70486 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_volta_tensor_op.cu
--rw-r--r--   0 root         (0) root         (0)    25293 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_tensor_op.cu
--rw-r--r--   0 root         (0) root         (0)    13012 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_testbed.h
--rw-r--r--   0 root         (0) root         (0)     7743 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_wmma_tensor_op_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    19178 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/output_tile_threadmap.cu
--rw-r--r--   0 root         (0) root         (0)    28433 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/predicated_tile_iterator.cu
--rw-r--r--   0 root         (0) root         (0)    11038 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/testbed.h
--rw-r--r--   0 root         (0) root         (0)    11734 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/testbed_planar_complex.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:55.860833 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/warp/
--rw-r--r--   0 root         (0) root         (0)     6783 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_tensor_op.cu
--rw-r--r--   0 root         (0) root         (0)     7275 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_volta_tensor_op.cu
--rw-r--r--   0 root         (0) root         (0)     6616 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_wmma_tensor_op.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.481410 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:00.632117 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/
--rw-r--r--   0 root         (0) root         (0)    10189 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    17899 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8933 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_wmma_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    10164 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    17984 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8915 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_wmma_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16447 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_bf16n_bf16n_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    16575 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_bf16t_bf16t_bf16t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8318 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf32n_cf32t_cf32t_tensor_op_tf32_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8317 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf32t_cf32n_cf32t_tensor_op_tf32_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6714 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6735 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     7895 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7918 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     6516 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6537 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     9016 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9041 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     4628 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_direct_store_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6165 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6124 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     9634 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16357 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13189 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8845 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_volta_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    13583 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    13464 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     9571 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16239 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6140 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     9544 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16417 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13075 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8775 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_volta_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    11470 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6156 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6116 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     3528 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     3539 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7965 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16470 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13273 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3648 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8608 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_volta_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    13518 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     3645 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6096 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7845 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    18135 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13008 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8505 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_volta_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    11497 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    11090 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_singlestage_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6156 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6116 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    11066 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_singlestage_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    17114 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_broadcast_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3528 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     3540 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7964 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16457 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13266 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8933 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_volta_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    13551 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    13540 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6130 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     8160 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_singlestage_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7847 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16131 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13014 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8754 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_volta_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    11497 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6147 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6107 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    13518 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    13398 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7845 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16149 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6119 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7827 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16101 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9526 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7898 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_volta_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    11470 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     3584 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_bf16_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3473 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12967 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12931 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32t_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12930 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32t_f32n_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12895 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32t_f32t_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8349 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7288 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     8348 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7279 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)    10240 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_grouped_scheduler_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    26146 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    11339 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7346 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    12397 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6859 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     7239 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8121 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16882 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8407 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_wmma_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     8103 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    17111 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12637 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8388 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_wmma_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    10044 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    17544 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    10020 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    17544 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9588 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    11288 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7977 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16531 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5693 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_wmma_tensor_op_s32_sm72.cu
--rw-r--r--   0 root         (0) root         (0)     7959 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16691 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12408 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6864 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_wmma_tensor_op_s32_sm72.cu
--rw-r--r--   0 root         (0) root         (0)     7744 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16531 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6675 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_wmma_tensor_op_s32_sm72.cu
--rw-r--r--   0 root         (0) root         (0)     7752 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16484 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6663 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_wmma_tensor_op_s32_sm72.cu
--rw-r--r--   0 root         (0) root         (0)     4663 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_serial_tensor_op_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     4945 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_simt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     6616 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    10581 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16950 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32n_tf32n_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    16902 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32n_tf32t_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    15131 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32t_tf32n_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    16855 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32t_tf32t_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6854 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_u8t_u8n_s32t_wmma_tensor_op_s32_sm72.cu
--rw-r--r--   0 root         (0) root         (0)     6686 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf32n_cf32n_cf32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6755 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6687 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4726 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32n_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     4718 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32t_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16715 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_broadcast_f16n_f16n_f16n_tensorop_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    12841 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_reduction_f16n_f16n_f16n_tensorop_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     4544 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_reduction_f16t_f16n_f16n_tensorop_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13157 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemv.cu
--rw-r--r--   0 root         (0) root         (0)     6028 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6031 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6064 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6067 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4897 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64_cf64_cf64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     6088 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6037 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6040 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_rs_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5382 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5406 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5390 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64_cf64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)    13055 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64h_cf64n_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13027 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5388 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6939 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7677 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7725 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3842 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf64_cf64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     6396 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf64h_cf64n_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    10023 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/multistage_testbed.h
--rw-r--r--   0 root         (0) root         (0)     9189 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/multistage_testbed_interleaved.h
--rw-r--r--   0 root         (0) root         (0)    11186 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/rank_2k_grouped_scheduler_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    46795 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    54085 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     8318 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    46687 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     8411 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    46578 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    40533 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_nn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    47656 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_nt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    40441 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    40354 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_tt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     3513 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_f8gemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    89517 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_nn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    89304 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_nt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    89304 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    89091 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_tt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    69175 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_nn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    71438 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_nt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    67796 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    70056 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_tt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     7156 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61.cu
--rw-r--r--   0 root         (0) root         (0)     6067 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_perf.cu
--rw-r--r--   0 root         (0) root         (0)     9063 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_sliced_k.cu
--rw-r--r--   0 root         (0) root         (0)    35894 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_nn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    35813 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_nt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    35813 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    35732 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_tt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    70872 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    73136 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     8870 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    69488 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     8865 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    71755 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    33231 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_nn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    33156 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_nt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    33156 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    33081 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_tt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     5923 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5926 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5959 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5962 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4827 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64_cf64_cf64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     5983 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5932 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5935 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_rs_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    15203 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8623 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    15104 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4765 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64_f64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     8103 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8108 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8088 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8093 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8073 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8078 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8058 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8063 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    15071 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8551 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    14972 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32t_f32t_tensor_op_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5362 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5386 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5356 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5380 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5367 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64_cf64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)    12952 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5368 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7208 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5362 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7199 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64n_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7190 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64t_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4794 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f32n_f32n_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4783 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f32t_f32n_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4728 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64_f64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)    19145 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7991 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    11015 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7976 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12342 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7961 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12321 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64t_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4786 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_tf32n_f32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4775 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_tf32t_f32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4993 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5017 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4987 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5011 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5012 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64_cf64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     4996 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64n_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3793 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4990 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    16083 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f32n_f32t_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    16041 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f32t_f32t_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4518 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64_f64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     7451 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64n_f64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9401 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64t_f64n_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    16027 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_tf32n_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    15985 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_tf32t_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    20333 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed.h
--rw-r--r--   0 root         (0) root         (0)     8136 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_complex.h
--rw-r--r--   0 root         (0) root         (0)    20590 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_gemm_with_broadcast.h
--rw-r--r--   0 root         (0) root         (0)    19346 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_gemm_with_reduction.h
--rw-r--r--   0 root         (0) root         (0)    16502 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped.h
--rw-r--r--   0 root         (0) root         (0)    16562 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k.h
--rw-r--r--   0 root         (0) root         (0)    17002 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k_scheduler.h
--rw-r--r--   0 root         (0) root         (0)    14698 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_scheduler.h
--rw-r--r--   0 root         (0) root         (0)    10130 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_interleaved.h
--rw-r--r--   0 root         (0) root         (0)     9481 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)    20761 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_rank2k_universal.h
--rw-r--r--   0 root         (0) root         (0)    15562 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_rank_k_universal.h
--rw-r--r--   0 root         (0) root         (0)     8639 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_sanity.h
--rw-r--r--   0 root         (0) root         (0)    15769 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_sparse.h
--rw-r--r--   0 root         (0) root         (0)     6124 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_splitk.h
--rw-r--r--   0 root         (0) root         (0)    19861 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_symm_universal.h
--rw-r--r--   0 root         (0) root         (0)    20200 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_trmm_universal.h
--rw-r--r--   0 root         (0) root         (0)    17311 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_universal.h
--rw-r--r--   0 root         (0) root         (0)     2626 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_utils.h
--rw-r--r--   0 root         (0) root         (0)     9916 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9988 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4977 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64_cf64_cf64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     4992 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9762 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    15614 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8733 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    14089 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    14444 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32t_tensor_op_fast_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4596 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64_f64_f64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)    12798 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12809 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12764 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64t_f64t_tensor_op_f64_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12768 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12779 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    15504 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8673 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13989 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32n_tensor_op_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    14344 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32t_tensor_op_f32_ls_sm80.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:00.668100 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/kernel/
--rwxr-xr-x   0 root         (0) root         (0)    46470 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/kernel/batched_gemv.cu
--rwxr-xr-x   0 root         (0) root         (0)    14362 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/kernel/testbed_gemv.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:00.731975 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/thread/
--rw-r--r--   0 root         (0) root         (0)     4847 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    12503 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm60.cu
--rw-r--r--   0 root         (0) root         (0)     3109 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm61.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:00.767570 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/
--rw-r--r--   0 root         (0) root         (0)     5198 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/gemm_sm60_host.cu
--rw-r--r--   0 root         (0) root         (0)     7161 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/testbed_host.h
--rw-r--r--   0 root         (0) root         (0)     7124 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/thread/testbed.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:01.115895 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/
--rw-r--r--   0 root         (0) root         (0)    25036 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/batched_gemv.cu
--rw-r--r--   0 root         (0) root         (0)     4345 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/epilogue_workspace.cu
--rw-r--r--   0 root         (0) root         (0)   135045 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage.cu
--rw-r--r--   0 root         (0) root         (0)     4644 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_slicedk.cu
--rw-r--r--   0 root         (0) root         (0)    94442 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse.cu
--rw-r--r--   0 root         (0) root         (0)    17109 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse_testbed.h
--rw-r--r--   0 root         (0) root         (0)    13131 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed.h
--rw-r--r--   0 root         (0) root         (0)    14539 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed_slicedk.h
--rw-r--r--   0 root         (0) root         (0)    49052 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_simt.cu
--rw-r--r--   0 root         (0) root         (0)     8407 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_slicedk.cu
--rw-r--r--   0 root         (0) root         (0)    18705 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    78122 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    21051 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13413 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_testbed.h
--rw-r--r--   0 root         (0) root         (0)    14239 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_testbed_slicedk.h
--rw-r--r--   0 root         (0) root         (0)    29772 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    12395 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     3502 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_planar_complex_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12138 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_planar_complex_testbed.h
--rw-r--r--   0 root         (0) root         (0)    16308 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_singlestage_wmma_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    12502 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_singlestage_wmma_sm75.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:01.362003 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/
--rw-r--r--   0 root         (0) root         (0)    22128 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_complex_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    10904 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_complex_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     9873 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_gaussian_complex_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    18220 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     4920 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm60.cu
--rw-r--r--   0 root         (0) root         (0)     6291 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm61.cu
--rw-r--r--   0 root         (0) root         (0)     9297 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    37942 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    81659 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9077 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm90.cu
--rw-r--r--   0 root         (0) root         (0)    48928 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    45327 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/testbed.h
--rw-r--r--   0 root         (0) root         (0)    25780 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7544 2023-04-15 23:28:02.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm72.cu
--rw-r--r--   0 root         (0) root         (0)     6487 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm75.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:01.412411 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/layout/
--rw-r--r--   0 root         (0) root         (0)     5788 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/layout/matrix.cu
--rw-r--r--   0 root         (0) root         (0)     5984 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/layout/tensor.cu
--rw-r--r--   0 root         (0) root         (0)     7081 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/layout/tensor_nhwc.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.515245 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.500991 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/cutlass/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:01.439025 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/cutlass/nvrtc/
--rw-r--r--   0 root         (0) root         (0)     2096 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/cutlass/nvrtc/environment.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.510849 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/kernel/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:01.462366 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/kernel/thread/
--rw-r--r--   0 root         (0) root         (0)     2915 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/kernel/thread/testbed_kernel.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:01.496581 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/stdlib/
--rw-rw-r--   0 root         (0) root         (0)        0 2022-06-02 16:47:41.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/stdlib/assert.h
--rw-r--r--   0 root         (0) root         (0)     4250 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/stdlib/stdint.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:01.531978 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/
--rw-r--r--   0 root         (0) root         (0)     5727 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/gemm_nvrtc.cu
--rw-r--r--   0 root         (0) root         (0)    10328 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/testbed.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.538331 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:01.571043 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/device/
--rw-r--r--   0 root         (0) root         (0)    14684 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_contiguous.cu
--rw-r--r--   0 root         (0) root         (0)    15609 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_strided.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:01.606550 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/kernel/
--rw-r--r--   0 root         (0) root         (0)    11350 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk.cu
--rw-r--r--   0 root         (0) root         (0)     2228 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk_testbed.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:01.642085 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/thread/
--rw-r--r--   0 root         (0) root         (0)     3110 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/thread/reduction_thread.cu
--rw-r--r--   0 root         (0) root         (0)     6657 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/thread/testbed.h
--rw-r--r--   0 root         (0) root         (0)     2047 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/test_unit.cpp
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.548167 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/transform/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:01.680652 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/transform/threadblock/
--rw-r--r--   0 root         (0) root         (0)    25527 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/transform/threadblock/predicated_tile_iterator.cu
--rw-r--r--   0 root         (0) root         (0)     9501 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/transform/threadblock/regular_tile_iterator_tensor_op.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:01.716336 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/util/
--rw-r--r--   0 root         (0) root         (0)     2663 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/util/cutlass_test_levels.cu
--rw-r--r--   0 root         (0) root         (0)     7474 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/util/tensor_reduce.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.675409 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.649152 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.575322 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.580060 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:01.844772 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/
--rw-r--r--   0 root         (0) root         (0)     4118 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/arch_mappings.h
--rw-r--r--   0 root         (0) root         (0)    16013 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/handle.h
--rw-r--r--   0 root         (0) root         (0)    38340 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/library.h
--rw-r--r--   0 root         (0) root         (0)     4070 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/manifest.h
--rw-r--r--   0 root         (0) root         (0)    17934 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/operation_table.h
--rw-r--r--   0 root         (0) root         (0)     2724 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/singleton.h
--rw-r--r--   0 root         (0) root         (0)     7904 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/util.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.591539 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.596265 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.600930 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:01.906624 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/
--rw-r--r--   0 root         (0) root         (0)     2788 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/compiler.h
--rw-r--r--   0 root         (0) root         (0)     6223 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/cutlass.cpp
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:01.993954 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/
--rw-r--r--   0 root         (0) root         (0)     2851 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/arch.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:02.043948 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/
--rw-r--r--   0 root         (0) root         (0)     5897 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/conv_problem_size.h
--rw-r--r--   0 root         (0) root         (0)     4763 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/convolution.h
--rw-r--r--   0 root         (0) root         (0)     2650 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/host.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:02.076977 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/
--rw-r--r--   0 root         (0) root         (0)     6844 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_generic.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:02.280399 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/
--rw-r--r--   0 root         (0) root         (0)     3047 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/binary_ops.h
--rw-r--r--   0 root         (0) root         (0)     6134 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/unary_ops.h
--rw-r--r--   0 root         (0) root         (0)     4702 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_accumulator.h
--rw-r--r--   0 root         (0) root         (0)     8467 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_binary.h
--rw-r--r--   0 root         (0) root         (0)     8815 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_broadcast.h
--rw-r--r--   0 root         (0) root         (0)    13049 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_reduction.h
--rw-r--r--   0 root         (0) root         (0)     9493 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_linear_combination.h
--rw-r--r--   0 root         (0) root         (0)     9070 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_broadcast.h
--rw-r--r--   0 root         (0) root         (0)    12029 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_reduction.h
--rw-r--r--   0 root         (0) root         (0)     6177 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_input.h
--rw-r--r--   0 root         (0) root         (0)     8017 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_output.h
--rw-r--r--   0 root         (0) root         (0)     7235 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_unary.h
--rw-r--r--   0 root         (0) root         (0)    16970 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_with_layernorm.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:02.330936 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/
--rw-r--r--   0 root         (0) root         (0)     3673 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/gemm.h
--rw-r--r--   0 root         (0) root         (0)    22378 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/gemm_universal_with_visitor.h
--rw-r--r--   0 root         (0) root         (0)     2328 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/host.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:02.380843 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/
--rw-r--r--   0 root         (0) root         (0)     2115 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/layout.h
--rw-r--r--   0 root         (0) root         (0)     4337 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/matrix.h
--rw-r--r--   0 root         (0) root         (0)     3694 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/tensor.h
--rw-r--r--   0 root         (0) root         (0)     8624 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/swizzling.h
--rw-r--r--   0 root         (0) root         (0)     3902 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_coord.h
--rw-r--r--   0 root         (0) root         (0)     5563 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_ref_view.h
--rw-r--r--   0 root         (0) root         (0)     4855 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/types.h
--rw-r--r--   0 root         (0) root         (0)      811 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/library.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.640414 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:02.436543 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/
--rw-r--r--   0 root         (0) root         (0)     2651 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/conv_problems.h
--rw-r--r--   0 root         (0) root         (0)     2253 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/convolution.h
--rw-r--r--   0 root         (0) root         (0)     8826 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/host.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:02.472908 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/
--rw-r--r--   0 root         (0) root         (0)     2139 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/gemm.h
--rw-r--r--   0 root         (0) root         (0)    18930 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/host.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:02.673188 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/
--rw-r--r--   0 root         (0) root         (0)    22377 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/conv2d_operation.h
--rw-r--r--   0 root         (0) root         (0)    13851 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/conv3d_operation.h
--rw-r--r--   0 root         (0) root         (0)    42129 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/gemm_operation.h
--rw-r--r--   0 root         (0) root         (0)    35709 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/handle.cu
--rw-r--r--   0 root         (0) root         (0)    12616 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/library_internal.h
--rw-r--r--   0 root         (0) root         (0)     3782 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/manifest.cpp
--rw-r--r--   0 root         (0) root         (0)     5468 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/operation_table.cu
--rw-r--r--   0 root         (0) root         (0)    12873 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/rank_2k_operation.h
--rw-r--r--   0 root         (0) root         (0)    11367 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/rank_k_operation.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:02.723985 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reduction/
--rw-r--r--   0 root         (0) root         (0)     3190 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reduction/init_reduction_operations.cu
--rw-r--r--   0 root         (0) root         (0)     6367 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reduction/reduction_device.cu
--rw-r--r--   0 root         (0) root         (0)    10270 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reduction/reduction_operation.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:02.828175 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reference/
--rw-r--r--   0 root         (0) root         (0)     6746 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reference/conv2d.cu
--rw-r--r--   0 root         (0) root         (0)     6286 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reference/conv3d.cu
--rw-r--r--   0 root         (0) root         (0)    17191 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reference/conv_reference_operation.h
--rw-r--r--   0 root         (0) root         (0)     7199 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reference/gemm.cu
--rw-r--r--   0 root         (0) root         (0)    14732 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reference/gemm_reference_operation.h
--rw-r--r--   0 root         (0) root         (0)     2857 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reference/initialize_reference_operations.cu
--rw-r--r--   0 root         (0) root         (0)     2669 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/singleton.cu
--rw-r--r--   0 root         (0) root         (0)    13134 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/symm_operation.h
--rw-r--r--   0 root         (0) root         (0)    11698 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/trmm_operation.h
--rw-r--r--   0 root         (0) root         (0)    43704 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/util.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.668611 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:03.504174 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/
--rw-r--r--   0 root         (0) root         (0)    54128 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/conv2d_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)    18170 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/conv2d_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)    48659 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/conv3d_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)    16043 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/conv3d_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)    36462 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/cublas_helpers.cu
--rw-r--r--   0 root         (0) root         (0)    10627 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/cublas_helpers.h
--rw-r--r--   0 root         (0) root         (0)    17049 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/cudnn_helpers.cpp
--rw-r--r--   0 root         (0) root         (0)    20433 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/cudnn_helpers.h
--rw-r--r--   0 root         (0) root         (0)     7233 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/cutlass_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     3233 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/cutlass_profiler.h
--rw-r--r--   0 root         (0) root         (0)     2453 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/debug.h
--rw-r--r--   0 root         (0) root         (0)    53643 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/device_allocation.cu
--rw-r--r--   0 root         (0) root         (0)     7217 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/device_allocation.h
--rw-r--r--   0 root         (0) root         (0)     6841 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/device_context.cu
--rw-r--r--   0 root         (0) root         (0)     4300 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/device_context.h
--rw-r--r--   0 root         (0) root         (0)     8296 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/enumerated_types.cpp
--rw-r--r--   0 root         (0) root         (0)     6421 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/enumerated_types.h
--rw-r--r--   0 root         (0) root         (0)    41919 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/gemm_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     8544 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/gemm_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)     3874 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/gpu_timer.cpp
--rw-r--r--   0 root         (0) root         (0)     2724 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/gpu_timer.h
--rw-r--r--   0 root         (0) root         (0)     2340 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/main.cpp
--rw-r--r--   0 root         (0) root         (0)    20944 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     7876 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)    27172 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/options.cu
--rw-r--r--   0 root         (0) root         (0)     8773 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/options.h
--rw-r--r--   0 root         (0) root         (0)    14192 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/performance_report.cpp
--rw-r--r--   0 root         (0) root         (0)     4337 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/performance_report.h
--rw-r--r--   0 root         (0) root         (0)     2494 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/performance_result.cu
--rw-r--r--   0 root         (0) root         (0)     3941 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/performance_result.h
--rw-r--r--   0 root         (0) root         (0)    37487 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/problem_space.cpp
--rw-r--r--   0 root         (0) root         (0)    27747 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/problem_space.h
--rw-r--r--   0 root         (0) root         (0)    25014 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/rank_2k_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     6891 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/rank_2k_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)    24253 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/rank_k_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     6830 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/rank_k_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)     5452 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/reduction_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)    20688 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     6471 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)    26610 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/symm_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     6933 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/symm_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)    24431 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/trmm_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     6599 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/trmm_operation_profiler.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.680166 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.684853 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.689468 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:03.828015 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/
--rw-r--r--   0 root         (0) root         (0)     9774 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/command_line.h
--rw-r--r--   0 root         (0) root         (0)     5104 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/debug.h
--rw-r--r--   0 root         (0) root         (0)     5953 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_dump.h
--rw-r--r--   0 root         (0) root         (0)    17695 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_groupnorm.h
--rw-r--r--   0 root         (0) root         (0)    20880 2023-04-16 04:29:29.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_layernorm.h
--rw-r--r--   0 root         (0) root         (0)    10561 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_memory.h
--rw-r--r--   0 root         (0) root         (0)     5219 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nchw_to_nhwc.h
--rw-r--r--   0 root         (0) root         (0)    11067 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_padding.h
--rw-r--r--   0 root         (0) root         (0)    18653 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_pooling.h
--rw-r--r--   0 root         (0) root         (0)     5214 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_to_nchw.h
--rw-r--r--   0 root         (0) root         (0)     4007 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_utils.h
--rw-r--r--   0 root         (0) root         (0)     4597 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/distribution.h
--rw-r--r--   0 root         (0) root         (0)     2674 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/exceptions.h
--rw-r--r--   0 root         (0) root         (0)     4821 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_reorder.h
--rw-r--r--   0 root         (0) root         (0)    16745 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_tensor.h
--rw-r--r--   0 root         (0) root         (0)    20354 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_tensor_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)     5890 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_uncompress.h
--rw-r--r--   0 root         (0) root         (0)     1962 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/index_sequence.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:18:44.720217 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:03.866755 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/
--rw-r--r--   0 root         (0) root         (0)     4606 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/inner_product.h
--rw-r--r--   0 root         (0) root         (0)     3527 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/linear_to_coordinate.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:04.041005 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/
--rw-r--r--   0 root         (0) root         (0)    48350 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/convolution.h
--rw-r--r--   0 root         (0) root         (0)    14296 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm.h
--rw-r--r--   0 root         (0) root         (0)    10524 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm_complex.h
--rw-r--r--   0 root         (0) root         (0)     9652 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm_planar_complex.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:04.091718 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/
--rw-r--r--   0 root         (0) root         (0)     5381 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/gemm.h
--rw-r--r--   0 root         (0) root         (0)     6198 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_elementwise.h
--rw-r--r--   0 root         (0) root         (0)     5126 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_foreach.h
--rw-r--r--   0 root         (0) root         (0)    11615 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/rank_2k_complex.h
--rw-r--r--   0 root         (0) root         (0)     7278 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_compare.h
--rw-r--r--   0 root         (0) root         (0)    46444 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_fill.h
--rw-r--r--   0 root         (0) root         (0)     5293 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_foreach.h
--rw-r--r--   0 root         (0) root         (0)    15964 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_reduce.h
--rw-r--r--   0 root         (0) root         (0)     4589 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_relu.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:04.111833 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/thread/
--rw-r--r--   0 root         (0) root         (0)     5872 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/thread/gemm.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:04.411078 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/
--rw-r--r--   0 root         (0) root         (0)    28439 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/convolution.h
--rw-r--r--   0 root         (0) root         (0)     2766 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/error_metrics.h
--rw-r--r--   0 root         (0) root         (0)    17163 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm.h
--rw-r--r--   0 root         (0) root         (0)     7097 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm_complex.h
--rw-r--r--   0 root         (0) root         (0)     7708 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)     9441 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k.h
--rw-r--r--   0 root         (0) root         (0)    11444 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k_complex.h
--rw-r--r--   0 root         (0) root         (0)     8148 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_k_complex.h
--rw-r--r--   0 root         (0) root         (0)    10509 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/symm.h
--rw-r--r--   0 root         (0) root         (0)    12296 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/symm_complex.h
--rw-r--r--   0 root         (0) root         (0)     8440 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_compare.h
--rw-r--r--   0 root         (0) root         (0)     8317 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_copy.h
--rw-r--r--   0 root         (0) root         (0)     9027 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_elementwise.h
--rw-r--r--   0 root         (0) root         (0)    43961 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_fill.h
--rw-r--r--   0 root         (0) root         (0)     4756 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_foreach.h
--rw-r--r--   0 root         (0) root         (0)     2133 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_norm.h
--rw-r--r--   0 root         (0) root         (0)     6111 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_reduce.h
--rw-r--r--   0 root         (0) root         (0)     7670 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/trmm.h
--rw-r--r--   0 root         (0) root         (0)     9874 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/trmm_complex.h
--rw-r--r--   0 root         (0) root         (0)     8285 2023-04-15 23:28:03.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/tensor_view_io.h
--rw-r--r--   0 root         (0) root         (0)     8809 2023-04-12 22:58:14.000000 flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/type_traits.h
--rw-r--r--   0 root         (0) root         (0)    33267 2023-04-26 16:18:56.000000 flash_attn-1.0.9/csrc/flash_attn/fmha_api.cpp
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:05.745953 flash_attn-1.0.9/csrc/flash_attn/src/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:05.858058 flash_attn-1.0.9/csrc/flash_attn/src/fmha/
--rw-r--r--   0 root         (0) root         (0)    17999 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha/gemm.h
--rw-r--r--   0 root         (0) root         (0)    22872 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha/gmem_tile.h
--rw-r--r--   0 root         (0) root         (0)     5997 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha/kernel_traits.h
--rw-r--r--   0 root         (0) root         (0)     4362 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha/mask.h
--rw-r--r--   0 root         (0) root         (0)    74010 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha/smem_tile.h
--rw-r--r--   0 root         (0) root         (0)    25514 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha/softmax.h
--rw-r--r--   0 root         (0) root         (0)    41059 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha/utils.h
--rw-r--r--   0 root         (0) root         (0)     7237 2023-04-16 04:28:21.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha.h
--rw-r--r--   0 root         (0) root         (0)     4118 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_block_dgrad_fp16_kernel_loop.sm80.cu
--rw-r--r--   0 root         (0) root         (0)    33506 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_block_dgrad_kernel_1xN_loop.h
--rw-r--r--   0 root         (0) root         (0)     5292 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_block_fprop_fp16_kernel.sm80.cu
--rw-r--r--   0 root         (0) root         (0)    23207 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_block_fprop_kernel_1xN.h
--rw-r--r--   0 root         (0) root         (0)     2502 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_blockmask.h
--rw-r--r--   0 root         (0) root         (0)      465 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_bwd_hdim128.cu
--rw-r--r--   0 root         (0) root         (0)      727 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_bwd_hdim32.cu
--rw-r--r--   0 root         (0) root         (0)     1713 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_bwd_hdim64.cu
--rw-r--r--   0 root         (0) root         (0)     6453 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_bwd_launch_template.h
--rw-r--r--   0 root         (0) root         (0)    37168 2023-04-16 04:28:21.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_dgrad_kernel_1xN_loop.h
--rw-r--r--   0 root         (0) root         (0)    31042 2023-04-16 04:28:21.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_fprop_kernel_1xN.h
--rw-r--r--   0 root         (0) root         (0)      445 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_fwd_hdim128.cu
--rw-r--r--   0 root         (0) root         (0)      724 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_fwd_hdim32.cu
--rw-r--r--   0 root         (0) root         (0)      725 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_fwd_hdim64.cu
--rw-r--r--   0 root         (0) root         (0)     4393 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_fwd_launch_template.h
--rw-r--r--   0 root         (0) root         (0)     3104 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_kernel.h
--rw-r--r--   0 root         (0) root         (0)     4892 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/fmha_utils.h
--rw-r--r--   0 root         (0) root         (0)     5462 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/philox.cuh
--rw-r--r--   0 root         (0) root         (0)     1686 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/flash_attn/src/static_switch.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:05.881065 flash_attn-1.0.9/csrc/flash_gen/
--rw-r--r--   0 root         (0) root         (0)     7018 2022-11-21 06:35:03.000000 flash_attn-1.0.9/csrc/flash_gen/decoder_masked_multihead_attention.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:05.977571 flash_attn-1.0.9/csrc/ft_attention/
--rw-r--r--   0 root         (0) root         (0)     8253 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/ft_attention/cuda_bf16_fallbacks.cuh
--rw-r--r--   0 root         (0) root         (0)      867 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/ft_attention/cuda_bf16_wrapper.h
--rw-r--r--   0 root         (0) root         (0)     7069 2023-06-06 06:13:59.000000 flash_attn-1.0.9/csrc/ft_attention/decoder_masked_multihead_attention.cu
--rw-r--r--   0 root         (0) root         (0)     7627 2023-07-02 20:11:51.000000 flash_attn-1.0.9/csrc/ft_attention/decoder_masked_multihead_attention.h
--rw-r--r--   0 root         (0) root         (0)    64946 2023-07-03 16:25:44.000000 flash_attn-1.0.9/csrc/ft_attention/decoder_masked_multihead_attention_utils.h
--rw-r--r--   0 root         (0) root         (0)    10119 2023-07-06 22:24:25.000000 flash_attn-1.0.9/csrc/ft_attention/ft_attention.cpp
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:06.013082 flash_attn-1.0.9/csrc/fused_dense_lib/
--rw-r--r--   0 root         (0) root         (0)    10179 2023-05-30 21:13:46.000000 flash_attn-1.0.9/csrc/fused_dense_lib/fused_dense.cpp
--rw-r--r--   0 root         (0) root         (0)    24690 2023-05-30 21:14:57.000000 flash_attn-1.0.9/csrc/fused_dense_lib/fused_dense_cuda.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:06.110020 flash_attn-1.0.9/csrc/fused_softmax/
--rw-r--r--   0 root         (0) root         (0)     5037 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/fused_softmax/fused_softmax.cpp
--rw-r--r--   0 root         (0) root         (0)    23616 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/fused_softmax/scaled_masked_softmax.h
--rw-r--r--   0 root         (0) root         (0)     4209 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/fused_softmax/scaled_masked_softmax_cuda.cu
--rw-r--r--   0 root         (0) root         (0)    24659 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/fused_softmax/scaled_upper_triang_masked_softmax.h
--rw-r--r--   0 root         (0) root         (0)     3154 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/fused_softmax/scaled_upper_triang_masked_softmax_cuda.cu
--rw-r--r--   0 root         (0) root         (0)     1216 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/fused_softmax/type_shim.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:07.226441 flash_attn-1.0.9/csrc/layer_norm/
--rw-r--r--   0 root         (0) root         (0)     7248 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/layer_norm/ln.h
--rw-r--r--   0 root         (0) root         (0)    36418 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/layer_norm/ln_api.cpp
--rw-r--r--   0 root         (0) root         (0)      987 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_1024.cu
--rw-r--r--   0 root         (0) root         (0)      987 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_1280.cu
--rw-r--r--   0 root         (0) root         (0)      977 2023-04-16 00:48:36.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_1536.cu
--rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_2048.cu
--rw-r--r--   0 root         (0) root         (0)      977 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_256.cu
--rw-r--r--   0 root         (0) root         (0)      977 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_2560.cu
--rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_3072.cu
--rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_4096.cu
--rw-r--r--   0 root         (0) root         (0)      977 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_512.cu
--rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_5120.cu
--rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_6144.cu
--rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_7168.cu
--rw-r--r--   0 root         (0) root         (0)      977 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_768.cu
--rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_8192.cu
--rw-r--r--   0 root         (0) root         (0)    25647 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_kernels.cuh
--rw-r--r--   0 root         (0) root         (0)    19944 2023-01-19 07:34:02.000000 flash_attn-1.0.9/csrc/layer_norm/ln_bwd_semi_cuda_kernel_old.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_1024.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-01-22 09:05:55.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_10240.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-01-22 09:07:15.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_12288.cu
--rw-r--r--   0 root         (0) root         (0)      925 2022-12-05 08:45:58.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_128.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_1280.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_1536.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_2048.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_256.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_2560.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_3072.cu
--rw-r--r--   0 root         (0) root         (0)      925 2022-12-05 08:50:57.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_384.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_4096.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_512.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_5120.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_6144.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_7168.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_768.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_8192.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-01-22 08:41:06.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_9216.cu
--rw-r--r--   0 root         (0) root         (0)    18000 2022-12-06 21:18:58.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_cuda_kernel_old.cu
--rw-r--r--   0 root         (0) root         (0)    12721 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_fwd_kernels.cuh
--rw-r--r--   0 root         (0) root         (0)     6655 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_kernel_traits.h
--rw-r--r--   0 root         (0) root         (0)     1095 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_1024.cu
--rw-r--r--   0 root         (0) root         (0)     1095 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_1280.cu
--rw-r--r--   0 root         (0) root         (0)     1085 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_1536.cu
--rw-r--r--   0 root         (0) root         (0)     1084 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_2048.cu
--rw-r--r--   0 root         (0) root         (0)     1085 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_256.cu
--rw-r--r--   0 root         (0) root         (0)     1085 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_2560.cu
--rw-r--r--   0 root         (0) root         (0)     1084 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_3072.cu
--rw-r--r--   0 root         (0) root         (0)     1145 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_4096.cu
--rw-r--r--   0 root         (0) root         (0)     1085 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_512.cu
--rw-r--r--   0 root         (0) root         (0)     1145 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_5120.cu
--rw-r--r--   0 root         (0) root         (0)     1084 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_6144.cu
--rw-r--r--   0 root         (0) root         (0)     1084 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_7168.cu
--rw-r--r--   0 root         (0) root         (0)     1085 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_768.cu
--rw-r--r--   0 root         (0) root         (0)     1084 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_8192.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_1024.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_1280.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_1536.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_2048.cu
--rw-r--r--   0 root         (0) root         (0)     1032 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_256.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_2560.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_3072.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_4096.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_512.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_5120.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_6144.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_7168.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_768.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_8192.cu
--rw-r--r--   0 root         (0) root         (0)    24916 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_residual_bwd_kernels.cuh
--rw-r--r--   0 root         (0) root         (0)    12530 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_parallel_residual_fwd_kernels.cuh
--rw-r--r--   0 root         (0) root         (0)    29989 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/ln_utils.cuh
--rw-r--r--   0 root         (0) root         (0)     1278 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/layer_norm/static_switch.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:07.261442 flash_attn-1.0.9/csrc/rotary/
--rw-r--r--   0 root         (0) root         (0)     1806 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/rotary/rotary.cpp
--rw-r--r--   0 root         (0) root         (0)     1984 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/rotary/rotary_cuda.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:07.297073 flash_attn-1.0.9/csrc/xentropy/
--rw-r--r--   0 root         (0) root         (0)     2290 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/xentropy/interface.cpp
--rw-r--r--   0 root         (0) root         (0)    25783 2023-04-16 00:48:37.000000 flash_attn-1.0.9/csrc/xentropy/xentropy_kernel.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:07.497263 flash_attn-1.0.9/flash_attn/
--rw-rw-r--   0 root         (0) root         (0)       22 2023-07-17 10:16:17.000000 flash_attn-1.0.9/flash_attn/__init__.py
--rw-rw-r--   0 root         (0) root         (0)    20845 2022-10-31 02:25:05.000000 flash_attn-1.0.9/flash_attn/attention_kernl.py
--rw-r--r--   0 root         (0) root         (0)     5898 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/bert_padding.py
--rw-r--r--   0 root         (0) root         (0)     4722 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/flash_attention.py
--rw-r--r--   0 root         (0) root         (0)    20510 2023-07-17 10:13:51.000000 flash_attn-1.0.9/flash_attn/flash_attn_interface.py
--rw-r--r--   0 root         (0) root         (0)    38148 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/flash_attn_triton.py
--rw-r--r--   0 root         (0) root         (0)    10593 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/flash_attn_triton_og.py
--rw-r--r--   0 root         (0) root         (0)     8255 2022-11-18 03:30:00.000000 flash_attn-1.0.9/flash_attn/flash_attn_triton_single_query.py
--rw-r--r--   0 root         (0) root         (0)    37797 2023-03-17 09:16:10.000000 flash_attn-1.0.9/flash_attn/flash_attn_triton_tmp.py
--rw-r--r--   0 root         (0) root         (0)    10640 2023-03-12 08:48:14.000000 flash_attn-1.0.9/flash_attn/flash_attn_triton_tmp_og.py
--rw-r--r--   0 root         (0) root         (0)     6819 2022-06-26 00:59:43.000000 flash_attn-1.0.9/flash_attn/flash_blocksparse_attention.py
--rw-r--r--   0 root         (0) root         (0)     7036 2022-06-26 00:59:43.000000 flash_attn-1.0.9/flash_attn/flash_blocksparse_attn_interface.py
--rw-r--r--   0 root         (0) root         (0)     7902 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/fused_softmax.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:07.616086 flash_attn-1.0.9/flash_attn/layers/
--rw-r--r--   0 root         (0) root         (0)        0 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/layers/__init__.py
--rw-r--r--   0 root         (0) root         (0)     2039 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/layers/patch_embed.py
--rw-r--r--   0 root         (0) root         (0)    12738 2023-07-17 10:14:25.000000 flash_attn-1.0.9/flash_attn/layers/rotary.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:07.650704 flash_attn-1.0.9/flash_attn/losses/
--rw-r--r--   0 root         (0) root         (0)        0 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/losses/__init__.py
--rw-r--r--   0 root         (0) root         (0)     6697 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/losses/cross_entropy.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:07.776771 flash_attn-1.0.9/flash_attn/models/
--rw-r--r--   0 root         (0) root         (0)        0 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/models/__init__.py
--rw-r--r--   0 root         (0) root         (0)    26570 2023-04-18 20:33:07.000000 flash_attn-1.0.9/flash_attn/models/bert.py
--rw-r--r--   0 root         (0) root         (0)    38025 2023-06-02 19:10:34.000000 flash_attn-1.0.9/flash_attn/models/gpt.py
--rw-r--r--   0 root         (0) root         (0)     5025 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/models/gpt_neox.py
--rw-r--r--   0 root         (0) root         (0)     4365 2023-04-18 20:33:24.000000 flash_attn-1.0.9/flash_attn/models/gptj.py
--rw-r--r--   0 root         (0) root         (0)     5761 2023-04-19 04:11:30.000000 flash_attn-1.0.9/flash_attn/models/llama.py
--rw-r--r--   0 root         (0) root         (0)     5130 2023-04-18 20:33:17.000000 flash_attn-1.0.9/flash_attn/models/opt.py
--rw-r--r--   0 root         (0) root         (0)    13621 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/models/vit.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:07.857983 flash_attn-1.0.9/flash_attn/modules/
--rw-r--r--   0 root         (0) root         (0)        0 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/modules/__init__.py
--rw-r--r--   0 root         (0) root         (0)    16313 2023-07-17 10:18:12.000000 flash_attn-1.0.9/flash_attn/modules/block.py
--rw-r--r--   0 root         (0) root         (0)     8620 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/modules/embedding.py
--rw-r--r--   0 root         (0) root         (0)    34955 2023-07-17 10:18:12.000000 flash_attn-1.0.9/flash_attn/modules/mha.py
--rw-r--r--   0 root         (0) root         (0)     2221 2023-07-02 06:31:30.000000 flash_attn-1.0.9/flash_attn/modules/mlp.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:07.939536 flash_attn-1.0.9/flash_attn/ops/
--rw-r--r--   0 root         (0) root         (0)        0 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/ops/__init__.py
--rw-r--r--   0 root         (0) root         (0)     3002 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/ops/activations.py
--rw-r--r--   0 root         (0) root         (0)    26087 2023-04-18 10:32:08.000000 flash_attn-1.0.9/flash_attn/ops/fused_dense.py
--rw-r--r--   0 root         (0) root         (0)    19306 2023-07-04 21:52:07.000000 flash_attn-1.0.9/flash_attn/ops/layer_norm.py
--rw-r--r--   0 root         (0) root         (0)     3672 2023-04-18 22:26:53.000000 flash_attn-1.0.9/flash_attn/ops/rms_norm.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:08.020019 flash_attn-1.0.9/flash_attn/utils/
--rw-r--r--   0 root         (0) root         (0)        0 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/utils/__init__.py
--rw-r--r--   0 root         (0) root         (0)     5909 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/utils/benchmark.py
--rw-r--r--   0 root         (0) root         (0)     5545 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/utils/distributed.py
--rw-r--r--   0 root         (0) root         (0)    14105 2023-04-21 18:28:20.000000 flash_attn-1.0.9/flash_attn/utils/generation.py
--rw-r--r--   0 root         (0) root         (0)     1824 2023-04-16 00:48:37.000000 flash_attn-1.0.9/flash_attn/utils/pretrained.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 10:19:07.567049 flash_attn-1.0.9/flash_attn.egg-info/
--rw-rw-r--   0 root         (0) root         (0)    10197 2023-07-17 10:18:39.000000 flash_attn-1.0.9/flash_attn.egg-info/PKG-INFO
--rw-rw-r--   0 root         (0) root         (0)   105073 2023-07-17 10:18:43.000000 flash_attn-1.0.9/flash_attn.egg-info/SOURCES.txt
--rw-rw-r--   0 root         (0) root         (0)        1 2023-07-17 10:18:39.000000 flash_attn-1.0.9/flash_attn.egg-info/dependency_links.txt
--rw-rw-r--   0 root         (0) root         (0)       29 2023-07-17 10:18:39.000000 flash_attn-1.0.9/flash_attn.egg-info/requires.txt
--rw-rw-r--   0 root         (0) root         (0)       27 2023-07-17 10:18:39.000000 flash_attn-1.0.9/flash_attn.egg-info/top_level.txt
--rw-rw-r--   0 root         (0) root         (0)       38 2023-07-17 10:19:08.028764 flash_attn-1.0.9/setup.cfg
--rw-r--r--   0 root         (0) root         (0)     8044 2023-07-06 03:55:40.000000 flash_attn-1.0.9/setup.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:05.327620 flash_attn-2.0.0/
+-rw-r--r--   0 root         (0) root         (0)       29 2023-07-17 10:25:58.000000 flash_attn-2.0.0/AUTHORS
+-rw-r--r--   0 root         (0) root         (0)     1558 2022-09-09 19:08:03.000000 flash_attn-2.0.0/LICENSE
+-rw-r--r--   0 root         (0) root         (0)      251 2023-04-16 00:48:36.000000 flash_attn-2.0.0/MANIFEST.in
+-rw-rw-r--   0 root         (0) root         (0)      476 2023-07-17 12:48:05.324191 flash_attn-2.0.0/PKG-INFO
+-rw-r--r--   0 root         (0) root         (0)     8742 2023-07-17 12:34:39.000000 flash_attn-2.0.0/README.md
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.482451 flash_attn-2.0.0/csrc/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.451577 flash_attn-2.0.0/csrc/cutlass/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.505007 flash_attn-2.0.0/csrc/cutlass/cmake/
+-rw-rw-r--   0 root         (0) root         (0)     2023 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/cmake/nop.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.393671 flash_attn-2.0.0/csrc/cutlass/examples/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.508178 flash_attn-2.0.0/csrc/cutlass/examples/00_basic_gemm/
+-rw-r--r--   0 root         (0) root         (0)    14698 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/00_basic_gemm/basic_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.511874 flash_attn-2.0.0/csrc/cutlass/examples/01_cutlass_utilities/
+-rw-rw-r--   0 root         (0) root         (0)    13255 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/01_cutlass_utilities/cutlass_utilities.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.515010 flash_attn-2.0.0/csrc/cutlass/examples/02_dump_reg_shmem/
+-rw-rw-r--   0 root         (0) root         (0)     7157 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/02_dump_reg_shmem/dump_reg_shmem.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.526741 flash_attn-2.0.0/csrc/cutlass/examples/03_visualize_layout/
+-rw-rw-r--   0 root         (0) root         (0)     4478 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/03_visualize_layout/options.h
+-rw-rw-r--   0 root         (0) root         (0)     7081 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/03_visualize_layout/register_layout.cu
+-rw-rw-r--   0 root         (0) root         (0)     2691 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/03_visualize_layout/register_layout.h
+-rw-rw-r--   0 root         (0) root         (0)     5819 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/03_visualize_layout/visualize_layout.cpp
+-rw-rw-r--   0 root         (0) root         (0)    11415 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/03_visualize_layout/visualize_layout.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.531888 flash_attn-2.0.0/csrc/cutlass/examples/04_tile_iterator/
+-rw-rw-r--   0 root         (0) root         (0)     8226 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/04_tile_iterator/tile_iterator.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.534891 flash_attn-2.0.0/csrc/cutlass/examples/05_batched_gemm/
+-rw-rw-r--   0 root         (0) root         (0)    15161 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/05_batched_gemm/batched_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.537757 flash_attn-2.0.0/csrc/cutlass/examples/06_splitK_gemm/
+-rw-rw-r--   0 root         (0) root         (0)    17570 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/06_splitK_gemm/splitk_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.540655 flash_attn-2.0.0/csrc/cutlass/examples/07_volta_tensorop_gemm/
+-rw-r--r--   0 root         (0) root         (0)    18280 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/07_volta_tensorop_gemm/volta_tensorop_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.543459 flash_attn-2.0.0/csrc/cutlass/examples/08_turing_tensorop_gemm/
+-rw-r--r--   0 root         (0) root         (0)    18226 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/08_turing_tensorop_gemm/turing_tensorop_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.546468 flash_attn-2.0.0/csrc/cutlass/examples/09_turing_tensorop_conv2dfprop/
+-rw-r--r--   0 root         (0) root         (0)    28124 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/09_turing_tensorop_conv2dfprop/turing_tensorop_conv2dfprop.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.549370 flash_attn-2.0.0/csrc/cutlass/examples/10_planar_complex/
+-rw-rw-r--   0 root         (0) root         (0)    21947 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/10_planar_complex/planar_complex.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.552873 flash_attn-2.0.0/csrc/cutlass/examples/11_planar_complex_array/
+-rw-rw-r--   0 root         (0) root         (0)    23244 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/11_planar_complex_array/planar_complex_array.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.555706 flash_attn-2.0.0/csrc/cutlass/examples/12_gemm_bias_relu/
+-rw-rw-r--   0 root         (0) root         (0)    13151 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/12_gemm_bias_relu/gemm_bias_relu.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.603096 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/
+-rw-rw-r--   0 root         (0) root         (0)    26102 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/b2b_conv2d_run.h
+-rw-r--r--   0 root         (0) root         (0)    22877 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/b2b_gemm_run.h
+-rw-rw-r--   0 root         (0) root         (0)    28268 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_conv2d_run.h
+-rw-r--r--   0 root         (0) root         (0)    24493 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_gemm_run.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.608088 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/device/
+-rw-r--r--   0 root         (0) root         (0)    15552 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/device/b2b_gemm.h
+-rw-rw-r--   0 root         (0) root         (0)    11520 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/device/b2b_implicit_gemm_convolution.h
+-rw-rw-r--   0 root         (0) root         (0)     8756 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_rf.cu
+-rw-rw-r--   0 root         (0) root         (0)     8759 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_shmem.cu
+-rw-rw-r--   0 root         (0) root         (0)     8712 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_rf.cu
+-rw-rw-r--   0 root         (0) root         (0)     8762 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_shmem.cu
+-rw-rw-r--   0 root         (0) root         (0)     8787 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_rf.cu
+-rw-rw-r--   0 root         (0) root         (0)     8793 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_shmem.cu
+-rw-rw-r--   0 root         (0) root         (0)     8711 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_rf.cu
+-rw-rw-r--   0 root         (0) root         (0)     8775 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_shmem.cu
+-rw-rw-r--   0 root         (0) root         (0)     7269 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_rf.cu
+-rw-rw-r--   0 root         (0) root         (0)     7338 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_shmem.cu
+-rw-rw-r--   0 root         (0) root         (0)     7294 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_rf.cu
+-rw-rw-r--   0 root         (0) root         (0)     7359 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_shmem.cu
+-rw-rw-r--   0 root         (0) root         (0)     7362 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_rf.cu
+-rw-rw-r--   0 root         (0) root         (0)     7430 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_shmem.cu
+-rw-r--r--   0 root         (0) root         (0)     7627 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_rf.cu
+-rw-r--r--   0 root         (0) root         (0)     7634 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_shmem.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.632527 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/kernel/
+-rw-r--r--   0 root         (0) root         (0)    16152 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_gemm.h
+-rw-rw-r--   0 root         (0) root         (0)    18151 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_implicit_gemm_convolution.h
+-rw-rw-r--   0 root         (0) root         (0)     3973 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop.h
+-rw-rw-r--   0 root         (0) root         (0)    26762 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm75.h
+-rw-rw-r--   0 root         (0) root         (0)    26775 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm80.h
+-rw-rw-r--   0 root         (0) root         (0)    28422 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm75.h
+-rw-rw-r--   0 root         (0) root         (0)    28073 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm80.h
+-rw-r--r--   0 root         (0) root         (0)    17111 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm.h
+-rw-r--r--   0 root         (0) root         (0)    15658 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm_smem_accumulator.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.358025 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/reference/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.635802 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/reference/device/
+-rw-r--r--   0 root         (0) root         (0)    10368 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/reference/device/tensor_scale_bias.h
+-rw-rw-r--   0 root         (0) root         (0)     3577 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/test_run.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.657209 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/threadblock/
+-rw-rw-r--   0 root         (0) root         (0)    31616 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage.h
+-rw-rw-r--   0 root         (0) root         (0)    31443 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage_smem_accumulator.h
+-rw-r--r--   0 root         (0) root         (0)    21010 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)    20493 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined_smem_accumulator.h
+-rw-rw-r--   0 root         (0) root         (0)     7983 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base.h
+-rw-rw-r--   0 root         (0) root         (0)     6047 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base_smem_accumulator.h
+-rw-r--r--   0 root         (0) root         (0)    33788 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    33506 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage_smem_accumulator.h
+-rw-r--r--   0 root         (0) root         (0)    21451 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)    21065 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined_smem_accumulator.h
+-rw-rw-r--   0 root         (0) root         (0)    27144 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma.h
+-rw-rw-r--   0 root         (0) root         (0)    27400 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma_smem_accumulator.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.667796 flash_attn-2.0.0/csrc/cutlass/examples/14_ampere_tf32_tensorop_gemm/
+-rw-rw-r--   0 root         (0) root         (0)    18020 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/14_ampere_tf32_tensorop_gemm/ampere_tf32_tensorop_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.670638 flash_attn-2.0.0/csrc/cutlass/examples/15_ampere_sparse_tensorop_gemm/
+-rw-rw-r--   0 root         (0) root         (0)    15042 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/15_ampere_sparse_tensorop_gemm/ampere_sparse_tensorop_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.673508 flash_attn-2.0.0/csrc/cutlass/examples/16_ampere_tensorop_conv2dfprop/
+-rw-r--r--   0 root         (0) root         (0)    27755 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/16_ampere_tensorop_conv2dfprop/ampere_tensorop_conv2dfprop.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.676381 flash_attn-2.0.0/csrc/cutlass/examples/17_fprop_per_channel_bias/
+-rw-rw-r--   0 root         (0) root         (0)    12580 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/17_fprop_per_channel_bias/fprop_per_channel_bias.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.679257 flash_attn-2.0.0/csrc/cutlass/examples/18_ampere_fp64_tensorop_affine2_gemm/
+-rw-rw-r--   0 root         (0) root         (0)    14007 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/18_ampere_fp64_tensorop_affine2_gemm/ampere_fp64_tensorop_affine2_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.682078 flash_attn-2.0.0/csrc/cutlass/examples/19_tensorop_canonical/
+-rw-rw-r--   0 root         (0) root         (0)    13401 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/19_tensorop_canonical/tensorop_canonical.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.684945 flash_attn-2.0.0/csrc/cutlass/examples/20_simt_canonical/
+-rw-rw-r--   0 root         (0) root         (0)    12556 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/20_simt_canonical/simt_canonical.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.687808 flash_attn-2.0.0/csrc/cutlass/examples/21_quaternion_gemm/
+-rw-rw-r--   0 root         (0) root         (0)    17319 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/21_quaternion_gemm/quaternion_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.690732 flash_attn-2.0.0/csrc/cutlass/examples/22_quaternion_conv/
+-rw-r--r--   0 root         (0) root         (0)    21495 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/22_quaternion_conv/quaternion_conv.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.693740 flash_attn-2.0.0/csrc/cutlass/examples/23_ampere_gemm_operand_reduction_fusion/
+-rw-r--r--   0 root         (0) root         (0)    27530 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/23_ampere_gemm_operand_reduction_fusion/ampere_gemm_operand_reduction_fusion.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.696727 flash_attn-2.0.0/csrc/cutlass/examples/24_gemm_grouped/
+-rw-r--r--   0 root         (0) root         (0)    50967 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/24_gemm_grouped/gemm_grouped.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.699657 flash_attn-2.0.0/csrc/cutlass/examples/25_ampere_fprop_mainloop_fusion/
+-rw-rw-r--   0 root         (0) root         (0)    26547 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_3d_fprop_mainloop_fusion.cu
+-rw-rw-r--   0 root         (0) root         (0)    25628 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_fprop_mainloop_fusion.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.705127 flash_attn-2.0.0/csrc/cutlass/examples/26_ampere_wgrad_mainloop_fusion/
+-rw-rw-r--   0 root         (0) root         (0)    25538 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/26_ampere_wgrad_mainloop_fusion/ampere_wgrad_mainloop_fusion.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.708031 flash_attn-2.0.0/csrc/cutlass/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/
+-rw-rw-r--   0 root         (0) root         (0)    30446 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.712608 flash_attn-2.0.0/csrc/cutlass/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/
+-rw-rw-r--   0 root         (0) root         (0)    28159 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/ampere_3xtf32_fast_accurate_tensorop_fprop.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.715709 flash_attn-2.0.0/csrc/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/
+-rw-r--r--   0 root         (0) root         (0)    28403 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.718643 flash_attn-2.0.0/csrc/cutlass/examples/30_wgrad_split_k/
+-rw-rw-r--   0 root         (0) root         (0)    27329 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/30_wgrad_split_k/30_wgrad_split_k.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.721542 flash_attn-2.0.0/csrc/cutlass/examples/31_basic_syrk/
+-rw-r--r--   0 root         (0) root         (0)    15206 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/31_basic_syrk/basic_syrk.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.724349 flash_attn-2.0.0/csrc/cutlass/examples/32_basic_trmm/
+-rw-r--r--   0 root         (0) root         (0)    15907 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/32_basic_trmm/basic_trmm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.727352 flash_attn-2.0.0/csrc/cutlass/examples/33_ampere_3xtf32_tensorop_symm/
+-rw-rw-r--   0 root         (0) root         (0)    31803 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/33_ampere_3xtf32_tensorop_symm/ampere_3xtf32_tensorop_symm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.730248 flash_attn-2.0.0/csrc/cutlass/examples/34_transposed_conv2d/
+-rw-rw-r--   0 root         (0) root         (0)    22378 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/34_transposed_conv2d/34_transposed_conv2d.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.733142 flash_attn-2.0.0/csrc/cutlass/examples/35_gemm_softmax/
+-rw-rw-r--   0 root         (0) root         (0)    23114 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/35_gemm_softmax/gemm_softmax.cu
+-rw-rw-r--   0 root         (0) root         (0)    16723 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/35_gemm_softmax/gemm_with_epilogue_visitor.h
+-rw-r--r--   0 root         (0) root         (0)    18713 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/35_gemm_softmax/gemm_with_softmax.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.740651 flash_attn-2.0.0/csrc/cutlass/examples/36_gather_scatter_fusion/
+-rw-rw-r--   0 root         (0) root         (0)    20795 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/36_gather_scatter_fusion/gather_scatter_fusion.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.743518 flash_attn-2.0.0/csrc/cutlass/examples/37_gemm_layernorm_gemm_fusion/
+-rw-rw-r--   0 root         (0) root         (0)    31111 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu
+-rw-rw-r--   0 root         (0) root         (0)    13982 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h
+-rw-rw-r--   0 root         (0) root         (0)    33905 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.752811 flash_attn-2.0.0/csrc/cutlass/examples/38_syr2k_grouped/
+-rw-rw-r--   0 root         (0) root         (0)    47455 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/38_syr2k_grouped/syr2k_grouped.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.755790 flash_attn-2.0.0/csrc/cutlass/examples/39_gemm_permute/
+-rw-r--r--   0 root         (0) root         (0)    37896 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/39_gemm_permute/gemm_permute.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.767608 flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/
+-rw-rw-r--   0 root         (0) root         (0)    11865 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/debug_utils.h
+-rw-r--r--   0 root         (0) root         (0)     9885 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/default_fmha_grouped.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.777557 flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/epilogue/
+-rw-rw-r--   0 root         (0) root         (0)    22349 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_pipelined.h
+-rw-rw-r--   0 root         (0) root         (0)     9162 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_rescale_output.h
+-rw-rw-r--   0 root         (0) root         (0)     6111 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_thread_apply_logsumexp.h
+-rw-r--r--   0 root         (0) root         (0)    34379 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h
+-rw-rw-r--   0 root         (0) root         (0)     6666 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/fmha_grouped_problem_visitor.h
+-rw-r--r--   0 root         (0) root         (0)    38173 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu
+-rw-r--r--   0 root         (0) root         (0)    39997 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.793352 flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/gemm/
+-rw-rw-r--   0 root         (0) root         (0)     3994 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma.h
+-rw-rw-r--   0 root         (0) root         (0)     6241 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_base.h
+-rw-r--r--   0 root         (0) root         (0)    27198 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    14090 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_pipelined.h
+-rw-rw-r--   0 root         (0) root         (0)     6782 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/gemm/find_default_mma.h
+-rw-rw-r--   0 root         (0) root         (0)    13959 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/gemm/mma_accum_lambda_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    72983 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/gemm/mma_from_smem.h
+-rw-r--r--   0 root         (0) root         (0)    10878 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/gemm_kernel_utils.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.812720 flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/iterators/
+-rw-rw-r--   0 root         (0) root         (0)    23855 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/iterators/epilogue_predicated_tile_iterator.h
+-rw-rw-r--   0 root         (0) root         (0)     3142 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/iterators/make_residual_last.h
+-rw-r--r--   0 root         (0) root         (0)    64480 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_access_iterator_residual_last.h
+-rw-rw-r--   0 root         (0) root         (0)    64500 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_iterator_residual_last.h
+-rw-r--r--   0 root         (0) root         (0)     2435 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/iterators/transpose_warp_iterator.h
+-rw-r--r--   0 root         (0) root         (0)     9497 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/iterators/warp_iterator_from_smem.h
+-rw-r--r--   0 root         (0) root         (0)    48655 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.819219 flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/transform/
+-rw-r--r--   0 root         (0) root         (0)     3747 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/transform/tile_smem_loader.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.821900 flash_attn-2.0.0/csrc/cutlass/examples/42_ampere_tensorop_group_conv/
+-rw-rw-r--   0 root         (0) root         (0)    23901 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/42_ampere_tensorop_group_conv/ampere_tensorop_group_conv.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.824890 flash_attn-2.0.0/csrc/cutlass/examples/43_ell_block_sparse_gemm/
+-rw-rw-r--   0 root         (0) root         (0)    23867 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/43_ell_block_sparse_gemm/ell_block_sparse_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:39.831241 flash_attn-2.0.0/csrc/cutlass/examples/44_multi_gemm_ir_and_codegen/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.382742 flash_attn-2.0.0/csrc/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.383786 flash_attn-2.0.0/csrc/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:39.835072 flash_attn-2.0.0/csrc/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/
+-rw-rw-r--   0 root         (0) root         (0)     6370 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_bias_act_epilogue_tensor_op.h
+-rw-rw-r--   0 root         (0) root         (0)     4099 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_thread_map_tensor_op_for_fused_bias.h
+-rw-rw-r--   0 root         (0) root         (0)     8285 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/fused_bias_act_epilogue.h
+-rw-rw-r--   0 root         (0) root         (0)    10439 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/output_tile_thread_map_for_fused_bias.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:39.845579 flash_attn-2.0.0/csrc/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/warp/
+-rw-rw-r--   0 root         (0) root         (0)     6848 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/warp/fused_bias_act_fragment_iterator_tensor_op.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.386104 flash_attn-2.0.0/csrc/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:39.849129 flash_attn-2.0.0/csrc/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/warp/
+-rw-rw-r--   0 root         (0) root         (0)    14747 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/warp/mma_tensor_op_fragment_iterator_without_output_op.h
+-rw-rw-r--   0 root         (0) root         (0)    10231 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/44_multi_gemm_ir_and_codegen/leaky_bias.h
+-rw-rw-r--   0 root         (0) root         (0)     3745 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/44_multi_gemm_ir_and_codegen/utils.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:39.852038 flash_attn-2.0.0/csrc/cutlass/examples/45_dual_gemm/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:39.862558 flash_attn-2.0.0/csrc/cutlass/examples/45_dual_gemm/device/
+-rw-r--r--   0 root         (0) root         (0)    16955 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/45_dual_gemm/device/dual_gemm.h
+-rw-r--r--   0 root         (0) root         (0)    12669 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/45_dual_gemm/dual_gemm.cu
+-rw-rw-r--   0 root         (0) root         (0)     2366 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/45_dual_gemm/dual_gemm_common.h
+-rw-r--r--   0 root         (0) root         (0)    31360 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/45_dual_gemm/dual_gemm_run.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:39.865867 flash_attn-2.0.0/csrc/cutlass/examples/45_dual_gemm/kernel/
+-rw-r--r--   0 root         (0) root         (0)    18422 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/45_dual_gemm/kernel/dual_gemm.h
+-rw-rw-r--   0 root         (0) root         (0)     3577 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/45_dual_gemm/test_run.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:39.869106 flash_attn-2.0.0/csrc/cutlass/examples/45_dual_gemm/thread/
+-rw-rw-r--   0 root         (0) root         (0)     5818 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/45_dual_gemm/thread/left_silu_and_mul.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:39.872623 flash_attn-2.0.0/csrc/cutlass/examples/45_dual_gemm/threadblock/
+-rw-rw-r--   0 root         (0) root         (0)    15613 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/45_dual_gemm/threadblock/dual_epilogue.h
+-rw-rw-r--   0 root         (0) root         (0)     7920 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/45_dual_gemm/threadblock/dual_mma_base.h
+-rw-r--r--   0 root         (0) root         (0)    29976 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/45_dual_gemm/threadblock/dual_mma_multistage.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:39.880711 flash_attn-2.0.0/csrc/cutlass/examples/46_depthwise_simt_conv2dfprop/
+-rw-r--r--   0 root         (0) root         (0)    24464 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/46_depthwise_simt_conv2dfprop/depthwise_simt_conv2dfprop.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:39.884105 flash_attn-2.0.0/csrc/cutlass/examples/47_ampere_gemm_universal_streamk/
+-rw-r--r--   0 root         (0) root         (0)    22676 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/47_ampere_gemm_universal_streamk/ampere_gemm_universal_streamk.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:39.887682 flash_attn-2.0.0/csrc/cutlass/examples/48_hopper_warp_specialized_gemm/
+-rw-r--r--   0 root         (0) root         (0)    16736 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/48_hopper_warp_specialized_gemm/48_hopper_warp_specialized_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:39.891053 flash_attn-2.0.0/csrc/cutlass/examples/49_hopper_gemm_schedules_with_collective_builder/
+-rw-r--r--   0 root         (0) root         (0)    22631 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/49_hopper_gemm_schedules_with_collective_builder/49_hopper_gemm_schedules_with_collective_builder.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:39.894395 flash_attn-2.0.0/csrc/cutlass/examples/50_hopper_gemm_with_epilogue_swizzle/
+-rw-r--r--   0 root         (0) root         (0)    18635 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/50_hopper_gemm_with_epilogue_swizzle/50_hopper_gemm_with_epilogue_swizzle.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:39.897728 flash_attn-2.0.0/csrc/cutlass/examples/60_cutlass_import/
+-rw-rw-r--   0 root         (0) root         (0)     2849 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/60_cutlass_import/main.cpp
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:39.901338 flash_attn-2.0.0/csrc/cutlass/examples/common/
+-rw-r--r--   0 root         (0) root         (0)     4449 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/examples/common/helper.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.395987 flash_attn-2.0.0/csrc/cutlass/examples/cute/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:39.904791 flash_attn-2.0.0/csrc/cutlass/examples/cute/tutorial/
+-rw-rw-r--   0 root         (0) root         (0)    14342 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/examples/cute/tutorial/sgemm_nt_1.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.397928 flash_attn-2.0.0/csrc/cutlass/include/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:39.996122 flash_attn-2.0.0/csrc/cutlass/include/cutlass/
+-rw-rw-r--   0 root         (0) root         (0)     3793 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/aligned_buffer.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:40.062034 flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/
+-rw-r--r--   0 root         (0) root         (0)     3538 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/arch.h
+-rw-r--r--   0 root         (0) root         (0)    12127 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/barrier.h
+-rw-rw-r--   0 root         (0) root         (0)     2691 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/cache_operation.h
+-rw-r--r--   0 root         (0) root         (0)    14313 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/memory.h
+-rw-r--r--   0 root         (0) root         (0)     8511 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/memory_sm75.h
+-rw-rw-r--   0 root         (0) root         (0)    15166 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/memory_sm80.h
+-rw-r--r--   0 root         (0) root         (0)     8072 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/mma.h
+-rw-rw-r--   0 root         (0) root         (0)    11096 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/mma_sm50.h
+-rw-rw-r--   0 root         (0) root         (0)     7040 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/mma_sm60.h
+-rw-rw-r--   0 root         (0) root         (0)     4193 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/mma_sm61.h
+-rw-rw-r--   0 root         (0) root         (0)    16554 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/mma_sm70.h
+-rw-rw-r--   0 root         (0) root         (0)    31682 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/mma_sm75.h
+-rw-rw-r--   0 root         (0) root         (0)    55577 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/mma_sm80.h
+-rw-r--r--   0 root         (0) root         (0)     8254 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/mma_sm90.h
+-rw-rw-r--   0 root         (0) root         (0)    43978 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/mma_sparse_sm80.h
+-rw-rw-r--   0 root         (0) root         (0)     2622 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/reg_reconfig.h
+-rw-rw-r--   0 root         (0) root         (0)     3998 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/simd.h
+-rw-rw-r--   0 root         (0) root         (0)     3656 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/simd_sm60.h
+-rw-rw-r--   0 root         (0) root         (0)     5102 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/simd_sm61.h
+-rw-rw-r--   0 root         (0) root         (0)     8473 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/wmma.h
+-rw-rw-r--   0 root         (0) root         (0)     5286 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/wmma_sm70.h
+-rw-rw-r--   0 root         (0) root         (0)     7746 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/wmma_sm72.h
+-rw-rw-r--   0 root         (0) root         (0)     7616 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/wmma_sm75.h
+-rw-rw-r--   0 root         (0) root         (0)    62709 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/array.h
+-rw-rw-r--   0 root         (0) root         (0)     3662 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/array_planar_complex.h
+-rw-rw-r--   0 root         (0) root         (0)    13128 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/array_subbyte.h
+-rw-r--r--   0 root         (0) root         (0)     6371 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/barrier.h
+-rw-rw-r--   0 root         (0) root         (0)    13371 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/bfloat16.h
+-rw-rw-r--   0 root         (0) root         (0)     6338 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/blas3.h
+-rw-rw-r--   0 root         (0) root         (0)     9372 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/block_striped.h
+-rw-r--r--   0 root         (0) root         (0)    19422 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/complex.h
+-rw-rw-r--   0 root         (0) root         (0)    47943 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/constants.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:40.064828 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/
+-rw-r--r--   0 root         (0) root         (0)    22725 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/conv2d_problem_size.h
+-rw-rw-r--   0 root         (0) root         (0)    16292 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/conv3d_problem_size.h
+-rw-r--r--   0 root         (0) root         (0)     6664 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/convolution.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:40.073222 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/device/
+-rw-rw-r--   0 root         (0) root         (0)     9744 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/device/direct_convolution.h
+-rw-rw-r--   0 root         (0) root         (0)    12078 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/device/implicit_gemm_convolution.h
+-rw-rw-r--   0 root         (0) root         (0)    10044 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/device/implicit_gemm_convolution_fusion.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:40.120365 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/
+-rw-rw-r--   0 root         (0) root         (0)     7671 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/default_conv2d.h
+-rw-rw-r--   0 root         (0) root         (0)    53546 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/default_conv2d_dgrad.h
+-rw-rw-r--   0 root         (0) root         (0)    56838 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop.h
+-rw-rw-r--   0 root         (0) root         (0)    11953 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_fusion.h
+-rw-r--r--   0 root         (0) root         (0)     4690 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_broadcast.h
+-rw-r--r--   0 root         (0) root         (0)     4660 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_reduction.h
+-rw-r--r--   0 root         (0) root         (0)    15891 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/default_conv2d_group_fprop.h
+-rw-rw-r--   0 root         (0) root         (0)    28745 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad.h
+-rw-rw-r--   0 root         (0) root         (0)    10459 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad_fusion.h
+-rw-rw-r--   0 root         (0) root         (0)     9324 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/default_conv3d_dgrad.h
+-rw-rw-r--   0 root         (0) root         (0)    14864 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop.h
+-rw-rw-r--   0 root         (0) root         (0)    11980 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop_fusion.h
+-rw-rw-r--   0 root         (0) root         (0)    14883 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/default_conv3d_wgrad.h
+-rw-r--r--   0 root         (0) root         (0)    19294 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/default_depthwise_fprop.h
+-rw-rw-r--   0 root         (0) root         (0)    18048 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/direct_convolution.h
+-rw-rw-r--   0 root         (0) root         (0)    15430 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution.h
+-rw-rw-r--   0 root         (0) root         (0)    15685 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_fusion.h
+-rw-rw-r--   0 root         (0) root         (0)    17107 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_strided_dgrad.h
+-rw-rw-r--   0 root         (0) root         (0)    16725 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_with_fused_epilogue.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:40.128287 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/thread/
+-rw-rw-r--   0 root         (0) root         (0)     9689 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/thread/depthwise_mma.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:42.248590 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/
+-rw-rw-r--   0 root         (0) root         (0)    15306 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_analytic.h
+-rw-rw-r--   0 root         (0) root         (0)    19735 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_optimized.h
+-rw-rw-r--   0 root         (0) root         (0)    18940 2023-07-17 10:57:19.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    26137 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)    10953 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_analytic.h
+-rw-rw-r--   0 root         (0) root         (0)    11529 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_few_channels.h
+-rw-rw-r--   0 root         (0) root         (0)    11333 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_fixed_channels.h
+-rw-r--r--   0 root         (0) root         (0)    13664 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)    10627 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_analytic.h
+-rw-rw-r--   0 root         (0) root         (0)     9314 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_few_channels.h
+-rw-rw-r--   0 root         (0) root         (0)     9018 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_fixed_channels.h
+-rw-r--r--   0 root         (0) root         (0)    10387 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_optimized.h
+-rw-rw-r--   0 root         (0) root         (0)    30197 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_params.h
+-rw-rw-r--   0 root         (0) root         (0)    11202 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    10350 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    11520 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)     9043 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    10832 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_optimized.h
+-rw-rw-r--   0 root         (0) root         (0)     8450 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_analytic.h
+-rw-rw-r--   0 root         (0) root         (0)     9569 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_optimized.h
+-rw-rw-r--   0 root         (0) root         (0)    11020 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_analytic.h
+-rw-rw-r--   0 root         (0) root         (0)    15014 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_optimized.h
+-rw-rw-r--   0 root         (0) root         (0)     9634 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_analytic.h
+-rw-rw-r--   0 root         (0) root         (0)    15132 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_optimized.h
+-rw-rw-r--   0 root         (0) root         (0)     7945 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_analytic.h
+-rw-rw-r--   0 root         (0) root         (0)     8891 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_optimized.h
+-rw-rw-r--   0 root         (0) root         (0)    18249 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv3d_params.h
+-rw-r--r--   0 root         (0) root         (0)     9971 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    12024 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)     8821 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    10744 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_optimized.h
+-rw-rw-r--   0 root         (0) root         (0)     8871 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/depthwise_direct_conv_params.h
+-rw-rw-r--   0 root         (0) root         (0)    10747 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_fixed_stride_dilation.h
+-rw-rw-r--   0 root         (0) root         (0)     9899 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_optimized.h
+-rw-rw-r--   0 root         (0) root         (0)    20899 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_direct_conv_multistage.h
+-rw-rw-r--   0 root         (0) root         (0)     8921 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_filter_tile_access_iterator_direct_conv_optimized.h
+-rw-r--r--   0 root         (0) root         (0)    12744 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_pipelined.h
+-rw-rw-r--   0 root         (0) root         (0)     8097 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/depthwise_mma_base.h
+-rw-rw-r--   0 root         (0) root         (0)    36697 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/depthwise_mma_core_with_lane_access_size.h
+-rw-rw-r--   0 root         (0) root         (0)    30106 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/implicit_gemm_fprop_fusion_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    20086 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/implicit_gemm_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    12174 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/implicit_gemm_pipelined.h
+-rw-rw-r--   0 root         (0) root         (0)    26320 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/implicit_gemm_wgrad_fusion_multistage.h
+-rw-rw-r--   0 root         (0) root         (0)    16915 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_access_iterator.h
+-rw-rw-r--   0 root         (0) root         (0)    12476 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_iterator.h
+-rw-r--r--   0 root         (0) root         (0)     8050 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/threadblock_swizzle.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:42.256711 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/warp/
+-rw-rw-r--   0 root         (0) root         (0)    12419 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/warp/mma_depthwise_simt.h
+-rw-rw-r--   0 root         (0) root         (0)    30655 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/warp/mma_depthwise_simt_tile_iterator.h
+-rw-rw-r--   0 root         (0) root         (0)     8772 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/warp/scale_bias_relu_transform.h
+-rw-rw-r--   0 root         (0) root         (0)    11827 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/coord.h
+-rw-r--r--   0 root         (0) root         (0)    11077 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/core_io.h
+-rw-r--r--   0 root         (0) root         (0)     8697 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/cutlass.h
+-rw-r--r--   0 root         (0) root         (0)     4216 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/device_kernel.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.405172 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:42.311924 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/
+-rw-r--r--   0 root         (0) root         (0)    18909 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/activation.h
+-rw-rw-r--   0 root         (0) root         (0)     4691 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/conversion_op.h
+-rw-r--r--   0 root         (0) root         (0)    11563 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination.h
+-rw-r--r--   0 root         (0) root         (0)     8423 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_elementwise.h
+-rw-r--r--   0 root         (0) root         (0)    13569 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_relu.h
+-rw-r--r--   0 root         (0) root         (0)    23649 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_clamp.h
+-rw-r--r--   0 root         (0) root         (0)     9067 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_dgelu.h
+-rw-r--r--   0 root         (0) root         (0)    15195 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_drelu.h
+-rw-rw-r--   0 root         (0) root         (0)     3669 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_gelu.h
+-rw-r--r--   0 root         (0) root         (0)     8065 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_generic.h
+-rw-rw-r--   0 root         (0) root         (0)     3693 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_hardswish.h
+-rw-r--r--   0 root         (0) root         (0)     8344 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_leaky_relu.h
+-rw-r--r--   0 root         (0) root         (0)     3058 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_params.h
+-rw-rw-r--   0 root         (0) root         (0)     9351 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)    20486 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_relu.h
+-rw-r--r--   0 root         (0) root         (0)    19348 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_relu0.h
+-rw-r--r--   0 root         (0) root         (0)    12033 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_residual_block.h
+-rw-rw-r--   0 root         (0) root         (0)     3688 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_sigmoid.h
+-rw-rw-r--   0 root         (0) root         (0)     3669 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_silu.h
+-rw-r--r--   0 root         (0) root         (0)     8662 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_with_elementwise.h
+-rw-rw-r--   0 root         (0) root         (0)     3416 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/reduction_op.h
+-rw-rw-r--   0 root         (0) root         (0)     2656 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/scale_type.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:44.430909 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/
+-rw-r--r--   0 root         (0) root         (0)     9142 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     9441 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op_blas3.h
+-rw-rw-r--   0 root         (0) root         (0)     3234 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_direct_store.h
+-rw-rw-r--   0 root         (0) root         (0)     7209 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_planar_complex.h
+-rw-rw-r--   0 root         (0) root         (0)    13385 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_simt.h
+-rw-rw-r--   0 root         (0) root         (0)    28290 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op.h
+-rw-rw-r--   0 root         (0) root         (0)     7129 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op_blas3.h
+-rw-rw-r--   0 root         (0) root         (0)    10846 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     5817 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_broadcast.h
+-rw-rw-r--   0 root         (0) root         (0)     5763 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_reduction.h
+-rw-rw-r--   0 root         (0) root         (0)     5947 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h
+-rw-rw-r--   0 root         (0) root         (0)     4409 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_simt.h
+-rw-rw-r--   0 root         (0) root         (0)     7398 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_tensor_op.h
+-rw-rw-r--   0 root         (0) root         (0)     7303 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_volta_tensor_op.h
+-rw-rw-r--   0 root         (0) root         (0)     4098 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_wmma_tensor_op.h
+-rw-rw-r--   0 root         (0) root         (0)     4678 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/direct_store_epilogue_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    20099 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/epilogue.h
+-rw-rw-r--   0 root         (0) root         (0)     8279 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/epilogue_base.h
+-rw-rw-r--   0 root         (0) root         (0)     7455 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/epilogue_base_streamk.h
+-rw-rw-r--   0 root         (0) root         (0)    13424 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/epilogue_depthwise.h
+-rw-rw-r--   0 root         (0) root         (0)    13933 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/epilogue_direct_store.h
+-rw-rw-r--   0 root         (0) root         (0)     7401 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/epilogue_gemm_k_reduction.h
+-rw-rw-r--   0 root         (0) root         (0)    14610 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/epilogue_planar_complex.h
+-rw-rw-r--   0 root         (0) root         (0)     9073 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/epilogue_smem_accumulator.h
+-rw-rw-r--   0 root         (0) root         (0)    16804 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/epilogue_visitor_with_softmax.h
+-rw-r--r--   0 root         (0) root         (0)    52430 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_broadcast.h
+-rw-rw-r--   0 root         (0) root         (0)    29199 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_reduction.h
+-rw-rw-r--   0 root         (0) root         (0)    13454 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h
+-rw-rw-r--   0 root         (0) root         (0)     7308 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/epilogue_workspace.h
+-rw-rw-r--   0 root         (0) root         (0)    14359 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/interleaved_epilogue.h
+-rw-rw-r--   0 root         (0) root         (0)     2912 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/output_iterator_parameter.h
+-rw-rw-r--   0 root         (0) root         (0)    19750 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/output_tile_thread_map.h
+-rw-r--r--   0 root         (0) root         (0)    40870 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator.h
+-rw-rw-r--   0 root         (0) root         (0)    18821 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine.h
+-rw-rw-r--   0 root         (0) root         (0)     5636 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine_layout_params.h
+-rw-rw-r--   0 root         (0) root         (0)    21249 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_blas3.h
+-rw-r--r--   0 root         (0) root         (0)    13872 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_direct_conv.h
+-rw-rw-r--   0 root         (0) root         (0)    14496 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_params.h
+-rw-rw-r--   0 root         (0) root         (0)     9146 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_predicates.h
+-rw-r--r--   0 root         (0) root         (0)    15536 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_strided_dgrad.h
+-rw-rw-r--   0 root         (0) root         (0)     7487 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    17756 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_mixed.h
+-rw-rw-r--   0 root         (0) root         (0)     7394 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_pitch_liner.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:45.488350 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/
+-rw-rw-r--   0 root         (0) root         (0)     7055 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/fragment_iterator_complex_tensor_op.h
+-rw-rw-r--   0 root         (0) root         (0)     7736 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/fragment_iterator_gaussian_complex_tensor_op.h
+-rw-rw-r--   0 root         (0) root         (0)     5880 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/fragment_iterator_simt.h
+-rw-rw-r--   0 root         (0) root         (0)     9883 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/fragment_iterator_tensor_op.h
+-rw-rw-r--   0 root         (0) root         (0)     8924 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/fragment_iterator_volta_tensor_op.h
+-rw-rw-r--   0 root         (0) root         (0)     6045 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/fragment_iterator_wmma_tensor_op.h
+-rw-rw-r--   0 root         (0) root         (0)     4864 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/simt_policy.h
+-rw-rw-r--   0 root         (0) root         (0)     5979 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/tensor_op_policy.h
+-rw-rw-r--   0 root         (0) root         (0)    25658 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/tile_iterator_simt.h
+-rw-rw-r--   0 root         (0) root         (0)    20290 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)    22922 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op_mixed.h
+-rw-rw-r--   0 root         (0) root         (0)    14258 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/tile_iterator_volta_tensor_op.h
+-rw-rw-r--   0 root         (0) root         (0)     7704 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/tile_iterator_wmma_tensor_op.h
+-rw-rw-r--   0 root         (0) root         (0)     7485 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/volta_tensor_op_policy.h
+-rw-rw-r--   0 root         (0) root         (0)     3916 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/wmma_tensor_op_policy.h
+-rw-rw-r--   0 root         (0) root         (0)    26026 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/fast_math.h
+-rw-r--r--   0 root         (0) root         (0)    35369 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/float8.h
+-rw-rw-r--   0 root         (0) root         (0)     2645 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/floating_point_nvrtc.h
+-rw-r--r--   0 root         (0) root         (0)    12668 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/functional.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:45.491145 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:45.543667 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/
+-rw-r--r--   0 root         (0) root         (0)    17028 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/base_grouped.h
+-rw-r--r--   0 root         (0) root         (0)    24413 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/default_gemm_configuration.h
+-rw-r--r--   0 root         (0) root         (0)    27616 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/ell_gemm.h
+-rw-r--r--   0 root         (0) root         (0)    25202 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/gemm.h
+-rw-r--r--   0 root         (0) root         (0)    22367 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/gemm_array.h
+-rw-r--r--   0 root         (0) root         (0)    22375 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/gemm_batched.h
+-rw-r--r--   0 root         (0) root         (0)    22725 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/gemm_complex.h
+-rw-rw-r--   0 root         (0) root         (0)     2591 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/gemm_grouped.h
+-rw-r--r--   0 root         (0) root         (0)    13736 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/gemm_layernorm_mainloop_fusion.h
+-rw-rw-r--   0 root         (0) root         (0)    17329 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/gemm_sparse.h
+-rw-rw-r--   0 root         (0) root         (0)    20450 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/gemm_splitk_parallel.h
+-rw-r--r--   0 root         (0) root         (0)    14902 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/gemm_universal.h
+-rw-r--r--   0 root         (0) root         (0)    21594 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/gemm_universal_adapter.h
+-rw-r--r--   0 root         (0) root         (0)    13362 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/gemm_universal_base.h
+-rw-r--r--   0 root         (0) root         (0)    13968 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/gemm_universal_with_broadcast.h
+-rw-r--r--   0 root         (0) root         (0)    14853 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/gemm_with_k_reduction.h
+-rw-r--r--   0 root         (0) root         (0)     5690 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/gemv.h
+-rw-r--r--   0 root         (0) root         (0)    18127 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/rank_2k.h
+-rw-rw-r--   0 root         (0) root         (0)     2747 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/rank_2k_grouped.h
+-rw-r--r--   0 root         (0) root         (0)    16719 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/rank_k.h
+-rwxr-xr-x   0 root         (0) root         (0)    21050 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/symm.h
+-rw-r--r--   0 root         (0) root         (0)    26464 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/trmm.h
+-rw-r--r--   0 root         (0) root         (0)    15946 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/gemm.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:46.695715 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/
+-rw-rw-r--   0 root         (0) root         (0)    29360 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_ell_gemm.h
+-rw-r--r--   0 root         (0) root         (0)    37758 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_gemm.h
+-rw-rw-r--   0 root         (0) root         (0)    16130 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_gemm_complex.h
+-rw-rw-r--   0 root         (0) root         (0)    12385 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped.h
+-rw-rw-r--   0 root         (0) root         (0)     6592 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped_softmax_mainloop_fusion.h
+-rw-rw-r--   0 root         (0) root         (0)     5848 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_gemm_layernorm_mainloop_fusion.h
+-rw-rw-r--   0 root         (0) root         (0)    11104 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_gemm_planar_complex_universal.h
+-rw-rw-r--   0 root         (0) root         (0)     7983 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_gemm_sparse.h
+-rw-rw-r--   0 root         (0) root         (0)     4932 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_gemm_splitk_parallel.h
+-rw-r--r--   0 root         (0) root         (0)    11951 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_gemm_universal.h
+-rw-r--r--   0 root         (0) root         (0)     8125 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_gemm_with_broadcast.h
+-rw-rw-r--   0 root         (0) root         (0)     6457 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_gemm_with_k_reduction.h
+-rw-r--r--   0 root         (0) root         (0)     8086 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_gemm_with_reduction.h
+-rwxrwxr-x   0 root         (0) root         (0)     5349 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_gemv.h
+-rw-rw-r--   0 root         (0) root         (0)    11560 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_rank_2k.h
+-rw-rw-r--   0 root         (0) root         (0)    20509 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_rank_2k_complex.h
+-rw-rw-r--   0 root         (0) root         (0)    12470 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_rank_2k_grouped.h
+-rw-rw-r--   0 root         (0) root         (0)    10620 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_rank_2k_universal.h
+-rw-rw-r--   0 root         (0) root         (0)     9872 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_rank_k.h
+-rw-rw-r--   0 root         (0) root         (0)    16990 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_rank_k_complex.h
+-rw-rw-r--   0 root         (0) root         (0)     9444 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_rank_k_universal.h
+-rwxrwxr-x   0 root         (0) root         (0)    13375 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_symm.h
+-rwxrwxr-x   0 root         (0) root         (0)    21830 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_symm_complex.h
+-rwxrwxr-x   0 root         (0) root         (0)    10315 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_symm_universal.h
+-rw-rw-r--   0 root         (0) root         (0)    10873 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_trmm.h
+-rw-rw-r--   0 root         (0) root         (0)    10730 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_trmm_complex.h
+-rw-rw-r--   0 root         (0) root         (0)    10850 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_trmm_universal.h
+-rw-rw-r--   0 root         (0) root         (0)    28916 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/ell_gemm.h
+-rw-rw-r--   0 root         (0) root         (0)    13357 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm.h
+-rw-rw-r--   0 root         (0) root         (0)     8693 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_array.h
+-rw-rw-r--   0 root         (0) root         (0)     8761 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_batched.h
+-rw-rw-r--   0 root         (0) root         (0)    14687 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_grouped.h
+-rw-rw-r--   0 root         (0) root         (0)     4691 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_grouped_problem_visitor.h
+-rw-rw-r--   0 root         (0) root         (0)    15623 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_grouped_softmax_mainloop_fusion.h
+-rw-r--r--   0 root         (0) root         (0)    27281 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_layernorm_mainloop_fusion.h
+-rwxrwxr-x   0 root         (0) root         (0)     6144 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_params.h
+-rw-rw-r--   0 root         (0) root         (0)     5141 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)    22949 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)    18937 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex_array.h
+-rw-rw-r--   0 root         (0) root         (0)     8142 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_splitk_parallel.h
+-rw-rw-r--   0 root         (0) root         (0)     4291 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_transpose_operands.h
+-rw-r--r--   0 root         (0) root         (0)    23216 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_universal.h
+-rw-r--r--   0 root         (0) root         (0)    39288 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_universal_streamk.h
+-rw-r--r--   0 root         (0) root         (0)    47186 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_with_fused_epilogue.h
+-rw-r--r--   0 root         (0) root         (0)    23605 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_with_k_reduction.h
+-rw-r--r--   0 root         (0) root         (0)     8090 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemv.h
+-rwxrwxr-x   0 root         (0) root         (0)     8979 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemv_batched_strided.h
+-rw-r--r--   0 root         (0) root         (0)    16849 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/grouped_problem_visitor.h
+-rw-rw-r--   0 root         (0) root         (0)     7148 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/params_universal_base.h
+-rw-rw-r--   0 root         (0) root         (0)    22938 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped.h
+-rw-rw-r--   0 root         (0) root         (0)    16100 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped_problem_visitor.h
+-rw-rw-r--   0 root         (0) root         (0)     4334 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/rank_2k_transpose_operands.h
+-rw-rw-r--   0 root         (0) root         (0)    24138 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/rank_2k_universal.h
+-rw-rw-r--   0 root         (0) root         (0)    17543 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/rank_k_universal.h
+-rw-rw-r--   0 root         (0) root         (0)    13586 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/sparse_gemm.h
+-rwxrwxr-x   0 root         (0) root         (0)    23876 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/symm_universal.h
+-rw-rw-r--   0 root         (0) root         (0)    19513 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/trmm_universal.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:46.706422 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/thread/
+-rw-rw-r--   0 root         (0) root         (0)     3567 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/thread/mma.h
+-rw-rw-r--   0 root         (0) root         (0)    15373 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/thread/mma_sm50.h
+-rw-rw-r--   0 root         (0) root         (0)    29987 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/thread/mma_sm60.h
+-rw-rw-r--   0 root         (0) root         (0)     8142 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/thread/mma_sm61.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:46.820156 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/
+-rw-rw-r--   0 root         (0) root         (0)    31930 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_ell_mma.h
+-rwxrwxr-x   0 root         (0) root         (0)     6979 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_gemv_core.h
+-rw-r--r--   0 root         (0) root         (0)    34241 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_mma.h
+-rw-rw-r--   0 root         (0) root         (0)     5123 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_mma_core.h
+-rw-rw-r--   0 root         (0) root         (0)    57426 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_mma_core_simt.h
+-rw-rw-r--   0 root         (0) root         (0)    19257 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm70.h
+-rw-rw-r--   0 root         (0) root         (0)    42310 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm75.h
+-rw-rw-r--   0 root         (0) root         (0)   103000 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm80.h
+-rw-rw-r--   0 root         (0) root         (0)    32106 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h
+-rw-r--r--   0 root         (0) root         (0)    12645 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_access_size.h
+-rw-rw-r--   0 root         (0) root         (0)     7387 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_reduction.h
+-rw-rw-r--   0 root         (0) root         (0)    20975 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_mma_core_wmma.h
+-rw-rw-r--   0 root         (0) root         (0)     7998 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_mma_layernorm_mainloop_fusion.h
+-rw-rw-r--   0 root         (0) root         (0)     5110 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_multistage.h
+-rw-rw-r--   0 root         (0) root         (0)     4627 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_pipelined.h
+-rw-rw-r--   0 root         (0) root         (0)     7113 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_mma_softmax_mainloop_fusion.h
+-rw-rw-r--   0 root         (0) root         (0)     6323 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_mma_with_reduction.h
+-rw-rw-r--   0 root         (0) root         (0)     7121 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex.h
+-rw-rw-r--   0 root         (0) root         (0)     4959 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core.h
+-rw-rw-r--   0 root         (0) root         (0)    65201 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core_sm80.h
+-rw-rw-r--   0 root         (0) root         (0)    25495 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_multistage_trmm_complex.h
+-rw-rw-r--   0 root         (0) root         (0)     8509 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_sparse_mma.h
+-rw-rw-r--   0 root         (0) root         (0)    19515 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_trmm.h
+-rw-r--r--   0 root         (0) root         (0)    24047 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/ell_mma_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    13836 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/ell_mma_pipelined.h
+-rwxrwxr-x   0 root         (0) root         (0)     4726 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/gemv.h
+-rw-rw-r--   0 root         (0) root         (0)     3652 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/index_remat.h
+-rw-rw-r--   0 root         (0) root         (0)     7823 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/mma_base.h
+-rw-r--r--   0 root         (0) root         (0)    27415 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/mma_blas3_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    32894 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/mma_layernorm_mainloop_fusion_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    28015 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/mma_multistage.h
+-rw-rw-r--   0 root         (0) root         (0)    15995 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/mma_pipelined.h
+-rw-rw-r--   0 root         (0) root         (0)     6901 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_base.h
+-rw-r--r--   0 root         (0) root         (0)    22653 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    14746 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_pipelined.h
+-rw-rw-r--   0 root         (0) root         (0)     9864 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/mma_singlestage.h
+-rw-r--r--   0 root         (0) root         (0)    27061 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/mma_softmax_mainloop_fusion_multistage.h
+-rw-rw-r--   0 root         (0) root         (0)     9210 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/mma_sparse_base.h
+-rw-r--r--   0 root         (0) root         (0)    25333 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/mma_sparse_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    20473 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/mma_with_reduction_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    15007 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle.h
+-rw-r--r--   0 root         (0) root         (0)    26450 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle_streamk.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:47.921030 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/
+-rw-rw-r--   0 root         (0) root         (0)    20553 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/default_mma_complex_tensor_op.h
+-rw-rw-r--   0 root         (0) root         (0)     6684 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/default_mma_sparse_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     5160 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op.h
+-rw-rw-r--   0 root         (0) root         (0)     9026 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op_sm80.h
+-rw-rw-r--   0 root         (0) root         (0)     4053 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/default_mma_with_reduction_tensor_op.h
+-rw-rw-r--   0 root         (0) root         (0)     4685 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/default_mma_wmma_tensor_op.h
+-rw-rw-r--   0 root         (0) root         (0)     5725 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/layernorm_scale_bias_transform.h
+-rw-rw-r--   0 root         (0) root         (0)     2619 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma.h
+-rw-r--r--   0 root         (0) root         (0)    37705 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op.h
+-rw-rw-r--   0 root         (0) root         (0)    23132 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_fast_f32.h
+-rw-rw-r--   0 root         (0) root         (0)    78615 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_tile_iterator_sm80.h
+-rw-rw-r--   0 root         (0) root         (0)    21205 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op.h
+-rw-rw-r--   0 root         (0) root         (0)    14589 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op_tile_iterator_sm80.h
+-rw-rw-r--   0 root         (0) root         (0)     6144 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_planar_complex.h
+-rw-rw-r--   0 root         (0) root         (0)     8446 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_simt.h
+-rw-rw-r--   0 root         (0) root         (0)     3079 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_simt_policy.h
+-rw-rw-r--   0 root         (0) root         (0)    59793 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_simt_tile_iterator.h
+-rw-rw-r--   0 root         (0) root         (0)    11758 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_sparse_tensor_op.h
+-rw-rw-r--   0 root         (0) root         (0)    14407 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_tensor_op.h
+-rw-rw-r--   0 root         (0) root         (0)    15721 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_tensor_op_fast_f32.h
+-rw-rw-r--   0 root         (0) root         (0)    18643 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_tensor_op_fragment_iterator.h
+-rw-rw-r--   0 root         (0) root         (0)     2939 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_tensor_op_policy.h
+-rw-rw-r--   0 root         (0) root         (0)     8966 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_tensor_op_sm70.h
+-rw-rw-r--   0 root         (0) root         (0)    11017 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_access_iterator.h
+-rw-rw-r--   0 root         (0) root         (0)   136033 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator.h
+-rw-rw-r--   0 root         (0) root         (0)    99649 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm70.h
+-rw-rw-r--   0 root         (0) root         (0)    75179 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm80.h
+-rw-rw-r--   0 root         (0) root         (0)    13151 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sparse.h
+-rw-rw-r--   0 root         (0) root         (0)    27101 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_wmma.h
+-rw-rw-r--   0 root         (0) root         (0)     7241 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_tensor_op_wmma.h
+-rw-rw-r--   0 root         (0) root         (0)    17303 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_with_reduction_tensor_op.h
+-rw-rw-r--   0 root         (0) root         (0)    19125 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/scale_bias_tile_iterator.h
+-rw-rw-r--   0 root         (0) root         (0)     4610 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/softmax_scale_bias_transform.h
+-rw-rw-r--   0 root         (0) root         (0)     8728 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/tile_iterator_planar_complex.h
+-rw-rw-r--   0 root         (0) root         (0)    23615 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/half.h
+-rw-rw-r--   0 root         (0) root         (0)     6893 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/integer_subbyte.h
+-rw-rw-r--   0 root         (0) root         (0)     2801 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/kernel_launch.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:47.945546 flash_attn-2.0.0/csrc/cutlass/include/cutlass/layout/
+-rw-rw-r--   0 root         (0) root         (0)     3020 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/layout/layout.h
+-rw-r--r--   0 root         (0) root         (0)    35369 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/layout/matrix.h
+-rw-r--r--   0 root         (0) root         (0)     9133 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/layout/permute.h
+-rw-rw-r--   0 root         (0) root         (0)     4696 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/layout/pitch_linear.h
+-rw-rw-r--   0 root         (0) root         (0)    18295 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/layout/tensor.h
+-rw-rw-r--   0 root         (0) root         (0)    29599 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm70.h
+-rw-rw-r--   0 root         (0) root         (0)    33137 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm75.h
+-rw-rw-r--   0 root         (0) root         (0)    29336 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm80.h
+-rw-rw-r--   0 root         (0) root         (0)     3328 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/layout/vector.h
+-rw-rw-r--   0 root         (0) root         (0)   364115 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/matrix.h
+-rw-rw-r--   0 root         (0) root         (0)     4991 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/matrix_coord.h
+-rw-rw-r--   0 root         (0) root         (0)     2726 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/matrix_shape.h
+-rw-r--r--   0 root         (0) root         (0)    71278 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/numeric_conversion.h
+-rw-rw-r--   0 root         (0) root         (0)     3505 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/numeric_types.h
+-rw-rw-r--   0 root         (0) root         (0)     5492 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/pitch_linear_coord.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:47.948434 flash_attn-2.0.0/csrc/cutlass/include/cutlass/platform/
+-rw-r--r--   0 root         (0) root         (0)    26097 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/platform/platform.h
+-rw-rw-r--   0 root         (0) root         (0)    15565 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/predicate_vector.h
+-rw-rw-r--   0 root         (0) root         (0)    20900 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/quaternion.h
+-rw-rw-r--   0 root         (0) root         (0)     2369 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/real.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:47.951427 flash_attn-2.0.0/csrc/cutlass/include/cutlass/reduction/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:47.964358 flash_attn-2.0.0/csrc/cutlass/include/cutlass/reduction/device/
+-rw-rw-r--   0 root         (0) root         (0)     6823 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/reduction/device/reduce_split_k.h
+-rw-rw-r--   0 root         (0) root         (0)     8152 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/reduction/device/tensor_reduce.h
+-rw-rw-r--   0 root         (0) root         (0)    11579 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_contiguous.h
+-rw-rw-r--   0 root         (0) root         (0)    11448 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_strided.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:47.968023 flash_attn-2.0.0/csrc/cutlass/include/cutlass/reduction/kernel/
+-rw-rw-r--   0 root         (0) root         (0)     8762 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/reduction/kernel/reduce_softmax_final.h
+-rw-rw-r--   0 root         (0) root         (0)     7897 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/reduction/kernel/reduce_split_k.h
+-rw-r--r--   0 root         (0) root         (0)    20685 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_contiguous.h
+-rw-r--r--   0 root         (0) root         (0)    21662 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_strided.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:47.979019 flash_attn-2.0.0/csrc/cutlass/include/cutlass/reduction/thread/
+-rw-rw-r--   0 root         (0) root         (0)     7208 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/reduction/thread/reduce.h
+-rw-rw-r--   0 root         (0) root         (0)     6790 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/reduction/thread/reduction_operators.h
+-rw-rw-r--   0 root         (0) root         (0)     2936 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/reduction/threadblock_swizzle.h
+-rw-r--r--   0 root         (0) root         (0)     5929 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/relatively_equal.h
+-rw-r--r--   0 root         (0) root         (0)     4186 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/semaphore.h
+-rw-rw-r--   0 root         (0) root         (0)    17243 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/subbyte_reference.h
+-rw-rw-r--   0 root         (0) root         (0)     8964 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/tensor_coord.h
+-rw-rw-r--   0 root         (0) root         (0)    12207 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/tensor_ref.h
+-rw-rw-r--   0 root         (0) root         (0)    11201 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/tensor_ref_planar_complex.h
+-rw-rw-r--   0 root         (0) root         (0)     9509 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/tensor_view.h
+-rw-rw-r--   0 root         (0) root         (0)    10250 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/tensor_view_planar_complex.h
+-rw-rw-r--   0 root         (0) root         (0)    13017 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/tfloat32.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:47.984622 flash_attn-2.0.0/csrc/cutlass/include/cutlass/thread/
+-rw-rw-r--   0 root         (0) root         (0)     5931 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/thread/matrix.h
+-rw-rw-r--   0 root         (0) root         (0)     2581 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/trace.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:47.987496 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/
+-rw-rw-r--   0 root         (0) root         (0)    33349 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/pitch_linear_thread_map.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:47.990979 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/thread/
+-rw-rw-r--   0 root         (0) root         (0)     3835 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/thread/transpose.h
+-rw-rw-r--   0 root         (0) root         (0)     4309 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/thread/unary_op.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:48.060712 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/
+-rw-rw-r--   0 root         (0) root         (0)     6181 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/ell_iterator.h
+-rw-rw-r--   0 root         (0) root         (0)    44443 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_access_iterator.h
+-rw-rw-r--   0 root         (0) root         (0)    44309 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_iterator.h
+-rw-rw-r--   0 root         (0) root         (0)    12890 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_access_iterator.h
+-rw-rw-r--   0 root         (0) root         (0)    11097 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    70684 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator.h
+-rw-rw-r--   0 root         (0) root         (0)    28232 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_2dthreadtile.h
+-rwxrwxr-x   0 root         (0) root         (0)    10243 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_params.h
+-rw-rw-r--   0 root         (0) root         (0)    31412 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_triangular_matrix.h
+-rw-r--r--   0 root         (0) root         (0)    62672 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator.h
+-rw-rw-r--   0 root         (0) root         (0)    27175 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h
+-rw-rw-r--   0 root         (0) root         (0)    28064 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_triangular_matrix.h
+-rw-rw-r--   0 root         (0) root         (0)    13088 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/predicated_vector_access_iterator.h
+-rw-rw-r--   0 root         (0) root         (0)     8232 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/regular_scale_bias_vector_access_iterator.h
+-rw-rw-r--   0 root         (0) root         (0)     2638 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator.h
+-rw-rw-r--   0 root         (0) root         (0)    13283 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear.h
+-rw-rw-r--   0 root         (0) root         (0)    18623 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear_direct_conv.h
+-rw-rw-r--   0 root         (0) root         (0)    27922 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op.h
+-rw-rw-r--   0 root         (0) root         (0)    47789 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op_sm80.h
+-rw-rw-r--   0 root         (0) root         (0)     2616 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    16510 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear.h
+-rw-rw-r--   0 root         (0) root         (0)    15486 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear_2dthreadtile.h
+-rw-rw-r--   0 root         (0) root         (0)    36050 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)    43663 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op_sm70.h
+-rw-rw-r--   0 root         (0) root         (0)     5226 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/vector_iterator.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:48.063938 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/warp/
+-rw-rw-r--   0 root         (0) root         (0)     8828 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/warp/vector_fragment_iterator.h
+-rw-r--r--   0 root         (0) root         (0)     8179 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/uint128.h
+-rw-rw-r--   0 root         (0) root         (0)     4543 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/include/cutlass/wmma_array.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.420875 flash_attn-2.0.0/csrc/cutlass/test/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:48.067030 flash_attn-2.0.0/csrc/cutlass/test/unit/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:48.070165 flash_attn-2.0.0/csrc/cutlass/test/unit/common/
+-rw-rw-r--   0 root         (0) root         (0)     4900 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/common/cutlass_unit_test.h
+-rw-rw-r--   0 root         (0) root         (0)     5381 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/common/filter_architecture.cpp
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.424279 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:49.236350 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/
+-rw-rw-r--   0 root         (0) root         (0)    21797 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/cache_testbed_output.h
+-rw-rw-r--   0 root         (0) root         (0)     5344 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)     5443 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    11470 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5239 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)     9110 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)     8485 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5243 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5378 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    12054 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_few_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     9603 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_fixed_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5267 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)     5357 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5089 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
+-rw-rw-r--   0 root         (0) root         (0)    13690 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5390 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5191 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)    11136 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)     5291 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     3551 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)     5157 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu
+-rwxrwxr-x   0 root         (0) root         (0)     8278 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_qf32nhwc_qf32nhwc_qf32nhwc_simt_f32_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)    20555 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)    20647 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5155 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)     5239 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    26114 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)    26210 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5111 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)     5194 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5738 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5439 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)     7363 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)     3984 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_with_reduction_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    39452 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_problems.h
+-rw-rw-r--   0 root         (0) root         (0)    14471 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     4662 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    26224 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_testbed.h
+-rw-r--r--   0 root         (0) root         (0)    22092 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_testbed_interleaved.h
+-rw-rw-r--   0 root         (0) root         (0)     5179 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)     5358 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5264 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     3615 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)     7591 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)    10514 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5157 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5772 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    23526 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_with_broadcast_testbed.h
+-rw-r--r--   0 root         (0) root         (0)    21512 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_with_reduction_testbed.h
+-rw-rw-r--   0 root         (0) root         (0)     5135 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5347 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     3736 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)     6560 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5257 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    12276 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv3d_problems.h
+-rw-r--r--   0 root         (0) root         (0)    21643 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv3d_testbed.h
+-rw-rw-r--   0 root         (0) root         (0)     3622 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)     6560 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5256 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    17700 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/depthwise_conv2d_direct_conv_testbed.h
+-rw-rw-r--   0 root         (0) root         (0)    18451 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
+-rw-rw-r--   0 root         (0) root         (0)    22194 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_fixed_stride_dilation_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
+-rw-rw-r--   0 root         (0) root         (0)     9383 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
+-rw-r--r--   0 root         (0) root         (0)    16100 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/group_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:49.283451 flash_attn-2.0.0/csrc/cutlass/test/unit/core/
+-rw-rw-r--   0 root         (0) root         (0)     7365 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/core/array.cu
+-rw-rw-r--   0 root         (0) root         (0)     7353 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/core/bfloat16.cu
+-rw-rw-r--   0 root         (0) root         (0)     6981 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/core/complex.cu
+-rw-rw-r--   0 root         (0) root         (0)     4009 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/core/float8.cu
+-rw-rw-r--   0 root         (0) root         (0)    13001 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/core/functional.cu
+-rw-rw-r--   0 root         (0) root         (0)     3553 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/core/half.cu
+-rw-rw-r--   0 root         (0) root         (0)     5295 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/core/matrix.cu
+-rw-rw-r--   0 root         (0) root         (0)     8592 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/core/matrix_coord.cu
+-rw-rw-r--   0 root         (0) root         (0)    10684 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/core/numeric_conversion.cu
+-rw-rw-r--   0 root         (0) root         (0)     8148 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/core/predicate_vector.cu
+-rw-rw-r--   0 root         (0) root         (0)     5777 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/core/quaternion.cu
+-rw-rw-r--   0 root         (0) root         (0)     6746 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/core/tensor_ref.cu
+-rw-rw-r--   0 root         (0) root         (0)     8885 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/core/tensor_view.cu
+-rw-rw-r--   0 root         (0) root         (0)     2050 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/core/test_unit_core.cpp
+-rw-rw-r--   0 root         (0) root         (0)     7088 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/core/tfloat32.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.426642 flash_attn-2.0.0/csrc/cutlass/test/unit/cute/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:49.292205 flash_attn-2.0.0/csrc/cutlass/test/unit/cute/ampere/
+-rw-rw-r--   0 root         (0) root         (0)     3527 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/cute/ampere/cp_async.cu
+-rw-rw-r--   0 root         (0) root         (0)    14320 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/cute/ampere/ldsm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:49.317008 flash_attn-2.0.0/csrc/cutlass/test/unit/cute/core/
+-rw-rw-r--   0 root         (0) root         (0)     3332 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/cute/core/bitfield.cpp
+-rw-rw-r--   0 root         (0) root         (0)     4861 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/cute/core/coalesce.cpp
+-rw-rw-r--   0 root         (0) root         (0)     5620 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/cute/core/compare.cpp
+-rw-rw-r--   0 root         (0) root         (0)     7178 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/cute/core/complement.cpp
+-rw-rw-r--   0 root         (0) root         (0)    12569 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/cute/core/composition.cpp
+-rw-rw-r--   0 root         (0) root         (0)     4856 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/cute/core/inverse_left.cpp
+-rw-rw-r--   0 root         (0) root         (0)     6702 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/cute/core/inverse_right.cpp
+-rw-rw-r--   0 root         (0) root         (0)     6734 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/cute/core/logical_divide.cpp
+-rw-rw-r--   0 root         (0) root         (0)     5914 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/cute/core/logical_product.cpp
+-rw-rw-r--   0 root         (0) root         (0)     3488 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/cute/core/mixedbits.cpp
+-rw-rw-r--   0 root         (0) root         (0)     2342 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/cute/core/transform.cpp
+-rw-rw-r--   0 root         (0) root         (0)    13304 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/cute/core/tuple.cpp
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:49.327120 flash_attn-2.0.0/csrc/cutlass/test/unit/cute/hopper/
+-rw-r--r--   0 root         (0) root         (0)    14365 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/cute/hopper/stsm.cu
+-rw-r--r--   0 root         (0) root         (0)    18990 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/cute/hopper/tma_load.cu
+-rw-r--r--   0 root         (0) root         (0)    13875 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/cute/hopper/tma_store.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:49.335377 flash_attn-2.0.0/csrc/cutlass/test/unit/cute/layout/
+-rw-rw-r--   0 root         (0) root         (0)     4544 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/cute/layout/layout_operator.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.430388 flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:49.339001 flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/thread/
+-rw-rw-r--   0 root         (0) root         (0)    15818 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/thread/activation.cu
+-rw-rw-r--   0 root         (0) root         (0)     6534 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/thread/linear_combination.cu
+-rw-rw-r--   0 root         (0) root         (0)     9964 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/thread/linear_combination_planar_complex.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:50.379322 flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/threadblock/
+-rw-rw-r--   0 root         (0) root         (0)    13824 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/threadblock/epilogue_planar_complex.cu
+-rw-rw-r--   0 root         (0) root         (0)    27176 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/threadblock/epilogue_simt.cu
+-rw-rw-r--   0 root         (0) root         (0)    12061 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm60.cu
+-rw-rw-r--   0 root         (0) root         (0)    25275 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm61.cu
+-rw-rw-r--   0 root         (0) root         (0)    84612 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/threadblock/epilogue_tensor_op.cu
+-rw-rw-r--   0 root         (0) root         (0)    70486 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/threadblock/epilogue_volta_tensor_op.cu
+-rw-rw-r--   0 root         (0) root         (0)    25293 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_tensor_op.cu
+-rw-rw-r--   0 root         (0) root         (0)    13012 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_testbed.h
+-rw-rw-r--   0 root         (0) root         (0)     7743 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/threadblock/epilogue_wmma_tensor_op_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)    19178 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/threadblock/output_tile_threadmap.cu
+-rw-rw-r--   0 root         (0) root         (0)    28433 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/threadblock/predicated_tile_iterator.cu
+-rw-rw-r--   0 root         (0) root         (0)    11038 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/threadblock/testbed.h
+-rw-rw-r--   0 root         (0) root         (0)    11734 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/threadblock/testbed_planar_complex.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:50.382651 flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/warp/
+-rw-rw-r--   0 root         (0) root         (0)     6783 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/warp/fragment_iterator_tensor_op.cu
+-rw-rw-r--   0 root         (0) root         (0)     7275 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/warp/fragment_iterator_volta_tensor_op.cu
+-rw-rw-r--   0 root         (0) root         (0)     6616 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/warp/fragment_iterator_wmma_tensor_op.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.433301 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:00.263579 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/
+-rw-rw-r--   0 root         (0) root         (0)    10189 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)    17899 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     8933 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_wmma_tensor_op_s32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)    10164 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)    17984 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     8915 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_wmma_tensor_op_s32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)    16447 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_bf16n_bf16n_f32t_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    16575 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_bf16t_bf16t_bf16t_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     8318 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_cf32n_cf32t_cf32t_tensor_op_tf32_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     8317 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_cf32t_cf32n_cf32t_tensor_op_tf32_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     6714 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     6747 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm90.cu
+-rw-rw-r--   0 root         (0) root         (0)     7895 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     7930 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm90.cu
+-rw-rw-r--   0 root         (0) root         (0)     6516 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     6549 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm90.cu
+-rw-rw-r--   0 root         (0) root         (0)     9016 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     9053 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm90.cu
+-rw-rw-r--   0 root         (0) root         (0)     4628 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_direct_store_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     6165 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f16_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)     6124 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f32_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)     9634 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)    16357 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13189 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sparse_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     8845 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_volta_tensor_op_f32_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)    13583 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f16_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)    13464 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f32_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)     9571 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)    16239 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     6140 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_wmma_tensor_op_f32_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)     9544 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)    16417 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    13075 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     8775 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_volta_tensor_op_f32_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)    11470 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_wmma_tensor_op_f32_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)     6156 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f16_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)     6116 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f32_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)     3528 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)     3539 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     7965 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)    16470 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    13273 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sparse_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     3648 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     8608 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_volta_tensor_op_f16_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)    13518 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f16_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)     3645 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f32_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)     6096 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32n_wmma_tensor_op_f32_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)     7845 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)    18135 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    13008 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     8505 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_volta_tensor_op_f32_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)    11497 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_wmma_tensor_op_f32_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)    11090 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_singlestage_wmma_tensor_op_f16_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)     6156 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f16_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)     6116 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f32_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)    11066 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_singlestage_wmma_tensor_op_f16_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)    17114 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_broadcast_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     3528 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)     3540 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     7964 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)    16457 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    13266 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sparse_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     8933 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_volta_tensor_op_f16_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)    13551 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f16_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)    13540 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f32_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)     6130 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32n_wmma_tensor_op_f32_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)     8160 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_singlestage_wmma_tensor_op_f32_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)     7847 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)    16131 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    13014 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     8754 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_volta_tensor_op_f32_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)    11497 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_wmma_tensor_op_f32_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)     6147 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f16_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)     6107 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f32_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)    13518 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f16_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)    13398 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f32_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)     7845 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)    16149 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     6119 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_wmma_tensor_op_f32_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)     7827 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)    16101 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     9526 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     7898 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_volta_tensor_op_f32_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)    11470 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_wmma_tensor_op_f32_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)     3584 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_bf16_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     3473 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    12967 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    12931 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f32n_f32t_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    12930 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f32t_f32n_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    12895 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f32t_f32t_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     8349 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     7300 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm90.cu
+-rw-rw-r--   0 root         (0) root         (0)     8348 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     7291 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm90.cu
+-rw-rw-r--   0 root         (0) root         (0)    10240 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_grouped_scheduler_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    26146 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_grouped_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    11339 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)     7346 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)    12397 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     6859 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)     7239 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     8121 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)    16882 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     8407 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_wmma_tensor_op_s32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)     8103 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)    17111 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    12637 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sparse_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     8388 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_wmma_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    10044 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    17544 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    10020 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    17544 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     9588 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)    11288 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     7977 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)    16531 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5693 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_wmma_tensor_op_s32_sm72.cu
+-rw-rw-r--   0 root         (0) root         (0)     7959 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)    16691 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    12408 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sparse_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     6864 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_wmma_tensor_op_s32_sm72.cu
+-rw-r--r--   0 root         (0) root         (0)     7744 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16531 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     6675 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_wmma_tensor_op_s32_sm72.cu
+-rw-r--r--   0 root         (0) root         (0)     7752 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16484 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     6663 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_wmma_tensor_op_s32_sm72.cu
+-rw-rw-r--   0 root         (0) root         (0)     4663 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_splitk_serial_tensor_op_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)     4945 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_splitk_simt_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)     6616 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)    10581 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)    16950 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_tf32n_tf32n_f32t_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    16902 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_tf32n_tf32t_f32t_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    15131 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_tf32t_tf32n_f32t_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    16855 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_tf32t_tf32t_f32t_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     6854 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_u8t_u8n_s32t_wmma_tensor_op_s32_sm72.cu
+-rw-rw-r--   0 root         (0) root         (0)     6686 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_universal_cf32n_cf32n_cf32n_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     6755 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     6687 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     4726 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32n_tensor_op_f32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)     4718 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32t_tensor_op_f32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)    16715 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_with_broadcast_f16n_f16n_f16n_tensorop_f32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)    12841 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_with_reduction_f16n_f16n_f16n_tensorop_f32_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)     4544 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_with_reduction_f16t_f16n_f16n_tensorop_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13157 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemv.cu
+-rw-rw-r--   0 root         (0) root         (0)     6028 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_ls_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     6031 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_rs_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     6064 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_ls_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     6067 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_rs_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     4909 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/hemm_cf64_cf64_cf64_tensor_op_f64_sm90.cu
+-rw-rw-r--   0 root         (0) root         (0)     6088 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     6037 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     6040 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_rs_f64_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5382 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5406 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_fast_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5402 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/her2k_cf64_cf64_tensor_op_f64_sm90.cu
+-rw-rw-r--   0 root         (0) root         (0)    13055 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/her2k_cf64h_cf64n_tensor_op_f64_grouped_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    13027 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5388 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     6939 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/her2k_cf64n_cf64t_tensor_op_f64_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     7677 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     7725 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_fast_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     3854 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/herk_cf64_cf64_tensor_op_f64_sm90.cu
+-rw-rw-r--   0 root         (0) root         (0)     6396 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/herk_cf64h_cf64n_tensor_op_f64_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    10157 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/multistage_testbed.h
+-rw-rw-r--   0 root         (0) root         (0)    10306 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/multistage_testbed_interleaved.h
+-rw-rw-r--   0 root         (0) root         (0)    11186 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/rank_2k_grouped_scheduler_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    46795 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_cgemm_nn_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)    54085 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)     8318 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    46687 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)     8411 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    46578 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_cgemm_tt_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)    40533 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_dgemm_nn_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)    47656 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_dgemm_nt_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)    40441 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_dgemm_tn_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)    40354 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_dgemm_tt_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)     3513 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_f8gemm_tn_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)    89517 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_hgemm_nn_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)    89304 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_hgemm_nt_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)    89304 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_hgemm_tn_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)    89091 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_hgemm_tt_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)    69175 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_igemm_nn_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)    71438 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_igemm_nt_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)    67796 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_igemm_tn_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)    70056 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_igemm_tt_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)     7156 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61.cu
+-rw-rw-r--   0 root         (0) root         (0)     6067 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_perf.cu
+-rw-rw-r--   0 root         (0) root         (0)     9063 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_sliced_k.cu
+-rw-rw-r--   0 root         (0) root         (0)    35894 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_qgemm_nn_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)    35813 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_qgemm_nt_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)    35813 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_qgemm_tn_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)    35732 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_qgemm_tt_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)    70872 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_sgemm_nn_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)    73136 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)     8870 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    69488 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)     8865 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    71755 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_sgemm_tt_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)    33231 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_zgemm_nn_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)    33156 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_zgemm_nt_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)    33156 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_zgemm_tn_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)    33081 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_zgemm_tt_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)     5238 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/sm50_gemm_f32_f32_f32_simt.cu
+-rw-rw-r--   0 root         (0) root         (0)     5253 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/sm50_gemm_f64_f64_f64_simt.cu
+-rw-rw-r--   0 root         (0) root         (0)     5357 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/sm61_gemm_s8_s8_s32_simt.cu
+-rw-rw-r--   0 root         (0) root         (0)     5479 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/sm80_gemm_f16_f16_f32_tensor_op_f32.cu
+-rw-rw-r--   0 root         (0) root         (0)     5238 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/sm80_gemm_f32_f32_f32_simt.cu
+-rw-rw-r--   0 root         (0) root         (0)     5253 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/sm80_gemm_f64_f64_f64_simt.cu
+-rw-rw-r--   0 root         (0) root         (0)     3875 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/sm80_gemm_f64_f64_f64_tensor_op_f64.cu
+-rw-rw-r--   0 root         (0) root         (0)     3734 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/sm80_gemm_s8_s8_s32_tensor_op.cu
+-rw-rw-r--   0 root         (0) root         (0)     5387 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/sm80_gemm_tf32_tf32_f32_tensor_op_f32.cu
+-rw-r--r--   0 root         (0) root         (0)     7436 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/sm90_gemm_bf16_bf16_bf16_alignx_tensor_op_f32.cu
+-rw-r--r--   0 root         (0) root         (0)     7409 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/sm90_gemm_bf16_bf16_bf16_tensor_op_f32.cu
+-rw-r--r--   0 root         (0) root         (0)    17391 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_alignx_tensor_op.cu
+-rw-r--r--   0 root         (0) root         (0)    42504 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op.cu
+-rw-r--r--   0 root         (0) root         (0)    22602 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_unspecialized.cu
+-rw-r--r--   0 root         (0) root         (0)    22874 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized.cu
+-rw-r--r--   0 root         (0) root         (0)    42526 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_persistent.cu
+-rw-r--r--   0 root         (0) root         (0)     3810 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/sm90_gemm_f32_f32_f32_tensor_op_f32.cu
+-rw-r--r--   0 root         (0) root         (0)     5976 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/sm90_gemm_s8_s8_s8_alignx_tensor_op_s32.cu
+-rw-r--r--   0 root         (0) root         (0)     9313 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/sm90_gemm_s8_s8_s8_tensor_op_s32.cu
+-rw-r--r--   0 root         (0) root         (0)     5986 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_alignx_tensor_op_f32.cu
+-rw-r--r--   0 root         (0) root         (0)     7268 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_tensor_op_f32.cu
+-rw-rw-r--   0 root         (0) root         (0)     5923 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_ls_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5926 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_rs_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5959 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_ls_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5962 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_rs_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     4839 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_cf64_cf64_cf64_tensor_op_f64_sm90.cu
+-rw-rw-r--   0 root         (0) root         (0)     5983 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5932 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5935 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_rs_f64_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    15203 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     8623 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_rs_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    15104 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     4777 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_f64_f64_tensor_op_f64_sm90.cu
+-rw-rw-r--   0 root         (0) root         (0)     8103 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_ls_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     8108 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_rs_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     8088 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_ls_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     8093 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_rs_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     8073 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_ls_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     8078 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_rs_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     8058 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_ls_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     8063 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_rs_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    15071 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_ls_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     8551 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_rs_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    14972 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_tf32t_f32t_tensor_op_f32_ls_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5362 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5386 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_fast_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5356 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5380 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_fast_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5379 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_cf64_cf64_tensor_op_f64_sm90.cu
+-rw-rw-r--   0 root         (0) root         (0)    12952 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5368 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     7208 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_grouped_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5362 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     7199 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64n_tensor_op_f64_grouped_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     7190 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64t_tensor_op_f64_grouped_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     4794 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_f32n_f32n_tensor_op_fast_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     4783 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_f32t_f32n_tensor_op_fast_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     4740 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_f64_f64_tensor_op_f64_sm90.cu
+-rw-rw-r--   0 root         (0) root         (0)    19145 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_grouped_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     7991 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    11015 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_grouped_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     7976 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    12342 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_grouped_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     7961 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    12321 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_f64t_f64t_tensor_op_f64_grouped_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     4786 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_tf32n_f32n_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     4775 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_tf32t_f32n_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     4993 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5017 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_fast_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     4987 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5011 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_fast_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     5024 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syrk_cf64_cf64_tensor_op_f64_sm90.cu
+-rw-rw-r--   0 root         (0) root         (0)     4996 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syrk_cf64n_cf64n_tensor_op_f64_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     3793 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     4990 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    16083 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syrk_f32n_f32t_tensor_op_fast_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    16041 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syrk_f32t_f32t_tensor_op_fast_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     4530 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syrk_f64_f64_tensor_op_f64_sm90.cu
+-rw-rw-r--   0 root         (0) root         (0)     7451 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syrk_f64n_f64t_tensor_op_f64_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     9401 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syrk_f64t_f64n_tensor_op_f64_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    16027 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syrk_tf32n_f32t_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    15985 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syrk_tf32t_f32t_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    20465 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed.h
+-rw-rw-r--   0 root         (0) root         (0)     8264 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_complex.h
+-rw-rw-r--   0 root         (0) root         (0)    20736 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_gemm_with_broadcast.h
+-rw-rw-r--   0 root         (0) root         (0)    19479 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_gemm_with_reduction.h
+-rw-rw-r--   0 root         (0) root         (0)    16502 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_grouped.h
+-rw-rw-r--   0 root         (0) root         (0)    16562 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k.h
+-rw-rw-r--   0 root         (0) root         (0)    17002 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k_scheduler.h
+-rw-rw-r--   0 root         (0) root         (0)    14698 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_grouped_scheduler.h
+-rw-rw-r--   0 root         (0) root         (0)    10262 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_interleaved.h
+-rw-rw-r--   0 root         (0) root         (0)     9481 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_planar_complex.h
+-rw-rw-r--   0 root         (0) root         (0)    20898 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_rank2k_universal.h
+-rw-rw-r--   0 root         (0) root         (0)    15652 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_rank_k_universal.h
+-rw-rw-r--   0 root         (0) root         (0)     8639 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_sanity.h
+-rw-r--r--   0 root         (0) root         (0)    15901 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_sparse.h
+-rw-rw-r--   0 root         (0) root         (0)     6124 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_splitk.h
+-rw-rw-r--   0 root         (0) root         (0)    19993 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_symm_universal.h
+-rw-rw-r--   0 root         (0) root         (0)    20332 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_trmm_universal.h
+-rw-rw-r--   0 root         (0) root         (0)    17443 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_universal.h
+-rw-rw-r--   0 root         (0) root         (0)     2626 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_utils.h
+-rw-rw-r--   0 root         (0) root         (0)     9916 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     9988 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_fast_f32_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     4989 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_cf64_cf64_cf64_tensor_op_f64_sm90.cu
+-rw-rw-r--   0 root         (0) root         (0)     4992 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     9762 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    15614 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     8733 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_rs_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    14089 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    14444 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32t_tensor_op_fast_f32_ls_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     4608 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_f64_f64_f64_tensor_op_f64_sm90.cu
+-rw-rw-r--   0 root         (0) root         (0)    12798 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_ls_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    12809 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_rs_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    12764 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_f64n_f64t_f64t_tensor_op_f64_rs_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    12768 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_ls_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    12779 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_rs_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    15504 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_ls_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     8673 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_rs_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    13989 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32n_tensor_op_f32_ls_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    14344 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32t_tensor_op_f32_ls_sm80.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:00.269579 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/kernel/
+-rwxrwxr-x   0 root         (0) root         (0)    46470 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/kernel/batched_gemv.cu
+-rwxrwxr-x   0 root         (0) root         (0)    14362 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/kernel/testbed_gemv.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:00.274991 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/thread/
+-rw-rw-r--   0 root         (0) root         (0)     4847 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/thread/gemm_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)    12503 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/thread/gemm_sm60.cu
+-rw-rw-r--   0 root         (0) root         (0)     3109 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/thread/gemm_sm61.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:00.285556 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/thread/host/
+-rw-rw-r--   0 root         (0) root         (0)     5198 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/thread/host/gemm_sm60_host.cu
+-rw-rw-r--   0 root         (0) root         (0)     7161 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/thread/host/testbed_host.h
+-rw-rw-r--   0 root         (0) root         (0)     7124 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/thread/testbed.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:00.343243 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/
+-rw-rw-r--   0 root         (0) root         (0)    25036 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/batched_gemv.cu
+-rw-rw-r--   0 root         (0) root         (0)     4345 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/epilogue_workspace.cu
+-rw-rw-r--   0 root         (0) root         (0)   135045 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_multistage.cu
+-rw-rw-r--   0 root         (0) root         (0)     4644 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_multistage_slicedk.cu
+-rw-rw-r--   0 root         (0) root         (0)    94442 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse.cu
+-rw-rw-r--   0 root         (0) root         (0)    17090 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse_testbed.h
+-rw-rw-r--   0 root         (0) root         (0)    13897 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed.h
+-rw-rw-r--   0 root         (0) root         (0)    14539 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed_slicedk.h
+-rw-rw-r--   0 root         (0) root         (0)    49052 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_pipelined_simt.cu
+-rw-rw-r--   0 root         (0) root         (0)     8407 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_pipelined_slicedk.cu
+-rw-rw-r--   0 root         (0) root         (0)    18705 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)    78122 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)    21051 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    13771 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_pipelined_testbed.h
+-rw-rw-r--   0 root         (0) root         (0)    14239 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_pipelined_testbed_slicedk.h
+-rw-rw-r--   0 root         (0) root         (0)    29772 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)    12395 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)     3502 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_planar_complex_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    12138 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_planar_complex_testbed.h
+-rw-rw-r--   0 root         (0) root         (0)    16308 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_singlestage_wmma_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)    12502 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_singlestage_wmma_sm75.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:00.387772 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/
+-rw-rw-r--   0 root         (0) root         (0)    22128 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/gemm_complex_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    10916 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/gemm_complex_sm90.cu
+-rw-rw-r--   0 root         (0) root         (0)     9873 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/gemm_gaussian_complex_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    18220 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/gemm_sm50.cu
+-rw-rw-r--   0 root         (0) root         (0)     4920 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/gemm_sm60.cu
+-rw-rw-r--   0 root         (0) root         (0)     6291 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/gemm_sm61.cu
+-rw-rw-r--   0 root         (0) root         (0)     9297 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/gemm_sm70.cu
+-rw-rw-r--   0 root         (0) root         (0)    37942 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/gemm_sm75.cu
+-rw-rw-r--   0 root         (0) root         (0)    81659 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/gemm_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)     9089 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/gemm_sm90.cu
+-rw-rw-r--   0 root         (0) root         (0)    48928 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/gemm_sparse_sm80.cu
+-rw-rw-r--   0 root         (0) root         (0)    49647 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/testbed.h
+-rw-rw-r--   0 root         (0) root         (0)    25780 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/wmma_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     7544 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/wmma_sm72.cu
+-rw-rw-r--   0 root         (0) root         (0)     6487 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/wmma_sm75.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:00.394017 flash_attn-2.0.0/csrc/cutlass/test/unit/layout/
+-rw-rw-r--   0 root         (0) root         (0)     5788 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/layout/matrix.cu
+-rw-rw-r--   0 root         (0) root         (0)     5984 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/layout/tensor.cu
+-rw-rw-r--   0 root         (0) root         (0)     7081 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/layout/tensor_nhwc.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.439506 flash_attn-2.0.0/csrc/cutlass/test/unit/nvrtc/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.440529 flash_attn-2.0.0/csrc/cutlass/test/unit/nvrtc/cutlass/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:00.402545 flash_attn-2.0.0/csrc/cutlass/test/unit/nvrtc/cutlass/nvrtc/
+-rw-rw-r--   0 root         (0) root         (0)     2096 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/nvrtc/cutlass/nvrtc/environment.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.442665 flash_attn-2.0.0/csrc/cutlass/test/unit/nvrtc/kernel/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:00.405936 flash_attn-2.0.0/csrc/cutlass/test/unit/nvrtc/kernel/thread/
+-rw-rw-r--   0 root         (0) root         (0)     2915 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/nvrtc/kernel/thread/testbed_kernel.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:00.409219 flash_attn-2.0.0/csrc/cutlass/test/unit/nvrtc/stdlib/
+-rw-rw-r--   0 root         (0) root         (0)        0 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/nvrtc/stdlib/assert.h
+-rw-rw-r--   0 root         (0) root         (0)     4250 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/nvrtc/stdlib/stdint.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:00.414857 flash_attn-2.0.0/csrc/cutlass/test/unit/nvrtc/thread/
+-rw-rw-r--   0 root         (0) root         (0)     5727 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/nvrtc/thread/gemm_nvrtc.cu
+-rw-rw-r--   0 root         (0) root         (0)    10328 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/nvrtc/thread/testbed.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:00.430369 flash_attn-2.0.0/csrc/cutlass/test/unit/pipeline/
+-rw-r--r--   0 root         (0) root         (0)    15532 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/pipeline/pipeline_async.cu
+-rw-r--r--   0 root         (0) root         (0)    15490 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/pipeline/pipeline_tma_async.cu
+-rw-r--r--   0 root         (0) root         (0)    17098 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/pipeline/pipeline_tma_async_warp_specialized.cu
+-rw-r--r--   0 root         (0) root         (0)    20252 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/pipeline/pipeline_tma_async_warp_specialized_persistent.cu
+-rw-r--r--   0 root         (0) root         (0)     7623 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/pipeline/sequence_barrier.cu
+-rw-rw-r--   0 root         (0) root         (0)     4327 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/pipeline/testbed.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.446674 flash_attn-2.0.0/csrc/cutlass/test/unit/reduction/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:00.436308 flash_attn-2.0.0/csrc/cutlass/test/unit/reduction/device/
+-rw-rw-r--   0 root         (0) root         (0)    14684 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/reduction/device/tensor_reduce_contiguous.cu
+-rw-rw-r--   0 root         (0) root         (0)    15609 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/reduction/device/tensor_reduce_strided.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:00.442165 flash_attn-2.0.0/csrc/cutlass/test/unit/reduction/kernel/
+-rw-rw-r--   0 root         (0) root         (0)    11350 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/reduction/kernel/reduce_splitk.cu
+-rw-rw-r--   0 root         (0) root         (0)     2228 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/reduction/kernel/reduce_splitk_testbed.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:00.447744 flash_attn-2.0.0/csrc/cutlass/test/unit/reduction/thread/
+-rw-rw-r--   0 root         (0) root         (0)     3110 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/reduction/thread/reduction_thread.cu
+-rw-rw-r--   0 root         (0) root         (0)     6657 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/reduction/thread/testbed.h
+-rw-rw-r--   0 root         (0) root         (0)     2047 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/test_unit.cpp
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.449911 flash_attn-2.0.0/csrc/cutlass/test/unit/transform/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:00.453683 flash_attn-2.0.0/csrc/cutlass/test/unit/transform/threadblock/
+-rw-rw-r--   0 root         (0) root         (0)    25527 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/transform/threadblock/predicated_tile_iterator.cu
+-rw-rw-r--   0 root         (0) root         (0)     9501 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/transform/threadblock/regular_tile_iterator_tensor_op.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:00.459347 flash_attn-2.0.0/csrc/cutlass/test/unit/util/
+-rw-rw-r--   0 root         (0) root         (0)     2663 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/util/cutlass_test_levels.cu
+-rw-rw-r--   0 root         (0) root         (0)     7474 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/test/unit/util/tensor_reduce.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.470791 flash_attn-2.0.0/csrc/cutlass/tools/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.468204 flash_attn-2.0.0/csrc/cutlass/tools/library/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.454674 flash_attn-2.0.0/csrc/cutlass/tools/library/include/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.455785 flash_attn-2.0.0/csrc/cutlass/tools/library/include/cutlass/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:01.481237 flash_attn-2.0.0/csrc/cutlass/tools/library/include/cutlass/library/
+-rw-r--r--   0 root         (0) root         (0)     4118 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/include/cutlass/library/arch_mappings.h
+-rw-r--r--   0 root         (0) root         (0)    16013 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/include/cutlass/library/handle.h
+-rw-r--r--   0 root         (0) root         (0)    38761 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/include/cutlass/library/library.h
+-rw-rw-r--   0 root         (0) root         (0)     4070 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/include/cutlass/library/manifest.h
+-rw-r--r--   0 root         (0) root         (0)    17934 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/include/cutlass/library/operation_table.h
+-rw-rw-r--   0 root         (0) root         (0)     2724 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/include/cutlass/library/singleton.h
+-rw-rw-r--   0 root         (0) root         (0)     7904 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/include/cutlass/library/util.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.457656 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.458696 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.459710 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:01.487204 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/
+-rw-r--r--   0 root         (0) root         (0)     2788 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/compiler.h
+-rw-r--r--   0 root         (0) root         (0)     2460 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/cute.cpp
+-rw-r--r--   0 root         (0) root         (0)     6224 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/cutlass.cpp
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:01.506630 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/
+-rw-r--r--   0 root         (0) root         (0)     2851 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/arch.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:01.509776 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/
+-rw-r--r--   0 root         (0) root         (0)     5897 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/conv_problem_size.h
+-rw-r--r--   0 root         (0) root         (0)     4763 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/convolution.h
+-rw-r--r--   0 root         (0) root         (0)     2650 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/host.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:01.517339 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/
+-rw-r--r--   0 root         (0) root         (0)     6845 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_generic.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:02.550437 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/
+-rw-r--r--   0 root         (0) root         (0)     3003 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/binary_ops.h
+-rw-r--r--   0 root         (0) root         (0)     6103 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/unary_ops.h
+-rw-r--r--   0 root         (0) root         (0)     4698 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_accumulator.h
+-rw-r--r--   0 root         (0) root         (0)     8397 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_binary.h
+-rw-r--r--   0 root         (0) root         (0)     8830 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_broadcast.h
+-rw-r--r--   0 root         (0) root         (0)    12999 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_reduction.h
+-rw-r--r--   0 root         (0) root         (0)     9454 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_linear_combination.h
+-rw-r--r--   0 root         (0) root         (0)     9085 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_broadcast.h
+-rw-r--r--   0 root         (0) root         (0)    11976 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_reduction.h
+-rw-r--r--   0 root         (0) root         (0)     6177 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_input.h
+-rw-r--r--   0 root         (0) root         (0)     8017 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_output.h
+-rw-r--r--   0 root         (0) root         (0)     7201 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_unary.h
+-rw-r--r--   0 root         (0) root         (0)    16970 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_with_layernorm.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:02.560219 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/
+-rw-r--r--   0 root         (0) root         (0)     3673 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/gemm.h
+-rw-r--r--   0 root         (0) root         (0)    21504 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/gemm_universal_with_visitor.h
+-rw-r--r--   0 root         (0) root         (0)     2328 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/host.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:02.568439 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/
+-rw-r--r--   0 root         (0) root         (0)     2115 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/layout.h
+-rw-r--r--   0 root         (0) root         (0)     4337 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/matrix.h
+-rw-r--r--   0 root         (0) root         (0)     3694 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/tensor.h
+-rw-r--r--   0 root         (0) root         (0)     8565 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/swizzling.h
+-rw-r--r--   0 root         (0) root         (0)     3902 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_coord.h
+-rw-r--r--   0 root         (0) root         (0)     5563 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_ref_view.h
+-rw-r--r--   0 root         (0) root         (0)     4855 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/types.h
+-rw-r--r--   0 root         (0) root         (0)      811 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/library.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.466772 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/test/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:02.576713 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/
+-rw-r--r--   0 root         (0) root         (0)     2651 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/conv_problems.h
+-rw-r--r--   0 root         (0) root         (0)     2253 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/convolution.h
+-rw-r--r--   0 root         (0) root         (0)     8826 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/host.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:02.584637 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/
+-rw-r--r--   0 root         (0) root         (0)     2139 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/gemm.h
+-rw-r--r--   0 root         (0) root         (0)    18930 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/host.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:02.618979 flash_attn-2.0.0/csrc/cutlass/tools/library/src/
+-rw-r--r--   0 root         (0) root         (0)    22377 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/src/conv2d_operation.h
+-rw-r--r--   0 root         (0) root         (0)    13851 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/src/conv3d_operation.h
+-rw-r--r--   0 root         (0) root         (0)    42129 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/src/gemm_operation.h
+-rw-r--r--   0 root         (0) root         (0)    35777 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/src/handle.cu
+-rw-r--r--   0 root         (0) root         (0)    12616 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/src/library_internal.h
+-rw-r--r--   0 root         (0) root         (0)     3782 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/src/manifest.cpp
+-rw-r--r--   0 root         (0) root         (0)     5468 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/src/operation_table.cu
+-rw-r--r--   0 root         (0) root         (0)    12873 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/src/rank_2k_operation.h
+-rw-rw-r--   0 root         (0) root         (0)    11367 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/src/rank_k_operation.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:02.622352 flash_attn-2.0.0/csrc/cutlass/tools/library/src/reduction/
+-rw-r--r--   0 root         (0) root         (0)     3190 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/src/reduction/init_reduction_operations.cu
+-rw-r--r--   0 root         (0) root         (0)     6367 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/src/reduction/reduction_device.cu
+-rw-r--r--   0 root         (0) root         (0)    10270 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/src/reduction/reduction_operation.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:02.640340 flash_attn-2.0.0/csrc/cutlass/tools/library/src/reference/
+-rw-rw-r--   0 root         (0) root         (0)     6746 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/src/reference/conv2d.cu
+-rw-rw-r--   0 root         (0) root         (0)     6286 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/src/reference/conv3d.cu
+-rw-r--r--   0 root         (0) root         (0)    17191 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/src/reference/conv_reference_operation.h
+-rw-r--r--   0 root         (0) root         (0)     7199 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/src/reference/gemm.cu
+-rw-r--r--   0 root         (0) root         (0)    14732 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/src/reference/gemm_reference_operation.h
+-rw-rw-r--   0 root         (0) root         (0)     2857 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/src/reference/initialize_reference_operations.cu
+-rw-rw-r--   0 root         (0) root         (0)     2669 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/src/singleton.cu
+-rw-r--r--   0 root         (0) root         (0)    13134 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/src/symm_operation.h
+-rw-rw-r--   0 root         (0) root         (0)    11698 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/src/trmm_operation.h
+-rw-r--r--   0 root         (0) root         (0)    43704 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/library/src/util.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.471787 flash_attn-2.0.0/csrc/cutlass/tools/profiler/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:03.746438 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/
+-rw-r--r--   0 root         (0) root         (0)    54128 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/conv2d_operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)    18170 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/conv2d_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)    48659 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/conv3d_operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)    16043 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/conv3d_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)    36462 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/cublas_helpers.cu
+-rw-r--r--   0 root         (0) root         (0)    10627 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/cublas_helpers.h
+-rw-r--r--   0 root         (0) root         (0)    17049 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/cudnn_helpers.cpp
+-rw-r--r--   0 root         (0) root         (0)    20433 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/cudnn_helpers.h
+-rw-r--r--   0 root         (0) root         (0)     7233 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/cutlass_profiler.cu
+-rw-rw-r--   0 root         (0) root         (0)     3233 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/cutlass_profiler.h
+-rw-r--r--   0 root         (0) root         (0)     2453 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/debug.h
+-rw-r--r--   0 root         (0) root         (0)    53643 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/device_allocation.cu
+-rw-r--r--   0 root         (0) root         (0)     7217 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/device_allocation.h
+-rw-r--r--   0 root         (0) root         (0)     6841 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/device_context.cu
+-rw-r--r--   0 root         (0) root         (0)     4300 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/device_context.h
+-rw-rw-r--   0 root         (0) root         (0)     8296 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/enumerated_types.cpp
+-rw-rw-r--   0 root         (0) root         (0)     6421 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/enumerated_types.h
+-rw-r--r--   0 root         (0) root         (0)    42366 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/gemm_operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)     8544 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/gemm_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)     3874 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/gpu_timer.cpp
+-rw-r--r--   0 root         (0) root         (0)     2724 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/gpu_timer.h
+-rw-rw-r--   0 root         (0) root         (0)     2340 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/main.cpp
+-rw-r--r--   0 root         (0) root         (0)    22087 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)     7876 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)    27172 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/options.cu
+-rw-r--r--   0 root         (0) root         (0)     8773 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/options.h
+-rw-rw-r--   0 root         (0) root         (0)    14192 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/performance_report.cpp
+-rw-rw-r--   0 root         (0) root         (0)     4337 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/performance_report.h
+-rw-rw-r--   0 root         (0) root         (0)     2494 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/performance_result.cu
+-rw-rw-r--   0 root         (0) root         (0)     3941 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/performance_result.h
+-rw-rw-r--   0 root         (0) root         (0)    37487 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/problem_space.cpp
+-rw-r--r--   0 root         (0) root         (0)    27747 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/problem_space.h
+-rw-r--r--   0 root         (0) root         (0)    25014 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/rank_2k_operation_profiler.cu
+-rw-rw-r--   0 root         (0) root         (0)     6891 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/rank_2k_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)    24253 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/rank_k_operation_profiler.cu
+-rw-rw-r--   0 root         (0) root         (0)     6830 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/rank_k_operation_profiler.h
+-rw-rw-r--   0 root         (0) root         (0)     5452 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/reduction_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)    20688 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.cu
+-rw-rw-r--   0 root         (0) root         (0)     6471 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)    26610 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/symm_operation_profiler.cu
+-rw-rw-r--   0 root         (0) root         (0)     6933 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/symm_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)    24431 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/trmm_operation_profiler.cu
+-rw-rw-r--   0 root         (0) root         (0)     6599 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/trmm_operation_profiler.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.473566 flash_attn-2.0.0/csrc/cutlass/tools/util/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.474624 flash_attn-2.0.0/csrc/cutlass/tools/util/include/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.475694 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:03.802701 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/
+-rw-r--r--   0 root         (0) root         (0)     9774 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/command_line.h
+-rw-rw-r--   0 root         (0) root         (0)     5104 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/debug.h
+-rw-rw-r--   0 root         (0) root         (0)     5953 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/device_dump.h
+-rw-r--r--   0 root         (0) root         (0)    17695 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/device_groupnorm.h
+-rw-rw-r--   0 root         (0) root         (0)    20881 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/device_layernorm.h
+-rw-rw-r--   0 root         (0) root         (0)    10561 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/device_memory.h
+-rw-rw-r--   0 root         (0) root         (0)     5219 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/device_nchw_to_nhwc.h
+-rw-r--r--   0 root         (0) root         (0)    11067 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/device_nhwc_padding.h
+-rw-rw-r--   0 root         (0) root         (0)    18653 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/device_nhwc_pooling.h
+-rw-rw-r--   0 root         (0) root         (0)     5214 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/device_nhwc_to_nchw.h
+-rw-rw-r--   0 root         (0) root         (0)     4007 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/device_utils.h
+-rw-rw-r--   0 root         (0) root         (0)     4597 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/distribution.h
+-rw-rw-r--   0 root         (0) root         (0)     2674 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/exceptions.h
+-rw-rw-r--   0 root         (0) root         (0)     4821 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/host_reorder.h
+-rw-rw-r--   0 root         (0) root         (0)    16745 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/host_tensor.h
+-rw-rw-r--   0 root         (0) root         (0)    20354 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/host_tensor_planar_complex.h
+-rw-rw-r--   0 root         (0) root         (0)     5890 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/host_uncompress.h
+-rw-rw-r--   0 root         (0) root         (0)     1962 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/index_sequence.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:47:38.478026 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:03.806097 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/detail/
+-rw-rw-r--   0 root         (0) root         (0)     4606 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/detail/inner_product.h
+-rw-rw-r--   0 root         (0) root         (0)     3527 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/detail/linear_to_coordinate.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:03.833428 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/
+-rw-rw-r--   0 root         (0) root         (0)    48350 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/convolution.h
+-rw-r--r--   0 root         (0) root         (0)    14296 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/gemm.h
+-rw-r--r--   0 root         (0) root         (0)    10524 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/gemm_complex.h
+-rw-rw-r--   0 root         (0) root         (0)     9652 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/gemm_planar_complex.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:03.841020 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/kernel/
+-rw-rw-r--   0 root         (0) root         (0)     5381 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/kernel/gemm.h
+-rw-rw-r--   0 root         (0) root         (0)     6198 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_elementwise.h
+-rw-r--r--   0 root         (0) root         (0)     5126 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_foreach.h
+-rw-rw-r--   0 root         (0) root         (0)    11615 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/rank_2k_complex.h
+-rw-rw-r--   0 root         (0) root         (0)     7278 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/tensor_compare.h
+-rw-r--r--   0 root         (0) root         (0)    46444 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/tensor_fill.h
+-rw-r--r--   0 root         (0) root         (0)     5293 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/tensor_foreach.h
+-rw-rw-r--   0 root         (0) root         (0)    15964 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/tensor_reduce.h
+-rw-rw-r--   0 root         (0) root         (0)     4589 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/tensor_relu.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:03.849147 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/thread/
+-rw-rw-r--   0 root         (0) root         (0)     5872 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/thread/gemm.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:04.894578 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/
+-rw-r--r--   0 root         (0) root         (0)    28439 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/convolution.h
+-rw-rw-r--   0 root         (0) root         (0)     2766 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/error_metrics.h
+-rw-r--r--   0 root         (0) root         (0)    17163 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/gemm.h
+-rw-r--r--   0 root         (0) root         (0)     7097 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/gemm_complex.h
+-rw-rw-r--   0 root         (0) root         (0)     7708 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/gemm_planar_complex.h
+-rw-rw-r--   0 root         (0) root         (0)     9441 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k.h
+-rw-rw-r--   0 root         (0) root         (0)    11444 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k_complex.h
+-rw-rw-r--   0 root         (0) root         (0)     8148 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/rank_k_complex.h
+-rw-rw-r--   0 root         (0) root         (0)    10509 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/symm.h
+-rw-rw-r--   0 root         (0) root         (0)    12296 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/symm_complex.h
+-rw-r--r--   0 root         (0) root         (0)     8440 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/tensor_compare.h
+-rw-rw-r--   0 root         (0) root         (0)     8317 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/tensor_copy.h
+-rw-rw-r--   0 root         (0) root         (0)     9027 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/tensor_elementwise.h
+-rw-r--r--   0 root         (0) root         (0)    43961 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/tensor_fill.h
+-rw-r--r--   0 root         (0) root         (0)     4756 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/tensor_foreach.h
+-rw-rw-r--   0 root         (0) root         (0)     2133 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/tensor_norm.h
+-rw-rw-r--   0 root         (0) root         (0)     6111 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/tensor_reduce.h
+-rw-rw-r--   0 root         (0) root         (0)     7670 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/trmm.h
+-rw-rw-r--   0 root         (0) root         (0)     9874 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/trmm_complex.h
+-rw-r--r--   0 root         (0) root         (0)     8285 2023-07-17 10:57:41.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/tensor_view_io.h
+-rw-rw-r--   0 root         (0) root         (0)     8809 2023-07-17 10:57:20.000000 flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/type_traits.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:04.901924 flash_attn-2.0.0/csrc/flash_attn/
+-rw-r--r--   0 root         (0) root         (0)    41827 2023-07-12 04:28:42.000000 flash_attn-2.0.0/csrc/flash_attn/flash_api.cpp
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:04.986086 flash_attn-2.0.0/csrc/flash_attn/src/
+-rw-r--r--   0 root         (0) root         (0)     1664 2023-04-16 00:15:33.000000 flash_attn-2.0.0/csrc/flash_attn/src/block_info.h
+-rw-r--r--   0 root         (0) root         (0)     4109 2023-07-09 22:07:48.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash.h
+-rw-r--r--   0 root         (0) root         (0)      879 2023-07-13 13:50:11.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_bwd_hdim128_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     1569 2023-07-13 13:50:11.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_bwd_hdim128_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      361 2023-07-11 01:26:51.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_bwd_hdim160_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      353 2023-07-11 01:26:38.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_bwd_hdim160_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      361 2023-07-11 01:48:51.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_bwd_hdim192_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      353 2023-07-11 01:48:58.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_bwd_hdim192_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      361 2023-07-11 04:28:48.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_bwd_hdim224_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      353 2023-07-11 04:28:56.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_bwd_hdim224_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      361 2023-07-11 04:43:54.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_bwd_hdim256_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      353 2023-07-11 04:44:02.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_bwd_hdim256_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      667 2023-07-11 04:54:51.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_bwd_hdim32_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      651 2023-07-11 04:54:51.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_bwd_hdim32_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      667 2023-07-11 04:54:51.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_bwd_hdim64_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     2751 2023-07-11 04:54:51.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_bwd_hdim64_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      874 2023-07-13 13:50:11.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_bwd_hdim96_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     1031 2023-07-13 13:50:11.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_bwd_hdim96_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    85689 2023-07-13 13:50:11.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_bwd_kernel.h
+-rw-r--r--   0 root         (0) root         (0)    20375 2023-07-13 13:50:11.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_bwd_launch_template.h
+-rw-r--r--   0 root         (0) root         (0)      783 2023-07-09 22:55:47.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     1951 2023-07-09 22:55:33.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      682 2023-07-09 22:56:37.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     1674 2023-07-09 22:56:52.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      682 2023-07-09 22:57:38.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     1647 2023-07-09 22:57:25.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      329 2023-07-09 22:58:41.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_fwd_hdim224_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      321 2023-07-09 22:58:59.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_fwd_hdim224_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      329 2023-07-09 22:59:19.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_fwd_hdim256_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      321 2023-07-09 22:59:41.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_fwd_hdim256_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      326 2023-07-10 04:44:44.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     1151 2023-07-09 23:04:52.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      777 2023-07-09 22:36:36.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     1463 2023-07-09 22:35:48.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      677 2023-07-09 22:54:57.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     1328 2023-07-09 22:55:13.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    30177 2023-07-11 22:09:11.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_fwd_kernel.h
+-rw-r--r--   0 root         (0) root         (0)    15208 2023-07-12 09:05:19.000000 flash_attn-2.0.0/csrc/flash_attn/src/flash_fwd_launch_template.h
+-rw-r--r--   0 root         (0) root         (0)    18396 2023-07-11 05:28:03.000000 flash_attn-2.0.0/csrc/flash_attn/src/kernel_traits.h
+-rw-r--r--   0 root         (0) root         (0)     7555 2023-04-22 02:44:59.000000 flash_attn-2.0.0/csrc/flash_attn/src/kernel_traits_sm90.h
+-rw-r--r--   0 root         (0) root         (0)     5372 2023-04-16 00:15:33.000000 flash_attn-2.0.0/csrc/flash_attn/src/philox.cuh
+-rw-r--r--   0 root         (0) root         (0)    14205 2023-04-16 00:15:33.000000 flash_attn-2.0.0/csrc/flash_attn/src/softmax.h
+-rw-r--r--   0 root         (0) root         (0)     2961 2023-07-09 23:03:20.000000 flash_attn-2.0.0/csrc/flash_attn/src/static_switch.h
+-rw-r--r--   0 root         (0) root         (0)    16378 2023-04-16 00:15:33.000000 flash_attn-2.0.0/csrc/flash_attn/src/utils.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:04.994878 flash_attn-2.0.0/csrc/flash_gen/
+-rw-r--r--   0 root         (0) root         (0)     7018 2022-11-21 06:35:03.000000 flash_attn-2.0.0/csrc/flash_gen/decoder_masked_multihead_attention.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:05.008012 flash_attn-2.0.0/csrc/ft_attention/
+-rw-r--r--   0 root         (0) root         (0)     8253 2023-04-16 00:48:36.000000 flash_attn-2.0.0/csrc/ft_attention/cuda_bf16_fallbacks.cuh
+-rw-r--r--   0 root         (0) root         (0)      867 2023-04-16 00:48:36.000000 flash_attn-2.0.0/csrc/ft_attention/cuda_bf16_wrapper.h
+-rw-r--r--   0 root         (0) root         (0)     7069 2023-06-06 06:13:59.000000 flash_attn-2.0.0/csrc/ft_attention/decoder_masked_multihead_attention.cu
+-rw-r--r--   0 root         (0) root         (0)     7627 2023-07-02 20:11:51.000000 flash_attn-2.0.0/csrc/ft_attention/decoder_masked_multihead_attention.h
+-rw-r--r--   0 root         (0) root         (0)    64946 2023-07-03 16:25:44.000000 flash_attn-2.0.0/csrc/ft_attention/decoder_masked_multihead_attention_utils.h
+-rw-r--r--   0 root         (0) root         (0)    10119 2023-07-06 22:24:25.000000 flash_attn-2.0.0/csrc/ft_attention/ft_attention.cpp
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:05.013738 flash_attn-2.0.0/csrc/fused_dense_lib/
+-rw-r--r--   0 root         (0) root         (0)    10179 2023-05-30 21:13:46.000000 flash_attn-2.0.0/csrc/fused_dense_lib/fused_dense.cpp
+-rw-r--r--   0 root         (0) root         (0)    24690 2023-05-30 21:14:57.000000 flash_attn-2.0.0/csrc/fused_dense_lib/fused_dense_cuda.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:05.030424 flash_attn-2.0.0/csrc/fused_softmax/
+-rw-r--r--   0 root         (0) root         (0)     5037 2023-04-16 00:48:36.000000 flash_attn-2.0.0/csrc/fused_softmax/fused_softmax.cpp
+-rw-r--r--   0 root         (0) root         (0)    23616 2023-04-16 00:48:36.000000 flash_attn-2.0.0/csrc/fused_softmax/scaled_masked_softmax.h
+-rw-r--r--   0 root         (0) root         (0)     4209 2023-04-16 00:48:36.000000 flash_attn-2.0.0/csrc/fused_softmax/scaled_masked_softmax_cuda.cu
+-rw-r--r--   0 root         (0) root         (0)    24659 2023-04-16 00:48:36.000000 flash_attn-2.0.0/csrc/fused_softmax/scaled_upper_triang_masked_softmax.h
+-rw-r--r--   0 root         (0) root         (0)     3154 2023-04-16 00:48:36.000000 flash_attn-2.0.0/csrc/fused_softmax/scaled_upper_triang_masked_softmax_cuda.cu
+-rw-r--r--   0 root         (0) root         (0)     1216 2023-04-16 00:48:36.000000 flash_attn-2.0.0/csrc/fused_softmax/type_shim.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:05.203307 flash_attn-2.0.0/csrc/layer_norm/
+-rw-r--r--   0 root         (0) root         (0)     7248 2023-04-16 00:48:36.000000 flash_attn-2.0.0/csrc/layer_norm/ln.h
+-rw-r--r--   0 root         (0) root         (0)    36418 2023-04-16 00:48:36.000000 flash_attn-2.0.0/csrc/layer_norm/ln_api.cpp
+-rw-r--r--   0 root         (0) root         (0)      987 2023-04-16 00:48:36.000000 flash_attn-2.0.0/csrc/layer_norm/ln_bwd_1024.cu
+-rw-r--r--   0 root         (0) root         (0)      987 2023-04-16 00:48:36.000000 flash_attn-2.0.0/csrc/layer_norm/ln_bwd_1280.cu
+-rw-r--r--   0 root         (0) root         (0)      977 2023-04-16 00:48:36.000000 flash_attn-2.0.0/csrc/layer_norm/ln_bwd_1536.cu
+-rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_bwd_2048.cu
+-rw-r--r--   0 root         (0) root         (0)      977 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_bwd_256.cu
+-rw-r--r--   0 root         (0) root         (0)      977 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_bwd_2560.cu
+-rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_bwd_3072.cu
+-rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_bwd_4096.cu
+-rw-r--r--   0 root         (0) root         (0)      977 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_bwd_512.cu
+-rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_bwd_5120.cu
+-rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_bwd_6144.cu
+-rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_bwd_7168.cu
+-rw-r--r--   0 root         (0) root         (0)      977 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_bwd_768.cu
+-rw-r--r--   0 root         (0) root         (0)      976 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_bwd_8192.cu
+-rw-r--r--   0 root         (0) root         (0)    25647 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_bwd_kernels.cuh
+-rw-r--r--   0 root         (0) root         (0)    19944 2023-01-19 07:34:02.000000 flash_attn-2.0.0/csrc/layer_norm/ln_bwd_semi_cuda_kernel_old.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_fwd_1024.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-01-22 09:05:55.000000 flash_attn-2.0.0/csrc/layer_norm/ln_fwd_10240.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-01-22 09:07:15.000000 flash_attn-2.0.0/csrc/layer_norm/ln_fwd_12288.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2022-12-05 08:45:58.000000 flash_attn-2.0.0/csrc/layer_norm/ln_fwd_128.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_fwd_1280.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_fwd_1536.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_fwd_2048.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_fwd_256.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_fwd_2560.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_fwd_3072.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2022-12-05 08:50:57.000000 flash_attn-2.0.0/csrc/layer_norm/ln_fwd_384.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_fwd_4096.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_fwd_512.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_fwd_5120.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_fwd_6144.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_fwd_7168.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_fwd_768.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_fwd_8192.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-01-22 08:41:06.000000 flash_attn-2.0.0/csrc/layer_norm/ln_fwd_9216.cu
+-rw-r--r--   0 root         (0) root         (0)    18000 2022-12-06 21:18:58.000000 flash_attn-2.0.0/csrc/layer_norm/ln_fwd_cuda_kernel_old.cu
+-rw-r--r--   0 root         (0) root         (0)    12721 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_fwd_kernels.cuh
+-rw-r--r--   0 root         (0) root         (0)     6655 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_kernel_traits.h
+-rw-r--r--   0 root         (0) root         (0)     1095 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_parallel_bwd_1024.cu
+-rw-r--r--   0 root         (0) root         (0)     1095 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_parallel_bwd_1280.cu
+-rw-r--r--   0 root         (0) root         (0)     1085 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_parallel_bwd_1536.cu
+-rw-r--r--   0 root         (0) root         (0)     1084 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_parallel_bwd_2048.cu
+-rw-r--r--   0 root         (0) root         (0)     1085 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_parallel_bwd_256.cu
+-rw-r--r--   0 root         (0) root         (0)     1085 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_parallel_bwd_2560.cu
+-rw-r--r--   0 root         (0) root         (0)     1084 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_parallel_bwd_3072.cu
+-rw-r--r--   0 root         (0) root         (0)     1145 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_parallel_bwd_4096.cu
+-rw-r--r--   0 root         (0) root         (0)     1085 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_parallel_bwd_512.cu
+-rw-r--r--   0 root         (0) root         (0)     1145 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_parallel_bwd_5120.cu
+-rw-r--r--   0 root         (0) root         (0)     1084 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_parallel_bwd_6144.cu
+-rw-r--r--   0 root         (0) root         (0)     1084 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_parallel_bwd_7168.cu
+-rw-r--r--   0 root         (0) root         (0)     1085 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_parallel_bwd_768.cu
+-rw-r--r--   0 root         (0) root         (0)     1084 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_parallel_bwd_8192.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_parallel_fwd_1024.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_parallel_fwd_1280.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_parallel_fwd_1536.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_parallel_fwd_2048.cu
+-rw-r--r--   0 root         (0) root         (0)     1032 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_parallel_fwd_256.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_parallel_fwd_2560.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_parallel_fwd_3072.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_parallel_fwd_4096.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_parallel_fwd_512.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_parallel_fwd_5120.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_parallel_fwd_6144.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_parallel_fwd_7168.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_parallel_fwd_768.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_parallel_fwd_8192.cu
+-rw-r--r--   0 root         (0) root         (0)    24916 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_parallel_residual_bwd_kernels.cuh
+-rw-r--r--   0 root         (0) root         (0)    12530 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_parallel_residual_fwd_kernels.cuh
+-rw-r--r--   0 root         (0) root         (0)    29989 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/ln_utils.cuh
+-rw-r--r--   0 root         (0) root         (0)     1278 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/layer_norm/static_switch.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:05.213945 flash_attn-2.0.0/csrc/rotary/
+-rw-r--r--   0 root         (0) root         (0)     1806 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/rotary/rotary.cpp
+-rw-r--r--   0 root         (0) root         (0)     1984 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/rotary/rotary_cuda.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:05.222524 flash_attn-2.0.0/csrc/xentropy/
+-rw-r--r--   0 root         (0) root         (0)     2290 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/xentropy/interface.cpp
+-rw-r--r--   0 root         (0) root         (0)    25783 2023-04-16 00:48:37.000000 flash_attn-2.0.0/csrc/xentropy/xentropy_kernel.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:05.250166 flash_attn-2.0.0/flash_attn/
+-rw-rw-r--   0 root         (0) root         (0)      442 2023-07-17 10:29:29.000000 flash_attn-2.0.0/flash_attn/__init__.py
+-rw-rw-r--   0 root         (0) root         (0)    20845 2022-10-31 02:25:05.000000 flash_attn-2.0.0/flash_attn/attention_kernl.py
+-rw-r--r--   0 root         (0) root         (0)     5898 2023-04-16 00:48:37.000000 flash_attn-2.0.0/flash_attn/bert_padding.py
+-rw-r--r--   0 root         (0) root         (0)    28027 2023-07-12 12:19:57.000000 flash_attn-2.0.0/flash_attn/fav2_interface.py
+-rw-r--r--   0 root         (0) root         (0)     4575 2023-07-17 11:38:20.000000 flash_attn-2.0.0/flash_attn/flash_attention.py
+-rw-r--r--   0 root         (0) root         (0)    28021 2023-07-17 12:03:52.000000 flash_attn-2.0.0/flash_attn/flash_attn_interface.py
+-rw-r--r--   0 root         (0) root         (0)    38148 2023-04-16 00:48:37.000000 flash_attn-2.0.0/flash_attn/flash_attn_triton.py
+-rw-r--r--   0 root         (0) root         (0)    10593 2023-04-16 00:48:37.000000 flash_attn-2.0.0/flash_attn/flash_attn_triton_og.py
+-rw-r--r--   0 root         (0) root         (0)     8255 2022-11-18 03:30:00.000000 flash_attn-2.0.0/flash_attn/flash_attn_triton_single_query.py
+-rw-r--r--   0 root         (0) root         (0)    37797 2023-03-17 09:16:10.000000 flash_attn-2.0.0/flash_attn/flash_attn_triton_tmp.py
+-rw-r--r--   0 root         (0) root         (0)    10640 2023-03-12 08:48:14.000000 flash_attn-2.0.0/flash_attn/flash_attn_triton_tmp_og.py
+-rw-r--r--   0 root         (0) root         (0)     6819 2022-06-26 00:59:43.000000 flash_attn-2.0.0/flash_attn/flash_blocksparse_attention.py
+-rw-r--r--   0 root         (0) root         (0)     7036 2022-06-26 00:59:43.000000 flash_attn-2.0.0/flash_attn/flash_blocksparse_attn_interface.py
+-rw-r--r--   0 root         (0) root         (0)     7902 2023-04-16 00:48:37.000000 flash_attn-2.0.0/flash_attn/fused_softmax.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:05.271193 flash_attn-2.0.0/flash_attn/layers/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-04-16 00:48:37.000000 flash_attn-2.0.0/flash_attn/layers/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     2039 2023-04-16 00:48:37.000000 flash_attn-2.0.0/flash_attn/layers/patch_embed.py
+-rw-r--r--   0 root         (0) root         (0)    12738 2023-07-17 10:14:25.000000 flash_attn-2.0.0/flash_attn/layers/rotary.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:05.277933 flash_attn-2.0.0/flash_attn/losses/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-04-16 00:48:37.000000 flash_attn-2.0.0/flash_attn/losses/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     6697 2023-04-16 00:48:37.000000 flash_attn-2.0.0/flash_attn/losses/cross_entropy.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:05.290410 flash_attn-2.0.0/flash_attn/models/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-04-16 00:48:37.000000 flash_attn-2.0.0/flash_attn/models/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    26570 2023-04-18 20:33:07.000000 flash_attn-2.0.0/flash_attn/models/bert.py
+-rw-r--r--   0 root         (0) root         (0)    38025 2023-06-02 19:10:34.000000 flash_attn-2.0.0/flash_attn/models/gpt.py
+-rw-r--r--   0 root         (0) root         (0)     5025 2023-04-16 00:48:37.000000 flash_attn-2.0.0/flash_attn/models/gpt_neox.py
+-rw-r--r--   0 root         (0) root         (0)     4365 2023-04-18 20:33:24.000000 flash_attn-2.0.0/flash_attn/models/gptj.py
+-rw-r--r--   0 root         (0) root         (0)     5761 2023-04-19 04:11:30.000000 flash_attn-2.0.0/flash_attn/models/llama.py
+-rw-r--r--   0 root         (0) root         (0)     5130 2023-04-18 20:33:17.000000 flash_attn-2.0.0/flash_attn/models/opt.py
+-rw-r--r--   0 root         (0) root         (0)    13621 2023-04-16 00:48:37.000000 flash_attn-2.0.0/flash_attn/models/vit.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:05.306638 flash_attn-2.0.0/flash_attn/modules/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-04-16 00:48:37.000000 flash_attn-2.0.0/flash_attn/modules/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    16417 2023-07-17 11:13:14.000000 flash_attn-2.0.0/flash_attn/modules/block.py
+-rw-r--r--   0 root         (0) root         (0)     8620 2023-04-16 00:48:37.000000 flash_attn-2.0.0/flash_attn/modules/embedding.py
+-rw-r--r--   0 root         (0) root         (0)    32847 2023-07-17 11:37:55.000000 flash_attn-2.0.0/flash_attn/modules/mha.py
+-rw-r--r--   0 root         (0) root         (0)     2221 2023-07-02 06:31:30.000000 flash_attn-2.0.0/flash_attn/modules/mlp.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:05.317888 flash_attn-2.0.0/flash_attn/ops/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-04-16 00:48:37.000000 flash_attn-2.0.0/flash_attn/ops/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     3002 2023-04-16 00:48:37.000000 flash_attn-2.0.0/flash_attn/ops/activations.py
+-rw-r--r--   0 root         (0) root         (0)    26087 2023-04-18 10:32:08.000000 flash_attn-2.0.0/flash_attn/ops/fused_dense.py
+-rw-r--r--   0 root         (0) root         (0)    19306 2023-07-04 21:52:07.000000 flash_attn-2.0.0/flash_attn/ops/layer_norm.py
+-rw-r--r--   0 root         (0) root         (0)     3672 2023-04-18 22:26:53.000000 flash_attn-2.0.0/flash_attn/ops/rms_norm.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:05.328369 flash_attn-2.0.0/flash_attn/utils/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-04-16 00:48:37.000000 flash_attn-2.0.0/flash_attn/utils/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     5909 2023-04-16 00:48:37.000000 flash_attn-2.0.0/flash_attn/utils/benchmark.py
+-rw-r--r--   0 root         (0) root         (0)     5545 2023-04-16 00:48:37.000000 flash_attn-2.0.0/flash_attn/utils/distributed.py
+-rw-r--r--   0 root         (0) root         (0)    14105 2023-04-21 18:28:20.000000 flash_attn-2.0.0/flash_attn/utils/generation.py
+-rw-r--r--   0 root         (0) root         (0)     1824 2023-04-16 00:48:37.000000 flash_attn-2.0.0/flash_attn/utils/pretrained.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-07-17 12:48:05.268468 flash_attn-2.0.0/flash_attn.egg-info/
+-rw-rw-r--   0 root         (0) root         (0)      476 2023-07-17 12:47:37.000000 flash_attn-2.0.0/flash_attn.egg-info/PKG-INFO
+-rw-rw-r--   0 root         (0) root         (0)    96582 2023-07-17 12:47:38.000000 flash_attn-2.0.0/flash_attn.egg-info/SOURCES.txt
+-rw-rw-r--   0 root         (0) root         (0)        1 2023-07-17 12:47:37.000000 flash_attn-2.0.0/flash_attn.egg-info/dependency_links.txt
+-rw-rw-r--   0 root         (0) root         (0)       29 2023-07-17 12:47:37.000000 flash_attn-2.0.0/flash_attn.egg-info/requires.txt
+-rw-rw-r--   0 root         (0) root         (0)       29 2023-07-17 12:47:37.000000 flash_attn-2.0.0/flash_attn.egg-info/top_level.txt
+-rw-rw-r--   0 root         (0) root         (0)       38 2023-07-17 12:48:05.325361 flash_attn-2.0.0/setup.cfg
+-rw-r--r--   0 root         (0) root         (0)     9562 2023-07-17 10:25:17.000000 flash_attn-2.0.0/setup.py
```

### Comparing `flash_attn-1.0.9/LICENSE` & `flash_attn-2.0.0/LICENSE`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/cmake/nop.cu` & `flash_attn-2.0.0/csrc/cutlass/cmake/nop.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/00_basic_gemm/basic_gemm.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/00_basic_gemm/basic_gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/01_cutlass_utilities/cutlass_utilities.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/01_cutlass_utilities/cutlass_utilities.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/02_dump_reg_shmem/dump_reg_shmem.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/02_dump_reg_shmem/dump_reg_shmem.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/03_visualize_layout/options.h` & `flash_attn-2.0.0/csrc/cutlass/examples/03_visualize_layout/options.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/03_visualize_layout/register_layout.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/03_visualize_layout/register_layout.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/03_visualize_layout/register_layout.h` & `flash_attn-2.0.0/csrc/cutlass/examples/03_visualize_layout/register_layout.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/03_visualize_layout/visualize_layout.cpp` & `flash_attn-2.0.0/csrc/cutlass/examples/03_visualize_layout/visualize_layout.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/03_visualize_layout/visualize_layout.h` & `flash_attn-2.0.0/csrc/cutlass/examples/03_visualize_layout/visualize_layout.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/04_tile_iterator/tile_iterator.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/04_tile_iterator/tile_iterator.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/05_batched_gemm/batched_gemm.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/05_batched_gemm/batched_gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/06_splitK_gemm/splitk_gemm.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/06_splitK_gemm/splitk_gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/07_volta_tensorop_gemm/volta_tensorop_gemm.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/07_volta_tensorop_gemm/volta_tensorop_gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/08_turing_tensorop_gemm/turing_tensorop_gemm.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/08_turing_tensorop_gemm/turing_tensorop_gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/09_turing_tensorop_conv2dfprop/turing_tensorop_conv2dfprop.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/09_turing_tensorop_conv2dfprop/turing_tensorop_conv2dfprop.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/10_planar_complex/planar_complex.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/10_planar_complex/planar_complex.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/11_planar_complex_array/planar_complex_array.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/11_planar_complex_array/planar_complex_array.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/12_gemm_bias_relu/gemm_bias_relu.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/12_gemm_bias_relu/gemm_bias_relu.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_conv2d_run.h` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/b2b_conv2d_run.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_gemm_run.h` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/b2b_gemm_run.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_conv2d_run.h` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_conv2d_run.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_gemm_run.h` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_gemm_run.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/b2b_gemm.h` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/device/b2b_gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/b2b_implicit_gemm_convolution.h` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/device/b2b_implicit_gemm_convolution.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_rf.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_rf.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_shmem.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_shmem.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_rf.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_rf.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_shmem.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_shmem.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_rf.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_rf.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_shmem.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_shmem.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_rf.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_rf.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_shmem.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_shmem.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_rf.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_rf.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_shmem.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_shmem.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_rf.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_rf.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_shmem.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_shmem.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_rf.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_rf.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_shmem.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_shmem.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_rf.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_rf.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_shmem.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_shmem.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_gemm.h` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_implicit_gemm_convolution.h` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_implicit_gemm_convolution.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop.h` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm75.h` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm75.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm80.h` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm80.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm75.h` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm75.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm80.h` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm80.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm.h` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm_smem_accumulator.h` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm_smem_accumulator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/reference/device/tensor_scale_bias.h` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/reference/device/tensor_scale_bias.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/test_run.h` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/test_run.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage.h` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage_smem_accumulator.h` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage_smem_accumulator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined.h` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined_smem_accumulator.h` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined_smem_accumulator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base.h` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base_smem_accumulator.h` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base_smem_accumulator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage.h` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage_smem_accumulator.h` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage_smem_accumulator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined.h` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined_smem_accumulator.h` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined_smem_accumulator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma.h` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma_smem_accumulator.h` & `flash_attn-2.0.0/csrc/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma_smem_accumulator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/14_ampere_tf32_tensorop_gemm/ampere_tf32_tensorop_gemm.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/14_ampere_tf32_tensorop_gemm/ampere_tf32_tensorop_gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/15_ampere_sparse_tensorop_gemm/ampere_sparse_tensorop_gemm.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/15_ampere_sparse_tensorop_gemm/ampere_sparse_tensorop_gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/16_ampere_tensorop_conv2dfprop/ampere_tensorop_conv2dfprop.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/16_ampere_tensorop_conv2dfprop/ampere_tensorop_conv2dfprop.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/17_fprop_per_channel_bias/fprop_per_channel_bias.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/17_fprop_per_channel_bias/fprop_per_channel_bias.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/18_ampere_fp64_tensorop_affine2_gemm/ampere_fp64_tensorop_affine2_gemm.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/18_ampere_fp64_tensorop_affine2_gemm/ampere_fp64_tensorop_affine2_gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/19_tensorop_canonical/tensorop_canonical.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/19_tensorop_canonical/tensorop_canonical.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/20_simt_canonical/simt_canonical.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/20_simt_canonical/simt_canonical.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/21_quaternion_gemm/quaternion_gemm.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/21_quaternion_gemm/quaternion_gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/22_quaternion_conv/quaternion_conv.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/22_quaternion_conv/quaternion_conv.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/23_ampere_gemm_operand_reduction_fusion/ampere_gemm_operand_reduction_fusion.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/23_ampere_gemm_operand_reduction_fusion/ampere_gemm_operand_reduction_fusion.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/24_gemm_grouped/gemm_grouped.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/24_gemm_grouped/gemm_grouped.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1483,16 +1483,16 @@
 
   using LayoutA = cutlass::layout::ColumnMajor;
   using LayoutB = cutlass::layout::ColumnMajor;
   using LayoutC = cutlass::layout::ColumnMajor;
 
   // Gemm operator cutlass_tensorop_f16_s16816gemm_f16_128x128_32x4_nt_align8
   using GemmBatched = cutlass::gemm::device::GemmUniversal<
-    cutlass::half_t, LayoutA,
-    cutlass::half_t, LayoutB,
+    ElementA, LayoutA,
+    ElementB, LayoutB,
     ElementOutput,   LayoutC,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<128, 128, 32>,
     cutlass::gemm::GemmShape<64, 64, 32>,
     cutlass::gemm::GemmShape<16, 8, 16>,
@@ -1506,19 +1506,19 @@
     4
   >;
 
   // Define a grouped GEMM kernel with all template parameters set except
   // for scheduling mode. This will be used as the template for all scheduling
   // modes executed.
   using GemmKernel = typename cutlass::gemm::kernel::DefaultGemmGrouped<
-    cutlass::half_t, 
+    ElementA,
     LayoutA,
     cutlass::ComplexTransform::kNone,
     8,
-    cutlass::half_t,
+    ElementB,
     LayoutB,
     cutlass::ComplexTransform::kNone,
     8,
     ElementOutput, LayoutC,
     ElementAccumulator, 
     cutlass::arch::OpClassTensorOp, 
     cutlass::arch::Sm80,
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_3d_fprop_mainloop_fusion.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_3d_fprop_mainloop_fusion.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_fprop_mainloop_fusion.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_fprop_mainloop_fusion.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/26_ampere_wgrad_mainloop_fusion/ampere_wgrad_mainloop_fusion.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/26_ampere_wgrad_mainloop_fusion/ampere_wgrad_mainloop_fusion.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/ampere_3xtf32_fast_accurate_tensorop_fprop.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/ampere_3xtf32_fast_accurate_tensorop_fprop.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/30_wgrad_split_k/30_wgrad_split_k.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/30_wgrad_split_k/30_wgrad_split_k.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/31_basic_syrk/basic_syrk.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/31_basic_syrk/basic_syrk.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/32_basic_trmm/basic_trmm.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/32_basic_trmm/basic_trmm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/33_ampere_3xtf32_tensorop_symm/ampere_3xtf32_tensorop_symm.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/33_ampere_3xtf32_tensorop_symm/ampere_3xtf32_tensorop_symm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/34_transposed_conv2d/34_transposed_conv2d.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/34_transposed_conv2d/34_transposed_conv2d.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_softmax.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/35_gemm_softmax/gemm_softmax.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_with_epilogue_visitor.h` & `flash_attn-2.0.0/csrc/cutlass/examples/35_gemm_softmax/gemm_with_epilogue_visitor.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_with_softmax.h` & `flash_attn-2.0.0/csrc/cutlass/examples/35_gemm_softmax/gemm_with_softmax.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/36_gather_scatter_fusion/gather_scatter_fusion.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/36_gather_scatter_fusion/gather_scatter_fusion.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h` & `flash_attn-2.0.0/csrc/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h` & `flash_attn-2.0.0/csrc/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h`

 * *Files 0% similar despite different names*

```diff
@@ -31,15 +31,15 @@
 
 /*! \file
     \brief A file contains all functioning classes needed by GemmLayernorm.
 
     GemmLayernorm example =  GEMM0 with partial reduction fused in epilogue (EpilogueVisitorLayerNorm)
                           +  lightweight full reduction kernel (ApplyFinalReduction)
                           +  GEMM1 with elemenwise operations fused in mainloop (GemmLayernormMainloopFusion)
-                          
+
 */
 
 #pragma once
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 #include <cmath>
@@ -73,29 +73,29 @@
 
 template <
   typename ElementVariance_,
   typename ElementMean_,
   typename ElementLayernormCompute_,
   typename ElementOutput,
   typename ThreadblockShape_,
-  bool IsShiftedVariance_ = false 
+  bool IsShiftedVariance_ = false
 >
 class ApplyFinalReduction {
 public:
 
   using ElementVariance = ElementVariance_;
   using ElementMean = ElementMean_;
   using ElementLayernormCompute = ElementLayernormCompute_;
   using ThreadblockShape = ThreadblockShape_;
 
   // Pre-processing has ensured the layout equivelent to RowMajor
   using Layout = cutlass::layout::RowMajor;
 
   using TensorVariance = TensorRef<ElementVariance, Layout>;
-  using TensorMean = TensorRef<ElementMean, Layout>;  
+  using TensorMean = TensorRef<ElementMean, Layout>;
 
   static bool const kIsShiftedVariance = IsShiftedVariance_;
 
   //
   // Arguments
   //
 
@@ -459,15 +459,15 @@
       CUTLASS_PRAGMA_UNROLL
       for (int iter_idx = 0; iter_idx < kIterations; ++iter_idx) {
         int step_offset = iter_idx * OutputTileIterator::Shape::kRow;
         CUTLASS_PRAGMA_UNROLL
         for (int rid = 0; rid < kRowIterations; ++rid) {
           int row_step_offset = rid * kDeltaRow;
           int row_offset = thread_offset_row_base + step_offset + row_step_offset;
-          bool is_load = (row_offset < extent_.row());  
+          bool is_load = (row_offset < extent_.row());
           shift_k_frag_[iter_idx * kRowIterations + rid] = load_shift_k_(row_offset, is_load);
         }
 
       }
 
     }
 
@@ -500,17 +500,17 @@
     int frag_idx,
     AccumulatorFragment const &accum) {
 
     using Mul = cutlass::multiplies<ElementLayernormCompute>;
     using Minus = cutlass::minus<ElementLayernormCompute>;
     using Exp   = cutlass::fast_exp_op<ElementLayernormCompute>;
 
-    Minus     minus;
-    Mul       mul;
-    Exp       exponential;
+    [[maybe_unused]] Minus minus;
+    [[maybe_unused]] Mul   mul;
+    [[maybe_unused]] Exp   exponential;
 
     LayernormFragment result;
 
     thread_offset_ =
       iterator_D_.thread_start() +
       OutputTileIterator::ThreadMap::iteration_offset(frag_idx);
 
@@ -601,24 +601,24 @@
   }
 
 private:
 
   CUTLASS_DEVICE
   ElementLayernormCompute load_shift_k_(int row_offset, bool is_load) {
     using ConvertShiftK = cutlass::NumericConverter<ElementLayernormCompute, ElementOutput>;
-    ConvertShiftK convert_shift_k;    
+    ConvertShiftK convert_shift_k;
     ElementOutput shift_k_val;
 
     // Computes the address to load shift_k element
     ElementOutput *curr_ptr_shift_k = params_.ptr_Shifted_K + row_offset;
     // Conditionally loads from global memory
     arch::global_load<ElementOutput, sizeof(ElementOutput)>(shift_k_val, (void *)curr_ptr_shift_k, is_load);
     // Converts data type to return
     ElementLayernormCompute converted_shift_k_val = convert_shift_k(shift_k_val);
-    
+
     return converted_shift_k_val;
   }
 
   CUTLASS_DEVICE
   ElementLayernormCompute square_sum_accumulator_(LayernormFragment const &accum) {
     ElementLayernormCompute sum_ = ElementLayernormCompute(0);
 
@@ -685,46 +685,46 @@
 public:
 
   ///////////////////////////////////////////////////////////////////////////////////////////////
 
   //
   // Type definitions
   //
-  
+
   static bool const kInternalTranspose = cutlass::platform::is_same<LayoutOutput_, cutlass::layout::ColumnMajor>::value;
   static bool const kIsShiftedVariance = IsShiftedVariance_;
 
   // These is mandatory layout.
   using LayoutInputScaleBias = cutlass::layout::RowMajor;
 
   // These are mandatory data types.
   using ElementLayernormCompute = float;
   using ElementInputScaleBias = cutlass::half_t;
 
   // These are mandatory params required by mainloop fusion
   using OperatorClass       = cutlass::arch::OpClassTensorOp;
   using ArchTag             = cutlass::arch::Sm80;
 
-  // These are mandatory layouts and data types 
+  // These are mandatory layouts and data types
   // that are inheritated from pre-defined params
-  
+
   using LayoutSumSqr = LayoutInputScaleBias;
   using LayoutSum = LayoutInputScaleBias;
 
   using ElementMean = ElementInputScaleBias;
-  using ElementVariance = ElementInputScaleBias;  
+  using ElementVariance = ElementInputScaleBias;
 
   ///////////////////////////////////////////////////////////////////////////////////////////////
 
   using LayoutInputA0 = LayoutInputA0_;
   using LayoutInputB0 = LayoutInputB0_;
   using LayoutInputA1 = LayoutOutput_;
   using LayoutInputB1 = LayoutOutput_;
   using LayoutOutputC0 = LayoutOutput_;
-  using LayoutOutputC1 = LayoutOutput_;  
+  using LayoutOutputC1 = LayoutOutput_;
 
   using ElementInputA0 = ElementInputA0_;
   using ElementInputB0 = ElementInputB0_;
   using ElementOutputC0 = ElementOutput_;
   using ElementCompute = ElementCompute_;
   using ElementInputB1 = ElementInputB0_;
 
@@ -743,15 +743,15 @@
   using WarpShape        = WarpShape_;
   using InstructionShape = InstructionShape_;
 
   static int const kStages0 = Stages0;
   static int const kStages1 = Stages1;
 
   using SwizzleThreadBlock = cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>;
-  
+
   ///////////////////////////////////////////////////////////////////////////////////////////////
 
   using MapArguments = cutlass::gemm::kernel::detail::MapArguments<
     ElementInputA0,
     LayoutInputA0,
     cutlass::ComplexTransform::kNone,
     128 / cutlass::sizeof_bits<ElementInputA0>::value,
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/38_syr2k_grouped/syr2k_grouped.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/38_syr2k_grouped/syr2k_grouped.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/39_gemm_permute/gemm_permute.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/39_gemm_permute/gemm_permute.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/attention_scaling_coefs_updater.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse_testbed.h`

 * *Files 25% similar despite different names*

```diff
@@ -8,15 +8,15 @@
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
  *
  * 2. Redistributions in binary form must reproduce the above copyright notice,
  * this list of conditions and the following disclaimer in the documentation
  * and/or other materials provided with the distribution.
  *
- * 3. Neither the name of the copyright holdvr nor the names of its
+ * 3. Neither the name of the copyright holder nor the names of its
  * contributors may be used to endorse or promote products derived from
  * this software without specific prior written permission.
  *
  * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
  * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
  * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
@@ -24,490 +24,415 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
+/*! \file
+    \brief Unit testbed for kernel-level GEMM
+*/
 
 #pragma once
 
-#include "cutlass/functional.h"
-#include "cutlass/gemm/warp/mma_simt_tile_iterator.h"
-#include "cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm70.h"
-#include "cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm80.h"
-#include "cutlass/matrix_shape.h"
-#include "gemm_kernel_utils.h"
-
-namespace {
-
-static CUTLASS_DEVICE float atomicMaxFloat(float* addr, float value) {
-  // source: https://stackoverflow.com/a/51549250
-  return (value >= 0)
-      ? __int_as_float(atomicMax((int*)addr, __float_as_int(value)))
-      : __uint_as_float(atomicMin((unsigned int*)addr, __float_as_uint(value)));
-}
-} // namespace
+#include "../../common/cutlass_unit_test.h"
+#include "cutlass/aligned_buffer.h"
+#include "cutlass/array.h"
+#include "cutlass/core_io.h"
+#include "cutlass/gemm/gemm.h"
+#include "cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h"
+#include "cutlass/layout/matrix.h"
+#include "cutlass/numeric_types.h"
+#include "cutlass/transform/threadblock/predicated_tile_access_iterator.h"
+#include "cutlass/util/distribution.h"
+#include "cutlass/util/host_tensor.h"
+#include "cutlass/util/reference/host/gemm.h"
+#include "cutlass/util/reference/host/tensor_compare.h"
+#include "cutlass/util/reference/host/tensor_norm.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/tensor_view_io.h"
+#include "cutlass/util/host_reorder.h"
+#include "cutlass/util/host_uncompress.h"
+
+namespace test {
+namespace gemm {
+namespace threadblock {
+
+////////////////////////////////////////////////////////////////////////////////
+
+template <typename Mma>
+__global__ void kernel_multistage_mma_sparse(cutlass::gemm::GemmCoord problem_size,
+                                      typename Mma::IteratorA::Params params_A,
+                                      typename Mma::IteratorA::TensorRef ref_A,
+                                      typename Mma::IteratorB::Params params_B,
+                                      typename Mma::IteratorB::TensorRef ref_B,
+                                      typename Mma::ElementC *ptr_C,
+                                      typename Mma::LayoutC::Stride::Index ldc,
+                                      typename Mma::IteratorE::Params params_E,
+                                      typename Mma::IteratorE::TensorRef ref_E) {
+  // Shared storage needed by threadblock-scoped matrix multiply-
+  // Dynamic shared memory base pointer
+  extern __shared__ int GemmSharedStorageBase[];
+
+  // Declare pointer to dynamic shared memory.
+  typename Mma::SharedStorage *shared_storage =
+      reinterpret_cast<typename Mma::SharedStorage *>(GemmSharedStorageBase);
+
+  // Compute threadblock location
+  cutlass::gemm::GemmCoord tb_tile_offset = {int(blockIdx.x), int(blockIdx.y),
+                                             0};
+
+  cutlass::MatrixCoord tb_offset_A{tb_tile_offset.m() * Mma::Shape::kM,
+                                   tb_tile_offset.k() / Mma::kSparse};
+
+  cutlass::MatrixCoord tb_offset_B{tb_tile_offset.k(),
+                                   tb_tile_offset.n() * Mma::Shape::kN};
+
+  cutlass::MatrixCoord tb_offset_E{tb_tile_offset.m() * Mma::Shape::kM,
+                                   tb_tile_offset.k() / Mma::kSparse};
+
+  // Compute position within threadblock
+  int tb_thread_id = threadIdx.y * blockDim.x + threadIdx.x;
+
+  // Construct iterators to A and B operands
+  typename Mma::IteratorA iterator_A(params_A, ref_A.data(),
+                                     {problem_size.m(), problem_size.k() / Mma::kSparse},
+                                     tb_thread_id, tb_offset_A);
+
+  typename Mma::IteratorB iterator_B(params_B, ref_B.data(),
+                                     {problem_size.k(), problem_size.n()},
+                                     tb_thread_id, tb_offset_B);
+
+  typename Mma::IteratorE iterator_E(
+      params_E, ref_E.data(),
+      {problem_size.m(),
+       problem_size.k() / Mma::kSparse / Mma::kElementsPerElementE},
+      tb_thread_id, tb_offset_E);
+
+  int warp_id = __shfl_sync(0xffffffff, threadIdx.y, 0);
+
+  // Construct thread-scoped matrix multiply
+  Mma mma(*shared_storage, tb_thread_id, warp_id, threadIdx.x);
+
+  typename Mma::FragmentC accum;
+
+  accum.clear();
+
+  int gemm_k_iterations = (problem_size.k() + Mma::Shape::kK - 1) / Mma::Shape::kK;
+
+  // Compute threadblock-scoped matrix multiply-add
+  mma(gemm_k_iterations, accum, iterator_A, iterator_B, iterator_E, accum);
+
+  // Output results
+  typename Mma::Operator::IteratorC iterator_C({ptr_C, ldc}, threadIdx.x);
+
+  iterator_C.add_tile_offset(
+      {(tb_tile_offset.m() * Mma::WarpCount::kM) +
+           (warp_id % Mma::WarpCount::kM),
+       (tb_tile_offset.n() * Mma::WarpCount::kN) +
+           (warp_id / Mma::WarpCount::kM)});
 
-/* Iterates on the accumulator and corresponding position on result matrix
+  iterator_C.store(accum);
+}
 
-(1) Update `mi[r]` to the max value of the row `r`
-(2) In a second iteration do the following:
-    (a) accum   <- exp(accum - mi)
-    (b) m_prime <- exp(m_prime - mi)
-    (c) s_prime <- s_prime * m_prime + sum(accum)
+////////////////////////////////////////////////////////////////////////////////
 
-All of this is done on registers, before we store all of this
-on shared memory for the next matmul with Value.
+/// Structure to compute the matrix product
+template <
+    /// Threadblock-level matrix multiply-accumulate
+    typename MmaCore_>
+struct SparseTestbed {
+  /// Threadblock-level GEMM implementation
+  using MmaCore = MmaCore_;
+  using ThreadblockShape = typename MmaCore::Shape;
+  using WarpShape = typename MmaCore::WarpShape;
+  using InstructionShape = typename MmaCore::InstructionShape;
+  using ElementA = typename MmaCore::ElementA;
+  using LayoutA = typename MmaCore::LayoutA;
+  using ElementB = typename MmaCore::ElementB;
+  using LayoutB = typename MmaCore::LayoutB;
+  using ElementC = typename MmaCore::ElementC;
+  using LayoutC = typename MmaCore::LayoutC;
+  using ElementE = typename MmaCore::ElementE;
+  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
+  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
+  using ThreadMapE = typename MmaCore::IteratorThreadMapE;
+  using AccessTypeA = cutlass::Array<ElementA, ThreadMapA::kElementsPerAccess>;
+  using AccessTypeB = cutlass::Array<ElementB, ThreadMapB::kElementsPerAccess>;
+  using AccessTypeE = cutlass::Array<ElementE, ThreadMapE::kElementsPerAccess>;
+  static int const Stages = MmaCore::kStages;
+  static cutlass::arch::CacheOperation::Kind const CacheOpA =
+      MmaCore::kCacheOpA;
+  static cutlass::arch::CacheOperation::Kind const CacheOpB =
+      MmaCore::kCacheOpB;
+  static cutlass::arch::CacheOperation::Kind const CacheOpE =
+      MmaCore::kCacheOpE;
+
+  static int const Sparse = MmaCore::kSparse;
+  static int const MetaSizeInBits = MmaCore::kMetaSizeInBits;
+  static int const MaxID2 = MmaCore::kMaxID2;
+
+  using LayoutE = cutlass::layout::RowMajor;
+  using ReorderedLayoutE = typename MmaCore::GmemLayoutE;
+
+  static int const ElementsPerElementE = MmaCore::kElementsPerElementE;
+
+  // Define iterators over tiles from the A operand
+  using IteratorA =
+      cutlass::transform::threadblock::PredicatedTileAccessIterator<
+          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK / Sparse>,
+          ElementA, LayoutA, 1, ThreadMapA, AccessTypeA>;
+
+  // Define iterators over tiles from the B operand
+  using IteratorB =
+      cutlass::transform::threadblock::PredicatedTileAccessIterator<
+          cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
+          ElementB, LayoutB, 0, ThreadMapB, AccessTypeB>;
+
+  // Define iterators over tiles from the E operand
+  using IteratorE =
+      cutlass::transform::threadblock::PredicatedTileAccessIterator<
+          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK /
+                                                         Sparse /
+                                                         ElementsPerElementE>,
+          ElementE, ReorderedLayoutE, 1, ThreadMapE, AccessTypeE>;
+
+  // Define the threadblock-scoped pipelined matrix multiply
+  using Mma = cutlass::gemm::threadblock::SparseMmaMultistage<
+      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
+      CacheOpA, IteratorB, typename MmaCore::SmemIteratorB, CacheOpB, ElementC,
+      LayoutC, IteratorE, typename MmaCore::SmemIteratorE, CacheOpE,
+      typename MmaCore::MmaPolicy, Stages>;
+
+  //
+  // Data members
+  //
+
+  cutlass::HostTensor<ElementA, LayoutA> matrix_A;
+  cutlass::HostTensor<ElementA, LayoutA> matrix_A_uncompressed;
+  cutlass::HostTensor<ElementB, LayoutB> matrix_B;
+  cutlass::HostTensor<ElementC, LayoutC> matrix_C_computed;
+  cutlass::HostTensor<ElementC, LayoutC> matrix_C_reference;
+  cutlass::HostTensor<ElementE, LayoutE> matrix_E;
+  cutlass::HostTensor<ElementE, ReorderedLayoutE> matrix_E_reordered;
+
+  cutlass::gemm::GemmCoord problem_size;
+  float alpha, beta;
+
+  //
+  // Methods
+  //
+
+  /// Allocates workspace in device memory
+  SparseTestbed(int m, int n, int k, float alpha_ = float(1), float beta_ = float(0))
+      : problem_size(m, n, k), alpha(alpha_), beta(beta_) {
+    matrix_A.reset(cutlass::make_Coord(m, k / Sparse));
+    matrix_A_uncompressed.reset(cutlass::make_Coord(m, k));
+    matrix_B.reset(cutlass::make_Coord(k, n));
+    matrix_C_computed.reset(cutlass::make_Coord(m, n));
+    matrix_C_reference.reset(cutlass::make_Coord(m, n), false);
+    matrix_E.reset(cutlass::make_Coord(m, k / Sparse / ElementsPerElementE));
+    matrix_E_reordered.reset(
+        cutlass::make_Coord(m, k / Sparse / ElementsPerElementE));
+  }
 
-We have multiple implementations, because each configuration has a different way
-of iterating in the accumulators.
-*/
+  /// Returns true if the CUDA device is sufficient to execute the kernel.
+  bool sufficient() const {
+    //
+    // Determine SMEM requirements and waive if not satisfied
+    //
+
+    cudaDeviceProp properties;
+    int device_idx;
+    cudaError_t result = cudaGetDevice(&device_idx);
 
-template <typename BASE, typename T, typename accum_t, int kWarpSize>
-struct RegisterOps {
-  template <
-      int kQueriesPerBlock,
-      bool kFullColumns,
-      bool kIsFirst,
-      bool kKeepOutputInRF>
-  CUTLASS_DEVICE static void update(
-      typename T::Fragment& frag_o, // output so far
-      typename T::Fragment& frag,
-      cutlass::Array<accum_t, kQueriesPerBlock>& mi,
-      cutlass::Array<accum_t, kQueriesPerBlock>& m_prime,
-      cutlass::Array<accum_t, kQueriesPerBlock>& s_prime,
-      int8_t lane_id,
-      int8_t thread_id,
-      int8_t warp_id,
-      int16_t max_col,
-      typename T::TensorCoord const& tile_offset,
-      float scaling) {
-    // Convert to `accum_t` (rather than double)
-    constexpr float kLog2e = 1.4426950408889634074; // log_2(e) = M_LOG2E
-    if (!kIsFirst) {
-      if (thread_id < kQueriesPerBlock) {
-        m_prime[thread_id] = mi[thread_id];
-      }
-      __syncthreads();
+    if (result != cudaSuccess) {
+      throw std::runtime_error("cudaGetDevice() API call failed.");
     }
 
-    auto lane_offset = BASE::get_lane_offset(lane_id, warp_id, tile_offset);
+    result = cudaGetDeviceProperties(&properties, device_idx);
 
-    // First update `mi` to the max per-row
-    {
-      accum_t max;
-      BASE::iterateRows(
-          lane_offset,
-          [&](int accum_m) {
-            max = -cutlass::platform::numeric_limits<accum_t>::infinity();
-          },
-          [&](int accum_m, int accum_n, int idx) {
-            if (kFullColumns || accum_n < max_col) {
-              max = cutlass::fast_max(max, frag[idx]);
-            }
-          },
-          [&](int accum_m) {
-            // Having 4x atomicMax seems faster than reduce within warp
-            // first...
-            atomicMaxFloat(&mi[accum_m], max * scaling);
-          });
+    if (result != cudaSuccess) {
+      throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
-    frag = cutlass::multiplies<typename T::Fragment>()(scaling * kLog2e, frag);
 
-    // Make sure we all share the update values for `mi`
-    __syncthreads();
+    return true;
+  }
 
-    if (thread_id < kQueriesPerBlock) {
-      auto m_prime_exp = exp2f(kLog2e * (m_prime[thread_id] - mi[thread_id]));
-      m_prime[thread_id] = m_prime_exp;
-      s_prime[thread_id] *= m_prime_exp;
+  /// Runs the test
+  bool run(
+      dim3 grid, dim3 block,
+      cutlass::Distribution::Kind init_A = cutlass::Distribution::Uniform,
+      cutlass::Distribution::Kind init_B = cutlass::Distribution::Uniform,
+      cutlass::Distribution::Kind init_E = cutlass::Distribution::Uniform) {
+
+    // Waive the test
+    if (!sufficient()) {
+      return true;
     }
-    __syncthreads(); // Update output fragments
-    if (kKeepOutputInRF && !kIsFirst) {
-      accum_t mp;
-      BASE::iterateRows(
-          lane_offset,
-          [&](int accum_m) { mp = m_prime[accum_m]; },
-          [&](int accum_m, int accum_n, int idx) { frag_o[idx] *= mp; },
-          [&](int accum_m) {});
-      __syncthreads();
-    }
-    // Update accum_m, accum_n, ...
-    {
-      accum_t mi_row, total_row;
-      BASE::iterateRows(
-          lane_offset,
-          [&](int accum_m) { mi_row = kLog2e * mi[accum_m]; },
-          [&](int accum_m, int accum_n, int idx) {
-            frag[idx] = (kFullColumns || accum_n < max_col)
-                ? exp2f(frag[idx] - mi_row)
-                : accum_t(0.0);
-          },
-          [&](int accum_m) {});
-      BASE::iterateRows(
-          lane_offset,
-          [&](int accum_m) { total_row = 0.0; },
-          [&](int accum_m, int accum_n, int idx) { total_row += frag[idx]; },
-          [&](int accum_m) {
-            if (BASE::reduceSameRow(
-                    lane_id, total_row, [](accum_t a, accum_t b) {
-                      return a + b;
-                    })) {
-              atomicAdd(&s_prime[accum_m], total_row);
-            }
-          });
+
+    //
+    // initialize device memory
+    //
+
+    if (init_A == cutlass::Distribution::Uniform) {
+
+      int scope_max = 8;
+      int scope_min = -8;
+
+      if (cutlass::sizeof_bits<ElementA>::value == 4) {
+        scope_max = 2;
+        scope_min = -2;
+      } else if (cutlass::sizeof_bits<ElementA>::value == 1) {
+        scope_max = 2;
+        scope_min = 0;
+      }
+
+      uint64_t seed = 7;
+      cutlass::reference::host::TensorFillRandomUniform(
+          matrix_A.host_view(), seed, scope_max, scope_min, 0);
+    } else if (init_A == cutlass::Distribution::Sequential) {
+      cutlass::reference::host::BlockFillSequential(matrix_A.host_data(),
+                                                    matrix_A.capacity());
+    } else if (init_A == cutlass::Distribution::Identity) {
+      cutlass::reference::host::TensorFillIdentity(matrix_A.host_view());
+    } else {
+      // TODO: Implement the rest
+      return false;
     }
-  }
-};
 
-template <typename T, typename accum_t, int kWarpSize>
-struct AttentionScalingCoefsUpdaterSm80
-    : RegisterOps<
-          AttentionScalingCoefsUpdaterSm80<T, accum_t, kWarpSize>,
-          T,
-          accum_t,
-          kWarpSize> {
-  static_assert(
-      cutlass::platform::
-          is_same<typename T::Layout, cutlass::layout::RowMajor>::value,
-      "only RowMajor is supported");
-
-  using Policy = typename T::Policy;
-  using InstructionShape = typename T::InstructionShape;
-  using OpDelta = typename T::OpDelta;
-  using Shape = typename T::Shape;
-  static int const kElementsPerAccess = InstructionShape::kN / 4;
-  static int const kRowsPerTile = 8;
-  static int const kAccumulatorRows = InstructionShape::kM / kRowsPerTile;
-
-  static cutlass::MatrixCoord CUTLASS_DEVICE get_lane_offset(
-      int8_t lane_id,
-      int8_t warp_id,
-      typename T::TensorCoord const& tile_offset) {
-    int quad = (lane_id >> 2);
-    int lane_in_quad = (lane_id & 3);
-    return cutlass::MatrixCoord(
-        quad + tile_offset.row() * Shape::kRow,
-        lane_in_quad * kElementsPerAccess +
-            tile_offset.column() * Shape::kColumn);
-  }
+    if (init_B == cutlass::Distribution::Uniform) {
 
-  template <typename FA, typename FB, typename FC>
-  CUTLASS_DEVICE static void iterateRows(
-      cutlass::MatrixCoord& lane_offset,
-      FA beginRow,
-      FB op,
-      FC endRow) {
-    // See cutlass/gemm/warp/mma_tensor_op_tile_iterator.h
-    CUTLASS_PRAGMA_UNROLL
-    for (int mma_m = 0; mma_m < Policy::MmaIterations::kRow; ++mma_m) {
-      CUTLASS_PRAGMA_UNROLL
-      for (int row = 0; row < kAccumulatorRows; ++row) {
-        int accum_m = mma_m * InstructionShape::kM * OpDelta::kRow +
-            row * kRowsPerTile + lane_offset.row();
-        beginRow(accum_m);
-
-        CUTLASS_PRAGMA_UNROLL
-        for (int mma_n = 0; mma_n < Policy::MmaIterations::kColumn; ++mma_n) {
-          int mma_accum_start = kAccumulatorRows * kElementsPerAccess *
-              (mma_n * Policy::MmaIterations::kRow + mma_m);
-          CUTLASS_PRAGMA_UNROLL
-          for (int col = 0; col < kElementsPerAccess; ++col) {
-            int accum_n = mma_n * InstructionShape::kN * OpDelta::kColumn +
-                col + lane_offset.column();
-            int idx = mma_accum_start + row * kElementsPerAccess + col;
-            op(accum_m, accum_n, idx);
-          }
-        }
+      int scope_max = 8;
+      int scope_min = -8;
 
-        endRow(accum_m);
+      if (cutlass::sizeof_bits<ElementB>::value == 4) {
+        scope_max = 2;
+        scope_min = -2;
+      } else if (cutlass::sizeof_bits<ElementB>::value == 1) {
+        scope_max = 2;
+        scope_min = 0;
       }
+
+      uint64_t seed = 7;
+      cutlass::reference::host::TensorFillRandomUniform(
+          matrix_B.host_view(), seed + 16, scope_max, scope_min, 0);
+    } else if (init_B == cutlass::Distribution::Sequential) {
+      cutlass::reference::host::BlockFillSequential(matrix_B.host_data(),
+                                                    matrix_B.capacity());
+    } else if (init_B == cutlass::Distribution::Identity) {
+      cutlass::reference::host::TensorFillIdentity(matrix_B.host_view());
+    } else {
+      // TODO: Implement the rest
+      return false;
     }
-  }
 
-  template <typename DT, typename F>
-  CUTLASS_DEVICE static bool reduceSameRow(int lane_id, DT& myValue, F fn) {
-    // In each warp, 4 threads will work on the same row
-    // - the ones with the same `quad`
-    auto otherV = __shfl_xor_sync(0xffffffff, myValue, 1);
-    myValue = fn(myValue, otherV);
-    otherV = __shfl_xor_sync(0xffffffff, myValue, 2);
-    myValue = fn(myValue, otherV);
-    int lane_in_quad = (lane_id & 3);
-    return lane_in_quad == 0;
-  }
-};
+    cutlass::reference::host::TensorFill(matrix_C_computed.host_view());
+
+    cutlass::reference::host::TensorFill(matrix_C_reference.host_view());
 
-template <typename T, typename accum_t, int kWarpSize>
-struct AttentionScalingCoefsUpdaterVolta
-    : RegisterOps<
-          AttentionScalingCoefsUpdaterVolta<T, accum_t, kWarpSize>,
-          T,
-          accum_t,
-          kWarpSize> {
-  static_assert(
-      cutlass::platform::
-          is_same<typename T::Layout, cutlass::layout::RowMajor>::value,
-      "only RowMajor is supported");
-
-  using Policy = typename T::Policy;
-  using InstructionShape = typename T::InstructionShape;
-  using OpDelta = typename T::OpDelta;
-  using Shape = typename T::Shape;
-  using Element = accum_t;
-
-  static int const kElementsPerPartial = 4;
-  using EleShapePerPatial = typename cutlass::platform::conditional<
-      cutlass::platform::is_same<Element, float>::value,
-      cutlass::MatrixShape<2, 2>,
-      cutlass::MatrixShape<1, 4>>::type;
-  static int const kElementsPerMma = 8;
-  static int const kAccumulatorPatials = 2;
-  using QuadShapePerPatialMma = cutlass::MatrixShape<4, 4>;
-
-  static cutlass::MatrixCoord CUTLASS_DEVICE get_lane_offset(
-      int8_t lane_id,
-      int8_t warp_id,
-      typename T::TensorCoord const& tile_offset) {
-    int quad = (lane_id >> 2);
-    int lane_in_quad = (lane_id & 3);
-    int accum_m, accum_n;
-
-    if (cutlass::platform::is_same<Element, float>::value) {
-      // (quad[2],quad[0])+lane_in_quad[0]
-      accum_m = (((quad & 0x4) >> 1) + (quad & 0x1)) * 8 + (lane_in_quad & 1);
-      // (quad[1])+lane_in_quad[1]
-      accum_n =
-          ((quad >> 1) & 0x1) * kElementsPerPartial * kAccumulatorPatials +
-          (lane_in_quad & 2);
+    if (init_E == cutlass::Distribution::Uniform) {
+      uint64_t seed = 7;
+      cutlass::reference::host::TensorFillRandomSparseMeta(
+          matrix_E.host_view(), seed, MetaSizeInBits);
+    } else if (init_E == cutlass::Distribution::Identity) {
+      uint32_t content = (MaxID2 == 1) ? 0x44444444 : 0x4444;
+      cutlass::reference::host::TensorFill(matrix_E.host_view(),
+                                           (ElementE)(content));
     } else {
-      accum_m = (((quad & 0x4) >> 1) + (quad & 0x1)) * 8 +
-          lane_in_quad; // (quad[2],quad[0])
-      accum_n = ((quad >> 1) & 0x1) * kElementsPerPartial * kAccumulatorPatials;
+      // TODO: Implement the rest
+      return false;
     }
-    return cutlass::MatrixCoord(
-        accum_m + tile_offset.row() * Shape::kRow,
-        accum_n + tile_offset.column() * Shape::kColumn);
-  }
 
-  template <typename DT, typename F>
-  CUTLASS_DEVICE static bool reduceSameRow(int lane_id, DT& myValue, F fn) {
-    static_assert(
-        cutlass::platform::is_same<Element, float>::value,
-        "update to support non-float accum");
-    // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-fragment-mma-884-f16
-    // T0 & T2 share same line within a quad
-    auto otherV = __shfl_xor_sync(0xffffffff, myValue, 1 << 1);
-    myValue = fn(myValue, otherV);
-    // quad 0 and quad 2 are on the same lines
-    otherV = __shfl_xor_sync(0xffffffff, myValue, 1 << 3);
-    myValue = fn(myValue, otherV);
-    return (lane_id & ((1 << 1) | (1 << 3))) == 0;
-  }
+    cutlass::reorder_meta(matrix_E_reordered.host_ref(), matrix_E.host_ref(),
+                          {problem_size.m(), problem_size.n(),
+                           problem_size.k() / Sparse / ElementsPerElementE});
+
+    matrix_A.sync_device();
+    matrix_B.sync_device();
+    matrix_C_computed.sync_device();
+    matrix_E_reordered.sync_device();
+
+    typename IteratorA::Params params_A(matrix_A.layout());
+    typename IteratorB::Params params_B(matrix_B.layout());
+    typename IteratorE::Params params_E(matrix_E_reordered.layout());
+
+    cudaError_t result;
+
+    int smem_size = int(sizeof(typename Mma::SharedStorage));
+    if (smem_size >= (48 << 10)) {
+      result = cudaFuncSetAttribute(
+          test::gemm::threadblock::kernel_multistage_mma_sparse<Mma>,
+          cudaFuncAttributeMaxDynamicSharedMemorySize, smem_size);
 
-  template <typename FA, typename FB, typename FC>
-  CUTLASS_DEVICE static void iterateRows(
-      cutlass::MatrixCoord& lane_offset,
-      FA beginRow,
-      FB op,
-      FC endRow) {
-    CUTLASS_PRAGMA_UNROLL
-    for (int tile_m = 0; tile_m < Policy::TileIterations::kRow; ++tile_m) {
-      CUTLASS_PRAGMA_UNROLL
-      for (int mma_m = 0; mma_m < Policy::MmaIterations::kRow; ++mma_m) {
-        CUTLASS_PRAGMA_UNROLL
-        for (int m = 0; m < EleShapePerPatial::kRow; ++m) {
-          int accum_m = tile_m * Policy::InterleavedTile::kRow +
-              mma_m * QuadShapePerPatialMma::kRow + m * 2 + lane_offset.row();
-          beginRow(accum_m);
-
-          CUTLASS_PRAGMA_UNROLL
-          for (int tile_n = 0; tile_n < Policy::TileIterations::kColumn;
-               ++tile_n) {
-            CUTLASS_PRAGMA_UNROLL
-            for (int mma_n = 0; mma_n < Policy::MmaIterations::kColumn;
-                 ++mma_n) {
-              CUTLASS_PRAGMA_UNROLL
-              for (int p = 0; p < kAccumulatorPatials; ++p) {
-                CUTLASS_PRAGMA_UNROLL
-                for (int n = 0; n < EleShapePerPatial::kColumn; ++n) {
-                  int mma_accum_start =
-                      (((tile_n * Policy::TileIterations::kRow + tile_m) *
-                            Policy::MmaIterations::kColumn +
-                        mma_n) *
-                           Policy::MmaIterations::kRow +
-                       mma_m) *
-                      kElementsPerMma;
-                  int accum_n = tile_n * Policy::InterleavedTile::kColumn +
-                      mma_n * QuadShapePerPatialMma::kColumn +
-                      p * Policy::InterleavedTile::kColumn / 2 + n +
-                      lane_offset.column();
-                  int idx = mma_accum_start + p * kElementsPerPartial +
-                      m * EleShapePerPatial::kColumn + n;
-                  op(accum_m, accum_n, idx);
-                }
-              }
-            }
-          }
-          endRow(accum_m);
-        }
+      if (result != cudaSuccess) {
+          return true;
       }
-    }
-  }
-};
 
-template <typename T, typename accum_t, int kWarpSize>
-struct AttentionScalingCoefsUpdaterSimt
-    : RegisterOps<
-          AttentionScalingCoefsUpdaterSimt<T, accum_t, kWarpSize>,
-          T,
-          accum_t,
-          kWarpSize> {
-  using Policy = typename T::Policy;
-  using Iterations = typename T::Iterations;
-  using Element = typename T::Element;
-  using Delta = typename T::Delta;
-  using Shape = typename T::Shape;
-  static_assert(
-      cutlass::platform::
-          is_same<typename T::Layout, cutlass::layout::RowMajor>::value,
-      "only RowMajor is supported");
-
-  template <typename DT, typename F>
-  CUTLASS_DEVICE static bool reduceSameRow(int lane_id, DT& myValue, F fn) {
-    CUTLASS_PRAGMA_UNROLL
-    for (int bit = 1; bit < Policy::WarpShape::kColumn; bit *= 2) {
-      auto otherV = __shfl_xor_sync(0xffffffff, myValue, bit);
-      myValue = fn(myValue, otherV);
-    }
-    return (lane_id & (Policy::WarpShape::kColumn - 1)) == 0;
-  }
+      result = cudaFuncSetAttribute(
+          test::gemm::threadblock::kernel_multistage_mma_sparse<Mma>,
+          cudaFuncAttributePreferredSharedMemoryCarveout, 100);
 
-  template <typename FA, typename FB, typename FC>
-  CUTLASS_DEVICE static void iterateRows(
-      cutlass::MatrixCoord& lane_offset,
-      FA beginRow,
-      FB op,
-      FC endRow) {
-    CUTLASS_PRAGMA_UNROLL
-    for (int mma_m = 0; mma_m < Iterations::kRow; ++mma_m) {
-      CUTLASS_PRAGMA_UNROLL
-      for (int m = 0; m < Policy::LaneMmaShape::kM; ++m) {
-        int accum_m = mma_m * Delta::kRow + m + lane_offset.row();
-        beginRow(accum_m);
-
-        CUTLASS_PRAGMA_UNROLL
-        for (int mma_n = 0; mma_n < Iterations::kColumn; ++mma_n) {
-          int accum_n =
-              mma_n * Policy::WarpShape::kColumn * Policy::LaneMmaShape::kN +
-              lane_offset.column();
-          CUTLASS_PRAGMA_UNROLL
-          for (int n = 0; n < Policy::LaneMmaShape::kN; ++n) {
-            int idx = n +
-                Policy::LaneMmaShape::kN *
-                    (mma_n +
-                     Iterations::kColumn *
-                         (m + mma_m * Policy::LaneMmaShape::kM));
-            op(accum_m, accum_n + n, idx);
-          }
-        }
-        endRow(accum_m);
+      if (result != cudaSuccess) {
+          return true;
       }
     }
-  }
 
-  static cutlass::MatrixCoord CUTLASS_DEVICE get_lane_offset(
-      int8_t lane_id,
-      int8_t warp_id,
-      typename T::TensorCoord const& tile_offset) {
-    static_assert(
-        cutlass::platform::is_same<
-            typename Policy::LaneLayout,
-            cutlass::layout::RowMajorInterleaved<1>>::value,
-        "");
-    typename Policy::LaneLayout lane_layout = Policy::get_lane_layout();
-
-    cutlass::MatrixCoord lane_offset = lane_layout.inverse(lane_id) *
-        cutlass::MatrixCoord(Policy::LaneMmaShape::kM,
-                             Policy::LaneMmaShape::kN);
-    return lane_offset +
-        tile_offset * cutlass::MatrixCoord(Shape::kRow, Shape::kColumn);
-  }
-};
+    test::gemm::threadblock::kernel_multistage_mma_sparse<Mma>
+        <<<grid, block, smem_size, 0>>>(
+            problem_size, params_A, matrix_A.device_ref(), params_B,
+            matrix_B.device_ref(), matrix_C_computed.device_data(),
+            matrix_C_computed.layout().stride(0), params_E,
+            matrix_E_reordered.device_ref());
+
+    //
+    // Check error code
+    //
+
+    result = cudaDeviceSynchronize();
+    EXPECT_EQ(result, cudaSuccess)
+        << " kernel error: " << cudaGetErrorString(result);
+
+    matrix_C_computed.sync_host();
+
+    cutlass::uncompress(matrix_A_uncompressed.host_ref(), matrix_A.host_ref(),
+                        matrix_E.host_ref(), problem_size.m(),
+                        problem_size.k());
+
+    cutlass::reference::host::Gemm<ElementA, LayoutA, ElementB, LayoutB,
+                                   ElementC, LayoutC, ElementC, ElementC>
+        reference_gemm;
+
+    reference_gemm(problem_size, ElementC(alpha),
+                   matrix_A_uncompressed.host_view(), matrix_B.host_view(),
+                   ElementC(beta), matrix_C_reference.host_view());
+
+    bool passed = cutlass::reference::host::TensorEquals(
+        matrix_C_computed.host_view(), matrix_C_reference.host_view());
+
+    EXPECT_TRUE(passed);
+
+    if (!passed && CUTLASS_TEST_UNIT_ENABLE_WARNINGS) {
+
+      std::cout
+        << __FILE__ << ":" << __LINE__ << "  "
+        << "A:\n" << matrix_A.host_view() << "\n"
+        << "B:\n" << matrix_B.host_view() << "\n"
+        << "E:\n" << matrix_E.host_view() << "\n"
+        << "Reference:\n"
+        << matrix_C_reference.host_view() << "\n"
+        << "Computed:\n"
+        << matrix_C_computed.host_view() << "\n";
+    }
 
-template <typename T, typename accum_t, int kWarpSize>
-struct DefaultAttentionScalingCoefsUpdater;
+    EXPECT_GT(cutlass::reference::host::TensorNorm(matrix_C_reference.host_view()), 0);
+    EXPECT_GT(cutlass::reference::host::TensorNorm(matrix_C_computed.host_view()), 0);
 
-// Simt
-template <typename S, typename P, typename accum_t, int kWarpSize>
-struct DefaultAttentionScalingCoefsUpdater<
-    cutlass::gemm::warp::MmaSimtTileIterator<
-        S,
-        cutlass::gemm::Operand::kC,
-        accum_t,
-        cutlass::layout::RowMajor,
-        P,
-        1,
-        1>,
-    accum_t,
-    kWarpSize> {
-  using Iterator = typename cutlass::gemm::warp::MmaSimtTileIterator<
-      S,
-      cutlass::gemm::Operand::kC,
-      accum_t,
-      cutlass::layout::RowMajor,
-      P,
-      1,
-      1>;
-  using Updater =
-      AttentionScalingCoefsUpdaterSimt<Iterator, accum_t, kWarpSize>;
+    return passed;
+  }
 };
 
-// TensorOp - Volta
-template <typename S1, typename S2, typename accum_t, int kWarpSize>
-struct DefaultAttentionScalingCoefsUpdater<
-    cutlass::gemm::warp::MmaVoltaTensorOpAccumulatorTileIterator<
-        S1,
-        accum_t,
-        cutlass::layout::RowMajor,
-        S2,
-        cutlass::MatrixShape<1, 1>>,
-    accum_t,
-    kWarpSize> {
-  using Iterator =
-      typename cutlass::gemm::warp::MmaVoltaTensorOpAccumulatorTileIterator<
-          S1,
-          accum_t,
-          cutlass::layout::RowMajor,
-          S2,
-          cutlass::MatrixShape<1, 1>>;
-  using Updater =
-      AttentionScalingCoefsUpdaterVolta<Iterator, accum_t, kWarpSize>;
-};
+////////////////////////////////////////////////////////////////////////////////
 
-// TensorOp - Sm75+
-template <
-    typename S1,
-    typename S2,
-    typename S3,
-    typename accum_t,
-    int kWarpSize>
-struct DefaultAttentionScalingCoefsUpdater<
-    cutlass::gemm::warp::MmaTensorOpAccumulatorTileIterator<
-        S1,
-        accum_t,
-        cutlass::layout::RowMajor,
-        S2,
-        S3>,
-    accum_t,
-    kWarpSize> {
-  using Iterator =
-      typename cutlass::gemm::warp::MmaTensorOpAccumulatorTileIterator<
-          S1,
-          accum_t,
-          cutlass::layout::RowMajor,
-          S2,
-          S3>;
-  using Updater =
-      AttentionScalingCoefsUpdaterSm80<Iterator, accum_t, kWarpSize>;
-};
+}  // namespace threadblock
+}  // namespace gemm
+}  // namespace test
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/default_fmha_grouped.h` & `flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/default_fmha_grouped.h`

 * *Files 1% similar despite different names*

```diff
@@ -46,17 +46,16 @@
 
 #include "cutlass/complex.h"
 #include "cutlass/layout/matrix.h"
 #include "cutlass/numeric_types.h"
 
 #include "fmha_grouped.h"
 #include "gemm_kernel_utils.h"
-#include "find_default_mma.h"
-#include "attention_scaling_coefs_updater.h"
-#include "mma_from_smem.h"
+#include "gemm/find_default_mma.h"
+#include "gemm/mma_from_smem.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace kernel {
 
@@ -150,18 +149,18 @@
         Operator
         >::DefaultMma;
 
     using MmaCore = typename DefaultMma::MmaCore;
     using IteratorA = typename DefaultMma::IteratorA;
     using IteratorB = typename DefaultMma::IteratorB;
     using Mma = typename DefaultMma::ThreadblockMma;
-    using ScalingCoefsUpdater = typename DefaultAttentionScalingCoefsUpdater<
+    using AccumLambdaIterator = typename DefaultMmaAccumLambdaIterator<
         typename Mma::Operator::IteratorC,
         ElementAccumulator,
-        kWarpSize>::Updater;
+        kWarpSize>::Iterator;
 
     static_assert(MmaCore::WarpCount::kCount == kNumWarpsPerBlock, "");
 
     // Epilogue to store to shared-memory in a format that we can use later for
     // the second matmul
     using B2bGemm = typename cutlass::gemm::threadblock::B2bGemm<
         typename Mma::Operator::IteratorC,
@@ -236,15 +235,16 @@
         kStages,
         kSplitKSerial,
         Operator>;
 
     using DefaultMmaFromSmem =
         typename cutlass::gemm::threadblock::DefaultMmaFromSharedMemory<
             typename DefaultGemm::Mma,
-            typename MM0::AccumulatorSharedStorage>;
+            typename MM0::AccumulatorSharedStorage,
+            false>; // kScaleOperandA
 
     using Mma = typename DefaultMmaFromSmem::Mma;
     using IteratorB = typename Mma::IteratorB;
     using WarpCount = typename Mma::WarpCount;
     static_assert(WarpCount::kCount == kNumWarpsPerBlock, "");
 
     using DefaultEpilogue = typename DefaultGemm::Epilogue;
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_pipelined.h` & `flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_pipelined.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_rescale_output.h` & `flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_rescale_output.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_thread_apply_logsumexp.h` & `flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_thread_apply_logsumexp.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/find_default_mma.h` & `flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/gemm/find_default_mma.h`

 * *Files 2% similar despite different names*

```diff
@@ -38,14 +38,16 @@
    FastF32). kStages=3 uses too much shared memory and we want to use kStages=2,
    so we just copy-pasted some code from `default_mma.h` and
    `default_mma_core.h` files and wrapped this template to allow our usecase.
 
     This is really only for the FastF32 case - aka using TensorCores with fp32.
 */
 
+#pragma once
+
 #include "cutlass/gemm/threadblock/default_mma.h"
 #include "cutlass/gemm/threadblock/default_mma_core_simt.h"
 #include "cutlass/gemm/threadblock/default_mma_core_sm70.h"
 #include "cutlass/gemm/threadblock/default_mma_core_sm75.h"
 #include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
 
 namespace cutlass {
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h` & `flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h`

 * *Files 10% similar despite different names*

```diff
@@ -44,15 +44,26 @@
 
 #include "cutlass/layout/matrix.h"
 #include "cutlass/trace.h"
 #include "cutlass/gemm/kernel/gemm_transpose_operands.h"
 
 #include "fmha_grouped_problem_visitor.h"
 #include "gemm_kernel_utils.h"
-#include "epilogue_rescale_output.h"
+#include "gemm/mma_accum_lambda_iterator.h"
+#include "epilogue/epilogue_rescale_output.h"
+
+
+namespace {
+  static CUTLASS_DEVICE float atomicMaxFloat(float* addr, float value) {
+  // source: https://stackoverflow.com/a/51549250
+  return (value >= 0)
+      ? __int_as_float(atomicMax((int*)addr, __float_as_int(value)))
+      : __uint_as_float(atomicMin((unsigned int*)addr, __float_as_uint(value)));
+}
+}
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace kernel {
 
@@ -124,14 +135,17 @@
   static int const kAlignmentV = 1;
 
   using ThreadblockShape = typename MM0::ThreadblockShape;
 
   static int const kQueriesPerBlock = ThreadblockShape::kM;
   static int const kKeysPerBlock = ThreadblockShape::kN;
 
+  static constexpr bool kSupportsDropout = false;
+  static constexpr bool kSupportsBias = false;
+
   /// Warp count (concept: GemmShape)
   using WarpCount = typename MM1::WarpCount;
   static int const kThreadsPerWarp = 32;
   static int const kThreadCount = kThreadsPerWarp * WarpCount::kCount;
 
   using ProblemVisitor = FMHAGroupedProblemVisitor<
                             ThreadblockShape,
@@ -164,48 +178,52 @@
     ElementOAccum ** ptr_O_accum;
 
     typename LayoutQ::Stride::LongIndex *ldq;
     typename LayoutK::Stride::LongIndex *ldk;
     typename LayoutP::Stride::LongIndex *ldv;
     typename LayoutO::Stride::LongIndex *ldo;
 
+    // Scale
+    ElementAccumulator scale;
+
     // Whether causal masking is to be performed
     bool causal;
 
     // Only used by device-level operator
     GemmCoord *host_problem_sizes;
 
     //
     // Methods
     //
 
     /// Default ctor
     CUTLASS_HOST_DEVICE
-    Arguments(): 
+    Arguments():
       problem_count(0),
-      threadblock_count(0), 
+      threadblock_count(0),
       ptr_Q(nullptr),
       ptr_K(nullptr),
       ptr_P(nullptr),
       ptr_V(nullptr),
       ptr_O(nullptr),
       ptr_O_accum(nullptr),
       ldq(nullptr),
       ldk(nullptr),
       ldv(nullptr),
       ldo(nullptr),
+      scale(0),
       causal(false),
       host_problem_sizes(nullptr)
     {
 
     }
 
     /// Ctor
     CUTLASS_HOST_DEVICE
-    Arguments(    
+    Arguments(
       GemmCoord *problem_sizes0,
       GemmCoord *problem_sizes1,
       int problem_count,
       int threadblock_count,
       ElementQ ** ptr_Q,
       ElementK ** ptr_K,
       ElementP ** ptr_P,
@@ -214,16 +232,17 @@
       ElementOAccum ** ptr_O_accum,
       typename LayoutQ::Stride::LongIndex *ldq,
       typename LayoutK::Stride::LongIndex *ldk,
       typename LayoutP::Stride::LongIndex *ldp,
       typename LayoutV::Stride::LongIndex *ldv,
       typename LayoutO::Stride::LongIndex *ldo,
       bool causal,
+      ElementAccumulator scale,
       GemmCoord *host_problem_sizes=nullptr
-    ): 
+    ):
       problem_sizes0(problem_sizes0),
       problem_sizes1(problem_sizes1),
       problem_count(problem_count),
       threadblock_count(threadblock_count),
       ptr_Q(ptr_Q),
       ptr_K(ptr_K),
       ptr_P(ptr_P),
@@ -231,14 +250,15 @@
       ptr_O(ptr_O),
       ptr_O_accum(kNeedsOutputAccumulatorBuffer ? ptr_O_accum : (accum_t**)ptr_O),
       ldq(ldq),
       ldk(ldk),
       ldv(ldv),
       ldo(ldo),
       causal(causal),
+      scale(scale),
       host_problem_sizes(host_problem_sizes)
     {
 
     }
 
     bool __host__ check_supported() {
       CHECK_ALIGNED_PTR(ptr_Q, kAlignmentQ);
@@ -269,14 +289,15 @@
     ElementOAccum ** ptr_O_accum;
 
     typename LayoutQ::Stride::LongIndex *ldq;
     typename LayoutK::Stride::LongIndex *ldk;
     typename LayoutP::Stride::LongIndex *ldv;
     typename LayoutO::Stride::LongIndex *ldo;
 
+    ElementAccumulator scale;
     bool causal;
 
     //
     // Methods
     //
 
     CUTLASS_HOST_DEVICE
@@ -287,15 +308,16 @@
       ptr_V(nullptr),
       ptr_O(nullptr),
       ptr_O_accum(nullptr),
       ldq(nullptr),
       ldk(nullptr),
       ldv(nullptr),
       ldo(nullptr),
-      causal(false)
+      causal(false),
+      scale(0)
     { }
 
     CUTLASS_HOST_DEVICE
     Params(Arguments const &args,
           void *workspace = nullptr,
           int tile_count = 0):
       problem_visitor(args.problem_sizes0, args.problem_sizes1, args.problem_count, workspace, tile_count),
@@ -306,15 +328,16 @@
       ptr_V(args.ptr_V),
       ptr_O(args.ptr_O),
       ptr_O_accum(kNeedsOutputAccumulatorBuffer ? args.ptr_O_accum : (accum_t**)args.ptr_O),
       ldq(args.ldq),
       ldk(args.ldk),
       ldv(args.ldv),
       ldo(args.ldo),
-      causal(args.causal)
+      causal(args.causal),
+      scale(args.scale)
     { 
 
     }
 
     CUTLASS_HOST_DEVICE
     void update(
       Arguments const &args,
@@ -333,14 +356,15 @@
       ptr_O = args.ptr_O;
       ptr_O_accum = kNeedsOutputAccumulatorBuffer ? args.ptr_O_accum : (accum_t**)args.ptr_O;
       ldq = args.ldq;
       ldk = args.ldk;
       ldv = args.ldv;
       ldo = args.ldo;
       causal = args.causal;
+      scale = args.scale;
     }
   };
 
   // Shared storage - depends on kernel params
   struct ScalingCoefs {
     cutlass::Array<ElementAccumulator, kQueriesPerBlock> m_prime;
     cutlass::Array<ElementAccumulator, kQueriesPerBlock> s_prime;
@@ -460,15 +484,15 @@
   }
 
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
     auto& m_prime = shared_storage.m_prime;
     auto& s_prime = shared_storage.s_prime;
-    auto& si = shared_storage.after_mm0.si;
+    [[maybe_unused]] auto& si = shared_storage.after_mm0.si;
     auto& mi = shared_storage.mi;
 
     ProblemVisitor problem_visitor(
       params.problem_visitor,
       shared_storage.problem_visitor,
       blockIdx.x);
 
@@ -605,18 +629,18 @@
           iteratorC_tile_offset = {
               (warp_id() % MM0::Mma::WarpCount::kM),
               (warp_id() / MM0::Mma::WarpCount::kM)
             };
 
         // Mask out last if causal
         if (params.causal && num_keys - iter_key_start <= kKeysPerBlock) {
-          auto lane_offset = MM0::ScalingCoefsUpdater::get_lane_offset(
+          auto lane_offset = MM0::AccumLambdaIterator::get_lane_offset(
               lane_id(), warp_id(), iteratorC_tile_offset);
           int32_t last_col;
-          MM0::ScalingCoefsUpdater::iterateRows(
+          MM0::AccumLambdaIterator::iterateRows(
               lane_offset,
               [&](int accum_m) {
                 last_col = TileParams::query_start(threadblock_idx) + accum_m - iter_key_start;
               },
               [&](int accum_m, int accum_n, int idx) {
                 if (accum_n > last_col) {
                   accum[idx] =
@@ -627,33 +651,30 @@
         }
         DISPATCH_BOOL(iter_key_start == 0, kIsFirst, ([&] {
                 DISPATCH_BOOL(
                     num_keys - iter_key_start >= kKeysPerBlock,
                     kFullColumns,
                     ([&] {
                       // Update `mi` from accum stored in registers
-                      // Also updates `accum` with accum[i] <-
-                      // exp(accum[i] * scale
-                      // - mi)
-                      MM0::ScalingCoefsUpdater::update<
-                          kQueriesPerBlock,
+                      // Also does accum[i] <- exp(accum[i] - mi)
+                      iterative_softmax<
+                          typename MM0::Mma::Operator::IteratorC,
                           kFullColumns,
-                          kIsFirst,
-                          kKeepOutputInRF>(
+                          kIsFirst>(
                           accum_o,
                           accum,
                           mi,
                           m_prime,
                           s_prime,
                           lane_id(),
                           thread_id(),
                           warp_id(),
                           num_keys - iter_key_start,
                           iteratorC_tile_offset,
-                          1.0f / cutlass::fast_sqrt(float(problem_size0.k())));
+                          kSupportsBias ? 1.0f : params.scale);
                     }));
               }));
 
         // Output results to shared-memory
         int warp_idx_mn_0 = warp_id() %
             (MM0::Mma::Base::WarpCount::kM * MM0::Mma::Base::WarpCount::kN);
         auto output_tile_coords = cutlass::MatrixCoord{
@@ -824,14 +845,124 @@
         epilogue(rescale, dest_iter, accum_o);
       }
 
       // Next tile
       problem_visitor.advance(gridDim.x);
     }
   }
+
+  template <
+      typename WarpIteratorC,
+      bool kFullColumns,
+      bool kIsFirst>
+  CUTLASS_DEVICE static void iterative_softmax(
+      typename WarpIteratorC::Fragment& frag_o, // output so far
+      typename WarpIteratorC::Fragment& frag,
+      cutlass::Array<accum_t, kQueriesPerBlock>& mi,
+      cutlass::Array<accum_t, kQueriesPerBlock>& m_prime,
+      cutlass::Array<accum_t, kQueriesPerBlock>& s_prime,
+      int8_t lane_id,
+      int8_t thread_id,
+      int8_t warp_id,
+      int16_t max_col,
+      typename WarpIteratorC::TensorCoord const& tile_offset,
+      float scaling) {
+    /* Iterates on the accumulator and corresponding position on result matrix
+
+    (1) Update `mi[r]` to the max value of the row `r`
+    (2) In a second iteration do the following:
+        (a) accum   <- exp(accum - mi)
+        (b) m_prime <- exp(m_prime - mi)
+        (c) s_prime <- s_prime * m_prime + sum(accum)
+
+    All of this is done on registers, before we store all of this
+    on shared memory for the next matmul with Value.
+    */
+    using Fragment = typename WarpIteratorC::Fragment;
+    using LambdaIterator = typename DefaultMmaAccumLambdaIterator<
+        WarpIteratorC,
+        accum_t,
+        kThreadsPerWarp>::Iterator;
+    // Convert to `accum_t` (rather than double)
+    constexpr float kLog2e = 1.4426950408889634074; // log_2(e) = M_LOG2E
+    if (!kIsFirst) {
+      if (thread_id < kQueriesPerBlock) {
+        m_prime[thread_id] = mi[thread_id];
+      }
+      __syncthreads();
+    }
+
+    auto lane_offset =
+        LambdaIterator::get_lane_offset(lane_id, warp_id, tile_offset);
+
+    // First update `mi` to the max per-row
+    {
+      accum_t max;
+      LambdaIterator::iterateRows(
+          lane_offset,
+          [&](int accum_m) {
+            max = -cutlass::platform::numeric_limits<accum_t>::infinity();
+          },
+          [&](int accum_m, int accum_n, int idx) {
+            if (kFullColumns || accum_n < max_col) {
+              max = cutlass::fast_max(max, frag[idx]);
+            }
+          },
+          [&](int accum_m) {
+            // Having 4x atomicMax seems faster than reduce within warp
+            // first...
+            atomicMaxFloat(&mi[accum_m], max * scaling);
+          });
+    }
+    frag = cutlass::multiplies<Fragment>()(scaling * kLog2e, frag);
+
+    // Make sure we all share the update values for `mi`
+    __syncthreads();
+
+    if (thread_id < kQueriesPerBlock) {
+      auto m_prime_exp = exp2f(kLog2e * (m_prime[thread_id] - mi[thread_id]));
+      m_prime[thread_id] = m_prime_exp;
+      s_prime[thread_id] *= m_prime_exp;
+    }
+    __syncthreads(); // Update output fragments
+    if (kKeepOutputInRF && !kIsFirst) {
+      accum_t mp;
+      LambdaIterator::iterateRows(
+          lane_offset,
+          [&](int accum_m) { mp = m_prime[accum_m]; },
+          [&](int accum_m, int accum_n, int idx) { frag_o[idx] *= mp; },
+          [&](int accum_m) {});
+      __syncthreads();
+    }
+    // Update accum_m, accum_n, ...
+    {
+      accum_t mi_row, total_row;
+      LambdaIterator::iterateRows(
+          lane_offset,
+          [&](int accum_m) { mi_row = kLog2e * mi[accum_m]; },
+          [&](int accum_m, int accum_n, int idx) {
+            frag[idx] = (kFullColumns || accum_n < max_col)
+                ? exp2f(frag[idx] - mi_row)
+                : accum_t(0.0);
+          },
+          [&](int accum_m) {});
+      LambdaIterator::iterateRows(
+          lane_offset,
+          [&](int accum_m) { total_row = 0.0; },
+          [&](int accum_m, int accum_n, int idx) { total_row += frag[idx]; },
+          [&](int accum_m) {
+            if (LambdaIterator::reduceSameRow(
+                    lane_id, total_row, [](accum_t a, accum_t b) {
+                      return a + b;
+                    })) {
+              atomicAdd(&s_prime[accum_m], total_row);
+            }
+          });
+    }
+  }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace kernel
 } // namespace gemm
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped_problem_visitor.h` & `flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/fmha_grouped_problem_visitor.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu`

 * *Files 4% similar despite different names*

```diff
@@ -500,45 +500,59 @@
     ldq_host.resize(problem_count());
     ldk_host.resize(problem_count());
     ldp_host.resize(problem_count());
     ldv_host.resize(problem_count());
     ldo_host.resize(problem_count());
     seqlen_host.resize(problem_count());
 
-    for (int32_t i = 0; i < problem_count(); ++i) {
-
-      auto problem0 = options.problem_sizes0.at(i);
-      auto problem1 = options.problem_sizes1.at(i);
-
-      ldq_host.at(i) = LayoutQ::packed({problem0.m(), problem0.k()}).stride(0);
-      ldk_host.at(i) = LayoutK::packed({problem0.k(), problem0.n()}).stride(0);
-      ldp_host.at(i) = LayoutP::packed({problem0.m(), problem0.n()}).stride(0);
-      ldv_host.at(i) = LayoutV::packed({problem1.k(), problem1.n()}).stride(0);
-      ldo_host.at(i) = LayoutO::packed({problem1.m(), problem1.n()}).stride(0);
-
-      // m = n for attention problems.
-      seqlen_host.at(i) = problem0.m();
-
-      offset_Q.push_back(total_elements_Q);
-      offset_K.push_back(total_elements_K);
-      offset_P.push_back(total_elements_P);
-      offset_V.push_back(total_elements_V);
-      offset_O.push_back(total_elements_O);
-
-      int64_t elements_Q = problem0.m() * problem0.k();
-      int64_t elements_K = problem0.k() * problem0.n();
-      int64_t elements_P = problem0.m() * problem0.n();
-      int64_t elements_V = problem1.k() * problem1.n();
-      int64_t elements_O = problem1.m() * problem1.n();
-
-      total_elements_Q += elements_Q;
-      total_elements_K += elements_K;
-      total_elements_P += elements_P;
-      total_elements_V += elements_V;
-      total_elements_O += elements_O;
+    // Create tensors in BMHK format, where
+    // B = batch_size
+    // M = sequence length
+    // H = num_heads
+    // K = embedding size per head
+    int64_t batch_offset_Q, batch_offset_K, batch_offset_V, batch_offset_O;
+
+    for (int32_t b = 0; b < options.batch_size; ++b) {
+      batch_offset_Q = total_elements_Q;
+      batch_offset_K = total_elements_K;
+      batch_offset_V = total_elements_V;
+      batch_offset_O = total_elements_O;
+      for (int32_t h = 0; h < options.head_number; ++h) {
+        int32_t i = h + b * options.head_number;
+
+        auto problem0 = options.problem_sizes0.at(i);
+        auto problem1 = options.problem_sizes1.at(i);
+
+        ldq_host.at(i) = LayoutQ::packed({problem0.m(), options.head_number * problem0.k()}).stride(0);
+        ldk_host.at(i) = LayoutK::packed({options.head_number * problem0.k(), problem0.n()}).stride(0);
+        ldp_host.at(i) = LayoutP::packed({problem0.m(), problem0.n()}).stride(0);
+        ldv_host.at(i) = LayoutV::packed({problem1.k(), options.head_number * problem1.n()}).stride(0);
+        ldo_host.at(i) = LayoutO::packed({problem1.m(), options.head_number * problem1.n()}).stride(0);
+
+        // m = n for attention problems.
+        seqlen_host.at(i) = problem0.m();
+
+        offset_Q.push_back(batch_offset_Q + h * problem0.k());
+        offset_K.push_back(batch_offset_K + h * problem0.k());
+        offset_P.push_back(total_elements_P);
+        offset_V.push_back(batch_offset_V + h * problem0.k());
+        offset_O.push_back(batch_offset_O + h * problem1.n());
+
+        int64_t elements_Q = problem0.m() * problem0.k();
+        int64_t elements_K = problem0.k() * problem0.n();
+        int64_t elements_P = problem0.m() * problem0.n();
+        int64_t elements_V = problem1.k() * problem1.n();
+        int64_t elements_O = problem1.m() * problem1.n();
+
+        total_elements_Q += elements_Q;
+        total_elements_K += elements_K;
+        total_elements_P += elements_P;
+        total_elements_V += elements_V;
+        total_elements_O += elements_O;
+      }
     }
 
     problem_sizes_device0.reset(problem_count());
     problem_sizes_device1.reset(problem_count());
     problem_sizes_device0.copy_from_host(options.problem_sizes0.data());
     problem_sizes_device1.copy_from_host(options.problem_sizes1.data());
 
@@ -645,138 +659,141 @@
   }
 
   /// Verifies the result is a GEMM
   bool verify_() {
 
     bool passed = true;
 
-    for (int32_t i = 0; i < problem_count(); ++i) {
-      cutlass::gemm::GemmCoord problem0 = options.problem_sizes0.at(i);
-      cutlass::gemm::GemmCoord problem1 = options.problem_sizes1.at(i);
-
-      LayoutQ layout_Q(ldq_host.at(i));
-      LayoutK layout_K(ldk_host.at(i));
-      LayoutP layout_P(ldp_host.at(i));
-      LayoutV layout_V(ldv_host.at(i));
-      LayoutO layout_O(ldo_host.at(i));
+    for (int32_t b = 0; b < options.batch_size; ++b) {
+      int32_t i = b * options.head_number;
+      // Problem size is the same for all heads
+      cutlass::gemm::GemmCoord problem0 = options.problem_sizes0.at(b * options.head_number);
+      cutlass::gemm::GemmCoord problem1 = options.problem_sizes1.at(b * options.head_number);
 
       MatrixCoord extent_Q{problem0.m(), problem0.k()};
       MatrixCoord extent_K{problem0.k(), problem0.n()};
       MatrixCoord extent_P{problem0.m(), problem0.n()};
       MatrixCoord extent_V{problem1.k(), problem1.n()};
       MatrixCoord extent_O{problem1.m(), problem1.n()};
 
-      cutlass::TensorView<ElementQ, LayoutQ> view_Q(block_Q.get() + offset_Q.at(i), layout_Q, extent_Q);
-      cutlass::TensorView<ElementK, LayoutK> view_K(block_K.get() + offset_K.at(i), layout_K, extent_K);
-      cutlass::TensorView<ElementP, LayoutP> view_P(block_P.get() + offset_P.at(i), layout_P, extent_P);
-      cutlass::TensorView<ElementV, LayoutV> view_V(block_V.get() + offset_V.at(i), layout_V, extent_V);
+      LayoutO layout_O(ldo_host.at(i));
+      std::vector<ElementO> matrix_O(layout_O.capacity(extent_O));
+      cutlass::device_memory::copy_to_host(matrix_O.data(),   block_O.get() + offset_O.at(i), matrix_O.size());
+      cutlass::DeviceAllocation<ElementO>    block_Ref_O(layout_O.capacity(extent_O));
 
-      cutlass::DeviceAllocation<ElementP>    block_Ref(layout_P.capacity(extent_P));
-      cutlass::TensorView<ElementP, LayoutP> view_Ref_device(block_Ref.get(), layout_P, extent_P);
+      for (int32_t h = 0; h < options.head_number; ++h) {
+        i = h + b * options.head_number;
 
-      cutlass::DeviceAllocation<ElementO>    block_Ref_O(layout_O.capacity(extent_O));
-      cutlass::TensorView<ElementO, LayoutO> view_Ref_O_device(block_Ref_O.get(), layout_O, extent_O);
+        LayoutQ layout_Q(ldq_host.at(i));
+        LayoutK layout_K(ldk_host.at(i));
+        LayoutP layout_P(ldp_host.at(i));
+        LayoutV layout_V(ldv_host.at(i));
+
+        cutlass::TensorView<ElementQ, LayoutQ> view_Q(block_Q.get() + offset_Q.at(i), layout_Q, extent_Q);
+        cutlass::TensorView<ElementK, LayoutK> view_K(block_K.get() + offset_K.at(i), layout_K, extent_K);
+        cutlass::TensorView<ElementV, LayoutV> view_V(block_V.get() + offset_V.at(i), layout_V, extent_V);
+        cutlass::TensorView<ElementO, LayoutO> view_Ref_O_device(block_Ref_O.get() + offset_O.at(i) - offset_O.at(b * options.head_number), layout_O, extent_O);
+
+        cutlass::DeviceAllocation<ElementP>    block_Ref_P(layout_P.capacity(extent_P));
+        cutlass::TensorView<ElementP, LayoutP> view_Ref_P_device(block_Ref_P.get(), layout_P, extent_P);
+
+        // Reference GEMM
+        cutlass::reference::device::GemmComplex<
+            ElementQ, LayoutQ,
+            ElementK, LayoutK,
+            ElementP, LayoutP, 
+            ElementCompute, ElementAccumulator
+        >(
+          problem0,
+          ElementAccumulator(options.alpha0), 
+          view_Q,
+          Attention::MM0::Mma::kTransformA,
+          view_K,
+          Attention::MM0::Mma::kTransformB,
+          ElementAccumulator(options.beta), 
+          view_Ref_P_device, 
+          view_Ref_P_device, 
+          ElementAccumulator(0)
+        );
+
+        // Compute softmax for P. We need to explicitly compute softmax
+        // over P because softmax is fused to the second GEMM in the
+        // profiled implementation.
+        std::vector<ElementP> matrix_Ref(layout_P.capacity(extent_P));
+        cutlass::device_memory::copy_to_host(matrix_Ref.data(), block_Ref_P.get(), matrix_Ref.size());
+        cutlass::TensorView<ElementP, LayoutP> view_Ref_host(matrix_Ref.data(), layout_P, extent_P);
+        std::vector<ElementNorm> vector_Norm_Ref(problem0.m());
+        std::vector<ElementSum> vector_Sum_Ref(problem0.m());
 
-      // Reference GEMM
-      cutlass::reference::device::GemmComplex<
-          ElementQ, LayoutQ,
-          ElementK, LayoutK,
-          ElementP, LayoutP, 
-          ElementCompute, ElementAccumulator
-      >(
-        problem0,
-        ElementAccumulator(options.alpha0), 
-        view_Q,
-        Attention::MM0::Mma::kTransformA,
-        view_K,
-        Attention::MM0::Mma::kTransformB,
-        ElementAccumulator(options.beta), 
-        view_P, 
-        view_Ref_device, 
-        ElementAccumulator(0)
-      );
-
-      // Compute softmax for P. We need to explicitly compute softmax
-      // over P because softmax is fused to the second GEMM in the
-      // profiled implementation.
-      std::vector<ElementP> matrix_Ref(layout_P.capacity(extent_P));
-      cutlass::device_memory::copy_to_host(matrix_Ref.data(), block_Ref.get(), matrix_Ref.size());
-      cutlass::TensorView<ElementP, LayoutP> view_Ref_host(matrix_Ref.data(), layout_P, extent_P);
-      std::vector<ElementNorm> vector_Norm_Ref(problem0.m());
-      std::vector<ElementSum> vector_Sum_Ref(problem0.m());
-
-      int n_dim = options.use_mask ? options.problem_sizes0_real.at(i).n() : problem0.n();
-
-      // Compute softmax for referece matrix
-      for (int m = 0; m < problem0.m(); m++) {
-        int n_dim_row = n_dim;
-        if (options.causal) {
-          n_dim_row = std::min(m + 1, n_dim);
-        }
-        ElementSoftmaxCompute max = ElementSoftmaxCompute(view_Ref_host.ref().at({m, 0}));
-        for (int n = 1; n < n_dim_row; n++) {
-           max = std::max(max, ElementSoftmaxCompute(view_Ref_host.ref().at({m, n})));
-        }
+        int n_dim = options.use_mask ? options.problem_sizes0_real.at(i).n() : problem0.n();
+
+        // Compute softmax for reference matrix
+        for (int m = 0; m < problem0.m(); m++) {
+          int n_dim_row = n_dim;
+          if (options.causal) {
+            n_dim_row = std::min(m + 1, n_dim);
+          }
+          ElementSoftmaxCompute max = ElementSoftmaxCompute(view_Ref_host.ref().at({m, 0}));
+          for (int n = 1; n < n_dim_row; n++) {
+            max = std::max(max, ElementSoftmaxCompute(view_Ref_host.ref().at({m, n})));
+          }
 
-        vector_Norm_Ref.at(m) = ElementNorm(max);
+          vector_Norm_Ref.at(m) = ElementNorm(max);
 
-        ElementSoftmaxCompute sum = ElementSoftmaxCompute();
-        for (int n = 0; n < n_dim_row; n++) {
-          sum += std::exp( ElementSoftmaxCompute(view_Ref_host.ref().at({m, n})) - max );
-        }
-        ElementSoftmaxCompute inv_sum = ElementSoftmaxCompute(1.0f / sum);
+          ElementSoftmaxCompute sum = ElementSoftmaxCompute();
+          for (int n = 0; n < n_dim_row; n++) {
+            sum += std::exp( ElementSoftmaxCompute(view_Ref_host.ref().at({m, n})) - max );
+          }
+          ElementSoftmaxCompute inv_sum = ElementSoftmaxCompute(1.0f / sum);
 
-        vector_Sum_Ref.at(m) = ElementSum(inv_sum);
+          vector_Sum_Ref.at(m) = ElementSum(inv_sum);
 
-        for (int n = 0; n < n_dim_row; n++) {
-          view_Ref_host.ref().at({m, n}) = ElementP(
-            std::exp( ElementSoftmaxCompute(view_Ref_host.ref().at({m, n})) - max ) * inv_sum
-          );
-        }
-        // Mask out the rest of the attention matrix
-        for (int n = n_dim_row; n < n_dim; ++n) {
-          view_Ref_host.ref().at({m, n}) = ElementP(0);
+          for (int n = 0; n < n_dim_row; n++) {
+            view_Ref_host.ref().at({m, n}) = ElementP(
+              std::exp( ElementSoftmaxCompute(view_Ref_host.ref().at({m, n})) - max ) * inv_sum
+            );
+          }
+          // Mask out the rest of the attention matrix
+          for (int n = n_dim_row; n < n_dim; ++n) {
+            view_Ref_host.ref().at({m, n}) = ElementP(0);
+          }
         }
-      }
 
-      // when not using mask, problem_real and problem share the same sizes
-      if (options.use_mask) {
-        for (int m = 0; m < problem0.m(); m++) {
-          for (int n = n_dim; n < problem0.n(); n++) {
-            view_Ref_host.ref().at({m, n}) = ElementP(0);
+        // when not using mask, problem_real and problem share the same sizes
+        if (options.use_mask) {
+          for (int m = 0; m < problem0.m(); m++) {
+            for (int n = n_dim; n < problem0.n(); n++) {
+              view_Ref_host.ref().at({m, n}) = ElementP(0);
+            }
           }
         }
-      }
 
-      cutlass::device_memory::copy_to_device(block_P.get() + offset_P.at(i), matrix_Ref.data(), matrix_Ref.size());
+        cutlass::device_memory::copy_to_device(block_Ref_P.get(), matrix_Ref.data(), matrix_Ref.size());
 
-      // Reference GEMM
-      cutlass::reference::device::GemmComplex<
-          ElementP, LayoutP,
-          ElementV, LayoutV,
-          ElementO, LayoutO, 
-          ElementCompute, ElementAccumulator
-      >(
-        problem1,
-        ElementAccumulator(options.alpha1), 
-        view_P,
-        Attention::MM0::Mma::kTransformA,
-        view_V,
-        Attention::MM0::Mma::kTransformB,
-        ElementAccumulator(options.beta), 
-        view_Ref_O_device, 
-        view_Ref_O_device, 
-        ElementAccumulator(0)
-      );
+        // Reference GEMM
+        cutlass::reference::device::GemmComplex<
+            ElementP, LayoutP,
+            ElementV, LayoutV,
+            ElementO, LayoutO, 
+            ElementCompute, ElementAccumulator
+        >(
+          problem1,
+          ElementAccumulator(options.alpha1), 
+          view_Ref_P_device,
+          Attention::MM0::Mma::kTransformA,
+          view_V,
+          Attention::MM0::Mma::kTransformB,
+          ElementAccumulator(options.beta), 
+          view_Ref_O_device, 
+          view_Ref_O_device, 
+          ElementAccumulator(0)
+        );
+      }
 
       // Copy to host memory
-      cutlass::TensorView<ElementP, LayoutP> view_Ref(matrix_Ref.data(), layout_P, extent_P);
-
-      std::vector<ElementO> matrix_O(layout_O.capacity(extent_O));
-      cutlass::device_memory::copy_to_host(matrix_O.data(),   block_O.get() + offset_O.at(i), matrix_O.size());
       std::vector<ElementO> matrix_Ref_O(layout_O.capacity(extent_O));
       cutlass::device_memory::copy_to_host(matrix_Ref_O.data(), block_Ref_O.get(), matrix_Ref_O.size());
 
       // printf("Pb %d: \n    Q=(offset=%d, ldq=%d)\n    K=(offset=%d, ldk=%d)\n    O=(offset=%d, ldo=%d)\n",
       //   int(i), int(offset_Q[i]), int(ldq_host[i]), int(offset_K[i]), int(ldk_host[i]), int(offset_O[i]), int(ldo_host[i]));
   
       bool verified_O = false;
@@ -784,15 +801,15 @@
       if (!verified_O) {
         verified_O = verify_tensor_<ElementO>(matrix_O, matrix_Ref_O);
       }
 
       passed = passed && verified_O;
 
       if (!passed) {
-        std::cerr << "\n***\nError - problem " << i << " failed the QA check\n***\n" << std::endl;
+        std::cerr << "\n***\nError - problem " << i << " (batch " << b << ") failed the QA check\n***\n" << std::endl;
 
         if (!verified_O) {
           std::cout << "Final matrix output is incorrect" << std::endl;
         }
 
         return passed;
       }
@@ -827,34 +844,37 @@
 
       // TODO: support arbitrary seq lengths
       // if (cu_seqlens_q.has_value()) {
       //   p.cu_seqlens_q_ptr = (int32_t*)cu_seqlens_q->data_ptr();
       //   p.cu_seqlens_k_ptr = (int32_t*)cu_seqlens_k->data_ptr();
       // }
 
+      p.scale = options.alpha0;
+
       p.num_heads = options.head_number;
       p.num_batches = options.batch_size;
       p.head_dim = options.head_size;
       p.head_dim_value = options.head_size_v;
       p.num_queries = options.seq_length;
       p.num_keys = options.seq_length_kv;
-      p.causal = options.causal;
+      if (options.causal) {
+        p.custom_mask_type = Attention::CausalFromTopLeft;
+      }
 
-      // TODO: This might overflow for big tensors
+      // All tensors are in BMHK shapes
+      p.q_strideH = options.head_size;
+      p.k_strideH = options.head_size;
+      p.v_strideH = options.head_size_v;
       p.q_strideM = int32_t(ldq_host[0]);
       p.k_strideM = int32_t(ldk_host[0]);
       p.v_strideM = int32_t(ldv_host[0]);
-      p.q_strideH = p.q_strideM * options.seq_length;
-      p.k_strideH = p.k_strideM * options.seq_length_kv;
-      p.v_strideH = p.v_strideM * options.seq_length_kv;
-      p.o_strideH = options.head_size_v * options.seq_length;
-      p.q_strideB = p.q_strideH * options.head_number;
-      p.k_strideB = p.k_strideH * options.head_number;
-      p.v_strideB = p.v_strideH * options.head_number;
-      p.o_strideB = options.head_size_v * options.seq_length * options.head_number;
+      p.q_strideB = p.q_strideM * options.seq_length;
+      p.k_strideB = p.k_strideM * options.seq_length_kv;
+      p.v_strideB = p.v_strideM * options.seq_length_kv;
+      p.o_strideM = p.head_dim_value * p.num_heads;
     }
 
     // launch kernel :)
     constexpr auto kernel_fn = attention_kernel_batched_impl<Attention>;
     int smem_bytes = sizeof(typename Attention::SharedStorage);
     if (smem_bytes > 0xc000) {
       cudaFuncSetAttribute(kernel_fn, cudaFuncAttributeMaxDynamicSharedMemorySize, smem_bytes);
@@ -984,15 +1004,17 @@
 int run_attention(Options& options) {
   using Attention = AttentionKernel<
     cutlass::half_t,      // scalar_t
     cutlass::arch::Sm80,  // ArchTag
     true,                 // Memory is aligned
     kQueriesPerBlock,
     kKeysPerBlock,
-    kSingleValueIteration
+    kSingleValueIteration,
+    false,                // Supports dropout
+    false                 // Supports bias
   >;
 
   //
   // Test and profile
   //
 
   TestbedAttention<Attention> testbed(options);
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu`

 * *Files 0% similar despite different names*

```diff
@@ -917,14 +917,15 @@
       ptr_O_accumulate.get(),
       ldq.get(),
       ldk.get(),
       ldp.get(),
       ldv.get(),
       ldo.get(),
       options.causal,
+      options.alpha0,
       options.problem_sizes1.data()
     );
 
     Attention fmha;
 
     size_t workspace_size = fmha.get_workspace_size(args);
     cutlass::DeviceAllocation<uint8_t> workspace(workspace_size);
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma.h` & `flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_base.h` & `flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_base.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_multistage.h` & `flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_pipelined.h` & `flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_pipelined.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm_kernel_utils.h` & `flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/gemm_kernel_utils.h`

 * *Files 17% similar despite different names*

```diff
@@ -32,28 +32,28 @@
 #pragma once
 
 #include "cutlass/arch/mma.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 // Some helper functions
 ////////////////////////////////////////////////////////////////////////////////
-#define DISPATCH_TYPES(tensor, func)                                        \
-  {                                                                         \
-    if (query.scalar_type() == at::ScalarType::Float) {                     \
-      using scalar_t = float;                                               \
-      func();                                                               \
-    } else if (query.scalar_type() == at::ScalarType::Half) {               \
-      using scalar_t = cutlass::half_t;                                     \
-      func();                                                               \
-    } else if (query.scalar_type() == at::ScalarType::BFloat16) {           \
-      using scalar_t = cutlass::bfloat16_t;                                 \
-      func();                                                               \
-    } else {                                                                \
-      TORCH_CHECK(false, "Only fp32, half & bf16 supported at the moment"); \
-    }                                                                       \
+#define DISPATCH_TYPES(tensor, func)                                           \
+  {                                                                            \
+    if (query.scalar_type() == at::ScalarType::Float) {                        \
+      using scalar_t = float;                                                  \
+      func();                                                                  \
+    } else if (query.scalar_type() == at::ScalarType::Half) {                  \
+      using scalar_t = cutlass::half_t;                                        \
+      func();                                                                  \
+    } else if (query.scalar_type() == at::ScalarType::BFloat16) {              \
+      using scalar_t = cutlass::bfloat16_t;                                    \
+      func();                                                                  \
+    } else {                                                                   \
+      XFORMERS_CHECK(false, "Only fp32, half & bf16 supported at the moment"); \
+    }                                                                          \
   }
 
 #define DISPATCH_BOOL(BOOL_V, BOOL_NAME, F) \
   {                                         \
     if (BOOL_V) {                           \
       constexpr bool BOOL_NAME = true;      \
       F();                                  \
@@ -73,125 +73,78 @@
     } else if (CC >= 70) {                                                \
       using ArchTag = cutlass::arch::Sm70;                                \
       func();                                                             \
     } else if (CC >= 50) {                                                \
       using ArchTag = cutlass::arch::Sm50;                                \
       func();                                                             \
     } else {                                                              \
-      TORCH_CHECK(                                                        \
+      XFORMERS_CHECK(                                                     \
           false,                                                          \
           "Your device is too old. We require compute capability >= 50"); \
     }                                                                     \
   }
 
-#define CHECK_NOSPARSE_CONTIGUOUS_CUDA(TENSOR)                         \
-  TORCH_CHECK(TENSOR.is_cuda(), #TENSOR " must be a CUDA tensor");     \
-  TORCH_CHECK(!TENSOR.is_sparse(), #TENSOR " must be a dense tensor"); \
-  TORCH_CHECK(TENSOR.is_contiguous());
-
-#define CHECK_NOSPARSE_LASTCONTIGUOUS_CUDA(TENSOR)                     \
-  TORCH_CHECK(TENSOR.is_cuda(), #TENSOR " must be a CUDA tensor");     \
-  TORCH_CHECK(!TENSOR.is_sparse(), #TENSOR " must be a dense tensor"); \
-  TORCH_CHECK(                                                         \
+#define CHECK_NOSPARSE_CONTIGUOUS_CUDA(TENSOR)                            \
+  XFORMERS_CHECK(TENSOR.is_cuda(), #TENSOR " must be a CUDA tensor");     \
+  XFORMERS_CHECK(!TENSOR.is_sparse(), #TENSOR " must be a dense tensor"); \
+  XFORMERS_CHECK(TENSOR.is_contiguous());
+
+#define CHECK_NOSPARSE_LASTCONTIGUOUS_CUDA(TENSOR)                        \
+  XFORMERS_CHECK(TENSOR.is_cuda(), #TENSOR " must be a CUDA tensor");     \
+  XFORMERS_CHECK(!TENSOR.is_sparse(), #TENSOR " must be a dense tensor"); \
+  XFORMERS_CHECK(                                                         \
       TENSOR.stride(-1) == 1, #TENSOR ": last dimension must be contiguous");
 
-#ifdef HAS_PYTORCH
+#ifdef TORCH_CHECK
 #define CHECK_ALIGNED_PTR(PTR, ALIGNMENT) \
-  TORCH_CHECK(uint64_t(PTR) % ALIGNMENT == 0, #PTR " is not correctly aligned")
+  XFORMERS_CHECK(                         \
+      uint64_t(PTR) % ALIGNMENT == 0, #PTR " is not correctly aligned")
 #define XFORMERS_CHECK TORCH_CHECK
 #elif defined(__CUDACC_RTC__)
 #define CHECK_ALIGNED_PTR(PTR, ALIGNMENT)  \
   if (!(uint64_t(PTR) % ALIGNMENT == 0)) { \
     return false;                          \
   }
 #define XFORMERS_CHECK(COND, ERR) \
   if (!(COND)) {                  \
     return false;                 \
   }
 #else
+#include <iostream>
 #define CHECK_ALIGNED_PTR(PTR, ALIGNMENT)            \
   if (!(uint64_t(PTR) % ALIGNMENT == 0)) {           \
     std::cerr << #PTR " is not correctly aligned\n"; \
     return false;                                    \
   }
 #define XFORMERS_CHECK(COND, ERR)   \
   if (!(COND)) {                    \
     std::cerr << #COND " failed\n"; \
     return false;                   \
   }
 #endif
 
-#define ASSIGN_CHECK_OVERFLOW(A, B)                                \
-  {                                                                \
-    A = B;                                                         \
-    TORCH_CHECK(                                                   \
-        B < cutlass::platform::numeric_limits<decltype(A)>::max(), \
-        #B " overflows");                                          \
+#define ASSIGN_CHECK_OVERFLOW(A, B)                                    \
+  {                                                                    \
+    A = B;                                                             \
+    XFORMERS_CHECK(                                                    \
+        B < std::numeric_limits<decltype(A)>::max(), #B " overflows"); \
   }
 
 namespace gemm_kernel_utils {
 
-#ifdef HAS_PYTORCH
-template <typename scalar_t>
-struct TypeTraits;
-
-template <>
-struct TypeTraits<cutlass::half_t> {
-  using scalar_t = cutlass::half_t;
-
-  static constexpr __host__ at::ScalarType atScalarType() {
-    return at::ScalarType::Half;
-  }
-  template <int nDim>
-  static __host__ at::PackedTensorAccessor32<scalar_t, nDim> packed_accessor(
-      at::Tensor const& tensor) {
-    return at::PackedTensorAccessor32<scalar_t, nDim>(
-        (scalar_t*)(tensor.data_ptr()),
-        tensor.sizes().data(),
-        tensor.strides().data());
-  }
-};
-
-template <>
-struct TypeTraits<cutlass::bfloat16_t> {
-  using scalar_t = cutlass::bfloat16_t;
-
-  static constexpr __host__ at::ScalarType atScalarType() {
-    return at::ScalarType::BFloat16;
-  }
-  template <int nDim>
-  static __host__ at::PackedTensorAccessor32<scalar_t, nDim> packed_accessor(
-      at::Tensor const& tensor) {
-    return at::PackedTensorAccessor32<scalar_t, nDim>(
-        (scalar_t*)(tensor.data_ptr()),
-        tensor.sizes().data(),
-        tensor.strides().data());
-  }
-};
-
-template <>
-struct TypeTraits<float> {
-  using scalar_t = float;
-
-  static constexpr __host__ at::ScalarType atScalarType() {
-    return at::ScalarType::Float;
-  }
-  template <int nDim>
-  static __host__ at::PackedTensorAccessor32<scalar_t, nDim> packed_accessor(
-      at::Tensor const& tensor) {
-    return tensor.packed_accessor32<scalar_t, nDim>();
-  }
-};
-#endif
-
 template <typename integer>
 constexpr CUTLASS_HOST_DEVICE integer ceil_div(integer n, integer m) {
   return (n + m - 1) / m;
 }
 
+template <typename integer>
+constexpr CUTLASS_HOST_DEVICE integer align_up(integer n, integer m) {
+  return ((n + m - 1) / m) * m;
+}
+
 ////////////////////////////////////////////////////////////////////////////////
 // Determine the type of GEMM we do (TensorCores or not, Shapes ...)
 // TODO: Maybe we could rely on Cutlass's DefaultGemm templates
 ////////////////////////////////////////////////////////////////////////////////
 
 // Fallback to Simt (FMA on cuda cores) if not in a special case below
 template <typename ArchTag, typename scalar_t_, typename Enable = void>
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/epilogue_predicated_tile_iterator.h` & `flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/iterators/epilogue_predicated_tile_iterator.h`

 * *Files 0% similar despite different names*

```diff
@@ -307,17 +307,17 @@
 
           CUTLASS_PRAGMA_UNROLL
           for (int column = 0; column < ThreadMap::Iterations::kColumn;
                ++column) {
             // on windows using unsigned long here gives the error
             // error: asm operand type size(4) does not match
             // type/size implied by constraint 'l'
-            uint64_t addr = (uint64_t)(
-                (void*)&memory_pointer
-                    [column * ThreadMap::Delta::kColumn / kElementsPerAccess]);
+            uint64_t addr = (uint64_t)((void*)&memory_pointer
+                                           [column * ThreadMap::Delta::kColumn /
+                                            kElementsPerAccess]);
             asm volatile("prefetch.global.L1 [ %1 ];" : "=l"(addr) : "l"(addr));
           }
 
           if (row + 1 < ThreadMap::Iterations::kRow) {
             if (!ScatterD) {
               byte_pointer += params_.increment_row;
             }
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/make_residual_last.h` & `flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/iterators/make_residual_last.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_access_iterator_residual_last.h` & `flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_access_iterator_residual_last.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_iterator_residual_last.h` & `flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_iterator_residual_last.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h` & `flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h`

 * *Files 17% similar despite different names*

```diff
@@ -28,30 +28,31 @@
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 #pragma once
 
 #ifdef HAS_PYTORCH
-#include <ATen/ATen.h>
-#include <ATen/cuda/CUDAContext.h>
-#include <c10/cuda/CUDAGuard.h>
-#include <torch/library.h>
+#include <ATen/cuda/CUDAGeneratorImpl.h>
+#include <ATen/cuda/CUDAGraphsUtils.cuh>
 #endif
 
+#include <curand_kernel.h>
 #include <cmath>
 #include <vector>
 
 #include "cutlass/bfloat16.h"
+#include "cutlass/fast_math.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/layout/matrix.h"
 #include "cutlass/layout/vector.h"
+#include "cutlass/matrix.h"
 #include "cutlass/numeric_types.h"
+#include "cutlass/tensor_ref.h"
 
-#include "attention_scaling_coefs_updater.h"
 #include "cutlass/epilogue/threadblock/default_epilogue_simt.h"
 #include "cutlass/epilogue/threadblock/default_epilogue_tensor_op.h"
 #include "cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h"
 #include "cutlass/gemm/device/default_gemm_configuration.h"
 #include "cutlass/gemm/kernel/default_gemm.h"
 #include "cutlass/gemm/threadblock/default_mma.h"
 #include "cutlass/gemm/threadblock/default_mma_core_simt.h"
@@ -59,56 +60,77 @@
 #include "cutlass/gemm/threadblock/default_mma_core_sm75.h"
 #include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
 #include "cutlass/gemm/threadblock/threadblock_swizzle.h"
 #include "cutlass/matrix_shape.h"
 #include "cutlass/platform/platform.h"
 #include "cutlass/transform/threadblock/predicated_tile_iterator.h"
 #include "debug_utils.h"
-#include "epilogue_pipelined.h"
-#include "epilogue_rescale_output.h"
-#include "find_default_mma.h"
+#include "epilogue/epilogue_pipelined.h"
+#include "epilogue/epilogue_rescale_output.h"
+#include "gemm/find_default_mma.h"
+#include "gemm/mma_from_smem.h"
 #include "gemm_kernel_utils.h"
-#include "mma_from_smem.h"
+#include "transform/tile_smem_loader.h"
 
 #include <inttypes.h>
 
 using namespace gemm_kernel_utils;
 
 namespace {
 template <typename scalar_t, typename Arch>
 constexpr int getWarpsPerSm() {
   return (
       Arch::kMinComputeCapability >= 80 &&
               !cutlass::platform::is_same<scalar_t, float>::value
           ? 16
           : 12);
 }
+static CUTLASS_DEVICE float atomicMaxFloat(float* addr, float value) {
+  // source: https://stackoverflow.com/a/51549250
+  return (value >= 0)
+      ? __int_as_float(atomicMax((int*)addr, __float_as_int(value)))
+      : __uint_as_float(atomicMin((unsigned int*)addr, __float_as_uint(value)));
+}
 } // namespace
 
 template <
     // The datatype of Q/K/V
     typename scalar_t_,
     // Architecture we are targeting (eg `cutlass::arch::Sm80`)
     typename ArchTag,
     // If Q/K/V are correctly aligned in memory and we can run a fast kernel
     bool isAligned_,
     int kQueriesPerBlock,
-    int kKeysPerBlock,
-    bool kSingleValueIteration // = `value.shape[-1] <= kKeysPerBlock`
-    >
+    int kKeysPerBlock_,
+    bool kSingleValueIteration_, // = `value.shape[-1] <= kKeysPerBlock`
+    // This is quite slower on V100 for some reason
+    // Set to false if you know at compile-time you will never need dropout
+    bool kSupportsDropout_ = true,
+    bool kSupportsBias_ = true>
 struct AttentionKernel {
+  enum CustomMaskType {
+    NoCustomMask = 0,
+    CausalFromTopLeft = 1,
+    CausalFromBottomRight = 2,
+    NumCustomMaskTypes,
+  };
+
   using scalar_t = scalar_t_;
   using accum_t = float;
   using lse_scalar_t = float;
   using output_t = scalar_t;
   // Accumulator between 2 iterations
   // Using `accum_t` improves perf on f16 at the cost of
   // numerical errors
   using output_accum_t = accum_t;
+  static constexpr bool kSupportsDropout = kSupportsDropout_;
+  static constexpr bool kSupportsBias = kSupportsBias_;
+  static constexpr int kKeysPerBlock = kKeysPerBlock_;
   static constexpr bool kIsAligned = isAligned_;
+  static constexpr bool kSingleValueIteration = kSingleValueIteration_;
   static constexpr int32_t kAlignLSE = 32; // block size of backward
   static constexpr bool kPreloadV = ArchTag::kMinComputeCapability >= 80 &&
       cutlass::sizeof_bits<scalar_t>::value == 16;
   static constexpr bool kKeepOutputInRF = kSingleValueIteration;
   static constexpr bool kNeedsOutputAccumulatorBuffer = !kKeepOutputInRF &&
       !cutlass::platform::is_same<output_accum_t, output_t>::value;
 
@@ -122,139 +144,215 @@
   static constexpr int kNumThreads = kWarpSize * kNumWarpsPerBlock;
   static constexpr int kMinBlocksPerSm =
       getWarpsPerSm<scalar_t, ArchTag>() / kNumWarpsPerBlock;
 
   struct Params {
     // Input tensors
     scalar_t* query_ptr; // [num_queries, num_heads, head_dim]
-    scalar_t* key_ptr;   // [num_keys, num_heads, head_dim]
+    scalar_t* key_ptr; // [num_keys, num_heads, head_dim]
     scalar_t* value_ptr; // [num_keys, num_heads, head_dim_value]
-    int32_t* cu_seqlens_q_ptr = nullptr;
-    int32_t* cu_seqlens_k_ptr = nullptr;
+    scalar_t* attn_bias_ptr = nullptr; // [num_heads, num_queries, num_keys]
+    int32_t* seqstart_q_ptr = nullptr;
+    int32_t* seqstart_k_ptr = nullptr;
+
+    int32_t* causal_diagonal_ptr = nullptr;
+    int32_t* seqlen_k_ptr = nullptr;
+    uint32_t causal_diagonal_offset = 0;
 
     // Output tensors
     output_t* output_ptr; // [num_queries, num_heads, head_dim_value]
     output_accum_t*
         output_accum_ptr; // [num_queries, num_heads, head_dim_value]
     lse_scalar_t* logsumexp_ptr; // [num_heads, num_queries] - can be null
 
+    // Scale
+    accum_t scale;
+
     // Dimensions/strides
     int32_t head_dim;
     int32_t head_dim_value;
     int32_t num_queries;
     int32_t num_keys;
 
-    bool causal;
+    uint8_t custom_mask_type = NoCustomMask;
 
     int32_t q_strideM;
     int32_t k_strideM;
     int32_t v_strideM;
+    int32_t bias_strideM = 0;
+
+    int32_t o_strideM = 0;
 
     // Everything below is only used in `advance_to_block`
     // and shouldn't use registers
     int32_t q_strideH;
     int32_t k_strideH;
     int32_t v_strideH;
-    int32_t o_strideH;
+    int32_t bias_strideH = 0;
+
     int64_t q_strideB;
     int64_t k_strideB;
     int64_t v_strideB;
-    int64_t o_strideB;
+    int32_t bias_strideB = 0;
+
     int32_t num_batches;
     int32_t num_heads;
 
-    CUTLASS_HOST_DEVICE int32_t o_strideM() const {
-      return head_dim_value;
-    }
+    // dropout
+    bool use_dropout;
+    unsigned long long dropout_batch_head_rng_offset;
+    float dropout_prob;
+#ifdef HAS_PYTORCH
+    at::PhiloxCudaState rng_engine_inputs;
+#endif
 
     // Moves pointers to what we should process
     // Returns "false" if there is no work to do
     CUTLASS_DEVICE bool advance_to_block() {
       auto batch_id = blockIdx.z;
       auto head_id = blockIdx.y;
       auto query_start = blockIdx.x * kQueriesPerBlock;
 
       auto lse_dim = ceil_div((int32_t)num_queries, kAlignLSE) * kAlignLSE;
 
+      if (kSupportsDropout) {
+        dropout_batch_head_rng_offset =
+            batch_id * num_heads * num_queries * num_keys +
+            head_id * num_queries * num_keys;
+      }
+
       int64_t q_start, k_start;
       // Advance to current batch - in case of different sequence lengths
-      if (cu_seqlens_q_ptr != nullptr) {
-        assert(cu_seqlens_k_ptr != nullptr);
-        cu_seqlens_q_ptr += batch_id;
-        cu_seqlens_k_ptr += batch_id;
-        q_start = cu_seqlens_q_ptr[0];
-        k_start = cu_seqlens_k_ptr[0];
-        int64_t q_next_start = cu_seqlens_q_ptr[1];
-        int64_t k_next_start = cu_seqlens_k_ptr[1];
+      if (seqstart_q_ptr != nullptr) {
+        assert(seqstart_k_ptr != nullptr);
+        seqstart_q_ptr += batch_id;
+
+        q_start = seqstart_q_ptr[0];
+        int64_t q_next_start = seqstart_q_ptr[1];
+        int64_t k_end;
+        seqstart_k_ptr += batch_id;
+
+        if (seqlen_k_ptr) {
+          k_start = seqstart_k_ptr[0];
+          k_end = k_start + seqlen_k_ptr[batch_id];
+        } else {
+          k_start = seqstart_k_ptr[0];
+          k_end = seqstart_k_ptr[1];
+        }
+
         num_queries = q_next_start - q_start;
-        num_keys = k_next_start - k_start;
+        num_keys = k_end - k_start;
 
         if (query_start >= num_queries) {
           return false;
         }
       } else {
         query_ptr += batch_id * q_strideB;
         key_ptr += batch_id * k_strideB;
         value_ptr += batch_id * v_strideB;
-        output_ptr += batch_id * o_strideB;
+        output_ptr += int64_t(batch_id * num_queries) * o_strideM;
         if (output_accum_ptr != nullptr) {
-          output_accum_ptr += batch_id * o_strideB;
+          output_accum_ptr +=
+              int64_t(batch_id * num_queries) * (head_dim_value * num_heads);
         }
         q_start = 0;
         k_start = 0;
       }
 
       // Advance to the current batch / head / query_start
       query_ptr += (q_start + query_start) * q_strideM + head_id * q_strideH;
       key_ptr += k_start * k_strideM + head_id * k_strideH;
+
       value_ptr += k_start * v_strideM + head_id * v_strideH;
-      output_ptr += int64_t(q_start + query_start) * o_strideM() +
-          head_id * o_strideH;
+      output_ptr +=
+          int64_t(q_start + query_start) * o_strideM + head_id * head_dim_value;
 
+      if (kSupportsBias && attn_bias_ptr != nullptr) {
+        attn_bias_ptr += (batch_id * bias_strideB) + (head_id * bias_strideH);
+      }
       if (output_accum_ptr != nullptr) {
-        output_accum_ptr += int64_t(q_start + query_start) * o_strideM() +
-            head_id * o_strideH;
+        output_accum_ptr +=
+            int64_t(q_start + query_start) * (head_dim_value * num_heads) +
+            head_id * head_dim_value;
       } else {
         // Accumulate directly in the destination buffer (eg for f32)
         output_accum_ptr = (accum_t*)output_ptr;
       }
+
       if (logsumexp_ptr != nullptr) {
         // lse[batch_id, head_id, query_start]
         logsumexp_ptr +=
             batch_id * lse_dim * num_heads + head_id * lse_dim + query_start;
       }
 
-      num_queries -= query_start;
-      if (causal) {
+      // Custom masking
+      if (causal_diagonal_ptr) {
+        causal_diagonal_offset = causal_diagonal_ptr[batch_id];
+      }
+      if (custom_mask_type == CausalFromBottomRight) {
+        causal_diagonal_offset += num_keys - num_queries;
+      }
+      if (custom_mask_type == CausalFromTopLeft ||
+          custom_mask_type == CausalFromBottomRight) {
+        // the bottom row of the current block is query_start + kQueriesPerBlock
+        // the last active key is then query_start + causal_diagonal_offset +
+        // kQueriesPerBlock so num_keys is the min between actual num_keys and
+        // this to avoid extra computations
         num_keys = cutlass::fast_min(
-            int32_t(query_start + kQueriesPerBlock), num_keys);
+            int32_t(query_start + causal_diagonal_offset + kQueriesPerBlock),
+            num_keys);
       }
+
+      num_queries -= query_start;
       num_batches = 0; // no longer used after
 
+      // If num_queries == 1, and there is only one key head we're wasting
+      // 15/16th of tensor core compute In that case :
+      //  - we only launch kernels for head_id % kQueriesPerBlock == 0
+      //  - we iterate over heads instead of queries (strideM = strideH)
+      if (num_queries == 1 && k_strideH == 0 && v_strideH == 0) {
+        if (head_id % kQueriesPerBlock != 0)
+          return false;
+        q_strideM = q_strideH;
+        num_queries = num_heads;
+        num_heads = 1; // unused but here for intent
+        // remove causal since n_query = 1
+        // otherwise, offset would change with head !
+        custom_mask_type = NoCustomMask;
+        o_strideM = head_dim_value;
+      }
+
       // Make sure the compiler knows these variables are the same on all
       // the threads of the warp.
       query_ptr = warp_uniform(query_ptr);
       key_ptr = warp_uniform(key_ptr);
       value_ptr = warp_uniform(value_ptr);
+      if (kSupportsBias) {
+        attn_bias_ptr = warp_uniform(attn_bias_ptr);
+      }
       output_ptr = warp_uniform(output_ptr);
       output_accum_ptr = warp_uniform(output_accum_ptr);
       logsumexp_ptr = warp_uniform(logsumexp_ptr);
       num_queries = warp_uniform(num_queries);
       num_keys = warp_uniform(num_keys);
+      num_heads = warp_uniform(num_heads);
       head_dim = warp_uniform(head_dim);
       head_dim_value = warp_uniform(head_dim_value);
+      o_strideM = warp_uniform(o_strideM);
+      custom_mask_type = warp_uniform(custom_mask_type);
       return true;
     }
 
     __host__ dim3 getBlocksGrid() const {
       return dim3(
           ceil_div(num_queries, (int32_t)kQueriesPerBlock),
           num_heads,
           num_batches);
     }
+
     __host__ dim3 getThreadsGrid() const {
       return dim3(kWarpSize, kNumWarpsPerBlock, 1);
     }
   };
 
   struct MM0 {
     /*
@@ -301,24 +399,32 @@
                                 // uses too much smem
         typename GemmType::Operator // Operator
         >::DefaultMma;
     using MmaCore = typename DefaultMma::MmaCore;
     using IteratorA = typename DefaultMma::IteratorA;
     using IteratorB = typename DefaultMma::IteratorB;
     using Mma = typename DefaultMma::ThreadblockMma;
-    using ScalingCoefsUpdater = typename DefaultAttentionScalingCoefsUpdater<
+    using AccumLambdaIterator = typename DefaultMmaAccumLambdaIterator<
         typename Mma::Operator::IteratorC,
         accum_t,
-        kWarpSize>::Updater;
+        kWarpSize>::Iterator;
     static_assert(
         MmaCore::WarpCount::kM * MmaCore::WarpCount::kN *
                 MmaCore::WarpCount::kK ==
             kNumWarpsPerBlock,
         "");
 
+    // used for efficient load of bias tile Bij from global to shared memory
+    using BiasLoader = TileSmemLoader<
+        scalar_t,
+        cutlass::MatrixShape<kQueriesPerBlock, kKeysPerBlock>,
+        MmaCore::kThreads,
+        // input restriction: kv_len has to be a multiple of this value
+        128 / cutlass::sizeof_bits<scalar_t>::value>;
+
     // Epilogue to store to shared-memory in a format that we can use later for
     // the second matmul
     using B2bGemm = typename cutlass::gemm::threadblock::B2bGemm<
         typename Mma::Operator::IteratorC,
         typename Mma::Operator,
         scalar_t,
         WarpShape,
@@ -372,15 +478,16 @@
         DefaultConfig::kStages,
         false, // SplitKSerial
         typename GemmType::Operator>;
 
     using DefaultMmaFromSmem =
         typename cutlass::gemm::threadblock::DefaultMmaFromSharedMemory<
             typename DefaultGemm::Mma,
-            typename MM0::AccumulatorSharedStorage>;
+            typename MM0::AccumulatorSharedStorage,
+            false>; // kScaleOperandA
     using Mma = typename DefaultMmaFromSmem::Mma;
     using IteratorB = typename Mma::IteratorB;
     using WarpCount = typename Mma::WarpCount;
     static_assert(
         WarpCount::kM * WarpCount::kN * WarpCount::kK == kNumWarpsPerBlock,
         "");
 
@@ -409,15 +516,18 @@
     cutlass::Array<accum_t, kQueriesPerBlock> s_prime;
     cutlass::Array<accum_t, kQueriesPerBlock> mi;
   };
 
   struct SharedStorageEpilogueAtEnd : ScalingCoefs {
     struct SharedStorageAfterMM0 {
       // Everything here might be overwritten during MM0
-      typename MM0::AccumulatorSharedStorage si;
+      union {
+        typename MM0::BiasLoader::SmemTile bias;
+        typename MM0::AccumulatorSharedStorage si;
+      };
       typename MM1::SharedStorageMM1 mm1;
     };
 
     union {
       typename MM0::Mma::SharedStorage mm0;
       SharedStorageAfterMM0 after_mm0;
       typename MM1::DefaultEpilogue::SharedStorage epilogue;
@@ -428,15 +538,18 @@
       return epilogue;
     }
   };
 
   struct SharedStorageEpilogueInLoop : ScalingCoefs {
     struct SharedStorageAfterMM0 {
       // Everything here might be overwritten during MM0
-      typename MM0::AccumulatorSharedStorage si;
+      union {
+        typename MM0::BiasLoader::SmemTile bias;
+        typename MM0::AccumulatorSharedStorage si;
+      };
       typename MM1::SharedStorageMM1 mm1;
       typename MM1::DefaultEpilogue::SharedStorage epilogue;
     };
 
     union {
       typename MM0::Mma::SharedStorage mm0;
       SharedStorageAfterMM0 after_mm0;
@@ -453,74 +566,114 @@
       SharedStorageEpilogueAtEnd,
       SharedStorageEpilogueInLoop>::type;
 
   static bool __host__ check_supported(Params const& p) {
     CHECK_ALIGNED_PTR(p.query_ptr, kAlignmentQ);
     CHECK_ALIGNED_PTR(p.key_ptr, kAlignmentK);
     CHECK_ALIGNED_PTR(p.value_ptr, kAlignmentV);
+    if (kSupportsBias) {
+      CHECK_ALIGNED_PTR(p.attn_bias_ptr, kAlignmentQ);
+      XFORMERS_CHECK(
+          p.bias_strideB % kAlignmentQ == 0,
+          "attn_bias is not correctly aligned");
+      XFORMERS_CHECK(
+          p.bias_strideH % kAlignmentQ == 0,
+          "attn_bias is not correctly aligned");
+      XFORMERS_CHECK(
+          p.bias_strideM % kAlignmentQ == 0,
+          "attn_bias is not correctly aligned");
+    }
     XFORMERS_CHECK(
         p.q_strideM % kAlignmentQ == 0, "query is not correctly aligned");
     XFORMERS_CHECK(
         p.k_strideM % kAlignmentK == 0, "key is not correctly aligned");
     XFORMERS_CHECK(
         p.v_strideM % kAlignmentV == 0, "value is not correctly aligned");
     XFORMERS_CHECK(
         p.q_strideH % kAlignmentQ == 0, "query is not correctly aligned");
     XFORMERS_CHECK(
         p.k_strideH % kAlignmentK == 0, "key is not correctly aligned");
     XFORMERS_CHECK(
         p.v_strideH % kAlignmentV == 0, "value is not correctly aligned");
+    XFORMERS_CHECK(
+        p.causal_diagonal_ptr == nullptr || p.custom_mask_type != NoCustomMask,
+        "`causal_diagonal_ptr` is only useful when `custom_mask_type` is causal");
+    XFORMERS_CHECK(
+        p.custom_mask_type < NumCustomMaskTypes,
+        "invalid value for `custom_mask_type`");
     return true;
   }
 
   static void CUTLASS_DEVICE attention_kernel(Params& p) {
     // In this block, we will only ever:
     // - read query[query_start:query_end, :]
     // - write to output[query_start:query_end, :]
 
     extern __shared__ char smem_buffer[];
     SharedStorage& shared_storage = *((SharedStorage*)smem_buffer);
     auto& m_prime = shared_storage.m_prime;
     auto& s_prime = shared_storage.s_prime;
-    auto& si = shared_storage.after_mm0.si;
     auto& mi = shared_storage.mi;
+    const uint32_t query_start = blockIdx.x * kQueriesPerBlock;
 
     static_assert(kQueriesPerBlock < kNumWarpsPerBlock * kWarpSize, "");
     if (thread_id() < kQueriesPerBlock) {
       s_prime[thread_id()] = accum_t(0);
       m_prime[thread_id()] =
           -cutlass::platform::numeric_limits<accum_t>::infinity();
       mi[thread_id()] = -cutlass::platform::numeric_limits<accum_t>::infinity();
     }
     typename MM1::Mma::FragmentC accum_o;
     accum_o.clear();
 
     auto createOutputIter = [&](int col) -> typename MM1::OutputTileIterator {
       using OutputTileIterator = typename MM1::OutputTileIterator;
       return OutputTileIterator(
-          typename OutputTileIterator::Params{(int32_t)p.o_strideM()},
+          typename OutputTileIterator::Params{(int32_t)p.o_strideM},
           p.output_ptr,
           typename OutputTileIterator::TensorCoord{
               p.num_queries, p.head_dim_value},
           thread_id(),
           {0, col});
     };
 
     auto createOutputAccumIter = [&](int col) ->
         typename MM1::OutputTileIteratorAccum {
           using OutputTileIteratorAccum = typename MM1::OutputTileIteratorAccum;
           return OutputTileIteratorAccum(
-              typename OutputTileIteratorAccum::Params{(int32_t)p.o_strideM()},
+              typename OutputTileIteratorAccum::Params{
+                  (int32_t)(p.head_dim_value * p.num_heads)},
               p.output_accum_ptr,
               typename OutputTileIteratorAccum::TensorCoord{
                   p.num_queries, p.head_dim_value},
               thread_id(),
               {0, col});
         };
 
+#ifdef HAS_PYTORCH
+    curandStatePhilox4_32_10_t curand_state_init;
+    if (kSupportsDropout && p.use_dropout) {
+      const auto seeds = at::cuda::philox::unpack(p.rng_engine_inputs);
+
+      // each element of the attention matrix P with shape
+      // (batch_sz, n_heads, n_queries, n_keys) is associated with a single
+      // offset in RNG sequence. we initialize the RNG state with offset that
+      // starts at the beginning of a (n_queries, n_keys) matrix for this
+      // block's batch_id and head_id
+      // initializing rng state is very expensive, so we run once per kernel,
+      // rather than once per iteration. each iteration takes a copy of the
+      // initialized RNG state and offsets it as needed.
+      curand_init(
+          std::get<0>(seeds),
+          0,
+          std::get<1>(seeds) + p.dropout_batch_head_rng_offset,
+          &curand_state_init);
+    }
+#endif
+
     // Iterate through keys
     for (int32_t iter_key_start = 0; iter_key_start < p.num_keys;
          iter_key_start += kKeysPerBlock) {
       int32_t problem_size_0_m =
           cutlass::fast_min((int32_t)kQueriesPerBlock, p.num_queries);
       int32_t problem_size_0_n = cutlass::fast_min(
           int32_t(kKeysPerBlock), p.num_keys - iter_key_start);
@@ -605,24 +758,73 @@
       typename MM0::Mma::Operator::IteratorC::TensorCoord
           iteratorC_tile_offset = {
               (tb_tile_offset.m() * MM0::Mma::WarpCount::kM) +
                   (my_warp_id % MM0::Mma::WarpCount::kM),
               (tb_tile_offset.n() * MM0::Mma::WarpCount::kN) +
                   (my_warp_id / MM0::Mma::WarpCount::kM)};
 
+      // multiply by scaling factor
+      if (kSupportsBias) {
+        accum =
+            cutlass::multiplies<typename MM0::Mma::FragmentC>()(p.scale, accum);
+      }
+
+      // apply attention bias if applicable
+      if (kSupportsBias && p.attn_bias_ptr != nullptr) {
+        // load bias tile Bij into shared memory
+        typename MM0::BiasLoader::GmemTileIterator bias_iter(
+            {cutlass::layout::RowMajor(p.bias_strideM)},
+            // attn_bias_pointer points to matrix of size (n_queries, n_keys)
+            // for the relevant batch_id and head_id
+            p.attn_bias_ptr + query_start * p.bias_strideM + iter_key_start,
+            {problem_size_0_m, problem_size_0_n},
+            thread_id());
+        cutlass::TensorRef<scalar_t, cutlass::layout::RowMajor> bias_tensor_ref(
+            shared_storage.after_mm0.bias.data(),
+            cutlass::layout::RowMajor(MM0::ThreadblockShape::kN));
+        typename MM0::BiasLoader::SmemTileIterator smem_tile_iter(
+            bias_tensor_ref, thread_id());
+        MM0::BiasLoader::load(bias_iter, smem_tile_iter);
+
+        // Pij += Bij, Pij is in register fragment and Bij is in shared memory
+        auto lane_offset = MM0::AccumLambdaIterator::get_lane_offset(
+            lane_id(), warp_id(), iteratorC_tile_offset);
+        MM0::AccumLambdaIterator::iterateRows(
+            lane_offset,
+            [&](int accum_m) {},
+            [&](int accum_m, int accum_n, int idx) {
+              if (accum_m < problem_size_0_m && accum_n < problem_size_0_n) {
+                accum[idx] += bias_tensor_ref.at({accum_m, accum_n});
+              }
+            },
+            [&](int accum_m) {});
+      }
+
       // Mask out last if causal
-      if (p.causal && p.num_keys - iter_key_start <= kKeysPerBlock) {
+      // This is only needed if upper-right corner of current query / key block
+      // intersects the mask Coordinates of upper-right corner of current block
+      // is y=query_start x=min(iter_key_start + kKeysPerBlock, num_keys)) The
+      // first masked element is x = y + offset -> query_start + offset There is
+      // intersection (and we need to mask) if min(iter_key_start +
+      // kKeysPerBlock, num_keys)) >= query_start + offset
+      if (p.custom_mask_type &&
+          cutlass::fast_min(iter_key_start + kKeysPerBlock, p.num_keys) >=
+              (query_start + p.causal_diagonal_offset)) {
         auto query_start = blockIdx.x * kQueriesPerBlock;
-        auto lane_offset = MM0::ScalingCoefsUpdater::get_lane_offset(
+        auto lane_offset = MM0::AccumLambdaIterator::get_lane_offset(
             lane_id(), warp_id(), iteratorC_tile_offset);
         int32_t last_col;
-        MM0::ScalingCoefsUpdater::iterateRows(
+        MM0::AccumLambdaIterator::iterateRows(
             lane_offset,
             [&](int accum_m) {
-              last_col = query_start + accum_m - iter_key_start;
+              // last absolute col is (last absolute query + offset)
+              // last local col is (last absolute query + offset -
+              // iter_key_start)
+              last_col = query_start + accum_m + p.causal_diagonal_offset -
+                  iter_key_start;
             },
             [&](int accum_m, int accum_n, int idx) {
               if (accum_n > last_col) {
                 accum[idx] =
                     -cutlass::platform::numeric_limits<accum_t>::infinity();
               }
             },
@@ -630,33 +832,30 @@
       }
       DISPATCH_BOOL(iter_key_start == 0, kIsFirst, ([&] {
                       DISPATCH_BOOL(
                           p.num_keys - iter_key_start >= kKeysPerBlock,
                           kFullColumns,
                           ([&] {
                             // Update `mi` from accum stored in registers
-                            // Also updates `accum` with accum[i] <-
-                            // exp(accum[i] * scale
-                            // - mi)
-                            MM0::ScalingCoefsUpdater::update<
-                                kQueriesPerBlock,
+                            // Also does accum[i] <- exp(accum[i] - mi)
+                            iterative_softmax<
+                                typename MM0::Mma::Operator::IteratorC,
                                 kFullColumns,
-                                kIsFirst,
-                                kKeepOutputInRF>(
+                                kIsFirst>(
                                 accum_o,
                                 accum,
                                 mi,
                                 m_prime,
                                 s_prime,
                                 lane_id(),
                                 thread_id(),
                                 warp_id(),
                                 p.num_keys - iter_key_start,
                                 iteratorC_tile_offset,
-                                1.0f / cutlass::fast_sqrt(float(p.head_dim)));
+                                kSupportsBias ? 1.0f : p.scale);
                           }));
                     }));
 
       // Output results to shared-memory
       int warp_idx_mn_0 = my_warp_id %
           (MM0::Mma::Base::WarpCount::kM * MM0::Mma::Base::WarpCount::kN);
       auto output_tile_coords = cutlass::MatrixCoord{
@@ -664,14 +863,77 @@
           warp_idx_mn_0 / MM0::Mma::Base::WarpCount::kM};
 
       MM0::B2bGemm::accumToSmem(
           shared_storage.after_mm0.si, accum, my_lane_id, output_tile_coords);
 
       __syncthreads();
 
+#ifdef HAS_PYTORCH
+      // apply dropout (if applicable) after we've written Pij to smem.
+      // dropout is applied by multiplying each element of Pij by:
+      // - 0 with probability dropout_p
+      // - 1 / (1 - dropout_p) with probability 1 - dropout_p
+      //
+      // for backward purposes we want to be able to map each element of the
+      // attention matrix to the same random uniform number as the one we used
+      // in forward, without needing to use the same iteration order or having
+      // to store the dropout matrix. its possible to do this in registers but
+      // it ends up being very slow because each thread having noncontiguous
+      // strips of the Pij tile means we have to skip around a lot, and also
+      // have to generate a single random number at a time
+      if (kSupportsDropout && p.use_dropout) {
+        auto si = shared_storage.after_mm0.si.accum_ref();
+        // each thread handles a contiguous sequence of elements from Sij, all
+        // coming from the same row. the reason they have to come from the same
+        // row is that the sampling random numbers from a contiguous random
+        // number sequence is much more efficient than jumping around, and the
+        // linear offset of each element of S (the global matrix) maps to an
+        // offset in a random number sequence. for S, the end of a row and the
+        // beginning of the next have adjacent offsets, but for Sij, this is not
+        // necessarily the case.
+        const int num_threads = blockDim.x * blockDim.y * blockDim.z;
+        const int threads_per_row =
+            cutlass::fast_min(num_threads / problem_size_0_m, problem_size_0_n);
+        const int elts_per_thread = cutlass::round_nearest(
+            cutlass::ceil_div(problem_size_0_n, threads_per_row), 4);
+
+        const int thread_i = thread_id() / threads_per_row;
+        const int thread_start_j =
+            (thread_id() % threads_per_row) * elts_per_thread;
+
+        if (thread_i < problem_size_0_m && thread_start_j < problem_size_0_n) {
+          curandStatePhilox4_32_10_t curand_state = curand_state_init;
+          skipahead(
+              static_cast<unsigned long long>(
+                  (query_start + thread_i) * p.num_keys +
+                  (iter_key_start + thread_start_j)),
+              &curand_state);
+          const float dropout_scale = 1.0 / (1.0 - p.dropout_prob);
+
+          // apply dropout scaling to elements this thread is responsible for,
+          // in chunks of 4
+          for (int sij_start_col_idx = thread_start_j; sij_start_col_idx <
+               cutlass::fast_min(thread_start_j + elts_per_thread,
+                                 problem_size_0_n);
+               sij_start_col_idx += 4) {
+            const float4 rand_uniform_quad = curand_uniform4(&curand_state);
+
+            CUTLASS_PRAGMA_UNROLL
+            for (int quad_idx = 0; quad_idx < 4; ++quad_idx) {
+              si.at({thread_i, sij_start_col_idx + quad_idx}) *=
+                  static_cast<scalar_t>(
+                      dropout_scale *
+                      ((&rand_uniform_quad.x)[quad_idx] > p.dropout_prob));
+            }
+          }
+        }
+        __syncthreads(); // p.use_dropout should have same value kernel-wide
+      }
+#endif
+
       //
       // MATMUL: Attn . V
       // Run the matmul `attn @ V` for a block of attn and V.
       // `attn` is read from shared memory (in `shared_storage_si`)
       // `V` is read from global memory (with iterator_B)
       //
 
@@ -835,14 +1097,124 @@
       } else if (thread_id() < lse_dim) {
         p.logsumexp_ptr[thread_id()] =
             cutlass::platform::numeric_limits<accum_t>::infinity();
       }
     }
   }
 
+  template <
+      typename WarpIteratorC,
+      bool kFullColumns,
+      bool kIsFirst>
+  CUTLASS_DEVICE static void iterative_softmax(
+      typename WarpIteratorC::Fragment& frag_o, // output so far
+      typename WarpIteratorC::Fragment& frag,
+      cutlass::Array<accum_t, kQueriesPerBlock>& mi,
+      cutlass::Array<accum_t, kQueriesPerBlock>& m_prime,
+      cutlass::Array<accum_t, kQueriesPerBlock>& s_prime,
+      int8_t lane_id,
+      int8_t thread_id,
+      int8_t warp_id,
+      int16_t max_col,
+      typename WarpIteratorC::TensorCoord const& tile_offset,
+      float scaling) {
+    /* Iterates on the accumulator and corresponding position on result matrix
+
+    (1) Update `mi[r]` to the max value of the row `r`
+    (2) In a second iteration do the following:
+        (a) accum   <- exp(accum - mi)
+        (b) m_prime <- exp(m_prime - mi)
+        (c) s_prime <- s_prime * m_prime + sum(accum)
+
+    All of this is done on registers, before we store all of this
+    on shared memory for the next matmul with Value.
+    */
+    using Fragment = typename WarpIteratorC::Fragment;
+    using LambdaIterator = typename DefaultMmaAccumLambdaIterator<
+        WarpIteratorC,
+        accum_t,
+        kWarpSize>::Iterator;
+    // Convert to `accum_t` (rather than double)
+    constexpr float kLog2e = 1.4426950408889634074; // log_2(e) = M_LOG2E
+    if (!kIsFirst) {
+      if (thread_id < kQueriesPerBlock) {
+        m_prime[thread_id] = mi[thread_id];
+      }
+      __syncthreads();
+    }
+
+    auto lane_offset =
+        LambdaIterator::get_lane_offset(lane_id, warp_id, tile_offset);
+
+    // First update `mi` to the max per-row
+    {
+      accum_t max;
+      LambdaIterator::iterateRows(
+          lane_offset,
+          [&](int accum_m) {
+            max = -cutlass::platform::numeric_limits<accum_t>::infinity();
+          },
+          [&](int accum_m, int accum_n, int idx) {
+            if (kFullColumns || accum_n < max_col) {
+              max = cutlass::fast_max(max, frag[idx]);
+            }
+          },
+          [&](int accum_m) {
+            // Having 4x atomicMax seems faster than reduce within warp
+            // first...
+            atomicMaxFloat(&mi[accum_m], max * scaling);
+          });
+    }
+    frag = cutlass::multiplies<Fragment>()(scaling * kLog2e, frag);
+
+    // Make sure we all share the update values for `mi`
+    __syncthreads();
+
+    if (thread_id < kQueriesPerBlock) {
+      auto m_prime_exp = exp2f(kLog2e * (m_prime[thread_id] - mi[thread_id]));
+      m_prime[thread_id] = m_prime_exp;
+      s_prime[thread_id] *= m_prime_exp;
+    }
+    __syncthreads(); // Update output fragments
+    if (kKeepOutputInRF && !kIsFirst) {
+      accum_t mp;
+      LambdaIterator::iterateRows(
+          lane_offset,
+          [&](int accum_m) { mp = m_prime[accum_m]; },
+          [&](int accum_m, int accum_n, int idx) { frag_o[idx] *= mp; },
+          [&](int accum_m) {});
+      __syncthreads();
+    }
+    // Update accum_m, accum_n, ...
+    {
+      accum_t mi_row, total_row;
+      LambdaIterator::iterateRows(
+          lane_offset,
+          [&](int accum_m) { mi_row = kLog2e * mi[accum_m]; },
+          [&](int accum_m, int accum_n, int idx) {
+            frag[idx] = (kFullColumns || accum_n < max_col)
+                ? exp2f(frag[idx] - mi_row)
+                : accum_t(0.0);
+          },
+          [&](int accum_m) {});
+      LambdaIterator::iterateRows(
+          lane_offset,
+          [&](int accum_m) { total_row = 0.0; },
+          [&](int accum_m, int accum_n, int idx) { total_row += frag[idx]; },
+          [&](int accum_m) {
+            if (LambdaIterator::reduceSameRow(
+                    lane_id, total_row, [](accum_t a, accum_t b) {
+                      return a + b;
+                    })) {
+              atomicAdd(&s_prime[accum_m], total_row);
+            }
+          });
+    }
+  }
+
   static CUTLASS_DEVICE int8_t lane_id() {
     return threadIdx.x;
   }
   static CUTLASS_DEVICE int8_t warp_id() {
     return threadIdx.y;
   }
   static CUTLASS_DEVICE int16_t thread_id() {
@@ -858,93 +1230,7 @@
   }
   AK::attention_kernel(p);
 }
 
 template <typename AK>
 __global__ void __launch_bounds__(AK::kNumThreads, AK::kMinBlocksPerSm)
     attention_kernel_batched(typename AK::Params params);
-
-#define _ATTENTION_KERNEL_FORWARD_BEGIN(...)                                  \
-  template <>                                                                 \
-  __global__ void __launch_bounds__(                                          \
-      __VA_ARGS__::kNumThreads, __VA_ARGS__::kMinBlocksPerSm)                 \
-      attention_kernel_batched<__VA_ARGS__>(typename __VA_ARGS__::Params p) { \
-    using Kernel = __VA_ARGS__;
-#define _ATTENTION_KERNEL_FORWARD_END() }
-
-#ifdef __CUDA_ARCH__
-#define __CUDA_ARCH_OR_ZERO__ __CUDA_ARCH__
-#else
-#define __CUDA_ARCH_OR_ZERO__ 0
-#endif
-
-#define INSTANTIATE_ATTENTION_KERNEL_FORWARD(              \
-    ARCH,                                                  \
-    SCALAR_T,                                              \
-    IS_ALIGNED,                                            \
-    QUERIES_PER_BLOCK,                                     \
-    KEYS_PER_BLOCK,                                        \
-    SINGLE_VALUE_ITER)                                     \
-  _ATTENTION_KERNEL_FORWARD_BEGIN(AttentionKernel<         \
-                                  SCALAR_T,                \
-                                  cutlass::arch::Sm##ARCH, \
-                                  IS_ALIGNED,              \
-                                  QUERIES_PER_BLOCK,       \
-                                  KEYS_PER_BLOCK,          \
-                                  SINGLE_VALUE_ITER>)      \
-  if (!p.advance_to_block()) {                             \
-    return;                                                \
-  }                                                        \
-  Kernel::attention_kernel(p);                             \
-  _ATTENTION_KERNEL_FORWARD_END();
-
-#define INSTANTIATE_ATTENTION_KERNEL_FORWARD_DISABLED(              \
-    ARCH,                                                           \
-    SCALAR_T,                                                       \
-    IS_ALIGNED,                                                     \
-    QUERIES_PER_BLOCK,                                              \
-    KEYS_PER_BLOCK,                                                 \
-    SINGLE_VALUE_ITER)                                              \
-  _ATTENTION_KERNEL_FORWARD_BEGIN(AttentionKernel<                  \
-                                  SCALAR_T,                         \
-                                  cutlass::arch::Sm##ARCH,          \
-                                  IS_ALIGNED,                       \
-                                  QUERIES_PER_BLOCK,                \
-                                  KEYS_PER_BLOCK,                   \
-                                  SINGLE_VALUE_ITER>)               \
-  printf(                                                           \
-      "FATAL: this function is for sm%d, but was built for sm%d\n", \
-      int(ARCH),                                                    \
-      int(__CUDA_ARCH_OR_ZERO__));                                  \
-  _ATTENTION_KERNEL_FORWARD_END();
-
-// All kernels are disabled by default
-#define INSTANTIATE_ATTENTION_KERNEL_FORWARD_SM50(...) \
-  INSTANTIATE_ATTENTION_KERNEL_FORWARD_DISABLED(50, __VA_ARGS__)
-#define INSTANTIATE_ATTENTION_KERNEL_FORWARD_SM70(...) \
-  INSTANTIATE_ATTENTION_KERNEL_FORWARD_DISABLED(70, __VA_ARGS__)
-#define INSTANTIATE_ATTENTION_KERNEL_FORWARD_SM75(...) \
-  INSTANTIATE_ATTENTION_KERNEL_FORWARD_DISABLED(75, __VA_ARGS__)
-#define INSTANTIATE_ATTENTION_KERNEL_FORWARD_SM80(...) \
-  INSTANTIATE_ATTENTION_KERNEL_FORWARD_DISABLED(80, __VA_ARGS__)
-
-// Enable the right one based on __CUDA_ARCH__
-#ifndef __CUDA_ARCH__
-#elif __CUDA_ARCH__ < 500
-#error "Need cuda arch at least 5.0"
-#elif __CUDA_ARCH__ < 700
-#undef INSTANTIATE_ATTENTION_KERNEL_FORWARD_SM50
-#define INSTANTIATE_ATTENTION_KERNEL_FORWARD_SM50(...) \
-  INSTANTIATE_ATTENTION_KERNEL_FORWARD(50, __VA_ARGS__)
-#elif __CUDA_ARCH__ < 750
-#undef INSTANTIATE_ATTENTION_KERNEL_FORWARD_SM70
-#define INSTANTIATE_ATTENTION_KERNEL_FORWARD_SM70(...) \
-  INSTANTIATE_ATTENTION_KERNEL_FORWARD(70, __VA_ARGS__)
-#elif __CUDA_ARCH__ < 800
-#undef INSTANTIATE_ATTENTION_KERNEL_FORWARD_SM75
-#define INSTANTIATE_ATTENTION_KERNEL_FORWARD_SM75(...) \
-  INSTANTIATE_ATTENTION_KERNEL_FORWARD(75, __VA_ARGS__)
-#elif __CUDA_ARCH__ >= 800
-#undef INSTANTIATE_ATTENTION_KERNEL_FORWARD_SM80
-#define INSTANTIATE_ATTENTION_KERNEL_FORWARD_SM80(...) \
-  INSTANTIATE_ATTENTION_KERNEL_FORWARD(80, __VA_ARGS__)
-#endif
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h` & `flash_attn-2.0.0/csrc/cutlass/examples/41_fused_multi_head_attention/gemm/mma_from_smem.h`

 * *Files 8% similar despite different names*

```diff
@@ -39,30 +39,34 @@
 #include "cutlass/arch/memory.h"
 #include "cutlass/array.h"
 #include "cutlass/cutlass.h"
 #include "cutlass/epilogue/thread/linear_combination.h"
 #include "cutlass/epilogue/threadblock/default_epilogue_simt.h"
 #include "cutlass/epilogue/threadblock/default_epilogue_tensor_op.h"
 #include "cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h"
+#include "cutlass/functional.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/gemm/warp/mma_tensor_op_fragment_iterator.h"
 #include "cutlass/matrix_shape.h"
 #include "cutlass/numeric_conversion.h"
 #include "cutlass/numeric_types.h"
+#include "cutlass/platform/platform.h"
 #include "cutlass/transform/threadblock/vector_iterator.h"
 
-#include "attention_scaling_coefs_updater.h"
+#include "../epilogue/epilogue_thread_apply_logsumexp.h"
+#include "../gemm/mma_accum_lambda_iterator.h"
+#include "../gemm_kernel_utils.h"
+#include "../iterators/make_residual_last.h"
+#include "../iterators/transpose_warp_iterator.h"
+#include "../iterators/warp_iterator_from_smem.h"
 #include "cutlass/epilogue/threadblock/epilogue_smem_accumulator.h"
 #include "cutlass/gemm/threadblock/mma_base.h"
+#include "cutlass/gemm/threadblock/mma_multistage.h"
+#include "cutlass/gemm/threadblock/mma_pipelined.h"
 #include "cutlass/gemm/warp/mma_tensor_op_tile_access_iterator.h"
-#include "epilogue_thread_apply_logsumexp.h"
-#include "gemm_kernel_utils.h"
-#include "iterators/make_residual_last.h"
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace threadblock {
 
 /// Shared storage object needed by accumulator
 /// From 13_two_tensor_op_fusion/threadblock/b2b_mma_base_smem_accumulator.h
@@ -242,27 +246,103 @@
       ///< ID of warp
       int warp_idx,
       ///< ID of each thread within a warp
       int lane_idx)
       : warp_tile_iterator_B_(shared_storage.operand_B_ref(), lane_idx) {}
 };
 
+namespace {
+
+// has necessary trait compliance with WarpIteratorFromSmem but doesn't do
+// anything, can be default initialized, and uses fragment that takes up
+// (almost) no space. this warp iterator is selected at compile time when
+// elementwise on-the-fly scaling for operand A is disabled, in which case
+// operations related to loading scale factors for operand A get wiped out by
+// the compiler.
+template <typename TensorRef>
+class NoOpWarpIteratorScale {
+ public:
+  // in pipelined+multistage MMA implementations we keep an array of fragments.
+  // if we aren't using scaling we don't want to waste registers on fragments
+  // of scale elements, so ideally this would be sized 0.
+  // using size 1 is kind of a hack to get around arrays of zero-sized objects
+  // not being allowed. the compiler is probably smart enough to wipe it out
+  // anyways.
+  using Fragment = cutlass::Array<char, 1>;
+
+  CUTLASS_HOST_DEVICE
+  NoOpWarpIteratorScale() {}
+
+  CUTLASS_HOST_DEVICE
+  NoOpWarpIteratorScale(TensorRef const&, int) {}
+
+  CUTLASS_HOST_DEVICE
+  NoOpWarpIteratorScale& add_tile_offset(
+      typename TensorRef::TensorCoord const&) {
+    return *this;
+  }
+
+  CUTLASS_HOST_DEVICE
+  NoOpWarpIteratorScale& operator++() {
+    return *this;
+  }
+
+  CUTLASS_DEVICE
+  void load(Fragment&) const {}
+};
+
+// if scaling is enabled, performs fragment elementwise multiplication between
+// fragment and its scaling factor.
+template <typename Fragment, typename FragmentScale, bool ScalingEnabled>
+class FragmentElementwiseScaler;
+
+// specialization for scaling being enabled.
+template <typename Fragment, typename FragmentScale>
+class FragmentElementwiseScaler<Fragment, FragmentScale, true> {
+ public:
+  // cast scale_frag to correct type then apply elementwise to fragment
+  CUTLASS_DEVICE
+  static Fragment apply(Fragment frag, FragmentScale const& scale_frag) {
+    Fragment converted_scale_frag = cutlass::NumericArrayConverter<
+        typename Fragment::Element,
+        typename FragmentScale::Element,
+        FragmentScale::kElements>()(scale_frag);
+    return cutlass::multiplies<Fragment>()(frag, converted_scale_frag);
+  }
+};
+
+// specialization for scaling being disabled. doesn't do anything and should
+// just get wiped out by the compiler.
+template <typename Fragment, typename FragmentScale>
+class FragmentElementwiseScaler<Fragment, FragmentScale, false> {
+ public:
+  CUTLASS_DEVICE
+  static Fragment apply(Fragment frag, FragmentScale const&) {
+    return frag;
+  }
+};
+} // namespace
+
 ////////////////////////////////////////////////////////////////////////////////
 // Taken from
 // https://github.com/NVIDIA/cutlass/blob/master/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined_smem_accumulator.h
 ////////////////////////////////////////////////////////////////////////////////
 
 /// Structure to compute the matrix product targeting CUDA cores and SIMT math
 /// instructions.
 template <
     /// Size of the Gemm problem - concept: gemm::GemmShape<>
     typename Shape_,
     // BEGIN smem
     /// Iterates over the intermediate accumulator tile in shared memory
     typename WarpIteratorA,
+    /// whether or not to perform elementwise multiplication of A
+    //  by another matrix (A_scale) that is also kept in shared memory prior
+    //  to matmul A @ B
+    bool ScaleOperandA_,
     // Accumulator type
     typename AccumulatorSharedStorage,
     // END smem
     /// Iterates over tiles of B operand in global memory
     //  (concept: ReadableTileIterator | ForwardTileIterator |
     //  MaskedTileIterator)
     typename IteratorB_,
@@ -293,14 +373,23 @@
       Shape_,
       AccumulatorSharedStorage::Shape::kN,
       Policy_,
       2>;
 
   using Shape =
       Shape_; ///< Size of the Gemm problem - concept: gemm::GemmShape<>
+  static constexpr bool ScaleOperandA = ScaleOperandA_;
+
+  ///< loads fragments of A_scale from shared memory if operand A scaling is
+  ///< enabled. otherwise no-op.
+  using WarpIteratorAScale = typename cutlass::platform::conditional<
+      ScaleOperandA,
+      WarpIteratorA,
+      NoOpWarpIteratorScale<typename WarpIteratorA::TensorRef>>::type;
+
   using IteratorB =
       IteratorB_; ///< Iterates over tiles of B operand in global memory
   using ElementC = ElementC_; ///< Data type of accumulator matrix
   using LayoutC = LayoutC_; ///< Layout of accumulator matrix
   using Policy = Policy_; ///< Policy describing tuning details
 
   using SmemIteratorB = SmemIteratorB_;
@@ -329,28 +418,79 @@
   // staticaly assert kStages for MmaPipelined is two (Double-buffered pipeline)
   static_assert(
       (Base::kStages == 2),
       "MmaPipelined requires kStages set to value 2");
 
  private:
   using WarpFragmentA = typename Operator::FragmentA;
+
+  /// fragment type of OperandA elementwise scaling matrix. (almost) empty
+  /// if operand A scaling is disabled.
+  using WarpFragmentAScale = typename WarpIteratorAScale::Fragment;
+
   using WarpFragmentB = typename Operator::FragmentB;
 
+  /// applies scaling factor to operand A fragment if operand A scaling is
+  /// enabled. otherwise no-op.
+  using FragmentAScaler = FragmentElementwiseScaler<
+      WarpFragmentA,
+      WarpFragmentAScale,
+      ScaleOperandA>;
+
  protected:
   // /// Iterator to write threadblock-scoped tile of A operand to shared memory
   // SmemIteratorA smem_iterator_A_;
 
   /// Iterator to write threadblock-scoped tile of B operand to shared memory
   SmemIteratorB smem_iterator_B_;
 
   /// Iterator to load a warp-scoped tile of A operand from intermediate
   /// accumulator tile
   WarpIteratorA warp_tile_iterator_A_;
 
+  /// Iterator to load a warp-scoped tile of A_scale from intermediate
+  /// accumulator tile (only used if ScaleOperandA_ is true)
+  WarpIteratorAScale warp_tile_iterator_A_scale_;
+
  public:
+  /// constructor for MMA with operand A scaling enabled.
+  CUTLASS_DEVICE
+  MmaPipelinedFromSharedMemory(
+      // shared storage needed for internal use by threadblock-scoped GEMM
+      typename Base::SharedStorage& shared_storage,
+      // warp iterator over A tile held in shared memory
+      WarpIteratorA warp_iter_a,
+      // warp iterator over A_scale tile held in shared memory
+      WarpIteratorAScale warp_iter_a_scale,
+      int thread_idx,
+      int warp_idx,
+      int lane_idx)
+      : Base(shared_storage, thread_idx, warp_idx, lane_idx),
+        warp_tile_iterator_A_(warp_iter_a),
+        warp_tile_iterator_A_scale_(warp_iter_a_scale),
+        smem_iterator_B_(shared_storage.operand_B_ref(), thread_idx) {
+    // Compute warp location within threadblock tile by mapping the warp_id to
+    // three coordinates:
+    //   _m: the warp's position within the threadblock along the M dimension
+    //   _n: the warp's position within the threadblock along the N dimension
+    //   _k: the warp's position within the threadblock along the K dimension
+    int warp_idx_mn = warp_idx % (Base::WarpCount::kM * Base::WarpCount::kN);
+    int warp_idx_k = warp_idx / (Base::WarpCount::kM * Base::WarpCount::kN);
+    int warp_idx_m = warp_idx_mn % Base::WarpCount::kM;
+    int warp_idx_n = warp_idx_mn / Base::WarpCount::kM;
+
+    // Add per-warp offsets in units of warp-level tiles
+    this->warp_tile_iterator_A_.add_tile_offset(
+        {warp_idx_m, Base::kWarpGemmIterations * warp_idx_k});
+    this->warp_tile_iterator_A_scale_.add_tile_offset(
+        {warp_idx_m, Base::kWarpGemmIterations * warp_idx_k});
+    this->warp_tile_iterator_B_.add_tile_offset(
+        {Base::kWarpGemmIterations * warp_idx_k, warp_idx_n});
+  }
+
   /// Construct from tensor references
   CUTLASS_DEVICE
   MmaPipelinedFromSharedMemory(
       typename Base::SharedStorage&
           shared_storage, ///< Shared storage needed for internal use by
                           ///< threadblock-scoped GEMM
       AccumulatorSharedStorage& accumulator_shared_storage,
@@ -380,15 +520,15 @@
         {Base::kWarpGemmIterations * warp_idx_k, warp_idx_n});
   }
 
   // For API compatibility with MmaMultistageFromSharedMemory
   // but not supported as it worsens perf: older gpus < sm80 don't
   // support async tranfers and have to waste registers
   CUTLASS_DEVICE
-  bool set_prologue_done(bool value) {}
+  void set_prologue_done(bool value) {}
   CUTLASS_DEVICE
   static void prologue(
       typename Base::SharedStorage& shared_storage,
       IteratorB iterator_B1,
       int thread_idx,
       int problem_size_0_n) {}
 
@@ -425,27 +565,34 @@
 
     this->smem_iterator_B_.store(transform_B(tb_frag_B));
 
     ++this->smem_iterator_B_;
 
     __syncthreads();
 
+    // remember that WarpFragmentAScale and WarpIteratorAScale are empty/no-op
+    // if scaling is disabled.
+
     // Pair of fragments used to overlap shared memory loads and math
     // instructions
     WarpFragmentA warp_frag_A[2];
+    WarpFragmentAScale warp_frag_A_scale[2];
     WarpFragmentB warp_frag_B[2];
     warp_frag_A[0].clear();
+    warp_frag_A_scale[0].clear();
     warp_frag_B[0].clear();
 
     this->warp_tile_iterator_B_.set_kgroup_index(0);
 
     this->warp_tile_iterator_A_.load(warp_frag_A[0]);
+    this->warp_tile_iterator_A_scale_.load(warp_frag_A_scale[0]);
     this->warp_tile_iterator_B_.load(warp_frag_B[0]);
 
     ++this->warp_tile_iterator_A_;
+    ++this->warp_tile_iterator_A_scale_;
     ++this->warp_tile_iterator_B_;
 
     Operator warp_mma;
 
     int smem_write_stage_idx = 1;
 
     // Avoid reading out of bounds
@@ -499,17 +646,20 @@
 
         // Only read the next if we need to
         if (hasNext) {
           this->warp_tile_iterator_B_.set_kgroup_index(
               (warp_mma_k + 1) % Base::kWarpGemmIterations);
 
           this->warp_tile_iterator_A_.load(warp_frag_A[(warp_mma_k + 1) % 2]);
+          this->warp_tile_iterator_A_scale_.load(
+              warp_frag_A_scale[(warp_mma_k + 1) % 2]);
           this->warp_tile_iterator_B_.load(warp_frag_B[(warp_mma_k + 1) % 2]);
 
           ++this->warp_tile_iterator_A_;
+          ++this->warp_tile_iterator_A_scale_;
           ++this->warp_tile_iterator_B_;
 
           if (warp_mma_k == 0) {
             iterator_B.load(tb_frag_B);
 
             ++iterator_B;
 
@@ -517,15 +667,16 @@
             iterator_B.set_residual_tile(gemm_k_iterations == 3);
             iterator_B.clear_mask(gemm_k_iterations <= 2);
           }
         }
 
         warp_mma(
             accum,
-            warp_frag_A[warp_mma_k % 2],
+            FragmentAScaler::apply(
+                warp_frag_A[warp_mma_k % 2], warp_frag_A_scale[warp_mma_k % 2]),
             warp_frag_B[warp_mma_k % 2],
             accum);
       }
     }
   }
 };
 
@@ -537,14 +688,18 @@
 /// Structure to compute the matrix product targeting CUDA cores and SIMT math
 /// instructions.
 template <
     /// Size of the Gemm problem - concept: gemm::GemmShape<>
     typename Shape1_,
     /// Iterates over the intermediate accumulator tile in shared memory
     typename WarpIteratorA1_,
+    /// whether or not to perform elementwise multiplication of A
+    //  by another matrix (A_scale) that is also kept in shared memory prior
+    //  to matmul A @ B
+    bool ScaleOperandA_,
     // Accumulator type
     typename AccumulatorSharedStorage,
     /// Iterates over tiles of B operand in global memory
     //  (concept: ReadableTileIterator | ForwardTileIterator |
     //  MaskedTileIterator)
     typename IteratorB1_,
     /// Iterates over tiles of B operand in shared memory
@@ -556,41 +711,42 @@
     typename ElementC_,
     /// Data type of accumulator matrix
     typename LayoutC_,
     /// Policy describing tuning details (concept: MmaPolicy)
     typename Policy1_,
     /// Number of stages,
     int Stages_,
+    int kMaxK_,
     /// Used for partial specialization
     typename Enable = bool>
-class MmaMultistageFromSharedMemory : public MmaBaseFromSharedMemory<
-                                          Shape1_,
-                                          AccumulatorSharedStorage::Shape::kN,
-                                          Policy1_,
-                                          Stages_> {
+class MmaMultistageFromSharedMemory
+    : public MmaBaseFromSharedMemory<Shape1_, kMaxK_, Policy1_, Stages_> {
  public:
   ///< Base class
-  using Base = MmaBaseFromSharedMemory<
-      Shape1_,
-      AccumulatorSharedStorage::Shape::kN,
-      Policy1_,
-      Stages_>;
+  using Base = MmaBaseFromSharedMemory<Shape1_, kMaxK_, Policy1_, Stages_>;
 
   ///< Size of the Gemm problem - concept: gemm::GemmShape<>
   using Shape1 = Shape1_;
   ///< Iterates over tiles of B operand in global memory
   using IteratorB1 = IteratorB1_;
   using IteratorB = IteratorB1;
   ///< Policy describing tuning details
   using Policy1 = Policy1_;
 
   using SmemIteratorB1 = SmemIteratorB1_;
   using WarpIteratorA1 = WarpIteratorA1_; ///< Iterates over the intermediate
                                           ///< accumulator tile in shared memory
+  static constexpr bool ScaleOperandA = ScaleOperandA_;
 
+  ///< warp level iterator over A_scale matrix tile kept in shared memory.
+  ///< if elementwise A scaling is disabled then everything this does is no-op.
+  using WarpIteratorAScale = typename cutlass::platform::conditional<
+      ScaleOperandA,
+      WarpIteratorA1,
+      NoOpWarpIteratorScale<typename WarpIteratorA1::TensorRef>>::type;
   ///< Data type of accumulator matrix
   using ElementC = ElementC_;
   ///< Layout of accumulator matrix
   using LayoutC = LayoutC_;
 
   static cutlass::arch::CacheOperation::Kind const kCacheOpB1 = CacheOpB1;
   static constexpr bool kSmemContainsEntireB = Base::kSmemContainsEntireB;
@@ -630,33 +786,85 @@
   };
 
   static constexpr int kNumStagesConcurrentLoad =
       kSmemContainsEntireB ? Base::kStages : Base::kStages - 1;
 
  private:
   using WarpLoadedFragmentA1 = typename Operator1::FragmentA;
+  /// fragment of OperandA scale matrix. if operand A scaling is disabled this
+  /// is (almost) empty.
+  using WarpLoadedFragmentA1Scale = typename WarpIteratorAScale::Fragment;
   using WarpLoadedFragmentB1 = typename Operator1::FragmentB;
   using WarpTransformedFragmentA1 = typename Operator1::TransformedFragmentA;
   using WarpTransformedFragmentB1 = typename Operator1::TransformedFragmentB;
 
+  /// applies elementwise scaling to fragment of A. if operand A scaling is
+  /// disabled this is a no-op.
+  using FragmentAScaler = FragmentElementwiseScaler<
+      WarpLoadedFragmentA1,
+      WarpLoadedFragmentA1Scale,
+      ScaleOperandA>;
+
  private:
   //
   // Data members
   //
 
   /// Iterator to load a warp-scoped tile of A1 operand from intermediate
   /// accumulator tile
   WarpIteratorA1 warp_tile_iterator_A1_;
 
+  /// Iterator to load a warp-scoped tile of A1_scale operand from shared memory
+  /// if operand A scaling is disabled everything this does is a no-op.
+  WarpIteratorAScale warp_tile_iterator_A1_scale_;
+
   /// Iterator to write threadblock-scoped tile of B operand to shared memory
   SmemIteratorB1 smem_iterator_B1_;
 
   bool prologue_done_;
 
  public:
+  /// constructor for MMA with operand A scaling enabled.
+  CUTLASS_DEVICE
+  MmaMultistageFromSharedMemory(
+      // shared storage needed for internal use by threadblock-scoped GEMM
+      typename Base::SharedStorage& shared_storage,
+      // warp level iterator over operand A tile kept in shared memory
+      WarpIteratorA1 warp_tile_iterator_A1,
+      // warp level iterator over operand A elementwise scale tile kept in
+      // shared memory.
+      WarpIteratorAScale warp_tile_iterator_A1_scale,
+      int thread_idx,
+      int warp_idx,
+      int lane_idx)
+      : Base(shared_storage, thread_idx, warp_idx, lane_idx),
+        warp_tile_iterator_A1_(warp_tile_iterator_A1),
+        warp_tile_iterator_A1_scale_(warp_tile_iterator_A1_scale),
+        smem_iterator_B1_(shared_storage.operand_B_ref(), thread_idx),
+        prologue_done_(false) {
+    // Compute warp location within threadblock tile by mapping the warp_id to
+    // three coordinates:
+    //   _m: the warp's position within the threadblock along the M dimension
+    //   _n: the warp's position within the threadblock along the N dimension
+    //   _k: the warp's position within the threadblock along the K dimension
+    int warp_idx_mn_1 =
+        warp_idx % (Base::WarpCount1::kM * Base::WarpCount1::kN);
+    int warp_idx_k_1 = warp_idx / (Base::WarpCount1::kM * Base::WarpCount1::kN);
+    int warp_idx_m_1 = warp_idx_mn_1 % Base::WarpCount1::kM;
+    int warp_idx_n_1 = warp_idx_mn_1 / Base::WarpCount1::kM;
+
+    // Add per-warp offsets in units of warp-level tiles
+    warp_tile_iterator_A1_.add_tile_offset(
+        {warp_idx_m_1, Base::kWarpGemmIterations1 * warp_idx_k_1});
+    warp_tile_iterator_A1_scale_.add_tile_offset(
+        {warp_idx_m_1, Base::kWarpGemmIterations1 * warp_idx_k_1});
+    this->warp_tile_iterator_B_.add_tile_offset(
+        {Base::kWarpGemmIterations1 * warp_idx_k_1, warp_idx_n_1});
+  }
+
   /// Construct from tensor references
   CUTLASS_DEVICE
   MmaMultistageFromSharedMemory(
       typename Base::SharedStorage&
           shared_storage, ///< Shared storage needed for internal use by
                           ///< threadblock-scoped GEMM
       AccumulatorSharedStorage& accumulator_shared_storage,
@@ -691,15 +899,15 @@
     warp_tile_iterator_A1_.add_tile_offset(
         {warp_idx_m_1, Base::kWarpGemmIterations1 * warp_idx_k_1});
     this->warp_tile_iterator_B_.add_tile_offset(
         {Base::kWarpGemmIterations1 * warp_idx_k_1, warp_idx_n_1});
   }
 
   CUTLASS_DEVICE
-  bool set_prologue_done(bool value) {
+  void set_prologue_done(bool value) {
     prologue_done_ = value;
   }
 
   CUTLASS_DEVICE
   static void prologue(
       typename Base::SharedStorage& shared_storage,
       IteratorB iterator_B1,
@@ -844,37 +1052,45 @@
       iterator_B1.clear_mask(gemm_k_iterations_1 <= 0);
     }
 
     // DEPBAR+SYNC
     cutlass::arch::cp_async_wait<kNumStagesConcurrentLoad - 1>();
     __syncthreads();
 
+    // remember that WarpFragmentAScale and WarpIteratorAScale are no-op/empty
+    // if scaling is disabled.
+
     // Pair of fragments used to overlap shared memory loads and math
     // instructions
     WarpLoadedFragmentA1 warp_loaded_frag_A1[2];
+    WarpLoadedFragmentA1Scale warp_loaded_frag_A1_scale[2];
     WarpLoadedFragmentB1 warp_loaded_frag_B1[2];
     WarpTransformedFragmentA1 warp_transformed_frag_A1[2];
     WarpTransformedFragmentB1 warp_transformed_frag_B1[2];
 
     Operator1 warp_mma1;
 
     warp_tile_iterator_A1_.load(warp_loaded_frag_A1[0]);
     ++warp_tile_iterator_A1_;
 
+    warp_tile_iterator_A1_scale_.load(warp_loaded_frag_A1_scale[0]);
+    ++warp_tile_iterator_A1_scale_;
+
     this->warp_tile_iterator_B_.set_kgroup_index(0);
     this->warp_tile_iterator_B_.load(warp_loaded_frag_B1[0]);
     ++this->warp_tile_iterator_B_;
 
     int smem_write_stage_idx = Base::kStages - 1;
     int smem_read_stage_idx = 0;
 
     warp_mma1.transform(
         warp_transformed_frag_A1[0],
         warp_transformed_frag_B1[0],
-        warp_loaded_frag_A1[0],
+        FragmentAScaler::apply(
+            warp_loaded_frag_A1[0], warp_loaded_frag_A1_scale[0]),
         warp_loaded_frag_B1[0]);
 
     // tf32x3 kernels use staging accumulation. warp_mma uses a temporary
     // accumulator and this temporary accumulator is added to the final
     // accumulator once in every mainloop iteration.
     plus<FragmentC1> plus_accum;
 
@@ -911,25 +1127,30 @@
         this->warp_tile_iterator_B_.set_kgroup_index(
             (warp_mma_k + 1) % Base::kWarpGemmIterations1);
         // skip warp tile loading for the last kgroup (we are out of the buf)
         if (gemm_k_iterations_1 > (-Base::kStages + 2) ||
             warp_mma_k < Base::kWarpGemmIterations1 - 1) {
           warp_tile_iterator_A1_.load(
               warp_loaded_frag_A1[(warp_mma_k + 1) % 2]);
+          warp_tile_iterator_A1_scale_.load(
+              warp_loaded_frag_A1_scale[(warp_mma_k + 1) % 2]);
           this->warp_tile_iterator_B_.load(
               warp_loaded_frag_B1[(warp_mma_k + 1) % 2]);
         }
         ++warp_tile_iterator_A1_;
+        ++warp_tile_iterator_A1_scale_;
         ++this->warp_tile_iterator_B_;
 
         if (warp_mma_k > 0)
           warp_mma1.transform(
               warp_transformed_frag_A1[warp_mma_k % 2],
               warp_transformed_frag_B1[warp_mma_k % 2],
-              warp_loaded_frag_A1[warp_mma_k % 2],
+              FragmentAScaler::apply(
+                  warp_loaded_frag_A1[warp_mma_k % 2],
+                  warp_loaded_frag_A1_scale[warp_mma_k % 2]),
               warp_loaded_frag_B1[warp_mma_k % 2]);
 
         if (platform::is_same<
                 typename Operator1::MathOperator,
                 arch::OpMultiplyAddFastF32>::value ||
             platform::is_same<
                 typename Operator1::MathOperator,
@@ -1011,15 +1232,17 @@
 
         // Do any conversions feeding the first stage at the end of the loop so
         // we can start right away on mma instructions
         if (warp_mma_k + 1 == Base::kWarpGemmIterations1)
           warp_mma1.transform(
               warp_transformed_frag_A1[(warp_mma_k + 1) % 2],
               warp_transformed_frag_B1[(warp_mma_k + 1) % 2],
-              warp_loaded_frag_A1[(warp_mma_k + 1) % 2],
+              FragmentAScaler::apply(
+                  warp_loaded_frag_A1[(warp_mma_k + 1) % 2],
+                  warp_loaded_frag_A1_scale[(warp_mma_k + 1) % 2]),
               warp_loaded_frag_B1[(warp_mma_k + 1) % 2]);
       }
     }
 
     if (platform::is_same<
             typename Operator1::MathOperator,
             arch::OpMultiplyAddFastF32>::value ||
@@ -1031,24 +1254,47 @@
   }
 };
 
 template <
     typename WarpShape,
     typename InstructionShape,
     typename RegularWarpIterator,
-    typename Policy>
+    typename Policy,
+    typename Enable = void>
 struct DefaultWarpIteratorAFromSharedMemory {};
 
-// TensorOp - Ampere
+// TensorOp - Ampere half
+template <typename RegularWarpIterator, typename Policy>
+struct DefaultWarpIteratorAFromSharedMemory<
+    cutlass::gemm::GemmShape<32, 32, 32>,
+    cutlass::gemm::GemmShape<16, 8, 8>,
+    RegularWarpIterator,
+    Policy,
+    typename platform::enable_if<(
+        sizeof_bits<typename RegularWarpIterator::Element>::value == 16 &&
+        Policy::Operator::Policy::OpDelta::kRow == 1)>::type> {
+  static constexpr auto kWarpSize = 32;
+  using OpDelta = typename Policy::Operator::Policy::OpDelta;
+  using WarpShape = cutlass::MatrixShape<32, 32>;
+
+  using WarpIterator = cutlass::gemm::warp::WarpIteratorFromSmem<
+      cutlass::gemm::Operand::kA,
+      typename RegularWarpIterator::Element>;
+};
+
+// TensorOp - Ampere f32
 template <typename WarpShape, typename RegularWarpIterator, typename Policy>
 struct DefaultWarpIteratorAFromSharedMemory<
     WarpShape,
     cutlass::gemm::GemmShape<16, 8, 8>,
     RegularWarpIterator,
-    Policy> {
+    Policy,
+    typename platform::enable_if<(
+        sizeof_bits<typename RegularWarpIterator::Element>::value != 16 ||
+        Policy::Operator::Policy::OpDelta::kRow != 1)>::type> {
   using InstructionShape = cutlass::gemm::GemmShape<16, 8, 8>;
   static constexpr auto kWarpSize = 32;
   using OpDelta = typename Policy::Operator::Policy::OpDelta;
 
   using WarpIterator =
       cutlass::gemm::warp::MmaTensorOpMultiplicandTileAccessIterator<
           cutlass::MatrixShape<WarpShape::kM, WarpShape::kK>,
@@ -1095,15 +1341,21 @@
 
   // We just use the same iterator, as we reproduced the same shared-memory
   // schema. Just modify it to handle non-complete tiles.
   using WarpIterator = RegularWarpIterator;
 };
 
 // Converts a "regular" Mma into their counterpart from shared memory
-template <typename Mma_, typename AccumulatorSharedStorage>
+template <
+    typename Mma_,
+    typename AccumulatorSharedStorage,
+    /// whether or not to apply elementwise multiplication of operand A by
+    /// another matrix in shared memory before usage in A @ B
+    bool kScaleOperandA,
+    bool kTransposeA = false>
 struct DefaultMmaFromSharedMemory;
 
 // Mma pipelined
 template <
     /// Size of the Gemm problem - concept: gemm::GemmShape<>
     typename Shape_,
     /// Iterates over tiles of A operand in global memory
@@ -1126,28 +1378,34 @@
     typename LayoutC_,
     /// Policy describing tuning details (concept: MmaPolicy)
     typename Policy_,
     /// Transformation applied to A operand
     typename TransformA_,
     /// Transformation applied to B operand
     typename TransformB_,
-    typename AccumulatorSharedStorage_>
+    typename AccumulatorSharedStorage_,
+    /// whether or not to apply elementwise multiplication of operand A by
+    /// another matrix in shared memory before usage in A @ B
+    bool kScaleOperandA,
+    bool kTransposeA>
 struct DefaultMmaFromSharedMemory<
     MmaPipelined<
         Shape_,
         IteratorA_,
         SmemIteratorA_,
         IteratorB_,
         SmemIteratorB_,
         ElementC_,
         LayoutC_,
         Policy_,
         TransformA_,
         TransformB_>,
-    AccumulatorSharedStorage_> {
+    AccumulatorSharedStorage_,
+    kScaleOperandA,
+    kTransposeA> {
   static constexpr int kWarpSize = 32;
   using SmemAccumulatorLayout = cutlass::layout::RowMajor;
 
   using RegularMma = MmaPipelined<
       Shape_,
       IteratorA_,
       SmemIteratorA_,
@@ -1159,26 +1417,28 @@
       TransformA_,
       TransformB_>;
 
   using WarpShape = typename Policy_::Operator::Shape;
   using InstructionShape = typename Policy_::Operator::InstructionShape;
   using ArchMmaOperator = typename Policy_::Operator;
 
+  static constexpr bool kIsTransposedA = false;
   using WarpIteratorA = typename DefaultWarpIteratorAFromSharedMemory<
       WarpShape,
       InstructionShape,
       typename RegularMma::Operator::IteratorA,
       Policy_>::WarpIterator;
   using IteratorB =
       typename cutlass::transform::threadblock::MakeIteratorResidualLast<
           IteratorB_>::Iterator;
 
   using Mma = typename cutlass::gemm::threadblock::MmaPipelinedFromSharedMemory<
       Shape_,
       WarpIteratorA,
+      kScaleOperandA,
       AccumulatorSharedStorage_,
       IteratorB,
       SmemIteratorB_,
       ElementC_,
       LayoutC_,
       Policy_>;
 };
@@ -1210,30 +1470,36 @@
     typename LayoutC_,
     /// Policy describing tuning details (concept: MmaPolicy)
     typename Policy_,
     /// Number of stages,
     int Stages,
     /// Use zfill or predicate for out-of-bound cp.async
     SharedMemoryClearOption SharedMemoryClear,
-    typename AccumulatorSharedStorage_>
+    typename AccumulatorSharedStorage_,
+    /// whether or not to apply elementwise multiplication of operand A by
+    /// another matrix in shared memory before usage in A @ B
+    bool kScaleOperandA,
+    bool kTransposeA>
 struct DefaultMmaFromSharedMemory<
     MmaMultistage<
         Shape_,
         IteratorA_,
         SmemIteratorA_,
         CacheOpA,
         IteratorB_,
         SmemIteratorB_,
         CacheOpB,
         ElementC_,
         LayoutC_,
         Policy_,
         Stages,
         SharedMemoryClear>,
-    AccumulatorSharedStorage_> {
+    AccumulatorSharedStorage_,
+    kScaleOperandA,
+    kTransposeA> {
   static constexpr int kWarpSize = 32;
 
   using RegularMma = MmaMultistage<
       Shape_,
       IteratorA_,
       SmemIteratorA_,
       CacheOpA,
@@ -1244,41 +1510,52 @@
       LayoutC_,
       Policy_,
       Stages,
       SharedMemoryClear>;
 
   using WarpShape = typename Policy_::Operator::Shape;
   using InstructionShape = typename Policy_::Operator::InstructionShape;
-  using WarpIteratorA = typename DefaultWarpIteratorAFromSharedMemory<
+  using WarpIteratorA_ = typename DefaultWarpIteratorAFromSharedMemory<
       WarpShape,
       InstructionShape,
       typename RegularMma::Operator::IteratorA,
       Policy_>::WarpIterator;
-
-  static int constexpr kMaxK = AccumulatorSharedStorage_::Shape::kN;
+  using WarpIteratorTranspose = TransposeWarpIterator<WarpIteratorA_>;
+  static constexpr bool kIsTransposedA =
+      WarpIteratorTranspose::kSupportsTranspose && kTransposeA;
+  using WarpIteratorA = typename platform::conditional<
+      kIsTransposedA,
+      typename WarpIteratorTranspose::Iterator,
+      WarpIteratorA_>::type;
+
+  static int constexpr kMaxK = kIsTransposedA
+      ? AccumulatorSharedStorage_::Shape::kM
+      : AccumulatorSharedStorage_::Shape::kN;
   // Reduce the number of stages if we don't need that many
   static int constexpr kStagesMax =
       (kMaxK + int(Shape_::kK) - 1) / int(Shape_::kK);
   static int constexpr kStages = cutlass::const_min(Stages, kStagesMax);
 
   using IteratorB =
       typename cutlass::transform::threadblock::MakeIteratorResidualLast<
           IteratorB_>::Iterator;
   using Mma =
       typename cutlass::gemm::threadblock::MmaMultistageFromSharedMemory<
           Shape_,
           WarpIteratorA,
+          kScaleOperandA,
           AccumulatorSharedStorage_,
           IteratorB,
           SmemIteratorB_,
           RegularMma::kCacheOpB,
           ElementC_,
           LayoutC_,
           Policy_,
-          kStages>;
+          kStages,
+          kMaxK>;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
     typename IteratorC,
     typename Operator,
@@ -1598,26 +1875,25 @@
       int warp_id,
       int lane_id,
       cutlass::MatrixCoord const& tile_coords) {
     // Non-optimized way to apply LSE to registers
     // NOTE: accum is attn.T
     // TODO: Optimize for each architecture
     static constexpr int WarpSize = 32;
-    using RegistersIter = typename DefaultAttentionScalingCoefsUpdater<
-        IteratorC,
-        accum_t,
-        WarpSize>::Updater;
+    using AccumLambdaIterator =
+        typename DefaultMmaAccumLambdaIterator<IteratorC, accum_t, WarpSize>::
+            Iterator;
     auto lane_offset =
-        RegistersIter::get_lane_offset(lane_id, warp_id, tile_coords);
+        AccumLambdaIterator::get_lane_offset(lane_id, warp_id, tile_coords);
 
     cutlass::Array<lse_scalar_t, IteratorC::Fragment::kElements> lse_prefetched;
     lse_prefetched.clear();
     int rowIdx = 0;
     int colIdx = 0;
-    RegistersIter::iterateRows(
+    AccumLambdaIterator::iterateRows(
         lane_offset,
         [&](int accum_m) {
           ++rowIdx;
           colIdx = 0;
         },
         [&](int accum_m, int accum_n, int idx) {
           if (rowIdx == 1) {
@@ -1738,26 +2014,25 @@
       int warp_id,
       int lane_id,
       cutlass::MatrixCoord const& tile_coords) {
     // Non-optimized way to apply LSE to registers
     // NOTE: accum is attn.T
     // TODO: Optimize for each architecture
     static constexpr int WarpSize = 32;
-    using RegistersIter = typename DefaultAttentionScalingCoefsUpdater<
-        IteratorC,
-        accum_t,
-        WarpSize>::Updater;
+    using AccumLambdaIterator =
+        typename DefaultMmaAccumLambdaIterator<IteratorC, accum_t, WarpSize>::
+            Iterator;
     auto lane_offset =
-        RegistersIter::get_lane_offset(lane_id, warp_id, tile_coords);
+        AccumLambdaIterator::get_lane_offset(lane_id, warp_id, tile_coords);
 
     cutlass::Array<lse_scalar_t, IteratorC::Fragment::kElements> lse_prefetched;
     lse_prefetched.clear();
     int rowIdx = 0;
     int colIdx = 0;
-    RegistersIter::iterateRows(
+    AccumLambdaIterator::iterateRows(
         lane_offset,
         [&](int accum_m) {
           ++rowIdx;
           colIdx = 0;
         },
         [&](int accum_m, int accum_n, int idx) {
           if (rowIdx == 1) {
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/42_ampere_tensorop_group_conv/ampere_tensorop_group_conv.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/42_ampere_tensorop_group_conv/ampere_tensorop_group_conv.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/43_ell_block_sparse_gemm/ell_block_sparse_gemm.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/43_ell_block_sparse_gemm/ell_block_sparse_gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_bias_act_epilogue_tensor_op.h` & `flash_attn-2.0.0/csrc/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_bias_act_epilogue_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_thread_map_tensor_op_for_fused_bias.h` & `flash_attn-2.0.0/csrc/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_thread_map_tensor_op_for_fused_bias.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/fused_bias_act_epilogue.h` & `flash_attn-2.0.0/csrc/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/fused_bias_act_epilogue.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/output_tile_thread_map_for_fused_bias.h` & `flash_attn-2.0.0/csrc/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/output_tile_thread_map_for_fused_bias.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/warp/fused_bias_act_fragment_iterator_tensor_op.h` & `flash_attn-2.0.0/csrc/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/warp/fused_bias_act_fragment_iterator_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/warp/mma_tensor_op_fragment_iterator_without_output_op.h` & `flash_attn-2.0.0/csrc/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/warp/mma_tensor_op_fragment_iterator_without_output_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/leaky_bias.h` & `flash_attn-2.0.0/csrc/cutlass/examples/44_multi_gemm_ir_and_codegen/leaky_bias.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/utils.h` & `flash_attn-2.0.0/csrc/cutlass/examples/44_multi_gemm_ir_and_codegen/utils.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/device/dual_gemm.h` & `flash_attn-2.0.0/csrc/cutlass/examples/45_dual_gemm/device/dual_gemm.h`

 * *Files 6% similar despite different names*

```diff
@@ -48,14 +48,15 @@
 
 #include "cutlass/gemm/device/default_gemm_configuration.h"
 #include "cutlass/gemm/threadblock/default_mma.h"
 #include "cutlass/epilogue/thread/linear_combination_relu.h"
 #include "cutlass/epilogue/threadblock/default_epilogue_tensor_op.h"
 
 #include "../kernel/dual_gemm.h"
+#include "../dual_gemm_common.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace device {
 
@@ -64,16 +65,18 @@
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
     typename LayoutA_,
     /// Element type for B matrix operand
     typename ElementB_,
-    /// Layout type for B matrix operand
-    typename LayoutB_,
+    /// Layout type for B0 matrix operand
+    typename LayoutB0_,
+    /// Layout type for B1 matrix operand
+    typename LayoutB1_,
     /// Element type for C and D matrix operands
     typename ElementC_,
     /// Layout type for C and D matrix operands
     typename LayoutC_,
     /// Element type for internal accumulation
     typename ElementAccumulator_,
     /// Operator class tag
@@ -115,16 +118,18 @@
 class DualGemm {
  public:
 
   using ElementA = ElementA_;
   using LayoutA = LayoutA_;
   using TensorRefA = TensorRef<ElementA const, LayoutA>;
   using ElementB = ElementB_;
-  using LayoutB = LayoutB_;
-  using TensorRefB = TensorRef<ElementB const, LayoutB>;
+  using LayoutB0 = LayoutB0_;
+  using LayoutB1 = LayoutB1_;
+  using TensorRefB0 = TensorRef<ElementB const, LayoutB0>;
+  using TensorRefB1 = TensorRef<ElementB const, LayoutB1>;
   using ElementC = ElementC_;
   using LayoutC = LayoutC_;
   using TensorRefC = TensorRef<ElementC const, LayoutC>;
   using TensorRefD = TensorRef<ElementC, LayoutC>;
   using ElementAccumulator = ElementAccumulator_;
   using OperatorClass = OperatorClass_;
   using ArchTag = ArchTag_;
@@ -147,44 +152,52 @@
   static ComplexTransform const kTransformB = ComplexTransform::kNone;
 
   using LayoutScaleBias = layout::RowMajor;
   /// Define the kernel
   /// Define the threadblock-scoped matrix multiply-accumulate
   static_assert(ArchTag::kMinComputeCapability >= 80, "Only multistage is implemented");
   static_assert(kStages >= 3, "Only multistage is implemented");
-  using Mma = typename cutlass::gemm::threadblock::DefaultMma<
-      ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB,
+  using Mma0 = typename cutlass::gemm::threadblock::DefaultMma<
+      ElementA, LayoutA, kAlignmentA, ElementB, LayoutB0, kAlignmentB,
+      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, ArchTag,
+      ThreadblockShape, WarpShape, 
+      InstructionShape, Stages, Operator>::ThreadblockMma;
+  using Mma1 = typename cutlass::gemm::threadblock::DefaultMma<
+      ElementA, LayoutA, kAlignmentA, ElementB, LayoutB1, kAlignmentB,
       ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, ArchTag,
       ThreadblockShape, WarpShape, 
       InstructionShape, Stages, Operator>::ThreadblockMma;
   using DualMma = threadblock::DualMmaMultistage<
-    typename Mma::Shape,
-    typename Mma::IteratorA,
-    typename Mma::SmemIteratorA,
-    Mma::kCacheOpA,
-    typename Mma::IteratorB,
-    typename Mma::SmemIteratorB,
-    Mma::kCacheOpB,
-    typename Mma::ElementC,
-    typename Mma::LayoutC,
-    typename Mma::Policy,
-    Mma::kStages,
+    typename Mma0::Shape,
+    typename Mma0::IteratorA,
+    typename Mma0::SmemIteratorA,
+    Mma0::kCacheOpA,
+    typename Mma0::IteratorB,
+    typename Mma0::SmemIteratorB,
+    Mma0::kCacheOpB,
+    typename Mma1::IteratorB,
+    typename Mma1::SmemIteratorB,
+    typename Mma0::ElementC,
+    typename Mma0::LayoutC,
+    typename Mma0::Policy,
+    typename Mma1::Policy,
+    Mma0::kStages,
     SharedMemoryClearOption::kNone
   >;
 
   static const int kPartitionsK = ThreadblockShape::kK / WarpShape::kK;
 
   /// Define the epilogue
   using Epilogue0 =
       typename cutlass::epilogue::threadblock::DefaultEpilogueTensorOp<
-          ThreadblockShape, typename DualMma::Operator, kPartitionsK, EpilogueOutputOp0,
+          ThreadblockShape, typename DualMma::Operator0, kPartitionsK, EpilogueOutputOp0,
           EpilogueOutputOp0::kCount>::Epilogue;
   using Epilogue1 =
       typename cutlass::epilogue::threadblock::DefaultEpilogueTensorOp<
-          ThreadblockShape, typename DualMma::Operator, kPartitionsK, EpilogueOutputOp1,
+          ThreadblockShape, typename DualMma::Operator1, kPartitionsK, EpilogueOutputOp1,
           EpilogueOutputOp1::kCount>::Epilogue;
 
   /// Define the kernel-level GEMM operator.
   using DualGemmKernel = kernel::DualGemm<
     DualMma,
     Epilogue0, Epilogue1, EpilogueOutputOp2,
     ThreadblockSwizzle, kSplitKSerial,
@@ -193,71 +206,93 @@
   /// Argument structure
   struct Arguments {
 
     //
     // Data members
     //
 
+    DualGemmMode mode;
     GemmCoord problem_size;
     TensorRef<ElementA const, LayoutA> ref_A0;
-    TensorRef<ElementB const, LayoutB> ref_B0;
+    TensorRef<ElementB const, LayoutB0> ref_B0;
     TensorRef<ElementC const, LayoutC> ref_C0;
     TensorRef<ElementC, LayoutC> ref_D0;
-    TensorRef<ElementB const, LayoutB> ref_B1;
+    TensorRef<ElementB const, LayoutB1> ref_B1;
     TensorRef<ElementC const, LayoutC> ref_C1;
     TensorRef<ElementC, LayoutC> ref_D1;
     TensorRef<ElementC, LayoutC> ref_D2;
     typename EpilogueOutputOp0::Params epilogue0;
     typename EpilogueOutputOp1::Params epilogue1;
     typename EpilogueOutputOp2::Params epilogue2;
     int split_k_slices;
 
+    int batch_count;
+    int64_t batch_stride_A;
+    int64_t batch_stride_B0;
+    int64_t batch_stride_B1;
+    int64_t batch_stride_C;
+    int64_t batch_stride_D;
+
     //
     // Methods
     //
 
     /// Default ctor
     CUTLASS_HOST_DEVICE
     Arguments(): problem_size(0, 0, 0), split_k_slices(1) {
 
     }
 
     /// Constructs an Arguments structure 
     CUTLASS_HOST_DEVICE
     Arguments(
+      DualGemmMode mode,
       GemmCoord problem_size_,
       TensorRef<ElementA const, LayoutA> ref_A0_,
-      TensorRef<ElementB const, LayoutB> ref_B0_,
+      TensorRef<ElementB const, LayoutB0> ref_B0_,
       TensorRef<ElementC const, LayoutC> ref_C0_,
       TensorRef<ElementC, LayoutC> ref_D0_,
-      TensorRef<ElementB const, LayoutB> ref_B1_,
+      TensorRef<ElementB const, LayoutB1> ref_B1_,
       TensorRef<ElementC const, LayoutC> ref_C1_,
       TensorRef<ElementC, LayoutC> ref_D1_,
       TensorRef<ElementC, LayoutC> ref_D2_,
       typename EpilogueOutputOp0::Params epilogue0_ =
         typename EpilogueOutputOp0::Params(),
       typename EpilogueOutputOp1::Params epilogue1_ =
         typename EpilogueOutputOp1::Params(),
       typename EpilogueOutputOp2::Params epilogue2_ =
         typename EpilogueOutputOp2::Params(),
-      int split_k_slices_ = 1
+      int split_k_slices_ = 1,
+      int batch_count = 1,
+      int64_t batch_stride_A = 0,
+      int64_t batch_stride_B0 = 0,
+      int64_t batch_stride_B1 = 0,
+      int64_t batch_stride_C = 0,
+      int64_t batch_stride_D = 0
     ):
+      mode(mode),
       problem_size(problem_size_),
       ref_A0(ref_A0_),
       ref_B0(ref_B0_),
       ref_C0(ref_C0_),
       ref_D0(ref_D0_),
       ref_B1(ref_B1_),
       ref_C1(ref_C1_),
       ref_D1(ref_D1_),
       ref_D2(ref_D2_),
       epilogue0(epilogue0_),
       epilogue1(epilogue1_),
       epilogue2(epilogue2_),
-      split_k_slices(split_k_slices_) {
+      split_k_slices(split_k_slices_),
+      batch_count(batch_count),
+      batch_stride_A(batch_stride_A),
+      batch_stride_B0(batch_stride_B0),
+      batch_stride_B1(batch_stride_B1),
+      batch_stride_C(batch_stride_C),
+      batch_stride_D(batch_stride_D) {
 
     }
   };
 
 private:
 
   /// Kernel parameters object
@@ -267,14 +302,17 @@
 
   /// Constructs the GEMM.
   DualGemm() = default;
 
   /// Determines whether the GEMM can execute the given problem.
   static Status can_implement(Arguments const &args) {
 
+    if (args.mode == DualGemmMode::kBatched && kSplitKSerial) {
+      return Status::kErrorInvalidProblem;
+    }
     if (!kSplitKSerial && args.split_k_slices > 1) {
       return Status::kErrorInvalidProblem;
     }
     if (kStoreD0 != (args.ref_D0.data() != nullptr)) {
       return Status::kErrorInternal;
     }
     if (kStoreD1 != (args.ref_D1.data() != nullptr)) {
@@ -300,25 +338,23 @@
     return Status::kSuccess;
   }
 
   /// Gets the workspace size
   static size_t get_workspace_size(Arguments const &args) {
 
     size_t bytes = 0;
-      
-    // Determine grid shape
-    ThreadblockSwizzle threadblock_swizzle;
-
-    cutlass::gemm::GemmCoord tiled_shape = threadblock_swizzle.get_tiled_shape(
-      args.problem_size, 
-      {ThreadblockShape::kM, ThreadblockShape::kN, ThreadblockShape::kK},
-      args.split_k_slices);
 
     if (kSplitKSerial && args.split_k_slices > 1) {
+      // Determine grid shape
+      ThreadblockSwizzle threadblock_swizzle;
 
+      cutlass::gemm::GemmCoord tiled_shape = threadblock_swizzle.get_tiled_shape(
+        args.problem_size, 
+        {ThreadblockShape::kM, ThreadblockShape::kN, ThreadblockShape::kK},
+        args.split_k_slices);
 
       bytes += sizeof(int) * size_t(tiled_shape.m()) * size_t(tiled_shape.n());
     }
 
     return bytes;
   }
 
@@ -327,15 +363,15 @@
 
     // Determine grid shape
     ThreadblockSwizzle threadblock_swizzle;
 
     cutlass::gemm::GemmCoord grid_shape = threadblock_swizzle.get_tiled_shape(
       args.problem_size, 
       {ThreadblockShape::kM, ThreadblockShape::kN, ThreadblockShape::kK},
-      args.split_k_slices);
+      args.mode == DualGemmMode::kBatched ? args.batch_count : args.split_k_slices);
 
     if (kSplitKSerial) {
       if (args.split_k_slices > 1) {
         if (!workspace) {
           return Status::kErrorWorkspaceNull;
         }
 
@@ -353,28 +389,34 @@
       if (args.split_k_slices > 1) {
         return Status::kErrorInvalidProblem;
       }
     }
 
     // Initialize the Params structure
     params_ = typename DualGemmKernel::Params{
+      args.mode,
       args.problem_size,
       grid_shape,
       args.ref_A0.non_const_ref(),
       args.ref_B0.non_const_ref(),
       args.ref_C0.non_const_ref(),
       args.ref_D0,
       args.ref_B1.non_const_ref(),
       args.ref_C1.non_const_ref(),
       args.ref_D1,
       args.ref_D2,
       args.epilogue0,
       args.epilogue1,
       args.epilogue2,
       reinterpret_cast<int *>(workspace),
+      args.batch_stride_A,
+      args.batch_stride_B0,
+      args.batch_stride_B1,
+      args.batch_stride_C,
+      args.batch_stride_D,
     };
 
     return Status::kSuccess;
   }
 
   /// Lightweight update given a subset of arguments
   Status update(Arguments const &args, void *workspace = nullptr) {
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_volta_tensor_op_f16_sm70.cu`

 * *Files 19% similar despite different names*

```diff
@@ -24,239 +24,244 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
 /*! \file
-    \brief CUTLASS Dual-GEMM Example.
-
-    Fused kernel that outputs `D0` and `D1`.
-    We assume that B0/B1 have the same shape/layout
-
-```
-D0 = epilogue0(X @ B0, C0)
-D1 = epilogue1(X @ B1, C1)
-D2 = element_wise(D0, D1)
-```
-    D0 and D1 will be optionally stored in gmem (`kStoreD0` / `kStoreD1`)
+    \brief Tests for device-wide GEMM interface
 */
 
-// #define IS_PROFILING
-
 #include <iostream>
 
 #include "cutlass/cutlass.h"
 #include "cutlass/gemm/device/gemm.h"
 
-#include "cutlass/util/host_tensor.h"
-#include "cutlass/util/tensor_view_io.h"
-#include "cutlass/util/reference/host/tensor_fill.h"
-#include "cutlass/util/reference/host/tensor_copy.h"
-#include "cutlass/util/reference/host/tensor_compare.h"
-#include "cutlass/util/reference/host/gemm.h"
-
-#include "device/dual_gemm.h"
-#include "thread/left_silu_and_mul.h"
-#include "dual_gemm_run.h"
-#include "test_run.h"
-
-
-////////////////////////////////////////////////////////////////////////////////
-
-cutlass::gemm::GemmCoord problem_size(4096, 4096, 8192);
-
-constexpr int kStages = 3;
-constexpr bool kSplitKSerial = false;
-constexpr bool kUseBias = true;
-
-
-#if 0
-using ElementOperandA = cutlass::bfloat16_t;
-using ElementOperandB = cutlass::bfloat16_t;
-using ElementOutput = cutlass::bfloat16_t;
-using ElementAccumulator = float;
-using ElementCompute = float;
-#else
-using ElementOperandA = cutlass::half_t;
-using ElementOperandB = cutlass::half_t;
-using ElementOutput = cutlass::half_t;
-using ElementAccumulator = cutlass::half_t;
-using ElementCompute = cutlass::half_t;
-#endif
+#include "../../common/cutlass_unit_test.h"
 
-constexpr auto kScaleType = kUseBias ? cutlass::epilogue::thread::ScaleType::NoBetaScaling : (
-  // No bias
-  kSplitKSerial ? cutlass::epilogue::thread::ScaleType::Default : cutlass::epilogue::thread::ScaleType::Nothing
-);
-using EpilogueOutputOp0 = cutlass::epilogue::thread::LinearCombination<
-  ElementOutput,
-  128 / cutlass::sizeof_bits<ElementOutput>::value,
-  ElementAccumulator,
-  ElementCompute,
-  kScaleType
->;
-using EpilogueOutputOp1 = cutlass::epilogue::thread::LinearCombination<
-  ElementOutput,
-  128 / cutlass::sizeof_bits<ElementOutput>::value,
-  ElementAccumulator,
-  ElementCompute,
-  kScaleType
->;
-using EpilogueOutputOp2 = cutlass::epilogue::thread::LeftSiLUAndMul<
-  ElementOutput,
-  128 / cutlass::sizeof_bits<ElementOutput>::value,
-  ElementOutput,
-  ElementCompute
->;
+#include "testbed.h"
 
-const ElementCompute alpha0 = ElementCompute(1);
-const ElementCompute beta0 = ElementCompute(kUseBias ? 1 : 0);
-const ElementCompute alpha1 = ElementCompute(1);
-const ElementCompute beta1 = ElementCompute(kUseBias ? 1 : 0);
+#if defined(CUTLASS_ARCH_MMA_SM70_SUPPORTED)
 
-bool run_nonfused_gemm_f16_sm80() {
-  using ThreadblockShape = cutlass::gemm::GemmShape<128, 128, 32>;
-  using WarpShape = cutlass::gemm::GemmShape<64, 64, 32>;
-  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 16>;
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-  using Gemm0 = cutlass::gemm::device::Gemm<
-    ElementOperandA,
-    cutlass::layout::RowMajor,
-    ElementOperandB,
+TEST(SM70_Device_Gemm_f16n_f16t_f16t_volta_tensor_op_f16, 128x256x32_64x64x32) {
+
+  using ElementOutput = cutlass::half_t;
+  using ElementAccumulator = cutlass::half_t;
+
+  using Gemm = cutlass::gemm::device::Gemm<
+    cutlass::half_t,
     cutlass::layout::ColumnMajor,
+    cutlass::half_t,
+    cutlass::layout::RowMajor,
     ElementOutput,
     cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    ThreadblockShape,
-    WarpShape,
-    InstructionShape,
-    EpilogueOutputOp0,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>,
-    kStages,
-    8,
-    8,
-    kSplitKSerial
+    cutlass::arch::Sm70,
+    cutlass::gemm::GemmShape<128, 256, 32>,
+    cutlass::gemm::GemmShape<64, 64, 32>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementOutput,
+      128 / cutlass::sizeof_bits<ElementOutput>::value,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    2
   >;
-  using Gemm1 = cutlass::gemm::device::Gemm<
-    ElementOperandA,
-    cutlass::layout::RowMajor,
-    ElementOperandB,
+  
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+}
+
+TEST(SM70_Device_Gemm_f16n_f16t_f16t_volta_tensor_op_f16, 256x128x32_64x64x32) {
+
+  using ElementOutput = cutlass::half_t;
+  using ElementAccumulator = cutlass::half_t;
+
+  using Gemm = cutlass::gemm::device::Gemm<
+    cutlass::half_t,
     cutlass::layout::ColumnMajor,
+    cutlass::half_t,
+    cutlass::layout::RowMajor,
     ElementOutput,
     cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    ThreadblockShape,
-    WarpShape,
-    InstructionShape,
-    EpilogueOutputOp1,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>,
-    kStages,
-    8,
-    8,
-    kSplitKSerial
+    cutlass::arch::Sm70,
+    cutlass::gemm::GemmShape<256, 128, 32>,
+    cutlass::gemm::GemmShape<64, 64, 32>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementOutput,
+      128 / cutlass::sizeof_bits<ElementOutput>::value,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    2
   >;
 
-  NonFusedDualGemmRun<Gemm0, Gemm1> nonFusedGemm;
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+}
+
+TEST(SM70_Device_Gemm_f16n_f16t_f16t_volta_tensor_op_f16, 128x128x32_64x64x32) {
 
-  std::cout << "Running Non-fused GEMMs FP16 TN GEMMs...\n";
-  bool pass = nonFusedGemm.run(problem_size, alpha0, beta0, alpha1, beta1);
-  if(pass)
-    std::cout << "Pass\n";
-  else
-    std::cout << "Fail\n";
+  using ElementOutput = cutlass::half_t;
+  using ElementAccumulator = cutlass::half_t;
 
-  return pass;
+  using Gemm = cutlass::gemm::device::Gemm<
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor,
+    cutlass::half_t,
+    cutlass::layout::RowMajor,
+    ElementOutput,
+    cutlass::layout::RowMajor,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm70,
+    cutlass::gemm::GemmShape<128, 128, 32>,
+    cutlass::gemm::GemmShape<64, 64, 32>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementOutput,
+      128 / cutlass::sizeof_bits<ElementOutput>::value,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    2
+  >;
+
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
-template <typename T>
-struct LeftSiLUAndMul {
-  struct Params{};
-  CUTLASS_HOST_DEVICE LeftSiLUAndMul(Params p) {}
+TEST(SM70_Device_Gemm_f16n_f16t_f16t_volta_tensor_op_f16, 128x64x32_64x32x32) {
 
-  CUTLASS_HOST_DEVICE void set_k_partition(int, int) {}
+  using ElementOutput = cutlass::half_t;
+  using ElementAccumulator = cutlass::half_t;
 
-  CUTLASS_HOST_DEVICE T operator() (
-    T const &lhs, 
-    T const &rhs) const {
-    cutlass::epilogue::thread::SiLu<T> silu;
-    cutlass::multiplies<T> mul;
-    auto silu_lhs = silu(lhs);
-    return mul(silu_lhs, rhs);
-  }
+  using Gemm = cutlass::gemm::device::Gemm<
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor,
+    cutlass::half_t,
+    cutlass::layout::RowMajor,
+    ElementOutput,
+    cutlass::layout::RowMajor,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm70,
+    cutlass::gemm::GemmShape<128, 64, 32>,
+    cutlass::gemm::GemmShape<64, 32, 32>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementOutput,
+      128 / cutlass::sizeof_bits<ElementOutput>::value,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    2
+  >;
 
-  template <int kCount>
-  CUTLASS_HOST_DEVICE cutlass::Array<T, kCount> operator() (
-    cutlass::Array<T, kCount> const &lhs, 
-    cutlass::Array<T, kCount> const &rhs) const {
-    cutlass::epilogue::thread::SiLu<T> silu;
-    cutlass::multiplies<T> mul;
-    auto silu_lhs = silu(lhs);
-    return mul(silu_lhs, rhs);
-  }
-};
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+}
 
-bool run_fused_gemm_f16_sm80_shmem() {
-  using ThreadblockShape = cutlass::gemm::GemmShape<128, 64, 32>;
-  using WarpShape = cutlass::gemm::GemmShape<64, 32, 32>;
-  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 16>;
+TEST(SM70_Device_Gemm_f16n_f16t_f16t_volta_tensor_op_f16, 64x128x32_32x64x32) {
 
-  // Optionally, we might not need intermediate GEMM outputs
-  constexpr bool kStoreD0 = true;
-  constexpr bool kStoreD1 = true;
+  using ElementOutput = cutlass::half_t;
+  using ElementAccumulator = cutlass::half_t;
 
-  using DualGemm = cutlass::gemm::device::DualGemm<
-    ElementOperandA,
-    cutlass::layout::RowMajor,
-    ElementOperandB,
+  using Gemm = cutlass::gemm::device::Gemm<
+    cutlass::half_t,
     cutlass::layout::ColumnMajor,
+    cutlass::half_t,
+    cutlass::layout::RowMajor,
     ElementOutput,
     cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    ThreadblockShape,
-    WarpShape,
-    InstructionShape,
-    EpilogueOutputOp0,
-    EpilogueOutputOp1,
-    EpilogueOutputOp2,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<1>,
-    kStages,
-    kStoreD0,
-    kStoreD1,
-    kSplitKSerial
+    cutlass::arch::Sm70,
+    cutlass::gemm::GemmShape<64, 128, 32>,
+    cutlass::gemm::GemmShape<32, 64, 32>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementOutput,
+      128 / cutlass::sizeof_bits<ElementOutput>::value,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    2
   >;
 
-  DualFusedGemmRun<DualGemm> fusedGemm;
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+}
 
-  std::cout << "Running Fused FP16 TN GEMMs + Epilogue2...\n";
-  bool passed = fusedGemm.run(problem_size, alpha0, beta0, alpha1, beta1);
-  if(passed)
-    std::cout << "Pass\n";
-  else
-    std::cout << "Fail\n";
+TEST(SM70_Device_Gemm_f16n_f16t_f16t_volta_tensor_op_f16, 64x64x32_64x64x32) {
 
-  return passed;
+  using ElementOutput = cutlass::half_t;
+  using ElementAccumulator = cutlass::half_t;
 
+  using Gemm = cutlass::gemm::device::Gemm<
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor,
+    cutlass::half_t,
+    cutlass::layout::RowMajor,
+    ElementOutput,
+    cutlass::layout::RowMajor,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm70,
+    cutlass::gemm::GemmShape<64, 64, 32>,
+    cutlass::gemm::GemmShape<64, 64, 32>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementOutput,
+      128 / cutlass::sizeof_bits<ElementOutput>::value,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    2
+  >;
+
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
-int main() {
+TEST(SM70_Device_Gemm_f16n_f16t_f16t_volta_tensor_op_f16, 64x64x32_32x32x32) {
 
-  std::vector<bool (*)()>funcs = {
-    &run_nonfused_gemm_f16_sm80,
-    &run_fused_gemm_f16_sm80_shmem
-  };
+  using ElementOutput = cutlass::half_t;
+  using ElementAccumulator = cutlass::half_t;
 
-  std::string test_name = "dual-gemm f16 bias=" + std::to_string(kUseBias) + " split_k_serial=" + std::to_string(kSplitKSerial);
-  return testRun(80, funcs, test_name);
-}
+  using Gemm = cutlass::gemm::device::Gemm<
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor,
+    cutlass::half_t,
+    cutlass::layout::RowMajor,
+    ElementOutput,
+    cutlass::layout::RowMajor,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm70,
+    cutlass::gemm::GemmShape<64, 64, 32>,
+    cutlass::gemm::GemmShape<32, 32, 32>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementOutput,
+      128 / cutlass::sizeof_bits<ElementOutput>::value,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    2
+  >;
 
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+}
 
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-////////////////////////////////////////////////////////////////////////////////
+#endif
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm_run.h` & `flash_attn-2.0.0/csrc/cutlass/examples/45_dual_gemm/dual_gemm_run.h`

 * *Files 19% similar despite different names*

```diff
@@ -29,25 +29,30 @@
  *
  **************************************************************************************************/
 #pragma once
 
 #include <iostream>
 #include <fstream>
 #include <sstream>
+#include <type_traits>
 
 #include "cutlass/util/host_tensor.h"
 #include "cutlass/util/tensor_view_io.h"
 #include "cutlass/util/distribution.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_norm.h"
 #include "cutlass/util/reference/device/gemm.h"
 #include "cutlass/util/reference/device/tensor_relu.h"
 
+#include "cutlass/gemm/gemm.h"
+#include "cutlass/gemm/device/gemm_universal.h"
+
+#include "dual_gemm_common.h"
 #include "helper.h"
 
 #define CHECK_GT(val1, val2) \
     if((val1) <= (val2)) \
         std::cerr << __FILE__ << " " << __LINE__ << ": CHECK_GT failed\n";
 #define CHECK_TRUE(val) \
     if(!(val)) \
@@ -201,14 +206,15 @@
   /// Executes one test
   bool run(
     cutlass::gemm::GemmCoord problem_size,
     ElementCompute alpha0 = ElementCompute(1),
     ElementCompute beta0 = ElementCompute(0),
     ElementCompute alpha1 = ElementCompute(1),
     ElementCompute beta1 = ElementCompute(0),
+    bool is_profiling = true,
     bool relu = false,
     int warm_ups = 1,
     int runs = 100) {
     
     //
     // Allocate the GEMM workspace
     //
@@ -331,49 +337,50 @@
 
     for(int i = 0; i < warm_ups; i++) {
         status = gemm_op_0();
         CUTLASS_CHECK(status);
         status = gemm_op_1();
         CUTLASS_CHECK(status);
     }
-#ifdef IS_PROFILING
-    return true;
-#endif
-    //
-    // Run the GEMM
-    //
-    cudaEvent_t start, stop1, stop2;
-    cudaEventCreate(&start);
-    cudaEventCreate(&stop1);
-    cudaEventCreate(&stop2);
 
-    cudaEventRecord(start);
-
-    for(int i = 0; i < runs; i++) {
-        status = gemm_op_0();
-    
-        CUTLASS_CHECK(status);
-    }
-    cudaEventRecord(stop1);
-    for(int i = 0; i < runs; i++) {
-        status = gemm_op_1();
-    
-        CUTLASS_CHECK(status);
+    if (is_profiling) {
+      //
+      // Profile the GEMM
+      //
+
+      cudaEvent_t start, stop1, stop2;
+      cudaEventCreate(&start);
+      cudaEventCreate(&stop1);
+      cudaEventCreate(&stop2);
+
+      cudaEventRecord(start);
+
+      for(int i = 0; i < runs; i++) {
+          status = gemm_op_0();
+      
+          CUTLASS_CHECK(status);
+      }
+      cudaEventRecord(stop1);
+      for(int i = 0; i < runs; i++) {
+          status = gemm_op_1();
+      
+          CUTLASS_CHECK(status);
+      }
+
+      cudaEventRecord(stop2);
+      cudaDeviceSynchronize();
+      float gemm0Time, gemm1Time, totalTime;
+      cudaEventElapsedTime(&gemm0Time, start, stop1);
+      cudaEventElapsedTime(&gemm1Time, stop1, stop2);
+      cudaEventElapsedTime(&totalTime, start, stop2);
+      std::cout << "gemm 0 time " << gemm0Time / (float)runs << " ms\n";
+      std::cout << "gemm 1 time " << gemm1Time / (float)runs << " ms\n";
+      std::cout << "Non-fusion GEMM only time " << totalTime / (float)runs << " ms\n";
     }
 
-    cudaEventRecord(stop2);
-    cudaDeviceSynchronize();
-    float gemm0Time, gemm1Time, totalTime;
-    cudaEventElapsedTime(&gemm0Time, start, stop1);
-    cudaEventElapsedTime(&gemm1Time, stop1, stop2);
-    cudaEventElapsedTime(&totalTime, start, stop2);
-    std::cout << "gemm 0 time " << gemm0Time / (float)runs << " ms\n";
-    std::cout << "gemm 1 time " << gemm1Time / (float)runs << " ms\n";
-    std::cout << "Non-fusion GEMM only time " << totalTime / (float)runs << " ms\n";
-
     tensor_D0.sync_host();
     tensor_D1.sync_host();
 
     //
     // Verify
     //
     cutlass::reference::device::Gemm<
@@ -539,73 +546,112 @@
   /// Executes one test
   bool run(
     cutlass::gemm::GemmCoord problem_size,
     ElementCompute alpha0 = ElementCompute(1),
     ElementCompute beta0 = ElementCompute(1),
     ElementCompute alpha1 = ElementCompute(1),
     ElementCompute beta1 = ElementCompute(1),
+    int batch_count = 1,
+    bool broadcast_b1 = false,
+    bool is_profiling = true,
     bool relu = false,
     int warm_ups = 1,
     int runs = 100) {
     
     //
     // Allocate the GEMM workspace
     //
 
     cutlass::HostTensor<
       typename DualGemm::ElementA,
-      typename DualGemm::LayoutA> tensor_A0(problem_size.mk());
+      typename DualGemm::LayoutA> tensor_A0(
+        std::is_same<typename DualGemm::LayoutA, cutlass::layout::RowMajor>::value ? 
+          cutlass::MatrixCoord(batch_count * problem_size.m(), problem_size.k()) : 
+          cutlass::MatrixCoord(problem_size.m(), batch_count * problem_size.k()));
 
     cutlass::HostTensor<
       typename DualGemm::ElementB,
-      typename DualGemm::LayoutB> tensor_B0(problem_size.kn());
+      typename DualGemm::LayoutB0> tensor_B0(
+        std::is_same<typename DualGemm::LayoutB0, cutlass::layout::RowMajor>::value ? 
+          cutlass::MatrixCoord(batch_count * problem_size.k(), problem_size.n()) : 
+          cutlass::MatrixCoord(problem_size.k(), batch_count * problem_size.n()));
 
     cutlass::HostTensor<
       typename DualGemm::ElementC,
-      typename DualGemm::LayoutC> tensor_C0(problem_size.mn());
+      typename DualGemm::LayoutC> tensor_C0(
+        std::is_same<typename DualGemm::LayoutC, cutlass::layout::RowMajor>::value ? 
+          cutlass::MatrixCoord(batch_count * problem_size.m(), problem_size.n()) : 
+          cutlass::MatrixCoord(problem_size.m(), batch_count * problem_size.n()));
 
     cutlass::HostTensor<
       typename DualGemm::ElementC,
-      typename DualGemm::LayoutScaleBias> tensor_Bias0({1, problem_size.n()});
+      typename DualGemm::LayoutScaleBias> tensor_Bias0({batch_count, problem_size.n()});
 
     cutlass::HostTensor<
       typename DualGemm::ElementC,
-      typename DualGemm::LayoutC> tensor_D0(problem_size.mn());
+      typename DualGemm::LayoutC> tensor_D0(
+        std::is_same<typename DualGemm::LayoutC, cutlass::layout::RowMajor>::value ? 
+          cutlass::MatrixCoord(batch_count * problem_size.m(), problem_size.n()) : 
+          cutlass::MatrixCoord(problem_size.m(), batch_count * problem_size.n()));
 
     cutlass::HostTensor<
       typename DualGemm::ElementC,
-      typename DualGemm::LayoutC> reference_D0(problem_size.mn());
+      typename DualGemm::LayoutC> reference_D0(
+        std::is_same<typename DualGemm::LayoutC, cutlass::layout::RowMajor>::value ? 
+          cutlass::MatrixCoord(batch_count * problem_size.m(), problem_size.n()) : 
+          cutlass::MatrixCoord(problem_size.m(), batch_count * problem_size.n()));
 
     cutlass::HostTensor<
       typename DualGemm::ElementB,
-      typename DualGemm::LayoutB> tensor_B1(problem_size.kn());
+      typename DualGemm::LayoutB1> tensor_B1(
+        std::is_same<typename DualGemm::LayoutB1, cutlass::layout::RowMajor>::value ? 
+          cutlass::MatrixCoord(batch_count * problem_size.k(), problem_size.n()) : 
+          cutlass::MatrixCoord(problem_size.k(), batch_count * problem_size.n()));
+    if (broadcast_b1) {
+      tensor_B1.resize({problem_size.k(), batch_count});
+    }
 
     cutlass::HostTensor<
       typename DualGemm::ElementC,
-      typename DualGemm::LayoutC> tensor_C1(problem_size.mn());
+      typename DualGemm::LayoutC> tensor_C1(
+        std::is_same<typename DualGemm::LayoutC, cutlass::layout::RowMajor>::value ? 
+          cutlass::MatrixCoord(batch_count * problem_size.m(), problem_size.n()) : 
+          cutlass::MatrixCoord(problem_size.m(), batch_count * problem_size.n()));
 
     cutlass::HostTensor<
       typename DualGemm::ElementC,
-      typename DualGemm::LayoutScaleBias> tensor_Bias1({1, problem_size.n()});
+      typename DualGemm::LayoutScaleBias> tensor_Bias1({batch_count, problem_size.n()});
 
     cutlass::HostTensor<
       typename DualGemm::ElementC,
-      typename DualGemm::LayoutC> tensor_D1(problem_size.mn());
+      typename DualGemm::LayoutC> tensor_D1(
+        std::is_same<typename DualGemm::LayoutC, cutlass::layout::RowMajor>::value ? 
+          cutlass::MatrixCoord(batch_count * problem_size.m(), problem_size.n()) : 
+          cutlass::MatrixCoord(problem_size.m(), batch_count * problem_size.n()));
 
     cutlass::HostTensor<
       typename DualGemm::ElementC,
-      typename DualGemm::LayoutC> tensor_D2(problem_size.mn());
+      typename DualGemm::LayoutC> tensor_D2(
+        std::is_same<typename DualGemm::LayoutC, cutlass::layout::RowMajor>::value ? 
+          cutlass::MatrixCoord(batch_count * problem_size.m(), problem_size.n()) : 
+          cutlass::MatrixCoord(problem_size.m(), batch_count * problem_size.n()));
 
     cutlass::HostTensor<
       typename DualGemm::ElementC,
-      typename DualGemm::LayoutC> reference_D1(problem_size.mn());
+      typename DualGemm::LayoutC> reference_D1(
+        std::is_same<typename DualGemm::LayoutC, cutlass::layout::RowMajor>::value ? 
+          cutlass::MatrixCoord(batch_count * problem_size.m(), problem_size.n()) : 
+          cutlass::MatrixCoord(problem_size.m(), batch_count * problem_size.n()));
 
     cutlass::HostTensor<
       typename DualGemm::ElementC,
-      typename DualGemm::LayoutC> reference_D2(problem_size.mn());
+      typename DualGemm::LayoutC> reference_D2(
+        std::is_same<typename DualGemm::LayoutC, cutlass::layout::RowMajor>::value ? 
+          cutlass::MatrixCoord(batch_count * problem_size.m(), problem_size.n()) : 
+          cutlass::MatrixCoord(problem_size.m(), batch_count * problem_size.n()));
 
     CHECK_TRUE(initialize_tensor(tensor_A0.host_view(), init_A, seed + 2019));
     CHECK_TRUE(initialize_tensor(tensor_B0.host_view(), init_B, seed + 2118));
     CHECK_TRUE(initialize_tensor(tensor_C0.host_view(), init_C, seed + 2017));
     CHECK_TRUE(initialize_tensor(tensor_Bias0.host_view(), init_Bias, seed + 2011));
     CHECK_TRUE(initialize_tensor(tensor_B1.host_view(), init_B, seed + 2113));
     CHECK_TRUE(initialize_tensor(tensor_C1.host_view(), init_C, seed + 2015));
@@ -635,42 +681,71 @@
     tensor_D1.sync_device();
     tensor_D2.sync_device();
     reference_D0.sync_device();
     reference_D1.sync_device();
     reference_D2.sync_device();
 
     //
+    // Batch strides (irrelevant when batch_count == 1)
+    //
+
+    int64_t batch_stride_A = problem_size.m() * problem_size.k();
+    int64_t batch_stride_B0 = problem_size.k() * problem_size.n();
+    int64_t batch_stride_B1 = problem_size.k() * problem_size.n();
+    if (broadcast_b1) {
+      // B1 is a (column) vector
+      batch_stride_B1 = problem_size.k();
+    }
+    int64_t batch_stride_Bias = problem_size.n();
+    int64_t batch_stride_D = problem_size.m() * problem_size.n();
+
+    //
     // Initialize the GEMM operator
     //
 
     int split_k_slices = DualGemm::kSplitKSerial ? 2 : 1;
     typename cutlass::TensorRef<typename DualGemm::ElementC, typename DualGemm::LayoutC> nullptr_ref{};
     decltype(nullptr_ref) ref_B0, ref_B1;
     if (beta0 != ElementCompute(0)) {
       ref_B0 = {tensor_Bias0.device_data(), typename DualGemm::LayoutC::Stride(0)};
     }
     if (beta1 != ElementCompute(0)) {
       ref_B1 = {tensor_Bias1.device_data(), typename DualGemm::LayoutC::Stride(0)};
     }
     typename DualGemm::Arguments arguments{
+      (batch_count > 1 ? 
+        cutlass::gemm::DualGemmMode::kBatched : 
+        cutlass::gemm::DualGemmMode::kGemm),
       problem_size,
       tensor_A0.device_ref(),
       tensor_B0.device_ref(),
       ref_B0,
       DualGemm::kStoreD0 ? tensor_D0.device_ref() : nullptr_ref,
-      tensor_B1.device_ref(),
+      (broadcast_b1 ? 
+        typename DualGemm::TensorRefB1(tensor_B1.device_data(), 0) : 
+        tensor_B1.device_ref()),
       ref_B1,
       DualGemm::kStoreD1 ? tensor_D1.device_ref() : nullptr_ref,
       tensor_D2.device_ref(),
       {alpha0, beta0},
       {alpha1, beta1},
       {},
-      split_k_slices
+      split_k_slices,
+      batch_count,
+      batch_stride_A,
+      batch_stride_B0,
+      batch_stride_B1,
+      batch_stride_Bias,
+      batch_stride_D,
     };
 
+    //
+    // Run the GEMM
+    //
+
     DualGemm b2b_gemm_op;
 
     cutlass::device_memory::allocation<uint8_t> workspace(b2b_gemm_op.get_workspace_size(arguments));
   
     cutlass::Status status = b2b_gemm_op.can_implement(arguments);
 
     CUTLASS_CHECK(status);
@@ -680,86 +755,120 @@
     CUTLASS_CHECK(status);
 
     for(int i = 0; i < warm_ups; i++) {
         status = b2b_gemm_op();
         CUTLASS_CHECK(status);
     }
 
-#ifdef IS_PROFILING
-    return true;
-#endif
-    //
-    // Run the GEMM
-    //
-
-    cudaEvent_t start, stop;
-    cudaEventCreate(&start);
-    cudaEventCreate(&stop);
-
-    cudaEventRecord(start);
-
-    for(int i = 0; i < runs; i++) {
-        status = b2b_gemm_op();
-
-        CUTLASS_CHECK(status);
+    if (is_profiling) {
+      //
+      // Profile the GEMM
+      //
+
+      cudaEvent_t start, stop;
+      cudaEventCreate(&start);
+      cudaEventCreate(&stop);
+
+      cudaEventRecord(start);
+
+      for(int i = 0; i < runs; i++) {
+          status = b2b_gemm_op();
+          CUTLASS_CHECK(status);
+      }
+
+      cudaEventRecord(stop);
+      cudaDeviceSynchronize();
+      float gemmTime;
+      cudaEventElapsedTime(&gemmTime, start, stop);
+      std::cout << "Fusion time " << gemmTime / (float)runs << " ms\n";
     }
 
-    cudaEventRecord(stop);
-    cudaDeviceSynchronize();
-    float gemmTime;
-    cudaEventElapsedTime(&gemmTime, start, stop);
-    std::cout << "Fusion time " << gemmTime / (float)runs << " ms\n";
-
     tensor_D0.sync_host();
     tensor_D1.sync_host();
     tensor_D2.sync_host();
 
     //
     // Verify
     //
 
-    cutlass::reference::device::Gemm<
-        typename DualGemm::ElementA, typename DualGemm::LayoutA,
-        typename DualGemm::ElementB, typename DualGemm::LayoutB,
-        typename DualGemm::ElementC, typename DualGemm::LayoutC, 
-        ElementAccumulator, ElementAccumulator>
-        reference_gemm_0;
+    using GemmUniversal0 = cutlass::gemm::device::GemmUniversal<
+      typename DualGemm::ElementA, typename DualGemm::LayoutA,
+      typename DualGemm::ElementB, typename DualGemm::LayoutB0,
+      typename DualGemm::ElementC, typename DualGemm::LayoutC, 
+      ElementAccumulator
+    >;
+
+    GemmUniversal0 reference_gemm0;
+
+    typename GemmUniversal0::Arguments args0 {
+      (batch_count > 1 ? 
+        cutlass::gemm::GemmUniversalMode::kBatched : 
+        cutlass::gemm::GemmUniversalMode::kGemm),
+      problem_size,
+      batch_count,
+      {alpha0, beta0},
+      tensor_A0.device_data(),
+      tensor_B0.device_data(),
+      tensor_Bias0.device_data(),
+      reference_D0.device_data(),
+      batch_stride_A,
+      batch_stride_B0,
+      batch_stride_Bias,
+      batch_stride_D,
+      tensor_A0.stride(0),
+      tensor_B0.stride(0),
+      0,  // zero stride for the bias vector
+      reference_D0.stride(0),
+    };
 
-    cutlass::reference::device::Gemm<
-        typename DualGemm::ElementA, typename DualGemm::LayoutA,
-        typename DualGemm::ElementB, typename DualGemm::LayoutB,
-        typename DualGemm::ElementC, typename DualGemm::LayoutC, ElementCompute,
-        ElementAccumulator, typename DualGemm::Operator>
-        reference_gemm_1;
+    status = reference_gemm0.can_implement(args0);
+    CUTLASS_CHECK(status);
+    status = reference_gemm0(args0);
+    CUTLASS_CHECK(status);
 
-    reference_gemm_0(
+    using GemmUniversal1 = cutlass::gemm::device::GemmUniversal<
+      typename DualGemm::ElementA, typename DualGemm::LayoutA,
+      typename DualGemm::ElementB, typename DualGemm::LayoutB1,
+      typename DualGemm::ElementC, typename DualGemm::LayoutC, 
+      ElementAccumulator
+    >;
+
+    GemmUniversal1 reference_gemm1;
+
+    typename GemmUniversal1::Arguments args1 {
+      (batch_count > 1 ? 
+        cutlass::gemm::GemmUniversalMode::kBatched : 
+        cutlass::gemm::GemmUniversalMode::kGemm),
       problem_size,
-      alpha0,
-      tensor_A0.device_ref(), 
-      tensor_B0.device_ref(), 
-      beta0,
-      {tensor_Bias0.device_data(), typename DualGemm::LayoutC::Stride(0)},
-      reference_D0.device_ref()
-    );
-    if(relu) {
-       cutlass::reference::device::TensorReLu(reference_D0.device_view()); 
-    }
+      batch_count,
+      {alpha1, beta1},
+      tensor_A0.device_data(),
+      tensor_B1.device_data(),
+      tensor_Bias1.device_data(),
+      reference_D1.device_data(),
+      batch_stride_A,
+      batch_stride_B1,
+      batch_stride_Bias,
+      batch_stride_D,
+      tensor_A0.stride(0),
+      (broadcast_b1 ? 0 : tensor_B1.stride(0)),
+      0,  // zero stride for the bias vector
+      reference_D1.stride(0),
+    };
+
+    status = reference_gemm1.can_implement(args1);
+    CUTLASS_CHECK(status);
+    status = reference_gemm1(args1);
+    CUTLASS_CHECK(status);
 
-    reference_gemm_1(
-      problem_size,
-      alpha1, 
-      tensor_A0.device_ref(), 
-      tensor_B1.device_ref(), 
-      beta1,
-      {tensor_Bias1.device_data(), typename DualGemm::LayoutC::Stride(0)},
-      reference_D1.device_ref()
-    );
     if(relu) {
+       cutlass::reference::device::TensorReLu(reference_D0.device_view()); 
        cutlass::reference::device::TensorReLu(reference_D1.device_view()); 
     }
+
     TensorEpilogueForEach<EpilogueOutputOp2>(reference_D0.device_view(), reference_D1.device_view(), reference_D2.device_view());
     cudaDeviceSynchronize();
     reference_D0.sync_host();
     reference_D1.sync_host();
     reference_D2.sync_host();
 
     CHECK_GT(cutlass::reference::host::TensorNorm(reference_D0.host_view()), 0);
@@ -789,15 +898,14 @@
       reference_D2.host_view(), 
       tensor_D2.host_view());
     CHECK_TRUE(passed_out2);
 
     bool passed = passed_out0 && passed_out1 && passed_out2;
     if (!passed)
     {
-
       std::stringstream fname;
 
       fname << "error_DualGemm_device_fused.txt";
       std::cerr << "Dumping results in " << fname.str() << "\n";
 
       std::ofstream file(fname.str());
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/kernel/dual_gemm.h` & `flash_attn-2.0.0/csrc/cutlass/examples/45_dual_gemm/kernel/dual_gemm.h`

 * *Files 6% similar despite different names*

```diff
@@ -38,14 +38,15 @@
 
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/matrix_coord.h"
 #include "cutlass/semaphore.h"
 
 #include "../threadblock/dual_mma_multistage.h"
 #include "../threadblock/dual_epilogue.h"
+#include "../dual_gemm_common.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace kernel {
 
@@ -88,82 +89,100 @@
       typename Epilogue0::Padding,
       kStoreD0,
       kStoreD1,
       Epilogue0::kFragmentsPerIteration,
       true // IterationsUnroll
   >;
 
+  using ElementA = typename DualMma::IteratorA::Element;
+  using ElementB = typename DualMma::IteratorB0::Element;
+  using ElementC = typename DualEpilogue::OutputTileIterator::Element;
+
   static bool const kSplitKSerial = SplitKSerial;
   static_assert(!kSplitKSerial || (kStoreD0 && kStoreD1),
     "Split-K serial requires buffers for D0/D1 for reduction");
 
   /// Warp count (concept: GemmShape)
   using WarpCount0 = typename DualMma::WarpCount;
   static int const kThreadCount = 32 * WarpCount0::kCount;
 
   /// Parameters structure
   struct Params {
+    DualGemmMode mode;
     cutlass::gemm::GemmCoord problem_size;
     cutlass::gemm::GemmCoord grid_tiled_shape;
     int swizzle_log_tile;
 
     // Mma0
     typename DualMma::IteratorA::Params params_A0;
     typename DualMma::IteratorA::TensorRef ref_A0;
-    typename DualMma::IteratorB::Params params_B0;
-    typename DualMma::IteratorB::TensorRef ref_B0;
+    typename DualMma::IteratorB0::Params params_B0;
+    typename DualMma::IteratorB0::TensorRef ref_B0;
     typename Epilogue0::OutputTileIterator::Params params_C0;
     typename Epilogue0::OutputTileIterator::TensorRef ref_C0;
     typename Epilogue0::OutputTileIterator::Params params_D0;
     typename Epilogue0::OutputTileIterator::TensorRef ref_D0;
     typename OutputOp0::Params output_op_0;
 
     // Mma1
-    typename DualMma::IteratorB::Params params_B1;
-    typename DualMma::IteratorB::TensorRef ref_B1;
+    typename DualMma::IteratorB1::Params params_B1;
+    typename DualMma::IteratorB1::TensorRef ref_B1;
     typename Epilogue1::OutputTileIterator::Params params_C1;
     typename Epilogue1::OutputTileIterator::TensorRef ref_C1;
     typename Epilogue1::OutputTileIterator::Params params_D1;
     typename Epilogue1::OutputTileIterator::TensorRef ref_D1;
     typename OutputOp1::Params output_op_1;
 
     typename Epilogue1::OutputTileIterator::Params params_D2;
     typename Epilogue1::OutputTileIterator::TensorRef ref_D2;
     typename OutputOp2::Params output_op_2;
 
     int *semaphore;
     int gemm_k_size;
 
+    int64_t batch_stride_A;
+    int64_t batch_stride_B0;
+    int64_t batch_stride_B1;
+    int64_t batch_stride_C;
+    int64_t batch_stride_D;
+
     //
     // Methods
     //
 
     CUTLASS_HOST_DEVICE
     Params(): swizzle_log_tile(0), semaphore(0), gemm_k_size(0) { }
 
     CUTLASS_HOST_DEVICE
     Params(
+      DualGemmMode mode,
       cutlass::gemm::GemmCoord const & problem_size,
       cutlass::gemm::GemmCoord const & grid_tiled_shape,
       // Mma0: D0 = A @ B0 + C0
       typename DualMma::IteratorA::TensorRef ref_A0,
-      typename DualMma::IteratorB::TensorRef ref_B0,
+      typename DualMma::IteratorB0::TensorRef ref_B0,
       typename Epilogue0::OutputTileIterator::TensorRef ref_C0,
       typename Epilogue0::OutputTileIterator::TensorRef ref_D0,
       // Mma1: D1 = A @ B1 + C1
-      typename DualMma::IteratorB::TensorRef ref_B1,
+      typename DualMma::IteratorB1::TensorRef ref_B1,
       typename Epilogue1::OutputTileIterator::TensorRef ref_C1,
       typename Epilogue1::OutputTileIterator::TensorRef ref_D1,
 
       typename Epilogue1::OutputTileIterator::TensorRef ref_D2,
       typename OutputOp0::Params output_op_0 = typename OutputOp0::Params(),
       typename OutputOp1::Params output_op_1 = typename OutputOp1::Params(),
       typename OutputOp2::Params output_op_2 = typename OutputOp2::Params(),
-      int *workspace = nullptr
+      int *workspace = nullptr,
+      int64_t batch_stride_A = 1,
+      int64_t batch_stride_B0 = 1,
+      int64_t batch_stride_B1 = 1,
+      int64_t batch_stride_C = 1,
+      int64_t batch_stride_D = 1
     ):
+      mode(mode),
       problem_size(problem_size),
       grid_tiled_shape(grid_tiled_shape),
       swizzle_log_tile(ThreadblockSwizzle().get_log_tile(grid_tiled_shape)),
       // Mma0
       params_A0(ref_A0.layout()),
       ref_A0(ref_A0),
       params_B0(ref_B0.layout()),
@@ -179,21 +198,26 @@
       ref_C1(ref_C1),
       params_D1(ref_D1.layout()),
       ref_D1(ref_D1),
       params_D2(ref_D2.layout()),
       ref_D2(ref_D2),
       output_op_0(output_op_0),
       output_op_1(output_op_1),
-      output_op_2(output_op_2) {
+      output_op_2(output_op_2),
+      batch_stride_A(batch_stride_A),
+      batch_stride_B0(batch_stride_B0),
+      batch_stride_B1(batch_stride_B1),
+      batch_stride_C(batch_stride_C),
+      batch_stride_D(batch_stride_D) {
 
       int total_gemm_k_iterations = (problem_size.k() + DualMma::Shape::kK - 1) / DualMma::Shape::kK;
       int gemm_k_iterations = (total_gemm_k_iterations + grid_tiled_shape.k() - 1) / grid_tiled_shape.k();
       gemm_k_size = gemm_k_iterations * DualMma::Shape::kK;
 
-    semaphore = workspace;
+      semaphore = workspace;
     }
   };
 
   /// Shared memory storage structure
   union SharedStorage {
     typename DualMma::SharedStorage main_loop;
     typename DualEpilogue::SharedStorage epilogue;
@@ -206,24 +230,24 @@
   CUTLASS_HOST_DEVICE
   DualGemm() { }
 
   /// Determines whether kernel satisfies alignment
     static Status can_implement(
       cutlass::gemm::GemmCoord const & problem_size,
       typename DualMma::IteratorA::TensorRef ref_A0,
-      typename DualMma::IteratorB::TensorRef ref_B0,
+      typename DualMma::IteratorB0::TensorRef ref_B0,
       typename Epilogue0::OutputTileIterator::TensorRef ref_C0,
       typename Epilogue0::OutputTileIterator::TensorRef ref_D0,
-      typename DualMma::IteratorB::TensorRef ref_B1,
+      typename DualMma::IteratorB1::TensorRef ref_B1,
       typename Epilogue1::OutputTileIterator::TensorRef ref_C1,
       typename Epilogue1::OutputTileIterator::TensorRef ref_D1,
       typename Epilogue1::OutputTileIterator::TensorRef ref_D2) {
 
     static int const kAlignmentA = DualMma::IteratorA::AccessType::kElements;
-    static int const kAlignmentB = DualMma::IteratorB::AccessType::kElements;
+    static int const kAlignmentB = DualMma::IteratorB0::AccessType::kElements;
     static int const kAlignmentC = Epilogue0::OutputTileIterator::kElementsPerAccess;
 
     if (!TensorRef_aligned(ref_A0, kAlignmentA)) {
       return Status::kErrorMisalignedOperand;
     }
 
     if (!TensorRef_aligned(ref_B0, kAlignmentB)) {
@@ -269,60 +293,74 @@
     // Early exit if CTA is out of range
     if (params.grid_tiled_shape.m() <= threadblock_tile_offset.m() ||
       params.grid_tiled_shape.n() <= threadblock_tile_offset.n()) {
 
       return;
     }
 
+    int offset_k = 0;
+    int problem_size_k = params.problem_size.k();
+
+    ElementA *ptr_A0 = static_cast<ElementA *>(params.ref_A0.data()); 
+    ElementB *ptr_B0 = static_cast<ElementB *>(params.ref_B0.data());
+    ElementB *ptr_B1 = static_cast<ElementB *>(params.ref_B1.data());
+
+    //
+    // Fetch pointers based on mode.
+    //
+    if (params.mode == DualGemmMode::kGemm) {
+      if (threadblock_tile_offset.k() + 1 < params.grid_tiled_shape.k()) {
+        problem_size_k = (threadblock_tile_offset.k() + 1) * params.gemm_k_size; 
+      }
+
+      offset_k = threadblock_tile_offset.k() * params.gemm_k_size;
+    }
+    else if (params.mode == DualGemmMode::kBatched) {
+      ptr_A0 += threadblock_tile_offset.k() * params.batch_stride_A;
+      ptr_B0 += threadblock_tile_offset.k() * params.batch_stride_B0;
+      ptr_B1 += threadblock_tile_offset.k() * params.batch_stride_B1;
+    }
+
     // Compute initial location in logical coordinates
     cutlass::MatrixCoord tb_offset_A0{
       threadblock_tile_offset.m() * DualMma::Shape::kM,
-      threadblock_tile_offset.k() * params.gemm_k_size,
+      offset_k,
     };
 
     cutlass::MatrixCoord tb_offset_B0{
-      threadblock_tile_offset.k() * params.gemm_k_size,
+      offset_k,
       threadblock_tile_offset.n() * DualMma::Shape::kN
     };
 
     cutlass::MatrixCoord tb_offset_B1{
-      threadblock_tile_offset.k() * params.gemm_k_size,
+      offset_k,
       threadblock_tile_offset.n() * DualMma::Shape::kN
     };
 
-    // Problem size is a function of threadblock index in the K dimension
-    int problem_size_k =
-      (params.problem_size.k() < (threadblock_tile_offset.k() + 1) * params.gemm_k_size) ?
-       params.problem_size.k() :
-       (threadblock_tile_offset.k() + 1) * params.gemm_k_size;
-
-    // Compute threadblock-scoped matrix multiply-add
-    int gemm_k_iterations = (problem_size_k - tb_offset_A0.column() + DualMma::Shape::kK - 1) / DualMma::Shape::kK;
-
     // Compute position within threadblock
     int thread_idx = threadIdx.x;
 
     // Construct iterators to A and B operands
     typename DualMma::IteratorA iterator_A0(
       params.params_A0,
-      params.ref_A0.data(),
+      ptr_A0,
       {params.problem_size.m(), problem_size_k},
       thread_idx,
       tb_offset_A0);
 
-    typename DualMma::IteratorB iterator_B0(
+    typename DualMma::IteratorB0 iterator_B0(
       params.params_B0,
-      params.ref_B0.data(),
+      ptr_B0,
       {problem_size_k, params.problem_size.n()},
       thread_idx,
       tb_offset_B0);
 
-    typename DualMma::IteratorB iterator_B1(
+    typename DualMma::IteratorB1 iterator_B1(
       params.params_B1,
-      params.ref_B1.data(),
+      ptr_B1,
       {problem_size_k, params.problem_size.n()},
       thread_idx,
       tb_offset_B1);
 
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
@@ -336,14 +374,17 @@
 
     // Construct thread-scoped matrix multiply
     typename DualMma::FragmentC accum0;
     typename DualMma::FragmentC accum1;
     accum0.clear();
     accum1.clear();
 
+    // Compute threadblock-scoped matrix multiply-add
+    int gemm_k_iterations = (problem_size_k - offset_k + DualMma::Shape::kK - 1) / DualMma::Shape::kK;
+
     DualMma mma(shared_storage.main_loop, thread_idx, warp_idx, lane_idx);
     if (!kSplitKSerial || gemm_k_iterations > 0) {
       // Compute threadblock-scoped matrix multiply-add
       mma(gemm_k_iterations,
         accum0, accum1,
         iterator_A0, iterator_B0, iterator_B1,
         accum0, accum1);
@@ -368,62 +409,77 @@
     MatrixCoord threadblock_offset(
       threadblock_tile_offset.m() * DualMma::Shape::kM,
       threadblock_tile_offset.n() * DualMma::Shape::kN
     );
 
     int block_idx = threadblock_tile_offset.m() + threadblock_tile_offset.n() * params.grid_tiled_shape.m();
 
+    ElementC *ptr_C0 = static_cast<ElementC *>(params.ref_C0.data()); 
+    ElementC *ptr_C1 = static_cast<ElementC *>(params.ref_C1.data()); 
+    ElementC *ptr_D0 = static_cast<ElementC *>(params.ref_D0.data()); 
+    ElementC *ptr_D1 = static_cast<ElementC *>(params.ref_D1.data()); 
+    ElementC *ptr_D2 = static_cast<ElementC *>(params.ref_D2.data()); 
+
     // Construct the semaphore.
     Semaphore semaphore(params.semaphore + block_idx, thread_idx);
 
-    // If performing a reduction via split-K, fetch the initial synchronization
-    if (kSplitKSerial && params.grid_tiled_shape.k() > 1) {
-      
-      // Fetch the synchronization lock initially but do not block.
-      semaphore.fetch();
-
-      // Indicate which position in a serial reduction the output operator is currently updating
-      output_op_0.set_k_partition(threadblock_tile_offset.k(), params.grid_tiled_shape.k());
-      output_op_1.set_k_partition(threadblock_tile_offset.k(), params.grid_tiled_shape.k());
+    if (params.mode == DualGemmMode::kGemm) {
+      // If performing a reduction via split-K, fetch the initial synchronization
+      if (kSplitKSerial && params.grid_tiled_shape.k() > 1) {
+        
+        // Fetch the synchronization lock initially but do not block.
+        semaphore.fetch();
+
+        // Indicate which position in a serial reduction the output operator is currently updating
+        output_op_0.set_k_partition(threadblock_tile_offset.k(), params.grid_tiled_shape.k());
+        output_op_1.set_k_partition(threadblock_tile_offset.k(), params.grid_tiled_shape.k());
+      }
+    }
+    else if (params.mode == DualGemmMode::kBatched) {
+      ptr_C0 += threadblock_tile_offset.k() * params.batch_stride_C;
+      ptr_C1 += threadblock_tile_offset.k() * params.batch_stride_C;
+      ptr_D0 += threadblock_tile_offset.k() * params.batch_stride_D;
+      ptr_D1 += threadblock_tile_offset.k() * params.batch_stride_D;
+      ptr_D2 += threadblock_tile_offset.k() * params.batch_stride_D;
     }
 
     // Tile iterator loading from source tensor.
     typename Epilogue0::OutputTileIterator iterator_C0(
       params.params_C0,
-      params.ref_C0.data(),
+      ptr_C0,
       params.problem_size.mn(),
       thread_idx,
       threadblock_offset
     );
     typename Epilogue1::OutputTileIterator iterator_C1(
       params.params_C1,
-      params.ref_C1.data(),
+      ptr_C1,
       params.problem_size.mn(),
       thread_idx,
       threadblock_offset
     );
 
     // Tile iterator writing to destination tensor.
     typename Epilogue0::OutputTileIterator iterator_D0(
       params.params_D0,
-      params.ref_D0.data(),
+      ptr_D0,
       params.problem_size.mn(),
       thread_idx,
       threadblock_offset
     );
     typename Epilogue1::OutputTileIterator iterator_D1(
       params.params_D1,
-      params.ref_D1.data(),
+      ptr_D1,
       params.problem_size.mn(),
       thread_idx,
       threadblock_offset
     );
     typename Epilogue1::OutputTileIterator iterator_D2(
       params.params_D2,
-      params.ref_D2.data(),
+      ptr_D2,
       params.problem_size.mn(),
       thread_idx,
       threadblock_offset
     );
 
     DualEpilogue epilogue(
       shared_storage.epilogue,
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/test_run.h` & `flash_attn-2.0.0/csrc/cutlass/examples/45_dual_gemm/test_run.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/thread/left_silu_and_mul.h` & `flash_attn-2.0.0/csrc/cutlass/examples/45_dual_gemm/thread/left_silu_and_mul.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_epilogue.h` & `flash_attn-2.0.0/csrc/cutlass/examples/45_dual_gemm/threadblock/dual_epilogue.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_base.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_base.h`

 * *Files 11% similar despite different names*

```diff
@@ -38,16 +38,14 @@
 #include "cutlass/arch/memory.h"
 #include "cutlass/array.h"
 #include "cutlass/cutlass.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/matrix_shape.h"
 #include "cutlass/numeric_types.h"
 
-#include "cutlass/gemm/threadblock/mma_base.h"
-
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace threadblock {
 
 ////////////////////////////////////////////////////////////////////////////////
@@ -59,15 +57,15 @@
     typename Shape_,
     /// Policy describing tuning details (concept: MmaPolicy)
     typename Policy_,
     /// Number of stages,
     int Stages,
     /// Used for partial specialization
     typename Enable = bool>
-class DualMmaBase {
+class MmaPlanarComplexBase {
  public:
   ///< Size of the Gemm problem - concept: gemm::GemmShape<>
   using Shape = Shape_;
 
   ///< Policy describing tuning details
   using Policy = Policy_;
 
@@ -96,21 +94,14 @@
 
   /// Tensor reference to the A operand
   using TensorRefA = TensorRef<typename Operator::ElementA, typename Operator::LayoutA>;
 
   /// Tensor reference to the B operand
   using TensorRefB = TensorRef<typename Operator::ElementB, typename Operator::LayoutB>;
 
-  static_assert(kWarpGemmIterations > 1,
-                "The pipelined structure requires at least two warp-level "
-                "GEMM operations.");
-
-  static_assert((kWarpGemmIterations % 2) == 0,
-                "Inner loop iteration must be an even number.");
-
   //
   // Nested structs
   //
 
   /// Shared storage object needed by threadblock-scoped GEMM
   class SharedStorage {
    public:
@@ -119,30 +110,35 @@
     //
 
     /// Shape of the A matrix operand in shared memory
     using ShapeA = MatrixShape<Shape::kM + Policy::SmemPaddingA::kRow,
                                Shape::kK * kStages +
                                    Policy::SmemPaddingA::kColumn>;
 
+    /// Stride to the imaginary part of the A operand
+    static int const kImaginaryStrideA = ShapeA::kCount;
+
     /// Shape of the B matrix operand in shared memory
     using ShapeB =
         MatrixShape<Shape::kK * kStages + Policy::SmemPaddingB::kRow,
                     Shape::kN + Policy::SmemPaddingB::kColumn>;
 
+    /// Stride to the imaginary part of the A operand
+    static int const kImaginaryStrideB = ShapeB::kCount;
+
    public:
     //
     // Data members
     //
 
     /// Buffer for A operand
-    AlignedBuffer<typename Operator::ElementA, ShapeA::kCount> operand_A;
+    AlignedBuffer<typename Operator::ElementA, ShapeA::kCount + kImaginaryStrideA> operand_A;
 
     /// Buffer for B operand
-    AlignedBuffer<typename Operator::ElementB, ShapeB::kCount> operand_B0;
-    AlignedBuffer<typename Operator::ElementB, ShapeB::kCount> operand_B1;
+    AlignedBuffer<typename Operator::ElementB, ShapeB::kCount + kImaginaryStrideB> operand_B;
 
    public:
 
     //
     // Methods
     //
 
@@ -162,53 +158,47 @@
     CUTLASS_HOST_DEVICE
     TensorRefA operand_A_ref() {
       return TensorRefA{operand_A.data(), LayoutA()};
     }
 
     /// Returns a TensorRef to the B operand
     CUTLASS_HOST_DEVICE
-    TensorRefB operand_B0_ref() {
-      return TensorRefB{operand_B0.data(), LayoutB()};
-    }
-    CUTLASS_HOST_DEVICE
-    TensorRefB operand_B1_ref() {
-      return TensorRefB{operand_B1.data(), LayoutB()};
+    TensorRefB operand_B_ref() {
+      return TensorRefB{operand_B.data(), LayoutB()};
     }
   };
 
  protected:
 
   //
   // Data members
   //
 
   /// Iterator to load a warp-scoped tile of A operand from shared memory
   typename Operator::IteratorA warp_tile_iterator_A_;
 
   /// Iterator to load a warp-scoped tile of B operand from shared memory
-  typename Operator::IteratorB warp_tile_iterator_B0_;
-  typename Operator::IteratorB warp_tile_iterator_B1_;
+  typename Operator::IteratorB warp_tile_iterator_B_;
 
 public:
 
   /// Construct from tensor references
   CUTLASS_DEVICE
-  DualMmaBase(
+  MmaPlanarComplexBase(
       ///< Shared storage needed for internal use by threadblock-scoped GEMM
       SharedStorage &shared_storage,
       ///< ID within the threadblock
       int thread_idx,
       ///< ID of warp
       int warp_idx,
       ///< ID of each thread within a warp
       int lane_idx
     ):
       warp_tile_iterator_A_(shared_storage.operand_A_ref(), lane_idx),
-      warp_tile_iterator_B0_(shared_storage.operand_B0_ref(), lane_idx),
-      warp_tile_iterator_B1_(shared_storage.operand_B1_ref(), lane_idx) {
+      warp_tile_iterator_B_(shared_storage.operand_B_ref(), lane_idx) {
 
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 }  // namespace threadblock
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_multistage.h` & `flash_attn-2.0.0/csrc/cutlass/examples/45_dual_gemm/threadblock/dual_mma_multistage.h`

 * *Files 3% similar despite different names*

```diff
@@ -63,88 +63,103 @@
     //  MaskedTileIterator)
     typename IteratorA_,
     /// Iterates over tiles of A operand in shared memory
     /// (concept: WriteableTileIterator | RandomAccessTileIterator)
     typename SmemIteratorA_,
     /// Cache operation for operand A
     cutlass::arch::CacheOperation::Kind CacheOpA,
-    /// Iterates over tiles of B operand in global memory
+    /// Iterates over tiles of B0 operand in global memory
     //  (concept: ReadableTileIterator | ForwardTileIterator |
     //  MaskedTileIterator)
-    typename IteratorB_,
-    /// Iterates over tiles of B operand in shared memory
+    typename IteratorB0_,
+    /// Iterates over tiles of B0 operand in shared memory
     /// (concept: WriteableTileIterator | RandomAccessTileIterator)
-    typename SmemIteratorB_,
+    typename SmemIteratorB0_,
     /// Cache operation for operand B
     cutlass::arch::CacheOperation::Kind CacheOpB,
+    /// Iterates over tiles of B1 operand in global memory
+    //  (concept: ReadableTileIterator | ForwardTileIterator |
+    //  MaskedTileIterator)
+    typename IteratorB1_,
+    /// Iterates over tiles of B1 operand in shared memory
+    /// (concept: WriteableTileIterator | RandomAccessTileIterator)
+    typename SmemIteratorB1_,
     /// Data type of accumulator matrix
     typename ElementC_,
     /// Data type of accumulator matrix
     typename LayoutC_,
     /// Policy describing tuning details (concept: MmaPolicy)
-    typename Policy_,
+    typename Policy0_,
+    /// B1-specific version of the policy (concept: MmaPolicy)
+    typename Policy1_,
     /// Number of stages,
     int Stages,
     /// Use zfill or predicate for out-of-bound cp.async
     SharedMemoryClearOption SharedMemoryClear = SharedMemoryClearOption::kNone,
     /// Used for partial specialization
     typename Enable = bool>
 class DualMmaMultistage : 
-  public DualMmaBase<Shape_, Policy_, Stages> {
+  public DualMmaBase<Shape_, Policy0_, Policy1_, Stages> {
 public:
   ///< Base class
-  using Base = DualMmaBase<Shape_, Policy_, Stages>;
+  using Base = DualMmaBase<Shape_, Policy0_, Policy1_, Stages>;
   ///< Size of the Gemm problem - concept: gemm::GemmShape<>
   using Shape = Shape_;
   ///< Iterates over tiles of A operand in global memory
   using IteratorA = IteratorA_;
-  ///< Iterates over tiles of B operand in global memory
-  using IteratorB = IteratorB_;
+  ///< Iterates over tiles of B0 operand in global memory
+  using IteratorB0 = IteratorB0_;
+  ///< Iterates over tiles of B1 operand in global memory
+  using IteratorB1 = IteratorB1_;
   ///< Data type of accumulator matrix
   using ElementC = ElementC_;
   ///< Layout of accumulator matrix
   using LayoutC = LayoutC_;
   ///< Policy describing tuning details
-  using Policy = Policy_;
+  using Policy0 = Policy0_;
+  using Policy1 = Policy1_;
 
   using SmemIteratorA = SmemIteratorA_;
-  using SmemIteratorB = SmemIteratorB_;
+  using SmemIteratorB0 = SmemIteratorB0_;
+  using SmemIteratorB1 = SmemIteratorB1_;
 
   static cutlass::arch::CacheOperation::Kind const kCacheOpA = CacheOpA;
   static cutlass::arch::CacheOperation::Kind const kCacheOpB = CacheOpB;
 
   //
   // Dependent types
   //
 
   /// Fragment of accumulator tile
-  using FragmentC = typename Policy::Operator::FragmentC;
+  using FragmentC = typename Policy0::Operator::FragmentC;
 
   /// Warp-level Mma
-  using Operator = typename Policy::Operator;
+  using Operator0 = typename Policy0::Operator;
+  using Operator1 = typename Policy1::Operator;
 
   /// Minimum architecture is Sm80 to support cp.async
   using ArchTag = arch::Sm80;
   
   /// Complex transform on A operand
-  static ComplexTransform const kTransformA = Operator::kTransformA;
+  static ComplexTransform const kTransformA = Operator0::kTransformA;
 
   /// Complex transform on B operand
-  static ComplexTransform const kTransformB = Operator::kTransformB;
+  static ComplexTransform const kTransformB0 = Operator0::kTransformB;
+  static ComplexTransform const kTransformB1 = Operator1::kTransformB;
 
   /// Internal structure exposed for introspection.
   struct Detail {
 
     /// Number of cp.async instructions to load one stage of operand A
     static int const AsyncCopyIterationsPerStageA =
         IteratorA::ThreadMap::Iterations::kCount;
 
     /// Number of cp.async instructions to load one stage of operand B
     static int const AsyncCopyIterationsPerStageB =
-        IteratorB::ThreadMap::Iterations::kCount;
+        IteratorB0::ThreadMap::Iterations::kCount;
 
     /// Number of stages
     static int const kStages = Stages;
 
     /// Number of cp.async instructions to load on group of operand A
     static int const kAccessesPerGroupA =
         (AsyncCopyIterationsPerStageA + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
@@ -152,31 +167,33 @@
     /// Number of cp.async instructions to load on group of operand B
     static int const kAccessesPerGroupB =
         (AsyncCopyIterationsPerStageB + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
   };
 
  private:
 
-  using WarpLoadedFragmentA = typename Operator::FragmentA;
-  using WarpLoadedFragmentB = typename Operator::FragmentB;
-  using WarpTransformedFragmentA = typename Operator::TransformedFragmentA;
-  using WarpTransformedFragmentB = typename Operator::TransformedFragmentB;
+  using WarpLoadedFragmentA = typename Operator0::FragmentA;
+  using WarpLoadedFragmentB0 = typename Operator0::FragmentB;
+  using WarpLoadedFragmentB1 = typename Operator1::FragmentB;
+  using WarpTransformedFragmentA = typename Operator0::TransformedFragmentA;
+  using WarpTransformedFragmentB0 = typename Operator0::TransformedFragmentB;
+  using WarpTransformedFragmentB1 = typename Operator1::TransformedFragmentB;
 
  private:
 
   //
   // Data members
   //
 
   /// Iterator to write threadblock-scoped tile of A operand to shared memory
   SmemIteratorA smem_iterator_A_;
 
   /// Iterator to write threadblock-scoped tile of B operand to shared memory
-  SmemIteratorB smem_iterator_B0_;
-  SmemIteratorB smem_iterator_B1_;
+  SmemIteratorB0 smem_iterator_B0_;
+  SmemIteratorB1 smem_iterator_B1_;
 
 public:
 
   /// Construct from tensor references
   CUTLASS_DEVICE
   DualMmaMultistage(
       ///< Shared storage needed for internal use by threadblock-scoped GEMM
@@ -211,15 +228,15 @@
     this->warp_tile_iterator_B0_.add_tile_offset(
         {Base::kWarpGemmIterations * warp_idx_k, warp_idx_n});
     this->warp_tile_iterator_B1_.add_tile_offset(
         {Base::kWarpGemmIterations * warp_idx_k, warp_idx_n});
   }
 
   CUTLASS_DEVICE
-  void copy_tiles_and_advance(IteratorA &iterator_A, IteratorB &iterator_B0, IteratorB &iterator_B1,
+  void copy_tiles_and_advance(IteratorA &iterator_A, IteratorB0 &iterator_B0, IteratorB1 &iterator_B1,
                               int group_start_A = 0, int group_start_B = 0) {
     iterator_A.set_iteration_index(group_start_A *
                                    IteratorA::kAccessesPerVector);
     this->smem_iterator_A_.set_iteration_index(group_start_A);
 
     // Async Copy for operand A
     CUTLASS_PRAGMA_UNROLL
@@ -249,34 +266,34 @@
         }
 
         ++this->smem_iterator_A_;
       }
     }
 
     iterator_B0.set_iteration_index(group_start_B *
-                                   IteratorB::kAccessesPerVector);
+                                   IteratorB0::kAccessesPerVector);
     iterator_B1.set_iteration_index(group_start_B *
-                                   IteratorB::kAccessesPerVector);
+                                   IteratorB1::kAccessesPerVector);
     this->smem_iterator_B0_.set_iteration_index(group_start_B);
     this->smem_iterator_B1_.set_iteration_index(group_start_B);
 
     // Async Copy for operand B0
     CUTLASS_PRAGMA_UNROLL
     for (int j = 0; j < Detail::kAccessesPerGroupB; ++j) {
       if (group_start_B + j < Detail::AsyncCopyIterationsPerStageB) {
-        typename IteratorB::AccessType *dst_ptr =
-            reinterpret_cast<typename IteratorB::AccessType *>(
+        typename IteratorB0::AccessType *dst_ptr =
+            reinterpret_cast<typename IteratorB0::AccessType *>(
                 this->smem_iterator_B0_.get());
 
-        int const kSrcBytes = sizeof_bits<typename IteratorB::Element>::value *
-                              IteratorB::ThreadMap::kElementsPerAccess /
-                              IteratorB::kAccessesPerVector / 8;
+        int const kSrcBytes = sizeof_bits<typename IteratorB0::Element>::value *
+                              IteratorB0::ThreadMap::kElementsPerAccess /
+                              IteratorB0::kAccessesPerVector / 8;
 
         CUTLASS_PRAGMA_UNROLL
-        for (int v = 0; v < IteratorB::kAccessesPerVector; ++v) {
+        for (int v = 0; v < IteratorB0::kAccessesPerVector; ++v) {
           auto gmem_ptr = iterator_B0.get();
 
           if (SharedMemoryClear == SharedMemoryClearOption::kZfill) {
             cutlass::arch::cp_async_zfill<kSrcBytes, kCacheOpB>(
                 dst_ptr + v, gmem_ptr, iterator_B0.valid());
           } else {
             cutlass::arch::cp_async<kSrcBytes, kCacheOpB>(
@@ -288,24 +305,24 @@
         ++this->smem_iterator_B0_;
       }
     }
     // Async Copy for operand B1
     CUTLASS_PRAGMA_UNROLL
     for (int j = 0; j < Detail::kAccessesPerGroupB; ++j) {
       if (group_start_B + j < Detail::AsyncCopyIterationsPerStageB) {
-        typename IteratorB::AccessType *dst_ptr =
-            reinterpret_cast<typename IteratorB::AccessType *>(
+        typename IteratorB1::AccessType *dst_ptr =
+            reinterpret_cast<typename IteratorB1::AccessType *>(
                 this->smem_iterator_B1_.get());
 
-        int const kSrcBytes = sizeof_bits<typename IteratorB::Element>::value *
-                              IteratorB::ThreadMap::kElementsPerAccess /
-                              IteratorB::kAccessesPerVector / 8;
+        int const kSrcBytes = sizeof_bits<typename IteratorB1::Element>::value *
+                              IteratorB1::ThreadMap::kElementsPerAccess /
+                              IteratorB1::kAccessesPerVector / 8;
 
         CUTLASS_PRAGMA_UNROLL
-        for (int v = 0; v < IteratorB::kAccessesPerVector; ++v) {
+        for (int v = 0; v < IteratorB1::kAccessesPerVector; ++v) {
           auto gmem_ptr = iterator_B1.get();
 
           if (SharedMemoryClear == SharedMemoryClearOption::kZfill) {
             cutlass::arch::cp_async_zfill<kSrcBytes, kCacheOpB>(
                 dst_ptr + v, gmem_ptr, iterator_B1.valid());
           } else {
             cutlass::arch::cp_async<kSrcBytes, kCacheOpB>(
@@ -326,16 +343,16 @@
       int gemm_k_iterations,
       ///< destination accumulator tile
       FragmentC &accum0,
       FragmentC &accum1,
       ///< iterator over A operand in global memory
       IteratorA iterator_A,
       ///< iterator over B operand in global memory
-      IteratorB iterator_B0,
-      IteratorB iterator_B1,
+      IteratorB0 iterator_B0,
+      IteratorB1 iterator_B1,
       ///< initial value of accumulator
       FragmentC const &src_accum0,
       FragmentC const &src_accum1
     ) {
 
     //
     // Prologue
@@ -382,46 +399,46 @@
       iterator_B1.set_iteration_index(0);
       this->smem_iterator_B0_.set_iteration_index(0);
       this->smem_iterator_B1_.set_iteration_index(0);
 
       // Async Copy for operand B0
       CUTLASS_PRAGMA_UNROLL
       for (int j = 0; j < Detail::AsyncCopyIterationsPerStageB; ++j) {
-        typename IteratorB::AccessType *dst_ptr =
-            reinterpret_cast<typename IteratorB::AccessType *>(
+        typename IteratorB0::AccessType *dst_ptr =
+            reinterpret_cast<typename IteratorB0::AccessType *>(
                 this->smem_iterator_B0_.get());
 
         CUTLASS_PRAGMA_UNROLL
-        for (int v = 0; v < IteratorB::kAccessesPerVector; ++v) {
+        for (int v = 0; v < IteratorB0::kAccessesPerVector; ++v) {
           int const kSrcBytes =
-              sizeof_bits<typename IteratorB::Element>::value *
-              IteratorB::ThreadMap::kElementsPerAccess /
-              IteratorB::kAccessesPerVector / 8;
+              sizeof_bits<typename IteratorB0::Element>::value *
+              IteratorB0::ThreadMap::kElementsPerAccess /
+              IteratorB0::kAccessesPerVector / 8;
 
           cutlass::arch::cp_async_zfill<kSrcBytes, kCacheOpB>(
               dst_ptr + v, iterator_B0.get(), iterator_B0.valid());
 
           ++iterator_B0;
         }
 
         ++this->smem_iterator_B0_;
       }
       // Async Copy for operand B1
       CUTLASS_PRAGMA_UNROLL
       for (int j = 0; j < Detail::AsyncCopyIterationsPerStageB; ++j) {
-        typename IteratorB::AccessType *dst_ptr =
-            reinterpret_cast<typename IteratorB::AccessType *>(
+        typename IteratorB1::AccessType *dst_ptr =
+            reinterpret_cast<typename IteratorB1::AccessType *>(
                 this->smem_iterator_B1_.get());
 
         CUTLASS_PRAGMA_UNROLL
-        for (int v = 0; v < IteratorB::kAccessesPerVector; ++v) {
+        for (int v = 0; v < IteratorB1::kAccessesPerVector; ++v) {
           int const kSrcBytes =
-              sizeof_bits<typename IteratorB::Element>::value *
-              IteratorB::ThreadMap::kElementsPerAccess /
-              IteratorB::kAccessesPerVector / 8;
+              sizeof_bits<typename IteratorB1::Element>::value *
+              IteratorB1::ThreadMap::kElementsPerAccess /
+              IteratorB1::kAccessesPerVector / 8;
 
           cutlass::arch::cp_async_zfill<kSrcBytes, kCacheOpB>(
               dst_ptr + v, iterator_B1.get(), iterator_B1.valid());
 
           ++iterator_B1;
         }
 
@@ -469,43 +486,43 @@
                 last_smem_iterator_A.get());
 
         *dst_ptr = zero_A;
 
         ++last_smem_iterator_A;
       }
 
-      typename IteratorB::AccessType zero_B;
+      typename IteratorB0::AccessType zero_B;
       zero_B.clear();
 
       /// Iterator to write threadblock-scoped tile of B0 operand to shared memory
-      SmemIteratorB last_smem_iterator_B0(this->smem_iterator_B0_);
+      SmemIteratorB0 last_smem_iterator_B0(this->smem_iterator_B0_);
       last_smem_iterator_B0.set_iteration_index(0);
 
-      // Async Copy for operand B
+      // Async Copy for operand B0
       CUTLASS_PRAGMA_UNROLL
       for (int j = 0; j < Detail::AsyncCopyIterationsPerStageB; ++j) {
-
-        typename IteratorB::AccessType *dst_ptr =
-            reinterpret_cast<typename IteratorB::AccessType *>(
+        typename IteratorB0::AccessType *dst_ptr =
+            reinterpret_cast<typename IteratorB0::AccessType *>(
                 last_smem_iterator_B0.get());
 
         *dst_ptr = zero_B;
 
         ++last_smem_iterator_B0;
       }
+
       /// Iterator to write threadblock-scoped tile of B1 operand to shared memory
-      SmemIteratorB last_smem_iterator_B1(this->smem_iterator_B1_);
+      SmemIteratorB1 last_smem_iterator_B1(this->smem_iterator_B1_);
       last_smem_iterator_B1.set_iteration_index(0);
 
-      // Async Copy for operand B
+      // Async Copy for operand B1
       CUTLASS_PRAGMA_UNROLL
       for (int j = 0; j < Detail::AsyncCopyIterationsPerStageB; ++j) {
 
-        typename IteratorB::AccessType *dst_ptr =
-            reinterpret_cast<typename IteratorB::AccessType *>(
+        typename IteratorB1::AccessType *dst_ptr =
+            reinterpret_cast<typename IteratorB1::AccessType *>(
                 last_smem_iterator_B1.get());
 
         *dst_ptr = zero_B;
 
         ++last_smem_iterator_B1;
       }
     }
@@ -513,21 +530,22 @@
     // Waits until stages up to the previous (kStages-2)th stage have committed.
     cutlass::arch::cp_async_wait<Base::kStages - 2>();
     __syncthreads();
 
     // Pair of fragments used to overlap shared memory loads and math
     // instructions
     WarpLoadedFragmentA warp_loaded_frag_A[2];
-    WarpLoadedFragmentB warp_loaded_frag_B0[2];
-    WarpLoadedFragmentB warp_loaded_frag_B1[2];
+    WarpLoadedFragmentB0 warp_loaded_frag_B0[2];
+    WarpLoadedFragmentB1 warp_loaded_frag_B1[2];
     WarpTransformedFragmentA warp_transformed_frag_A[2];
-    WarpTransformedFragmentB warp_transformed_frag_B0[2];
-    WarpTransformedFragmentB warp_transformed_frag_B1[2];
+    WarpTransformedFragmentB0 warp_transformed_frag_B0[2];
+    WarpTransformedFragmentB1 warp_transformed_frag_B1[2];
 
-    Operator warp_mma;
+    Operator0 warp_mma0;
+    Operator1 warp_mma1;
 
     this->warp_tile_iterator_A_.set_kgroup_index(0);
     this->warp_tile_iterator_B0_.set_kgroup_index(0);
     this->warp_tile_iterator_B1_.set_kgroup_index(0);
 
     this->warp_tile_iterator_A_.load(warp_loaded_frag_A[0]);
     this->warp_tile_iterator_B0_.load(warp_loaded_frag_B0[0]);
@@ -540,29 +558,29 @@
     iterator_A.clear_mask(gemm_k_iterations == 0);
     iterator_B0.clear_mask(gemm_k_iterations == 0);
     iterator_B1.clear_mask(gemm_k_iterations == 0);
 
     int smem_write_stage_idx = Base::kStages - 1;
     int smem_read_stage_idx = 0;
 
-    warp_mma.transform(warp_transformed_frag_A[0], warp_transformed_frag_B0[0],
-                       warp_loaded_frag_A[0], warp_loaded_frag_B0[0]);
-    warp_mma.transform(warp_transformed_frag_A[0], warp_transformed_frag_B1[0],
-                       warp_loaded_frag_A[0], warp_loaded_frag_B1[0]);
+    warp_mma0.transform(warp_transformed_frag_A[0], warp_transformed_frag_B0[0],
+                        warp_loaded_frag_A[0], warp_loaded_frag_B0[0]);
+    warp_mma1.transform(warp_transformed_frag_A[0], warp_transformed_frag_B1[0],
+                        warp_loaded_frag_A[0], warp_loaded_frag_B1[0]);
 
     // tf32x3 kernels use staging accumulation. warp_mma uses a temporary
     // accumulator and this temporary accumulator is added to the final
     // accumulator once in every mainloop iteration.
     plus<FragmentC> plus_accum;
 
     FragmentC tmp_accum0, tmp_accum1;
 
-    if (platform::is_same<typename Operator::MathOperator,
+    if (platform::is_same<typename Operator0::MathOperator,
                           arch::OpMultiplyAddFastF32>::value
-      || platform::is_same<typename Operator::MathOperator,
+      || platform::is_same<typename Operator0::MathOperator,
                            arch::OpMultiplyAddComplexFastF32>::value) {
 
       tmp_accum0.clear();
       tmp_accum1.clear();
     }
 
     //
@@ -593,56 +611,56 @@
         this->warp_tile_iterator_B1_.load(warp_loaded_frag_B1[(warp_mma_k + 1) % 2]);
 
         ++this->warp_tile_iterator_A_;
         ++this->warp_tile_iterator_B0_;
         ++this->warp_tile_iterator_B1_;
 
         if (warp_mma_k > 0) {
-          warp_mma.transform(warp_transformed_frag_A[warp_mma_k % 2],
-                             warp_transformed_frag_B0[warp_mma_k % 2],
-                             warp_loaded_frag_A[warp_mma_k % 2],
-                             warp_loaded_frag_B0[warp_mma_k % 2]);
-          warp_mma.transform(warp_transformed_frag_A[warp_mma_k % 2],
-                             warp_transformed_frag_B1[warp_mma_k % 2],
-                             warp_loaded_frag_A[warp_mma_k % 2],
-                             warp_loaded_frag_B1[warp_mma_k % 2]);
+          warp_mma0.transform(warp_transformed_frag_A[warp_mma_k % 2],
+                              warp_transformed_frag_B0[warp_mma_k % 2],
+                              warp_loaded_frag_A[warp_mma_k % 2],
+                              warp_loaded_frag_B0[warp_mma_k % 2]);
+          warp_mma1.transform(warp_transformed_frag_A[warp_mma_k % 2],
+                              warp_transformed_frag_B1[warp_mma_k % 2],
+                              warp_loaded_frag_A[warp_mma_k % 2],
+                              warp_loaded_frag_B1[warp_mma_k % 2]);
         }
 
-        if (platform::is_same<typename Operator::MathOperator,
+        if (platform::is_same<typename Operator0::MathOperator,
                               arch::OpMultiplyAddFastF32>::value
-          || platform::is_same<typename Operator::MathOperator,
+          || platform::is_same<typename Operator0::MathOperator,
                                arch::OpMultiplyAddComplexFastF32>::value) {
 
-          warp_mma(
+          warp_mma0(
             tmp_accum0,
             warp_transformed_frag_A[warp_mma_k % 2],
             warp_transformed_frag_B0[warp_mma_k % 2], 
             tmp_accum0
           );
-          warp_mma(
+          warp_mma1(
             tmp_accum1,
             warp_transformed_frag_A[warp_mma_k % 2],
             warp_transformed_frag_B1[warp_mma_k % 2], 
             tmp_accum1
           );
 
           if (warp_mma_k == 0) {
             accum0 = plus_accum(accum0, tmp_accum0);
             accum1 = plus_accum(accum1, tmp_accum1);
             tmp_accum0.clear();
             tmp_accum1.clear();
           }
         } else {
-          warp_mma(
+          warp_mma0(
             accum0,
             warp_transformed_frag_A[warp_mma_k % 2],
             warp_transformed_frag_B0[warp_mma_k % 2],
             accum0
           );
-          warp_mma(
+          warp_mma1(
             accum1,
             warp_transformed_frag_A[warp_mma_k % 2],
             warp_transformed_frag_B1[warp_mma_k % 2],
             accum1
           );
         }
 
@@ -692,22 +710,22 @@
             smem_write_stage_idx = 0;
           } else {
             ++smem_write_stage_idx;
           }
 
           if (smem_read_stage_idx == (Base::kStages - 1)) {
             this->warp_tile_iterator_A_.add_tile_offset(
-                {0, -Base::kStages * Policy::kPartitionsK *
+                {0, -Base::kStages * Policy0::kPartitionsK *
                         Base::kWarpGemmIterations});
             this->warp_tile_iterator_B0_.add_tile_offset(
-                {-Base::kStages * Policy::kPartitionsK *
+                {-Base::kStages * Policy0::kPartitionsK *
                      Base::kWarpGemmIterations,
                  0});
             this->warp_tile_iterator_B1_.add_tile_offset(
-                {-Base::kStages * Policy::kPartitionsK *
+                {-Base::kStages * Policy1::kPartitionsK *
                      Base::kWarpGemmIterations,
                  0});
             smem_read_stage_idx = 0;
           } else {
             ++smem_read_stage_idx;
           }
 
@@ -716,30 +734,30 @@
           iterator_B0.clear_mask(gemm_k_iterations == 0);
           iterator_B1.clear_mask(gemm_k_iterations == 0);
         }
 
         // Do any conversions feeding the first stage at the end of the loop so
         // we can start right away on mma instructions
         if (warp_mma_k + 1 == Base::kWarpGemmIterations) {
-          warp_mma.transform(warp_transformed_frag_A[(warp_mma_k + 1) % 2],
-                             warp_transformed_frag_B0[(warp_mma_k + 1) % 2],
-                             warp_loaded_frag_A[(warp_mma_k + 1) % 2],
-                             warp_loaded_frag_B0[(warp_mma_k + 1) % 2]);
-          warp_mma.transform(warp_transformed_frag_A[(warp_mma_k + 1) % 2],
-                             warp_transformed_frag_B1[(warp_mma_k + 1) % 2],
-                             warp_loaded_frag_A[(warp_mma_k + 1) % 2],
-                             warp_loaded_frag_B1[(warp_mma_k + 1) % 2]);
+          warp_mma0.transform(warp_transformed_frag_A[(warp_mma_k + 1) % 2],
+                              warp_transformed_frag_B0[(warp_mma_k + 1) % 2],
+                              warp_loaded_frag_A[(warp_mma_k + 1) % 2],
+                              warp_loaded_frag_B0[(warp_mma_k + 1) % 2]);
+          warp_mma1.transform(warp_transformed_frag_A[(warp_mma_k + 1) % 2],
+                              warp_transformed_frag_B1[(warp_mma_k + 1) % 2],
+                              warp_loaded_frag_A[(warp_mma_k + 1) % 2],
+                              warp_loaded_frag_B1[(warp_mma_k + 1) % 2]);
         }
       }
 
     }
 
-    if (platform::is_same<typename Operator::MathOperator,
+    if (platform::is_same<typename Operator0::MathOperator,
                           arch::OpMultiplyAddFastF32>::value
-      || platform::is_same<typename Operator::MathOperator,
+      || platform::is_same<typename Operator0::MathOperator,
                            arch::OpMultiplyAddComplexFastF32>::value) {
       accum0 = plus_accum(accum0, tmp_accum0); 
       accum1 = plus_accum(accum1, tmp_accum1); 
     }
  
     if (SharedMemoryClear == SharedMemoryClearOption::kZfill) {
       // commit and drain all pending and predicated cp.async pnz from the GEMM mainloop
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/46_depthwise_simt_conv2dfprop/depthwise_simt_conv2dfprop.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/46_depthwise_simt_conv2dfprop/depthwise_simt_conv2dfprop.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/47_ampere_gemm_universal_streamk/ampere_gemm_universal_streamk.cu` & `flash_attn-2.0.0/csrc/cutlass/examples/47_ampere_gemm_universal_streamk/ampere_gemm_universal_streamk.cu`

 * *Files 0% similar despite different names*

```diff
@@ -30,15 +30,15 @@
  **************************************************************************************************/
 
 /***************************************************************************************************
  Example contrasting the Stream-K parallel decomposition for GEMM threadblocks versus the
  "classic data-parallel" and "Split-K" decompositions.
 
  For more details regarding the Stream-K method, see "Stream-K: Work-centric Parallel Decomposition
- for Dense Matrix-Matrix Multiplication on the GPU" (https://arxiv.org/abs/2301.03598) 
+ for Dense Matrix-Matrix Multiplication on the GPU" (https://arxiv.org/abs/2301.03598)
 
  Requires NVIDIA Ampere or newer device (SM80+).
 
  - To lock persistence mode, power (400W), clocks (1005MHz) for evaluation (assumes device 0 and A100)
 
      cutlass$ sudo nvidia-smi -pm 1 -i 0
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/examples/60_cutlass_import/main.cpp` & `flash_attn-2.0.0/csrc/cutlass/examples/60_cutlass_import/main.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/aligned_buffer.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/aligned_buffer.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/arch.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/arch.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/cache_operation.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/cache_operation.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/memory.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/memory.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm80.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/memory_sm80.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/mma.h`

 * *Files 0% similar despite different names*

```diff
@@ -220,9 +220,8 @@
 #include "cutlass/arch/mma_sm60.h"
 #include "cutlass/arch/mma_sm61.h"
 #include "cutlass/arch/mma_sm70.h"
 #include "cutlass/arch/mma_sm75.h"
 #include "cutlass/arch/mma_sm80.h"
 #include "cutlass/arch/mma_sparse_sm80.h"
 #include "cutlass/arch/mma_sm90.h"
-
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm50.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/mma_sm50.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm60.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/mma_sm60.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm61.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/mma_sm61.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm70.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/mma_sm70.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm75.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/mma_sm75.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm80.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/mma_sm80.h`

 * *Files 0% similar despite different names*

```diff
@@ -2162,15 +2162,15 @@
         "{%4,%5,%6,%7}, "
         "{%8,%9}, {%10,%11,%12,%13};\n"
         : "=r"(D[0]), "=r"(D[1]), "=r"(D[2]), "=r"(D[3])
         : "r"(A[0]), "r"(A[1]), "r"(A[2]), "r"(A[3]), "r"(B[0]), "r"(B[1]),
           "r"(C[0]), "r"(C[1]), "r"(C[2]), "r"(C[3]));
 
 #else
-
+    
     CUTLASS_UNUSED(a);
     CUTLASS_UNUSED(b);
     CUTLASS_UNUSED(c);
     CUTLASS_UNUSED(d);
     assert(0);
 
 #endif // defined(CUTLASS_ARCH_MMA_SM80_ENABLED)
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm90.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/herk_cf64_cf64_tensor_op_f64_sm90.cu`

 * *Files 25% similar despite different names*

```diff
@@ -25,107 +25,69 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Matrix multiply
+    \brief Tests for device-wide HERK interface
 */
 
-#pragma once
+#include <iostream>
 
-#if defined(__CUDACC_RTC__)
-#include <cuda/std/cassert>
-#else
-#include <assert.h>
-#endif
-
-#include "mma.h"
-#include "cutlass/layout/matrix.h"
-#include "cutlass/numeric_types.h"
-
-////////////////////////////////////////////////////////////////////////////////
-
-#if ((__CUDACC_VER_MAJOR__ > 11) || (__CUDACC_VER_MAJOR__ == 11 && __CUDACC_VER_MINOR__ >= 8))
-#define CUTLASS_ARCH_MMA_SM90_SUPPORTED 1
-#if (defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 900))
-#define CUTLASS_ARCH_MMA_SM90_ENABLED
-#endif
-#endif
-
-////////////////////////////////////////////////////////////////////////////////
-
-namespace cutlass {
-namespace arch {
-
-////////////////////////////////////////////////////////////////////////////////
-/// Matrix Multiply-Add 16x8x4 fp64
-////////////////////////////////////////////////////////////////////////////////
-
-/// Matrix multiply-add operation: F64 = F64 * F64 + F64
-template <>
-struct Mma<
-  gemm::GemmShape<16,8,4>,
-  32,
-  double,
-  layout::RowMajor,
-  double,
-  layout::ColumnMajor,
-  double,
-  layout::RowMajor,
-  OpMultiplyAdd> {
-
-  using Shape = gemm::GemmShape<16,8,4>;
-
-  using ElementA = double;
-  using LayoutA = layout::RowMajor;
-  using FragmentA = Array<double, 2>;
-
-  using ElementB = double;
-  using LayoutB = layout::ColumnMajor;
-  using FragmentB = Array<double, 1>;
-
-  using ElementC = double;
-  using LayoutC = layout::RowMajor;
-  using FragmentC = Array<double, 4>;
-
-  using Operator = OpMultiplyAdd;
-
-  using ArchTag = arch::Sm90;
-
-  CUTLASS_HOST_DEVICE
-  void operator()(FragmentC &d, FragmentA const &a, FragmentB const &b,
-                  FragmentC const &c) const {
-
-#if defined(CUTLASS_ARCH_MMA_SM90_ENABLED)
-
-  double const *A = reinterpret_cast<double const *>(&a);
-  double const *B = reinterpret_cast<double const *>(&b);
-
-  double const *C = reinterpret_cast<double const *>(&c);
-  double *D = reinterpret_cast<double *>(&d);
-
-  asm volatile("mma.sync.aligned.m16n8k4.row.col.f64.f64.f64.f64 {%0, %1, %2, %3}, {%4, %5}, {%6}, {%7, %8, %9, %10};\n"
-      : "=d"(D[0]), "=d"(D[1]), "=d"(D[2]), "=d"(D[3])
-      : "d"(A[0]), "d"(A[1]), 
-        "d"(B[0]), 
-        "d"(C[0]), "d"(C[1]), "d"(C[2]), "d"(C[3]));
-
-#else
-
-    CUTLASS_UNUSED(d);
-    CUTLASS_UNUSED(a);
-    CUTLASS_UNUSED(b);
-    CUTLASS_UNUSED(c);
-    CUTLASS_NOT_IMPLEMENTED();
-    
-#endif
-  }
-};
+#include "../../common/cutlass_unit_test.h"
+#include "cutlass/blas3.h"
+#include "cutlass/gemm/device/rank_k.h"
+#include "cutlass/util/host_tensor.h"
+#include "cutlass/util/reference/host/rank_k_complex.h"
+#include "cutlass/util/reference/host/tensor_compare.h"
+#include "cutlass/util/reference/host/tensor_copy.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/tensor_view_io.h"
+
+#include "testbed_rank_k_universal.h"
+
+#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
+// HERK operator on CUBLAS_OP_C (row-major + conj) input layouts
+TEST(SM90_Device_Herk_cf64h_cf64n_l_tensor_op_f64, 64x64x16_32x32x16) {
 
-} // namespace arch
-} // namespace cutlass
+  using ElementA = cutlass::complex<double>;
+  using LayoutA = cutlass::layout::RowMajor;
 
+  using ElementC = cutlass::complex<double>;
+  using LayoutC = cutlass::layout::ColumnMajor;
+  using ElementAccumulator = cutlass::complex<double>;
+
+  using RankK = cutlass::gemm::device::RankK<
+    ElementA,
+    LayoutA,
+    ElementC,
+    LayoutC,
+    cutlass::FillMode::kLower,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm90,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
+    cutlass::gemm::GemmShape<16, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementC,
+      1,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    4,     // kStages 
+    1,     // AlignmentA
+    false, // SplitKSerial
+    cutlass::arch::OpMultiplyAddComplex,
+    cutlass::ComplexTransform::kConjugate,
+    cutlass::BlasMode::kHermitian
+  >;
+
+  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
+}
 /////////////////////////////////////////////////////////////////////////////////////////////////
+
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sparse_sm80.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/mma_sparse_sm80.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/simd.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/simd.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/simd_sm60.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/simd_sm60.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/simd_sm61.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/simd_sm61.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/wmma.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/wmma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm70.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/wmma_sm70.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm72.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/wmma_sm72.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm75.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/arch/wmma_sm75.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/array.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/array.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/array_planar_complex.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/array_planar_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/array_subbyte.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/array_subbyte.h`

 * *Files 1% similar despite different names*

```diff
@@ -366,16 +366,14 @@
   public:
 
     CUTLASS_HOST_DEVICE
     reverse_iterator(): ptr_(nullptr), idx_(0) { }
 
     CUTLASS_HOST_DEVICE
     reverse_iterator(Storage *ptr, int idx = 0): ptr_(ptr), idx_(idx) { }
-
-    // TODO
   };
 
   /// Bidirectional constant iterator over elements
   class const_reverse_iterator {
 
     /// Pointer to storage element
     Storage const *ptr_;
@@ -386,16 +384,14 @@
   public:
 
     CUTLASS_HOST_DEVICE
     const_reverse_iterator(): ptr_(nullptr), idx_(0) { }
 
     CUTLASS_HOST_DEVICE
     const_reverse_iterator(Storage const *ptr, int idx = 0): ptr_(ptr), idx_(idx) { }
-
-    // TODO
   };
 
 private:
 
   /// Internal storage
   Storage storage[kStorageElements];
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/barrier.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/barrier.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/bfloat16.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/bfloat16.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/blas3.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/blas3.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/block_striped.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/block_striped.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/complex.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/constants.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/constants.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/conv2d_problem_size.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/conv2d_problem_size.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/conv3d_problem_size.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/conv3d_problem_size.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/convolution.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/convolution.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/device/direct_convolution.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/device/direct_convolution.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/device/implicit_gemm_convolution.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/device/implicit_gemm_convolution.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/device/implicit_gemm_convolution_fusion.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/device/implicit_gemm_convolution_fusion.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/default_conv2d.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_dgrad.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/default_conv2d_dgrad.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_fusion.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_fusion.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_broadcast.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_broadcast.h`

 * *Files 2% similar despite different names*

```diff
@@ -103,15 +103,15 @@
   using Epilogue = typename cutlass::conv::kernel::detail::DefaultConvEpilogueWithBroadcastTensorOp<
     ArchTag,
     typename ImplicitGemmBase::Epilogue::Shape,
     typename ImplicitGemmBase::Epilogue::WarpMmaOperator,
     ImplicitGemmBase::Epilogue::kPartitionsK,
     ElementC,
     typename EpilogueOutputOp::ElementT,
-    ElementC,
+    typename EpilogueOutputOp::ElementVector,
     EpilogueOutputOp,
     ImplicitGemmBase::Epilogue::kElementsPerAccess
   >::Epilogue;
 
   // Define the kernel
   using Kernel = cutlass::conv::kernel::ImplicitGemmConvolutionWithFusedEpilogue<
     typename ImplicitGemmBase::Mma,
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_reduction.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_reduction.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_group_fprop.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/default_conv2d_group_fprop.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad_fusion.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad_fusion.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_dgrad.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/default_conv3d_dgrad.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop_fusion.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop_fusion.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_wgrad.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/default_conv3d_wgrad.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_depthwise_fprop.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/default_depthwise_fprop.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/direct_convolution.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/direct_convolution.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution.h`

 * *Files 1% similar despite different names*

```diff
@@ -328,15 +328,15 @@
         threadblock_tile_idx.k() * Mma::Shape::kK,
         threadblock_tile_idx.n() * Mma::Shape::kN
       )
     );
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
-    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
+    int warp_idx = canonical_warp_idx();
     int lane_idx = threadIdx.x % 32;
 
     //
     // Main loop
     //
 
     // Construct thread-scoped matrix multiply
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_fusion.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_fusion.h`

 * *Files 1% similar despite different names*

```diff
@@ -335,15 +335,15 @@
                   // Wgrad
                   (threadblock_tile_idx.n() * Mma::Shape::kN)
       )
     );
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
-    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
+    int warp_idx = canonical_warp_idx();
     int lane_idx = threadIdx.x % 32;
 
     //
     // Main loop
     //
 
     // Construct thread-scoped matrix multiply
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_strided_dgrad.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_strided_dgrad.h`

 * *Files 1% similar despite different names*

```diff
@@ -331,15 +331,15 @@
 
     typename Mma::FragmentC accumulators;
 
     accumulators.clear();
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
-    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
+    int warp_idx = canonical_warp_idx();
     int lane_idx = threadIdx.x % 32;
 
     // Check if CTA contributes valid MMA (Dy * w) and accumulator will be non-zero after MMA
     if (start_r < params.problem_size.R && start_s < params.problem_size.S) {
       // Scale gemm_k_iterations for strided dgrad
       int gemm_k_iterations = (params.gemm_k_iterations / (params.problem_size.R * params.problem_size.S)
                               ) * params.problem_size.num_gemm_k_filter_positions(start_r, start_s);
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_with_fused_epilogue.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_with_fused_epilogue.h`

 * *Files 2% similar despite different names*

```diff
@@ -337,15 +337,15 @@
         threadblock_tile_idx.k() * Mma::Shape::kK,
         threadblock_tile_idx.n() * Mma::Shape::kN
       )
     );
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
-    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
+    int warp_idx = canonical_warp_idx();
     int lane_idx = threadIdx.x % 32;
 
     //
     // Main loop
     //
 
     // Construct thread-scoped matrix multiply
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/thread/depthwise_mma.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/thread/depthwise_mma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_analytic.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_analytic.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_optimized.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_analytic.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_analytic.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_optimized.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_analytic.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_analytic.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_few_channels.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_few_channels.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_fixed_channels.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_fixed_channels.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_optimized.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_analytic.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_analytic.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_few_channels.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_few_channels.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_fixed_channels.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_fixed_channels.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_optimized.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_params.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_params.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_tile_iterator.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_tile_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_analytic.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_analytic.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_optimized.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_analytic.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_analytic.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_optimized.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_analytic.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_analytic.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_optimized.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_analytic.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_analytic.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_optimized.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_analytic.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_analytic.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_optimized.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_analytic.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_analytic.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_optimized.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_params.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv3d_params.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_analytic.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_analytic.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_optimized.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_analytic.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_analytic.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_optimized.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_direct_conv_params.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/depthwise_direct_conv_params.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_fixed_stride_dilation.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_fixed_stride_dilation.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_optimized.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_direct_conv_multistage.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_direct_conv_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_filter_tile_access_iterator_direct_conv_optimized.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_filter_tile_access_iterator_direct_conv_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_pipelined.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_pipelined.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_mma_base.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/depthwise_mma_base.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_mma_core_with_lane_access_size.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/depthwise_mma_core_with_lane_access_size.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_fprop_fusion_multistage.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/implicit_gemm_fprop_fusion_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_multistage.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/implicit_gemm_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_pipelined.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/implicit_gemm_pipelined.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_wgrad_fusion_multistage.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/implicit_gemm_wgrad_fusion_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_access_iterator.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_access_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_iterator.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/threadblock_swizzle.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/threadblock/threadblock_swizzle.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/warp/mma_depthwise_simt.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/warp/mma_depthwise_simt.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/warp/mma_depthwise_simt_tile_iterator.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/warp/mma_depthwise_simt_tile_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/conv/warp/scale_bias_relu_transform.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/conv/warp/scale_bias_relu_transform.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/coord.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/coord.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/core_io.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/core_io.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/cutlass.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/cutlass.h`

 * *Files 7% similar despite different names*

```diff
@@ -68,28 +68,28 @@
   #define CUTLASS_UNUSED(expr) do { ; } while (&expr != &expr)
 #endif
 
 #if !defined(__CUDACC_RTC__)
 
 #include <assert.h>
 
-#if defined(_MSC_VER)
-  #define CUTLASS_NOT_IMPLEMENTED() assert(0 && __FUNCSIG__)
-#else
-  #define CUTLASS_NOT_IMPLEMENTED() assert(0 && __PRETTY_FUNCTION__)
-#endif
-
-#else
-
-#if defined(_MSC_VER)
-  #define CUTLASS_NOT_IMPLEMENTED() assert(0 && __FUNCSIG__)
-#else
-  #define CUTLASS_NOT_IMPLEMENTED() assert(0 && __PRETTY_FUNCTION__)
-#endif
+  #if defined(__CUDA_ARCH__)
+    #if defined(_MSC_VER)
+      #define CUTLASS_NOT_IMPLEMENTED() { printf("%s not implemented\n", __FUNCSIG__); asm volatile ("brkpt;\n"); }
+    #else
+      #define CUTLASS_NOT_IMPLEMENTED() { printf("%s not implemented\n", __PRETTY_FUNCTION__); asm volatile ("brkpt;\n"); }
+    #endif
 
+  #else
+    #if defined(_MSC_VER)
+      #define CUTLASS_NOT_IMPLEMENTED() assert(0 && __FUNCSIG__)
+    #else
+      #define CUTLASS_NOT_IMPLEMENTED() assert(0 && __PRETTY_FUNCTION__)
+    #endif
+  #endif
 #endif
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 
 /// Status code returned by CUTLASS operations
@@ -177,28 +177,51 @@
     #define CUTLASS_PRAGMA_NO_UNROLL
     #define CUTLASS_GEMM_LOOP
 
 #endif
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
-static const int NUM_THREADS_PER_WARP = 32;
-static const int NUM_THREADS_PER_HALF_WARP = NUM_THREADS_PER_WARP / 2;
-static const int NUM_THREADS_PER_QUAD = 4;
-static const int NUM_THREADS_PER_QUAD_PAIR = NUM_THREADS_PER_QUAD * 2;
+static const int NumThreadsPerWarp = 32;
+static const int NumThreadsPerWarpGroup = 128;
+static const int NumThreadsPerHalfWarp = NumThreadsPerWarp / 2;
+static const int NumThreadsPerQuad = 4;
+static const int NumThreadsPerQuadPair = NumThreadsPerQuad * 2;
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Helper function to return true when called by thread 0 of threadblock 0.
 CUTLASS_HOST_DEVICE bool thread0() {
   #if defined(__CUDA_ARCH__)
     return (!threadIdx.x && !threadIdx.y && !threadIdx.z) && (!blockIdx.x && !blockIdx.y && !blockIdx.z);
   #else
     return false;
   #endif
 }
 
+/// Returns a warp-uniform value indicating the canonical warp index of the calling threads.
+/// Threads within the warp must be converged.
+CUTLASS_DEVICE
+int canonical_warp_idx() { 
+  #if defined(__CUDA_ARCH__)
+    return __shfl_sync(0xffffffff, threadIdx.x / NumThreadsPerWarp, 0);
+  #else
+    return 0;
+  #endif
+}
+
+/// Returns a warp-uniform value indicating the canonical warp group index of the calling threads.
+/// Threads within the warp must be converged.
+CUTLASS_DEVICE
+int canonical_warp_group_idx() {
+  #if defined(__CUDA_ARCH__)
+    return __shfl_sync(0xffffffff, threadIdx.x / NumThreadsPerWarpGroup, 0);
+  #else
+    return 0;
+  #endif
+}
+
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 }  // namespace cutlass
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/device_kernel.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/real.h`

 * *Files 22% similar despite different names*

```diff
@@ -24,56 +24,38 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/*! \file
-    \brief Template for generic CUTLASS kernel.
+/**
+  \file
+  \brief This class provides helpers to support real<> and complex<> types in generic code.
 */
 
 #pragma once
 
-#include "cutlass/cutlass.h"
-////////////////////////////////////////////////////////////////////////////////
-
 namespace cutlass {
 
-////////////////////////////////////////////////////////////////////////////////
-
-/// Generic CUTLASS kernel template.
-template <typename Operator>
-__global__
-void Kernel(typename Operator::Params params) {
-  // Dynamic shared memory base pointer
-  extern __shared__ int SharedStorageBase[];
-
-  // Declare pointer to dynamic shared memory.
-  typename Operator::SharedStorage *shared_storage =
-      reinterpret_cast<typename Operator::SharedStorage *>(SharedStorageBase);
-
-  Operator op;
-
-  op(params, *shared_storage);
-}
-
-
-/// Generic CUTLASS kernel template.
-template <typename Operator>
-__global__
-void Kernel2(typename Operator::Params params) {
-  // Dynamic shared memory base pointer
-  extern __shared__ int SharedStorageBase[];
-
-  // Declare pointer to dynamic shared memory.
-  typename Operator::SharedStorage *shared_storage =
-      reinterpret_cast<typename Operator::SharedStorage *>(SharedStorageBase);
-
-  Operator::invoke(params, *shared_storage);
-
+/// Used to determine the real-valued underlying type of a numeric type T.
+template <typename T>
+struct RealType {
+  using Type = T;
+
+  /// Number of elements
+  static int const kExtent = 1;
+
+CUTLASS_HOST_DEVICE
+  static T from_real(double x) {
+    return static_cast<T>(x);
+  }
+};
+
+template <typename T>
+CUTLASS_HOST_DEVICE
+static T from_real(double r) {
+  return T(r);
 }
 
 
-////////////////////////////////////////////////////////////////////////////////
-} /// namespace cutlass
-
+} // namespace cutlass
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/activation.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/activation.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/conversion_op.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/conversion_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_with_elementwise.h`

 * *Files 15% similar despite different names*

```diff
@@ -25,224 +25,207 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-  \brief Functor performing linear combination operations used by epilogues.
+  
+  \brief Functor performing linear combination with elementwise
 */
 
 #pragma once
 
+#include <cutlass/half.h>
 #include "cutlass/cutlass.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/array.h"
+#include "cutlass/constants.h"
+#include "cutlass/fast_math.h"
 #include "cutlass/functional.h"
 #include "cutlass/numeric_conversion.h"
-#include "cutlass/epilogue/thread/scale_type.h"
-#include "cutlass/epilogue/thread/linear_combination_params.h"
+#include "cutlass/epilogue/thread/activation.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace thread {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Applies a linear combination operator to an array of elements.
 ///
 /// D = alpha * accumulator + beta * source + uniform
 ///
 template <
-  typename ElementOutput_,                             ///< Data type used to load and store tensors
-  int Count,                                           ///< Number of elements computed per operation.
+  typename ElementCompute_,                            ///< Data type returned by this functor
+  typename ElementAccumulator_,                        ///< Data type of accumulators
+  typename ElementSource_,                             ///< Data type of source tensor
+  typename ElementTensor_,                             ///< Data type of additional tensor
+  int Count,                                           ///< Number of elements computed per operation
                                                        ///< Usually it is 128/sizeof_bits<ElementOutput_>,
                                                        ///< but we use 64 or 32 sometimes when there are not enough data to store
-  typename ElementAccumulator_ = ElementOutput_,       ///< Accumulator data type
-  typename ElementCompute_ = ElementOutput_,           ///< Data type used to compute linear combination
-  ScaleType::Kind Scale = ScaleType::Default,          ///< Control Alpha and Beta scaling
   FloatRoundStyle Round = FloatRoundStyle::round_to_nearest
 >
-class LinearCombination {
+class LinearCombinationWithElementwise {
 public:
 
-  using ElementOutput = ElementOutput_;
-  using ElementAccumulator = ElementAccumulator_;
+  using ElementOutput = ElementSource_;
   using ElementCompute = ElementCompute_;
+  using ElementAccumulator = ElementAccumulator_;
+  using ElementSource = ElementSource_;
+  using ElementTensor = ElementTensor_;
+
+  static bool const kIsHeavy = true;
 
   static int const kCount = Count;
-  static const ScaleType::Kind kScale = Scale;
-  using FragmentOutput = Array<ElementOutput, kCount>;
+
+  using FragmentCompute = Array<ElementCompute, kCount>;
   using FragmentAccumulator = Array<ElementAccumulator, kCount>;
-  using ComputeFragment = Array<ElementCompute, kCount>;
+  using FragmentSource = Array<ElementSource, kCount>;
+  using FragmentTensor = Array<ElementTensor, kCount>;
 
-  using ParamsBase = LinearCombinationParams;
-  
   static FloatRoundStyle const kRound = Round;
 
   /// Host-constructable parameters structure
-  struct Params : ParamsBase{
+  struct Params {
+
     ElementCompute alpha;                  ///< scales accumulators
     ElementCompute beta;                   ///< scales source tensor
+    ElementCompute threshold;              ///< minimum value that is output 
     ElementCompute const *alpha_ptr;       ///< pointer to accumulator scalar - if not null, loads it from memory
     ElementCompute const *beta_ptr;        ///< pointer to source scalar - if not null, loads it from memory
+    //
+    // Methods
+    //
 
     CUTLASS_HOST_DEVICE
     Params(): 
-      ParamsBase(
-        ElementCompute(1), 
-        ElementCompute(0)
-      ),
       alpha(ElementCompute(1)), 
-      beta(ElementCompute(0)), 
+      beta(ElementCompute(0)),
+      threshold(ElementCompute(0)), 
       alpha_ptr(nullptr), 
       beta_ptr(nullptr) { }
 
     CUTLASS_HOST_DEVICE
     Params(
       ElementCompute alpha,
-      ElementCompute beta
-    ): 
-      ParamsBase(alpha, beta),
-      alpha(alpha), beta(beta), alpha_ptr(nullptr), beta_ptr(nullptr) { } 
+      ElementCompute beta,
+      ElementCompute threshold = ElementCompute(0)
+    ): alpha(alpha), beta(beta), threshold(threshold), alpha_ptr(nullptr), beta_ptr(nullptr) {
 
-    CUTLASS_HOST_DEVICE
-    Params(
-      ElementCompute alpha
-    ): 
-      ParamsBase(alpha, ElementCompute(0)),
-      alpha(alpha), beta(0), alpha_ptr(nullptr), beta_ptr(nullptr) { }
+    }
 
     CUTLASS_HOST_DEVICE
     Params(
       ElementCompute const *alpha_ptr,
-      ElementCompute const *beta_ptr
-    ): 
-      ParamsBase(*alpha_ptr, *beta_ptr),
-      alpha(0), beta(0), alpha_ptr(alpha_ptr), beta_ptr(beta_ptr) { }
-
-    CUTLASS_HOST_DEVICE
-    Params(
-      ElementCompute const *alpha_ptr
-    ):
-      ParamsBase(*alpha_ptr, ElementCompute(0)),
-      alpha(0), beta(0), alpha_ptr(alpha_ptr), beta_ptr(nullptr) { }
+      ElementCompute const *beta_ptr,
+      ElementCompute threshold = ElementCompute(0)
+    ): alpha(0), beta(0), threshold(threshold), alpha_ptr(alpha_ptr), beta_ptr(beta_ptr) {
 
-    CUTLASS_HOST_DEVICE
-    Params(
-      ParamsBase const& base
-    ): ParamsBase(base), alpha_ptr(nullptr), beta_ptr(nullptr) { 
-      #if defined(__CUDA_ARCH__)
-      alpha = reinterpret_cast<ElementCompute const&>(base.alpha_data);
-      beta = reinterpret_cast<ElementCompute const&>(base.beta_data);
-      #else
-      memcpy( alpha, base.alpha_data, sizeof(ElementCompute) ); 
-      memcpy( beta, base.alpha_data, sizeof(ElementCompute) ); 
-      #endif
     }
   };
 
 private:
 
   //
   // Data members
   //
 
   ElementCompute alpha_;
   ElementCompute beta_;
+  ElementCompute threshold_;
+  bool participates_in_reduction_;
 
 public:
 
   /// Constructs the function object, possibly loading from pointers in host memory
   CUTLASS_HOST_DEVICE
-  LinearCombination(Params const &params) {
+  LinearCombinationWithElementwise(Params const &params) {
+
     alpha_ = (params.alpha_ptr ? *params.alpha_ptr : params.alpha);
     beta_ = (params.beta_ptr ? *params.beta_ptr : params.beta);
+    threshold_ = params.threshold;
+    participates_in_reduction_ = true;
   }
 
   /// Returns true if source is needed
   CUTLASS_HOST_DEVICE
   bool is_source_needed() const {
-    if (Scale == ScaleType::NoBetaScaling) return true;
-
-    if (Scale == ScaleType::OnlyAlphaScaling) return false;
-
-    if (Scale == ScaleType::Nothing) return false;
-
     return beta_ != ElementCompute(0);
   }
 
+  /// Returns true if the threadblock computes the reduction
+  CUTLASS_HOST_DEVICE
+  bool participates_in_reduction() const {
+    return participates_in_reduction_;
+  }
+
   /// Functionally required for serial reduction in the epilogue
   CUTLASS_HOST_DEVICE
   void set_k_partition(int k_partition, int k_partition_count) {
     if (k_partition) {
       beta_ = ElementCompute(1);
     }
-  }
 
+    if (k_partition != k_partition_count - 1) {
+      // set to NaN to make ReLU no-op for all except last k partitions
+      int64_t allones = -1;
+      threshold_ = reinterpret_cast<ElementCompute const &>(allones);
+      // Avoid computing the reduction if this isn't the final Split-K slice
+      participates_in_reduction_ = false;
+    }
+  }
+  
   /// Computes linear scaling: D = alpha * accumulator + beta * source
   CUTLASS_HOST_DEVICE
-  FragmentOutput operator()(
+  FragmentCompute operator()(
     FragmentAccumulator const &accumulator, 
-    FragmentOutput const &source) const {
+    FragmentSource const &source,
+    FragmentTensor const &tensor) const {
 
     // Convert source to interal compute numeric type
-    NumericArrayConverter<ElementCompute, ElementOutput, kCount, Round> source_converter;
+    NumericArrayConverter<ElementCompute, ElementSource, kCount, Round> source_converter;
     NumericArrayConverter<ElementCompute, ElementAccumulator, kCount, Round> accumulator_converter;
 
-    // Convert to destination numeric type
-    NumericArrayConverter<ElementOutput, ElementCompute, kCount, Round> destination_converter;
-
-    ComputeFragment converted_source = source_converter(source);
-    ComputeFragment converted_accumulator = accumulator_converter(accumulator);
-
-    if (Scale == ScaleType::Nothing)
-      return destination_converter(converted_accumulator);
+    FragmentCompute converted_source = source_converter(source);
+    FragmentCompute converted_accumulator = accumulator_converter(accumulator);
 
     // Perform binary operations
-    ComputeFragment intermediate;
+    FragmentCompute intermediate;
 
-    multiplies<ComputeFragment> mul_add_source;
-    multiply_add<ComputeFragment> mul_add_accumulator;
-
-    if (Scale == ScaleType::NoBetaScaling)
-      intermediate = converted_source;
-    else
-      intermediate = mul_add_source(beta_, converted_source);                             // X =  beta * C + uniform
+    multiplies<FragmentCompute> mul_add_source;
+    multiply_add<FragmentCompute> mul_add_accumulator;
 
+    intermediate = mul_add_source(beta_, converted_source);                             // X =  beta * C + uniform
     intermediate = mul_add_accumulator(alpha_, converted_accumulator, intermediate);    // D = alpha * Accum + X
 
-    return destination_converter(intermediate);
+    return intermediate;
   }
 
   /// Computes linear scaling: D = alpha * accumulator
   CUTLASS_HOST_DEVICE
-  FragmentOutput operator()(
-    FragmentAccumulator const &accumulator) const {
+  FragmentCompute operator()(
+    FragmentAccumulator const &accumulator,
+    FragmentTensor const &tensor) const {
 
     // Convert source to interal compute numeric type
     NumericArrayConverter<ElementCompute, ElementAccumulator, kCount, Round> accumulator_converter;
 
-    // Convert to destination numeric type
-    NumericArrayConverter<ElementOutput, ElementCompute, kCount, Round> destination_converter;
-
-    ComputeFragment converted_accumulator = accumulator_converter(accumulator);
-
-    if (Scale == ScaleType::Nothing)
-      return destination_converter(converted_accumulator);
+    FragmentCompute converted_accumulator = accumulator_converter(accumulator);
 
     // Perform binary operations
-    ComputeFragment intermediate;
-    multiplies<ComputeFragment> mul_accumulator;
+    FragmentCompute intermediate;
+
+    multiplies<FragmentCompute> mul_accumulator;
 
-    intermediate = mul_accumulator(alpha_, converted_accumulator);    // D = alpha * Accum 
+    intermediate = mul_accumulator(alpha_, converted_accumulator);    // D = alpha * Accum
 
-    return destination_converter(intermediate);
+    return intermediate;
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace thread
 } // namespace epilogue
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_elementwise.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_elementwise.h`

 * *Files 1% similar despite different names*

```diff
@@ -57,25 +57,27 @@
   typename ElementC_,
   typename ElementAccumulator_,
   typename ElementCompute_,
   typename ElementZ_,
   typename ElementT_,
   int ElementsPerAccess,
   typename ElementwiseOp_ = Identity<ElementCompute_>,
-  typename BinaryOp_ = plus<ElementCompute_>
+  typename BinaryOp_ = plus<ElementCompute_>,
+  typename ElementVector_ = ElementC_
 >
 class LinearCombinationBiasElementwise {
 public:
 
   using ElementOutput = ElementC_;
   using ElementC = ElementC_;
   using ElementAccumulator = ElementAccumulator_;
   using ElementCompute = ElementCompute_;
   using ElementZ = ElementZ_;
   using ElementT = ElementT_;
+  using ElementVector = ElementVector_;
   static int const kElementsPerAccess = ElementsPerAccess;
   static int const kCount = kElementsPerAccess;
 
   using ElementwiseOp = ElementwiseOp_;
   using BinaryOp = BinaryOp_;
 
   // Indicates that this epilogue applies only one binary operation
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_relu.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_relu.h`

 * *Files 1% similar despite different names*

```diff
@@ -200,24 +200,26 @@
 /// EpilogueWithBroadcast::OutputOp
 template <
   typename ElementC_,
   typename ElementAccumulator_,
   typename ElementCompute_,
   typename ElementZ_,
   int ElementsPerAccess,
-  bool StoreT = true
+  bool StoreT = true,
+  typename ElementVector_ = ElementC_
 >
 class LinearCombinationBiasRelu {
 public:
 
   using ElementOutput = ElementC_;
   using ElementC = ElementC_;
   using ElementAccumulator = ElementAccumulator_;
   using ElementCompute = ElementCompute_;
   using ElementZ = ElementZ_;
+  using ElementVector = ElementVector_;
 
   using ElementT = uint1b_t;
 
   static int const kElementsPerAccess = ElementsPerAccess;
   static int const kCount = kElementsPerAccess;
 
   using ElementwiseOp = ReLu<ElementCompute>;
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_clamp.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_clamp.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_dgelu.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_dgelu.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_drelu.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_drelu.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_gelu.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_gelu.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_generic.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_generic.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_hardswish.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_hardswish.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_leaky_relu.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_leaky_relu.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_params.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_params.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_planar_complex.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_planar_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_relu.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_relu.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_relu0.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_relu0.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_residual_block.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_residual_block.h`

 * *Files 2% similar despite different names*

```diff
@@ -55,23 +55,25 @@
 
 /// Models a residual block of the form: UnaryOp(BinaryOp(BinaryOp(ActivationOp(TensorOp(X) + bias), residual1), residual2))
 template <typename ElementOutput_, typename ElementAccumulator_,
           typename ElementCompute_, typename ElementC_, int ElementsPerAccess,
           template <typename T> class ActivationOp_,
           template <typename T> class BinaryOp1_,
           template <typename T> class UnaryOp_,
-          template <typename T> class BinaryOp2_ = detail::NoOp>
+          template <typename T> class BinaryOp2_ = detail::NoOp,
+          typename ElementVector_ = ElementC_>
 class LinearCombinationResidualBlock {
 public:
   static bool const kIsSingleSource = false;
 
   using ElementOutput = ElementC_;
   using ElementC = ElementC_;
   using ElementAccumulator = ElementAccumulator_;
   using ElementCompute = ElementCompute_;
+  using ElementVector = ElementVector_;
   static int const kElementsPerAccess = ElementsPerAccess;
   static int const kCount = kElementsPerAccess;
 
   using UnaryOp = UnaryOp_<Array<ElementCompute, kCount>>;
   using BinaryOp1 = BinaryOp1_<Array<ElementCompute, kCount>>;
   using BinaryOp2 = BinaryOp2_<Array<ElementCompute, kCount>>;
   using ActivationOp = ActivationOp_<Array<ElementCompute, kCount>>;
@@ -175,26 +177,28 @@
 };
 
 /// Models a residual block of the form: UnaryOp(BinaryOp(ActivationOp(TensorOp(X) + bias), residual))
 template <typename ElementOutput_, typename ElementAccumulator_,
           typename ElementCompute_, typename ElementC_, int ElementsPerAccess,
           template <typename T> class ActivationOp_,
           template <typename T> class BinaryOp1_,
-          template <typename T> class UnaryOp_>
+          template <typename T> class UnaryOp_,
+          typename ElementVector_>
 class LinearCombinationResidualBlock<ElementOutput_, ElementAccumulator_,
           ElementCompute_, ElementC_, ElementsPerAccess,
           ActivationOp_, BinaryOp1_, UnaryOp_,
-          detail::NoOp> {
+          detail::NoOp, ElementVector_> {
 public:
   static bool const kIsSingleSource = true;
 
   using ElementOutput = ElementC_;
   using ElementC = ElementC_;
   using ElementAccumulator = ElementAccumulator_;
   using ElementCompute = ElementCompute_;
+  using ElementVector = ElementVector_;
   static int const kElementsPerAccess = ElementsPerAccess;
   static int const kCount = kElementsPerAccess;
 
   using UnaryOp = UnaryOp_<Array<ElementCompute, kCount>>;
   using BinaryOp = BinaryOp1_<Array<ElementCompute, kCount>>;
   using ActivationOp = ActivationOp_<Array<ElementCompute, kCount>>;
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_sigmoid.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_sigmoid.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_silu.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination_silu.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_with_elementwise.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/linear_combination.h`

 * *Files 22% similar despite different names*

```diff
@@ -25,207 +25,279 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-  
-  \brief Functor performing linear combination with elementwise
+  \brief Functor performing linear combination operations used by epilogues.
 */
 
 #pragma once
 
-#include <cutlass/half.h>
 #include "cutlass/cutlass.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/array.h"
-#include "cutlass/constants.h"
-#include "cutlass/fast_math.h"
 #include "cutlass/functional.h"
 #include "cutlass/numeric_conversion.h"
-#include "cutlass/epilogue/thread/activation.h"
+#include "cutlass/epilogue/thread/scale_type.h"
+#include "cutlass/epilogue/thread/linear_combination_params.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace thread {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Applies a linear combination operator to an array of elements.
 ///
 /// D = alpha * accumulator + beta * source + uniform
 ///
 template <
-  typename ElementCompute_,                            ///< Data type returned by this functor
-  typename ElementAccumulator_,                        ///< Data type of accumulators
-  typename ElementSource_,                             ///< Data type of source tensor
-  typename ElementTensor_,                             ///< Data type of additional tensor
-  int Count,                                           ///< Number of elements computed per operation
+  typename ElementOutput_,                             ///< Data type used to load and store tensors
+  int Count,                                           ///< Number of elements computed per operation.
                                                        ///< Usually it is 128/sizeof_bits<ElementOutput_>,
                                                        ///< but we use 64 or 32 sometimes when there are not enough data to store
-  FloatRoundStyle Round = FloatRoundStyle::round_to_nearest
+  typename ElementAccumulator_ = ElementOutput_,       ///< Accumulator data type
+  typename ElementCompute_ = ElementOutput_,           ///< Data type used to compute linear combination
+  ScaleType::Kind Scale = ScaleType::Default,          ///< Control Alpha and Beta scaling
+  FloatRoundStyle Round = FloatRoundStyle::round_to_nearest,
+  typename ElementSource_ = ElementOutput_
 >
-class LinearCombinationWithElementwise {
+class LinearCombination {
 public:
 
-  using ElementOutput = ElementSource_;
-  using ElementCompute = ElementCompute_;
+  using ElementOutput = ElementOutput_;
   using ElementAccumulator = ElementAccumulator_;
-  using ElementSource = ElementSource_;
-  using ElementTensor = ElementTensor_;
-
-  static bool const kIsHeavy = true;
+  using ElementCompute = ElementCompute_;
+  using ElementC = ElementSource_;
+  using ElementD = ElementOutput_;
 
   static int const kCount = Count;
-
-  using FragmentCompute = Array<ElementCompute, kCount>;
+  static const ScaleType::Kind kScale = Scale;
+  using FragmentOutput = Array<ElementOutput, kCount>;
   using FragmentAccumulator = Array<ElementAccumulator, kCount>;
-  using FragmentSource = Array<ElementSource, kCount>;
-  using FragmentTensor = Array<ElementTensor, kCount>;
+  using ComputeFragment = Array<ElementCompute, kCount>;
 
+  using ParamsBase = LinearCombinationParams;
   static FloatRoundStyle const kRound = Round;
 
   /// Host-constructable parameters structure
-  struct Params {
-
+  struct Params : ParamsBase{
     ElementCompute alpha;                  ///< scales accumulators
     ElementCompute beta;                   ///< scales source tensor
-    ElementCompute threshold;              ///< minimum value that is output 
     ElementCompute const *alpha_ptr;       ///< pointer to accumulator scalar - if not null, loads it from memory
     ElementCompute const *beta_ptr;        ///< pointer to source scalar - if not null, loads it from memory
-    //
-    // Methods
-    //
 
     CUTLASS_HOST_DEVICE
-    Params(): 
-      alpha(ElementCompute(1)), 
+    Params():
+      ParamsBase(
+        ElementCompute(1),
+        ElementCompute(0)
+      ),
+      alpha(ElementCompute(1)),
       beta(ElementCompute(0)),
-      threshold(ElementCompute(0)), 
-      alpha_ptr(nullptr), 
+      alpha_ptr(nullptr),
       beta_ptr(nullptr) { }
 
     CUTLASS_HOST_DEVICE
     Params(
       ElementCompute alpha,
-      ElementCompute beta,
-      ElementCompute threshold = ElementCompute(0)
-    ): alpha(alpha), beta(beta), threshold(threshold), alpha_ptr(nullptr), beta_ptr(nullptr) {
+      ElementCompute beta
+    ):
+      ParamsBase(alpha, beta),
+      alpha(alpha), beta(beta), alpha_ptr(nullptr), beta_ptr(nullptr) { }
 
-    }
+    CUTLASS_HOST_DEVICE
+    Params(
+      ElementCompute alpha
+    ):
+      ParamsBase(alpha, ElementCompute(0)),
+      alpha(alpha), beta(0), alpha_ptr(nullptr), beta_ptr(nullptr) { }
 
     CUTLASS_HOST_DEVICE
     Params(
       ElementCompute const *alpha_ptr,
-      ElementCompute const *beta_ptr,
-      ElementCompute threshold = ElementCompute(0)
-    ): alpha(0), beta(0), threshold(threshold), alpha_ptr(alpha_ptr), beta_ptr(beta_ptr) {
+      ElementCompute const *beta_ptr
+    ):
+      ParamsBase(*alpha_ptr, *beta_ptr),
+      alpha(0), beta(0), alpha_ptr(alpha_ptr), beta_ptr(beta_ptr) { }
+
+    CUTLASS_HOST_DEVICE
+    Params(
+      ElementCompute const *alpha_ptr
+    ):
+      ParamsBase(*alpha_ptr, ElementCompute(0)),
+      alpha(0), beta(0), alpha_ptr(alpha_ptr), beta_ptr(nullptr) { }
 
+    CUTLASS_HOST_DEVICE
+    Params(
+      ParamsBase const& base
+    ): ParamsBase(base), alpha_ptr(nullptr), beta_ptr(nullptr) {
+      #if defined(__CUDA_ARCH__)
+      alpha = reinterpret_cast<ElementCompute const&>(base.alpha_data);
+      beta = reinterpret_cast<ElementCompute const&>(base.beta_data);
+      #else
+      memcpy( alpha, base.alpha_data, sizeof(ElementCompute) );
+      memcpy( beta, base.alpha_data, sizeof(ElementCompute) );
+      #endif
     }
   };
 
 private:
 
   //
   // Data members
   //
 
   ElementCompute alpha_;
   ElementCompute beta_;
-  ElementCompute threshold_;
-  bool participates_in_reduction_;
 
 public:
 
   /// Constructs the function object, possibly loading from pointers in host memory
   CUTLASS_HOST_DEVICE
-  LinearCombinationWithElementwise(Params const &params) {
-
+  LinearCombination(Params const &params) {
     alpha_ = (params.alpha_ptr ? *params.alpha_ptr : params.alpha);
     beta_ = (params.beta_ptr ? *params.beta_ptr : params.beta);
-    threshold_ = params.threshold;
-    participates_in_reduction_ = true;
   }
 
   /// Returns true if source is needed
   CUTLASS_HOST_DEVICE
   bool is_source_needed() const {
-    return beta_ != ElementCompute(0);
-  }
+    if (Scale == ScaleType::NoBetaScaling) return true;
 
-  /// Returns true if the threadblock computes the reduction
-  CUTLASS_HOST_DEVICE
-  bool participates_in_reduction() const {
-    return participates_in_reduction_;
+    if (Scale == ScaleType::OnlyAlphaScaling) return false;
+
+    if (Scale == ScaleType::Nothing) return false;
+
+    return beta_ != ElementCompute(0);
   }
 
   /// Functionally required for serial reduction in the epilogue
   CUTLASS_HOST_DEVICE
   void set_k_partition(int k_partition, int k_partition_count) {
     if (k_partition) {
       beta_ = ElementCompute(1);
     }
-
-    if (k_partition != k_partition_count - 1) {
-      // set to NaN to make ReLU no-op for all except last k partitions
-      int64_t allones = -1;
-      threshold_ = reinterpret_cast<ElementCompute const &>(allones);
-      // Avoid computing the reduction if this isn't the final Split-K slice
-      participates_in_reduction_ = false;
-    }
   }
-  
+
   /// Computes linear scaling: D = alpha * accumulator + beta * source
   CUTLASS_HOST_DEVICE
-  FragmentCompute operator()(
-    FragmentAccumulator const &accumulator, 
-    FragmentSource const &source,
-    FragmentTensor const &tensor) const {
+  FragmentOutput operator()(
+    FragmentAccumulator const &accumulator,
+    FragmentOutput const &source) const {
 
     // Convert source to interal compute numeric type
-    NumericArrayConverter<ElementCompute, ElementSource, kCount, Round> source_converter;
+    NumericArrayConverter<ElementCompute, ElementOutput, kCount, Round> source_converter;
     NumericArrayConverter<ElementCompute, ElementAccumulator, kCount, Round> accumulator_converter;
 
-    FragmentCompute converted_source = source_converter(source);
-    FragmentCompute converted_accumulator = accumulator_converter(accumulator);
+    // Convert to destination numeric type
+    NumericArrayConverter<ElementOutput, ElementCompute, kCount, Round> destination_converter;
+
+    ComputeFragment converted_source = source_converter(source);
+    ComputeFragment converted_accumulator = accumulator_converter(accumulator);
+
+    if (Scale == ScaleType::Nothing)
+      return destination_converter(converted_accumulator);
 
     // Perform binary operations
-    FragmentCompute intermediate;
+    ComputeFragment intermediate;
+
+    multiplies<ComputeFragment> mul_add_source;
+    multiply_add<ComputeFragment> mul_add_accumulator;
 
-    multiplies<FragmentCompute> mul_add_source;
-    multiply_add<FragmentCompute> mul_add_accumulator;
+    if (Scale == ScaleType::NoBetaScaling)
+      intermediate = converted_source;
+    else
+      intermediate = mul_add_source(beta_, converted_source);                             // X =  beta * C + uniform
 
-    intermediate = mul_add_source(beta_, converted_source);                             // X =  beta * C + uniform
     intermediate = mul_add_accumulator(alpha_, converted_accumulator, intermediate);    // D = alpha * Accum + X
 
-    return intermediate;
+    return destination_converter(intermediate);
   }
 
   /// Computes linear scaling: D = alpha * accumulator
   CUTLASS_HOST_DEVICE
-  FragmentCompute operator()(
-    FragmentAccumulator const &accumulator,
-    FragmentTensor const &tensor) const {
+  FragmentOutput operator()(
+    FragmentAccumulator const &accumulator) const {
 
     // Convert source to interal compute numeric type
     NumericArrayConverter<ElementCompute, ElementAccumulator, kCount, Round> accumulator_converter;
 
-    FragmentCompute converted_accumulator = accumulator_converter(accumulator);
+    // Convert to destination numeric type
+    NumericArrayConverter<ElementOutput, ElementCompute, kCount, Round> destination_converter;
 
-    // Perform binary operations
-    FragmentCompute intermediate;
+    ComputeFragment converted_accumulator = accumulator_converter(accumulator);
 
-    multiplies<FragmentCompute> mul_accumulator;
+    if (Scale == ScaleType::Nothing)
+      return destination_converter(converted_accumulator);
+
+    // Perform binary operations
+    ComputeFragment intermediate;
+    multiplies<ComputeFragment> mul_accumulator;
 
     intermediate = mul_accumulator(alpha_, converted_accumulator);    // D = alpha * Accum
 
-    return intermediate;
+    return destination_converter(intermediate);
+  }
+
+  //
+  // Specializations for scalar (for use with cute::collective::DefaultEpilogue)
+  //
+  CUTLASS_HOST_DEVICE
+  ElementD operator()(ElementAccumulator const accumulator, ElementC const source) const {
+    // Convert everything to Compute type, do compute, and then store to output type
+    NumericConverter<ElementCompute, ElementAccumulator, Round> accumulator_converter;
+    [[maybe_unused]] NumericConverter<ElementCompute, ElementC, Round> source_converter;
+    NumericConverter<ElementD, ElementCompute, Round> destination_converter;
+
+    // Convert to destination numeric type
+
+    ElementCompute converted_accumulator = accumulator_converter(accumulator);
+    if constexpr (Scale == ScaleType::Nothing) {
+      return destination_converter(converted_accumulator);
+    }
+
+    // Perform binary operations
+    ElementCompute intermediate;
+    multiplies<ElementCompute> multiply;
+    multiply_add<ElementCompute> madd;
+
+    if constexpr (Scale == ScaleType::NoBetaScaling) {
+      intermediate = source_converter(source);
+    }
+    else {
+      intermediate = multiply(beta_, source);                            // X =  beta * C + uniform
+    }
+
+    intermediate = madd(alpha_, converted_accumulator, intermediate);    // D = alpha * Accum + X
+    return destination_converter(intermediate);
+  }
+
+  CUTLASS_HOST_DEVICE
+  ElementD operator()(ElementAccumulator const accumulator) const {
+    // Convert everything to Compute type, do compute, and then store to output type
+    NumericConverter<ElementCompute, ElementAccumulator, Round> accumulator_converter;
+    NumericConverter<ElementD, ElementCompute, Round> destination_converter;
+    ElementCompute converted_accumulator = accumulator_converter(accumulator);
+
+    // Convert to destination numeric type
+    if constexpr (Scale == ScaleType::Nothing) {
+      return destination_converter(converted_accumulator);
+    }
+
+    // Perform binary operations
+    ElementCompute intermediate;
+    multiplies<ElementCompute> multiply;
+
+    intermediate = multiply(alpha_, accumulator);    // D = alpha * Accum
+    return destination_converter(intermediate);
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace thread
 } // namespace epilogue
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/reduction_op.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/reduction_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/scale_type.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/thread/scale_type.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op_blas3.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op_blas3.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_direct_store.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_direct_store.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_planar_complex.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_planar_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_simt.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_simt.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op.h`

 * *Files 3% similar despite different names*

```diff
@@ -132,22 +132,23 @@
     ThreadMap,
     float
   >;
 
   static int const kFragmentsPerIteration = 2;
 };
 
-/// Partial specialization for int32_t <= int32_t x 4
+/// Partial specialization for int32_t <= int32_t
 template <
+  int ElementsPerAccess,
   typename ThreadblockShape,
   typename WarpShape,
   typename InstructionShape,
   typename ThreadMap
 >
-struct DefaultIteratorsTensorOp<int32_t, int32_t, 4, ThreadblockShape, WarpShape, InstructionShape, ThreadMap> {
+struct DefaultIteratorsTensorOp<int32_t, int32_t, ElementsPerAccess, ThreadblockShape, WarpShape, InstructionShape, ThreadMap> {
   
   using WarpTileIterator = cutlass::epilogue::warp::TileIteratorTensorOp<
     WarpShape,
     InstructionShape,
     int32_t,
     layout::RowMajor
   >;
@@ -156,22 +157,23 @@
     ThreadMap,
     int32_t
   >;
 
   static int const kFragmentsPerIteration = 1;
 };
 
-/// Partial specialization for float <= int32_t x 4
+/// Partial specialization for float <= int32_t
 template <
+  int ElementsPerAccess,
   typename ThreadblockShape,
   typename WarpShape,
   typename InstructionShape,
   typename ThreadMap
 >
-struct DefaultIteratorsTensorOp<float, int32_t, 4, ThreadblockShape, WarpShape, InstructionShape, ThreadMap> {
+struct DefaultIteratorsTensorOp<float, int32_t, ElementsPerAccess, ThreadblockShape, WarpShape, InstructionShape, ThreadMap> {
 
   using WarpTileIterator = cutlass::epilogue::warp::TileIteratorTensorOp<
     WarpShape,
     InstructionShape,
     int32_t,
     layout::RowMajor
   >;
@@ -218,14 +220,52 @@
     8,
     8
   >;
 
   static int const kFragmentsPerIteration = 2;
 };
 
+/// Partial specialization for half <= int32_t x 8 epilogues avoids shared memory bank conflicts.
+template <
+  typename ThreadblockShape,
+  typename WarpShape,
+  typename InstructionShape,
+  typename ThreadMap
+>
+struct DefaultIteratorsTensorOp<
+  half_t, 
+  int32_t, 
+  8, 
+  ThreadblockShape, 
+  WarpShape, 
+  InstructionShape, 
+  ThreadMap> {
+  
+  using WarpTileIterator = cutlass::epilogue::warp::TileIteratorTensorOpMixed<
+    WarpShape,
+    InstructionShape,
+    int32_t,
+    32,
+    16,
+    8,
+    8
+  >;
+
+  using SharedLoadIterator = cutlass::epilogue::threadblock::SharedLoadIteratorMixed<
+    ThreadMap,
+    int32_t,
+    32,
+    16,
+    8,
+    8
+  >;
+
+  static int const kFragmentsPerIteration = 2;
+};
+
 /// Partial specialization for int8/int4b_t <= int32 x 16/8 epilogues avoids shared memory bank conflicts.
 /// Threadblock::kN = 256 still has bank conflicts.
 template <
   typename ElementOutput,
   int ElementsPerAccess,
   typename ThreadblockShape,
   typename WarpShape,
@@ -264,15 +304,15 @@
     WarpShape,
     InstructionShape,
     int32_t,
     layout::RowMajor
   >;
 
   using WarpTileIterator = typename platform::conditional<
-                             (ThreadblockShape::kN == 256),
+                             (ThreadblockShape::kN == 256) || (ThreadblockShape::kN == 128 && ElementsPerAccess == 8),
                              WarpTileIteratorNotMixed,
                              WarpTileIteratorMixed>::type;
 
   using SharedLoadIteratorMixed = cutlass::epilogue::threadblock::SharedLoadIteratorMixed<
     ThreadMap,
     int32_t,
     32,
@@ -283,15 +323,15 @@
 
   using SharedLoadIteratorNotMixed = cutlass::epilogue::threadblock::SharedLoadIterator<
     ThreadMap,
     int32_t
   >;
 
   using SharedLoadIterator = typename platform::conditional<
-                             (ThreadblockShape::kN == 256),
+                             (ThreadblockShape::kN == 256) || (ThreadblockShape::kN == 128 && ElementsPerAccess == 8),
                              SharedLoadIteratorNotMixed,
                              SharedLoadIteratorMixed>::type;
 
   static int const kFragmentsPerIteration = 1;
 };
 
 /// Partial specialization for float_e4m3_t <= float x 16/8 epilogues avoids shared memory bank conflicts.
@@ -331,15 +371,15 @@
     WarpShape,
     InstructionShape,
     float,
     layout::RowMajor
   >;
 
   using WarpTileIterator = typename platform::conditional<
-                             (ThreadblockShape::kN == 256),
+                             (ThreadblockShape::kN == 256) || (ThreadblockShape::kN == 128 && ElementsPerAccess == 8),
                              WarpTileIteratorNotMixed,
                              WarpTileIteratorMixed>::type;
 
   using SharedLoadIteratorMixed = cutlass::epilogue::threadblock::SharedLoadIteratorMixed<
     ThreadMap,
     float,
     32,
@@ -350,15 +390,15 @@
 
   using SharedLoadIteratorNotMixed = cutlass::epilogue::threadblock::SharedLoadIterator<
     ThreadMap,
     float
   >;
 
   using SharedLoadIterator = typename platform::conditional<
-                             (ThreadblockShape::kN == 256),
+                             (ThreadblockShape::kN == 256) || (ThreadblockShape::kN == 128 && ElementsPerAccess == 8),
                              SharedLoadIteratorNotMixed,
                              SharedLoadIteratorMixed>::type;
 
   static int const kFragmentsPerIteration = 1;
 };
 
 /// Partial specialization for float_e5m2_t <= float x 16/8 epilogues avoids shared memory bank conflicts.
@@ -398,15 +438,15 @@
     WarpShape,
     InstructionShape,
     float,
     layout::RowMajor
   >;
 
   using WarpTileIterator = typename platform::conditional<
-                             (ThreadblockShape::kN == 256),
+                             (ThreadblockShape::kN == 256) || (ThreadblockShape::kN == 128 && ElementsPerAccess == 8),
                              WarpTileIteratorNotMixed,
                              WarpTileIteratorMixed>::type;
 
   using SharedLoadIteratorMixed = cutlass::epilogue::threadblock::SharedLoadIteratorMixed<
     ThreadMap,
     float,
     32,
@@ -417,15 +457,15 @@
 
   using SharedLoadIteratorNotMixed = cutlass::epilogue::threadblock::SharedLoadIterator<
     ThreadMap,
     float
   >;
 
   using SharedLoadIterator = typename platform::conditional<
-                             (ThreadblockShape::kN == 256),
+                             (ThreadblockShape::kN == 256) || (ThreadblockShape::kN == 128 && ElementsPerAccess == 8),
                              SharedLoadIteratorNotMixed,
                              SharedLoadIteratorMixed>::type;
 
   static int const kFragmentsPerIteration = 1;
 };
 
 } // namespace detail
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op_blas3.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op_blas3.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_broadcast.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_broadcast.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_reduction.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_reduction.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_simt.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_simt.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_tensor_op.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_volta_tensor_op.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_volta_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_wmma_tensor_op.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_wmma_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/direct_store_epilogue_iterator.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/direct_store_epilogue_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/epilogue.h`

 * *Files 3% similar despite different names*

```diff
@@ -421,14 +421,55 @@
       source_iterator.clear_mask();
       __syncthreads();  // Dummy (CUDA 11.0)
     }
 
     operator()(output_op, destination_iterator, accumulators, SourceAspectNeeded(source_iterator));
   }
 
+  template<class Seq>
+  struct acc2smem;
+
+  template <size_t... Seq>
+  struct acc2smem<cutlass::index_sequence<Seq...>> {
+    template<int Advance>
+    CUTLASS_DEVICE
+    static void helper(AccumulatorFragmentIterator accum_fragment_iterator,
+                      WarpTileIterator &warp_tile_iterator) {
+      CUTLASS_PRAGMA_UNROLL
+      for (int i = 0; i < Advance; i++) {
+        ++accum_fragment_iterator;
+      }
+
+      CUTLASS_PRAGMA_UNROLL
+      for (int p = 0; p < Base::kFragmentsPerIteration; ++p) {
+        typename AccumulatorFragmentIterator::Fragment accum_fragment;
+
+        accum_fragment_iterator.load(accum_fragment);
+        ++accum_fragment_iterator;
+
+        warp_tile_iterator.store(accum_fragment);
+        if (p < Base::kFragmentsPerIteration - 1) {
+          warp_tile_iterator.add_pointer_offset(kSmemPointerOffset);
+        }
+      }
+
+      if (Base::kFragmentsPerIteration > 1) {
+        warp_tile_iterator.add_pointer_offset(kSmemPointerOffset *
+                                              (1 - Base::kFragmentsPerIteration));
+      }
+    }
+
+    CUTLASS_DEVICE
+    static void push(size_t pos,
+                    AccumulatorFragmentIterator const &iterator_begin,
+                    WarpTileIterator &warp_tile_iterator) {
+      int dummy[] = {(pos == Seq) && (helper<Seq>(iterator_begin, warp_tile_iterator), 0)...};
+    }
+  };
+
 
   /// Streams the result to global memory
   template <typename SourceAspect>
   CUTLASS_DEVICE
   void operator()(
     OutputOp const &output_op,                      ///< Output operator
     OutputTileIterator destination_iterator,        ///< Tile iterator for destination
@@ -448,33 +489,16 @@
 
       //
       // Convert and store fragment
       //
 
       __syncthreads();
 
-      CUTLASS_PRAGMA_UNROLL
-      for (int p = 0; p < Base::kFragmentsPerIteration; ++p)
-      {
-        typename AccumulatorFragmentIterator::Fragment accum_fragment;
-
-        accum_fragment_iterator.load(accum_fragment);
-        ++accum_fragment_iterator;
-
-        this->warp_tile_iterator_.store(accum_fragment);
-
-        if (p < Base::kFragmentsPerIteration - 1) {
-          this->warp_tile_iterator_.add_pointer_offset(kSmemPointerOffset);
-        }
-      }
-
-      if (Base::kFragmentsPerIteration > 1) {
-        this->warp_tile_iterator_.add_pointer_offset(kSmemPointerOffset * (1 - Base::kFragmentsPerIteration));
-      }
-
+      acc2smem<cutlass::make_index_sequence<OutputTileIterator::kIterations>>::push(
+        iter, accum_fragment_iterator, this->warp_tile_iterator_);
 
       //
       // Load fragments from shared memory
       //
 
       __syncthreads();
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_base.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/epilogue_base.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_base_streamk.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/epilogue_base_streamk.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_depthwise.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/epilogue_depthwise.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_direct_store.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/epilogue_direct_store.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_gemm_k_reduction.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/epilogue_gemm_k_reduction.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_planar_complex.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/epilogue_planar_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_smem_accumulator.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/epilogue_smem_accumulator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_visitor_with_softmax.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/epilogue_visitor_with_softmax.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_broadcast.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_broadcast.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_reduction.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_reduction.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_workspace.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/epilogue_workspace.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/interleaved_epilogue.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/interleaved_epilogue.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/output_iterator_parameter.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/output_iterator_parameter.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/output_tile_thread_map.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/output_tile_thread_map.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine_layout_params.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine_layout_params.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_blas3.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_blas3.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_direct_conv.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_direct_conv.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_params.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_params.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_predicates.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_predicates.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_strided_dgrad.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_strided_dgrad.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_mixed.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_mixed.h`

 * *Files 1% similar despite different names*

```diff
@@ -66,30 +66,31 @@
 ///
 template <
   typename ThreadMap_,       ///< Thread map (conept: OutputTileThreadMap)
   typename Element_,         ///< Accumulator data type
   int ElementSizeBits_,      ///< Size of accumulator in bits
   int OutputSizeBits_,       ///< Size of output element in bits
   int ElementsPerAccess,     ///< Vector length of output vector
-  int ContiguousLanes        ///< Number of lanes in the warp writing to contiguous elements
+  int ContiguousLanes,       ///< Number of lanes in the warp writing to contiguous elements
                              ///  in the global memory tensor
+  bool EightBitsOutputOrLess = (OutputSizeBits_ <= 8)
 >
 class SharedLoadIteratorMixed;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Tile iterator used to load output tile from shared memory in epilogue.
 ///
 /// Satisfies: ReadableTileIterator
 ///
 template <
   typename ThreadMap_,       ///< Thread map (conept: OutputTileThreadMap)
   typename Element_          ///< Accumulator data type
 >
-class SharedLoadIteratorMixed<ThreadMap_, Element_, 32, 16, 8, 8> {
+class SharedLoadIteratorMixed<ThreadMap_, Element_, 32, 16, 8, 8, false> {
 public:
   using ThreadMap = ThreadMap_;
   using Shape = typename ThreadMap::Shape;
 
   using Element = Element_;
 
   using Layout = layout::RowMajor;
@@ -249,15 +250,15 @@
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for int32_t x 16 => int8_t/int4b_t x 16 
 template <
   typename ThreadMap_,      ///< Thread map (conept: OutputTileThreadMap)
   int OutputSizeBits_       ///< Size of output element in bits
 >
-class SharedLoadIteratorMixed<ThreadMap_, int32_t, 32, OutputSizeBits_, 16, 8> {
+class SharedLoadIteratorMixed<ThreadMap_, int32_t, 32, OutputSizeBits_, 16, 8, true> {
 public:
   using ThreadMap = ThreadMap_;
   using Shape = typename ThreadMap::Shape;
 
   using Element = int32_t;
 
   using Layout = layout::RowMajor;
@@ -414,15 +415,15 @@
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for int32_t x 8 => int8_t/int4b_t x 8
 template <
   typename ThreadMap_,      ///< Thread map (conept: OutputTileThreadMap)
   int OutputSizeBits_
 >
-class SharedLoadIteratorMixed<ThreadMap_, int32_t, 32, OutputSizeBits_, 8, 8> {
+class SharedLoadIteratorMixed<ThreadMap_, int32_t, 32, OutputSizeBits_, 8, 8, true> {
 public:
   using ThreadMap = ThreadMap_;
   using Shape = typename ThreadMap::Shape;
 
   using Element = int32_t;
 
   using Layout = layout::RowMajor;
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_pitch_liner.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_pitch_liner.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_complex_tensor_op.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/fragment_iterator_complex_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_gaussian_complex_tensor_op.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/fragment_iterator_gaussian_complex_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_simt.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/fragment_iterator_simt.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_tensor_op.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/fragment_iterator_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_volta_tensor_op.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/fragment_iterator_volta_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_wmma_tensor_op.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/fragment_iterator_wmma_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/simt_policy.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/simt_policy.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tensor_op_policy.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/tensor_op_policy.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_simt.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/tile_iterator_simt.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op_mixed.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op_mixed.h`

 * *Files 1% similar despite different names*

```diff
@@ -60,15 +60,16 @@
 template <
   typename WarpShape_,            ///< shape of warp-level GEMM (concept: GemmShape)
   typename OperatorShape_,        ///< matrix multiply operation shape (concept: gemm::GemmShape)
   typename Element_,              ///< data type of accumulator element
   int ElementSizeBits,            ///< Size of accumulator element in bits
   int OutputSizeBits,             ///< Size of output element in bits
   int OutputElementCount,         ///< number of elements in output vector
-  int ContiguousLanes             ///< Number of consecutive lanes writing to contiguous memory
+  int ContiguousLanes,            ///< Number of consecutive lanes writing to contiguous memory
+  bool EightBitsOutputOrLess = (OutputSizeBits <= 8)
 >
 class TileIteratorTensorOpMixed {
 public:
 
   using WarpShape = WarpShape_;
   using OperatorShape = OperatorShape_;
   using Element = Element_;
@@ -315,15 +316,15 @@
 
 /// Partial specialization for int32_t x 16 => int8_t/int4b_t x 16
 template <
   typename WarpShape_,            ///< shape of warp-level GEMM (concept: GemmShape)
   typename OperatorShape_,        ///< matrix multiply operation shape (concept: gemm::GemmShape),
   int OutputSizeBits              ///< Size of output element in bits
 >
-class TileIteratorTensorOpMixed<WarpShape_, OperatorShape_, int32_t, 32, OutputSizeBits, 16, 8> {
+class TileIteratorTensorOpMixed<WarpShape_, OperatorShape_, int32_t, 32, OutputSizeBits, 16, 8, true> {
 public:
 
   using WarpShape = WarpShape_;
   using OperatorShape = OperatorShape_;
   using Element = int32_t;
   using Layout = layout::RowMajor;
   static int const kOutputElementCount = 16;
@@ -522,15 +523,15 @@
 
 /// Partial specialization for int32_t x 8 => int8_t/int4b_t x 8
 template <
   typename WarpShape_,            ///< shape of warp-level GEMM (concept: GemmShape)
   typename OperatorShape_,        ///< matrix multiply operation shape (concept: gemm::GemmShape)
   int OutputSizeBits              ///< Size of output element in bits
 >
-class TileIteratorTensorOpMixed<WarpShape_, OperatorShape_, int32_t, 32, OutputSizeBits, 8, 8> {
+class TileIteratorTensorOpMixed<WarpShape_, OperatorShape_, int32_t, 32, OutputSizeBits, 8, 8, true> {
 public:
 
   using WarpShape = WarpShape_;
   using OperatorShape = OperatorShape_;
   using Element = int32_t;
   using Layout = layout::RowMajor;
   static int const kOutputElementCount = 8;
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_volta_tensor_op.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/tile_iterator_volta_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_wmma_tensor_op.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/tile_iterator_wmma_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/volta_tensor_op_policy.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/volta_tensor_op_policy.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/wmma_tensor_op_policy.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/epilogue/warp/wmma_tensor_op_policy.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/fast_math.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/fast_math.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/float8.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/float8.h`

 * *Files 0% similar despite different names*

```diff
@@ -395,41 +395,41 @@
     #if defined(CUDA_PTX_FP8_CVT_ENABLED)
         uint16_t tmp = 0;
         uint32_t bits = reinterpret_cast<uint16_t const &>(flt);
         asm volatile("cvt.rn.satfinite.e4m3x2.f16x2 %0, %1;" : "=h"(tmp) : "r"(bits));
 
         return *reinterpret_cast<float_e4m3_t *>(&tmp);
     #else
-        return bitcast(Base::convert_float_to_fp8(float(flt)));
+        return bitcast(Base::convert_float_to_fp8(__half2float(flt)));
     #endif
     }
 
     // E4M3 -> half
     CUTLASS_HOST_DEVICE
     static half to_half(float_e4m3_t const& x) {
     #if defined(CUDA_PTX_FP8_CVT_ENABLED)
         uint16_t bits = x.storage;
         uint32_t packed;
         asm volatile("cvt.rn.f16x2.e4m3x2 %0, %1;\n" : "=r"(packed) : "h"(bits));
 
         return reinterpret_cast<half2 const &>(packed).x;
     #else
-        return half(Base::convert_fp8_to_float(x.storage));
+        return __float2half(Base::convert_fp8_to_float(x.storage));
     #endif
     }
 
     // E4M3 -> Float
     CUTLASS_HOST_DEVICE
     static float to_float(float_e4m3_t const& x) {
     #if defined(CUDA_PTX_FP8_CVT_ENABLED)
         uint16_t bits = x.storage;
         uint32_t packed;
         asm volatile("cvt.rn.f16x2.e4m3x2 %0, %1;\n" : "=r"(packed) : "h"(bits));
 
-        return float(reinterpret_cast<half2 const &>(packed).x);
+        return __half2float(reinterpret_cast<half2 const &>(packed).x);
     #else
         return Base::convert_fp8_to_float(x.storage);
     #endif
     }
 
     //
     // Methods
@@ -605,41 +605,41 @@
     #if defined(CUDA_PTX_FP8_CVT_ENABLED)
         uint16_t tmp = 0;
         uint32_t bits = reinterpret_cast<uint16_t const &>(flt);
         asm volatile("cvt.rn.satfinite.e5m2x2.f16x2 %0, %1;" : "=h"(tmp) : "r"(bits));
 
         return *reinterpret_cast<float_e5m2_t *>(&tmp);
     #else
-        return bitcast(Base::convert_float_to_fp8(float(flt)));
+        return bitcast(Base::convert_float_to_fp8(__half2float(flt)));
     #endif
     }
 
     // E5M2 -> half
     CUTLASS_HOST_DEVICE
     static half to_half(float_e5m2_t const& x) {
     #if defined(CUDA_PTX_FP8_CVT_ENABLED)
         uint16_t bits = x.storage;
         uint32_t packed;
         asm volatile("cvt.rn.f16x2.e5m2x2 %0, %1;\n" : "=r"(packed) : "h"(bits));
 
         return reinterpret_cast<half2 const &>(packed).x;
     #else
-        return half(Base::convert_fp8_to_float(x.storage));
+        return __float2half(Base::convert_fp8_to_float(x.storage));
     #endif
     }
 
     // E5M2 -> Float
     CUTLASS_HOST_DEVICE
     static float to_float(float_e5m2_t const& x) {
     #if defined(CUDA_PTX_FP8_CVT_ENABLED)
         uint16_t bits = x.storage;
         uint32_t packed;
         asm volatile("cvt.rn.f16x2.e5m2x2 %0, %1;\n" : "=r"(packed) : "h"(bits));
 
-        return float(reinterpret_cast<half2 const &>(packed).x);
+        return __half2float(reinterpret_cast<half2 const &>(packed).x);
     #else
         return Base::convert_fp8_to_float(x.storage);
     #endif
     }
 
     //
     // Methods
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/floating_point_nvrtc.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/floating_point_nvrtc.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/functional.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/functional.h`

 * *Files 14% similar despite different names*

```diff
@@ -52,14 +52,20 @@
 struct absolute_value_op {
   CUTLASS_HOST_DEVICE
   T operator()(T lhs) const {
     return abs(lhs);
   }
 };
 
+template <>
+struct absolute_value_op<float> {
+  CUTLASS_HOST_DEVICE
+  float operator()(float lhs) const { return fabs(lhs); }
+};
+
 template <typename T>
 struct plus {
   CUTLASS_HOST_DEVICE
   T operator()(T lhs, T const &rhs) const {
     lhs += rhs;
     return lhs;
   }
@@ -79,14 +85,91 @@
   CUTLASS_HOST_DEVICE
   T operator()(T lhs, T const &rhs) const {
     lhs *= rhs;
     return lhs;
   }
 };
 
+#if defined(__CUDA_ARCH__)
+/// Partial specializations needed when __CUDA_NO_HALF2_OPERATORS__ is set
+template<>
+struct plus<__half2> {
+  CUTLASS_HOST_DEVICE
+  __half2 operator()(__half2 lhs, __half2 const &rhs) const {
+    return __hadd2(lhs, rhs);
+  }
+};
+
+template<>
+struct minus<__half2> {
+  CUTLASS_HOST_DEVICE
+  __half2 operator()(__half2 lhs, __half2 const &rhs) const {
+    return __hsub2(lhs, rhs);
+  }
+};
+
+template<>
+struct multiplies<__half2> {
+  CUTLASS_HOST_DEVICE
+  __half2 operator()(__half2 lhs, __half2 const &rhs) const {
+    return __hmul2(lhs, rhs);
+  }
+};
+
+/// Partial specializations needed when __CUDA_NO_HALF_OPERATORS__ is set
+template<>
+struct plus<__half> {
+  CUTLASS_HOST_DEVICE
+  __half operator()(__half lhs, __half const &rhs) const {
+    return __hadd(lhs, rhs);
+  }
+};
+
+template<>
+struct minus<__half> {
+  CUTLASS_HOST_DEVICE
+  __half operator()(__half lhs, __half const &rhs) const {
+    return __hsub(lhs, rhs);
+  }
+};
+
+template<>
+struct multiplies<__half> {
+  CUTLASS_HOST_DEVICE
+  __half operator()(__half lhs, __half const &rhs) const {
+    return __hmul(lhs, rhs);
+  }
+};
+#endif // defined(__CUDA_ARCH__)
+
+
+// Maximum with nan propogation
+// To propgate the NANs, the "max" of a two element that contains NaNs should also return a NaN 
+template <typename T>
+struct maximum_with_nan_propogation {
+  CUTLASS_HOST_DEVICE
+  T operator()(T const &lhs, T const &rhs) const {
+    return lhs > rhs or std::isnan(lhs) ? lhs : rhs;
+  }
+};
+
+template <>
+struct maximum_with_nan_propogation<float> {
+  CUTLASS_HOST_DEVICE
+  float operator()(float const lhs, float const rhs) const {
+    float res;
+#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
+    asm volatile("max.NaN.f32 %0, %1, %2;\n" : "=f"(res) : "f"(lhs), "f"(rhs));
+#else
+    res = lhs > rhs or std::isnan(lhs) ? lhs : rhs;
+#endif
+    return res;
+  }
+};
+
 /// Squares with optional conversion
 template <typename T, typename Output = T>
 struct square {
   CUTLASS_HOST_DEVICE
   Output operator()(T lhs) const {
     multiplies<Output> mul_op;
 
@@ -377,44 +460,23 @@
 /// Reduces value into the data pointed to by ptr (half2 specialization)
 template<>
 struct red<half2>
 {
   CUTLASS_DEVICE
   void operator()(half2 *ptr, const half2 &data)
   {
-#if !defined(__CUDA_ARCH__)
+#if !defined(__CUDA_ARCH__) || (defined(__CUDA_ARCH__)  && (__CUDA_ARCH__ < 600))
       CUTLASS_UNUSED(ptr);
       CUTLASS_UNUSED(data);
-#elif (__CUDA_ARCH__ >= 600)
+#else
 
     // Vector-2 atomic reduction requires .target sm_60 or higher
     uint32_t word = reinterpret_cast<const uint32_t&>(data);
     asm volatile ("red.gpu.global.add.noftz.f16x2 [%0], %1;\n" : : "l"(ptr), "r"(word));
 
-#else
-
-    // Use CAS loop
-    uint32_t *ptr_int = reinterpret_cast<uint32_t *>(ptr);
-    uint32_t old_int = *ptr_int;
-    uint32_t assumed_int;
-
-    do
-    {
-      half2 old = reinterpret_cast<half2&>(old_int);
-
-      half hi = __hadd(__high2half(old), __high2half(data));
-      half lo = __hadd(__low2half(old), __low2half(data));
-      half2 update = __halves2half2(hi, lo);
-      uint32_t update_int = reinterpret_cast<const uint32_t&>(update);
-
-      assumed_int = old_int;
-      old_int = atomicCAS(ptr_int, assumed_int, update_int);
-
-    } while (assumed_int != old_int);
-
 #endif // (__CUDA_ARCH__ >= 600)
   }
 };
 
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 //
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/base_grouped.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/base_grouped.h`

 * *Files 0% similar despite different names*

```diff
@@ -338,15 +338,15 @@
 
     // Choose between the full wave of threadblocks and the tile count. If there
     // are fewer tiles in the group than threadblocks in the full wave, only
     // some threadblocks will be assigned tiles. Those threadblocks
     // which are not assigned tiles still need to perform the work of iterating through
     // problem sizes to determine that they have no work to do. This competes for cycles
     // with those threadblocks that are assigned tiles to compute.
-    return min(total_tiles, occupancy_based_block_count);
+    return std::min(total_tiles, occupancy_based_block_count);
   }
 
 
   /// Initializes GEMM state from arguments.
   Status initialize(Arguments const &args, void *workspace = nullptr, cudaStream_t stream = nullptr) {
 
     CUTLASS_TRACE_HOST("BaseGrouped::initialize() - workspace "
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/default_gemm_configuration.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/default_gemm_configuration.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/ell_gemm.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/ell_gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_array.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/gemm_array.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_batched.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/gemm_batched.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_complex.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/gemm_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_grouped.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/gemm_grouped.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_layernorm_mainloop_fusion.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/gemm_layernorm_mainloop_fusion.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_sparse.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/gemm_sparse.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_splitk_parallel.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/gemm_splitk_parallel.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/gemm_universal.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_adapter.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_mma_with_reduction.h`

 * *Files 26% similar despite different names*

```diff
@@ -24,179 +24,118 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/*! 
-  \file
-  \brief The universal GEMM accommodates serial reductions, parallel reductions, batched strided, and 
-    batched array variants.
+/*! \file
+    \brief Template for a pipelined GEMM kernel. Does not compute batching or support split-K.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
-#include "cutlass/gemm/device/gemm_universal_base.h"
-#include "cutlass/gemm/kernel/gemm_transpose_operands.h"
+#include "cutlass/numeric_types.h"
+#include "cutlass/arch/arch.h"
+
+#include "cutlass/layout/matrix.h"
+#include "cutlass/transform/threadblock/predicated_tile_iterator.h"
+#include "cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h"
+#include "cutlass/gemm/threadblock/default_mma_core_with_reduction.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
-namespace device {
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
+namespace threadblock {
 
-template <typename GemmKernel_>
-class GemmUniversalAdapter {
-public:
-
-  using GemmKernel = GemmKernel_;
-
-  static bool const kInternalTranspose = 
-    platform::is_same<typename GemmKernel::LayoutC, cutlass::layout::RowMajor>::value;
-
-  using ThreadblockShape = typename GemmKernel::Mma::Shape;
-  using WarpShape = typename GemmKernel::WarpShape;
-  using InstructionShape = typename GemmKernel::InstructionShape;
-
-  // warp-level, arch-level (instruction), math operator 
-  using WarpMmaOperator = typename GemmKernel::Mma::Policy::Operator;
-  using ArchMmaOperator = typename WarpMmaOperator::ArchMmaOperator;
-  using MathOperator = typename WarpMmaOperator::MathOperator;
-  
-  // Operator class and arch tag extract bottom-up 
-  // set it for top-level gemm device-level template
-  using OperatorClass = typename WarpMmaOperator::OperatorClass;
-  using ArchTag = typename WarpMmaOperator::ArchTag;
-
-  // Type, layout, and complex transform deliberately exchanged with B
-  using MapArguments = kernel::detail::MapArguments<
-    typename GemmKernel::ElementA,
-    typename GemmKernel::LayoutA,
-    GemmKernel::kTransformA,
-    GemmKernel::kAlignmentA,
-    typename GemmKernel::ElementB,
-    typename GemmKernel::LayoutB,
-    GemmKernel::kTransformB,
-    GemmKernel::kAlignmentB,
-    typename GemmKernel::LayoutC,
-    kInternalTranspose
-  >;
-
-  using ElementA = typename MapArguments::ElementA;
-  using LayoutA = typename MapArguments::LayoutA;
-  static ComplexTransform const kTransformA = MapArguments::kTransformA;
-  static int const kAlignmentA = MapArguments::kAlignmentA;
-
-  using ElementB = typename MapArguments::ElementB;
-  using LayoutB = typename MapArguments::LayoutB;
-  static ComplexTransform const kTransformB = MapArguments::kTransformB;
-  static int const kAlignmentB = MapArguments::kAlignmentB;
-  
-  using ElementC = typename GemmKernel::ElementC;
-  using LayoutC = typename MapArguments::LayoutC;
-  static int const kAlignmentC = GemmKernel::kAlignmentC;
- 
-  using TensorRefA = TensorRef<ElementA const, LayoutA>;
-  using TensorRefB = TensorRef<ElementB const, LayoutB>;
-  using TensorRefC = TensorRef<ElementC const, LayoutC>;
-  using TensorRefD = TensorRef<ElementC, LayoutC>;
-
-  static int const kStages = GemmKernel::Mma::kStages;
-
-  using EpilogueOutputOp = typename GemmKernel::EpilogueOutputOp;
-  using ElementAccumulator = typename EpilogueOutputOp::ElementAccumulator;
-  using ThreadblockSwizzle = typename GemmKernel::ThreadblockSwizzle;
-  using UnderlyingOperator = GemmUniversalBase<GemmKernel>;
-  using Arguments = typename UnderlyingOperator::Arguments;
-
-private:
-
-  UnderlyingOperator underlying_operator_;
-
-public:
-
-  /// Constructs the GEMM.
-  GemmUniversalAdapter() { }
-
-  /// Helper to construct a transposed equivalent for the underying GEMM operator
-  static Arguments to_underlying_arguments(Arguments const &args) {
-    if (kInternalTranspose) {
-      return args.transposed_problem();
-    }
-    else {
-      return args;
-    }
-  }
-
-  /// Determines whether the GEMM can execute the given problem.
-  static Status can_implement(Arguments const &args) {
-
-    return UnderlyingOperator::can_implement(to_underlying_arguments(args));
-  }
-
-  /// Gets the workspace size
-  static size_t get_workspace_size(Arguments const &args) {
-    
-    return UnderlyingOperator::get_workspace_size(to_underlying_arguments(args));
-  }
-
-  /// Computes the grid shape
-  static dim3 get_grid_shape(Arguments const &args) { 
-    return UnderlyingOperator::get_grid_shape(to_underlying_arguments(args));
-  }
-
-  /// Computes the maximum number of active blocks per multiprocessor
-  static int maximum_active_blocks(int smem_capacity = -1) {
-    return UnderlyingOperator::maximum_active_blocks(smem_capacity);
-  }
-
-  /// Initializes GEMM state from arguments.
-  Status initialize(Arguments const &args, void *workspace = nullptr, cudaStream_t stream = nullptr) {
-
-    return underlying_operator_.initialize(to_underlying_arguments(args), workspace, stream);
-  }
-
-  /// Lightweight update given a subset of arguments.  Problem geometry is assumed to
-  /// remain the same.
-  Status update(Arguments const &args) {
-
-    return underlying_operator_.update(to_underlying_arguments(args));
-  }
-
-  /// Runs the kernel using initialized state.
-  Status run(cudaStream_t stream = nullptr) {
-
-    return underlying_operator_.run(stream);
-  }
-
-  /// Runs the kernel using initialized state.
-  Status operator()(cudaStream_t stream = nullptr) {
-    return run(stream);
-  }
-
-  /// Runs the kernel using initialized state.
-  Status operator()(
-    Arguments const &args, 
-    void *workspace = nullptr, 
-    cudaStream_t stream = nullptr) {
-    
-    Status status = initialize(args, workspace, stream);
-    
-    if (status == Status::kSuccess) {
-      status = run(stream);
-    }
+////////////////////////////////////////////////////////////////////////////////
 
-    return status;
-  }
+template <
+    /// Element type for A matrix operand
+    typename ElementA,
+    /// Layout type for A matrix operand
+    typename LayoutA,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentA,
+    /// Element type for B matrix operand
+    typename ElementB,
+    /// Layout type for B matrix operand
+    typename LayoutB,
+    /// Access granularity of B matrix in units of elements
+    int kAlignmentB,
+    /// Element type for internal accumulation
+    typename ElementAccumulator,
+    /// Layout type for C and D matrix operands
+    typename LayoutC,
+    /// Operator class tag
+    typename OperatorClass,
+    ///                                                                                               
+    bool ReduceKForA_,
+    /// Tag indicating architecture to tune for
+    typename ArchTag,
+    /// Threadblock-level tile size (concept: GemmShape)
+    typename ThreadblockShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename WarpShape,
+    /// Instruction-level tile size (concept: GemmShape)
+    typename InstructionShape,
+    /// Number of stages used in the pipelined mainloop
+    int Stages,
+    /// Operation perfomed by GEMM
+    typename Operator,
+    /// Store the accumulators in row major or column major.  Row major is used
+    /// when output layout is interleaved.
+    bool AccumulatorsInRowMajor = false,
+    /// Use zfill or predicate for SM80 out-of-bound cp.async 
+    SharedMemoryClearOption SharedMemoryClear = SharedMemoryClearOption::kNone
+    >
+struct DefaultMmaWithReduction {
+
+  static cutlass::arch::CacheOperation::Kind const CacheOpA =
+      ((sizeof_bits<ElementA>::value * kAlignmentA) == 128)
+          ? cutlass::arch::CacheOperation::Global
+          : cutlass::arch::CacheOperation::Always;
+
+  static cutlass::arch::CacheOperation::Kind const CacheOpB =
+      ((sizeof_bits<ElementB>::value * kAlignmentB) == 128)
+          ? cutlass::arch::CacheOperation::Global
+          : cutlass::arch::CacheOperation::Always;
+
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaWithReductionCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
+      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
+      ReduceKForA_,  Stages, Operator, false, CacheOpA, CacheOpB>;
+
+  // Define iterators over tiles from the A operand
+  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
+  using AccessTypeA = cutlass::Array<ElementA, kAlignmentA>;
+  using IteratorA =
+      cutlass::transform::threadblock::PredicatedTileAccessIterator<
+          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
+          ElementA, LayoutA, 1, ThreadMapA, AccessTypeA>;
+
+  // Define iterators over tiles from the B operand
+  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
+  using AccessTypeB = cutlass::Array<ElementB, kAlignmentB>;
+  using IteratorB =
+      cutlass::transform::threadblock::PredicatedTileAccessIterator<
+          cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
+          ElementB, LayoutB, 0, ThreadMapB, AccessTypeB>;
+
+  // Define the threadblock-scoped multistage matrix multiply
+  using ThreadblockMma = cutlass::gemm::threadblock::MmaWithReductionMultistage<
+      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
+      MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
+      MmaCore::kCacheOpB, ElementAccumulator, layout::RowMajor,
+      typename MmaCore::MmaPolicy, Stages, SharedMemoryClear>;
 };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
-} // namespace device
+} // namespace threadblock
 } // namespace gemm
-} // namespace cutlass
+} // namespace cutlass 
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_base.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/gemm_universal_base.h`

 * *Files 0% similar despite different names*

```diff
@@ -302,15 +302,15 @@
   //---------------------------------------------------------------------------------------------
   // Stateful API
   //---------------------------------------------------------------------------------------------
 
   /// Initializes GEMM state from arguments and workspace memory
   Status initialize(
     Arguments const &args,
-    void *workspace,
+    void *workspace = nullptr,
     cudaStream_t stream = nullptr)
   {
     CUTLASS_TRACE_HOST("GemmUniversalBase::initialize() - workspace "
       << workspace << ", stream: " << (stream ? "non-null" : "null"));
 
     // Initialize parameters from args
     Status result = init_params(args);
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_with_broadcast.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/gemm_universal_with_broadcast.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_with_k_reduction.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/gemm_with_k_reduction.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemv.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/gemv.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_2k.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/rank_2k.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_2k_grouped.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/rank_2k_grouped.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_k.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/rank_k.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/symm.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/symm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/device/trmm.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/device/trmm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_ell_gemm.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_ell_gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_gemm.h`

 * *Files 1% similar despite different names*

```diff
@@ -258,16 +258,16 @@
 >
 struct DefaultGemm<ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB, ElementC,
                    LayoutC, ElementAccumulator, arch::OpClassTensorOp,
                    arch::Sm80, ThreadblockShape, WarpShape, InstructionShape,
                    EpilogueOutputOp, ThreadblockSwizzle, Stages, SplitKSerial,
                    Operator, SharedMemoryClear, GatherA, GatherB, ScatterD, PermuteDLayout> {
 
-  static_assert(platform::is_same<LayoutC, layout::RowMajor>::value
-             || platform::is_same<LayoutC, layout::AffineRankN<2>>::value,
+  static_assert((platform::is_same<LayoutC, layout::RowMajor>::value
+             || platform::is_same<LayoutC, layout::AffineRankN<2>>::value),
              "Epilogue in the kernel level must be row major");
 
   /// Define the threadblock-scoped matrix multiply-accumulate
   using Mma = typename cutlass::gemm::threadblock::DefaultMma<
       ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB,
       ElementAccumulator, LayoutC, arch::OpClassTensorOp, arch::Sm80,
       ThreadblockShape, WarpShape, InstructionShape, Stages,
@@ -710,16 +710,16 @@
     SharedMemoryClear,
     GatherA,
     GatherB,
     ScatterD,
     PermuteDLayout,
     typename platform::enable_if< ! platform::is_same<ArchTag, arch::Sm80>::value >::type > {
 
-  static_assert(platform::is_same<LayoutC, layout::RowMajor>::value
-             || platform::is_same<LayoutC, layout::AffineRankN<2>>::value,
+  static_assert((platform::is_same<LayoutC, layout::RowMajor>::value
+             || platform::is_same<LayoutC, layout::AffineRankN<2>>::value),
              "Epilogue in the kernel level must be row major");
 
   /// Define the threadblock-scoped matrix multiply-accumulate
   using Mma = typename cutlass::gemm::threadblock::DefaultMma<
       ElementA,
       LayoutA,
       kAlignmentA,
@@ -837,16 +837,16 @@
                    Operator,
                    SharedMemoryClear,
                    GatherA,
                    GatherB,
                    ScatterD,
                    PermuteDLayout> {
 
-  static_assert(platform::is_same<LayoutC, layout::RowMajor>::value
-             || platform::is_same<LayoutC, layout::AffineRankN<2>>::value,
+  static_assert((platform::is_same<LayoutC, layout::RowMajor>::value
+             || platform::is_same<LayoutC, layout::AffineRankN<2>>::value),
              "Epilogue in the kernel level must be row major");
 
   /// Define the threadblock-scoped matrix multiply-accumulate
   using Mma = typename cutlass::gemm::threadblock::DefaultMma<
       ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB,
       ElementAccumulator, LayoutC, arch::OpClassSimt, arch::Sm80,
       ThreadblockShape, WarpShape, GemmShape<1, 1, 1>, Stages,
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_complex.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_gemm_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped_softmax_mainloop_fusion.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped_softmax_mainloop_fusion.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_layernorm_mainloop_fusion.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_gemm_layernorm_mainloop_fusion.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_planar_complex_universal.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_gemm_planar_complex_universal.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_sparse.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_gemm_sparse.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_splitk_parallel.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_gemm_splitk_parallel.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_universal.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_gemm_universal.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_broadcast.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_gemm_with_broadcast.h`

 * *Files 2% similar despite different names*

```diff
@@ -117,15 +117,15 @@
   // Replace epilogue
   using Epilogue = typename cutlass::epilogue::threadblock::DefaultEpilogueWithBroadcastTensorOp<
     typename GemmBase::Epilogue::Shape,
     typename GemmBase::Epilogue::WarpMmaOperator,
     GemmBase::Epilogue::kPartitionsK,
     ElementC_,
     typename EpilogueOutputOp::ElementT,
-    ElementC_,
+    typename EpilogueOutputOp::ElementVector,
     EpilogueOutputOp,
     GemmBase::Epilogue::kElementsPerAccess
   >::Epilogue;
 
   // Compose the GEMM kernel
   using GemmKernel = GemmWithFusedEpilogue<
     typename GemmBase::Mma,
@@ -217,15 +217,15 @@
   // Replace epilogue
   using Epilogue = typename cutlass::epilogue::threadblock::DefaultEpilogueWithBroadcastVoltaTensorOp<
     typename GemmBase::Epilogue::Shape,
     typename GemmBase::Epilogue::WarpMmaOperator,
     GemmBase::Epilogue::kPartitionsK,
     ElementC_,
     typename EpilogueOutputOp::ElementT,
-    ElementC_,
+    typename EpilogueOutputOp::ElementVector,
     EpilogueOutputOp,
     GemmBase::Epilogue::kElementsPerAccess
   >::Epilogue;
 
   // Compose the GEMM kernel
   using GemmKernel = GemmWithFusedEpilogue<
     typename GemmBase::Mma,
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_k_reduction.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_gemm_with_k_reduction.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_reduction.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_gemm_with_reduction.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemv.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_gemv.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_rank_2k.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_complex.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_rank_2k_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_grouped.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_rank_2k_grouped.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_universal.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_rank_2k_universal.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_rank_k.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k_complex.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_rank_k_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k_universal.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_rank_k_universal.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_symm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm_complex.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_symm_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm_universal.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_symm_universal.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_trmm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm_complex.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_trmm_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm_universal.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/default_trmm_universal.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/ell_gemm.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/ell_gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm.h`

 * *Files 1% similar despite different names*

```diff
@@ -252,15 +252,15 @@
       {problem_size_k, params.problem_size.n()},
       thread_idx,
       tb_offset_B,
       params.gather_B_indices);
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
-    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
+    int warp_idx = canonical_warp_idx();
     int lane_idx = threadIdx.x % 32;
 
     //
     // Main loop
     //
 
     // Construct thread-scoped matrix multiply
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_array.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_array.h`

 * *Files 4% similar despite different names*

```diff
@@ -189,15 +189,15 @@
 
       //
       // Main loop
       //
       
       // Broadcast the warp_id computed by lane 0 to ensure dependent code
       // is compiled as warp-uniform.
-      int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
+      int warp_idx = canonical_warp_idx();
 
       int lane_idx = threadIdx.x % 32;
       
       Mma mma(shared_storage.main_loop, thread_idx, warp_idx, lane_idx);
 
       typename Mma::FragmentC accumulators;
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_batched.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_batched.h`

 * *Files 2% similar despite different names*

```diff
@@ -200,15 +200,15 @@
 
       //
       // Main loop
       //
 
       // Broadcast the warp_id computed by lane 0 to ensure dependent code
       // is compiled as warp-uniform.
-      int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
+      int warp_idx = canonical_warp_idx();
 
       int lane_idx = threadIdx.x % 32;
       
       Mma mma(shared_storage.main_loop, thread_idx, warp_idx, lane_idx);
 
       typename Mma::FragmentC accumulators;
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_grouped.h`

 * *Files 2% similar despite different names*

```diff
@@ -391,15 +391,15 @@
 
       typename Mma::FragmentC accumulators;
 
       accumulators.clear();
       
       // Broadcast the warp_id computed by lane 0 to ensure dependent code
       // is compiled as warp-uniform.
-      int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
+      int warp_idx = canonical_warp_idx();
 
       int lane_idx = threadIdx.x % 32;
 
       //
       // Matrix multiply phase
       //
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped_problem_visitor.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_grouped_problem_visitor.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped_softmax_mainloop_fusion.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_grouped_softmax_mainloop_fusion.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_layernorm_mainloop_fusion.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_layernorm_mainloop_fusion.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_params.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_params.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_pipelined.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_pipelined.h`

 * *Files 5% similar despite different names*

```diff
@@ -107,15 +107,15 @@
   typename Mma::IteratorB iterator_B(
     params_B,
     ref_B.data(),
     {problem_size.k(), problem_size.n()},
     tb_thread_id,
     tb_offset_B);
 
-  int warp_id = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
+  int warp_id = canonical_warp_idx();
   int lane_id = threadIdx.x % 32;
 
   //
   // Main loop
   //
 
   // Construct thread-scoped matrix multiply
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex.h`

 * *Files 1% similar despite different names*

```diff
@@ -521,15 +521,15 @@
       ptr_B_imag,
       {problem_size_k, params.problem_size.n()},
       thread_idx,
       tb_offset_B);
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
-    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
+    int warp_idx = canonical_warp_idx();
 
     int lane_idx = threadIdx.x % 32;
 
     //
     // Main loop
     //
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex_array.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex_array.h`

 * *Files 1% similar despite different names*

```diff
@@ -463,15 +463,15 @@
         //
         // Compute indices within threadblock and warp.
         //
         int thread_idx = threadIdx.x;
 
         // Broadcast the warp_id computed by lane 0 to ensure dependent code
         // is compiled as warp-uniform.
-        int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
+        int warp_idx = canonical_warp_idx();
         int lane_idx = threadIdx.x % 32;
     
         //
         // Proceed with regular GEMM logic.
         //
 
         // Compute initial location in logical coordinates
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_splitk_parallel.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_splitk_parallel.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_transpose_operands.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_transpose_operands.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_universal.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_universal.h`

 * *Files 2% similar despite different names*

```diff
@@ -38,14 +38,16 @@
 #include "cutlass/cutlass.h"
 
 #include "cutlass/arch/arch.h"
 #include "cutlass/fast_math.h"
 #include "cutlass/matrix_coord.h"
 #include "cutlass/complex.h"
 #include "cutlass/semaphore.h"
+#include "cutlass/gemm/kernel/gemm_universal.hpp"
+
 #include "cutlass/layout/matrix.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/gemm/kernel/params_universal_base.h"
 
 #include "cutlass/trace.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
@@ -57,15 +59,23 @@
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
   typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate
   typename Epilogue_,             ///! Epilogue
   typename ThreadblockSwizzle_    ///! Threadblock swizzling function
 >
-struct GemmUniversal {
+class GemmUniversal<
+  Mma_,
+  Epilogue_,
+  ThreadblockSwizzle_,
+  void,
+  // 3.x kernels use the first template argument to define the ProblemShape tuple
+  // We use this invariant to SFINAE dispatch against either the 2.x API or the 3.x API
+  std::enable_if_t<not cute::is_tuple<Mma_>::value>
+> {
 public:
 
   using Mma = Mma_;
   using Epilogue = Epilogue_;
   using EpilogueOutputOp = typename Epilogue::OutputOp;
   using ThreadblockSwizzle = ThreadblockSwizzle_;
 
@@ -524,15 +534,15 @@
       {problem_size_k, params.problem_size.n()},
       thread_idx,
       tb_offset_B,
       params.ptr_gather_B_indices);
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
-    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
+    int warp_idx = canonical_warp_idx();
 
     int lane_idx = threadIdx.x % 32;
 
     //
     // Main loop
     //
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_universal_streamk.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_universal_streamk.h`

 * *Files 4% similar despite different names*

```diff
@@ -266,16 +266,14 @@
     int64_t batch_stride_A;
     int64_t batch_stride_B;
 
     GemmUniversalMode mode;
 
     ThreadblockSwizzle block_mapping;
 
-    bool quick_dp;
-
     void *barrier_workspace;
     void *partials_workspace;
 
     typename EpilogueOutputOp::Params output_op;
 
     void * ptr_D;
     void * ptr_C;
@@ -363,21 +361,14 @@
         args.mode,
         args.problem_size,
         {ThreadblockShape::kM, ThreadblockShape::kN, ThreadblockShape::kK},
         args.batch_count,
         sm_occupancy,
         device_sms,
         avail_sms);
-
-      quick_dp =
-        (block_mapping.sk_waves == 0) &&
-        (mode == GemmUniversalMode::kGemm) &&
-        !block_mapping.cohort_raster &&
-        !EpilogueOutputOp(output_op).is_source_needed();
-
     }
 
 
     /// Returns the workspace size (in bytes) needed for these parameters
     size_t get_workspace_size() const
     {
       return
@@ -870,15 +861,15 @@
         params.params_D,
         ptr_D,
         params.block_mapping.problem_size.mn(),
         thread_idx,
         threadblock_item_begin);
 
     // Execute the epilogue operator to update the destination tensor.
-    epilogue.unified(
+    epilogue(
         EpilogueOutputOp(params.output_op),
         iterator_D,
         accumulator_tile,
         iterator_C);
   }
 
 
@@ -957,21 +948,22 @@
     typename Mma::IteratorA iterator_A = init_iterator_A(tile_work, params.mode);
     typename Mma::IteratorB iterator_B = init_iterator_B(tile_work, params.mode);
 
     // Initialize accumulators
     AccumulatorTile accumulator_tile;
     accumulator_tile.clear();
 
-    // Perform this tile's range of multiply-accumulate (MAC) iterations
+    // Initialize MMA abstraction
     Mma mma(
       shared_storage.main_loop,
       thread_idx,
       warp_idx,
       lane_idx);
 
+    // Perform this tile's range of multiply-accumulate (MAC) iterations
     mma(tile_work.k_iters_remaining, accumulator_tile, iterator_A, iterator_B, accumulator_tile);
 
     if ((ThreadblockSwizzle::kReductionStrategy == ThreadblockSwizzle::kAtomic) ||
         (params.block_mapping.reduction_blocks == 0) ||
         (block_idx >= dp_start_block_idx))
     {
       //
@@ -1016,37 +1008,35 @@
 
 
   /// Executes one GEMM
   CUTLASS_DEVICE
   void gemm()
   {
     // Initialize block's iteration range
-    int tile_idx, block_iter_begin, block_iters_remaining;
+    int tile_idx = 0;
+    int block_iter_begin = 0;
+    int block_iters_remaining = 0;
+
+    int block_idx = params.block_mapping.get_block_idx();
 
     int sk_padding_start_block_idx =  params.block_mapping.sk_regions() * params.block_mapping.sk_blocks_per_region();
     int dp_start_block_idx = params.block_mapping.sk_waves * params.block_mapping.avail_sms;
     int reduce_start_block_idx = dp_start_block_idx + params.block_mapping.dp_blocks;
     int grid_padding_start_block_idx = reduce_start_block_idx + params.block_mapping.reduction_blocks;
 
-    int block_idx = params.block_mapping.get_block_idx();
-    if (block_idx < sk_padding_start_block_idx)
-    {
-      // This is a SK block
-      int block_iter_end;
-      params.block_mapping.get_iter_extents(block_idx, block_iter_begin, block_iter_end);
-      block_iters_remaining = block_iter_end - block_iter_begin;
+    // Initialize tile work descriptor
+    TileWorkDesc tile_work;
 
-      tile_idx = params.block_mapping.get_sk_tile_idx(block_iter_end - 1);
-    }
-    else if (block_idx < dp_start_block_idx)
-    {
-      // This is a filler block
-      return;
-    }
-    else if (block_idx < reduce_start_block_idx)
+    bool dp_block = (block_idx >= dp_start_block_idx) && (block_idx < reduce_start_block_idx);
+    bool sk_block = (block_idx < sk_padding_start_block_idx);
+    bool reduce_block = (block_idx >= reduce_start_block_idx) &&
+            (block_idx < grid_padding_start_block_idx) &&
+            (ThreadblockSwizzle::kReductionStrategy == ThreadblockSwizzle::kMixed);
+
+    if (dp_block)
     {
       // This is a DP block
       int dp_block_idx = block_idx - dp_start_block_idx;
       int first_dp_tile = (params.block_mapping.cohort_raster) ? 0 : params.block_mapping.sk_tiles;
 
       // Blocks in first DP wave get configured number of tiles
       tile_idx = first_dp_tile + dp_block_idx;
@@ -1054,140 +1044,86 @@
 
       // Blocks in subsequent DP waves get 1 tile
       if (dp_block_idx >= params.block_mapping.avail_sms) {
           tile_allottment = 1;
           tile_idx += (params.block_mapping.dp_first_wave_tiles - 1) * params.block_mapping.avail_sms;
       }
 
-      block_iter_begin = 0;
       block_iters_remaining = params.block_mapping.iters_per_tile() * tile_allottment;
-    }
 
-    else if ((ThreadblockSwizzle::kReductionStrategy == ThreadblockSwizzle::kMixed) &&
-             (block_idx < grid_padding_start_block_idx))
+      init_dp_tile_work(tile_work, tile_idx);
+
+      // DP blocks exit if out of bounds or overlap an SK tile (only possible during cohort rasterization, where dp_first_wave_tiles must be 1)
+      if ((tile_idx < params.block_mapping.sk_tiles) ||
+          (tile_work.tiled_coord.m() >= params.block_mapping.tiled_shape().m()) ||
+          (tile_work.tiled_coord.n() >= params.block_mapping.tiled_shape().n()))
+      {
+        return;
+      }
+    }
+    else if (sk_block)
     {
-      // This is a reduction threadblock
-      int reduce_block_idx = block_idx - reduce_start_block_idx;
-      separate_reduction(reduce_block_idx);
-      return;
+      // This is a SK block
+      int block_iter_end;
+      params.block_mapping.get_iter_extents(block_idx, block_iter_begin, block_iter_end);
+      block_iters_remaining = block_iter_end - block_iter_begin;
+
+      tile_idx = params.block_mapping.get_sk_tile_idx(block_iter_end - 1);
+      init_sk_tile_work(tile_work, tile_idx, block_iter_begin, block_iter_begin + block_iters_remaining);
     }
     else
     {
-      // This is a filler block
+      if (reduce_block)
+      {
+        // This is a reduction threadblock
+        int reduce_block_idx = block_idx - reduce_start_block_idx;
+        separate_reduction(reduce_block_idx);
+      }
+
       return;
     }
 
     // Iteration-processing loop body
     CUTLASS_PRAGMA_NO_UNROLL
     while (true)
     {
-      // Initialize tile work descriptor
-      TileWorkDesc tile_work;
-      if (block_idx >= dp_start_block_idx)
-      {
-        init_dp_tile_work(tile_work, tile_idx);
-
-        // DP blocks exit if out of bounds or overlap an SK tile (only possible during cohort rasterization, where dp_first_wave_tiles must be 1)
-        if ((tile_idx < params.block_mapping.sk_tiles) ||
-          (tile_work.tiled_coord.m() >= params.block_mapping.tiled_shape().m()) ||
-          (tile_work.tiled_coord.n() >= params.block_mapping.tiled_shape().n()))
-        {
-          break;
-        }
-      }
-      else
-      {
-        init_sk_tile_work(tile_work, tile_idx, block_iter_begin, block_iter_begin + block_iters_remaining);
-      }
-
       // Perform this block's share of work for this tile
-      process_tile(tile_work, block_idx, dp_start_block_idx, block_iter_begin);
+      process_tile(
+        tile_work,
+        block_idx,
+        dp_start_block_idx,
+        block_iter_begin);
 
-      // Update remaining work for this block
       block_iters_remaining -= tile_work.k_iters_remaining;
-      if (block_iters_remaining == 0) {
-        // Done
+
+      if (block_iters_remaining == 0)
+      {
         break;
       }
 
       // Continue to next tile
       __syncthreads();
 
       if (block_idx >= dp_start_block_idx)
       {
         // DP block consume their tiles at stride
         tile_idx += params.block_mapping.avail_sms;
+        init_dp_tile_work(tile_work, tile_idx);
       }
       else
       {
         // SK blocks consume their tiles in backwards order
         tile_idx--;
+        init_sk_tile_work(tile_work, tile_idx, block_iter_begin, block_iter_begin + block_iters_remaining);
       }
     }
 
   }
 
 
-  /// Executes one DP-only GEMM
-  CUTLASS_DEVICE
-  void gemm_dp()
-  {
-    int block_idx = blockIdx.x;
-    int tile_idx = block_idx;
-
-    TileWorkDesc tile_work;
-    tile_work.tile_idx = tile_idx;
-    tile_work.iter_begin = tile_idx * params.block_mapping.iters_per_tile();
-    tile_work.k_iters_remaining = params.block_mapping.iters_per_tile();
-    tile_work.k_begin = 0;
-    tile_work.k_end = params.block_mapping.problem_size.k();
-    tile_work.tiled_coord = params.block_mapping.get_tile_offset_row_major(tile_work.tile_idx);
-
-    // Initialize input iterators
-    typename Mma::IteratorA iterator_A = init_iterator_A(tile_work, params.mode);
-    typename Mma::IteratorB iterator_B = init_iterator_B(tile_work, params.mode);
-
-    // Initialize accumulators
-    AccumulatorTile accumulator_tile;
-    accumulator_tile.clear();
-
-    // Perform this tile's range of multiply-accumulate (MAC) iterations
-    Mma mma(
-      shared_storage.main_loop,
-      thread_idx,
-      warp_idx,
-      lane_idx);
-
-    mma(tile_work.k_iters_remaining, accumulator_tile, iterator_A, iterator_B, accumulator_tile);
-
-    ElementC *ptr_D = static_cast<ElementC *>(params.ptr_D);
-
-    // Location of this tile in item-coords
-    MatrixCoord threadblock_item_begin(
-      tile_work.tiled_coord.m() * Mma::Shape::kM,
-      tile_work.tiled_coord.n() * Mma::Shape::kN
-    );
-
-    // Tile iterator writing to destination tensor.
-    typename Epilogue::OutputTileIterator iterator_D(
-        params.params_D,
-        ptr_D,
-        params.block_mapping.problem_size.mn(),
-        thread_idx,
-        threadblock_item_begin);
-
-    // Execute the epilogue operator to update the destination tensor.
-    epilogue(
-        EpilogueOutputOp(params.output_op),
-        iterator_D,
-        accumulator_tile);
-  }
-
-
-
 public:
 
   //
   // Device-only API
   //
 
   // Factory invocation
@@ -1220,24 +1156,14 @@
   {}
 
 
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()()
   {
-#if (__CUDACC_VER_MAJOR__ > 10)
-    if (params.quick_dp)
-    {
-      // Simple (low-bootstrap latency) GEMM code path for data-parallel only.  (kBatched and kArray
-      // modes will only be launched using a data-parallel configurations)
-      gemm_dp();
-      return;
-    }
-#endif
-
     // Generic SK code path
     gemm();
 
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_with_fused_epilogue.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_with_fused_epilogue.h`

 * *Files 0% similar despite different names*

```diff
@@ -914,15 +914,15 @@
       batch_stride_B(batch_stride_B), 
       batch_stride_C(batch_stride_C), 
       batch_stride_Vector(batch_stride_Vector),
       batch_stride_Tensor(batch_stride_Tensor),
       lda(lda), ldb(ldb), ldc(ldc), ldd(ldd), ldr(ldr), ldt(ldt)
     {
       CUTLASS_TRACE_HOST("GemmWithFusedEpilogue::Arguments::Arguments() - problem_size: " << problem_size);
-      CUTLASS_TRACE_HOST("  ptr_Reduction: " << (void *)this->ptr_Reduction);
+      CUTLASS_TRACE_HOST("  ptr_Vector: " << (void *)this->ptr_Vector);
       CUTLASS_TRACE_HOST("  ptr_Tensor: " << (void *)this->ptr_Tensor);
       CUTLASS_TRACE_HOST("  ldr: " << this->ldr);
       CUTLASS_TRACE_HOST("  ldt: " << this->ldt);
     }
 
     /// Returns arguments for the transposed problem
     Arguments transposed_problem() const {
@@ -1015,15 +1015,15 @@
       batch_stride_A(args.batch_stride_A),
       batch_stride_B(args.batch_stride_B),
       batch_stride_C(args.batch_stride_C),
       batch_stride_Vector(args.batch_stride_Vector),
       batch_stride_Tensor(args.batch_stride_Tensor)
     {
       CUTLASS_TRACE_HOST("GemmWithFusedEpilogue::Params::Params() - problem_size: " << problem_size);
-      CUTLASS_TRACE_HOST("  ptr_Reduction: " << (void *)this->ptr_Reduction);
+      CUTLASS_TRACE_HOST("  ptr_Vector: " << (void *)this->ptr_Vector);
       CUTLASS_TRACE_HOST("  ptr_Tensor: " << (void *)this->ptr_Tensor);
       CUTLASS_TRACE_HOST("  ldr: " << this->ldr);
       CUTLASS_TRACE_HOST("  ldt: " << args.ldt);
     }
 
     /// Lightweight update given a subset of arguments.  Problem geometry is assumed
     /// to remain the same.
@@ -1218,15 +1218,15 @@
       ptr_B,
       {problem_size_k, params.problem_size.n()},
       thread_idx,
       tb_offset_B);
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
-    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
+    int warp_idx = canonical_warp_idx();
 
     int lane_idx = threadIdx.x % 32;
 
     //
     // Main loop
     //
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_with_k_reduction.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemm_with_k_reduction.h`

 * *Files 0% similar despite different names*

```diff
@@ -501,15 +501,15 @@
       ptr_B,
       {problem_size_k, params.problem_size.n()},
       thread_idx,
       tb_offset_B);
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
-    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
+    int warp_idx = canonical_warp_idx();
 
     int lane_idx = threadIdx.x % 32;
 
     //
     // Main loop
     //
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemv.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemv.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemv_batched_strided.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/gemv_batched_strided.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/grouped_problem_visitor.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/grouped_problem_visitor.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/params_universal_base.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/params_universal_base.h`

 * *Files 2% similar despite different names*

```diff
@@ -185,34 +185,34 @@
 
   /// Assign and initialize the specified workspace buffer.  Assumes
   /// the memory allocated to workspace is at least as large as get_workspace_size().
   Status init_workspace(
     void *workspace,
     cudaStream_t stream = nullptr)
   {
+    semaphore = static_cast<int *>(workspace);
     // Zero-initialize entire workspace
-    if (workspace)
+    if (semaphore)
     {
       size_t workspace_bytes = get_workspace_size();
 
       CUTLASS_TRACE_HOST("  Initialize " << workspace_bytes << " workspace bytes");
 
       cudaError_t result = cudaMemsetAsync(
-        workspace,
+        semaphore,
         0,
         workspace_bytes,
         stream);
 
       if (result != cudaSuccess) {
         CUTLASS_TRACE_HOST("  cudaMemsetAsync() returned error " << cudaGetErrorString(result));
         return Status::kErrorInternal;
       }
     }
 
-    semaphore = static_cast<int *>(workspace);
     return Status::kSuccess;
   }
 
 
   /// Returns the GEMM volume in thread block tiles
   GemmCoord get_tiled_shape() const
   {
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped.h`

 * *Files 1% similar despite different names*

```diff
@@ -521,15 +521,15 @@
         ptr_A,
         {problem_size_k, problem_size.n()},
         thread_idx,
         tb_offset_KxN);
 
       // Broadcast the warp_id computed by lane 0 to ensure dependent code
       // is compiled as warp-uniform.
-      int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
+      int warp_idx = canonical_warp_idx();
 
       int lane_idx = threadIdx.x % 32;
 
       //
       // Main loop
       //
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped_problem_visitor.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped_problem_visitor.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_transpose_operands.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/rank_2k_transpose_operands.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_universal.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/rank_2k_universal.h`

 * *Files 1% similar despite different names*

```diff
@@ -446,15 +446,15 @@
       ptr_A,
       {problem_size_k, params.problem_size.n()},
       thread_idx,
       tb_offset_KxN);
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
-    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
+    int warp_idx = canonical_warp_idx();
 
     int lane_idx = threadIdx.x % 32;
 
     //
     // Main loop
     //
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_k_universal.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/rank_k_universal.h`

 * *Files 1% similar despite different names*

```diff
@@ -399,15 +399,15 @@
       ptr_B,
       {problem_size_k, params.problem_size.n()},
       thread_idx,
       tb_offset_B);
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
-    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
+    int warp_idx = canonical_warp_idx();
 
     int lane_idx = threadIdx.x % 32;
 
     //
     // Main loop
     //
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/sparse_gemm.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/sparse_gemm.h`

 * *Files 1% similar despite different names*

```diff
@@ -273,15 +273,15 @@
         params.params_E, params.ref_E.data(),
         {params.problem_size.m(),
          problem_size_k / kSparse / kElementsPerElementE},
         thread_idx, tb_offset_E);
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
-    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
+    int warp_idx = canonical_warp_idx();
     int lane_idx = threadIdx.x % 32;
 
     //
     // Main loop
     //
 
     // Construct thread-scoped matrix multiply
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/symm_universal.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/symm_universal.h`

 * *Files 0% similar despite different names*

```diff
@@ -411,15 +411,15 @@
     };
 
     // Compute position within threadblock
     int thread_idx = threadIdx.x;
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
-    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
+    int warp_idx = canonical_warp_idx();
 
     int lane_idx = threadIdx.x % 32;
 
     //
     // Main loop
     //
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/trmm_universal.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/kernel/trmm_universal.h`

 * *Files 1% similar despite different names*

```diff
@@ -376,15 +376,15 @@
     };
 
     // Compute position within threadblock
     int thread_idx = threadIdx.x;
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
-    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
+    int warp_idx = canonical_warp_idx();
 
     int lane_idx = threadIdx.x % 32;
 
     //
     // Main loop
     //
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/thread/mma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm50.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/thread/mma_sm50.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm60.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/thread/mma_sm60.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm61.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/thread/mma_sm61.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_ell_mma.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_ell_mma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_gemv_core.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_gemv_core.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_mma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_mma_core.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_simt.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_mma_core_simt.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm70.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm70.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm75.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm75.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm80.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm80.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_access_size.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_access_size.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_reduction.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_reduction.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_wmma.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_mma_core_wmma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_layernorm_mainloop_fusion.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_mma_layernorm_mainloop_fusion.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_multistage.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_pipelined.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_pipelined.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_softmax_mainloop_fusion.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_mma_softmax_mainloop_fusion.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_with_reduction.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex.h`

 * *Files 15% similar despite different names*

```diff
@@ -24,118 +24,136 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
+
 /*! \file
-    \brief Template for a pipelined GEMM kernel. Does not compute batching or support split-K.
+    \brief Template for a multistage GEMM kernel. Does not compute batching or support split-K.
 */
 
 #pragma once
 
+#include "cutlass/arch/arch.h"
 #include "cutlass/cutlass.h"
+#include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
 #include "cutlass/numeric_types.h"
-#include "cutlass/arch/arch.h"
-
-#include "cutlass/layout/matrix.h"
 #include "cutlass/transform/threadblock/predicated_tile_iterator.h"
-#include "cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h"
-#include "cutlass/gemm/threadblock/default_mma_core_with_reduction.h"
+#include "cutlass/gemm/threadblock/default_multistage_mma_complex_core_sm80.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace threadblock {
 
 ////////////////////////////////////////////////////////////////////////////////
 
 template <
     /// Element type for A matrix operand
+    typename ElementA_,
+    /// Layout type for A matrix operand
+    typename LayoutA_,
+    /// Element type for B matrix operand
+    typename ElementB_,
+    /// Layout type for B matrix operand
+    typename LayoutB_,
+    /// Element type for internal accumulation
+    typename ElementAccumulator_,
+    /// Layout type for C and D matrix operands
+    typename LayoutC_,
+    /// Operator class tag
+    typename OperatorClass_,
+    /// Tag indicating architecture to tune for
+    typename ArchTag_,
+    /// Threadblock-level tile size (concept: GemmShape)
+    typename ThreadblockShape_,
+    /// Warp-level tile size (concept: GemmShape)
+    typename WarpShape_,
+    /// Instruction-level tile size (concept: GemmShape)
+    typename InstructionShape_,
+    /// Number of stages used in the pipelined mainloop
+    int Stages,
+    /// Complex transformation on operand A
+    ComplexTransform TransformA = ComplexTransform::kNone,
+    /// Complex transformation on operand B
+    ComplexTransform TransformB = ComplexTransform::kNone,
+    /// Multiply-add operator (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
+    typename Operator = arch::OpMultiplyAddComplex,
+    /// Store the accumulators in row major or column major.  Row major is used
+    /// when output layout is interleaved.
+    bool AccumulatorsInRowMajor = false>
+struct DefaultMultistageMmaComplex;
+
+////////////////////////////////////////////////////////////////////////////////
+
+/// Specialization for row-major output
+template <
+    /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentA,
     /// Element type for B matrix operand
     typename ElementB,
     /// Layout type for B matrix operand
     typename LayoutB,
-    /// Access granularity of B matrix in units of elements
-    int kAlignmentB,
     /// Element type for internal accumulation
     typename ElementAccumulator,
-    /// Layout type for C and D matrix operands
-    typename LayoutC,
-    /// Operator class tag
+    /// Tag indicating architecture to tune for
     typename OperatorClass,
-    ///                                                                                               
-    bool ReduceKForA_,
     /// Tag indicating architecture to tune for
     typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape,
     /// Instruction-level tile size (concept: GemmShape)
     typename InstructionShape,
-    /// Number of stages used in the pipelined mainloop
+    /// Number of stages used in the multistage mainloop
     int Stages,
-    /// Operation perfomed by GEMM
-    typename Operator,
-    /// Store the accumulators in row major or column major.  Row major is used
-    /// when output layout is interleaved.
-    bool AccumulatorsInRowMajor = false,
-    /// Use zfill or predicate for SM80 out-of-bound cp.async 
-    SharedMemoryClearOption SharedMemoryClear = SharedMemoryClearOption::kNone
-    >
-struct DefaultMmaWithReduction {
-
-  static cutlass::arch::CacheOperation::Kind const CacheOpA =
-      ((sizeof_bits<ElementA>::value * kAlignmentA) == 128)
-          ? cutlass::arch::CacheOperation::Global
-          : cutlass::arch::CacheOperation::Always;
-
-  static cutlass::arch::CacheOperation::Kind const CacheOpB =
-      ((sizeof_bits<ElementB>::value * kAlignmentB) == 128)
-          ? cutlass::arch::CacheOperation::Global
-          : cutlass::arch::CacheOperation::Always;
-
+    /// Complex transformation on operand A
+    ComplexTransform TransformA,
+    /// Complex transformation on operand B
+    ComplexTransform TransformB,
+    /// Multiply-add operator (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
+    typename Operator>
+struct DefaultMultistageMmaComplex<ElementA, LayoutA, ElementB, LayoutB,
+                            ElementAccumulator, layout::RowMajor, OperatorClass,
+                            ArchTag, ThreadblockShape, WarpShape,
+                            InstructionShape, Stages, TransformA, TransformB, Operator> {
   // Define the MmaCore components
-  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaWithReductionCore<
-      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
-      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
-      ReduceKForA_,  Stages, Operator, false, CacheOpA, CacheOpB>;
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplexCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA, 
+      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, OperatorClass,
+      Stages, TransformA, TransformB, Operator>;
 
   // Define iterators over tiles from the A operand
   using ThreadMapA = typename MmaCore::IteratorThreadMapA;
-  using AccessTypeA = cutlass::Array<ElementA, kAlignmentA>;
+  using AccessTypeA = cutlass::Array<ElementA, ThreadMapA::kElementsPerAccess>;
   using IteratorA =
       cutlass::transform::threadblock::PredicatedTileAccessIterator<
           cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
           ElementA, LayoutA, 1, ThreadMapA, AccessTypeA>;
 
   // Define iterators over tiles from the B operand
   using ThreadMapB = typename MmaCore::IteratorThreadMapB;
-  using AccessTypeB = cutlass::Array<ElementB, kAlignmentB>;
+  using AccessTypeB = cutlass::Array<ElementB, ThreadMapB::kElementsPerAccess>;
   using IteratorB =
       cutlass::transform::threadblock::PredicatedTileAccessIterator<
           cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
           ElementB, LayoutB, 0, ThreadMapB, AccessTypeB>;
 
   // Define the threadblock-scoped multistage matrix multiply
-  using ThreadblockMma = cutlass::gemm::threadblock::MmaWithReductionMultistage<
+  using ThreadblockMma = cutlass::gemm::threadblock::MmaMultistage<
       typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
       MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
       MmaCore::kCacheOpB, ElementAccumulator, layout::RowMajor,
-      typename MmaCore::MmaPolicy, Stages, SharedMemoryClear>;
+      typename MmaCore::MmaPolicy, Stages>;
 };
 
-////////////////////////////////////////////////////////////////////////////////
-
-} // namespace threadblock
-} // namespace gemm
-} // namespace cutlass 
+}  // namespace threadblock
+}  // namespace gemm
+}  // namespace cutlass
 
 ////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_sparse_mma.h`

 * *Files 23% similar despite different names*

```diff
@@ -24,45 +24,57 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
 /*! \file
-    \brief Template for a multistage GEMM kernel. Does not compute batching or support split-K.
+    \brief Template for a pipelined GEMM kernel. Does not compute batching or support split-K.
 */
 
 #pragma once
 
-#include "cutlass/arch/arch.h"
 #include "cutlass/cutlass.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
 #include "cutlass/numeric_types.h"
+#include "cutlass/arch/arch.h"
+#include "cutlass/arch/wmma.h"
+
+#include "cutlass/layout/matrix.h"
 #include "cutlass/transform/threadblock/predicated_tile_iterator.h"
-#include "cutlass/gemm/threadblock/default_multistage_mma_complex_core_sm80.h"
+#include "cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h"
+#include "cutlass/gemm/threadblock/default_mma_core_sm70.h"
+#include "cutlass/gemm/threadblock/default_mma_core_sm75.h"
+#include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
+#include "cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h"
+#if defined(CUTLASS_ARCH_WMMA_ENABLED)
+#include "cutlass/gemm/threadblock/default_mma_core_wmma.h"
+#endif //CUTLASS_ARCH_WMMA_ENABLED
 
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace threadblock {
 
 ////////////////////////////////////////////////////////////////////////////////
 
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
     typename LayoutA_,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentA,
     /// Element type for B matrix operand
     typename ElementB_,
     /// Layout type for B matrix operand
     typename LayoutB_,
+    /// Access granularity of B matrix in units of elements
+    int kAlignmentB,
     /// Element type for internal accumulation
     typename ElementAccumulator_,
     /// Layout type for C and D matrix operands
     typename LayoutC_,
     /// Operator class tag
     typename OperatorClass_,
     /// Tag indicating architecture to tune for
@@ -71,89 +83,114 @@
     typename ThreadblockShape_,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape_,
     /// Instruction-level tile size (concept: GemmShape)
     typename InstructionShape_,
     /// Number of stages used in the pipelined mainloop
     int Stages,
-    /// Complex transformation on operand A
-    ComplexTransform TransformA = ComplexTransform::kNone,
-    /// Complex transformation on operand B
-    ComplexTransform TransformB = ComplexTransform::kNone,
-    /// Multiply-add operator (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
-    typename Operator = arch::OpMultiplyAddComplex,
+    /// Operation perfomed by GEMM
+    typename Operator,
     /// Store the accumulators in row major or column major.  Row major is used
     /// when output layout is interleaved.
-    bool AccumulatorsInRowMajor = false>
-struct DefaultMultistageMmaComplex;
+    bool AccumulatorsInRowMajor = false
+    >
+struct DefaultSparseMma;
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Specialization for row-major output
+/// Specialization for row-major output (OperatorClass TensorOp)
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentA,
     /// Element type for B matrix operand
     typename ElementB,
     /// Layout type for B matrix operand
     typename LayoutB,
+    /// Access granularity of B matrix in units of elements
+    int kAlignmentB,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Tag indicating architecture to tune for
-    typename OperatorClass,
-    /// Tag indicating architecture to tune for
     typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape,
     /// Instruction-level tile size (concept: GemmShape)
     typename InstructionShape,
     /// Number of stages used in the multistage mainloop
     int Stages,
-    /// Complex transformation on operand A
-    ComplexTransform TransformA,
-    /// Complex transformation on operand B
-    ComplexTransform TransformB,
-    /// Multiply-add operator (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
-    typename Operator>
-struct DefaultMultistageMmaComplex<ElementA, LayoutA, ElementB, LayoutB,
-                            ElementAccumulator, layout::RowMajor, OperatorClass,
-                            ArchTag, ThreadblockShape, WarpShape,
-                            InstructionShape, Stages, TransformA, TransformB, Operator> {
+    /// Operation perfomed by GEMM
+    typename Operator
+    >
+struct DefaultSparseMma<ElementA, LayoutA, kAlignmentA, ElementB, LayoutB,
+                  kAlignmentB, ElementAccumulator, layout::RowMajor,
+                  arch::OpClassTensorOp, ArchTag, ThreadblockShape, WarpShape,
+                  InstructionShape, Stages, Operator, false> {
+  static cutlass::arch::CacheOperation::Kind const CacheOpA =
+      ((sizeof_bits<ElementA>::value * kAlignmentA) == 128)
+          ? cutlass::arch::CacheOperation::Global
+          : cutlass::arch::CacheOperation::Always;
+
+  static cutlass::arch::CacheOperation::Kind const CacheOpB =
+      ((sizeof_bits<ElementB>::value * kAlignmentB) == 128)
+          ? cutlass::arch::CacheOperation::Global
+          : cutlass::arch::CacheOperation::Always;
+  
+
   // Define the MmaCore components
-  using MmaCore = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplexCore<
-      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA, 
-      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, OperatorClass,
-      Stages, TransformA, TransformB, Operator>;
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultSparseMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
+      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
+      Stages, Operator, false, CacheOpA, CacheOpB>;
+
+  static int const kSparse = MmaCore::kSparse;
 
   // Define iterators over tiles from the A operand
   using ThreadMapA = typename MmaCore::IteratorThreadMapA;
-  using AccessTypeA = cutlass::Array<ElementA, ThreadMapA::kElementsPerAccess>;
+  using AccessTypeA = cutlass::Array<ElementA, kAlignmentA>;
   using IteratorA =
       cutlass::transform::threadblock::PredicatedTileAccessIterator<
-          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
+          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK / kSparse>,
           ElementA, LayoutA, 1, ThreadMapA, AccessTypeA>;
 
   // Define iterators over tiles from the B operand
   using ThreadMapB = typename MmaCore::IteratorThreadMapB;
-  using AccessTypeB = cutlass::Array<ElementB, ThreadMapB::kElementsPerAccess>;
+  using AccessTypeB = cutlass::Array<ElementB, kAlignmentB>;
   using IteratorB =
       cutlass::transform::threadblock::PredicatedTileAccessIterator<
           cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
           ElementB, LayoutB, 0, ThreadMapB, AccessTypeB>;
 
+  // Define iterators over tiles from the E operand
+  using ElementE = typename MmaCore::ElementE;
+  using LayoutE = typename MmaCore::GmemLayoutE;
+  using ThreadMapE = typename MmaCore::IteratorThreadMapE;
+  using AccessTypeE =
+      cutlass::Array<ElementE, 128 / sizeof_bits<ElementE>::value>;
+  using IteratorE =
+      cutlass::transform::threadblock::PredicatedTileAccessIterator<
+          cutlass::MatrixShape<ThreadblockShape::kM,
+                               ThreadblockShape::kK / kSparse /
+                                   MmaCore::kElementsPerElementE>,
+          ElementE, LayoutE, 1, ThreadMapE, AccessTypeE>;
+
   // Define the threadblock-scoped multistage matrix multiply
-  using ThreadblockMma = cutlass::gemm::threadblock::MmaMultistage<
+  using ThreadblockMma = cutlass::gemm::threadblock::SparseMmaMultistage<
       typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
       MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
       MmaCore::kCacheOpB, ElementAccumulator, layout::RowMajor,
+      IteratorE, typename MmaCore::SmemIteratorE, MmaCore::kCacheOpE,
       typename MmaCore::MmaPolicy, Stages>;
 };
 
-}  // namespace threadblock
-}  // namespace gemm
-}  // namespace cutlass
+////////////////////////////////////////////////////////////////////////////////
+
+} // namespace threadblock
+} // namespace gemm
+} // namespace cutlass 
 
 ////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core_sm80.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core_sm80.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_trmm_complex.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_multistage_trmm_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_sparse_mma.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_simt.h`

 * *Files 21% similar despite different names*

```diff
@@ -25,172 +25,240 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Template for a pipelined GEMM kernel. Does not compute batching or support split-K.
+    \brief Templates implementing warp-level matrix multiply-accumulate operations.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
+#include "cutlass/array.h"
 #include "cutlass/numeric_types.h"
-#include "cutlass/arch/arch.h"
-#include "cutlass/arch/wmma.h"
+#include "cutlass/matrix_shape.h"
+#include "cutlass/gemm/gemm.h"
+#include "cutlass/gemm/warp/mma.h"
 
-#include "cutlass/layout/matrix.h"
-#include "cutlass/transform/threadblock/predicated_tile_iterator.h"
-#include "cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sm70.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sm75.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h"
-#if defined(CUTLASS_ARCH_WMMA_ENABLED)
-#include "cutlass/gemm/threadblock/default_mma_core_wmma.h"
-#endif //CUTLASS_ARCH_WMMA_ENABLED
+#include "cutlass/gemm/thread/mma.h"
 
-////////////////////////////////////////////////////////////////////////////////
+#include "cutlass/gemm/warp/mma_simt_tile_iterator.h"
+#include "cutlass/gemm/warp/mma_simt_policy.h"
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
-namespace threadblock {
+namespace warp {
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
+/// Structure to compute the matrix product targeting CUDA cores and SIMT math instructions.
 template <
-    /// Element type for A matrix operand
-    typename ElementA_,
-    /// Layout type for A matrix operand
-    typename LayoutA_,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentA,
-    /// Element type for B matrix operand
-    typename ElementB_,
-    /// Layout type for B matrix operand
-    typename LayoutB_,
-    /// Access granularity of B matrix in units of elements
-    int kAlignmentB,
-    /// Element type for internal accumulation
-    typename ElementAccumulator_,
-    /// Layout type for C and D matrix operands
-    typename LayoutC_,
-    /// Operator class tag
-    typename OperatorClass_,
-    /// Tag indicating architecture to tune for
-    typename ArchTag_,
-    /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape_,
-    /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape_,
-    /// Instruction-level tile size (concept: GemmShape)
-    typename InstructionShape_,
-    /// Number of stages used in the pipelined mainloop
-    int Stages,
-    /// Operation perfomed by GEMM
-    typename Operator,
-    /// Store the accumulators in row major or column major.  Row major is used
-    /// when output layout is interleaved.
-    bool AccumulatorsInRowMajor = false
-    >
-struct DefaultSparseMma;
+  /// Size of the Gemm problem - concept: gemm::GemmShape<>
+  typename Shape_,
+  /// Data type of A elements
+  typename ElementA_,
+  /// Layout of A matrix (concept: MatrixLayout)
+  typename LayoutA_,
+  /// Data type of B elements
+  typename ElementB_,
+  /// Layout of B matrix (concept: MatrixLayout)
+  typename LayoutB_,
+  /// Element type of C matrix
+  typename ElementC_,
+  /// Layout of C matrix (concept: MatrixLayout)
+  typename LayoutC_,
+  /// Shape of the warp in units of thread (concept: MmaSimtPolicy)
+  typename Policy_,
+  /// Number of partitions along K dimension
+  int PartitionsK = 1,
+  /// Complex transformation on operand A
+  ComplexTransform TransformA = ComplexTransform::kNone,
+  /// Complex transformation on operand B
+  ComplexTransform TransformB = ComplexTransform::kNone,
+  /// Used for partial specialization
+  typename Enable = bool
+>
+class MmaSimt {
+public:
+  /// Shape of warp-level matrix operation (concept: GemmShape)
+  using Shape = Shape_;
+
+  /// Data type of multiplicand A
+  using ElementA = ElementA_;
+
+  /// Layout of multiplicand A
+  using LayoutA = LayoutA_;
+
+  /// Data type of multiplicand B
+  using ElementB = ElementB_;
+
+  /// Layout of multiplicand B
+  using LayoutB = LayoutB_;
+
+  /// Data type of accumulator matrix C
+  using ElementC = ElementC_;
+
+  /// Layout of accumulator matrix C
+  using LayoutC = LayoutC_;
+
+  /// Shape of the warp in units of thread (concept: MmaLanePolicySimt)
+  using Policy = Policy_;
+
+  /// Indicates class of matrix operator
+  using OperatorClass = arch::OpClassSimt;
+
+  /// Hard-coded for now
+  using ArchTag = arch::Sm50;
+
+  /// Complex transform on A operand
+  static ComplexTransform const kTransformA = TransformA;
+
+  /// Complex transform on B operand
+  static ComplexTransform const kTransformB = TransformB;
+
+  /// Layout of threads
+  using ThreadLayoutA = typename platform::conditional< platform::is_same< layout::ColumnMajorInterleaved<4>, LayoutA >::value,
+                  layout::ColumnMajor,
+                  typename platform::conditional < platform::is_same< layout::RowMajorInterleaved<4>, LayoutA >::value,
+                      layout::RowMajor,
+                      LayoutA>::type
+                 >::type;
+  
+  using ThreadLayoutB = typename platform::conditional< platform::is_same< layout::ColumnMajorInterleaved<4>, LayoutB >::value,
+                  layout::ColumnMajor,
+                  typename platform::conditional < platform::is_same< layout::RowMajorInterleaved<4>, LayoutB >::value,
+                      layout::RowMajor,
+                      LayoutB>::type
+                 >::type;
+
+  static constexpr bool use_dp4a = (platform::is_same< layout::ColumnMajorInterleaved<4>, LayoutA>::value || 
+                                    platform::is_same< layout::RowMajorInterleaved<4>, LayoutA >::value) && 
+                                    platform::is_same< ElementA, int8_t >::value && 
+                                    platform::is_same< ElementB, int8_t >::value;
+
+  using dp4a_type = typename platform::conditional< use_dp4a , int8_t, bool >::type;
+
+  /// Thread-level matrix multiply accumulate operator
+  using ThreadMma = thread::Mma<
+    GemmShape<
+      Shape::kM / Policy::WarpShape::kRow,
+      Shape::kN / Policy::WarpShape::kColumn,
+      Policy::LaneMmaShape::kK>,
+    ElementA,
+    ThreadLayoutA,
+    ElementB,
+    ThreadLayoutB,
+    ElementC,
+    LayoutC,
+    arch::OpMultiplyAdd,
+    dp4a_type
+  >;
 
-////////////////////////////////////////////////////////////////////////////////
+  /// Underlying matrix multiply operator (concept: arch::Mma)
+  using ArchMmaOperator = typename ThreadMma::ArchMmaOperator;
 
-/// Specialization for row-major output (OperatorClass TensorOp)
-template <
-    /// Element type for A matrix operand
-    typename ElementA,
-    /// Layout type for A matrix operand
-    typename LayoutA,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentA,
-    /// Element type for B matrix operand
-    typename ElementB,
-    /// Layout type for B matrix operand
-    typename LayoutB,
-    /// Access granularity of B matrix in units of elements
-    int kAlignmentB,
-    /// Element type for internal accumulation
-    typename ElementAccumulator,
-    /// Tag indicating architecture to tune for
-    typename ArchTag,
-    /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape,
-    /// Instruction-level tile size (concept: GemmShape)
-    typename InstructionShape,
-    /// Number of stages used in the multistage mainloop
-    int Stages,
-    /// Operation perfomed by GEMM
-    typename Operator
-    >
-struct DefaultSparseMma<ElementA, LayoutA, kAlignmentA, ElementB, LayoutB,
-                  kAlignmentB, ElementAccumulator, layout::RowMajor,
-                  arch::OpClassTensorOp, ArchTag, ThreadblockShape, WarpShape,
-                  InstructionShape, Stages, Operator, false> {
-  static cutlass::arch::CacheOperation::Kind const CacheOpA =
-      ((sizeof_bits<ElementA>::value * kAlignmentA) == 128)
-          ? cutlass::arch::CacheOperation::Global
-          : cutlass::arch::CacheOperation::Always;
-
-  static cutlass::arch::CacheOperation::Kind const CacheOpB =
-      ((sizeof_bits<ElementB>::value * kAlignmentB) == 128)
-          ? cutlass::arch::CacheOperation::Global
-          : cutlass::arch::CacheOperation::Always;
+  /// Indicates math operator 
+  using MathOperator = typename ArchMmaOperator::Operator;
   
+  /// Shape of the underlying instruction
+  using InstructionShape = GemmShape<1,1,use_dp4a ? 4 : 1>;
 
-  // Define the MmaCore components
-  using MmaCore = typename cutlass::gemm::threadblock::DefaultSparseMmaCore<
-      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
-      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
-      Stages, Operator, false, CacheOpA, CacheOpB>;
-
-  static int const kSparse = MmaCore::kSparse;
-
-  // Define iterators over tiles from the A operand
-  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
-  using AccessTypeA = cutlass::Array<ElementA, kAlignmentA>;
-  using IteratorA =
-      cutlass::transform::threadblock::PredicatedTileAccessIterator<
-          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK / kSparse>,
-          ElementA, LayoutA, 1, ThreadMapA, AccessTypeA>;
-
-  // Define iterators over tiles from the B operand
-  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
-  using AccessTypeB = cutlass::Array<ElementB, kAlignmentB>;
-  using IteratorB =
-      cutlass::transform::threadblock::PredicatedTileAccessIterator<
-          cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
-          ElementB, LayoutB, 0, ThreadMapB, AccessTypeB>;
-
-  // Define iterators over tiles from the E operand
-  using ElementE = typename MmaCore::ElementE;
-  using LayoutE = typename MmaCore::GmemLayoutE;
-  using ThreadMapE = typename MmaCore::IteratorThreadMapE;
-  using AccessTypeE =
-      cutlass::Array<ElementE, 128 / sizeof_bits<ElementE>::value>;
-  using IteratorE =
-      cutlass::transform::threadblock::PredicatedTileAccessIterator<
-          cutlass::MatrixShape<ThreadblockShape::kM,
-                               ThreadblockShape::kK / kSparse /
-                                   MmaCore::kElementsPerElementE>,
-          ElementE, LayoutE, 1, ThreadMapE, AccessTypeE>;
-
-  // Define the threadblock-scoped multistage matrix multiply
-  using ThreadblockMma = cutlass::gemm::threadblock::SparseMmaMultistage<
-      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
-      MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
-      MmaCore::kCacheOpB, ElementAccumulator, layout::RowMajor,
-      IteratorE, typename MmaCore::SmemIteratorE, MmaCore::kCacheOpE,
-      typename MmaCore::MmaPolicy, Stages>;
+public:
+
+  /// Iterates over the A operand in memory
+  using IteratorA = MmaSimtTileIterator<
+    MatrixShape<Shape::kM, Policy::LaneMmaShape::kK>,
+    Operand::kA,
+    ElementA,
+    LayoutA,
+    Policy,
+    PartitionsK,
+    Shape::kK
+  >;
+
+  /// Storage for A tile
+  using FragmentA = typename IteratorA::Fragment;
+
+  /// Storage for transformed A tile
+  using TransformedFragmentA = FragmentA;
+
+  /// Iterates over the B operand in memory
+  using IteratorB = MmaSimtTileIterator<
+    MatrixShape<Policy::LaneMmaShape::kK, Shape::kN>,
+    Operand::kB,
+    ElementB,
+    LayoutB,
+    Policy,
+    PartitionsK,
+    Shape::kK
+  >;
+
+  /// Storage for B tile
+  using FragmentB = typename IteratorB::Fragment;
+
+  /// Storage for transformed A tile
+  using TransformedFragmentB = FragmentB;
+
+  /// Iterates over the C operand in memory
+  using IteratorC = MmaSimtTileIterator<
+    MatrixShape<Shape::kM, Shape::kN>,
+    Operand::kC,
+    ElementC,
+    LayoutC,
+    Policy
+  >;
+
+  /// Storage for C tile
+  using FragmentC = typename ThreadMma::FragmentC;
+
+public:
+
+  //
+  // Methods
+  //
+
+  /// Ctor
+  CUTLASS_DEVICE
+  MmaSimt() {}
+
+  /// Performs a warp-level matrix multiply-accumulate operation
+  CUTLASS_DEVICE
+  void operator()(
+    FragmentC &d, 
+    FragmentA a, 
+    FragmentB b, 
+    FragmentC const &c, int group_idx = 0) const {
+
+    ThreadMma mma;
+
+    if (kTransformA == ComplexTransform::kConjugate) {
+      conjugate<FragmentA> conj_a;
+      a = conj_a(a);
+    }
+
+    if (kTransformB == ComplexTransform::kConjugate) {
+      conjugate<FragmentB> conj_b;
+      b = conj_b(b);
+    }
+
+    mma(d, a, b, c);
+  }
+
+  /// Transform the mma operands to the required types
+  CUTLASS_DEVICE
+  void transform(TransformedFragmentA &dst_A, TransformedFragmentB &dst_B,
+                 FragmentA const &A, FragmentB const &B) const {
+    //TODO: Implement this
+    dst_A = A;
+    dst_B = B;
+  }
 };
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-} // namespace threadblock
+} // namespace warp
 } // namespace gemm
-} // namespace cutlass 
-
-////////////////////////////////////////////////////////////////////////////////
+} // namespace cutlass
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_trmm.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/default_trmm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/ell_mma_multistage.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/ell_mma_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/ell_mma_pipelined.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/ell_mma_pipelined.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/gemv.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/gemv.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/index_remat.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/index_remat.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_base.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/mma_base.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_blas3_multistage.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/mma_blas3_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_layernorm_mainloop_fusion_multistage.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/mma_layernorm_mainloop_fusion_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_multistage.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/mma_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_pipelined.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/mma_pipelined.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_base.h` & `flash_attn-2.0.0/csrc/cutlass/examples/45_dual_gemm/threadblock/dual_mma_base.h`

 * *Files 14% similar despite different names*

```diff
@@ -38,167 +38,191 @@
 #include "cutlass/arch/memory.h"
 #include "cutlass/array.h"
 #include "cutlass/cutlass.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/matrix_shape.h"
 #include "cutlass/numeric_types.h"
 
+#include "cutlass/gemm/threadblock/mma_base.h"
+
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace threadblock {
 
 ////////////////////////////////////////////////////////////////////////////////
 
 /// Structure to compute the matrix product targeting CUDA cores and SIMT math
 /// instructions.
 template <
     /// Size of the Gemm problem - concept: gemm::GemmShape<>
     typename Shape_,
     /// Policy describing tuning details (concept: MmaPolicy)
-    typename Policy_,
+    typename Policy0_,
+    /// B1-specific version of the policy (concept: MmaPolicy)
+    typename Policy1_,
     /// Number of stages,
     int Stages,
     /// Used for partial specialization
     typename Enable = bool>
-class MmaPlanarComplexBase {
+class DualMmaBase {
  public:
   ///< Size of the Gemm problem - concept: gemm::GemmShape<>
   using Shape = Shape_;
 
   ///< Policy describing tuning details
-  using Policy = Policy_;
+  using Policy0 = Policy0_;
+  using Policy1 = Policy1_;
 
   //
   // Dependent types
   //
 
   /// Warp-level Mma
-  using Operator = typename Policy::Operator;
+  using Operator0 = typename Policy0::Operator;
+  using Operator1 = typename Policy1::Operator;
 
   /// Shape describing the overall GEMM computed from shared memory
   /// by each warp.
-  using WarpGemm = typename Policy::Operator::Shape;
+  using WarpGemm = typename Policy0::Operator::Shape;
 
   /// Shape describing the number of warps filling the CTA
   using WarpCount = GemmShape<Shape::kM / WarpGemm::kM,
                               Shape::kN / WarpGemm::kN,
                               Shape::kK / WarpGemm::kK>;
 
   /// Number of warp-level GEMM oeprations
   static int const kWarpGemmIterations =
-      (WarpGemm::kK / Operator::Policy::MmaShape::kK);
+      (WarpGemm::kK / Operator0::Policy::MmaShape::kK);
 
   /// Number of stages
   static int const kStages = Stages;
 
   /// Tensor reference to the A operand
-  using TensorRefA = TensorRef<typename Operator::ElementA, typename Operator::LayoutA>;
+  using TensorRefA = TensorRef<typename Operator0::ElementA, typename Operator0::LayoutA>;
 
   /// Tensor reference to the B operand
-  using TensorRefB = TensorRef<typename Operator::ElementB, typename Operator::LayoutB>;
+  using TensorRefB0 = TensorRef<typename Operator0::ElementB, typename Operator0::LayoutB>;
+  using TensorRefB1 = TensorRef<typename Operator1::ElementB, typename Operator1::LayoutB>;
+
+  static_assert(kWarpGemmIterations > 1,
+                "The pipelined structure requires at least two warp-level "
+                "GEMM operations.");
+
+  static_assert((kWarpGemmIterations % 2) == 0,
+                "Inner loop iteration must be an even number.");
 
   //
   // Nested structs
   //
 
   /// Shared storage object needed by threadblock-scoped GEMM
   class SharedStorage {
    public:
     //
     // Type definitions
     //
 
     /// Shape of the A matrix operand in shared memory
-    using ShapeA = MatrixShape<Shape::kM + Policy::SmemPaddingA::kRow,
+    using ShapeA = MatrixShape<Shape::kM + Policy0::SmemPaddingA::kRow,
                                Shape::kK * kStages +
-                                   Policy::SmemPaddingA::kColumn>;
-
-    /// Stride to the imaginary part of the A operand
-    static int const kImaginaryStrideA = ShapeA::kCount;
+                                   Policy0::SmemPaddingA::kColumn>;
 
     /// Shape of the B matrix operand in shared memory
-    using ShapeB =
-        MatrixShape<Shape::kK * kStages + Policy::SmemPaddingB::kRow,
-                    Shape::kN + Policy::SmemPaddingB::kColumn>;
-
-    /// Stride to the imaginary part of the A operand
-    static int const kImaginaryStrideB = ShapeB::kCount;
+    using ShapeB0 =
+        MatrixShape<Shape::kK * kStages + Policy0::SmemPaddingB::kRow,
+                    Shape::kN + Policy0::SmemPaddingB::kColumn>;
+    using ShapeB1 =
+        MatrixShape<Shape::kK * kStages + Policy1::SmemPaddingB::kRow,
+                    Shape::kN + Policy1::SmemPaddingB::kColumn>;
 
    public:
     //
     // Data members
     //
 
     /// Buffer for A operand
-    AlignedBuffer<typename Operator::ElementA, ShapeA::kCount + kImaginaryStrideA> operand_A;
+    AlignedBuffer<typename Operator0::ElementA, ShapeA::kCount> operand_A;
 
     /// Buffer for B operand
-    AlignedBuffer<typename Operator::ElementB, ShapeB::kCount + kImaginaryStrideB> operand_B;
+    AlignedBuffer<typename Operator0::ElementB, ShapeB0::kCount> operand_B0;
+    AlignedBuffer<typename Operator1::ElementB, ShapeB1::kCount> operand_B1;
 
    public:
 
     //
     // Methods
     //
 
     /// Returns a layout object for the A matrix
     CUTLASS_DEVICE
-    static typename Operator::LayoutA LayoutA() {
-      return Operator::LayoutA::packed({ShapeA::kRow, ShapeA::kColumn});
+    static typename Operator0::LayoutA LayoutA() {
+      return Operator0::LayoutA::packed({ShapeA::kRow, ShapeA::kColumn});
     }
 
     /// Returns a layout object for the B matrix
     CUTLASS_HOST_DEVICE
-    static typename Operator::LayoutB LayoutB() {
-      return Operator::LayoutB::packed({ShapeB::kRow, ShapeB::kColumn});
+    static typename Operator0::LayoutB LayoutB0() {
+      return Operator0::LayoutB::packed({ShapeB0::kRow, ShapeB0::kColumn});
+    }
+
+    /// Returns a layout object for the B matrix
+    CUTLASS_HOST_DEVICE
+    static typename Operator1::LayoutB LayoutB1() {
+      return Operator1::LayoutB::packed({ShapeB1::kRow, ShapeB1::kColumn});
     }
 
     /// Returns a TensorRef to the A operand
     CUTLASS_HOST_DEVICE
     TensorRefA operand_A_ref() {
       return TensorRefA{operand_A.data(), LayoutA()};
     }
 
     /// Returns a TensorRef to the B operand
     CUTLASS_HOST_DEVICE
-    TensorRefB operand_B_ref() {
-      return TensorRefB{operand_B.data(), LayoutB()};
+    TensorRefB0 operand_B0_ref() {
+      return TensorRefB0{operand_B0.data(), LayoutB0()};
+    }
+    CUTLASS_HOST_DEVICE
+    TensorRefB1 operand_B1_ref() {
+      return TensorRefB1{operand_B1.data(), LayoutB1()};
     }
   };
 
  protected:
 
   //
   // Data members
   //
 
   /// Iterator to load a warp-scoped tile of A operand from shared memory
-  typename Operator::IteratorA warp_tile_iterator_A_;
+  typename Operator0::IteratorA warp_tile_iterator_A_;
 
   /// Iterator to load a warp-scoped tile of B operand from shared memory
-  typename Operator::IteratorB warp_tile_iterator_B_;
+  typename Operator0::IteratorB warp_tile_iterator_B0_;
+  typename Operator1::IteratorB warp_tile_iterator_B1_;
 
 public:
 
   /// Construct from tensor references
   CUTLASS_DEVICE
-  MmaPlanarComplexBase(
+  DualMmaBase(
       ///< Shared storage needed for internal use by threadblock-scoped GEMM
       SharedStorage &shared_storage,
       ///< ID within the threadblock
       int thread_idx,
       ///< ID of warp
       int warp_idx,
       ///< ID of each thread within a warp
       int lane_idx
     ):
       warp_tile_iterator_A_(shared_storage.operand_A_ref(), lane_idx),
-      warp_tile_iterator_B_(shared_storage.operand_B_ref(), lane_idx) {
+      warp_tile_iterator_B0_(shared_storage.operand_B0_ref(), lane_idx),
+      warp_tile_iterator_B1_(shared_storage.operand_B1_ref(), lane_idx) {
 
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 }  // namespace threadblock
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_multistage.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_pipelined.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_pipelined.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_singlestage.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/mma_singlestage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_softmax_mainloop_fusion_multistage.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/mma_softmax_mainloop_fusion_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_sparse_base.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/mma_sparse_base.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_sparse_multistage.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/mma_sparse_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_with_reduction_multistage.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/mma_with_reduction_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle_streamk.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle_streamk.h`

 * *Files 0% similar despite different names*

```diff
@@ -633,34 +633,27 @@
 // Guards needed for PyCUTLASS library generation
 #if defined(__NVCC__) || (defined(__clang__) && defined(__CUDA__)) || defined(__CUDACC_RTC__)
 
   //
   // Device-side interface
   //
 
-  /// Proves to the compiler that val is warp-uniform
-  CUTLASS_DEVICE
-  int uniform(int val) const
-  {
-    return __shfl_sync(0xffffffff, val, 0);
-  }
-
   /// Obtains number of threadblocks per GEMM
   CUTLASS_DEVICE
   int device_num_blocks() const
   {
     return gridDim.x;
   }
 
   /// Obtains tile index for the given sk iteration
   CUTLASS_DEVICE
   int get_sk_tile_idx(int iter) const
   {
     int tile_idx = div_mod_iters_per_tile.div(iter);
-    return uniform(tile_idx);
+    return tile_idx;
   }
 
   /// Obtains the batch index
   CUTLASS_DEVICE
   int get_batch_idx() const
   {
     return RematerializeBlockIdxZ();
@@ -730,15 +723,15 @@
     {
       int block_in_region;
       int region;
       div_mod_sk_regions(block_in_region, region, block_idx);
       block_idx = (region * sk_blocks_per_region()) + block_in_region;
     }
 
-    return uniform(block_idx);
+    return block_idx;
   }
 
 
   /// Obtains calling linear threadblock index of the first block to work on the given tile
   CUTLASS_DEVICE
   int get_sk_block_idx(int iter) const
   {
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_complex_tensor_op.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/default_mma_complex_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_sparse_tensor_op.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/default_mma_sparse_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op_sm80.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op_sm80.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_with_reduction_tensor_op.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/default_mma_with_reduction_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_wmma_tensor_op.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/default_mma_wmma_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/layernorm_scale_bias_transform.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/layernorm_scale_bias_transform.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_fast_f32.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_fast_f32.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_tile_iterator_sm80.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_tile_iterator_sm80.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op_tile_iterator_sm80.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op_tile_iterator_sm80.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_planar_complex.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_planar_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_tensor_op_sm70.h`

 * *Files 19% similar despite different names*

```diff
@@ -25,30 +25,35 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Templates implementing warp-level matrix multiply-accumulate operations.
+    \brief Templates implementing warp-level matrix multiply-accumulate operations targeting
+      Tensor Cores.
+
+    This is a work in progress.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/array.h"
+
 #include "cutlass/numeric_types.h"
 #include "cutlass/matrix_shape.h"
+
+#include "cutlass/arch/mma.h"
+
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/gemm/warp/mma.h"
 
-#include "cutlass/gemm/thread/mma.h"
-
-#include "cutlass/gemm/warp/mma_simt_tile_iterator.h"
-#include "cutlass/gemm/warp/mma_simt_policy.h"
+#include "cutlass/gemm/warp/mma_tensor_op_policy.h"
+#include "cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm70.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace warp {
 
@@ -66,26 +71,20 @@
   typename ElementB_,
   /// Layout of B matrix (concept: MatrixLayout)
   typename LayoutB_,
   /// Element type of C matrix
   typename ElementC_,
   /// Layout of C matrix (concept: MatrixLayout)
   typename LayoutC_,
-  /// Shape of the warp in units of thread (concept: MmaSimtPolicy)
+  /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
   typename Policy_,
-  /// Number of partitions along K dimension
-  int PartitionsK = 1,
-  /// Complex transformation on operand A
-  ComplexTransform TransformA = ComplexTransform::kNone,
-  /// Complex transformation on operand B
-  ComplexTransform TransformB = ComplexTransform::kNone,
   /// Used for partial specialization
   typename Enable = bool
 >
-class MmaSimt {
+class MmaVoltaTensorOp {
 public:
   /// Shape of warp-level matrix operation (concept: GemmShape)
   using Shape = Shape_;
 
   /// Data type of multiplicand A
   using ElementA = ElementA_;
 
@@ -104,160 +103,177 @@
   /// Layout of accumulator matrix C
   using LayoutC = LayoutC_;
 
   /// Shape of the warp in units of thread (concept: MmaLanePolicySimt)
   using Policy = Policy_;
 
   /// Indicates class of matrix operator
-  using OperatorClass = arch::OpClassSimt;
+  using OperatorClass = arch::OpClassTensorOp;
 
-  /// Hard-coded for now
-  using ArchTag = arch::Sm50;
-
-  /// Complex transform on A operand
-  static ComplexTransform const kTransformA = TransformA;
-
-  /// Complex transform on B operand
-  static ComplexTransform const kTransformB = TransformB;
-
-  /// Layout of threads
-  using ThreadLayoutA = typename platform::conditional< platform::is_same< layout::ColumnMajorInterleaved<4>, LayoutA >::value,
-                  layout::ColumnMajor,
-                  typename platform::conditional < platform::is_same< layout::RowMajorInterleaved<4>, LayoutA >::value,
-                      layout::RowMajor,
-                      LayoutA>::type
-                 >::type;
-  
-  using ThreadLayoutB = typename platform::conditional< platform::is_same< layout::ColumnMajorInterleaved<4>, LayoutB >::value,
-                  layout::ColumnMajor,
-                  typename platform::conditional < platform::is_same< layout::RowMajorInterleaved<4>, LayoutB >::value,
-                      layout::RowMajor,
-                      LayoutB>::type
-                 >::type;
-
-  static constexpr bool use_dp4a = (platform::is_same< layout::ColumnMajorInterleaved<4>, LayoutA>::value || 
-                                    platform::is_same< layout::RowMajorInterleaved<4>, LayoutA >::value) && 
-                                    platform::is_same< ElementA, int8_t >::value && 
-                                    platform::is_same< ElementB, int8_t >::value;
-
-  using dp4a_type = typename platform::conditional< use_dp4a , int8_t, bool >::type;
-
-  /// Thread-level matrix multiply accumulate operator
-  using ThreadMma = thread::Mma<
-    GemmShape<
-      Shape::kM / Policy::WarpShape::kRow,
-      Shape::kN / Policy::WarpShape::kColumn,
-      Policy::LaneMmaShape::kK>,
-    ElementA,
-    ThreadLayoutA,
-    ElementB,
-    ThreadLayoutB,
-    ElementC,
-    LayoutC,
-    arch::OpMultiplyAdd,
-    dp4a_type
-  >;
+  /// Architecture tag
+  using ArchTag = arch::Sm70;
 
   /// Underlying matrix multiply operator (concept: arch::Mma)
-  using ArchMmaOperator = typename ThreadMma::ArchMmaOperator;
+  using ArchMmaOperator = typename Policy::Operator;
 
   /// Indicates math operator 
   using MathOperator = typename ArchMmaOperator::Operator;
   
-  /// Shape of the underlying instruction
-  using InstructionShape = GemmShape<1,1,use_dp4a ? 4 : 1>;
+  /// Underlying instruction shape
+  using InstructionShape = typename ArchMmaOperator::Shape;
+
+  /// Complex transform on A operand
+  static ComplexTransform const kTransformA = ComplexTransform::kNone;
+
+  /// Complex transform on B operand
+  static ComplexTransform const kTransformB = ComplexTransform::kNone;
+
+  /// Number of threads participating in warp-level matrix product
+  static int const kThreadCount = 32;
 
+  /// interleaved 32x32 tiles
+  using InterleavedTileShape = GemmShape<32, 32, 4>;
+
+  static_assert(!(Shape::kM % InterleavedTileShape::kM) &&
+                !(Shape::kN % InterleavedTileShape::kN),
+                "Shape must be a multiple of InterleavedTileShape.");
 public:
 
   /// Iterates over the A operand in memory
-  using IteratorA = MmaSimtTileIterator<
-    MatrixShape<Shape::kM, Policy::LaneMmaShape::kK>,
+  using IteratorA = MmaVoltaTensorOpMultiplicandTileIterator<
+    MatrixShape<Shape::kM, Shape::kK>,
     Operand::kA,
     ElementA,
     LayoutA,
-    Policy,
-    PartitionsK,
-    Shape::kK
+    MatrixShape<
+      ArchMmaOperator::Shape::kM,
+      ArchMmaOperator::Shape::kK
+    >,
+    Policy::OpDelta::kRow,
+    kThreadCount
   >;
 
   /// Storage for A tile
   using FragmentA = typename IteratorA::Fragment;
 
-  /// Storage for transformed A tile
-  using TransformedFragmentA = FragmentA;
-
   /// Iterates over the B operand in memory
-  using IteratorB = MmaSimtTileIterator<
-    MatrixShape<Policy::LaneMmaShape::kK, Shape::kN>,
+  using IteratorB = MmaVoltaTensorOpMultiplicandTileIterator<
+    MatrixShape<Shape::kK, Shape::kN>,
     Operand::kB,
     ElementB,
     LayoutB,
-    Policy,
-    PartitionsK,
-    Shape::kK
+    MatrixShape<
+      ArchMmaOperator::Shape::kK,
+      ArchMmaOperator::Shape::kN
+    >,
+    Policy::OpDelta::kRow,
+    kThreadCount
   >;
 
   /// Storage for B tile
   using FragmentB = typename IteratorB::Fragment;
 
-  /// Storage for transformed A tile
-  using TransformedFragmentB = FragmentB;
-
   /// Iterates over the C operand in memory
-  using IteratorC = MmaSimtTileIterator<
+  using IteratorC = MmaVoltaTensorOpAccumulatorTileIterator<
     MatrixShape<Shape::kM, Shape::kN>,
-    Operand::kC,
     ElementC,
     LayoutC,
-    Policy
+    typename ArchMmaOperator::Shape,
+    typename Policy::OpDelta
   >;
 
   /// Storage for C tile
-  using FragmentC = typename ThreadMma::FragmentC;
+  using FragmentC = typename IteratorC::Fragment;
+
+private:
+
+  static_assert(
+    !(Shape::kM % ArchMmaOperator::Shape::kM) && 
+    !(Shape::kN % ArchMmaOperator::Shape::kN),
+    "Shape of warp-level Mma must be divisible by operator shape.");
+
+  /// Number of mma operations performed
+  using MmaIterations = MatrixShape<
+    InterleavedTileShape::kM / ArchMmaOperator::Shape::kM,
+    InterleavedTileShape::kN / ArchMmaOperator::Shape::kN
+  >;
+  using TileIterations = MatrixShape<
+    Shape::kM / InterleavedTileShape::kM,
+    Shape::kN / InterleavedTileShape::kN
+  >;
+
+  // Whether matrix B is reordered
+  bool reorder_B_;
+
+public:
+
+  /// Underlying matrix multiply operator (concept: arch::Mma)
+  ArchMmaOperator mma;
 
 public:
 
   //
   // Methods
   //
-
+  
   /// Ctor
   CUTLASS_DEVICE
-  MmaSimt() {}
+  MmaVoltaTensorOp() {}
 
   /// Performs a warp-level matrix multiply-accumulate operation
   CUTLASS_DEVICE
   void operator()(
-    FragmentC &d, 
-    FragmentA a, 
-    FragmentB b, 
-    FragmentC const &c, int group_idx = 0) const {
-
-    ThreadMma mma;
-
-    if (kTransformA == ComplexTransform::kConjugate) {
-      conjugate<FragmentA> conj_a;
-      a = conj_a(a);
-    }
-
-    if (kTransformB == ComplexTransform::kConjugate) {
-      conjugate<FragmentB> conj_b;
-      b = conj_b(b);
+    FragmentC &D, 
+    FragmentA const &A, 
+    FragmentB const &B, 
+    FragmentC const &C)  {
+
+    using MmaOperandA = typename ArchMmaOperator::FragmentA;
+    using MmaOperandB = typename ArchMmaOperator::FragmentB;
+    using MmaOperandC = typename ArchMmaOperator::FragmentC;
+
+    D = C;
+
+    MmaOperandA const *ptr_A = reinterpret_cast<MmaOperandA const *>(&A);
+    MmaOperandB const *ptr_B = reinterpret_cast<MmaOperandB const *>(&B);
+    MmaOperandC *ptr_D = reinterpret_cast<MmaOperandC *>(&D);
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int outer_col = 0; outer_col < TileIterations::kColumn; ++outer_col) {
+      CUTLASS_PRAGMA_UNROLL
+      for (int inner_col = 0; inner_col < MmaIterations::kColumn; ++inner_col) {
+        CUTLASS_PRAGMA_UNROLL
+        for (int outer_row = 0; outer_row < TileIterations::kRow; ++outer_row) {
+          CUTLASS_PRAGMA_UNROLL
+
+          for (int inner_row = 0; inner_row < MmaIterations::kRow; ++inner_row) {
+      
+            int op_col = inner_col + MmaIterations::kColumn * outer_col;
+
+            // Column-major serpentine sequence to maximize reuse of A operand.
+            int inner_row_serp = inner_row;
+            int outer_row_serp = outer_row;
+            if (op_col & 1) {
+              inner_row_serp = MmaIterations::kRow - inner_row - 1;
+              outer_row_serp = TileIterations::kRow - outer_row - 1;
+            }
+            int op_row = inner_row_serp + MmaIterations::kRow * outer_row_serp;
+            int op_idx = inner_row_serp + MmaIterations::kRow * 
+                         (inner_col + MmaIterations::kColumn * 
+                          (outer_row_serp + TileIterations::kRow * outer_col));
+            mma(
+              ptr_D[op_idx],
+              ptr_A[op_row],
+              ptr_B[op_col],
+              ptr_D[op_idx]);
+
+          }
+        }
+      }
     }
-
-    mma(d, a, b, c);
-  }
-
-  /// Transform the mma operands to the required types
-  CUTLASS_DEVICE
-  void transform(TransformedFragmentA &dst_A, TransformedFragmentB &dst_B,
-                 FragmentA const &A, FragmentB const &B) const {
-    //TODO: Implement this
-    dst_A = A;
-    dst_B = B;
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace warp
 } // namespace gemm
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt_policy.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_simt_policy.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt_tile_iterator.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_simt_tile_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_sparse_tensor_op.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_sparse_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_fast_f32.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_tensor_op_fast_f32.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_fragment_iterator.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_tensor_op_fragment_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_policy.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_tensor_op_policy.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_sm70.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_tensor_op_wmma.h`

 * *Files 20% similar despite different names*

```diff
@@ -27,254 +27,197 @@
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
     \brief Templates implementing warp-level matrix multiply-accumulate operations targeting
       Tensor Cores.
-
-    This is a work in progress.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
-#include "cutlass/array.h"
+#include "cutlass/arch/wmma.h"
+
+#if defined(CUTLASS_ARCH_WMMA_ENABLED)
 
+#include "cutlass/wmma_array.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/matrix_shape.h"
 
-#include "cutlass/arch/mma.h"
+#include "cutlass/arch/memory_sm75.h"
+#include "cutlass/arch/mma_sm75.h"
+#include "cutlass/arch/mma_sm80.h"
 
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/gemm/warp/mma.h"
 
 #include "cutlass/gemm/warp/mma_tensor_op_policy.h"
-#include "cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm70.h"
+
+#include "cutlass/gemm/warp/mma_tensor_op_tile_iterator_wmma.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace warp {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Structure to compute the matrix product targeting CUDA cores and SIMT math instructions.
-template <
-  /// Size of the Gemm problem - concept: gemm::GemmShape<>
+///< Structure to compute the matrix product targeting CUDA cores via WMMA.
+template < 
+  ///< Size of the Gemm problem - concept: gemm::GemmShape<>
   typename Shape_,
-  /// Data type of A elements
+  ///< Data type of A elements
   typename ElementA_,
-  /// Layout of A matrix (concept: MatrixLayout)
+  ///< Layout of A matrix (concept: MatrixLayout)
   typename LayoutA_,
-  /// Data type of B elements
+  ///< Data type of B elements
   typename ElementB_,
   /// Layout of B matrix (concept: MatrixLayout)
   typename LayoutB_,
-  /// Element type of C matrix
+  ///< Element type of C matrix
   typename ElementC_,
-  /// Layout of C matrix (concept: MatrixLayout)
+  ///< Layout of C matrix (concept: MatrixLayout)
   typename LayoutC_,
-  /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
+  ///< Policy describing warp-level Wmma operation (concept: MmaTensorOpPolicy)
   typename Policy_,
-  /// Used for partial specialization
+  ///< Number of partitions along K dimension
+  int PartitionsK_ = 1,
+  ///< Used for partial specialization
   typename Enable = bool
 >
-class MmaVoltaTensorOp {
+class MmaTensorOpWmma {
 public:
-  /// Shape of warp-level matrix operation (concept: GemmShape)
+  ///< Shape of warp-level matrix operation (concept: GemmShape)
   using Shape = Shape_;
 
-  /// Data type of multiplicand A
+  ///< Data type of multiplicand A
   using ElementA = ElementA_;
 
-  /// Layout of multiplicand A
+  ///< Layout of multiplicand A
   using LayoutA = LayoutA_;
 
-  /// Data type of multiplicand B
+  ///< Data type of multiplicand B
   using ElementB = ElementB_;
 
-  /// Layout of multiplicand B
+  ///< Layout of multiplicand B
   using LayoutB = LayoutB_;
 
-  /// Data type of accumulator matrix C
+  ///< Data type of accumulator matrix C
   using ElementC = ElementC_;
 
-  /// Layout of accumulator matrix C
+  ///< Layout of accumulator matrix C
   using LayoutC = LayoutC_;
 
-  /// Shape of the warp in units of thread (concept: MmaLanePolicySimt)
+  /// Shape of the warp in units of thread (concept: MmaTensorOpPolicy)
   using Policy = Policy_;
 
-  /// Indicates class of matrix operator
-  using OperatorClass = arch::OpClassTensorOp;
-
-  /// Architecture tag
-  using ArchTag = arch::Sm70;
+  /// Underlying instruction shape
+  using InstructionShape = typename Policy::Operator::Shape;
 
   /// Underlying matrix multiply operator (concept: arch::Mma)
   using ArchMmaOperator = typename Policy::Operator;
 
   /// Indicates math operator 
   using MathOperator = typename ArchMmaOperator::Operator;
   
-  /// Underlying instruction shape
-  using InstructionShape = typename ArchMmaOperator::Shape;
+  /// Underlying architecture tag
+  using ArchTag = typename Policy::Operator::ArchTag;
 
   /// Complex transform on A operand
   static ComplexTransform const kTransformA = ComplexTransform::kNone;
 
   /// Complex transform on B operand
   static ComplexTransform const kTransformB = ComplexTransform::kNone;
 
+  /// Indicates class of matrix operator
+  using OperatorClass = arch::OpClassWmmaTensorOp;
+
   /// Number of threads participating in warp-level matrix product
   static int const kThreadCount = 32;
 
-  /// interleaved 32x32 tiles
-  using InterleavedTileShape = GemmShape<32, 32, 4>;
+  /// Number of partitions along K dimension
+  static int const kPartitionsK = PartitionsK_;
 
-  static_assert(!(Shape::kM % InterleavedTileShape::kM) &&
-                !(Shape::kN % InterleavedTileShape::kN),
-                "Shape must be a multiple of InterleavedTileShape.");
 public:
 
   /// Iterates over the A operand in memory
-  using IteratorA = MmaVoltaTensorOpMultiplicandTileIterator<
-    MatrixShape<Shape::kM, Shape::kK>,
-    Operand::kA,
-    ElementA,
-    LayoutA,
-    MatrixShape<
-      ArchMmaOperator::Shape::kM,
-      ArchMmaOperator::Shape::kK
-    >,
-    Policy::OpDelta::kRow,
-    kThreadCount
-  >;
+  using IteratorA = MmaTensorOpWmmaMultiplicandTileIterator<
+     MatrixShape<Shape::kM, Shape::kK>, Operand::kA, ElementA, LayoutA,
+     Policy::OpDelta::kRow, kThreadCount, Policy>;
 
   /// Storage for A tile
   using FragmentA = typename IteratorA::Fragment;
 
   /// Iterates over the B operand in memory
-  using IteratorB = MmaVoltaTensorOpMultiplicandTileIterator<
-    MatrixShape<Shape::kK, Shape::kN>,
-    Operand::kB,
-    ElementB,
-    LayoutB,
-    MatrixShape<
-      ArchMmaOperator::Shape::kK,
-      ArchMmaOperator::Shape::kN
-    >,
-    Policy::OpDelta::kRow,
-    kThreadCount
-  >;
+  using IteratorB = MmaTensorOpWmmaMultiplicandTileIterator<
+     MatrixShape<Shape::kK, Shape::kN>, Operand::kB, ElementB, LayoutB,
+     Policy::OpDelta::kRow, kThreadCount, Policy>;
 
   /// Storage for B tile
   using FragmentB = typename IteratorB::Fragment;
 
   /// Iterates over the C operand in memory
-  using IteratorC = MmaVoltaTensorOpAccumulatorTileIterator<
-    MatrixShape<Shape::kM, Shape::kN>,
-    ElementC,
-    LayoutC,
-    typename ArchMmaOperator::Shape,
-    typename Policy::OpDelta
-  >;
+  using IteratorC = MmaTensorOpWmmaAccumulatorTileIterator<
+     MatrixShape<Shape::kM, Shape::kN>, ElementC, LayoutC,
+    typename Policy::OpDelta, Policy>;
 
   /// Storage for C tile
   using FragmentC = typename IteratorC::Fragment;
 
 private:
 
   static_assert(
-    !(Shape::kM % ArchMmaOperator::Shape::kM) && 
-    !(Shape::kN % ArchMmaOperator::Shape::kN),
-    "Shape of warp-level Mma must be divisible by operator shape.");
-
-  /// Number of mma operations performed
-  using MmaIterations = MatrixShape<
-    InterleavedTileShape::kM / ArchMmaOperator::Shape::kM,
-    InterleavedTileShape::kN / ArchMmaOperator::Shape::kN
-  >;
-  using TileIterations = MatrixShape<
-    Shape::kM / InterleavedTileShape::kM,
-    Shape::kN / InterleavedTileShape::kN
+    !(Shape::kM % Policy::Operator::Shape::kM) && 
+    !(Shape::kN % Policy::Operator::Shape::kN),
+    "Shape of warp-level Wmma must be divisible by operator shape (wmma native size)");
+
+  /// Number of wmma operations performed
+  using WmmaIterations = MatrixShape<
+    Shape::kM / Policy::Operator::Shape::kM,
+    Shape::kN / Policy::Operator::Shape::kN 
   >;
 
-  // Whether matrix B is reordered
-  bool reorder_B_;
-
 public:
 
-  /// Underlying matrix multiply operator (concept: arch::Mma)
-  ArchMmaOperator mma;
+  /// Underlying matrix multiply operator (concept: cutlass::arch::Wmma)
+  typename Policy::Operator wmma;
 
 public:
 
   //
   // Methods
   //
-  
+
   /// Ctor
   CUTLASS_DEVICE
-  MmaVoltaTensorOp() {}
+  MmaTensorOpWmma() {}
 
   /// Performs a warp-level matrix multiply-accumulate operation
   CUTLASS_DEVICE
   void operator()(
     FragmentC &D, 
     FragmentA const &A, 
     FragmentB const &B, 
-    FragmentC const &C)  {
-
-    using MmaOperandA = typename ArchMmaOperator::FragmentA;
-    using MmaOperandB = typename ArchMmaOperator::FragmentB;
-    using MmaOperandC = typename ArchMmaOperator::FragmentC;
-
-    D = C;
-
-    MmaOperandA const *ptr_A = reinterpret_cast<MmaOperandA const *>(&A);
-    MmaOperandB const *ptr_B = reinterpret_cast<MmaOperandB const *>(&B);
-    MmaOperandC *ptr_D = reinterpret_cast<MmaOperandC *>(&D);
+    FragmentC const &C) const {
 
     CUTLASS_PRAGMA_UNROLL
-    for (int outer_col = 0; outer_col < TileIterations::kColumn; ++outer_col) {
+    for (int n = 0; n < WmmaIterations::kColumn; ++n) {
       CUTLASS_PRAGMA_UNROLL
-      for (int inner_col = 0; inner_col < MmaIterations::kColumn; ++inner_col) {
-        CUTLASS_PRAGMA_UNROLL
-        for (int outer_row = 0; outer_row < TileIterations::kRow; ++outer_row) {
-          CUTLASS_PRAGMA_UNROLL
-
-          for (int inner_row = 0; inner_row < MmaIterations::kRow; ++inner_row) {
-      
-            int op_col = inner_col + MmaIterations::kColumn * outer_col;
-
-            // Column-major serpentine sequence to maximize reuse of A operand.
-            int inner_row_serp = inner_row;
-            int outer_row_serp = outer_row;
-            if (op_col & 1) {
-              inner_row_serp = MmaIterations::kRow - inner_row - 1;
-              outer_row_serp = TileIterations::kRow - outer_row - 1;
-            }
-            int op_row = inner_row_serp + MmaIterations::kRow * outer_row_serp;
-            int op_idx = inner_row_serp + MmaIterations::kRow * 
-                         (inner_col + MmaIterations::kColumn * 
-                          (outer_row_serp + TileIterations::kRow * outer_col));
-            mma(
-              ptr_D[op_idx],
-              ptr_A[op_row],
-              ptr_B[op_col],
-              ptr_D[op_idx]);
+      for (int m = 0; m < WmmaIterations::kRow; ++m) {
 
-          }
-        }
+        // accumulate wmma mma
+        wmma(D[m * WmmaIterations::kColumn + n], A[m], B[n], C[m * WmmaIterations::kColumn + n]);
       }
-    }
+    }  
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace warp
 } // namespace gemm
 } // namespace cutlass
+
+#endif // if defined(CUTLASS_ARCH_WMMA_ENABLED)
+
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_access_iterator.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_access_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm70.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm70.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm80.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm80.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sparse.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sparse.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_wmma.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_wmma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_with_reduction_tensor_op.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/mma_with_reduction_tensor_op.h`

 * *Files 1% similar despite different names*

```diff
@@ -40,15 +40,15 @@
 #include "cutlass/platform/platform.h"
 
 #include "cutlass/numeric_conversion.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/matrix_shape.h"
 
 #include "cutlass/arch/memory_sm75.h"
-#include "cutlass/arch/mma_sm75.h" 
+#include "cutlass/arch/mma_sm75.h"
 #include "cutlass/arch/mma_sm80.h"
 
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/gemm/warp/mma.h"
 
 #include "cutlass/gemm/warp/mma_tensor_op_policy.h"
 #include "cutlass/gemm/warp/mma_tensor_op.h"
@@ -116,17 +116,17 @@
 
   /// Shape of the warp in units of thread (concept: MmaLanePolicySimt)
   using Policy = Policy_;
 
   /// Underlying matrix multiply operator (concept: arch::Mma)
   using ArchMmaOperator = typename Policy::Operator;
 
-  /// Indicates math operator 
+  /// Indicates math operator
   using MathOperator = typename ArchMmaOperator::Operator;
-  
+
   /// Architecture tag from underlying instruction
   using ArchTag = typename ArchMmaOperator::ArchTag;
 
   /// Indicates class of matrix operator
   using OperatorClass = arch::OpClassTensorOp;
 
   /// Shape of underlying instruction
@@ -219,30 +219,30 @@
   /// Ctor
   CUTLASS_DEVICE
   MmaWithReductionTensorOp() {}
 
   /// Performs a warp-level matrix multiply-accumulate operation
   CUTLASS_DEVICE
   void operator()(
-    FragmentC &D, 
-    TransformedFragmentA const &A, 
-    TransformedFragmentB const &B, 
+    FragmentC &D,
+    TransformedFragmentA const &A,
+    TransformedFragmentB const &B,
     FragmentC const &C,
     FragmentReduction &gemm_k_reduction
   ) const {
 
     using MmaOperandA = typename ArchMmaOperator::FragmentA;
     using MmaOperandB = typename ArchMmaOperator::FragmentB;
     using MmaOperandC = typename ArchMmaOperator::FragmentC;
 
     D = C;
 
-    MmaOperandA const *ptr_A = reinterpret_cast<MmaOperandA const *>(&A);
-    MmaOperandB const *ptr_B = reinterpret_cast<MmaOperandB const *>(&B);
-    MmaOperandC *ptr_D = reinterpret_cast<MmaOperandC *>(&D);
+    [[maybe_unused]] MmaOperandA const *ptr_A = reinterpret_cast<MmaOperandA const *>(&A);
+    [[maybe_unused]] MmaOperandB const *ptr_B = reinterpret_cast<MmaOperandB const *>(&B);
+    [[maybe_unused]] MmaOperandC *ptr_D = reinterpret_cast<MmaOperandC *>(&D);
 
     #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 800)
       assert(0);
     #elif defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
       // Serpentine visitation order maximizing reuse of Ra
       CUTLASS_PRAGMA_UNROLL
       for (int m = 0; m < MmaIterations::kRow; ++m) {
@@ -254,15 +254,15 @@
 
           mma(ptr_D[m + n_serpentine * MmaIterations::kRow],
               ptr_A[m],
               ptr_B[n_serpentine],
               ptr_D[m + n_serpentine * MmaIterations::kRow]);
 
           if (!kReduceKForA && m == 0) {
-            #if 0 
+            #if 0
             gemm_k_reduction[n_serpentine] += float(B[n_serpentine * 4]);
             gemm_k_reduction[n_serpentine] += float(B[n_serpentine * 4 + 1]);
             gemm_k_reduction[n_serpentine] += float(B[n_serpentine * 4 + 2]);
             gemm_k_reduction[n_serpentine] += float(B[n_serpentine * 4 + 3]);
             #else
             uint32_t const *tmp = reinterpret_cast<uint32_t const *>(&B);
 
@@ -302,20 +302,20 @@
             } else {
                 assert(0);
             }
             #endif
           }
 
           if (kReduceKForA && (n == 0)) {
-            #if 0 
+            #if 0
             gemm_k_reduction[m * 2] += float(A[m * 8]);
             gemm_k_reduction[m * 2] += float(A[m * 8 + 1]);
             gemm_k_reduction[m * 2] += float(A[m * 8 + 4]);
             gemm_k_reduction[m * 2] += float(A[m * 8 + 5]);
-  
+
             gemm_k_reduction[m * 2 + 1] += float(A[m * 8 + 2]);
             gemm_k_reduction[m * 2 + 1] += float(A[m * 8 + 3]);
             gemm_k_reduction[m * 2 + 1] += float(A[m * 8 + 6]);
             gemm_k_reduction[m * 2 + 1] += float(A[m * 8 + 7]);
             #else
             uint32_t const *tmp = reinterpret_cast<uint32_t const *>(&A);
 
@@ -407,17 +407,17 @@
                             FragmentB::kElements / 2, kRoundB>
           convert_B;
       Array<ElementB, FragmentB::kElements / 2> const *ptr_B =
           reinterpret_cast<Array<ElementB, FragmentB::kElements / 2> const *>(&B);
       Array<typename ArchMmaOperator::ElementB, FragmentB::kElements / 2> *
           ptr_dst_B = reinterpret_cast<Array<typename ArchMmaOperator::ElementB,
                                              FragmentB::kElements / 2> *>(&dst_B);
-  
+
       dst_A = convert_A(A);
-  
+
       ptr_dst_B[0] = convert_B(ptr_B[0]);
       ptr_dst_B[1] = convert_B(ptr_B[1]);
 
     #elif defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
       detail::ConvertAndPack<typename ArchMmaOperator::ElementA, ElementA,
                             FragmentA::kElements / 2, kRoundA>
           convert_A;
@@ -425,17 +425,17 @@
                             FragmentB::kElements, kRoundB>
           convert_B;
       Array<ElementA, FragmentA::kElements / 2> const *ptr_A =
           reinterpret_cast<Array<ElementA, FragmentA::kElements / 2> const *>(&A);
       Array<typename ArchMmaOperator::ElementA, FragmentA::kElements / 2> *
           ptr_dst_A = reinterpret_cast<Array<typename ArchMmaOperator::ElementA,
                                              FragmentA::kElements / 2> *>(&dst_A);
-  
+
       dst_B = convert_B(B);
-  
+
       ptr_dst_A[0] = convert_A(ptr_A[0]);
       ptr_dst_A[1] = convert_A(ptr_A[1]);
     #else
       assert(0);
     #endif
   }
 };
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/scale_bias_tile_iterator.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/scale_bias_tile_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/softmax_scale_bias_transform.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/softmax_scale_bias_transform.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/tile_iterator_planar_complex.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/gemm/warp/tile_iterator_planar_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/half.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/half.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/integer_subbyte.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/integer_subbyte.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/kernel_launch.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/kernel_launch.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/layout.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/layout/layout.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/matrix.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/layout/matrix.h`

 * *Files 2% similar despite different names*

```diff
@@ -35,14 +35,16 @@
     data to describe strides between elements.
 
     Layout functions must implement all members in the public interface of IdentityTensorLayout<>
     defined in cutlass/tensor_ref.h.
 */
 #pragma once
 
+#include "cute/layout.hpp"
+
 #include "cutlass/cutlass.h"
 #include "cutlass/fast_math.h"
 #include "cutlass/matrix_coord.h"
 #include "cutlass/pitch_linear_coord.h"
 
 namespace cutlass {
 namespace layout {
@@ -139,14 +141,23 @@
   }
 
   /// Compute the number of contiguous elements needed to store a tensor with the given size
   CUTLASS_HOST_DEVICE
   LongIndex capacity(MatrixCoord const &extent) const {
     return LongIndex(extent.row()) * LongIndex(stride_[0]);
   }
+
+  CUTLASS_HOST_DEVICE
+  cute::Layout<cute::Shape<int, int>, cute::Stride<int64_t, cute::Int<1> > > 
+  to_cute_layout(MatrixCoord const &extent) const {
+    return cute::Layout<cute::Shape<int, int>, cute::Stride<int64_t, cute::Int<1> > >{
+      {extent[0], extent[1]},
+      {stride(0), cute::Int<1>{}}
+    };
+  }
 };
 
 /// Mapping function for column-major matrices.
 class ColumnMajor {
 public:
   /// Logical rank of tensor
   static int const kRank = 2;
@@ -232,14 +243,23 @@
   }
 
   /// Compute the number of contiguous elements needed to store a tensor with the given size
   CUTLASS_HOST_DEVICE
   LongIndex capacity(MatrixCoord const &extent) const {
     return LongIndex(extent.column()) * LongIndex(stride_[0]);
   }
+
+  CUTLASS_HOST_DEVICE
+  cute::Layout<cute::Shape<int, int>, cute::Stride< cute::Int<1>, int64_t> > 
+  to_cute_layout(MatrixCoord const &extent) const {
+    return cute::Layout<cute::Shape<int, int>, cute::Stride<cute::Int<1>, int64_t> >{
+      {extent[0], extent[1]},
+      {cute::Int<1>{}, stride(0)}
+    };
+  }
 };
 
 /// Mapping function for interleaved matrices. Matrix is structured
 /// as row-major arrangement of fixed-size columns.
 template <int Interleave>
 struct RowMajorInterleaved {
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/permute.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/layout/permute.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/pitch_linear.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/layout/pitch_linear.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/tensor.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/layout/tensor.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm70.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm70.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm75.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm75.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm80.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm80.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/layout/vector.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/layout/vector.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/matrix.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/matrix.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/matrix_coord.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/matrix_coord.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/matrix_shape.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/matrix_shape.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/numeric_conversion.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/numeric_conversion.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/numeric_types.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/numeric_types.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/pitch_linear_coord.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/pitch_linear_coord.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/platform/platform.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/platform/platform.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/predicate_vector.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/predicate_vector.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/quaternion.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/quaternion.h`

 * *Files 0% similar despite different names*

```diff
@@ -741,14 +741,13 @@
     w += -a.y() * b.y();
     w += -a.z() * b.z();
 
     return cutlass::make_Quaternion(x, y, z, w);
   }
 };
 
-
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/real.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/convolution.h`

 * *Files 15% similar despite different names*

```diff
@@ -24,38 +24,26 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/**
-  \file
-  \brief This class provides helpers to support real<> and complex<> types in generic code.
+/* \file
+   \brief Bind convolution related types to python
 */
-
 #pragma once
+#include <pybind11/pybind11.h>
+#include <pybind11/stl_bind.h>
 
-namespace cutlass {
-
-/// Used to determine the real-valued underlying type of a numeric type T.
-template <typename T>
-struct RealType {
-  using Type = T;
+#include "conv_problems.h"
+#include "host.h"
 
-  /// Number of elements
-  static int const kExtent = 1;
+namespace py = pybind11;
 
-CUTLASS_HOST_DEVICE
-  static T from_real(double x) {
-    return static_cast<T>(x);
-  }
-};
+void bind_convolution_test(py::module &m) {
+    // Conv problem sizes
+    bind_conv_problem_size_test(m);
 
-template <typename T>
-CUTLASS_HOST_DEVICE
-static T from_real(double r) {
-  return T(r);
+    py::module_ host_submodule = m.def_submodule("host");
+    bind_conv_host_references(host_submodule);
 }
-
-
-} // namespace cutlass
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/device/reduce_split_k.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/reduction/device/reduce_split_k.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/reduction/device/tensor_reduce.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_contiguous.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_contiguous.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_strided.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_strided.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/reduce_softmax_final.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/reduction/kernel/reduce_softmax_final.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/reduce_split_k.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/reduction/kernel/reduce_split_k.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_contiguous.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_contiguous.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_strided.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_strided.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/reduce.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/reduction/thread/reduce.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/reduction_operators.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/reduction/thread/reduction_operators.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/reduction/threadblock_swizzle.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/reduction/threadblock_swizzle.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/relatively_equal.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/relatively_equal.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/semaphore.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/semaphore.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/subbyte_reference.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/subbyte_reference.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/tensor_coord.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/tensor_coord.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/tensor_ref.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/tensor_ref.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/tensor_ref_planar_complex.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/tensor_ref_planar_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/tensor_view.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/tensor_view.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/tensor_view_planar_complex.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/tensor_view_planar_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/tfloat32.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/tfloat32.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/thread/matrix.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/thread/matrix.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/trace.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/trace.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/pitch_linear_thread_map.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/pitch_linear_thread_map.h`

 * *Files 1% similar despite different names*

```diff
@@ -25,15 +25,15 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Templates implementing how threads are mapped to a given tile. 
+    \brief Templates implementing how threads are mapped to a given tile.
 
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/array.h"
@@ -159,17 +159,17 @@
   using TensorCoord = layout::PitchLinearCoord;
 
   static int const kThreads = Threads;
   static int const kElementsPerAccess = ElementsPerAccess;
 
   using Iterations = layout::PitchLinearShape<
                       Shape::kContiguous / (kThreads * kElementsPerAccess),
-                      Shape::kStrided>;                      
+                      Shape::kStrided>;
 
-  using Delta = layout::PitchLinearShape<1, 1>;  
+  using Delta = layout::PitchLinearShape<1, 1>;
 
   CUTLASS_HOST_DEVICE
   static TensorCoord initial_offset(int thread_id)
   {
     return TensorCoord(thread_id * Iterations::kContiguous * kElementsPerAccess, 0);
   }
 };
@@ -179,32 +179,32 @@
   int Threads,
   int ElementsPerAccess = 1
 >
 struct PitchLinearTilePolicyStripminedThreadStrided
 {
   static_assert((Shape::kStrided % Threads == 0),
                 "Strided shape must divide number of threads");
-  
+
   using TensorCoord = layout::PitchLinearCoord;
 
   static int const kThreads = Threads;
   static int const kElementsPerAccess = ElementsPerAccess;
 
   using Iterations = layout::PitchLinearShape<
                       Shape::kContiguous / kElementsPerAccess,
-                      Shape::kStrided / kThreads>;       
+                      Shape::kStrided / kThreads>;
 
-  using Delta = layout::PitchLinearShape<1, 1>;  
+  using Delta = layout::PitchLinearShape<1, 1>;
 
   using ShapeVec = Shape;
 
   CUTLASS_HOST_DEVICE
   static TensorCoord initial_offset(int thread_id)
   {
-    
+
     return TensorCoord(0, thread_id * Iterations::kStrided);
   }
 };
 
 
 ////////////////////////////////////////////////////////////////////////////////
 
@@ -330,15 +330,15 @@
     // This is the offset of a specific thread within a warp (units of vectors)
     layout::PitchLinearCoord thread_offset_in_warp{
       lane_id % Detail::WarpThreadArrangement::kContiguous,
       lane_id / Detail::WarpThreadArrangement::kContiguous
     };
 
     // This is the offset of a thread within a threadblock tile (units of vectors)
-    layout::PitchLinearCoord thread_offset_in_threadblock_tile_vec = 
+    layout::PitchLinearCoord thread_offset_in_threadblock_tile_vec =
       warp_footprint * warp_offset + thread_offset_in_warp;
 
     // This is the offset of a thread within a threadblock tile (units of elements)
     layout::PitchLinearCoord thread_offset_in_threadblock_tile_base{
       thread_offset_in_threadblock_tile_vec.contiguous() * kElementsPerAccess,
       thread_offset_in_threadblock_tile_vec.strided()
     };
@@ -456,15 +456,15 @@
     // This is the offset of a specific thread within a warp (units of vectors)
     layout::PitchLinearCoord thread_offset_in_warp{
       lane_id % Detail::WarpThreadArrangement::kContiguous,
       lane_id / Detail::WarpThreadArrangement::kContiguous
     };
 
     // This is the offset of a thread within a threadblock tile (units of vectors)
-    layout::PitchLinearCoord thread_offset_in_threadblock_tile_vec = 
+    layout::PitchLinearCoord thread_offset_in_threadblock_tile_vec =
       warp_footprint * warp_offset + thread_offset_in_warp;
 
     // This is the offset of a thread within a threadblock tile (units of elements)
     layout::PitchLinearCoord thread_offset_in_threadblock_tile_base{
       thread_offset_in_threadblock_tile_vec.contiguous() * kElementsPerAccess,
       thread_offset_in_threadblock_tile_vec.strided()
     };
@@ -597,29 +597,29 @@
     static int const kThreads = ThreadMap::kThreads;
 
     /// Extract vector length from Layout
     static int const kElementsPerAccess = ThreadMap::kElementsPerAccess;
 
     static_assert(kElementsPerAccess == 1 , "Simt transpose requires elements per access to be 1");
     ///< Iterations along each dimension (concept: PitchLinearShape)
-    using Iterations = 
+    using Iterations =
         layout::PitchLinearShape<ThreadMap::Iterations::kStrided,
         ThreadMap::Iterations::kContiguous>;
 
     static_assert(Iterations::kCount, "Number of iterations must be non-zero");
 
     static_assert(Iterations::kStrided == 1,
       "Strided iteration has to be one to reuse the same shared store function with those that don't need transpose");
 
     /// Shape of access by each thread
     using ThreadAccessShape = typename ThreadMap::ThreadAccessShape;
 
     ///< Delta betweeen accesses (units of elements, concept: PitchLinearShape)
     using Delta =
-        layout::PitchLinearShape<ThreadMap::Delta::kStrided, 
+        layout::PitchLinearShape<ThreadMap::Delta::kStrided,
         ThreadMap::Delta::kContiguous>;
 
 
     /// Maps thread ID to a coordinate offset within the tensor's logical
     /// coordinate space Note this is slightly different from the one of
     /// PitchLinearWarpRakedThreadMap.
     CUTLASS_HOST_DEVICE
@@ -689,20 +689,20 @@
     using WarpAccessIterations = layout::PitchLinearShape<
       ShapeInAccesses::kContiguous / WarpThreadArrangement::kContiguous,
       ShapeInAccesses::kStrided / WarpThreadArrangement::kStrided
     >;
 
     // Divide it into the number of warps, first partitioning the strided dimension then the
     // contiguous.
-    static int const kWarpsStrided = 
-      (WarpAccessIterations::kStrided >= kWarpCount 
+    static int const kWarpsStrided =
+      (WarpAccessIterations::kStrided >= kWarpCount
         ? kWarpCount : (kWarpCount / WarpAccessIterations::kStrided));
 
-    static int const kWarpsContiguous = 
-      (kWarpCount > WarpAccessIterations::kStrided ? 
+    static int const kWarpsContiguous =
+      (kWarpCount > WarpAccessIterations::kStrided ?
         WarpAccessIterations::kContiguous / kWarpsStrided : 1);
 
     /// Arrangement of warps within a threadblock-scoped tile
     using WarpArrangement = layout::PitchLinearShape<
       kWarpsContiguous, kWarpsStrided
     >;
   };
@@ -748,15 +748,15 @@
     // This is the offset of a specific thread within a warp (units of vectors)
     layout::PitchLinearCoord thread_offset_in_warp{
       lane_id % Detail::WarpThreadArrangement::kContiguous,
       lane_id / Detail::WarpThreadArrangement::kContiguous
     };
 
     // This is the offset of a thread within a threadblock tile (units of vectors)
-    layout::PitchLinearCoord thread_offset_in_threadblock_tile_vec = 
+    layout::PitchLinearCoord thread_offset_in_threadblock_tile_vec =
       warp_footprint * warp_offset + thread_offset_in_warp;
 
     // This is the offset of a thread within a threadblock tile (units of elements)
     layout::PitchLinearCoord thread_offset_in_threadblock_tile_base{
       thread_offset_in_threadblock_tile_vec.contiguous() * kElementsPerAccess,
       thread_offset_in_threadblock_tile_vec.strided()
     };
@@ -772,15 +772,15 @@
 /// The tile must be divisible by the thread count such that all threads may execute the same
 /// number of iterations with the same delta to exhaustively cover the tile.
 ///
 /// This class satisfies the "RegularThreadMapping" concept.
 template <
   typename Shape_,
   int Threads,
-	typename ThreadTileShape
+        typename ThreadTileShape
 >
 struct PitchLinear2DThreadTileStripminedThreadMap;
 
 
 template <
   typename Shape_,
   int Threads
@@ -863,15 +863,15 @@
 
     return TensorCoord(
       (thread_id % Detail::ShapeVec::kContiguous) * ThreadAccessShape::kContiguous,
       (thread_id / Detail::ShapeVec::kContiguous) * ThreadAccessShape::kStrided);
   }
 };
 
-/// Thread Mapping a 2D threadtiled mapping as a tranposed Pitchlinear2DThreadTile mapping
+/// Thread Mapping a 2D threadtiled mapping as a transposed Pitchlinear2DThreadTile mapping
 template <typename ThreadMap_>
 struct TransposePitchLinearThreadMap2DThreadTile {
     /// Underlying ThreadMap
     using ThreadMap = ThreadMap_;
 
     /// Tensor coordinate
     using TensorCoord = typename ThreadMap::TensorCoord;
@@ -884,26 +884,26 @@
 
     /// Extract vector length from Layout
     static int const kElementsPerAccess = ThreadMap::kElementsPerAccess;
 
 
     static_assert(kElementsPerAccess > 1 , "Simt transpose requires elements per access to be 1");
     ///< Iterations along each dimension (concept: PitchLinearShape)
-    using Iterations = 
+    using Iterations =
         layout::PitchLinearShape<ThreadMap::Iterations::kStrided,
         ThreadMap::Iterations::kContiguous>;
 
     static_assert(Iterations::kCount, "Number of iterations must be non-zero");
 
     /// Shape of access by each thread
     using ThreadAccessShape = typename ThreadMap::ThreadAccessShape;
 
     ///< Delta betweeen accesses (units of elements, concept: PitchLinearShape)
     using Delta =
-        layout::PitchLinearShape<ThreadMap::Delta::kStrided, 
+        layout::PitchLinearShape<ThreadMap::Delta::kStrided,
         ThreadMap::Delta::kContiguous>;
 
 
     /// Maps thread ID to a coordinate offset within the tensor's logical
     /// coordinate space Note this is slightly different from the one of
     /// PitchLinearWarpRakedThreadMap.
     CUTLASS_HOST_DEVICE
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/thread/transpose.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/thread/transpose.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/thread/unary_op.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/thread/unary_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_iterator.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/ell_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_access_iterator.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_access_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_iterator.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_access_iterator.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_access_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_iterator.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_2dthreadtile.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_2dthreadtile.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_params.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_params.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_triangular_matrix.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_triangular_matrix.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_triangular_matrix.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_triangular_matrix.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_vector_access_iterator.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/predicated_vector_access_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_scale_bias_vector_access_iterator.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/regular_scale_bias_vector_access_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear_direct_conv.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear_direct_conv.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op_sm80.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op_sm80.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear_2dthreadtile.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear_2dthreadtile.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op_sm70.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op_sm70.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/vector_iterator.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/threadblock/vector_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/transform/warp/vector_fragment_iterator.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/transform/warp/vector_fragment_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/uint128.h` & `flash_attn-2.0.0/csrc/cutlass/include/cutlass/uint128.h`

 * *Files 2% similar despite different names*

```diff
@@ -67,15 +67,15 @@
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 ///! Unsigned 128b integer type
 struct uint128_t {
 
   /// Size of one part of the uint's storage in bits
-  int const kPartSize = sizeof_bits<uint64_t>::value;
+  static constexpr int kPartSize = sizeof_bits<uint64_t>::value;
 
   struct hilo {
     uint64_t lo;
     uint64_t hi;
 
     hilo() = default;
 
@@ -154,15 +154,15 @@
 #endif
     return y;
   }
 
   /// Multiply by unsigned 64b integer yielding 128b integer
   CUTLASS_HOST_DEVICE
   uint128_t operator*(uint64_t const &rhs) const {
-    uint128_t y;
+    uint128_t y{};
 #if defined(CUTLASS_UINT128_NATIVE)
     y.native = native * rhs;
 #elif defined(CUTLASS_INT128_ARITHMETIC)
     // Multiply by the low part
     y.hilo_.lo = _umul128(hilo_.lo, rhs, &y.hilo_.hi);
 
     // Add the high part and ignore the overflow
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/include/cutlass/wmma_array.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/cute.cpp`

 * *Files 19% similar despite different names*

```diff
@@ -24,70 +24,31 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/*! \file
-    \brief Statically sized array of elements that accommodates all CUTLASS-supported numeric types
-           and is safe to use in a union.
+/* \file
+   \brief binding CuTe C++ APIs to Python
 */
 
-#pragma once
+#include <pybind11/pybind11.h>
+#include <pybind11/stl_bind.h>
 
-#include "cutlass/arch/wmma.h"
+#include "cute/arch/mma_sm90_gmma.hpp"
 
-#if defined(CUTLASS_ARCH_WMMA_ENABLED)
+namespace py = pybind11;
 
-#include "cutlass/cutlass.h"
-#include "cutlass/array.h"
-#include "cutlass/functional.h"
 
-namespace cutlass {
+PYBIND11_MODULE(cute, m) {
 
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Wmma array type (WmmaFragmentArray holds elements of of type nvcuda::wmma::fragment)
-template <
-  /// Element type
-  typename T,
-  /// Number of elements in the array
-  int N
->
-class WmmaFragmentArray: public Array<T, N, true> {
-public:
-
-  /// Efficient clear method (override Array::clear())
-  CUTLASS_HOST_DEVICE
-  void clear()
-  {
-    for(int i = 0; i < Array<T, N, true>::kElements; i++)
-    {
-      nvcuda::wmma::fill_fragment((*this)[i], (typename T::element_type)0);
-    }
-  }
-
-  CUTLASS_HOST_DEVICE
-  WmmaFragmentArray<T, N>& operator+=(const WmmaFragmentArray<T, N>& rhs)
-  {
-    using element_type = typename T::element_type;
-    plus<T> add;
-
-    for (int i = 0; i < Array<T, N, true>::kElements; i++)
-    {
-      (*this)[i] = add((*this)[i], rhs[i]);
-    }
-
-    return *this;
-  }
-
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-} // namespace cutlass
-
-///////////////////////////////////////////////////////////////////////////////////////////////////
-
-#endif // if defined(CUTLASS_ARCH_WMMA_ENABLED)
+    // module doc
+    m.doc() = "CuTe C++ bindings";
 
+    py::enum_<cute::GMMA::Major>(m, "GMMAMajor",
+        R"pbdoc(classification of CuTe GMMA tensor major specification)pbdoc")
+        .value("K", cute::GMMA::Major::K,
+            R"pbdoc(Tensor is contiguous in reduction dimension)pbdoc")
+        .value("MN", cute::GMMA::Major::MN,
+            R"pbdoc(Tensor is contiguous in non-reduction dimension)pbdoc");
+}
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/common/cutlass_unit_test.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/common/cutlass_unit_test.h`

 * *Files 20% similar despite different names*

```diff
@@ -26,37 +26,47 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 #pragma once
-#pragma warning (disable : 4068 ) /* disable unknown pragma warnings for vistual studio */
+#pragma warning (disable : 4068 ) /* disable unknown pragma warnings for visual studio */
 
 #pragma nv_diag_suppress boolean_controlling_expr_is_constant
 #include <gtest/gtest.h>
 #pragma nv_diag_warning boolean_controlling_expr_is_constant
 #pragma warning( disable : 4503)
 
 #include <cstdlib>
 #include <string>
+
+#include <cuda_runtime_api.h>
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Gets a CUDA device
+cudaDeviceProp GetCudaDevice();
+
+/// Prints device properties
+std::ostream &operator<<(std::ostream &out, cudaDeviceProp const &device);
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Sets flags for Unit test
 void FilterArchitecture();
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Reads environment variable `CUTLASS_UNIT_TEST_PROBLEM_COUNT` to control the number and order
 //  of problem sizes run by CUTLASS unit tests
 int CutlassUnitTestProblemCount();
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-
 // active test macro
 #define CUTLASS_TEST_LEVEL_ACTIVE(LEVEL,NAME_STATIC,NAME_DYNAMIC,...) \
     TEST(NAME_STATIC,L##LEVEL##_##NAME_DYNAMIC) __VA_ARGS__
 
 // disabled test macro
 #define CUTLASS_TEST_LEVEL_DISABLED(LEVEL,NAME_STATIC,NAME_DYNAMIC,...) \
     TEST(NAME_STATIC,DISABLED_L##LEVEL##_##NAME_DYNAMIC) {}
@@ -74,7 +84,19 @@
 #define CUTLASS_TEST_L1(NAME_STATIC,NAME_DYNAMIC,...)   CUTLASS_TEST_LEVEL_ACTIVE(1,NAME_STATIC,NAME_DYNAMIC,__VA_ARGS__)
 #define CUTLASS_TEST_L2(NAME_STATIC,NAME_DYNAMIC,...)   CUTLASS_TEST_LEVEL_ACTIVE(2,NAME_STATIC,NAME_DYNAMIC,__VA_ARGS__)
 #endif
 
 #if !defined(CUTLASS_TEST_UNIT_ENABLE_WARNINGS)
 #define CUTLASS_TEST_UNIT_ENABLE_WARNINGS false
 #endif
+
+#if (__CUDACC_VER_MAJOR__ >= 12)
+  #define CUDA_12_0_SM90_FEATURES_SUPPORTED true
+#else
+  #define CUDA_12_0_SM90_FEATURES_SUPPORTED false
+#endif
+
+#include <cutlass/cutlass.h>
+#include <cutlass/numeric_types.h>
+#include <cutlass/trace.h>
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/common/filter_architecture.cpp` & `flash_attn-2.0.0/csrc/cutlass/test/unit/common/filter_architecture.cpp`

 * *Files 23% similar despite different names*

```diff
@@ -31,17 +31,57 @@
 
 #include <cuda_runtime_api.h>
 
 #include "cutlass_unit_test.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
+/// Gets a CUDA device
+cudaDeviceProp GetCudaDevice() {
+
+  cudaError_t err;
+
+  int cudaDeviceId;
+  err = cudaGetDevice(&cudaDeviceId);
+  if (cudaSuccess != err) {
+    std::cerr << "*** Error: Could not detect active GPU device ID"
+              << " [" << cudaGetErrorString(err) << "]" << std::endl;
+    exit(1);
+  }
+
+  cudaDeviceProp deviceProperties;
+  err = cudaGetDeviceProperties(&deviceProperties, cudaDeviceId);
+
+  return deviceProperties;
+}
+
+/// Prints device properties
+std::ostream &operator<<(std::ostream &out, cudaDeviceProp const &deviceProperties) {
+
+  int deviceMajorMinor = deviceProperties.major * 10 + deviceProperties.minor;
+  if (deviceMajorMinor) {
+    int32_t clock_MHz = deviceProperties.clockRate / 1000;
+    out << "GPU(compute_"
+      << deviceMajorMinor << ", "
+      << deviceProperties.multiProcessorCount << " SMs @ " << clock_MHz << " MHz)";
+  }
+  else {
+    out << "No CUDA device.";
+  }
+
+  return out;
+}
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
 /// Sets flags for Unit test
 void FilterArchitecture() {
   // Default flags can be overwritten by --gtest_filter from commandline
+
+  int const kMaxDevice = 999;
+
   cudaError_t err;
 
   int cudaDeviceId;
   err = cudaGetDevice(&cudaDeviceId);
   if (cudaSuccess != err) {
     std::cerr << "*** Error: Could not detect active GPU device ID"
               << " [" << cudaGetErrorString(err) << "]" << std::endl;
@@ -53,15 +93,14 @@
   if (cudaSuccess != err) {
     std::cerr << "*** Error: Could not get device properties for GPU " << cudaDeviceId << " ["
               << cudaGetErrorString(err) << "]" << std::endl;
     exit(1);
   }
 
   int deviceMajorMinor = deviceProperties.major * 10 + deviceProperties.minor;
-  int const kMaxDevice = 999;
 
   // Defines text filters for each GEMM kernel based on minimum supported compute capability
   struct {
 
     /// Unit test filter string
     char const *filter;
 
@@ -74,15 +113,15 @@
   test_filters[] = {
     { "SM50*",                      50, kMaxDevice},
     { "SM60*",                      60, kMaxDevice},
     { "SM61*",                      61, kMaxDevice},
     { "SM70*",                      70, 75},
     { "SM75*",                      75, kMaxDevice},
     { "SM80*",                      80, kMaxDevice},
-    { "SM90*",                      90, kMaxDevice},
+    { "SM90*",                      90, 90        },
     { 0, 0, false }
   };
 
   // Set negative test filters
   std::stringstream ss;
   ss << "-";
   for (int i = 0, j = 0; test_filters[i].filter; ++i) {
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/cache_testbed_output.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/cache_testbed_output.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_few_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_few_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_fixed_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_fixed_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_qf32nhwc_qf32nhwc_qf32nhwc_simt_f32_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_qf32nhwc_qf32nhwc_qf32nhwc_simt_f32_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_reduction_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_fprop_with_reduction_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_problems.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_problems.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_testbed.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_testbed.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_testbed_interleaved.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_testbed_interleaved.h`

 * *Files 4% similar despite different names*

```diff
@@ -182,21 +182,57 @@
     tensor_B.sync_device();
     tensor_B_reordered.sync_device();
     tensor_C.sync_device();
     tensor_D_computed.sync_device();
     tensor_D_reference.sync_device();
   }
 
+  bool sufficient() const {
+    //
+    // Determine SMEM requirements and waive if not satisfied
+    //
+
+    int smem_size = int(sizeof(typename Conv2d::UnderlyingKernel::SharedStorage));
+
+    cudaDeviceProp properties;
+    int device_idx;
+    cudaError_t result = cudaGetDevice(&device_idx);
+
+    if (result != cudaSuccess) {
+      throw std::runtime_error("cudaGetDevice() API call failed.");
+    }
+
+    result = cudaGetDeviceProperties(&properties, device_idx);
+
+    if (result != cudaSuccess) {
+      throw std::runtime_error("cudaGetDeviceProperties() failed");
+    }
+
+    if (properties.sharedMemPerMultiprocessor < smem_size) {
+      return false;
+    }
+
+    return true;
+  }
+
   /// Executes one test
   bool run(
     cutlass::conv::Conv2dProblemSize const &problem_size,
     cutlass::conv::SplitKMode const &split_k_mode = cutlass::conv::SplitKMode::kSerial,
     ElementCompute alpha = ElementCompute(1),
     ElementCompute beta = ElementCompute(0)) {
 
+    // Waive test if insufficient CUDA device
+    if (!sufficient()) {
+      if (CUTLASS_TEST_UNIT_ENABLE_WARNINGS) {
+        std::cerr << "Test waived due to insufficient CUDA device." << std::endl;
+      }
+      return true;
+    }
+
 #if 0 //display conv2d problem size for debugging
     std::cout << problem_size << std::endl
               << "alpha, beta: (" << float(alpha) << ", " << float(beta) << ")" << std::endl
               << "split_k_mode: " << ((split_k_mode == cutlass::conv::SplitKMode::kSerial) ? "(serial)" : "(parallel)") << std::endl
               << std::endl;
 #endif
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_with_broadcast_testbed.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_with_broadcast_testbed.h`

 * *Files 0% similar despite different names*

```diff
@@ -116,14 +116,15 @@
   using ElementC = typename Conv2d::ElementC;
   using LayoutC = typename Conv2d::LayoutC;
   using ElementAccumulator = typename Conv2d::ElementAccumulator;
   using ElementCompute = typename Conv2d::ElementCompute;
   using EpilogueOutputOp = typename Conv2d::EpilogueOutputOp;
   using ElementZ = typename EpilogueOutputOp::ElementZ;
   using ElementT = typename EpilogueOutputOp::ElementT;
+  using ElementVector = typename EpilogueOutputOp::ElementVector;
 
   static cutlass::conv::Operator const kConvolutionalOperator = Conv2d::kConvolutionalOperator;
   static const bool kAddBroadcastFirst = AddBroadcastFirst;
   static const bool kStoreT = EpilogueOutputOp::kStoreT;
 
 public:
 
@@ -138,15 +139,15 @@
   cutlass::HostTensor<ElementC, LayoutC> tensor_C;
   cutlass::HostTensor<ElementAccumulator, LayoutC> tensor_C_reference;
   cutlass::HostTensor<ElementZ, LayoutC> tensor_Z_computed;
   cutlass::HostTensor<ElementZ, LayoutC> tensor_Z_reference;
   cutlass::HostTensor<ElementT, LayoutC> tensor_T_computed;
   cutlass::HostTensor<ElementT, LayoutC> tensor_T_reference;
   cutlass::HostTensor<ElementAccumulator, LayoutC> tensor_Y_reference;
-  cutlass::HostTensor<ElementC, LayoutC> tensor_Broadcast;                 // Input Broadcast
+  cutlass::HostTensor<ElementVector, LayoutC> tensor_Broadcast;            // Input Broadcast
 
 public:
 
   TestbedConv2dWithBroadcast(
     cutlass::Distribution::Kind init_A_ = cutlass::Distribution::Uniform,
     cutlass::Distribution::Kind init_B_ = cutlass::Distribution::Uniform,
     cutlass::Distribution::Kind init_C_ = cutlass::Distribution::Uniform,
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_with_reduction_testbed.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv2d_with_reduction_testbed.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_problems.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv3d_problems.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_testbed.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv3d_testbed.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_direct_conv_testbed.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/depthwise_conv2d_direct_conv_testbed.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu`

 * *Files 0% similar despite different names*

```diff
@@ -324,14 +324,15 @@
   using Direct2dConv = cutlass::conv::device::DirectConvolution<DepthwiseDirect2dConv>;
 
   /// Run all unit test sizes with device-level Conv2d instance
   EXPECT_TRUE(test::conv::device::TestSpecificDepthwiseDirectConv2d<Direct2dConv>(
       DepthwiseFpropProblemSizes_filter5x5()));
 }
 
+#if 0
 ////////////////////////////////////////////////////////////////////////////////
 TEST(
     SM60_Device_Depthwise_conv2d_Fprop_Direct_Conv_Optimized_f16nhwc_f16nhwc_f16nhwc_simt_f16,
     64x32_3_16x32_5x37) {
 
   using ElementInputA = cutlass::half_t;
   using ElementInputB = cutlass::half_t;
@@ -420,7 +421,9 @@
 
   using Direct2dConv = cutlass::conv::device::DirectConvolution<DepthwiseDirect2dConv>;
 
   /// Run all unit test sizes with device-level Conv2d instance
   EXPECT_TRUE(test::conv::device::TestSpecificDepthwiseDirectConv2d<Direct2dConv>(
       DepthwiseFpropProblemSizes_filter5x37()));
 }
+#endif
+
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_fixed_stride_dilation_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_fixed_stride_dilation_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/conv/device/group_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/conv/device/group_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/array.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/core/array.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/bfloat16.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/core/bfloat16.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/complex.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/core/complex.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/float8.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/core/float8.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/functional.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/core/functional.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/half.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/core/half.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/matrix.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/core/matrix.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/matrix_coord.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/core/matrix_coord.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/numeric_conversion.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/core/numeric_conversion.cu`

 * *Files 4% similar despite different names*

```diff
@@ -148,17 +148,15 @@
   test::core::kernel::run_test<Destination, Source, kN>();
 }
 
 TEST(NumericConversion, f32_to_fe5m2_rn_array) {
   int const kN = 27;
   using Source = float;
   using Destination = cutlass::float_e5m2_t;
-
   test::core::kernel::run_test<Destination, Source, kN>();
-
 }
 
 TEST(NumericConversion, f16_to_fe4m3_rn) {
   int const kN = 1;
   using Source = cutlass::half_t;
   using Destination = cutlass::float_e4m3_t;
   test::core::kernel::run_test<Destination, Source, kN>();
@@ -246,24 +244,27 @@
 TEST(NumericConversion, fe4m3_to_f32_rn) {
   int const kN = 1;
   using Source = cutlass::float_e4m3_t;
   using Destination = float;
   test::core::kernel::run_test<Destination, Source, kN>();
 }
 
-TEST(NumericConversion, fe4m3_to_f32_array) {
-  int const kN = 27;
-  using Source = cutlass::float_e4m3_t;
-  using Destination = float;
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+TEST(NumericConversion, f32x8_to_s8x8_rn) {
+
+  int const kN = 8;
+  using Source = float;
+  using Destination = int8_t;
   test::core::kernel::run_test<Destination, Source, kN>();
 }
 
-TEST(NumericConversion, fe5m2_to_f32_rn) {
-  int const kN = 1;
-  using Source = cutlass::float_e5m2_t;
+TEST(NumericConversion, fe4m3_to_f32_array) {
+  int const kN = 27;
+  using Source = cutlass::float_e4m3_t;
   using Destination = float;
   test::core::kernel::run_test<Destination, Source, kN>();
 }
 
 TEST(NumericConversion, fe5m2_to_f32_array) {
   int const kN = 27;
   using Source = cutlass::float_e5m2_t;
@@ -324,39 +325,7 @@
   int const kN = 27;
   using Source = cutlass::float_e5m2_t;
   using Destination = cutlass::bfloat16_t;
   test::core::kernel::run_test<Destination, Source, kN>();
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-
-TEST(NumericConversion, f32x8_to_s8x8_rn) {
-
-  int const kN = 8;
-  using Source = float;
-  using Destination = int8_t;
-
-  dim3 grid(1, 1);
-  dim3 block(1, 1);
-
-  cutlass::HostTensor<Destination, cutlass::layout::RowMajor> destination({1, kN});
-  cutlass::HostTensor<Source, cutlass::layout::RowMajor> source({1, kN});
-
-  for (int i = 0; i < kN; ++i) {
-    source.host_data()[i] = float(i);
-  }
-
-  source.sync_device();
-
-  test::core::kernel::convert<Destination, Source, kN><<< grid, block >>>(
-    reinterpret_cast<cutlass::Array<Destination, kN> *>(destination.device_data()),
-    reinterpret_cast<cutlass::Array<Source, kN> const *>(source.device_data())
-  );
-
-  destination.sync_host();
-
-  for (int i = 0; i < kN; ++i) {
-    EXPECT_TRUE(float(destination.host_data()[i]) == source.host_data()[i]);
-  }
-}
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/predicate_vector.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/core/predicate_vector.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/quaternion.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/core/quaternion.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/tensor_ref.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/core/tensor_ref.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/tensor_view.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/core/tensor_view.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/test_unit_core.cpp` & `flash_attn-2.0.0/csrc/cutlass/test/unit/core/test_unit_core.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/core/tfloat32.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/core/tfloat32.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/thread/activation.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/thread/activation.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/thread/linear_combination.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination_planar_complex.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/thread/linear_combination_planar_complex.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_planar_complex.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/threadblock/epilogue_planar_complex.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/threadblock/epilogue_simt.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm60.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm60.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm61.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm61.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_tensor_op.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/threadblock/epilogue_tensor_op.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_volta_tensor_op.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/threadblock/epilogue_volta_tensor_op.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_tensor_op.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_tensor_op.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_testbed.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_testbed.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_wmma_tensor_op_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/threadblock/epilogue_wmma_tensor_op_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/output_tile_threadmap.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/threadblock/output_tile_threadmap.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/predicated_tile_iterator.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/threadblock/predicated_tile_iterator.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/testbed.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/threadblock/testbed.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/testbed_planar_complex.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/threadblock/testbed_planar_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_tensor_op.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/warp/fragment_iterator_tensor_op.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_volta_tensor_op.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/warp/fragment_iterator_volta_tensor_op.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_wmma_tensor_op.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/epilogue/warp/fragment_iterator_wmma_tensor_op.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_wmma_tensor_op_s32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_wmma_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_wmma_tensor_op_s32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_wmma_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_bf16n_bf16n_f32t_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_bf16n_bf16n_f32t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_bf16t_bf16t_bf16t_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_bf16t_bf16t_bf16t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf32n_cf32t_cf32t_tensor_op_tf32_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_cf32n_cf32t_cf32t_tensor_op_tf32_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf32t_cf32n_cf32t_tensor_op_tf32_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_cf32t_cf32n_cf32t_tensor_op_tf32_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm90.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm90.cu`

 * *Files 2% similar despite different names*

```diff
@@ -46,15 +46,15 @@
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_complex.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 TEST(SM90_Device_Gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian, 32x32x16_16x16x16) {
 
   using Element = cutlass::complex<double>; 
 
@@ -189,10 +189,10 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemmComplex<Gemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm90.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm90.cu`

 * *Files 4% similar despite different names*

```diff
@@ -46,15 +46,15 @@
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_complex.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 TEST(SM90_Device_Gemm_cf64n_cf64t_cf64t_tensor_op_f64, 32x32x16_16x16x16) {
 
   using Element = cutlass::complex<double>;
 
@@ -243,10 +243,10 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemmComplex<Gemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm90.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm90.cu`

 * *Files 2% similar despite different names*

```diff
@@ -46,15 +46,15 @@
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_complex.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 TEST(SM90_Device_Gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian, 32x32x8_16x16x8) {
   
   using Element = cutlass::complex<double>;
 
@@ -187,11 +187,11 @@
 
   EXPECT_TRUE(test::gemm::device::TestAllGemmComplex<Gemm>());
 }
 
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm90.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm90.cu`

 * *Files 1% similar despite different names*

```diff
@@ -46,15 +46,15 @@
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_complex.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 TEST(SM90_Device_Gemm_cf64t_cf64n_cf64t_tensor_op_f64, 32x32x8_16x16x8) {
   
   using Element = cutlass::complex<double>;
 
@@ -295,11 +295,11 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemmComplex<Gemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_direct_store_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_direct_store_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f16_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f16_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sparse_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_volta_tensor_op_f32_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_volta_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f16_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f16_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_volta_tensor_op_f32_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_volta_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f16_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f16_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sparse_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sparse_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_volta_tensor_op_f16_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_volta_tensor_op_f32_sm70.cu`

 * *Files 4% similar despite different names*

```diff
@@ -41,18 +41,18 @@
 
 #include "testbed.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM70_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM70_Device_Gemm_f16n_f16t_f16t_volta_tensor_op_f16, 128x256x32_64x64x32) {
+TEST(SM70_Device_Gemm_f16n_f16t_f32t_volta_tensor_op_f32, 128x256x32_64x64x32) {
 
-  using ElementOutput = cutlass::half_t;
-  using ElementAccumulator = cutlass::half_t;
+  using ElementOutput = float;
+  using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::half_t,
     cutlass::layout::ColumnMajor,
     cutlass::half_t,
     cutlass::layout::RowMajor,
     ElementOutput,
@@ -68,22 +68,22 @@
       128 / cutlass::sizeof_bits<ElementOutput>::value,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
-  
+
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
-TEST(SM70_Device_Gemm_f16n_f16t_f16t_volta_tensor_op_f16, 256x128x32_64x64x32) {
+TEST(SM70_Device_Gemm_f16n_f16t_f32t_volta_tensor_op_f32, 256x128x32_64x64x32) {
 
-  using ElementOutput = cutlass::half_t;
-  using ElementAccumulator = cutlass::half_t;
+  using ElementOutput = float;
+  using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::half_t,
     cutlass::layout::ColumnMajor,
     cutlass::half_t,
     cutlass::layout::RowMajor,
     ElementOutput,
@@ -103,18 +103,18 @@
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
-TEST(SM70_Device_Gemm_f16n_f16t_f16t_volta_tensor_op_f16, 128x128x32_64x64x32) {
+TEST(SM70_Device_Gemm_f16n_f16t_f32t_volta_tensor_op_f32, 128x128x32_64x64x32) {
 
-  using ElementOutput = cutlass::half_t;
-  using ElementAccumulator = cutlass::half_t;
+  using ElementOutput = float;
+  using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::half_t,
     cutlass::layout::ColumnMajor,
     cutlass::half_t,
     cutlass::layout::RowMajor,
     ElementOutput,
@@ -134,18 +134,18 @@
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
-TEST(SM70_Device_Gemm_f16n_f16t_f16t_volta_tensor_op_f16, 128x64x32_64x32x32) {
+TEST(SM70_Device_Gemm_f16n_f16t_f32t_volta_tensor_op_f32, 128x64x32_64x32x32) {
 
-  using ElementOutput = cutlass::half_t;
-  using ElementAccumulator = cutlass::half_t;
+  using ElementOutput = float;
+  using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::half_t,
     cutlass::layout::ColumnMajor,
     cutlass::half_t,
     cutlass::layout::RowMajor,
     ElementOutput,
@@ -165,18 +165,18 @@
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
-TEST(SM70_Device_Gemm_f16n_f16t_f16t_volta_tensor_op_f16, 64x128x32_32x64x32) {
+TEST(SM70_Device_Gemm_f16n_f16t_f32t_volta_tensor_op_f32, 64x128x32_32x64x32) {
 
-  using ElementOutput = cutlass::half_t;
-  using ElementAccumulator = cutlass::half_t;
+  using ElementOutput = float;
+  using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::half_t,
     cutlass::layout::ColumnMajor,
     cutlass::half_t,
     cutlass::layout::RowMajor,
     ElementOutput,
@@ -196,18 +196,18 @@
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
-TEST(SM70_Device_Gemm_f16n_f16t_f16t_volta_tensor_op_f16, 64x64x32_64x64x32) {
+TEST(SM70_Device_Gemm_f16n_f16t_f32t_volta_tensor_op_f32, 64x64x32_64x64x32) {
 
-  using ElementOutput = cutlass::half_t;
-  using ElementAccumulator = cutlass::half_t;
+  using ElementOutput = float;
+  using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::half_t,
     cutlass::layout::ColumnMajor,
     cutlass::half_t,
     cutlass::layout::RowMajor,
     ElementOutput,
@@ -227,18 +227,18 @@
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
-TEST(SM70_Device_Gemm_f16n_f16t_f16t_volta_tensor_op_f16, 64x64x32_32x32x32) {
+TEST(SM70_Device_Gemm_f16n_f16t_f32t_volta_tensor_op_f32, 64x64x32_32x32x32) {
 
-  using ElementOutput = cutlass::half_t;
-  using ElementAccumulator = cutlass::half_t;
+  using ElementOutput = float;
+  using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::half_t,
     cutlass::layout::ColumnMajor,
     cutlass::half_t,
     cutlass::layout::RowMajor,
     ElementOutput,
@@ -260,8 +260,8 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif
+#endif // if (CUTLASS_ENABLE_TENSOR_CORE_MMA)
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f16_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f16_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32n_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_volta_tensor_op_f32_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_volta_tensor_op_f32_sm70.cu`

 * *Files 3% similar despite different names*

```diff
@@ -35,30 +35,37 @@
 #include <iostream>
 
 #include "cutlass/cutlass.h"
 #include "cutlass/gemm/device/gemm.h"
 
 #include "../../common/cutlass_unit_test.h"
 
+#include "cutlass/util/host_tensor.h"
+#include "cutlass/util/tensor_view_io.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/reference/host/tensor_copy.h"
+#include "cutlass/util/reference/host/tensor_compare.h"
+#include "cutlass/util/reference/host/gemm.h"
+
 #include "testbed.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM70_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM70_Device_Gemm_f16n_f16t_f32t_volta_tensor_op_f32, 128x256x32_64x64x32) {
+TEST(SM70_Device_Gemm_f16t_f16n_f32t_volta_tensor_op_f32, 128x256x32_64x64x32) {
 
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::half_t,
-    cutlass::layout::ColumnMajor,
-    cutlass::half_t,
     cutlass::layout::RowMajor,
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor,
     ElementOutput,
     cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm70,
     cutlass::gemm::GemmShape<128, 256, 32>,
     cutlass::gemm::GemmShape<64, 64, 32>,
@@ -72,24 +79,24 @@
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
-TEST(SM70_Device_Gemm_f16n_f16t_f32t_volta_tensor_op_f32, 256x128x32_64x64x32) {
+TEST(SM70_Device_Gemm_f16t_f16n_f32t_volta_tensor_op_f32, 256x128x32_64x64x32) {
 
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::half_t,
-    cutlass::layout::ColumnMajor,
-    cutlass::half_t,
     cutlass::layout::RowMajor,
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor,
     ElementOutput,
     cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm70,
     cutlass::gemm::GemmShape<256, 128, 32>,
     cutlass::gemm::GemmShape<64, 64, 32>,
@@ -103,24 +110,24 @@
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
-TEST(SM70_Device_Gemm_f16n_f16t_f32t_volta_tensor_op_f32, 128x128x32_64x64x32) {
+TEST(SM70_Device_Gemm_f16t_f16n_f32t_volta_tensor_op_f32, 128x128x32_64x64x32) {
 
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::half_t,
-    cutlass::layout::ColumnMajor,
-    cutlass::half_t,
     cutlass::layout::RowMajor,
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor,
     ElementOutput,
     cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm70,
     cutlass::gemm::GemmShape<128, 128, 32>,
     cutlass::gemm::GemmShape<64, 64, 32>,
@@ -134,24 +141,24 @@
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
-TEST(SM70_Device_Gemm_f16n_f16t_f32t_volta_tensor_op_f32, 128x64x32_64x32x32) {
+TEST(SM70_Device_Gemm_f16t_f16n_f32t_volta_tensor_op_f32, 128x64x32_64x32x32) {
 
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::half_t,
-    cutlass::layout::ColumnMajor,
-    cutlass::half_t,
     cutlass::layout::RowMajor,
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor,
     ElementOutput,
     cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm70,
     cutlass::gemm::GemmShape<128, 64, 32>,
     cutlass::gemm::GemmShape<64, 32, 32>,
@@ -165,24 +172,24 @@
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
-TEST(SM70_Device_Gemm_f16n_f16t_f32t_volta_tensor_op_f32, 64x128x32_32x64x32) {
+TEST(SM70_Device_Gemm_f16t_f16n_f32t_volta_tensor_op_f32, 64x128x32_32x64x32) {
 
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::half_t,
-    cutlass::layout::ColumnMajor,
-    cutlass::half_t,
     cutlass::layout::RowMajor,
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor,
     ElementOutput,
     cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm70,
     cutlass::gemm::GemmShape<64, 128, 32>,
     cutlass::gemm::GemmShape<32, 64, 32>,
@@ -196,24 +203,24 @@
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
-TEST(SM70_Device_Gemm_f16n_f16t_f32t_volta_tensor_op_f32, 64x64x32_64x64x32) {
+TEST(SM70_Device_Gemm_f16t_f16n_f32t_volta_tensor_op_f32, 64x64x32_64x64x32) {
 
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::half_t,
-    cutlass::layout::ColumnMajor,
-    cutlass::half_t,
     cutlass::layout::RowMajor,
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor,
     ElementOutput,
     cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm70,
     cutlass::gemm::GemmShape<64, 64, 32>,
     cutlass::gemm::GemmShape<64, 64, 32>,
@@ -227,24 +234,24 @@
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
-TEST(SM70_Device_Gemm_f16n_f16t_f32t_volta_tensor_op_f32, 64x64x32_32x32x32) {
+TEST(SM70_Device_Gemm_f16t_f16n_f32t_volta_tensor_op_f32, 64x64x32_32x32x32) {
 
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::half_t,
-    cutlass::layout::ColumnMajor,
-    cutlass::half_t,
     cutlass::layout::RowMajor,
+    cutlass::half_t,
+    cutlass::layout::ColumnMajor,
     ElementOutput,
     cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm70,
     cutlass::gemm::GemmShape<64, 64, 32>,
     cutlass::gemm::GemmShape<32, 32, 32>,
@@ -260,8 +267,8 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // if (CUTLASS_ENABLE_TENSOR_CORE_MMA)
+#endif
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_singlestage_wmma_tensor_op_f16_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_singlestage_wmma_tensor_op_f16_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f16_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f16_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_singlestage_wmma_tensor_op_f16_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_singlestage_wmma_tensor_op_f16_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_broadcast_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_broadcast_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sparse_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sparse_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_volta_tensor_op_f16_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_volta_tensor_op_f16_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f16_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f16_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32n_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_singlestage_wmma_tensor_op_f32_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_singlestage_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_volta_tensor_op_f32_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_volta_tensor_op_f32_sm70.cu`

 * *Files 2% similar despite different names*

```diff
@@ -48,24 +48,24 @@
 
 #include "testbed.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM70_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM70_Device_Gemm_f16t_f16n_f32t_volta_tensor_op_f32, 128x256x32_64x64x32) {
+TEST(SM70_Device_Gemm_f16t_f16t_f32t_volta_tensor_op_f32, 128x256x32_64x64x32) {
 
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::half_t,
     cutlass::layout::RowMajor,
     cutlass::half_t,
-    cutlass::layout::ColumnMajor,
+    cutlass::layout::RowMajor,
     ElementOutput,
     cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm70,
     cutlass::gemm::GemmShape<128, 256, 32>,
     cutlass::gemm::GemmShape<64, 64, 32>,
@@ -79,24 +79,24 @@
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
-TEST(SM70_Device_Gemm_f16t_f16n_f32t_volta_tensor_op_f32, 256x128x32_64x64x32) {
+TEST(SM70_Device_Gemm_f16t_f16t_f32t_volta_tensor_op_f32, 256x128x32_64x64x32) {
 
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::half_t,
     cutlass::layout::RowMajor,
     cutlass::half_t,
-    cutlass::layout::ColumnMajor,
+    cutlass::layout::RowMajor,
     ElementOutput,
     cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm70,
     cutlass::gemm::GemmShape<256, 128, 32>,
     cutlass::gemm::GemmShape<64, 64, 32>,
@@ -110,24 +110,24 @@
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
-TEST(SM70_Device_Gemm_f16t_f16n_f32t_volta_tensor_op_f32, 128x128x32_64x64x32) {
+TEST(SM70_Device_Gemm_f16t_f16t_f32t_volta_tensor_op_f32, 128x128x32_64x64x32) {
 
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::half_t,
     cutlass::layout::RowMajor,
     cutlass::half_t,
-    cutlass::layout::ColumnMajor,
+    cutlass::layout::RowMajor,
     ElementOutput,
     cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm70,
     cutlass::gemm::GemmShape<128, 128, 32>,
     cutlass::gemm::GemmShape<64, 64, 32>,
@@ -137,59 +137,28 @@
       128 / cutlass::sizeof_bits<ElementOutput>::value,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
-
+  
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
-TEST(SM70_Device_Gemm_f16t_f16n_f32t_volta_tensor_op_f32, 128x64x32_64x32x32) {
+TEST(SM70_Device_Gemm_f16t_f16t_f32t_volta_tensor_op_f32, 64x128x32_32x64x32) {
 
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::half_t,
     cutlass::layout::RowMajor,
     cutlass::half_t,
-    cutlass::layout::ColumnMajor,
-    ElementOutput,
-    cutlass::layout::RowMajor,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm70,
-    cutlass::gemm::GemmShape<128, 64, 32>,
-    cutlass::gemm::GemmShape<64, 32, 32>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
-    cutlass::epilogue::thread::LinearCombination<
-      ElementOutput,
-      128 / cutlass::sizeof_bits<ElementOutput>::value,
-      ElementAccumulator,
-      ElementAccumulator
-    >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    2
-  >;
-
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-}
-
-TEST(SM70_Device_Gemm_f16t_f16n_f32t_volta_tensor_op_f32, 64x128x32_32x64x32) {
-
-  using ElementOutput = float;
-  using ElementAccumulator = float;
-
-  using Gemm = cutlass::gemm::device::Gemm<
-    cutlass::half_t,
     cutlass::layout::RowMajor,
-    cutlass::half_t,
-    cutlass::layout::ColumnMajor,
     ElementOutput,
     cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm70,
     cutlass::gemm::GemmShape<64, 128, 32>,
     cutlass::gemm::GemmShape<32, 64, 32>,
@@ -203,55 +172,55 @@
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
-TEST(SM70_Device_Gemm_f16t_f16n_f32t_volta_tensor_op_f32, 64x64x32_64x64x32) {
+TEST(SM70_Device_Gemm_f16t_f16t_f32t_volta_tensor_op_f32, 128x64x32_64x32x32) {
 
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::half_t,
     cutlass::layout::RowMajor,
     cutlass::half_t,
-    cutlass::layout::ColumnMajor,
+    cutlass::layout::RowMajor,
     ElementOutput,
     cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm70,
-    cutlass::gemm::GemmShape<64, 64, 32>,
-    cutlass::gemm::GemmShape<64, 64, 32>,
+    cutlass::gemm::GemmShape<128, 64, 32>,
+    cutlass::gemm::GemmShape<64, 32, 32>,
     cutlass::gemm::GemmShape<8, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
       ElementOutput,
       128 / cutlass::sizeof_bits<ElementOutput>::value,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
-TEST(SM70_Device_Gemm_f16t_f16n_f32t_volta_tensor_op_f32, 64x64x32_32x32x32) {
+TEST(SM70_Device_Gemm_f16t_f16t_f32t_volta_tensor_op_f32, 64x64x32_32x32x32) {
 
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
     cutlass::half_t,
     cutlass::layout::RowMajor,
     cutlass::half_t,
-    cutlass::layout::ColumnMajor,
+    cutlass::layout::RowMajor,
     ElementOutput,
     cutlass::layout::RowMajor,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm70,
     cutlass::gemm::GemmShape<64, 64, 32>,
     cutlass::gemm::GemmShape<32, 32, 32>,
@@ -267,8 +236,8 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif
+#endif // if (CUTLASS_ENABLE_TENSOR_CORE_MMA)
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f16_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f16_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f16_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f16_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_volta_tensor_op_f32_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_sm80.cu`

 * *Files 12% similar despite different names*

```diff
@@ -25,219 +25,229 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide GEMM interface
+    \brief Tests for device-wide SYRK interface
+  
 */
 
 #include <iostream>
 
-#include "cutlass/cutlass.h"
-#include "cutlass/gemm/device/gemm.h"
-
 #include "../../common/cutlass_unit_test.h"
-
+#include "cutlass/blas3.h"
+#include "cutlass/gemm/device/rank_2k.h"
 #include "cutlass/util/host_tensor.h"
-#include "cutlass/util/tensor_view_io.h"
-#include "cutlass/util/reference/host/tensor_fill.h"
-#include "cutlass/util/reference/host/tensor_copy.h"
+#include "cutlass/util/reference/host/rank_2k.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
-#include "cutlass/util/reference/host/gemm.h"
+#include "cutlass/util/reference/host/tensor_copy.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/tensor_view_io.h"
 
-#include "testbed.h"
+#include "testbed_rank2k_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM70_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM70_Device_Gemm_f16t_f16t_f32t_volta_tensor_op_f32, 128x256x32_64x64x32) {
+TEST(SM80_Device_Syr2k_f64n_f64n_l_tensor_op_f64, 32x32x16_16x16x16) {
 
-  using ElementOutput = float;
-  using ElementAccumulator = float;
+  using ElementA = double;
+  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementB = double;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = double;
+  using LayoutC = cutlass::layout::ColumnMajor;
+  using ElementAccumulator = double;
 
-  using Gemm = cutlass::gemm::device::Gemm<
-    cutlass::half_t,
-    cutlass::layout::RowMajor,
-    cutlass::half_t,
-    cutlass::layout::RowMajor,
-    ElementOutput,
-    cutlass::layout::RowMajor,
+  using Rank2K = cutlass::gemm::device::Rank2K<
+    ElementA,
+    LayoutA,
+    ElementB,
+    LayoutB,
+    ElementC,
+    LayoutC,
+    cutlass::FillMode::kLower,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm70,
-    cutlass::gemm::GemmShape<128, 256, 32>,
-    cutlass::gemm::GemmShape<64, 64, 32>,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
     cutlass::gemm::GemmShape<8, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
-      ElementOutput,
-      128 / cutlass::sizeof_bits<ElementOutput>::value,
+      ElementC,
+      1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    2
+    4
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
+
 }
 
-TEST(SM70_Device_Gemm_f16t_f16t_f32t_volta_tensor_op_f32, 256x128x32_64x64x32) {
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM80_Device_Syr2k_f64n_f64n_l_tensor_op_f64, 64x64x16_32x32x16) {
 
-  using ElementOutput = float;
-  using ElementAccumulator = float;
+  using ElementA = double;
+  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementB = double;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = double;
+  using LayoutC = cutlass::layout::ColumnMajor;
+  using ElementAccumulator = double;
 
-  using Gemm = cutlass::gemm::device::Gemm<
-    cutlass::half_t,
-    cutlass::layout::RowMajor,
-    cutlass::half_t,
-    cutlass::layout::RowMajor,
-    ElementOutput,
-    cutlass::layout::RowMajor,
+  using Rank2K = cutlass::gemm::device::Rank2K<
+    ElementA,
+    LayoutA,
+    ElementB,
+    LayoutB,
+    ElementC,
+    LayoutC,
+    cutlass::FillMode::kLower,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm70,
-    cutlass::gemm::GemmShape<256, 128, 32>,
-    cutlass::gemm::GemmShape<64, 64, 32>,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<64, 64, 16>,
+    cutlass::gemm::GemmShape<32, 32, 16>,
     cutlass::gemm::GemmShape<8, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
-      ElementOutput,
-      128 / cutlass::sizeof_bits<ElementOutput>::value,
+      ElementC,
+      1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    2
+    4
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
-}
-
-TEST(SM70_Device_Gemm_f16t_f16t_f32t_volta_tensor_op_f32, 128x128x32_64x64x32) {
+  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 
-  using ElementOutput = float;
-  using ElementAccumulator = float;
-
-  using Gemm = cutlass::gemm::device::Gemm<
-    cutlass::half_t,
-    cutlass::layout::RowMajor,
-    cutlass::half_t,
-    cutlass::layout::RowMajor,
-    ElementOutput,
-    cutlass::layout::RowMajor,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm70,
-    cutlass::gemm::GemmShape<128, 128, 32>,
-    cutlass::gemm::GemmShape<64, 64, 32>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
-    cutlass::epilogue::thread::LinearCombination<
-      ElementOutput,
-      128 / cutlass::sizeof_bits<ElementOutput>::value,
-      ElementAccumulator,
-      ElementAccumulator
-    >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    2
-  >;
-  
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
-TEST(SM70_Device_Gemm_f16t_f16t_f32t_volta_tensor_op_f32, 64x128x32_32x64x32) {
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM80_Device_Syr2k_f64n_f64n_l_tensor_op_f64, 128x64x16_64x32x16) {
 
-  using ElementOutput = float;
-  using ElementAccumulator = float;
+  using ElementA = double;
+  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementB = double;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = double;
+  using LayoutC = cutlass::layout::ColumnMajor;
+  using ElementAccumulator = double;
 
-  using Gemm = cutlass::gemm::device::Gemm<
-    cutlass::half_t,
-    cutlass::layout::RowMajor,
-    cutlass::half_t,
-    cutlass::layout::RowMajor,
-    ElementOutput,
-    cutlass::layout::RowMajor,
+  using Rank2K = cutlass::gemm::device::Rank2K<
+    ElementA,
+    LayoutA,
+    ElementB,
+    LayoutB,
+    ElementC,
+    LayoutC,
+    cutlass::FillMode::kLower,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm70,
-    cutlass::gemm::GemmShape<64, 128, 32>,
-    cutlass::gemm::GemmShape<32, 64, 32>,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<128, 64, 16>,
+    cutlass::gemm::GemmShape<64, 32, 16>,
     cutlass::gemm::GemmShape<8, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
-      ElementOutput,
-      128 / cutlass::sizeof_bits<ElementOutput>::value,
+      ElementC,
+      1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    2
+    4
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
+
 }
 
-TEST(SM70_Device_Gemm_f16t_f16t_f32t_volta_tensor_op_f32, 128x64x32_64x32x32) {
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM80_Device_Syr2k_f64n_f64n_l_tensor_op_f64, 128x128x16_32x64x16) {
 
-  using ElementOutput = float;
-  using ElementAccumulator = float;
+  using ElementA = double;
+  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementB = double;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = double;
+  using LayoutC = cutlass::layout::ColumnMajor;
+  using ElementAccumulator = double;
 
-  using Gemm = cutlass::gemm::device::Gemm<
-    cutlass::half_t,
-    cutlass::layout::RowMajor,
-    cutlass::half_t,
-    cutlass::layout::RowMajor,
-    ElementOutput,
-    cutlass::layout::RowMajor,
+  using Rank2K = cutlass::gemm::device::Rank2K<
+    ElementA,
+    LayoutA,
+    ElementB,
+    LayoutB,
+    ElementC,
+    LayoutC,
+    cutlass::FillMode::kLower,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm70,
-    cutlass::gemm::GemmShape<128, 64, 32>,
-    cutlass::gemm::GemmShape<64, 32, 32>,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<128, 128, 16>,
+    cutlass::gemm::GemmShape<32, 64, 16>,
     cutlass::gemm::GemmShape<8, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
-      ElementOutput,
-      128 / cutlass::sizeof_bits<ElementOutput>::value,
+      ElementC,
+      1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    2
+    3
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
+
 }
 
-TEST(SM70_Device_Gemm_f16t_f16t_f32t_volta_tensor_op_f32, 64x64x32_32x32x32) {
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM80_Device_Syr2k_f64n_f64n_u_tensor_op_f64, 32x32x16_16x16x16) {
 
-  using ElementOutput = float;
-  using ElementAccumulator = float;
+  using ElementA = double;
+  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementB = double;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = double;
+  using LayoutC = cutlass::layout::ColumnMajor;
+  using ElementAccumulator = double;
 
-  using Gemm = cutlass::gemm::device::Gemm<
-    cutlass::half_t,
-    cutlass::layout::RowMajor,
-    cutlass::half_t,
-    cutlass::layout::RowMajor,
-    ElementOutput,
-    cutlass::layout::RowMajor,
+  using Rank2K = cutlass::gemm::device::Rank2K<
+    ElementA,
+    LayoutA,
+    ElementB,
+    LayoutB,
+    ElementC,
+    LayoutC,
+    cutlass::FillMode::kUpper,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm70,
-    cutlass::gemm::GemmShape<64, 64, 32>,
-    cutlass::gemm::GemmShape<32, 32, 32>,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
     cutlass::gemm::GemmShape<8, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
-      ElementOutput,
-      128 / cutlass::sizeof_bits<ElementOutput>::value,
+      ElementC,
+      1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    2
+    4
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
+
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-
-#endif // if (CUTLASS_ENABLE_TENSOR_CORE_MMA)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_wmma_tensor_op_f32_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_bf16_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_bf16_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32t_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f32n_f32t_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32t_f32n_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f32t_f32n_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32t_f32t_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f32t_f32t_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm90.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm90.cu`

 * *Files 3% similar despite different names*

```diff
@@ -42,15 +42,15 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 TEST(SM90_Device_Gemm_f64n_f64t_f64t_tensor_op_f64, 32x32x16_16x16x16_16x8x4) {
 
   using ElementOutput = double;
   using ElementAccumulator = double;
@@ -216,8 +216,8 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#endif // if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm90.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm90.cu`

 * *Files 2% similar despite different names*

```diff
@@ -42,15 +42,15 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 TEST(SM90_Device_Gemm_f64t_f64n_f64t_tensor_op_f64, 32x32x16_16x16x16_16x8x4) {
 
   using ElementOutput = double;
   using ElementAccumulator = double;
@@ -216,8 +216,8 @@
     3
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // if (CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#endif // if (CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_grouped_scheduler_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_grouped_scheduler_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_grouped_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_grouped_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_wmma_tensor_op_s32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_wmma_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sparse_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sparse_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_wmma_tensor_op_s32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_wmma_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_wmma_tensor_op_s32_sm72.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_wmma_tensor_op_s32_sm72.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sparse_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sparse_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_wmma_tensor_op_s32_sm72.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_wmma_tensor_op_s32_sm72.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_wmma_tensor_op_s32_sm72.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_wmma_tensor_op_s32_sm72.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_wmma_tensor_op_s32_sm72.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_wmma_tensor_op_s32_sm72.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_serial_tensor_op_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_splitk_serial_tensor_op_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_simt_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_splitk_simt_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32n_tf32n_f32t_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_tf32n_tf32n_f32t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32n_tf32t_f32t_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_tf32n_tf32t_f32t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32t_tf32n_f32t_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_tf32t_tf32n_f32t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32t_tf32t_f32t_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_tf32t_tf32t_f32t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_u8t_u8n_s32t_wmma_tensor_op_s32_sm72.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_u8t_u8n_s32t_wmma_tensor_op_s32_sm72.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf32n_cf32n_cf32n_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_universal_cf32n_cf32n_cf32n_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32n_tensor_op_f32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32n_tensor_op_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32t_tensor_op_f32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32t_tensor_op_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_broadcast_f16n_f16n_f16n_tensorop_f32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_with_broadcast_f16n_f16n_f16n_tensorop_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_reduction_f16n_f16n_f16n_tensorop_f32_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_with_reduction_f16n_f16n_f16n_tensorop_f32_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_reduction_f16t_f16n_f16n_tensorop_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemm_with_reduction_f16t_f16n_f16n_tensorop_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/gemv.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/gemv.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_ls_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_rs_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_rs_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_ls_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_rs_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_rs_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64_cf64_cf64_tensor_op_f64_sm90.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/hemm_cf64_cf64_cf64_tensor_op_f64_sm90.cu`

 * *Files 2% similar despite different names*

```diff
@@ -44,15 +44,15 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_symm_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 TEST(SM90_Device_Hemm_cf64h_cf64n_ls_l_tensor_op_f64_gaussian, 32x32x16_16x16x16) {
 
   using ElementOutput = cutlass::complex<double>;
   using ElementAccumulator = cutlass::complex<double>;
@@ -128,8 +128,8 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllSymmUniversal<Hemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_rs_f64_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_rs_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_fast_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_fast_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64_cf64_tensor_op_f64_sm90.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_sm80.cu`

 * *Files 2% similar despite different names*

```diff
@@ -42,19 +42,19 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_rank2k_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Her2k_cf64n_cf64n_l_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM80_Device_Her2k_cf64n_cf64n_l_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::ColumnMajor;
 
   using ElementB = cutlass::complex<double>;
   using LayoutB = cutlass::layout::ColumnMajor;
 
@@ -68,18 +68,18 @@
     ElementB,
     LayoutB,
     ElementC,
     LayoutC,
     cutlass::FillMode::kLower,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm90,
+    cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<32, 32, 16>,
     cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<16, 8, 4>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
@@ -94,15 +94,15 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllRank2KHermitianUniversal<Rank2K>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Her2k_cf64c_cf64n_u_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM80_Device_Her2k_cf64h_cf64n_u_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::RowMajor;
 
   using ElementB = cutlass::complex<double>;
   using LayoutB = cutlass::layout::RowMajor;
 
@@ -116,18 +116,18 @@
     ElementB,
     LayoutB,
     ElementC,
     LayoutC,
     cutlass::FillMode::kUpper,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm90,
+    cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<32, 32, 16>,
     cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<16, 8, 4>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
@@ -142,8 +142,8 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllRank2KHermitianUniversal<Rank2K>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64h_cf64n_tensor_op_f64_grouped_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/her2k_cf64h_cf64n_tensor_op_f64_grouped_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/her2k_cf64_cf64_tensor_op_f64_sm90.cu`

 * *Files 3% similar despite different names*

```diff
@@ -42,19 +42,19 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_rank2k_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Her2k_cf64n_cf64n_l_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM90_Device_Her2k_cf64n_cf64n_l_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::ColumnMajor;
 
   using ElementB = cutlass::complex<double>;
   using LayoutB = cutlass::layout::ColumnMajor;
 
@@ -68,18 +68,18 @@
     ElementB,
     LayoutB,
     ElementC,
     LayoutC,
     cutlass::FillMode::kLower,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
+    cutlass::arch::Sm90,
     cutlass::gemm::GemmShape<32, 32, 16>,
     cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::gemm::GemmShape<16, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
@@ -94,15 +94,15 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllRank2KHermitianUniversal<Rank2K>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Her2k_cf64h_cf64n_u_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM90_Device_Her2k_cf64c_cf64n_u_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::RowMajor;
 
   using ElementB = cutlass::complex<double>;
   using LayoutB = cutlass::layout::RowMajor;
 
@@ -116,18 +116,18 @@
     ElementB,
     LayoutB,
     ElementC,
     LayoutC,
     cutlass::FillMode::kUpper,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
+    cutlass::arch::Sm90,
     cutlass::gemm::GemmShape<32, 32, 16>,
     cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::gemm::GemmShape<16, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
@@ -142,8 +142,8 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllRank2KHermitianUniversal<Rank2K>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64t_tensor_op_f64_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/her2k_cf64n_cf64t_tensor_op_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_fast_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_fast_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf64_cf64_tensor_op_f64_sm90.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu`

 * *Files 5% similar despite different names*

```diff
@@ -25,15 +25,16 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide HERK interface
+    \brief Tests for device-wide SYRK interface
+  
 */
 
 #include <iostream>
 
 #include "../../common/cutlass_unit_test.h"
 #include "cutlass/blas3.h"
 #include "cutlass/gemm/device/rank_k.h"
@@ -42,52 +43,53 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_rank_k_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-// HERK operator on CUBLAS_OP_C (row-major + conj) input layouts
-TEST(SM90_Device_Herk_cf64h_cf64n_l_tensor_op_f64, 64x64x16_32x32x16) {
+
+TEST(SM80_Device_Syrk_cf64n_cf64t_l_tensor_op_f64_gaussian, 32x32x16_16x16x16) {
 
   using ElementA = cutlass::complex<double>;
-  using LayoutA = cutlass::layout::RowMajor;
+  using LayoutA = cutlass::layout::ColumnMajor;
 
   using ElementC = cutlass::complex<double>;
-  using LayoutC = cutlass::layout::ColumnMajor;
+  using LayoutC = cutlass::layout::RowMajor;
   using ElementAccumulator = cutlass::complex<double>;
 
   using RankK = cutlass::gemm::device::RankK<
     ElementA,
     LayoutA,
     ElementC,
     LayoutC,
     cutlass::FillMode::kLower,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm90,
+    cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<32, 32, 16>,
     cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<16, 8, 4>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     4,     // kStages 
     1,     // AlignmentA
     false, // SplitKSerial
-    cutlass::arch::OpMultiplyAddComplex,
-    cutlass::ComplexTransform::kConjugate,
-    cutlass::BlasMode::kHermitian
+    cutlass::arch::OpMultiplyAddGaussianComplex,
+    cutlass::ComplexTransform::kNone,
+    cutlass::BlasMode::kSymmetric
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf64h_cf64n_tensor_op_f64_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/herk_cf64h_cf64n_tensor_op_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/multistage_testbed.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/multistage_testbed.h`

 * *Files 3% similar despite different names*

```diff
@@ -54,14 +54,19 @@
 namespace gemm {
 namespace device {
 
 ////////////////////////////////////////////////////////////////////////////////
 
 template <typename Gemm>
 struct MultistageTestbed {
+
+  using ElementA = typename Gemm::ElementA;
+  using ElementB = typename Gemm::ElementB;
+  using ElementC = typename Gemm::ElementC;
+
   using ElementAccumulator = typename Gemm::ElementAccumulator;
   using ElementCompute =
       typename Gemm::GemmKernel::Epilogue::OutputOp::ElementCompute;
 
   /// Initialization
   cutlass::Distribution::Kind init_A;
   cutlass::Distribution::Kind init_B;
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/multistage_testbed_interleaved.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/multistage_testbed_interleaved.h`

 * *Files 4% similar despite different names*

```diff
@@ -55,14 +55,17 @@
 namespace device {
 
 ////////////////////////////////////////////////////////////////////////////////
 
 template <typename Gemm, int InterleavedK>
 struct MultistageInterleavedTestbed {
 
+  using ElementA = typename Gemm::ElementA;
+  using ElementB = typename Gemm::ElementB;
+  using ElementC = typename Gemm::ElementC;
   using ElementAccumulator = typename Gemm::ElementAccumulator;
   using ElementCompute = typename Gemm::GemmKernel::Epilogue::OutputOp::ElementCompute;
 
   /// Initialization
   cutlass::Distribution::Kind init_A;
   cutlass::Distribution::Kind init_B;
   cutlass::Distribution::Kind init_C;
@@ -106,20 +109,57 @@
       EXPECT_TRUE(false) << "Not implemented";
       return false;
     }
 
     return true;
   }
 
+  /// Returns true if the CUDA device is sufficient to execute the kernel.
+  bool sufficient() const {
+    //
+    // Determine SMEM requirements and waive if not satisfied
+    //
+
+    int smem_size = int(sizeof(typename Gemm::GemmKernel::SharedStorage));
+
+    cudaDeviceProp properties;
+    int device_idx;
+    cudaError_t result = cudaGetDevice(&device_idx);
+
+    if (result != cudaSuccess) {
+      throw std::runtime_error("cudaGetDevice() API call failed.");
+    }
+
+    result = cudaGetDeviceProperties(&properties, device_idx);
+
+    if (result != cudaSuccess) {
+      throw std::runtime_error("cudaGetDeviceProperties() failed");
+    }
+
+    if (properties.sharedMemPerMultiprocessor < smem_size) {
+      return false;
+    }
+
+    return true;
+  }
+
   /// Executes one test
   bool run(
     cutlass::gemm::GemmCoord problem_size, 
     ElementCompute alpha = ElementCompute(1), 
     ElementCompute beta = ElementCompute(0)) {
     
+    // Waive test if insufficient CUDA device
+    if (!sufficient()) {
+      if (CUTLASS_TEST_UNIT_ENABLE_WARNINGS) {
+        std::cerr << "Test waived due to insufficient CUDA device." << std::endl;
+      }
+      return true;
+    }
+
     //
     // Allocate the GEMM workspace
     //
 
     cutlass::HostTensor<
       typename Gemm::ElementA, 
       typename Gemm::LayoutA> tensor_A(problem_size.mk());
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/rank_2k_grouped_scheduler_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/rank_2k_grouped_scheduler_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nn_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_cgemm_nn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tt_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_cgemm_tt_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_nn_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_dgemm_nn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_nt_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_dgemm_nt_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_tn_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_dgemm_tn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_tt_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_dgemm_tt_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_f8gemm_tn_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_f8gemm_tn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_nn_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_hgemm_nn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_nt_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_hgemm_nt_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_tn_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_hgemm_tn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_tt_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_hgemm_tt_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_nn_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_igemm_nn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_nt_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_igemm_nt_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_tn_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_igemm_tn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_tt_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_igemm_tt_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_perf.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_perf.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_sliced_k.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_sliced_k.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_nn_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_qgemm_nn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_nt_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_qgemm_nt_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_tn_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_qgemm_tn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_tt_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_qgemm_tt_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nn_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_sgemm_nn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tt_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_sgemm_tt_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_nn_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_zgemm_nn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_nt_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_zgemm_nt_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_tn_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_zgemm_tn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_tt_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/simt_zgemm_tt_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_ls_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_rs_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_rs_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_ls_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_rs_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_rs_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64_cf64_cf64_tensor_op_f64_sm90.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_cf64_cf64_cf64_tensor_op_f64_sm90.cu`

 * *Files 2% similar despite different names*

```diff
@@ -44,15 +44,15 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_symm_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 TEST(SM90_Device_Symm_cf64n_cf64n_ls_l_tensor_op_f64_gaussian, 32x32x16_16x16x16) {
 
   using ElementOutput = cutlass::complex<double>;
   using ElementAccumulator = cutlass::complex<double>;
@@ -126,8 +126,8 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllSymmUniversal<Symm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_rs_f64_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_rs_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_rs_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_rs_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64_f64_tensor_op_f64_sm90.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_f64_f64_tensor_op_f64_sm90.cu`

 * *Files 2% similar despite different names*

```diff
@@ -43,15 +43,15 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_symm_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 TEST(SM90_Device_Symm_f64n_f64n_rs_l_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = double;
   using LayoutA = cutlass::layout::ColumnMajor;
@@ -128,8 +128,8 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllSymmUniversal<Symm>());
 
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_ls_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_rs_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_rs_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_ls_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_rs_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_rs_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_ls_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_rs_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_rs_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_ls_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_rs_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_rs_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_ls_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_rs_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_rs_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32t_f32t_tensor_op_f32_ls_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/symm_tf32t_f32t_tensor_op_f32_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_fast_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_fast_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_fast_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_fast_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64_cf64_tensor_op_f64_sm90.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_cf64_cf64_tensor_op_f64_sm90.cu`

 * *Files 2% similar despite different names*

```diff
@@ -43,15 +43,15 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_rank2k_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 TEST(SM90_Device_Syr2k_cf64n_cf64n_l_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::ColumnMajor;
@@ -143,8 +143,8 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_grouped_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_grouped_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64n_tensor_op_f64_grouped_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64n_tensor_op_f64_grouped_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64t_tensor_op_f64_grouped_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64t_tensor_op_f64_grouped_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f32n_f32n_tensor_op_fast_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_f32n_f32n_tensor_op_fast_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f32t_f32n_tensor_op_fast_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_f32t_f32n_tensor_op_fast_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64_f64_tensor_op_f64_sm90.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_f64_f64_tensor_op_f64_sm90.cu`

 * *Files 2% similar despite different names*

```diff
@@ -43,15 +43,15 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_rank2k_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 TEST(SM90_Device_Syr2k_f64n_f64n_l_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = double;
   using LayoutA = cutlass::layout::ColumnMajor;
@@ -127,8 +127,8 @@
 
   EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_grouped_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_grouped_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -47,22 +47,22 @@
 
 #include "testbed_rank2k_universal.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_f64n_f64n_l_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM80_Device_Syr2k_f64n_f64t_l_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = double;
   using LayoutA = cutlass::layout::ColumnMajor;
   using ElementB = double;
   using LayoutB = cutlass::layout::ColumnMajor;
   using ElementC = double;
-  using LayoutC = cutlass::layout::ColumnMajor;
+  using LayoutC = cutlass::layout::RowMajor;
   using ElementAccumulator = double;
 
   using Rank2K = cutlass::gemm::device::Rank2K<
     ElementA,
     LayoutA,
     ElementB,
     LayoutB,
@@ -87,22 +87,22 @@
 
   EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_f64n_f64n_l_tensor_op_f64, 64x64x16_32x32x16) {
+TEST(SM80_Device_Syr2k_f64n_f64t_l_tensor_op_f64, 64x64x16_32x32x16) {
 
   using ElementA = double;
   using LayoutA = cutlass::layout::ColumnMajor;
   using ElementB = double;
   using LayoutB = cutlass::layout::ColumnMajor;
   using ElementC = double;
-  using LayoutC = cutlass::layout::ColumnMajor;
+  using LayoutC = cutlass::layout::RowMajor;
   using ElementAccumulator = double;
 
   using Rank2K = cutlass::gemm::device::Rank2K<
     ElementA,
     LayoutA,
     ElementB,
     LayoutB,
@@ -127,22 +127,22 @@
 
   EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_f64n_f64n_l_tensor_op_f64, 128x64x16_64x32x16) {
+TEST(SM80_Device_Syr2k_f64n_f64t_l_tensor_op_f64, 128x64x16_64x32x16) {
 
   using ElementA = double;
   using LayoutA = cutlass::layout::ColumnMajor;
   using ElementB = double;
   using LayoutB = cutlass::layout::ColumnMajor;
   using ElementC = double;
-  using LayoutC = cutlass::layout::ColumnMajor;
+  using LayoutC = cutlass::layout::RowMajor;
   using ElementAccumulator = double;
 
   using Rank2K = cutlass::gemm::device::Rank2K<
     ElementA,
     LayoutA,
     ElementB,
     LayoutB,
@@ -167,22 +167,22 @@
 
   EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_f64n_f64n_l_tensor_op_f64, 128x128x16_32x64x16) {
+TEST(SM80_Device_Syr2k_f64n_f64t_l_tensor_op_f64, 128x128x16_32x64x16) {
 
   using ElementA = double;
   using LayoutA = cutlass::layout::ColumnMajor;
   using ElementB = double;
   using LayoutB = cutlass::layout::ColumnMajor;
   using ElementC = double;
-  using LayoutC = cutlass::layout::ColumnMajor;
+  using LayoutC = cutlass::layout::RowMajor;
   using ElementAccumulator = double;
 
   using Rank2K = cutlass::gemm::device::Rank2K<
     ElementA,
     LayoutA,
     ElementB,
     LayoutB,
@@ -207,22 +207,22 @@
 
   EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_f64n_f64n_u_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM80_Device_Syr2k_f64n_f64t_u_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = double;
   using LayoutA = cutlass::layout::ColumnMajor;
   using ElementB = double;
   using LayoutB = cutlass::layout::ColumnMajor;
   using ElementC = double;
-  using LayoutC = cutlass::layout::ColumnMajor;
+  using LayoutC = cutlass::layout::RowMajor;
   using ElementAccumulator = double;
 
   using Rank2K = cutlass::gemm::device::Rank2K<
     ElementA,
     LayoutA,
     ElementB,
     LayoutB,
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_grouped_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_grouped_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_sm80.cu`

 * *Files 2% similar despite different names*

```diff
@@ -47,22 +47,22 @@
 
 #include "testbed_rank2k_universal.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_f64n_f64t_l_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM80_Device_Syr2k_f64t_f64n_l_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = double;
-  using LayoutA = cutlass::layout::ColumnMajor;
+  using LayoutA = cutlass::layout::RowMajor;
   using ElementB = double;
-  using LayoutB = cutlass::layout::ColumnMajor;
+  using LayoutB = cutlass::layout::RowMajor;
   using ElementC = double;
-  using LayoutC = cutlass::layout::RowMajor;
+  using LayoutC = cutlass::layout::ColumnMajor;
   using ElementAccumulator = double;
 
   using Rank2K = cutlass::gemm::device::Rank2K<
     ElementA,
     LayoutA,
     ElementB,
     LayoutB,
@@ -87,22 +87,22 @@
 
   EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_f64n_f64t_l_tensor_op_f64, 64x64x16_32x32x16) {
+TEST(SM80_Device_Syr2k_f64t_f64n_l_tensor_op_f64, 64x64x16_32x32x16) {
 
   using ElementA = double;
-  using LayoutA = cutlass::layout::ColumnMajor;
+  using LayoutA = cutlass::layout::RowMajor;
   using ElementB = double;
-  using LayoutB = cutlass::layout::ColumnMajor;
+  using LayoutB = cutlass::layout::RowMajor;
   using ElementC = double;
-  using LayoutC = cutlass::layout::RowMajor;
+  using LayoutC = cutlass::layout::ColumnMajor;
   using ElementAccumulator = double;
 
   using Rank2K = cutlass::gemm::device::Rank2K<
     ElementA,
     LayoutA,
     ElementB,
     LayoutB,
@@ -127,22 +127,22 @@
 
   EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_f64n_f64t_l_tensor_op_f64, 128x64x16_64x32x16) {
+TEST(SM80_Device_Syr2k_f64t_f64n_l_tensor_op_f64, 128x64x16_64x32x16) {
 
   using ElementA = double;
-  using LayoutA = cutlass::layout::ColumnMajor;
+  using LayoutA = cutlass::layout::RowMajor;
   using ElementB = double;
-  using LayoutB = cutlass::layout::ColumnMajor;
+  using LayoutB = cutlass::layout::RowMajor;
   using ElementC = double;
-  using LayoutC = cutlass::layout::RowMajor;
+  using LayoutC = cutlass::layout::ColumnMajor;
   using ElementAccumulator = double;
 
   using Rank2K = cutlass::gemm::device::Rank2K<
     ElementA,
     LayoutA,
     ElementB,
     LayoutB,
@@ -167,22 +167,22 @@
 
   EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_f64n_f64t_l_tensor_op_f64, 128x128x16_32x64x16) {
+TEST(SM80_Device_Syr2k_f64t_f64n_l_tensor_op_f64, 128x128x16_32x64x16) {
 
   using ElementA = double;
-  using LayoutA = cutlass::layout::ColumnMajor;
+  using LayoutA = cutlass::layout::RowMajor;
   using ElementB = double;
-  using LayoutB = cutlass::layout::ColumnMajor;
+  using LayoutB = cutlass::layout::RowMajor;
   using ElementC = double;
-  using LayoutC = cutlass::layout::RowMajor;
+  using LayoutC = cutlass::layout::ColumnMajor;
   using ElementAccumulator = double;
 
   using Rank2K = cutlass::gemm::device::Rank2K<
     ElementA,
     LayoutA,
     ElementB,
     LayoutB,
@@ -207,22 +207,22 @@
 
   EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
 
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_f64n_f64t_u_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM80_Device_Syr2k_f64t_f64n_u_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = double;
-  using LayoutA = cutlass::layout::ColumnMajor;
+  using LayoutA = cutlass::layout::RowMajor;
   using ElementB = double;
-  using LayoutB = cutlass::layout::ColumnMajor;
+  using LayoutB = cutlass::layout::RowMajor;
   using ElementC = double;
-  using LayoutC = cutlass::layout::RowMajor;
+  using LayoutC = cutlass::layout::ColumnMajor;
   using ElementAccumulator = double;
 
   using Rank2K = cutlass::gemm::device::Rank2K<
     ElementA,
     LayoutA,
     ElementB,
     LayoutB,
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_grouped_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_grouped_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syrk_f64t_f64n_tensor_op_f64_sm80.cu`

 * *Files 14% similar despite different names*

```diff
@@ -33,43 +33,39 @@
   
 */
 
 #include <iostream>
 
 #include "../../common/cutlass_unit_test.h"
 #include "cutlass/blas3.h"
-#include "cutlass/gemm/device/rank_2k.h"
+#include "cutlass/gemm/device/rank_k.h"
 #include "cutlass/util/host_tensor.h"
-#include "cutlass/util/reference/host/rank_2k.h"
+#include "cutlass/util/reference/host/rank_k_complex.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
-#include "testbed_rank2k_universal.h"
+#include "testbed_rank_k_universal.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_f64t_f64n_l_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM80_Device_Syrk_f64t_f64n_l_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = double;
   using LayoutA = cutlass::layout::RowMajor;
-  using ElementB = double;
-  using LayoutB = cutlass::layout::RowMajor;
   using ElementC = double;
   using LayoutC = cutlass::layout::ColumnMajor;
   using ElementAccumulator = double;
 
-  using Rank2K = cutlass::gemm::device::Rank2K<
+  using RankK = cutlass::gemm::device::RankK<
     ElementA,
     LayoutA,
-    ElementB,
-    LayoutB,
     ElementC,
     LayoutC,
     cutlass::FillMode::kLower,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<32, 32, 16>,
@@ -81,35 +77,30 @@
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     4
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
-
+  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_f64t_f64n_l_tensor_op_f64, 64x64x16_32x32x16) {
+TEST(SM80_Device_Syrk_f64t_f64n_l_tensor_op_f64, 64x64x16_32x32x16) {
 
   using ElementA = double;
   using LayoutA = cutlass::layout::RowMajor;
-  using ElementB = double;
-  using LayoutB = cutlass::layout::RowMajor;
   using ElementC = double;
   using LayoutC = cutlass::layout::ColumnMajor;
   using ElementAccumulator = double;
 
-  using Rank2K = cutlass::gemm::device::Rank2K<
+  using RankK = cutlass::gemm::device::RankK<
     ElementA,
     LayoutA,
-    ElementB,
-    LayoutB,
     ElementC,
     LayoutC,
     cutlass::FillMode::kLower,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<64, 64, 16>,
@@ -121,35 +112,30 @@
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     4
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
-
+  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_f64t_f64n_l_tensor_op_f64, 128x64x16_64x32x16) {
+TEST(SM80_Device_Syrk_f64t_f64n_l_tensor_op_f64, 128x64x16_64x32x16) {
 
   using ElementA = double;
   using LayoutA = cutlass::layout::RowMajor;
-  using ElementB = double;
-  using LayoutB = cutlass::layout::RowMajor;
   using ElementC = double;
   using LayoutC = cutlass::layout::ColumnMajor;
   using ElementAccumulator = double;
 
-  using Rank2K = cutlass::gemm::device::Rank2K<
+  using RankK = cutlass::gemm::device::RankK<
     ElementA,
     LayoutA,
-    ElementB,
-    LayoutB,
     ElementC,
     LayoutC,
     cutlass::FillMode::kLower,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<128, 64, 16>,
@@ -161,35 +147,30 @@
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     4
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
-
+  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_f64t_f64n_l_tensor_op_f64, 128x128x16_32x64x16) {
+TEST(SM80_Device_Syrk_f64t_f64n_l_tensor_op_f64, 128x128x16_32x64x16) {
 
   using ElementA = double;
   using LayoutA = cutlass::layout::RowMajor;
-  using ElementB = double;
-  using LayoutB = cutlass::layout::RowMajor;
   using ElementC = double;
   using LayoutC = cutlass::layout::ColumnMajor;
   using ElementAccumulator = double;
 
-  using Rank2K = cutlass::gemm::device::Rank2K<
+  using RankK = cutlass::gemm::device::RankK<
     ElementA,
     LayoutA,
-    ElementB,
-    LayoutB,
     ElementC,
     LayoutC,
     cutlass::FillMode::kLower,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<128, 128, 16>,
@@ -201,35 +182,30 @@
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     3
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
-
+  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syr2k_f64t_f64n_u_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM80_Device_Syrk_f64t_f64n_u_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = double;
   using LayoutA = cutlass::layout::RowMajor;
-  using ElementB = double;
-  using LayoutB = cutlass::layout::RowMajor;
   using ElementC = double;
   using LayoutC = cutlass::layout::ColumnMajor;
   using ElementAccumulator = double;
 
-  using Rank2K = cutlass::gemm::device::Rank2K<
+  using RankK = cutlass::gemm::device::RankK<
     ElementA,
     LayoutA,
-    ElementB,
-    LayoutB,
     ElementC,
     LayoutC,
     cutlass::FillMode::kUpper,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<32, 32, 16>,
@@ -241,13 +217,85 @@
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     4
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllRank2KUniversal<Rank2K>());
+  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
+}
+
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM80_Device_Syrk_f64t_f64n_u_tensor_op_f64, 128x64x16_64x32x16) {
+
+  using ElementA = double;
+  using LayoutA = cutlass::layout::RowMajor;
+  using ElementC = double;
+  using LayoutC = cutlass::layout::ColumnMajor;
+  using ElementAccumulator = double;
+
+  using RankK = cutlass::gemm::device::RankK<
+    ElementA,
+    LayoutA,
+    ElementC,
+    LayoutC,
+    cutlass::FillMode::kUpper,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<128, 64, 16>,
+    cutlass::gemm::GemmShape<64, 32, 16>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementC,
+      1,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    4
+  >;
+
+  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
+}
+
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM80_Device_Syrk_f64t_f64n_u_tensor_op_f64, 128x128x16_32x64x16) {
+
+  using ElementA = double;
+  using LayoutA = cutlass::layout::RowMajor;
+  using ElementC = double;
+  using LayoutC = cutlass::layout::ColumnMajor;
+  using ElementAccumulator = double;
 
+  using RankK = cutlass::gemm::device::RankK<
+    ElementA,
+    LayoutA,
+    ElementC,
+    LayoutC,
+    cutlass::FillMode::kUpper,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm80,
+    cutlass::gemm::GemmShape<128, 128, 16>,
+    cutlass::gemm::GemmShape<32, 64, 16>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementC,
+      1,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    3
+  >;
+
+  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
+
 #endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64t_tensor_op_f64_grouped_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_f64t_f64t_tensor_op_f64_grouped_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_tf32n_f32n_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_tf32n_f32n_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_tf32t_f32n_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syr2k_tf32t_f32n_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_fast_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_fast_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_fast_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_fast_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64_cf64_tensor_op_f64_sm90.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syrk_cf64n_cf64n_tensor_op_f64_sm80.cu`

 * *Files 2% similar despite different names*

```diff
@@ -43,19 +43,19 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_rank_k_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Syrk_cf64n_cf64n_l_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM80_Device_Syrk_cf64n_cf64n_l_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::ColumnMajor;
 
   using ElementC = cutlass::complex<double>;
   using LayoutC = cutlass::layout::ColumnMajor;
   using ElementAccumulator = cutlass::complex<double>;
@@ -64,18 +64,18 @@
     ElementA,
     LayoutA,
     ElementC,
     LayoutC,
     cutlass::FillMode::kLower,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm90,
+    cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<32, 32, 16>,
     cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<16, 8, 4>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
@@ -88,49 +88,49 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM90_Device_Syrk_cf64n_cf64t_l_tensor_op_f64_gaussian, 32x32x16_16x16x16) {
+TEST(SM80_Device_Syrk_cf64n_cf64n_u_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::ColumnMajor;
 
   using ElementC = cutlass::complex<double>;
-  using LayoutC = cutlass::layout::RowMajor;
+  using LayoutC = cutlass::layout::ColumnMajor;
   using ElementAccumulator = cutlass::complex<double>;
 
   using RankK = cutlass::gemm::device::RankK<
     ElementA,
     LayoutA,
     ElementC,
     LayoutC,
-    cutlass::FillMode::kLower,
+    cutlass::FillMode::kUpper,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm90,
+    cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<32, 32, 16>,
     cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<16, 8, 4>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     4,     // kStages 
     1,     // AlignmentA
     false, // SplitKSerial
-    cutlass::arch::OpMultiplyAddGaussianComplex,
+    cutlass::arch::OpMultiplyAddComplex,
     cutlass::ComplexTransform::kNone,
     cutlass::BlasMode::kSymmetric
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64n_tensor_op_f64_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_sm80.cu`

 * *Files 2% similar despite different names*

```diff
@@ -47,21 +47,21 @@
 
 #include "testbed_rank_k_universal.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syrk_cf64n_cf64n_l_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM80_Device_Syrk_cf64n_cf64t_l_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::ColumnMajor;
 
   using ElementC = cutlass::complex<double>;
-  using LayoutC = cutlass::layout::ColumnMajor;
+  using LayoutC = cutlass::layout::RowMajor;
   using ElementAccumulator = cutlass::complex<double>;
 
   using RankK = cutlass::gemm::device::RankK<
     ElementA,
     LayoutA,
     ElementC,
     LayoutC,
@@ -88,21 +88,21 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syrk_cf64n_cf64n_u_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM80_Device_Syrk_cf64n_cf64t_u_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::ColumnMajor;
 
   using ElementC = cutlass::complex<double>;
-  using LayoutC = cutlass::layout::ColumnMajor;
+  using LayoutC = cutlass::layout::RowMajor;
   using ElementAccumulator = cutlass::complex<double>;
 
   using RankK = cutlass::gemm::device::RankK<
     ElementA,
     LayoutA,
     ElementC,
     LayoutC,
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/sm80_gemm_f64_f64_f64_tensor_op_f64.cu`

 * *Files 17% similar despite different names*

```diff
@@ -25,71 +25,74 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide SYRK interface
-  
+    \brief Tests for device-wide GEMM interface
 */
 
 #include <iostream>
 
+#include "cutlass/cutlass.h"
+#include "cute/tensor.hpp"
+#include "cute/atom/mma_atom.hpp"
+
+#include "cutlass/numeric_types.h"
+
+#include "cutlass/gemm/device/gemm_universal_adapter.h"
+#include "default_gemm_configuration.hpp"
+
 #include "../../common/cutlass_unit_test.h"
-#include "cutlass/blas3.h"
-#include "cutlass/gemm/device/rank_k.h"
-#include "cutlass/util/host_tensor.h"
-#include "cutlass/util/reference/host/rank_k_complex.h"
-#include "cutlass/util/reference/host/tensor_compare.h"
-#include "cutlass/util/reference/host/tensor_copy.h"
-#include "cutlass/util/reference/host/tensor_fill.h"
-#include "cutlass/util/tensor_view_io.h"
 
-#include "testbed_rank_k_universal.h"
+#include "gemm_testbed_3x.hpp"
 
-#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+using namespace cute;
+
+//#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syrk_cf64n_cf64t_l_tensor_op_f64_gaussian, 32x32x16_16x16x16) {
+TEST(SM80_Device_Gemm_f64n_f64t_f64n_tensor_op_f64, 128x128x64_64x64x64) {
+
+  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
+    cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
+    double, cutlass::layout::ColumnMajor,
+    double, cutlass::layout::ColumnMajor,
+    double, cutlass::layout::ColumnMajor,
+    double>;
+
+  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
+      Shape<int,int,int,int>,
+      Config::CollectiveMainloop,
+      Config::CollectiveEpilogue
+  >;
+
+  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
+  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+}
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-  using ElementA = cutlass::complex<double>;
-  using LayoutA = cutlass::layout::ColumnMajor;
+TEST(SM80_Device_Gemm_f64t_f64n_f64n_tensor_op_f64, 128x128x64_64x64x64) {
 
-  using ElementC = cutlass::complex<double>;
-  using LayoutC = cutlass::layout::RowMajor;
-  using ElementAccumulator = cutlass::complex<double>;
-
-  using RankK = cutlass::gemm::device::RankK<
-    ElementA,
-    LayoutA,
-    ElementC,
-    LayoutC,
-    cutlass::FillMode::kLower,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<32, 32, 16>,
-    cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
-    cutlass::epilogue::thread::LinearCombination<
-      ElementC,
-      1,
-      ElementAccumulator,
-      ElementAccumulator
-    >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    4,     // kStages 
-    1,     // AlignmentA
-    false, // SplitKSerial
-    cutlass::arch::OpMultiplyAddGaussianComplex,
-    cutlass::ComplexTransform::kNone,
-    cutlass::BlasMode::kSymmetric
+  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
+    cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
+    double, cutlass::layout::RowMajor,
+    double, cutlass::layout::ColumnMajor,
+    double, cutlass::layout::ColumnMajor,
+    double>;
+
+  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
+      Shape<int,int,int,int>,
+      Config::CollectiveMainloop,
+      Config::CollectiveEpilogue
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
+  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
+  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+// #endif
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syrk_cf64_cf64_tensor_op_f64_sm90.cu`

 * *Files 3% similar despite different names*

```diff
@@ -43,39 +43,39 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_rank_k_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syrk_cf64n_cf64t_l_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM90_Device_Syrk_cf64n_cf64n_l_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::ColumnMajor;
 
   using ElementC = cutlass::complex<double>;
-  using LayoutC = cutlass::layout::RowMajor;
+  using LayoutC = cutlass::layout::ColumnMajor;
   using ElementAccumulator = cutlass::complex<double>;
 
   using RankK = cutlass::gemm::device::RankK<
     ElementA,
     LayoutA,
     ElementC,
     LayoutC,
     cutlass::FillMode::kLower,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
+    cutlass::arch::Sm90,
     cutlass::gemm::GemmShape<32, 32, 16>,
     cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::gemm::GemmShape<16, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
@@ -88,49 +88,49 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syrk_cf64n_cf64t_u_tensor_op_f64, 32x32x16_16x16x16) {
+TEST(SM90_Device_Syrk_cf64n_cf64t_l_tensor_op_f64_gaussian, 32x32x16_16x16x16) {
 
   using ElementA = cutlass::complex<double>;
   using LayoutA = cutlass::layout::ColumnMajor;
 
   using ElementC = cutlass::complex<double>;
   using LayoutC = cutlass::layout::RowMajor;
   using ElementAccumulator = cutlass::complex<double>;
 
   using RankK = cutlass::gemm::device::RankK<
     ElementA,
     LayoutA,
     ElementC,
     LayoutC,
-    cutlass::FillMode::kUpper,
+    cutlass::FillMode::kLower,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
+    cutlass::arch::Sm90,
     cutlass::gemm::GemmShape<32, 32, 16>,
     cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::gemm::GemmShape<16, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     4,     // kStages 
     1,     // AlignmentA
     false, // SplitKSerial
-    cutlass::arch::OpMultiplyAddComplex,
+    cutlass::arch::OpMultiplyAddGaussianComplex,
     cutlass::ComplexTransform::kNone,
     cutlass::BlasMode::kSymmetric
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f32n_f32t_tensor_op_fast_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syrk_f32n_f32t_tensor_op_fast_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f32t_f32t_tensor_op_fast_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syrk_f32t_f32t_tensor_op_fast_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64_f64_tensor_op_f64_sm90.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syrk_f64_f64_tensor_op_f64_sm90.cu`

 * *Files 2% similar despite different names*

```diff
@@ -43,15 +43,15 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_rank_k_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 TEST(SM90_Device_Syrk_f64n_f64t_l_tensor_op_f64, 128x64x16_64x32x16) {
 
   using ElementA = double;
   using LayoutA = cutlass::layout::ColumnMajor;
@@ -119,8 +119,8 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64n_f64t_tensor_op_f64_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syrk_f64n_f64t_tensor_op_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64t_f64n_tensor_op_f64_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_rs_sm80.cu`

 * *Files 19% similar despite different names*

```diff
@@ -25,277 +25,228 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide SYRK interface
+    \brief Tests for device-wide TRMM interface
+
   
 */
 
 #include <iostream>
 
 #include "../../common/cutlass_unit_test.h"
 #include "cutlass/blas3.h"
-#include "cutlass/gemm/device/rank_k.h"
+#include "cutlass/gemm/device/trmm.h"
 #include "cutlass/util/host_tensor.h"
-#include "cutlass/util/reference/host/rank_k_complex.h"
+#include "cutlass/util/reference/host/trmm.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
-#include "testbed_rank_k_universal.h"
+#include "testbed_trmm_universal.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-TEST(SM80_Device_Syrk_f64t_f64n_l_tensor_op_f64, 32x32x16_16x16x16) {
-
-  using ElementA = double;
-  using LayoutA = cutlass::layout::RowMajor;
-  using ElementC = double;
-  using LayoutC = cutlass::layout::ColumnMajor;
-  using ElementAccumulator = double;
-
-  using RankK = cutlass::gemm::device::RankK<
-    ElementA,
-    LayoutA,
-    ElementC,
-    LayoutC,
-    cutlass::FillMode::kLower,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<32, 32, 16>,
-    cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
-    cutlass::epilogue::thread::LinearCombination<
-      ElementC,
-      1,
-      ElementAccumulator,
-      ElementAccumulator
-    >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    4
-  >;
-
-  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
-}
+////////////////////////////////////////////Test name//////////////////////////////////////////////////
+//                             
+// SM80_Device_Trmm_{ElementA}{LayoutA}_{ElementB}{LayoutB}_{ElementC}{LayoutC}_{SideMode}_{FillMode}\
+//    _{DiagType}_tensor_op_{ElementAccumulator}_align{AlignmentA}_align{AlignmentB}
+//
+///////////////////////////////////////////////////////////////////////////////////////////////////////
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syrk_f64t_f64n_l_tensor_op_f64, 64x64x16_32x32x16) {
+TEST(SM80_Device_Trmm_f32n_f32t_f32t_rs_u_nu_tensor_op_fast_f32_align1_align1, 64x128x32_32x64x32) {
 
-  using ElementA = double;
-  using LayoutA = cutlass::layout::RowMajor;
-  using ElementC = double;
-  using LayoutC = cutlass::layout::ColumnMajor;
-  using ElementAccumulator = double;
-
-  using RankK = cutlass::gemm::device::RankK<
-    ElementA,
-    LayoutA,
-    ElementC,
-    LayoutC,
-    cutlass::FillMode::kLower,
-    ElementAccumulator,
+using Trmm = cutlass::gemm::device::Trmm<
+    float, cutlass::layout::ColumnMajor,
+    cutlass::SideMode::kRight, cutlass::FillMode::kUpper, cutlass::DiagType::kNonUnit,
+    float, cutlass::layout::RowMajor,
+    float, cutlass::layout::RowMajor,
+    float,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<64, 64, 16>,
-    cutlass::gemm::GemmShape<32, 32, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::gemm::GemmShape<64, 128, 32>,
+    cutlass::gemm::GemmShape<32, 64, 32>,
+    cutlass::gemm::GemmShape<16, 8, 8>,
     cutlass::epilogue::thread::LinearCombination<
-      ElementC,
+      float,
       1,
-      ElementAccumulator,
-      ElementAccumulator
+      float,
+      float
     >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    4
-  >;
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<8>,
+    3,
+    1,
+    1,
+    false,
+    cutlass::arch::OpMultiplyAddFastF32
+>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
+  EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
 }
-
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syrk_f64t_f64n_l_tensor_op_f64, 128x64x16_64x32x16) {
+TEST(SM80_Device_Trmm_f32n_f32t_f32t_rs_u_nu_tensor_op_fast_f32_align1_align1, 128x64x32_32x64x32) {
 
-  using ElementA = double;
-  using LayoutA = cutlass::layout::RowMajor;
-  using ElementC = double;
-  using LayoutC = cutlass::layout::ColumnMajor;
-  using ElementAccumulator = double;
-
-  using RankK = cutlass::gemm::device::RankK<
-    ElementA,
-    LayoutA,
-    ElementC,
-    LayoutC,
-    cutlass::FillMode::kLower,
-    ElementAccumulator,
+using Trmm = cutlass::gemm::device::Trmm<
+    float, cutlass::layout::ColumnMajor,
+    cutlass::SideMode::kRight, cutlass::FillMode::kUpper, cutlass::DiagType::kNonUnit,
+    float, cutlass::layout::RowMajor,
+    float, cutlass::layout::RowMajor,
+    float,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<128, 64, 16>,
-    cutlass::gemm::GemmShape<64, 32, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::gemm::GemmShape<128, 64, 32>,
+    cutlass::gemm::GemmShape<32, 64, 32>,
+    cutlass::gemm::GemmShape<16, 8, 8>,
     cutlass::epilogue::thread::LinearCombination<
-      ElementC,
+      float,
       1,
-      ElementAccumulator,
-      ElementAccumulator
+      float,
+      float
     >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    4
-  >;
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<8>,
+    3,
+    1,
+    1,
+    false,
+    cutlass::arch::OpMultiplyAddFastF32
+>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
+  EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syrk_f64t_f64n_l_tensor_op_f64, 128x128x16_32x64x16) {
+TEST(SM80_Device_Trmm_f32n_f32t_f32t_rs_l_nu_tensor_op_fast_f32_align1_align1, 64x128x32_32x64x32) {
 
-  using ElementA = double;
-  using LayoutA = cutlass::layout::RowMajor;
-  using ElementC = double;
-  using LayoutC = cutlass::layout::ColumnMajor;
-  using ElementAccumulator = double;
-
-  using RankK = cutlass::gemm::device::RankK<
-    ElementA,
-    LayoutA,
-    ElementC,
-    LayoutC,
-    cutlass::FillMode::kLower,
-    ElementAccumulator,
+using Trmm = cutlass::gemm::device::Trmm<
+    float, cutlass::layout::ColumnMajor,
+    cutlass::SideMode::kRight, cutlass::FillMode::kLower, cutlass::DiagType::kNonUnit,
+    float, cutlass::layout::RowMajor,
+    float, cutlass::layout::RowMajor,
+    float,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<128, 128, 16>,
-    cutlass::gemm::GemmShape<32, 64, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::gemm::GemmShape<64, 128, 32>,
+    cutlass::gemm::GemmShape<32, 64, 32>,
+    cutlass::gemm::GemmShape<16, 8, 8>,
     cutlass::epilogue::thread::LinearCombination<
-      ElementC,
+      float,
       1,
-      ElementAccumulator,
-      ElementAccumulator
+      float,
+      float
     >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3
-  >;
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<8>,
+    3,
+    1,
+    1,
+    false,
+    cutlass::arch::OpMultiplyAddFastF32
+>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
+  EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syrk_f64t_f64n_u_tensor_op_f64, 32x32x16_16x16x16) {
 
-  using ElementA = double;
-  using LayoutA = cutlass::layout::RowMajor;
-  using ElementC = double;
-  using LayoutC = cutlass::layout::ColumnMajor;
-  using ElementAccumulator = double;
-
-  using RankK = cutlass::gemm::device::RankK<
-    ElementA,
-    LayoutA,
-    ElementC,
-    LayoutC,
-    cutlass::FillMode::kUpper,
-    ElementAccumulator,
+TEST(SM80_Device_Trmm_f32n_f32t_f32t_rs_u_nu_tensor_op_fast_f32_align1_align4, 64x128x32_32x64x32) {
+
+using Trmm = cutlass::gemm::device::Trmm<
+    float, cutlass::layout::ColumnMajor,
+    cutlass::SideMode::kRight, cutlass::FillMode::kUpper, cutlass::DiagType::kNonUnit,
+    float, cutlass::layout::RowMajor,
+    float, cutlass::layout::RowMajor,
+    float,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<32, 32, 16>,
-    cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::gemm::GemmShape<64, 128, 32>,
+    cutlass::gemm::GemmShape<32, 64, 32>,
+    cutlass::gemm::GemmShape<16, 8, 8>,
     cutlass::epilogue::thread::LinearCombination<
-      ElementC,
+      float,
       1,
-      ElementAccumulator,
-      ElementAccumulator
+      float,
+      float
     >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    4
-  >;
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<8>,
+    3,
+    1,
+    4,
+    false,
+    cutlass::arch::OpMultiplyAddFastF32
+>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
+  EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
 }
-
-
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syrk_f64t_f64n_u_tensor_op_f64, 128x64x16_64x32x16) {
+TEST(SM80_Device_Trmm_f32n_f32t_f32t_rs_u_nu_tensor_op_fast_f32_align1_align4, 128x64x32_32x64x32) {
 
-  using ElementA = double;
-  using LayoutA = cutlass::layout::RowMajor;
-  using ElementC = double;
-  using LayoutC = cutlass::layout::ColumnMajor;
-  using ElementAccumulator = double;
-
-  using RankK = cutlass::gemm::device::RankK<
-    ElementA,
-    LayoutA,
-    ElementC,
-    LayoutC,
-    cutlass::FillMode::kUpper,
-    ElementAccumulator,
+using Trmm = cutlass::gemm::device::Trmm<
+    float, cutlass::layout::ColumnMajor,
+    cutlass::SideMode::kRight, cutlass::FillMode::kUpper, cutlass::DiagType::kNonUnit,
+    float, cutlass::layout::RowMajor,
+    float, cutlass::layout::RowMajor,
+    float,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<128, 64, 16>,
-    cutlass::gemm::GemmShape<64, 32, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::gemm::GemmShape<128, 64, 32>,
+    cutlass::gemm::GemmShape<32, 64, 32>,
+    cutlass::gemm::GemmShape<16, 8, 8>,
     cutlass::epilogue::thread::LinearCombination<
-      ElementC,
+      float,
       1,
-      ElementAccumulator,
-      ElementAccumulator
+      float,
+      float
     >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    4
-  >;
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<8>,
+    3,
+    1,
+    4,
+    false,
+    cutlass::arch::OpMultiplyAddFastF32
+>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
+  EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
 }
 
-
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syrk_f64t_f64n_u_tensor_op_f64, 128x128x16_32x64x16) {
+TEST(SM80_Device_Trmm_f32n_f32t_f32t_rs_l_nu_tensor_op_fast_f32_align1_align4, 64x128x32_32x64x32) {
 
-  using ElementA = double;
-  using LayoutA = cutlass::layout::RowMajor;
-  using ElementC = double;
-  using LayoutC = cutlass::layout::ColumnMajor;
-  using ElementAccumulator = double;
-
-  using RankK = cutlass::gemm::device::RankK<
-    ElementA,
-    LayoutA,
-    ElementC,
-    LayoutC,
-    cutlass::FillMode::kUpper,
-    ElementAccumulator,
+using Trmm = cutlass::gemm::device::Trmm<
+    float, cutlass::layout::ColumnMajor,
+    cutlass::SideMode::kRight, cutlass::FillMode::kLower, cutlass::DiagType::kNonUnit,
+    float, cutlass::layout::RowMajor,
+    float, cutlass::layout::RowMajor,
+    float,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<128, 128, 16>,
-    cutlass::gemm::GemmShape<32, 64, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
+    cutlass::gemm::GemmShape<64, 128, 32>,
+    cutlass::gemm::GemmShape<32, 64, 32>,
+    cutlass::gemm::GemmShape<16, 8, 8>,
     cutlass::epilogue::thread::LinearCombination<
-      ElementC,
+      float,
       1,
-      ElementAccumulator,
-      ElementAccumulator
+      float,
+      float
     >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3
-  >;
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<8>,
+    3,
+    1,
+    4,
+    false,
+    cutlass::arch::OpMultiplyAddFastF32
+>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
+  EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-
 #endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_tf32n_f32t_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syrk_tf32n_f32t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_tf32t_f32t_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/syrk_tf32t_f32t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed.h`

 * *Files 1% similar despite different names*

```diff
@@ -61,14 +61,17 @@
 namespace device {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename Gemm, bool Relu = false>
 struct Testbed {
 
+  using ElementA = typename Gemm::ElementA;
+  using ElementB = typename Gemm::ElementB;
+  using ElementC = typename Gemm::ElementC;
   using ElementAccumulator = typename Gemm::ElementAccumulator;
   using ElementCompute = typename Gemm::GemmKernel::Epilogue::OutputOp::ElementCompute;
 
   /// Initialization
   typename Gemm::LayoutA::Stride stride_factor_A;
   typename Gemm::LayoutB::Stride stride_factor_B;
   typename Gemm::LayoutC::Stride stride_factor_C;
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_complex.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_complex.h`

 * *Files 4% similar despite different names*

```diff
@@ -59,14 +59,17 @@
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename Gemm>
 struct TestbedComplex : public Testbed<Gemm> {
 
   using Base = Testbed<Gemm>;
+  using ElementA = typename Gemm::ElementA;
+  using ElementB = typename Gemm::ElementB;
+  using ElementC = typename Gemm::ElementC;
   using ElementAccumulator = typename Gemm::ElementAccumulator;
   using ElementCompute = typename Gemm::GemmKernel::Epilogue::OutputOp::ElementCompute;
 
 
   //
   // Methods
   //
@@ -127,15 +130,15 @@
     if (result != cudaSuccess) {
     	throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
     
     if (properties.sharedMemPerBlockOptin < smem_size) {
     	return false;
     }
-    
+
     return true;
   }
 
   /// Executes one test
   bool run(
     cutlass::gemm::GemmCoord problem_size, 
     int split_k_slices = 1,
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_gemm_with_broadcast.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_gemm_with_broadcast.h`

 * *Files 1% similar despite different names*

```diff
@@ -96,31 +96,34 @@
 
 template <
   typename Gemm, 
   typename ReferenceOp = GemmWithBroadcastReferenceOp<Gemm>
 >
 struct TestbedGemmWithBroadcast {
 
+  using ElementA = typename Gemm::ElementA;
+  using ElementB = typename Gemm::ElementB;
   using OutputOp = typename Gemm::GemmKernel::Epilogue::OutputOp;
   using ElementC = typename Gemm::ElementC;
   using ElementAccumulator = typename Gemm::ElementAccumulator;
-  using ElementCOmpute = typename OutputOp::ElementCompute;
+  using ElementCompute = typename OutputOp::ElementCompute;
+  using ElementVector = typename OutputOp::ElementVector;
   using ElementZ = typename OutputOp::ElementZ;
   using ElementT = typename OutputOp::ElementT;
 
   /// Initialization
   cutlass::Distribution::Kind init_A;
   cutlass::Distribution::Kind init_B;
   cutlass::Distribution::Kind init_C;
   uint64_t seed;
 
   cutlass::HostTensor<typename Gemm::ElementA, typename Gemm::LayoutA> tensor_A;          // Input A
   cutlass::HostTensor<typename Gemm::ElementB, typename Gemm::LayoutB> tensor_B;          // Input B
   cutlass::HostTensor<ElementC, typename Gemm::LayoutC> tensor_C;                         // Input C
-  cutlass::HostTensor<ElementC, typename Gemm::LayoutC> tensor_Broadcast;                 // Input Broadcast
+  cutlass::HostTensor<ElementVector, typename Gemm::LayoutC> tensor_Broadcast;            // Input Broadcast
 
   cutlass::HostTensor<ElementZ, typename Gemm::LayoutC> tensor_Z;
   cutlass::HostTensor<ElementT, typename Gemm::LayoutC> tensor_T;
 
   cutlass::HostTensor<ElementAccumulator, typename Gemm::LayoutC> tensor_C_ref;
   cutlass::HostTensor<ElementAccumulator, typename Gemm::LayoutC> tensor_Y_ref;
   cutlass::HostTensor<ElementZ, typename Gemm::LayoutC> tensor_Z_ref;
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_gemm_with_reduction.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_gemm_with_reduction.h`

 * *Files 1% similar despite different names*

```diff
@@ -57,14 +57,15 @@
 namespace gemm {
 namespace device {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename Gemm, typename BinaryOp>
 struct GemmWithReductionReference {
+
   using ElementAccumulator = typename Gemm::ElementAccumulator;
   using ElementCompute = typename Gemm::GemmKernel::Epilogue::ElementCompute;
   using ElementC = typename Gemm::ElementC;
   using ElementT = typename Gemm::GemmKernel::Epilogue::ElementTensor;
   //
   // Data members
   //
@@ -89,14 +90,17 @@
 
 template <
   typename Gemm,
   typename ReferenceOp
 >
 struct TestbedGemmWithReduction {
 
+  using ElementA = typename Gemm::ElementA;
+  using ElementB = typename Gemm::ElementB;
+  using ElementC = typename Gemm::ElementC;
   using ElementAccumulator = typename Gemm::ElementAccumulator;
   using ElementT = typename Gemm::GemmKernel::Epilogue::ElementTensor;
 
   /// Initialization
   cutlass::Distribution::Kind init_A;
   cutlass::Distribution::Kind init_B;
   cutlass::Distribution::Kind init_C;
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_grouped.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k_scheduler.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k_scheduler.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_scheduler.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_grouped_scheduler.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_interleaved.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_interleaved.h`

 * *Files 2% similar despite different names*

```diff
@@ -53,14 +53,17 @@
 namespace device {
 
 ////////////////////////////////////////////////////////////////////////////////
 
 template <typename Gemm, int InterleavedK>
 struct InterleavedTestbed {
 
+  using ElementA = typename Gemm::ElementA;
+  using ElementB = typename Gemm::ElementB;
+  using ElementC = typename Gemm::ElementC;
   using ElementAccumulator = typename Gemm::ElementAccumulator;
   using ElementCompute = typename Gemm::GemmKernel::Epilogue::OutputOp::ElementCompute;
 
   /// Initialization
   cutlass::Distribution::Kind init_A;
   cutlass::Distribution::Kind init_B;
   cutlass::Distribution::Kind init_C;
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_planar_complex.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_planar_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_rank2k_universal.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_rank2k_universal.h`

 * *Files 1% similar despite different names*

```diff
@@ -60,14 +60,17 @@
 namespace device {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename Rank2K>
 struct TestbedRank2KUniversal {
 
+  using ElementA = typename Rank2K::ElementA;
+  using ElementB = typename Rank2K::ElementB;
+  using ElementC = typename Rank2K::ElementC;
   using ElementAccumulator = typename Rank2K::ElementAccumulator;
   using ElementCompute = typename Rank2K::Rank2Kkernel::Epilogue::OutputOp::ElementCompute;
 
   /// Initialization
   cutlass::Distribution::Kind init_A;
   cutlass::Distribution::Kind init_B;
   cutlass::Distribution::Kind init_C;
@@ -297,15 +300,14 @@
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
     if (properties.sharedMemPerBlockOptin < smem_size) {
       return false;
     }
-
     return true;
   }
 
   /// Executes one test
   bool run(
     cutlass::gemm::GemmUniversalMode mode,
     cutlass::gemm::GemmCoord problem_size,
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_rank_k_universal.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_rank_k_universal.h`

 * *Files 2% similar despite different names*

```diff
@@ -59,14 +59,16 @@
 namespace device {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename RankK>
 struct TestbedRank2KUniversal {
 
+  using ElementA = typename RankK::ElementA;
+  using ElementC = typename RankK::ElementC;
   using ElementAccumulator = typename RankK::ElementAccumulator;
   using ElementCompute = typename RankK::RankKkernel::Epilogue::OutputOp::ElementCompute;
 
   /// Initialization
   cutlass::Distribution::Kind init_A;
   cutlass::Distribution::Kind init_C;
   uint64_t seed;
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_sanity.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_sanity.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_sparse.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_sparse.h`

 * *Files 3% similar despite different names*

```diff
@@ -60,14 +60,17 @@
 namespace device {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename Gemm>
 struct SparseTestbed {
 
+  using ElementA = typename Gemm::ElementA;
+  using ElementB = typename Gemm::ElementB;
+  using ElementC = typename Gemm::ElementC;
   using ElementAccumulator = typename Gemm::ElementAccumulator;
   using ElementCompute = typename Gemm::GemmKernel::Epilogue::OutputOp::ElementCompute;
 
   static int const kSparse = Gemm::GemmKernel::kSparse;
   static int const kMetaSizeInBits = Gemm::GemmKernel::kMetaSizeInBits;
   static int const kMaxID2 = Gemm::GemmKernel::kMaxID2;
   static int const kElementsPerElementE = Gemm::GemmKernel::kElementsPerElementE;
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_splitk.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_splitk.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_symm_universal.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_symm_universal.h`

 * *Files 1% similar despite different names*

```diff
@@ -60,14 +60,17 @@
 namespace device {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename Symm>
 struct TestbedSymmUniversal {
 
+  using ElementA = typename Symm::ElementA;
+  using ElementB = typename Symm::ElementB;
+  using ElementC = typename Symm::ElementC;
   using ElementAccumulator = typename Symm::ElementAccumulator;
   using ElementCompute = typename Symm::SymmKernel::Epilogue::OutputOp::ElementCompute;
 
   /// Initialization
   cutlass::Distribution::Kind init_A;
   cutlass::Distribution::Kind init_B;
   cutlass::Distribution::Kind init_C;
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_trmm_universal.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_trmm_universal.h`

 * *Files 2% similar despite different names*

```diff
@@ -62,14 +62,17 @@
 namespace device {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename Trmm>
 struct TestbedTrmmUniversal {
 
+  using ElementA = typename Trmm::ElementA;
+  using ElementB = typename Trmm::ElementB;
+  using ElementC = typename Trmm::ElementC;
   using ElementAccumulator = typename Trmm::ElementAccumulator;
   using ElementCompute = typename Trmm::TrmmKernel::Epilogue::OutputOp::ElementCompute;
 
   /// Initialization
   cutlass::Distribution::Kind init_A;
   cutlass::Distribution::Kind init_B;
   cutlass::Distribution::Kind init_D;
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_universal.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_universal.h`

 * *Files 2% similar despite different names*

```diff
@@ -57,14 +57,17 @@
 namespace device {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename Gemm, bool Relu = false>
 struct TestbedUniversal {
 
+  using ElementA = typename Gemm::ElementA;
+  using ElementB = typename Gemm::ElementB;
+  using ElementC = typename Gemm::ElementC;
   using ElementAccumulator = typename Gemm::ElementAccumulator;
   using ElementCompute = typename Gemm::GemmKernel::Epilogue::OutputOp::ElementCompute;
 
   /// Initialization
   cutlass::Distribution::Kind init_A;
   cutlass::Distribution::Kind init_B;
   cutlass::Distribution::Kind init_C;
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_utils.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/testbed_utils.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_fast_f32_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_fast_f32_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64_cf64_cf64_tensor_op_f64_sm90.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_cf64_cf64_cf64_tensor_op_f64_sm90.cu`

 * *Files 2% similar despite different names*

```diff
@@ -44,15 +44,15 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_trmm_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 TEST(SM90_Device_Trmm_cf64n_cf64n_cf64t_ls_u_nu_tensor_op_f64_gaussian, 32x32x16_16x16x16) {
 
   using ElementOutput = cutlass::complex<double>;
   using ElementAccumulator = cutlass::complex<double>;
@@ -130,8 +130,8 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_rs_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_rs_sm80.cu`

 * *Files 7% similar despite different names*

```diff
@@ -55,15 +55,15 @@
 // SM80_Device_Trmm_{ElementA}{LayoutA}_{ElementB}{LayoutB}_{ElementC}{LayoutC}_{SideMode}_{FillMode}\
 //    _{DiagType}_tensor_op_{ElementAccumulator}_align{AlignmentA}_align{AlignmentB}
 //
 ///////////////////////////////////////////////////////////////////////////////////////////////////////
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Trmm_f32n_f32t_f32t_rs_u_nu_tensor_op_fast_f32_align1_align1, 64x128x32_32x64x32) {
+TEST(SM80_Device_Trmm_tf32n_tf32t_f32t_rs_u_nu_tensor_op_f32_align1_align1, 64x128x32_32x64x32) {
 
 using Trmm = cutlass::gemm::device::Trmm<
     float, cutlass::layout::ColumnMajor,
     cutlass::SideMode::kRight, cutlass::FillMode::kUpper, cutlass::DiagType::kNonUnit,
     float, cutlass::layout::RowMajor,
     float, cutlass::layout::RowMajor,
     float,
@@ -79,22 +79,22 @@
       float
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<8>,
     3,
     1,
     1,
     false,
-    cutlass::arch::OpMultiplyAddFastF32
+    cutlass::arch::OpMultiplyAdd
 >;
 
   EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
 }
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Trmm_f32n_f32t_f32t_rs_u_nu_tensor_op_fast_f32_align1_align1, 128x64x32_32x64x32) {
+TEST(SM80_Device_Trmm_tf32n_tf32t_f32t_rs_u_nu_tensor_op_f32_align1_align1, 128x64x32_32x64x32) {
 
 using Trmm = cutlass::gemm::device::Trmm<
     float, cutlass::layout::ColumnMajor,
     cutlass::SideMode::kRight, cutlass::FillMode::kUpper, cutlass::DiagType::kNonUnit,
     float, cutlass::layout::RowMajor,
     float, cutlass::layout::RowMajor,
     float,
@@ -110,23 +110,23 @@
       float
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<8>,
     3,
     1,
     1,
     false,
-    cutlass::arch::OpMultiplyAddFastF32
+    cutlass::arch::OpMultiplyAdd
 >;
 
   EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Trmm_f32n_f32t_f32t_rs_l_nu_tensor_op_fast_f32_align1_align1, 64x128x32_32x64x32) {
+TEST(SM80_Device_Trmm_tf32n_tf32t_f32t_rs_l_nu_tensor_op_f32_align1_align1, 64x128x32_32x64x32) {
 
 using Trmm = cutlass::gemm::device::Trmm<
     float, cutlass::layout::ColumnMajor,
     cutlass::SideMode::kRight, cutlass::FillMode::kLower, cutlass::DiagType::kNonUnit,
     float, cutlass::layout::RowMajor,
     float, cutlass::layout::RowMajor,
     float,
@@ -142,24 +142,24 @@
       float
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<8>,
     3,
     1,
     1,
     false,
-    cutlass::arch::OpMultiplyAddFastF32
+    cutlass::arch::OpMultiplyAdd
 >;
 
   EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 
-TEST(SM80_Device_Trmm_f32n_f32t_f32t_rs_u_nu_tensor_op_fast_f32_align1_align4, 64x128x32_32x64x32) {
+TEST(SM80_Device_Trmm_tf32n_tf32t_f32t_rs_u_nu_tensor_op_f32_align1_align4, 64x128x32_32x64x32) {
 
 using Trmm = cutlass::gemm::device::Trmm<
     float, cutlass::layout::ColumnMajor,
     cutlass::SideMode::kRight, cutlass::FillMode::kUpper, cutlass::DiagType::kNonUnit,
     float, cutlass::layout::RowMajor,
     float, cutlass::layout::RowMajor,
     float,
@@ -175,22 +175,22 @@
       float
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<8>,
     3,
     1,
     4,
     false,
-    cutlass::arch::OpMultiplyAddFastF32
+    cutlass::arch::OpMultiplyAdd
 >;
 
   EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
 }
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Trmm_f32n_f32t_f32t_rs_u_nu_tensor_op_fast_f32_align1_align4, 128x64x32_32x64x32) {
+TEST(SM80_Device_Trmm_tf32n_tf32t_f32t_rs_u_nu_tensor_op_f32_align1_align4, 128x64x32_32x64x32) {
 
 using Trmm = cutlass::gemm::device::Trmm<
     float, cutlass::layout::ColumnMajor,
     cutlass::SideMode::kRight, cutlass::FillMode::kUpper, cutlass::DiagType::kNonUnit,
     float, cutlass::layout::RowMajor,
     float, cutlass::layout::RowMajor,
     float,
@@ -206,23 +206,23 @@
       float
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<8>,
     3,
     1,
     4,
     false,
-    cutlass::arch::OpMultiplyAddFastF32
+    cutlass::arch::OpMultiplyAdd
 >;
 
   EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Trmm_f32n_f32t_f32t_rs_l_nu_tensor_op_fast_f32_align1_align4, 64x128x32_32x64x32) {
+TEST(SM80_Device_Trmm_tf32n_tf32t_f32t_rs_l_nu_tensor_op_f32_align1_align4, 64x128x32_32x64x32) {
 
 using Trmm = cutlass::gemm::device::Trmm<
     float, cutlass::layout::ColumnMajor,
     cutlass::SideMode::kRight, cutlass::FillMode::kLower, cutlass::DiagType::kNonUnit,
     float, cutlass::layout::RowMajor,
     float, cutlass::layout::RowMajor,
     float,
@@ -238,15 +238,15 @@
       float
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<8>,
     3,
     1,
     4,
     false,
-    cutlass::arch::OpMultiplyAddFastF32
+    cutlass::arch::OpMultiplyAdd
 >;
 
   EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 #endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32t_tensor_op_fast_f32_ls_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32t_tensor_op_fast_f32_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64_f64_f64_tensor_op_f64_sm90.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_f64_f64_f64_tensor_op_f64_sm90.cu`

 * *Files 1% similar despite different names*

```diff
@@ -44,15 +44,15 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_trmm_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 TEST(SM90_Device_Trmm_f64n_f64n_f64t_rs_l_nu_tensor_op_f64, 32x32x16_16x16x16) {
 
   using ElementOutput = double;
   using ElementAccumulator = double;
@@ -120,8 +120,8 @@
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_ls_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_rs_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_rs_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64t_f64t_tensor_op_f64_rs_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_f64n_f64t_f64t_tensor_op_f64_rs_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_ls_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_rs_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_rs_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_ls_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_rs_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/gemm_sm70.cu`

 * *Files 25% similar despite different names*

```diff
@@ -25,228 +25,271 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide TRMM interface
-
-  
+    \brief Unit tests for thread-level GEMM
 */
 
-#include <iostream>
-
 #include "../../common/cutlass_unit_test.h"
-#include "cutlass/blas3.h"
-#include "cutlass/gemm/device/trmm.h"
-#include "cutlass/util/host_tensor.h"
-#include "cutlass/util/reference/host/trmm.h"
-#include "cutlass/util/reference/host/tensor_compare.h"
-#include "cutlass/util/reference/host/tensor_copy.h"
-#include "cutlass/util/reference/host/tensor_fill.h"
-#include "cutlass/util/tensor_view_io.h"
 
-#include "testbed_trmm_universal.h"
+#include "cutlass/aligned_buffer.h"
+#include "cutlass/half.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+#include "cutlass/gemm/warp/mma_tensor_op_sm70.h"
 
-////////////////////////////////////////////Test name//////////////////////////////////////////////////
-//                             
-// SM80_Device_Trmm_{ElementA}{LayoutA}_{ElementB}{LayoutB}_{ElementC}{LayoutC}_{SideMode}_{FillMode}\
-//    _{DiagType}_tensor_op_{ElementAccumulator}_align{AlignmentA}_align{AlignmentB}
-//
-///////////////////////////////////////////////////////////////////////////////////////////////////////
+#include "cutlass/core_io.h"
+#include "cutlass/util/host_tensor.h"
+#include "cutlass/util/tensor_view_io.h"
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/reference/host/tensor_compare.h"
+#include "cutlass/util/reference/host/gemm.h"
 
-TEST(SM80_Device_Trmm_tf32n_tf32t_f32t_rs_u_nu_tensor_op_f32_align1_align1, 64x128x32_32x64x32) {
+#include "testbed.h"
 
-using Trmm = cutlass::gemm::device::Trmm<
-    float, cutlass::layout::ColumnMajor,
-    cutlass::SideMode::kRight, cutlass::FillMode::kUpper, cutlass::DiagType::kNonUnit,
-    float, cutlass::layout::RowMajor,
-    float, cutlass::layout::RowMajor,
-    float,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<64, 128, 32>,
-    cutlass::gemm::GemmShape<32, 64, 32>,
-    cutlass::gemm::GemmShape<16, 8, 8>,
-    cutlass::epilogue::thread::LinearCombination<
-      float,
-      1,
-      float,
-      float
-    >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<8>,
-    3,
-    1,
-    1,
-    false,
-    cutlass::arch::OpMultiplyAdd
->;
+#if defined(CUTLASS_ARCH_MMA_SM70_SUPPORTED)
 
-  EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
-}
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Trmm_tf32n_tf32t_f32t_rs_u_nu_tensor_op_f32_align1_align1, 128x64x32_32x64x32) {
+TEST(SM70_warp_gemm_tensor_op_congruous, 128x128x16_64x64x16_16x16x4) {
+
+  using Shape = cutlass::gemm::GemmShape<64, 64, 16>;
+  using ElementA = cutlass::half_t;
+  using ElementB = cutlass::half_t;
+  using ElementC = cutlass::half_t;
+  using LayoutA = cutlass::layout::ColumnMajorVoltaTensorOpMultiplicandCongruous<cutlass::sizeof_bits<ElementA>::value>;
+  using LayoutB = cutlass::layout::RowMajorVoltaTensorOpMultiplicandBCongruous<cutlass::sizeof_bits<ElementB>::value>;
+
+  using Policy = cutlass::gemm::warp::MmaTensorOpPolicy<
+    cutlass::arch::Mma<
+      cutlass::gemm::GemmShape<16, 16, 4>,
+      32,
+      ElementA,
+      cutlass::layout::ColumnMajor,
+      ElementB,
+      cutlass::layout::RowMajor,
+      ElementC,
+      cutlass::layout::RowMajor,
+      cutlass::arch::OpMultiplyAdd
+    >,
+    cutlass::MatrixShape<1, 1>
+  >;
 
-using Trmm = cutlass::gemm::device::Trmm<
-    float, cutlass::layout::ColumnMajor,
-    cutlass::SideMode::kRight, cutlass::FillMode::kUpper, cutlass::DiagType::kNonUnit,
-    float, cutlass::layout::RowMajor,
-    float, cutlass::layout::RowMajor,
-    float,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<128, 64, 32>,
-    cutlass::gemm::GemmShape<32, 64, 32>,
-    cutlass::gemm::GemmShape<16, 8, 8>,
-    cutlass::epilogue::thread::LinearCombination<
-      float,
-      1,
-      float,
-      float
-    >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<8>,
-    3,
-    1,
-    1,
-    false,
-    cutlass::arch::OpMultiplyAdd
->;
+  using MmaTensorOp = cutlass::gemm::warp::MmaVoltaTensorOp<
+    Shape,
+    ElementA,
+    LayoutA,
+    ElementB,
+    LayoutB,
+    ElementC,
+    cutlass::layout::RowMajor,
+    Policy
+  >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
+  test::gemm::warp::Testbed<MmaTensorOp, cutlass::gemm::GemmShape<128, 128, 16> >().run();
 }
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+TEST(SM70_warp_gemm_tensor_op_congruous, 128x64x4_64x64x4_16x16x4) {
 
-TEST(SM80_Device_Trmm_tf32n_tf32t_f32t_rs_l_nu_tensor_op_f32_align1_align1, 64x128x32_32x64x32) {
+  using Shape = cutlass::gemm::GemmShape<64, 64, 4>;
+  using ElementA = cutlass::half_t;
+  using ElementB = cutlass::half_t;
+  using ElementC = cutlass::half_t;
+  using LayoutA = cutlass::layout::ColumnMajorVoltaTensorOpMultiplicandCongruous<cutlass::sizeof_bits<ElementA>::value>;
+  using LayoutB = cutlass::layout::RowMajorVoltaTensorOpMultiplicandBCongruous<cutlass::sizeof_bits<ElementB>::value>;
+
+  using Policy = cutlass::gemm::warp::MmaTensorOpPolicy<
+    cutlass::arch::Mma<
+      cutlass::gemm::GemmShape<16, 16, 4>,
+      32,
+      ElementA,
+      cutlass::layout::ColumnMajor,
+      ElementB,
+      cutlass::layout::RowMajor,
+      ElementC,
+      cutlass::layout::RowMajor,
+      cutlass::arch::OpMultiplyAdd
+    >,
+    cutlass::MatrixShape<1, 1>
+  >;
 
-using Trmm = cutlass::gemm::device::Trmm<
-    float, cutlass::layout::ColumnMajor,
-    cutlass::SideMode::kRight, cutlass::FillMode::kLower, cutlass::DiagType::kNonUnit,
-    float, cutlass::layout::RowMajor,
-    float, cutlass::layout::RowMajor,
-    float,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<64, 128, 32>,
-    cutlass::gemm::GemmShape<32, 64, 32>,
-    cutlass::gemm::GemmShape<16, 8, 8>,
-    cutlass::epilogue::thread::LinearCombination<
-      float,
-      1,
-      float,
-      float
-    >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<8>,
-    3,
-    1,
-    1,
-    false,
-    cutlass::arch::OpMultiplyAdd
->;
+  using MmaTensorOp = cutlass::gemm::warp::MmaVoltaTensorOp<
+    Shape,
+    ElementA,
+    LayoutA,
+    ElementB,
+    LayoutB,
+    ElementC,
+    cutlass::layout::RowMajor,
+    Policy
+  >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
+  test::gemm::warp::Testbed<MmaTensorOp, cutlass::gemm::GemmShape<128, 64, 4> >().run();
 }
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
+TEST(SM70_warp_gemm_tensor_op_congruous, 128x128x4_32x32x4_16x16x4) {
 
-TEST(SM80_Device_Trmm_tf32n_tf32t_f32t_rs_u_nu_tensor_op_f32_align1_align4, 64x128x32_32x64x32) {
+  using Shape = cutlass::gemm::GemmShape<32, 32, 4>;
+  using ElementA = cutlass::half_t;
+  using ElementB = cutlass::half_t;
+  using ElementC = cutlass::half_t;
+  using LayoutA = cutlass::layout::ColumnMajorVoltaTensorOpMultiplicandCongruous<cutlass::sizeof_bits<ElementA>::value>;
+  using LayoutB = cutlass::layout::RowMajorVoltaTensorOpMultiplicandBCongruous<cutlass::sizeof_bits<ElementB>::value>;
+
+  using Policy = cutlass::gemm::warp::MmaTensorOpPolicy<
+    cutlass::arch::Mma<
+      cutlass::gemm::GemmShape<16, 16, 4>,
+      32,
+      ElementA,
+      cutlass::layout::ColumnMajor,
+      ElementB,
+      cutlass::layout::RowMajor,
+      ElementC,
+      cutlass::layout::RowMajor,
+      cutlass::arch::OpMultiplyAdd
+    >,
+    cutlass::MatrixShape<1, 1>
+  >;
 
-using Trmm = cutlass::gemm::device::Trmm<
-    float, cutlass::layout::ColumnMajor,
-    cutlass::SideMode::kRight, cutlass::FillMode::kUpper, cutlass::DiagType::kNonUnit,
-    float, cutlass::layout::RowMajor,
-    float, cutlass::layout::RowMajor,
-    float,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<64, 128, 32>,
-    cutlass::gemm::GemmShape<32, 64, 32>,
-    cutlass::gemm::GemmShape<16, 8, 8>,
-    cutlass::epilogue::thread::LinearCombination<
-      float,
-      1,
-      float,
-      float
-    >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<8>,
-    3,
-    1,
-    4,
-    false,
-    cutlass::arch::OpMultiplyAdd
->;
+  using MmaTensorOp = cutlass::gemm::warp::MmaVoltaTensorOp<
+    Shape,
+    ElementA,
+    LayoutA,
+    ElementB,
+    LayoutB,
+    ElementC,
+    cutlass::layout::RowMajor,
+    Policy
+  >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
+  test::gemm::warp::Testbed<MmaTensorOp, cutlass::gemm::GemmShape<128, 128, 4> >().run();
 }
-/////////////////////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Trmm_tf32n_tf32t_f32t_rs_u_nu_tensor_op_f32_align1_align4, 128x64x32_32x64x32) {
+TEST(SM70_warp_gemm_tensor_op_crosswise, 64x64x32_64x64x32_16x16x4) {
+  using Shape = cutlass::gemm::GemmShape<64, 64, 32>;
+  using ElementA = cutlass::half_t;
+  using ElementB = cutlass::half_t;
+  using ElementC = cutlass::half_t;
+  using LayoutA = cutlass::layout::RowMajorVoltaTensorOpMultiplicandCrosswise<
+      cutlass::sizeof_bits<ElementA>::value, 32>;
+  using LayoutB = cutlass::layout::ColumnMajorVoltaTensorOpMultiplicandCrosswise<
+      cutlass::sizeof_bits<ElementB>::value, 32>;
+
+  using Policy = cutlass::gemm::warp::MmaTensorOpPolicy<
+    cutlass::arch::Mma<
+      cutlass::gemm::GemmShape<16, 16, 4>,
+      32,
+      ElementA,
+      cutlass::layout::RowMajor,
+      ElementB,
+      cutlass::layout::ColumnMajor,
+      ElementC,
+      cutlass::layout::RowMajor,
+      cutlass::arch::OpMultiplyAdd
+    >,
+    cutlass::MatrixShape<1, 1>
+  >;
 
-using Trmm = cutlass::gemm::device::Trmm<
-    float, cutlass::layout::ColumnMajor,
-    cutlass::SideMode::kRight, cutlass::FillMode::kUpper, cutlass::DiagType::kNonUnit,
-    float, cutlass::layout::RowMajor,
-    float, cutlass::layout::RowMajor,
-    float,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<128, 64, 32>,
-    cutlass::gemm::GemmShape<32, 64, 32>,
-    cutlass::gemm::GemmShape<16, 8, 8>,
-    cutlass::epilogue::thread::LinearCombination<
-      float,
-      1,
-      float,
-      float
-    >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<8>,
-    3,
-    1,
-    4,
-    false,
-    cutlass::arch::OpMultiplyAdd
->;
+  using MmaTensorOp = cutlass::gemm::warp::MmaVoltaTensorOp<
+    Shape,
+    ElementA,
+    LayoutA,
+    ElementB,
+    LayoutB,
+    ElementC,
+    cutlass::layout::RowMajor,
+    Policy
+  >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
+  test::gemm::warp::Testbed<MmaTensorOp, cutlass::gemm::GemmShape<64, 64, 32> >().run();
 }
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Trmm_tf32n_tf32t_f32t_rs_l_nu_tensor_op_f32_align1_align4, 64x128x32_32x64x32) {
+TEST(SM70_warp_gemm_volta_tensor_op_canonical_f32_row_col, 64x64x16_64x64x4_8x8x4) {
+  
+  using Shape = cutlass::gemm::GemmShape<64, 64, 4>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
+  using ElementA = cutlass::half_t;
+  using ElementB = cutlass::half_t;
+  using ElementC = float;
+  using LayoutA = cutlass::layout::RowMajor;
+  using LayoutB = cutlass::layout::ColumnMajor;
+
+  using Policy = cutlass::gemm::warp::MmaTensorOpPolicy<
+    cutlass::arch::Mma<
+      cutlass::gemm::GemmShape<16, 16, 4>,
+      32,
+      ElementA,
+      cutlass::layout::RowMajor,
+      ElementB,
+      cutlass::layout::ColumnMajor,
+      ElementC,
+      cutlass::layout::RowMajor,
+      cutlass::arch::OpMultiplyAdd
+    >,
+    cutlass::MatrixShape<1, 1>
+  >;
 
-using Trmm = cutlass::gemm::device::Trmm<
-    float, cutlass::layout::ColumnMajor,
-    cutlass::SideMode::kRight, cutlass::FillMode::kLower, cutlass::DiagType::kNonUnit,
-    float, cutlass::layout::RowMajor,
-    float, cutlass::layout::RowMajor,
-    float,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<64, 128, 32>,
-    cutlass::gemm::GemmShape<32, 64, 32>,
-    cutlass::gemm::GemmShape<16, 8, 8>,
-    cutlass::epilogue::thread::LinearCombination<
-      float,
-      1,
-      float,
-      float
-    >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<8>,
-    3,
-    1,
-    4,
-    false,
-    cutlass::arch::OpMultiplyAdd
->;
+  using MmaTensorOp = cutlass::gemm::warp::MmaVoltaTensorOp<
+    Shape,
+    ElementA,
+    LayoutA,
+    ElementB,
+    LayoutB,
+    ElementC,
+    cutlass::layout::RowMajor,
+    Policy
+  >;
+
+  test::gemm::warp::Testbed<MmaTensorOp,
+                            cutlass::gemm::GemmShape<64, 64, 16> >()
+      .run();
+}
 
-  EXPECT_TRUE(test::gemm::device::TestAllTrmmUniversal<Trmm>());
+TEST(SM70_warp_gemm_volta_tensor_op_canonical_f32_col_row, 64x64x16_64x64x4_8x8x4) {
+  
+  using Shape = cutlass::gemm::GemmShape<64, 64, 4>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
+  using ElementA = cutlass::half_t;
+  using ElementB = cutlass::half_t;
+  using ElementC = float;
+  using LayoutA = cutlass::layout::ColumnMajor;
+  using LayoutB = cutlass::layout::RowMajor;
+
+  using Policy = cutlass::gemm::warp::MmaTensorOpPolicy<
+    cutlass::arch::Mma<
+      cutlass::gemm::GemmShape<16, 16, 4>,
+      32,
+      ElementA,
+      LayoutA,
+      ElementB,
+      LayoutB,
+      ElementC,
+      cutlass::layout::RowMajor,
+      cutlass::arch::OpMultiplyAdd
+    >,
+    cutlass::MatrixShape<1, 1>
+  >;
+
+  using MmaTensorOp = cutlass::gemm::warp::MmaVoltaTensorOp<
+    Shape,
+    ElementA,
+    LayoutA,
+    ElementB,
+    LayoutB,
+    ElementC,
+    cutlass::layout::RowMajor,
+    Policy
+  >;
+
+  test::gemm::warp::Testbed<MmaTensorOp,
+                            cutlass::gemm::GemmShape<64, 64, 16> >()
+      .run();
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+
+#endif // CUTLASS_ARCH_MMA_SM70_SUPPORTED
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32n_tensor_op_f32_ls_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32n_tensor_op_f32_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32t_tensor_op_f32_ls_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32t_tensor_op_f32_ls_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/kernel/batched_gemv.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/kernel/batched_gemv.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/kernel/testbed_gemv.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/kernel/testbed_gemv.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/thread/gemm_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm60.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/thread/gemm_sm60.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm61.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/thread/gemm_sm61.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/gemm_sm60_host.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/thread/host/gemm_sm60_host.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/testbed_host.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/thread/host/testbed_host.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/thread/testbed.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/thread/testbed.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/batched_gemv.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/batched_gemv.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/epilogue_workspace.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/epilogue_workspace.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_multistage.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_slicedk.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_multistage_slicedk.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse_testbed.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed_slicedk.h`

 * *Files 14% similar despite different names*

```diff
@@ -24,262 +24,204 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
+
 /*! \file
     \brief Unit testbed for kernel-level GEMM
 */
 
 #pragma once
 
+#include <fstream>
+
 #include "../../common/cutlass_unit_test.h"
+
 #include "cutlass/aligned_buffer.h"
-#include "cutlass/array.h"
-#include "cutlass/core_io.h"
 #include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h"
 #include "cutlass/layout/matrix.h"
+#include "cutlass/layout/vector.h"
 #include "cutlass/numeric_types.h"
-#include "cutlass/transform/threadblock/predicated_tile_access_iterator.h"
-#include "cutlass/util/distribution.h"
+
+#include "cutlass/core_io.h"
 #include "cutlass/util/host_tensor.h"
+#include "cutlass/util/tensor_view_io.h"
+
+#include "cutlass/util/distribution.h"
 #include "cutlass/util/reference/host/gemm.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
-#include "cutlass/util/reference/host/tensor_norm.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
-#include "cutlass/util/tensor_view_io.h"
-#include "cutlass/util/host_reorder.h"
-#include "cutlass/util/host_uncompress.h"
+
+#include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
+#include "cutlass/transform/threadblock/predicated_tile_access_iterator.h"
+#include "cutlass/cutlass.h"
+#include "cutlass/platform/platform.h"
 
 namespace test {
 namespace gemm {
 namespace threadblock {
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename Mma>
-__global__ void kernel_multistage_mma_sparse(cutlass::gemm::GemmCoord problem_size,
-                                      typename Mma::IteratorA::Params params_A,
-                                      typename Mma::IteratorA::TensorRef ref_A,
-                                      typename Mma::IteratorB::Params params_B,
-                                      typename Mma::IteratorB::TensorRef ref_B,
-                                      typename Mma::ElementC *ptr_C,
-                                      typename Mma::LayoutC::Stride::Index ldc,
-                                      typename Mma::IteratorE::Params params_E,
-                                      typename Mma::IteratorE::TensorRef ref_E) {
-  // Shared storage needed by threadblock-scoped matrix multiply-
+__global__ void kernel_multistage_mma(cutlass::gemm::GemmCoord problem_size,
+                           typename Mma::IteratorA::Params params_A,
+                           typename Mma::IteratorA::TensorRef ref_A,
+                           typename Mma::IteratorB::Params params_B,
+                           typename Mma::IteratorB::TensorRef ref_B,
+                           typename Mma::ElementC **ptr_C,
+                           typename Mma::LayoutC::Stride::Index ldc) {
+  // Shared storage needed by threadblock-scoped matrix multiply-accumulate
+
   // Dynamic shared memory base pointer
   extern __shared__ int GemmSharedStorageBase[];
 
   // Declare pointer to dynamic shared memory.
   typename Mma::SharedStorage *shared_storage =
       reinterpret_cast<typename Mma::SharedStorage *>(GemmSharedStorageBase);
 
   // Compute threadblock location
   cutlass::gemm::GemmCoord tb_tile_offset = {int(blockIdx.x), int(blockIdx.y),
                                              0};
 
   cutlass::MatrixCoord tb_offset_A{tb_tile_offset.m() * Mma::Shape::kM,
-                                   tb_tile_offset.k() / Mma::kSparse};
+                                   tb_tile_offset.k()};
 
   cutlass::MatrixCoord tb_offset_B{tb_tile_offset.k(),
                                    tb_tile_offset.n() * Mma::Shape::kN};
 
-  cutlass::MatrixCoord tb_offset_E{tb_tile_offset.m() * Mma::Shape::kM,
-                                   tb_tile_offset.k() / Mma::kSparse};
-
   // Compute position within threadblock
   int tb_thread_id = threadIdx.y * blockDim.x + threadIdx.x;
 
   // Construct iterators to A and B operands
   typename Mma::IteratorA iterator_A(params_A, ref_A.data(),
-                                     {problem_size.m(), problem_size.k() / Mma::kSparse},
+                                     {problem_size.m(), problem_size.k()},
                                      tb_thread_id, tb_offset_A);
 
   typename Mma::IteratorB iterator_B(params_B, ref_B.data(),
                                      {problem_size.k(), problem_size.n()},
                                      tb_thread_id, tb_offset_B);
 
-  typename Mma::IteratorE iterator_E(
-      params_E, ref_E.data(),
-      {problem_size.m(),
-       problem_size.k() / Mma::kSparse / Mma::kElementsPerElementE},
-      tb_thread_id, tb_offset_E);
-
   int warp_id = __shfl_sync(0xffffffff, threadIdx.y, 0);
+  int lane_id = threadIdx.x;
+
+  int partitionsK_idx = warp_id / (Mma::WarpCount::kM * Mma::WarpCount::kN);
 
   // Construct thread-scoped matrix multiply
   Mma mma(*shared_storage, tb_thread_id, warp_id, threadIdx.x);
 
   typename Mma::FragmentC accum;
 
   accum.clear();
 
   int gemm_k_iterations = (problem_size.k() + Mma::Shape::kK - 1) / Mma::Shape::kK;
 
   // Compute threadblock-scoped matrix multiply-add
-  mma(gemm_k_iterations, accum, iterator_A, iterator_B, iterator_E, accum);
+  mma(gemm_k_iterations, accum, iterator_A, iterator_B, accum);
 
   // Output results
-  typename Mma::Operator::IteratorC iterator_C({ptr_C, ldc}, threadIdx.x);
+  typename Mma::Operator::IteratorC iterator_C({ptr_C[partitionsK_idx], ldc}, lane_id);
 
+  int warp_idx_mn = warp_id % (Mma::WarpCount::kM * Mma::WarpCount::kN);
   iterator_C.add_tile_offset(
       {(tb_tile_offset.m() * Mma::WarpCount::kM) +
-           (warp_id % Mma::WarpCount::kM),
+           (warp_idx_mn % Mma::WarpCount::kM),
        (tb_tile_offset.n() * Mma::WarpCount::kN) +
-           (warp_id / Mma::WarpCount::kM)});
+           (warp_idx_mn / Mma::WarpCount::kM)});
 
   iterator_C.store(accum);
 }
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Structure to compute the matrix product
 template <
     /// Threadblock-level matrix multiply-accumulate
     typename MmaCore_>
-struct SparseTestbed {
+struct Testbed {
   /// Threadblock-level GEMM implementation
   using MmaCore = MmaCore_;
   using ThreadblockShape = typename MmaCore::Shape;
   using WarpShape = typename MmaCore::WarpShape;
   using InstructionShape = typename MmaCore::InstructionShape;
   using ElementA = typename MmaCore::ElementA;
   using LayoutA = typename MmaCore::LayoutA;
   using ElementB = typename MmaCore::ElementB;
   using LayoutB = typename MmaCore::LayoutB;
   using ElementC = typename MmaCore::ElementC;
   using LayoutC = typename MmaCore::LayoutC;
-  using ElementE = typename MmaCore::ElementE;
   using ThreadMapA = typename MmaCore::IteratorThreadMapA;
   using ThreadMapB = typename MmaCore::IteratorThreadMapB;
-  using ThreadMapE = typename MmaCore::IteratorThreadMapE;
   using AccessTypeA = cutlass::Array<ElementA, ThreadMapA::kElementsPerAccess>;
   using AccessTypeB = cutlass::Array<ElementB, ThreadMapB::kElementsPerAccess>;
-  using AccessTypeE = cutlass::Array<ElementE, ThreadMapE::kElementsPerAccess>;
   static int const Stages = MmaCore::kStages;
   static cutlass::arch::CacheOperation::Kind const CacheOpA =
       MmaCore::kCacheOpA;
   static cutlass::arch::CacheOperation::Kind const CacheOpB =
       MmaCore::kCacheOpB;
-  static cutlass::arch::CacheOperation::Kind const CacheOpE =
-      MmaCore::kCacheOpE;
-
-  static int const Sparse = MmaCore::kSparse;
-  static int const MetaSizeInBits = MmaCore::kMetaSizeInBits;
-  static int const MaxID2 = MmaCore::kMaxID2;
-
-  using LayoutE = cutlass::layout::RowMajor;
-  using ReorderedLayoutE = typename MmaCore::GmemLayoutE;
-
-  static int const ElementsPerElementE = MmaCore::kElementsPerElementE;
 
   // Define iterators over tiles from the A operand
   using IteratorA =
       cutlass::transform::threadblock::PredicatedTileAccessIterator<
-          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK / Sparse>,
+          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
           ElementA, LayoutA, 1, ThreadMapA, AccessTypeA>;
 
   // Define iterators over tiles from the B operand
   using IteratorB =
       cutlass::transform::threadblock::PredicatedTileAccessIterator<
           cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
           ElementB, LayoutB, 0, ThreadMapB, AccessTypeB>;
 
-  // Define iterators over tiles from the E operand
-  using IteratorE =
-      cutlass::transform::threadblock::PredicatedTileAccessIterator<
-          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK /
-                                                         Sparse /
-                                                         ElementsPerElementE>,
-          ElementE, ReorderedLayoutE, 1, ThreadMapE, AccessTypeE>;
-
   // Define the threadblock-scoped pipelined matrix multiply
-  using Mma = cutlass::gemm::threadblock::SparseMmaMultistage<
-      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
-      CacheOpA, IteratorB, typename MmaCore::SmemIteratorB, CacheOpB, ElementC,
-      LayoutC, IteratorE, typename MmaCore::SmemIteratorE, CacheOpE,
+  using Mma = cutlass::gemm::threadblock::MmaMultistage<
+      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA, CacheOpA,
+      IteratorB, typename MmaCore::SmemIteratorB, CacheOpB, ElementC, LayoutC,
       typename MmaCore::MmaPolicy, Stages>;
 
+  static int const kPartitionsK = MmaCore::MmaPolicy::kPartitionsK; 
+
   //
   // Data members
   //
 
   cutlass::HostTensor<ElementA, LayoutA> matrix_A;
-  cutlass::HostTensor<ElementA, LayoutA> matrix_A_uncompressed;
   cutlass::HostTensor<ElementB, LayoutB> matrix_B;
-  cutlass::HostTensor<ElementC, LayoutC> matrix_C_computed;
+  cutlass::HostTensor<ElementC, LayoutC> matrix_C_computed[kPartitionsK];
   cutlass::HostTensor<ElementC, LayoutC> matrix_C_reference;
-  cutlass::HostTensor<ElementE, LayoutE> matrix_E;
-  cutlass::HostTensor<ElementE, ReorderedLayoutE> matrix_E_reordered;
+  cutlass::HostTensor<ElementC*, cutlass::layout::PackedVectorLayout> matrix_C_pointers;
 
   cutlass::gemm::GemmCoord problem_size;
   float alpha, beta;
 
   //
   // Methods
   //
 
   /// Allocates workspace in device memory
-  SparseTestbed(int m, int n, int k, float alpha_ = float(1), float beta_ = float(0))
+  Testbed(int m, int n, int k, float alpha_ = float(1), float beta_ = float(0))
       : problem_size(m, n, k), alpha(alpha_), beta(beta_) {
-    matrix_A.reset(cutlass::make_Coord(m, k / Sparse));
-    matrix_A_uncompressed.reset(cutlass::make_Coord(m, k));
+    matrix_A.reset(cutlass::make_Coord(m, k));
     matrix_B.reset(cutlass::make_Coord(k, n));
-    matrix_C_computed.reset(cutlass::make_Coord(m, n));
-    matrix_C_reference.reset(cutlass::make_Coord(m, n), false);
-    matrix_E.reset(cutlass::make_Coord(m, k / Sparse / ElementsPerElementE));
-    matrix_E_reordered.reset(
-        cutlass::make_Coord(m, k / Sparse / ElementsPerElementE));
-  }
-
-  /// Returns true if the CUDA device is sufficient to execute the kernel.
-  bool sufficient() const {
-    //
-    // Determine SMEM requirements and waive if not satisfied
-    //
-
-    int smem_size = int(sizeof(typename Mma::SharedStorage));
 
-    cudaDeviceProp properties;
-    int device_idx;
-    cudaError_t result = cudaGetDevice(&device_idx);
+    CUTLASS_PRAGMA_UNROLL
+    for(int k = 0; k < kPartitionsK; k++)
+      matrix_C_computed[k].reset(cutlass::make_Coord(m, n));
 
-    if (result != cudaSuccess) {
-      throw std::runtime_error("cudaGetDevice() API call failed.");
-    }
-
-    result = cudaGetDeviceProperties(&properties, device_idx);
-
-    if (result != cudaSuccess) {
-      throw std::runtime_error("cudaGetDeviceProperties() failed");
-    }
-
-    if (properties.sharedMemPerBlockOptin < smem_size) {
-      return false;
-    }
-
-    return true;
+    matrix_C_reference.reset(cutlass::make_Coord(m, n), false);
+    matrix_C_pointers.reset(cutlass::Coord<1>(kPartitionsK));
   }
 
   /// Runs the test
   bool run(
       dim3 grid, dim3 block,
       cutlass::Distribution::Kind init_A = cutlass::Distribution::Uniform,
-      cutlass::Distribution::Kind init_B = cutlass::Distribution::Uniform,
-      cutlass::Distribution::Kind init_E = cutlass::Distribution::Uniform) {
-
-    // Waive the test
-    if (!sufficient()) {
-      return true;
-    }
-
+      cutlass::Distribution::Kind init_B = cutlass::Distribution::Uniform) {
     //
     // initialize device memory
     //
 
     if (init_A == cutlass::Distribution::Uniform) {
 
       int scope_max = 8;
@@ -328,111 +270,120 @@
     } else if (init_B == cutlass::Distribution::Identity) {
       cutlass::reference::host::TensorFillIdentity(matrix_B.host_view());
     } else {
       // TODO: Implement the rest
       return false;
     }
 
-    cutlass::reference::host::TensorFill(matrix_C_computed.host_view());
+    CUTLASS_PRAGMA_UNROLL
+    for(int k = 0; k < kPartitionsK; k++)
+      cutlass::reference::host::TensorFill(matrix_C_computed[k].host_view());
 
     cutlass::reference::host::TensorFill(matrix_C_reference.host_view());
 
-    if (init_E == cutlass::Distribution::Uniform) {
-      uint64_t seed = 7;
-      cutlass::reference::host::TensorFillRandomSparseMeta(
-          matrix_E.host_view(), seed, MetaSizeInBits);
-    } else if (init_E == cutlass::Distribution::Identity) {
-      uint32_t content = (MaxID2 == 1) ? 0x44444444 : 0x4444;
-      cutlass::reference::host::TensorFill(matrix_E.host_view(),
-                                           (ElementE)(content));
-    } else {
-      // TODO: Implement the rest
-      return false;
-    }
-
-    cutlass::reorder_meta(matrix_E_reordered.host_ref(), matrix_E.host_ref(),
-                          {problem_size.m(), problem_size.n(),
-                           problem_size.k() / Sparse / ElementsPerElementE});
-
     matrix_A.sync_device();
     matrix_B.sync_device();
-    matrix_C_computed.sync_device();
-    matrix_E_reordered.sync_device();
+
+    CUTLASS_PRAGMA_UNROLL
+    for(int k = 0; k < kPartitionsK; k++)
+      matrix_C_computed[k].sync_device();
 
     typename IteratorA::Params params_A(matrix_A.layout());
     typename IteratorB::Params params_B(matrix_B.layout());
-    typename IteratorE::Params params_E(matrix_E_reordered.layout());
+
+    CUTLASS_PRAGMA_UNROLL
+    for(int k = 0; k < kPartitionsK; k++)
+      matrix_C_pointers.at(cutlass::Coord<1>(k)) = matrix_C_computed[k].device_data();
+
+    matrix_C_pointers.sync_device();
 
     cudaError_t result;
 
     int smem_size = int(sizeof(typename Mma::SharedStorage));
     if (smem_size >= (48 << 10)) {
       result = cudaFuncSetAttribute(
-          test::gemm::threadblock::kernel_multistage_mma_sparse<Mma>,
+          test::gemm::threadblock::kernel_multistage_mma<Mma>,
           cudaFuncAttributeMaxDynamicSharedMemorySize, smem_size);
 
-      if (result != cudaSuccess) {
-          return true;
-      }
+      EXPECT_EQ(result, cudaSuccess)
+          << " cudaFuncSetAttribute "
+             "cudaFuncAttributeMaxDynamicSharedMemorySize error: "
+          << cudaGetErrorString(result);
 
       result = cudaFuncSetAttribute(
-          test::gemm::threadblock::kernel_multistage_mma_sparse<Mma>,
+          test::gemm::threadblock::kernel_multistage_mma<Mma>,
           cudaFuncAttributePreferredSharedMemoryCarveout, 100);
 
-      if (result != cudaSuccess) {
-          return true;
-      }
+      EXPECT_EQ(result, cudaSuccess)
+          << " cudaFuncSetAttribute "
+             "cudaFuncAttributePreferredSharedMemoryCarveout error: "
+          << cudaGetErrorString(result);
     }
 
-    test::gemm::threadblock::kernel_multistage_mma_sparse<Mma>
-        <<<grid, block, smem_size, 0>>>(
-            problem_size, params_A, matrix_A.device_ref(), params_B,
-            matrix_B.device_ref(), matrix_C_computed.device_data(),
-            matrix_C_computed.layout().stride(0), params_E,
-            matrix_E_reordered.device_ref());
+    test::gemm::threadblock::kernel_multistage_mma<Mma><<<grid, block, smem_size, 0>>>(
+        problem_size, params_A, matrix_A.device_ref(), params_B,
+        matrix_B.device_ref(), matrix_C_pointers.device_data(),
+        matrix_C_computed[0].layout().stride(0));
 
     //
     // Check error code
     //
 
     result = cudaDeviceSynchronize();
     EXPECT_EQ(result, cudaSuccess)
         << " kernel error: " << cudaGetErrorString(result);
 
-    matrix_C_computed.sync_host();
+    CUTLASS_PRAGMA_UNROLL
+    for(int k = 0; k < kPartitionsK; k++)
+      matrix_C_computed[k].sync_host();
 
-    cutlass::uncompress(matrix_A_uncompressed.host_ref(), matrix_A.host_ref(),
-                        matrix_E.host_ref(), problem_size.m(),
-                        problem_size.k());
+    // TODO: this is temporary. it will be removed after slicing can de
+    // reduction
+    //
+    // Reduce matrix_C_computed
+    //
+    CUTLASS_PRAGMA_UNROLL
+    for(int k = 1; k < kPartitionsK; k++) {
+      CUTLASS_PRAGMA_UNROLL
+      for(int m = 0; m < matrix_C_computed[0].extent().row(); m++){
+        CUTLASS_PRAGMA_UNROLL
+        for(int n = 0; n < matrix_C_computed[0].extent().column(); n++){
+          matrix_C_computed[0].at({m, n}) += matrix_C_computed[k].at({m, n});
+        }
+      }
+    }
 
     cutlass::reference::host::Gemm<ElementA, LayoutA, ElementB, LayoutB,
-                                   ElementC, LayoutC, ElementC, ElementC>
+                                   ElementC, LayoutC, ElementC, ElementC,
+                                   typename MmaCore::Operator>
         reference_gemm;
 
-    reference_gemm(problem_size, ElementC(alpha),
-                   matrix_A_uncompressed.host_view(), matrix_B.host_view(),
-                   ElementC(beta), matrix_C_reference.host_view());
+    reference_gemm(
+        problem_size, ElementC(alpha), matrix_A.host_view(),
+        matrix_B.host_view(), ElementC(beta), matrix_C_reference.host_view());
 
     bool passed = cutlass::reference::host::TensorEquals(
-        matrix_C_computed.host_view(), matrix_C_reference.host_view());
+        matrix_C_computed[0].host_view(), matrix_C_reference.host_view());
+
+    EXPECT_TRUE(passed);
 
-    EXPECT_TRUE(passed)
+    if (!passed) {
+      std::ofstream output("mma_multistage_testbed_errors.txt");
+
+      output
         << "A:\n" << matrix_A.host_view() << "\n"
         << "B:\n" << matrix_B.host_view() << "\n"
-        << "E:\n" << matrix_E.host_view() << "\n"
         << "Reference:\n"
         << matrix_C_reference.host_view() << "\n"
         << "Computed:\n"
-        << matrix_C_computed.host_view() << "\n";
-
-    EXPECT_GT(cutlass::reference::host::TensorNorm(matrix_C_reference.host_view()), 0);
-    EXPECT_GT(cutlass::reference::host::TensorNorm(matrix_C_computed.host_view()), 0);
+        << matrix_C_computed[0].host_view() << "\n";
+    }
 
     return passed;
   }
 };
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 }  // namespace threadblock
 }  // namespace gemm
 }  // namespace test
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed.h`

 * *Files 6% similar despite different names*

```diff
@@ -189,19 +189,48 @@
       : problem_size(m, n, k), alpha(alpha_), beta(beta_) {
     matrix_A.reset(cutlass::make_Coord(m, k));
     matrix_B.reset(cutlass::make_Coord(k, n));
     matrix_C_computed.reset(cutlass::make_Coord(m, n));
     matrix_C_reference.reset(cutlass::make_Coord(m, n), false);
   }
 
+  /// Returns true if the CUDA device is sufficient to execute the kernel.
+  bool sufficient() const {
+
+    //
+    // Determine SMEM requirements and waive if not satisfied
+    //
+
+    cudaDeviceProp properties;
+    int device_idx;
+    cudaError_t result = cudaGetDevice(&device_idx);
+
+    if (result != cudaSuccess) {
+      throw std::runtime_error("cudaGetDevice() API call failed.");
+    }
+
+    result = cudaGetDeviceProperties(&properties, device_idx);
+
+    if (result != cudaSuccess) {
+      throw std::runtime_error("cudaGetDeviceProperties() failed");
+    }
+
+    return true;
+  }
+
   /// Runs the test
   bool run(
       dim3 grid, dim3 block,
       cutlass::Distribution::Kind init_A = cutlass::Distribution::Uniform,
       cutlass::Distribution::Kind init_B = cutlass::Distribution::Uniform) {
+
+    if (!sufficient()) {
+      return true;
+    }
+
     //
     // initialize device memory
     //
 
     if (init_A == cutlass::Distribution::Uniform) {
 
       int scope_max = 8;
@@ -314,21 +343,26 @@
     reference_gemm(
         problem_size, ElementC(alpha), matrix_A.host_view(),
         matrix_B.host_view(), ElementC(beta), matrix_C_reference.host_view());
 
     bool passed = cutlass::reference::host::TensorEquals(
         matrix_C_computed.host_view(), matrix_C_reference.host_view());
 
-    EXPECT_TRUE(passed) 
+    EXPECT_TRUE(passed);
+
+    if (!passed && CUTLASS_TEST_UNIT_ENABLE_WARNINGS) {
+      std::cout
+        << __FILE__ << ":" << __LINE__ << "  "
         << "A:\n" << matrix_A.host_view() << "\n"
         << "B:\n" << matrix_B.host_view() << "\n"
         << "Reference:\n"
         << matrix_C_reference.host_view() << "\n"
         << "Computed:\n"
         << matrix_C_computed.host_view() << "\n";
+    }
 
     EXPECT_GT(cutlass::reference::host::TensorNorm(matrix_C_reference.host_view()), 0);
     EXPECT_GT(cutlass::reference::host::TensorNorm(matrix_C_computed.host_view()), 0);
 
     return passed;
   }
 };
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed_slicedk.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_pipelined_testbed_slicedk.h`

 * *Files 8% similar despite different names*

```diff
@@ -50,41 +50,38 @@
 #include "cutlass/util/tensor_view_io.h"
 
 #include "cutlass/util/distribution.h"
 #include "cutlass/util/reference/host/gemm.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 
-#include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
-#include "cutlass/transform/threadblock/predicated_tile_access_iterator.h"
+#include "cutlass/gemm/threadblock/default_mma_core_simt.h"
+#include "cutlass/gemm/threadblock/default_mma_core_sm75.h"
+#include "cutlass/gemm/threadblock/default_mma_core_sm70.h"
+#include "cutlass/transform/threadblock/predicated_tile_iterator.h"
+#include "cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h"
 #include "cutlass/cutlass.h"
 #include "cutlass/platform/platform.h"
 
 namespace test {
 namespace gemm {
 namespace threadblock {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename Mma>
-__global__ void kernel_multistage_mma(cutlass::gemm::GemmCoord problem_size,
+__global__ void kernel_mma(cutlass::gemm::GemmCoord problem_size,
                            typename Mma::IteratorA::Params params_A,
                            typename Mma::IteratorA::TensorRef ref_A,
                            typename Mma::IteratorB::Params params_B,
                            typename Mma::IteratorB::TensorRef ref_B,
                            typename Mma::ElementC **ptr_C,
                            typename Mma::LayoutC::Stride::Index ldc) {
   // Shared storage needed by threadblock-scoped matrix multiply-accumulate
-
-  // Dynamic shared memory base pointer
-  extern __shared__ int GemmSharedStorageBase[];
-
-  // Declare pointer to dynamic shared memory.
-  typename Mma::SharedStorage *shared_storage =
-      reinterpret_cast<typename Mma::SharedStorage *>(GemmSharedStorageBase);
+  __shared__ typename Mma::SharedStorage shared_storage;
 
   // Compute threadblock location
   cutlass::gemm::GemmCoord tb_tile_offset = {int(blockIdx.x), int(blockIdx.y),
                                              0};
 
   cutlass::MatrixCoord tb_offset_A{tb_tile_offset.m() * Mma::Shape::kM,
                                    tb_tile_offset.k()};
@@ -100,34 +97,35 @@
                                      {problem_size.m(), problem_size.k()},
                                      tb_thread_id, tb_offset_A);
 
   typename Mma::IteratorB iterator_B(params_B, ref_B.data(),
                                      {problem_size.k(), problem_size.n()},
                                      tb_thread_id, tb_offset_B);
 
-  int warp_id = __shfl_sync(0xffffffff, threadIdx.y, 0);
+  int warp_id = threadIdx.y;
   int lane_id = threadIdx.x;
 
   int partitionsK_idx = warp_id / (Mma::WarpCount::kM * Mma::WarpCount::kN);
 
   // Construct thread-scoped matrix multiply
-  Mma mma(*shared_storage, tb_thread_id, warp_id, threadIdx.x);
+  Mma mma(shared_storage, tb_thread_id, warp_id, threadIdx.x);
 
   typename Mma::FragmentC accum;
 
   accum.clear();
 
   int gemm_k_iterations = (problem_size.k() + Mma::Shape::kK - 1) / Mma::Shape::kK;
 
   // Compute threadblock-scoped matrix multiply-add
   mma(gemm_k_iterations, accum, iterator_A, iterator_B, accum);
 
   // Output results
   typename Mma::Operator::IteratorC iterator_C({ptr_C[partitionsK_idx], ldc}, lane_id);
 
+
   int warp_idx_mn = warp_id % (Mma::WarpCount::kM * Mma::WarpCount::kN);
   iterator_C.add_tile_offset(
       {(tb_tile_offset.m() * Mma::WarpCount::kM) +
            (warp_idx_mn % Mma::WarpCount::kM),
        (tb_tile_offset.n() * Mma::WarpCount::kN) +
            (warp_idx_mn / Mma::WarpCount::kM)});
 
@@ -148,41 +146,49 @@
   using InstructionShape = typename MmaCore::InstructionShape;
   using ElementA = typename MmaCore::ElementA;
   using LayoutA = typename MmaCore::LayoutA;
   using ElementB = typename MmaCore::ElementB;
   using LayoutB = typename MmaCore::LayoutB;
   using ElementC = typename MmaCore::ElementC;
   using LayoutC = typename MmaCore::LayoutC;
-  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
-  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
-  using AccessTypeA = cutlass::Array<ElementA, ThreadMapA::kElementsPerAccess>;
-  using AccessTypeB = cutlass::Array<ElementB, ThreadMapB::kElementsPerAccess>;
-  static int const Stages = MmaCore::kStages;
-  static cutlass::arch::CacheOperation::Kind const CacheOpA =
-      MmaCore::kCacheOpA;
-  static cutlass::arch::CacheOperation::Kind const CacheOpB =
-      MmaCore::kCacheOpB;
 
   // Define iterators over tiles from the A operand
-  using IteratorA =
-      cutlass::transform::threadblock::PredicatedTileAccessIterator<
+  static const bool use_idp4a = cutlass::platform::is_same<ElementA, int8_t>::value && 
+                                cutlass::platform::is_same<ElementB, int8_t>::value && 
+                                cutlass::platform::is_same<typename MmaCore::OperatorClass, cutlass::arch::OpClassSimt>::value;
+
+  static const bool transposeA =  cutlass::platform::is_same< LayoutA, cutlass::layout::ColumnMajor >::value;
+  static const bool transposeB =  cutlass::platform::is_same< LayoutB, cutlass::layout::RowMajor >::value;
+
+  using IteratorA = typename cutlass::platform::conditional< use_idp4a,
+      cutlass::transform::threadblock::PredicatedTileIterator2dThreadTile<
+          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
+          ElementA, LayoutA, 1, typename MmaCore::IteratorThreadMapA, transposeA> ,
+        
+      cutlass::transform::threadblock::PredicatedTileIterator<
           cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
-          ElementA, LayoutA, 1, ThreadMapA, AccessTypeA>;
+          ElementA, LayoutA, 1, typename MmaCore::IteratorThreadMapA>
+      >::type;
 
   // Define iterators over tiles from the B operand
-  using IteratorB =
-      cutlass::transform::threadblock::PredicatedTileAccessIterator<
+  using IteratorB = typename cutlass::platform::conditional< use_idp4a,
+      cutlass::transform::threadblock::PredicatedTileIterator2dThreadTile<
           cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
-          ElementB, LayoutB, 0, ThreadMapB, AccessTypeB>;
+          ElementB, LayoutB, 0, typename MmaCore::IteratorThreadMapB, transposeB> ,
+
+      cutlass::transform::threadblock::PredicatedTileIterator<
+          cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
+          ElementB, LayoutB, 0, typename MmaCore::IteratorThreadMapB>
+      >::type;
 
   // Define the threadblock-scoped pipelined matrix multiply
-  using Mma = cutlass::gemm::threadblock::MmaMultistage<
-      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA, CacheOpA,
-      IteratorB, typename MmaCore::SmemIteratorB, CacheOpB, ElementC, LayoutC,
-      typename MmaCore::MmaPolicy, Stages>;
+  using Mma = cutlass::gemm::threadblock::MmaPipelined<
+      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
+      IteratorB, typename MmaCore::SmemIteratorB, ElementC, LayoutC,
+      typename MmaCore::MmaPolicy>;
 
   static int const kPartitionsK = MmaCore::MmaPolicy::kPartitionsK; 
 
   //
   // Data members
   //
 
@@ -196,15 +202,15 @@
   float alpha, beta;
 
   //
   // Methods
   //
 
   /// Allocates workspace in device memory
-  Testbed(int m, int n, int k, float alpha_ = float(1), float beta_ = float(0))
+  Testbed(int m, int n, int k, float alpha_, float beta_)
       : problem_size(m, n, k), alpha(alpha_), beta(beta_) {
     matrix_A.reset(cutlass::make_Coord(m, k));
     matrix_B.reset(cutlass::make_Coord(k, n));
 
     CUTLASS_PRAGMA_UNROLL
     for(int k = 0; k < kPartitionsK; k++)
       matrix_C_computed[k].reset(cutlass::make_Coord(m, n));
@@ -292,47 +298,24 @@
 
     CUTLASS_PRAGMA_UNROLL
     for(int k = 0; k < kPartitionsK; k++)
       matrix_C_pointers.at(cutlass::Coord<1>(k)) = matrix_C_computed[k].device_data();
 
     matrix_C_pointers.sync_device();
 
-    cudaError_t result;
-
-    int smem_size = int(sizeof(typename Mma::SharedStorage));
-    if (smem_size >= (48 << 10)) {
-      result = cudaFuncSetAttribute(
-          test::gemm::threadblock::kernel_multistage_mma<Mma>,
-          cudaFuncAttributeMaxDynamicSharedMemorySize, smem_size);
-
-      EXPECT_EQ(result, cudaSuccess)
-          << " cudaFuncSetAttribute "
-             "cudaFuncAttributeMaxDynamicSharedMemorySize error: "
-          << cudaGetErrorString(result);
-
-      result = cudaFuncSetAttribute(
-          test::gemm::threadblock::kernel_multistage_mma<Mma>,
-          cudaFuncAttributePreferredSharedMemoryCarveout, 100);
-
-      EXPECT_EQ(result, cudaSuccess)
-          << " cudaFuncSetAttribute "
-             "cudaFuncAttributePreferredSharedMemoryCarveout error: "
-          << cudaGetErrorString(result);
-    }
-
-    test::gemm::threadblock::kernel_multistage_mma<Mma><<<grid, block, smem_size, 0>>>(
+    test::gemm::threadblock::kernel_mma<Mma><<<grid, block>>>(
         problem_size, params_A, matrix_A.device_ref(), params_B,
         matrix_B.device_ref(), matrix_C_pointers.device_data(),
         matrix_C_computed[0].layout().stride(0));
 
     //
     // Check error code
     //
 
-    result = cudaDeviceSynchronize();
+    cudaError_t result = cudaDeviceSynchronize();
     EXPECT_EQ(result, cudaSuccess)
         << " kernel error: " << cudaGetErrorString(result);
 
     CUTLASS_PRAGMA_UNROLL
     for(int k = 0; k < kPartitionsK; k++)
       matrix_C_computed[k].sync_host();
 
@@ -363,15 +346,15 @@
 
     bool passed = cutlass::reference::host::TensorEquals(
         matrix_C_computed[0].host_view(), matrix_C_reference.host_view());
 
     EXPECT_TRUE(passed);
 
     if (!passed) {
-      std::ofstream output("mma_multistage_testbed_errors.txt");
+      std::ofstream output("mma_pipelined_testbed_errors.txt");
 
       output
         << "A:\n" << matrix_A.host_view() << "\n"
         << "B:\n" << matrix_B.host_view() << "\n"
         << "Reference:\n"
         << matrix_C_reference.host_view() << "\n"
         << "Computed:\n"
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_simt.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_pipelined_simt.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_slicedk.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_pipelined_slicedk.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_testbed.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_pipelined_testbed.h`

 * *Files 2% similar despite different names*

```diff
@@ -213,19 +213,33 @@
       : problem_size(m, n, k), alpha(alpha_), beta(beta_) {
     matrix_A.reset(cutlass::make_Coord(m, k));
     matrix_B.reset(cutlass::make_Coord(k, n));
     matrix_C_computed.reset(cutlass::make_Coord(m, n));
     matrix_C_reference.reset(cutlass::make_Coord(m, n), false);
   }
 
+  bool sufficient() {
+    return true;
+  }
+
   /// Runs the test
   bool run(
       dim3 grid, dim3 block,
       cutlass::Distribution::Kind init_A = cutlass::Distribution::Uniform,
       cutlass::Distribution::Kind init_B = cutlass::Distribution::Uniform) {
+
+    // Waive test if insufficient CUDA device
+    if (!sufficient()) {
+      if (CUTLASS_TEST_UNIT_ENABLE_WARNINGS) {
+        std::cerr << "Test waived due to insufficient CUDA device." << std::endl;
+      }
+      return true;
+    }
+
+
     //
     // initialize device memory
     //
 
     if (init_A == cutlass::Distribution::Uniform) {
 
       int scope_max = 8;
@@ -296,15 +310,15 @@
 
     //
     // Check error code
     //
 
     cudaError_t result = cudaDeviceSynchronize();
     EXPECT_EQ(result, cudaSuccess)
-        << " kernel error: " << cudaGetErrorString(result);
+        << " kernel error: " << cudaGetErrorString(result) << " on device " << GetCudaDevice();
 
     matrix_C_computed.sync_host();
 
     cutlass::reference::host::Gemm<ElementA, LayoutA, ElementB, LayoutB,
                                    ElementC, LayoutC, ElementC, ElementC,
                                    typename MmaCore::Operator>
         reference_gemm;
@@ -312,15 +326,15 @@
     reference_gemm(
         problem_size, ElementC(alpha), matrix_A.host_view(),
         matrix_B.host_view(), ElementC(beta), matrix_C_reference.host_view());
 
     bool passed = cutlass::reference::host::TensorEquals(
         matrix_C_computed.host_view(), matrix_C_reference.host_view());
 
-    EXPECT_TRUE(passed);
+    EXPECT_TRUE(passed) << "Failed on device " << GetCudaDevice();
 
     if (!passed) {
       std::ofstream output("mma_pipelined_testbed_errors.txt");
 
       output
         << "A:\n" << matrix_A.host_view() << "\n"
         << "B:\n" << matrix_B.host_view() << "\n"
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_testbed_slicedk.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_planar_complex_testbed.h`

 * *Files 12% similar despite different names*

```diff
@@ -24,62 +24,62 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
 /*! \file
     \brief Unit testbed for kernel-level GEMM
 */
 
 #pragma once
 
 #include <fstream>
 
 #include "../../common/cutlass_unit_test.h"
 
+#include "cutlass/cutlass.h"
+#include "cutlass/platform/platform.h"
+
 #include "cutlass/aligned_buffer.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/layout/matrix.h"
 #include "cutlass/layout/vector.h"
 #include "cutlass/numeric_types.h"
 
 #include "cutlass/core_io.h"
-#include "cutlass/util/host_tensor.h"
+#include "cutlass/util/host_tensor_planar_complex.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "cutlass/util/distribution.h"
-#include "cutlass/util/reference/host/gemm.h"
+#include "cutlass/util/reference/host/gemm_planar_complex.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 
-#include "cutlass/gemm/threadblock/default_mma_core_simt.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sm75.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sm70.h"
-#include "cutlass/transform/threadblock/predicated_tile_iterator.h"
-#include "cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h"
-#include "cutlass/cutlass.h"
-#include "cutlass/platform/platform.h"
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace test {
 namespace gemm {
 namespace threadblock {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename Mma>
-__global__ void kernel_mma(cutlass::gemm::GemmCoord problem_size,
-                           typename Mma::IteratorA::Params params_A,
-                           typename Mma::IteratorA::TensorRef ref_A,
-                           typename Mma::IteratorB::Params params_B,
-                           typename Mma::IteratorB::TensorRef ref_B,
-                           typename Mma::ElementC **ptr_C,
-                           typename Mma::LayoutC::Stride::Index ldc) {
+__global__ void kernel_mma_planar_complex(
+  cutlass::gemm::GemmCoord problem_size,
+  typename Mma::IteratorA::Params params_A,
+  typename Mma::IteratorA::Element *ptr_A,
+  int64_t imaginary_stride_A,
+  typename Mma::IteratorB::Params params_B,
+  typename Mma::IteratorB::Element *ptr_B,
+  int64_t imaginary_stride_B,
+  typename Mma::ElementC *ptr_C, 
+  typename Mma::LayoutC::Stride::Index ldc, int64_t imaginary_stride_C) {
+
   // Shared storage needed by threadblock-scoped matrix multiply-accumulate
   __shared__ typename Mma::SharedStorage shared_storage;
 
   // Compute threadblock location
   cutlass::gemm::GemmCoord tb_tile_offset = {int(blockIdx.x), int(blockIdx.y),
                                              0};
 
@@ -88,281 +88,261 @@
 
   cutlass::MatrixCoord tb_offset_B{tb_tile_offset.k(),
                                    tb_tile_offset.n() * Mma::Shape::kN};
 
   // Compute position within threadblock
   int tb_thread_id = threadIdx.y * blockDim.x + threadIdx.x;
 
-  // Construct iterators to A and B operands
-  typename Mma::IteratorA iterator_A(params_A, ref_A.data(),
+  // Construct iterators to A operand
+  typename Mma::IteratorA iterator_A_real(params_A, ptr_A,
+                                     {problem_size.m(), problem_size.k()},
+                                     tb_thread_id, tb_offset_A);
+  
+  typename Mma::IteratorA iterator_A_imag(params_A, ptr_A + imaginary_stride_A,
                                      {problem_size.m(), problem_size.k()},
                                      tb_thread_id, tb_offset_A);
+  
+  // Construct iterators to B operand
+  typename Mma::IteratorB iterator_B_real(params_B, ptr_B,
+                                     {problem_size.k(), problem_size.n()},
+                                     tb_thread_id, tb_offset_B);
 
-  typename Mma::IteratorB iterator_B(params_B, ref_B.data(),
+  typename Mma::IteratorB iterator_B_imag(params_B, ptr_B + imaginary_stride_B,
                                      {problem_size.k(), problem_size.n()},
                                      tb_thread_id, tb_offset_B);
 
   int warp_id = threadIdx.y;
   int lane_id = threadIdx.x;
 
-  int partitionsK_idx = warp_id / (Mma::WarpCount::kM * Mma::WarpCount::kN);
-
   // Construct thread-scoped matrix multiply
   Mma mma(shared_storage, tb_thread_id, warp_id, threadIdx.x);
 
   typename Mma::FragmentC accum;
 
   accum.clear();
 
   int gemm_k_iterations = (problem_size.k() + Mma::Shape::kK - 1) / Mma::Shape::kK;
 
   // Compute threadblock-scoped matrix multiply-add
-  mma(gemm_k_iterations, accum, iterator_A, iterator_B, accum);
+  mma(gemm_k_iterations, accum, iterator_A_real, iterator_A_imag, iterator_B_real, iterator_B_imag, accum);
 
   // Output results
-  typename Mma::Operator::IteratorC iterator_C({ptr_C[partitionsK_idx], ldc}, lane_id);
+  typename Mma::Operator::IteratorC iterator_C({ptr_C, ldc}, lane_id);
 
-
-  int warp_idx_mn = warp_id % (Mma::WarpCount::kM * Mma::WarpCount::kN);
   iterator_C.add_tile_offset(
       {(tb_tile_offset.m() * Mma::WarpCount::kM) +
-           (warp_idx_mn % Mma::WarpCount::kM),
+           (warp_id % Mma::WarpCount::kM),
        (tb_tile_offset.n() * Mma::WarpCount::kN) +
-           (warp_idx_mn / Mma::WarpCount::kM)});
+           (warp_id / Mma::WarpCount::kM)});
+
+  iterator_C.store(accum.real);
 
-  iterator_C.store(accum);
+  iterator_C.store_with_pointer_offset(accum.imag, imaginary_stride_C);
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Structure to compute the matrix product
 template <
     /// Threadblock-level matrix multiply-accumulate
-    typename MmaCore_>
-struct Testbed {
-  /// Threadblock-level GEMM implementation
-  using MmaCore = MmaCore_;
-  using ThreadblockShape = typename MmaCore::Shape;
-  using WarpShape = typename MmaCore::WarpShape;
-  using InstructionShape = typename MmaCore::InstructionShape;
-  using ElementA = typename MmaCore::ElementA;
-  using LayoutA = typename MmaCore::LayoutA;
-  using ElementB = typename MmaCore::ElementB;
-  using LayoutB = typename MmaCore::LayoutB;
-  using ElementC = typename MmaCore::ElementC;
-  using LayoutC = typename MmaCore::LayoutC;
-
-  // Define iterators over tiles from the A operand
-  static const bool use_idp4a = cutlass::platform::is_same<ElementA, int8_t>::value && 
-                                cutlass::platform::is_same<ElementB, int8_t>::value && 
-                                cutlass::platform::is_same<typename MmaCore::OperatorClass, cutlass::arch::OpClassSimt>::value;
-
-  static const bool transposeA =  cutlass::platform::is_same< LayoutA, cutlass::layout::ColumnMajor >::value;
-  static const bool transposeB =  cutlass::platform::is_same< LayoutB, cutlass::layout::RowMajor >::value;
-
-  using IteratorA = typename cutlass::platform::conditional< use_idp4a,
-      cutlass::transform::threadblock::PredicatedTileIterator2dThreadTile<
-          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
-          ElementA, LayoutA, 1, typename MmaCore::IteratorThreadMapA, transposeA> ,
-        
-      cutlass::transform::threadblock::PredicatedTileIterator<
-          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
-          ElementA, LayoutA, 1, typename MmaCore::IteratorThreadMapA>
-      >::type;
-
-  // Define iterators over tiles from the B operand
-  using IteratorB = typename cutlass::platform::conditional< use_idp4a,
-      cutlass::transform::threadblock::PredicatedTileIterator2dThreadTile<
-          cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
-          ElementB, LayoutB, 0, typename MmaCore::IteratorThreadMapB, transposeB> ,
-
-      cutlass::transform::threadblock::PredicatedTileIterator<
-          cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
-          ElementB, LayoutB, 0, typename MmaCore::IteratorThreadMapB>
-      >::type;
-
-  // Define the threadblock-scoped pipelined matrix multiply
-  using Mma = cutlass::gemm::threadblock::MmaPipelined<
-      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
-      IteratorB, typename MmaCore::SmemIteratorB, ElementC, LayoutC,
-      typename MmaCore::MmaPolicy>;
+    typename Mma_>
+struct TestbedPlanarComplex {
 
-  static int const kPartitionsK = MmaCore::MmaPolicy::kPartitionsK; 
+  using Mma = Mma_;
+  using ThreadblockShape = typename Mma::Shape;
+  using IteratorA = typename Mma::IteratorA;
+  using ElementA = typename Mma::IteratorA::Element;
+  using LayoutA = typename Mma::IteratorA::Layout;
+  using IteratorB = typename Mma::IteratorB;
+  using ElementB = typename Mma::IteratorB::Element;
+  using LayoutB = typename Mma::IteratorB::Layout;
+  using ElementC = typename Mma::ElementC;
+  using ElementAccumulator = typename Mma::ElementC;
+  using LayoutC = typename Mma::LayoutC;
+  using ThreadMapA = typename Mma::IteratorA::ThreadMap;
+  using ThreadMapB = typename Mma::IteratorB::ThreadMap;
+  using AccessTypeA = cutlass::Array<ElementA, ThreadMapA::kElementsPerAccess>;
+  using AccessTypeB = cutlass::Array<ElementB, ThreadMapB::kElementsPerAccess>;
+  static int const Stages = Mma::kStages;
+  static cutlass::arch::CacheOperation::Kind const CacheOpA =
+      Mma::kCacheOpA;
+  static cutlass::arch::CacheOperation::Kind const CacheOpB =
+      Mma::kCacheOpB;
 
   //
   // Data members
   //
 
-  cutlass::HostTensor<ElementA, LayoutA> matrix_A;
-  cutlass::HostTensor<ElementB, LayoutB> matrix_B;
-  cutlass::HostTensor<ElementC, LayoutC> matrix_C_computed[kPartitionsK];
-  cutlass::HostTensor<ElementC, LayoutC> matrix_C_reference;
-  cutlass::HostTensor<ElementC*, cutlass::layout::PackedVectorLayout> matrix_C_pointers;
+  cutlass::HostTensorPlanarComplex<ElementA, LayoutA> matrix_A;
+  cutlass::HostTensorPlanarComplex<ElementB, LayoutB> matrix_B;
+  cutlass::HostTensorPlanarComplex<ElementC, LayoutC> matrix_C_computed;
+  cutlass::HostTensorPlanarComplex<ElementC, LayoutC> matrix_C_reference;
 
   cutlass::gemm::GemmCoord problem_size;
-  float alpha, beta;
 
   //
   // Methods
   //
 
   /// Allocates workspace in device memory
-  Testbed(int m, int n, int k, float alpha_, float beta_)
-      : problem_size(m, n, k), alpha(alpha_), beta(beta_) {
+  TestbedPlanarComplex(int m, int n, int k)
+      : problem_size(m, n, k) {
+
     matrix_A.reset(cutlass::make_Coord(m, k));
     matrix_B.reset(cutlass::make_Coord(k, n));
-
-    CUTLASS_PRAGMA_UNROLL
-    for(int k = 0; k < kPartitionsK; k++)
-      matrix_C_computed[k].reset(cutlass::make_Coord(m, n));
-
+    matrix_C_computed.reset(cutlass::make_Coord(m, n));
     matrix_C_reference.reset(cutlass::make_Coord(m, n), false);
-    matrix_C_pointers.reset(cutlass::Coord<1>(kPartitionsK));
   }
 
   /// Runs the test
   bool run(
       dim3 grid, dim3 block,
       cutlass::Distribution::Kind init_A = cutlass::Distribution::Uniform,
       cutlass::Distribution::Kind init_B = cutlass::Distribution::Uniform) {
+
     //
     // initialize device memory
     //
 
     if (init_A == cutlass::Distribution::Uniform) {
-
+      
       int scope_max = 8;
       int scope_min = -8;
 
       if (cutlass::sizeof_bits<ElementA>::value == 4) {
         scope_max = 2;
         scope_min = -2;
       } else if (cutlass::sizeof_bits<ElementA>::value == 1) {
         scope_max = 2;
         scope_min = 0;
       }
 
       uint64_t seed = 7;
       cutlass::reference::host::TensorFillRandomUniform(
           matrix_A.host_view(), seed, scope_max, scope_min, 0);
+      
     } else if (init_A == cutlass::Distribution::Sequential) {
+      
+      for (int i = 0; i < matrix_A.capacity() * 2; ++i) {
+        matrix_A.host_data()[i] = cutlass::half_t(float(i % 5) - 2);
+      }
+      /*
       cutlass::reference::host::BlockFillSequential(matrix_A.host_data(),
-                                                    matrix_A.capacity());
+                                                    matrix_A.capacity() * 2);
+      */
     } else if (init_A == cutlass::Distribution::Identity) {
-      cutlass::reference::host::TensorFillIdentity(matrix_A.host_view());
+      //cutlass::reference::host::TensorFillIdentity(matrix_A.host_view());
     } else {
       // TODO: Implement the rest
       return false;
     }
 
     if (init_B == cutlass::Distribution::Uniform) {
 
+      
       int scope_max = 8;
       int scope_min = -8;
 
       if (cutlass::sizeof_bits<ElementB>::value == 4) {
         scope_max = 2;
         scope_min = -2;
       } else if (cutlass::sizeof_bits<ElementB>::value == 1) {
         scope_max = 2;
         scope_min = 0;
       }
 
       uint64_t seed = 7;
       cutlass::reference::host::TensorFillRandomUniform(
           matrix_B.host_view(), seed + 16, scope_max, scope_min, 0);
+      
+
     } else if (init_B == cutlass::Distribution::Sequential) {
+
       cutlass::reference::host::BlockFillSequential(matrix_B.host_data(),
-                                                    matrix_B.capacity());
+                                                    matrix_B.capacity() * 2);
+
+      for (int i = 0; i < matrix_B.capacity() * 2; ++i) {
+        matrix_B.host_data()[i] = cutlass::half_t(float((i + 3) % 5) - 2);
+      }
+
+
     } else if (init_B == cutlass::Distribution::Identity) {
-      cutlass::reference::host::TensorFillIdentity(matrix_B.host_view());
+
+      //cutlass::reference::host::TensorFillIdentity(matrix_B.host_view());
+
     } else {
       // TODO: Implement the rest
       return false;
     }
 
-    CUTLASS_PRAGMA_UNROLL
-    for(int k = 0; k < kPartitionsK; k++)
-      cutlass::reference::host::TensorFill(matrix_C_computed[k].host_view());
-
-    cutlass::reference::host::TensorFill(matrix_C_reference.host_view());
-
     matrix_A.sync_device();
     matrix_B.sync_device();
-
-    CUTLASS_PRAGMA_UNROLL
-    for(int k = 0; k < kPartitionsK; k++)
-      matrix_C_computed[k].sync_device();
+    matrix_C_computed.sync_device();
 
     typename IteratorA::Params params_A(matrix_A.layout());
     typename IteratorB::Params params_B(matrix_B.layout());
 
-    CUTLASS_PRAGMA_UNROLL
-    for(int k = 0; k < kPartitionsK; k++)
-      matrix_C_pointers.at(cutlass::Coord<1>(k)) = matrix_C_computed[k].device_data();
-
-    matrix_C_pointers.sync_device();
-
-    test::gemm::threadblock::kernel_mma<Mma><<<grid, block>>>(
-        problem_size, params_A, matrix_A.device_ref(), params_B,
-        matrix_B.device_ref(), matrix_C_pointers.device_data(),
-        matrix_C_computed[0].layout().stride(0));
+    test::gemm::threadblock::kernel_mma_planar_complex<Mma><<<grid, block>>>(
+        problem_size, 
+        params_A, 
+        matrix_A.device_data(),
+        matrix_A.imaginary_stride(),
+        params_B,
+        matrix_B.device_data(), 
+        matrix_B.imaginary_stride(),
+        matrix_C_computed.device_data(),
+        matrix_C_computed.layout().stride(0), 
+        matrix_C_computed.imaginary_stride()
+      );
+
 
     //
     // Check error code
     //
 
     cudaError_t result = cudaDeviceSynchronize();
     EXPECT_EQ(result, cudaSuccess)
         << " kernel error: " << cudaGetErrorString(result);
 
-    CUTLASS_PRAGMA_UNROLL
-    for(int k = 0; k < kPartitionsK; k++)
-      matrix_C_computed[k].sync_host();
-
-    // TODO: this is temporary. it will be removed after slicing can de
-    // reduction
-    //
-    // Reduce matrix_C_computed
-    //
-    CUTLASS_PRAGMA_UNROLL
-    for(int k = 1; k < kPartitionsK; k++) {
-      CUTLASS_PRAGMA_UNROLL
-      for(int m = 0; m < matrix_C_computed[0].extent().row(); m++){
-        CUTLASS_PRAGMA_UNROLL
-        for(int n = 0; n < matrix_C_computed[0].extent().column(); n++){
-          matrix_C_computed[0].at({m, n}) += matrix_C_computed[k].at({m, n});
-        }
-      }
-    }
-
-    cutlass::reference::host::Gemm<ElementA, LayoutA, ElementB, LayoutB,
-                                   ElementC, LayoutC, ElementC, ElementC,
-                                   typename MmaCore::Operator>
-        reference_gemm;
-
-    reference_gemm(
-        problem_size, ElementC(alpha), matrix_A.host_view(),
-        matrix_B.host_view(), ElementC(beta), matrix_C_reference.host_view());
+    matrix_C_computed.sync_host();
 
+    cutlass::reference::host::GemmPlanarComplex<
+      ElementA, LayoutA,
+      ElementB, LayoutB,
+      ElementC, LayoutC,
+      ElementAccumulator
+    >(
+      problem_size,
+      cutlass::complex<ElementAccumulator>(ElementAccumulator(1)),
+      matrix_A.host_ref(),
+      Mma::kTransformA,
+      matrix_B.host_ref(),
+      Mma::kTransformB,
+      cutlass::complex<ElementAccumulator>(ElementAccumulator(0)),
+      matrix_C_reference.host_ref(),
+      matrix_C_reference.host_ref()
+    );
+    
     bool passed = cutlass::reference::host::TensorEquals(
-        matrix_C_computed[0].host_view(), matrix_C_reference.host_view());
+      matrix_C_computed.host_view(), 
+      matrix_C_reference.host_view()
+    );
 
     EXPECT_TRUE(passed);
 
     if (!passed) {
       std::ofstream output("mma_pipelined_testbed_errors.txt");
 
       output
         << "A:\n" << matrix_A.host_view() << "\n"
         << "B:\n" << matrix_B.host_view() << "\n"
         << "Reference:\n"
         << matrix_C_reference.host_view() << "\n"
         << "Computed:\n"
-        << matrix_C_computed[0].host_view() << "\n";
+        << matrix_C_computed.host_view() << "\n";
     }
 
     return passed;
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_planar_complex_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_planar_complex_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_singlestage_wmma_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_singlestage_wmma_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_singlestage_wmma_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/threadblock/mma_singlestage_wmma_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_complex_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/gemm_complex_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_complex_sm90.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/gemm_complex_sm90.cu`

 * *Files 0% similar despite different names*

```diff
@@ -46,15 +46,15 @@
 
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/gemm.h"
 
 #include "testbed.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
 
 TEST(SM90_warp_gemm_complex_tensor_op_f64, 16x8x4_16x8x4_nt) {
 
   using Shape = cutlass::gemm::GemmShape<16, 8, 4>;
   using InstructionShape = cutlass::gemm::GemmShape<16, 8, 4>;
   
   using Element = cutlass::complex<double>;
@@ -327,8 +327,8 @@
     ElementC,
     cutlass::layout::RowMajor
   >::Type;
 
   test::gemm::warp::TestbedComplex<MmaTensorOp, Shape>().run();
 }
 
-#endif // if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+#endif // if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_gaussian_complex_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/gemm_gaussian_complex_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm50.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/gemm_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm60.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/gemm_sm60.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm61.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/gemm_sm61.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/gemm_sm90.cu`

 * *Files 20% similar despite different names*

```diff
@@ -24,272 +24,183 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/*! \file
-    \brief Unit tests for thread-level GEMM
+/*! \file 
+
+    \brief Unit tests for thread-level GEMM with Hopper FP64
 */
 
 #include "../../common/cutlass_unit_test.h"
 
 #include "cutlass/aligned_buffer.h"
 #include "cutlass/half.h"
 
-#include "cutlass/gemm/warp/mma_tensor_op_sm70.h"
+#include "cutlass/gemm/warp/default_mma_tensor_op.h"
 
 #include "cutlass/core_io.h"
 #include "cutlass/util/host_tensor.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/gemm.h"
 
 #include "testbed.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM70_SUPPORTED)
+#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+
+TEST(SM90_warp_gemm_tensor_op_congruous_f64, 16x16x4_16x16x4_16x8x4) {
+  using Shape = cutlass::gemm::GemmShape<16, 16, 4>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 4>;
+  using Element = double;
+  using ElementC = double;
+  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous64b;
+  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous64b;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaTensorOp<
+      Shape, InstructionShape, Element, LayoutA, Element, LayoutB, ElementC,
+      cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>::Type;
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+  test::gemm::warp::Testbed<MmaTensorOp,
+                            cutlass::gemm::GemmShape<16, 16, 4> >()
+      .run();
+}
 
-TEST(SM70_warp_gemm_tensor_op_congruous, 128x128x16_64x64x16_16x16x4) {
+////////////////////////////////////////////////////////////////////////////////
 
-  using Shape = cutlass::gemm::GemmShape<64, 64, 16>;
-  using ElementA = cutlass::half_t;
-  using ElementB = cutlass::half_t;
-  using ElementC = cutlass::half_t;
-  using LayoutA = cutlass::layout::ColumnMajorVoltaTensorOpMultiplicandCongruous<cutlass::sizeof_bits<ElementA>::value>;
-  using LayoutB = cutlass::layout::RowMajorVoltaTensorOpMultiplicandBCongruous<cutlass::sizeof_bits<ElementB>::value>;
-
-  using Policy = cutlass::gemm::warp::MmaTensorOpPolicy<
-    cutlass::arch::Mma<
-      cutlass::gemm::GemmShape<16, 16, 4>,
-      32,
-      ElementA,
-      cutlass::layout::ColumnMajor,
-      ElementB,
-      cutlass::layout::RowMajor,
-      ElementC,
-      cutlass::layout::RowMajor,
-      cutlass::arch::OpMultiplyAdd
-    >,
-    cutlass::MatrixShape<1, 1>
-  >;
-
-  using MmaTensorOp = cutlass::gemm::warp::MmaVoltaTensorOp<
-    Shape,
-    ElementA,
-    LayoutA,
-    ElementB,
-    LayoutB,
-    ElementC,
-    cutlass::layout::RowMajor,
-    Policy
-  >;
+TEST(SM90_warp_gemm_tensor_op_congruous_f64, 32x16x4_32x16x4_16x8x4) {
+  using Shape = cutlass::gemm::GemmShape<32, 16, 4>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 4>;
+  using Element = double;
+  using ElementC = double;
+  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous64b;
+  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous64b;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaTensorOp<
+      Shape, InstructionShape, Element, LayoutA, Element, LayoutB, ElementC,
+      cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>::Type;
 
-  test::gemm::warp::Testbed<MmaTensorOp, cutlass::gemm::GemmShape<128, 128, 16> >().run();
+  test::gemm::warp::Testbed<MmaTensorOp,
+                            cutlass::gemm::GemmShape<32, 16, 4> >()
+      .run();
 }
 
-TEST(SM70_warp_gemm_tensor_op_congruous, 128x64x4_64x64x4_16x16x4) {
+////////////////////////////////////////////////////////////////////////////////
 
-  using Shape = cutlass::gemm::GemmShape<64, 64, 4>;
-  using ElementA = cutlass::half_t;
-  using ElementB = cutlass::half_t;
-  using ElementC = cutlass::half_t;
-  using LayoutA = cutlass::layout::ColumnMajorVoltaTensorOpMultiplicandCongruous<cutlass::sizeof_bits<ElementA>::value>;
-  using LayoutB = cutlass::layout::RowMajorVoltaTensorOpMultiplicandBCongruous<cutlass::sizeof_bits<ElementB>::value>;
-
-  using Policy = cutlass::gemm::warp::MmaTensorOpPolicy<
-    cutlass::arch::Mma<
-      cutlass::gemm::GemmShape<16, 16, 4>,
-      32,
-      ElementA,
-      cutlass::layout::ColumnMajor,
-      ElementB,
-      cutlass::layout::RowMajor,
-      ElementC,
-      cutlass::layout::RowMajor,
-      cutlass::arch::OpMultiplyAdd
-    >,
-    cutlass::MatrixShape<1, 1>
-  >;
-
-  using MmaTensorOp = cutlass::gemm::warp::MmaVoltaTensorOp<
-    Shape,
-    ElementA,
-    LayoutA,
-    ElementB,
-    LayoutB,
-    ElementC,
-    cutlass::layout::RowMajor,
-    Policy
-  >;
+TEST(SM90_warp_gemm_tensor_op_congruous_f64, 32x32x4_32x32x4_16x8x4) {
+  using Shape = cutlass::gemm::GemmShape<32, 32, 4>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 4>;
+  using Element = double;
+  using ElementC = double;
+  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous64b;
+  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous64b;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaTensorOp<
+      Shape, InstructionShape, Element, LayoutA, Element, LayoutB, ElementC,
+      cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>::Type;
 
-  test::gemm::warp::Testbed<MmaTensorOp, cutlass::gemm::GemmShape<128, 64, 4> >().run();
+  test::gemm::warp::Testbed<MmaTensorOp,
+                            cutlass::gemm::GemmShape<32, 32, 4> >()
+      .run();
 }
 
-TEST(SM70_warp_gemm_tensor_op_congruous, 128x128x4_32x32x4_16x16x4) {
+////////////////////////////////////////////////////////////////////////////////
 
-  using Shape = cutlass::gemm::GemmShape<32, 32, 4>;
-  using ElementA = cutlass::half_t;
-  using ElementB = cutlass::half_t;
-  using ElementC = cutlass::half_t;
-  using LayoutA = cutlass::layout::ColumnMajorVoltaTensorOpMultiplicandCongruous<cutlass::sizeof_bits<ElementA>::value>;
-  using LayoutB = cutlass::layout::RowMajorVoltaTensorOpMultiplicandBCongruous<cutlass::sizeof_bits<ElementB>::value>;
-
-  using Policy = cutlass::gemm::warp::MmaTensorOpPolicy<
-    cutlass::arch::Mma<
-      cutlass::gemm::GemmShape<16, 16, 4>,
-      32,
-      ElementA,
-      cutlass::layout::ColumnMajor,
-      ElementB,
-      cutlass::layout::RowMajor,
-      ElementC,
-      cutlass::layout::RowMajor,
-      cutlass::arch::OpMultiplyAdd
-    >,
-    cutlass::MatrixShape<1, 1>
-  >;
-
-  using MmaTensorOp = cutlass::gemm::warp::MmaVoltaTensorOp<
-    Shape,
-    ElementA,
-    LayoutA,
-    ElementB,
-    LayoutB,
-    ElementC,
-    cutlass::layout::RowMajor,
-    Policy
-  >;
+TEST(SM90_warp_gemm_tensor_op_congruous_f64, 32x64x4_32x64x4_16x8x4) {
+  using Shape = cutlass::gemm::GemmShape<32, 64, 4>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 4>;
+  using Element = double;
+  using ElementC = double;
+  using LayoutA = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous64b;
+  using LayoutB = cutlass::layout::RowMajorTensorOpMultiplicandCongruous64b;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaTensorOp<
+      Shape, InstructionShape, Element, LayoutA, Element, LayoutB, ElementC,
+      cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>::Type;
 
-  test::gemm::warp::Testbed<MmaTensorOp, cutlass::gemm::GemmShape<128, 128, 4> >().run();
+  test::gemm::warp::Testbed<MmaTensorOp,
+                            cutlass::gemm::GemmShape<32, 64, 4> >()
+      .run();
 }
 
-TEST(SM70_warp_gemm_tensor_op_crosswise, 64x64x32_64x64x32_16x16x4) {
-  using Shape = cutlass::gemm::GemmShape<64, 64, 32>;
-  using ElementA = cutlass::half_t;
-  using ElementB = cutlass::half_t;
-  using ElementC = cutlass::half_t;
-  using LayoutA = cutlass::layout::RowMajorVoltaTensorOpMultiplicandCrosswise<
-      cutlass::sizeof_bits<ElementA>::value, 32>;
-  using LayoutB = cutlass::layout::ColumnMajorVoltaTensorOpMultiplicandCrosswise<
-      cutlass::sizeof_bits<ElementB>::value, 32>;
-
-  using Policy = cutlass::gemm::warp::MmaTensorOpPolicy<
-    cutlass::arch::Mma<
-      cutlass::gemm::GemmShape<16, 16, 4>,
-      32,
-      ElementA,
-      cutlass::layout::RowMajor,
-      ElementB,
-      cutlass::layout::ColumnMajor,
-      ElementC,
-      cutlass::layout::RowMajor,
-      cutlass::arch::OpMultiplyAdd
-    >,
-    cutlass::MatrixShape<1, 1>
-  >;
-
-  using MmaTensorOp = cutlass::gemm::warp::MmaVoltaTensorOp<
-    Shape,
-    ElementA,
-    LayoutA,
-    ElementB,
-    LayoutB,
-    ElementC,
-    cutlass::layout::RowMajor,
-    Policy
-  >;
+////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM90_warp_gemm_tensor_op_crosswise_f64, 16x16x16_16x16x16_16x8x4) {
+  using Shape = cutlass::gemm::GemmShape<16, 16, 16>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 4>;
+  using Element = double;
+  using ElementC = double;
+  using LayoutA = cutlass::layout::RowMajorTensorOpMultiplicand64bCrosswise;
+  using LayoutB = cutlass::layout::ColumnMajorTensorOpMultiplicand64bCrosswise;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaTensorOp<
+      Shape, InstructionShape, Element, LayoutA, Element, LayoutB, ElementC,
+      cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>::Type;
 
-  test::gemm::warp::Testbed<MmaTensorOp, cutlass::gemm::GemmShape<64, 64, 32> >().run();
+  test::gemm::warp::Testbed<MmaTensorOp,
+                            cutlass::gemm::GemmShape<16, 16, 16> >()
+      .run();
 }
 
 ////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM70_warp_gemm_volta_tensor_op_canonical_f32_row_col, 64x64x16_64x64x4_8x8x4) {
-  
-  using Shape = cutlass::gemm::GemmShape<64, 64, 4>;
-  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
-  using ElementA = cutlass::half_t;
-  using ElementB = cutlass::half_t;
-  using ElementC = float;
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-
-  using Policy = cutlass::gemm::warp::MmaTensorOpPolicy<
-    cutlass::arch::Mma<
-      cutlass::gemm::GemmShape<16, 16, 4>,
-      32,
-      ElementA,
-      cutlass::layout::RowMajor,
-      ElementB,
-      cutlass::layout::ColumnMajor,
-      ElementC,
-      cutlass::layout::RowMajor,
-      cutlass::arch::OpMultiplyAdd
-    >,
-    cutlass::MatrixShape<1, 1>
-  >;
-
-  using MmaTensorOp = cutlass::gemm::warp::MmaVoltaTensorOp<
-    Shape,
-    ElementA,
-    LayoutA,
-    ElementB,
-    LayoutB,
-    ElementC,
-    cutlass::layout::RowMajor,
-    Policy
-  >;
+TEST(SM90_warp_gemm_tensor_op_crosswise_f64, 32x32x16_32x32x16_16x8x4) {
+  using Shape = cutlass::gemm::GemmShape<32, 32, 16>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 4>;
+  using Element = double;
+  using ElementC = double;
+  using LayoutA = cutlass::layout::RowMajorTensorOpMultiplicand64bCrosswise;
+  using LayoutB = cutlass::layout::ColumnMajorTensorOpMultiplicand64bCrosswise;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaTensorOp<
+      Shape, InstructionShape, Element, LayoutA, Element, LayoutB, ElementC,
+      cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>::Type;
 
   test::gemm::warp::Testbed<MmaTensorOp,
-                            cutlass::gemm::GemmShape<64, 64, 16> >()
+                            cutlass::gemm::GemmShape<32, 32, 16> >()
       .run();
 }
 
-TEST(SM70_warp_gemm_volta_tensor_op_canonical_f32_col_row, 64x64x16_64x64x4_8x8x4) {
-  
-  using Shape = cutlass::gemm::GemmShape<64, 64, 4>;
-  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
-  using ElementA = cutlass::half_t;
-  using ElementB = cutlass::half_t;
-  using ElementC = float;
-  using LayoutA = cutlass::layout::ColumnMajor;
-  using LayoutB = cutlass::layout::RowMajor;
-
-  using Policy = cutlass::gemm::warp::MmaTensorOpPolicy<
-    cutlass::arch::Mma<
-      cutlass::gemm::GemmShape<16, 16, 4>,
-      32,
-      ElementA,
-      LayoutA,
-      ElementB,
-      LayoutB,
-      ElementC,
-      cutlass::layout::RowMajor,
-      cutlass::arch::OpMultiplyAdd
-    >,
-    cutlass::MatrixShape<1, 1>
-  >;
-
-  using MmaTensorOp = cutlass::gemm::warp::MmaVoltaTensorOp<
-    Shape,
-    ElementA,
-    LayoutA,
-    ElementB,
-    LayoutB,
-    ElementC,
-    cutlass::layout::RowMajor,
-    Policy
-  >;
+////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM90_warp_gemm_tensor_op_crosswise_f64, 64x32x16_64x32x16_16x8x4) {
+  using Shape = cutlass::gemm::GemmShape<64, 32, 16>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 4>;
+  using Element = double;
+  using ElementC = double;
+  using LayoutA = cutlass::layout::RowMajorTensorOpMultiplicand64bCrosswise;
+  using LayoutB = cutlass::layout::ColumnMajorTensorOpMultiplicand64bCrosswise;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaTensorOp<
+      Shape, InstructionShape, Element, LayoutA, Element, LayoutB, ElementC,
+      cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>::Type;
 
   test::gemm::warp::Testbed<MmaTensorOp,
-                            cutlass::gemm::GemmShape<64, 64, 16> >()
+                            cutlass::gemm::GemmShape<64, 32, 16> >()
       .run();
 }
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM90_warp_gemm_tensor_op_crosswise_f64, 32x64x16_32x64x16_16x8x4) {
+  using Shape = cutlass::gemm::GemmShape<32, 64, 16>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 4>;
+  using Element = double;
+  using ElementC = double;
+  using LayoutA = cutlass::layout::RowMajorTensorOpMultiplicand64bCrosswise;
+  using LayoutB = cutlass::layout::ColumnMajorTensorOpMultiplicand64bCrosswise;
+
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultMmaTensorOp<
+      Shape, InstructionShape, Element, LayoutA, Element, LayoutB, ElementC,
+      cutlass::layout::RowMajor, cutlass::arch::OpMultiplyAdd>::Type;
+
+  test::gemm::warp::Testbed<MmaTensorOp,
+                            cutlass::gemm::GemmShape<32, 64, 16> >()
+      .run();
+}
+////////////////////////////////////////////////////////////////////////////////
 
-#endif // CUTLASS_ARCH_MMA_SM70_SUPPORTED
+#endif // if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/gemm_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/gemm_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sparse_sm80.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/gemm_sparse_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/testbed.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/testbed.h`

 * *Files 2% similar despite different names*

```diff
@@ -187,18 +187,55 @@
     tensor_A.reset(cutlass::make_Coord(ThreadblockShape::kM, ThreadblockShape::kK));
     tensor_B.reset(cutlass::make_Coord(ThreadblockShape::kK, ThreadblockShape::kN));
     tensor_C.reset(cutlass::make_Coord(Shape::kM, Shape::kN));
     tensor_D_computed.reset(cutlass::make_Coord(Shape::kM, Shape::kN));
     tensor_D_reference.reset(cutlass::make_Coord(Shape::kM, Shape::kN), false);
   }
 
+  /// Returns true if the CUDA device is sufficient to execute the kernel.
+  bool sufficient() const {
+
+    cudaDeviceProp properties;
+    int device_idx;
+    cudaError_t result = cudaGetDevice(&device_idx);
+
+    if (result != cudaSuccess) {
+      throw std::runtime_error("cudaGetDevice() API call failed.");
+    }
+
+    result = cudaGetDeviceProperties(&properties, device_idx);
+
+    if (result != cudaSuccess) {
+      throw std::runtime_error("cudaGetDeviceProperties() failed");
+    }
+
+    if (properties.major == 9) {
+      // NVIDIA Hopper drops support for several data types
+      if (
+        cutlass::sizeof_bits<ElementA>::value < 8 ||
+        cutlass::sizeof_bits<ElementB>::value < 8 ||
+        cutlass::sizeof_bits<ElementC>::value < 8) {
+
+        return false;
+      }
+    }
+
+    return true;
+  }
+
+
   /// Runs the test
   bool run(
       cutlass::Distribution::Kind init_A = cutlass::Distribution::Uniform,
       cutlass::Distribution::Kind init_B = cutlass::Distribution::Uniform) {
+
+    if (!sufficient()) {
+      return true;
+    }
+
     //
     // initialize device memory
     //
 
     if (init_A == cutlass::Distribution::Uniform) {
       int scope_max = 8;
       int scope_min = -8;
@@ -397,18 +434,54 @@
     tensor_A.reset(cutlass::make_Coord(ThreadblockShape::kM, ThreadblockShape::kK));
     tensor_B.reset(cutlass::make_Coord(ThreadblockShape::kK, ThreadblockShape::kN));
     tensor_C.reset(cutlass::make_Coord(Shape::kM, Shape::kN));
     tensor_D_computed.reset(cutlass::make_Coord(Shape::kM, Shape::kN));
     tensor_D_reference.reset(cutlass::make_Coord(Shape::kM, Shape::kN), false);
   }
 
+  /// Returns true if the CUDA device is sufficient to execute the kernel.
+  bool sufficient() const {
+
+    cudaDeviceProp properties;
+    int device_idx;
+    cudaError_t result = cudaGetDevice(&device_idx);
+
+    if (result != cudaSuccess) {
+      throw std::runtime_error("cudaGetDevice() API call failed.");
+    }
+
+    result = cudaGetDeviceProperties(&properties, device_idx);
+
+    if (result != cudaSuccess) {
+      throw std::runtime_error("cudaGetDeviceProperties() failed");
+    }
+
+    if (properties.major == 9) {
+      // NVIDIA Hopper drops support for several data types
+      if (
+        cutlass::sizeof_bits<ElementA>::value < 8 ||
+        cutlass::sizeof_bits<ElementB>::value < 8 ||
+        cutlass::sizeof_bits<ElementC>::value < 8) {
+
+        return false;
+      }
+    }
+
+    return true;
+  }
+
   /// Runs the test
   bool run(
       cutlass::Distribution::Kind init_A = cutlass::Distribution::Uniform,
       cutlass::Distribution::Kind init_B = cutlass::Distribution::Uniform) {
+
+    if (!sufficient()) {
+      return true;
+    }
+
     //
     // initialize device memory
     //
 
     if (init_A == cutlass::Distribution::Uniform) {
       uint64_t seed = 7;
       cutlass::reference::host::TensorFillRandomUniform(tensor_A.host_view(),
@@ -672,18 +745,54 @@
     tensor_A.reset(cutlass::make_Coord(ThreadblockShape::kM, ThreadblockShape::kK));
     tensor_B.reset(cutlass::make_Coord(ThreadblockShape::kK, ThreadblockShape::kN));
     tensor_C.reset(cutlass::make_Coord(Shape::kM, Shape::kN));
     tensor_D_computed.reset(cutlass::make_Coord(Shape::kM, Shape::kN));
     tensor_D_reference.reset(cutlass::make_Coord(Shape::kM, Shape::kN), false);
   }
 
+  /// Returns true if the CUDA device is sufficient to execute the kernel.
+  bool sufficient() const {
+
+    cudaDeviceProp properties;
+    int device_idx;
+    cudaError_t result = cudaGetDevice(&device_idx);
+
+    if (result != cudaSuccess) {
+      throw std::runtime_error("cudaGetDevice() API call failed.");
+    }
+
+    result = cudaGetDeviceProperties(&properties, device_idx);
+
+    if (result != cudaSuccess) {
+      throw std::runtime_error("cudaGetDeviceProperties() failed");
+    }
+
+    if (properties.major == 9) {
+      // NVIDIA Hopper drops support for several data types
+      if (
+        cutlass::sizeof_bits<ElementA>::value < 8 ||
+        cutlass::sizeof_bits<ElementB>::value < 8 ||
+        cutlass::sizeof_bits<ElementC>::value < 8) {
+
+        return false;
+      }
+    }
+
+    return true;
+  }
+
   /// Runs the test
   bool run(
       cutlass::Distribution::Kind init_A = cutlass::Distribution::Uniform,
       cutlass::Distribution::Kind init_B = cutlass::Distribution::Uniform) {
+
+    if (!sufficient()) {
+      return true;
+    }
+
     //
     // initialize device memory
     //
 
     if (init_A == cutlass::Distribution::Uniform) {
       int scope_max = 8;
       int scope_min = -8;
@@ -874,18 +983,54 @@
     tensor_A.reset(cutlass::make_Coord(ThreadblockShape::kM, ThreadblockShape::kK));
     tensor_B.reset(cutlass::make_Coord(ThreadblockShape::kK, ThreadblockShape::kN));
     tensor_C.reset(cutlass::make_Coord(Shape::kM, Shape::kN));
     tensor_D_computed.reset(cutlass::make_Coord(Shape::kM, Shape::kN));
     tensor_D_reference.reset(cutlass::make_Coord(Shape::kM, Shape::kN), false);
   }
 
+  /// Returns true if the CUDA device is sufficient to execute the kernel.
+  bool sufficient() const {
+
+    cudaDeviceProp properties;
+    int device_idx;
+    cudaError_t result = cudaGetDevice(&device_idx);
+
+    if (result != cudaSuccess) {
+      throw std::runtime_error("cudaGetDevice() API call failed.");
+    }
+
+    result = cudaGetDeviceProperties(&properties, device_idx);
+
+    if (result != cudaSuccess) {
+      throw std::runtime_error("cudaGetDeviceProperties() failed");
+    }
+
+    if (properties.major == 9) {
+      // NVIDIA Hopper drops support for several data types
+      if (
+        cutlass::sizeof_bits<ElementA>::value < 8 ||
+        cutlass::sizeof_bits<ElementB>::value < 8 ||
+        cutlass::sizeof_bits<ElementC>::value < 8) {
+
+        return false;
+      }
+    }
+
+    return true;
+  }
+
   /// Runs the test
   bool run(
       cutlass::Distribution::Kind init_A = cutlass::Distribution::Uniform,
       cutlass::Distribution::Kind init_B = cutlass::Distribution::Uniform) {
+
+    if (!sufficient()) {
+      return true;
+    }
+
     //
     // initialize device memory
     //
 
     if (init_A == cutlass::Distribution::Uniform) {
       uint64_t seed = 7;
       cutlass::reference::host::TensorFillRandomUniform(tensor_A.host_view(),
@@ -1195,20 +1340,55 @@
     tensor_D_reference.reset(cutlass::make_Coord(Shape::kM, Shape::kN), false);
     tensor_E.reset(cutlass::make_Coord(
         Shape::kM, Shape::kK / Sparse / ElementsPerElementE));
     tensor_E_reordered.reset(cutlass::make_Coord(
         Shape::kM, Shape::kK / Sparse / ElementsPerElementE));
   }
 
+  /// Returns true if the CUDA device is sufficient to execute the kernel.
+  bool sufficient() const {
+
+    cudaDeviceProp properties;
+    int device_idx;
+    cudaError_t result = cudaGetDevice(&device_idx);
+
+    if (result != cudaSuccess) {
+      throw std::runtime_error("cudaGetDevice() API call failed.");
+    }
+
+    result = cudaGetDeviceProperties(&properties, device_idx);
+
+    if (result != cudaSuccess) {
+      throw std::runtime_error("cudaGetDeviceProperties() failed");
+    }
+
+    if (properties.major == 9) {
+      // NVIDIA Hopper drops support for several data types
+      if (
+        cutlass::sizeof_bits<ElementA>::value < 8 ||
+        cutlass::sizeof_bits<ElementB>::value < 8 ||
+        cutlass::sizeof_bits<ElementC>::value < 8) {
+
+        return false;
+      }
+    }
+
+    return true;
+  }
+
   /// Runs the test
   bool run(
       cutlass::Distribution::Kind init_A = cutlass::Distribution::Uniform,
       cutlass::Distribution::Kind init_B = cutlass::Distribution::Uniform,
       cutlass::Distribution::Kind init_E = cutlass::Distribution::Uniform) {
 
+    if (!sufficient()) {
+      return true;
+    }
+
     //
     // initialize device memory
     //
 
     if (init_A == cutlass::Distribution::Uniform) {
       int scope_max = 8;
       int scope_min = -8;
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm70.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/wmma_sm70.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm72.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/wmma_sm72.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm75.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/gemm/warp/wmma_sm75.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/layout/matrix.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/layout/matrix.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/layout/tensor.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/layout/tensor.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/layout/tensor_nhwc.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/layout/tensor_nhwc.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/cutlass/nvrtc/environment.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/nvrtc/cutlass/nvrtc/environment.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/kernel/thread/testbed_kernel.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/nvrtc/kernel/thread/testbed_kernel.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/stdlib/stdint.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/nvrtc/stdlib/stdint.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/gemm_nvrtc.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/nvrtc/thread/gemm_nvrtc.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/testbed.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/nvrtc/thread/testbed.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_contiguous.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/reduction/device/tensor_reduce_contiguous.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_strided.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/reduction/device/tensor_reduce_strided.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/reduction/kernel/reduce_splitk.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk_testbed.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/reduction/kernel/reduce_splitk_testbed.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/thread/reduction_thread.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/reduction/thread/reduction_thread.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/reduction/thread/testbed.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/reduction/thread/testbed.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/test_unit.cpp` & `flash_attn-2.0.0/csrc/cutlass/test/unit/test_unit.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/transform/threadblock/predicated_tile_iterator.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/transform/threadblock/predicated_tile_iterator.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/transform/threadblock/regular_tile_iterator_tensor_op.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/transform/threadblock/regular_tile_iterator_tensor_op.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/util/cutlass_test_levels.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/util/cutlass_test_levels.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/test/unit/util/tensor_reduce.cu` & `flash_attn-2.0.0/csrc/cutlass/test/unit/util/tensor_reduce.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/arch_mappings.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/include/cutlass/library/arch_mappings.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/handle.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/include/cutlass/library/handle.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/library.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/include/cutlass/library/library.h`

 * *Files 1% similar despite different names*

```diff
@@ -350,32 +350,37 @@
 
   /// Minimum compute capability (e.g. 70, 75) of a device eligible to run the operation.
   int minimum_compute_capability;
 
   /// Minimum compute capability (e.g. 70, 75) of a device eligible to run the operation.
   int maximum_compute_capability;
 
+  /// Describes the shape of a cluster (in blocks)
+  cutlass::gemm::GemmCoord cluster_shape;
+
   //
   // Methods
   //
 
   TileDescription(
     cutlass::gemm::GemmCoord threadblock_shape = cutlass::gemm::GemmCoord(),
     int threadblock_stages = 0,
     cutlass::gemm::GemmCoord warp_count = cutlass::gemm::GemmCoord(),
     MathInstructionDescription math_instruction = MathInstructionDescription(),
     int minimum_compute_capability = 0,
-    int maximum_compute_capability = 0
+    int maximum_compute_capability = 0,
+    cutlass::gemm::GemmCoord cluster_shape = cutlass::gemm::GemmCoord(1,1,1)
   ):
     threadblock_shape(threadblock_shape), 
     threadblock_stages(threadblock_stages), 
     warp_count(warp_count),
     math_instruction(math_instruction),
     minimum_compute_capability(minimum_compute_capability),
-    maximum_compute_capability(maximum_compute_capability) { }
+    maximum_compute_capability(maximum_compute_capability),
+    cluster_shape(cluster_shape) { }
 
   // Equality operator
   inline
   bool operator==(TileDescription const& rhs) const{
     return (
       (threadblock_shape == rhs.threadblock_shape) &&
       (threadblock_stages == rhs.threadblock_stages) &&
@@ -987,24 +992,33 @@
   int64_t lda;
   int64_t ldb;
   int64_t ldc;
   int64_t ldd;
 };
 
 struct GemmUniversalArguments {
+  // NOTE: these are replicated for 3.0 interfaces 
+  gemm::GemmCoord problem_size;
+  int batch_count;
 
   void const *A;
   void const *B;
   void const *C;
   void *D;
 
   void const *alpha;
   void const *beta;
   ScalarPointerMode pointer_mode;
 
+  // NOTE: these are replicated for 3.0 interfaces
+  int64_t lda;
+  int64_t ldb;
+  int64_t ldc;
+  int64_t ldd;
+
   int64_t batch_stride_A;
   int64_t batch_stride_B;
   int64_t batch_stride_C;
   int64_t batch_stride_D;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/manifest.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/include/cutlass/library/manifest.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/operation_table.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/include/cutlass/library/operation_table.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/singleton.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/include/cutlass/library/singleton.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/util.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/include/cutlass/library/util.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/compiler.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/compiler.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/cutlass.cpp` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/cutlass.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -25,16 +25,17 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /* \file
-   \brief binding cutlass C++ APIs to python
+   \brief binding CUTLASS C++ APIs to Python
 */
+
 #include <pybind11/pybind11.h>
 #include <pybind11/stl_bind.h>
 
 #include "builtin_types.h"
 #include "device_launch_parameters.h"
 #include "stddef.h"
 #include "cutlass/cutlass.h"
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/arch.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/arch.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/conv_problem_size.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/conv_problem_size.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/convolution.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/convolution.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/host.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/host.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_generic.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_generic.h`

 * *Files 0% similar despite different names*

```diff
@@ -30,14 +30,15 @@
  **************************************************************************************************/
 
 /*! \file
 
   \brief A generic wrapper around an epilogue visitor operation
 */
 
+
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/arch/memory.h"
 #include "cutlass/arch/memory_sm75.h"
 #include "cutlass/gemm/kernel/gemm_transpose_operands.h"
 #include "cutlass/gemm/kernel/default_gemm.h"
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/binary_ops.h` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/main.cpp`

 * *Files 14% similar despite different names*

```diff
@@ -10,75 +10,44 @@
  *
  * 2. Redistributions in binary form must reproduce the above copyright notice,
  * this list of conditions and the following disclaimer in the documentation
  * and/or other materials provided with the distribution.
  *
  * 3. Neither the name of the copyright holder nor the names of its
  * contributors may be used to endorse or promote products derived from
- * this layernormware without specific prior written permission.
+ * this software without specific prior written permission.
  *
  * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
  * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
  * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
  * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
-/*! \file
-
-  \brief Binary operations to be used within the epilogue visitor model.
+/* \file
+   \brief 
 */
 
-#pragma once
-#include "cutlass/cutlass.h"
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-namespace cutlass {
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-
-/// Elementwise addition of two arrays
-template <typename T, int N>
-struct VectorAdd {
-
-    struct Arguments {
-        int tmp;
-
-        CUTLASS_HOST_DEVICE
-        Arguments():tmp(0){ }
+#include <iostream>
 
-        CUTLASS_HOST_DEVICE
-        Arguments(int tmp): tmp(tmp) { }
-    };
-    
-    struct Params {
+#include "options.h"
 
-        CUTLASS_HOST_DEVICE
-        Params(Arguments const &args) { }
-    };
+#include "cutlass_profiler.h"
 
-    CUTLASS_HOST_DEVICE
-    VectorAdd(
-        Params const &params
-    ) { }
+///////////////////////////////////////////////////////////////////////////////////////////////////
 
-    CUTLASS_HOST_DEVICE
-    Array<T, N> operator()(Array<T, N> const &lhs, Array<T, N> const &rhs) const {
-        cutlass::plus<Array<T, N>> add_op;
-        return add_op(lhs, rhs);
-    }
+int main(int argc, char const *arg[]) {
 
-};
+  cutlass::CommandLine cmdline(argc, arg);
+  cutlass::profiler::Options options(cmdline);
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+  cutlass::profiler::CutlassProfiler profiler(options);
 
-} // namespace cutlass
+  return profiler();
+}
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+///////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/unary_ops.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/unary_ops.h`

 * *Files 2% similar despite different names*

```diff
@@ -26,16 +26,16 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-
-  \brief Unary operations to be used within the epilogue visitor model.
+  
+  \brief A file contains the unary ops
 */
 
 #pragma once
 #include "cutlass/cutlass.h"
 #include "cutlass/epilogue/thread/activation.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_accumulator.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_accumulator.h`

 * *Files 2% similar despite different names*

```diff
@@ -26,16 +26,16 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-
-  \brief Epilogue visitor operation that simply returns the accumulator
+  
+  \brief A file contains the epilogue visitor Op with accumulator
 */
 
 #pragma once
 #include "cutlass/cutlass.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_binary.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_binary.h`

 * *Files 2% similar despite different names*

```diff
@@ -26,16 +26,16 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-
-  \brief Epilogue visitor operator performing a binary operation between two visitor nodes
+  
+  \brief A file contains the epilogue visitor Op with Binary op
 */
 
 #pragma once
 #include "cutlass/cutlass.h"
 #include "binary_ops.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
@@ -80,15 +80,14 @@
 
     /// Fragment type returned by this visitor
     using VisitAccessType = Array<ElementCompute, kElementsPerAccess>; 
 
     /// Fragment type of accumulator
     using AccumulatorAccessType = Array<ElementAccumulator, kElementsPerAccess>;
 
-    /// Combination Op TODO: generalize this
     using BinaryOp = BinaryOp_<ElementCompute, kElementsPerAccess>;
 
     static_assert(kElementsPerAccess==VisitAccessTypeA::kElements, "kElementsPerAccess mismatches with Visitor A");
     static_assert(kElementsPerAccess==VisitAccessTypeB::kElements, "kElementsPerAccess misnatches with Visitor B");
 
     /// SMEM buffer class required in the epilogue visitor
     struct SharedStorage {
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_broadcast.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_broadcast.h`

 * *Files 1% similar despite different names*

```diff
@@ -26,16 +26,16 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-
-  \brief Epilogue visitor operation that broadcasts a vector to all columns
+  
+  \brief A file contains the epilogue visitor Op with broadcasting vector to all columns
 */
 
 #pragma once
 #include "cutlass/cutlass.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_reduction.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_reduction.h`

 * *Files 1% similar despite different names*

```diff
@@ -26,16 +26,16 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-
-  \brief Epilogue visitor operation that performs a column-wise reduction within a threadblock
+  
+  \brief A file contains the epilogue visitor Op with reduction over columns in CTA
 */
 
 #pragma once
 #include "cutlass/cutlass.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
@@ -64,15 +64,14 @@
     using ElementReduction = ElementReduction_;
     using OutputTileIterator = OutputTileIterator_;
     using ThreadblockShape = ThreadblockShape_;
     using Visitor = Visitor_;
 
     static int const kElementsPerAccess = OutputTileIterator::kElementsPerAccess;
 
-    // TODO: generalize the reduction op
     using ReductionOp = cutlass::plus<Array<ElementReductionAccumulator, kElementsPerAccess>>;
     using ReductionOpScalar = cutlass::plus<ElementReductionAccumulator>;
     using ElementOutput = typename OutputTileIterator::Element;
 
     
 
     /// Fragment type returned from Visitor
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_linear_combination.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_linear_combination.h`

 * *Files 2% similar despite different names*

```diff
@@ -26,16 +26,16 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-
-  \brief Epilogue visitor operation that performs a linear combination of two visitor nodes
+  
+  \brief A file contains the epilogue visitor Op with Linear Combination
 */
 
 #pragma once
 #include "cutlass/cutlass.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
@@ -78,15 +78,15 @@
 
     /// Fragment type returned by this visitor
     using VisitAccessType = Array<ElementCompute, kElementsPerAccess>; 
 
     /// Fragment type of accumulator
     using AccumulatorAccessType = Array<ElementAccumulator, kElementsPerAccess>;
 
-    /// Combination Op TODO: generalize this
+    /// Combination Op
     using CombinationOp = cutlass::plus<VisitAccessType>;
 
     static_assert(kElementsPerAccess==VisitAccessTypeA::kElements, "kElementsPerAccess mismatches with Visitor A");
     static_assert(kElementsPerAccess==VisitAccessTypeB::kElements, "kElementsPerAccess misnatches with Visitor B");
 
     /// SMEM buffer class required in the epilogue visitor
     struct SharedStorage {
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_broadcast.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_broadcast.h`

 * *Files 2% similar despite different names*

```diff
@@ -26,16 +26,16 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-
-  \brief Epilogue visitor operation that broadcasts a vector to all rows
+  
+  \brief A file contains the epilogue visitor Op with broadcasting vector to all rows
 */
 
 #pragma once
 #include "cutlass/cutlass.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_reduction.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_reduction.h`

 * *Files 2% similar despite different names*

```diff
@@ -26,16 +26,16 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-
-  \brief Epilogue visitor operation that performs a column-wise reduction within a threadblock
+  
+  \brief A file contains the epilogue visitor Op with reduction over rows in CTA
 */
 
 #pragma once
 #include "cutlass/cutlass.h"
 #include "stdio.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
@@ -65,15 +65,14 @@
     using ElementReduction = ElementReduction_;
     using OutputTileIterator = OutputTileIterator_;
     using ThreadblockShape = ThreadblockShape_;
     using Visitor = Visitor_;
 
     static int const kElementsPerAccess = OutputTileIterator::kElementsPerAccess;
 
-    // TODO: generalize the reduction op
     using ReductionOp = cutlass::plus<Array<ElementReductionAccumulator, kElementsPerAccess>>;
     using ReductionOpScalar = cutlass::plus<ElementReductionAccumulator>;
     using ElementOutput = typename OutputTileIterator::Element;
 
     /// Fragment type returned from Visitor
     using VisitAccessTypeVisitor = typename Visitor::VisitAccessType;
     using ElementVisitor = typename VisitAccessTypeVisitor::Element;
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_input.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_input.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_output.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_output.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_unary.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_unary.h`

 * *Files 2% similar despite different names*

```diff
@@ -26,16 +26,16 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-
-  \brief Epilogue visitor operator performing a unary operation atop a visitor node
+  
+  \brief A file contains the epilogue visitor Op with Unary operation
 */
 
 #pragma once
 #include "cutlass/cutlass.h"
 #include "unary_ops.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
@@ -75,15 +75,15 @@
 
     /// Fragment type returned by this visitor
     using VisitAccessType = Array<ElementCompute, kElementsPerAccess>; 
 
     /// Fragment type of accumulator
     using AccumulatorAccessType = Array<ElementAccumulator, kElementsPerAccess>;
 
-    /// Combination Op TODO: generalize this
+    /// Combination Op
     using UnaryOp = UnaryOp_<ElementCompute, kElementsPerAccess>;
 
     static_assert(kElementsPerAccess==VisitAccessTypeVisitor::kElements, "kElementsPerAccess mismatches with Visitor");
 
     /// SMEM buffer class required in the epilogue visitor
     struct SharedStorage {
         typename Visitor::SharedStorage storage_visitor;
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_with_layernorm.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_with_layernorm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/gemm.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/gemm_universal_with_visitor.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/gemm_universal_with_visitor.h`

 * *Files 3% similar despite different names*

```diff
@@ -26,15 +26,15 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-    \brief 
+    \brief
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/fast_math.h"
 #include "cutlass/gemm/gemm.h"
@@ -135,16 +135,16 @@
     int const * ptr_gather_A_indices;
     int const * ptr_gather_B_indices;
     int const * ptr_scatter_D_indices;
 
     //
     // Methods
     //
-    
-    Arguments(): 
+
+    Arguments():
       ptr_A(nullptr), ptr_B(nullptr), ptr_C(nullptr), ptr_D(nullptr),
       ptr_gather_A_indices(nullptr),
       ptr_gather_B_indices(nullptr),
       ptr_scatter_D_indices(nullptr) {}
 
     /// constructs an arguments structure
     Arguments(
@@ -165,16 +165,16 @@
       typename LayoutC::Stride stride_c,
       typename LayoutC::Stride stride_d,
       int const *ptr_gather_A_indices = nullptr,
       int const *ptr_gather_B_indices = nullptr,
       int const *ptr_scatter_D_indices = nullptr
     ):
       UniversalArgumentsBase(mode, problem_size, batch_count, batch_stride_D),
-      epilogue_visitor(epilogue_visitor), 
-      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D), 
+      epilogue_visitor(epilogue_visitor),
+      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D),
       batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C),
       stride_a(stride_a), stride_b(stride_b), stride_c(stride_c), stride_d(stride_d),
       ptr_gather_A_indices(ptr_gather_A_indices), ptr_gather_B_indices(ptr_gather_B_indices),
       ptr_scatter_D_indices(ptr_scatter_D_indices) {
       lda = 0;
       ldb = 0;
       ldc = 0;
@@ -201,31 +201,31 @@
       typename LayoutC::Stride::LongIndex ldc,
       typename LayoutC::Stride::LongIndex ldd,
       int const *ptr_gather_A_indices = nullptr,
       int const *ptr_gather_B_indices = nullptr,
       int const *ptr_scatter_D_indices = nullptr
     ):
       UniversalArgumentsBase(mode, problem_size, batch_count, batch_stride_D),
-      epilogue_visitor(epilogue_visitor), 
-      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D), 
+      epilogue_visitor(epilogue_visitor),
+      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D),
       batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C),
       lda(lda), ldb(ldb), ldc(ldc), ldd(ldd),
       ptr_gather_A_indices(ptr_gather_A_indices), ptr_gather_B_indices(ptr_gather_B_indices),
       ptr_scatter_D_indices(ptr_scatter_D_indices) {
       stride_a = make_Coord(lda);
       stride_b = make_Coord(ldb);
       stride_c = make_Coord(ldc);
       stride_d = make_Coord(ldd);
       CUTLASS_TRACE_HOST("GemmUniversal::Arguments::Arguments() - problem_size: " << problem_size);
       }
 
     /// Returns arguments for the transposed problem
     Arguments transposed_problem() const {
       Arguments args(*this);
-      
+
       std::swap(args.problem_size.m(), args.problem_size.n());
       std::swap(args.ptr_A, args.ptr_B);
       std::swap(args.lda, args.ldb);
       std::swap(args.stride_a, args.stride_b);
       std::swap(args.batch_stride_A, args.batch_stride_B);
       std::swap(args.ptr_gather_A_indices, args.ptr_gather_B_indices);
 
@@ -252,15 +252,15 @@
       ElementB,
       ElementC>;
 
     typename Mma::IteratorA::Params params_A;
     typename Mma::IteratorB::Params params_B;
     typename EpilogueVisitor::OutputTileIterator::Params params_C;
     typename EpilogueVisitor::OutputTileIterator::Params params_D;
-    
+
     typename EpilogueVisitor::Params epilogue_visitor;
 
     void * ptr_A;
     void * ptr_B;
     void * ptr_C;
     void * ptr_D;
 
@@ -321,15 +321,15 @@
       ptr_scatter_D_indices = const_cast<int *>(args.ptr_scatter_D_indices);
 
       batch_stride_A = args.batch_stride_A;
       batch_stride_B = args.batch_stride_B;
       batch_stride_C = args.batch_stride_C;
 
       epilogue_visitor = args.epilogue_visitor;
-      
+
       semaphore = static_cast<int *>(workspace);
       CUTLASS_TRACE_HOST("GemmUniversal::Params::update()");
     }
   };
 
   /// Shared memory storage structure
   union SharedStorage {
@@ -341,15 +341,15 @@
 public:
 
   //
   // Methods
   //
 
   CUTLASS_DEVICE
-  GemmUniversalwithEpilogueVisitor() { } 
+  GemmUniversalwithEpilogueVisitor() { }
 
   /// Determines whether kernel satisfies alignment
   static Status can_implement(
     cutlass::gemm::GemmCoord const & problem_size) {
 
     CUTLASS_TRACE_HOST("GemmUniversalwithEpilogueVisitor::can_implement()");
 
@@ -451,20 +451,20 @@
 
     ElementA *ptr_A = static_cast<ElementA *>(params.ptr_A);
     ElementB *ptr_B = static_cast<ElementB *>(params.ptr_B);
 
     //
     // Fetch pointers based on mode.
     //
-    if (params.mode == GemmUniversalMode::kGemm || 
+    if (params.mode == GemmUniversalMode::kGemm ||
       params.mode == GemmUniversalMode::kGemmSplitKParallel) {
 
       if (threadblock_tile_offset.k() + 1 < params.grid_tiled_shape.k()) {
 
-        problem_size_k = (threadblock_tile_offset.k() + 1) * params.gemm_k_size; 
+        problem_size_k = (threadblock_tile_offset.k() + 1) * params.gemm_k_size;
       }
 
       offset_k = threadblock_tile_offset.k() * params.gemm_k_size;
     }
     else if (params.mode == GemmUniversalMode::kBatched) {
       ptr_A += threadblock_tile_offset.k() * params.batch_stride_A;
       ptr_B += threadblock_tile_offset.k() * params.batch_stride_B;
@@ -525,18 +525,18 @@
     accumulators.clear();
 
     // Compute threadblock-scoped matrix multiply-add
     int gemm_k_iterations = (problem_size_k - offset_k + Mma::Shape::kK - 1) / Mma::Shape::kK;
 
     // Compute threadblock-scoped matrix multiply-add
     mma(
-      gemm_k_iterations, 
-      accumulators, 
-      iterator_A, 
-      iterator_B, 
+      gemm_k_iterations,
+      accumulators,
+      iterator_A,
+      iterator_B,
       accumulators);
 
     //
     // Epilogue
     //
 
     // EpilogueOutputOp output_op(params.output_op);
@@ -551,95 +551,73 @@
     MatrixCoord threadblock_offset(
       threadblock_tile_offset.m() * Mma::Shape::kM,
       threadblock_tile_offset.n() * Mma::Shape::kN
     );
 
     int block_idx = threadblock_tile_offset.m() + threadblock_tile_offset.n() * params.grid_tiled_shape.m();
 
-    ElementC *ptr_C = static_cast<ElementC *>(params.ptr_C); 
+    ElementC *ptr_C = static_cast<ElementC *>(params.ptr_C);
     ElementC *ptr_D = static_cast<ElementC *>(params.ptr_D);
 
     //
     // Fetch pointers based on mode.
     //
-    
+
     // Construct the semaphore.
     Semaphore semaphore(params.semaphore + block_idx, thread_idx);
 
-    // if (params.mode == GemmUniversalMode::kGemm) {
-
-    //   // TODO: fix this order
-    //   // If performing a reduction via split-K, fetch the initial synchronization
-    //   if (params.grid_tiled_shape.k() > 1) {
-        
-    //     // Fetch the synchronization lock initially but do not block.
-    //     semaphore.fetch();
-
-    //     // Indicate which position in a serial reduction the output operator is currently updating
-    //     output_op.set_k_partition(threadblock_tile_offset.k(), params.grid_tiled_shape.k());
-    //   }
-    // }
-    
     // Tile iterator loading from source tensor.
 
     EpilogueVisitor epilogue_visitor(
         params.epilogue_visitor,
         shared_storage.visitor,
         threadblock_offset,
         threadblock_tile_offset,
         thread_idx,
         params.problem_size.mn()
     );
 
-    // if (params.mode == GemmUniversalMode::kGemmSplitKParallel) {
-    //   ptr_D += threadblock_tile_offset.k() * params.batch_stride_D;
-    // }
     if (params.mode == GemmUniversalMode::kBatched || params.mode == GemmUniversalMode::kArray) {
       epilogue_visitor.set_batch_index(threadblock_tile_offset.k());
     }
 
     Epilogue epilogue(
       shared_storage.epilogue,
       thread_idx,
       warp_idx,
       lane_idx);
 
     // Wait on the semaphore - this latency may have been covered by iterator construction
     if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) {
-        
-      // For subsequent threadblocks, the source matrix is held in the 'D' tensor.
-      // TODO: ???
-      // if (threadblock_tile_offset.k()) {
-      //   iterator_C = iterator_D;
-      // }
 
+      // For subsequent threadblocks, the source matrix is held in the 'D' tensor.
       semaphore.wait(threadblock_tile_offset.k());
     }
 
 
     // Execute the epilogue operator to update the destination tensor.
-    epilogue(epilogue_visitor, accumulators); 
-    
+    epilogue(epilogue_visitor, accumulators);
+
     //
     // Release the semaphore
     //
 
-    if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) { 
+    if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) {
 
       int lock = 0;
       if (params.grid_tiled_shape.k() == threadblock_tile_offset.k() + 1) {
 
         // The final threadblock resets the semaphore for subsequent grids.
         lock = 0;
       }
       else {
         // Otherwise, the semaphore is incremented
         lock = threadblock_tile_offset.k() + 1;
       }
-      
+
       semaphore.release(lock);
     }
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/host.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/host.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/layout.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/layout.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/matrix.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/matrix.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/tensor.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/tensor.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/swizzling.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/swizzling.h`

 * *Files 2% similar despite different names*

```diff
@@ -79,15 +79,14 @@
                 &T::get_tiled_shape, py::const_
             ), py::arg("conv_operator"), py::arg("problem_size"), py::arg("tile_size"), py::arg("split_k_slices"),
             R"pbdoc(Returns the shape of the problem in units of logical tiles
             
             :param problem_size: Implicit gemm problem size conv_operator(NZPQK, NDHWC, KTRSC)
             :type problem_size: :class:`cutlass.gemm.GemmCoord`)
             )pbdoc")
-        // TODO: the returned dim3 is not usable in python
         .def("get_grid_shape", &T::get_grid_shape,
             py::arg("tiled_shape"), 
             R"pbdoc(Computes CUDA grid dimensions given a size in units of logical tiles)pbdoc")
         .def("tag", [](const T & swizzle){
             return demangle(typeid(T).name());
         }, R"pbdoc(Returns the c++ name of the swizzling for code emittion)pbdoc");
 }
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_coord.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_coord.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_ref_view.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_ref_view.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/types.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/include/types.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/library.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/library.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/conv_problems.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/conv_problems.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/convolution.h` & `flash_attn-2.0.0/csrc/cutlass/test/unit/cute/core/transform.cpp`

 * *Files 14% similar despite different names*

```diff
@@ -24,26 +24,26 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/* \file
-   \brief Bind convolution related types to python
-*/
-#pragma once
-#include <pybind11/pybind11.h>
-#include <pybind11/stl_bind.h>
 
-#include "conv_problems.h"
-#include "host.h"
+#include "cutlass_unit_test.h"
 
-namespace py = pybind11;
+#include <cutlass/trace.h>
+#include <cute/tensor.hpp>
+#include <cute/numeric/complex.hpp>
 
-void bind_convolution_test(py::module &m) {
-    // Conv problem sizes
-    bind_conv_problem_size_test(m);
-
-    py::module_ host_submodule = m.def_submodule("host");
-    bind_conv_host_references(host_submodule);
+TEST(CuTe_core, Transform) {
+  using namespace cute;
+  complex<float> array[4] = {{0,0}, {1,0}, {0,1}, {1,1}};
+  complex<float> correct[4] = {{0,0}, {1,0}, {0,-1}, {1,-1}};
+  auto tensor = make_tensor(static_cast<complex<float>*>(array), make_layout(make_shape(4)));
+  conjugate conj;
+  transform(tensor, conj);
+  for (int i = 0; i < 4; ++i)
+  {
+    EXPECT_EQ(tensor(i), correct[i]);
+  }
 }
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/host.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/host.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/gemm.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/host.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/host.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/conv2d_operation.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/src/conv2d_operation.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/conv3d_operation.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/src/conv3d_operation.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/gemm_operation.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/src/gemm_operation.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/handle.cu` & `flash_attn-2.0.0/csrc/cutlass/tools/library/src/handle.cu`

 * *Files 1% similar despite different names*

```diff
@@ -618,21 +618,27 @@
   if (uint64_t(kHostWorkspaceSize) < host_workspace_size_needed) {
     return cutlass::Status::kErrorNotSupported;
   }
 
   char host_workspace[kHostWorkspaceSize];
 
   GemmUniversalArguments arguments{
+    {M, N, K},
+    batch_count,
     ptr_A,
     ptr_B,
     ptr_C,
     ptr_D,
     alpha,
     beta,
     scalar_pointer_mode_,
+    lda,
+    ldb,
+    ldc,
+    ldd,
     batch_stride_A,
     batch_stride_B,
     batch_stride_C,
     batch_stride_D
   };
 
   // Query device workspace size
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/library_internal.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/src/library_internal.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/manifest.cpp` & `flash_attn-2.0.0/csrc/cutlass/tools/library/src/manifest.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/operation_table.cu` & `flash_attn-2.0.0/csrc/cutlass/tools/library/src/operation_table.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/rank_2k_operation.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/src/rank_2k_operation.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/rank_k_operation.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/src/rank_k_operation.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reduction/init_reduction_operations.cu` & `flash_attn-2.0.0/csrc/cutlass/tools/library/src/reduction/init_reduction_operations.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reduction/reduction_device.cu` & `flash_attn-2.0.0/csrc/cutlass/tools/library/src/reduction/reduction_device.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reduction/reduction_operation.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/src/reduction/reduction_operation.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reference/conv2d.cu` & `flash_attn-2.0.0/csrc/cutlass/tools/library/src/reference/conv2d.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reference/conv3d.cu` & `flash_attn-2.0.0/csrc/cutlass/tools/library/src/reference/conv3d.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reference/conv_reference_operation.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/src/reference/conv_reference_operation.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reference/gemm.cu` & `flash_attn-2.0.0/csrc/cutlass/tools/library/src/reference/gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reference/gemm_reference_operation.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/src/reference/gemm_reference_operation.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/reference/initialize_reference_operations.cu` & `flash_attn-2.0.0/csrc/cutlass/tools/library/src/reference/initialize_reference_operations.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/singleton.cu` & `flash_attn-2.0.0/csrc/cutlass/tools/library/src/singleton.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/symm_operation.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/src/symm_operation.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/trmm_operation.h` & `flash_attn-2.0.0/csrc/cutlass/tools/library/src/trmm_operation.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/library/src/util.cu` & `flash_attn-2.0.0/csrc/cutlass/tools/library/src/util.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/conv2d_operation_profiler.cu` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/conv2d_operation_profiler.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/conv2d_operation_profiler.h` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/conv2d_operation_profiler.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/conv3d_operation_profiler.cu` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/conv3d_operation_profiler.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/conv3d_operation_profiler.h` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/conv3d_operation_profiler.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/cublas_helpers.cu` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/cublas_helpers.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/cublas_helpers.h` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/cublas_helpers.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/cudnn_helpers.cpp` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/cudnn_helpers.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/cudnn_helpers.h` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/cudnn_helpers.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/cutlass_profiler.cu` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/cutlass_profiler.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/cutlass_profiler.h` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/cutlass_profiler.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/debug.h` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/debug.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/device_allocation.cu` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/device_allocation.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/device_allocation.h` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/device_allocation.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/device_context.cu` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/device_context.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/device_context.h` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/device_context.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/enumerated_types.cpp` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/enumerated_types.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/enumerated_types.h` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/enumerated_types.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/gemm_operation_profiler.cu` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/gemm_operation_profiler.cu`

 * *Files 1% similar despite different names*

```diff
@@ -533,14 +533,21 @@
       {int(problem_.m), int(problem_.n)},
       {int(problem_.ldc)},
       problem_.batch_count * gemm_workspace_.problem_count
     );
 
     gemm_workspace_.Reference->copy_from_device(gemm_workspace_.C->data());
 
+    // NOTE: the leading non-batch strides are duplicated here for 3.0 API kernels
+    gemm_workspace_.arguments.problem_size = {int(problem_.m), int(problem_.n), int(problem_.k)};
+    gemm_workspace_.arguments.batch_count = problem_.batch_count;
+    gemm_workspace_.arguments.lda = problem_.lda;
+    gemm_workspace_.arguments.ldb = problem_.ldb;
+    gemm_workspace_.arguments.ldc = problem_.ldc;
+    gemm_workspace_.arguments.ldd = problem_.ldc;
     gemm_workspace_.arguments.batch_stride_A = gemm_workspace_.A->batch_stride();
     gemm_workspace_.arguments.batch_stride_B = gemm_workspace_.B->batch_stride();
     gemm_workspace_.arguments.batch_stride_C = gemm_workspace_.C->batch_stride();
     gemm_workspace_.arguments.batch_stride_D = gemm_workspace_.Computed->batch_stride();
   }
 
   //
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/gemm_operation_profiler.h` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/gemm_operation_profiler.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/gpu_timer.cpp` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/gpu_timer.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/gpu_timer.h` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/gpu_timer.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/main.cpp` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/tensor_norm.h`

 * *Files 9% similar despite different names*

```diff
@@ -24,30 +24,19 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/* \file
-   \brief 
-*/
+#pragma once
 
-#include <iostream>
 
-#include "options.h"
+#include "cutlass/cutlass.h"
 
-#include "cutlass_profiler.h"
+// The contents of this file have been moved  to 'tensor_reduce' to cover other types of reductions.
 
-///////////////////////////////////////////////////////////////////////////////////////////////////
-
-int main(int argc, char const *arg[]) {
-
-  cutlass::CommandLine cmdline(argc, arg);
-  cutlass::profiler::Options options(cmdline);
+#include "cutlass/util/reference/host/tensor_reduce.h"
 
-  cutlass::profiler::CutlassProfiler profiler(options);
+///////////////////////////////////////////////////////////////////////////////////////////////////
 
-  return profiler();
-}
 
-///////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/operation_profiler.cu` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/operation_profiler.cu`

 * *Files 15% similar despite different names*

```diff
@@ -71,14 +71,17 @@
 
   ArgumentDescriptionVector tile_description_arguments{
     {ArgumentTypeID::kEnumerated, {"op_class", "opcode-class"}, "Class of math instruction (simt, tensorop, wmmatensorop, wmma)"},
     {ArgumentTypeID::kEnumerated, {"accum", "accumulator-type"}, "Math instruction accumulator data type"},
     {ArgumentTypeID::kInteger, {"cta_m", "threadblock-shape::m"}, "Threadblock shape in the M dimension"},
     {ArgumentTypeID::kInteger, {"cta_n", "threadblock-shape::n"}, "Threadblock shape in the N dimension"},
     {ArgumentTypeID::kInteger, {"cta_k", "threadblock-shape::k"}, "Threadblock shape in the K dimension"},
+    {ArgumentTypeID::kInteger, {"cluster_m", "cluster-shape::m"}, "Cluster shape in the M dimension"},
+    {ArgumentTypeID::kInteger, {"cluster_n", "cluster-shape::n"}, "Cluster shape in the N dimension"},
+    {ArgumentTypeID::kInteger, {"cluster_k", "cluster-shape::k"}, "Cluster shape in the K dimension"},
     {ArgumentTypeID::kInteger, {"stages", "threadblock-stages"}, "Number of stages of threadblock-scoped matrix multiply"},
     {ArgumentTypeID::kInteger, {"warps_m", "warp-count::m"}, "Number of warps within threadblock along the M dimension"},
     {ArgumentTypeID::kInteger, {"warps_n", "warp-count::n"}, "Number of warps within threadblock along the N dimension"},
     {ArgumentTypeID::kInteger, {"warps_k", "warp-count::k"}, "Number of warps within threadblock along the K dimension"},
     {ArgumentTypeID::kInteger, {"inst_m", "instruction-shape::m"}, "Math instruction shape in the M dimension"},
     {ArgumentTypeID::kInteger, {"inst_n", "instruction-shape::n"}, "Math instruction shape in the N dimension"},
     {ArgumentTypeID::kInteger, {"inst_k", "instruction-shape::k"}, "Math instruction shape in the K dimension"},
@@ -194,14 +197,32 @@
 
   if (arg_as_int(int_value, "cta_k", problem_space, problem)) {
     if (int64_t(op_desc.tile_description.threadblock_shape.k()) != int_value) {
       return false;
     }
   }
 
+  if (arg_as_int(int_value, "cluster_m", problem_space, problem)) {
+    if (int64_t(op_desc.tile_description.cluster_shape.m()) != int_value) {
+      return false;
+    }
+  }
+
+  if (arg_as_int(int_value, "cluster_n", problem_space, problem)) {
+    if (int64_t(op_desc.tile_description.cluster_shape.n()) != int_value) {
+      return false;
+    }
+  }
+
+  if (arg_as_int(int_value, "cluster_k", problem_space, problem)) {
+    if (int64_t(op_desc.tile_description.cluster_shape.k()) != int_value) {
+      return false;
+    }
+  }
+
   if (arg_as_int(int_value, "stages", problem_space, problem)) {
     if (int64_t(op_desc.tile_description.threadblock_stages) != int_value) {
       return false;
     }
   }
 
   if (arg_as_int(int_value, "warps_m", problem_space, problem)) {
@@ -592,14 +613,17 @@
 
   set_argument(result, "accum", problem_space,
     library::to_string(operation_desc.tile_description.math_instruction.element_accumulator));
 
   set_argument(result, "cta_m", problem_space, operation_desc.tile_description.threadblock_shape.m());
   set_argument(result, "cta_n", problem_space, operation_desc.tile_description.threadblock_shape.n());
   set_argument(result, "cta_k", problem_space, operation_desc.tile_description.threadblock_shape.k());
+  set_argument(result, "cluster_m", problem_space, operation_desc.tile_description.cluster_shape.m());
+  set_argument(result, "cluster_n", problem_space, operation_desc.tile_description.cluster_shape.n());
+  set_argument(result, "cluster_k", problem_space, operation_desc.tile_description.cluster_shape.k());
   set_argument(result, "stages", problem_space, operation_desc.tile_description.threadblock_stages);
   set_argument(result, "warps_m", problem_space, operation_desc.tile_description.warp_count.m());
   set_argument(result, "warps_n", problem_space, operation_desc.tile_description.warp_count.n());
   set_argument(result, "warps_k", problem_space, operation_desc.tile_description.warp_count.k());
   set_argument(result, "inst_m", problem_space, operation_desc.tile_description.math_instruction.instruction_shape.m());
   set_argument(result, "inst_n", problem_space, operation_desc.tile_description.math_instruction.instruction_shape.n());
   set_argument(result, "inst_k", problem_space, operation_desc.tile_description.math_instruction.instruction_shape.k());
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/operation_profiler.h` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/operation_profiler.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/options.cu` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/options.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/options.h` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/options.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/performance_report.cpp` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/performance_report.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/performance_report.h` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/performance_report.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/performance_result.cu` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/performance_result.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/performance_result.h` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/performance_result.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/problem_space.cpp` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/problem_space.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/problem_space.h` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/problem_space.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/rank_2k_operation_profiler.cu` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/rank_2k_operation_profiler.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/rank_2k_operation_profiler.h` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/rank_2k_operation_profiler.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/rank_k_operation_profiler.cu` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/rank_k_operation_profiler.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/rank_k_operation_profiler.h` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/rank_k_operation_profiler.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/reduction_operation_profiler.h` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/reduction_operation_profiler.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.cu` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.h` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/symm_operation_profiler.cu` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/symm_operation_profiler.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/symm_operation_profiler.h` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/symm_operation_profiler.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/trmm_operation_profiler.cu` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/trmm_operation_profiler.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/profiler/src/trmm_operation_profiler.h` & `flash_attn-2.0.0/csrc/cutlass/tools/profiler/src/trmm_operation_profiler.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/command_line.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/command_line.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/debug.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/debug.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_dump.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/device_dump.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_groupnorm.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/device_groupnorm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_layernorm.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/device_layernorm.h`

 * *Files 0% similar despite different names*

```diff
@@ -452,15 +452,15 @@
   const T* beta = ref_beta.data();
   dim3 grid(m);
   dim3 block((n + 31)/32*32);
   if (block.x > 1024){
     block.x = 1024;
   }
   // TODO : There should be better configs for different cases, we only use several samples to show how to use here
-  // TODO : using registers to store values locally can reduce the ldgs from global memory and speedup the kernels.
+  // TODO : using registers to store values locally can reduce the loads from global memory and speedup the kernels.
   if ((n % 4 == 0) && (n >= 128) && (n <= 4096)) {
     block.x = (n/4 + 31)/32*32;
     if (std::is_same<T, float>::value) {
       layernorm_twoPassAlgo_stored_locally_e4<float4, float, 1><<<grid, block, 0, stream>>>(
         (float4*)output,
         (const float4*)input,
         (const float4*)gamma,
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_memory.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/device_memory.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nchw_to_nhwc.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/device_nchw_to_nhwc.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_padding.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/device_nhwc_padding.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_pooling.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/device_nhwc_pooling.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_to_nchw.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/device_nhwc_to_nchw.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_utils.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/device_utils.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/distribution.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/distribution.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/exceptions.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/exceptions.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_reorder.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/host_reorder.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_tensor.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/host_tensor.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_tensor_planar_complex.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/host_tensor_planar_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_uncompress.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/host_uncompress.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/index_sequence.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/index_sequence.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/inner_product.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/detail/inner_product.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/linear_to_coordinate.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/detail/linear_to_coordinate.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/convolution.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/convolution.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm_complex.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/gemm_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm_planar_complex.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/gemm_planar_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/gemm.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/kernel/gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_elementwise.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_elementwise.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_foreach.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_foreach.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/rank_2k_complex.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/rank_2k_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_compare.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/tensor_compare.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_fill.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/tensor_fill.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_foreach.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/tensor_foreach.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_reduce.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/tensor_reduce.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_relu.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/tensor_relu.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/thread/gemm.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/device/thread/gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/convolution.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/convolution.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/error_metrics.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/error_metrics.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm_complex.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/gemm_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm_planar_complex.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/gemm_planar_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k_complex.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_k_complex.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/rank_k_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/symm.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/symm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/symm_complex.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/symm_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_compare.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/tensor_compare.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_copy.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/tensor_copy.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_elementwise.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/tensor_elementwise.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_fill.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/tensor_fill.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_foreach.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/tensor_foreach.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_norm.h` & `flash_attn-2.0.0/csrc/cutlass/examples/45_dual_gemm/dual_gemm_common.h`

 * *Files 15% similar despite different names*

```diff
@@ -24,19 +24,29 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
+/*! \file
+    \brief Defines common types used for all DualGemm operators.
+*/
 #pragma once
 
+namespace cutlass {
+namespace gemm {
 
-#include "cutlass/cutlass.h"
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-// The contents of this file have been moved  to 'tensor_reduce' to cover other types of reductions.
+enum class DualGemmMode {
+  kGemm,
+  kBatched,
+  kInvalid
+};
 
-#include "cutlass/util/reference/host/tensor_reduce.h"
-
-///////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
+} // namespace gemm
+} // namespace cutlass
 
+////////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_reduce.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/tensor_reduce.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/trmm.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/trmm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/trmm_complex.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/reference/host/trmm_complex.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/tensor_view_io.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/tensor_view_io.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/type_traits.h` & `flash_attn-2.0.0/csrc/cutlass/tools/util/include/cutlass/util/type_traits.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/flash_attn/fmha_api.cpp` & `flash_attn-2.0.0/csrc/flash_attn/flash_api.cpp`

 * *Files 26% similar despite different names*

```diff
@@ -1,796 +1,912 @@
 /******************************************************************************
- * Copyright (c) 2022, Tri Dao.
- * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
- * 
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *     * Redistributions of source code must retain the above copyright
- *       notice, this list of conditions and the following disclaimer.
- *     * Redistributions in binary form must reproduce the above copyright
- *       notice, this list of conditions and the following disclaimer in the
- *       documentation and/or other materials provided with the distribution.
- *     * Neither the name of the NVIDIA CORPORATION nor the
- *       names of its contributors may be used to endorse or promote products
- *       derived from this software without specific prior written permission.
- * 
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
- * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
- * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
- * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
- * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
- * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
- * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
- * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
- * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- *
+ * Copyright (c) 2023, Tri Dao.
  ******************************************************************************/
 
 #include <torch/extension.h>
 #include <ATen/cuda/CUDAContext.h>
 #include <c10/cuda/CUDAGuard.h>
 
-#include "fmha.h"
+#include <cutlass/numeric_types.h>
+
+#include "flash.h"
+#include "static_switch.h"
 
 #define CHECK_SHAPE(x, ...) TORCH_CHECK(x.sizes() == torch::IntArrayRef({__VA_ARGS__}), #x " must have shape (" #__VA_ARGS__ ")")
 
 
-void set_params_fprop(FMHA_fprop_params &params,
+void set_params_fprop(Flash_fwd_params &params,
                       // sizes
                       const size_t b,
                       const size_t seqlen_q,
                       const size_t seqlen_k,
+                      const size_t seqlen_q_rounded,
+                      const size_t seqlen_k_rounded,
                       const size_t h,
+                      const size_t h_k,
                       const size_t d,
+                      const size_t d_rounded,
                       // device pointers
                       const at::Tensor q,
                       const at::Tensor k,
                       const at::Tensor v,
                       at::Tensor out,
                       void *cu_seqlens_q_d,
                       void *cu_seqlens_k_d,
-                      void *o_tmp_d,
-                      void *s_d,
+                      void *p_d,
                       void *softmax_lse_d,
                       float p_dropout,
                       float softmax_scale,
-                      bool is_causal,
-                      int num_splits) {
-
-    Data_type acc_type = DATA_TYPE_FP32;
-    Data_type data_type = !(q.dtype() == torch::kBFloat16) ? DATA_TYPE_FP16 : DATA_TYPE_BF16;
+                      bool is_causal) {
 
     // Reset the parameters
     memset(&params, 0, sizeof(params));
 
     params.is_bf16 = q.dtype() == torch::kBFloat16;
 
     // Set the pointers and strides.
     params.q_ptr = q.data_ptr();
     params.k_ptr = k.data_ptr();
     params.v_ptr = v.data_ptr();
-    params.q_row_stride_in_elts = q.stride(0);
-    params.k_row_stride_in_elts = k.stride(0);
-    params.v_row_stride_in_elts = v.stride(0);
-    params.q_head_stride_in_elts = q.stride(1);
-    params.k_head_stride_in_elts = k.stride(1);
-    params.v_head_stride_in_elts = v.stride(1);
+    // All stride are in elements, not bytes.
+    params.q_row_stride = q.stride(-3);
+    params.k_row_stride = k.stride(-3);
+    params.v_row_stride = v.stride(-3);
+    params.q_head_stride = q.stride(-2);
+    params.k_head_stride = k.stride(-2);
+    params.v_head_stride = v.stride(-2);
     params.o_ptr = out.data_ptr();
-    params.o_row_stride_in_elts = out.stride(0);
-    params.o_head_stride_in_elts = out.stride(1);
-    params.o_tmp_ptr = o_tmp_d;
-    params.o_tmp_row_stride_in_elts = h * d;
-    params.o_tmp_head_stride_in_elts = d;
+    params.o_row_stride = out.stride(-3);
+    params.o_head_stride = out.stride(-2);
+
+    if (cu_seqlens_q_d == nullptr) {
+        params.q_batch_stride = q.stride(0);
+        params.k_batch_stride = k.stride(0);
+        params.v_batch_stride = v.stride(0);
+        params.o_batch_stride = out.stride(0);
+    }
 
     params.cu_seqlens_q = static_cast<int *>(cu_seqlens_q_d);
     params.cu_seqlens_k = static_cast<int *>(cu_seqlens_k_d);
 
-    // S = softmax(P)
-    params.s_ptr = s_d;
-    params.s_stride_in_bytes = get_size_in_bytes(b * h * seqlen_k, data_type);
+    // P = softmax(QK^T)
+    params.p_ptr = p_d;
 
     // Softmax sum
     params.softmax_lse_ptr = softmax_lse_d;
 
     // Set the dimensions.
     params.b = b;
     params.h = h;
+    params.h_k = h_k;
+    params.h_h_k_ratio = h / h_k;
     params.seqlen_q = seqlen_q;
     params.seqlen_k = seqlen_k;
+    params.seqlen_q_rounded = seqlen_q_rounded;
+    params.seqlen_k_rounded = seqlen_k_rounded;
     params.d = d;
+    params.d_rounded = d_rounded;
 
     // Set the different scale values.
-    // const float scale_bmm1 = 1.f / sqrtf(d);
-    const float scale_bmm1 = softmax_scale;
-
-    params.scale_bmm1f = scale_bmm1;
-    set_alpha(params.scale_bmm1, scale_bmm1, data_type);
+    params.scale_softmax = softmax_scale;
+    params.scale_softmax_log2 = softmax_scale * M_LOG2E;
 
     // Set this to probability of keeping an element to simplify things.
     params.p_dropout = 1.f - p_dropout;
     // Convert p from float to int so we don't have to convert the random uint to float to compare.
     // [Minor] We want to round down since when we do the comparison we use <= instead of <
-    params.p_dropout_in_uint = uint32_t(std::floor(params.p_dropout * 4294967295.0));
-    params.p_dropout_in_uint16_t = uint16_t(std::floor(params.p_dropout * 65535.0));
+    // params.p_dropout_in_uint = uint32_t(std::floor(params.p_dropout * 4294967295.0));
+    // params.p_dropout_in_uint16_t = uint16_t(std::floor(params.p_dropout * 65535.0));
+    params.p_dropout_in_uint8_t = uint8_t(std::floor(params.p_dropout * 255.0));
     params.rp_dropout = 1.f / params.p_dropout;
-    params.scale_bmm1_rp_dropout = params.rp_dropout * params.scale_bmm1f;
+    params.scale_softmax_rp_dropout = params.rp_dropout * params.scale_softmax;
     TORCH_CHECK(p_dropout < 1.f);
-    set_alpha(params.scale_dropout, params.rp_dropout, data_type);
 
     params.is_causal = is_causal;
-    params.num_splits = num_splits;
 }
 
-void set_params_dgrad(FMHA_dgrad_params &params,
+void set_params_dgrad(Flash_bwd_params &params,
                       // sizes
                       const size_t b,
                       const size_t seqlen_q,
                       const size_t seqlen_k,
+                      const size_t seqlen_q_rounded,
+                      const size_t seqlen_k_rounded,
                       const size_t h,
+                      const size_t h_k,
                       const size_t d,
+                      const size_t d_rounded,
                       // device pointers
                       const at::Tensor q,
                       const at::Tensor k,
                       const at::Tensor v,
                       const at::Tensor out,
+                      const at::Tensor dout,
                       at::Tensor dq,
                       at::Tensor dk,
                       at::Tensor dv,
                       void *cu_seqlens_q_d,
                       void *cu_seqlens_k_d,
-                      void *dq_tmp_d,
-                      void *do_packed_d,
+                      void *dq_accum_d,
+                      void *dk_accum_d,
+                      void *dv_accum_d,
                       void *softmax_lse_d,
                       void *dsoftmax_sum_d,
                       float p_dropout,
                       float softmax_scale,
-                      bool is_causal,
-                      int num_splits) {
+                      bool is_causal) {
 
     set_params_fprop(params,
-                     b, seqlen_q, seqlen_k, h, d,
+                     b, seqlen_q, seqlen_k, seqlen_q_rounded, seqlen_k_rounded, h, h_k, d, d_rounded,
                      q, k, v, out,
                      cu_seqlens_q_d,
                      cu_seqlens_k_d,
-                     dq_tmp_d,  // Reusing the o_tmp_ptr variable to store dq_tmp
                      nullptr,
                      softmax_lse_d,
                      p_dropout,
                      softmax_scale,
-                     is_causal,
-                     num_splits);
+                     is_causal);
 
     // Set the pointers and strides.
+    params.do_ptr = dout.data_ptr();
+    params.do_row_stride = dout.stride(-3);
+    params.do_head_stride = dout.stride(-2);
     params.dq_ptr = dq.data_ptr();
     params.dk_ptr = dk.data_ptr();
     params.dv_ptr = dv.data_ptr();
-    params.dq_row_stride_in_elts = dq.stride(0);
-    params.dk_row_stride_in_elts = dk.stride(0);
-    params.dv_row_stride_in_elts = dv.stride(0);
-    params.dq_head_stride_in_elts = dq.stride(1);
-    params.dk_head_stride_in_elts = dk.stride(1);
-    params.dv_head_stride_in_elts = dv.stride(1);
-    params.do_ptr = do_packed_d;
+    params.dq_row_stride = dq.stride(-3);
+    params.dk_row_stride = dk.stride(-3);
+    params.dv_row_stride = dv.stride(-3);
+    params.dq_head_stride = dq.stride(-2);
+    params.dk_head_stride = dk.stride(-2);
+    params.dv_head_stride = dv.stride(-2);
+
+    if (cu_seqlens_q_d == nullptr) {
+        params.do_batch_stride = dout.stride(0);
+        params.dq_batch_stride = dq.stride(0);
+        params.dk_batch_stride = dk.stride(0);
+        params.dv_batch_stride = dv.stride(0);
+    }
+
+    params.dq_accum_ptr = dq_accum_d;
+    params.dk_accum_ptr = dk_accum_d;
+    params.dv_accum_ptr = dv_accum_d;
 
     // Softmax sum
     params.dsoftmax_sum = dsoftmax_sum_d;
 }
 
-void run_fmha_fwd(Launch_params<FMHA_fprop_params> &launch_params) {
-    if (launch_params.params.d <= 32) {
-        run_fmha_fwd_hdim32(launch_params);
-    } else if (launch_params.params.d <= 64) {
-        run_fmha_fwd_hdim64(launch_params);
-    } else if (launch_params.params.d <= 128) {
-        run_fmha_fwd_hdim128(launch_params);
-    }
+void run_mha_fwd(Flash_fwd_params &params, cudaStream_t stream) {
+    FP16_SWITCH(!params.is_bf16, [&] {
+        FWD_HEADDIM_SWITCH(params.d, [&] {
+            run_mha_fwd_<elem_type, kHeadDim>(params, stream);
+        });
+    });
 }
 
 std::vector<at::Tensor>
-mha_fwd(const at::Tensor &q,         // total_q x num_heads x head_size, total_q := \sum_{i=0}^{b} s_i
-        const at::Tensor &k,         // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
-        const at::Tensor &v,         // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
-        at::Tensor &out,             // total_q x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
-        const at::Tensor &cu_seqlens_q,  // b+1
-        const at::Tensor &cu_seqlens_k,  // b+1
-        const int max_seqlen_q_,
-        const int max_seqlen_k_,
+mha_fwd(const at::Tensor &q,         // batch_size x seqlen_q x num_heads x head_size
+        const at::Tensor &k,         // batch_size x seqlen_k x num_heads_k x head_size
+        const at::Tensor &v,         // batch_size x seqlen_k x num_heads_k x head_size
+        c10::optional<at::Tensor> &out_,             // batch_size x seqlen_q x num_heads x head_size
         const float p_dropout,
         const float softmax_scale,
-        const bool zero_tensors,
         const bool is_causal,
         const bool return_softmax,
-        const int num_splits,
         c10::optional<at::Generator> gen_) {
 
     auto dprops = at::cuda::getCurrentDeviceProperties();
-    bool is_sm75 = dprops->major == 7 && dprops->minor == 5;
-    bool is_sm80 = dprops->major == 8 && dprops->minor == 0;
+    // bool is_sm75 = dprops->major == 7 && dprops->minor == 5;
     bool is_sm8x = dprops->major == 8 && dprops->minor >= 0;
     bool is_sm90 = dprops->major == 9 && dprops->minor == 0;
-    TORCH_CHECK(is_sm90 || is_sm8x || is_sm75);
-    auto stream = at::cuda::getCurrentCUDAStream().stream();
-    bool is_dropout = p_dropout > 0.0;
-    Launch_params<FMHA_fprop_params> launch_params(dprops, stream, is_dropout, return_softmax);
+    TORCH_CHECK(is_sm90 || is_sm8x, "FlashAttention only supports Ampere GPUs or newer.");
+    // We will support Turing in the near future
+    // TORCH_CHECK(is_sm90 || is_sm8x || is_sm75, "FlashAttention only supports Turing GPUs or newer.");
 
     auto q_dtype = q.dtype();
-    TORCH_CHECK(q_dtype == torch::kFloat16 || ((is_sm8x || is_sm90) && q_dtype == torch::kBFloat16));
-    TORCH_CHECK(k.dtype() == q_dtype);
-    TORCH_CHECK(v.dtype() == q_dtype);
-    TORCH_CHECK(out.dtype() == q_dtype);
-    TORCH_CHECK(cu_seqlens_q.dtype() == torch::kInt32);
-    TORCH_CHECK(cu_seqlens_k.dtype() == torch::kInt32);
-
-    TORCH_CHECK(q.is_cuda());
-    TORCH_CHECK(k.is_cuda());
-    TORCH_CHECK(v.is_cuda());
-    TORCH_CHECK(out.is_cuda());
-    TORCH_CHECK(cu_seqlens_q.is_cuda());
-    TORCH_CHECK(cu_seqlens_k.is_cuda());
-
-    TORCH_CHECK(q.stride(-1) == 1);
-    TORCH_CHECK(k.stride(-1) == 1);
-    TORCH_CHECK(v.stride(-1) == 1);
-    TORCH_CHECK(out.stride(-1) == 1);
-    TORCH_CHECK(cu_seqlens_q.is_contiguous());
-    TORCH_CHECK(cu_seqlens_k.is_contiguous());
+    TORCH_CHECK(q_dtype == torch::kFloat16 || q_dtype == torch::kBFloat16,
+                "FlashAttention only support fp16 and bf16 data type");
+    if (q_dtype == torch::kBFloat16) {
+        TORCH_CHECK(is_sm90 || is_sm8x, "bfloat16 is only supported on Ampere GPUs or newer");
+    }
+    TORCH_CHECK(k.dtype() == q_dtype, "query and key must have the same dtype");
+    TORCH_CHECK(v.dtype() == q_dtype, "query and value must have the same dtype");
+
+    TORCH_CHECK(q.is_cuda(), "Input tensor must be on CUDA device");
+    TORCH_CHECK(k.is_cuda(), "Input tensor must be on CUDA device");
+    TORCH_CHECK(v.is_cuda(), "Input tensor must be on CUDA device");
+
+    TORCH_CHECK(q.stride(-1) == 1, "Input tensor must have contiguous last dimension");
+    TORCH_CHECK(k.stride(-1) == 1, "Input tensor must have contiguous last dimension");
+    TORCH_CHECK(v.stride(-1) == 1, "Input tensor must have contiguous last dimension");
 
     const auto sizes = q.sizes();
 
-    const int batch_size = cu_seqlens_q.numel() - 1;
-    const int total_q = sizes[TOTAL_DIM];
-    const int num_heads = sizes[H_DIM];
-    const int head_size = sizes[D_DIM];
-    const int total_k = k.size(TOTAL_DIM);
-    TORCH_CHECK(batch_size > 0);
-    TORCH_CHECK((head_size % 8 == 0) && (head_size <= 128));
-
-    CHECK_SHAPE(q, total_q, num_heads, head_size);
-    CHECK_SHAPE(k, total_k, num_heads, head_size);
-    CHECK_SHAPE(v, total_k, num_heads, head_size);
-    CHECK_SHAPE(out, total_q, num_heads, head_size);
-    CHECK_SHAPE(cu_seqlens_q, batch_size + 1);
-    CHECK_SHAPE(cu_seqlens_k, batch_size + 1);
-
-    int blocksize_c = head_size > 64 ? 128 : 256;
-    // Need to round max_seqlen_k to multiples of blocksize_c
-    int max_seqlen_k = ((max_seqlen_k_ + blocksize_c - 1) / blocksize_c) * blocksize_c;
-    if( max_seqlen_k_ <= 128 ) {
-        max_seqlen_k = 128;
-    } else if( max_seqlen_k_ <= 256 ) {
-        max_seqlen_k = 256;
-    }
-    int max_seqlen_q = ((max_seqlen_q_ + 16 - 1) / 16) * 16;
-    bool loop = max_seqlen_k > blocksize_c;
+    const int batch_size = sizes[0];
+    const int seqlen_q = sizes[1];
+    const int num_heads = sizes[2];
+    const int head_size_og = sizes[3];
+    const int seqlen_k = k.size(1);
+    const int num_heads_k = k.size(2);
+    TORCH_CHECK(batch_size > 0, "batch size must be postive");
+    TORCH_CHECK(head_size_og <= 256, "FlashAttention forward only supports head dimension at most 256");
+    TORCH_CHECK(num_heads % num_heads_k == 0, "Number of heads in key/value must divide number of heads in query");
+
+    CHECK_SHAPE(q, batch_size, seqlen_q, num_heads, head_size_og);
+    CHECK_SHAPE(k, batch_size, seqlen_k, num_heads_k, head_size_og);
+    CHECK_SHAPE(v, batch_size, seqlen_k, num_heads_k, head_size_og);
+
+    at::Tensor q_padded, k_padded, v_padded;
+    if (head_size_og % 8 != 0) {
+        q_padded = torch::nn::functional::pad(q, torch::nn::functional::PadFuncOptions({0, 8 - head_size_og % 8}));
+        k_padded = torch::nn::functional::pad(k, torch::nn::functional::PadFuncOptions({0, 8 - head_size_og % 8}));
+        v_padded = torch::nn::functional::pad(v, torch::nn::functional::PadFuncOptions({0, 8 - head_size_og % 8}));
+    } else {
+        q_padded = q;
+        k_padded = k;
+        v_padded = v;
+    }
+
+    at::Tensor out;
+    if (out_.has_value()) {
+        out = out_.value();
+        TORCH_CHECK(out.dtype() == q_dtype, "Output must have the same dtype as inputs");
+        TORCH_CHECK(out.is_cuda(), "Output tensor must be on CUDA device");
+        TORCH_CHECK(out.stride(-1) == 1, "Output tensor must have contiguous last dimension");
+        CHECK_SHAPE(out, batch_size, seqlen_q, num_heads, head_size_og);
+        if (head_size_og % 8 != 0) { out = torch::empty_like(q_padded); }
+    } else {
+        out = torch::empty_like(q_padded);
+    }
+
+    auto round_multiple = [](int x, int m) { return (x + m - 1) / m * m; };
+    const int head_size = round_multiple(head_size_og, 8);
+    const int head_size_rounded = round_multiple(head_size, 32);
+    const int seqlen_q_rounded = round_multiple(seqlen_q, 128);
+    const int seqlen_k_rounded = round_multiple(seqlen_k, 128);
 
     // Otherwise the kernel will be launched from cuda:0 device
     // Cast to char to avoid compiler warning about narrowing
     at::cuda::CUDAGuard device_guard{(char)q.get_device()};
 
     auto opts = q.options();
 
-    // auto o = torch::empty({ total_q, num_heads, head_size }, opts);
-
-    at::Tensor o_tmp;
-    if (loop) { o_tmp = torch::empty({total_q, num_heads, head_size}, opts.dtype(at::kFloat)); }
-
-    auto softmax_lse = torch::empty({batch_size, num_heads, max_seqlen_q}, opts.dtype(at::kFloat));
-    // auto softmax_lse = torch::full({batch_size, num_heads, max_seqlen_k}, -std::numeric_limits<float>::infinity(), opts.dtype(at::kFloat));
-
-    at::Tensor s;
-    if (return_softmax) { s = torch::empty({ batch_size, num_heads, max_seqlen_q, max_seqlen_k }, opts); }
-
-    if( zero_tensors ) {
-        out.zero_();
-        softmax_lse.fill_(-std::numeric_limits<float>::infinity());
-        if (return_softmax) {s.zero_();}
+    auto softmax_lse = torch::empty({batch_size, num_heads, seqlen_q}, opts.dtype(at::kFloat));
+    at::Tensor p;
+    // Only return softmax if there's dropout to reduce compilation time
+    if (return_softmax) {
+        TORCH_CHECK(p_dropout > 0.0f, "return_softmax is only supported when p_dropout > 0.0");
+        p = torch::empty({ batch_size, num_heads, seqlen_q_rounded, seqlen_k_rounded }, opts);
     }
 
-    auto gen = at::get_generator_or_default<at::CUDAGeneratorImpl>(
-        gen_, at::cuda::detail::getDefaultCUDAGenerator());
-
-    set_params_fprop(launch_params.params,
+    Flash_fwd_params params;
+    set_params_fprop(params,
                      batch_size,
-                     max_seqlen_q,
-                     max_seqlen_k,
-                     num_heads,
-                     head_size,
-                     q, k, v, out,
-                     cu_seqlens_q.data_ptr(),
-                     cu_seqlens_k.data_ptr(),
-                     loop ? o_tmp.data_ptr() : nullptr,
-                     return_softmax ? s.data_ptr() : nullptr,
+                     seqlen_q, seqlen_k,
+                     seqlen_q_rounded, seqlen_k_rounded,
+                     num_heads, num_heads_k,
+                     head_size, head_size_rounded,
+                     q_padded, k_padded, v_padded, out,
+                     /*cu_seqlens_q_d=*/nullptr,
+                     /*cu_seqlens_k_d=*/nullptr,
+                     return_softmax ? p.data_ptr() : nullptr,
                      softmax_lse.data_ptr(),
                      p_dropout,
                      softmax_scale,
-                     is_causal,
-                     num_splits);
-
-    // number of times random will be generated per thread, to offset philox counter in thc random
-    // state
-    // We use a custom RNG that increases the offset by batch_size * nheads * 32.
-    int64_t counter_offset = launch_params.params.b * launch_params.params.h * 32;
-    auto options = torch::TensorOptions().dtype(torch::kFloat32).device(torch::kCUDA);
-    auto rng_state = torch::empty({2}, options.dtype(torch::kInt64));
-    // Forward kernel will populate memory with the seed and offset.
-    launch_params.params.rng_state = reinterpret_cast<uint64_t*>(rng_state.data_ptr());
+                     is_causal);
 
-    if( is_dropout ) {
+    if (p_dropout > 0.0)  {
+        // number of times random will be generated per thread, to offset philox counter in thc random
+        // state
+        // We use a custom RNG that increases the offset by batch_size * nheads * 32.
+        int64_t counter_offset = params.b * params.h * 32;
+        auto gen = at::get_generator_or_default<at::CUDAGeneratorImpl>(
+            gen_, at::cuda::detail::getDefaultCUDAGenerator());
         // See Note [Acquire lock when using random generators]
         std::lock_guard<std::mutex> lock(gen->mutex_);
-        launch_params.params.philox_args = gen->philox_cuda_state(counter_offset);
+        params.philox_args = gen->philox_cuda_state(counter_offset);
     }
 
-    run_fmha_fwd(launch_params);
+    auto stream = at::cuda::getCurrentCUDAStream().stream();
+    run_mha_fwd(params, stream);
 
-    std::vector<at::Tensor> result = {softmax_lse};
-    result.push_back(rng_state);
-    if (return_softmax) {result.push_back(s);}
-    return result;
-}
+    at::Tensor out_padded = out;
+    if (head_size_og % 8 != 0) {
+        out = out.index({"...", torch::indexing::Slice(torch::indexing::None, head_size_og)});
+        if (out_.has_value()) { out_.value().copy_(out); }
+    }
 
-void run_fmha_bwd(FMHA_dgrad_params &params, cudaStream_t stream, const bool configure) {
-  if (params.d <= 32) {
-      run_fmha_bwd_hdim32(params, stream, configure);
-  } else if (params.d <= 64) {
-      run_fmha_bwd_hdim64(params, stream, configure);
-  } else if (params.d <= 128) {
-      run_fmha_bwd_hdim128(params, stream, configure);
-  }
+    return {out, q_padded, k_padded, v_padded, out_padded, softmax_lse, p};
 }
 
 std::vector<at::Tensor>
-mha_bwd(const at::Tensor &dout,  // total_q x num_heads, x head_size
-        const at::Tensor &q,   // total_q x num_heads x head_size, total_q := \sum_{i=0}^{b} s_i
-        const at::Tensor &k,   // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
-        const at::Tensor &v,   // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
-        const at::Tensor &out,   // total_q x num_heads x head_size
-        const at::Tensor &softmax_lse_,     // b x h x s softmax logsumexp
-        at::Tensor &dq,   // total_q x num_heads x head_size, total_q := \sum_{i=0}^{b} s_i
-        at::Tensor &dk,   // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
-        at::Tensor &dv,   // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
-        const at::Tensor &cu_seqlens_q,  // b+1
-        const at::Tensor &cu_seqlens_k,  // b+1
-        const int max_seqlen_q_,
-        const int max_seqlen_k_,          // max sequence length to choose the kernel
-        const float p_dropout,         // probability to drop
-        const float softmax_scale,
-        const bool zero_tensors,
-        const bool is_causal,
-        const int num_splits,
-        c10::optional<at::Generator> gen_,
-        c10::optional<at::Tensor> &rng_state
-) {
+mha_varlen_fwd(const at::Tensor &q,  // total_q x num_heads x head_size, total_q := \sum_{i=0}^{b} s_i
+               const at::Tensor &k,  // total_k x num_heads_k x head_size, total_k := \sum_{i=0}^{b} s_i
+               const at::Tensor &v,  // total_k x num_heads_k x head_size, total_k := \sum_{i=0}^{b} s_i
+               c10::optional<at::Tensor> &out_, // total_q x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
+               const at::Tensor &cu_seqlens_q,  // b+1
+               const at::Tensor &cu_seqlens_k,  // b+1
+               const int max_seqlen_q,
+               const int max_seqlen_k,
+               const float p_dropout,
+               const float softmax_scale,
+               const bool zero_tensors,
+               const bool is_causal,
+               const bool return_softmax,
+               c10::optional<at::Generator> gen_) {
+
     auto dprops = at::cuda::getCurrentDeviceProperties();
-    bool is_sm75 = dprops->major == 7 && dprops->minor == 5;
-    bool is_sm80 = dprops->major == 8 && dprops->minor == 0;
+    // bool is_sm75 = dprops->major == 7 && dprops->minor == 5;
     bool is_sm8x = dprops->major == 8 && dprops->minor >= 0;
     bool is_sm90 = dprops->major == 9 && dprops->minor == 0;
-    TORCH_CHECK(is_sm90 || is_sm8x || is_sm75);
-    auto launch = &run_fmha_bwd;
-
-    bool is_dropout = p_dropout > 0.0;
-    auto stream = at::cuda::getCurrentCUDAStream().stream();
+    TORCH_CHECK(is_sm90 || is_sm8x, "FlashAttention only supports Ampere GPUs or newer.");
+    // We will support Turing in the near future
+    // TORCH_CHECK(is_sm90 || is_sm8x || is_sm75, "FlashAttention only supports Turing GPUs or newer.");
 
     auto q_dtype = q.dtype();
-    TORCH_CHECK(q_dtype == torch::kFloat16 || ((is_sm8x || is_sm90) && q_dtype == torch::kBFloat16));
-    TORCH_CHECK(k.dtype() == q_dtype);
-    TORCH_CHECK(v.dtype() == q_dtype);
-    TORCH_CHECK(out.dtype() == q_dtype);
-    TORCH_CHECK(dout.dtype() == q_dtype);
-    TORCH_CHECK(dq.dtype() == q_dtype);
-    TORCH_CHECK(dk.dtype() == q_dtype);
-    TORCH_CHECK(dv.dtype() == q_dtype);
-    TORCH_CHECK(cu_seqlens_q.dtype() == torch::kInt32);
-    TORCH_CHECK(cu_seqlens_k.dtype() == torch::kInt32);
-
-    TORCH_CHECK(q.is_cuda());
-    TORCH_CHECK(k.is_cuda());
-    TORCH_CHECK(v.is_cuda());
-    TORCH_CHECK(out.is_cuda());
-    TORCH_CHECK(dout.is_cuda());
-    TORCH_CHECK(softmax_lse_.is_cuda());
-    TORCH_CHECK(cu_seqlens_q.is_cuda());
-    TORCH_CHECK(cu_seqlens_k.is_cuda());
-
-    TORCH_CHECK(q.stride(-1) == 1);
-    TORCH_CHECK(k.stride(-1) == 1);
-    TORCH_CHECK(v.stride(-1) == 1);
-    TORCH_CHECK(out.is_contiguous());
-    TORCH_CHECK(dout.is_contiguous());
-    TORCH_CHECK(dq.stride(-1) == 1);
-    TORCH_CHECK(dk.stride(-1) == 1);
-    TORCH_CHECK(dv.stride(-1) == 1);
-    TORCH_CHECK(cu_seqlens_q.is_contiguous());
-    TORCH_CHECK(cu_seqlens_k.is_contiguous());
+    TORCH_CHECK(q_dtype == torch::kFloat16 || q_dtype == torch::kBFloat16,
+                "FlashAttention only support fp16 and bf16 data type");
+    if (q_dtype == torch::kBFloat16) {
+        TORCH_CHECK(is_sm90 || is_sm8x, "bfloat16 is only supported on Ampere GPUs or newer");
+    }
+    TORCH_CHECK(k.dtype() == q_dtype, "query and key must have the same dtype");
+    TORCH_CHECK(v.dtype() == q_dtype, "query and value must have the same dtype");
+    TORCH_CHECK(cu_seqlens_q.dtype() == torch::kInt32, "cu_seqlens_q must have dtype int32");
+    TORCH_CHECK(cu_seqlens_k.dtype() == torch::kInt32, "cu_seqlens_k must have dtype int32");
+
+    TORCH_CHECK(q.is_cuda(), "Input tensor must be on CUDA device");
+    TORCH_CHECK(k.is_cuda(), "Input tensor must be on CUDA device");
+    TORCH_CHECK(v.is_cuda(), "Input tensor must be on CUDA device");
+    TORCH_CHECK(cu_seqlens_q.is_cuda(), "cu_seqlens_q must be on CUDA device");
+    TORCH_CHECK(cu_seqlens_k.is_cuda(), "cu_seqlens_k must be on CUDA device");
+
+    TORCH_CHECK(q.stride(-1) == 1, "Input tensor must have contiguous last dimension");
+    TORCH_CHECK(k.stride(-1) == 1, "Input tensor must have contiguous last dimension");
+    TORCH_CHECK(v.stride(-1) == 1, "Input tensor must have contiguous last dimension");
+    TORCH_CHECK(cu_seqlens_q.is_contiguous(), "cu_seqlens_q must be contiguous");
+    TORCH_CHECK(cu_seqlens_k.is_contiguous(), "cu_seqlens_k must be contiguous");
 
     const auto sizes = q.sizes();
 
+    const int total_q = sizes[0];
     const int batch_size = cu_seqlens_q.numel() - 1;
-    const int total_q = sizes[TOTAL_DIM];
-    const int num_heads = sizes[H_DIM];
-    const int head_size = sizes[D_DIM];
-    const int total_k = k.size(TOTAL_DIM);
-    TORCH_CHECK(batch_size > 0);
-    TORCH_CHECK((head_size % 8 == 0) && (head_size <= 128));
-    if (head_size > 64) {
-        TORCH_CHECK(is_sm80 || is_sm90, "FlashAttention backward for head dim > 64 requires A100 or H100 GPUs as the implementation needs a large amount of shared memory.");
-    }
-
-    CHECK_SHAPE(q, total_q, num_heads, head_size);
-    CHECK_SHAPE(k, total_k, num_heads, head_size);
-    CHECK_SHAPE(v, total_k, num_heads, head_size);
-    CHECK_SHAPE(out, total_q, num_heads, head_size);
-    CHECK_SHAPE(dout, total_q, num_heads, head_size);
-    CHECK_SHAPE(dq, total_q, num_heads, head_size);
-    CHECK_SHAPE(dk, total_k, num_heads, head_size);
-    CHECK_SHAPE(dv, total_k, num_heads, head_size);
+    const int num_heads = sizes[1];
+    const int head_size_og = sizes[2];
+    const int total_k = k.size(0);
+    const int num_heads_k = k.size(1);
+    TORCH_CHECK(batch_size > 0, "batch size must be positive");
+    TORCH_CHECK(head_size_og <= 256, "FlashAttention forward only supports head dimension at most 256");
+    TORCH_CHECK(num_heads % num_heads_k == 0, "Number of heads in key/value must divide number of heads in query");
+
+    CHECK_SHAPE(q, total_q, num_heads, head_size_og);
+    CHECK_SHAPE(k, total_k, num_heads_k, head_size_og);
+    CHECK_SHAPE(v, total_k, num_heads_k, head_size_og);
     CHECK_SHAPE(cu_seqlens_q, batch_size + 1);
     CHECK_SHAPE(cu_seqlens_k, batch_size + 1);
 
-    int blocksize_c = (head_size > 64 || (is_sm75 && head_size > 32)) ? 128 : 256;
-    int max_seqlen_k = ((max_seqlen_k_ + blocksize_c - 1) / blocksize_c) * blocksize_c;
-    if( max_seqlen_k_ <= 128 ) {
-        max_seqlen_k = 128;
-    } else if( max_seqlen_k_ <= 256 ) {
-        max_seqlen_k = 256;
-    }
-    int max_seqlen_q = ((max_seqlen_q_ + 16 - 1) / 16) * 16;
-    bool loop = max_seqlen_k > blocksize_c;
+    at::Tensor q_padded, k_padded, v_padded;
+    if (head_size_og % 8 != 0) {
+        q_padded = torch::nn::functional::pad(q, torch::nn::functional::PadFuncOptions({0, 8 - head_size_og % 8}));
+        k_padded = torch::nn::functional::pad(k, torch::nn::functional::PadFuncOptions({0, 8 - head_size_og % 8}));
+        v_padded = torch::nn::functional::pad(v, torch::nn::functional::PadFuncOptions({0, 8 - head_size_og % 8}));
+    } else {
+        q_padded = q;
+        k_padded = k;
+        v_padded = v;
+    }
+
+    at::Tensor out;
+    if (out_.has_value()) {
+        out = out_.value();
+        TORCH_CHECK(out.dtype() == q_dtype, "Output must have the same dtype as inputs");
+        TORCH_CHECK(out.is_cuda(), "Output tensor must be on CUDA device");
+        TORCH_CHECK(out.stride(-1) == 1, "Output tensor must have contiguous last dimension");
+        CHECK_SHAPE(out, total_q, num_heads, head_size_og);
+        if (head_size_og % 8 != 0) { out = torch::empty_like(q_padded); }
+    } else {
+        out = torch::empty_like(q_padded);
+    }
+
+    auto round_multiple = [](int x, int m) { return (x + m - 1) / m * m; };
+    const int head_size = round_multiple(head_size_og, 8);
+    const int head_size_rounded = round_multiple(head_size, 32);
+    const int seqlen_q_rounded = round_multiple(max_seqlen_q, 128);
+    const int seqlen_k_rounded = round_multiple(max_seqlen_k, 128);
 
     // Otherwise the kernel will be launched from cuda:0 device
     // Cast to char to avoid compiler warning about narrowing
     at::cuda::CUDAGuard device_guard{(char)q.get_device()};
 
-    // It's possible the softmax_lse_ from the fwd has a different length since blocksize_c could be different.
-    auto softmax_lse = softmax_lse_.index({torch::indexing::Slice(), torch::indexing::Slice(), torch::indexing::Slice(torch::indexing::None, max_seqlen_q)}).contiguous();
-
     auto opts = q.options();
-    auto softmax_d = torch::empty({batch_size, num_heads, max_seqlen_q}, opts.dtype(at::kFloat));
-    at::Tensor dq_tmp;
-    if (loop) { dq_tmp = torch::empty({total_q, num_heads, head_size}, opts.dtype(at::kFloat)); }
 
-    if( zero_tensors ) {
-        dq.zero_();
-        dk.zero_();
-        dv.zero_();
-        softmax_d.zero_();
+    auto softmax_lse = torch::empty({batch_size, num_heads, max_seqlen_q}, opts.dtype(at::kFloat));
+    at::Tensor p;
+    // Only return softmax if there's dropout to reduce compilation time
+    if (return_softmax) {
+        TORCH_CHECK(p_dropout > 0.0f, "return_softmax is only supported when p_dropout > 0.0");
+        p = torch::empty({ batch_size, num_heads, seqlen_q_rounded, seqlen_k_rounded }, opts);
     }
 
-    FMHA_dgrad_params params;
+    if (zero_tensors) {
+        out.zero_();
+        softmax_lse.fill_(-std::numeric_limits<float>::infinity());
+        if (return_softmax) {p.zero_();}
+    }
 
-    set_params_dgrad(params,
+    Flash_fwd_params params;
+    set_params_fprop(params,
                      batch_size,
-                     max_seqlen_q,
-                     max_seqlen_k,
-                     num_heads,
-                     head_size,
-                     q, k, v, out,
-                     dq, dk, dv,
+                     max_seqlen_q, max_seqlen_k,
+                     seqlen_q_rounded, seqlen_k_rounded,
+                     num_heads, num_heads_k,
+                     head_size, head_size_rounded,
+                     q_padded, k_padded, v_padded, out,
                      cu_seqlens_q.data_ptr(),
                      cu_seqlens_k.data_ptr(),
-                     loop ? dq_tmp.data_ptr() : nullptr,
-                     dout.data_ptr(),
+                     return_softmax ? p.data_ptr() : nullptr,
                      softmax_lse.data_ptr(),
-                     softmax_d.data_ptr(),
                      p_dropout,
                      softmax_scale,
-                     is_causal,
-                     num_splits);
+                     is_causal);
 
-    launch(params, stream, /*configure=*/true);
-
-    if (params.num_splits > 1) {
-        if (!dq_tmp.defined()) {
-            dq_tmp = torch::zeros({total_q, num_heads, head_size}, opts.dtype(at::kFloat));
-            params.o_tmp_ptr = dq_tmp.data_ptr();  // o_tmp stores dq_tmp in the backward pass
-        } else {
-            dq_tmp.zero_();
-        }
-    }
-
-    auto gen = at::get_generator_or_default<at::CUDAGeneratorImpl>(
-        gen_, at::cuda::detail::getDefaultCUDAGenerator());
-
-    // We use a custom RNG that increases the offset by batch_size * nheads * 32.
-    int64_t counter_offset = params.b * params.h * 32;
-    if ( rng_state.has_value() ) {
-        params.rng_state = reinterpret_cast<uint64_t*>(rng_state.value().data_ptr());
-    } else if( is_dropout ) {
+    if (p_dropout > 0.0)  {
+        // number of times random will be generated per thread, to offset philox counter in thc random
+        // state
+        // We use a custom RNG that increases the offset by batch_size * nheads * 32.
+        int64_t counter_offset = params.b * params.h * 32;
+        auto gen = at::get_generator_or_default<at::CUDAGeneratorImpl>(
+            gen_, at::cuda::detail::getDefaultCUDAGenerator());
         // See Note [Acquire lock when using random generators]
         std::lock_guard<std::mutex> lock(gen->mutex_);
         params.philox_args = gen->philox_cuda_state(counter_offset);
-        auto seeds = at::cuda::philox::unpack(params.philox_args);
-        params.rng_state[0] = std::get<0>(seeds);
-        params.rng_state[1] = std::get<1>(seeds);
     }
 
-    launch(params, stream, /*configure=*/false);
+    auto stream = at::cuda::getCurrentCUDAStream().stream();
+    run_mha_fwd(params, stream);
 
-    if (params.num_splits > 1) {
-        dq.copy_(dq_tmp);
+    at::Tensor out_padded = out;
+    if (head_size_og % 8 != 0) {
+        out = out.index({"...", torch::indexing::Slice(torch::indexing::None, head_size_og)});
+        if (out_.has_value()) { out_.value().copy_(out); }
     }
 
-    return { dq, dk, dv, softmax_d };
+    return {out, q_padded, k_padded, v_padded, out_padded, softmax_lse, p};
 }
 
-std::vector<at::Tensor>
-mha_fwd_block(const at::Tensor &q,         // total_q x num_heads x head_size, total := \sum_{i=0}^{b} s_i
-              const at::Tensor &k,         // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
-              const at::Tensor &v,         // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
-              const at::Tensor &cu_seqlens_q,  // b+1
-              const at::Tensor &cu_seqlens_k,  // b+1
-              const at::Tensor &blockmask,   // (seqlen / 256, seqlen / 16)
-              const int max_seqlen_q_,
-              const int max_seqlen_k_,
-              const float p_dropout,
-              const float softmax_scale,
-              const bool is_causal,
-              const bool return_softmax,
-              c10::optional<at::Generator> gen_) {
+void run_mha_bwd(Flash_bwd_params &params, cudaStream_t stream, const bool configure) {
+    FP16_SWITCH(!params.is_bf16, [&] {
+        if (params.d <= 32) {
+            run_mha_bwd_<elem_type, 32>(params, stream, configure);
+        } else if (params.d <= 64) {
+            run_mha_bwd_<elem_type, 64>(params, stream, configure);
+        } else if (params.d <= 96) {
+            run_mha_bwd_<elem_type, 96>(params, stream, configure);
+        } else if (params.d <= 128) {
+            run_mha_bwd_<elem_type, 128>(params, stream, configure);
+        } else if (params.d <= 160) {
+            run_mha_bwd_<elem_type, 160>(params, stream, configure);
+        } else if (params.d <= 192) {
+            run_mha_bwd_<elem_type, 192>(params, stream, configure);
+        } else if (params.d <= 224) {
+          run_mha_bwd_<elem_type, 224>(params, stream, configure);
+        } else if (params.d <= 256) {
+          run_mha_bwd_<elem_type, 256>(params, stream, configure);
+        }
+    });
+}
 
+std::vector<at::Tensor>
+mha_bwd(const at::Tensor &dout,  // batch_size x seqlen_q x num_heads, x head_size_og
+        const at::Tensor &q,   // batch_size x seqlen_q x num_heads x head_size
+        const at::Tensor &k,   // batch_size x seqlen_k x num_heads_k x head_size
+        const at::Tensor &v,   // batch_size x seqlen_k x num_heads_k x head_size
+        const at::Tensor &out,   // batch_size x seqlen_q x num_heads x head_size
+        const at::Tensor &softmax_lse,     // b x h x seqlen_q
+        c10::optional<at::Tensor> &dq_,   // batch_size x seqlen_q x num_heads x head_size
+        c10::optional<at::Tensor> &dk_,   // batch_size x seqlen_k x num_heads_k x head_size
+        c10::optional<at::Tensor> &dv_,   // batch_size x seqlen_k x num_heads_k x head_size
+        const float p_dropout,         // probability to drop
+        const float softmax_scale,
+        const bool is_causal,
+        c10::optional<at::Generator> gen_) {
     auto dprops = at::cuda::getCurrentDeviceProperties();
-    bool is_sm80 = dprops->major == 8 && dprops->minor == 0;
+    // bool is_sm75 = dprops->major == 7 && dprops->minor == 5;
     bool is_sm8x = dprops->major == 8 && dprops->minor >= 0;
+    bool is_sm80 = dprops->major == 8 && dprops->minor == 0;
     bool is_sm90 = dprops->major == 9 && dprops->minor == 0;
-    TORCH_CHECK(is_sm8x || is_sm90);
-    auto stream = at::cuda::getCurrentCUDAStream().stream();
+    TORCH_CHECK(is_sm90 || is_sm8x, "FlashAttention only supports Ampere GPUs or newer.");
+    // We will support Turing in the near future
+    // TORCH_CHECK(is_sm90 || is_sm8x || is_sm75, "FlashAttention only supports Turing GPUs or newer.");
+
     bool is_dropout = p_dropout > 0.0;
-    Launch_params<FMHA_fprop_params> launch_params(dprops, stream, is_dropout, return_softmax);
+    auto stream = at::cuda::getCurrentCUDAStream().stream();
 
-    TORCH_CHECK(q.dtype() == torch::kFloat16);
-    TORCH_CHECK(k.dtype() == torch::kFloat16);
-    TORCH_CHECK(v.dtype() == torch::kFloat16);
-    TORCH_CHECK(cu_seqlens_q.dtype() == torch::kInt32);
-    TORCH_CHECK(cu_seqlens_k.dtype() == torch::kInt32);
-    TORCH_CHECK(blockmask.dtype() == torch::kInt32);
-
-    TORCH_CHECK(q.is_cuda());
-    TORCH_CHECK(k.is_cuda());
-    TORCH_CHECK(v.is_cuda());
-    TORCH_CHECK(cu_seqlens_q.is_cuda());
-    TORCH_CHECK(cu_seqlens_k.is_cuda());
-    TORCH_CHECK(blockmask.is_cuda())
-
-    TORCH_CHECK(q.stride(-1) == 1);
-    TORCH_CHECK(k.stride(-1) == 1);
-    TORCH_CHECK(v.stride(-1) == 1);
-    TORCH_CHECK(cu_seqlens_k.is_contiguous());
-    TORCH_CHECK(cu_seqlens_k.is_contiguous());
-    TORCH_CHECK(blockmask.is_contiguous())
+    auto q_dtype = q.dtype();
+    TORCH_CHECK(q_dtype == torch::kFloat16 || q_dtype == torch::kBFloat16,
+                "FlashAttention only support fp16 and bf16 data type");
+    if (q_dtype == torch::kBFloat16) {
+        TORCH_CHECK(is_sm90 || is_sm8x, "bfloat16 is only supported on Ampere GPUs or newer");
+    }
+    TORCH_CHECK(k.dtype() == q_dtype, "query and key must have the same dtype");
+    TORCH_CHECK(v.dtype() == q_dtype, "query and value must have the same dtype");
+    TORCH_CHECK(out.dtype() == q_dtype, "query and out must have the same dtype");
+    TORCH_CHECK(dout.dtype() == q_dtype, "query and dout must have the same dtype");
+
+    TORCH_CHECK(q.is_cuda(), "Input tensor must be on CUDA device");
+    TORCH_CHECK(k.is_cuda(), "Input tensor must be on CUDA device");
+    TORCH_CHECK(v.is_cuda(), "Input tensor must be on CUDA device");
+    TORCH_CHECK(out.is_cuda(), "out tensor must be on CUDA device");
+    TORCH_CHECK(dout.is_cuda(), "dout tensor must be on CUDA device");
+    TORCH_CHECK(softmax_lse.is_cuda(), "softmax_lse tensor must be on CUDA device");
+
+    TORCH_CHECK(q.stride(-1) == 1, "Input tensor must have contiguous last dimension");
+    TORCH_CHECK(k.stride(-1) == 1, "Input tensor must have contiguous last dimension");
+    TORCH_CHECK(v.stride(-1) == 1, "Input tensor must have contiguous last dimension");
+    TORCH_CHECK(out.stride(-1) == 1, "out tensor must have contiguous last dimension");
+    TORCH_CHECK(dout.stride(-1) == 1, "dout tensor must have contiguous last dimension");
 
     const auto sizes = q.sizes();
 
-    const int batch_size = cu_seqlens_q.numel() - 1;
-    const int total_q = sizes[TOTAL_DIM];
-    const int num_heads = sizes[H_DIM];
-    const int head_size = sizes[D_DIM];
-    const int total_k = k.size(TOTAL_DIM);
-    TORCH_CHECK(batch_size > 0);
-    TORCH_CHECK(head_size == 16 || head_size == 32 || head_size == 64 || head_size == 128);
-
-    CHECK_SHAPE(q, total_q, num_heads, head_size);
-    CHECK_SHAPE(k, total_k, num_heads, head_size);
-    CHECK_SHAPE(v, total_k, num_heads, head_size);
-    CHECK_SHAPE(cu_seqlens_q, batch_size + 1);
-    CHECK_SHAPE(cu_seqlens_k, batch_size + 1);
+    const int batch_size = sizes[0];
+    const int seqlen_q = sizes[1];
+    const int num_heads = sizes[2];
+    const int head_size_og = dout.size(3);
+    const int head_size = sizes[3];
+    const int seqlen_k = k.size(1);
+    const int num_heads_k = k.size(2);
+    TORCH_CHECK(batch_size > 0, "batch size must be positive");
+    TORCH_CHECK(head_size % 8 == 0, "head_size should be a multiple of 8");
+    TORCH_CHECK(head_size <= 256, "FlashAttention backward only supports head dimension at most 256");
+    if (head_size > 192) {
+        TORCH_CHECK(is_sm80 || is_sm90, "FlashAttention backward for head dim > 192 requires A100/A800 or H100/H800");
+    }
+    TORCH_CHECK(num_heads % num_heads_k == 0, "Number of heads in key/value must divide number of heads in query");
+
+    auto round_multiple = [](int x, int m) { return (x + m - 1) / m * m; };
+    const int head_size_rounded = round_multiple(head_size, 32);
+    const int seqlen_q_rounded = round_multiple(seqlen_q, 128);
+    const int seqlen_k_rounded = round_multiple(seqlen_k, 128);
+
+    TORCH_CHECK(head_size == round_multiple(head_size_og, 8), "head_size must be head_size_og rounded to a multiple of 8");
+
+    CHECK_SHAPE(q, batch_size, seqlen_q, num_heads, head_size);
+    CHECK_SHAPE(k, batch_size, seqlen_k, num_heads_k, head_size);
+    CHECK_SHAPE(v, batch_size, seqlen_k, num_heads_k, head_size);
+    CHECK_SHAPE(out, batch_size, seqlen_q, num_heads, head_size);
+    CHECK_SHAPE(dout, batch_size, seqlen_q, num_heads, head_size_og);
+
+    at::Tensor dq, dk, dv;
+    if (dq_.has_value()) {
+        dq = dq_.value();
+        TORCH_CHECK(dq.dtype() == q_dtype, "dq must have the same dtype as q");
+        TORCH_CHECK(dq.is_cuda(), "dq must be on CUDA device");
+        TORCH_CHECK(dq.stride(-1) == 1, "dq must have contiguous last dimension");
+        CHECK_SHAPE(dq, batch_size, seqlen_q, num_heads, head_size);
+    } else {
+        dq = torch::empty_like(q);
+    }
+    if (dk_.has_value()) {
+        dk = dk_.value();
+        TORCH_CHECK(dk.dtype() == q_dtype, "dk must have the same dtype as q");
+        TORCH_CHECK(dk.is_cuda(), "dk must be on CUDA device");
+        TORCH_CHECK(dk.stride(-1) == 1, "dk must have contiguous last dimension");
+        CHECK_SHAPE(dk, batch_size, seqlen_k, num_heads_k, head_size);
+    } else {
+        dk = torch::empty_like(k);
+    }
+    if (dv_.has_value()) {
+        dv = dv_.value();
+        TORCH_CHECK(dv.dtype() == q_dtype, "dv must have the same dtype as q");
+        TORCH_CHECK(dv.is_cuda(), "dv must be on CUDA device");
+        TORCH_CHECK(dv.stride(-1) == 1, "dv must have contiguous last dimension");
+        CHECK_SHAPE(dv, batch_size, seqlen_k, num_heads_k, head_size);
+    } else {
+        dv = torch::empty_like(k);
+    }
+
+    at::Tensor dout_padded;
+    if (head_size_og % 8 != 0) {
+        dout_padded = torch::nn::functional::pad(dout, torch::nn::functional::PadFuncOptions({0, 8 - head_size_og % 8}));
+    } else {
+        dout_padded = dout;
+    }
+
+    // bool loop = seqlen_k > blocksize_c;
+    // TODO: change later, for now set to true for simplicity
+    bool loop = true;
 
-    int max_seqlen_k = ((max_seqlen_k_ + 256 - 1) / 256) * 256;
-    if( max_seqlen_k <= 256 ) {
-        max_seqlen_k = 256;
-    }
-    int max_seqlen_q = ((max_seqlen_q_ + 16 - 1) / 16) * 16;
-    bool loop = max_seqlen_k > 256;
-    CHECK_SHAPE(blockmask, max_seqlen_k / 256, max_seqlen_q / 16);
+    // Otherwise the kernel will be launched from cuda:0 device
+    // Cast to char to avoid compiler warning about narrowing
+    at::cuda::CUDAGuard device_guard{(char)q.get_device()};
 
     auto opts = q.options();
-
-    auto o = torch::zeros({ total_q, num_heads, head_size }, opts);
-
-    at::Tensor o_tmp;
+    auto softmax_d = torch::empty({batch_size, num_heads, seqlen_q_rounded}, opts.dtype(at::kFloat));
+    at::Tensor dq_accum;
+    at::Tensor dk_accum, dv_accum;
     if (loop) {
-        // o_tmp = torch::zeros({total, num_heads, head_size}, opts.dtype(at::kFloat));
-        o_tmp = torch::empty({total_q, num_heads, head_size}, opts.dtype(at::kFloat));
+        dq_accum = torch::empty({batch_size, num_heads, seqlen_q_rounded, head_size_rounded}, opts.dtype(at::kFloat));
+        // dk_accum = torch::empty({batch_size, num_heads_k, seqlen_k_rounded, head_size_rounded}, opts.dtype(at::kFloat));
+        // dv_accum = torch::empty({batch_size, num_heads_k, seqlen_k_rounded, head_size_rounded}, opts.dtype(at::kFloat));
     }
 
-    // auto softmax_lse = torch::full({batch_size, num_heads, max_seqlen_k}, -std::numeric_limits<float>::infinity(), opts.dtype(at::kFloat));
-    auto softmax_lse = torch::empty({batch_size, num_heads, max_seqlen_q}, opts.dtype(at::kFloat));
-
-    at::Tensor s;
-    if (return_softmax) {
-        s = torch::zeros({ batch_size, num_heads, max_seqlen_q, max_seqlen_k }, opts);
+    at::Tensor dk_expanded, dv_expanded;
+    if (num_heads_k != num_heads) {  // MQA / GQA
+        dk_expanded = torch::empty({batch_size, seqlen_k, num_heads, head_size}, opts);
+        dv_expanded = torch::empty({batch_size, seqlen_k, num_heads, head_size}, opts);
+    } else {
+        dk_expanded = dk;
+        dv_expanded = dv;
     }
 
-    auto gen = at::get_generator_or_default<at::CUDAGeneratorImpl>(
-        gen_, at::cuda::detail::getDefaultCUDAGenerator());
+    Flash_bwd_params params;
 
-    set_params_fprop(launch_params.params,
+    set_params_dgrad(params,
                      batch_size,
-                     max_seqlen_q,
-                     max_seqlen_k,
-                     num_heads,
-                     head_size,
-                     q, k, v, o,
-                     cu_seqlens_q.data_ptr(),
-                     cu_seqlens_k.data_ptr(),
-                     loop ? o_tmp.data_ptr() : nullptr,
-                     return_softmax ? s.data_ptr() : nullptr,
+                     seqlen_q, seqlen_k,
+                     seqlen_q_rounded, seqlen_k_rounded,
+                     num_heads, num_heads_k,
+                     head_size, head_size_rounded,
+                     q, k, v, out,
+                     dout_padded, dq, dk_expanded, dv_expanded,
+                     nullptr,
+                     nullptr,
+                     loop ? dq_accum.data_ptr() : nullptr,
+                     // loop ? dk_accum.data_ptr() : nullptr,
+                     // loop ? dv_accum.data_ptr() : nullptr,
+                     nullptr,
+                     nullptr,
                      softmax_lse.data_ptr(),
+                     softmax_d.data_ptr(),
                      p_dropout,
                      softmax_scale,
-                     is_causal,
-                     /*num_splits=*/1);
-    launch_params.params.blockmask = static_cast<int *>(blockmask.data_ptr());
-
-    run_fmha_block_fp16_sm80(launch_params, /*configure=*/ true);
-    // number of times random will be generated per thread, to offset philox counter in thc random
-    // state
-    int64_t counter_offset = launch_params.elts_per_thread;
+                     is_causal);
 
-    if( is_dropout ) {
+    auto launch = &run_mha_bwd;
+    // launch(params, stream, /*configure=*/true);
+
+    auto gen = at::get_generator_or_default<at::CUDAGeneratorImpl>(
+        gen_, at::cuda::detail::getDefaultCUDAGenerator());
+
+    // We use a custom RNG that increases the offset by batch_size * nheads * 32.
+    int64_t counter_offset = params.b * params.h * 32;
+
+    if (is_dropout) {
         // See Note [Acquire lock when using random generators]
         std::lock_guard<std::mutex> lock(gen->mutex_);
-        launch_params.params.philox_args = gen->philox_cuda_state(counter_offset);
+        params.philox_args = gen->philox_cuda_state(counter_offset);
     }
 
-    run_fmha_block_fp16_sm80(launch_params, /*configure=*/false);
+    launch(params, stream, /*configure=*/false);
 
-    std::vector<at::Tensor> result = {o, softmax_lse};
-    if (return_softmax) {result.push_back(s);}
-    return result;
+    // For MQA/GQA we need to sum dK and dV across the groups
+    if (num_heads_k != num_heads) {
+        at::sum_out(dk, at::reshape(dk_expanded, {batch_size, seqlen_k, num_heads_k, num_heads / num_heads_k, head_size}), {3});
+        at::sum_out(dv, at::reshape(dv_expanded, {batch_size, seqlen_k, num_heads_k, num_heads / num_heads_k, head_size}), {3});
+    }
+    if (head_size_og % 8 != 0) {
+        dq = dq.index({"...", torch::indexing::Slice(torch::indexing::None, head_size_og)});
+        dk = dk.index({"...", torch::indexing::Slice(torch::indexing::None, head_size_og)});
+        dv = dv.index({"...", torch::indexing::Slice(torch::indexing::None, head_size_og)});
+    }
+
+    return { dq, dk, dv, softmax_d };
 }
 
 std::vector<at::Tensor>
-mha_bwd_block(const at::Tensor &dout,  // total x num_heads, x head_size
-              const at::Tensor &q,   // total_q x num_heads x head_size, total_q := \sum_{i=0}^{b} s_i
-              const at::Tensor &k,   // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
-              const at::Tensor &v,   // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
-              const at::Tensor &out,   // total_q x num_heads x head_size
-              const at::Tensor &softmax_lse_,     // b x h x s softmax logsumexp
-              at::Tensor &dq,   // total_q x num_heads x head_size, total_q := \sum_{i=0}^{b} s_i
-              at::Tensor &dk,   // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
-              at::Tensor &dv,   // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
-              const at::Tensor &cu_seqlens_q,  // b+1
-              const at::Tensor &cu_seqlens_k,  // b+1
-              const at::Tensor &blockmask,   // (seqlen / 256, seqlen / 16)
-              const int max_seqlen_q_,
-              const int max_seqlen_k_,          // max sequence length to choose the kernel
-              const float p_dropout,         // probability to drop
-              const float softmax_scale,
-              const bool is_causal,
-              c10::optional<at::Generator> gen_
+mha_varlen_bwd(const at::Tensor &dout,  // total_q x num_heads, x head_size
+               const at::Tensor &q,   // total_q x num_heads x head_size, total_q := \sum_{i=0}^{b} s_i
+               const at::Tensor &k,   // total_k x num_heads_k x head_size, total_k := \sum_{i=0}^{b} s_i
+               const at::Tensor &v,   // total_k x num_heads_k x head_size, total_k := \sum_{i=0}^{b} s_i
+               const at::Tensor &out,   // total_q x num_heads x head_size
+               const at::Tensor &softmax_lse,     // b x h x s   softmax logsumexp
+               c10::optional<at::Tensor> &dq_,   // total_q x num_heads x head_size, total_q := \sum_{i=0}^{b} s_i
+               c10::optional<at::Tensor> &dk_,   // total_k x num_heads_k x head_size, total_k := \sum_{i=0}^{b} s_i
+               c10::optional<at::Tensor> &dv_,   // total_k x num_heads_k x head_size, total_k := \sum_{i=0}^{b} s_i
+               const at::Tensor &cu_seqlens_q,  // b+1
+               const at::Tensor &cu_seqlens_k,  // b+1
+               const int max_seqlen_q,
+               const int max_seqlen_k,          // max sequence length to choose the kernel
+               const float p_dropout,         // probability to drop
+               const float softmax_scale,
+               const bool zero_tensors,
+               const bool is_causal,
+               c10::optional<at::Generator> gen_
 ) {
     auto dprops = at::cuda::getCurrentDeviceProperties();
-    bool is_sm80 = dprops->major == 8 && dprops->minor == 0;
+    // bool is_sm75 = dprops->major == 7 && dprops->minor == 5;
     bool is_sm8x = dprops->major == 8 && dprops->minor >= 0;
+    bool is_sm80 = dprops->major == 8 && dprops->minor == 0;
     bool is_sm90 = dprops->major == 9 && dprops->minor == 0;
-    TORCH_CHECK(is_sm8x || is_sm90);
-    auto launch = &run_fmha_block_dgrad_fp16_sm80;
-
+    TORCH_CHECK(is_sm90 || is_sm8x, "FlashAttention only supports Ampere GPUs or newer.");
+    // We will support Turing in the near future
+    // TORCH_CHECK(is_sm90 || is_sm8x || is_sm75, "FlashAttention only supports Turing GPUs or newer.");
     bool is_dropout = p_dropout > 0.0;
     auto stream = at::cuda::getCurrentCUDAStream().stream();
 
-    TORCH_CHECK(q.dtype() == torch::kFloat16);
-    TORCH_CHECK(k.dtype() == torch::kFloat16);
-    TORCH_CHECK(v.dtype() == torch::kFloat16);
-    TORCH_CHECK(out.dtype() == torch::kFloat16);
-    TORCH_CHECK(dout.dtype() == torch::kFloat16);
-    TORCH_CHECK(dq.dtype() == torch::kFloat16);
-    TORCH_CHECK(dk.dtype() == torch::kFloat16);
-    TORCH_CHECK(dv.dtype() == torch::kFloat16);
-    TORCH_CHECK(cu_seqlens_q.dtype() == torch::kInt32);
-    TORCH_CHECK(cu_seqlens_k.dtype() == torch::kInt32);
-    TORCH_CHECK(blockmask.dtype() == torch::kInt32);
-
-    TORCH_CHECK(q.is_cuda());
-    TORCH_CHECK(k.is_cuda());
-    TORCH_CHECK(v.is_cuda());
-    TORCH_CHECK(out.is_cuda());
-    TORCH_CHECK(dout.is_cuda());
-    TORCH_CHECK(softmax_lse_.is_cuda());
-    TORCH_CHECK(cu_seqlens_q.is_cuda());
-    TORCH_CHECK(cu_seqlens_k.is_cuda());
-    TORCH_CHECK(blockmask.is_cuda());
-
-    TORCH_CHECK(q.stride(-1) == 1);
-    TORCH_CHECK(k.stride(-1) == 1);
-    TORCH_CHECK(v.stride(-1) == 1);
-    TORCH_CHECK(out.is_contiguous());
-    TORCH_CHECK(dout.is_contiguous());
-    TORCH_CHECK(dq.stride(-1) == 1);
-    TORCH_CHECK(dk.stride(-1) == 1);
-    TORCH_CHECK(dv.stride(-1) == 1);
-    TORCH_CHECK(cu_seqlens_q.is_contiguous());
-    TORCH_CHECK(cu_seqlens_k.is_contiguous());
-    TORCH_CHECK(blockmask.is_contiguous());
+    auto q_dtype = q.dtype();
+    TORCH_CHECK(q_dtype == torch::kFloat16 || q_dtype == torch::kBFloat16,
+                "FlashAttention only support fp16 and bf16 data type");
+    if (q_dtype == torch::kBFloat16) {
+        TORCH_CHECK(is_sm90 || is_sm8x, "bfloat16 is only supported on Ampere GPUs or newer");
+    }
+    TORCH_CHECK(k.dtype() == q_dtype, "query and key must have the same dtype");
+    TORCH_CHECK(v.dtype() == q_dtype, "query and value must have the same dtype");
+    TORCH_CHECK(out.dtype() == q_dtype, "query and out must have the same dtype");
+    TORCH_CHECK(dout.dtype() == q_dtype, "query and dout must have the same dtype");
+    TORCH_CHECK(cu_seqlens_q.dtype() == torch::kInt32, "cu_seqlens_q must have dtype int32");
+    TORCH_CHECK(cu_seqlens_k.dtype() == torch::kInt32, "cu_seqlens_k must have dtype int32");
+
+    TORCH_CHECK(q.is_cuda(), "Input tensor must be on CUDA device");
+    TORCH_CHECK(k.is_cuda(), "Input tensor must be on CUDA device");
+    TORCH_CHECK(v.is_cuda(), "Input tensor must be on CUDA device");
+    TORCH_CHECK(out.is_cuda(), "out tensor must be on CUDA device");
+    TORCH_CHECK(dout.is_cuda(), "dout tensor must be on CUDA device");
+    TORCH_CHECK(softmax_lse.is_cuda(), "softmax_lse tensor must be on CUDA device");
+    TORCH_CHECK(cu_seqlens_q.is_cuda(), "cu_seqlens_q must be on CUDA device");
+    TORCH_CHECK(cu_seqlens_k.is_cuda(), "cu_seqlens_k must be on CUDA device");
+
+    TORCH_CHECK(q.stride(-1) == 1, "Input tensor must have contiguous last dimension");
+    TORCH_CHECK(k.stride(-1) == 1, "Input tensor must have contiguous last dimension");
+    TORCH_CHECK(v.stride(-1) == 1, "Input tensor must have contiguous last dimension");
+    TORCH_CHECK(out.stride(-1) == 1, "out tensor must have contiguous last dimension");
+    TORCH_CHECK(dout.stride(-1) == 1, "dout tensor must have contiguous last dimension");
+    TORCH_CHECK(cu_seqlens_q.is_contiguous(), "cu_seqlens_q must be contiguous");
+    TORCH_CHECK(cu_seqlens_k.is_contiguous(), "cu_seqlens_k must be contiguous");
 
     const auto sizes = q.sizes();
 
+    const int total_q = sizes[0];
     const int batch_size = cu_seqlens_q.numel() - 1;
-    const int total_q = sizes[TOTAL_DIM];
-    const int num_heads = sizes[H_DIM];
-    const int head_size = sizes[D_DIM];
-    const int total_k = k.size(TOTAL_DIM);
-    TORCH_CHECK(batch_size > 0);
-    TORCH_CHECK(head_size == 16 || head_size == 32 || head_size == 64 || head_size == 128);
-    if (head_size == 128) {  // TODO: eventually we should support SM86 and SM70 with d=128 as well
-        TORCH_CHECK(is_sm80 || is_sm90);
-    }
+    const int num_heads = sizes[1];
+    const int head_size_og = dout.size(2);
+    const int head_size = sizes[2];
+    const int total_k = k.size(0);
+    const int num_heads_k = k.size(1);
+    TORCH_CHECK(batch_size > 0, "batch size must be positive");
+    TORCH_CHECK(head_size % 8 == 0, "head_size should be a multiple of 8");
+    TORCH_CHECK(head_size <= 256, "FlashAttention backward only supports head dimension at most 256");
+    if (head_size > 192) {
+        TORCH_CHECK(is_sm80 || is_sm90, "FlashAttention backward for head dim > 192 requires A100/A800 or H100/H800");
+    }
+    TORCH_CHECK(num_heads % num_heads_k == 0, "Number of heads in key/value must divide number of heads in query");
+
+    auto round_multiple = [](int x, int m) { return (x + m - 1) / m * m; };
+    const int head_size_rounded = round_multiple(head_size, 32);
+    const int seqlen_q_rounded = round_multiple(max_seqlen_q, 128);
+    const int seqlen_k_rounded = round_multiple(max_seqlen_k, 128);
+
+    TORCH_CHECK(head_size == round_multiple(head_size_og, 8), "head_size must be head_size_og rounded to a multiple of 8");
 
     CHECK_SHAPE(q, total_q, num_heads, head_size);
-    CHECK_SHAPE(k, total_k, num_heads, head_size);
-    CHECK_SHAPE(v, total_k, num_heads, head_size);
+    CHECK_SHAPE(k, total_k, num_heads_k, head_size);
+    CHECK_SHAPE(v, total_k, num_heads_k, head_size);
     CHECK_SHAPE(out, total_q, num_heads, head_size);
-    CHECK_SHAPE(dout, total_q, num_heads, head_size);
-    CHECK_SHAPE(dq, total_q, num_heads, head_size);
-    CHECK_SHAPE(dk, total_k, num_heads, head_size);
-    CHECK_SHAPE(dv, total_k, num_heads, head_size);
+    CHECK_SHAPE(dout, total_q, num_heads, head_size_og);
     CHECK_SHAPE(cu_seqlens_q, batch_size + 1);
     CHECK_SHAPE(cu_seqlens_k, batch_size + 1);
 
-    int max_seqlen_k = ((max_seqlen_k_ + 256 - 1) / 256) * 256;
-    if( max_seqlen_k <= 256 ) {
-        max_seqlen_k = 256;
-    }
-    int max_seqlen_q = ((max_seqlen_q_ + 16 - 1) / 16) * 16;
-    bool loop = max_seqlen_k > 256;
-    CHECK_SHAPE(blockmask, max_seqlen_k / 256, max_seqlen_q / 16);
+    at::Tensor dq, dk, dv;
+    if (dq_.has_value()) {
+        dq = dq_.value();
+        TORCH_CHECK(dq.dtype() == q_dtype, "dq must have the same dtype as q");
+        TORCH_CHECK(dq.is_cuda(), "dq must be on CUDA device");
+        TORCH_CHECK(dq.stride(-1) == 1, "dq must have contiguous last dimension");
+        CHECK_SHAPE(dq, total_q, num_heads, head_size);
+    } else {
+        dq = torch::empty_like(q);
+    }
+    if (dk_.has_value()) {
+        dk = dk_.value();
+        TORCH_CHECK(dk.dtype() == q_dtype, "dk must have the same dtype as q");
+        TORCH_CHECK(dk.is_cuda(), "dk must be on CUDA device");
+        TORCH_CHECK(dk.stride(-1) == 1, "dk must have contiguous last dimension");
+        CHECK_SHAPE(dk, total_k, num_heads_k, head_size);
+    } else {
+        dk = torch::empty_like(k);
+    }
+    if (dv_.has_value()) {
+        dv = dv_.value();
+        TORCH_CHECK(dv.dtype() == q_dtype, "dv must have the same dtype as q");
+        TORCH_CHECK(dv.is_cuda(), "dv must be on CUDA device");
+        TORCH_CHECK(dv.stride(-1) == 1, "dv must have contiguous last dimension");
+        CHECK_SHAPE(dv, total_k, num_heads_k, head_size);
+    } else {
+        dv = torch::empty_like(k);
+    }
+
+    at::Tensor dout_padded;
+    if (head_size_og % 8 != 0) {
+        dout_padded = torch::nn::functional::pad(dout, torch::nn::functional::PadFuncOptions({0, 8 - head_size_og % 8}));
+    } else {
+        dout_padded = dout;
+    }
+
+    // bool loop = max_seqlen_k > blocksize_c;
+    // TODO: change later, for now set to true for simplicity
+    bool loop = true;
 
-    // It's possible the softmax_lse_ from the fwd has a different length since blocksize_c could be different.
-    auto softmax_lse = softmax_lse_.index({torch::indexing::Slice(), torch::indexing::Slice(), torch::indexing::Slice(torch::indexing::None, max_seqlen_q)}).contiguous();
+    // Otherwise the kernel will be launched from cuda:0 device
+    // Cast to char to avoid compiler warning about narrowing
+    at::cuda::CUDAGuard device_guard{(char)q.get_device()};
 
     auto opts = q.options();
-    auto softmax_d = torch::empty({batch_size, num_heads, max_seqlen_q}, opts.dtype(at::kFloat));
-    at::Tensor dq_tmp;
+    auto softmax_d = torch::empty({batch_size, num_heads, seqlen_q_rounded}, opts.dtype(at::kFloat));
+    at::Tensor dq_accum;
     if (loop) {
-        // dq_tmp = torch::zeros({total, num_heads, head_size}, opts.dtype(at::kFloat));
-        dq_tmp = torch::empty({total_q, num_heads, head_size}, opts.dtype(at::kFloat));
+        dq_accum = torch::empty({batch_size, num_heads, seqlen_q_rounded, head_size_rounded}, opts.dtype(at::kFloat));
+    }
+
+    at::Tensor dk_expanded, dv_expanded;
+    if (num_heads_k != num_heads) {  // MQA / GQA
+        dk_expanded = torch::empty({total_k, num_heads, head_size}, opts);
+        dv_expanded = torch::empty({total_k, num_heads, head_size}, opts);
+    } else {
+        dk_expanded = dk;
+        dv_expanded = dv;
+    }
+
+    if( zero_tensors ) {
+        dq.zero_();
+        dk_expanded.zero_();
+        dv_expanded.zero_();
+        softmax_d.zero_();
     }
 
-    FMHA_dgrad_params params;
+    Flash_bwd_params params;
 
     set_params_dgrad(params,
                      batch_size,
-                     max_seqlen_q,
-                     max_seqlen_k,
-                     num_heads,
-                     head_size,
+                     max_seqlen_q, max_seqlen_k,
+                     seqlen_q_rounded, seqlen_k_rounded,
+                     num_heads, num_heads_k,
+                     head_size, head_size_rounded,
                      q, k, v, out,
-                     dq, dk, dv,
+                     dout_padded, dq, dk_expanded, dv_expanded,
                      cu_seqlens_q.data_ptr(),
                      cu_seqlens_k.data_ptr(),
-                     loop ? dq_tmp.data_ptr() : nullptr,
-                     dout.data_ptr(),
+                     loop ? dq_accum.data_ptr() : nullptr,
+                     nullptr,
+                     nullptr,
                      softmax_lse.data_ptr(),
                      softmax_d.data_ptr(),
                      p_dropout,
                      softmax_scale,
-                     is_causal,
-                     /*num_splits=*/1);
-    params.blockmask = static_cast<int *>(blockmask.data_ptr());
+                     is_causal);
+
+    auto launch = &run_mha_bwd;
+    // launch(params, stream, /*configure=*/true);
 
     auto gen = at::get_generator_or_default<at::CUDAGeneratorImpl>(
         gen_, at::cuda::detail::getDefaultCUDAGenerator());
 
-    // We're gonna reset the rng state in Python after this kernel, so the counter offset
-    // here doesn't matter at all. We just choose an arbitrary number;
-    int64_t counter_offset = 4;
+    // We use a custom RNG that increases the offset by batch_size * nheads * 32.
+    int64_t counter_offset = params.b * params.h * 32;
 
-    if( is_dropout ) {
+    if (is_dropout) {
         // See Note [Acquire lock when using random generators]
         std::lock_guard<std::mutex> lock(gen->mutex_);
         params.philox_args = gen->philox_cuda_state(counter_offset);
     }
 
-    launch(params, stream);
+    launch(params, stream, /*configure=*/false);
+
+    // For MQA/GQA we need to sum dK and dV across the groups
+    if (num_heads_k != num_heads) {
+        at::sum_out(dk, at::reshape(dk_expanded, {total_k, num_heads_k, num_heads / num_heads_k, head_size}), {2});
+        at::sum_out(dv, at::reshape(dv_expanded, {total_k, num_heads_k, num_heads / num_heads_k, head_size}), {2});
+    }
+    if (head_size_og % 8 != 0) {
+        dq = dq.index({"...", torch::indexing::Slice(torch::indexing::None, head_size_og)});
+        dk = dk.index({"...", torch::indexing::Slice(torch::indexing::None, head_size_og)});
+        dv = dv.index({"...", torch::indexing::Slice(torch::indexing::None, head_size_og)});
+    }
+
     return { dq, dk, dv, softmax_d };
 }
 
-
 PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
-    m.doc() = "Fused Multi-head Self-attention";
+    m.doc() = "FlashAttention";
     m.def("fwd", &mha_fwd, "Forward pass");
+    m.def("varlen_fwd", &mha_varlen_fwd, "Forward pass (variable length)");
     m.def("bwd", &mha_bwd, "Backward pass");
-    m.def("fwd_block", &mha_fwd_block, "Forward pass (blocksparse)");
-    m.def("bwd_block", &mha_bwd_block, "Backward pass (blocksparse)");
+    m.def("varlen_bwd", &mha_varlen_bwd, "Backward pass (variable length)");
 }
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/src/fmha_fprop_kernel_1xN.h` & `flash_attn-2.0.0/csrc/flash_attn/src/flash_fwd_kernel.h`

 * *Files 26% similar despite different names*

```diff
@@ -1,707 +1,576 @@
-/***************************************************************************************************
- * Copyright (c) 2022, Tri Dao.
- * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
- * 
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *     * Redistributions of source code must retain the above copyright
- *       notice, this list of conditions and the following disclaimer.
- *     * Redistributions in binary form must reproduce the above copyright
- *       notice, this list of conditions and the following disclaimer in the
- *       documentation and/or other materials provided with the distribution.
- *     * Neither the name of the NVIDIA CORPORATION nor the
- *       names of its contributors may be used to endorse or promote products
- *       derived from this software without specific prior written permission.
- * 
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
- * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
- * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
- * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
- * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
- * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
- * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
- * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
- * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- *
+/******************************************************************************
+ * Copyright (c) 2023, Tri Dao.
  ******************************************************************************/
 
 #pragma once
 
-#include "fmha_kernel.h"
-#include <fmha/kernel_traits.h>
-#include <fmha/gemm.h>
-#include <fmha/utils.h>
+#include <cmath>
+#include <cute/algorithm/copy.hpp>
+#include <cute/algorithm/gemm.hpp>
+
+#include <cutlass/cutlass.h>
+#include <cutlass/array.h>
+#include <cutlass/numeric_types.h>
+#include <cutlass/numeric_conversion.h>
+
+#include "block_info.h"
+#include "kernel_traits.h"
+#include "utils.h"
+#include "softmax.h"
+#include "philox.cuh"
 
-namespace fmha {
+namespace flash {
 
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template<typename Kernel_traits>
-struct Gemm_Q_K_base {
-    using Smem_tile_o = typename Kernel_traits::Smem_tile_o;
-    using Smem_tile_q = typename Kernel_traits::Smem_tile_q;
-    using Smem_tile_k = typename Kernel_traits::Smem_tile_k;
-    using Fragment_q = typename Smem_tile_q::Fragment;
-    using Fragment_k = typename Smem_tile_k::Fragment;
-
-    // The description of the CTA tile for the 1st batched GEMM.
-    using Cta_tile_p = typename Kernel_traits::Cta_tile_p;
-
-    // The MMA tile for the 1st GEMM.
-    using Mma_tile_p = fmha::Hmma_tile<Cta_tile_p>;
-
-    static constexpr int SMEM_BYTES_SOFTMAX = Cta_tile_p::M * Cta_tile_p::WARPS_N * sizeof(float) * 2;
-
-    __device__ inline Gemm_Q_K_base(char * smem_ptr_q, char * smem_ptr_k, const int tidx) 
-        : smem_q(smem_ptr_q, tidx)
-        , smem_k(smem_ptr_k, tidx) {
-
-    }
+using namespace cute;
 
-    __device__ inline void load_q() {
-        smem_q.load(frag_q[0], 0);
-    }
-
-    __device__ inline void reload_q() {
-        smem_q.load(frag_q[0], 0);
-    }
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
-    Fragment_q frag_q[2][Mma_tile_p::MMAS_M];
-    Smem_tile_q smem_q;
-    Smem_tile_k smem_k;
-};
+template <int MMA_M,
+          class... Args,
+          class TiledMMA>
+CUTE_HOST_DEVICE
+auto
+make_tiled_copy_A_warpcontiguousM(Copy_Atom<Args...> const& copy_atom,
+                                 TiledMMA           const& tiled_mma) {
+    using TileShape_MNK = typename TiledMMA::TiledShape_MNK;
+    using AtomShape_MNK = typename TiledMMA::AtomShape_MNK;
+    constexpr int AtomShape_M = decltype(size<0>(AtomShape_MNK{}))::value;
+    constexpr int kNWarps = decltype(size<0>(TileShape_MNK{}))::value / AtomShape_M;
+    constexpr int MMAStride_M = MMA_M * AtomShape_M;
+    auto t = make_tile(Layout<Shape<Int<AtomShape_M>, Int<kNWarps>>,
+                              Stride<_1, Int<MMAStride_M>> >{},
+                       make_layout(size<2>(TileShape_MNK{})));
+    // if (cute::thread0()) {printf("make_tiled_copy_A_warpcontiguousM "); print(t); printf("\n");  }
+    return make_tiled_copy_impl(copy_atom, tiled_mma.get_layoutA_TV(), t);
+}
 
-template<typename Kernel_traits, bool K_in_regs, typename elem_type_=__half>
-struct Gemm_Q_K : public Gemm_Q_K_base<Kernel_traits> {
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
-    using Base = Gemm_Q_K_base<Kernel_traits>;
-    using Smem_tile_o = typename Base::Smem_tile_o;
-    using Smem_tile_q = typename Base::Smem_tile_q;
-    using Smem_tile_k = typename Base::Smem_tile_k;
-    using Fragment_k = typename Base::Fragment_k;
-    using Mma_tile_p = typename Base::Mma_tile_p;
-    using elem_type = elem_type_;
-
-    static constexpr bool SHARE_SMEM_FOR_K_AND_V = Kernel_traits::SHARE_SMEM_FOR_K_AND_V;
-    // If V is stored in shared memory, we can't load K using the same shared memory.
-    static_assert(Kernel_traits::V_IN_REGS);
-
-    static constexpr int SMEM_OFFSET_O = Smem_tile_q::BYTES_PER_TILE;
-    static constexpr int SMEM_OFFSET_SOFTMAX = SMEM_OFFSET_O + Smem_tile_o::BYTES_PER_TILE;
-    static constexpr int SMEM_OFFSET_V = Smem_tile_q::BYTES_PER_TILE + (SHARE_SMEM_FOR_K_AND_V ? 0 : Smem_tile_k::BYTES_PER_TILE);
-
-    // Q | K / V
-    //   | O | SOFTMAX
-    static constexpr int SMEM_BYTES = Smem_tile_q::BYTES_PER_TILE 
-                                    + std::max((SHARE_SMEM_FOR_K_AND_V ? 1 : 2) * Smem_tile_k::BYTES_PER_TILE,
-                                               Smem_tile_o::BYTES_PER_TILE + Base::SMEM_BYTES_SOFTMAX);
+template <int MMA_M,
+          class... Args,
+          class TiledMMA>
+CUTE_HOST_DEVICE
+auto
+make_tiled_copy_C_warpcontiguousM(Copy_Atom<Args...> const& copy_atom,
+                                 TiledMMA           const& tiled_mma) {
+    using TileShape_MNK = typename TiledMMA::TiledShape_MNK;
+    using AtomShape_MNK = typename TiledMMA::AtomShape_MNK;
+    constexpr int AtomShape_M = decltype(size<0>(AtomShape_MNK{}))::value;
+    constexpr int kNWarps = decltype(size<0>(TileShape_MNK{}))::value / AtomShape_M;
+    constexpr int MMAStride_M = MMA_M * AtomShape_M;
+    auto t = make_tile(Layout<Shape<Int<AtomShape_M>, Int<kNWarps>>,
+                              Stride<_1, Int<MMAStride_M>> >{},
+                       // TODO: Shouldn't this be size<1>?
+                       make_layout(size<2>(TileShape_MNK{})));
+    // if (cute::thread0()) {printf("make_tiled_copy_C_warpcontiguousM "); print(t); printf("\n");  }
+    return make_tiled_copy_impl(copy_atom, tiled_mma.get_layoutC_TV(), t);
+}
 
-    __device__ inline Gemm_Q_K(char * smem_, const int tidx) 
-        : Base(smem_, smem_ + Smem_tile_q::BYTES_PER_TILE, tidx) {
-    }
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
-    __device__ inline void load_k(){
+template<bool Is_first, bool Check_inf=false, typename Tensor0, typename Tensor1, typename Tensor2>
+inline __device__ void softmax_rescale_o(Tensor0 &scores, Tensor1 &scores_max, Tensor1 &scores_sum,
+                                         Tensor2 &acc_o, float softmax_scale_log2) {
+    if (Is_first) {
+        flash::template reduce_max</*zero_init=*/true>(scores, scores_max);
+        flash::scale_apply_exp2(scores, scores_max, softmax_scale_log2);
+        flash::reduce_sum(scores, scores_sum);
+    } else {
+        Tensor scores_max_prev = make_fragment_like(scores_max);
+        copy(scores_max, scores_max_prev);
+        flash::template reduce_max</*zero_init=*/false>(scores, scores_max);
+        // Reshape acc_o from (MMA=4, MMA_M, MMA_K) to (nrow=(2, MMA_M), ncol=(2, MMA_K))
+        Tensor acc_o_rowcol = make_tensor(acc_o.data(), flash::convert_layout_acc_rowcol(acc_o.layout()));
         #pragma unroll
-        for( int ki = 0; ki < Mma_tile_p::MMAS_K; ++ki ) {
-            Base::smem_k.load(frag_k[ki], ki);
+        for (int mi = 0; mi < size(scores_max); ++mi) {
+            float scores_max_cur = !Check_inf
+                ? scores_max(mi)
+                : (scores_max(mi) == -INFINITY ? 0.0f : scores_max(mi));
+            float scores_scale = exp2f((scores_max_prev(mi) - scores_max_cur) * softmax_scale_log2);
+            scores_sum(mi) *= scores_scale;
+            #pragma unroll
+            for (int ni = 0; ni < size<1>(acc_o_rowcol); ++ni) { acc_o_rowcol(mi, ni) *= scores_scale; }
         }
-    }
-
-    template<typename Acc, int M, int N>
-    __device__ inline void operator()(Acc (&acc_p)[M][N]){
-        // Do this part of P^T = (Q * K^T)^T.
+        flash::scale_apply_exp2(scores, scores_max, softmax_scale_log2);
+        Tensor scores_sum_cur = make_fragment_like(scores_sum);
+        flash::reduce_sum(scores, scores_sum_cur);
         #pragma unroll
-        for( int ki = 1; ki < Mma_tile_p::MMAS_K; ++ki ) {
-            // Trigger the load from shared memory for the next series of Q values.
-            Base::smem_q.load(Base::frag_q[ki & 1], ki);
-            // Do the math for the values already in registers.
-            fmha::gemm_cl<elem_type>(acc_p, Base::frag_q[(ki - 1) & 1], frag_k[(ki - 1)]);
-        }
-        // Do the final stage of math.
-        {
-            int ki = Mma_tile_p::MMAS_K;
-            fmha::gemm_cl<elem_type>(acc_p, Base::frag_q[(ki - 1) & 1], frag_k[(ki - 1)]);
-        }
+        for (int mi = 0; mi < size(scores_sum); ++mi) { scores_sum(mi) += scores_sum_cur(mi); }
     }
-
-    __device__ inline void reload_k(){
-        // Noop.
-    }
-
-    Fragment_k frag_k[Mma_tile_p::MMAS_K][Mma_tile_p::MMAS_N];
 };
 
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
-template<typename Kernel_traits, typename elem_type_>
-struct Gemm_Q_K<Kernel_traits, false, elem_type_> : public Gemm_Q_K_base<Kernel_traits> {
-    using Base = Gemm_Q_K_base<Kernel_traits>;
-    using Smem_tile_o = typename Base::Smem_tile_o;
-    using Smem_tile_q = typename Base::Smem_tile_q;
-    using Smem_tile_k = typename Base::Smem_tile_k;
-    using Smem_tile_v = typename Kernel_traits::Smem_tile_v;
-    using Fragment_k = typename Base::Fragment_k;
-    using Mma_tile_p = typename Base::Mma_tile_p;
-    using elem_type = elem_type_;
-    Fragment_k frag_k[2][Mma_tile_p::MMAS_N];
-
-    static constexpr bool SHARE_SMEM_FOR_K_AND_V = Kernel_traits::SHARE_SMEM_FOR_K_AND_V;
-    static constexpr bool V_IN_REGS = Kernel_traits::V_IN_REGS;
-    static_assert(V_IN_REGS || !SHARE_SMEM_FOR_K_AND_V);
-
-    static constexpr int SMEM_OFFSET_V = Smem_tile_q::BYTES_PER_TILE + (SHARE_SMEM_FOR_K_AND_V ? 0 : Smem_tile_k::BYTES_PER_TILE);
-    static_assert(Smem_tile_v::BYTES_PER_TILE == (int) Smem_tile_k::BYTES_PER_TILE);
-    static constexpr int SMEM_OFFSET_O = SMEM_OFFSET_V + Smem_tile_v::BYTES_PER_TILE;
-    static constexpr int SMEM_OFFSET_SOFTMAX = SMEM_OFFSET_O + Smem_tile_o::BYTES_PER_TILE;
-
-    // If V_IN_REGS and SHARE_SMEM_FOR_K_AND_V:      Q | K/V | O | SOFTMAX
-    // If !V_IN_REGS (then !SHARE_SMEM_FOR_K_AND_V): Q | K   | V | O | SOFTMAX
-    static constexpr int SMEM_BYTES = Smem_tile_q::BYTES_PER_TILE
-                                    + (SHARE_SMEM_FOR_K_AND_V ? 1 : 2) * Smem_tile_k::BYTES_PER_TILE 
-                                    + Smem_tile_o::BYTES_PER_TILE + Base::SMEM_BYTES_SOFTMAX;
-
-    __device__ inline Gemm_Q_K(char * smem_, const int tidx) 
-      : Base(smem_, smem_ + Smem_tile_q::BYTES_PER_TILE, tidx) {
-    }
-
-    __device__ inline void load_k(){
-        Base::smem_k.load(frag_k[0], 0);
-    }
-
-    template<typename Acc, int M, int N>
-    __device__ inline void operator()(Acc (&acc_p)[M][N]){
-        // Do this part of P^T = (Q * K^T)^T.
-        #pragma unroll
-        for( int ki = 1; ki < Mma_tile_p::MMAS_K; ++ki ) {
-            // Trigger the load from shared memory for the next series of Q values.
-            Base::smem_q.load(Base::frag_q[ki & 1], ki);
-            Base::smem_k.load(frag_k[ki & 1], ki);
-            // Do the math for the values already in registers.
-            fmha::gemm_cl<elem_type>(acc_p, Base::frag_q[(ki - 1) & 1], frag_k[(ki - 1) & 1]);
-        }
-        // Do the final stage of math.
-        {
-            int ki = Mma_tile_p::MMAS_K;
-            fmha::gemm_cl<elem_type>(acc_p, Base::frag_q[(ki - 1) & 1], frag_k[(ki - 1) & 1]);
-        }
-    }
-
-    __device__ inline void reload_k(){
-        Base::smem_k.load(frag_k[0], 0);
+template<typename Engine0, typename Layout0, typename Engine1, typename Layout1, typename TiledCopy>
+inline __device__ void write_softmax_to_gmem(
+    Tensor<Engine0, Layout0> const &tOrP, Tensor<Engine1, Layout1> &tPgP, TiledCopy gmem_thr_copy_P
+) {
+    // Reshape tOrP from (8, MMA_M, MMA_N) to (8, MMA_M * MMA_N)
+    Layout l = tOrP.layout();
+    Tensor tPrP = make_tensor(tOrP.data(), make_layout(get<0>(l), make_layout(get<1>(l), get<2>(l))));
+    CUTE_STATIC_ASSERT_V(size<2>(tPgP) == _1{});
+    CUTE_STATIC_ASSERT_V(size<1>(tPrP) == size<1>(tPgP));
+    #pragma unroll
+    for (int mi = 0; mi < size<1>(tPrP); ++mi) {
+        copy(gmem_thr_copy_P, tPrP(_, mi), tPgP(_, mi, 0));
     }
 };
 
-template<typename Kernel_traits>
-constexpr size_t get_dynamic_smem_size(){
-    return Gemm_Q_K<Kernel_traits, Kernel_traits::K_IN_REGS>::SMEM_BYTES;
-}
-
-template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Return_softmax, bool Is_first, bool Is_last, typename Params, typename Prng>
-inline __device__ void device_1xN_(const Params &params, const int bidb, const int bidh, int steps, Prng &ph, const int loop_step_idx) {
-
-#if defined(__CUDA_ARCH__) &&  __CUDA_ARCH__ >= 800
-    using elem_type = typename Kernel_traits::elem_type;
-#else
-    constexpr bool is_fp16_type = std::is_same<typename Kernel_traits::elem_type, __half>::value;
-    assert(is_fp16_type);
-    using elem_type = __half;
-#endif
-
-    // The description of the CTA tile for the 1st batched GEMM.
-    using Cta_tile_p = typename Kernel_traits::Cta_tile_p;
-    // The description of the CTA tile for the 2nd batched GEMM.
-    using Cta_tile_o = typename Kernel_traits::Cta_tile_o;
-
-    // The MMA tile for the 1st GEMM.
-    using Mma_tile_p = fmha::Hmma_tile<Cta_tile_p>;
-    // The MMA tile for the 2nd GEMM.
-    using Mma_tile_o = fmha::Hmma_tile<Cta_tile_o>;
-
-    // The global memory tile to load Q.
-    using Gmem_tile_q = typename Kernel_traits::Gmem_tile_q;
-
-    // The global memory tile to load K.
-    using Gmem_tile_k = typename Kernel_traits::Gmem_tile_k;
-
-    // The global memory tile to load V.
-    using Gmem_tile_v = typename Kernel_traits::Gmem_tile_v;
-    // The shared memory tile to swizzle V.
-    using Smem_tile_v = typename Kernel_traits::Smem_tile_v;
-
-    // The global memory tile to store O.
-    using Gmem_tile_o = typename Kernel_traits::Gmem_tile_o;
-    using Gmem_tile_o_tmp = fmha::Gmem_tile_o<Cta_tile_o, 4>;
-    // The shared memory tile to swizzle O.
-    using Smem_tile_o = typename Kernel_traits::Smem_tile_o;
-
-    using Gmem_tile_s = typename Kernel_traits::Gmem_tile_s;
-
-    using Gmem_softmax_sum = typename Kernel_traits::Gmem_softmax_sum;
-
-    using Smem_softmax_sum = typename Kernel_traits::Smem_dp_sum;
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
-    using Gemm1 = Gemm_Q_K<Kernel_traits, Kernel_traits::K_IN_REGS, elem_type>;
+template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Is_even_N, bool Is_even_K, bool Return_softmax, typename Params>
+inline __device__ void compute_attn_1rowblock(const Params &params, const int bidb, const int bidh, const int m_block) {
 
-    using Softmax = fmha::Softmax<Cta_tile_p, Kernel_traits>;
+    using Element = typename Kernel_traits::Element;
+    using ElementAccum = typename Kernel_traits::ElementAccum;
+    using index_t = typename Kernel_traits::index_t;
 
     // Shared memory.
     extern __shared__ char smem_[];
 
     // The thread index.
     const int tidx = threadIdx.x;
 
-    // How many steps to jump per iteration, which is the same as params.num_splits.
-    const int step_stride = gridDim.z;
-
-    const BlockInfoPadded<Kernel_traits::THREADS> binfo(params, bidb, bidh, tidx);
-    // if( binfo.stop_early() ) return;
-    if( binfo.stop_early(loop_step_idx * Cta_tile_p::N) ) return;
-
-    Gemm1 gemm_q_k(smem_, tidx);
-    // Allocate the global memory tile loader for Q.
-    Gmem_tile_q gmem_q(params.q_ptr, params.q_row_stride_in_elts, params.q_head_stride_in_elts,
-                       params.d, binfo, tidx, true);
-    // Allocate the global memory tile loader for O.
-    Gmem_tile_o gmem_o(params.o_ptr, params.o_row_stride_in_elts, params.o_head_stride_in_elts,
-                       params.d, binfo, tidx);
-    Gmem_tile_o_tmp gmem_o_tmp(params.o_tmp_ptr, params.o_tmp_row_stride_in_elts,
-                               params.o_tmp_head_stride_in_elts, params.d, binfo, tidx);
-    // Allocate the global memory tile loader for S.
-    Gmem_tile_s gmem_s(params, binfo, tidx);
-    Gmem_softmax_sum gmem_softmax_lse(params.softmax_lse_ptr, params, tidx);
-
-    // Wind gmem tiles to the correct position.
-    static_assert(Cta_tile_p::N % Cta_tile_p::M == 0);
-    int begin = Is_causal ? loop_step_idx * Cta_tile_p::N / Cta_tile_p::M : 0;
-    // We want begin to be a multiple of gridDim.z
-    // This is because the row indices processed by each threadblock must align between the
-    // loop steps, otherwise we have a dependency between the blocks.
-    // For example, threadblock with blockIdx.z == 1 must process row indices that are
-    // k * gridDim.z + 1 for integer k.
-    const int begin_mod_z = begin % gridDim.z;
-    begin = begin_mod_z <= blockIdx.z ? begin - begin_mod_z : begin + gridDim.z - begin_mod_z;
-    // Otherwise we'd be reading out-of-bound memory before the loop
-    if ((begin + blockIdx.z) * Cta_tile_p::M >= binfo.actual_seqlen_q) return;
-    const int steps_og = steps;
-    steps -= begin;
-    gmem_q.move(begin + blockIdx.z);
-    gmem_o.move(begin + blockIdx.z);
-    gmem_o_tmp.move(begin + blockIdx.z);
-    if (Return_softmax) {
-        gmem_s.move(begin + blockIdx.z);
-    }
-    gmem_softmax_lse.move(begin + blockIdx.z);
-    // if ((threadIdx.x == 0) && (blockIdx.x == 0) && (blockIdx.y == 0)) {
-    //     printf("begin = %d, steps = %d\n", begin, steps);
+    constexpr int kBlockM = Kernel_traits::kBlockM;
+    constexpr int kBlockN = Kernel_traits::kBlockN;
+    constexpr int kHeadDim = Kernel_traits::kHeadDim;
+    constexpr int kNWarps = Kernel_traits::kNWarps;
+    constexpr int MMA_M = kBlockM / decltype(size<0>(typename Kernel_traits::TiledMma::TiledShape_MNK{}))::value;
+
+    const BlockInfo</*Varlen=*/!Is_even_N> binfo(params, bidb);
+    if (m_block * kBlockM >= binfo.actual_seqlen_q || binfo.actual_seqlen_k == 0) return;
+
+    int n_block_max = cute::ceil_div(binfo.actual_seqlen_k, kBlockN);
+    if (Is_causal) {
+        n_block_max = std::min(n_block_max, cute::ceil_div((m_block + 1) * kBlockM, kBlockN));
+        // if (threadIdx.x == 0 && blockIdx.y == 0 && blockIdx.z == 0) {
+        //     printf("m_block = %d, n_block_max = %d\n", m_block, n_block_max);
+        // }
+    }
+
+    // We iterate over the blocks in reverse order. This is because the last block is the only one
+    // that needs masking when we read K and V from global memory. Moreover, iterating in reverse
+    // might save us 1 register (we just need n_block instead of both n_block and n_block_max).
+
+    const index_t row_offset_q = binfo.q_offset(params.q_batch_stride, params.q_row_stride, bidb)
+        + m_block * kBlockM * params.q_row_stride + bidh * params.q_head_stride;
+    // We move K and V to the last block.
+    const index_t row_offset_k = binfo.k_offset(params.k_batch_stride, params.k_row_stride, bidb)
+        + (n_block_max - 1) * kBlockN * params.k_row_stride + (bidh / params.h_h_k_ratio) * params.k_head_stride;
+    const index_t row_offset_v = binfo.k_offset(params.v_batch_stride, params.v_row_stride, bidb)
+        + (n_block_max - 1) * kBlockN * params.v_row_stride + (bidh / params.h_h_k_ratio) * params.v_head_stride;
+    const index_t row_offset_p = ((bidb * params.h + bidh) * params.seqlen_q_rounded
+        + m_block * kBlockM) * params.seqlen_k_rounded + (n_block_max - 1) * kBlockN;
+
+    Tensor gQ = make_tensor(make_gmem_ptr(reinterpret_cast<Element *>(params.q_ptr) + row_offset_q),
+                            Shape<Int<kBlockM>, Int<kHeadDim>>{},
+                            make_stride(params.q_row_stride, _1{}));
+    Tensor gK = make_tensor(make_gmem_ptr(reinterpret_cast<Element *>(params.k_ptr) + row_offset_k),
+                            Shape<Int<kBlockN>, Int<kHeadDim>>{},
+                            make_stride(params.k_row_stride, _1{}));
+    Tensor gV = make_tensor(make_gmem_ptr(reinterpret_cast<Element *>(params.v_ptr) + row_offset_v),
+                            Shape<Int<kBlockN>, Int<kHeadDim>>{},
+                            make_stride(params.v_row_stride, _1{}));
+    Tensor gP = make_tensor(make_gmem_ptr(reinterpret_cast<Element *>(params.p_ptr) + row_offset_p),
+                            Shape<Int<kBlockM>, Int<kBlockN>>{},
+                            make_stride(params.seqlen_k_rounded, _1{}));
+
+    Tensor sQ = make_tensor(make_smem_ptr(reinterpret_cast<Element *>(smem_)),
+                            typename Kernel_traits::SmemLayoutQ{});
+    // Careful we're using the same smem for sQ and sK | sV if Share_Q_K_smem;
+    Tensor sK = make_tensor(sQ.data() + (Kernel_traits::Share_Q_K_smem ? 0 : size(sQ)),
+                            typename Kernel_traits::SmemLayoutKV{});
+    Tensor sV = make_tensor(sK.data() + size(sK), typename Kernel_traits::SmemLayoutKV{});
+    Tensor sVt = make_tensor(sV.data(), typename Kernel_traits::SmemLayoutVtransposed{});
+    Tensor sVtNoSwizzle = make_tensor(sV.data(), typename Kernel_traits::SmemLayoutVtransposedNoSwizzle{});
+
+    auto gmem_thr_copy_QKV = typename Kernel_traits::GmemTiledCopyQKV{}.get_thread_slice(tidx);
+    auto gmem_thr_copy_P = typename Kernel_traits::GmemTiledCopyP{}.get_thread_slice(tidx);
+
+    Tensor tQgQ = gmem_thr_copy_QKV.partition_S(gQ);
+    Tensor tQsQ = gmem_thr_copy_QKV.partition_D(sQ);
+    Tensor tKgK = gmem_thr_copy_QKV.partition_S(gK);  // (KCPY, KCPY_N, KCPY_K)
+    Tensor tKsK = gmem_thr_copy_QKV.partition_D(sK);
+    Tensor tVgV = gmem_thr_copy_QKV.partition_S(gV);  // (VCPY, VCPY_N, VCPY_K)
+    Tensor tVsV = gmem_thr_copy_QKV.partition_D(sV);
+    Tensor tPgP = gmem_thr_copy_P.partition_D(gP);
+
+    typename Kernel_traits::TiledMma tiled_mma;
+    auto thr_mma = tiled_mma.get_thread_slice(tidx);
+    Tensor tSrQ  = thr_mma.partition_fragment_A(sQ);                           // (MMA,MMA_M,MMA_K)
+    Tensor tSrK  = thr_mma.partition_fragment_B(sK);                           // (MMA,MMA_N,MMA_K)
+    Tensor tOrVt  = thr_mma.partition_fragment_B(sVtNoSwizzle);                // (MMA, MMA_K,MMA_N)
+
+    Tensor acc_o = partition_fragment_C(tiled_mma, Shape<Int<kBlockM>, Int<kHeadDim>>{});  // MMA, MMA_M, MMA_K
+
+    //
+    // Copy Atom retiling
+    //
+
+    auto smem_thr_copy_Q = make_tiled_copy_A(typename Kernel_traits::SmemCopyAtom{}, tiled_mma).get_thread_slice(tidx);
+    // auto smem_thr_copy_Q = make_tiled_copy_A_warpcontiguousM<MMA_M>(typename Kernel_traits::SmemCopyAtom{}, tiled_mma).get_thread_slice(tidx);
+    // if (cute::thread0()) {smem_thr_copy_Q.print_all();}
+    Tensor tSsQ = smem_thr_copy_Q.partition_S(sQ);
+    // if (cute::thread0()) {print(tSsQ.layout()); printf("\n");}
+
+    auto smem_thr_copy_K = make_tiled_copy_B(typename Kernel_traits::SmemCopyAtom{}, tiled_mma).get_thread_slice(tidx);
+    Tensor tSsK = smem_thr_copy_K.partition_S(sK);
+
+    auto smem_thr_copy_V = make_tiled_copy_B(typename Kernel_traits::SmemCopyAtomTransposed{}, tiled_mma).get_thread_slice(tidx);
+    Tensor tOsVt = smem_thr_copy_V.partition_S(sVt);
+
+    // TODO: this might need to change if we change the mma instruction in SM70
+    Tensor scores_max = make_tensor<ElementAccum>(Shape<Int<2 * size<1>(acc_o)>>{});
+    Tensor scores_sum = make_fragment_like(scores_max);
+
+    //
+    // PREDICATES
+    //
+
+    // // Allocate predicate tensors for m and n
+    // Tensor tQpQ = make_tensor<bool>(make_shape(size<1>(tQsQ), size<2>(tQsQ)), Stride<_1,_0>{});
+    // Tensor tKVpKV = make_tensor<bool>(make_shape(size<1>(tKsK), size<2>(tKsK)), Stride<_1,_0>{});
+
+    // Construct identity layout for sQ and sK
+    Tensor cQ = make_identity_tensor(make_shape(size<0>(sQ), size<1>(sQ)));    // (BLK_M,BLK_K) -> (blk_m,blk_k)
+    Tensor cKV = make_identity_tensor(make_shape(size<0>(sK), size<1>(sK)));    // (BLK_N,BLK_K) -> (blk_n,blk_k)
+    // Tensor tScQ = thr_mma.partition_A(cQ);                           // (MMA,MMA_M,MMA_K)
+    // if (cute::thread0()) {
+    //     print(tScQ.layout()); printf("\n");
+    //     for (int i = 0; i < size(tScQ); ++i) {
+    //         printf("%d ", get<0>(tScQ(i)));
+    //     }
+    //     printf("\n");
+    //     for (int i = 0; i < size(tScQ); ++i) {
+    //         printf("%d ", get<1>(tScQ(i)));
+    //     }
+    //     printf("\n");
     // }
 
-    fmha::Mask<Cta_tile_p, Is_causal> mask(binfo, tidx, loop_step_idx);
+    // Repeat the partitioning with identity layouts
+    Tensor tQcQ = gmem_thr_copy_QKV.partition_S(cQ);       // (ACPY,ACPY_M,ACPY_K) -> (blk_m,blk_k)
+    Tensor tKVcKV = gmem_thr_copy_QKV.partition_S(cKV);   // (BCPY,BCPY_N,BCPY_K) -> (blk_n,blk_k)
+
+    // Allocate predicate tensors for k
+    Tensor tQpQ = make_tensor<bool>(make_shape(size<2>(tQsQ)));
+    Tensor tKVpKV = make_tensor<bool>(make_shape(size<2>(tKsK)));
 
-    // Allocate the global memory tile loader for K.
-    Gmem_tile_k gmem_k(params.k_ptr, params.k_row_stride_in_elts, params.k_head_stride_in_elts,
-                       params.d, binfo, tidx, false);
-    // Allocate the global memory tile loader for V.
-    Gmem_tile_v gmem_v(params.v_ptr, params.v_row_stride_in_elts, params.v_head_stride_in_elts,
-                       params.d, binfo, tidx, false);
-    // The base pointer of smem_v;
-    char *smem_v_ = &smem_[Gemm1::SMEM_OFFSET_V];
-    
-    // Allocate the shared memory tile loader for V. We use the same as K so be careful!!!
-    Smem_tile_v smem_v(smem_v_, tidx);
-
-    // Allocate the shared memory tile loader for O. We use the same as K so be careful!!!
-    Smem_tile_o smem_o(&smem_[Gemm1::SMEM_OFFSET_O], tidx);
-
-    if (!Is_first) {
-        gmem_k.move(loop_step_idx);
-        gmem_v.move(loop_step_idx);
-        if (Return_softmax) { gmem_s.move(loop_step_idx * steps_og); }
-    }
-
-    // Trigger the loads for K.
-    gmem_k.load();
-    // Trigger the loads for Q.
-    gmem_q.load();
-    // Trigger the loads for V.
-    gmem_v.load();
-
-    if (!Is_first) { __syncthreads(); }
-
-    float p_prev_lse[Mma_tile_p::MMAS_M * 2];
-    if (!Is_first) {
-        gmem_softmax_lse.load(reinterpret_cast<uint32_t(&)[Mma_tile_p::MMAS_M * 2]>(p_prev_lse));
-    }
-
-    // Commit the data for Q and V to shared memory.
-    gmem_q.commit(gemm_q_k.smem_q);
-    gmem_v.commit(smem_v);
-
-    // const uint32_t scale_bmm1 = reinterpret_cast<const uint32_t&>(params.scale_bmm1);
-    // #pragma unroll
-    // for(int it=0;it < Gmem_tile_k::LDGS;it++){
-    //     gmem_k.fetch_[it] = fmha::hmul8(scale_bmm1, gmem_k.fetch_[it]);
-    // }
-
-    // Commit the data for K to shared memory.
-    if( !Kernel_traits::SHARE_SMEM_FOR_K_AND_V ) {
-        gmem_k.commit(gemm_q_k.smem_k);
+    // Set predicates for k bounds
+    if (!Is_even_K) {
+        #pragma unroll
+        for (int k = 0; k < size(tQpQ); ++k) { tQpQ(k) = get<1>(tQcQ(0, 0, k)) < params.d; }
+        #pragma unroll
+        for (int k = 0; k < size(tKVpKV); ++k) { tKVpKV(k) = get<1>(tKVcKV(0, 0, k)) < params.d; }
     }
 
-    __syncthreads();
-
-    // Load the fragments for Q.
-    gemm_q_k.load_q();
+    // Prologue
 
-    // Load the fragments for V. We keep the data in registers during the entire kernel.
-    typename Smem_tile_v::Fragment frag_v[Mma_tile_o::MMAS_K][Mma_tile_o::MMAS_N];
-    #pragma unroll
-    for( int ki = 0; ki < Mma_tile_o::MMAS_K; ++ki ) {
-        smem_v.load(frag_v[ki], ki);
-    }
+    Tensor tQrQ = make_fragment_like(tQgQ);
+    // We don't need to clear the sQ smem tiles since we'll only write out the valid outputs
+    flash::copy</*Is_even_MN=*/false, Is_even_K>(gmem_thr_copy_QKV, tQgQ, tQsQ, tQcQ, tQpQ,
+                                                 binfo.actual_seqlen_q - m_block * kBlockM);
+    if (Kernel_traits::Is_Q_in_regs) { cute::cp_async_fence(); }
+
+    // // Copy rmem to smem
+    // // copy(tQrQ, tQsQ);
+    // flash::cp_async_wait<0>();
+    // __syncthreads();
+    // // if (cute::thread(1, 0)) { print(tQsQ); }
+    // // Tensor sQNoSwizzle = make_tensor(make_smem_ptr(reinterpret_cast<Element *>(smem_)), typename Kernel_traits::SmemLayoutQNoSwizzle{});
+    // // if (cute::thread0()) { print(sQNoSwizzle); }
 
-    // Commit the data for V to shared memory if it has not been done already.
-    if( Kernel_traits::SHARE_SMEM_FOR_K_AND_V ) {
-        // Make sure we are done loading the fragments for K.
+    if (Kernel_traits::Share_Q_K_smem) {
+        flash::cp_async_wait<0>();
         __syncthreads();
-
-        // Commit the data to shared memory for V.
-        gmem_k.commit(gemm_q_k.smem_k);
-
-        // Make sure the data is in shared memory.
+        Tensor tSrQ_copy_view = smem_thr_copy_Q.retile_D(tSrQ);
+        CUTE_STATIC_ASSERT_V(size<1>(tSsQ) == size<1>(tSrQ_copy_view));            // M
+        copy(smem_thr_copy_Q, tSsQ, tSrQ_copy_view);
         __syncthreads();
     }
 
-    // Load the fragments for K. 
-    gemm_q_k.load_k();
-
-    // Create the object to do the softmax.
-    Softmax softmax(params, &smem_[Gemm1::SMEM_OFFSET_SOFTMAX], tidx);
-
-    Smem_softmax_sum smem_softmax_lse(reinterpret_cast<float *>(&smem_[Gemm1::SMEM_BYTES]), tidx);
+    int n_block = n_block_max - 1;
+    // We don't need to clear the sK smem tiles since we'll mask out the scores anyway.
+    flash::copy<Is_even_N, Is_even_K>(gmem_thr_copy_QKV, tKgK, tKsK, tKVcKV, tKVpKV,
+                                      binfo.actual_seqlen_k - n_block * kBlockN);
+    cute::cp_async_fence();
+    // if (threadIdx.x == 0 && blockIdx.y == 0 && blockIdx.z < 2) { print(tKgK); }
+    // __syncthreads();
 
-    // Load over the entire sequence length.
-    for (int l = blockIdx.z; l < steps; l += step_stride) {
-        // if ((threadIdx.x == 0) && (blockIdx.x == 0) && (blockIdx.y == 0) && (blockIdx.z <= 1)) {
-        //     printf("l = %d\n", l);
-        // }
-        if ((begin + l) * Cta_tile_p::M >= binfo.actual_seqlen_q) break;
+    if (Kernel_traits::Is_Q_in_regs && !Kernel_traits::Share_Q_K_smem) {
+        flash::cp_async_wait<1>();
+        __syncthreads();
+        Tensor tSrQ_copy_view = smem_thr_copy_Q.retile_D(tSrQ);
+        CUTE_STATIC_ASSERT_V(size<1>(tSsQ) == size<1>(tSrQ_copy_view));            // M
+        copy(smem_thr_copy_Q, tSsQ, tSrQ_copy_view);
+    }
 
-        // Declare the accumulators for the 1st gemm.
-        fmha::Fragment_accumulator acc_p[Mma_tile_p::MMAS_M][Mma_tile_p::MMAS_N];
-        fmha::Clear_accumulator<typename fmha::Accumulator_type, Cta_tile_p::WARPS_K>::apply(acc_p);
+    auto seeds = at::cuda::philox::unpack(params.philox_args);
+    unsigned long long seed = std::get<0>(seeds);
+    unsigned long long offset = std::get<1>(seeds) + (bidb * params.h + bidh) * 32 + tidx % 32;
 
-        // Do this part of P = Q * K^T.
-        gemm_q_k(acc_p);
+    clear(acc_o);
 
-        // if ((threadIdx.x == 0) && (blockIdx.x == 0) && (blockIdx.y == 0) && (l == 0))  {
-        //     printf("acc_p=%.6f, %.6f\n", acc_p[0][0].elt(0), acc_p[0][0].elt(1));
-        // }
+    // For performance reason, we separate out two kinds of iterations:
+    // those that need masking on S, and those that don't.
+    // We need masking on S for the very last block when K and V has length not multiple of kBlockN.
+    // We also need masking on S if it's causal, for the last ceil_div(kBlockM, kBlockN) blocks.
+    // We will have at least 1 "masking" iteration.
 
-        uint4 out[Gmem_tile_o::STGS_PER_LOOP];
-        if (!Is_first) { gmem_o_tmp.load(out, 0); }
+    constexpr int n_masking_steps = Is_causal ? cute::ceil_div(kBlockM, kBlockN) : 1;
+    #pragma unroll
+    for (int masking_step = 0; masking_step < n_masking_steps; ++masking_step, --n_block) {
+        Tensor acc_s = partition_fragment_C(tiled_mma, Shape<Int<kBlockM>, Int<kBlockN>>{});  // (MMA=4, MMA_M, MMA_N)
+        clear(acc_s);
+        flash::cp_async_wait<0>();
+        __syncthreads();
 
-        // Trigger the load for the next Q values.
-        if (l + step_stride < steps) {
-            gemm_q_k.smem_q.move_to_next_write_buffer();
-            gmem_q.move(step_stride);
-            gmem_q.load();
+        // Advance gV
+        if (masking_step > 0) {
+            tVgV.data() = tVgV.data() + (-int(kBlockN * params.v_row_stride));
+            flash::copy</*Is_even_MN=*/true, Is_even_K>(gmem_thr_copy_QKV, tVgV, tVsV, tKVcKV, tKVpKV);
+        } else {
+            // Clear the smem tiles to account for predicated off loads
+            flash::copy<Is_even_N, Is_even_K, /*Clear_OOB_MN=*/true>(
+                gmem_thr_copy_QKV, tVgV, tVsV, tKVcKV, tKVpKV, binfo.actual_seqlen_k - n_block * kBlockN
+            );
+        }
+        cute::cp_async_fence();
+
+        flash::gemm</*A_in_regs=*/Kernel_traits::Is_Q_in_regs>(
+            acc_s, tSrQ, tSrK, tSsQ, tSsK, tiled_mma, smem_thr_copy_Q, smem_thr_copy_K
+        );
+        // if (cute::thread0()) { print(acc_s); }
+
+        // Reshape acc_s from (MMA=4, MMA_M, MMA_N) to (nrow=(2, MMA_M), ncol=(2, MMA_N))
+        Tensor scores = make_tensor(acc_s.data(), flash::convert_layout_acc_rowcol(acc_s.layout()));
+        // if (cute::thread0()) { print(scores); }
+        // We don't put the masking before the matmul S = Q K^T because we don't clear sK
+        // for rows outside actual_seqlen_k. So those rows could have Inf / NaN, and the matmul
+        // can produce Inf / NaN.
+        if (!Is_causal) {
+            if (!Is_even_N) { flash::apply_mask(scores, binfo.actual_seqlen_k - n_block * kBlockN); }
+        } else {
+            // Tensor caccS = make_identity_tensor(Shape<Int<kBlockM>, Int<kBlockN>>{});    // (BLK_M,BLK_N) -> (blk_m,blk_n)
+            // Tensor taccScS = thr_mma.partition_C(caccS);                           // (MMA,MMA_M,MMA_N)
+            // static_assert(decltype(size<0>(taccScS))::value == 4);
+            // // Convert to ((2, 2), MMA_M, MMA_N) then take only the row indices.
+            // Tensor idx_row = logical_divide(taccScS, Shape<_2>{})(make_coord(0, _), _, 0);
+            // Tensor idx_rowcol = make_tensor(taccScS.data(), flash::convert_layout_acc_rowcol(taccScS.layout()));
+            // flash::apply_mask_causal_w_idx(scores, idx_rowcol, n_block * kBlockN, binfo.actual_seqlen_k,
+            //                               m_block * kBlockM);
+            // Idk why it's get<1> and not get<0> of the stride.
+            // if (cute::thread0()) { print(idx_row.layout()); print(stride<1>(idx_row)); printf("stride = %d \n", get<1>(stride<1>(idx_row))); }
+            // I can't get the stride from idx_row
+            flash::apply_mask_causal(scores, n_block * kBlockN, binfo.actual_seqlen_k,
+                                     // m_block * kBlockM + get<0>(idx_row(0)),
+                                     m_block * kBlockM + (tidx / 32) * 16 + (tidx % 32) / 4,
+                                     kNWarps * 16);
+                                     // m_block * kBlockM + (tidx / 32) * 16, kNWarps * 16);
+                                     // m_block * kBlockM + (tidx / 32) * (kBlockM / kNWarps), 16);
         }
 
-        // Load the mask for that iteration.
-        mask.load(begin + l);
-
-        // Convert from the accumulator type to FP32 for Softmax.
-        softmax.unpack_noscale(acc_p);
-
-        // Apply the mask.
-        softmax.apply_mask(mask);
-
-        if( Kernel_traits::SHARE_SMEM_FOR_K_AND_V && l < step_stride ) {
-            // if we share K and V, it could be that V was not fully read yet but we write into smem for reduction
-            __syncthreads();
+        flash::cp_async_wait<0>();
+        __syncthreads();
+        if (n_block > 0) {
+            // Advance gK
+            tKgK.data() = tKgK.data() + (-int(kBlockN * params.k_row_stride));
+            flash::copy</*Is_even_MN=*/true, Is_even_K>(gmem_thr_copy_QKV, tKgK, tKsK, tKVcKV, tKVpKV);
+            // This cp_async_fence needs to be in the if block, otherwise the synchronization
+            // isn't right and we get race conditions.
+            cute::cp_async_fence();
+        }
+
+        // TODO: when we have key_padding_mask we'll need to Check_inf
+        masking_step == 0
+            ? softmax_rescale_o</*Is_first=*/true,  /*Check_inf=*/Is_causal>(scores, scores_max, scores_sum, acc_o, params.scale_softmax_log2)
+            : softmax_rescale_o</*Is_first=*/false, /*Check_inf=*/Is_causal>(scores, scores_max, scores_sum, acc_o, params.scale_softmax_log2);
+
+        // Convert scores from fp32 to fp16/bf16
+        Tensor rP = flash::convert_type<Element>(scores);
+        // Reshape rP from (nrow=(2, MMA_M), ncol=(2, MMA_N)) to ((2, 2, 2), MMA_M, MMA_N / 2)
+        // if using m16n8k16 or ((2, 2, 1), MMA_M, MMA_N) if using m16n8k8.
+        Tensor tOrP = make_tensor(rP.data(), flash::convert_layout_rowcol_Aregs<Kernel_traits::TiledMma>(rP.layout()));
+        uint32_t block_row_idx = m_block * (kBlockM / 16) + tidx / 32;
+        uint32_t block_col_idx = n_block * (kBlockN / 32);
+        if (Return_softmax) {
+            Tensor tOrP_copy = make_fragment_like(tOrP);
+            copy(tOrP, tOrP_copy);
+            flash::apply_dropout</*encode_dropout_in_sign_bit=*/true>(
+                tOrP_copy, params.p_dropout_in_uint8_t, seed, offset,
+                block_row_idx, block_col_idx, kNWarps
+            );
+            flash::write_softmax_to_gmem(tOrP_copy, tPgP, gmem_thr_copy_P);
+            tPgP.data() = tPgP.data() + (-kBlockN);
         }
-        // if (!Is_first) {
-        //     if ((threadIdx.x == 0) && (blockIdx.x == 0) && (blockIdx.y == 0) && (l >= 0))  {
-        //         printf("p_prev_lse=%.6f, %.6f\n", p_prev_lse[0], p_prev_lse[1]);
-        //     }
-        // }
-        // Compute the max.
-        float p_max[Mma_tile_p::MMAS_M * 2];
-        if (!Is_first) {
-            smem_softmax_lse.store_pair(p_prev_lse);
-            // for (int mi = 0; mi < Mma_tile_p::MMAS_M * 2; mi++) { p_max[mi] = p_prev_lse[mi]; }
-            for (int mi = 0; mi < Mma_tile_p::MMAS_M * 2; mi++) { p_max[mi] = p_prev_lse[mi] / params.scale_bmm1f; }
-        }
-
-        // Trigger the load for the next LSE values.
-        if (l + step_stride < steps) {
-            if (!Is_first) {
-                gmem_softmax_lse.load_next(reinterpret_cast<uint32_t(&)[Mma_tile_p::MMAS_M * 2]>(p_prev_lse),
-                                           step_stride);
-            }
+        if (Is_dropout) {
+            flash::apply_dropout(tOrP, params.p_dropout_in_uint8_t, seed, offset,
+                                 block_row_idx, block_col_idx, kNWarps);
         }
+        // if (cute::thread0()) { print(tOrP); }
 
-        softmax.template reduce_max</*zero_init=*/Is_first>(p_max);
-
-        // if ((threadIdx.x == 0) && (l == 38)) {
-        //     printf("loop_step_idx %d, p_max = %.6f, %.6f., p_prev_lse = %.6f, %.6f\n", loop_step_idx, p_max[0], p_max[1], Is_first ? -10000.f : p_prev_lse[0], Is_first ? -10000.f : p_prev_lse[1]);
-        // }
+        flash::gemm_A_in_regs(acc_o, tOrP, tOrVt, tOsVt, tiled_mma, smem_thr_copy_V);
+        // if (cute::thread0()) { print(scores); }
 
-        // if (!Is_first) {
-        //     if ((threadIdx.x == 0) && (blockIdx.x == 0) && (blockIdx.y == 0) && (l == 0))  {
-        //         printf("after reduce_max=%.6f, %.6f\n", softmax.elt_[0][0], softmax.elt_[0][1]);
-        //     }
-        // }
-
-        // Compute the exponential value.
-        // softmax.apply_exp(p_max);
-        softmax.scale_apply_exp(p_max, params.scale_bmm1f);
-
-        // if (!Is_first) {
-        //     if ((threadIdx.x == 0) && (blockIdx.x == 0) && (blockIdx.y == 0) && (l == 0))  {
-        //         printf("after apply_exp=%.6f, %.6f\n", softmax.elt_[0][0], softmax.elt_[0][1]);
-        //     }
-        // }
-
-        // Compute the sum.
-        float p_sum[Mma_tile_p::MMAS_M * 2];
-        // if (!Is_first) {
-        //     int warp = tidx / Cta_tile_p::THREADS_PER_WARP;
-        //     int lane = tidx % Cta_tile_p::THREADS_PER_WARP;
-        //     for (int mi = 0; mi < Mma_tile_p::MMAS_M * 2; mi++) {
-        //         p_sum[mi] = ((warp == 0) && (lane % 4 == 0)) ? expf(p_prev_lse[mi] - p_max[mi]) : 0;
-        //     }
-        // }
-        // softmax.reduce_sum(p_sum);
-        softmax.reduce_sum_before_sync_(p_sum);
-        // softmax.template reduce_sum_before_sync_</*zero_init=*/Is_first>(p_sum);
-
-        // float p_sum_log[Mma_tile_p::MMAS_M * 2];
-        // for (int mi = 0; mi  < Mma_tile_p::MMAS_M * 2; ++mi) {
-        //     float sum = p_sum[mi];
-        //     // p_sum_log[mi] = (sum == 0.f || sum != sum) ? INFINITY : p_max[mi] + __logf(sum);
-        //     constexpr float kLog2e = M_LOG2E;
-        //     p_sum_log[mi] = (sum == 0.f || sum != sum) ? INFINITY : p_max[mi] * kLog2e + __log2f(sum);
-        // }
-        // // gmem_softmax_lse.store(reinterpret_cast<uint32_t(&)[Mma_tile_p::MMAS_M * 2]>(p_sum));
-        // gmem_softmax_lse.store(reinterpret_cast<uint32_t(&)[Mma_tile_p::MMAS_M * 2]>(p_sum_log));
-        // gmem_softmax_lse.move();
+        // This check is at the end of the loop since we always have at least 1 iteration
+        if (n_masking_steps > 1 && n_block <= 0) {
+            --n_block;
+            break;
+        }
+    }
 
-        // // Finalize softmax on the accumulators of P^T.
-        // softmax.scale(p_sum);
+    // These are the iterations where we don't need masking on S
+    for (; n_block >= 0; --n_block) {
+        Tensor acc_s = partition_fragment_C(tiled_mma, Shape<Int<kBlockM>, Int<kBlockN>>{});  // (MMA=4, MMA_M, MMA_N)
+        clear(acc_s);
+        flash::cp_async_wait<0>();
+        __syncthreads();
+        // Advance gV
+        tVgV.data() = tVgV.data() + (-int(kBlockN * params.v_row_stride));
+        flash::copy</*Is_even_MN=*/true, Is_even_K>(gmem_thr_copy_QKV, tVgV, tVsV, tKVcKV, tKVpKV);
+        cute::cp_async_fence();
+
+        flash::gemm</*A_in_regs=*/Kernel_traits::Is_Q_in_regs>(
+            acc_s, tSrQ, tSrK, tSsQ, tSsK, tiled_mma, smem_thr_copy_Q, smem_thr_copy_K
+        );
 
-        constexpr bool encode_dropout_in_sign_bit = Return_softmax;
-        if (Is_dropout) {
-            // softmax.template apply_dropout<encode_dropout_in_sign_bit>(ph, params.p_dropout_in_uint);
-            // softmax.template apply_dropout<encode_dropout_in_sign_bit>(ph, ph1, params.p_dropout_in_uint);
-            // softmax.template apply_dropout_16bits<encode_dropout_in_sign_bit>(ph, ph1, params.p_dropout_in_uint16_t);
-            unsigned int warp_idx = threadIdx.x / 32;
-            // TODO: this should change after we rearrange the warps (e.g. cutlass branch)
-            unsigned int block_col_idx = loop_step_idx * Cta_tile_p::N / 16 + warp_idx;
-            // We want to use actual_seqlen_k, not seqlen_k, since seqlen_k could be rounded
-            // differently in the fwd and bwd pass. E.g., for d=128 on A100, fwd rounds seqlen_k
-            // to multiples of 256 while bwd rounds seqlen_k to multiples of 128.
-            unsigned long long philox_subsequence = (begin + l) * (binfo.actual_seqlen_k / 16) + block_col_idx;
-            softmax.template apply_dropout_16bits<encode_dropout_in_sign_bit>(ph, params.p_dropout_in_uint16_t, philox_subsequence);
-        }
-
-        using Frag_p = fmha::Fragment_a<fmha::Row>;
-        Frag_p frag_p[Mma_tile_o::MMAS_K][Mma_tile_o::MMAS_M];
-        static_assert(Mma_tile_o::MMAS_M == Mma_tile_p::MMAS_M);
-        static_assert(Mma_tile_o::MMAS_K == Mma_tile_p::MMAS_N);
-        softmax.template pack<elem_type>(frag_p);
+        flash::cp_async_wait<0>();
+        __syncthreads();
+        if (n_block > 0) {
+            // Advance gK
+            tKgK.data() = tKgK.data() + (-int(kBlockN * params.k_row_stride));
+            flash::copy</*Is_even_MN=*/true, Is_even_K>(gmem_thr_copy_QKV, tKgK, tKsK, tKVcKV, tKVpKV);
+            // This cp_async_fence needs to be in the if block, otherwise the synchronization
+            // isn't right and we get race conditions.
+            cute::cp_async_fence();
+        }
+
+        // Reshape acc_s from (MMA=4, MMA_M, MMA_N) to (nrow=(2, MMA_M), ncol=(2, MMA_N))
+        Tensor scores = make_tensor(acc_s.data(), flash::convert_layout_acc_rowcol(acc_s.layout()));
+        softmax_rescale_o</*Is_first=*/false>(scores, scores_max, scores_sum, acc_o, params.scale_softmax_log2);
+
+        Tensor rP = flash::convert_type<Element>(scores);
+        // Reshape rP from (nrow=(2, MMA_M), ncol=(2, MMA_N)) to ((2, 2, 2), MMA_M, MMA_N / 2)
+        // if using m16n8k16 or ((2, 2, 1), MMA_M, MMA_N) if using m16n8k8.
+        Tensor tOrP = make_tensor(rP.data(), flash::convert_layout_rowcol_Aregs<Kernel_traits::TiledMma>(rP.layout()));
+        uint32_t block_row_idx = m_block * (kBlockM / 16) + tidx / 32;
+        uint32_t block_col_idx = n_block * (kBlockN / 32);
         if (Return_softmax) {
-            gmem_s.store(frag_p, mask);
-            gmem_s.move(step_stride);
+            Tensor tOrP_copy = make_fragment_like(tOrP);
+            copy(tOrP, tOrP_copy);
+            flash::apply_dropout</*encode_dropout_in_sign_bit=*/true>(
+                tOrP_copy, params.p_dropout_in_uint8_t, seed, offset,
+                block_row_idx, block_col_idx, kNWarps
+            );
+            flash::write_softmax_to_gmem(tOrP_copy, tPgP, gmem_thr_copy_P);
+            tPgP.data() = tPgP.data() + (-kBlockN);
         }
-
-        // Commit the values for Q into shared memory.
-        if (l + step_stride < steps) {
-            gmem_q.commit(gemm_q_k.smem_q);
+        if (Is_dropout) {
+            flash::apply_dropout(tOrP, params.p_dropout_in_uint8_t, seed, offset,
+                                 block_row_idx, block_col_idx, kNWarps);
         }
 
-        if (Is_dropout && encode_dropout_in_sign_bit) {
-            #pragma unroll
-            for( int ki = 0; ki < Mma_tile_o::MMAS_K; ki++ ) {
-                #pragma unroll
-                for( int mi = 0; mi < Mma_tile_o::MMAS_M; mi++ ) {
-                    frag_p[ki][mi].template hrelu_<elem_type>();
-                }
-            }
-        }
+        flash::gemm_A_in_regs(acc_o, tOrP, tOrVt, tOsVt, tiled_mma, smem_thr_copy_V);
+    }
 
-        // Declare the accumulators for the 2nd gemm.
-        fmha::Fragment_accumulator acc_o[Mma_tile_o::MMAS_M][Mma_tile_o::MMAS_N];
-        fmha::Clear_accumulator<typename fmha::Accumulator_type, Cta_tile_o::WARPS_K>::apply(acc_o);
+    // Epilogue
 
-        // Do this part of O = P^T * V^T.
+    // Reshape acc_o from (MMA=4, MMA_M, MMA_K) to (nrow=(2, MMA_M), ncol=(2, MMA_K))
+    Tensor acc_o_rowcol = make_tensor(acc_o.data(), flash::convert_layout_acc_rowcol(acc_o.layout()));
+    Tensor lse = make_fragment_like(scores_sum);
+    #pragma unroll
+    for (int mi = 0; mi < size<0>(acc_o_rowcol); ++mi) {
+        float sum = scores_sum(mi);
+        float inv_sum = (sum == 0.f || sum != sum) ? 1.f : 1.f / sum;
+        lse(mi) = (sum == 0.f || sum != sum) ? INFINITY : scores_max(mi) * params.scale_softmax + __logf(sum);
+        float scale = !Is_dropout ? inv_sum : inv_sum * params.rp_dropout;
         #pragma unroll
-        for( int ki = 0; ki < Mma_tile_o::MMAS_K; ++ki ) {
-            fmha::gemm_cl<elem_type>(acc_o, frag_p[ki], frag_v[ki]);
-            // if ((threadIdx.x == 4) && (blockIdx.x == 0) && (blockIdx.y == 0) && (l == 0))  {
-            //     float2 tmp_p = __half22float2(reinterpret_cast<__half2 &>(frag_p[ki]));
-            //     float2 tmp_v = __half22float2(reinterpret_cast<__half2 &>(frag_v[ki]));
-            //     printf("Per warp, threadIdx.x = %d, frag_p = %.6f, %.6f, frag_v = %.6f, %.6f, acc_o=%.6f\n", threadIdx.x, tmp_p.x, tmp_p.y, tmp_v.x, tmp_v.y, acc_o[0][0].elt(0));
-            // }
-        }
-
-        // if ((threadIdx.x % 32 == 16) && (blockIdx.x == 0) && (blockIdx.y == 0) && (l == 0))  {
-        //     printf("Per warp, threadIdx.x = %d, acc_o=%.6f\n", threadIdx.x, acc_o[0][2].elt(0));
-        // }
-
-        // The mapping from tidx to rows changes between the softmax and the
-        // O-reduction. So we recalculate the max.
-        float p_max_o[Gmem_tile_o::STGS_PER_LOOP][Mma_tile_o::MMAS_M];
-        int rows[Gmem_tile_o::STGS_PER_LOOP];
-        for (int jj = 0; jj < Gmem_tile_o::STGS_PER_LOOP; jj++) {
-            rows[jj] = tidx / Gmem_tile_o::THREADS_PER_ROW + jj * Gmem_tile_o::ROWS_PER_STG;
-        }
-        softmax.reduce_max_after_sync_(p_max_o, rows);
-        static_assert(Mma_tile_o::MMAS_M == 1);
-        for (int jj = 0; jj < Gmem_tile_o::STGS_PER_LOOP; jj++) {
-            p_max_o[jj][0] *= params.scale_bmm1f;
-        }
-        float p_prev_scale_o[Gmem_tile_o::STGS_PER_LOOP];
-        if (!Is_first) {
-            smem_softmax_lse.load(p_prev_scale_o, rows);
-        }
-        // if (!Is_first) {
-        //     if ((threadIdx.x == 0) && (blockIdx.x == 0) && (blockIdx.y == 0) && (l == 0))  {
-        //         printf("p_prev_scale_o=%.6f\n", p_prev_scale_o[0]);
-        //     }
-        // }
+        for (int ni = 0; ni < size<1>(acc_o_rowcol); ++ni) { acc_o_rowcol(mi, ni) *= scale; }
+    }
 
-        static_assert(Gmem_tile_o::LOOPS == 1);
+    // if (cute::thread0()) { print(acc_o_rowcol); }
 
-        // Swizzle the elements and do the final reduction.
-        smem_o.store(acc_o, 0);
+    // Convert acc_o from fp32 to fp16/bf16
+    Tensor rO = flash::convert_type<Element>(acc_o);
+    Tensor sO = make_tensor(sQ.data(), typename Kernel_traits::SmemLayoutO{});    // (SMEM_M,SMEM_N)
+    // Partition sO to match the accumulator partitioning
+    auto smem_thr_copy_O = make_tiled_copy_C(typename Kernel_traits::SmemCopyAtomO{}, tiled_mma).get_thread_slice(tidx);
+    // auto smem_thr_copy_O = make_tiled_copy_C_warpcontiguousM<MMA_M>(typename Kernel_traits::SmemCopyAtomO{}, tiled_mma).get_thread_slice(tidx);
+    Tensor taccOrO = smem_thr_copy_O.retile_S(rO);        // ((Atom,AtomNum), MMA_M, MMA_N)
+    Tensor taccOsO = smem_thr_copy_O.partition_D(sO);     // ((Atom,AtomNum),PIPE_M,PIPE_N)
+
+    // sO has the same size as sQ, so we don't need to sync here.
+    if (Kernel_traits::Share_Q_K_smem) { __syncthreads(); }
+
+    copy(smem_thr_copy_O, taccOrO, taccOsO);
+
+    const index_t row_offset_o = binfo.q_offset(params.o_batch_stride, params.o_row_stride, bidb)
+        + m_block * kBlockM * params.o_row_stride + bidh * params.o_head_stride;
+    const index_t row_offset_lse = (bidb * params.h + bidh) * params.seqlen_q + m_block * kBlockM;
+    Tensor gO = make_tensor(make_gmem_ptr(reinterpret_cast<Element *>(params.o_ptr) + row_offset_o),
+                            Shape<Int<kBlockM>, Int<kHeadDim>>{},
+                            make_stride(params.o_row_stride, _1{}));
+    Tensor gLSE = make_tensor(make_gmem_ptr(reinterpret_cast<ElementAccum *>(params.softmax_lse_ptr) + row_offset_lse),
+                              Shape<Int<kBlockM>>{}, Stride<_1>{});
+
+    auto gmem_thr_copy_O = typename Kernel_traits::GmemTiledCopyO{}.get_thread_slice(tidx);
+    Tensor tOsO = gmem_thr_copy_O.partition_S(sO);        // ((Atom,AtomNum),ATOM_M,ATOM_N)
+    Tensor tOgO = gmem_thr_copy_O.partition_D(gO);
 
-        // Make sure the data is in shared memory.
-        __syncthreads();
+    __syncthreads();
 
-        static_assert(Mma_tile_o::MMAS_M == 1);
-        float p_sum_o[Gmem_tile_o::STGS_PER_LOOP][Mma_tile_o::MMAS_M];
-        softmax.reduce_sum_after_sync_(p_sum_o, rows);
-        if (!Is_first) {
-            for (int jj = 0; jj < Gmem_tile_o::STGS_PER_LOOP; jj++) {
-                p_prev_scale_o[jj] = expf(p_prev_scale_o[jj] - p_max_o[jj][0]);
-                p_sum_o[jj][0] += p_prev_scale_o[jj];
-            }
-        }
+    Tensor tOrO = make_tensor<Element>(shape(tOgO));
+    copy(gmem_thr_copy_O, tOsO, tOrO);
 
-        float p_sum_log[Gmem_tile_o::STGS_PER_LOOP][Mma_tile_o::MMAS_M];
+    Tensor caccO = make_identity_tensor(Shape<Int<kBlockM>, Int<kHeadDim>>{});    // (BLK_M,BLK_K) -> (blk_m,blk_k)
+    Tensor taccOcO = thr_mma.partition_C(caccO);                           // (MMA,MMA_M,MMA_K)
+    static_assert(decltype(size<0>(taccOcO))::value == 4);
+    // Convert to ((2, 2), MMA_M, MMA_K) then take only the row indices.
+    Tensor taccOcO_row = logical_divide(taccOcO, Shape<_2>{})(make_coord(0, _), _, 0);
+    CUTE_STATIC_ASSERT_V(size(lse) == size(taccOcO_row));                     // MMA_M
+    if (get<1>(taccOcO_row(0)) == 0) {
         #pragma unroll
-        for (int jj = 0; jj < Gmem_tile_o::STGS_PER_LOOP; jj++) {
-            float sum = p_sum_o[jj][0];
-            p_sum_log[jj][0] = (sum == 0.f || sum != sum) ? -INFINITY : p_max_o[jj][0] + __logf(sum);
-            // if (sum == 0.f || sum != sum) {
-            //     printf("loop_step_idx = %d, l = %d, tidx = %d, sum = %.6f, p_max_o = %.6f\n", loop_step_idx, l, tidx, sum, p_max_o[jj][0]);
-            // }
-            // if (Is_first) {
-            //     if ((threadIdx.x == 0) && (blockIdx.x == 0) && (blockIdx.y == 0) && (l == 0))  {
-            //         printf("p_sum_log=%.6f\n", p_sum_log[jj][0]);
-            //     }
-            // }
-            if (tidx % Gmem_tile_o::THREADS_PER_ROW == 0) {
-                gmem_softmax_lse.store_row(
-                    reinterpret_cast<uint32_t(&)[Mma_tile_p::MMAS_M]>(p_sum_log[jj]), rows[jj]);
-            }
-        }
-        gmem_softmax_lse.move(step_stride);
-
-        // Load from shared memory.
-        if (!Is_first) {
-            for (int jj = 0; jj < Gmem_tile_o::STGS_PER_LOOP; jj++) {
-                out[jj] = fmha::fmul4(out[jj], p_prev_scale_o[jj]);
-            }
-        }
-        smem_o.template load</*zero_init=*/Is_first>(out);
-
-        const bool is_final_write =
-            Is_last
-            || ((loop_step_idx + 1) * Cta_tile_p::N >= binfo.actual_seqlen_k)
-            || ((Is_causal) && ((begin + l) * Cta_tile_p::M < (loop_step_idx + 1) * Cta_tile_p::N));
-        #pragma unroll
-        for (int jj = 0; jj < Gmem_tile_o::STGS_PER_LOOP; jj++) {
-            float sum = p_sum_o[jj][0];
-            float inv_sum = (sum == 0.f || sum != sum) ? 1.f : 1.f / sum;
-            if (Is_dropout && is_final_write) {
-                inv_sum *= params.rp_dropout;
-            }
-            out[jj] = fmha::fmul4(out[jj], inv_sum);
-        }
-
-        // if (Is_dropout && Is_last) {
-        //     for (int jj = 0; jj < Gmem_tile_o::STGS_PER_LOOP; jj++) {
-        //         out[jj] = fmha::fmul4(out[jj], params.rp_dropout);
-        //     }
-        // }
-
-        // Output the values.
-        if (is_final_write) {
-            gmem_o.template store<elem_type>(out, 0);
-            gmem_o.move(step_stride);
-        } else {
-            gmem_o_tmp.store(out, 0);
+        for (int mi = 0; mi < size(lse); ++mi) {
+            const int row = get<0>(taccOcO_row(mi));
+            if (row < binfo.actual_seqlen_q - m_block * kBlockM) { gLSE(row) = lse(mi); }
         }
+    }
 
-        // Move to the next part of the output.
-        if (!(Is_first && Is_last)) { gmem_o_tmp.move(step_stride); }
-        gemm_q_k.reload_k();
-
-        // Make sure we are reading from the correct buffer.
-        gemm_q_k.smem_q.move_to_next_read_buffer();
-        // Trigger the load from shared memory for the next series of Q values.
-        if (l + step_stride < steps) {
-            gemm_q_k.reload_q();
-        }
-    }  // Outer loop over the sequence length.
+    // Construct identity layout for sO
+    Tensor cO = make_identity_tensor(make_shape(size<0>(sO), size<1>(sO)));    // (BLK_M,BLK_K) -> (blk_m,blk_k)
+    // Repeat the partitioning with identity layouts
+    Tensor tOcO = gmem_thr_copy_O.partition_D(cO);                           // (ACPY,ACPY_M,ACPY_K) -> (blk_m,blk_k)
+    Tensor tOpO = make_tensor<bool>(make_shape(size<2>(tOgO)));
+    if (!Is_even_K) {
+        #pragma unroll
+        for (int k = 0; k < size(tOpO); ++k) { tOpO(k) = get<1>(tOcO(0, 0, k)) < params.d; }
+    }
+    // Clear_OOB_K must be false since we don't want to write zeros to gmem
+    flash::copy</*Is_even_MN=*/false, Is_even_K, /*Clear_OOB_MN=*/false, /*Clear_OOB_K=*/false>(
+        gmem_thr_copy_O, tOrO, tOgO, tOcO, tOpO, binfo.actual_seqlen_q - m_block * kBlockM
+    );
 }
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
-template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Return_softmax, typename Params>
-inline __device__ void device_1xN_loop(const Params &params) {
-
+template<typename Kernel_traits, bool Is_dropout, bool Is_causal, bool Is_even_N, bool Is_even_K, bool Return_softmax, typename Params>
+inline __device__ void compute_attn(const Params &params) {
+    const int m_block = blockIdx.x;
     // The block index for the batch.
-    const int bidb = blockIdx.x;
+    const int bidb = blockIdx.y;
     // The block index for the head.
-    const int bidh = blockIdx.y;
-    // The block index.
-    const int bidx = gridDim.x * bidh + bidb;
-    // The thread index.
-    const int tidx = threadIdx.x;
+    const int bidh = blockIdx.z;
 
     // We want the fwd and bwd to generate the same dropout pattern (RNG), without restricting
     // them to have the same number of threads or have to traverse the attention matrix
     // in the same order.
     // In the Philox RNG, we use the offset to store the batch, head, and the lane id
-    // (within a warp). We use the subsequence to store the location of the 16 x 16 blocks within
+    // (within a warp). We use the subsequence to store the location of the 16 x 32 blocks within
     // the attention matrix. This way, as long as we have the batch, head, and the location of
-    // the 16 x 16 block within the attention matrix, we can generate the exact same dropout pattern.
-    auto seeds = at::cuda::philox::unpack(params.philox_args);
-    if (bidx == 0 && tidx == 0) {
-        params.rng_state[0] = std::get<0>(seeds);
-        params.rng_state[1] = std::get<1>(seeds);
-    }
-    Philox ph(std::get<0>(seeds), 0, std::get<1>(seeds) + (bidb * params.h + bidh) * 32 + tidx % 32);
-    constexpr int M = Kernel_traits::Cta_tile_p::M;
-    const int STEPS = (params.seqlen_q + M - 1) / M;
-
-    constexpr int blocksize_c = Kernel_traits::Cta_tile_p::N;
-    if (params.seqlen_k == blocksize_c) {
-        fmha::device_1xN_<Kernel_traits, Is_dropout, Is_causal, Return_softmax, true, true>(params, bidb, bidh, STEPS, ph, 0);
-    } else {
-        const int max_loop_steps = (params.seqlen_k + blocksize_c - 1) / blocksize_c;
-        fmha::device_1xN_<Kernel_traits, Is_dropout, Is_causal, Return_softmax, true, false>(params, bidb, bidh, STEPS, ph, 0);
-        for (int loop_step_idx = 1; loop_step_idx < max_loop_steps - 1; loop_step_idx++) {
-            fmha::device_1xN_<Kernel_traits, Is_dropout, Is_causal, Return_softmax, false, false>(params, bidb, bidh, STEPS, ph, loop_step_idx);
-        }
-        fmha::device_1xN_<Kernel_traits, Is_dropout, Is_causal, Return_softmax, false, true>(params, bidb, bidh, STEPS, ph, max_loop_steps - 1);
-    }
+    // the 16 x 32 block within the attention matrix, we can generate the exact same dropout pattern.
+
+    flash::compute_attn_1rowblock<Kernel_traits, Is_dropout, Is_causal, Is_even_N, Is_even_K, Return_softmax>(params, bidb, bidh, m_block);
 }
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
-} // namespace fmha
-
+} // namespace flash
```

### Comparing `flash_attn-1.0.9/csrc/flash_attn/src/philox.cuh` & `flash_attn-2.0.0/csrc/flash_attn/src/philox.cuh`

 * *Files 10% similar despite different names*

```diff
@@ -1,101 +1,144 @@
-// Adapted from https://github.com/NVIDIA/apex/blob/master/apex/contrib/csrc/multihead_attn/philox.cuh
-// Pytorch also has an implementation of Philox RNG: https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/codegen/cuda/runtime/random_numbers.cu
+// Pytorch also has an implementation of Philox RNG: https://github.com/pytorch/pytorch/blob/8ca3c881db3e3510fcb7725389f6a0633c9b992c/torch/csrc/jit/tensorexpr/cuda_random.h
 #pragma once
 // Philox CUDA.
 
+namespace flash {
+
+struct ull2 {
+    unsigned long long x;
+    unsigned long long y;
+};
+
+inline __device__ uint2 mulhilo32(const unsigned int a, const unsigned int b) {
+    uint2 *res;
+    unsigned long long tmp;
+    asm ("mul.wide.u32 %0, %1, %2;\n\t"
+          : "=l"(tmp)
+          : "r"(a), "r"(b));
+    res = (uint2*)(&tmp);
+    return *res;
+}
+
+inline __device__ uint4 philox_single_round(const uint4 ctr, const uint2 key) {
+    constexpr unsigned long kPhiloxSA = 0xD2511F53;
+    constexpr unsigned long kPhiloxSB = 0xCD9E8D57;
+    uint2 res0 = mulhilo32(kPhiloxSA, ctr.x);
+    uint2 res1 = mulhilo32(kPhiloxSB, ctr.z);
+    uint4 ret = {res1.y ^ ctr.y ^ key.x, res1.x, res0.y ^ ctr.w ^ key.y, res0.x};
+    return ret;
+}
+
+inline __device__ uint4 philox(unsigned long long seed,
+                               unsigned long long subsequence,
+                               unsigned long long offset) {
+    constexpr unsigned long kPhilox10A = 0x9E3779B9;
+    constexpr unsigned long kPhilox10B = 0xBB67AE85;
+    uint2 key = reinterpret_cast<uint2&>(seed);
+    uint4 counter;
+    ull2 *tmp = reinterpret_cast<ull2*>(&counter);
+    tmp->x = offset;
+    tmp->y = subsequence;
+    #pragma unroll
+    for (int i = 0; i < 6; i++) {
+        counter = philox_single_round(counter, key);
+        key.x += (kPhilox10A);
+        key.y += (kPhilox10B);
+    }
+    uint4 output = philox_single_round(counter, key);
+    return output;
+}
+
+} // namespace flash
+
 namespace {
 
 class Philox {
 public:
   __device__ inline Philox(unsigned long long seed,
                            unsigned long long subsequence,
                            unsigned long long offset)
-    : key(reinterpret_cast<const uint2&>(seed)) {
+      : STATE(0)
+      , seed_(seed)
+      , offset_(offset)
+      , key(reinterpret_cast<const uint2&>(seed)) {
     //key.x = (unsigned int)seed;
     //key.y = (unsigned int)(seed >> 32);
     //counter = make_uint4(0, 0, 0, 0);
     //counter.z = (unsigned int)(subsequence);
     //counter.w = (unsigned int)(subsequence >> 32);
     //STATE = 0;
     //incr_n(offset / 4);
 
+    // key = reinterpret_cast<const uint2&>(seed);
     ull2 * tmp = reinterpret_cast<ull2*>(&counter);
     tmp->x = offset / 4;
     tmp->y = subsequence;
     // if ((threadIdx.x == 0) && (blockIdx.x == 0) && (blockIdx.y == 0)) {
     //     printf("Philox counter: %d, %d, %d, %d\n", counter.x, counter.y, counter.z, counter.w);
     // }
   }
-
   __device__ inline uint4 operator()() {
-    uint4 counter_ = counter;
-    uint2 key_ = key;
-    // 7-round philox
-    #pragma unroll
-    for (int i = 0; i < 6; i++) {
-      counter_ = single_round(counter_, key_);
-      key_.x += (kPhilox10A);
-      key_.y += (kPhilox10B);
-    }
-    uint4 output = single_round(counter_, key_);
-    // if ((threadIdx.x == 0) && (blockIdx.x == 0) && (blockIdx.y == 0)) {
-    //     printf("Philox counter: %u, %u, %u, %u\n", counter.x, counter.y, counter.z, counter.w);
-    //     printf("Philox output: %u, %u, %u, %u\n", output.x, output.y, output.z, output.w);
-    // }
-    incr();
-    return output;
-  }
-
-  __device__ inline uint4 operator()(const unsigned long long subsequence) {
-    uint4 counter_ = counter;
-    ull2 * tmp = reinterpret_cast<ull2*>(&counter_);
-    tmp->y = subsequence;
-    // if ((threadIdx.x % 32 == 0) && (blockIdx.x == 0) && (blockIdx.y == 0)) {
-    //     printf("tidx = %d, counter_: %u, %u, %u, %u\n", threadIdx.x, counter_.x, counter_.y, counter_.z, counter_.w);
-    // }
-    uint2 key_ = key;
-    // 7-round philox
-    #pragma unroll
-    for (int i = 0; i < 6; i++) {
-      counter_ = single_round(counter_, key_);
-      key_.x += (kPhilox10A);
-      key_.y += (kPhilox10B);
-    }
-    uint4 output = single_round(counter_, key_);
-    // if ((threadIdx.x == 0) && (blockIdx.x == 0) && (blockIdx.y == 0)) {
-    //     printf("Philox counter: %u, %u, %u, %u\n", counter.x, counter.y, counter.z, counter.w);
-    //     printf("Philox output: %u, %u, %u, %u\n", output.x, output.y, output.z, output.w);
-    // }
-    return output;
+    // // if (STATE == 0) {
+    //   uint4 counter_ = counter;
+    //   uint2 key_ = key;
+    //   // 7-round philox
+    //   #pragma unroll
+    //   for (int i = 0; i < 6; i++) {
+    //       counter_ = flash::philox_single_round(counter_, key_);
+    //     key_.x += (kPhilox10A);
+    //     key_.y += (kPhilox10B);
+    //   }
+    //   // output = philox_single_round(counter_, key_);
+    //   uint4 output = flash::philox_single_round(counter_, key_);
+    //   // if ((threadIdx.x == 0) && (blockIdx.x == 0) && (blockIdx.y == 0)) {
+    //   //     printf("Philox counter: %u, %u, %u, %u\n", counter.x, counter.y, counter.z, counter.w);
+    //   //     printf("Philox output: %u, %u, %u, %u\n", output.x, output.y, output.z, output.w);
+    //   // }
+    //   incr();
+    // // }
+    // // return a float4 directly
+    // // unsigned long ret;
+    // // switch(STATE) {
+    // //  case 0: ret = output.x; break;
+    // //  case 1: ret = output.y; break;
+    // //  case 2: ret = output.z; break;
+    // //  case 3: ret = output.w; break;
+    // //}
+    // // STATE = (STATE + 1) % 4;
+    // return output;
+      return flash::philox(seed_, offset_, offset_);
   }
 
 private:
+  unsigned long long offset_, seed_;
   struct ull2 {
       uint64_t x;
       uint64_t y;
   };
   uint4 counter;
+  // uint4 output;
   const uint2 key;
+  unsigned int STATE;
+  __device__ inline void incr_n(unsigned long long n) {
+    unsigned int nlo = (unsigned int)(n);
+    unsigned int nhi = (unsigned int)(n >> 32);
+    counter.x += nlo;
+    if (counter.x < nlo)
+      nhi++;
+    counter.y += nhi;
+    if (nhi <= counter.y)
+      return;
+    if (++counter.z)
+      return;
+    ++counter.w;
+  }
 
-  // __device__ inline void incr_n(unsigned long long n) {
-  //   unsigned int nlo = (unsigned int)(n);
-  //   unsigned int nhi = (unsigned int)(n >> 32);
-  //   counter.x += nlo;
-  //   if (counter.x < nlo)
-  //     nhi++;
-  //   counter.y += nhi;
-  //   if (nhi <= counter.y)
-  //     return;
-  //   if (++counter.z)
-  //     return;
-  //   ++counter.w;
-  // }
-
-  __device__ uint4 incr(uint4 ctr) {
+  __device__ uint4 incr128 (uint4 ctr)
+  {
     uint4 res;
     asm ("add.cc.u32      %0, %4, %8;\n\t"
          "addc.cc.u32     %1, %5, %9;\n\t"
          "addc.cc.u32     %2, %6, %10;\n\t"
          "addc.u32        %3, %7, %11;\n\t"
          : "=r"(res.x), "=r"(res.y), "=r"(res.z), "=r"(res.w)
          : "r"(ctr.x), "r"(ctr.y), "r"(ctr.z), "r"(ctr.w),
@@ -103,55 +146,20 @@
     return res;
   }
 
   __device__ inline void incr() {
     // if ((threadIdx.x == 0) && (blockIdx.x == 0) && (blockIdx.y == 0)) {
     //     printf("Counter before: %u, %u, %u, %u\n", counter.x, counter.y, counter.z, counter.w);
     // }
-    counter = incr(counter);
+    counter = incr128(counter);
     // if ((threadIdx.x == 0) && (blockIdx.x == 0) && (blockIdx.y == 0)) {
     //     printf("Counter after: %u, %u, %u, %u\n", counter.x, counter.y, counter.z, counter.w);
     // }
   }
 
-  // __device__ unsigned int mulhilo32(unsigned int a, unsigned int b,
-  //                                   unsigned int *result_high) {
-  //   *result_high = __umulhi(a, b);
-  //   return a * b;
-  // }
-
-  __device__ uint2 mulhilo32(const unsigned int a, const unsigned int b) {
-    uint2 *res;
-    unsigned long long tmp;
-    asm ("mul.wide.u32      %0, %1, %2;\n\t"
-          : "=l"(tmp)
-          : "r"(a), "r"(b));
-    res = (uint2*)(&tmp);
-    return *res;
-  }
-
-  __device__ inline uint4 single_round(const uint4 ctr, const uint2 key) {
-    //unsigned int hi0;
-    //unsigned int hi1;
-    //unsigned int lo0 = mulhilo32(kPhiloxSA, ctr.x, &hi0);
-    //unsigned int lo1 = mulhilo32(kPhiloxSB, ctr.z, &hi1);
-    //uint4 ret = {hi1 ^ ctr.y ^ key.x, lo1, hi0 ^ ctr.w ^ key.y, lo0};
-    uint2 res0 = mulhilo32(kPhiloxSA, ctr.x);
-    uint2 res1 = mulhilo32(kPhiloxSB, ctr.z);
-    uint4 ret = {res1.y ^ ctr.y ^ key.x, res1.x, res0.y ^ ctr.w ^ key.y, res0.x};  
-    return ret;
-  }
-
   static const unsigned long kPhilox10A = 0x9E3779B9;
   static const unsigned long kPhilox10B = 0xBB67AE85;
-  static const unsigned long kPhiloxSA = 0xD2511F53;
-  static const unsigned long kPhiloxSB = 0xCD9E8D57;
+  // static const unsigned long kPhiloxSA = 0xD2511F53;
+  // static const unsigned long kPhiloxSB = 0xCD9E8D57;
 };
 
-// Inverse of 2^32.
-constexpr float M_RAN_INVM32 = 2.3283064e-10f;
-__device__ __inline__ float4 uniform4(const uint4 x) {
-  return make_float4(x.x * M_RAN_INVM32, x.y * M_RAN_INVM32, x.z * M_RAN_INVM32,
-                     x.w * M_RAN_INVM32);
-}
-
 } // namespace
```

### Comparing `flash_attn-1.0.9/csrc/flash_gen/decoder_masked_multihead_attention.cu` & `flash_attn-2.0.0/csrc/flash_gen/decoder_masked_multihead_attention.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/ft_attention/cuda_bf16_fallbacks.cuh` & `flash_attn-2.0.0/csrc/ft_attention/cuda_bf16_fallbacks.cuh`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/ft_attention/cuda_bf16_wrapper.h` & `flash_attn-2.0.0/csrc/ft_attention/cuda_bf16_wrapper.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/ft_attention/decoder_masked_multihead_attention.cu` & `flash_attn-2.0.0/csrc/ft_attention/decoder_masked_multihead_attention.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/ft_attention/decoder_masked_multihead_attention.h` & `flash_attn-2.0.0/csrc/ft_attention/decoder_masked_multihead_attention.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/ft_attention/decoder_masked_multihead_attention_utils.h` & `flash_attn-2.0.0/csrc/ft_attention/decoder_masked_multihead_attention_utils.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/ft_attention/ft_attention.cpp` & `flash_attn-2.0.0/csrc/ft_attention/ft_attention.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/fused_dense_lib/fused_dense.cpp` & `flash_attn-2.0.0/csrc/fused_dense_lib/fused_dense.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/fused_dense_lib/fused_dense_cuda.cu` & `flash_attn-2.0.0/csrc/fused_dense_lib/fused_dense_cuda.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/fused_softmax/fused_softmax.cpp` & `flash_attn-2.0.0/csrc/fused_softmax/fused_softmax.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/fused_softmax/scaled_masked_softmax.h` & `flash_attn-2.0.0/csrc/fused_softmax/scaled_masked_softmax.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/fused_softmax/scaled_masked_softmax_cuda.cu` & `flash_attn-2.0.0/csrc/fused_softmax/scaled_masked_softmax_cuda.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/fused_softmax/scaled_upper_triang_masked_softmax.h` & `flash_attn-2.0.0/csrc/fused_softmax/scaled_upper_triang_masked_softmax.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/fused_softmax/scaled_upper_triang_masked_softmax_cuda.cu` & `flash_attn-2.0.0/csrc/fused_softmax/scaled_upper_triang_masked_softmax_cuda.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/fused_softmax/type_shim.h` & `flash_attn-2.0.0/csrc/fused_softmax/type_shim.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln.h` & `flash_attn-2.0.0/csrc/layer_norm/ln.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_api.cpp` & `flash_attn-2.0.0/csrc/layer_norm/ln_api.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_1024.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_bwd_1024.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_1280.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_bwd_1280.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_1536.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_bwd_1536.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_2048.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_bwd_2048.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_256.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_bwd_256.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_2560.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_bwd_2560.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_3072.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_bwd_3072.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_4096.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_bwd_4096.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_512.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_bwd_512.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_5120.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_bwd_5120.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_6144.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_bwd_6144.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_7168.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_bwd_7168.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_768.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_bwd_768.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_8192.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_bwd_8192.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_kernels.cuh` & `flash_attn-2.0.0/csrc/layer_norm/ln_bwd_kernels.cuh`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_bwd_semi_cuda_kernel_old.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_bwd_semi_cuda_kernel_old.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_1024.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_fwd_1024.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_10240.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_fwd_10240.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_12288.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_fwd_12288.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_128.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_fwd_128.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_1280.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_fwd_1280.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_1536.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_fwd_1536.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_2048.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_fwd_2048.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_256.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_fwd_256.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_2560.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_fwd_2560.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_3072.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_fwd_3072.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_384.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_fwd_384.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_4096.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_fwd_4096.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_512.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_fwd_512.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_5120.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_fwd_5120.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_6144.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_fwd_6144.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_7168.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_fwd_7168.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_768.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_fwd_768.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_8192.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_fwd_8192.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_9216.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_fwd_9216.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_cuda_kernel_old.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_fwd_cuda_kernel_old.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_fwd_kernels.cuh` & `flash_attn-2.0.0/csrc/layer_norm/ln_fwd_kernels.cuh`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_kernel_traits.h` & `flash_attn-2.0.0/csrc/layer_norm/ln_kernel_traits.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_1024.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_parallel_bwd_1024.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_1280.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_parallel_bwd_1280.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_1536.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_parallel_bwd_1536.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_2048.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_parallel_bwd_2048.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_256.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_parallel_bwd_256.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_2560.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_parallel_bwd_2560.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_3072.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_parallel_bwd_3072.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_4096.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_parallel_bwd_4096.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_512.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_parallel_bwd_512.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_5120.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_parallel_bwd_5120.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_6144.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_parallel_bwd_6144.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_7168.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_parallel_bwd_7168.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_768.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_parallel_bwd_768.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_bwd_8192.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_parallel_bwd_8192.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_1024.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_parallel_fwd_1024.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_1280.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_parallel_fwd_1280.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_1536.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_parallel_fwd_1536.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_2048.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_parallel_fwd_2048.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_256.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_parallel_fwd_256.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_2560.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_parallel_fwd_2560.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_3072.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_parallel_fwd_3072.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_4096.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_parallel_fwd_4096.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_512.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_parallel_fwd_512.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_5120.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_parallel_fwd_5120.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_6144.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_parallel_fwd_6144.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_7168.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_parallel_fwd_7168.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_768.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_parallel_fwd_768.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_fwd_8192.cu` & `flash_attn-2.0.0/csrc/layer_norm/ln_parallel_fwd_8192.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_residual_bwd_kernels.cuh` & `flash_attn-2.0.0/csrc/layer_norm/ln_parallel_residual_bwd_kernels.cuh`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_parallel_residual_fwd_kernels.cuh` & `flash_attn-2.0.0/csrc/layer_norm/ln_parallel_residual_fwd_kernels.cuh`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/ln_utils.cuh` & `flash_attn-2.0.0/csrc/layer_norm/ln_utils.cuh`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/layer_norm/static_switch.h` & `flash_attn-2.0.0/csrc/layer_norm/static_switch.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/rotary/rotary.cpp` & `flash_attn-2.0.0/csrc/rotary/rotary.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/rotary/rotary_cuda.cu` & `flash_attn-2.0.0/csrc/rotary/rotary_cuda.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/xentropy/interface.cpp` & `flash_attn-2.0.0/csrc/xentropy/interface.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/csrc/xentropy/xentropy_kernel.cu` & `flash_attn-2.0.0/csrc/xentropy/xentropy_kernel.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/flash_attn/attention_kernl.py` & `flash_attn-2.0.0/flash_attn/attention_kernl.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/flash_attn/bert_padding.py` & `flash_attn-2.0.0/flash_attn/bert_padding.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/flash_attn/flash_attention.py` & `flash_attn-2.0.0/flash_attn/flash_attention.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 import math
 import torch
 import torch.nn as nn
 
 from einops import rearrange
 
-from flash_attn.flash_attn_interface import flash_attn_unpadded_qkvpacked_func
+from flash_attn import flash_attn_qkvpacked_func, flash_attn_varlen_qkvpacked_func
 from flash_attn.bert_padding import unpad_input, pad_input
 
 
 class FlashAttention(nn.Module):
     """Implement the scaled dot product attention with softmax.
     Arguments
     ---------
@@ -20,54 +20,53 @@
     """
     def __init__(self, softmax_scale=None, attention_dropout=0.0):
         super().__init__()
         self.softmax_scale = softmax_scale
         self.dropout_p = attention_dropout
 
     def forward(self, qkv, key_padding_mask=None, causal=False, cu_seqlens=None,
-                max_s=None, need_weights=False):
+                max_s=None):
         """Implements the multihead softmax attention.
         Arguments
         ---------
             qkv: The tensor containing the query, key, and value. (B, S, 3, H, D) if key_padding_mask is None
                 if unpadded: (nnz, 3, h, d)
             key_padding_mask: a bool tensor of shape (B, S)
         """
-        assert not need_weights
         assert qkv.dtype in [torch.float16, torch.bfloat16]
         assert qkv.is_cuda
 
         if cu_seqlens is None:
             batch_size = qkv.shape[0]
             seqlen = qkv.shape[1]
             if key_padding_mask is None:
                 qkv = rearrange(qkv, 'b s ... -> (b s) ...')
                 max_s = seqlen
                 cu_seqlens = torch.arange(0, (batch_size + 1) * seqlen, step=seqlen, dtype=torch.int32,
                                         device=qkv.device)
-                output = flash_attn_unpadded_qkvpacked_func(
+                output = flash_attn_varlen_qkvpacked_func(
                     qkv, cu_seqlens, max_s, self.dropout_p if self.training else 0.0,
                     softmax_scale=self.softmax_scale, causal=causal
                 )
                 output = rearrange(output, '(b s) ... -> b s ...', b=batch_size)
             else:
                 nheads = qkv.shape[-2]
                 x = rearrange(qkv, 'b s three h d -> b s (three h d)')
                 x_unpad, indices, cu_seqlens, max_s = unpad_input(x, key_padding_mask)
                 x_unpad = rearrange(x_unpad, 'nnz (three h d) -> nnz three h d', three=3, h=nheads)
-                output_unpad = flash_attn_unpadded_qkvpacked_func(
+                output_unpad = flash_attn_varlen_qkvpacked_func(
                     x_unpad, cu_seqlens, max_s, self.dropout_p if self.training else 0.0,
                     softmax_scale=self.softmax_scale, causal=causal
                 )
                 output = rearrange(pad_input(rearrange(output_unpad, 'nnz h d -> nnz (h d)'),
                                             indices, batch_size, seqlen),
                                 'b s (h d) -> b s h d', h=nheads)
         else:
             assert max_s is not None
-            output = flash_attn_unpadded_qkvpacked_func(
+            output = flash_attn_varlen_qkvpacked_func(
                 qkv, cu_seqlens, max_s, self.dropout_p if self.training else 0.0,
                 softmax_scale=self.softmax_scale, causal=causal
             )
 
         return output, None
 
 
@@ -80,22 +79,22 @@
         super().__init__()
         self.embed_dim = embed_dim
         self.causal = causal
 
         self.num_heads = num_heads
         assert self.embed_dim % num_heads == 0, "self.kdim must be divisible by num_heads"
         self.head_dim = self.embed_dim // num_heads
-        assert self.head_dim % 8 == 0 and self.head_dim <= 128, "Only support head_dim <= 128 and divisible by 8"
+        assert self.head_dim <= 256, "Only support head_dim <= 258"
 
         self.Wqkv = nn.Linear(embed_dim, 3 * embed_dim, bias=bias, **factory_kwargs)
         self.inner_attn = FlashAttention(attention_dropout=attention_dropout)
         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias, **factory_kwargs)
 
-    def forward(self, x, key_padding_mask=None, need_weights=False):
+    def forward(self, x, key_padding_mask=None):
         """x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim)
         key_padding_mask: bool tensor of shape (batch, seqlen)
         """
         qkv = self.Wqkv(x)
         qkv = rearrange(qkv, 'b s (three h d) -> b s three h d', three=3, h=self.num_heads)
         context, attn_weights = self.inner_attn(qkv, key_padding_mask=key_padding_mask,
-                                                need_weights=need_weights, causal=self.causal)
+                                                causal=self.causal)
         return self.out_proj(rearrange(context, 'b s h d -> b s (h d)')), attn_weights
```

### Comparing `flash_attn-1.0.9/flash_attn/flash_attn_triton.py` & `flash_attn-2.0.0/flash_attn/flash_attn_triton.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/flash_attn/flash_attn_triton_og.py` & `flash_attn-2.0.0/flash_attn/flash_attn_triton_og.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/flash_attn/flash_attn_triton_single_query.py` & `flash_attn-2.0.0/flash_attn/flash_attn_triton_single_query.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/flash_attn/flash_attn_triton_tmp.py` & `flash_attn-2.0.0/flash_attn/flash_attn_triton_tmp.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/flash_attn/flash_attn_triton_tmp_og.py` & `flash_attn-2.0.0/flash_attn/flash_attn_triton_tmp_og.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/flash_attn/flash_blocksparse_attention.py` & `flash_attn-2.0.0/flash_attn/flash_blocksparse_attention.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/flash_attn/flash_blocksparse_attn_interface.py` & `flash_attn-2.0.0/flash_attn/flash_blocksparse_attn_interface.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/flash_attn/fused_softmax.py` & `flash_attn-2.0.0/flash_attn/fused_softmax.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/flash_attn/layers/patch_embed.py` & `flash_attn-2.0.0/flash_attn/layers/patch_embed.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/flash_attn/layers/rotary.py` & `flash_attn-2.0.0/flash_attn/layers/rotary.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/flash_attn/losses/cross_entropy.py` & `flash_attn-2.0.0/flash_attn/losses/cross_entropy.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/flash_attn/models/bert.py` & `flash_attn-2.0.0/flash_attn/models/bert.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/flash_attn/models/gpt.py` & `flash_attn-2.0.0/flash_attn/models/gpt.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/flash_attn/models/gpt_neox.py` & `flash_attn-2.0.0/flash_attn/models/gpt_neox.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/flash_attn/models/gptj.py` & `flash_attn-2.0.0/flash_attn/models/gptj.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/flash_attn/models/llama.py` & `flash_attn-2.0.0/flash_attn/models/llama.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/flash_attn/models/opt.py` & `flash_attn-2.0.0/flash_attn/models/opt.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/flash_attn/models/vit.py` & `flash_attn-2.0.0/flash_attn/models/vit.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/flash_attn/modules/block.py` & `flash_attn-2.0.0/flash_attn/modules/block.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 from functools import partial
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 from torch import Tensor
 
-from torchvision.ops import StochasticDepth
+# from torchvision.ops import StochasticDepth
 
 from flash_attn.modules.mha import MHA
 from flash_attn.modules.mlp import Mlp
 
 try:
     from flash_attn.ops.layer_norm import dropout_add_layer_norm
 except ImportError:
@@ -66,20 +66,20 @@
             assert self.prenorm, 'residual_in_fp32 is only compatible with prenorm=True'
         if mixer_cls is None:
             mixer_cls = partial(MHA, num_heads=dim // 64)
         if mlp_cls is None:
             mlp_cls = partial(Mlp, hidden_features=4 * dim)
         self.mixer = mixer_cls(dim)
         self.dropout1 = dropout_cls(resid_dropout1)
-        self.drop_path1 = StochasticDepth(drop_path1, mode='row')
+        # self.drop_path1 = StochasticDepth(drop_path1, mode='row')
         self.norm1 = norm_cls(dim)
         self.mlp = mlp_cls(dim)
         if not isinstance(self.mlp, nn.Identity):
             self.dropout2 = dropout_cls(resid_dropout2)
-            self.drop_path2 = StochasticDepth(drop_path2, mode='row')
+            # self.drop_path2 = StochasticDepth(drop_path2, mode='row')
             self.norm2 = norm_cls(dim)
 
         if self.fused_dropout_add_ln:
             assert dropout_add_layer_norm is not None, 'dropout_layer_norm is not installed'
             assert dropout_add_rms_norm is not None, 'dropout_layer_norm is not installed'
             assert (isinstance(self.norm1, (nn.LayerNorm, RMSNorm))
                     and isinstance(self.dropout1, nn.Dropout))
@@ -125,21 +125,22 @@
             if not self.fused_dropout_add_ln:
                 dropped = self.drop_path1(self.dropout1(hidden_states))
                 residual = (dropped + residual) if residual is not None else dropped
                 hidden_states = self.norm1(residual.to(dtype=self.norm1.weight.dtype))
                 if self.residual_in_fp32:
                     residual = residual.to(torch.float32)
             else:
-                if self.drop_path1.p == 0 or not self.training:
-                    rowscale1 = None
-                else:
-                    rowscale1 = self.drop_path1(torch.ones(
-                        hidden_states.shape[:-1], device=hidden_states.device,
-                        dtype=hidden_states.dtype)
-                    )
+                rowscale1 = None
+                # if self.drop_path1.p == 0 or not self.training:
+                #     rowscale1 = None
+                # else:
+                #     rowscale1 = self.drop_path1(torch.ones(
+                #         hidden_states.shape[:-1], device=hidden_states.device,
+                #         dtype=hidden_states.dtype)
+                #     )
                 hidden_states, residual = fused_add_norm_fn(
                     hidden_states, residual, self.norm1.weight, self.norm1.bias,
                     self.dropout1.p if self.training else 0.0, self.norm1.eps,
                     rowscale=rowscale1, prenorm=True, residual_in_fp32=self.residual_in_fp32
                 )
             if mixer_kwargs is None:
                 mixer_kwargs = {}
@@ -152,21 +153,22 @@
                 if not self.fused_dropout_add_ln:
                     dropped = self.drop_path2(self.dropout2(hidden_states))
                     residual = (dropped + residual) if residual is not None else dropped
                     hidden_states = self.norm2(residual.to(dtype=self.norm2.weight.dtype))
                     if self.residual_in_fp32:
                         residual = residual.to(torch.float32)
                 else:
-                    if self.drop_path2.p == 0 or not self.training:
-                        rowscale2 = None
-                    else:
-                        rowscale2 = self.drop_path2(torch.ones(
-                            hidden_states.shape[:-1], device=hidden_states.device,
-                            dtype=hidden_states.dtype)
-                        )
+                    # if self.drop_path2.p == 0 or not self.training:
+                    #     rowscale2 = None
+                    # else:
+                    #     rowscale2 = self.drop_path2(torch.ones(
+                    #         hidden_states.shape[:-1], device=hidden_states.device,
+                    #         dtype=hidden_states.dtype)
+                    #     )
+                    rowscale2 = None
                     hidden_states, residual = fused_add_norm_fn(
                         hidden_states, residual, self.norm2.weight, self.norm2.bias,
                         self.dropout2.p if self.training else 0.0, self.norm2.eps,
                         rowscale=rowscale2, prenorm=True, residual_in_fp32=self.residual_in_fp32
                     )
                 hidden_states = self.mlp(hidden_states)
             return hidden_states, residual
```

### Comparing `flash_attn-1.0.9/flash_attn/modules/embedding.py` & `flash_attn-2.0.0/flash_attn/modules/embedding.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/flash_attn/modules/mha.py` & `flash_attn-2.0.0/flash_attn/modules/mha.py`

 * *Files 2% similar despite different names*

```diff
@@ -6,22 +6,18 @@
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
 from einops import rearrange
 
 try:
-    from flash_attn.flash_attn_interface import flash_attn_unpadded_qkvpacked_func
-    from flash_attn.flash_attn_interface import flash_attn_unpadded_kvpacked_func
-except ImportError:
-    flash_attn_unpadded_qkvpacked_func, flash_attn_unpadded_kvpacked_func = None, None
-
-try:
-    from flash_attn.ops.flash_attn_triton import flash_attn_qkvpacked_func, flash_attn_kvpacked_func
+    from flash_attn import flash_attn_varlen_qkvpacked_func, flash_attn_varlen_kvpacked_func
+    from flash_attn import flash_attn_qkvpacked_func, flash_attn_kvpacked_func
 except ImportError:
+    flash_attn_varlen_qkvpacked_func, flash_attn_varlen_kvpacked_func = None, None
     flash_attn_qkvpacked_func, flash_attn_kvpacked_func = None, None
 
 try:
     from flash_attn.ops.fused_dense import FusedDense, ColumnParallelLinear, RowParallelLinear
 except ImportError:
     FusedDense, ColumnParallelLinear, RowParallelLinear = None, None, None
 
@@ -42,25 +38,21 @@
     ---------
         softmax_scale: The temperature to use for the softmax attention.
                       (default: 1/sqrt(d_keys) where d_keys is computed at
                       runtime)
         attention_dropout: The dropout rate to apply to the attention
                            (default: 0.0)
     """
-    def __init__(self, causal=False, softmax_scale=None, attention_dropout=0.0,
-                 triton=False):
+    def __init__(self, causal=False, softmax_scale=None, attention_dropout=0.0):
         super().__init__()
-        if attention_dropout != 0.0 or not triton:
-            assert flash_attn_unpadded_qkvpacked_func is not None, 'FlashAttention is not installed'
-        if attention_dropout == 0.0 and triton:
-            assert flash_attn_qkvpacked_func is not None, 'FlashAttention Triton is not installed'
+        assert flash_attn_varlen_qkvpacked_func is not None, 'FlashAttention is not installed'
+        assert flash_attn_qkvpacked_func is not None, 'FlashAttention is not installed'
         self.causal = causal
         self.softmax_scale = softmax_scale
         self.drop = nn.Dropout(attention_dropout)
-        self.triton = triton
 
     def forward(self, qkv, causal=None, cu_seqlens=None, max_seqlen=None):
         """Implements the multihead softmax attention.
         Arguments
         ---------
             qkv: The tensor containing the query, key, and value.
                 If cu_seqlens is None and max_seqlen is None, then qkv has shape (B, S, 3, H, D).
@@ -79,65 +71,48 @@
         assert qkv.is_cuda
         causal = self.causal if causal is None else causal
         unpadded = cu_seqlens is not None
         if unpadded:
             assert cu_seqlens.dtype == torch.int32
             assert max_seqlen is not None
             assert isinstance(max_seqlen, int)
-            return flash_attn_unpadded_qkvpacked_func(
+            return flash_attn_varlen_qkvpacked_func(
                 qkv, cu_seqlens, max_seqlen, self.drop.p if self.training else 0.0,
                 softmax_scale=self.softmax_scale, causal=causal
             )
         else:
-            batch_size, seqlen = qkv.shape[0], qkv.shape[1]
-            # Triton version doesn't support dropout
-            if self.triton and (self.drop.p == 0 or not self.training):
-                output = flash_attn_qkvpacked_func(qkv, None, causal, self.softmax_scale)
-            else:
-                qkv = rearrange(qkv, 'b s ... -> (b s) ...')
-                max_seqlen = seqlen
-                cu_seqlens = torch.arange(0, (batch_size + 1) * seqlen, step=seqlen, dtype=torch.int32,
-                                        device=qkv.device)
-                output = flash_attn_unpadded_qkvpacked_func(
-                    qkv, cu_seqlens, max_seqlen, self.drop.p if self.training else 0.0,
-                    softmax_scale=self.softmax_scale, causal=causal
-                )
-                output = rearrange(output, '(b s) ... -> b s ...', b=batch_size)
-            return output
+            return flash_attn_qkvpacked_func(qkv, self.drop.p if self.training else 0.0,
+                                             softmax_scale=self.softmax_scale, causal=causal)
 
 
 class FlashCrossAttention(nn.Module):
     """Implement the scaled dot product attention with softmax.
     Arguments
     ---------
         softmax_scale: The temperature to use for the softmax attention.
                       (default: 1/sqrt(d_keys) where d_keys is computed at
                       runtime)
         attention_dropout: The dropout rate to apply to the attention
                            (default: 0.0)
     """
-    def __init__(self, causal=False, softmax_scale=None, attention_dropout=0.0,
-                 triton=False):
+    def __init__(self, causal=False, softmax_scale=None, attention_dropout=0.0):
         super().__init__()
-        if attention_dropout != 0.0 or not triton:
-            assert flash_attn_unpadded_kvpacked_func is not None, 'FlashAttention is not installed'
-        if attention_dropout == 0.0 and triton:
-            assert flash_attn_kvpacked_func is not None, 'FlashAttention Triton is not installed'
+        assert flash_attn_varlen_kvpacked_func is not None, 'FlashAttention is not installed'
+        assert flash_attn_kvpacked_func is not None, 'FlashAttention is not installed'
         self.causal = causal
         self.softmax_scale = softmax_scale
         self.drop = nn.Dropout(attention_dropout)
-        self.triton = triton
 
     def forward(self, q, kv, causal=None, cu_seqlens=None, max_seqlen=None,
                 cu_seqlens_k=None, max_seqlen_k=None):
         """Implements the multihead softmax attention.
         Arguments
         ---------
             q: The tensor containing the query. (B, Sq, H, D)
-            kv: The tensor containing the key and value. (B, Sk, 2, H, D)
+            kv: The tensor containing the key and value. (B, Sk, 2, H_k, D)
             causal: if passed, will override self.causal
             cu_seqlens: (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths
                 of the sequences in the batch, used to index into q.
             max_seqlen: int. Maximum sequence length in the batch of q.
             cu_seqlens_k: (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths
                 of the sequences in the batch, used to index into kv.
             max_seqlen_k: int. Maximum sequence length in the batch of k and v.
@@ -150,39 +125,25 @@
             assert cu_seqlens.dtype == torch.int32
             assert max_seqlen is not None
             assert isinstance(max_seqlen, int)
             assert cu_seqlens_k is not None
             assert cu_seqlens_k.dtype == torch.int32
             assert max_seqlen_k is not None
             assert isinstance(max_seqlen, int)
-            return flash_attn_unpadded_kvpacked_func(
+            return flash_attn_varlen_kvpacked_func(
                 q, kv, cu_seqlens, cu_seqlens_k, max_seqlen, max_seqlen_k,
                 self.drop.p if self.training else 0.0,
                 softmax_scale=self.softmax_scale, causal=causal
             )
         else:
             batch_size, seqlen_q = q.shape[0], q.shape[1]
             seqlen_k = kv.shape[1]
-            assert kv.shape[0] == batch_size and kv.shape[3] == q.shape[2] and kv.shape[4] == q.shape[3]
-            if self.triton and (self.drop.p == 0.0 or not self.training):  # Triton version doesn't support dropout
-                output = flash_attn_kvpacked_func(q, kv, None, causal, self.softmax_scale)
-            else:
-                q = rearrange(q, 'b s ... -> (b s) ...')
-                kv = rearrange(kv, 'b s ... -> (b s) ...')
-                cu_seqlens_q = torch.arange(0, (batch_size + 1) * seqlen_q, step=seqlen_q,
-                                            dtype=torch.int32, device=q.device)
-                cu_seqlens_k = torch.arange(0, (batch_size + 1) * seqlen_k, step=seqlen_k,
-                                            dtype=torch.int32, device=kv.device)
-                output = flash_attn_unpadded_kvpacked_func(
-                    q, kv, cu_seqlens_q, cu_seqlens_k, seqlen_q, seqlen_k,
-                    self.drop.p if self.training else 0.0,
-                    softmax_scale=self.softmax_scale, causal=causal
-                )
-                output = rearrange(output, '(b s) ... -> b s ...', b=batch_size)
-            return output
+            assert kv.shape[0] == batch_size and kv.shape[4] == q.shape[3]
+            return flash_attn_kvpacked_func(q, kv, self.drop.p if self.training else 0.0,
+                                            causal=causal, softmax_scale=self.softmax_scale)
 
 
 class SelfAttention(nn.Module):
     """Implement the scaled dot product attention with softmax.
     Arguments
     ---------
         softmax_scale: The temperature to use for the softmax attention.
```

### Comparing `flash_attn-1.0.9/flash_attn/modules/mlp.py` & `flash_attn-2.0.0/flash_attn/modules/mlp.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/flash_attn/ops/activations.py` & `flash_attn-2.0.0/flash_attn/ops/activations.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/flash_attn/ops/fused_dense.py` & `flash_attn-2.0.0/flash_attn/ops/fused_dense.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/flash_attn/ops/layer_norm.py` & `flash_attn-2.0.0/flash_attn/ops/layer_norm.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/flash_attn/ops/rms_norm.py` & `flash_attn-2.0.0/flash_attn/ops/rms_norm.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/flash_attn/utils/benchmark.py` & `flash_attn-2.0.0/flash_attn/utils/benchmark.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/flash_attn/utils/distributed.py` & `flash_attn-2.0.0/flash_attn/utils/distributed.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/flash_attn/utils/generation.py` & `flash_attn-2.0.0/flash_attn/utils/generation.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.9/flash_attn/utils/pretrained.py` & `flash_attn-2.0.0/flash_attn/utils/pretrained.py`

 * *Files identical despite different names*

