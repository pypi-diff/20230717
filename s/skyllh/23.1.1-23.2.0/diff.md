# Comparing `tmp/skyllh-23.1.1.tar.gz` & `tmp/skyllh-23.2.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "skyllh-23.1.1.tar", last modified: Fri Jun 23 14:37:48 2023, max compression
+gzip compressed data, was "skyllh-23.2.0.tar", last modified: Mon Jul 17 13:59:17 2023, max compression
```

## Comparing `skyllh-23.1.1.tar` & `skyllh-23.2.0.tar`

### file list

```diff
@@ -1,139 +1,140 @@
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:48.435640 skyllh-23.1.1/
--rw-r--r--   0 runner    (1001) docker     (123)    35149 2023-06-23 14:37:38.000000 skyllh-23.1.1/LICENSE.txt
--rw-r--r--   0 runner    (1001) docker     (123)     3098 2023-06-23 14:37:48.435640 skyllh-23.1.1/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)     1959 2023-06-23 14:37:38.000000 skyllh-23.1.1/README.md
--rw-r--r--   0 runner    (1001) docker     (123)      104 2023-06-23 14:37:38.000000 skyllh-23.1.1/pyproject.toml
--rw-r--r--   0 runner    (1001) docker     (123)     1592 2023-06-23 14:37:48.435640 skyllh-23.1.1/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (123)      135 2023-06-23 14:37:38.000000 skyllh-23.1.1/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:48.435640 skyllh-23.1.1/skyllh/
--rw-r--r--   0 runner    (1001) docker     (123)      750 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      498 2023-06-23 14:37:48.435640 skyllh-23.1.1/skyllh/_version.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:48.423640 skyllh-23.1.1/skyllh/analyses/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/analyses/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:48.423640 skyllh-23.1.1/skyllh/analyses/i3/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/analyses/i3/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:48.423640 skyllh-23.1.1/skyllh/analyses/i3/publicdata_ps/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/analyses/i3/publicdata_ps/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    13141 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/analyses/i3/publicdata_ps/aeff.py
--rw-r--r--   0 runner    (1001) docker     (123)    17863 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/analyses/i3/publicdata_ps/backgroundpdf.py
--rw-r--r--   0 runner    (1001) docker     (123)    13307 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/analyses/i3/publicdata_ps/bkg_flux.py
--rw-r--r--   0 runner    (1001) docker     (123)     9006 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/analyses/i3/publicdata_ps/detsigyield.py
--rw-r--r--   0 runner    (1001) docker     (123)    15690 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/analyses/i3/publicdata_ps/mcbkg_ps.py
--rw-r--r--   0 runner    (1001) docker     (123)     9417 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/analyses/i3/publicdata_ps/pdfratio.py
--rw-r--r--   0 runner    (1001) docker     (123)    25624 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/analyses/i3/publicdata_ps/signal_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)    15782 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/analyses/i3/publicdata_ps/signalpdf.py
--rw-r--r--   0 runner    (1001) docker     (123)    30526 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/analyses/i3/publicdata_ps/smearing_matrix.py
--rw-r--r--   0 runner    (1001) docker     (123)    18999 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/analyses/i3/publicdata_ps/time_dependent_ps.py
--rw-r--r--   0 runner    (1001) docker     (123)    15811 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/analyses/i3/publicdata_ps/time_integrated_ps.py
--rw-r--r--   0 runner    (1001) docker     (123)     8207 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/analyses/i3/publicdata_ps/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:48.423640 skyllh-23.1.1/skyllh/cluster/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/cluster/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2316 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/cluster/commands.py
--rw-r--r--   0 runner    (1001) docker     (123)     3926 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/cluster/compute_node.py
--rw-r--r--   0 runner    (1001) docker     (123)     3926 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/cluster/master_node.py
--rw-r--r--   0 runner    (1001) docker     (123)     3313 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/cluster/srvclt.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:48.427640 skyllh-23.1.1/skyllh/core/
--rw-r--r--   0 runner    (1001) docker     (123)      915 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    69120 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/analysis.py
--rw-r--r--   0 runner    (1001) docker     (123)    55665 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/analysis_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    20342 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/background_generation.py
--rw-r--r--   0 runner    (1001) docker     (123)     5087 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/background_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)     7148 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/backgroundpdf.py
--rw-r--r--   0 runner    (1001) docker     (123)    11334 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/binning.py
--rw-r--r--   0 runner    (1001) docker     (123)     8698 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/config.py
--rw-r--r--   0 runner    (1001) docker     (123)     3262 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/coords.py
--rw-r--r--   0 runner    (1001) docker     (123)    66267 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     6280 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/debugging.py
--rw-r--r--   0 runner    (1001) docker     (123)    13082 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/detsigyield.py
--rw-r--r--   0 runner    (1001) docker     (123)     1100 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/display.py
--rw-r--r--   0 runner    (1001) docker     (123)     4753 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/expectation_maximization.py
--rw-r--r--   0 runner    (1001) docker     (123)    18924 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/interpolate.py
--rw-r--r--   0 runner    (1001) docker     (123)    12677 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/livetime.py
--rw-r--r--   0 runner    (1001) docker     (123)    76116 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/llhratio.py
--rw-r--r--   0 runner    (1001) docker     (123)     3435 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/math.py
--rw-r--r--   0 runner    (1001) docker     (123)    34034 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/minimizer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:48.427640 skyllh-23.1.1/skyllh/core/minimizers/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/minimizers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7822 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/minimizers/iminuit.py
--rw-r--r--   0 runner    (1001) docker     (123)     4514 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/model.py
--rw-r--r--   0 runner    (1001) docker     (123)    14832 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/multiproc.py
--rw-r--r--   0 runner    (1001) docker     (123)    44046 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/optimize.py
--rw-r--r--   0 runner    (1001) docker     (123)    94140 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/parameters.py
--rw-r--r--   0 runner    (1001) docker     (123)    74012 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/pdf.py
--rw-r--r--   0 runner    (1001) docker     (123)    38060 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/pdfratio.py
--rw-r--r--   0 runner    (1001) docker     (123)     9412 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/progressbar.py
--rw-r--r--   0 runner    (1001) docker     (123)    20801 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/py.py
--rw-r--r--   0 runner    (1001) docker     (123)     2078 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/random.py
--rw-r--r--   0 runner    (1001) docker     (123)     8193 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/scrambling.py
--rw-r--r--   0 runner    (1001) docker     (123)     1183 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/session.py
--rw-r--r--   0 runner    (1001) docker     (123)     4241 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/signal_generation.py
--rw-r--r--   0 runner    (1001) docker     (123)    19833 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/signal_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)    30435 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/signalpdf.py
--rw-r--r--   0 runner    (1001) docker     (123)     6499 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/smoothing.py
--rw-r--r--   0 runner    (1001) docker     (123)     5999 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/source_hypo_group.py
--rw-r--r--   0 runner    (1001) docker     (123)     7582 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/source_hypothesis.py
--rw-r--r--   0 runner    (1001) docker     (123)    45323 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/storage.py
--rw-r--r--   0 runner    (1001) docker     (123)     5120 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/test_statistic.py
--rw-r--r--   0 runner    (1001) docker     (123)     3846 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/times.py
--rw-r--r--   0 runner    (1001) docker     (123)     8218 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/timing.py
--rw-r--r--   0 runner    (1001) docker     (123)    30990 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/trialdata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:48.431640 skyllh-23.1.1/skyllh/core/utils/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8777 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/utils/multidimgridpdf.py
--rw-r--r--   0 runner    (1001) docker     (123)     4066 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/utils/ndphotosplinepdf.py
--rw-r--r--   0 runner    (1001) docker     (123)     4335 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/core/utils/trials.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:48.431640 skyllh-23.1.1/skyllh/datasets/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/datasets/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:48.431640 skyllh-23.1.1/skyllh/datasets/i3/
--rw-r--r--   0 runner    (1001) docker     (123)    23450 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/datasets/i3/PublicData_10y_ps.py
--rw-r--r--   0 runner    (1001) docker     (123)    24084 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/datasets/i3/PublicData_10y_ps_wMC.py
--rw-r--r--   0 runner    (1001) docker     (123)    25463 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/datasets/i3/PublicData_10y_ps_wMCEq.py
--rw-r--r--   0 runner    (1001) docker     (123)      310 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/datasets/i3/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:48.431640 skyllh-23.1.1/skyllh/i3/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/i3/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2818 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/i3/background_generation.py
--rw-r--r--   0 runner    (1001) docker     (123)    14703 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/i3/backgroundpdf.py
--rw-r--r--   0 runner    (1001) docker     (123)      500 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/i3/config.py
--rw-r--r--   0 runner    (1001) docker     (123)     2030 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/i3/coords.py
--rw-r--r--   0 runner    (1001) docker     (123)    15327 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/i3/dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)    28806 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/i3/detsigyield.py
--rw-r--r--   0 runner    (1001) docker     (123)     2259 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/i3/livetime.py
--rw-r--r--   0 runner    (1001) docker     (123)     9915 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/i3/pdf.py
--rw-r--r--   0 runner    (1001) docker     (123)    11383 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/i3/pdfratio.py
--rw-r--r--   0 runner    (1001) docker     (123)     4325 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/i3/scrambling.py
--rw-r--r--   0 runner    (1001) docker     (123)    20331 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/i3/signal_generation.py
--rw-r--r--   0 runner    (1001) docker     (123)    10347 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/i3/signalpdf.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:48.431640 skyllh-23.1.1/skyllh/i3/utils/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/i3/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    16732 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/i3/utils/sensitivity.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:48.431640 skyllh-23.1.1/skyllh/physics/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/physics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    22865 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/physics/flux.py
--rw-r--r--   0 runner    (1001) docker     (123)    55097 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/physics/flux_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     3775 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/physics/model.py
--rw-r--r--   0 runner    (1001) docker     (123)     9872 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/physics/source.py
--rw-r--r--   0 runner    (1001) docker     (123)     8080 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/physics/time_profile.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:48.435640 skyllh-23.1.1/skyllh/plotting/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/plotting/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:48.435640 skyllh-23.1.1/skyllh/plotting/core/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/plotting/core/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5748 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/plotting/core/pdfratio.py
--rw-r--r--   0 runner    (1001) docker     (123)     6196 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/plotting/core/signalpdf.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:48.435640 skyllh-23.1.1/skyllh/plotting/i3/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/plotting/i3/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3986 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/plotting/i3/backgroundpdf.py
--rw-r--r--   0 runner    (1001) docker     (123)     4226 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/plotting/i3/pdf.py
--rw-r--r--   0 runner    (1001) docker     (123)     4945 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/plotting/i3/pdfratio.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:48.435640 skyllh-23.1.1/skyllh/plotting/utils/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/plotting/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    15164 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/plotting/utils/trials.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:48.435640 skyllh-23.1.1/skyllh/utils/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1198 2023-06-23 14:37:38.000000 skyllh-23.1.1/skyllh/utils/spline.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-23 14:37:48.423640 skyllh-23.1.1/skyllh.egg-info/
--rw-r--r--   0 runner    (1001) docker     (123)     3098 2023-06-23 14:37:48.000000 skyllh-23.1.1/skyllh.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)     3422 2023-06-23 14:37:48.000000 skyllh-23.1.1/skyllh.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-06-23 14:37:48.000000 skyllh-23.1.1/skyllh.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (123)       39 2023-06-23 14:37:48.000000 skyllh-23.1.1/skyllh.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (123)        7 2023-06-23 14:37:48.000000 skyllh-23.1.1/skyllh.egg-info/top_level.txt
--rw-r--r--   0 runner    (1001) docker     (123)    83607 2023-06-23 14:37:38.000000 skyllh-23.1.1/versioneer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:17.332327 skyllh-23.2.0/
+-rw-r--r--   0 runner    (1001) docker     (123)    35149 2023-07-17 13:59:04.000000 skyllh-23.2.0/LICENSE.txt
+-rw-r--r--   0 runner    (1001) docker     (123)     3411 2023-07-17 13:59:17.332327 skyllh-23.2.0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)     2272 2023-07-17 13:59:04.000000 skyllh-23.2.0/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)      104 2023-07-17 13:59:04.000000 skyllh-23.2.0/pyproject.toml
+-rw-r--r--   0 runner    (1001) docker     (123)     1351 2023-07-17 13:59:17.332327 skyllh-23.2.0/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (123)      135 2023-07-17 13:59:04.000000 skyllh-23.2.0/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:17.332327 skyllh-23.2.0/skyllh/
+-rw-r--r--   0 runner    (1001) docker     (123)      763 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      498 2023-07-17 13:59:17.332327 skyllh-23.2.0/skyllh/_version.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:17.296327 skyllh-23.2.0/skyllh/analyses/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/analyses/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:17.296327 skyllh-23.2.0/skyllh/analyses/i3/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/analyses/i3/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:17.296327 skyllh-23.2.0/skyllh/analyses/i3/publicdata_ps/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/analyses/i3/publicdata_ps/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13563 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/analyses/i3/publicdata_ps/aeff.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22656 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/analyses/i3/publicdata_ps/backgroundpdf.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13564 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/analyses/i3/publicdata_ps/bkg_flux.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9399 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/analyses/i3/publicdata_ps/detsigyield.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16254 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/analyses/i3/publicdata_ps/mcbkg_ps.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14433 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/analyses/i3/publicdata_ps/pdfratio.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24390 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/analyses/i3/publicdata_ps/signal_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14419 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/analyses/i3/publicdata_ps/signalpdf.py
+-rw-r--r--   0 runner    (1001) docker     (123)    31088 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/analyses/i3/publicdata_ps/smearing_matrix.py
+-rw-r--r--   0 runner    (1001) docker     (123)    37924 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/analyses/i3/publicdata_ps/time_dependent_ps.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16645 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/analyses/i3/publicdata_ps/time_integrated_ps.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10497 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/analyses/i3/publicdata_ps/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:17.300327 skyllh-23.2.0/skyllh/cluster/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/cluster/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2354 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/cluster/commands.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3888 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/cluster/compute_node.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3939 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/cluster/master_node.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3338 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/cluster/srvclt.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:17.324327 skyllh-23.2.0/skyllh/core/
+-rw-r--r--   0 runner    (1001) docker     (123)      305 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    73187 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/analysis.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22168 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/background_generation.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13329 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/background_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4321 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/backgroundpdf.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12382 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/binning.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1947 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/catalog.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12233 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2728 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/datafields.py
+-rw-r--r--   0 runner    (1001) docker     (123)    70878 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3965 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/debugging.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12215 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/detsigyield.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1098 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/display.py
+-rw-r--r--   0 runner    (1001) docker     (123)    47816 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/event_selection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5269 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/expectation_maximization.py
+-rw-r--r--   0 runner    (1001) docker     (123)    78360 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/flux_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23411 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/interpolate.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14965 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/livetime.py
+-rw-r--r--   0 runner    (1001) docker     (123)    53413 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/llhratio.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4059 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/math.py
+-rw-r--r--   0 runner    (1001) docker     (123)    35393 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/minimizer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:17.324327 skyllh-23.2.0/skyllh/core/minimizers/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/minimizers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8129 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/minimizers/iminuit.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5036 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/model.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15441 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/multiproc.py
+-rw-r--r--   0 runner    (1001) docker     (123)    72200 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/parameters.py
+-rw-r--r--   0 runner    (1001) docker     (123)    63872 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/pdf.py
+-rw-r--r--   0 runner    (1001) docker     (123)    37645 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/pdfratio.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11089 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/pdfratio_fill.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7407 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/progressbar.py
+-rw-r--r--   0 runner    (1001) docker     (123)    26326 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/py.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2238 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/random.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9480 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/scrambling.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22626 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/services.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1187 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/session.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4603 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/signal_generation.py
+-rw-r--r--   0 runner    (1001) docker     (123)    26328 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/signal_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)    37750 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/signalpdf.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7160 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/smoothing.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15359 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/source_hypo_grouping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9770 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/source_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)    56248 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/storage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7126 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/test_statistic.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4523 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/times.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8584 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/timing.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2153 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/tool.py
+-rw-r--r--   0 runner    (1001) docker     (123)    44097 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/trialdata.py
+-rw-r--r--   0 runner    (1001) docker     (123)      288 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/types.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:17.324327 skyllh-23.2.0/skyllh/core/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    57491 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/utils/analysis.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4517 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/utils/coords.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2442 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/utils/flux_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10269 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/utils/multidimgridpdf.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8071 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/utils/spline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4333 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/core/utils/trials.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:17.324327 skyllh-23.2.0/skyllh/datasets/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/datasets/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:17.324327 skyllh-23.2.0/skyllh/datasets/i3/
+-rw-r--r--   0 runner    (1001) docker     (123)    23252 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/datasets/i3/PublicData_10y_ps.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23886 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/datasets/i3/PublicData_10y_ps_wMC.py
+-rw-r--r--   0 runner    (1001) docker     (123)      242 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/datasets/i3/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:17.328327 skyllh-23.2.0/skyllh/i3/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/i3/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3089 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/i3/background_generation.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16964 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/i3/backgroundpdf.py
+-rw-r--r--   0 runner    (1001) docker     (123)      667 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/i3/config.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16256 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/i3/dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)    41776 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/i3/detsigyield.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3556 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/i3/livetime.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11996 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/i3/pdf.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20982 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/i3/pdfratio.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4856 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/i3/scrambling.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14423 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/i3/signal_generation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9974 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/i3/signalpdf.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:17.328327 skyllh-23.2.0/skyllh/i3/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/i3/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16788 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/i3/utils/analysis.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2351 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/i3/utils/coords.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:17.328327 skyllh-23.2.0/skyllh/plotting/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/plotting/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:17.328327 skyllh-23.2.0/skyllh/plotting/core/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/plotting/core/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5820 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/plotting/core/pdfratio.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6604 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/plotting/core/signalpdf.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:17.328327 skyllh-23.2.0/skyllh/plotting/i3/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/plotting/i3/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4151 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/plotting/i3/backgroundpdf.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4399 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/plotting/i3/pdf.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5095 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/plotting/i3/pdfratio.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:17.332327 skyllh-23.2.0/skyllh/plotting/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/plotting/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15387 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/plotting/utils/trials.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:17.332327 skyllh-23.2.0/skyllh/scripting/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/scripting/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4810 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/scripting/argparser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2261 2023-07-17 13:59:04.000000 skyllh-23.2.0/skyllh/scripting/logging.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-17 13:59:17.296327 skyllh-23.2.0/skyllh.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)     3411 2023-07-17 13:59:17.000000 skyllh-23.2.0/skyllh.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)     3461 2023-07-17 13:59:17.000000 skyllh-23.2.0/skyllh.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-07-17 13:59:17.000000 skyllh-23.2.0/skyllh.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       25 2023-07-17 13:59:17.000000 skyllh-23.2.0/skyllh.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        7 2023-07-17 13:59:17.000000 skyllh-23.2.0/skyllh.egg-info/top_level.txt
+-rw-r--r--   0 runner    (1001) docker     (123)    83607 2023-07-17 13:59:04.000000 skyllh-23.2.0/versioneer.py
```

### Comparing `skyllh-23.1.1/LICENSE.txt` & `skyllh-23.2.0/LICENSE.txt`

 * *Files identical despite different names*

### Comparing `skyllh-23.1.1/PKG-INFO` & `skyllh-23.2.0/PKG-INFO`

 * *Files 14% similar despite different names*

```diff
@@ -1,24 +1,24 @@
 Metadata-Version: 2.1
 Name: skyllh
-Version: 23.1.1
+Version: 23.2.0
 Summary: The SkyLLH framework is an open-source Python3-based package licensed under the GPLv3 license. It provides a modular framework for implementing custom likelihood functions and executing log-likelihood ratio hypothesis tests. The idea is to provide a class structure tied to the mathematical objects of the likelihood functions.
 Home-page: https://github.com/icecube/skyllh
 Author: Martin Wolf
 Author-email: martin.wolf@icecube.wisc.edu
 License: GPL-3+
 Project-URL: Bug Tracker, https://github.com/icecube/skyllh/issues
 Project-URL: Documentation, https://icecube.github.io/skyllh
 Project-URL: Source Code, https://github.com/icecube/skyllh
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: Environment :: Console
 Classifier: Intended Audience :: Science/Research
 Classifier: License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)
 Classifier: Operating System :: POSIX
-Classifier: Programming Language :: Python :: 3.6
+Classifier: Programming Language :: Python :: 3.8
 Classifier: Topic :: Scientific/Engineering :: Physics
 Description-Content-Type: text/markdown
 License-File: LICENSE.txt
 
 # SkyLLH
 
 [![Tests](https://github.com/icecube/skyllh/actions/workflows/pythonpackage.yml/badge.svg)](#)
@@ -36,20 +36,20 @@
 The latest `skyllh` release can be installed from [PyPI](https://pypi.org/project/skyllh/) repository:
 ```bash
 pip install skyllh
 ```
 
 The current development version can be installed using pip:
 ```bash
-pip install git+https://github.com/icecube/skyllh.git#egg=skyllh 
+pip install git+https://github.com/icecube/skyllh.git#egg=skyllh
 ```
 
 Optionally, the editable package version with a specified reference can be installed by:
 ```bash
-pip install -e git+https://github.com/icecube/skyllh.git@[ref]#egg=skyllh 
+pip install -e git+https://github.com/icecube/skyllh.git@[ref]#egg=skyllh
 ```
 where
 - `-e` is an editable flag
 - `[ref]` is an optional argument containing a specific commit hash, branch name or tag
 
 ## Cloning from GitHub
 
@@ -58,10 +58,19 @@
 ```python
 import sys
 
 sys.path.insert(0, '/path/to/skyllh')
 sys.path.insert(0, '/path/to/i3skyllh')  # optional
 ```
 
+# Publications
+
+Several publications about the SkyLLH software are available:
+
+- IceCube Collaboration, T. Kontrimas, M. Wolf, et al. PoS ICRC2021 (2022) 1073
+  [DOI](http://doi.org/10.22323/1.395.1073)
+- IceCube Collaboration, M. Wolf, et al. PoS ICRC2019 (2020) 1035
+  [DOI](https://doi.org/10.22323/1.358.1035)
+
 # i3skyllh
 
 The [`i3skyllh`](https://github.com/icecube/i3skyllh) package provides complementary pre-defined common analyses and datasets for the [IceCube Neutrino Observatory](https://icecube.wisc.edu) detector in a private [repository](https://github.com/icecube/i3skyllh).
```

### Comparing `skyllh-23.1.1/README.md` & `skyllh-23.2.0/README.md`

 * *Files 16% similar despite different names*

```diff
@@ -15,20 +15,20 @@
 The latest `skyllh` release can be installed from [PyPI](https://pypi.org/project/skyllh/) repository:
 ```bash
 pip install skyllh
 ```
 
 The current development version can be installed using pip:
 ```bash
-pip install git+https://github.com/icecube/skyllh.git#egg=skyllh 
+pip install git+https://github.com/icecube/skyllh.git#egg=skyllh
 ```
 
 Optionally, the editable package version with a specified reference can be installed by:
 ```bash
-pip install -e git+https://github.com/icecube/skyllh.git@[ref]#egg=skyllh 
+pip install -e git+https://github.com/icecube/skyllh.git@[ref]#egg=skyllh
 ```
 where
 - `-e` is an editable flag
 - `[ref]` is an optional argument containing a specific commit hash, branch name or tag
 
 ## Cloning from GitHub
 
@@ -37,10 +37,19 @@
 ```python
 import sys
 
 sys.path.insert(0, '/path/to/skyllh')
 sys.path.insert(0, '/path/to/i3skyllh')  # optional
 ```
 
+# Publications
+
+Several publications about the SkyLLH software are available:
+
+- IceCube Collaboration, T. Kontrimas, M. Wolf, et al. PoS ICRC2021 (2022) 1073
+  [DOI](http://doi.org/10.22323/1.395.1073)
+- IceCube Collaboration, M. Wolf, et al. PoS ICRC2019 (2020) 1035
+  [DOI](https://doi.org/10.22323/1.358.1035)
+
 # i3skyllh
 
 The [`i3skyllh`](https://github.com/icecube/i3skyllh) package provides complementary pre-defined common analyses and datasets for the [IceCube Neutrino Observatory](https://icecube.wisc.edu) detector in a private [repository](https://github.com/icecube/i3skyllh).
```

### Comparing `skyllh-23.1.1/setup.cfg` & `skyllh-23.2.0/setup.cfg`

 * *Files 18% similar despite different names*

```diff
@@ -2,47 +2,36 @@
 name = skyllh
 description = The SkyLLH framework is an open-source Python3-based package licensed under the GPLv3 license. It provides a modular framework for implementing custom likelihood functions and executing log-likelihood ratio hypothesis tests. The idea is to provide a class structure tied to the mathematical objects of the likelihood functions.
 long_description = file:README.md
 long_description_content_type = text/markdown
 url = https://github.com/icecube/skyllh
 author_email = martin.wolf@icecube.wisc.edu
 author = Martin Wolf
-requires_python = >=3.6.0
+requires_python = >=3.8.0
 license = GPL-3+
 classifiers = 
 	Development Status :: 5 - Production/Stable
 	Environment :: Console
 	Intended Audience :: Science/Research
 	License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)
 	Operating System :: POSIX
-	Programming Language :: Python :: 3.6
+	Programming Language :: Python :: 3.8
 	Topic :: Scientific/Engineering :: Physics
 project_urls = 
 	Bug Tracker = https://github.com/icecube/skyllh/issues
 	Documentation = https://icecube.github.io/skyllh
 	Source Code = https://github.com/icecube/skyllh
 
 [options]
 packages = find:
 install_requires = 
 	astropy
 	numpy
 	scipy
-	iminuit
-	matplotlib
-tests_require = 
-	pytest
-	pytest-codecov
-
-[tool:pytest]
-addopts = --color=yes --cov=skyllh --cov-report=term -ra --ignore=test --ignore=skyllh/_version.py
-log_cli = 1
-log_cli_level = INFO
-testpaths = test
-norecursedirs = test
+	tqdm
 
 [versioneer]
 VCS = git
 style = pep440
 versionfile_source = skyllh/_version.py
 versionfile_build = skyllh/_version.py
 tag_prefix = v
```

### Comparing `skyllh-23.1.1/skyllh/__init__.py` & `skyllh-23.2.0/skyllh/__init__.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,21 +1,24 @@
 # -*- coding: utf-8 -*-
 
+import logging
+import multiprocessing as mp
+
+from . import _version
+
 # Initialize top-level logger with a do-nothing NullHandler. It is required to
 # be able to log messages when user has not set up any handler for the logger.
-import logging
 logging.getLogger(__name__).addHandler(logging.NullHandler())
 
 # Change macOS default multiprocessing start method 'spawn' to 'fork'.
-import multiprocessing as mp
+
 try:
     mp.set_start_method("fork")
-except:
+except Exception:
     # It could be already set by another package.
     if mp.get_start_method() != "fork":
         logging.warning(
             "Couldn't set the multiprocessing start method to 'fork'. "
             "Parallel calculations using 'ncpu' argument != 1 may break."
         )
 
-from . import _version
 __version__ = _version.get_versions()['version']
```

### Comparing `skyllh-23.1.1/skyllh/analyses/i3/publicdata_ps/aeff.py` & `skyllh-23.2.0/skyllh/analyses/i3/publicdata_ps/aeff.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,21 +1,26 @@
 # -*- coding: utf-8 -*-
 
 import numpy as np
 
-from scipy import interpolate
-from scipy import integrate
+from scipy import (
+    integrate,
+    interpolate,
+)
 
+from skyllh.analyses.i3.publicdata_ps.utils import (
+    FctSpline2D,
+)
 from skyllh.core.binning import (
-    get_bincenters_from_binedges,
     get_bin_indices_from_lower_and_upper_binedges,
+    get_bincenters_from_binedges,
+)
+from skyllh.core.storage import (
+    create_FileLoader,
 )
-from skyllh.core.storage import create_FileLoader
-
-from skyllh.analyses.i3.publicdata_ps.utils import FctSpline2D
 
 
 def load_effective_area_array(pathfilenames):
     """Loads the (nbins_decnu, nbins_log10enu)-shaped 2D effective
     area array from the given data file.
 
     Parameters
@@ -54,20 +59,22 @@
 
     # Determine the binning for energy and declination.
     log10_enu_binedges_lower = np.unique(data['log10_enu_min'])
     log10_enu_binedges_upper = np.unique(data['log10_enu_max'])
     decnu_binedges_lower = np.unique(data['decnu_min'])
     decnu_binedges_upper = np.unique(data['decnu_max'])
 
-    if(len(log10_enu_binedges_lower) != len(log10_enu_binedges_upper)):
-        raise ValueError('Cannot extract the log10(E/GeV) binning of the '
+    if len(log10_enu_binedges_lower) != len(log10_enu_binedges_upper):
+        raise ValueError(
+            'Cannot extract the log10(E/GeV) binning of the '
             'effective area from data file "{}". The number of lower and upper '
             'bin edges is not equal!'.format(str(loader.pathfilename_list)))
-    if(len(decnu_binedges_lower) != len(decnu_binedges_upper)):
-        raise ValueError('Cannot extract the dec_nu binning of the effective '
+    if len(decnu_binedges_lower) != len(decnu_binedges_upper):
+        raise ValueError(
+            'Cannot extract the dec_nu binning of the effective '
             'area from data file "{}". The number of lower and upper bin edges '
             'is not equal!'.format(str(loader.pathfilename_list)))
 
     nbins_log10_enu = len(log10_enu_binedges_lower)
     nbins_decnu = len(decnu_binedges_lower)
 
     # Construct the 2d array for the effective area.
@@ -136,15 +143,15 @@
         ) = load_effective_area_array(pathfilenames)
 
         # Note: self._aeff_decnu_log10enu is numpy 2D ndarray of shape
         # (nbins_decnu, nbins_log10enu).
 
         # Cut the energies where all effective areas are zero.
         m = np.sum(self._aeff_decnu_log10enu, axis=0) > 0
-        self._aeff_decnu_log10enu = self._aeff_decnu_log10enu[:,m]
+        self._aeff_decnu_log10enu = self._aeff_decnu_log10enu[:, m]
         self._log10_enu_binedges_lower = self._log10_enu_binedges_lower[m]
         self._log10_enu_binedges_upper = self._log10_enu_binedges_upper[m]
 
         self._decnu_binedges = np.concatenate(
             (self._decnu_binedges_lower,
              self._decnu_binedges_upper[-1:])
         )
@@ -170,23 +177,24 @@
                     self._log10_enu_binedges_upper[-1],
                     max_log10enu)
 
             m = (
                 (self.log10_enu_bincenters >= min_log10enu) &
                 (self.log10_enu_bincenters < max_log10enu)
             )
-            bin_centers = self.log10_enu_bincenters[m]
             low_bin_edges = self._log10_enu_binedges_lower[m]
             high_bin_edges = self._log10_enu_binedges_upper[m]
 
             # Get the detection probability P(E_nu | sin(dec)) per bin.
             self.det_prob = self.get_detection_prob_for_decnu(
                 src_dec,
-                10**low_bin_edges, 10**high_bin_edges,
-                10**low_bin_edges[0], 10**high_bin_edges[-1]
+                10**low_bin_edges,
+                10**high_bin_edges,
+                10**low_bin_edges[0],
+                10**high_bin_edges[-1]
             )
 
     @property
     def decnu_binedges(self):
         """(read-only) The bin edges of the neutrino declination axis in
         radians.
         """
@@ -216,14 +224,28 @@
     def log10_enu_binedges(self):
         """(read-only) The bin edges of the log10(E_nu/GeV) neutrino energy
         axis.
         """
         return self._log10_enu_binedges
 
     @property
+    def log10_enu_binedges_lower(self):
+        """(read-only) The lower binedges of the log10(E_nu/GeV) neutrino energy
+        axis.
+        """
+        return self._log10_enu_binedges_lower
+
+    @property
+    def log10_enu_binedges_upper(self):
+        """(read-only) The upper binedges of the log10(E_nu/GeV) neutrino energy
+        axis.
+        """
+        return self._log10_enu_binedges_upper
+
+    @property
     def log10_enu_bincenters(self):
         """(read-only) The bin center values of the log10(E_nu/GeV) neutrino
         energy axis.
         """
         return get_bincenters_from_binedges(self._log10_enu_binedges)
 
     @property
@@ -354,15 +376,15 @@
         norm = integrate.quad(
             _eval_spl_func,
             enu_range_min,
             enu_range_max,
             limit=200,
             full_output=1
         )[0]
-        
+
         enu_min = np.atleast_1d(enu_min)
         enu_max = np.atleast_1d(enu_max)
 
         det_prob = np.empty((len(enu_min),), dtype=np.double)
         for i in range(len(enu_min)):
             integral = integrate.quad(
                 _eval_spl_func,
@@ -371,8 +393,7 @@
                 limit=200,
                 full_output=1
             )[0]
 
             det_prob[i] = integral / norm
 
         return det_prob
-
```

### Comparing `skyllh-23.1.1/skyllh/analyses/i3/publicdata_ps/backgroundpdf.py` & `skyllh-23.2.0/skyllh/i3/backgroundpdf.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,450 +1,480 @@
 # -*- coding: utf-8 -*-
 
+import scipy.interpolate
+
 import numpy as np
 
 from skyllh.core.binning import (
-    BinningDefinition,
     UsesBinning,
 )
 from skyllh.core.pdf import (
-    EnergyPDF,
     IsBackgroundPDF,
-    PDFAxis
+    SpatialPDF,
 )
-from skyllh.core.storage import DataFieldRecordArray
-from skyllh.core.timing import TaskTimer
-from skyllh.core.smoothing import (
-    UNSMOOTH_AXIS,
-    SmoothingFilter,
-    HistSmoothingMethod,
-    NoHistSmoothingMethod,
-    NeighboringBinHistSmoothingMethod
+from skyllh.core.py import (
+    classname,
+    int_cast,
+    issequence,
+    issequenceof,
+)
+from skyllh.core.storage import (
+    DataFieldRecordArray,
+)
+from skyllh.core.timing import (
+    TaskTimer,
+)
+from skyllh.i3.pdf import (
+    I3EnergyPDF,
 )
-from skyllh.core.timing import TaskTimer
-
-from scipy.stats import gaussian_kde
 
 
-class PDEnergyPDF(EnergyPDF, UsesBinning):
-    """This is the base class for IceCube specific energy PDF models.
-    IceCube energy PDFs depend solely on the energy and the
-    zenith angle, and hence, on the declination of the event.
+class BackgroundI3SpatialPDF(
+        SpatialPDF,
+        UsesBinning,
+        IsBackgroundPDF,
+):
+    """This is the base class for all IceCube specific spatial background PDF
+    models. IceCube spatial background PDFs depend solely on the zenith angle,
+    and hence, on the declination of the event.
 
-    The IceCube energy PDF is modeled as a 1d histogram in energy,
-    but for different sin(declination) bins, hence, stored as a 2d histogram.
+    The IceCube spatial background PDF is modeled as a 1d spline function in
+    sin(declination).
     """
-
-    _KDE_BW_NORTH = 0.4
-    _KDE_BW_SOUTH = 0.32
-
-    def __init__(self, data_logE, data_sinDec, data_mcweight, data_physicsweight,
-                 logE_binning, sinDec_binning, smoothing_filter, kde_smoothing=False):
-        """Creates a new IceCube energy PDF object.
+    def __init__(
+            self,
+            data_sin_dec,
+            data_weights,
+            sin_dec_binning,
+            spline_order_sin_dec,
+            **kwargs,
+    ):
+        """Creates a new IceCube spatial background PDF object.
 
         Parameters
         ----------
-        data_logE : 1d ndarray
-            The array holding the log10(E) values of the events.
-        data_sinDec : 1d ndarray
+        data_sin_dec : 1d ndarray
             The array holding the sin(dec) values of the events.
-        data_mcweight : 1d ndarray
-            The array holding the monte-carlo weights of the events.
-            The final data weight will be the product of data_mcweight and
-            data_physicsweight.
-        data_physicsweight : 1d ndarray
-            The array holding the physics weights of the events.
-            The final data weight will be the product of data_mcweight and
-            data_physicsweight.
-        logE_binning : BinningDefinition
-            The binning definition for the log(E) axis.
-        sinDec_binning : BinningDefinition
+        data_weights : 1d ndarray
+            The array holding the weight of each event used for histogramming.
+        sin_dec_binning : BinningDefinition
             The binning definition for the sin(declination) axis.
-        smoothing_filter : SmoothingFilter instance | None
-            The smoothing filter to use for smoothing the energy histogram.
-            If None, no smoothing will be applied.
-        kde_smoothing : bool
-            Apply a kde smoothing to the energy pdf for each bin in sin(dec).
-            This is useful for signal injections, because it ensures that the
-            background is not zero when injecting high energy events.
-            Default: False.
-        """
-        super(PDEnergyPDF, self).__init__()
+        spline_order_sin_dec : int
+            The order of the spline function for the logarithmic values of the
+            spatial background PDF along the sin(dec) axis.
+        """
+        super().__init__(
+            pmm=None,
+            ra_range=(0, 2*np.pi),
+            dec_range=(
+                np.arcsin(sin_dec_binning.lower_edge),
+                np.arcsin(sin_dec_binning.upper_edge)),
+            **kwargs)
+
+        self.add_binning(sin_dec_binning, 'sin_dec')
+        self.spline_order_sin_dec = spline_order_sin_dec
+
+        (h, bins) = np.histogram(
+            data_sin_dec,
+            bins=sin_dec_binning.binedges,
+            weights=data_weights,
+            range=sin_dec_binning.range)
+
+        # Save original histogram.
+        self._orig_hist = h
+
+        # Normalize histogram to get PDF.
+        h = h / h.sum() / (bins[1:] - bins[:-1])
+
+        # Check if there are any NaN values.
+        if np.any(np.isnan(h)):
+            nan_bcs = sin_dec_binning.bincenters[np.isnan(h)]
+            raise ValueError(
+                'The declination histogram contains NaN values! Check your '
+                'sin(dec) binning! The bins with NaN values are: '
+                f'{nan_bcs}')
 
-        # Define the PDF axes.
-        self.add_axis(PDFAxis(name='log_energy',
-                              vmin=logE_binning.lower_edge,
-                              vmax=logE_binning.upper_edge))
-        self.add_axis(PDFAxis(name='sin_dec',
-                              vmin=sinDec_binning.lower_edge,
-                              vmax=sinDec_binning.upper_edge))
-
-        self.add_binning(logE_binning, 'log_energy')
-        self.add_binning(sinDec_binning, 'sin_dec')
-
-        # Create the smoothing method instance tailored to the energy PDF.
-        # We will smooth only the first axis (logE).
-        if((smoothing_filter is not None) and
-           (not isinstance(smoothing_filter, SmoothingFilter))):
-            raise TypeError(
-                'The smoothing_filter argument must be None or an instance of SmoothingFilter!')
-        if(smoothing_filter is None):
-            self.hist_smoothing_method = NoHistSmoothingMethod()
-        else:
-            self.hist_smoothing_method = NeighboringBinHistSmoothingMethod(
-                (smoothing_filter.axis_kernel_array, UNSMOOTH_AXIS))
-
-        # We have to figure out, which histogram bins are zero due to no
-        # monte-carlo coverage, and which due to zero physics model
-        # contribution.
-
-        # Create a 2D histogram with only the MC events to determine the MC
-        # coverage.
-        (h, bins_logE, bins_sinDec) = np.histogram2d(
-            data_logE, data_sinDec,
-            bins=[
-                logE_binning.binedges, sinDec_binning.binedges],
-            range=[
-                logE_binning.range, sinDec_binning.range],
-            density=False)
-        h = self._hist_smoothing_method.smooth(h)
-        self._hist_mask_mc_covered = h > 0
-
-        # Select the events which have MC coverage but zero physics
-        # contribution, i.e. the physics model predicts zero contribution.
-        mask = data_physicsweight == 0.
-
-        # Create a 2D histogram with only the MC events that have zero physics
-        # contribution. Note: By construction the zero physics contribution bins
-        # are a subset of the MC covered bins.
-        (h, bins_logE, bins_sinDec) = np.histogram2d(
-            data_logE[mask], data_sinDec[mask],
-            bins=[
-                logE_binning.binedges, sinDec_binning.binedges],
-            range=[
-                logE_binning.range, sinDec_binning.range],
-            density=False)
-        h = self._hist_smoothing_method.smooth(h)
-        self._hist_mask_mc_covered_zero_physics = h > 0
-
-        if kde_smoothing:
-            # If a bandwidth is passed, apply a KDE-based smoothing with the given
-            # bw parameter as bandwidth for the fit.
-            if not isinstance(kde_smoothing, bool):
-                raise ValueError(
-                    "The bandwidth parameter must be True or False!")
-            kde_pdf = np.empty(
-                (len(sinDec_binning.bincenters),), dtype=object)
-            data_logE_masked = data_logE[~mask]
-            data_sinDec_masked = data_sinDec[~mask]
-            for i in range(len(sinDec_binning.bincenters)):
-                sindec_mask = np.logical_and(
-                    data_sinDec_masked >= sinDec_binning.binedges[i],
-                    data_sinDec_masked < sinDec_binning.binedges[i+1]
-                )
-                this_energy = data_logE_masked[sindec_mask]
-                if sinDec_binning.binedges[i] >= 0:
-                    kde_pdf[i] = gaussian_kde(
-                        this_energy, bw_method=self._KDE_BW_NORTH)
-                else:
-                    kde_pdf[i] = gaussian_kde(
-                        this_energy, bw_method=self._KDE_BW_SOUTH)
-            h = np.vstack(
-                [kde_pdf[i].evaluate(logE_binning.bincenters)
-                 for i in range(len(sinDec_binning.bincenters))]).T
-
-        else:
-            # Create a 2D histogram with only the data which has physics
-            # contribution. We will do the normalization along the logE
-            # axis manually.
-            data_weights = data_mcweight[~mask] * data_physicsweight[~mask]
-            (h, bins_logE, bins_sinDec) = np.histogram2d(
-                data_logE[~mask], data_sinDec[~mask],
-                bins=[
-                    logE_binning.binedges, sinDec_binning.binedges],
-                weights=data_weights,
-                range=[
-                    logE_binning.range, sinDec_binning.range],
-                density=False)
-
-        # Calculate the normalization for each logE bin. Hence we need to sum
-        # over the logE bins (axis 0) for each sin(dec) bin and need to divide
-        # by the logE bin widths along the sin(dec) bins. The result array norm
-        # is a 2D array of the same shape as h.
-        norms = np.sum(h, axis=(0,))[np.newaxis, ...] * \
-            np.diff(logE_binning.binedges)[..., np.newaxis]
-        h /= norms
-        h = self._hist_smoothing_method.smooth(h)
+        if np.any(h <= 0.):
+            empty_bcs = sin_dec_binning.bincenters[h <= 0.]
+            raise ValueError(
+                'Some declination histogram bins for the spatial background '
+                'PDF are empty, this must not happen! The empty bins are: '
+                f'{empty_bcs}')
+
+        # Create the logarithmic spline.
+        self._log_spline = scipy.interpolate.InterpolatedUnivariateSpline(
+            sin_dec_binning.bincenters, np.log(h), k=self.spline_order_sin_dec)
 
-        self._hist_logE_sinDec = h
+        # Save original spline.
+        self._orig_log_spline = self._log_spline
 
     @property
-    def hist_smoothing_method(self):
-        """The HistSmoothingMethod instance defining the smoothing filter of the
-        energy PDF histogram.
-        """
-        return self._hist_smoothing_method
+    def spline_order_sin_dec(self):
+        """The order (int) of the logarithmic spline function, that splines the
+        background PDF, along the sin(dec) axis.
+        """
+        return self._spline_order_sin_dec
+
+    @spline_order_sin_dec.setter
+    def spline_order_sin_dec(self, order):
+        self._spline_order_sin_dec = int_cast(
+            order,
+            'The spline_order_sin_dec property must be castable to type int!')
+
+    def add_events(self, events):
+        """Add events to spatial background PDF object and recalculate
+        logarithmic spline function.
 
-    @hist_smoothing_method.setter
-    def hist_smoothing_method(self, method):
-        if(not isinstance(method, HistSmoothingMethod)):
-            raise TypeError(
-                'The hist_smoothing_method property must be an instance of HistSmoothingMethod!')
-        self._hist_smoothing_method = method
+        Parameters
+        ----------
+        events : numpy record ndarray
+            The array holding the event data. The following data fields must
+            exist:
 
-    @property
-    def hist(self):
-        """(read-only) The 2D logE-sinDec histogram array.
-        """
-        return self._hist_logE_sinDec
+                sin_dec : float
+                    The sin(declination) value of the event.
 
-    @property
-    def hist_mask_mc_covered(self):
-        """(read-only) The boolean ndarray holding the mask of the 2D histogram
-        bins for which there is monte-carlo coverage.
         """
-        return self._hist_mask_mc_covered
+        data_sin_dec = events['sin_dec']
 
-    @property
-    def hist_mask_mc_covered_zero_physics(self):
-        """(read-only) The boolean ndarray holding the mask of the 2D histogram
-        bins for which there is monte-carlo coverage but zero physics
-        contribution.
+        sin_dec_binning = self.get_binning('sin_dec')
+
+        (h_upd, bins) = np.histogram(
+            data_sin_dec,
+            bins=sin_dec_binning.binedges,
+            range=sin_dec_binning.range)
+
+        # Construct histogram with added events.
+        h = self._orig_hist + h_upd
+
+        # Normalize histogram to get PDF.
+        h = h / h.sum() / (bins[1:] - bins[:-1])
+
+        # Create the updated logarithmic spline.
+        self._log_spline = scipy.interpolate.InterpolatedUnivariateSpline(
+            sin_dec_binning.bincenters, np.log(h), k=self.spline_order_sin_dec)
+
+    def reset(self):
+        """Reset the logarithmic spline to the original function, which was
+        calculated when the object was initialized.
         """
-        return self._hist_mask_mc_covered_zero_physics
+        self._log_spline = self._orig_log_spline
 
-    @property
-    def hist_mask_mc_covered_with_physics(self):
-        """(read-only) The boolean ndarray holding the mask of the 2D histogram
-        bins for which there is monte-carlo coverage and has physics
-        contribution.
+    def initialize_for_new_trial(
+            self,
+            tdm,
+            tl=None,
+            **kwargs):
+        """Pre-cumputes the probability density values when new trial data is
+        available.
         """
-        return self._hist_mask_mc_covered & ~self._hist_mask_mc_covered_zero_physics
+        with TaskTimer(tl, 'Evaluating bkg log-spline.'):
+            log_spline_val = self._log_spline(tdm.get_data('sin_dec'))
+
+        self._pd = 0.5 / np.pi * np.exp(log_spline_val)
 
-    def get_prob(self, tdm, fitparams=None, tl=None):
-        """Calculates the energy probability (in logE) of each event.
+    def get_pd(
+            self,
+            tdm,
+            params_recarray=None,
+            tl=None):
+        """Calculates the spatial background probability on the sphere of each
+        event.
 
         Parameters
         ----------
         tdm : instance of TrialDataManager
-            The TrialDataManager instance holding the data events for which the
-            probability should be calculated for. The following data fields must
-            exist:
+            The TrialDataManager instance holding the trial event data for which
+            to calculate the PDF values. The following data fields must exist:
 
-            - 'log_energy' : float
-                The logarithm of the energy value of the event.
-            - 'sin_dec' : float
-                The sin(declination) value of the event.
+                sin_dec : float
+                    The sin(declination) value of the event.
 
-        fitparams : None
+        params_recarray : None
             Unused interface parameter.
-        tl : TimeLord instance | None
+        tl : instance of TimeLord | None
             The optional TimeLord instance that should be used to measure
             timing information.
 
         Returns
         -------
-        prob : 1D (N_events,) shaped ndarray
-            The array with the energy probability for each event.
+        pd : instance of numpy ndarray
+            The (N_events,)-shaped numpy ndarray holding the background
+            probability density value for each event.
+        grads : dict
+            The dictionary holding the gradients of the probability density
+            w.r.t. each global fit parameter.
+            The background PDF does not depend on any global fit parameter,
+            hence, this is an empty dictionary.
+        """
+        return (self._pd, dict())
+
+
+class DataBackgroundI3SpatialPDF(
+        BackgroundI3SpatialPDF,
+):
+    """This is the IceCube spatial background PDF, which gets constructed from
+    experimental data.
+    """
+    def __init__(
+            self,
+            data_exp,
+            sin_dec_binning,
+            spline_order_sin_dec=2,
+            **kwargs,
+    ):
+        """Constructs a new IceCube spatial background PDF from experimental
+        data.
+
+        Parameters
+        ----------
+        data_exp : instance of DataFieldRecordArray
+            The instance of DataFieldRecordArray holding the experimental data.
+            The following data fields must exist:
+
+                sin_dec : float
+                    The sin(declination) of the data event.
+
+        sin_dec_binning : BinningDefinition
+            The binning definition for the sin(declination).
+        spline_order_sin_dec : int
+            The order of the spline function for the logarithmic values of the
+            spatial background PDF along the sin(dec) axis.
+            The default is 2.
         """
-        get_data = tdm.get_data
+        if not isinstance(data_exp, DataFieldRecordArray):
+            raise TypeError(
+                'The data_exp argument must be an instance of '
+                'DataFieldRecordArray! '
+                f'It is of type "{classname(data_exp)}"!')
 
-        logE_binning = self.get_binning('log_energy')
-        sinDec_binning = self.get_binning('sin_dec')
+        data_sin_dec = data_exp['sin_dec']
+        data_weights = np.ones((len(data_exp),))
 
-        logE_idx = np.digitize(
-            get_data('log_energy'), logE_binning.binedges) - 1
-        sinDec_idx = np.digitize(
-            get_data('sin_dec'), sinDec_binning.binedges) - 1
+        # Create the PDF using the base class.
+        super().__init__(
+            data_sin_dec=data_sin_dec,
+            data_weights=data_weights,
+            sin_dec_binning=sin_dec_binning,
+            spline_order_sin_dec=spline_order_sin_dec,
+            **kwargs)
 
-        with TaskTimer(tl, 'Evaluating logE-sinDec histogram.'):
-            prob = self._hist_logE_sinDec[(logE_idx, sinDec_idx)]
 
-        return prob
+class MCBackgroundI3SpatialPDF(
+        BackgroundI3SpatialPDF,
+):
+    """This is the IceCube spatial background PDF, which gets constructed from
+    monte-carlo data.
+    """
+    def __init__(
+            self,
+            data_mc,
+            physics_weight_field_names,
+            sin_dec_binning,
+            spline_order_sin_dec=2,
+            **kwargs,
+    ):
+        """Constructs a new IceCube spatial background PDF from monte-carlo
+        data.
+
+        Parameters
+        ----------
+        data_mc : instance of DataFieldRecordArray
+            The array holding the monte-carlo data. The following data fields
+            must exist:
 
+                sin_dec : float
+                    The sine of the reconstructed declination of the data event.
 
-class PDDataBackgroundI3EnergyPDF(PDEnergyPDF, IsBackgroundPDF):
+        physics_weight_field_names : str | list of str
+            The name or the list of names of the monte-carlo data fields, which
+            should be used as event weights. If a list is given, the weight
+            values of all the fields will be summed to construct the final event
+            weight.
+        sin_dec_binning : BinningDefinition
+            The binning definition for the sin(declination).
+        spline_order_sin_dec : int
+            The order of the spline function for the logarithmic values of the
+            spatial background PDF along the sin(dec) axis.
+            The default is 2.
+        """
+        if not isinstance(data_mc, DataFieldRecordArray):
+            raise TypeError(
+                'The data_mc argument must be and instance of '
+                'DataFieldRecordArray! '
+                f'It is of type {classname(data_mc)}')
+
+        if not issequence(physics_weight_field_names):
+            physics_weight_field_names = [physics_weight_field_names]
+        if not issequenceof(physics_weight_field_names, str):
+            raise TypeError(
+                'The physics_weight_field_names argument must be of type str '
+                'or a sequence of type str! It is of type '
+                f'"{classname(physics_weight_field_names)}"!')
+
+        data_sin_dec = data_mc['sin_dec']
+
+        # Calculate the event weights as the sum of all the given data fields
+        # for each event.
+        data_weights = np.zeros(len(data_mc), dtype=np.float64)
+        for name in physics_weight_field_names:
+            if name not in data_mc:
+                raise KeyError(
+                    f'The field "{name}" does not exist in the MC data!')
+            data_weights += data_mc[name]
+
+        # Create the PDF using the base class.
+        super().__init__(
+            data_sin_dec=data_sin_dec,
+            data_weights=data_weights,
+            sin_dec_binning=sin_dec_binning,
+            spline_order_sin_dec=spline_order_sin_dec,
+            **kwargs)
+
+
+class DataBackgroundI3EnergyPDF(
+        I3EnergyPDF,
+        IsBackgroundPDF,
+):
     """This is the IceCube energy background PDF, which gets constructed from
     experimental data. This class is derived from I3EnergyPDF.
     """
-
-    def __init__(self, data_exp, logE_binning, sinDec_binning,
-                 smoothing_filter=None, kde_smoothing=False):
+    def __init__(
+            self,
+            data_exp,
+            log10_energy_binning,
+            sin_dec_binning,
+            smoothing_filter=None,
+            **kwargs,
+    ):
         """Constructs a new IceCube energy background PDF from experimental
         data.
 
         Parameters
         ----------
         data_exp : instance of DataFieldRecordArray
             The array holding the experimental data. The following data fields
             must exist:
 
-            - 'log_energy' : float
-                The logarithm of the reconstructed energy value of the data
-                event.
-            - 'sin_dec' : float
-                The sine of the reconstructed declination of the data event.
+                log_energy : float
+                    The logarithm of the reconstructed energy value of the data
+                    event.
+                sin_dec : float
+                    The sine of the reconstructed declination of the data event.
 
-        logE_binning : BinningDefinition
+        log10_energy_binning : instance of BinningDefinition
             The binning definition for the binning in log10(E).
-        sinDec_binning : BinningDefinition
+        sin_dec_binning : instance of BinningDefinition
             The binning definition for the sin(declination).
-        smoothing_filter : SmoothingFilter instance | None
+        smoothing_filter : instance of SmoothingFilter | None
             The smoothing filter to use for smoothing the energy histogram.
             If None, no smoothing will be applied.
         """
-        if(not isinstance(data_exp, DataFieldRecordArray)):
-            raise TypeError('The data_exp argument must be an instance of '
-                            'DataFieldRecordArray!')
+        if not isinstance(data_exp, DataFieldRecordArray):
+            raise TypeError(
+                'The data_exp argument must be an instance of '
+                'DataFieldRecordArray! '
+                f'It is of type "{classname(data_exp)}"!')
 
-        data_logE = data_exp['log_energy']
-        data_sinDec = data_exp['sin_dec']
+        data_log10_energy = data_exp['log_energy']
+        data_sin_dec = data_exp['sin_dec']
         # For experimental data, the MC and physics weight are unity.
         data_mcweight = np.ones((len(data_exp),))
         data_physicsweight = data_mcweight
 
         # Create the PDF using the base class.
-        super(PDDataBackgroundI3EnergyPDF, self).__init__(
-            data_logE, data_sinDec, data_mcweight, data_physicsweight,
-            logE_binning, sinDec_binning, smoothing_filter, kde_smoothing
-        )
-
-
-class PDMCBackgroundI3EnergyPDF(EnergyPDF, IsBackgroundPDF, UsesBinning):
-    """This class provides a background energy PDF constructed from the public
-    data and a monte-carlo background flux model.
+        super().__init__(
+            pmm=None,
+            data_log10_energy=data_log10_energy,
+            data_sin_dec=data_sin_dec,
+            data_mcweight=data_mcweight,
+            data_physicsweight=data_physicsweight,
+            log10_energy_binning=log10_energy_binning,
+            sin_dec_binning=sin_dec_binning,
+            smoothing_filter=smoothing_filter,
+            **kwargs)
+
+
+class MCBackgroundI3EnergyPDF(
+        I3EnergyPDF,
+        IsBackgroundPDF,
+):
+    """This is the IceCube energy background PDF, which gets constructed from
+    monte-carlo data. This class is derived from I3EnergyPDF.
     """
-
     def __init__(
-            self, pdf_log10emu_sindecmu, log10emu_binning, sindecmu_binning,
-            **kwargs):
-        """Constructs a new background energy PDF with the given PDF data and
-        binning.
+            self,
+            data_mc,
+            physics_weight_field_names,
+            log10_energy_binning,
+            sin_dec_binning,
+            smoothing_filter=None,
+            **kwargs,
+    ):
+        """Constructs a new IceCube energy background PDF from monte-carlo
+        data.
 
         Parameters
         ----------
-        pdf_log10emu_sindecmu : 2D numpy ndarray
-            The (n_log10emu, n_sindecmu)-shaped 2D numpy ndarray holding the
-            PDF values in unit 1/log10(E_mu/GeV).
-            A copy of this data will be created and held within this class
-            instance.
-        log10emu_binning : BinningDefinition
-            The binning definition for the binning in log10(E_mu/GeV).
-        sindecmu_binning : BinningDefinition
-            The binning definition for the binning in sin(dec_mu).
+        data_mc : instance of DataFieldRecordArray
+            The array holding the monte-carlo data. The following data fields
+            must exist:
+
+                log_energy : float
+                    The logarithm of the reconstructed energy value of the data
+                    event.
+                sin_dec : float
+                    The sine of the reconstructed declination of the data event.
+                mcweight: float
+                    The monte-carlo weight of the event.
+
+        physics_weight_field_names : str | list of str
+            The name or the list of names of the monte-carlo data fields, which
+            should be used as physics event weights. If a list is given, the
+            weight values of all the fields will be summed to construct the
+            final event physics weight.
+        log10_energy_binning : BinningDefinition
+            The binning definition for the binning in log10(E).
+        sin_dec_binning : BinningDefinition
+            The binning definition for the sin(declination).
+        smoothing_filter : SmoothingFilter instance | None
+            The smoothing filter to use for smoothing the energy histogram.
+            If None, no smoothing will be applied.
         """
-        if not isinstance(pdf_log10emu_sindecmu, np.ndarray):
-            raise TypeError(
-                'The pdf_log10emu_sindecmu argument must be an instance of '
-                'numpy.ndarray!')
-        if not isinstance(sindecmu_binning, BinningDefinition):
+        if not isinstance(data_mc, DataFieldRecordArray):
             raise TypeError(
-                'The sindecmu_binning argument must be an instance of '
-                'BinningDefinition!')
-        if not isinstance(log10emu_binning, BinningDefinition):
+                'The data_mc argument must be an instance of '
+                'DataFieldRecordArray! '
+                f'It is of type "{classname(data_mc)}"!')
+
+        if not issequence(physics_weight_field_names):
+            physics_weight_field_names = [physics_weight_field_names]
+        if not issequenceof(physics_weight_field_names, str):
             raise TypeError(
-                'The log10emu_binning argument must be an instance of '
-                'BinningDefinition!')
+                'The physics_weight_field_names argument must be '
+                'of type str or a sequence of type str! '
+                f'It is of type {classname(physics_weight_field_names)}')
+
+        data_log10_energy = data_mc['log_energy']
+        data_sin_dec = data_mc['sin_dec']
+        data_mcweight = data_mc['mcweight']
+
+        # Calculate the event weights as the sum of all the given data fields
+        # for each event.
+        data_physicsweight = np.zeros(len(data_mc), dtype=np.float64)
+        for name in physics_weight_field_names:
+            if name not in data_mc:
+                raise KeyError(
+                    f'The field "{name}" does not exist in the MC data!')
+            data_physicsweight += data_mc[name]
 
-        super().__init__(**kwargs)
-
-        self.add_axis(PDFAxis(
-            log10emu_binning.name,
-            log10emu_binning.lower_edge,
-            log10emu_binning.upper_edge,
-        ))
-
-        self.add_axis(PDFAxis(
-            sindecmu_binning.name,
-            sindecmu_binning.lower_edge,
-            sindecmu_binning.upper_edge,
-        ))
-
-        self._hist_logE_sinDec = np.copy(pdf_log10emu_sindecmu)
-        self.add_binning(log10emu_binning, name='log_energy')
-        self.add_binning(sindecmu_binning, name='sin_dec')
-
-    def assert_is_valid_for_trial_data(self, tdm):
-        """Checks if this PDF covers the entire value range of the trail
-        data events.
-
-        Parameters
-        ----------
-        tdm : TrialDataManager instance
-            The TrialDataManager instance holding the data events.
-            The following data fields need to exist:
-
-                'sin_dec'
-
-                'log_energy'
-
-        Raises
-        ------
-        ValueError
-            If parts of the trial data is outside the value range of this
-            PDF.
-        """
-        sindecmu = tdm.get_data('sin_dec')
-        if np.min(sindecmu) < self.get_axis(0).vmin:
-            raise ValueError(
-                'The minimum sindecmu value %e of the trial data is lower '
-                'than the minimum value of the PDF %e!' % (
-                    np.min(sindecmu), self.get_axis(0).vmin))
-        if np.max(sindecmu) > self.get_axis(0).vmax:
-            raise ValueError(
-                'The maximum sindecmu value %e of the trial data is larger '
-                'than the maximum value of the PDF %e!' % (
-                    np.max(sindecmu), self.get_axis(0).vmax))
-
-        log10emu = tdm.get_data('log_energy')
-        if np.min(log10emu) < self.get_axis(1).vmin:
-            raise ValueError(
-                'The minimum log10emu value %e of the trial data is lower '
-                'than the minimum value of the PDF %e!' % (
-                    np.min(log10emu), self.get_axis(1).vmin))
-        if np.max(log10emu) > self.get_axis(1).vmax:
-            raise ValueError(
-                'The maximum log10emu value %e of the trial data is larger '
-                'than the maximum value of the PDF %e!' % (
-                    np.max(log10emu), self.get_axis(1).vmax))
-
-    def get_prob(self, tdm, params=None, tl=None):
-        """Gets the probability density for the given trial data events.
-
-        Parameters
-        ----------
-        tdm : TrialDataManager instance
-            The TrialDataManager instance holding the data events.
-            The following data fields need to exist:
-
-                'sin_dec'
-
-                'log_energy'
-
-        params : dict | None
-            The dictionary containing the parameter names and values for which
-            the probability should get calculated.
-            By definition of this PDF, this is ``one``, because this PDF does
-            not depend on any parameters.
-        tl : TimeLord instance | None
-            The optional TimeLord instance that should be used to measure
-            timing information.
-
-        Returns
-        -------
-        prob : (N_events,)-shaped numpy ndarray
-            The 1D numpy ndarray with the probability density for each event.
-        """
-        get_data = tdm.get_data
-
-        log10emu = get_data('log_energy')
-        sindecmu = get_data('sin_dec')
-
-        log10emu_idxs = np.digitize(
-            log10emu, self.get_binning('log_energy').binedges) - 1
-        sindecmu_idxs = np.digitize(
-            sindecmu, self.get_binning('sin_dec').binedges) - 1
-
-        with TaskTimer(tl, 'Evaluating sindecmu-log10emu PDF.'):
-            pd = self._hist_logE_sinDec[(log10emu_idxs, sindecmu_idxs)]
-
-        return pd
+        # Create the PDF using the base class.
+        super().__init__(
+            pmm=None,
+            data_log10_energy=data_log10_energy,
+            data_sin_dec=data_sin_dec,
+            data_mcweight=data_mcweight,
+            data_physicsweight=data_physicsweight,
+            log10_energy_binning=log10_energy_binning,
+            sin_dec_binning=sin_dec_binning,
+            smoothing_filter=smoothing_filter,
+            **kwargs)
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `skyllh-23.1.1/skyllh/analyses/i3/publicdata_ps/bkg_flux.py` & `skyllh-23.2.0/skyllh/analyses/i3/publicdata_ps/bkg_flux.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,14 +1,19 @@
 # -*- coding: utf-8 -*-
 
 import numpy as np
 import pickle
 
-from skyllh.physics.flux import PowerLawFlux
-from skyllh.core.binning import get_bincenters_from_binedges
+from skyllh.core.binning import (
+    get_bincenters_from_binedges,
+)
+from skyllh.core.flux_model import (
+    PowerLawEnergyFluxProfile,
+    SteadyPointlikeFFM,
+)
 
 
 def get_dOmega(dec_min, dec_max):
     """Calculates the solid angle given two declination angles.
 
     Parameters
     ----------
@@ -84,19 +89,19 @@
     n_enu = len(e_grid)
 
     # Calculate f_atmo(E_nu,dec_nu).
     f_atmo = np.zeros((n_decnu, n_enu))
     zero_zen_idx = np.digitize(0, zenith_angle_binedges) - 1
     for (decnu_idx, decnu) in enumerate(decnu_angles):
         if decnu < 0:
-            fl = flux_def['numu_total'][:,decnu_idx][m_e_grid]
+            fl = flux_def['numu_total'][:, decnu_idx][m_e_grid]
         else:
             # For up-going we use the flux calculation from the streight
             # downgoing.
-            fl = flux_def['numu_total'][:,zero_zen_idx][m_e_grid]
+            fl = flux_def['numu_total'][:, zero_zen_idx][m_e_grid]
         f_atmo[decnu_idx] = fl
 
     return (f_atmo, decnu_binedges, log10_enu_binedges)
 
 
 def get_flux_astro_decnu_log10enu(decnu_binedges, log10_enu_binedges):
     """Constructs the astrophysical neutrino flux function
@@ -118,22 +123,26 @@
         The numpy ndarray holding the astrophysical flux values in unit
         1/(GeV cm^2 sr s).
 
     References
     ----------
     [1] https://arxiv.org/pdf/2111.10299.pdf
     """
-    fluxmodel = PowerLawFlux(Phi0=1.44e-18, E0=100e3, gamma=2.37)
+    fluxmodel = SteadyPointlikeFFM(
+        Phi0=1.44e-18,
+        energy_profile=PowerLawEnergyFluxProfile(
+            E0=100e3,
+            gamma=2.37))
 
     n_decnu = len(decnu_binedges) - 1
 
     enu_binedges = np.power(10, log10_enu_binedges)
     enu_bincenters = get_bincenters_from_binedges(enu_binedges)
 
-    fl = fluxmodel(enu_bincenters)
+    fl = fluxmodel(E=enu_bincenters).squeeze()
     f_astro = np.tile(fl, (n_decnu, 1))
 
     return f_astro
 
 
 def convert_flux_bkg_to_pdf_bkg(f_bkg, decnu_binedges, log10_enu_binedges):
     """Converts the given background flux function f_bkg into a background flux
@@ -154,21 +163,22 @@
     -------
     p_bkg : (n_decnu, n_enu)-shaped 2D numpy ndarray
         The numpy ndarray holding the background flux pdf values.
     """
     d_decnu = np.diff(decnu_binedges)
     d_log10_enu = np.diff(log10_enu_binedges)
 
-    bin_area = d_decnu[:,np.newaxis] * d_log10_enu[np.newaxis,:]
+    bin_area = d_decnu[:, np.newaxis] * d_log10_enu[np.newaxis, :]
     p_bkg = f_bkg / np.sum(f_bkg*bin_area)
 
     # Cross-check the normalization of the PDF.
     if not np.isclose(np.sum(p_bkg*bin_area), 1):
         raise ValueError(
-            'The background PDF is not normalized! The integral is %f!'%(np.sum(p_bkg*bin_area)))
+            'The background PDF is not normalized! The integral is '
+            f'{np.sum(p_bkg*bin_area)}!')
 
     return p_bkg
 
 
 def get_pd_atmo_decnu_Enu(flux_pathfilename, log10_true_e_max=9):
     """Constructs the atmospheric neutrino PDF p_atmo(E_nu,dec_nu) in unit
     1/(GeV rad).
@@ -212,28 +222,29 @@
     n_decnu = len(decnu_angles)
     n_e_grid = len(e_grid)
 
     # Calculate p_atmo(E_nu,dec_nu).
     pd_atmo = np.zeros((n_decnu, n_e_grid))
     for (decnu_idx, decnu) in enumerate(decnu_angles):
         if decnu < 0:
-            fl = flux_def['numu_total'][:,decnu_idx][m_e_grid]
+            fl = flux_def['numu_total'][:, decnu_idx][m_e_grid]
         else:
             # For up-going we use the flux calculation from the streight
             # downgoing.
-            fl = flux_def['numu_total'][:,0][m_e_grid]
+            fl = flux_def['numu_total'][:, 0][m_e_grid]
         pd_atmo[decnu_idx] = fl
     # Normalize the PDF.
-    bin_area = d_decnu[:,np.newaxis] * np.diff(log10_e_grid_edges)[np.newaxis,:]
+    bin_area = d_decnu[:, np.newaxis] * np.diff(log10_e_grid_edges)[np.newaxis, :]
     pd_atmo /= np.sum(pd_atmo*bin_area)
 
     # Cross-check the normalization of the PDF.
     if not np.isclose(np.sum(pd_atmo*bin_area), 1):
         raise ValueError(
-            'The atmospheric true energy PDF is not normalized! The integral is %f!'%(np.sum(pd_atmo*bin_area)))
+            'The atmospheric true energy PDF is not normalized! The integral '
+            f'is {np.sum(pd_atmo*bin_area)}!')
 
     return (pd_atmo, decnu_binedges, log10_e_grid_edges)
 
 
 def get_pd_atmo_E_nu_sin_dec_nu(flux_pathfilename):
     """Constructs the atmospheric energy PDF p_atmo(E_nu|sin(dec_nu)) in
     unit 1/GeV.
@@ -279,23 +290,23 @@
     n_e_grid = len(e_grid)
     n_sin_dec = len(sin_dec_angles)
 
     # Calculate p_atmo(E_nu|sin(dec_nu)).
     pd_atmo = np.zeros((n_sin_dec, n_e_grid))
     for (sin_dec_idx, sin_dec) in enumerate(sin_dec_angles):
         if sin_dec < 0:
-            fl = flux_def['numu_total'][:,sin_dec_idx][m_e_grid]
+            fl = flux_def['numu_total'][:, sin_dec_idx][m_e_grid]
         else:
             # For up-going we use the flux calculation from the streight
             # downgoing.
-            fl = flux_def['numu_total'][:,0][m_e_grid]
+            fl = flux_def['numu_total'][:, 0][m_e_grid]
         pd_atmo[sin_dec_idx] = fl/np.sum(fl*dE)
 
     # Cross-check the normalization of the PDF.
-    if not np.all(np.isclose(np.sum(pd_atmo*dE[np.newaxis,:], axis=1), 1)):
+    if not np.all(np.isclose(np.sum(pd_atmo*dE[np.newaxis, :], axis=1), 1)):
         raise ValueError(
             'The atmospheric true energy PDF is not normalized!')
 
     return (pd_atmo, sin_dec_binedges, log10_e_grid_edges)
 
 
 def get_pd_astro_E_nu_sin_dec_nu(sin_dec_binedges, log10_e_grid_edges):
@@ -317,30 +328,33 @@
         The numpy ndarray holding the energy probability density values
         p(E_nu|sin_dec_nu) in unit 1/GeV.
 
     References
     ----------
     [1] https://arxiv.org/pdf/2111.10299.pdf
     """
-    fluxmodel = PowerLawFlux(Phi0=1.44e-18, E0=100e3, gamma=2.37)
+    fluxmodel = SteadyPointlikeFFM(
+        Phi0=1.44e-18,
+        energy_profile=PowerLawEnergyFluxProfile(
+            E0=100e3,
+            gamma=2.37))
 
     n_sin_dec = len(sin_dec_binedges) - 1
-    n_e_grid = len(log10_e_grid_edges) - 1
 
     e_grid_edges = 10**log10_e_grid_edges
     e_grid_bc = 0.5*(e_grid_edges[:-1] + e_grid_edges[1:])
 
     dE = np.diff(e_grid_edges)
 
-    fl = fluxmodel(e_grid_bc)
+    fl = fluxmodel(E=e_grid_bc).squeeze()
     pd = fl / np.sum(fl*dE)
     pd_astro = np.tile(pd, (n_sin_dec, 1))
 
     # Cross-check the normalization of the PDF.
-    if not np.all(np.isclose(np.sum(pd_astro*dE[np.newaxis,:], axis=1), 1)):
+    if not np.all(np.isclose(np.sum(pd_astro*dE[np.newaxis, :], axis=1), 1)):
         raise ValueError(
             'The astrophysical energy PDF is not normalized!')
 
     return pd_astro
 
 
 def get_pd_bkg_E_nu_sin_dec_nu(pd_atmo, pd_astro, log10_e_grid_edges):
@@ -365,17 +379,15 @@
         The numpy ndarray holding total background probability density values
         p_bkg(E_nu|sin(dec_nu)) in unit 1/GeV.
     """
     pd_bkg = pd_atmo + pd_astro
 
     dE = np.diff(10**log10_e_grid_edges)
 
-    s = np.sum(pd_bkg*dE[np.newaxis,:], axis=1, keepdims=True)
+    s = np.sum(pd_bkg*dE[np.newaxis, :], axis=1, keepdims=True)
     pd_bkg /= s
 
-    if not np.all(np.isclose(np.sum(pd_bkg*dE[np.newaxis,:], axis=1), 1)):
+    if not np.all(np.isclose(np.sum(pd_bkg*dE[np.newaxis, :], axis=1), 1)):
         raise ValueError(
             'The background energy PDF is not normalized!')
 
     return pd_bkg
-
-
```

### Comparing `skyllh-23.1.1/skyllh/analyses/i3/publicdata_ps/detsigyield.py` & `skyllh-23.2.0/skyllh/analyses/i3/publicdata_ps/detsigyield.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,214 +1,257 @@
 # -*- coding: utf-8 -*-
 
+from astropy import units
 import numpy as np
 
 import scipy.interpolate
 
-from skyllh.core import multiproc
-from skyllh.core.binning import BinningDefinition
-from skyllh.core.dataset import (
-    Dataset,
-    DatasetData
-)
-from skyllh.core.livetime import Livetime
-from skyllh.core.parameters import ParameterGrid
-from skyllh.core.detsigyield import (
-    get_integrated_livetime_in_days
-)
-from skyllh.physics.flux import (
-    PowerLawFlux,
-    get_conversion_factor_to_internal_flux_unit
+from skyllh.analyses.i3.publicdata_ps.aeff import (
+    load_effective_area_array,
 )
-from skyllh.i3.detsigyield import (
-    PowerLawFluxPointLikeSourceI3DetSigYieldImplMethod,
-    PowerLawFluxPointLikeSourceI3DetSigYield
+from skyllh.core import (
+    multiproc,
 )
-from skyllh.analyses.i3.publicdata_ps.aeff import (
-    load_effective_area_array
+from skyllh.core.binning import (
+    BinningDefinition,
+)
+from skyllh.core.flux_model import (
+    FactorizedFluxModel,
+)
+from skyllh.core.livetime import (
+    Livetime,
+)
+from skyllh.core.py import (
+    classname,
+    issequence,
+)
+from skyllh.i3.detsigyield import (
+    SingleParamFluxPointLikeSourceI3DetSigYieldBuilder,
+    SingleParamFluxPointLikeSourceI3DetSigYield,
 )
 
 
-class PublicDataPowerLawFluxPointLikeSourceI3DetSigYieldImplMethod(
-        PowerLawFluxPointLikeSourceI3DetSigYieldImplMethod,
-        multiproc.IsParallelizable):
-    """This detector signal yield constructor class constructs a
-    detector signal yield instance for a variable power law flux model, which
-    has the spectral index gamma as fit parameter, assuming a point-like source.
-    It constructs a two-dimensional spline function in sin(dec) and gamma, using
-    a :class:`scipy.interpolate.RectBivariateSpline`. Hence, the detector signal
-    yield can vary with the declination and the spectral index, gamma, of the
-    source.
-
-    This detector signal yield implementation method works with a
-    PowerLawFlux flux model.
+class PDSingleParamFluxPointLikeSourceI3DetSigYieldBuilder(
+        SingleParamFluxPointLikeSourceI3DetSigYieldBuilder,
+):
+    """This detector signal yield builder class constructs a
+    detector signal yield instance for a variable flux model of a single
+    parameter, assuming a point-like source.
+    It constructs a two-dimensional spline function in sin(dec) and the
+    parameter, using a :class:`scipy.interpolate.RectBivariateSpline`.
+    Hence, the detector signal yield can vary with the declination and the
+    parameter of the flux model.
 
     It is tailored to the IceCube detector at the South Pole, where the
     effective area depends solely on the zenith angle, and hence on the
     declination, of the source.
 
     It takes the effective area for the detector signal yield from the auxilary
     detector effective area data file given by the public data.
     """
 
     def __init__(
-            self, gamma_grid, spline_order_sinDec=2, spline_order_gamma=2,
-            ncpu=None):
-        """Creates a new IceCube detector signal yield constructor instance for
-        a power law flux model. It requires the effective area from the public
-        data, and a gamma parameter grid to compute the gamma dependency of the
-        detector signal yield.
+            self,
+            param_grid,
+            spline_order_sinDec=2,
+            spline_order_param=2,
+            ncpu=None,
+            **kwargs,
+    ):
+        """Creates a new IceCube detector signal yield builder instance for
+        a flux model with a single parameter.
+        It requires the effective area from the public data, and a parameter
+        grid to compute the parameter dependency of the detector signal yield.
 
         Parameters
         ----------
-        gamma_grid : ParameterGrid instance
-            The ParameterGrid instance which defines the grid of gamma values.
+        param_grid : instance of ParameterGrid
+            The instance of ParameterGrid which defines the grid of parameter
+            values.
         spline_order_sinDec : int
             The order of the spline function for the logarithmic values of the
             detector signal yield along the sin(dec) axis.
             The default is 2.
-        spline_order_gamma : int
+        spline_order_param : int
             The order of the spline function for the logarithmic values of the
-            detector signal yield along the gamma axis.
+            detector signal yield along the parameter axis.
             The default is 2.
         ncpu : int | None
-            The number of CPUs to utilize. Global setting will take place if
-            not specified, i.e. set to None.
+            The number of CPUs to utilize. If set to ``None``, global setting
+            will take place.
         """
         super().__init__(
-            gamma_grid=gamma_grid,
+            param_grid=param_grid,
             sin_dec_binning=None,
             spline_order_sinDec=spline_order_sinDec,
-            spline_order_gamma=spline_order_gamma,
-            ncpu=ncpu)
+            spline_order_param=spline_order_param,
+            ncpu=ncpu,
+            **kwargs)
+
+    def assert_types_of_construct_detsigyield_arguments(
+            self,
+            shgs,
+            **kwargs):
+        """Checks the correct types of the arguments for the
+        ``construct_detsigyield`` method.
+        """
+        super().assert_types_of_construct_detsigyield_arguments(
+            shgs=shgs,
+            **kwargs)
+
+        if not issequence(shgs):
+            shgs = [shgs]
+        for shg in shgs:
+            if not isinstance(shg.fluxmodel, FactorizedFluxModel):
+                raise TypeError(
+                    'The fluxmodel of the source hypothesis group must be an '
+                    'instance of FactorizedFluxModel! '
+                    f'Its current type is {classname(shg.fluxmodel)}!')
 
     def construct_detsigyield(
-            self, dataset, data, fluxmodel, livetime, ppbar=None):
+            self,
+            dataset,
+            data,
+            shg,
+            ppbar=None):
         """Constructs a detector signal yield 2-dimensional log spline
-        function for the given power law flux model with varying gamma values.
+        function for the given flux model with varying parameter values.
 
         Parameters
         ----------
-        dataset : Dataset instance
+        dataset : instance of Dataset
             The Dataset instance holding the sin(dec) binning definition.
-        data : DatasetData instance
-            The DatasetData instance holding the monte-carlo event data.
+        data : instance of DatasetData
+            The instance of DatasetData holding the monte-carlo event data.
             This implementation loads the effective area from the provided
             public data and hence does not need monte-carlo data.
-        fluxmodel : FluxModel
-            The flux model instance. Must be an instance of PowerLawFlux.
-        livetime : float | Livetime instance
-            The live-time in days or an instance of Livetime to use for the
-            detector signal yield.
+        shg : instance of SourceHypoGroup
+            The instance of SourceHypoGroup (i.e. sources and flux model) for
+            which the detector signal yield should get constructed.
         ppbar : ProgressBar instance | None
             The instance of ProgressBar of the optional parent progress bar.
 
         Returns
         -------
-        detsigyield : PowerLawFluxPointLikeSourceI3DetSigYield instance
-            The DetSigYield instance for a point-like source with a power law
-            flux with variable gamma parameter.
+        detsigyield : instance of SingleParamFluxPointLikeSourceI3DetSigYield
+            The DetSigYield instance for a point-like source with a flux model
+            of a single parameter.
         """
-        # Check for the correct data types of the input arguments.
-        if(not isinstance(dataset, Dataset)):
-            raise TypeError('The dataset argument must be an instance of '
-                            'Dataset!')
-        if(not isinstance(data, DatasetData)):
-            raise TypeError('The data argument must be an instance of '
-                            'DatasetData!')
-        if(not self.supports_fluxmodel(fluxmodel)):
-            raise TypeError('The DetSigYieldImplMethod "%s" does not support '
-                            'the flux model "%s"!' % (
-                                self.__class__.__name__,
-                                fluxmodel.__class__.__name__))
-        if((not isinstance(livetime, float)) and
-           (not isinstance(livetime, Livetime))):
-            raise TypeError('The livetime argument must be an instance of '
-                            'float or Livetime!')
+        self.assert_types_of_construct_detsigyield_arguments(
+            dataset=dataset,
+            data=data,
+            shgs=shg,
+            ppbar=ppbar,
+        )
 
         # Get integrated live-time in days.
-        livetime_days = get_integrated_livetime_in_days(livetime)
+        livetime_days = Livetime.get_integrated_livetime(data.livetime)
+
+        to_internal_time_unit_factor = self._cfg.to_internal_time_unit(
+            time_unit=units.day
+        )
 
         # Calculate conversion factor from the flux model unit into the internal
         # flux unit GeV^-1 cm^-2 s^-1.
-        toGeVcm2s = get_conversion_factor_to_internal_flux_unit(fluxmodel)
+        to_internal_flux_unit_factor = shg.fluxmodel.to_internal_flux_unit()
 
         # Load the effective area data from the public dataset.
         aeff_fnames = dataset.get_abs_pathfilename_list(
             dataset.get_aux_data_definition('eff_area_datafile'))
         (
             aeff_arr,
             sin_true_dec_binedges_lower,
             sin_true_dec_binedges_upper,
             log_true_e_binedges_lower,
             log_true_e_binedges_upper
         ) = load_effective_area_array(aeff_fnames)
 
         # Calculate the detector signal yield in sin_dec vs gamma.
-        def hist(
-                energy_bin_edges_lower, energy_bin_edges_upper,
-                aeff, fluxmodel):
+        def _create_hist(
+                energy_bin_edges_lower,
+                energy_bin_edges_upper,
+                aeff,
+                fluxmodel,
+                to_internal_flux_unit_factor,
+        ):
             """Creates a histogram of the detector signal yield for the given
             sin(dec) binning.
 
             Parameters
             ----------
             energy_bin_edges_lower : 1d ndarray
                 The array holding the lower bin edges in E_nu/GeV.
             energy_bin_edges_upper : 1d ndarray
                 The array holding the upper bin edges in E_nu/GeV.
             aeff : (n_bins_sin_dec, n_bins_log_energy)-shaped 2d ndarray
                 The effective area binned data array.
 
             Returns
             -------
-            h : (n_bins_sin_dec,)-shaped 1d ndarray
-                The numpy array containing the detector signal yield values for
-                the different sin_dec bins and the given flux model.
+            h : instance of ndarray
+                The (n_bins_sin_dec,)-shaped 1d numpy ndarray containing the
+                detector signal yield values for the different sin_dec bins and
+                the given flux model.
             """
-            # Create histogram for the number of neutrinos with each energy
-            # bin.
-            h_phi = fluxmodel.get_integral(
-                energy_bin_edges_lower, energy_bin_edges_upper)
+            h_phi = fluxmodel.energy_profile.get_integral(
+                E1=energy_bin_edges_lower,
+                E2=energy_bin_edges_upper)
+            h_phi *= to_internal_flux_unit_factor
 
             # Sum over the enegry bins for each sin_dec row.
             h = np.sum(aeff*h_phi, axis=1)
 
             return h
 
         energy_bin_edges_lower = np.power(10, log_true_e_binedges_lower)
         energy_bin_edges_upper = np.power(10, log_true_e_binedges_upper)
 
-        # Make a copy of the gamma grid and extend the grid by one bin on each
-        # side.
-        gamma_grid = self._gamma_grid.copy()
-        gamma_grid.add_extra_lower_and_upper_bin()
+        # Make a copy of the parameter grid and extend the grid by one bin on
+        # each side.
+        param_grid = self.param_grid.copy()
+        param_grid.add_extra_lower_and_upper_bin()
 
         # Construct the arguments for the hist function to be used in the
         # multiproc.parallelize function.
         args_list = [
-            ((energy_bin_edges_lower,
-              energy_bin_edges_upper,
-              aeff_arr,
-              fluxmodel.copy({'gamma': gamma})), {})
-            for gamma in gamma_grid.grid
+            (
+                (
+                    energy_bin_edges_lower,
+                    energy_bin_edges_upper,
+                    aeff_arr,
+                    shg.fluxmodel.copy({param_grid.name: param_val}),
+                    to_internal_flux_unit_factor,
+                ),
+                {}
+            )
+            for param_val in param_grid.grid
         ]
         h = np.vstack(
             multiproc.parallelize(
-                hist, args_list, self.ncpu, ppbar=ppbar)).T
-        h *= toGeVcm2s * livetime_days * 86400.
+                _create_hist, args_list, self.ncpu, ppbar=ppbar)).T
+        h *= livetime_days*to_internal_time_unit_factor
 
         # Create a 2d spline in log of the detector signal yield.
         sin_dec_bincenters = 0.5*(
             sin_true_dec_binedges_lower + sin_true_dec_binedges_upper)
-        log_spl_sinDec_gamma = scipy.interpolate.RectBivariateSpline(
-            sin_dec_bincenters, gamma_grid.grid, np.log(h),
-            kx=self.spline_order_sinDec, ky=self.spline_order_gamma, s=0)
+        log_spl_sinDec_param = scipy.interpolate.RectBivariateSpline(
+            sin_dec_bincenters,
+            param_grid.grid,
+            np.log(h),
+            kx=self.spline_order_sinDec,
+            ky=self.spline_order_param,
+            s=0)
 
         # Construct the detector signal yield instance with the created spline.
         sin_dec_binedges = np.concatenate(
             (sin_true_dec_binedges_lower, [sin_true_dec_binedges_upper[-1]]))
         sin_dec_binning = BinningDefinition('sin_dec', sin_dec_binedges)
-        detsigyield = PowerLawFluxPointLikeSourceI3DetSigYield(
-            self, dataset, fluxmodel, livetime, sin_dec_binning, log_spl_sinDec_gamma)
+
+        detsigyield = SingleParamFluxPointLikeSourceI3DetSigYield(
+            param_name=param_grid.name,
+            dataset=dataset,
+            fluxmodel=shg.fluxmodel,
+            livetime=data.livetime,
+            sin_dec_binning=sin_dec_binning,
+            log_spl_sinDec_param=log_spl_sinDec_param)
 
         return detsigyield
```

### Comparing `skyllh-23.1.1/skyllh/analyses/i3/publicdata_ps/mcbkg_ps.py` & `skyllh-23.2.0/skyllh/analyses/i3/publicdata_ps/time_integrated_ps.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,472 +1,548 @@
 # -*- coding: utf-8 -*-
 
-"""The mcbkg_ps analysis is a multi-dataset time-integrated single source
-analysis with a two-component likelihood function using a spacial and an energy
-event PDF. It initializes the background energy pdf using auxiliary fluxes and
-pdfs, which are generated by running `scripts/mceq_atm_bkg.py` script.
+"""The time_integrated_ps analysis is a multi-dataset time-integrated single
+source analysis with a two-component likelihood function using a spacial and an
+energy event PDF.
 """
 
-import argparse
-import logging
 import numpy as np
-import pickle
 
-from skyllh.core.progressbar import ProgressBar
-
-# Classes to define the source hypothesis.
-from skyllh.physics.source import PointLikeSource
-from skyllh.physics.flux import PowerLawFlux
-from skyllh.core.source_hypo_group import SourceHypoGroup
-from skyllh.core.source_hypothesis import SourceHypoGroupManager
-
-# Classes to define the fit parameters.
-from skyllh.core.parameters import (
-    SingleSourceFitParameterMapper,
-    FitParameter
+from skyllh.analyses.i3.publicdata_ps.backgroundpdf import (
+    PDDataBackgroundI3EnergyPDF,
+)
+from skyllh.analyses.i3.publicdata_ps.detsigyield import (
+    PDSingleParamFluxPointLikeSourceI3DetSigYieldBuilder,
+)
+from skyllh.analyses.i3.publicdata_ps.pdfratio import (
+    PDSigSetOverBkgPDFRatio,
+)
+from skyllh.analyses.i3.publicdata_ps.signal_generator import (
+    PDDatasetSignalGenerator,
+)
+from skyllh.analyses.i3.publicdata_ps.signalpdf import (
+    PDSignalEnergyPDFSet,
+)
+from skyllh.analyses.i3.publicdata_ps.utils import (
+    create_energy_cut_spline,
+    get_tdm_field_func_psi,
 )
 
-# Classes for the minimizer.
-from skyllh.core.minimizer import Minimizer, LBFGSMinimizerImpl
-
-# Classes for utility functionality.
-from skyllh.core.config import CFG
-from skyllh.core.random import RandomStateService
-from skyllh.core.optimize import SpatialBoxEventSelectionMethod
-from skyllh.core.smoothing import BlockSmoothingFilter
-from skyllh.core.timing import TimeLord
-from skyllh.core.trialdata import TrialDataManager
-
-# Classes for defining the analysis.
-from skyllh.core.test_statistic import TestStatisticWilks
 from skyllh.core.analysis import (
-    TimeIntegratedMultiDatasetSingleSourceAnalysis as Analysis
+    SingleSourceMultiDatasetLLHRatioAnalysis as Analysis,
 )
-
-# Classes to define the background generation.
-from skyllh.core.scrambling import DataScrambler, UniformRAScramblingMethod
-from skyllh.i3.background_generation import FixedScrambledExpDataI3BkgGenMethod
-
-# Classes to define the signal and background PDFs.
-from skyllh.core.signalpdf import RayleighPSFPointSourceSignalSpatialPDF
-from skyllh.i3.signalpdf import SignalI3EnergyPDFSet
-from skyllh.i3.backgroundpdf import (
-    DataBackgroundI3SpatialPDF,
-    DataBackgroundI3EnergyPDF
+from skyllh.core.background_generator import (
+    DatasetBackgroundGenerator,
+)
+from skyllh.core.config import (
+    Config,
+)
+from skyllh.core.debugging import (
+    get_logger,
+)
+from skyllh.core.event_selection import (
+    SpatialBoxEventSelectionMethod,
+)
+from skyllh.core.flux_model import (
+    PowerLawEnergyFluxProfile,
+    SteadyPointlikeFFM,
+)
+from skyllh.core.minimizer import (
+    Minimizer,
+    LBFGSMinimizerImpl,
+)
+from skyllh.core.minimizers.iminuit import (
+    IMinuitMinimizerImpl,
 )
-from skyllh.i3.pdfratio import (
-    I3EnergySigSetOverBkgPDFRatioSpline
+from skyllh.core.model import (
+    DetectorModel,
+)
+from skyllh.core.parameters import (
+    Parameter,
+    ParameterModelMapper,
 )
-# Classes to define the spatial and energy PDF ratios.
 from skyllh.core.pdfratio import (
-    SpatialSigOverBkgPDFRatio,
-    Skylab2SkylabPDFRatioFillMethod
+    SigOverBkgPDFRatio,
 )
-
-from skyllh.i3.signal_generation import PointLikeSourceI3SignalGenerationMethod
-
-# Analysis utilities.
-from skyllh.core.analysis_utils import (
-    pointlikesource_to_data_field_array
+from skyllh.core.progressbar import (
+    ProgressBar,
 )
-
-# Logging setup utilities.
-from skyllh.core.debugging import (
-    setup_logger,
-    setup_console_handler,
-    setup_file_handler
+from skyllh.core.random import (
+    RandomStateService,
 )
-
-# Pre-defined public IceCube data samples.
-from skyllh.datasets.i3 import data_samples
-
-# Analysis specific classes for working with the public data.
-from skyllh.analyses.i3.publicdata_ps.signal_generator import (
-    PDSignalGenerator
+from skyllh.core.scrambling import (
+    DataScrambler,
+    UniformRAScramblingMethod,
 )
-from skyllh.analyses.i3.publicdata_ps.detsigyield import (
-    PublicDataPowerLawFluxPointLikeSourceI3DetSigYieldImplMethod
+from skyllh.core.signal_generator import (
+    MultiDatasetSignalGenerator,
 )
-from skyllh.analyses.i3.publicdata_ps.signalpdf import (
-    PDSignalEnergyPDFSet
+from skyllh.core.signalpdf import (
+    RayleighPSFPointSourceSignalSpatialPDF,
 )
-from skyllh.analyses.i3.publicdata_ps.backgroundpdf import (
-    PDMCBackgroundI3EnergyPDF
+from skyllh.core.smoothing import (
+    BlockSmoothingFilter,
 )
-from skyllh.analyses.i3.publicdata_ps.pdfratio import (
-    PDPDFRatio
+from skyllh.core.source_hypo_grouping import (
+    SourceHypoGroup,
+    SourceHypoGroupManager,
+)
+from skyllh.core.source_model import (
+    PointLikeSource,
+)
+from skyllh.core.test_statistic import (
+    WilksTestStatistic,
+)
+from skyllh.core.timing import (
+    TimeLord,
+)
+from skyllh.core.trialdata import (
+    TrialDataManager,
+)
+from skyllh.core.utils.analysis import (
+    create_trial_data_file,
+    pointlikesource_to_data_field_array,
 )
 
+from skyllh.datasets.i3 import (
+    data_samples,
+)
 
-def psi_func(tdm, src_hypo_group_manager, fitparams):
-    """Function to calculate the opening angle between the source position
-    and the event's reconstructed position.
-    """
-    ra = tdm.get_data('ra')
-    dec = tdm.get_data('dec')
-
-    # Make the source position angles two-dimensional so the PDF value
-    # can be calculated via numpy broadcasting automatically for several
-    # sources. This is useful for stacking analyses.
-    src_ra = tdm.get_data('src_array')['ra'][:, np.newaxis]
-    src_dec = tdm.get_data('src_array')['dec'][:, np.newaxis]
-
-    delta_dec = np.abs(dec - src_dec)
-    delta_ra = np.abs(ra - src_ra)
-    x = (
-        (np.sin(delta_dec / 2.))**2. + np.cos(dec) *
-        np.cos(src_dec) * (np.sin(delta_ra / 2.))**2.
-    )
-
-    # Handle possible floating precision errors.
-    x[x < 0.] = 0.
-    x[x > 1.] = 1.
-
-    psi = (2.0*np.arcsin(np.sqrt(x)))
-
-    # For now we support only a single source, hence return psi[0].
-    return psi[0, :]
-
+from skyllh.i3.background_generation import (
+    FixedScrambledExpDataI3BkgGenMethod,
+)
+from skyllh.i3.backgroundpdf import (
+    DataBackgroundI3SpatialPDF,
+)
+from skyllh.i3.config import (
+    add_icecube_specific_analysis_required_data_fields,
+)
 
-def TXS_location():
-    src_ra = np.radians(77.358)
-    src_dec = np.radians(5.693)
-    return (src_ra, src_dec)
+from skyllh.scripting.argparser import (
+    create_argparser,
+)
+from skyllh.scripting.logging import (
+    setup_logging,
+)
 
 
 def create_analysis(
-    rss,
-    datasets,
-    source,
-    refplflux_Phi0=1,
-    refplflux_E0=1e3,
-    refplflux_gamma=2,
-    ns_seed=10.0,
-    gamma_seed=3,
-    cache_dir='.',
-    cap_ratio=False,
-    compress_data=False,
-    keep_data_fields=None,
-    optimize_delta_angle=10,
-    efficiency_mode=None,
-    tl=None,
-    ppbar=None
+        cfg,
+        datasets,
+        source,
+        refplflux_Phi0=1,
+        refplflux_E0=1e3,
+        refplflux_gamma=2.0,
+        ns_seed=100.0,
+        ns_min=0.,
+        ns_max=1e3,
+        gamma_seed=3.0,
+        gamma_min=1.,
+        gamma_max=5.,
+        kde_smoothing=False,
+        minimizer_impl='LBFGS',
+        cut_sindec=None,
+        spl_smooth=None,
+        cap_ratio=False,
+        compress_data=False,
+        keep_data_fields=None,
+        evt_sel_delta_angle_deg=10,
+        construct_sig_generator=True,
+        tl=None,
+        ppbar=None,
+        logger_name=None,
 ):
     """Creates the Analysis instance for this particular analysis.
 
-    Parameters:
-    -----------
+    Parameters
+    ----------
+    cfg : instance of Config
+        The instance of Config holding the local configuration.
     datasets : list of Dataset instances
         The list of Dataset instances, which should be used in the
         analysis.
     source : PointLikeSource instance
         The PointLikeSource instance defining the point source position.
     refplflux_Phi0 : float
         The flux normalization to use for the reference power law flux model.
     refplflux_E0 : float
         The reference energy to use for the reference power law flux model.
     refplflux_gamma : float
         The spectral index to use for the reference power law flux model.
     ns_seed : float
         Value to seed the minimizer with for the ns fit.
+    ns_min : float
+        Lower bound for ns fit.
+    ns_max : float
+        Upper bound for ns fit.
     gamma_seed : float | None
         Value to seed the minimizer with for the gamma fit. If set to None,
         the refplflux_gamma value will be set as gamma_seed.
-    cache_dir : str
-        The cache directory where to look for cached data, e.g. signal PDFs.
+    gamma_min : float
+        Lower bound for gamma fit.
+    gamma_max : float
+        Upper bound for gamma fit.
+    kde_smoothing : bool
+        Apply a KDE-based smoothing to the data-driven background pdf.
+        Default: False.
+    minimizer_impl : str
+        Minimizer implementation to be used. Supported options are ``"LBFGS"``
+        (L-BFG-S minimizer used from the :mod:`scipy.optimize` module), or
+        ``"minuit"`` (Minuit minimizer used by the :mod:`iminuit` module).
+        Default: "LBFGS".
+    cut_sindec : list of float | None
+        sin(dec) values at which the energy cut in the southern sky should
+        start. If None, np.sin(np.radians([-2, 0, -3, 0, 0])) is used.
+    spl_smooth : list of float
+        Smoothing parameters for the 1D spline for the energy cut. If None,
+        [0., 0.005, 0.05, 0.2, 0.3] is used.
+    cap_ratio : bool
+        If set to True, the energy PDF ratio will be capped to a finite value
+        where no background energy PDF information is available. This will
+        ensure that an energy PDF ratio is available for high energies where
+        no background is available from the experimental data.
+        If kde_smoothing is set to True, cap_ratio should be set to False!
+        Default is False.
     compress_data : bool
         Flag if the data should get converted from float64 into float32.
     keep_data_fields : list of str | None
         List of additional data field names that should get kept when loading
         the data.
-    optimize_delta_angle : float
+    evt_sel_delta_angle_deg : float
         The delta angle in degrees for the event selection optimization methods.
-    efficiency_mode : str | None
-        The efficiency mode the data should get loaded with. Possible values
-        are:
-
-            - 'memory':
-                The data will be load in a memory efficient way. This will
-                require more time, because all data records of a file will
-                be loaded sequentially.
-            - 'time':
-                The data will be loaded in a time efficient way. This will
-                require more memory, because each data file gets loaded in
-                memory at once.
-
-        The default value is ``'time'``. If set to ``None``, the default
-        value will be used.
+    construct_sig_generator : bool
+        Flag if the signal generator should be constructed (``True``) or not
+        (``False``).
     tl : TimeLord instance | None
         The TimeLord instance to use to time the creation of the analysis.
     ppbar : ProgressBar instance | None
         The instance of ProgressBar for the optional parent progress bar.
+    logger_name : str | None
+        The name of the logger to be used. If set to ``None``, ``__name__`` will
+        be used.
 
     Returns
     -------
-    analysis : SpatialEnergyTimeIntegratedMultiDatasetSingleSourceAnalysis
+    ana : instance of SingleSourceMultiDatasetLLHRatioAnalysis
         The Analysis instance for this analysis.
     """
-    # Define the flux model.
-    flux_model = PowerLawFlux(
-        Phi0=refplflux_Phi0, E0=refplflux_E0, gamma=refplflux_gamma)
+    add_icecube_specific_analysis_required_data_fields(cfg)
 
-    # Define the fit parameter ns.
-    fitparam_ns = FitParameter('ns', 0, 1e3, ns_seed)
+    if logger_name is None:
+        logger_name = __name__
+    logger = get_logger(logger_name)
 
-    # Define the gamma fit parameter.
-    fitparam_gamma = FitParameter(
-        'gamma', valmin=1, valmax=5, initial=gamma_seed)
-
-    # Define the detector signal efficiency implementation method for the
-    # IceCube detector and this source and flux_model.
-    # The sin(dec) binning will be taken by the implementation method
-    # automatically from the Dataset instance.
-    gamma_grid = fitparam_gamma.as_linear_grid(delta=0.1)
-    detsigyield_implmethod = \
-        PublicDataPowerLawFluxPointLikeSourceI3DetSigYieldImplMethod(
-            gamma_grid)
-
-    # Define the signal generation method.
-    #sig_gen_method = PointLikeSourceI3SignalGenerationMethod()
-    sig_gen_method = None
+    # Create the minimizer instance.
+    if minimizer_impl == "LBFGS":
+        minimizer = Minimizer(LBFGSMinimizerImpl(cfg=cfg))
+    elif minimizer_impl == "minuit":
+        minimizer = Minimizer(IMinuitMinimizerImpl(cfg=cfg, ftol=1e-8))
+    else:
+        raise NameError(
+            f"Minimizer implementation `{minimizer_impl}` is not supported "
+            "Please use `LBFGS` or `minuit`.")
 
-    # Create a source hypothesis group manager.
-    src_hypo_group_manager = SourceHypoGroupManager(
-        SourceHypoGroup(
-            source, flux_model, detsigyield_implmethod, sig_gen_method))
+    # Define the flux model.
+    fluxmodel = SteadyPointlikeFFM(
+        Phi0=refplflux_Phi0,
+        energy_profile=PowerLawEnergyFluxProfile(
+            E0=refplflux_E0,
+            gamma=refplflux_gamma,
+            cfg=cfg,
+        ),
+        cfg=cfg,
+    )
 
-    # Create a source fit parameter mapper and define the fit parameters.
-    src_fitparam_mapper = SingleSourceFitParameterMapper()
-    src_fitparam_mapper.def_fit_parameter(fitparam_gamma)
+    # Define the fit parameter ns.
+    param_ns = Parameter(
+        name='ns',
+        initial=ns_seed,
+        valmin=ns_min,
+        valmax=ns_max)
+
+    # Define the fit parameter gamma.
+    param_gamma = Parameter(
+        name='gamma',
+        initial=gamma_seed,
+        valmin=gamma_min,
+        valmax=gamma_max)
+
+    # Define the detector signal yield builder for the IceCube detector and this
+    # source and flux model.
+    # The sin(dec) binning will be taken by the builder automatically from the
+    # Dataset instance.
+    gamma_grid = param_gamma.as_linear_grid(delta=0.1)
+    detsigyield_builder =\
+        PDSingleParamFluxPointLikeSourceI3DetSigYieldBuilder(
+            cfg=cfg,
+            param_grid=gamma_grid)
+
+    # Create a source hypothesis group manager with a single source hypothesis
+    # group for the single source.
+    shg_mgr = SourceHypoGroupManager(
+        SourceHypoGroup(
+            sources=source,
+            fluxmodel=fluxmodel,
+            detsigyield_builders=detsigyield_builder,
+        ))
+    logger.info(str(shg_mgr))
+
+    # Define a detector model for the ns fit parameter.
+    detector_model = DetectorModel('IceCube')
+
+    # Define the parameter model mapper for the analysis, which will map global
+    # parameters to local source parameters.
+    pmm = ParameterModelMapper(
+        models=[detector_model, source])
+    pmm.map_param(param_ns, models=detector_model)
+    pmm.map_param(param_gamma, models=source)
+    logger.info(str(pmm))
 
     # Define the test statistic.
-    test_statistic = TestStatisticWilks()
+    test_statistic = WilksTestStatistic()
+
+    # Create the Analysis instance.
+    ana = Analysis(
+        cfg=cfg,
+        shg_mgr=shg_mgr,
+        pmm=pmm,
+        test_statistic=test_statistic,
+        sig_generator_cls=MultiDatasetSignalGenerator,
+    )
 
     # Define the data scrambler with its data scrambling method, which is used
     # for background generation.
     data_scrambler = DataScrambler(UniformRAScramblingMethod())
 
-    # Create background generation method.
-    bkg_gen_method = FixedScrambledExpDataI3BkgGenMethod(data_scrambler)
-
-    # Create the minimizer instance.
-    minimizer = Minimizer(LBFGSMinimizerImpl())
-
-    # Create the Analysis instance.
-    analysis = Analysis(
-        src_hypo_group_manager,
-        src_fitparam_mapper,
-        fitparam_ns,
-        test_statistic,
-        bkg_gen_method,
-        sig_generator_cls=PDSignalGenerator
-    )
+    # Create background generation method, which will be used for all datasets.
+    bkg_gen_method = FixedScrambledExpDataI3BkgGenMethod(
+        cfg=cfg,
+        data_scrambler=data_scrambler)
 
     # Define the event selection method for pure optimization purposes.
     # We will use the same method for all datasets.
     event_selection_method = SpatialBoxEventSelectionMethod(
-        src_hypo_group_manager, delta_angle=np.deg2rad(optimize_delta_angle))
-    #event_selection_method = None
+        shg_mgr=shg_mgr,
+        delta_angle=np.deg2rad(evt_sel_delta_angle_deg))
+
+    # Prepare the spline parameters for the signal generator.
+    if cut_sindec is None:
+        cut_sindec = np.sin(np.radians([-2, 0, -3, 0, 0]))
+    if spl_smooth is None:
+        spl_smooth = [0., 0.005, 0.05, 0.2, 0.3]
+    if len(spl_smooth) < len(datasets) or len(cut_sindec) < len(datasets):
+        raise AssertionError(
+            'The length of the spl_smooth and of the cut_sindec must be equal '
+            f'to the length of datasets: {len(datasets)}.')
 
     # Add the data sets to the analysis.
     pbar = ProgressBar(len(datasets), parent=ppbar).start()
-    for ds in datasets:
-        # Load the data of the data set.
+    for (ds_idx, ds) in enumerate(datasets):
         data = ds.load_and_prepare_data(
             keep_fields=keep_data_fields,
             compress=compress_data,
-            efficiency_mode=efficiency_mode,
             tl=tl)
 
-        # Create a trial data manager and add the required data fields.
-        tdm = TrialDataManager()
-        tdm.add_source_data_field('src_array',
-                                  pointlikesource_to_data_field_array)
-        tdm.add_data_field('psi', psi_func)
-
         sin_dec_binning = ds.get_binning_definition('sin_dec')
         log_energy_binning = ds.get_binning_definition('log_energy')
 
         # Create the spatial PDF ratio instance for this dataset.
         spatial_sigpdf = RayleighPSFPointSourceSignalSpatialPDF(
+            cfg=cfg,
             dec_range=np.arcsin(sin_dec_binning.range))
         spatial_bkgpdf = DataBackgroundI3SpatialPDF(
-            data.exp, sin_dec_binning)
-        spatial_pdfratio = SpatialSigOverBkgPDFRatio(
-            spatial_sigpdf, spatial_bkgpdf)
+            cfg=cfg,
+            data_exp=data.exp,
+            sin_dec_binning=sin_dec_binning)
+        spatial_pdfratio = SigOverBkgPDFRatio(
+            cfg=cfg,
+            sig_pdf=spatial_sigpdf,
+            bkg_pdf=spatial_bkgpdf)
 
         # Create the energy PDF ratio instance for this dataset.
         energy_sigpdfset = PDSignalEnergyPDFSet(
+            cfg=cfg,
             ds=ds,
             src_dec=source.dec,
-            flux_model=flux_model,
-            fitparam_grid_set=gamma_grid,
+            fluxmodel=fluxmodel,
+            param_grid_set=gamma_grid,
             ppbar=ppbar
         )
+        smoothing_filter = BlockSmoothingFilter(nbins=1)
+        energy_bkgpdf = PDDataBackgroundI3EnergyPDF(
+            cfg=cfg,
+            data_exp=data.exp,
+            logE_binning=log_energy_binning,
+            sinDec_binning=sin_dec_binning,
+            smoothing_filter=smoothing_filter,
+            kde_smoothing=kde_smoothing)
 
-        #smoothing_filter = BlockSmoothingFilter(nbins=1)
-        #energy_bkgpdf = DataBackgroundI3EnergyPDF(
-        #    data.exp, log_energy_binning, sin_dec_binning, smoothing_filter)
-
-        bkg_pdf_pathfilename = ds.get_abs_pathfilename_list(
-            ds.get_aux_data_definition('pdf_bkg_datafile'))[0]
-        with open(bkg_pdf_pathfilename, 'rb') as f:
-            bkg_pdf_data = pickle.load(f)
-        energy_bkgpdf = PDMCBackgroundI3EnergyPDF(
-            pdf_log10emu_sindecmu=bkg_pdf_data['pdf'],
-            sindecmu_binning=bkg_pdf_data['sindecmu_binning'],
-            log10emu_binning=bkg_pdf_data['log10emu_binning']
-        )
-
-        energy_pdfratio = PDPDFRatio(
+        energy_pdfratio = PDSigSetOverBkgPDFRatio(
+            cfg=cfg,
             sig_pdf_set=energy_sigpdfset,
             bkg_pdf=energy_bkgpdf,
-            cap_ratio=cap_ratio
+            cap_ratio=cap_ratio)
+
+        pdfratio = spatial_pdfratio * energy_pdfratio
+
+        # Create a trial data manager and add the required data fields.
+        tdm = TrialDataManager()
+        tdm.add_source_data_field(
+            name='src_array',
+            func=pointlikesource_to_data_field_array)
+        tdm.add_data_field(
+            name='psi',
+            func=get_tdm_field_func_psi(),
+            dt='dec',
+            is_srcevt_data=True)
+
+        energy_cut_spline = create_energy_cut_spline(
+            ds,
+            data.exp,
+            spl_smooth[ds_idx])
+
+        bkg_generator = DatasetBackgroundGenerator(
+            cfg=cfg,
+            dataset=ds,
+            data=data,
+            bkg_gen_method=bkg_gen_method,
         )
 
-        pdfratios = [spatial_pdfratio, energy_pdfratio]
+        sig_generator = PDDatasetSignalGenerator(
+            cfg=cfg,
+            shg_mgr=shg_mgr,
+            ds=ds,
+            ds_idx=ds_idx,
+            energy_cut_spline=energy_cut_spline,
+            cut_sindec=cut_sindec[ds_idx],
+        )
 
-        analysis.add_dataset(
-            ds, data, pdfratios, tdm, event_selection_method)
+        ana.add_dataset(
+            dataset=ds,
+            data=data,
+            pdfratio=pdfratio,
+            tdm=tdm,
+            event_selection_method=event_selection_method,
+            bkg_generator=bkg_generator,
+            sig_generator=sig_generator)
 
         pbar.increment()
     pbar.finish()
 
-    analysis.llhratio = analysis.construct_llhratio(minimizer, ppbar=ppbar)
+    ana.construct_services(
+        ppbar=ppbar)
 
-    # analysis.construct_signal_generator()
+    ana.llhratio = ana.construct_llhratio(
+        minimizer=minimizer,
+        ppbar=ppbar)
 
-    return analysis
+    if construct_sig_generator is True:
+        ana.construct_signal_generator()
 
+    return ana
 
-if(__name__ == '__main__'):
-    p = argparse.ArgumentParser(
+
+if __name__ == '__main__':
+    parser = create_argparser(
         description='Calculates TS for a given source location using the '
-        '10-year public point source sample.',
-        formatter_class=argparse.RawTextHelpFormatter
+                    '10-year public point source sample.',
     )
-    p.add_argument(
+
+    parser.add_argument(
         '--dec',
-        default=23.8,
+        dest='dec',
+        default=5.7,
         type=float,
         help='The source declination in degrees.'
     )
-    p.add_argument(
+    parser.add_argument(
         '--ra',
-        default=216.76,
+        dest='ra',
+        default=77.35,
         type=float,
         help='The source right-ascention in degrees.'
     )
-    p.add_argument(
+    parser.add_argument(
         '--gamma-seed',
+        dest='gamma_seed',
         default=3,
         type=float,
         help='The seed value of the gamma fit parameter.'
     )
-    p.add_argument(
-        '--data_base_path',
-        default=None,
-        type=str,
-        help='The base path to the data samples (default=None)'
-    )
-    p.add_argument(
-        '--pdf-seed',
-        default=1,
-        type=int,
-        help='The random number generator seed for generating the '
-             'signal PDF.'
-    )
-    p.add_argument(
-        '--seed',
-        default=1,
-        type=int,
-        help='The random number generator seed for the likelihood '
-             'minimization.'
-    )
-    p.add_argument(
-        '--ncpu',
-        default=1,
-        type=int,
-        help='The number of CPUs to utilize where parallelization is possible.'
-    )
-    p.add_argument(
-        '--cache-dir',
-        default='.',
-        type=str,
-        help='The cache directory to look for cached data, e.g. signal PDFs.')
-    p.add_argument(
+
+    parser.add_argument(
         '--cap-ratio',
+        dest='cap_ratio',
+        default=False,
         action='store_true',
         help='Switch to cap the energy PDF ratio.')
-    p.set_defaults(cap_ratio=False)
-    args = p.parse_args()
 
-    # Setup `skyllh` package logging.
-    # To optimize logging set the logging level to the lowest handling level.
-    setup_logger('skyllh', logging.DEBUG)
-    log_format = '%(asctime)s %(processName)s %(name)s %(levelname)s: '\
-                 '%(message)s'
-    setup_console_handler('skyllh', logging.INFO, log_format)
-    setup_file_handler('skyllh', 'debug.log',
-        log_level=logging.DEBUG,
-        log_format=log_format)
+    args = parser.parse_args()
 
-    CFG['multiproc']['ncpu'] = args.ncpu
+    cfg = Config.from_yaml(args.config)
+    cfg.set_enable_tracing(args.enable_tracing)
+    cfg.set_ncpu(args.n_cpu)
+
+    setup_logging(
+        cfg=cfg,
+        script_logger_name=__name__,
+        debug_pathfilename=args.debug_logfile)
 
     sample_seasons = [
-        #('PublicData_10y_ps', 'IC40'),
-        #('PublicData_10y_ps', 'IC59'),
-        #('PublicData_10y_ps', 'IC79'),
-        #('PublicData_10y_ps', 'IC86_I'),
-        ('PublicData_10y_ps', 'IC86_II'),
-        #('PublicData_10y_ps', 'IC86_II-VII')
+        ('PublicData_10y_ps', 'IC40'),
+        ('PublicData_10y_ps', 'IC59'),
+        ('PublicData_10y_ps', 'IC79'),
+        ('PublicData_10y_ps', 'IC86_I'),
+        ('PublicData_10y_ps', 'IC86_II-VII'),
     ]
 
     datasets = []
     for (sample, season) in sample_seasons:
         # Get the dataset from the correct dataset collection.
         dsc = data_samples[sample].create_dataset_collection(
-            args.data_base_path)
+            cfg=cfg,
+            base_path=args.data_basepath)
         datasets.append(dsc.get_dataset(season))
 
     # Define a random state service.
-    rss_pdf = RandomStateService(args.pdf_seed)
     rss = RandomStateService(args.seed)
+
     # Define the point source.
-    source = PointLikeSource(np.deg2rad(args.ra), np.deg2rad(args.dec))
-    print('source: ', str(source))
+    source = PointLikeSource(
+        name='My Point-Like-Source',
+        ra=np.deg2rad(args.ra),
+        dec=np.deg2rad(args.dec))
+    print(f'source: {source}')
 
     tl = TimeLord()
 
     with tl.task_timer('Creating analysis.'):
         ana = create_analysis(
-            rss_pdf,
-            datasets,
-            source,
-            cache_dir=args.cache_dir,
-            cap_ratio=args.cap_ratio,
+            cfg=cfg,
+            datasets=datasets,
+            source=source,
             gamma_seed=args.gamma_seed,
+            cap_ratio=args.cap_ratio,
             tl=tl)
 
     with tl.task_timer('Unblinding data.'):
-        (TS, fitparam_dict, status) = ana.unblind(rss)
+        (TS, param_dict, status) = ana.unblind(rss)
 
-    print('TS = %g' % (TS))
-    print('ns_fit = %g' % (fitparam_dict['ns']))
-    print('gamma_fit = %g' % (fitparam_dict['gamma']))
-
-
-    # Generate some signal events.
-    #ana.construct_signal_generator()
-    #with tl.task_timer('Generating signal events.'):
-    #    (n_sig, signal_events_dict) =\
-    #        ana.sig_generator.generate_signal_events(rss, 100)
-
-    #trials = ana.do_trials(
-    #       rss, 100, mean_n_sig=20
-    #)
-
-    #print('n_sig: %d'%n_sig)
-    #print('signal datasets: '+str(signal_events_dict.keys()))
+    print(f'TS = {TS:g}')
+    print(f'ns_fit = {param_dict["ns"]:g}')
+    print(f'gamma_fit = {param_dict["gamma"]:g}')
+    print(f'minimizer status = {status}')
 
+    print(tl)
 
+    tl = TimeLord()
+    rss = RandomStateService(seed=1)
+    (_, _, _, trials) = create_trial_data_file(
+        ana=ana,
+        rss=rss,
+        n_trials=10,
+        mean_n_sig=0,
+        pathfilename=None,
+        ncpu=1,
+        tl=tl)
+    print(f'trials: {trials}')
     print(tl)
```

### Comparing `skyllh-23.1.1/skyllh/analyses/i3/publicdata_ps/signal_generator.py` & `skyllh-23.2.0/skyllh/core/background_generation.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,658 +1,562 @@
 # -*- coding: utf-8 -*-
 
+import abc
+
 import numpy as np
-from scipy import interpolate
-import scipy.stats
 
+from skyllh.core.config import (
+    HasConfig,
+)
+from skyllh.core.datafields import (
+    DataFields,
+    DataFieldStages as DFS,
+)
+from skyllh.core.debugging import (
+    get_logger,
+)
+from skyllh.core.event_selection import (
+    AllEventSelectionMethod,
+    EventSelectionMethod,
+)
 from skyllh.core.py import (
-    issequenceof,
+    classname,
     float_cast,
-    int_cast
+    func_has_n_args,
+    issequenceof,
+)
+from skyllh.core.scrambling import (
+    DataScrambler,
 )
-from skyllh.core.py import module_classname
-from skyllh.core.debugging import get_logger
-from skyllh.core.signal_generator import SignalGeneratorBase
-from skyllh.core.llhratio import LLHRatio
-from skyllh.core.dataset import Dataset
-from skyllh.core.source_hypothesis import SourceHypoGroupManager
-from skyllh.core.storage import DataFieldRecordArray
-
-from skyllh.analyses.i3.publicdata_ps.utils import psi_to_dec_and_ra
-from skyllh.analyses.i3.publicdata_ps.smearing_matrix import (
-    PDSmearingMatrix
+from skyllh.core.timing import (
+    TaskTimer,
 )
-from skyllh.analyses.i3.publicdata_ps.aeff import PDAeff
 
 
-class PDDatasetSignalGenerator(object):
-    """This class provides a signal generation method for a point-like source
-    seen in the IceCube detector using one dataset of the 10 years public data
-    release. It is used by the PDSignalGenerator class in a loop over all the
-    datasets that have been added to the analysis.
+logger = get_logger(__name__)
+
+
+class BackgroundGenerationMethod(
+        HasConfig,
+        metaclass=abc.ABCMeta,
+):
+    """This is the abstract base class for a detector specific background
+    generation method.
     """
 
-    def __init__(self, ds, src_dec, effA=None, sm=None, **kwargs):
-        """Creates a new instance of the signal generator for generating
-        signal events from a specific public data dataset.
-
-        Parameters:
-        -----------
-        ds : Dataset instance
-            Dataset instance for which signal events should get
-            generated for.
-        src_dec : float
-            The declination of the source in radians.
-        effA : PDAeff | None
-            Representation of the effective area provided by the public data.
-        sm : PDSmearingMatrix | None
-            Representation of the smearing matrix provided by the public data.
+    def __init__(
+            self,
+            **kwargs,
+    ):
+        """Constructs a new background generation method instance.
         """
         super().__init__(**kwargs)
 
-        self._logger = get_logger(module_classname(self))
-
-        if sm is None:
-            self.smearing_matrix = PDSmearingMatrix(
-                pathfilenames=ds.get_abs_pathfilename_list(
-                    ds.get_aux_data_definition('smearing_datafile')))
-        else:
-            self.smearing_matrix = sm
-
-        if effA is None:
-            dec_idx = self.smearing_matrix.get_true_dec_idx(src_dec)
-            (min_log_true_e,
-             max_log_true_e) = \
-                self.smearing_matrix.get_true_log_e_range_with_valid_log_e_pdfs(
-                    dec_idx)
-            kwargs = {
-                'src_dec': src_dec,
-                'min_log10enu': min_log_true_e,
-                'max_log10enu': max_log_true_e
-            }
-            self.effA = PDAeff(
-                pathfilenames=ds.get_abs_pathfilename_list(
-                    ds.get_aux_data_definition('eff_area_datafile')),
-                **kwargs)
-
-        else:
-            self.effA = effA
+    def change_shg_mgr(self, shg_mgr):
+        """Notifies the background generation method about an updated
+        SourceHypoGroupManager instance.
 
-    def _generate_inv_cdf_spline(self, flux_model, log_e_min,
-                                 log_e_max):
-        """Sample the true neutrino energy from the power-law
-        re-weighted with the detection probability.
-        """
-        m = (self.effA.log10_enu_bincenters >= log_e_min) & (
-            self.effA.log10_enu_bincenters < log_e_max)
-        bin_centers = self.effA.log10_enu_bincenters[m]
-        low_bin_edges = self.effA._log10_enu_binedges_lower[m]
-        high_bin_edges = self.effA._log10_enu_binedges_upper[m]
-
-        # Flux probability P(E_nu | gamma) per bin.
-        flux_prob = flux_model.get_integral(
-            10**low_bin_edges, 10**high_bin_edges
-        ) / flux_model.get_integral(
-            10**low_bin_edges[0], 10**high_bin_edges[-1]
-        )
-
-        # Do the product and normalize again to a probability per bin.
-        product = flux_prob * self.effA.det_prob
-        prob_per_bin = product / np.sum(product)
-
-        # The probability per bin cannot be zero, otherwise the cumulative
-        # sum would not be increasing monotonically. So we set zero bins to
-        # 1000 times smaller than the smallest non-zero bin.
-        m = prob_per_bin == 0
-        prob_per_bin[m] = np.min(prob_per_bin[np.invert(m)]) / 1000
-        to_keep = np.where(prob_per_bin > 1e-15)[0]  # For numerical stability
-        prob_per_bin = prob_per_bin[to_keep]
-        prob_per_bin /= np.sum(prob_per_bin)
-
-        # Compute the cumulative distribution CDF.
-        cum_per_bin = [np.sum(prob_per_bin[:i])
-                       for i in range(prob_per_bin.size+1)]
-        if np.any(np.diff(cum_per_bin) == 0):
-            raise ValueError(
-                'The cumulative sum of the true energy probability is not '
-                'monotonically increasing! Values of the cumsum are '
-                f'{cum_per_bin}.')
-
-        bin_centers = bin_centers[to_keep]
-        bin_centers = np.concatenate(([low_bin_edges[0]], bin_centers))
-
-        # Build a spline for the inverse CDF.
-        return interpolate.splrep(cum_per_bin, bin_centers, k=1, s=0)
-
-    @staticmethod
-    def _eval_spline(x, spl):
-        x = np.asarray(x)
-        if (x.any() < 0 or x.any() > 1):
-            raise ValueError(
-                f'{x} is outside of the valid spline range. '
-                'The valid range is [0,1].')
-        values = interpolate.splev(x, spl, ext=3)
-        return values
-
-    def _generate_events(
-            self, rss, src_dec, src_ra, dec_idx,
-            log_true_e_inv_cdf_spl, n_events):
-        """Generates `n_events` signal events for the given source location
-        and flux model.
+        Parameters
+        ----------
+        shg_mgr : instance of SourceHypoGroupManager
+            The new instance of SourceHypoGroupManager.
+        """
+        pass
 
-        Note:
-            Some values can be NaN in cases where a PDF was not available!
+    @abc.abstractmethod
+    def generate_events(
+            self,
+            rss,
+            dataset,
+            data,
+            mean,
+            tl=None,
+            **kwargs,
+    ):
+        """This method is supposed to generate a `mean` number of background
+        events for the given dataset and its data.
 
         Parameters
         ----------
         rss : instance of RandomStateService
-            The instance of RandomStateService to use for drawing random
-            numbers.
-        src_dec : float
-            The declination of the source in radians.
-        src_ra : float
-            The right-ascention of the source in radians.
-
-        Returns
-        -------
-        events : numpy record array of size `n_events`
-            The numpy record array holding the event data.
-            It contains the following data fields:
-                - 'isvalid'
-                - 'log_true_energy'
-                - 'log_energy'
-                - 'sin_dec'
-            Single values can be NaN in cases where a pdf was not available.
-        """
-
-        # Create the output event DataFieldRecordArray.
-        out_dtype = [
-            ('isvalid', np.bool_),
-            ('log_true_energy', np.double),
-            ('log_energy', np.double),
-            ('dec', np.double),
-            ('ra', np.double),
-            ('sin_dec', np.double),
-            ('ang_err', np.double),
-            ('time', int),
-            ('azi', np.double),
-            ('zen', np.double),
-            ('run', int)
-        ]
-
-        data = dict(
-            [(out_dt[0], np.empty(
-                (n_events,),
-                dtype=out_dt[1])
-              ) for out_dt in out_dtype]
-        )
-
-        events = DataFieldRecordArray(data, copy=False)
-
-        sm = self.smearing_matrix
-
-        log_true_e = self._eval_spline(
-            rss.random.uniform(size=n_events), log_true_e_inv_cdf_spl)
-
-        events['log_true_energy'] = log_true_e
-
-        log_true_e_idxs = (
-            np.digitize(log_true_e, bins=sm.true_e_bin_edges) - 1
-        )
-
-        # Sample reconstructed energies given true neutrino energies.
-        (log_e_idxs, log_e) = sm.sample_log_e(rss, dec_idx, log_true_e_idxs)
-        events['log_energy'] = log_e
-
-        # Sample reconstructed psi values given true neutrino energy and
-        # reconstructed energy.
-        (psi_idxs, psi) = sm.sample_psi(
-            rss, dec_idx, log_true_e_idxs, log_e_idxs)
-
-        # Sample reconstructed ang_err values given true neutrino energy,
-        # reconstructed energy, and psi.
-        (ang_err_idxs, ang_err) = sm.sample_ang_err(
-            rss, dec_idx, log_true_e_idxs, log_e_idxs, psi_idxs)
-
-        isvalid = np.invert(
-            np.isnan(log_e) | np.isnan(psi) | np.isnan(ang_err))
-        events['isvalid'] = isvalid
-
-        # Convert the psf into a set of (r.a. and dec.). Only use non-nan
-        # values.
-        (dec, ra) = psi_to_dec_and_ra(rss, src_dec, src_ra, psi[isvalid])
-        events['ra'][isvalid] = ra
-        events['dec'][isvalid] = dec
-        events['sin_dec'][isvalid] = np.sin(dec)
-
-        # Add an angular error. Only use non-nan values.
-        events['ang_err'][isvalid] = ang_err[isvalid]
-
-        # Add fields required by the framework
-        events['time'] = np.ones(n_events)
-        events['azi'] = np.ones(n_events)
-        events['zen'] = np.ones(n_events)
-        events['run'] = -1 * np.ones(n_events)
-
-        return events
-
-    @staticmethod
-    @np.vectorize
-    def energy_filter(events, spline, cut_sindec, logger):
-        """The energy filter will select all events below `cut_sindec`
-        that have an energy smaller than the energy spline at their
-        declination.
-
-        Paramters
-        ---------
-        events : numpy record array
-            Numpy record array with the generated signal events.
-        energy_cut_splines : scipy.interpolate.UnivariateSpline
-            A spline of E(sin_dec) that defines the declination
-            dependent energy cut in the IceCube southern sky.
-        cut_sindec : float
-            The sine of the declination to start applying the energy cut. 
-            The cut will be applied from this declination down.
-        logger : logging.Logger
-            The Logger instance.
-
-        Returns
-        energy_filter : (len(events),)-shaped numpy ndarray
-            A mask of shape `len(events)` of the events to be cut.
-        """
-        if cut_sindec is None:
-            logger.warn(
-                'No `cut_sindec` has been specified. The energy cut will be '
-                'applied in [-90, 0] deg.')
-            cut_sindec = 0.
-        energy_filter = np.logical_and(
-            events['sin_dec'] < cut_sindec,
-            events['log_energy'] < spline(events['sin_dec']))
-
-        return energy_filter
-
-    def generate_signal_events(
-            self, rss, src_dec, src_ra, flux_model, n_events,
-            energy_cut_spline=None, cut_sindec=None):
-        """Generates ``n_events`` signal events for the given source location
-        and flux model.
-
-        Paramters
-        ---------
-        rss : RandomStateService
-        src_dec : float
-            Declination coordinate of the injection point.
-        src_ra : float
-            Right ascension coordinate of the injection point.
-        flux_model : FluxModel
-            Instance of the `FluxModel` class.
-        n_events : int
-            Number of signal events to be generated.
-        energy_cut_splines : scipy.interpolate.UnivariateSpline
-            A spline of E(sin_dec) that defines the declination
-            dependent energy cut in the IceCube southern sky.
-        cut_sindec : float
-            The sine of the declination to start applying the energy cut. 
-            The cut will be applied from this declination down.
+            The instance of RandomStateService that should be used to generate
+            random numbers from.
+        dataset : instance of Dataset
+            The Dataset instance describing the dataset for which background
+            events should get generated.
+        data : instance of DatasetData
+            The DatasetData instance holding the data of the dataset for which
+            background events should get generated.
+        mean : float
+            The mean number of background events to generate.
+        tl : instance of TimeLord | None
+            The optional instance of TimeLord that should be used to collect
+            timing information about this method.
+        **kwargs
+            Additional keyword arguments, which might be required for a
+            particular background generation method.
 
         Returns
         -------
-        events : numpy record array
-            The numpy record array holding the event data.
-            It contains the following data fields:
-                - 'isvalid'
-                - 'log_true_energy'
-                - 'log_energy'
-                - 'dec'
-                - 'ra'
-                - 'ang_err'
-        """
-        sm = self.smearing_matrix
-
-        # Find the declination bin index.
-        dec_idx = sm.get_true_dec_idx(src_dec)
-
-        # Determine the true energy range for which log_e PDFs are available.
-        (min_log_true_e,
-         max_log_true_e) = sm.get_true_log_e_range_with_valid_log_e_pdfs(
-             dec_idx)
-        # Build the spline for the inverse CDF and draw a true neutrino
-        # energy from the hypothesis spectrum.
-        log_true_e_inv_cdf_spl = self._generate_inv_cdf_spline(
-            flux_model, min_log_true_e, max_log_true_e)
-
-        events = None
-        n_evt_generated = 0
-        while n_evt_generated != n_events:
-            n_evt = n_events - n_evt_generated
-
-            events_ = self._generate_events(
-                rss, src_dec, src_ra, dec_idx, log_true_e_inv_cdf_spl, n_evt)
-
-            # Cut events that failed to be generated due to missing PDFs.
-            # Also cut low energy events if generating in the southern sky.
-            events_ = events_[events_['isvalid']]
-            if energy_cut_spline is not None:
-                to_cut = self.energy_filter(
-                    events_, energy_cut_spline, cut_sindec, self._logger)
-                events_ = events_[~to_cut]
-            if not len(events_) == 0:
-                n_evt_generated += len(events_)
-                if events is None:
-                    events = events_
-                else:
-                    events.append(events_)
-
-        return events
-
-
-class PDSignalGenerator(SignalGeneratorBase):
-    """This class provides a signal generation method for a point-like source
-    seen in the IceCube detector using the 10 years public data release.
+        n_bkg : int
+            The number of generated background events.
+        bkg_events : instance of DataFieldRecordArray
+            The instance of DataFieldRecordArray holding the generated
+            background events. The number of events in this array might be less
+            than ``n_bkg`` if an event selection method was used for
+            optimization purposes. The difference ``n_bkg - len(bkg_events)`` is
+            then the number of pure background events in the generated
+            background event sample.
+        """
+        pass
+
+
+class MCDataSamplingBkgGenMethod(
+        BackgroundGenerationMethod,
+):
+    """This class implements the method to generate background events from
+    monte-carlo (MC) data by sampling events from the MC data set according to a
+    probability value given for each event. Functions can be provided to get the
+    mean number of background events and the probability of each monte-carlo
+    event.
     """
-
-    def __init__(self, src_hypo_group_manager, dataset_list, data_list=None,
-                 llhratio=None, energy_cut_splines=None, cut_sindec=None):
-        """Constructs a new signal generator instance.
+    def __init__(
+            self,
+            get_event_prob_func,
+            get_mean_func=None,
+            unique_events=False,
+            data_scrambler=None,
+            mc_inplace_scrambling=False,
+            keep_mc_data_fields=None,
+            pre_event_selection_method=None,
+            **kwargs,
+    ):
+        """Creates a new instance of the MCDataSamplingBkgGenMethod class.
 
         Parameters
         ----------
-        src_hypo_group_manager : SourceHypoGroupManager instance
-            The SourceHypoGroupManager instance defining the source hypothesis
-            groups.
-        dataset_list : list of Dataset instances
-            The list of Dataset instances for which signal events should get
-            generated for.
-        data_list : list of DatasetData instances
-            The list of DatasetData instances holding the actual data of each
-            dataset. The order must match the order of ``dataset_list``.
-        llhratio : LLHRatio
-            The likelihood ratio object contains the datasets signal weights
-            needed for distributing the event generation among the different
-            datasets.
-        energy_cut_splines : list of UnivariateSpline
-            A list of splines of E(sin_dec) used to define the declination
-            dependent energy cut in the IceCube southern sky.
-        cut_sindec : list of float
-            The sine of the declination to start applying the energy cut. 
-            The cut will be applied from this declination down.
-        """
-        self.src_hypo_group_manager = src_hypo_group_manager
-        self.dataset_list = dataset_list
-        self.data_list = data_list
-        self.llhratio = llhratio
-        self.effA = [None] * len(self._dataset_list)
-        self.sm = [None] * len(self._dataset_list)
-        self.splines = energy_cut_splines
-        self.cut_sindec = cut_sindec
+        get_event_prob_func : callable
+            The function to get the background probability of each monte-carlo
+            event. The call signature of this function must be
+
+                __call__(dataset, data, events)
+
+            where ``dataset`` is an instance of Dataset and ``data`` is an
+            instance of DatasetData of the data set for which background events
+            needs to get generated. The ``events`` argument holds the actual
+            set of events, for which the background event probabilities need to
+            get calculated.
+        get_mean_func : callable | None
+            The function to get the mean number of background events.
+            The call signature of this function must be
+
+                __call__(dataset, data, events)
+
+            where ``dataset`` is an instance of Dataset and ``data`` is an
+            instance of DatasetData of the data set for which background events
+            needs to get generated. The `events` argument holds the actual set
+            of events, for which the mean number of background events should get
+            calculated. This argument can be `None`, which means that the mean
+            number of background events to generate needs to get specified
+            through the ``generate_events`` method. However, if an event
+            selection method is provided, this argument cannot be ``None``!
+        unique_events : bool
+            Flag if unique events should be drawn from the monte-carlo
+            (``True``), or if events can be drawn several times (``False``).
+            Default is ``False``.
+        data_scrambler : instance of DataScrambler | None
+            If set to an instance of DataScrambler, the drawn monte-carlo
+            background events will get scrambled. This can ensure more
+            independent data trials. It is especially important when monte-carlo
+            statistics are low.
+        mc_inplace_scrambling : bool
+            Flag if the scrambling of the monte-carlo data should be done
+            inplace, i.e. without creating a copy of the MC data first.
+            Default is False.
+        keep_mc_data_fields : str | list of str | None
+            The MC data field names that should be kept in order to be able to
+            calculate the background events rates by the functions
+            ``get_event_prob_func`` and ``get_mean_func``. All other MC fields
+            will get droped due to computational efficiency reasons.
+        pre_event_selection_method : instance of EventSelectionMethod | None
+            If set to an instance of EventSelectionMethod, this method will
+            pre-select the MC events that will be used for later background
+            event generation. Using this pre-selection a large portion of the
+            MC data can be reduced prior to background event generation.
+        """
+        super().__init__(
+            **kwargs)
+
+        self.get_event_prob_func = get_event_prob_func
+        self.get_mean_func = get_mean_func
+        self.unique_events = unique_events
+        self.data_scrambler = data_scrambler
+        self.mc_inplace_scrambling = mc_inplace_scrambling
+        self.keep_mc_data_field_names = keep_mc_data_fields
+        self.pre_event_selection_method = pre_event_selection_method
 
-    @property
-    def src_hypo_group_manager(self):
-        """The SourceHypoGroupManager instance defining the source groups with
-        their spectra.
-        """
-        return self._src_hypo_group_manager
-
-    @src_hypo_group_manager.setter
-    def src_hypo_group_manager(self, manager):
-        if(not isinstance(manager, SourceHypoGroupManager)):
-            raise TypeError('The src_hypo_group_manager property must be an '
-                            'instance of SourceHypoGroupManager!')
-        self._src_hypo_group_manager = manager
+        if (pre_event_selection_method is not None) and (get_mean_func is None):
+            raise ValueError(
+                'If an event pre-selection method is provided, a '
+                'get_mean_func needs to be provided as well!')
 
-    @property
-    def dataset_list(self):
-        """The list of Dataset instances for which signal events should get
-        generated for.
-        """
-        return self._dataset_list
-
-    @dataset_list.setter
-    def dataset_list(self, datasets):
-        if(not issequenceof(datasets, Dataset)):
-            raise TypeError('The dataset_list property must be a sequence of '
-                            'Dataset instances!')
-        self._dataset_list = list(datasets)
+        # Define cache members to cache the background probabilities for each
+        # monte-carlo event. The probabilities change only if the data changes.
+        self._cache_data_id = None
+        self._cache_mc_pre_selected = None
+        self._cache_mc_event_bkg_prob = None
+        self._cache_mc_event_bkg_prob_pre_selected = None
+        self._cache_mean = None
 
     @property
-    def llhratio(self):
-        """The log-likelihood ratio function for the analysis.
-        """
-        return self._llhratio
-
-    @llhratio.setter
-    def llhratio(self, llhratio):
-        if llhratio is not None:
-            if(not isinstance(llhratio, LLHRatio)):
-                raise TypeError('The llratio property must be an instance of '
-                                'LLHRatio!')
-        self._llhratio = llhratio
-
-    def generate_signal_events(self, rss, mean, poisson=True):
-        shg_list = self._src_hypo_group_manager.src_hypo_group_list
-        # Only supports a single source hypothesis group. Raise an error
-        # if more than one shg is in the source hypo group manager.
-        if len(shg_list) > 1:
-            raise RuntimeError(
-                'Signal injection for multiple source hypothesis groups is '
-                'not supported yet.')
-
-        tot_n_events = 0
-        signal_events_dict = {}
-
-        for shg in shg_list:
-            # Only supports single point source signal injection. Raise
-            # an error if more than one source is in the source hypo group.
-            if len(shg.source_list) > 1:
-                raise RuntimeError(
-                    'Signal injection for multiple sources within a source '
-                    'hypothesis group is not supported yet.')
-            # This only works with power-laws for now.
-            # Each source hypo group can have a different power-law
-            gamma = shg.fluxmodel.gamma
-            weights, _ = self.llhratio.dataset_signal_weights([mean, gamma])
-            for (ds_idx, w) in enumerate(weights):
-                w_mean = mean * w
-                if(poisson):
-                    n_events = rss.random.poisson(
-                        float_cast(
-                            w_mean,
-                            '`mean` must be castable to type of float!'
-                        )
-                    )
-                else:
-                    n_events = int_cast(
-                        w_mean,
-                        '`mean` must be castable to type of int!'
-                    )
-                tot_n_events += n_events
-
-                events_ = None
-                for (shg_src_idx, src) in enumerate(shg.source_list):
-                    ds = self._dataset_list[ds_idx]
-                    sig_gen = PDDatasetSignalGenerator(
-                        ds, src.dec, self.effA[ds_idx], self.sm[ds_idx])
-                    if self.effA[ds_idx] is None:
-                        self.effA[ds_idx] = sig_gen.effA
-                    if self.sm[ds_idx] is None:
-                        self.sm[ds_idx] = sig_gen.smearing_matrix
-                    # ToDo: here n_events should be split according to some
-                    # source weight
-                    events_ = sig_gen.generate_signal_events(
-                        rss,
-                        src.dec,
-                        src.ra,
-                        shg.fluxmodel,
-                        n_events,
-                        energy_cut_spline=self.splines[ds_idx],
-                        cut_sindec=self.cut_sindec[ds_idx]
-                    )
-                    if events_ is None:
-                        continue
+    def get_event_prob_func(self):
+        """The function to obtain the background probability for each
+        monte-carlo event of the data set.
+        """
+        return self._get_event_prob_func
+
+    @get_event_prob_func.setter
+    def get_event_prob_func(self, func):
+        if not callable(func):
+            raise TypeError(
+                'The get_event_prob_func property must be a callable! '
+                f'Its current type is {classname(func)}.')
+        if not func_has_n_args(func, 3):
+            raise TypeError(
+                'The function provided for the get_event_prob_func property '
+                'must have 3 arguments!')
+        self._get_event_prob_func = func
 
-                    if shg_src_idx == 0:
-                        signal_events_dict[ds_idx] = events_
-                    else:
-                        signal_events_dict[ds_idx].append(events_)
-
-        return tot_n_events, signal_events_dict
-
-
-class PDTimeDependentSignalGenerator(PDSignalGenerator):
-    """ The time dependent signal generator works so far only for one single
-    dataset. For multi datasets one needs to adjust the dataset weights
-    accordingly (scaling of the effective area with livetime of the flare in
-    the dataset).
-    """
+    @property
+    def get_mean_func(self):
+        """The function to obtain the mean number of background events for the
+        data set. This can be None, which means that the mean number of
+        background events to generate needs to be specified through the
+        `generate_events` method.
+        """
+        return self._get_mean_func
+
+    @get_mean_func.setter
+    def get_mean_func(self, func):
+        if func is None:
+            self._get_mean_func = None
+            return
+
+        if not callable(func):
+            raise TypeError(
+                'The get_mean_func property must be a callable! '
+                f'Its current type is {classname(func)}.')
+        if not func_has_n_args(func, 3):
+            raise TypeError(
+                'The function provided for the get_mean_func property must '
+                'have 3 arguments!')
+        self._get_mean_func = func
 
-    def __init__(self, src_hypo_group_manager, dataset_list, data_list=None,
-                 llhratio=None, energy_cut_splines=None, cut_sindec=None,
-                 gauss=None, box=None):
-        """
-        Parameters
-        ----------
-        src_hypo_group_manager : SourceHypoGroupManager instance
-            The instance of SourceHypoGroupManager that defines the list of
-            sources, i.e. the list of SourceModel instances.
-        dataset_list : list of Dataset instances
-            The list of Dataset instances for which signal events should get
-            generated for.
-        data_list : list of DatasetData instances
-            The list of DatasetData instances holding the actual data of each
-            dataset. The order must match the order of ``dataset_list``.
-        llhratio : LLHRatio
-            The likelihood ratio object contains the datasets signal weights
-            needed for distributing the event generation among the different
-            datsets.
-        energy_cut_splines : list of UnivariateSpline
-        cut_sindec : float
-        gauss : dict | None
-            None or dictionary with {"mu": float, "sigma": float}.
-        box : dict | None
-            None or dictionary with {"start": float, "end": float}.
-        """
-        if gauss is None and box is None:
-            raise ValueError(
-                "Either box or gauss keywords must define the neutrino flare.")
-        if gauss is not None and box is not None:
-            raise ValueError(
-                "Either box or gauss keywords must define the neutrino flare, "
-                "cannot use both.")
+    @property
+    def unique_events(self):
+        """Flag if unique events should be drawn from the monto-carlo (True),
+        or if the same event can be drawn multiple times from the monte-carlo.
+        """
+        return self._unique_events
+
+    @unique_events.setter
+    def unique_events(self, b):
+        if not isinstance(b, bool):
+            raise TypeError(
+                'The unique_events property must be of type bool! '
+                f'Its current type is {classname(b)}.')
+        self._unique_events = b
 
-        super().__init__(src_hypo_group_manager, dataset_list, data_list,
-                         llhratio, energy_cut_splines, cut_sindec)
-        self.box = box
-        self.gauss = gauss
-
-        self.time_pdf = self._get_time_pdf()
-
-    def _get_time_pdf(self):
-        """Get the neutrino flare time pdf given parameters.
-        Will be used to generate random numbers by calling `rvs()` method.
+    @property
+    def data_scrambler(self):
+        """The DataScrambler instance that should be used to scramble the drawn
+        monte-carlo background events to ensure more independent data trials.
+        This is especially important when monte-carlo statistics are low. It is
+        `None`, if no data scrambling should be used.
+        """
+        return self._data_scrambler
+
+    @data_scrambler.setter
+    def data_scrambler(self, scrambler):
+        if scrambler is None:
+            self._data_scrambler = None
+
+        if not isinstance(scrambler, DataScrambler):
+            raise TypeError(
+                'The data_scrambler property must be an instance of '
+                'DataScrambler! '
+                f'Its current type is {classname(scrambler)}.')
+        self._data_scrambler = scrambler
 
-        Returns
-        -------
-        time_pdf : instance of scipy.stats.rv_continuous base class
-            Has to base scipy.stats.rv_continuous.
-        """
-        # Make sure flare is in dataset.
-        for data_list in self.data_list:
-            grl = data_list.grl
-
-            if self.gauss is not None:
-                if (self.gauss["mu"] - 4 * self.gauss["sigma"] > grl["stop"][-1]) or (
-                        self.gauss["mu"] + 4 * self.gauss["sigma"] < grl["start"][0]):
-                    raise ValueError(
-                        f"Gaussian {str(self.gauss)} flare is not in dataset.")
-
-            if self.box is not None:
-                if (self.box["start"] > grl["stop"][-1]) or (
-                        self.box["end"] < grl["start"][0]):
-                    raise ValueError(
-                        f"Box {str(self.box)} flare is not in dataset.")
-
-        # Create `time_pdf`.
-        if self.gauss is not None:
-            time_pdf = scipy.stats.norm(self.gauss["mu"], self.gauss["sigma"])
-        if self.box is not None:
-            time_pdf = scipy.stats.uniform(
-                self.box["start"],
-                self.box["end"] - self.box["start"]
-            )
+    @property
+    def mc_inplace_scrambling(self):
+        """Flag if the scrambling of the monte-carlo data should be done
+        inplace, i.e. without creating a copy of the MC data first.
+        """
+        return self._mc_inplace_scrambling
+
+    @mc_inplace_scrambling.setter
+    def mc_inplace_scrambling(self, b):
+        if not isinstance(b, bool):
+            raise TypeError(
+                'The mc_inplace_scrambling property must be of type bool! '
+                f'Its current type is {classname(b)}.')
+        self._mc_inplace_scrambling = b
 
-        return time_pdf
+    @property
+    def keep_mc_data_field_names(self):
+        """The MC data field names that should be kept in order to be able to
+        calculate the background events rates by the functions
+        ``get_event_prob_func`` and ``get_mean_func``. All other MC fields
+        will get droped due to computational efficiency reasons.
+        """
+        return self._keep_mc_data_field_names
+
+    @keep_mc_data_field_names.setter
+    def keep_mc_data_field_names(self, names):
+        if names is None:
+            names = []
+        elif isinstance(names, str):
+            names = [names]
+        elif not issequenceof(names, str):
+            raise TypeError(
+                'The keep_mc_data_field_names must be None, an instance of '
+                'type str, or a sequence of instances of type str!')
+        self._keep_mc_data_field_names = names
 
-    def set_flare(self, gauss=None, box=None):
-        """Set the neutrino flare given parameters.
+    @property
+    def pre_event_selection_method(self):
+        """The instance of EventSelectionMethod that pre-selects the MC events,
+        which can be considered for background event generation.
+        """
+        return self._pre_event_selection_method
+
+    @pre_event_selection_method.setter
+    def pre_event_selection_method(self, method):
+        if method is None:
+            self._pre_event_selection_method = None
+            return
+
+        if not isinstance(method, EventSelectionMethod):
+            raise TypeError(
+                'The pre_event_selection_method property must be None, or an '
+                'instance of EventSelectionMethod!')
+
+        # If the event selection method selects all events, it's equivalent
+        # to have it set to None, because then no operation has to be
+        # performed.
+        if isinstance(method, AllEventSelectionMethod):
+            method = None
+
+        self._pre_event_selection_method = method
+
+    def change_shg_mgr(self, shg_mgr):
+        """Changes the instance of SourceHypoGroupManager of the
+        pre-event-selection method. Also it invalidates the data cache of this
+        background generation method.
 
         Parameters
         ----------
-        gauss : dict | None
-            None or dictionary with {"mu": float, "sigma": float}.
-        box : dict | None
-             None or dictionary with {"start": float, "end": float}.
+        shg_mgr : instance of SourceHypoGroupManager
+            The new instance of SourceHypoGroupManager.
         """
-        if gauss is None and box is None:
-            raise ValueError(
-                "Either box or gauss keywords must define the neutrino flare.")
-        if gauss is not None and box is not None:
-            raise ValueError(
-                "Either box or gauss keywords must define the neutrino flare, "
-                "cannot use both.")
-
-        self.box = box
-        self.gauss = gauss
-
-        self.time_pdf = self._get_time_pdf()
-
-    def is_in_grl(self, time, grl):
-        """Helper function to check if given times are in the grl ontime.
+        if self._pre_event_selection_method is not None:
+            self._pre_event_selection_method.change_shg_mgr(
+                shg_mgr=shg_mgr)
+
+        # Invalidate the data cache.
+        self._cache_data_id = None
+
+    def generate_events(
+            self,
+            rss,
+            dataset,
+            data,
+            mean=None,
+            poisson=True,
+            tl=None,
+    ):
+        """Generates a ``mean`` number of background events for the given
+        dataset and its data.
 
         Parameters
         ----------
-        time : 1d ndarray
-            Time values.
-        grl : ndarray
-            Array of the detector good run list.
+        rss : instance of RandomStateService
+            The instance of RandomStateService that should be used to generate
+            random numbers from.
+        dataset : instance of Dataset
+            The Dataset instance describing the dataset for which background
+            events should get generated.
+        data : instance of DatasetData
+            The DatasetData instance holding the data of the dataset for which
+            background events should get generated.
+        mean : float | None
+            The mean number of background events to generate.
+            Can be `None`. In that case the mean number of background events is
+            obtained through the `get_mean_func` function.
+        poisson : bool
+            If set to ``True`` (default), the actual number of generated
+            background events will be drawn from a Poisson distribution with the
+            given mean number of background events.
+            If set to ``False``, the argument ``mean`` specifies the actual
+            number of generated background events.
+        tl : instance of TimeLord | None
+            The optional instance of TimeLord that should be used to collect
+            timing information about this method.
 
         Returns
         -------
-        is_in_grl : 1d ndarray
-            Boolean mask of `time` in grl ontime.
-        """
-        def f(time, grl):
-            return np.any((grl["start"] <= time) & (time <= grl["stop"]))
-
-        # Vectorize `f`, but exclude `grl` argument from vectorization.
-        # This is needed to support `time` as an array argument.
-        f_v = np.vectorize(f, excluded=[1])
-        is_in_grl = f_v(time, grl)
-
-        return is_in_grl
-
-    def generate_signal_events(self, rss, mean, poisson=True):
-        """Same as in PDSignalGenerator, but we assign times here. 
-        """
-        # Call method from the parent class to generate signal events.
-        (tot_n_events, signal_events_dict) = super().generate_signal_events(
-            rss, mean, poisson=poisson)
-
-        # Assign times for flare. We can also use inverse transform
-        # sampling instead of the lazy version implemented here.
-        for (ds_idx, events_) in signal_events_dict.items():
-            grl = self.data_list[ds_idx].grl
-
-            # Optimized time injection version, based on csky implementation.
-            # https://github.com/icecube/csky/blob/7e969639c5ef6dbb42872dac9b761e1e8b0ccbe2/csky/inj.py#L1122
-            times = np.array([])
-            n_events = len(events_)
-            while len(times) < n_events:
-                times = np.concatenate(
-                    (times, self.time_pdf.rvs(n_events - len(times),
-                                              random_state=rss.random))
-                )
-                # Check if times is in grl.
-                is_in_grl_mask = self.is_in_grl(times, grl)
-                times = times[is_in_grl_mask]
+        n_bkg : int
+            The number of generated background events for the data set.
+        bkg_events : instance of DataFieldRecordArray
+            The instance of DataFieldRecordArray holding the generated
+            background events. The number of events can be less than `n_bkg`
+            if an event selection method is used.
+        """
+        tracing = self._cfg['debugging']['enable_tracing']
+
+        # Create aliases to avoid dot-lookup.
+        self__pre_event_selection_method = self._pre_event_selection_method
+
+        # Check if the data set has changed. In that case need to get new
+        # background probabilities for each monte-carlo event and a new mean
+        # number of background events.
+        data_id = id(data)
+        if self._cache_data_id != data_id:
+            if tracing:
+                logger.debug(
+                    f'DatasetData instance id of dataset "{dataset.name}" '
+                    f'changed from {self._cache_data_id} to {data_id}')
+            # Cache the current id of the data.
+            self._cache_data_id = data_id
+
+            # Create a copy of the MC data with all MC data fields removed,
+            # except the specified MC data fields to keep for the
+            # ``get_mean_func`` and ``get_event_prob_func`` functions.
+            keep_field_names = list(set(
+                DataFields.get_joint_names(
+                    datafields=self._cfg['datafields'],
+                    stages=(
+                        DFS.ANALYSIS_EXP
+                    )
+                ) +
+                data.exp_field_names +
+                self._keep_mc_data_field_names
+            ))
+            data_mc = data.mc.copy(keep_fields=keep_field_names)
+
+            if self._get_mean_func is not None:
+                with TaskTimer(
+                        tl,
+                        'Calculate total MC background mean.'):
+                    self._cache_mean = self._get_mean_func(
+                        dataset=dataset,
+                        data=data,
+                        events=data_mc)
+
+            with TaskTimer(
+                    tl,
+                    'Calculate MC background event probability cache.'):
+                self._cache_mc_event_bkg_prob = self._get_event_prob_func(
+                    dataset=dataset,
+                    data=data,
+                    events=data_mc)
+
+            if self__pre_event_selection_method is not None:
+                with TaskTimer(
+                        tl,
+                        'Pre-select MC events.'):
+                    (self._cache_mc_pre_selected,
+                     mc_pre_selected_src_evt_idxs,
+                     mc_pre_selected_idxs) =\
+                        self__pre_event_selection_method.select_events(
+                            events=data_mc,
+                            ret_original_evt_idxs=True,
+                            tl=tl)
+                self._cache_mc_event_bkg_prob_pre_selected = np.take(
+                    self._cache_mc_event_bkg_prob, mc_pre_selected_idxs)
+            else:
+                self._cache_mc_pre_selected = data_mc
+
+        if mean is None:
+            if self._cache_mean is None:
+                raise ValueError(
+                    'No mean number of background events and no '
+                    'get_mean_func were specified! One of the two must be '
+                    'specified!')
+            mean = self._cache_mean
+        else:
+            mean = float_cast(
+                mean,
+                'The mean number of background events must be castable to type '
+                'float!')
+
+        # Draw the number of background events from a poisson distribution with
+        # the given mean number of background events. This will be the number of
+        # background events for this data set.
+        n_bkg = (int(rss.random.poisson(mean)) if poisson else
+                 int(np.round(mean, 0)))
+
+        # Apply only event pre-selection before choosing events.
+        data_mc_selected = self._cache_mc_pre_selected
+
+        # Calculate the mean number of background events for the pre-selected
+        # MC events.
+        if self__pre_event_selection_method is None:
+            # No selection at all, use the total mean.
+            mean_selected = mean
+        else:
+            with TaskTimer(tl, 'Calculate selected MC background mean.'):
+                mean_selected = self._get_mean_func(
+                    dataset=dataset,
+                    data=data,
+                    events=data_mc_selected)
+
+        # Calculate the actual number of background events for the selected
+        # events.
+        p_binomial = mean_selected / mean
+        with TaskTimer(tl, 'Get p array.'):
+            if self__pre_event_selection_method is None:
+                p = self._cache_mc_event_bkg_prob
+            else:
+                # Pre-selection.
+                p = self._cache_mc_event_bkg_prob_pre_selected / p_binomial
+        n_bkg_selected = int(np.around(n_bkg * p_binomial, 0))
+
+        # Draw the actual background events from the selected events of the
+        # monto-carlo data set.
+        with TaskTimer(tl, 'Draw MC background indices.'):
+            bkg_event_indices = rss.random.choice(
+                data_mc_selected.indices,
+                size=n_bkg_selected,
+                p=p,
+                replace=(not self._unique_events))
+        with TaskTimer(tl, 'Select MC background events from indices.'):
+            bkg_events = data_mc_selected[bkg_event_indices]
+
+        # Scramble the drawn MC events if requested.
+        if self._data_scrambler is not None:
+            with TaskTimer(tl, 'Scramble MC background data.'):
+                bkg_events = self._data_scrambler.scramble_data(
+                    rss=rss,
+                    dataset=dataset,
+                    data=bkg_events,
+                    copy=False)
+
+        # Remove MC specific data fields from the background events record
+        # array. So the result contains only experimental data fields. The list
+        # of experimental data fields is defined as the unique set of the
+        # required experimental data fields defined by the data set, and the
+        # actual experimental data fields (in case there are additional kept
+        # data fields by the user).
+        with TaskTimer(tl, 'Remove MC specific data fields from MC events.'):
+            exp_field_names = list(set(
+                DataFields.get_joint_names(
+                    datafields=self._cfg['datafields'],
+                    stages=(
+                        DFS.ANALYSIS_EXP
+                    )
+                ) +
+                data.exp_field_names))
+            bkg_events.tidy_up(exp_field_names)
 
-            events_["time"] = times
-        return tot_n_events, signal_events_dict
+        return (n_bkg, bkg_events)
```

### Comparing `skyllh-23.1.1/skyllh/analyses/i3/publicdata_ps/signalpdf.py` & `skyllh-23.2.0/skyllh/analyses/i3/publicdata_ps/signalpdf.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,220 +1,272 @@
 # -*- coding: utf-8 -*-
 
 import numpy as np
+
 from scipy import integrate
 
-from skyllh.core.py import module_classname
-from skyllh.core.debugging import get_logger
-from skyllh.core.timing import TaskTimer
-from skyllh.core.binning import get_bincenters_from_binedges
-from skyllh.core.pdf import (
-    PDF,
-    PDFAxis,
-    PDFSet,
-    IsSignalPDF,
+from skyllh.analyses.i3.publicdata_ps.aeff import (
+    PDAeff,
+)
+from skyllh.analyses.i3.publicdata_ps.utils import (
+    FctSpline1D,
+)
+from skyllh.analyses.i3.publicdata_ps.smearing_matrix import (
+    PDSmearingMatrix,
+)
+from skyllh.core.binning import (
+    get_bincenters_from_binedges,
+)
+from skyllh.core.debugging import (
+    get_logger,
+)
+from skyllh.core.flux_model import (
+    FactorizedFluxModel,
 )
 from skyllh.core.multiproc import (
     IsParallelizable,
-    parallelize
+    parallelize,
 )
 from skyllh.core.parameters import (
     ParameterGrid,
-    ParameterGridSet
+    ParameterGridSet,
 )
-from skyllh.i3.dataset import I3Dataset
-from skyllh.physics.flux import FluxModel
-
-from skyllh.analyses.i3.publicdata_ps.aeff import PDAeff
-from skyllh.analyses.i3.publicdata_ps.utils import (
-    FctSpline1D,
+from skyllh.core.pdf import (
+    IsSignalPDF,
+    PDF,
+    PDFAxis,
+    PDFSet,
 )
-from skyllh.analyses.i3.publicdata_ps.smearing_matrix import (
-    PDSmearingMatrix
+from skyllh.core.py import (
+    classname,
+    module_classname,
+)
+from skyllh.core.timing import (
+    TaskTimer,
+)
+from skyllh.i3.dataset import (
+    I3Dataset,
 )
 
 
-class PDSignalEnergyPDF(PDF, IsSignalPDF):
+class PDSignalEnergyPDF(
+        PDF,
+        IsSignalPDF,
+):
     """This class provides a signal energy PDF for a spectrial index value.
     """
     def __init__(
-            self, f_e_spl, **kwargs):
+            self,
+            f_e_spl,
+            **kwargs,
+    ):
         """Creates a new signal energy PDF instance for a particular spectral
         index value.
 
         Parameters
         ----------
-        f_e_spl : FctSpline1D instance
-            The FctSpline1D instance representing the spline of the energy PDF.
+        f_e_spl : instance of FctSpline1D
+            The instance of FctSpline1D representing the spline of the energy
+            PDF.
         """
-        super().__init__(**kwargs)
+        super().__init__(
+            pmm=None,
+            **kwargs)
 
         if not isinstance(f_e_spl, FctSpline1D):
             raise TypeError(
-                'The f_e_spl argument must be an instance of FctSpline1D!')
+                'The f_e_spl argument must be an instance of FctSpline1D! '
+                f'Its current type is {classname(f_e_spl)}!')
 
         self.f_e_spl = f_e_spl
 
         self.log10_reco_e_lower_binedges = self.f_e_spl.x_binedges[:-1]
         self.log10_reco_e_upper_binedges = self.f_e_spl.x_binedges[1:]
 
         self.log10_reco_e_min = self.log10_reco_e_lower_binedges[0]
         self.log10_reco_e_max = self.log10_reco_e_upper_binedges[-1]
 
         # Add the PDF axes.
-        self.add_axis(PDFAxis(
-            name='log_energy',
-            vmin=self.log10_reco_e_min,
-            vmax=self.log10_reco_e_max)
-        )
+        self.add_axis(
+            PDFAxis(
+                name='log_energy',
+                vmin=self.log10_reco_e_min,
+                vmax=self.log10_reco_e_max))
 
         # Check integrity.
         integral = integrate.quad(
             self.f_e_spl.evaluate,
             self.log10_reco_e_min,
             self.log10_reco_e_max,
             limit=200,
             full_output=1
         )[0] / self.f_e_spl.norm
         if not np.isclose(integral, 1):
             raise ValueError(
                 'The integral over log10_reco_e of the energy term must be '
-                'unity! But it is {}!'.format(integral))
+                f'unity! But it is {integral}!')
 
-    def assert_is_valid_for_trial_data(self, tdm):
+    def assert_is_valid_for_trial_data(
+            self,
+            tdm,
+            tl=None):
         pass
 
-    def get_pd_by_log10_reco_e(self, log10_reco_e, tl=None):
+    def get_pd_by_log10_reco_e(
+            self,
+            log10_reco_e,
+            tl=None):
         """Calculates the probability density for the given log10(E_reco/GeV)
         values using the spline representation of the PDF.
 
         Parameters
         ----------
-        log10_reco_e : (n_log10_reco_e,)-shaped 1D numpy ndarray
-            The numpy ndarray holding the log10(E_reco/GeV) values for which
-            the energy PDF should get evaluated.
-        tl : TimeLord instance | None
-            The optional TimeLord instance that should be used to measure
+        log10_reco_e : instance of ndarray
+            The (n_log10_reco_e,)-shaped numpy ndarray holding the
+            log10(E_reco/GeV) values for which the energy PDF should get
+            evaluated.
+        tl : instance of TimeLord | None
+            The optional instance of TimeLord that should be used to measure
             timing information.
 
         Returns
         -------
-        pd : (N_events,)-shaped numpy ndarray
-            The 1D numpy ndarray with the probability density for each event.
+        pd : instance of numpy ndarray
+            The (n_log10_reco_e,)-shaped numpy ndarray with the probability
+            density for each energy value.
         """
-        # Select events that actually have a signal energy PDF.
-        # All other events will get zero signal probability density.
+        # Select energy values that actually have a signal energy PDF.
+        # All other energy values will get zero signal probability density.
         m = (
             (log10_reco_e >= self.log10_reco_e_min) &
             (log10_reco_e < self.log10_reco_e_max)
         )
 
         with TaskTimer(tl, 'Evaluate PDSignalEnergyPDF'):
             pd = np.zeros((len(log10_reco_e),), dtype=np.double)
             pd[m] = self.f_e_spl(log10_reco_e[m]) / self.f_e_spl.norm
 
         return pd
 
-    def get_prob(self, tdm, params=None, tl=None):
-        """Calculates the probability density for the events given by the
-        TrialDataManager.
+    def get_pd(
+            self,
+            tdm,
+            params_recarray=None,
+            tl=None):
+        """Calculates the probability density for all given trial data events
+        and sources.
 
         Parameters
         ----------
-        tdm : TrialDataManager instance
-            The TrialDataManager instance holding the data events for which the
-            probability should be looked up. The following data fields are
-            required:
-                - 'log_energy'
-                    The log10 of the reconstructed energy.
-        params : dict | None
-            The dictionary containing the parameter names and values for which
-            the probability should get calculated.
-            By definition this PDF does not depend on parameters.
-        tl : TimeLord instance | None
-            The optional TimeLord instance that should be used to measure
+        tdm : instance of TrialDataManager
+            The instance of TrialDataManager holding the trial data events for
+            which the probability density should be looked up.
+            The following data fields must be present:
+
+            log_energy : float
+                The base-10 logarithm of the reconstructed energy.
+
+        params_recarray : None
+            Unused interface argument.
+        tl : instance of TimeLord | None
+            The optional instance of TimeLord that should be used to measure
             timing information.
 
         Returns
         -------
-        pd : (N_events,)-shaped numpy ndarray
-            The 1D numpy ndarray with the probability density for each event.
-        grads : (N_fitparams,N_events)-shaped ndarray | None
-            The 2D numpy ndarray holding the gradients of the PDF w.r.t.
-            each fit parameter for each event. The order of the gradients
-            is the same as the order of floating parameters specified through
-            the ``param_set`` property.
-            It is ``None``, if this PDF does not depend on any parameters.
+        pd : instance of ndarray
+            The (N_values,)-shaped numpy ndarray holding the probability density
+            for each trial data event and source.
+        grads : dict
+            The dictionary holding the gradient values for each global fit
+            parameter. By definition this PDF does not depend on any fit
+            parameters, hence, this is an empty dictionary.
         """
-        log10_reco_e = tdm.get_data('log_energy')
+        evt_idxs = tdm.src_evt_idxs[1]
+
+        log10_reco_e = np.take(tdm['log_energy'], evt_idxs)
 
-        pd = self.get_pd_by_log10_reco_e(log10_reco_e, tl=tl)
+        pd = self.get_pd_by_log10_reco_e(
+            log10_reco_e=log10_reco_e,
+            tl=tl)
 
-        return (pd, None)
+        grads = dict()
 
+        return (pd, grads)
 
-class PDSignalEnergyPDFSet(PDFSet, IsSignalPDF, IsParallelizable):
-    """This class provides a signal energy PDF set for the public data.
+
+class PDSignalEnergyPDFSet(
+        PDFSet,
+        IsSignalPDF,
+        PDF,
+        IsParallelizable,
+):
+    """This class provides a signal energy PDF set using the public data.
     It creates a set of PDSignalEnergyPDF instances, one for each spectral
     index value on a grid.
     """
     def __init__(
             self,
             ds,
             src_dec,
-            flux_model,
-            fitparam_grid_set,
+            fluxmodel,
+            param_grid_set,
             ncpu=None,
             ppbar=None,
-            **kwargs):
+            **kwargs,
+    ):
         """Creates a new PDSignalEnergyPDFSet instance for the public data.
 
         Parameters
         ----------
-        ds : I3Dataset instance
-            The I3Dataset instance that defines the dataset of the public data.
+        ds : instance of Dataset
+            The instance of Dataset that defines the dataset of the public data.
         src_dec : float
             The declination of the source in radians.
-        flux_model : FluxModel instance
-            The FluxModel instance that defines the source's flux model.
-        fitparam_grid_set : ParameterGrid | ParameterGridSet instance
-            The parameter grid set defining the grids of the fit parameters.
+        fluxmodel : instance of FactorizedFluxModel
+            The instance of FactorizedFluxModel that defines the source's flux
+            model.
+        param_grid_set : instance of ParameterGrid | instance of ParameterGridSet
+            The parameter grid set defining the grids of the parameters this
+            energy PDF set depends on.
         ncpu : int | None
             The number of CPUs to utilize. Global setting will take place if
             not specified, i.e. set to None.
-        ppbar : ProgressBar instance | None
+        ppbar : instance of ProgressBar | None
             The instance of ProgressBar for the optional parent progress bar.
         """
         self._logger = get_logger(module_classname(self))
 
         # Check for the correct types of the arguments.
         if not isinstance(ds, I3Dataset):
             raise TypeError(
                 'The ds argument must be an instance of I3Dataset!')
 
-        if not isinstance(flux_model, FluxModel):
+        if not isinstance(fluxmodel, FactorizedFluxModel):
             raise TypeError(
-                'The flux_model argument must be an instance of FluxModel!')
+                'The fluxmodel argument must be an instance of '
+                'FactorizedFluxModel! '
+                f'Its current type is {classname(fluxmodel)}')
 
-        if (not isinstance(fitparam_grid_set, ParameterGrid)) and\
-           (not isinstance(fitparam_grid_set, ParameterGridSet)):
+        if (not isinstance(param_grid_set, ParameterGrid)) and\
+           (not isinstance(param_grid_set, ParameterGridSet)):
             raise TypeError(
-                'The fitparam_grid_set argument must be an instance of type '
-                'ParameterGrid or ParameterGridSet!')
+                'The param_grid_set argument must be an instance of type '
+                'ParameterGrid or ParameterGridSet! '
+                f'Its current type is {classname(param_grid_set)}!')
 
-        # Extend the fitparam_grid_set to allow for parameter interpolation
+        # Extend the param_grid_set to allow for parameter interpolation
         # values at the grid edges.
-        fitparam_grid_set = fitparam_grid_set.copy()
-        fitparam_grid_set.add_extra_lower_and_upper_bin()
+        param_grid_set = param_grid_set.copy()
+        param_grid_set.add_extra_lower_and_upper_bin()
 
         super().__init__(
-            pdf_type=PDF,
-            fitparams_grid_set=fitparam_grid_set,
-            ncpu=ncpu
-        )
+            param_grid_set=param_grid_set,
+            ncpu=ncpu,
+            **kwargs)
 
         # Load the smearing matrix.
         sm = PDSmearingMatrix(
             pathfilenames=ds.get_abs_pathfilename_list(
                 ds.get_aux_data_definition('smearing_datafile')))
 
         # Select the slice of the smearing matrix corresponding to the
@@ -230,19 +282,20 @@
         log_true_e_mask = np.logical_and(
             sm.log10_true_enu_binedges >= min_log_true_e,
             sm.log10_true_enu_binedges <= max_log_true_e)
         true_enu_binedges = np.power(
             10, sm.log10_true_enu_binedges[log_true_e_mask])
         true_enu_binedges_lower = true_enu_binedges[:-1]
         true_enu_binedges_upper = true_enu_binedges[1:]
-        valid_true_e_idxs = [sm.get_log10_true_e_idx(0.5 * (he + le))
-            for he,le in zip(
+        valid_true_e_idxs = [
+            sm.get_log10_true_e_idx(0.5 * (he + le))
+            for (he, le) in zip(
                 sm.log10_true_enu_binedges[log_true_e_mask][1:],
                 sm.log10_true_enu_binedges[log_true_e_mask][:-1])
-            ]
+        ]
 
         xvals_binedges = ds.get_binning_definition('log_energy').binedges
         xvals = get_bincenters_from_binedges(xvals_binedges)
 
         # Calculate the neutrino enegry bin widths in GeV.
         d_enu = np.diff(true_enu_binedges)
         self._logger.debug(
@@ -272,34 +325,32 @@
                 'The sum of the detection probabilities is not unity! It is '
                 '{}.'.format(np.sum(det_prob)))
 
         psi_edges_bw = sm.psi_upper_edges - sm.psi_lower_edges
         ang_err_bw = sm.ang_err_upper_edges - sm.ang_err_lower_edges
 
         # Create the energy pdf for different gamma values.
-        def create_energy_pdf(sm_pdf, flux_model, gridfitparams):
+        def create_energy_pdf(sm_pdf, fluxmodel, gridparams):
             """Creates an energy pdf for a specific gamma value.
             """
             # Create a copy of the FluxModel with the given flux parameters.
             # The copy is needed to not interfer with other CPU processes.
-            my_flux_model = flux_model.copy(newprop=gridfitparams)
+            my_fluxmodel = fluxmodel.copy(newparams=gridparams)
 
             self._logger.debug(
-                'Generate signal energy PDF for parameters {} in {} E_nu '
-                'bins.'.format(
-                    gridfitparams, len(valid_true_e_idxs))
-            )
+                f'Generate signal energy PDF for parameters {gridparams} in '
+                f'{len(valid_true_e_idxs)} E_nu bins.')
 
             # Calculate the flux probability p(E_nu|gamma).
             flux_prob = (
-                my_flux_model.get_integral(
+                my_fluxmodel.energy_profile.get_integral(
                     true_enu_binedges_lower,
                     true_enu_binedges_upper
                 ) /
-                my_flux_model.get_integral(
+                my_fluxmodel.energy_profile.get_integral(
                     true_enu_binedges[0],
                     true_enu_binedges[-1]
                 )
             )
             if not np.isclose(np.sum(flux_prob), 1):
                 self._logger.warn(
                     'The sum of the flux probabilities is not unity! It is '
@@ -339,83 +390,37 @@
                 p = f_e * true_e_prob[idx]
 
                 spline = FctSpline1D(p, log10_reco_e_binedges)
 
                 return spline(xvals)
 
             # Integrate over the true neutrino energy and spline the output.
-            sum_pdf = np.sum([
-                create_reco_e_pdf_for_true_e(i, true_e_idx)
-                for i,true_e_idx in enumerate(valid_true_e_idxs)
-            ], axis=0)
+            sum_pdf = np.sum(
+                [
+                    create_reco_e_pdf_for_true_e(i, true_e_idx)
+                    for (i, true_e_idx) in enumerate(valid_true_e_idxs)
+                ],
+                axis=0)
 
             spline = FctSpline1D(sum_pdf, xvals_binedges, norm=True)
 
-            pdf = PDSignalEnergyPDF(spline)
+            pdf = PDSignalEnergyPDF(spline, cfg=self._cfg)
 
             return pdf
 
         args_list = [
-            ((sm_pdf, flux_model, gridfitparams), {})
-            for gridfitparams in self.gridfitparams_list
+            ((sm_pdf, fluxmodel, gridparams), {})
+            for gridparams in self.gridparams_list
         ]
 
         pdf_list = parallelize(
             create_energy_pdf,
             args_list,
             ncpu=self.ncpu,
             ppbar=ppbar)
 
-        del(sm_pdf)
+        del sm_pdf
 
         # Save all the energy PDF objects in the PDFSet PDF registry with
         # the hash of the individual parameters as key.
-        for (gridfitparams, pdf) in zip(self.gridfitparams_list, pdf_list):
-            self.add_pdf(pdf, gridfitparams)
-
-    def get_prob(self, tdm, gridfitparams, tl=None):
-        """Calculates the signal probability density of each event for the
-        given set of signal fit parameters on a grid.
-
-        Parameters
-        ----------
-        tdm : instance of TrialDataManager
-            The TrialDataManager instance holding the data events for which the
-            probability should be calculated for. The following data fields must
-            exist:
-
-            - 'log_energy'
-                The log10 of the reconstructed energy.
-            - 'psi'
-                The opening angle from the source to the event in radians.
-            - 'ang_err'
-                The angular error of the event in radians.
-        gridfitparams : dict
-            The dictionary holding the signal parameter values for which the
-            signal energy probability should be calculated. Note, that the
-            parameter values must match a set of parameter grid values for which
-            a PDSignalPDF object has been created at construction time of this
-            PDSignalPDFSet object.
-        tl : TimeLord instance | None
-            The optional TimeLord instance that should be used to measure time.
-
-        Returns
-        -------
-        prob : 1d ndarray
-            The array with the signal energy probability for each event.
-        grads : (N_fitparams,N_events)-shaped ndarray | None
-            The 2D numpy ndarray holding the gradients of the PDF w.r.t.
-            each fit parameter for each event. The order of the gradients
-            is the same as the order of floating parameters specified through
-            the ``param_set`` property.
-            It is ``None``, if this PDF does not depend on any parameters.
-
-        Raises
-        ------
-        KeyError
-            If no energy PDF can be found for the given signal parameter values.
-        """
-        pdf = self.get_pdf(gridfitparams)
-
-        (prob, grads) = pdf.get_prob(tdm, tl=tl)
-
-        return (prob, grads)
+        for (gridparams, pdf) in zip(self.gridparams_list, pdf_list):
+            self.add_pdf(pdf, gridparams)
```

### Comparing `skyllh-23.1.1/skyllh/analyses/i3/publicdata_ps/smearing_matrix.py` & `skyllh-23.2.0/skyllh/analyses/i3/publicdata_ps/smearing_matrix.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,14 +1,16 @@
 # -*- coding: utf-8 -*-
 
 import numpy as np
 
 from skyllh.core.storage import create_FileLoader
 
-def load_smearing_histogram(pathfilenames):
+
+def load_smearing_histogram(
+        pathfilenames):
     """Loads the 5D smearing histogram from the given data file.
 
     Parameters
     ----------
     pathfilenames : str | list of str
         The file name of the data file.
 
@@ -75,18 +77,18 @@
         # Select only valid rows.
         mask = (upper_edges - lower_edges) > 0
         data = lower_edges[mask]
         # Go through the valid rows and search for the number of increasing
         # bin edge values.
         v0 = None
         for v in data:
-            if(v0 is not None and v < v0):
+            if (v0 is not None) and (v < v0):
                 # Reached the end of the edges block.
                 break
-            if(v0 is None or v > v0):
+            if (v0 is None) or (v > v0):
                 v0 = v
                 n += 1
         return n
 
     true_e_bin_edges = np.union1d(
         data['true_e_min'], data['true_e_max'])
     true_dec_bin_edges = np.union1d(
@@ -168,20 +170,23 @@
         psi_lower_edges,
         psi_upper_edges,
         ang_err_lower_edges,
         ang_err_upper_edges
     )
 
 
-class PDSmearingMatrix(object):
+class PDSmearingMatrix(
+        object):
     """This class is a helper class for dealing with the smearing matrix
     provided by the public data.
     """
     def __init__(
-            self, pathfilenames, **kwargs):
+            self,
+            pathfilenames,
+            **kwargs):
         """Creates a smearing matrix instance by loading the smearing matrix
         from the given file.
         """
         super().__init__(**kwargs)
 
         (
             self.histogram,
@@ -198,30 +203,30 @@
         self.n_psi_bins = self.histogram.shape[3]
         self.n_ang_err_bins = self.histogram.shape[4]
 
         # Create bin edges array for log10_reco_e.
         s = np.array(self.reco_e_lower_edges.shape)
         s[-1] += 1
         self.log10_reco_e_binedges = np.empty(s, dtype=np.double)
-        self.log10_reco_e_binedges[...,:-1] = self.reco_e_lower_edges
-        self.log10_reco_e_binedges[...,-1] = self.reco_e_upper_edges[...,-1]
+        self.log10_reco_e_binedges[..., :-1] = self.reco_e_lower_edges
+        self.log10_reco_e_binedges[..., -1] = self.reco_e_upper_edges[..., -1]
 
         # Create bin edges array for psi.
         s = np.array(self.psi_lower_edges.shape)
         s[-1] += 1
         self.psi_binedges = np.empty(s, dtype=np.double)
-        self.psi_binedges[...,:-1] = self.psi_lower_edges
-        self.psi_binedges[...,-1] = self.psi_upper_edges[...,-1]
+        self.psi_binedges[..., :-1] = self.psi_lower_edges
+        self.psi_binedges[..., -1] = self.psi_upper_edges[..., -1]
 
         # Create bin edges array for ang_err.
         s = np.array(self.ang_err_lower_edges.shape)
         s[-1] += 1
         self.ang_err_binedges = np.empty(s, dtype=np.double)
-        self.ang_err_binedges[...,:-1] = self.ang_err_lower_edges
-        self.ang_err_binedges[...,-1] = self.ang_err_upper_edges[...,-1]
+        self.ang_err_binedges[..., :-1] = self.ang_err_lower_edges
+        self.ang_err_binedges[..., -1] = self.ang_err_upper_edges[..., -1]
 
     @property
     def n_log10_true_e_bins(self):
         """(read-only) The number of log10 true energy bins.
         """
         return len(self._true_e_bin_edges) - 1
 
@@ -341,15 +346,17 @@
         # We do this only where the histogram actually has non-zero entries.
         pdf = np.copy(self.histogram)
         m = self.histogram != 0
         pdf[m] /= bin_volumes[m]
 
         return pdf
 
-    def get_true_dec_idx(self, true_dec):
+    def get_true_dec_idx(
+            self,
+            true_dec):
         """Returns the true declination index for the given true declination
         value.
 
         Parameters
         ----------
         true_dec : float
             The true declination value in radians.
@@ -357,22 +364,25 @@
         Returns
         -------
         true_dec_idx : int
             The index of the declination bin for the given declination value.
         """
         if (true_dec < self.true_dec_bin_edges[0]) or\
            (true_dec > self.true_dec_bin_edges[-1]):
-            raise ValueError('The declination {} degrees is not supported by '
-                'the smearing matrix!'.format(true_dec))
+            raise ValueError(
+                f'The declination {true_dec} degrees is not supported by the '
+                'smearing matrix!')
 
         true_dec_idx = np.digitize(true_dec, self.true_dec_bin_edges) - 1
 
         return true_dec_idx
 
-    def get_log10_true_e_idx(self, log10_true_e):
+    def get_log10_true_e_idx(
+            self,
+            log10_true_e):
         """Returns the bin index for the given true log10 energy value.
 
         Parameters
         ----------
         log10_true_e : float
             The log10 value of the true energy.
 
@@ -380,24 +390,28 @@
         -------
         log10_true_e_idx : int
             The index of the true log10 energy bin for the given log10 true
             energy value.
         """
         if (log10_true_e < self.true_e_bin_edges[0]) or\
            (log10_true_e > self.true_e_bin_edges[-1]):
-               raise ValueError(
-                   'The log10 true energy value {} is not supported by the '
-                   'smearing matrix!'.format(log10_true_e))
+            raise ValueError(
+                f'The log10 true energy value {log10_true_e} is not supported '
+                'by the smearing matrix!')
 
         log10_true_e_idx = np.digitize(
             log10_true_e, self._true_e_bin_edges) - 1
 
         return log10_true_e_idx
 
-    def get_reco_e_idx(self, true_e_idx, true_dec_idx, reco_e):
+    def get_reco_e_idx(
+            self,
+            true_e_idx,
+            true_dec_idx,
+            reco_e):
         """Returns the bin index for the given reco energy value given the
         given true energy and true declination bin indices.
 
         Parameters
         ----------
         true_e_idx : int
             The index of the true energy bin.
@@ -408,27 +422,32 @@
 
         Returns
         -------
         reco_e_idx : int | None
             The index of the reco energy bin the given reco energy value falls
             into. It returns None if the value is out of range.
         """
-        lower_edges = self.reco_e_lower_edges[true_e_idx,true_dec_idx]
-        upper_edges = self.reco_e_upper_edges[true_e_idx,true_dec_idx]
+        lower_edges = self.reco_e_lower_edges[true_e_idx, true_dec_idx]
+        upper_edges = self.reco_e_upper_edges[true_e_idx, true_dec_idx]
 
         m = (lower_edges <= reco_e) & (upper_edges > reco_e)
         idxs = np.nonzero(m)[0]
-        if(len(idxs) == 0):
+        if (len(idxs) == 0):
             return None
 
         reco_e_idx = idxs[0]
 
         return reco_e_idx
 
-    def get_psi_idx(self, true_e_idx, true_dec_idx, reco_e_idx, psi):
+    def get_psi_idx(
+            self,
+            true_e_idx,
+            true_dec_idx,
+            reco_e_idx,
+            psi):
         """Returns the bin index for the given psi value given the
         true energy, true declination and reco energy bin indices.
 
         Parameters
         ----------
         true_e_idx : int
             The index of the true energy bin.
@@ -442,28 +461,33 @@
 
         Returns
         -------
         psi_idx : int | None
             The index of the psi bin the given psi value falls into.
             It returns None if the value is out of range.
         """
-        lower_edges = self.psi_lower_edges[true_e_idx,true_dec_idx,reco_e_idx]
-        upper_edges = self.psi_upper_edges[true_e_idx,true_dec_idx,reco_e_idx]
+        lower_edges = self.psi_lower_edges[true_e_idx, true_dec_idx, reco_e_idx]
+        upper_edges = self.psi_upper_edges[true_e_idx, true_dec_idx, reco_e_idx]
 
         m = (lower_edges <= psi) & (upper_edges > psi)
         idxs = np.nonzero(m)[0]
-        if(len(idxs) == 0):
+        if len(idxs) == 0:
             return None
 
         psi_idx = idxs[0]
 
         return psi_idx
 
     def get_ang_err_idx(
-            self, true_e_idx, true_dec_idx, reco_e_idx, psi_idx, ang_err):
+            self,
+            true_e_idx,
+            true_dec_idx,
+            reco_e_idx,
+            psi_idx,
+            ang_err):
         """Returns the bin index for the given angular error value given the
         true energy, true declination, reco energy, and psi bin indices.
 
         Parameters
         ----------
         true_e_idx : int
             The index of the true energy bin.
@@ -480,28 +504,30 @@
         Returns
         -------
         ang_err_idx : int | None
             The index of the angular error bin the given angular error value
             falls into. It returns None if the value is out of range.
         """
         lower_edges = self.ang_err_lower_edges[
-            true_e_idx,true_dec_idx,reco_e_idx,psi_idx]
+            true_e_idx, true_dec_idx, reco_e_idx, psi_idx]
         upper_edges = self.ang_err_upper_edges[
-            true_e_idx,true_dec_idx,reco_e_idx,psi_idx]
+            true_e_idx, true_dec_idx, reco_e_idx, psi_idx]
 
         m = (lower_edges <= ang_err) & (upper_edges > ang_err)
         idxs = np.nonzero(m)[0]
-        if(len(idxs) == 0):
+        if len(idxs) == 0:
             return None
 
         ang_err_idx = idxs[0]
 
         return ang_err_idx
 
-    def get_true_log_e_range_with_valid_log_e_pdfs(self, dec_idx):
+    def get_true_log_e_range_with_valid_log_e_pdfs(
+            self,
+            dec_idx):
         """Determines the true log energy range for which log_e PDFs are
         available for the given declination bin.
 
         Parameters
         ----------
         dec_idx : int
             The declination bin index.
@@ -510,24 +536,26 @@
         -------
         min_log_true_e : float
             The minimum true log energy value.
         max_log_true_e : float
             The maximum true log energy value.
         """
         m = np.sum(
-            (self.reco_e_upper_edges[:,dec_idx] -
-             self.reco_e_lower_edges[:,dec_idx] > 0),
+            (self.reco_e_upper_edges[:, dec_idx] -
+             self.reco_e_lower_edges[:, dec_idx] > 0),
             axis=1) != 0
         min_log_true_e = np.min(self.true_e_bin_edges[:-1][m])
         max_log_true_e = np.max(self.true_e_bin_edges[1:][m])
 
         return (min_log_true_e, max_log_true_e)
 
     def get_log_e_pdf(
-            self, log_true_e_idx, dec_idx):
+            self,
+            log_true_e_idx,
+            dec_idx):
         """Retrieves the log_e PDF from the given true energy bin index and
         source bin index.
         Returns (None, None, None, None) if any of the bin indices are less then
         zero, or if the sum of all pdf bins is zero.
 
         Parameters
         ----------
@@ -567,15 +595,18 @@
 
         # Normalize the PDF.
         pdf /= np.sum(pdf) * bin_widths
 
         return (pdf, lower_bin_edges, upper_bin_edges, bin_widths)
 
     def get_psi_pdf(
-            self, log_true_e_idx, dec_idx, log_e_idx):
+            self,
+            log_true_e_idx,
+            dec_idx,
+            log_e_idx):
         """Retrieves the psi PDF from the given true energy bin index, the
         source bin index, and the log_e bin index.
         Returns (None, None, None, None) if any of the bin indices are less then
         zero, or if the sum of all pdf bins is zero.
 
         Parameters
         ----------
@@ -617,15 +648,19 @@
 
         # Normalize the PDF.
         pdf /= np.sum(pdf) * bin_widths
 
         return (pdf, lower_bin_edges, upper_bin_edges, bin_widths)
 
     def get_ang_err_pdf(
-            self, log_true_e_idx, dec_idx, log_e_idx, psi_idx):
+            self,
+            log_true_e_idx,
+            dec_idx,
+            log_e_idx,
+            psi_idx):
         """Retrieves the angular error PDF from the given true energy bin index,
         the source bin index, the log_e bin index, and the psi bin index.
         Returns (None, None, None, None) if any of the bin indices are less then
         zero, or if the sum of all pdf bins is zero.
 
         Parameters
         ----------
@@ -676,15 +711,18 @@
 
         # Normalize the PDF.
         pdf = pdf / (np.sum(pdf) * bin_widths)
 
         return (pdf, lower_bin_edges, upper_bin_edges, bin_widths)
 
     def sample_log_e(
-            self, rss, dec_idx, log_true_e_idxs):
+            self,
+            rss,
+            dec_idx,
+            log_true_e_idxs):
         """Samples log energy values for the given source declination and true
         energy bins.
 
         Parameters
         ----------
         rss : instance of RandomStateService
             The RandomStateService which should be used for drawing random
@@ -699,15 +737,15 @@
         log_e_idx : 1d ndarray of int
             The bin indices of the log_e pdf corresponding to the sampled
             log_e values.
         log_e : 1d ndarray of float
             The sampled log_e values.
         """
         n_evt = len(log_true_e_idxs)
-        log_e_idx = np.empty((n_evt,), dtype=np.int_)
+        log_e_idx = np.empty((n_evt,), dtype=np.int64)
         log_e = np.empty((n_evt,), dtype=np.double)
 
         unique_log_true_e_idxs = np.unique(log_true_e_idxs)
         for b_log_true_e_idx in unique_log_true_e_idxs:
             m = log_true_e_idxs == b_log_true_e_idx
             b_size = np.count_nonzero(m)
             (
@@ -735,15 +773,19 @@
 
             log_e_idx[m] = b_log_e_idx
             log_e[m] = b_log_e
 
         return (log_e_idx, log_e)
 
     def sample_psi(
-            self, rss, dec_idx, log_true_e_idxs, log_e_idxs):
+            self,
+            rss,
+            dec_idx,
+            log_true_e_idxs,
+            log_e_idxs):
         """Samples psi values for the given source declination, true
         energy bins, and log_e bins.
 
         Parameters
         ----------
         rss : instance of RandomStateService
             The RandomStateService which should be used for drawing random
@@ -759,20 +801,20 @@
         -------
         psi_idx : 1d ndarray of int
             The bin indices of the psi pdf corresponding to the sampled psi
             values.
         psi : 1d ndarray of float
             The sampled psi values in radians.
         """
-        if(len(log_true_e_idxs) != len(log_e_idxs)):
+        if len(log_true_e_idxs) != len(log_e_idxs):
             raise ValueError(
                 'The lengths of log_true_e_idxs and log_e_idxs must be equal!')
 
         n_evt = len(log_true_e_idxs)
-        psi_idx = np.empty((n_evt,), dtype=np.int_)
+        psi_idx = np.empty((n_evt,), dtype=np.int64)
         psi = np.empty((n_evt,), dtype=np.double)
 
         unique_log_true_e_idxs = np.unique(log_true_e_idxs)
         for b_log_true_e_idx in unique_log_true_e_idxs:
             m = log_true_e_idxs == b_log_true_e_idx
             bb_unique_log_e_idxs = np.unique(log_e_idxs[m])
             for bb_log_e_idx in bb_unique_log_e_idxs:
@@ -804,15 +846,20 @@
 
                 psi_idx[mm] = bb_psi_idx
                 psi[mm] = bb_psi
 
         return (psi_idx, psi)
 
     def sample_ang_err(
-            self, rss, dec_idx, log_true_e_idxs, log_e_idxs, psi_idxs):
+            self,
+            rss,
+            dec_idx,
+            log_true_e_idxs,
+            log_e_idxs,
+            psi_idxs):
         """Samples ang_err values for the given source declination, true
         energy bins, log_e bins, and psi bins.
 
         Parameters
         ----------
         rss : instance of RandomStateService
             The RandomStateService which should be used for drawing random
@@ -837,15 +884,15 @@
         if (len(log_true_e_idxs) != len(log_e_idxs)) and\
            (len(log_e_idxs) != len(psi_idxs)):
             raise ValueError(
                 'The lengths of log_true_e_idxs, log_e_idxs, and psi_idxs must '
                 'be equal!')
 
         n_evt = len(log_true_e_idxs)
-        ang_err_idx = np.empty((n_evt,), dtype=np.int_)
+        ang_err_idx = np.empty((n_evt,), dtype=np.int64)
         ang_err = np.empty((n_evt,), dtype=np.double)
 
         unique_log_true_e_idxs = np.unique(log_true_e_idxs)
         for b_log_true_e_idx in unique_log_true_e_idxs:
             m = log_true_e_idxs == b_log_true_e_idx
             bb_unique_log_e_idxs = np.unique(log_e_idxs[m])
             for bb_log_e_idx in bb_unique_log_e_idxs:
```

### Comparing `skyllh-23.1.1/skyllh/analyses/i3/publicdata_ps/time_dependent_ps.py` & `skyllh-23.2.0/skyllh/analyses/i3/publicdata_ps/mcbkg_ps.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,310 +1,160 @@
 # -*- coding: utf-8 -*-
 
-"""Setup the time-dependent analysis. For now this works on a single dataset.
+"""The mcbkg_ps analysis is a multi-dataset time-integrated single source
+analysis with a two-component likelihood function using a spacial and an energy
+event PDF. It initializes the background energy pdf using auxiliary fluxes and
+pdfs, which are generated by running `scripts/mceq_atm_bkg.py` script.
 """
 
 import argparse
 import logging
 import numpy as np
+import pickle
 
-from skyllh.core.progressbar import ProgressBar
-
-# Classes to define the source hypothesis.
-from skyllh.physics.source import PointLikeSource
-from skyllh.physics.flux import PowerLawFlux
-from skyllh.core.source_hypo_group import SourceHypoGroup
-from skyllh.core.source_hypothesis import SourceHypoGroupManager
-
-# Classes to define the fit parameters.
-from skyllh.core.parameters import (
-    SingleSourceFitParameterMapper,
-    FitParameter
+from skyllh.analyses.i3.publicdata_ps.backgroundpdf import (
+    PDMCBackgroundI3EnergyPDF,
+)
+from skyllh.analyses.i3.publicdata_ps.detsigyield import (
+    PDSingleParamFluxPointLikeSourceI3DetSigYieldBuilder,
+)
+from skyllh.analyses.i3.publicdata_ps.pdfratio import (
+    PDSigSetOverBkgPDFRatio,
+)
+from skyllh.analyses.i3.publicdata_ps.signal_generator import (
+    PDDatasetSignalGenerator,
+)
+from skyllh.analyses.i3.publicdata_ps.signalpdf import (
+    PDSignalEnergyPDFSet,
+)
+from skyllh.analyses.i3.publicdata_ps.utils import (
+    create_energy_cut_spline,
+    get_tdm_field_func_psi,
 )
-
-# Classes for the minimizer.
-from skyllh.core.minimizer import Minimizer, LBFGSMinimizerImpl
-from skyllh.core.minimizers.iminuit import IMinuitMinimizerImpl
-
-# Classes for utility functionality.
-from skyllh.core.config import CFG
-from skyllh.core.random import RandomStateService
-from skyllh.core.optimize import SpatialBoxEventSelectionMethod
-from skyllh.core.smoothing import BlockSmoothingFilter
-from skyllh.core.timing import TimeLord
-from skyllh.core.trialdata import TrialDataManager
-
-# Classes for defining the analysis.
-from skyllh.core.test_statistic import TestStatisticWilks
 from skyllh.core.analysis import (
-    TimeIntegratedMultiDatasetSingleSourceAnalysis,
+    SingleSourceMultiDatasetLLHRatioAnalysis as Analysis,
 )
-
-# Classes to define the background generation.
-from skyllh.core.scrambling import DataScrambler
-from skyllh.i3.scrambling import I3SeasonalVariationTimeScramblingMethod
-from skyllh.i3.background_generation import FixedScrambledExpDataI3BkgGenMethod
-
-# Classes to define the signal and background PDFs.
-from skyllh.core.signalpdf import (
-    RayleighPSFPointSourceSignalSpatialPDF,
-    SignalBoxTimePDF,
-    SignalGaussTimePDF
+from skyllh.core.config import (
+    Config,
 )
-from skyllh.core.backgroundpdf import BackgroundUniformTimePDF
-from skyllh.i3.backgroundpdf import (
-    DataBackgroundI3SpatialPDF
+from skyllh.core.debugging import (
+    get_logger,
+    setup_logger,
+    setup_console_handler,
+    setup_file_handler,
+)
+from skyllh.core.event_selection import (
+    SpatialBoxEventSelectionMethod,
+)
+from skyllh.core.flux_model import (
+    PowerLawEnergyFluxProfile,
+    SteadyPointlikeFFM,
+)
+from skyllh.core.minimizer import (
+    LBFGSMinimizerImpl,
+    Minimizer,
+)
+from skyllh.core.minimizers.iminuit import (
+    IMinuitMinimizerImpl,
+)
+from skyllh.core.model import (
+    DetectorModel,
+)
+from skyllh.core.parameters import (
+    Parameter,
+    ParameterModelMapper,
 )
-from skyllh.core.pdf import TimePDF
-
-# Classes to define the spatial and energy PDF ratios.
 from skyllh.core.pdfratio import (
-    SpatialSigOverBkgPDFRatio,
-    SigOverBkgPDFRatio
+    SigOverBkgPDFRatio,
 )
-
-# Analysis utilities.
-from skyllh.core.analysis_utils import (
-    pointlikesource_to_data_field_array
+from skyllh.core.progressbar import (
+    ProgressBar,
 )
-
-from skyllh.core.expectation_maximization import em_fit
-
-# Analysis specific classes for working with the public data.
-from skyllh.analyses.i3.publicdata_ps.signal_generator import (
-    PDTimeDependentSignalGenerator
+from skyllh.core.random import (
+    RandomStateService,
 )
-from skyllh.analyses.i3.publicdata_ps.detsigyield import (
-    PublicDataPowerLawFluxPointLikeSourceI3DetSigYieldImplMethod
+from skyllh.core.scrambling import (
+    DataScrambler,
+    UniformRAScramblingMethod,
 )
-from skyllh.analyses.i3.publicdata_ps.signalpdf import (
-    PDSignalEnergyPDFSet
+from skyllh.core.signal_generator import (
+    MultiDatasetSignalGenerator,
 )
-from skyllh.analyses.i3.publicdata_ps.pdfratio import (
-    PDPDFRatio
+from skyllh.core.signalpdf import (
+    RayleighPSFPointSourceSignalSpatialPDF,
 )
-from skyllh.analyses.i3.publicdata_ps.backgroundpdf import (
-    PDDataBackgroundI3EnergyPDF
+from skyllh.core.source_hypo_grouping import (
+    SourceHypoGroup,
+    SourceHypoGroupManager,
 )
-from skyllh.analyses.i3.publicdata_ps.utils import (
-    create_energy_cut_spline,
+from skyllh.core.source_model import (
+    PointLikeSource,
 )
-from skyllh.analyses.i3.publicdata_ps.time_integrated_ps import (
-    psi_func,
+from skyllh.core.test_statistic import (
+    WilksTestStatistic,
+)
+from skyllh.core.timing import (
+    TimeLord,
+)
+from skyllh.core.trialdata import (
+    TrialDataManager,
+)
+from skyllh.core.utils.analysis import (
+    pointlikesource_to_data_field_array,
+)
+from skyllh.datasets.i3 import (
+    data_samples,
+)
+from skyllh.i3.background_generation import (
+    FixedScrambledExpDataI3BkgGenMethod,
+)
+from skyllh.i3.backgroundpdf import (
+    DataBackgroundI3SpatialPDF,
 )
 
 
-def change_time_pdf(analysis, gauss=None, box=None):
-    """Changes the time pdf.
-
-    Parameters
-    ----------
-    gauss : dict | None
-        None or dictionary with {"mu": float, "sigma": float}.
-    box : dict | None
-        None or dictionary with {"start": float, "end": float}.
-    """
-
-    if gauss is None and box is None:
-        raise TypeError("Either gauss or box have to be specified as time pdf.")
-
-    grl = analysis._data_list[0].grl
-    # redo this in case the background pdf was not calculated before
-    time_bkgpdf = BackgroundUniformTimePDF(grl)
-    if gauss is not None:
-        time_sigpdf = SignalGaussTimePDF(grl, gauss['mu'], gauss['sigma'])
-    elif box is not None:
-        time_sigpdf = SignalBoxTimePDF(grl, box["start"], box["end"])
-
-    time_pdfratio = SigOverBkgPDFRatio(
-        sig_pdf=time_sigpdf,
-        bkg_pdf=time_bkgpdf,
-        pdf_type=TimePDF
-    )
-
-    # the next line seems to make no difference in the llh evaluation. We keep it for consistency
-    analysis._llhratio.llhratio_list[0].pdfratio_list[2] = time_pdfratio
-    # this line here is relevant for the llh evaluation
-    analysis._llhratio.llhratio_list[0]._pdfratioarray._pdfratio_list[2] = time_pdfratio
-
-    #  change detector signal yield with flare livetime in sample (1 / grl_norm in pdf),
-    #  rebuild the histograms if it is changed...
-
-
-def get_energy_spatial_signal_over_background(analysis, fitparams):
-    """Returns the signal over background ratio for
-    (spatial_signal * energy_signal) / (spatial_background * energy_background).
-
-    Parameter
-    ---------
-    fitparams : dict
-        Dictionary with {"gamma": float} for energy pdf.
-
-    Returns
-    -------
-    ratio : 1d ndarray
-        Product of spatial and energy signal over background pdfs.
-    """
-
-    ratio = analysis._llhratio.llhratio_list[0].pdfratio_list[0].get_ratio(analysis._tdm_list[0], fitparams)
-    ratio *= analysis._llhratio.llhratio_list[0].pdfratio_list[1].get_ratio(analysis._tdm_list[0], fitparams)
-
-    return ratio
-
-
-def change_fluxmodel_gamma(analysis, gamma):
-    """Set new gamma for the flux model.
-
-    Parameter
-    ---------
-    gamma : float
-        Spectral index for flux model.
-    """
-
-    analysis.src_hypo_group_manager.src_hypo_group_list[0].fluxmodel.gamma = gamma
-
-
-def change_signal_time(analysis, gauss=None, box=None):
-    """Change the signal injection to gauss or box.
-
-    Parameters
-    ----------
-    gauss : dict | None
-        None or dictionary {"mu": float, "sigma": float}.
-    box : dict | None
-        None or dictionary {"start" : float, "end" : float}.
-    """
-
-    analysis.sig_generator.set_flare(box=box, gauss=gauss)
-
-
-def calculate_TS(analysis, em_results, rss):
-    """Calculate the best TS value for the expectation maximization gamma scan.
-
-    Parameters
-    ----------
-    em_results : 1d ndarray of tuples
-        Gamma scan result.
-    rss : instance of RandomStateService
-        The instance of RandomStateService that should be used to generate
-        random numbers from.
-
-    Returns
-    -------
-    float maximized TS value
-    tuple(gamma from em scan [float], best fit mean time [float], best fit width [float])
-    (float ns, float gamma) fitparams from TS optimization
-    """
-
-    max_TS = 0
-    best_time = None
-    best_fitparams = None
-    for index, result in enumerate(em_results):
-        change_time_pdf(analysis,  gauss={"mu": result["mu"], "sigma": result["sigma"]})
-        (fitparamset, log_lambda_max, fitparam_values, status) = analysis.maximize_llhratio(rss)
-        TS = analysis.calculate_test_statistic(log_lambda_max, fitparam_values)
-        if TS > max_TS:
-            max_TS = TS
-            best_time = result
-            best_fitparams = fitparam_values
-
-    return max_TS, best_time, best_fitparams
-
-
-def run_gamma_scan_single_flare(analysis, remove_time=None, gamma_min=1, gamma_max=5, n_gamma=51):
-    """Run em for different gammas in the signal energy pdf
-
-    Parameters
-    ----------
-    remove_time : float
-        Time information of event that should be removed.
-    gamma_min : float
-        Lower bound for gamma scan.
-    gamma_max : float
-        Upper bound for gamma scan.
-    n_gamma : int
-        Number of steps for gamma scan.
-
-    Returns
-    -------
-    array with "gamma", "mu", "sigma", and scaling factor for flare "ns_em"
-    """
-    dtype = [("gamma", "f8"), ("mu", "f8"), ("sigma", "f8"), ("ns_em", "f8")]
-    results = np.empty(n_gamma, dtype=dtype)
-
-    time = analysis._tdm_list[0].get_data("time")
-
-    for index, g in enumerate(np.linspace(gamma_min, gamma_max, n_gamma)):
-        ratio = get_energy_spatial_signal_over_background(analysis, {"gamma": g})
-        mu, sigma, ns = em_fit(time, ratio, n=1, tol=1.e-200, iter_max=500, weight_thresh=0,
-                            initial_width=5000, remove_x=remove_time)
-        results[index] = (g, mu[0], sigma[0], ns[0])
-
-    return results
-
-
-def unblind_flare(analysis, remove_time=None):
-    """Run EM on unscrambled data. Similar to the original analysis, remove the alert event.
-
-    Parameters
-    ----------
-    remove_time : float
-        Time information of event that should be removed.
-        In the case of the TXS analysis: remove_time=58018.8711856
-
-    Returns
-    -------
-    array with "gamma", "mu", "sigma", and scaling factor for flare "ns_em"
-    """
-
-    # get the original unblinded data
-    rss = RandomStateService(seed=1)
-    analysis.unblind(rss)
-    time_results = run_gamma_scan_single_flare(analysis, remove_time=remove_time)
-    return time_results
+def TXS_location():
+    src_ra = np.radians(77.358)
+    src_dec = np.radians(5.693)
+    return (src_ra, src_dec)
 
 
 def create_analysis(
     datasets,
     source,
-    gauss=None,
-    box=None,
     refplflux_Phi0=1,
     refplflux_E0=1e3,
-    refplflux_gamma=2.0,
-    ns_seed=100.0,
-    ns_min=0.,
+    refplflux_gamma=2,
+    ns_seed=100,
+    ns_min=0,
     ns_max=1e3,
-    gamma_seed=3.0,
-    gamma_min=1.,
-    gamma_max=5.,
-    kde_smoothing=False,
-    minimizer_impl="LBFGS",
+    gamma_seed=3,
+    gamma_min=1,
+    gamma_max=5,
+    minimizer_impl='LBFGS',
     cut_sindec=None,
     spl_smooth=None,
     cap_ratio=False,
     compress_data=False,
     keep_data_fields=None,
-    optimize_delta_angle=10,
+    evt_sel_delta_angle_deg=10,
+    efficiency_mode=None,
     tl=None,
-    ppbar=None
+    ppbar=None,
+    logger_name=None,
 ):
     """Creates the Analysis instance for this particular analysis.
 
-    Parameters:
-    -----------
+    Parameters
+    ----------
     datasets : list of Dataset instances
         The list of Dataset instances, which should be used in the
         analysis.
     source : PointLikeSource instance
         The PointLikeSource instance defining the point source position.
-    gauss : None or dictionary with mu, sigma
-        None if no Gaussian time pdf. Else dictionary with {"mu": float, "sigma": float} of Gauss
-    box : None or dictionary with start, end
-        None if no Box shaped time pdf. Else dictionary with {"start": float, "end": float} of box.
     refplflux_Phi0 : float
         The flux normalization to use for the reference power law flux model.
     refplflux_E0 : float
         The reference energy to use for the reference power law flux model.
     refplflux_gamma : float
         The spectral index to use for the reference power law flux model.
     ns_seed : float
@@ -316,222 +166,357 @@
     gamma_seed : float | None
         Value to seed the minimizer with for the gamma fit. If set to None,
         the refplflux_gamma value will be set as gamma_seed.
     gamma_min : float
         Lower bound for gamma fit.
     gamma_max : float
         Upper bound for gamma fit.
-    kde_smoothing : bool
-        Apply a KDE-based smoothing to the data-driven background pdf.
-        Default: False.
-    minimizer_impl : str | "LBFGS"
-        Minimizer implementation to be used. Supported options are "LBFGS"
+    minimizer_impl : str
+        Minimizer implementation to be used. Supported options are ``"LBFGS"``
         (L-BFG-S minimizer used from the :mod:`scipy.optimize` module), or
-        "minuit" (Minuit minimizer used by the :mod:`iminuit` module).
+        ``"minuit"`` (Minuit minimizer used by the :mod:`iminuit` module).
         Default: "LBFGS".
     cut_sindec : list of float | None
         sin(dec) values at which the energy cut in the southern sky should
         start. If None, np.sin(np.radians([-2, 0, -3, 0, 0])) is used.
     spl_smooth : list of float
         Smoothing parameters for the 1D spline for the energy cut. If None,
         [0., 0.005, 0.05, 0.2, 0.3] is used.
-    cap_ratio : bool
-        If set to True, the energy PDF ratio will be capped to a finite value
-        where no background energy PDF information is available. This will
-        ensure that an energy PDF ratio is available for high energies where
-        no background is available from the experimental data.
-        If kde_smoothing is set to True, cap_ratio should be set to False!
-        Default is False.
     compress_data : bool
         Flag if the data should get converted from float64 into float32.
     keep_data_fields : list of str | None
         List of additional data field names that should get kept when loading
         the data.
-    optimize_delta_angle : float
+    evt_sel_delta_angle_deg : float
         The delta angle in degrees for the event selection optimization methods.
+    efficiency_mode : str | None
+        The efficiency mode the data should get loaded with. Possible values
+        are:
+
+        ``'memory'``:
+            The data will be load in a memory efficient way. This will
+            require more time, because all data records of a file will
+            be loaded sequentially.
+        ``'time'``:
+            The data will be loaded in a time efficient way. This will
+            require more memory, because each data file gets loaded in
+            memory at once.
+
+        The default value is ``'time'``. If set to ``None``, the default
+        value will be used.
     tl : TimeLord instance | None
         The TimeLord instance to use to time the creation of the analysis.
     ppbar : ProgressBar instance | None
         The instance of ProgressBar for the optional parent progress bar.
+    logger_name : str | None
+        The name of the logger to be used. If set to ``None``, ``__name__`` will
+        be used.
 
     Returns
     -------
-    analysis : TimeIntegratedMultiDatasetSingleSourceAnalysis
+    ana : instance of SingleSourceMultiDatasetLLHRatioAnalysis
         The Analysis instance for this analysis.
     """
-
-    if len(datasets) != 1:
-        raise RuntimeError(
-            'This analysis supports only analyses with only single datasets '
-            'at the moment!')
-
-    if gauss is None and box is None:
-        raise ValueError("No time pdf specified (box or gauss)")
-    if gauss is not None and box is not None:
-        raise ValueError(
-            "Time PDF cannot be both Gaussian and box shaped. "
-            "Please specify only one shape.")
+    if logger_name is None:
+        logger_name = __name__
+    logger = get_logger(logger_name)
 
     # Create the minimizer instance.
-    if minimizer_impl == "LBFGS":
+    if minimizer_impl == 'LBFGS':
         minimizer = Minimizer(LBFGSMinimizerImpl())
-    elif minimizer_impl == "minuit":
+    elif minimizer_impl == 'minuit':
         minimizer = Minimizer(IMinuitMinimizerImpl(ftol=1e-8))
     else:
         raise NameError(
             f"Minimizer implementation `{minimizer_impl}` is not supported "
             "Please use `LBFGS` or `minuit`.")
 
     # Define the flux model.
-    flux_model = PowerLawFlux(
-        Phi0=refplflux_Phi0, E0=refplflux_E0, gamma=refplflux_gamma)
+    fluxmodel = SteadyPointlikeFFM(
+        Phi0=refplflux_Phi0,
+        energy_profile=PowerLawEnergyFluxProfile(
+            E0=refplflux_E0,
+            gamma=refplflux_gamma))
 
     # Define the fit parameter ns.
-    fitparam_ns = FitParameter('ns', ns_min, ns_max, ns_seed)
-
-    # Define the gamma fit parameter.
-    fitparam_gamma = FitParameter(
-        'gamma', valmin=gamma_min, valmax=gamma_max, initial=gamma_seed)
+    param_ns = Parameter(
+        name='ns',
+        initial=ns_seed,
+        valmin=ns_min,
+        valmax=ns_max)
+
+    # Define the fit parameter gamma.
+    param_gamma = Parameter(
+        name='gamma',
+        initial=gamma_seed,
+        valmin=gamma_min,
+        valmax=gamma_max)
 
     # Define the detector signal efficiency implementation method for the
     # IceCube detector and this source and flux_model.
     # The sin(dec) binning will be taken by the implementation method
     # automatically from the Dataset instance.
-    gamma_grid = fitparam_gamma.as_linear_grid(delta=0.1)
-    detsigyield_implmethod = \
-        PublicDataPowerLawFluxPointLikeSourceI3DetSigYieldImplMethod(
-            gamma_grid)
-
-    # Define the signal generation method.
-    #sig_gen_method = PointLikeSourceI3SignalGenerationMethod()
-    sig_gen_method = None
+    gamma_grid = param_gamma.as_linear_grid(delta=0.1)
+    detsigyield_builder =\
+        PDSingleParamFluxPointLikeSourceI3DetSigYieldBuilder(
+            param_grid=gamma_grid)
 
     # Create a source hypothesis group manager.
-    src_hypo_group_manager = SourceHypoGroupManager(
+    shg_mgr = SourceHypoGroupManager(
         SourceHypoGroup(
-            source, flux_model, detsigyield_implmethod, sig_gen_method))
+            sources=source,
+            fluxmodel=fluxmodel,
+            detsigyield_builders=detsigyield_builder,
+            sig_gen_method=None))
+
+    # Define a detector model for the ns fit parameter.
+    detector_model = DetectorModel('IceCube')
+
+    # Define the parameter model mapper for the analysis, which will map global
+    # parameters to local source parameters.
+    pmm = ParameterModelMapper(
+        models=[detector_model, source])
+    pmm.map_param(param_ns, models=detector_model)
+    pmm.map_param(param_gamma, models=source)
 
-    # Create a source fit parameter mapper and define the fit parameters.
-    src_fitparam_mapper = SingleSourceFitParameterMapper()
-    src_fitparam_mapper.def_fit_parameter(fitparam_gamma)
+    logger.info(str(pmm))
 
     # Define the test statistic.
-    test_statistic = TestStatisticWilks()
+    test_statistic = WilksTestStatistic()
+
+    # Define the data scrambler with its data scrambling method, which is used
+    # for background generation.
+    data_scrambler = DataScrambler(UniformRAScramblingMethod())
+
+    # Create background generation method.
+    bkg_gen_method = FixedScrambledExpDataI3BkgGenMethod(data_scrambler)
+
+    # Create the minimizer instance.
+    minimizer = Minimizer(LBFGSMinimizerImpl())
 
     # Create the Analysis instance.
-    analysis = TimeIntegratedMultiDatasetSingleSourceAnalysis(
-        src_hypo_group_manager,
-        src_fitparam_mapper,
-        fitparam_ns,
-        test_statistic,
-        sig_generator_cls=PDTimeDependentSignalGenerator
+    ana = Analysis(
+        shg_mgr=shg_mgr,
+        pmm=pmm,
+        test_statistic=test_statistic,
+        bkg_gen_method=bkg_gen_method,
+        sig_generator_cls=MultiDatasetSignalGenerator,
     )
 
     # Define the event selection method for pure optimization purposes.
     # We will use the same method for all datasets.
     event_selection_method = SpatialBoxEventSelectionMethod(
-        src_hypo_group_manager, delta_angle=np.deg2rad(optimize_delta_angle))
+        shg_mgr=shg_mgr,
+        delta_angle=np.deg2rad(evt_sel_delta_angle_deg))
 
-    # Prepare the spline parameters.
+    # Prepare the spline parameters for the signal generator.
     if cut_sindec is None:
         cut_sindec = np.sin(np.radians([-2, 0, -3, 0, 0]))
     if spl_smooth is None:
         spl_smooth = [0., 0.005, 0.05, 0.2, 0.3]
     if len(spl_smooth) < len(datasets) or len(cut_sindec) < len(datasets):
         raise AssertionError(
-            "The length of the spl_smooth and of the cut_sindec must be equal "
-            f"to the length of datasets: {len(datasets)}.")
+            'The length of the spl_smooth and of the cut_sindec must be equal '
+            f'to the length of datasets: {len(datasets)}.')
 
     # Add the data sets to the analysis.
     pbar = ProgressBar(len(datasets), parent=ppbar).start()
-    data_list = []
-    energy_cut_splines = []
-    for idx, ds in enumerate(datasets):
+    for (ds_idx, ds) in enumerate(datasets):
         # Load the data of the data set.
         data = ds.load_and_prepare_data(
             keep_fields=keep_data_fields,
             compress=compress_data,
+            efficiency_mode=efficiency_mode,
             tl=tl)
-        data_list.append(data)
-
-        # Create a trial data manager and add the required data fields.
-        tdm = TrialDataManager()
-        tdm.add_source_data_field('src_array',
-                                  pointlikesource_to_data_field_array)
-        tdm.add_data_field('psi', psi_func)
 
         sin_dec_binning = ds.get_binning_definition('sin_dec')
-        log_energy_binning = ds.get_binning_definition('log_energy')
 
         # Create the spatial PDF ratio instance for this dataset.
         spatial_sigpdf = RayleighPSFPointSourceSignalSpatialPDF(
             dec_range=np.arcsin(sin_dec_binning.range))
         spatial_bkgpdf = DataBackgroundI3SpatialPDF(
-            data.exp, sin_dec_binning)
-        spatial_pdfratio = SpatialSigOverBkgPDFRatio(
-            spatial_sigpdf, spatial_bkgpdf)
+            data_exp=data.exp,
+            sin_dec_binning=sin_dec_binning)
+        spatial_pdfratio = SigOverBkgPDFRatio(
+            sig_pdf=spatial_sigpdf,
+            bkg_pdf=spatial_bkgpdf)
 
         # Create the energy PDF ratio instance for this dataset.
         energy_sigpdfset = PDSignalEnergyPDFSet(
             ds=ds,
             src_dec=source.dec,
-            flux_model=flux_model,
-            fitparam_grid_set=gamma_grid,
+            fluxmodel=fluxmodel,
+            param_grid_set=gamma_grid,
             ppbar=ppbar
         )
-        smoothing_filter = BlockSmoothingFilter(nbins=1)
-        energy_bkgpdf = PDDataBackgroundI3EnergyPDF(
-            data.exp, log_energy_binning, sin_dec_binning,
-            smoothing_filter, kde_smoothing)
 
-        energy_pdfratio = PDPDFRatio(
-            sig_pdf_set=energy_sigpdfset,
-            bkg_pdf=energy_bkgpdf,
-            cap_ratio=cap_ratio
+        bkg_pdf_pathfilename = ds.get_abs_pathfilename_list(
+            ds.get_aux_data_definition('pdf_bkg_datafile'))[0]
+        with open(bkg_pdf_pathfilename, 'rb') as f:
+            bkg_pdf_data = pickle.load(f)
+        energy_bkgpdf = PDMCBackgroundI3EnergyPDF(
+            pdf_log10emu_sindecmu=bkg_pdf_data['pdf'],
+            log10emu_binning=bkg_pdf_data['log10emu_binning'],
+            sindecmu_binning=bkg_pdf_data['sindecmu_binning'],
         )
 
-        pdfratios = [spatial_pdfratio, energy_pdfratio]
+        energy_pdfratio = PDSigSetOverBkgPDFRatio(
+            sig_pdf_set=energy_sigpdfset,
+            bkg_pdf=energy_bkgpdf,
+            cap_ratio=cap_ratio)
 
-        # Create the time PDF ratio instance for this dataset.
-        if gauss is not None or box is not None:
-            time_bkgpdf = BackgroundUniformTimePDF(data.grl)
-            if gauss is not None:
-                time_sigpdf = SignalGaussTimePDF(
-                    data.grl, gauss['mu'], gauss['sigma'])
-            elif box is not None:
-                time_sigpdf = SignalBoxTimePDF(
-                    data.grl, box["start"], box["end"])
-            time_pdfratio = SigOverBkgPDFRatio(
-                sig_pdf=time_sigpdf,
-                bkg_pdf=time_bkgpdf,
-                pdf_type=TimePDF
-            )
-            pdfratios.append(time_pdfratio)
+        pdfratio = spatial_pdfratio * energy_pdfratio
 
-        analysis.add_dataset(
-            ds, data, pdfratios, tdm, event_selection_method)
+        # Create a trial data manager and add the required data fields.
+        tdm = TrialDataManager()
+        tdm.add_source_data_field(
+            name='src_array',
+            func=pointlikesource_to_data_field_array)
+        tdm.add_data_field(
+            name='psi',
+            func=get_tdm_field_func_psi(),
+            dt='dec',
+            is_srcevt_data=True)
 
         energy_cut_spline = create_energy_cut_spline(
-            ds, data.exp, spl_smooth[idx])
-        energy_cut_splines.append(energy_cut_spline)
+            ds,
+            data.exp,
+            spl_smooth[ds_idx])
+
+        sig_generator = PDDatasetSignalGenerator(
+            shg_mgr=shg_mgr,
+            ds=ds,
+            ds_idx=ds_idx,
+            energy_cut_spline=energy_cut_spline,
+            cut_sindec=cut_sindec[ds_idx],
+        )
+
+        ana.add_dataset(
+            dataset=ds,
+            data=data,
+            pdfratio=pdfratio,
+            tdm=tdm,
+            event_selection_method=event_selection_method,
+            sig_generator=sig_generator)
 
         pbar.increment()
     pbar.finish()
 
-    analysis.llhratio = analysis.construct_llhratio(minimizer, ppbar=ppbar)
+    ana.construct_services(
+        ppbar=ppbar)
 
-    # Define the data scrambler with its data scrambling method, which is used
-    # for background generation.
+    ana.llhratio = ana.construct_llhratio(
+        minimizer=minimizer,
+        ppbar=ppbar)
 
-    # FIXME: Support multiple datasets for the DataScrambler.
-    data_scrambler = DataScrambler(I3SeasonalVariationTimeScramblingMethod(data_list[0]))
-    # Create background generation method.
-    bkg_gen_method = FixedScrambledExpDataI3BkgGenMethod(data_scrambler)
+    ana.construct_signal_generator()
+
+    return ana
+
+
+if __name__ == '__main__':
+    p = argparse.ArgumentParser(
+        description='Calculates TS for a given source location using the '
+        '10-year public point source sample.',
+        formatter_class=argparse.RawTextHelpFormatter
+    )
+    p.add_argument(
+        '--dec',
+        default=23.8,
+        type=float,
+        help='The source declination in degrees.'
+    )
+    p.add_argument(
+        '--ra',
+        default=216.76,
+        type=float,
+        help='The source right-ascention in degrees.'
+    )
+    p.add_argument(
+        '--gamma-seed',
+        default=3,
+        type=float,
+        help='The seed value of the gamma fit parameter.'
+    )
+    p.add_argument(
+        '--data_base_path',
+        default=None,
+        type=str,
+        help='The base path to the data samples (default=None)'
+    )
+    p.add_argument(
+        '--seed',
+        default=1,
+        type=int,
+        help='The random number generator seed for the likelihood '
+             'minimization.'
+    )
+    p.add_argument(
+        '--ncpu',
+        default=1,
+        type=int,
+        help='The number of CPUs to utilize where parallelization is possible.'
+    )
+    p.add_argument(
+        '--cap-ratio',
+        action='store_true',
+        help='Switch to cap the energy PDF ratio.')
+    p.set_defaults(cap_ratio=False)
+    args = p.parse_args()
+
+    # Setup `skyllh` package logging.
+    # To optimize logging set the logging level to the lowest handling level.
+    setup_logger('skyllh', logging.DEBUG)
+    log_format = '%(asctime)s %(processName)s %(name)s %(levelname)s: '\
+                 '%(message)s'
+    setup_console_handler(
+        'skyllh',
+        logging.INFO, log_format)
+    setup_file_handler(
+        'skyllh',
+        'debug.log',
+        log_level=logging.DEBUG,
+        log_format=log_format)
+
+    cfg = Config()
+    cfg.set_ncpu(args.ncpu)
+
+    sample_seasons = [
+        # ('PublicData_10y_ps', 'IC40'),
+        # ('PublicData_10y_ps', 'IC59'),
+        # ('PublicData_10y_ps', 'IC79'),
+        # ('PublicData_10y_ps', 'IC86_I'),
+        ('PublicData_10y_ps', 'IC86_II'),
+        # ('PublicData_10y_ps', 'IC86_II-VII')
+    ]
+
+    datasets = []
+    for (sample, season) in sample_seasons:
+        # Get the dataset from the correct dataset collection.
+        dsc = data_samples[sample].create_dataset_collection(
+            args.data_base_path)
+        datasets.append(dsc.get_dataset(season))
+
+    # Define a random state service.
+    rss = RandomStateService(args.seed)
+    # Define the point source.
+    source = PointLikeSource(np.deg2rad(args.ra), np.deg2rad(args.dec))
+    print('source: ', str(source))
+
+    tl = TimeLord()
+
+    with tl.task_timer('Creating analysis.'):
+        ana = create_analysis(
+            datasets=datasets,
+            source=source,
+            cap_ratio=args.cap_ratio,
+            gamma_seed=args.gamma_seed,
+            tl=tl)
 
-    analysis.bkg_gen_method = bkg_gen_method
-    analysis.construct_background_generator()
+    with tl.task_timer('Unblinding data.'):
+        (TS, fitparam_dict, status) = ana.unblind(rss)
 
-    analysis.construct_signal_generator(
-        llhratio=analysis.llhratio, energy_cut_splines=energy_cut_splines,
-        cut_sindec=cut_sindec, box=box, gauss=gauss)
+    print('TS = %g' % (TS))
+    print('ns_fit = %g' % (fitparam_dict['ns']))
+    print('gamma_fit = %g' % (fitparam_dict['gamma']))
 
-    return analysis
+    print(tl)
```

### Comparing `skyllh-23.1.1/skyllh/analyses/i3/publicdata_ps/utils.py` & `skyllh-23.2.0/skyllh/analyses/i3/publicdata_ps/utils.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,25 +1,37 @@
 # -*- coding: utf-8 -*-
 
 import numpy as np
 
-from scipy import interpolate
-from scipy import integrate
-
-from skyllh.core.binning import get_bincenters_from_binedges
+from scipy import (
+    integrate,
+    interpolate,
+)
+
+from skyllh.core.binning import (
+    get_bincenters_from_binedges,
+)
+from skyllh.core.utils.coords import (
+    angular_separation,
+)
 
 
 class FctSpline1D(object):
     """Class to represent a 1D function spline using the PchipInterpolator
     class from scipy.
 
     The evaluate the spline, use the ``__call__`` method.
     """
 
-    def __init__(self, f, x_binedges, norm=False, **kwargs):
+    def __init__(
+            self,
+            f,
+            x_binedges,
+            norm=False,
+            **kwargs):
         """Creates a new 1D function spline using the PchipInterpolator
         class from scipy.
 
         Parameters
         ----------
         f : (n_x,)-shaped 1D numpy ndarray
             The numpy ndarray holding the function values at the bin centers.
@@ -47,15 +59,18 @@
                 self.__call__,
                 self.x_min,
                 self.x_max,
                 limit=200,
                 full_output=1
             )[0]
 
-    def __call__(self, x, oor_value=0):
+    def __call__(
+            self,
+            x,
+            oor_value=0):
         """Evaluates the spline at the given x values. For x-values
         outside the spline's range, the oor_value is returned.
 
         Parameters
         ----------
         x : (n_x,)-shaped 1D numpy ndarray
             The numpy ndarray holding the x values at which the spline should
@@ -65,19 +80,22 @@
 
         Returns
         -------
         f : (n_x,)-shaped 1D numpy ndarray
             The numpy ndarray holding the evaluated values of the spline.
         """
         f = self.spl_f(x)
-        f = np.nan_to_num(f, nan=oor_value)
+        f = np.where(np.isnan(f), oor_value, f)
 
         return f
 
-    def evaluate(self, *args, **kwargs):
+    def evaluate(
+            self,
+            *args,
+            **kwargs):
         """Alias for the __call__ method.
         """
         return self(*args, **kwargs)
 
 
 class FctSpline2D(object):
     """Class to represent a 2D function spline using the RectBivariateSpline
@@ -85,15 +103,20 @@
 
     The spline is constructed in the log10 space of the function value to
     ensure a smooth spline.
 
     The evaluate the spline, use the ``__call__`` method.
     """
 
-    def __init__(self, f, x_binedges, y_binedges, **kwargs):
+    def __init__(
+            self,
+            f,
+            x_binedges,
+            y_binedges,
+            **kwargs):
         """Creates a new 2D function spline using the RectBivariateSpline
         class from scipy.
 
         Parameters
         ----------
         f : (n_x, n_y)-shaped 2D numpy ndarray
             he numpy ndarray holding the function values at the bin centers.
@@ -122,15 +145,19 @@
         m = f > 0
         z[m] = np.log10(f[m])
         z[np.invert(m)] = np.min(z[m]) - 3
 
         self.spl_log10_f = interpolate.RectBivariateSpline(
             x, y, z, kx=3, ky=3, s=0)
 
-    def __call__(self, x, y, oor_value=0):
+    def __call__(
+            self,
+            x,
+            y,
+            oor_value=0):
         """Evaluates the spline at the given coordinates. For coordinates
         outside the spline's range, the oor_value is returned.
 
         Parameters
         ----------
         x : (n_x,)-shaped 1D numpy ndarray
             The numpy ndarray holding the x values at which the spline should
@@ -154,15 +181,43 @@
 
         f = np.power(10, self.spl_log10_f(x, y))
         f[m_xy_oor] = oor_value
 
         return f
 
 
-def psi_to_dec_and_ra(rss, src_dec, src_ra, psi):
+def clip_grl_start_times(grl_data):
+    """Make sure that the start time of a run is not smaller than the stop time
+    of the previous run.
+
+    Parameters
+    ----------
+    grl_data : instance of numpy structured ndarray
+        The numpy structured ndarray of length N_runs, with the following
+        fields:
+
+        start : float
+            The start time of the run.
+        stop : float
+            The stop time of the run.
+    """
+    start = grl_data['start']
+    stop = grl_data['stop']
+
+    m = (start[1:] - stop[:-1]) < 0
+    new_start = np.where(m, stop[:-1], start[1:])
+
+    grl_data['start'][1:] = new_start
+
+
+def psi_to_dec_and_ra(
+        rss,
+        src_dec,
+        src_ra,
+        psi):
     """Generates random declinations and right-ascension coordinates for the
     given source location and opening angle `psi`.
 
     Parameters
     ----------
     rss : instance of RandomStateService
         The instance of RandomStateService to use for drawing random numbers.
@@ -214,16 +269,18 @@
 
     dec = np.pi/2 - zen
     ra = np.pi - azi
 
     return (dec, ra)
 
 
-def create_energy_cut_spline(ds, exp_data, spl_smooth):
-
+def create_energy_cut_spline(
+        ds,
+        exp_data,
+        spl_smooth):
     """Create the spline for the declination-dependent energy cut
     that the signal generator needs for injection in the southern sky
     Some special conditions are needed for IC79 and IC86_I, because
     their experimental dataset shows events that should probably have
     been cut by the IceCube selection.
     """
     data_exp = exp_data.copy(keep_fields=['sin_dec', 'log_energy'])
@@ -249,7 +306,51 @@
     del data_exp
     sindec_centers = 0.5 * (sindec_edges[1:]+sindec_edges[:-1])
 
     spline = interpolate.UnivariateSpline(
         sindec_centers, min_log_e, k=2, s=spl_smooth)
 
     return spline
+
+
+def get_tdm_field_func_psi(psi_floor=None):
+    """Returns the TrialDataManager (TDM) field function for psi with an
+    optional psi value floor.
+
+    Parameters
+    ----------
+    psi_floor : float | None
+        The optional floor value for psi. This should be ``None`` for a standard
+        point-source analysis that uses an analytic function for the detector's
+        point-spread-function (PSF).
+
+    Returns
+    -------
+    tdm_field_func_psi : function
+        TrialDataManager (TDM) field function for psi.
+    """
+    def tdm_field_func_psi(
+            tdm,
+            shg_mgr,
+            pmm):
+        """TDM data field function to calculate the opening angle between the
+        source positions and the event's reconstructed position.
+        """
+        (src_idxs, evt_idxs) = tdm.src_evt_idxs
+
+        ra = np.take(tdm.get_data('ra'), evt_idxs)
+        dec = np.take(tdm.get_data('dec'), evt_idxs)
+
+        src_array = tdm.get_data('src_array')
+        src_ra = np.take(src_array['ra'], src_idxs)
+        src_dec = np.take(src_array['dec'], src_idxs)
+
+        psi = angular_separation(
+            ra1=ra,
+            dec1=dec,
+            ra2=src_ra,
+            dec2=src_dec,
+            psi_floor=psi_floor)
+
+        return psi
+
+    return tdm_field_func_psi
```

### Comparing `skyllh-23.1.1/skyllh/cluster/commands.py` & `skyllh-23.2.0/skyllh/cluster/commands.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,17 @@
 # -*- coding: utf-8 -*-
 
 import pickle
 
-from skyllh.core.py import int_cast
+from skyllh.core.py import (
+    int_cast,
+)
 from skyllh.cluster.srvclt import (
     Message,
-    receive_object_from_socket
+    receive_object_from_socket,
 )
 
 
 class Command(object):
     """Base class for a command. A command has a command string plus optional
     additional data.
     """
@@ -72,28 +74,32 @@
         self.cn_live_time = cn_live_time
 
     @property
     def cn_start_time(self):
         """The CN's start time as unix time stamp.
         """
         return self._cn_start_time
+
     @cn_start_time.setter
     def cn_start_time(self, t):
-        t = int_cast(t,
+        t = int_cast(
+            t,
             'The cn_start_time property must be castable to type int!')
         self._cn_start_time = t
 
     @property
     def cn_live_time(self):
         """The CN's live time in seconds.
         """
         return self._cn_live_time
+
     @cn_live_time.setter
     def cn_live_time(self, t):
-        t = int_cast(t,
+        t = int_cast(
+            t,
             'The cn_live_time property must be castable to type int!')
         self._cn_live_time = t
 
 
 def receive_command_from_socket(sock, blocksize=2048):
     """Receives a command from the given socket.
     """
```

### Comparing `skyllh-23.1.1/skyllh/cluster/compute_node.py` & `skyllh-23.2.0/skyllh/cluster/compute_node.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,26 +1,23 @@
 # -*- coding: utf-8 -*-
 
-import sys
-sys.path.insert(0, '/home/mwolf/software/_skyllh/skyllh/trunk')
-
 import argparse
-import json
 import socket
 import time
 
-from skyllh.core.py import int_cast
 from skyllh.cluster.commands import (
     ACK,
     MSG,
     RegisterCN,
     ShutdownCN,
-    receive_command_from_socket
+    receive_command_from_socket,
+)
+from skyllh.core.py import (
+    int_cast,
 )
-
 
 
 class ComputeNode(object):
     """The ComputeNode class provides an entity for stand-alone program running
     on a dedicated compute node host.
     """
     def __init__(self, live_time, master_addr, master_port):
@@ -35,79 +32,86 @@
         # Register the compute node to the master node.
         self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
         self.sock.connect((self.master_addr, self.master_port))
 
         # Send the register command to the master and tell .
         RegisterCN(self._start_time, self._live_time).send(self.sock)
         reply = receive_command_from_socket(self.sock)
-        if(not reply.is_same_as(ACK)):
-            raise RuntimeError('The master node did not reply with an ACK '
-                'command!')
+        if not reply.is_same_as(ACK):
+            raise RuntimeError(
+                'The master node did not reply with an ACK command!')
 
-        print('Registered to master %s:%d'%(self._master_addr, self._master_port))
-        print('Runtime set to %d seconds'%(self._live_time))
+        print(f'Registered to master {self._master_addr}:{self._master_port}')
+        print(f'Runtime set to {self._live_time} seconds')
 
     def __del__(self):
         self.sock.close()
 
     @property
     def live_time(self):
         """The time in seconds this ComputeNode instance should be listening for
         requests.
         """
         return self._live_time
+
     @live_time.setter
     def live_time(self, t):
-        t = int_cast(t, 'The live_time property must be castable to type int!')
+        t = int_cast(
+            t,
+            'The live_time property must be castable to type int!')
         self._live_time = t
 
     @property
     def master_addr(self):
         """The address of the SkyLLH master program.
         """
         return self._master_addr
+
     @master_addr.setter
     def master_addr(self, addr):
-        if(not isinstance(addr, str)):
-            raise TypeError('The master_addr property must be of type str!')
+        if not isinstance(addr, str):
+            raise TypeError(
+                'The master_addr property must be of type str!')
         self._master_addr = addr
 
     @property
     def master_port(self):
         """The port number of the SkyLLH master program.
         """
         return self._master_port
+
     @master_port.setter
     def master_port(self, p):
-        p = int_cast(p,
+        p = int_cast(
+            p,
             'The master_port property must be castable to type int!')
         self._master_port = p
 
     def handle_requests(self):
-        if(time.time() > self._start_time + self._live_time):
+        if time.time() > self._start_time + self._live_time:
             raise RuntimeError('Live-time already exceeded!')
 
         while True:
             # Receive a command.
             cmd = receive_command_from_socket(self.sock)
-            if(cmd.is_same_as(MSG)):
-                print('Received general message: %s'%(cmd.msg))
-            elif(cmd.is_same_as(ShutdownCN)):
+            if cmd.is_same_as(MSG):
+                print(f'Received general message: {cmd.msg}')
+            elif cmd.is_same_as(ShutdownCN):
                 print('Received shutdown command. Shutting down.')
                 self.sock.close()
                 return
             else:
                 print('Received unknown command! Ignoring.')
 
-            if(time.time() > self._start_time + self._live_time):
+            if time.time() > self._start_time + self._live_time:
                 print('Live-time exceeded. Shutting down.')
                 return
 
 
-if(__name__ == '__main__'):
+if __name__ == '__main__':
 
     parser = argparse.ArgumentParser(description='SkyLLH Compute Node')
     parser.add_argument(
         'master_addr', type=str,
         help='The address (IP / hostname) of the SkyLLH master program.')
     parser.add_argument(
         'master_port', type=int, default=9999,
```

### Comparing `skyllh-23.1.1/skyllh/cluster/master_node.py` & `skyllh-23.2.0/skyllh/cluster/master_node.py`

 * *Files 6% similar despite different names*

```diff
@@ -5,17 +5,16 @@
 
 from skyllh.cluster.commands import (
     ACK,
     Command,
     MSG,
     ShutdownCN,
     RegisterCN,
-    receive_command_from_socket
+    receive_command_from_socket,
 )
-from skyllh.cluster.srvclt import Message
 
 
 class CNRegistryEntry(object):
     """This class provides an registry entry for a compute node. It holds the
     socket to the compute node.
     """
     def __init__(self, sock, addr, port, cn_start_time, cn_live_time):
@@ -30,19 +29,20 @@
     def __del__(self):
         self.sock.close()
 
     @property
     def key(self):
         """(read-only) The CN's identification key.
         """
-        return '%s:%d'%(self.addr, self.port)
+        return f'{self.addr:s}:{self.port:d}'
 
     def send_command(self, cmd):
-        if(not isinstance(cmd, Command)):
-            raise TypeError('The cmd argument must be an instance of Command!')
+        if not isinstance(cmd, Command):
+            raise TypeError(
+                'The cmd argument must be an instance of Command!')
         cmd.send(self.sock)
 
 
 class MasterNode(object):
     """The MasterNode class provides an entity to run the SkyLLH program as a
     master node, where compute nodes can register to, so the master node can
     distribute work to the compute nodes. The work distribution is handled
@@ -54,18 +54,20 @@
         self.cn_registry = dict()
 
     @property
     def cn_registry(self):
         """The dictionary with the registered compute nodes.
         """
         return self._cn_registry
+
     @cn_registry.setter
     def cn_registry(self, d):
-        if(not isinstance(d, dict)):
-            raise TypeError('The cn_registry property must be of type dict!')
+        if not isinstance(d, dict):
+            raise TypeError(
+                'The cn_registry property must be of type dict!')
         self._cn_registry = d
 
     def clear_cn_registry(self):
         # Close the sockets to all the CNs.
         for (cn_key, cn) in self.cn_registry.items():
             cn.sock.close()
 
@@ -96,17 +98,18 @@
                 # them.
                 (clientsock, (addr, port)) = serversock.accept()
                 logger.debug(
                     'Got inbound connection from %s:%d', addr, port)
 
                 cmd = receive_command_from_socket(
                     clientsock, blocksize=blocksize)
-                if(not cmd.is_same_as(RegisterCN)):
-                    raise RuntimeError('The compute node provided an unknown '
-                        'command "%s"!'%(cmd.as_message().msg))
+                if not cmd.is_same_as(RegisterCN):
+                    raise RuntimeError(
+                        'The compute node provided an unknown command '
+                        f'"{cmd.as_message().msg}"!')
                 ACK().send(clientsock)
 
                 cn = CNRegistryEntry(
                     clientsock, addr, port, cmd.cn_start_time, cmd.cn_live_time)
                 self._cn_registry[cn.key] = cn
         finally:
             serversock.close()
@@ -116,8 +119,7 @@
             cn.send_command(MSG(msg))
 
     def shutdown_compute_nodes(self):
         """Sends a stop command to all compute nodes.
         """
         for (cn_key, cn) in self._cn_registry.items():
             cn.send_command(ShutdownCN())
-
```

### Comparing `skyllh-23.1.1/skyllh/cluster/srvclt.py` & `skyllh-23.2.0/skyllh/cluster/srvclt.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,15 @@
 # -*- coding: utf-8 -*-
 
 import pickle
 
-from skyllh.core.py import str_cast
+from skyllh.core.py import (
+    str_cast,
+)
+
 
 class Message(object):
     @staticmethod
     def receive(sock, blocksize=2048, as_bytes=False):
         """Receives a message from the given socket.
 
         Parameters
@@ -27,15 +30,15 @@
         msglen = int.from_bytes(
             read_from_socket(sock, 2, blocksize=blocksize), 'little')
 
         # Read the message of length msglen bytes from the socket. Here, msg is
         # a bytes object.
         msg = read_from_socket(sock, msglen, blocksize=blocksize)
 
-        if(as_bytes):
+        if as_bytes:
             return Message(msg)
 
         return Message(str(msg, 'utf-8'))
 
     def __init__(self, msg):
         """Creates a new Message instance.
 
@@ -48,18 +51,20 @@
 
     @property
     def msg(self):
         """The message string. This is either a bytes instance or a str
         instance.
         """
         return self._msg
+
     @msg.setter
     def msg(self, m):
-        if(not isinstance(m, bytes)):
-            m = str_cast(m,
+        if not isinstance(m, bytes):
+            m = str_cast(
+                m,
                 'The msg property must be of type bytes or castable to type '
                 'str!')
         self._msg = m
 
     @property
     def length(self):
         """The length of the message in bytes.
@@ -67,15 +72,15 @@
         return len(self.msg)
 
     def as_socket_msg(self):
         """Converts this message to a bytes instance that can be send through a
         socket. The first two bytes hold the length of the message.
         """
         smsg = len(self.msg).to_bytes(2, 'little')
-        if(isinstance(self.msg, bytes)):
+        if isinstance(self.msg, bytes):
             smsg += self.msg
         else:
             smsg += bytes(self.msg, 'utf-8')
 
         return smsg
 
     def send(self, sock):
@@ -83,31 +88,33 @@
 
 
 def send_to_socket(sock, msg):
     msglen = len(msg)
     n_bytes_sent = 0
     while n_bytes_sent < msglen:
         sent = sock.send(msg[n_bytes_sent:])
-        if(sent == 0):
+        if sent == 0:
             raise RuntimeError('Socket connection broken!')
         n_bytes_sent += sent
 
+
 def read_from_socket(sock, size, blocksize=2048):
     """Reads ``size`` bytes from the socket ``sock``.
     """
     chunks = []
     n_bytes_recd = 0
     while (n_bytes_recd < size):
         chunk = sock.recv(min(size - n_bytes_recd, blocksize))
-        if(chunk == b''):
+        if chunk == b'':
             raise RuntimeError('Socket connection broken!')
         chunks.append(chunk)
         n_bytes_recd += len(chunk)
     return b''.join(chunks)
 
+
 def receive_object_from_socket(sock, blocksize=2048):
     """Receives a pickled Python object from the given socket.
 
     Parameters
     ----------
     sock : socket
     """
```

### Comparing `skyllh-23.1.1/skyllh/core/analysis.py` & `skyllh-23.2.0/skyllh/core/analysis.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,398 +1,597 @@
 # -*- coding: utf-8 -*-
 
 """The analysis module provides classes for pre-defined analyses.
 """
 
 import abc
+from astropy import units
 import numpy as np
 
-from skyllh.core.py import (
-    classname,
-    issequenceof
+
+from skyllh.core.background_generator import (
+    BackgroundGenerator,
+    MultiDatasetBackgroundGenerator,
+)
+from skyllh.core.config import (
+    HasConfig,
 )
-from skyllh.core.debugging import get_logger
-from skyllh.core.storage import DataFieldRecordArray
 from skyllh.core.dataset import (
     Dataset,
     DatasetData,
 )
-from skyllh.core.parameters import (
-    FitParameter,
-    SourceFitParameterMapper,
-    SingleSourceFitParameterMapper,
+from skyllh.core.debugging import (
+    get_logger,
+)
+from skyllh.core.event_selection import (
+    EventSelectionMethod,
 )
-from skyllh.core.pdfratio import PDFRatio
-from skyllh.core.progressbar import ProgressBar
-from skyllh.core.random import RandomStateService
 from skyllh.core.llhratio import (
     LLHRatio,
     MultiDatasetTCLLHRatio,
-    SingleSourceDatasetSignalWeights,
-    SingleSourceZeroSigH0SingleDatasetTCLLHRatio,
-    MultiSourceZeroSigH0SingleDatasetTCLLHRatio,
-    MultiSourceDatasetSignalWeights,
+    ZeroSigH0SingleDatasetTCLLHRatio,
+)
+from skyllh.core.multiproc import (
+    get_ncpu,
+    parallelize,
+)
+from skyllh.core.parameters import (
+    ParameterModelMapper,
+)
+from skyllh.core.pdfratio import (
+    PDFRatio,
+    SourceWeightedPDFRatio,
+)
+from skyllh.core.py import (
+    classname,
+    issequenceof,
+)
+from skyllh.core.random import (
+    RandomStateService,
+)
+from skyllh.core.services import (
+    DatasetSignalWeightFactorsService,
+    DetSigYieldService,
+    SrcDetSigYieldWeightsService,
 )
-from skyllh.core.timing import TaskTimer
-from skyllh.core.trialdata import TrialDataManager
-from skyllh.core.optimize import EventSelectionMethod
-from skyllh.core.source_hypothesis import SourceHypoGroupManager
-from skyllh.core.test_statistic import TestStatistic
-from skyllh.core.multiproc import get_ncpu, parallelize
-from skyllh.core.background_generation import BackgroundGenerationMethod
-from skyllh.core.background_generator import BackgroundGenerator
 from skyllh.core.signal_generator import (
-    SignalGeneratorBase,
     SignalGenerator,
+    MultiDatasetSignalGenerator,
+)
+from skyllh.core.source_hypo_grouping import (
+    SourceHypoGroupManager,
+)
+from skyllh.core.source_model import (
+    SourceModel,
+)
+from skyllh.core.storage import (
+    DataFieldRecordArray,
+)
+from skyllh.core.test_statistic import (
+    TestStatistic,
+)
+from skyllh.core.timing import (
+    TaskTimer,
+)
+from skyllh.core.trialdata import (
+    TrialDataManager,
 )
-from skyllh.physics.source import SourceModel
 
 
 logger = get_logger(__name__)
 
 
-class Analysis(object, metaclass=abc.ABCMeta):
-    """This is the abstract base class for all analysis classes. It contains
-    common properties required by all analyses and defines the overall analysis
-    interface howto set-up and run an analysis.
-
-    Note: This analysis base class assumes the analysis to be a log-likelihood
-          ratio test, i.e. requires a mathematical log-likelihood ratio
-          function.
-
-    To set-up and run an analysis the following procedure applies:
-
-        1. Create an analysis instance.
-        2. Add the datasets and their PDF ratio instances via the
-           :meth:`.add_dataset` method.
-        3. Construct the log-likelihood ratio function via the
-           :meth:`.construct_llhratio` method.
-        4. Call the :meth:`do_trial` or :meth:`unblind` method to perform a
-           random trial or to unblind the data. Both methods will fit the global
-           fit parameters using the set up data. Finally, the test statistic
-           is calculated via the :meth:`.calculate_test_statistic` method.
-
-    In order to calculate sensitivities and discovery potentials, analysis
-    trials have to be performed on random data samples with injected signal
-    events. To perform a trial with injected signal events, the signal generator
-    has to be constructed via the ``construct_signal_generator`` method before
-    any random trial data can be generated.
+class Analysis(
+        HasConfig,
+        metaclass=abc.ABCMeta,
+):
+    """This is the abstract base class for all analysis classes.
+    It contains common properties required by all analyses and defines the
+    overall analysis interface how to setup and run an analysis.
     """
-
-    def __init__(self, src_hypo_group_manager, src_fitparam_mapper,
-                 test_statistic, bkg_gen_method=None, sig_generator_cls=None):
+    def __init__(
+            self,
+            shg_mgr,
+            pmm,
+            test_statistic,
+            bkg_generator_cls=None,
+            sig_generator_cls=None,
+            **kwargs):
         """Constructor of the analysis base class.
 
         Parameters
         ----------
-        src_hypo_group_manager : instance of SourceHypoGroupManager
+        shg_mgr : instance of SourceHypoGroupManager
             The instance of SourceHypoGroupManager, which defines the groups of
             source hypotheses, their flux model, and their detector signal
-            efficiency implementation method.
-        src_fitparam_mapper : instance of SourceFitParameterMapper
-            The SourceFitParameterMapper instance managing the global fit
-            parameters and their relation to the individual sources.
+            yield implementation method.
+        pmm : instance of ParameterModelMapper
+            The ParameterModelMapper instance managing the global set of
+            parameters and their relation to individual models, e.g. sources.
         test_statistic : TestStatistic instance
             The TestStatistic instance that defines the test statistic function
             of the analysis.
-        bkg_gen_method : instance of BackgroundGenerationMethod | None
-            The instance of BackgroundGenerationMethod that should be used to
-            generate background events for pseudo data. This can be set to None,
-            if there is no need to generate background events.
-        sig_generator_cls : SignalGeneratorBase class | None
+        bkg_generator_cls : class of MultiDatasetBackgroundGenerator | None
+            The background generator class used to create the background
+            generator instance for multiple datasets.
+            If set to ``None``, the
+            :class:`skyllh.core.background_generator.MultiDatasetBackgroundGenerator`
+            class is used.
+        sig_generator_cls : class of MultiDatasetSignalGenerator | None
             The signal generator class used to create the signal generator
-            instance.
-            If set to None, the `SignalGenerator` class is used.
+            instance for multiple datasets.
+            If set to ``None``, the
+            :class:`~skyllh.core.signal_generator.MultiDatasetSignalGenerator`
+            class is used.
         """
-        # Call the super function to allow for multiple class inheritance.
-        super(Analysis, self).__init__()
+        super().__init__(
+            **kwargs)
 
-        self.src_hypo_group_manager = src_hypo_group_manager
-        self.src_fitparam_mapper = src_fitparam_mapper
+        self.shg_mgr = shg_mgr
+        self.pmm = pmm
         self.test_statistic = test_statistic
-        self.bkg_gen_method = bkg_gen_method
+        self.bkg_generator_cls = bkg_generator_cls
         self.sig_generator_cls = sig_generator_cls
 
         self._dataset_list = []
         self._data_list = []
         self._tdm_list = []
         self._event_selection_method_list = []
 
-        # Predefine the variable for the global fit parameter set, which holds
-        # all the global fit parameters.
-        self._fitparamset = None
-
-        # Predefine the variable for the log-likelihood ratio function.
-        self._llhratio = None
+        self._detsigyield_service = None
+        self._src_detsigyield_weights_service = None
+        self._ds_sig_weight_factors_service = None
 
-        # Predefine the variable for the background and signal generators.
+        self._bkg_generator_list = []
         self._bkg_generator = None
+        self._sig_generator_list = []
         self._sig_generator = None
 
     @property
-    def src_hypo_group_manager(self):
+    def shg_mgr(self):
         """The SourceHypoGroupManager instance, which defines the groups of
         source hypothesis, their flux model, and their detector signal
-        efficiency implementation method.
+        yield implementation method.
         """
-        return self._src_hypo_group_manager
-    @src_hypo_group_manager.setter
-    def src_hypo_group_manager(self, manager):
-        if(not isinstance(manager, SourceHypoGroupManager)):
-            raise TypeError('The src_hypo_group_manager property must be an '
-                'instance of SourceHypoGroupManager!')
-        self._src_hypo_group_manager = manager
+        return self._shg_mgr
+
+    @shg_mgr.setter
+    def shg_mgr(self, mgr):
+        if not isinstance(mgr, SourceHypoGroupManager):
+            raise TypeError(
+                'The shg_mgr property must be an instance of '
+                'SourceHypoGroupManager! '
+                f'Its current type is {classname(mgr)}.')
+        self._shg_mgr = mgr
 
     @property
-    def src_fitparam_mapper(self):
-        """The SourceFitParameterMapper instance that manages the global fit
-        parameters and their relation to the sources.
-        """
-        return self._src_fitparam_mapper
-    @src_fitparam_mapper.setter
-    def src_fitparam_mapper(self, mapper):
-        if(not isinstance(mapper, SourceFitParameterMapper)):
-            raise TypeError('The src_fitparam_mapper property must be an '
-                'instance of SourceFitParameterMapper!')
-        self._src_fitparam_mapper = mapper
+    def pmm(self):
+        """The ParameterModelMapper instance that manages the global set of
+        parameters and their relation to individual models, e.g. sources.
+        """
+        return self._pmm
+
+    @pmm.setter
+    def pmm(self, mapper):
+        if not isinstance(mapper, ParameterModelMapper):
+            raise TypeError(
+                'The pmm property must be an instance of '
+                'ParameterModelMapper! '
+                f'Its current type is {classname(mapper)}.')
+        self._pmm = mapper
 
     @property
     def test_statistic(self):
         """The TestStatistic instance that defines the test-statistic function
         of the analysis.
         """
         return self._test_statistic
+
     @test_statistic.setter
     def test_statistic(self, ts):
-        if(not isinstance(ts, TestStatistic)):
-            raise TypeError('The test_statistic property must be an instance '
-                'of TestStatistic, but is %s!'%(classname(ts)))
+        if not isinstance(ts, TestStatistic):
+            raise TypeError(
+                'The test_statistic property must be an instance of '
+                'TestStatistic! '
+                f'Its current type is {classname(ts)}.')
         self._test_statistic = ts
 
     @property
-    def bkg_gen_method(self):
-        """The BackgroundGenerationMethod instance that implements the
-        background event generation. This can be None if no background
-        generation method has been defined.
-        """
-        return self._bkg_gen_method
-    @bkg_gen_method.setter
-    def bkg_gen_method(self, method):
-        if(method is not None):
-            if(not isinstance(method, BackgroundGenerationMethod)):
-                raise TypeError('The bkg_gen_method property must be an '
-                    'instance of BackgroundGenerationMethod!')
-        self._bkg_gen_method = method
+    def bkg_generator_list(self):
+        """(read-only) The list of instance of BackgroundGenerator, one for each
+        dataset.
+        """
+        return self._bkg_generator_list
+
+    @property
+    def bkg_generator_cls(self):
+        """The background generator class that should be used to construct the
+        background generator instance.
+        """
+        return self._bkg_generator_cls
+
+    @bkg_generator_cls.setter
+    def bkg_generator_cls(self, cls):
+        if cls is None:
+            cls = MultiDatasetBackgroundGenerator
+        if not issubclass(cls, BackgroundGenerator):
+            raise TypeError(
+                'The bkg_generator_cls property must be a subclass of '
+                'BackgroundGenerator! '
+                f'Its current type is {classname(cls)}.')
+        self._bkg_generator_cls = cls
 
     @property
     def dataset_list(self):
         """The list of Dataset instances.
         """
         return self._dataset_list
+
     @dataset_list.setter
     def dataset_list(self, datasets):
-        if(not issequenceof(datasets, Dataset)):
-            raise TypeError('The dataset_list property must be a sequence '
-                'of Dataset instances!')
+        if not issequenceof(datasets, Dataset):
+            raise TypeError(
+                'The dataset_list property must be a sequence of Dataset '
+                'instances! '
+                f'Its current type is {classname(datasets)}.')
         self._dataset_list = list(datasets)
 
     @property
     def data_list(self):
         """The list of DatasetData instances holding the original data of the
         dataset.
         """
         return self._data_list
+
     @data_list.setter
     def data_list(self, datas):
-        if(not issequenceof(datas, DatasetData)):
-            raise TypeError('The data_list property must be a sequence '
-                'of DatasetData instances!')
+        if not issequenceof(datas, DatasetData):
+            raise TypeError(
+                'The data_list property must be a sequence of DatasetData '
+                'instances! '
+                f'Its current type is {classname(datas)}.')
         self._data_list = list(datas)
 
     @property
     def n_datasets(self):
         """(read-only) The number of datasets used in this analysis.
         """
         return len(self._dataset_list)
 
     @property
-    def fitparamset(self):
-        """(read-only) The instance of FitParameterSet holding all the global
-        fit parameters of the log-likelihood ratio function.
+    def bkg_generator(self):
+        """(read-only) The background generator instance. Is None of the
+        background generator has not been constructed via the
+        `construct_background_generator` method.
         """
-        return self._fitparamset
+        return self._bkg_generator
 
     @property
-    def llhratio(self):
-        """The log-likelihood ratio function instance. It is None, if it has
-        not been constructed yet.
+    def detsigyield_service(self):
+        """The instance of DetSigYieldService for the analysis.
         """
-        if(self._llhratio is None):
-            raise RuntimeError('The log-likelihood ratio function is not '
-                'defined yet. Call the construct_analysis method first!')
-        return self._llhratio
-    @llhratio.setter
-    def llhratio(self, obj):
-        if(not isinstance(obj, LLHRatio)):
-            raise TypeError('The llhratio property must be an instance of '
-                'LLHRatio!')
-        self._llhratio = obj
+        return self._detsigyield_service
+
+    @detsigyield_service.setter
+    def detsigyield_service(self, service):
+        if not isinstance(service, DetSigYieldService):
+            raise TypeError(
+                'The detsigyield_service property must be an instance of '
+                'DetSigYieldService! '
+                f'Its current type is {classname(service)}!')
+        self._detsigyield_service = service
 
     @property
-    def bkg_generator(self):
-        """(read-only) The background generator instance. Is None of the
-        background generator has not been constructed via the
-        `construct_background_generator` method.
+    def src_detsigyield_weights_service(self):
+        """The instance of SrcDetSigYieldWeightsService for the analysis.
         """
-        return self._bkg_generator
+        return self._src_detsigyield_weights_service
+
+    @src_detsigyield_weights_service.setter
+    def src_detsigyield_weights_service(self, service):
+        if not isinstance(service, SrcDetSigYieldWeightsService):
+            raise TypeError(
+                'The src_detsigyield_weights_service property must be an '
+                'instance of SrcDetSigYieldWeightsService! '
+                f'Its current type is {classname(service)}!')
+        self._src_detsigyield_weights_service = service
+
+    @property
+    def ds_sig_weight_factors_service(self):
+        """The instance of DatasetSignalWeightFactorsService for the analysis.
+        """
+        return self._ds_sig_weight_factors_service
+
+    @ds_sig_weight_factors_service.setter
+    def ds_sig_weight_factors_service(self, service):
+        if not isinstance(service, DatasetSignalWeightFactorsService):
+            raise TypeError(
+                'The ds_sig_weight_factors_service property must be an '
+                'instance of DatasetSignalWeightFactorsService! '
+                f'Its current type is {classname(service)}!')
+        self._ds_sig_weight_factors_service = service
+
+    @property
+    def sig_generator_list(self):
+        """(read-only) The list of instance of SignalGenerator, one for each
+        dataset.
+        """
+        return self._sig_generator_list
 
     @property
     def sig_generator_cls(self):
         """The signal generator class that should be used to construct the
-        signal generator instance.
+        signal generator instance handling all datasets.
         """
         return self._sig_generator_cls
+
     @sig_generator_cls.setter
     def sig_generator_cls(self, cls):
         if cls is None:
-            cls = SignalGenerator
-        if not issubclass(cls, SignalGeneratorBase):
+            cls = MultiDatasetSignalGenerator
+        if not issubclass(cls, SignalGenerator):
             raise TypeError(
-                'The sig_generator_cls property must be an subclass of '
-                'SignalGeneratorBase!')
+                'The sig_generator_cls property must be a subclass of '
+                'SignalGenerator! '
+                f'Its current type is {classname(cls)}.')
         self._sig_generator_cls = cls
 
     @property
     def sig_generator(self):
         """(read-only) The signal generator instance. Is None if the signal
         generator has not been constructed via the
         `construct_signal_generator` method.
         """
         return self._sig_generator
 
     @property
+    def tdm_list(self):
+        """The list of instance of TrialDataManager. One for each dataset.
+        """
+        return self._tdm_list
+
+    @tdm_list.setter
+    def tdm_list(self, tdms):
+        if not issequenceof(tdms, TrialDataManager):
+            raise TypeError(
+                'The tdm_list property must be a sequence of TrialDataManager '
+                'instances! '
+                f'Its current type is {classname(tdms)}.')
+        self._tdm_list = list(tdms)
+
+    @property
     def total_livetime(self):
         """(read-only) The total live-time in days of the loaded data.
         """
         livetime = 0
         for data in self._data_list:
             livetime += data.livetime
         return livetime
 
-    def add_dataset(self, dataset, data, tdm=None, event_selection_method=None):
+    def construct_services(
+            self,
+            ppbar=None,
+    ):
+        """Constructs the following services:
+
+            - detector signal yield service
+            - source detector signal yield weights service
+            - dataset signal weight factors service
+
+        Parameters
+        ----------
+        ppbar : instance of ProgressBar | None
+            The instance of ProgressBar of the optional parent progress bar.
+        """
+        self.detsigyield_service = DetSigYieldService(
+            shg_mgr=self._shg_mgr,
+            dataset_list=self._dataset_list,
+            data_list=self._data_list,
+            ppbar=ppbar,
+        )
+
+        self.src_detsigyield_weights_service = SrcDetSigYieldWeightsService(
+            detsigyield_service=self.detsigyield_service,
+        )
+
+        self.ds_sig_weight_factors_service = DatasetSignalWeightFactorsService(
+            src_detsigyield_weights_service=self.src_detsigyield_weights_service,
+        )
+
+    def add_dataset(  # noqa: C901
+            self,
+            dataset,
+            data,
+            tdm=None,
+            event_selection_method=None,
+            bkg_generator=None,
+            sig_generator=None,
+    ):
         """Adds the given dataset to the list of datasets for this analysis.
 
         Parameters
         ----------
-        dataset : Dataset instance
+        dataset : instance of Dataset
             The Dataset instance that should get added.
-        data : DatasetData instance
+        data : instance of DatasetData
             The DatasetData instance holding the original (prepared) data of the
             dataset.
-        tdm : TrialDataManager instance | None
+        tdm : instance of TrialDataManager | None
             The TrialDataManager instance managing the trial data and additional
             data fields of the data set. If set to None, it means that no
             additional data fields are defined.
         event_selection_method : instance of EventSelectionMethod | None
             The instance of EventSelectionMethod to use to select only
             signal-like events from the data. All other events
             will be treated as pure background events. This reduces the amount
             of log-likelihood-ratio function evaluations. If set to None, all
             events will be evaluated.
+        bkg_generator : instance of BackgroundGenerator | None
+            The optional instance of BackgroundGenerator, which should be used
+            to generate background events for this particular dataset.
+        sig_generator : instance of SignalGenerator | None
+            The optional instance of SignalGenerator, which should be used
+            to generate signal events for this particular dataset.
         """
-        if(not isinstance(dataset, Dataset)):
+        if not isinstance(dataset, Dataset):
             raise TypeError(
                 'The dataset argument must be an instance of Dataset!')
 
-        if(not isinstance(data, DatasetData)):
+        if not isinstance(data, DatasetData):
             raise TypeError(
                 'The data argument must be an instance of DatasetData!')
 
-        if(tdm is None):
+        if tdm is None:
             tdm = TrialDataManager()
-        if(not isinstance(tdm, TrialDataManager)):
+        if not isinstance(tdm, TrialDataManager):
             raise TypeError(
                 'The tdm argument must be None or an instance of '
-                'TrialDataManager!')
+                'TrialDataManager! '
+                f'Its current type is {classname(tdm)}!')
 
-        if(event_selection_method is not None):
-            if(not isinstance(event_selection_method, EventSelectionMethod)):
+        if event_selection_method is not None:
+            if not isinstance(event_selection_method, EventSelectionMethod):
                 raise TypeError(
                     'The event_selection_method argument must be None or an '
-                    'instance of EventSelectionMethod!')
+                    'instance of EventSelectionMethod! '
+                    f'Its current type is {classname(event_selection_method)}!')
+
+        if bkg_generator is not None:
+            if not isinstance(bkg_generator, BackgroundGenerator):
+                raise TypeError(
+                    'The bkg_generator argument must be None or an instance of'
+                    'BackgroundGenerator! '
+                    f'Its current type is {classname(bkg_generator)}!')
+
+        if sig_generator is not None:
+            if not isinstance(sig_generator, SignalGenerator):
+                raise TypeError(
+                    'The sig_generator argument must be None or an instance of '
+                    'SignalGenerator! '
+                    f'Its current type is {classname(sig_generator)}!')
 
         self._dataset_list.append(dataset)
         self._data_list.append(data)
         self._tdm_list.append(tdm)
         self._event_selection_method_list.append(event_selection_method)
+        self._bkg_generator_list.append(bkg_generator)
+        self._sig_generator_list.append(sig_generator)
+
+    def get_livetime(
+            self,
+            dataset_key=None,
+            unit=None):
+        """Retrieves the numeric livetime of the given dataset in the specified
+        unit. The dataset can be specified either through its index or its name.
+        If no dataset is specified, the total livetime, i.e. the sum of the
+        livetime of all datasets, is returned.
+
+        Parameters
+        ----------
+        dataset_key : int | str | None
+            The index or name of the dataset for which the livetime should get
+            retrieved. If set to ``None``, the total livetime of all datasets
+            will be returned.
+        unit : instance of astropy.units.Unit | None
+            The time unit in which the livetime should be returned. If set to
+            ``None``, ``astropy.units.day`` will be used.
+        """
+        if dataset_key is None:
+            livetime = np.sum([data.livetime for data in self._data_list])
+        else:
+            if isinstance(dataset_key, int):
+                dataset_idx = dataset_key
+            elif isinstance(dataset_key, str):
+                dataset_idx = None
+                for (idx, ds) in enumerate(self._dataset_list):
+                    if ds.name == dataset_key:
+                        dataset_idx = idx
+                        break
+                if dataset_idx is None:
+                    raise KeyError(
+                        f'The dataset of name "{dataset_key}" does not exist!')
+            else:
+                raise TypeError(
+                    'The dataset_key argument must be an instance of int, str, '
+                    'or None! '
+                    f'Its current type is {classname(dataset_key)}.')
+            livetime = self._data_list[dataset_idx].livetime
+
+        if isinstance(unit, units.Unit):
+            livetime *= units.day.to(unit)
+
+        return livetime
 
     def calculate_test_statistic(
-            self, log_lambda, fitparam_values, *args, **kwargs):
+            self,
+            log_lambda,
+            fitparam_values,
+            **kwargs):
         """Calculates the test statistic value by calling the ``evaluate``
         method of the TestStatistic class with the given log_lambda value and
         fit parameter values.
 
         Parameters
         ----------
         log_lambda : float
             The value of the log-likelihood ratio function. Usually, this is its
             maximum.
-        fitparam_values : (N_fitparam+1)-shaped 1D ndarray
-            The 1D ndarray holding the fit parameter values of the
-            log-likelihood ratio function for the given log_lambda value.
-
-        Additional arguments and keyword arguments
-        ------------------------------------------
-        Any additional arguments and keyword arguments are passed to the
-        evaluate method of the TestStatistic class instance.
+        fitparam_values : instance of numpy ndarray
+            The (N_fitparam,)-shaped 1D ndarray holding the global
+            fit parameter values of the log-likelihood ratio function for
+            the given log_lambda value.
+        **kwargs
+            Any additional keyword arguments are passed to the
+            ``__call__`` method of the TestStatistic instance.
 
         Returns
         -------
         TS : float
             The calculated test-statistic value.
         """
-        return self._test_statistic.evaluate(
-            self._llhratio, log_lambda, fitparam_values, *args, **kwargs)
-
-    @abc.abstractmethod
-    def construct_llhratio(self):
-        """This method is supposed to construct the log-likelihood ratio
-        function and sets it as the _llhratio property.
-        """
-        pass
+        return self._test_statistic(
+            pmm=self._pmm,
+            log_lambda=log_lambda,
+            fitparam_values=fitparam_values,
+            **kwargs)
 
     def construct_background_generator(self, **kwargs):
         """Constructs the background generator for all added datasets.
         This method must be called after all the datasets were added via the
         add_dataset method. It sets the `bkg_generator` property of this
         Analysis class instance.
         """
-        if(self._bkg_gen_method is None):
-            raise RuntimeError('No background generation method has been '
-                'defined for this analysis!')
-
-        self._bkg_generator = BackgroundGenerator(
-            self._bkg_gen_method, self._dataset_list, self._data_list,
+        self._bkg_generator = self.bkg_generator_cls(
+            cfg=self._cfg,
+            dataset_list=self._dataset_list,
+            data_list=self._data_list,
+            bkg_generator_list=self._bkg_generator_list,
             **kwargs)
 
     def construct_signal_generator(self, **kwargs):
         """Constructs the signal generator for all added datasets.
         This method must be called after all the datasets were added via the
         add_dataset method. It sets the `sig_generator` property of this
         Analysis class instance. The signal generation method has to be set
         through the source hypothesis group.
         """
         self._sig_generator = self.sig_generator_cls(
-            src_hypo_group_manager=self._src_hypo_group_manager,
+            cfg=self._cfg,
+            shg_mgr=self._shg_mgr,
             dataset_list=self._dataset_list,
             data_list=self._data_list,
+            sig_generator_list=self._sig_generator_list,
+            ds_sig_weight_factors_service=self.ds_sig_weight_factors_service,
             **kwargs)
 
     @abc.abstractmethod
-    def initialize_trial(self, events_list, n_events_list=None):
+    def initialize_trial(
+            self,
+            events_list,
+            n_events_list=None):
         """This method is supposed to initialize the log-likelihood ratio
         function with a new set of given trial data. This is a low-level method.
         For convenient methods see the `unblind` and `do_trial` methods.
 
         Parameters
         ----------
         events_list : list of numpy record ndarray
@@ -405,249 +604,420 @@
             argument in cases where an event selection method was already used.
             If set to None, the number of events is taken from the given
             `events_list` argument.
         """
         pass
 
     @abc.abstractmethod
-    def maximize_llhratio(self, rss, tl=None):
-        """This method is supposed to maximize the log-likelihood ratio
-        function, by calling the ``maximize`` method of the LLHRatio class.
+    def unblind(
+            self,
+            rss,
+            tl=None):
+        """This method is supposed to run the analysis on the experimental data,
+        i.e. unblinds the data.
 
         Parameters
         ----------
-        rss : RandomStateService instance
-            The RandomStateService instance to draw random numbers from.
-        tl : TimeLord instance | None
-            The optional TimeLord instance that should be used to time the
+        rss : instance of RandomStateService
+            The instance of RandomStateService that should be used draw random
+            numbers from. It can be used to generate random initial values for
+            fit parameters.
+        tl : instance of TimeLord | None
+            The optional instance of TimeLord that should be used to time the
             maximization of the LLH ratio function.
 
         Returns
         -------
-        fitparamset : FitParameterSet instance
-            The instance of FitParameterSet holding the global fit parameter
-            definitions used in the maximization process.
-        log_lambda_max : float
-            The value of the log-likelihood ratio function at its maximum.
-        fitparam_values : (N_fitparam,)-shaped 1D ndarray
-            The ndarray holding the global fit parameter values.
-            By definition, the first element is the value of the fit parameter
-            ns.
+        TS : float
+            The test-statistic value.
+        global_params_dict : dict
+            The dictionary holding the global parameter names and their
+            best fit values. It includes fixed and floating parameters.
         status : dict
-            The dictionary with status information about the maximization
-            process, i.e. from the minimizer.
+            The status dictionary with information about the performed
+            minimization process of the analysis.
         """
         pass
 
-    def unblind(self, rss):
-        """Evaluates the unscrambled data, i.e. unblinds the data.
+    @abc.abstractmethod
+    def do_trial_with_given_pseudo_data(
+            self,
+            rss,
+            mean_n_sig,
+            n_sig,
+            n_events_list,
+            events_list,
+            minimizer_status_dict=None,
+            tl=None,
+            **kwargs):
+        """This method is supposed to perform an analysis trial on a given
+        pseudo data.
 
         Parameters
         ----------
-        rss : RandomStateService instance
-            The RandomStateService instance that should be used draw random
-            numbers from.
+        rss : instance of RandomStateService
+            The instance of RandomStateService to use for generating random
+            numbers.
+        mean_n_sig : float
+            The mean number of signal events the pseudo data was generated with.
+        n_sig : int
+            The total number of actual signal events in the pseudo data.
+        n_events_list : list of int
+            The total number of events for each data set of the pseudo data.
+        events_list : list of instance of DataFieldRecordArray
+            The list of instance of DataFieldRecordArray containing the pseudo
+            data events for each data sample. The number of events for each
+            data sample can be less than the number of events given by
+            ``n_events_list`` if an event selection method was already utilized
+            when generating background events.
+        minimizer_status_dict : dict | None
+            If a dictionary is provided, it will be updated with the minimizer
+            status dictionary.
+        tl : instance of TimeLord | None
+            The optional instance of TimeLord that should be used to time
+            individual tasks.
 
         Returns
         -------
-        TS : float
-            The test-statistic value.
-        fitparam_dict : dict
-            The dictionary holding the global fit parameter names and their best
-            fit values.
-        status : dict
-            The status dictionary with information about the performed
-            minimization process of the negative of the log-likelihood ratio
-            function.
+        recarray : instance of numpy record ndarray
+            The numpy record ndarray holding the result of the trial. It must
+            contain the following data fields:
+
+            rss_seed : int
+                The RandomStateService seed.
+            mean_n_sig : float
+                The mean number of signal events.
+            n_sig : int
+                The actual number of injected signal events.
+            ts : float
+                The test-statistic value.
+            [<global_param_name> : float ]
+                Any additional parameters of the analysis.
         """
-        events_list = [ data.exp for data in self._data_list ]
-        self.initialize_trial(events_list)
-        (fitparamset, log_lambda_max, fitparam_values, status) = self.maximize_llhratio(rss)
-        TS = self.calculate_test_statistic(log_lambda_max, fitparam_values)
+        pass
+
+    def change_shg_mgr(
+            self,
+            shg_mgr):
+        """If the SourceHypoGroupManager instance changed, this method needs to
+        be called to propagate the change to all components of the analysis.
+
+        Parameters
+        ----------
+        shg_mgr : instance of SourceHypoGroupManager
+            The new instance of SourceHypoGroupManager.
+        """
+        for evt_selection_method in self._event_selection_method_list:
+            if evt_selection_method is not None:
+                evt_selection_method.change_shg_mgr(
+                    shg_mgr=shg_mgr)
+
+        for tdm in self._tdm_list:
+            tdm.change_shg_mgr(
+                shg_mgr=shg_mgr)
+
+        if self._detsigyield_service is not None:
+            self._detsigyield_service.change_shg_mgr(
+                shg_mgr=shg_mgr)
+
+        if self._src_detsigyield_weights_service is not None:
+            self._src_detsigyield_weights_service.change_shg_mgr(
+                shg_mgr=shg_mgr)
+
+        if self._bkg_generator is not None:
+            self._bkg_generator.change_shg_mgr(
+                shg_mgr=shg_mgr)
 
-        fitparam_dict = fitparamset.fitparam_values_to_dict(fitparam_values)
+        if self._sig_generator is not None:
+            self._sig_generator.change_shg_mgr(
+                shg_mgr=shg_mgr)
 
-        return (TS, fitparam_dict, status)
+    def do_trial_with_given_bkg_and_sig_pseudo_data(
+            self,
+            rss,
+            mean_n_sig,
+            n_sig,
+            n_bkg_events_list,
+            n_sig_events_list,
+            bkg_events_list,
+            sig_events_list,
+            minimizer_status_dict=None,
+            tl=None,
+            **kwargs):
+        """Performs an analysis trial on the given background and signal pseudo
+        data. This method merges the background and signal pseudo events and
+        calls the ``do_trial_with_given_pseudo_data`` method of this class.
+
+        Note
+        ----
+        This method alters the DataFieldRecordArray instances of the
+        bkg_events_list argument!
+
+        Parameters
+        ----------
+        rss : instance of RandomStateService
+            The instance of RandomStateService instance to use for generating
+            random numbers.
+        mean_n_sig : float
+            The mean number of signal events the pseudo data was generated with.
+        n_sig : int
+            The total number of actual signal events in the pseudo data.
+        n_bkg_events_list : list of int
+            The total number of background events for each data set of the
+            pseudo data.
+        n_sig_events_list : list of int
+            The total number of signal events for each data set of the
+            pseudo data.
+        bkg_events_list : list of instance of DataFieldRecordArray
+            The list of instance of DataFieldRecordArray containing the
+            background pseudo data events for each data set.
+        sig_events_list : list of instance of DataFieldRecordArray | None
+            The list of instance of DataFieldRecordArray containing the signal
+            pseudo data events for each data set. If a particular dataset has
+            no signal events, the entry for that dataset can be ``None``.
+        minimizer_status_dict : dict | None
+            If a dictionary is provided, it will be updated with the minimizer
+            status dictionary.
+        tl : instance of TimeLord | None
+            The instance of TimeLord that should be used to time individual
+            tasks.
+        **kwargs : dict
+            Additional keyword arguments are passed to the
+            :meth:`~skyllh.core.analysis.Analysis.do_trial_with_given_pseudo_data`
+            method.
+
+        Returns
+        -------
+        recarray : instance of numpy record ndarray
+            The numpy record ndarray holding the result of the trial.
+            See the documentation of the
+            :meth:`~skyllh.core.analysis.Analysis.do_trial_with_given_pseudo_data`
+            method for further information.
+        """
+        n_events_list = list(
+            np.array(n_bkg_events_list) +
+            np.array(n_sig_events_list)
+        )
+
+        events_list = bkg_events_list
+
+        # Add potential signal events to the background events.
+        for ds_idx in range(len(events_list)):
+            if sig_events_list[ds_idx] is not None:
+                if events_list[ds_idx] is None:
+                    events_list[ds_idx] = sig_events_list[ds_idx]
+                else:
+                    events_list[ds_idx].append(sig_events_list[ds_idx])
+
+        recarray = self.do_trial_with_given_pseudo_data(
+            rss=rss,
+            mean_n_sig=mean_n_sig,
+            n_sig=n_sig,
+            n_events_list=n_events_list,
+            events_list=events_list,
+            minimizer_status_dict=minimizer_status_dict,
+            tl=tl,
+            **kwargs)
+
+        return recarray
 
     def generate_background_events(
-            self, rss, mean_n_bkg_list=None, bkg_kwargs=None, tl=None):
+            self,
+            rss,
+            mean_n_bkg_list=None,
+            bkg_kwargs=None,
+            tl=None,
+    ):
         """Generates background events utilizing the background generator.
 
         Parameters
         ----------
-        rss : RandomStateService
-            The RandomStateService instance to use for generating random
+        rss : instance of RandomStateService
+            The instance of RandomStateService to use for generating random
             numbers.
         mean_n_bkg_list : list of float | None
             The mean number of background events that should be generated for
             each dataset. If set to None (the default), the background
             generation method needs to obtain this number itself.
+        bkg_kwargs : dict | None
+            Optional keyword arguments for the ``generate_background_events``
+            method of the background generator.
         tl : instance of TimeLord | None
-            The instance of TimeLord that should be used to time individual
-            tasks of this method.
+            The optional instance of TimeLord that should be used to time
+            individual tasks of this method.
 
         Returns
         -------
         n_events_list : list of int
             The list of the number of events that have been generated for each
             pseudo data set.
-        events_list : list of DataFieldRecordArray instances
-            The list of DataFieldRecordArray instances containing the pseudo
+        events_list : list of instance of DataFieldRecordArray
+            The list of instance of DataFieldRecordArray containing the pseudo
             data events for each data sample. The number of events for each
             data set can be less than the number of events given by
-            `n_events_list` if an event selection method was already utilized
+            ``n_events_list`` if an event selection method was already utilized
             when generating background events.
         """
-        n_datasets = self.n_datasets
-
-        if(not isinstance(rss, RandomStateService)):
-            raise TypeError(
-                'The rss argument must be an instance of RandomStateService!')
-
-        if(mean_n_bkg_list is None):
-            mean_n_bkg_list = [ None ] * n_datasets
-        if(not issequenceof(mean_n_bkg_list, (type(None), float))):
-            raise TypeError(
-                'The mean_n_bkg_list argument must be a sequence of None '
-                'and/or floats!')
-
-        if(bkg_kwargs is None):
+        if bkg_kwargs is None:
             bkg_kwargs = dict()
 
-        # Construct the background event generator in case it's not constructed
-        # yet.
-        if(self._bkg_generator is None):
+        if self._bkg_generator is None:
             self.construct_background_generator()
 
-        n_events_list = []
-        events_list = []
-        for ds_idx in range(n_datasets):
-            bkg_kwargs.update(mean=mean_n_bkg_list[ds_idx])
-            with TaskTimer(tl, 'Generating background events for data set '
-                '{:d}.'.format(ds_idx)):
-                (n_bkg, bkg_events) = self._bkg_generator.generate_background_events(
-                    rss, ds_idx, tl=tl, **bkg_kwargs)
-            n_events_list.append(n_bkg)
-            events_list.append(bkg_events)
+        (n_events_list, events_list) =\
+            self._bkg_generator.generate_background_events(
+                rss=rss,
+                mean_n_bkg_list=mean_n_bkg_list,
+                tl=tl,
+                **bkg_kwargs,
+            )
 
         return (n_events_list, events_list)
 
+    def _assert_input_arguments_of_generate_signal_events(
+            self,
+            rss,
+            n_events_list,
+            events_list):
+        """Checks the input arguments of the ``generate_signal_events`` method
+        for correct type and value.
+        """
+        n_datasets = self.n_datasets
+
+        if not isinstance(rss, RandomStateService):
+            raise TypeError(
+                'The rss argument must be an instance of RandomStateService! '
+                f'Its current type is {classname(rss)}.')
+
+        if not issequenceof(n_events_list, int):
+            raise TypeError(
+                'The n_events_list argument must be a sequence of '
+                'instances of type int! '
+                f'Its current type is {classname(n_events_list)}.')
+        if len(n_events_list) != n_datasets:
+            raise ValueError(
+                'The n_events_list argument must be a list of int of '
+                f'length {n_datasets}! Currently it is of length '
+                f'{len(n_events_list)}.')
+
+        if not issequenceof(events_list, (type(None), DataFieldRecordArray)):
+            raise TypeError(
+                'The events_list argument must be a sequence of '
+                'instances of type DataFieldRecordArray! '
+                f'Its current type is {classname(events_list)}.')
+        if len(events_list) != n_datasets:
+            raise ValueError(
+                'The events_list argument must be a list of instances of '
+                f'type DataFieldRecordArray with a length of {n_datasets}! '
+                f'Currently it is of length {len(events_list)}.')
+
     def generate_signal_events(
-            self, rss, mean_n_sig, sig_kwargs=None, n_events_list=None,
-            events_list=None, tl=None):
+            self,
+            rss,
+            mean_n_sig,
+            sig_kwargs=None,
+            n_events_list=None,
+            events_list=None,
+            tl=None):
         """Generates signal events utilizing the signal generator.
 
         Parameters
         ----------
-        rss : RandomStateService
-            The RandomStateService instance to use for generating random
+        rss : instance of RandomStateService
+            The instance of RandomStateService to use for generating random
             numbers.
         mean_n_sig : float
             The mean number of signal events that should be generated for the
             trial. The actual number of generated events will be drawn from a
             Poisson distribution with this given signal mean as mean.
         sig_kwargs : dict | None
-            Additional keyword arguments for the `generate_signal_events` method
-            of the `sig_generator_cls` class. An usual keyword argument is
-            `poisson`.
+            Additional keyword arguments for the ``generate_signal_events``
+            method of the ``sig_generator_cls`` class. An usual keyword argument
+            is ``poisson``.
         n_events_list : list of int | None
             If given, it specifies the number of events of each data set already
             present and the number of signal events will be added.
-        events_list : list of DataFieldRecordArray instances | None
+        events_list : list of instance of DataFieldRecordArray | None
             If given, it specifies the events of each data set already present
             and the signal events will be added.
         tl : instance of TimeLord | None
             The instance of TimeLord that should be used to time individual
             tasks of this method.
 
         Returns
         -------
         n_sig : int
             The actual number of injected signal events.
         n_events_list : list of int
-            The list of the number of events that have been generated for each
-            pseudo data set.
-        events_list : list of DataFieldRecordArray instances
-            The list of DataFieldRecordArray instances containing the pseudo
+            The list of the number of signal events that have been generated for
+            each data set.
+        events_list : list of instance of DataFieldRecordArray
+            The list of instance of DataFieldRecordArray containing the
             signal data events for each data set. An entry is None, if no signal
             events were generated for this particular data set.
         """
-        n_datasets = self.n_datasets
-
-        if(not isinstance(rss, RandomStateService)):
-            raise TypeError(
-                'The rss argument must be an instance of RandomStateService!')
-
-        if(sig_kwargs is None):
+        if sig_kwargs is None:
             sig_kwargs = dict()
 
-        if(n_events_list is None):
-            n_events_list = [0] * n_datasets
-        else:
-            if(not issequenceof(n_events_list, int)):
-                raise TypeError(
-                    'The n_events_list argument must be a sequence of '
-                    'instances of type int!')
-            if(len(n_events_list) != n_datasets):
-                raise ValueError(
-                    'The n_events_list argument must be a list of int of '
-                    'length {:d}! Currently it is of length {:d}.'.format(
-                        n_datasets, len(n_events_list)))
+        if n_events_list is None:
+            n_events_list = [0] * self.n_datasets
 
-        if(events_list is None):
-            events_list = [None] * n_datasets
-        else:
-            if(not issequenceof(
-                    events_list, (type(None), DataFieldRecordArray))):
-                raise TypeError(
-                    'The events_list argument must be a sequence of '
-                    'instances of type DataFieldRecordArray!')
-            if(len(events_list) != n_datasets):
-                raise ValueError(
-                    'The events_list argument must be a list of instances of '
-                    'type DataFieldRecordArray with a length of {:d}! '
-                    'Currently it is of length {:d}.'.format(
-                        n_datasets, len(events_list)))
+        if events_list is None:
+            events_list = [None] * self.n_datasets
+
+        self._assert_input_arguments_of_generate_signal_events(
+            rss=rss,
+            n_events_list=n_events_list,
+            events_list=events_list)
 
         n_sig = 0
 
-        if(mean_n_sig == 0):
+        if mean_n_sig == 0:
             return (n_sig, n_events_list, events_list)
 
         # Construct the signal generator if not done yet.
-        if(self._sig_generator is None):
+        if self._sig_generator is None:
             with TaskTimer(tl, 'Constructing signal generator.'):
                 self.construct_signal_generator()
+
         # Generate signal events with the given mean number of signal
         # events.
         sig_kwargs.update(mean=mean_n_sig)
         with TaskTimer(tl, 'Generating signal events.'):
-            (n_sig, ds_sig_events_dict) = self._sig_generator.generate_signal_events(
-                rss, **sig_kwargs)
+            (n_sig, ds_sig_events_dict) =\
+                self._sig_generator.generate_signal_events(
+                    rss=rss,
+                    **sig_kwargs)
+
         # Inject the signal events to the generated background data.
         for (ds_idx, sig_events) in ds_sig_events_dict.items():
             n_events_list[ds_idx] += len(sig_events)
-            if(events_list[ds_idx] is None):
+            if events_list[ds_idx] is None:
                 events_list[ds_idx] = sig_events
             else:
                 events_list[ds_idx].append(sig_events)
 
         return (n_sig, n_events_list, events_list)
 
     def generate_pseudo_data(
-            self, rss, mean_n_bkg_list=None, mean_n_sig=0, bkg_kwargs=None,
-            sig_kwargs=None, tl=None):
+            self,
+            rss,
+            mean_n_bkg_list=None,
+            mean_n_sig=0,
+            bkg_kwargs=None,
+            sig_kwargs=None,
+            tl=None):
         """Generates pseudo data with background and possible signal
         events for each data set using the background and signal generation
         methods of the analysis.
 
         Parameters
         ----------
-        rss : RandomStateService
-            The RandomStateService instance to use for generating random
+        rss : instance of RandomStateService
+            The instance of RandomStateService to use for generating random
             numbers.
         mean_n_bkg_list : list of float | None
             The mean number of background events that should be generated for
             each dataset. If set to None (the default), the background
             generation method needs to obtain this number itself.
         mean_n_sig : float
             The mean number of signal events that should be generated for the
@@ -658,25 +1028,25 @@
             background generation method class. An usual keyword argument is
             `poisson`.
         sig_kwargs : dict | None
             Additional keyword arguments for the `generate_signal_events` method
             of the `SignalGenerator` class. An usual keyword argument is
             `poisson`.
         tl : instance of TimeLord | None
-            The instance of TimeLord that should be used to time individual
-            tasks of this method.
+            The optional instance of TimeLord that should be used to time
+            individual tasks of this method.
 
         Returns
         -------
         n_sig : int
             The actual number of injected signal events.
         n_events_list : list of int
             The list of the number of events that have been generated for each
             pseudo data set.
-        events_list : list of DataFieldRecordArray instances
+        events_list : list of instance of DataFieldRecordArray
             The list of DataFieldRecordArray instances containing the pseudo
             data events for each data sample. The number of events for each
             data set can be less than the number of events given by
             `n_events_list` if an event selection method was already utilized
             when generating background events.
         """
         # Generate background events for each dataset.
@@ -694,891 +1064,847 @@
             sig_kwargs=sig_kwargs,
             n_events_list=n_events_list,
             events_list=events_list,
             tl=tl)
 
         return (n_sig, n_events_list, events_list)
 
-    def do_trial_with_given_pseudo_data(
-            self, rss, mean_n_sig, n_sig, n_events_list, events_list,
-            mean_n_sig_0=None,
-            minimizer_status_dict=None,
-            tl=None):
-        """Performs an analysis trial on the given pseudo data.
-
-        Parameters
-        ----------
-        rss : RandomStateService
-            The RandomStateService instance to use for generating random
-            numbers.
-        mean_n_sig : float
-            The mean number of signal events the pseudo data was generated with.
-        n_sig : int
-            The total number of actual signal events in the pseudo data.
-        n_events_list : list of int
-            The total number of events for each data set of the pseudo data.
-        events_list : list of DataFieldRecordArray instances
-            The list of DataFieldRecordArray instances containing the pseudo
-            data events for each data sample. The number of events for each
-            data sample can be less than the number of events given by
-            `n_events_list` if an event selection method was already utilized
-            when generating background events.
-        mean_n_sig_0 : float | None
-            The fixed mean number of signal events for the null-hypothesis,
-            when using a ns-profile log-likelihood-ratio function.
-            If set to None, this argument is interpreted as 0.
-        minimizer_status_dict : dict | None
-            If a dictionary is provided, it will be updated with the minimizer
-            status dictionary.
-        tl : instance of TimeLord | None
-            The instance of TimeLord that should be used to time individual
-            tasks.
-
-        Returns
-        -------
-        result : structured numpy ndarray
-            The structured numpy ndarray holding the result of the trial. It
-            contains the following data fields:
-
-            rss_seed : int
-                The RandomStateService seed.
-            mean_n_sig : float
-                The mean number of signal events.
-            n_sig : int
-                The actual number of injected signal events.
-            mean_n_sig_0 : float
-                The fixed mean number of signal events for the null-hypothesis.
-            ts : float
-                The test-statistic value.
-            [<fitparam_name> ... : float ]
-                Any additional fit parameters of the LLH function.
-        """
-        if(mean_n_sig_0 is not None):
-            self._llhratio.mean_n_sig_0 = mean_n_sig_0
-        else:
-            mean_n_sig_0 = 0
-
-        with TaskTimer(tl, 'Initializing trial.'):
-            self.initialize_trial(events_list, n_events_list)
-
-        with TaskTimer(tl, 'Maximizing LLH ratio function.'):
-            (fitparamset, log_lambda_max, fitparam_values, status) = self.maximize_llhratio(rss, tl=tl)
-        if(isinstance(minimizer_status_dict, dict)):
-            minimizer_status_dict.update(status)
-
-        with TaskTimer(tl, 'Calculating test statistic.'):
-            ts = self.calculate_test_statistic(log_lambda_max, fitparam_values)
-
-        # Create the structured array data type for the result array.
-        result_dtype = [
-            ('seed', np.int64),
-            ('mean_n_sig', np.float64),
-            ('n_sig', np.int64),
-            ('mean_n_sig_0', np.float64),
-            ('ts', np.float64)
-        ] + [
-            (fitparam_name, np.float64)
-                for fitparam_name in fitparamset.fitparam_name_list
-        ]
-        result = np.empty((1,), dtype=result_dtype)
-        result['seed'] = rss.seed
-        result['mean_n_sig'] = mean_n_sig
-        result['n_sig'] = n_sig
-        result['mean_n_sig_0'] = mean_n_sig_0
-        result['ts'] = ts
-        for (idx, fitparam_name) in enumerate(fitparamset.fitparam_name_list):
-            result[fitparam_name] = fitparam_values[idx]
-
-        return result
-
-    def do_trial_with_given_bkg_and_sig_pseudo_data(
-            self, rss, mean_n_sig, n_sig, n_bkg_events_list, n_sig_events_list,
-            bkg_events_list, sig_events_list,
-            mean_n_sig_0=None,
-            minimizer_status_dict=None,
-            tl=None):
-        """Performs an analysis trial on the given background and signal pseudo
-        data. This method merges the background and signal pseudo events and
-        calls the ``do_trial_with_given_pseudo_data`` method of this class.
-
-        Note
-        ----
-        This method alters the DataFieldRecordArray instances of the
-        bkg_events_list argument!
-
-        Parameters
-        ----------
-        rss : RandomStateService
-            The RandomStateService instance to use for generating random
-            numbers.
-        mean_n_sig : float
-            The mean number of signal events the pseudo data was generated with.
-        n_sig : int
-            The total number of actual signal events in the pseudo data.
-        n_bkg_events_list : list of int
-            The total number of background events for each data set of the
-            pseudo data.
-        n_sig_events_list : list of int
-            The total number of signal events for each data set of the
-            pseudo data.
-        bkg_events_list : list of DataFieldRecordArray instances
-            The list of DataFieldRecordArray instances containing the background
-            pseudo data events for each data set.
-        sig_events_list : list of DataFieldRecordArray instances or None
-            The list of DataFieldRecordArray instances containing the signal
-            pseudo data events for each data set. If a particular dataset has
-            no signal events, the entry for that dataset can be None.
-        mean_n_sig_0 : float | None
-            The fixed mean number of signal events for the null-hypothesis,
-            when using a ns-profile log-likelihood-ratio function.
-            If set to None, this argument is interpreted as 0.
-        minimizer_status_dict : dict | None
-            If a dictionary is provided, it will be updated with the minimizer
-            status dictionary.
-        tl : instance of TimeLord | None
-            The instance of TimeLord that should be used to time individual
-            tasks.
-
-        Returns
-        -------
-        result : structured numpy ndarray
-            The structured numpy ndarray holding the result of the trial.
-            See the documentation of the ``do_trial_with_given_pseudo_data``
-            method for further information.
-        """
-        n_events_list = list(
-            np.array(n_bkg_events_list) +
-            np.array(n_sig_events_list)
-        )
-
-        events_list = bkg_events_list
-
-        # Add potential signal events to the background events.
-        for ds_idx in range(len(events_list)):
-            if(sig_events_list[ds_idx] is not None):
-                if(events_list[ds_idx] is None):
-                    events_list[ds_idx] = sig_events_list[ds_idx]
-                else:
-                    events_list[ds_idx].append(sig_events_list[ds_idx])
-
-        return self.do_trial_with_given_pseudo_data(
-            rss = rss,
-            mean_n_sig = mean_n_sig,
-            n_sig = n_sig,
-            n_events_list = n_events_list,
-            events_list = events_list,
-            mean_n_sig_0 = mean_n_sig_0,
-            minimizer_status_dict = minimizer_status_dict,
-            tl = tl
-        )
-
     def do_trial(
-            self, rss, mean_n_bkg_list=None, mean_n_sig=0, mean_n_sig_0=None,
-            bkg_kwargs=None, sig_kwargs=None, minimizer_status_dict=None,
-            tl=None):
-        """Performs an analysis trial by generating a pseudo data sample with
-        background events and possible signal events, and performs the LLH
-        analysis on that random pseudo data sample.
+            self,
+            rss,
+            mean_n_bkg_list=None,
+            mean_n_sig=0,
+            bkg_kwargs=None,
+            sig_kwargs=None,
+            minimizer_status_dict=None,
+            tl=None,
+            **kwargs):
+        """This method performs an analysis trial by generating a
+        pseudo data sample with background events and possible signal events
+        via the :meth:`generate_pseudo_data` method, and performs the analysis
+        on that random pseudo data sample by calling the
+        :meth:`do_trial_with_given_pseudo_data` method.
 
         Parameters
         ----------
-        rss : RandomStateService
-            The RandomStateService instance to use for generating random
-            numbers.
+        rss : instance of RandomStateService
+            The instance of RandomStateService instance to use for generating
+            random numbers.
         mean_n_bkg_list : list of float | None
             The mean number of background events that should be generated for
             each dataset. If set to None (the default), the background
             generation method needs to obtain this number itself.
         mean_n_sig : float
             The mean number of signal events that should be generated for the
-            trial. The actual number of generated events will be drawn from a
-            Poisson distribution with this given signal mean as mean.
-        mean_n_sig_0 : float | None
-            The fixed mean number of signal events for the null-hypothesis,
-            when using a ns-profile log-likelihood-ratio function.
-            If set to None, this argument is interpreted as 0.
+            trial.
         bkg_kwargs : dict | None
             Additional keyword arguments for the `generate_events` method of the
             background generation method class. An usual keyword argument is
             `poisson`.
         sig_kwargs : dict | None
             Additional keyword arguments for the `generate_signal_events` method
             of the `SignalGenerator` class. An usual keyword argument is
             `poisson`.
         minimizer_status_dict : dict | None
             If a dictionary is provided, it will be updated with the minimizer
             status dictionary.
         tl : instance of TimeLord | None
-            The instance of TimeLord that should be used to time individual
-            tasks.
+            The optional instance of TimeLord that should be used to time
+            individual tasks.
+        **kwargs : dict
+            Additional keyword arguments are passed to the
+            :meth:`do_trial_with_given_pseudo_data` method.
 
         Returns
         -------
-        result : structured numpy ndarray
-            The structured numpy ndarray holding the result of the trial. It
-            contains the following data fields:
-
-            mean_n_sig : float
-                The mean number of signal events.
-            n_sig : int
-                The actual number of injected signal events.
-            mean_n_sig_0 : float
-                The fixed mean number of signal events for the null-hypothesis.
-            ts : float
-                The test-statistic value.
-            [<fitparam_name> ... : float ]
-                Any additional fit parameters of the LLH function.
+        recarray : instance of numpy record ndarray
+            The numpy record ndarray holding the result of the trial.
+            See the documentation of the
+            :py:meth:`~skyllh.core.analysis.Analysis.do_trial_with_given_pseudo_data`
+            method for further information.
         """
-        if(mean_n_sig_0 is not None):
-            self._llhratio.mean_n_sig_0 = mean_n_sig_0
-        else:
-            mean_n_sig_0 = 0
-
         with TaskTimer(tl, 'Generating pseudo data.'):
             (n_sig, n_events_list, events_list) = self.generate_pseudo_data(
-                rss=rss, mean_n_bkg_list=mean_n_bkg_list, mean_n_sig=mean_n_sig,
-                bkg_kwargs=bkg_kwargs, sig_kwargs=sig_kwargs, tl=tl)
+                rss=rss,
+                mean_n_bkg_list=mean_n_bkg_list,
+                mean_n_sig=mean_n_sig,
+                bkg_kwargs=bkg_kwargs,
+                sig_kwargs=sig_kwargs,
+                tl=tl)
 
-        result = self.do_trial_with_given_pseudo_data(
+        recarray = self.do_trial_with_given_pseudo_data(
             rss=rss,
             mean_n_sig=mean_n_sig,
             n_sig=n_sig,
             n_events_list=n_events_list,
             events_list=events_list,
-            mean_n_sig_0=mean_n_sig_0,
             minimizer_status_dict=minimizer_status_dict,
-            tl=tl
-        )
+            tl=tl,
+            **kwargs)
 
-        return result
+        return recarray
 
     def do_trials(
-            self, rss, n, mean_n_bkg_list=None, mean_n_sig=0, mean_n_sig_0=None,
-            bkg_kwargs=None, sig_kwargs=None, ncpu=None, tl=None, ppbar=None):
-        """Executes `do_trial` method `N` times with possible multi-processing.
-        One trial performs an analysis trial by generating a pseudo data sample
-        with background events and possible signal events, and performs the LLH
-        analysis on that random pseudo data sample.
+            self,
+            rss,
+            n,
+            ncpu=None,
+            tl=None,
+            ppbar=None,
+            **kwargs):
+        """Executes the :meth:`do_trial` method ``n`` times with possible
+        multi-processing.
 
         Parameters
         ----------
-        rss : RandomStateService
+        rss : instance of RandomStateService
             The RandomStateService instance to use for generating random
             numbers.
         n : int
             Number of trials to generate using the `do_trial` method.
-        mean_n_bkg_list : list of float | None
-            The mean number of background events that should be generated for
-            each dataset. If set to None (the default), the number of data
-            events of each data sample will be used as mean.
-        mean_n_sig : float
-            The mean number of signal events that should be generated for the
-            trial. The actual number of generated events will be drawn from a
-            Poisson distribution with this given signal mean as mean.
-        mean_n_sig_0 : float | None
-            The fixed mean number of signal events for the null-hypothesis,
-            when using a ns-profile log-likelihood-ratio function.
-        bkg_kwargs : dict | None
-            Additional keyword arguments for the `generate_events` method of the
-            background generation method class. An usual keyword argument is
-            `poisson`.
-        sig_kwargs : dict | None
-            Additional keyword arguments for the `generate_signal_events` method
-            of the `SignalGenerator` class. An usual keyword argument is
-            `poisson`. If `poisson` is set to True, the actual number of
-            generated signal events will be drawn from a Poisson distribution
-            with the given mean number of signal events.
-            If set to False, the argument ``mean_n_sig`` specifies the actual
-            number of generated signal events.
         ncpu : int | None
             The number of CPUs to use, i.e. the number of subprocesses to
             spawn. If set to None, the global setting will be used.
         tl : instance of TimeLord | None
-            The instance of TimeLord that should be used to time individual
-            tasks.
+            The optional instance of TimeLord that should be used to time
+            individual tasks.
         ppbar : instance of ProgressBar | None
             The possible parent ProgressBar instance.
+        **kwargs
+            Additional keyword arguments are passed to the :meth:`do_trial`
+            method. See the documentation of that method for allowed keyword
+            arguments.
 
         Returns
         -------
-        result : structured numpy ndarray
-            The structured numpy ndarray holding the result of the trial. It
-            contains the following data fields:
+        recarray : numpy record ndarray
+            The numpy record ndarray holding the result of all trials.
+            See the documentation of the
+            :py:meth:`~skyllh.core.analysis.Analysis.do_trial` method for the
+            list of data fields.
+        """
+        ncpu = get_ncpu(
+            cfg=self._cfg,
+            local_ncpu=ncpu)
 
-            n_sig : int
-                The actual number of injected signal events.
-            ts : float
-                The test-statistic value.
-            [<fitparam_name> ... : float ]
-                Any additional fit parameters of the LLH function.
-        """
-        ncpu = get_ncpu(ncpu)
-        args_list = [((), {
-            'mean_n_bkg_list': mean_n_bkg_list,
-            'mean_n_sig': mean_n_sig,
-            'mean_n_sig_0': mean_n_sig_0,
-            'bkg_kwargs': bkg_kwargs,
-            'sig_kwargs': sig_kwargs
-            }) for i in range(n)
-        ]
+        args_list = [((), kwargs) for i in range(n)]
         result_list = parallelize(
-            self.do_trial, args_list, ncpu, rss=rss, tl=tl, ppbar=ppbar)
+            func=self.do_trial,
+            args_list=args_list,
+            ncpu=ncpu,
+            rss=rss,
+            tl=tl,
+            ppbar=ppbar)
+
+        recarray_dtype = result_list[0].dtype
+        recarray = np.empty(n, dtype=recarray_dtype)
+        recarray[:] = np.array(result_list)[:, 0]
 
-        result_dtype = result_list[0].dtype
-        result = np.empty(n, dtype=result_dtype)
-        result[:] = np.array(result_list)[:,0]
-        return result
+        return recarray
 
 
-class TimeIntegratedMultiDatasetSingleSourceAnalysis(Analysis):
-    """This is an analysis class that implements a time-integrated LLH ratio
-    analysis for multiple datasets assuming a single source.
+class LLHRatioAnalysis(
+        Analysis,
+        metaclass=abc.ABCMeta):
+    """This is the abstract base class for all log-likelihood ratio analysis
+    classes. It requires a mathematical log-likelihood ratio function.
 
-    To run this analysis the following procedure applies:
+    To set-up and run an analysis the following procedure applies:
 
-        1. Add the datasets and their spatial and energy PDF ratio instances
-           via the :meth:`.add_dataset` method.
-        2. Construct the log-likelihood ratio function via the
+        1. Create an Analysis instance.
+        2. Add the datasets and their PDF ratio instances via the
+           :meth:`skyllh.core.analysis.Analysis.add_dataset` method.
+        3. Construct the log-likelihood ratio function via the
            :meth:`construct_llhratio` method.
-        3. Initialize a trial via the :meth:`initialize_trial` method.
-        4. Fit the global fit parameters to the trial data via the
-           :meth:`maximize_llhratio` method.
+        4. Initialize a trial via the :meth:`initialize_trial` method.
+        5. Fit the global fit parameters to the trial data via the
+           :meth:`maximize` method of the ``llhratio`` property.
+
+    Alternatively, one can use the convenient methods :meth:`do_trial` or
+    :meth:`unblind` to perform a random trial or to unblind the data,
+    respectively. Both methods will fit the global fit parameters using the set
+    up data. Finally, the test statistic is calculated via the
+    :meth:`calculate_test_statistic` method.
+
+    In order to calculate sensitivities and discovery potentials, analysis
+    trials have to be performed on random data samples with injected signal
+    events. To perform a trial with injected signal events, the signal generator
+    has to be constructed via the :meth:`construct_signal_generator` method
+    before any random trial data can be generated.
     """
-    def __init__(self, src_hypo_group_manager, src_fitparam_mapper, fitparam_ns,
-                 test_statistic, bkg_gen_method=None, sig_generator_cls=None):
-        """Creates a new time-integrated point-like source analysis assuming a
-        single source.
+
+    def __init__(
+            self,
+            shg_mgr,
+            pmm,
+            test_statistic,
+            bkg_generator_cls=None,
+            sig_generator_cls=None,
+            **kwargs):
+        """Constructs a new instance of LLHRatioAnalysis.
 
         Parameters
         ----------
-        src_hypo_group_manager : instance of SourceHypoGroupManager
+        shg_mgr : instance of SourceHypoGroupManager
             The instance of SourceHypoGroupManager, which defines the groups of
             source hypotheses, their flux model, and their detector signal
-            efficiency implementation method.
-        src_fitparam_mapper : instance of SingleSourceFitParameterMapper
-            The instance of SingleSourceFitParameterMapper defining the global
-            fit parameters and their mapping to the source fit parameters.
-        fitparam_ns : FitParameter instance
-            The FitParameter instance defining the fit parameter ns.
+            yield implementation method.
+        pmm : instance of ParameterModelMapper
+            The ParameterModelMapper instance managing the global set of
+            parameters and their relation to individual models, e.g. sources.
         test_statistic : TestStatistic instance
             The TestStatistic instance that defines the test statistic function
             of the analysis.
-        bkg_gen_method : instance of BackgroundGenerationMethod | None
-            The instance of BackgroundGenerationMethod that will be used to
-            generate background events for a new analysis trial. This can be set
-            to None, if no background events have to get generated.
-        sig_generator_cls : SignalGeneratorBase class | None
+        bkg_generator_cls : class of BackgroundGeneratorBase | None
+            The background generator class used to create the background
+            generator instance.
+            If set to ``None``, the
+            :class:`skyllh.core.background_generator.BackgroundGenerator` class
+            is used.
+        sig_generator_cls : class of SignalGenerator | None
             The signal generator class used to create the signal generator
             instance.
-            If set to None, the `SignalGenerator` class is used.
+            If set to None, the
+            :class:`~skyllh.core.signal_generator.MultiDatasetSignalGenerator`
+            class is used.
         """
-        if(not isinstance(src_fitparam_mapper, SingleSourceFitParameterMapper)):
-            raise TypeError('The src_fitparam_mapper argument must be an '
-                'instance of SingleSourceFitParameterMapper!')
-
         super().__init__(
-            src_hypo_group_manager=src_hypo_group_manager,
-            src_fitparam_mapper=src_fitparam_mapper,
+            shg_mgr=shg_mgr,
+            pmm=pmm,
             test_statistic=test_statistic,
-            bkg_gen_method=bkg_gen_method,
-            sig_generator_cls=sig_generator_cls)
+            bkg_generator_cls=bkg_generator_cls,
+            sig_generator_cls=sig_generator_cls,
+            **kwargs)
 
-        self.fitparam_ns = fitparam_ns
+        # Define the member variable for the list of PDFRatio instances, one for
+        # each dataset.
+        self._pdfratio_list = []
 
-        # Define the member for the list of PDF ratio lists. Each list entry is
-        # a list of PDF ratio instances for each data set.
-        self._pdfratio_list_list = []
-
-        # Create the FitParameterSet instance holding the fit parameter ns and
-        # all the other additional fit parameters. This set is used by the
-        # ``maximize_llhratio`` method.
-        self._fitparamset = self._src_fitparam_mapper.fitparamset.copy()
-        self._fitparamset.add_fitparam(self._fitparam_ns, atfront=True)
+        self._llhratio = None
 
     @property
-    def fitparam_ns(self):
-        """The FitParameter instance for the fit parameter ns.
+    def llhratio(self):
+        """The log-likelihood ratio function instance. It is None, if it has
+        not been constructed yet.
         """
-        return self._fitparam_ns
-    @fitparam_ns.setter
-    def fitparam_ns(self, fitparam):
-        if(not isinstance(fitparam, FitParameter)):
-            raise TypeError('The fitparam_ns property must be an instance of FitParameter!')
-        self._fitparam_ns = fitparam
+        if self._llhratio is None:
+            raise RuntimeError(
+                'The log-likelihood ratio function is not defined yet. '
+                'Call the "construct_llhratio" method first!')
+        return self._llhratio
+
+    @llhratio.setter
+    def llhratio(self, obj):
+        if not isinstance(obj, LLHRatio):
+            raise TypeError(
+                'The llhratio property must be an instance of LLHRatio! '
+                f'Its current type is {classname(obj)}.')
+        self._llhratio = obj
+
+    @abc.abstractmethod
+    def construct_llhratio(
+            self,
+            minimizer,
+            ppbar=None):
+        """This method is supposed to construct the LLH ratio function.
 
-    def add_dataset(self, dataset, data, pdfratios, tdm=None,
-                    event_selection_method=None):
+        Returns
+        -------
+        llhratio : instance of LLHRatio
+            The instance of LLHRatio that implements the
+            log-likelihood-ratio function of this LLH ratio analysis.
+        """
+        pass
+
+    def add_dataset(
+            self,
+            dataset,
+            data,
+            pdfratio,
+            tdm=None,
+            event_selection_method=None,
+            bkg_generator=None,
+            sig_generator=None):
         """Adds a dataset with its PDF ratio instances to the analysis.
 
         Parameters
         ----------
-        dataset : Dataset instance
-            The Dataset instance that should get added.
-        data : DatasetData instance
-            The DatasetData instance holding the original (prepared) data of the
-            dataset.
-        pdfratios : PDFRatio instance | sequence of PDFRatio instances
-            The PDFRatio instance or the sequence of PDFRatio instances for the
-            to-be-added data set.
-        tdm : TrialDataManager instance | None
+        dataset : instance of Dataset
+            The instance of Dataset that should get added.
+        data : instance of DatasetData
+            The instance of DatasetData holding the original (prepared) data of
+            the dataset.
+        pdfratio : instance of PDFRatio
+            The instance of PDFRatio for the to-be-added data set.
+        tdm : instance of TrialDataManager | None
             The TrialDataManager instance that manages the trial data and
             additional data fields for this data set.
         event_selection_method : instance of EventSelectionMethod | None
             The instance of EventSelectionMethod to use to select only
             signal-like events from the trial data. All other events
             will be treated as pure background events. This reduces the amount
             of log-likelihood-ratio function evaluations. If set to None, all
             events will be evaluated.
-        """
-        super(TimeIntegratedMultiDatasetSingleSourceAnalysis, self).add_dataset(
-            dataset, data, tdm, event_selection_method)
+        bkg_generator : instance of BackgroundGenerator | None
+            The optional instance of BackgroundGenerator, which should be used
+            to generate background events for this particular dataset.
+        sig_generator : instance of SignalGenerator | None
+            The optional instance of SignalGenerator, which should be used
+            to generate signal events for this particular dataset.
+        """
+        super().add_dataset(
+            dataset=dataset,
+            data=data,
+            tdm=tdm,
+            event_selection_method=event_selection_method,
+            bkg_generator=bkg_generator,
+            sig_generator=sig_generator)
 
-        if(isinstance(pdfratios, PDFRatio)):
-            pdfratios = [pdfratios]
-        if(not issequenceof(pdfratios, PDFRatio)):
-            raise TypeError('The pdfratios argument must be an instance of '
-                'PDFRatio or a sequence of PDFRatio!')
-
-        self._pdfratio_list_list.append(list(pdfratios))
-
-    def construct_llhratio(self, minimizer, ppbar=None):
-        """Constructs the log-likelihood-ratio (LLH-ratio) function of the
-        analysis. This setups all the necessary analysis
-        objects like detector signal efficiencies and dataset signal weights,
-        constructs the log-likelihood ratio functions for each dataset and the
-        final composite llh ratio function.
-
-        Parameters
-        ----------
-        minimizer : instance of Minimizer
-            The instance of Minimizer that should be used to minimize the
-            negative of the log-likelihood ratio function.
-        ppbar : ProgressBar instance | None
-            The instance of ProgressBar of the optional parent progress bar.
-
-        Returns
-        -------
-        llhratio : instance of MultiDatasetTCLLHRatio
-            The instance of MultiDatasetTCLLHRatio that implements the
-            log-likelihood-ratio function of the analysis.
-        """
-        # Create the detector signal yield instances for each dataset.
-        # Since this is for a single source, we don't have to have individual
-        # detector signal yield instances for each source as well.
-        detsigyield_list = []
-        fluxmodel = self._src_hypo_group_manager.get_fluxmodel_by_src_idx(0)
-        detsigyield_implmethod_list = self._src_hypo_group_manager.get_detsigyield_implmethod_list_by_src_idx(0)
-        if((len(detsigyield_implmethod_list) != 1) and
-           (len(detsigyield_implmethod_list) != self.n_datasets)):
-            raise ValueError('The number of detector signal yield '
-                'implementation methods is not 1 and does not match the number '
-                'of used datasets in the analysis!')
-        pbar = ProgressBar(len(self.dataset_list), parent=ppbar).start()
-        for (j, (dataset, data)) in enumerate(zip(self.dataset_list,
-                                                  self.data_list)):
-            if(len(detsigyield_implmethod_list) == 1):
-                # Only one detsigyield implementation method was defined, so we
-                # use it for all datasets.
-                detsigyield_implmethod = detsigyield_implmethod_list[0]
-            else:
-                detsigyield_implmethod = detsigyield_implmethod_list[j]
-
-            detsigyield = detsigyield_implmethod.construct_detsigyield(
-                dataset, data, fluxmodel, data.livetime, ppbar=pbar)
-            detsigyield_list.append(detsigyield)
-            pbar.increment()
-        pbar.finish()
-
-        # For multiple datasets we need a dataset signal weights instance in
-        # order to distribute ns over the different datasets.
-        dataset_signal_weights = SingleSourceDatasetSignalWeights(
-            self._src_hypo_group_manager, self._src_fitparam_mapper,
-            detsigyield_list)
-
-        # Create the list of log-likelihood ratio functions, one for each
-        # dataset.
-        llhratio_list = []
-        for j in range(self.n_datasets):
-            tdm = self._tdm_list[j]
-            pdfratio_list = self._pdfratio_list_list[j]
-            llhratio = SingleSourceZeroSigH0SingleDatasetTCLLHRatio(
-                minimizer,
-                self._src_hypo_group_manager,
-                self._src_fitparam_mapper,
-                tdm,
-                pdfratio_list
-            )
-            llhratio_list.append(llhratio)
-
-        # Create the final multi-dataset log-likelihood ratio function.
-        llhratio = MultiDatasetTCLLHRatio(
-            minimizer, dataset_signal_weights, llhratio_list)
-
-        return llhratio
-
-    def change_source(self, source):
-        """Changes the source of the analysis to the given source. It makes the
-        necessary changes to all the objects of the analysis.
-
-        Parameters
-        ----------
-        source : SourceModel instance
-            The SourceModel instance describing the new source.
-        """
-        if(not isinstance(source, SourceModel)):
-            raise TypeError('The source argument must be an instance of SourceModel')
-
-        if(self._llhratio is None):
-            raise RuntimeError('The LLH ratio function has to be constructed, '
-                'before the `change_source` method can be called!')
-
-        # Change the source in the SourceHypoGroupManager instance.
-        # Because this is a single source analysis, there can only be one source
-        # hypothesis group defined.
-        self._src_hypo_group_manager.src_hypo_group_list[0].source_list[0] = source
-
-        # Change the source hypo group manager of the EventSelectionMethod
-        # instance.
-        for event_selection_method in self._event_selection_method_list:
-            if(event_selection_method is not None):
-                event_selection_method.change_source_hypo_group_manager(
-                    self._src_hypo_group_manager)
-
-        # Change the source hypo group manager of the LLH ratio function
-        # instance.
-        self._llhratio.change_source_hypo_group_manager(self._src_hypo_group_manager)
-
-        # Change the source hypo group manager of the background generator
-        # instance.
-        if(self._bkg_generator is not None):
-            self._bkg_generator.change_source_hypo_group_manager(
-                self._src_hypo_group_manager)
+        if not isinstance(pdfratio, PDFRatio):
+            raise TypeError(
+                'The pdfratio argument must be an instance of PDFRatio! '
+                f'Its current type is {classname(pdfratio)}')
 
-        # Change the source hypo group manager of the signal generator instance.
-        if(self._sig_generator is not None):
-            self._sig_generator.change_source_hypo_group_manager(
-                self._src_hypo_group_manager)
+        self._pdfratio_list.append(pdfratio)
 
-    def change_sources(self, sources):
-        """Changes the sources of the analysis to the given source list. It
-        makes the necessary changes to all the objects of the analysis.
+    def change_shg_mgr(
+            self,
+            shg_mgr):
+        """If the SourceHypoGroupManager instance changed, this method needs to
+        be called to propagate the change to all components of the analysis.
 
         Parameters
         ----------
-        sources : list of SourceModel instances
-            The SourceModel instances describing new sources.
+        shg_mgr : instance of SourceHypoGroupManager
+            The new instance of SourceHypoGroupManager.
         """
-        if(isinstance(sources, SourceModel)):
-            sources = [sources]
-        if(not issequenceof(sources, SourceModel)):
-            raise TypeError('The sources argument must be a list of instances '
-                            'of SourceModel')
-
-        if(self._llhratio is None):
+        if self._llhratio is None:
             raise RuntimeError(
-                'The LLH ratio function has to be constructed, '
-                'before the `change_source` method can be called!')
-
-        # Change the source in the SourceHypoGroupManager instance.
-        # Because this is a single type sources analysis, there can only be one
-        # source hypothesis group defined.
-        self._src_hypo_group_manager.src_hypo_group_list[0].source_list = sources
+                'The LLH ratio function has to be constructed '
+                'before the `change_shg_mgr` method can be called!')
 
-        # Change the source hypo group manager of the EventSelectionMethod
-        # instance.
-        for event_selection_method in self._event_selection_method_list:
-            if(event_selection_method is not None):
-                event_selection_method.change_source_hypo_group_manager(
-                    self._src_hypo_group_manager)
+        super().change_shg_mgr(
+            shg_mgr=shg_mgr)
 
         # Change the source hypo group manager of the LLH ratio function
         # instance.
-        self._llhratio.change_source_hypo_group_manager(
-            self._src_hypo_group_manager)
-
-        # Change the source hypo group manager of the background generator
-        # instance.
-        if(self._bkg_generator is not None):
-            self._bkg_generator.change_source_hypo_group_manager(
-                self._src_hypo_group_manager)
-
-        # Change the source hypo group manager of the signal generator instance.
-        if(self._sig_generator is not None):
-            self._sig_generator.change_source_hypo_group_manager(
-                self._src_hypo_group_manager)
+        self._llhratio.change_shg_mgr(
+            shg_mgr=shg_mgr)
 
-    def initialize_trial(self, events_list, n_events_list=None, tl=None):
-        """This method initializes the multi-dataset log-likelihood ratio
+    def initialize_trial(
+            self,
+            events_list,
+            n_events_list=None,
+            tl=None):
+        """This method initializes the log-likelihood ratio
         function with a new set of given trial data. This is a low-level method.
-        For convenient methods see the `unblind` and `do_trial` methods.
+        For convenient methods see the ``unblind`` and ``do_trial`` methods.
 
         Parameters
         ----------
         events_list : list of DataFieldRecordArray instances
             The list of DataFieldRecordArray instances holding the data events
             to use for the log-likelihood function evaluation. The data arrays
             for the datasets must be in the same order than the added datasets.
         n_events_list : list of int | None
             The list of the number of events of each data set. If set to None,
             the number of events is taken from the size of the given events
             arrays.
-        tl : TimeLord | None
-            The optional TimeLord instance that should be used for timing
+        tl : instance of TimeLord | None
+            The optional instance of TimeLord that should be used for timing
             measurements.
         """
-        if(n_events_list is None):
+        if n_events_list is None:
             n_events_list = [None] * len(events_list)
 
-        for (idx, (tdm, events, n_events, evt_sel_method)) in enumerate(zip(
-                self._tdm_list, events_list, n_events_list,
-                self._event_selection_method_list)):
+        for (tdm, events, n_events, evt_sel_method) in zip(
+                self._tdm_list,
+                events_list,
+                n_events_list,
+                self._event_selection_method_list):
 
             # Initialize the trial data manager with the given raw events.
-            self._tdm_list[idx].initialize_trial(
-                self._src_hypo_group_manager, events, n_events, evt_sel_method,
+            tdm.initialize_trial(
+                shg_mgr=self._shg_mgr,
+                pmm=self._pmm,
+                events=events,
+                n_events=n_events,
+                evt_sel_method=evt_sel_method,
                 tl=tl)
 
-        self._llhratio.initialize_for_new_trial(tl=tl)
+        self._llhratio.initialize_for_new_trial(
+            tl=tl)
 
-    def maximize_llhratio(self, rss, tl=None):
-        """Maximizes the log-likelihood ratio function, by minimizing its
-        negative.
+    def unblind(
+            self,
+            rss,
+            tl=None):
+        """Evaluates the unscrambled data, i.e. unblinds the data.
 
         Parameters
         ----------
-        rss : RandomStateService instance
-            The RandomStateService instance that should be used to draw random
-            numbers from. It is used by the minimizer to generate random
-            fit parameter initial values.
-        tl : TimeLord instance | None
-            The optional TimeLord instance that should be used to time the
+        rss : instance of RandomStateService
+            The instance of RandomStateService that should be used draw random
+            numbers from.
+        tl : instance of TimeLord | None
+            The optional instance of TimeLord that should be used to time the
             maximization of the LLH ratio function.
 
         Returns
         -------
-        fitparamset : FitParameterSet instance
-            The instance of FitParameterSet holding the global fit parameter
-            definitions used in the maximization process.
-        log_lambda_max : float
-            The value of the log-likelihood ratio function at its maximum.
-        fitparam_values : (N_fitparam,)-shaped 1D ndarray
-            The ndarray holding the global fit parameter values.
-            By definition, the first element is the value of the fit parameter
-            ns.
+        TS : float
+            The test-statistic value.
+        global_params_dict : dict
+            The dictionary holding the global parameter names and their
+            best fit values. It includes fixed and floating parameters.
         status : dict
-            The dictionary with status information about the maximization
-            process, i.e. from the minimizer.
+            The status dictionary with information about the performed
+            minimization process of the negative of the log-likelihood ratio
+            function.
+        """
+        events_list = [data.exp for data in self._data_list]
+        self.initialize_trial(events_list)
+
+        (log_lambda, fitparam_values, status) = self._llhratio.maximize(
+            rss=rss,
+            tl=tl)
+
+        TS = self.calculate_test_statistic(
+            log_lambda=log_lambda,
+            fitparam_values=fitparam_values)
+
+        global_params_dict = self._pmm.create_global_params_dict(
+            gflp_values=fitparam_values)
+
+        return (TS, global_params_dict, status)
+
+    def do_trial_with_given_pseudo_data(
+            self,
+            rss,
+            mean_n_sig,
+            n_sig,
+            n_events_list,
+            events_list,
+            minimizer_status_dict=None,
+            tl=None,
+            mean_n_sig_0=None):
+        """Performs an analysis trial on the given pseudo data.
+
+        Parameters
+        ----------
+        rss : instance of RandomStateService
+            The instance of RandomStateService to use for generating random
+            numbers.
+        mean_n_sig : float
+            The mean number of signal events the pseudo data was generated with.
+        n_sig : int
+            The total number of actual signal events in the pseudo data.
+        n_events_list : list of int
+            The total number of events for each data set of the pseudo data.
+        events_list : list of instance of DataFieldRecordArray
+            The list of instance of DataFieldRecordArray containing the pseudo
+            data events for each data sample. The number of events for each
+            data sample can be less than the number of events given by
+            ``n_events_list`` if an event selection method was already utilized
+            when generating background events.
+        minimizer_status_dict : dict | None
+            If a dictionary is provided, it will be updated with the minimizer
+            status dictionary.
+        tl : instance of TimeLord | None
+            The optional instance of TimeLord that should be used to time
+            individual tasks.
+        mean_n_sig_0 : float | None
+            The fixed mean number of signal events for the null-hypothesis,
+            when using a ns-profile log-likelihood-ratio function.
+            If set to None, this argument is interpreted as 0.
+
+        Returns
+        -------
+        recarray : instance of numpy record ndarray
+            The numpy record ndarray holding the result of the trial. It
+            contains the following data fields:
+
+            rss_seed : int
+                The RandomStateService seed.
+            mean_n_sig : float
+                The mean number of signal events.
+            n_sig : int
+                The actual number of injected signal events.
+            mean_n_sig_0 : float
+                The fixed mean number of signal events for the null-hypothesis.
+            ts : float
+                The test-statistic value.
+            [<global_param_name> : float ]
+                Any additional parameters of the LLH ratio function.
+        """
+        if mean_n_sig_0 is None:
+            mean_n_sig_0 = 0
+
+        self._llhratio.mean_n_sig_0 = mean_n_sig_0
+
+        with TaskTimer(tl, 'Initializing trial.'):
+            self.initialize_trial(events_list, n_events_list)
+
+        with TaskTimer(tl, 'Maximizing LLH ratio function.'):
+            (log_lambda, fitparam_values, status) = self._llhratio.maximize(
+                rss=rss,
+                tl=tl)
+        if isinstance(minimizer_status_dict, dict):
+            minimizer_status_dict.update(status)
+
+        with TaskTimer(tl, 'Calculating test statistic.'):
+            ts = self.calculate_test_statistic(
+                log_lambda=log_lambda,
+                fitparam_values=fitparam_values)
+
+        # Get the dictionary holding all floating and fixed parameter names
+        # and values.
+        global_params_dict = self._pmm.create_global_params_dict(
+            gflp_values=fitparam_values)
+
+        # Create the structured array data type for the result array.
+        recarray_dtype = [
+            ('seed', np.int64),
+            ('mean_n_sig', np.float64),
+            ('n_sig', np.int64),
+            ('mean_n_sig_0', np.float64),
+            ('ts', np.float64)
+        ] + [
+            (param_name, np.float64)
+            for param_name in global_params_dict.keys()
+        ]
+        recarray = np.empty((1,), dtype=recarray_dtype)
+        recarray['seed'] = rss.seed
+        recarray['mean_n_sig'] = mean_n_sig
+        recarray['n_sig'] = n_sig
+        recarray['mean_n_sig_0'] = mean_n_sig_0
+        recarray['ts'] = ts
+        for (param_name, param_value) in global_params_dict.items():
+            recarray[param_name] = param_value
+
+        return recarray
+
+
+class SingleSourceMultiDatasetLLHRatioAnalysis(
+        LLHRatioAnalysis):
+    """This is an analysis class that implements a log-likelihood ratio analysis
+    for multiple datasets assuming a single source.
+    It is a special case of the multi-source analysis.
+
+    For more information how to construct and run the analysis see the
+    documentation of the :class:`~skyllh.core.analysis.LLHRatioAnalysis` class.
+    """
+    def __init__(
+            self,
+            shg_mgr,
+            pmm,
+            test_statistic,
+            bkg_generator_cls=None,
+            sig_generator_cls=None,
+            **kwargs):
+        """Creates a new time-integrated point-like source analysis assuming a
+        single source.
+
+        Parameters
+        ----------
+        shg_mgr : instance of SourceHypoGroupManager
+            The instance of SourceHypoGroupManager, which defines the groups of
+            source hypotheses, their flux model, and their detector signal
+            efficiency implementation method.
+        pmm : instance of ParameterModelMapper
+            The ParameterModelMapper instance managing the global set of
+            parameters and their relation to individual models, e.g. sources.
+        test_statistic : TestStatistic instance
+            The TestStatistic instance that defines the test statistic function
+            of the analysis.
+        bkg_generator_cls : class of BackgroundGeneratorBase | None
+            The background generator class used to create the background
+            generator instance.
+            If set to ``None``, the
+            :class:`skyllh.core.background_generator.BackgroundGenerator` class
+            is used.
+        sig_generator_cls : SignalGenerator class | None
+            The signal generator class that should be used to create the signal
+            generator instance for multiple datasets. If set to None, the
+            :class:`~skyllh.core.signal_generator.MultiDatasetSignalGenerator`
+            class is used.
+        """
+        super().__init__(
+            shg_mgr=shg_mgr,
+            pmm=pmm,
+            test_statistic=test_statistic,
+            bkg_generator_cls=bkg_generator_cls,
+            sig_generator_cls=sig_generator_cls,
+            **kwargs)
+
+    def construct_llhratio(
+            self,
+            minimizer,
+            ppbar=None):
+        """Constructs the log-likelihood (LLH) ratio function of the analysis.
+        This setups all the necessary analysis objects like detector signal
+        yields and dataset signal weights, constructs the log-likelihood ratio
+        functions for each dataset and the final composite LLH ratio function.
+
+        Parameters
+        ----------
+        minimizer : instance of Minimizer
+            The instance of Minimizer that should be used to minimize the
+            negative of the log-likelihood ratio function.
+        ppbar : instance of ProgressBar | None
+            The instance of ProgressBar of the optional parent progress bar.
+
+        Returns
+        -------
+        llhratio : instance of MultiDatasetTCLLHRatio
+            The instance of MultiDatasetTCLLHRatio that implements the
+            log-likelihood-ratio function of the analysis.
         """
-        (log_lambda_max, fitparam_values, status) = self._llhratio.maximize(
-            rss, self._fitparamset, tl=tl)
-        return (self._fitparamset, log_lambda_max, fitparam_values, status)
+        # Create the list of log-likelihood ratio functions, one for each
+        # dataset.
+        llhratio_list = [
+            ZeroSigH0SingleDatasetTCLLHRatio(
+                cfg=self._cfg,
+                pmm=self._pmm,
+                minimizer=minimizer,
+                shg_mgr=self._shg_mgr,
+                tdm=tdm,
+                pdfratio=pdfratio
+            )
+            for (tdm, pdfratio) in zip(self._tdm_list, self._pdfratio_list)
+        ]
+
+        # Create the final multi-dataset log-likelihood ratio function.
+        llhratio = MultiDatasetTCLLHRatio(
+            cfg=self._cfg,
+            pmm=self._pmm,
+            minimizer=minimizer,
+            src_detsigyield_weights_service=self.src_detsigyield_weights_service,
+            ds_sig_weight_factors_service=self.ds_sig_weight_factors_service,
+            llhratio_list=llhratio_list)
 
-    def calculate_fluxmodel_scaling_factor(self, mean_ns, fitparam_values):
+        return llhratio
+
+    def change_source(
+            self,
+            source):
+        """Changes the source of the analysis to the given source. It makes the
+        necessary changes to all the objects of the analysis.
+
+        Parameters
+        ----------
+        source : instance of SourceModel
+            The instance of SourceModel describing the new source.
+        """
+        if not isinstance(source, SourceModel):
+            raise TypeError(
+                'The source argument must be an instance of SourceModel! '
+                f'Its current type is {classname(source)}.')
+
+        # Change the source in the SourceHypoGroupManager instance.
+        # Because this is a single source analysis, there can only be one source
+        # hypothesis group defined.
+        self._shg_mgr.shg_list[0].source_list[0] = source
+
+        self.change_shg_mgr(
+            shg_mgr=self._shg_mgr)
+
+    def calculate_fluxmodel_scaling_factor(
+            self,
+            mean_ns,
+            fitparam_values):
         """Calculates the factor the source's fluxmodel has to be scaled
         in order to obtain the given mean number of signal events in the
         detector.
 
         Parameters
         ----------
         mean_ns : float
             The mean number of signal events in the detector for which the
             scaling factor is calculated.
-        fitparam_values : (N_fitparams,)-shaped 1D ndarray
-            The ndarray holding the fit parameter values that should be used for
-            the flux calculation.
+        fitparam_values : instance of numpy ndarray
+            The (N_fitparam,)-shaped 1D ndarray holding the values of the global
+            fit parameters, that should be used for the flux calculation.
+            The order of the values must match the order the fit parameters were
+            defined in the parameter model mapper.
 
         Returns
         -------
         factor : float
-            The factor the given fluxmodel needs to be scaled in order to obtain
-            the given mean number of signal events in the detector.
+            The factor the source's fluxmodel needs to be scaled in order to
+            obtain the given mean number of signal events in the detector.
         """
-        fitparams_arr = self._src_fitparam_mapper.get_fitparams_array(
-            fitparam_values)
-
-        # We use the DatasetSignalWeights class instance of this analysis to
-        # calculate the detector signal yield for all datasets.
-        dataset_signal_weights = self._llhratio.dataset_signal_weights
+        src_params_recarray =\
+            self._pmm.create_src_params_recarray(
+                gflp_values=fitparam_values)
 
         # Calculate the detector signal yield, i.e. the mean number of signal
         # events in the detector, for the given reference flux model.
         mean_ns_ref = 0
-        detsigyields = dataset_signal_weights.detsigyield_arr[0]
-        for detsigyield in detsigyields:
+
+        detsigyields = self.detsigyield_service.arr[:, 0]
+        for (j, detsigyield) in enumerate(detsigyields):
+            src_recarray =\
+                self.src_detsigyield_weights_service.src_recarray_list_list[j][0]
             (Yj, Yj_grads) = detsigyield(
-                dataset_signal_weights._src_arr_list[0], fitparams_arr)
+                src_recarray=src_recarray,
+                src_params_recarray=src_params_recarray)
             mean_ns_ref += Yj[0]
 
         factor = mean_ns / mean_ns_ref
 
         return factor
 
 
-class TimeIntegratedMultiDatasetMultiSourceAnalysis(
-        TimeIntegratedMultiDatasetSingleSourceAnalysis):
-    """This is an analysis class that implements a time-integrated LLH ratio
-    analysis for multiple datasets assuming multiple sources.
-
-    To run this analysis the following procedure applies:
-
-        1. Add the datasets and their spatial and energy PDF ratio instances
-           via the :meth:`.add_dataset` method.
-        2. Construct the log-likelihood ratio function via the
-           :meth:`construct_llhratio` method.
-        3. Initialize a trial via the :meth:`initialize_trial` method.
-        4. Fit the global fit parameters to the trial data via the
-           :meth:`maximize_llhratio` method.
+class MultiSourceMultiDatasetLLHRatioAnalysis(
+        LLHRatioAnalysis):
+    """This is an analysis class that implements a log-likelihood ratio analysis
+    for multiple datasets assuming a multiple sources.
+
+    For more information how to construct and run the analysis see the
+    documentation of the :class:`~skyllh.core.analysis.LLHRatioAnalysis` class.
     """
     def __init__(
-            self, src_hypo_group_manager, src_fitparam_mapper, fitparam_ns,
-            test_statistic, bkg_gen_method=None, sig_generator_cls=None):
-        """Creates a new time-integrated point-like source analysis assuming
-        multiple sources.
+            self,
+            shg_mgr,
+            pmm,
+            test_statistic,
+            bkg_generator_cls=None,
+            sig_generator_cls=None,
+            **kwargs):
+        """Constructs a new instance of MultiDatasetLLHRatioAnalysis.
 
         Parameters
         ----------
-        src_hypo_group_manager : instance of SourceHypoGroupManager
+        shg_mgr : instance of SourceHypoGroupManager
             The instance of SourceHypoGroupManager, which defines the groups of
             source hypotheses, their flux model, and their detector signal
-            efficiency implementation method.
-        src_fitparam_mapper : instance of SingleSourceFitParameterMapper
-            The instance of SingleSourceFitParameterMapper defining the global
-            fit parameters and their mapping to the source fit parameters.
-        fitparam_ns : FitParameter instance
-            The FitParameter instance defining the fit parameter ns.
+            yield implementation method.
+        pmm : instance of ParameterModelMapper
+            The ParameterModelMapper instance managing the global set of
+            parameters and their relation to individual models, e.g. sources.
         test_statistic : TestStatistic instance
             The TestStatistic instance that defines the test statistic function
             of the analysis.
-        bkg_gen_method : instance of BackgroundGenerationMethod | None
-            The instance of BackgroundGenerationMethod that will be used to
-            generate background events for a new analysis trial. This can be set
-            to None, if no background events have to get generated.
-        sig_generator_cls : SignalGeneratorBase class | None
-            The signal generator class used to create the signal generator
-            instance.
-            If set to None, the `SignalGenerator` class is used.
+        bkg_generator_cls : class of BackgroundGeneratorBase | None
+            The background generator class used to create the background
+            generator instance.
+            If set to ``None``, the
+            :class:`skyllh.core.background_generator.BackgroundGenerator` class
+            is used.
+        sig_generator_cls : subclass of SignalGenerator| None
+            The signal generator class that should be used to create the signal
+            generator instance handling multiple datasets.
+            If set to None, the
+            :class:`~skyllh.core.signal_generator.MultiDatasetSignalGenerator`
+            class is used.
         """
         super().__init__(
-            src_hypo_group_manager=src_hypo_group_manager,
-            src_fitparam_mapper=src_fitparam_mapper,
-            fitparam_ns=fitparam_ns,
+            shg_mgr=shg_mgr,
+            pmm=pmm,
             test_statistic=test_statistic,
-            bkg_gen_method=bkg_gen_method,
-            sig_generator_cls=sig_generator_cls)
+            bkg_generator_cls=bkg_generator_cls,
+            sig_generator_cls=sig_generator_cls,
+            **kwargs)
 
-    def construct_llhratio(self, minimizer, ppbar=None):
-        """Constructs the log-likelihood-ratio (LLH-ratio) function of the
-        analysis. This setups all the necessary analysis
-        objects like detector signal efficiencies and dataset signal weights,
-        constructs the log-likelihood ratio functions for each dataset and the
-        final composite llh ratio function.
+    def construct_llhratio(
+            self,
+            minimizer,
+            ppbar=None):
+        """Constructs the log-likelihood (LLH) ratio function of the analysis.
+        This setups all the necessary analysis objects like detector signal
+        yields and dataset signal weights, constructs the log-likelihood ratio
+        functions for each dataset and the final composite LLH ratio function.
 
         Parameters
         ----------
         minimizer : instance of Minimizer
             The instance of Minimizer that should be used to minimize the
             negative of the log-likelihood ratio function.
-        ppbar : ProgressBar instance | None
+        ppbar : instance of ProgressBar | None
             The instance of ProgressBar of the optional parent progress bar.
 
         Returns
         -------
         llhratio : instance of MultiDatasetTCLLHRatio
             The instance of MultiDatasetTCLLHRatio that implements the
             log-likelihood-ratio function of the analysis.
         """
-        # Create the detector signal yield instances for each dataset.
-        # Multi source analysis has to also support multiple source hypothesis
-        # groups.
-        # Initialize empty (N_source_hypo_groups, N_datasets)-shaped ndarray.
-        detsigyield_array = np.empty(
-            (self._src_hypo_group_manager.n_src_hypo_groups,
-             len(self.dataset_list)),
-            dtype=object
-        )
-
-        for (g, shg) in enumerate(self._src_hypo_group_manager._src_hypo_group_list):
-            fluxmodel = shg.fluxmodel
-            detsigyield_implmethod_list = shg.detsigyield_implmethod_list
-
-            if((len(detsigyield_implmethod_list) != 1) and
-               (len(detsigyield_implmethod_list) != self.n_datasets)):
-                raise ValueError(
-                    'The number of detector signal yield '
-                    'implementation methods is not 1 and does not match the number '
-                    'of used datasets in the analysis!')
-            pbar = ProgressBar(len(self.dataset_list), parent=ppbar).start()
-            for (j, (dataset, data)) in enumerate(zip(self.dataset_list,
-                                                      self.data_list)):
-                if(len(detsigyield_implmethod_list) == 1):
-                    # Only one detsigyield implementation method was defined, so we
-                    # use it for all datasets.
-                    detsigyield_implmethod = detsigyield_implmethod_list[0]
-                else:
-                    detsigyield_implmethod = detsigyield_implmethod_list[j]
-
-                detsigyield = detsigyield_implmethod.construct_detsigyield(
-                    dataset, data, fluxmodel, data.livetime, ppbar=pbar)
-                detsigyield_array[g, j] = detsigyield
-                pbar.increment()
-            pbar.finish()
-
-        # For multiple datasets we need a dataset signal weights instance in
-        # order to distribute ns over the different datasets.
-        dataset_signal_weights = MultiSourceDatasetSignalWeights(
-            self._src_hypo_group_manager, self._src_fitparam_mapper,
-            detsigyield_array)
-
         # Create the list of log-likelihood ratio functions, one for each
         # dataset.
-        llhratio_list = []
-        for j in range(self.n_datasets):
-            tdm = self._tdm_list[j]
-            pdfratio_list = self._pdfratio_list_list[j]
-            llhratio = MultiSourceZeroSigH0SingleDatasetTCLLHRatio(
-                minimizer,
-                self._src_hypo_group_manager,
-                self._src_fitparam_mapper,
-                tdm,
-                pdfratio_list,
-                detsigyield_array[:, j]
+        llhratio_list = [
+            ZeroSigH0SingleDatasetTCLLHRatio(
+                cfg=self._cfg,
+                pmm=self._pmm,
+                minimizer=minimizer,
+                shg_mgr=self._shg_mgr,
+                tdm=tdm,
+                pdfratio=SourceWeightedPDFRatio(
+                    cfg=self._cfg,
+                    dataset_idx=dataset_idx,
+                    src_detsigyield_weights_service=self.src_detsigyield_weights_service,
+                    pdfratio=pdfratio)
             )
-            llhratio_list.append(llhratio)
+            for (dataset_idx, (tdm, pdfratio)) in enumerate(
+                zip(self._tdm_list, self._pdfratio_list))
+        ]
 
         # Create the final multi-dataset log-likelihood ratio function.
         llhratio = MultiDatasetTCLLHRatio(
-            minimizer, dataset_signal_weights, llhratio_list)
+            cfg=self._cfg,
+            pmm=self._pmm,
+            minimizer=minimizer,
+            src_detsigyield_weights_service=self.src_detsigyield_weights_service,
+            ds_sig_weight_factors_service=self.ds_sig_weight_factors_service,
+            llhratio_list=llhratio_list)
 
         return llhratio
 
-    def initialize_trial(self, events_list, n_events_list=None, tl=None):
-        """This method initializes the multi-dataset log-likelihood ratio
-        function with a new set of given trial data. This is a low-level method.
-        For convenient methods see the `unblind` and `do_trial` methods.
+    def calculate_fluxmodel_scaling_factors(
+            self,
+            mean_ns,
+            fitparam_values):
+        """Calculates the factors the source's fluxmodel has to be scaled
+        in order to obtain the given mean number of signal events in the
+        detector.
 
         Parameters
         ----------
-        events_list : list of DataFieldRecordArray instances
-            The list of DataFieldRecordArray instances holding the data events
-            to use for the log-likelihood function evaluation. The data arrays
-            for the datasets must be in the same order than the added datasets.
-        n_events_list : list of int | None
-            The list of the number of events of each data set. If set to None,
-            the number of events is taken from the size of the given events
-            arrays.
-        tl : TimeLord | None
-            The optional TimeLord instance that should be used for timing
-            measurements.
+        mean_ns : float
+            The mean number of signal events in the detector for which the
+            scaling factors will be calculated.
+        fitparam_values : instance of numpy ndarray
+            The (N_fitparam,)-shaped 1D ndarray holding the values of the global
+            fit parameters, which should be used for the flux calculation.
+            The order of the values must match the order the fit parameters were
+            defined in the parameter model mapper.
+
+        Returns
+        -------
+        factors : instance of numpy ndarray
+            The (N_sources,)-shaped numpy ndarray of float holding the factors
+            the fluxmodels of the sources need to be scaled in order to obtain
+            the given mean number of signal events in the detector.
         """
-        if(n_events_list is None):
-            n_events_list = [None] * len(events_list)
+        src_params_recarray =\
+            self._pmm.create_src_params_recarray(
+                gflp_values=fitparam_values)
 
-        for (idx, (tdm, events, n_events, evt_sel_method)) in enumerate(zip(
-                self._tdm_list, events_list, n_events_list,
-                self._event_selection_method_list)):
+        # Calculate the detector signal yield, i.e. the mean number of signal
+        # events in the detector, for the given reference flux model.
+        mean_ns_ref = np.zeros((self._shg_mgr.n_sources,), dtype=np.float64)
 
-            # Initialize the trial data manager with the given raw events.
-            self._tdm_list[idx].initialize_trial(
-                self._src_hypo_group_manager, events, n_events, evt_sel_method,
-                store_src_ev_idxs=True, tl=tl)
+        for (g, shg) in enumerate(self._shg_mgr.shg_list):
+            shg_src_mask = self._shg_mgr.get_src_mask_of_shg(shg_idx=g)
+
+            detsigyields = self.detsigyield_service.arr[:, g]
+            for (j, detsigyield) in enumerate(detsigyields):
+                src_recarray =\
+                    self.src_detsigyield_weights_service.src_recarray_list_list[j][g]
+                (Yj, Yj_grads) = detsigyield(
+                    src_recarray=src_recarray,
+                    src_params_recarray=src_params_recarray)
+                mean_ns_ref[shg_src_mask] += Yj
+
+        factors = mean_ns / mean_ns_ref
 
-        self._llhratio.initialize_for_new_trial(tl=tl)
+        return factors
```

### Comparing `skyllh-23.1.1/skyllh/core/analysis_utils.py` & `skyllh-23.2.0/skyllh/core/utils/analysis.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,86 +1,109 @@
 # -*- coding: utf-8 -*-
 
-from __future__ import division
-
+import itertools
 import logging
 import numpy as np
-from numpy.lib import recfunctions as np_rfn
-import itertools
-from os import makedirs
+from numpy.lib import (
+    recfunctions as np_rfn,
+)
+from os import (
+    makedirs,
+)
 import os.path
+from scipy.interpolate import (
+    interp1d,
+)
+from scipy.stats import (
+    gamma,
+)
+
+try:
+    from iminuit import minimize
+except Exception:
+    IMINUIT_LOADED = False
+else:
+    IMINUIT_LOADED = True
 
-from skyllh.core.progressbar import ProgressBar
+from skyllh.core.progressbar import (
+    ProgressBar,
+)
 from skyllh.core.py import (
     float_cast,
     int_cast,
     issequence,
-    issequenceof
+    issequenceof,
+)
+from skyllh.core.session import (
+    is_interactive_session,
+)
+from skyllh.core.source_model import (
+    PointLikeSource,
+)
+from skyllh.core.storage import (
+    NPYFileLoader,
 )
-from skyllh.core.session import is_interactive_session
-from skyllh.core.storage import NPYFileLoader
-from skyllh.physics.source import PointLikeSource
-
-from scipy.interpolate import interp1d
-from scipy.stats import gamma
-from iminuit import minimize
+from skyllh.core.utils.spline import (
+    make_spline_1d,
+)
+
 
 """This module contains common utility functions useful for an analysis.
 """
 
+
 def pointlikesource_to_data_field_array(
-        tdm, src_hypo_group_manager):
+        tdm, shg_mgr, pmm):
     """Function to transform a list of PointLikeSource sources into a numpy
     record ndarray. The resulting numpy record ndarray contains the following
     fields:
 
         `ra`: float
             The right-ascention of the point-like source.
         `dec`: float
             The declination of the point-like source.
-        `src_w`: float
-            The nomalized detector weight of the point-like source.
-        `src_w_grad`: float
-            The normalized weight gradient of the point-like source.
-        `src_w_W`: float
-            The nomalized hypothesis weight of the point-like source.
+        `weight`: float
+            The weight of the point-like source.
 
     Parameters
     ----------
     tdm : instance of TrialDataManager
         The TrialDataManager instance.
-    src_hypo_group_manager : instance of SourceHypoGroupManager
+    shg_mgr : instance of SourceHypoGroupManager
         The instance of SourceHypoGroupManager that defines the sources.
+    pmm : instance of ParameterModelMapper
+        The instance of ParameterModelMapper that defines the mapping of global
+        parameters to local model parameters.
 
     Returns
     -------
     arr : (N_sources,)-shaped numpy record ndarray
-        The numpy record ndarray holding the source parameters `ra` and `dec`.
+        The numpy record ndarray holding the source parameters.
     """
-    sources = src_hypo_group_manager.source_list
+    sources = shg_mgr.source_list
 
-    if(not issequenceof(sources, PointLikeSource)):
-        raise TypeError('The sources of the SourceHypoGroupManager must be '
+    if not issequenceof(sources, PointLikeSource):
+        raise TypeError(
+            'The sources of the SourceHypoGroupManager must be '
             'PointLikeSource instances!')
 
     arr = np.empty(
         (len(sources),),
-        dtype=[('ra', np.float64),
-               ('dec', np.float64),
-               ('src_w', np.float64),
-               ('src_w_grad', np.float64),
-               ('src_w_W', np.float64)]
-              , order='F')
+        dtype=[
+            ('ra', np.float64),
+            ('dec', np.float64),
+            ('weight', np.float64),
+        ],
+        order='F')
 
     for (i, src) in enumerate(sources):
-        arr['ra'][i]         = src.ra
-        arr['dec'][i]        = src.dec
-        arr['src_w'][i]      = src.weight.src_w
-        arr['src_w_grad'][i] = src.weight.src_w_grad
-        arr['src_w_W'][i]    = src.weight.src_w_W
+        arr['ra'][i] = src.ra
+        arr['dec'][i] = src.dec
+        arr['weight'][i] = src.weight
+
     return arr
 
 
 def calculate_pval_from_trials(
         ts_vals, ts_threshold, comp_operator='greater'):
     """Calculates the percentage (p-value) of test-statistic trials that are
     above the given test-statistic critical value.
@@ -112,16 +135,19 @@
         )
 
     p_sigma = np.sqrt(p * (1 - p) / ts_vals.size)
 
     return (p, p_sigma)
 
 
-def calculate_pval_from_gammafit_to_trials(ts_vals, ts_threshold,
-        eta=3.0, n_max=500000):
+def calculate_pval_from_gammafit_to_trials(
+        ts_vals,
+        ts_threshold,
+        eta=3.0,
+        n_max=500000):
     """Calculates the probability (p-value) of test-statistic exceeding
     the given test-statistic threshold. This calculation relies on fitting
     a gamma distribution to a list of ts values.
 
     Parameters
     ----------
     ts_vals : (n_trials,)-shaped 1D ndarray of float
@@ -135,32 +161,43 @@
         The maximum number of trials that should be used during
         fitting. Default = 500,000
 
     Returns
     -------
     p, p_sigma: tuple(float, float)
     """
-    if(ts_threshold < eta):
+    if not IMINUIT_LOADED:
+        raise ImportError(
+            'The iminuit module was not imported! '
+            'This module is a requirement for the function '
+            '"calculate_pval_from_gammafit_to_trials"!')
+
+    if ts_threshold < eta:
         raise ValueError(
             'ts threshold value = %e, eta = %e. The calculation of the p-value'
             'from the fit is correct only for ts threshold larger than '
             'the truncation threshold eta.',
             ts_threshold, eta)
 
     if len(ts_vals) > n_max:
         ts_vals = ts_vals[:n_max]
 
     Ntot = len(ts_vals)
     ts_eta = ts_vals[ts_vals > eta]
     N_prime = len(ts_eta)
     alpha = N_prime/Ntot
 
-    obj = lambda x: truncated_gamma_logpdf(x[0], x[1], eta=eta,
-                                           ts_above_eta=ts_eta,
-                                           N_above_eta=N_prime)
+    def obj(x):
+        return truncated_gamma_logpdf(
+            x[0],
+            x[1],
+            eta=eta,
+            ts_above_eta=ts_eta,
+            N_above_eta=N_prime)
+
     x0 = [0.75, 1.8]  # Initial values of function parameters.
     bounds = [[0.1, 10], [0.1, 10]]  # Ranges for the minimization fitter.
     r = minimize(obj, x0, bounds=bounds)
     pars = r.x
 
     norm = alpha/gamma.sf(eta, a=pars[0], scale=pars[1])
     p = norm * gamma.sf(ts_threshold, a=pars[0], scale=pars[1])
@@ -169,16 +206,21 @@
     # fitting uncertainty remains to be implemented
     # return p_sigma = 0 for now for consistentcy with
     # calculate_pval_from_trials()
     p_sigma = 0.0
     return (p, p_sigma)
 
 
-def calculate_pval_from_trials_mixed(ts_vals, ts_threshold, switch_at_ts=3.0,
-        eta=None, n_max=500000, comp_operator='greater_equal'):
+def calculate_pval_from_trials_mixed(
+        ts_vals,
+        ts_threshold,
+        switch_at_ts=3.0,
+        eta=None,
+        n_max=500000,
+        comp_operator='greater_equal'):
     """Calculates the probability (p-value) of test-statistic exceeding
     the given test-statistic threshold. This calculation relies on fitting
     a gamma distribution to a list of ts values if ts_threshold is larger than
     switch_at_ts. If ts_threshold is smaller then the pvalue will be taken
     from the trials directly.
 
     Parameters
@@ -207,21 +249,32 @@
     """
     # Set `eta` to `switch_at_ts` as a default.
     # It makes sure that both functions return the same pval at `switch_at_ts`.
     if eta is None:
         eta = switch_at_ts
 
     if ts_threshold < switch_at_ts:
-        return calculate_pval_from_trials(ts_vals, ts_threshold, comp_operator=comp_operator)
+        return calculate_pval_from_trials(
+            ts_vals,
+            ts_threshold,
+            comp_operator=comp_operator)
     else:
-        return calculate_pval_from_gammafit_to_trials(ts_vals, ts_threshold, eta=eta, n_max=n_max)
+        return calculate_pval_from_gammafit_to_trials(
+            ts_vals,
+            ts_threshold,
+            eta=eta,
+            n_max=n_max)
 
 
 def truncated_gamma_logpdf(
-        a, scale, eta, ts_above_eta, N_above_eta):
+        a,
+        scale,
+        eta,
+        ts_above_eta,
+        N_above_eta):
     """Calculates the -log(likelihood) of a sample of random numbers
     generated from a gamma pdf truncated from below at x=eta.
 
     Parameters
     ----------
     a : float
         Shape parameter.
@@ -239,20 +292,28 @@
 
     Returns
     -------
     -logl : float
     """
     c0 = 1. - gamma.cdf(eta, a=a, scale=scale)
     c0 = 1./c0
-    logl = N_above_eta*np.log(c0) + np.sum(gamma.logpdf(ts_above_eta,
-                                                        a=a, scale=scale))
+    logl = N_above_eta*np.log(c0)
+    logl += np.sum(
+        gamma.logpdf(
+            ts_above_eta,
+            a=a,
+            scale=scale))
+
     return -logl
 
+
 def calculate_critical_ts_from_gamma(
-        ts, h0_ts_quantile, eta=3.0):
+        ts,
+        h0_ts_quantile,
+        eta=3.0):
     """Calculates the critical test-statistic value corresponding
     to h0_ts_quantile by fitting the ts distribution with a truncated
     gamma function.
 
     Parameters
     ----------
     ts : (n_trials,)-shaped 1D ndarray
@@ -263,40 +324,57 @@
         Test-statistic value at which the gamma function is truncated
         from below.
 
     Returns
     -------
     critical_ts : float
     """
+    if not IMINUIT_LOADED:
+        raise ImportError(
+            'The iminuit module was not imported! '
+            'This module is a requirement of the function '
+            '"calculate_critical_ts_from_gamma"!')
+
     Ntot = len(ts)
     ts_eta = ts[ts > eta]
     N_prime = len(ts_eta)
     alpha = N_prime/Ntot
 
-    obj = lambda x: truncated_gamma_logpdf(x[0], x[1], eta=eta,
-                                           ts_above_eta=ts_eta,
-                                           N_above_eta=N_prime)
+    def obj(x):
+        return truncated_gamma_logpdf(
+            x[0],
+            x[1],
+            eta=eta,
+            ts_above_eta=ts_eta,
+            N_above_eta=N_prime)
+
     x0 = [0.75, 1.8]  # Initial values of function parameters.
     bounds = [[0.1, 10], [0.1, 10]]  # Ranges for the minimization fitter.
     r = minimize(obj, x0, bounds=bounds)
     pars = r.x
 
     norm = alpha/gamma.sf(eta, a=pars[0], scale=pars[1])
     critical_ts = gamma.ppf(1 - 1./norm*h0_ts_quantile, a=pars[0], scale=pars[1])
 
-    if(critical_ts < eta):
+    if critical_ts < eta:
         raise ValueError(
             'Critical ts value = %e, eta = %e. The calculation of the critical '
             'ts value from the fit is correct only for critical ts larger than '
             'the truncation threshold eta.',
             critical_ts, eta)
 
     return critical_ts
 
-def polynomial_fit(ns, p, p_weight, deg, p_thr):
+
+def polynomial_fit(
+        ns,
+        p,
+        p_weight,
+        deg,
+        p_thr):
     """Performs a polynomial fit on the p-values of test-statistic trials
     associated to each ns..
     Using the fitted parameters it computes the number of signal events
     correponding to the given p-value critical value.
 
     Parameters
     ----------
@@ -317,38 +395,50 @@
     ns : float
     """
     (params, cov) = np.polyfit(ns, p, deg, w=p_weight, cov=True)
 
     # Check if the second order coefficient is positive and eventually
     # change to a polynomial fit of order 1 to avoid to overestimate
     # the mean number of signal events for the chosen ts quantile.
-    if(deg == 2 and params[0] > 0):
+    if deg == 2 and params[0] > 0:
         deg = 1
         (params, cov) = np.polyfit(ns, p, deg, w=p_weight, cov=True)
 
-    if(deg == 1):
+    if deg == 1:
         (a, b) = (params[0], params[1])
         ns = (p_thr - b)/a
         return ns
 
-    elif(deg == 2):
+    elif deg == 2:
         (a, b, c) = (params[0], params[1], params[2])
         ns = (- b + np.sqrt((b**2)-4*a*(c-p_thr))) / (2*a)
         return ns
 
     else:
         raise ValueError(
             'deg = %g is not valid. The order of the polynomial function '
             'must be 1 or 2.',
             deg)
 
-def estimate_mean_nsignal_for_ts_quantile(
-        ana, rss, p, eps_p, mu_range, critical_ts=None, h0_trials=None,
-        h0_ts_quantile=None, min_dmu=0.5, bkg_kwargs=None, sig_kwargs=None,
-        ppbar=None, tl=None, pathfilename=None):
+
+def estimate_mean_nsignal_for_ts_quantile(  # noqa: C901
+        ana,
+        rss,
+        p,
+        eps_p,
+        mu_range,
+        critical_ts=None,
+        h0_trials=None,
+        h0_ts_quantile=None,
+        min_dmu=0.5,
+        bkg_kwargs=None,
+        sig_kwargs=None,
+        ppbar=None,
+        tl=None,
+        pathfilename=None):
     """Calculates the mean number of signal events needed to be injected to
     reach a test statistic distribution with defined properties for the given
     analysis.
 
     Parameters
     ----------
     ana : Analysis instance
@@ -405,60 +495,73 @@
     mu_err : None
         Error estimate needs to be implemented.
     """
     logger = logging.getLogger(__name__)
 
     n_total_generated_trials = 0
 
-    if(critical_ts is None) and (h0_ts_quantile is None):
+    if (critical_ts is None) and (h0_ts_quantile is None):
         raise RuntimeError(
             "Both the critical test-statistic value and the null-hypothesis "
             "test-statistic quantile are set to None. One of the two is "
             "needed to have the critical test-statistic value that defines "
-            "the type of test to run."
-        )
-    elif(critical_ts is None):
+            "the type of test to run.")
+    elif critical_ts is None:
         n_trials_max = int(5.e5)
         # Via binomial statistics, calcuate the minimum number of trials
         # needed to get the required precision on the critial TS value.
         eps = min(0.005, h0_ts_quantile/10)
         n_trials_min = int(h0_ts_quantile*(1-h0_ts_quantile)/eps**2 + 0.5)
 
         # Compute either n_trials_max or n_trials_min trials depending on
         # which one is smaller. If n_trials_max trials are computed, a
         # fit to the ts distribution is performed to get the critial TS.
         n_trials_total = min(n_trials_min, n_trials_max)
-        if(h0_trials is None):
+        if h0_trials is None:
             h0_ts_vals = ana.do_trials(
-                rss, n_trials_total, mean_n_sig=0, bkg_kwargs=bkg_kwargs,
-                sig_kwargs=sig_kwargs, ppbar=ppbar, tl=tl)['ts']
+                rss=rss,
+                n=n_trials_total,
+                mean_n_sig=0,
+                bkg_kwargs=bkg_kwargs,
+                sig_kwargs=sig_kwargs,
+                ppbar=ppbar,
+                tl=tl)['ts']
 
             logger.debug(
                 'Generate %d null-hypothesis trials',
                 n_trials_total)
             n_total_generated_trials += n_trials_total
 
-            if(pathfilename is not None):
+            if pathfilename is not None:
                 makedirs(os.path.dirname(pathfilename), exist_ok=True)
                 np.save(pathfilename, h0_ts_vals)
         else:
-            if(h0_trials.size < n_trials_total):
+            if h0_trials.size < n_trials_total:
                 if not ('seed' in h0_trials.dtype.names):
                     logger.debug(
                         'Uploaded trials miss the rss_seed field. '
                         'Will not be possible to extend the trial file '
                         'safely. Uploaded trials will *not* be used.')
                     n_trials = n_trials_total
                     h0_ts_vals = ana.do_trials(
-                        rss, n_trials, mean_n_sig=0, bkg_kwargs=bkg_kwargs,
-                        sig_kwargs=sig_kwargs, ppbar=ppbar, tl=tl)['ts']
+                        rss=rss,
+                        n=n_trials,
+                        mean_n_sig=0,
+                        bkg_kwargs=bkg_kwargs,
+                        sig_kwargs=sig_kwargs,
+                        ppbar=ppbar,
+                        tl=tl)['ts']
                 else:
                     n_trials = n_trials_total - h0_trials.size
-                    h0_ts_vals = extend_trial_data_file(ana, rss,
-                        n_trials, trial_data=h0_trials, mean_n_sig=0,
+                    h0_ts_vals = extend_trial_data_file(
+                        ana,
+                        rss,
+                        n_trials,
+                        trial_data=h0_trials,
+                        mean_n_sig=0,
                         pathfilename=pathfilename)['ts']
                 logger.debug(
                     'Generate %d null-hypothesis trials',
                     n_trials)
                 n_total_generated_trials += n_trials
             else:
                 h0_ts_vals = h0_trials['ts']
@@ -471,22 +574,22 @@
             'Min / Max h0 TS value: %e / %e',
             np.min(h0_ts_vals), np.max(h0_ts_vals))
 
         # If the minimum number of trials needed to get the required precision
         # on the critical TS value is smaller then 500k, compute the critical ts
         # value directly from trials; otherwise calculate it from the gamma
         # function fitted to the ts distribution.
-        if(n_trials_min <= n_trials_max):
+        if n_trials_min <= n_trials_max:
             c = np.percentile(h0_ts_vals, (1 - h0_ts_quantile)*100)
         else:
             c = calculate_critical_ts_from_gamma(h0_ts_vals, h0_ts_quantile)
         logger.debug(
             'Critical ts value for bkg ts quantile %g: %e',
             h0_ts_quantile, c)
-    elif(h0_ts_quantile is None):
+    elif h0_ts_quantile is None:
         # Make sure that the critical ts is a float.
         if not isinstance(critical_ts, float):
             raise TypeError(
                 "The critical test-statistic value must be a float, not "
                 f"{type(critical_ts)}!"
             )
         c = critical_ts
@@ -495,34 +598,32 @@
             c)
     else:
         raise RuntimeError(
             "Both a critical ts value and a null-hypothesis test_statistic "
             "quantile were given. If you want to use your critical_ts "
             "value, set h0_ts_quantile to None; if you want to compute the "
             "critical ts from the background distribution, set critical_ts "
-            "to None."
-        )
-
+            "to None.")
 
     # Make sure ns_range is mutable.
     ns_range_ = list(mu_range)
 
     ns_lower_bound = 0
     ns_upper_bound = +np.inf
 
     # The number of required trials per mu point for the desired uncertainty in
     # probability can be estimated via binomial statistics.
     n_trials = int(p*(1-p)/eps_p**2 + 0.5)
 
     # Define the range of p-values that will be possible to fit with a
     # polynomial function of order not larger than 2.
     min_fit_p, max_fit_p = p - 0.35, p + 0.35
-    if(min_fit_p < 0.5):
+    if min_fit_p < 0.5:
         min_fit_p = 0.5
-    if(max_fit_p > 0.985):
+    if max_fit_p > 0.985:
         max_fit_p = 0.985
 
     (n_sig, p_vals, p_val_weights) = ([], [], [])
 
     while True:
         ns_range_[0] = np.max([ns_range_[0], 0])
         logger.debug(
@@ -534,21 +635,25 @@
         # Generate statistics (trials) for the current point ns0 as long as
         # the we are only 5sigma away from the desired propability and the
         # uncertainty of the probability is still larger than the desired
         # uncertainty ``eps_p``.
         # Initially generate trials for a 5-times larger uncertainty ``eps_p``
         # to catch ns0 points far away from the desired propability quicker.
         dn_trials = max(100, int(n_trials/5**2 + 0.5))
-        trial_vals0 = None
         (ts_vals0, p0_sigma, delta_p) = ([], 2*eps_p, 0)
         while (delta_p < p0_sigma*5) and (p0_sigma > eps_p):
             ts_vals0 = np.concatenate((
                 ts_vals0, ana.do_trials(
-                    rss, dn_trials, mean_n_sig=ns0, bkg_kwargs=bkg_kwargs,
-                    sig_kwargs=sig_kwargs, ppbar=ppbar, tl=tl)['ts']))
+                    rss=rss,
+                    n=dn_trials,
+                    mean_n_sig=ns0,
+                    bkg_kwargs=bkg_kwargs,
+                    sig_kwargs=sig_kwargs,
+                    ppbar=ppbar,
+                    tl=tl)['ts']))
             (p0, p0_sigma) = calculate_pval_from_trials(ts_vals0, c)
 
             n_total_generated_trials += dn_trials
 
             delta_p = np.abs(p0 - p)
 
             logger.debug(
@@ -559,60 +664,65 @@
             # After the initial number of trials generated the number of trials
             # to generate, dn_trials, for the next iteration of trial generation
             # to decrease p0_sigma can be set to the number of remaining trials
             # to reach n_trials. But do at least 100 trials more, in case the
             # n_trials estimate was initially too low.
             dn_trials = max(100, n_trials - ts_vals0.size)
 
-            if((p0_sigma < eps_p) and (delta_p < eps_p)):
+            if (p0_sigma < eps_p) and (delta_p < eps_p):
                 # We found the ns0 value that corresponds to the desired
                 # probability within the desired uncertainty.
 
-                if(p0 < max_fit_p and p0 > min_fit_p):
+                if (p0 < max_fit_p) and (p0 > min_fit_p):
                     n_sig.append(ns0)
                     p_vals.append(p0)
                     p_val_weights.append(1. / p0_sigma)
 
                 logger.debug(
                     'Found mu value %g with p value %g within uncertainty +-%g',
                     ns0, p0, p0_sigma)
 
-                if(p0 > p):
+                if p0 > p:
                     ns1 = ns_range_[0]
-                    if(np.abs(ns0 - ns1) > 1.0):
+                    if np.abs(ns0 - ns1) > 1.0:
                         ns1 = ns0 - 1.0
-                    if(np.abs(ns0 - ns1) < min_dmu):
+                    if np.abs(ns0 - ns1) < min_dmu:
                         ns1 = ns0 - min_dmu
                 else:
                     ns1 = ns_range_[1]
-                    if(np.abs(ns0 - ns1) > 1.0):
+                    if np.abs(ns0 - ns1) > 1.0:
                         ns1 = ns0 + 1.0
-                    if(np.abs(ns0 - ns1) < min_dmu):
+                    if np.abs(ns0 - ns1) < min_dmu:
                         ns1 = ns0 + min_dmu
 
                 ts_vals1 = ana.do_trials(
-                    rss, ts_vals0.size, mean_n_sig=ns1, bkg_kwargs=bkg_kwargs,
-                    sig_kwargs=sig_kwargs, ppbar=ppbar, tl=tl)['ts']
+                    rss=rss,
+                    n=ts_vals0.size,
+                    mean_n_sig=ns1,
+                    bkg_kwargs=bkg_kwargs,
+                    sig_kwargs=sig_kwargs,
+                    ppbar=ppbar,
+                    tl=tl)['ts']
                 n_total_generated_trials += ts_vals0.size
 
                 (p1, p1_sigma) = calculate_pval_from_trials(ts_vals1, c)
                 logger.debug(
                     'Final mu value is supposed to be within mu range (%g,%g) '
                     'corresponding to p=(%g +-%g, %g +-%g)',
                     ns0, ns1, p0, p0_sigma, p1, p1_sigma)
 
-                if(p1 < max_fit_p and p1 > min_fit_p):
+                if (p1 < max_fit_p) and (p1 > min_fit_p):
                     n_sig.append(ns1)
                     p_vals.append(p1)
                     p_val_weights.append(1. / p1_sigma)
 
-                if(len(n_sig)>2):
+                if len(n_sig) > 2:
                     scanned_range = np.max(n_sig) - np.min(n_sig)
 
-                    if(len(n_sig) < 5 or scanned_range < 1.5):
+                    if (len(n_sig) < 5) or (scanned_range < 1.5):
                         deg = 1
                     else:
                         deg = 2
 
                     logger.debug(
                         'Scanned mu range: [%g , %g]\nPoints to fit: %g\n '
                         'Using polynomial fit of order %g',
@@ -627,155 +737,171 @@
                     # between those points.
 
                     logger.debug(
                         'Scanned mu range: [%g , %g]\nPoints to fit: %g\n '
                         'Doing a linear interpolation.',
                         np.min(n_sig), np.max(n_sig), len(n_sig))
 
-                    # Check if p1 and p0 are equal, which would result in a divison
-                    # by zero.
-                    if(p0 == p1):
+                    # Check if p1 and p0 are equal, which would result in a
+                    # divison by zero.
+                    if p0 == p1:
                         mu = 0.5*(ns0 + ns1)
                         mu_err = 0.5*np.abs(ns1 - ns0)
 
                         logger.debug(
-                            'Probability for mu=%g and mu=%g has the same value %g',
+                            'Probability for mu=%g and mu=%g has the same '
+                            'value %g',
                             ns0, ns1, p0)
                     else:
                         dns_dp = np.abs((ns1 - ns0) / (p1 - p0))
 
                         logger.debug(
                             'Estimated |dmu/dp| = %g within mu range (%g,%g) '
                             'corresponding to p=(%g +-%g, %g +-%g)',
                             dns_dp, ns0, ns1, p0, p0_sigma, p1, p1_sigma)
-                        if(p0 > p):
+                        if p0 > p:
                             mu = ns0 - dns_dp * delta_p
                         else:
                             mu = ns0 + dns_dp * delta_p
                         mu_err = dns_dp * delta_p
 
                 logger.debug(
                     'Estimated final mu to be %g (error estimation to be '
                     'implemented)',
                     mu)
 
                 return (mu, mu_err)
 
-        if(delta_p < p0_sigma*5):
+        if delta_p < p0_sigma*5:
             # The desired probability is within the 5 sigma region of the
             # current probability. So we use a linear approximation to find the
             # next ns range.
             # For the current ns0 the uncertainty of p0 is smaller than the
             # required uncertainty, hence p0_sigma <= eps_p.
 
             # Store ns0 for the new lower or upper bound depending on where the
             # p0 lies.
 
-            if(p0 < max_fit_p and p0 > min_fit_p):
+            if (p0 < max_fit_p) and (p0 > min_fit_p):
                 n_sig.append(ns0)
                 p_vals.append(p0)
                 p_val_weights.append(1. / p0_sigma)
 
-            if(p0+p0_sigma+eps_p <= p):
+            if p0+p0_sigma+eps_p <= p:
                 ns_lower_bound = ns0
-            elif(p0-p0_sigma-eps_p >= p):
+            elif p0-p0_sigma-eps_p >= p:
                 ns_upper_bound = ns0
 
             ns1 = ns0 * (1 - np.sign(p0 - p) * 0.05)
-            if(np.abs(ns0 - ns1) < min_dmu):
-                if((p0 - p) < 0):
+            if np.abs(ns0 - ns1) < min_dmu:
+                if (p0 - p) < 0:
                     ns1 = ns0 + min_dmu
                 else:
                     ns1 = ns0 - min_dmu
 
             logger.debug(
                 'Do interpolation between ns=(%.3f, %.3f)',
                 ns0, ns1)
 
             ts_vals1 = ana.do_trials(
-                rss, ts_vals0.size, mean_n_sig=ns1, bkg_kwargs=bkg_kwargs,
-                sig_kwargs=sig_kwargs, ppbar=ppbar, tl=tl)['ts']
+                rss=rss,
+                n=ts_vals0.size,
+                mean_n_sig=ns1,
+                bkg_kwargs=bkg_kwargs,
+                sig_kwargs=sig_kwargs,
+                ppbar=ppbar,
+                tl=tl)['ts']
             n_total_generated_trials += ts_vals0.size
 
             (p1, p1_sigma) = calculate_pval_from_trials(ts_vals1, c)
 
-            if(p1 < max_fit_p and p1 > min_fit_p):
+            if (p1 < max_fit_p) and (p1 > min_fit_p):
                 n_sig.append(ns1)
                 p_vals.append(p1)
                 p_val_weights.append(1. / p1_sigma)
 
             # Check if p0 and p1 are equal, which would result into a division
             # by zero.
-            if(p0 == p1):
+            if p0 == p1:
                 dp = 0.5*(p0_sigma + p1_sigma)
                 logger.debug(
                     'p1 and p0 are equal to %g, causing division by zero. '
                     'p0_sigma=%g, p1_sigma=%g. Calculating dns/dp with dp=%g.',
                     p0, p0_sigma, p1_sigma, dp)
                 dns_dp = np.abs((ns1 - ns0) / dp)
             else:
                 dns_dp = np.abs((ns1 - ns0) / (p1 - p0))
                 # p0 and p1 might be very similar, resulting into a numerically
                 # infitite slope.
-                if(np.isinf(dns_dp)):
+                if np.isinf(dns_dp):
                     dp = 0.5*(p0_sigma + p1_sigma)
                     logger.debug(
                         'Infinite dns/dp dedected: ns0=%g, ns1=%g, p0=%g, '
                         'p0_sigma=%g, p1=%g, p1_sigma=%g. Recalculating dns/dp '
                         'with deviation %g.',
                         ns0, ns1, p0, p0_sigma, p1, p1_sigma, dp)
                     dns_dp = np.abs((ns1 - ns0) / dp)
             logger.debug('dns/dp = %g', dns_dp)
 
-            if(p0 > p):
+            if p0 > p:
                 ns_range_[0] = ns0 - dns_dp * (delta_p + p0_sigma)
                 ns_range_[1] = ns0 + dns_dp * p0_sigma
             else:
                 ns_range_[0] = ns0 - dns_dp * p0_sigma
                 ns_range_[1] = ns0 + dns_dp * (delta_p + p0_sigma)
 
             # Restrict the range to ns values we already know well.
             ns_range_[0] = np.max((ns_range_[0], ns_lower_bound))
             ns_range_[1] = np.min((ns_range_[1], ns_upper_bound))
 
             # In case the new calculated mu range is smaller than the minimum
             # delta mu, the mu range gets widened by half of the minimum delta
             # mu on both sides.
-            if(np.abs(ns_range_[1] - ns_range_[0]) < min_dmu):
+            if np.abs(ns_range_[1] - ns_range_[0]) < min_dmu:
                 ns_range_[0] -= 0.5*min_dmu
                 ns_range_[1] += 0.5*min_dmu
         else:
             # The current ns corresponds to a probability p0 that is at least
             # 5 sigma away from the desired probability p, hence
             # delta_p >= p0_sigma*5.
 
-            if(p0 < max_fit_p and p0 > min_fit_p):
+            if (p0 < max_fit_p) and (p0 > min_fit_p):
                 n_sig.append(ns0)
                 p_vals.append(p0)
                 p_val_weights.append(1. / p0_sigma)
 
-            if(p0 < p):
+            if p0 < p:
                 ns_range_[0] = ns0
             else:
                 ns_range_[1] = ns0
 
-            if(np.abs(ns_range_[1] - ns_range_[0]) < min_dmu):
+            if np.abs(ns_range_[1] - ns_range_[0]) < min_dmu:
                 # The mu range became smaller than the minimum delta mu and
                 # still beeing far away from the desired probability.
                 # So move the mu range towards the desired probability.
-                if(p0 < p):
+                if p0 < p:
                     ns_range_[1] += 10*min_dmu
                 else:
                     ns_range_[0] -= 10*min_dmu
 
 
 def estimate_sensitivity(
-        ana, rss, h0_trials=None, h0_ts_quantile=0.5, p=0.9, eps_p=0.005,
-        mu_range=None, min_dmu=0.5, bkg_kwargs=None, sig_kwargs=None,
-        ppbar=None, tl=None, pathfilename=None):
+        ana,
+        rss,
+        h0_trials=None,
+        h0_ts_quantile=0.5,
+        p=0.9,
+        eps_p=0.005,
+        mu_range=None,
+        min_dmu=0.5,
+        bkg_kwargs=None,
+        sig_kwargs=None,
+        ppbar=None,
+        tl=None,
+        pathfilename=None):
     """Estimates the mean number of signal events that whould have to be
     injected into the data such that the test-statistic value of p*100% of all
     trials are larger than the critical test-statistic value c, which
     corresponds to the test-statistic value where h0_ts_quantile*100% of all
     null hypothesis test-statistic values are larger than c.
 
     For sensitivity h0_ts_quantile, and p are usually set to 0.5, and 0.9,
@@ -829,15 +955,15 @@
     Returns
     -------
     mu : float
         Estimated median number of signal events to reach desired sensitivity.
     mu_err : float
         The uncertainty of the estimated mean number of signal events.
     """
-    if(mu_range is None):
+    if mu_range is None:
         mu_range = (0, 10)
 
     (mu, mu_err) = estimate_mean_nsignal_for_ts_quantile(
         ana=ana,
         rss=rss,
         h0_trials=h0_trials,
         h0_ts_quantile=h0_ts_quantile,
@@ -851,17 +977,27 @@
         tl=tl,
         pathfilename=pathfilename)
 
     return (mu, mu_err)
 
 
 def estimate_discovery_potential(
-        ana, rss, h0_trials=None, h0_ts_quantile=2.8665e-7, p=0.5, eps_p=0.005,
-        mu_range=None, min_dmu=0.5, bkg_kwargs=None, sig_kwargs=None,
-        ppbar=None, tl=None, pathfilename=None):
+        ana,
+        rss,
+        h0_trials=None,
+        h0_ts_quantile=2.8665e-7,
+        p=0.5,
+        eps_p=0.005,
+        mu_range=None,
+        min_dmu=0.5,
+        bkg_kwargs=None,
+        sig_kwargs=None,
+        ppbar=None,
+        tl=None,
+        pathfilename=None):
     """Estimates the mean number of signal events that whould have to be
     injected into the data such that the test-statistic value of p*100% of all
     trials are larger than the critical test-statistic value c, which
     corresponds to the test-statistic value where h0_ts_quantile*100% of all
     null hypothesis test-statistic values are larger than c.
 
     For the 5 sigma discovery potential `h0_ts_quantile`, and `p` are usually
@@ -915,37 +1051,48 @@
     -------
     mu : float
         Estimated mean number of injected signal events to reach the desired
         discovery potential.
     mu_err : float
         Estimated error of `mu`.
     """
-    if(mu_range is None):
+    if mu_range is None:
         mu_range = (0, 10)
 
     (mu, mu_err) = estimate_mean_nsignal_for_ts_quantile(
         ana=ana,
         rss=rss,
-        h0_trials=h0_trials,
-        h0_ts_quantile=h0_ts_quantile,
         p=p,
         eps_p=eps_p,
         mu_range=mu_range,
+        h0_trials=h0_trials,
+        h0_ts_quantile=h0_ts_quantile,
+        min_dmu=min_dmu,
         bkg_kwargs=bkg_kwargs,
         sig_kwargs=sig_kwargs,
         ppbar=ppbar,
         tl=tl,
         pathfilename=pathfilename)
 
     return (mu, mu_err)
 
 
 def generate_mu_of_p_spline_interpolation(
-        ana, rss, h0_ts_vals, h0_ts_quantile, eps_p, mu_range, mu_step,
-        kind='cubic', bkg_kwargs=None, sig_kwargs=None, ppbar=None, tl=None):
+        ana,
+        rss,
+        h0_ts_vals,
+        h0_ts_quantile,
+        eps_p,
+        mu_range,
+        mu_step,
+        kind='cubic',
+        bkg_kwargs=None,
+        sig_kwargs=None,
+        ppbar=None,
+        tl=None):
     """Generates a spline interpolation for mu(p) function for a pre-defined
     range of mu, where mu is the mean number of injected signal events and p the
     probability for the ts value larger than the ts value corresponding to the
     given quantile, h0_ts_quantile, of the null hypothesis test-statistic
     distribution.
 
     Parameters
@@ -993,20 +1140,25 @@
     spline : callable
         The spline function mu(p).
     """
     logger = logging.getLogger(__name__)
 
     n_total_generated_trials = 0
 
-    if(h0_ts_vals is None):
+    if h0_ts_vals is None:
         n_bkg = int(100/(1 - h0_ts_quantile))
         logger.debug('Generate %d null-hypothesis trials', n_bkg)
         h0_ts_vals = ana.do_trials(
-            rss, n_bkg, mean_n_sig=0, bkg_kwargs=bkg_kwargs,
-            sig_kwargs=sig_kwargs, ppbar=ppbar, tl=tl)['ts']
+            rss=rss,
+            n=n_bkg,
+            mean_n_sig=0,
+            bkg_kwargs=bkg_kwargs,
+            sig_kwargs=sig_kwargs,
+            ppbar=ppbar,
+            tl=tl)['ts']
         n_total_generated_trials += n_bkg
 
     n_h0_ts_vals = len(h0_ts_vals)
     h0_ts_vals = h0_ts_vals[np.isfinite(h0_ts_vals)]
     logger.debug(
         'Number of trials after finite cut: %d (%g%% of total)',
         len(h0_ts_vals), (len(h0_ts_vals)/n_h0_ts_vals)*100)
@@ -1022,83 +1174,91 @@
 
     logger.debug(
         'Generate trials for %d mu values',
         n_mu)
 
     # Create the progress bar if we are in an interactive session.
     pbar = None
-    if(is_interactive_session()):
+    if is_interactive_session():
         pbar = ProgressBar(len(mu_vals), parent=ppbar).start()
 
-    for (idx,mu) in enumerate(mu_vals):
+    for (idx, mu) in enumerate(mu_vals):
         p = None
         (ts_vals, p_sigma) = ([], 2*eps_p)
         while (p_sigma > eps_p):
             ts_vals = np.concatenate(
                 (ts_vals,
                  ana.do_trials(
-                     rss, 100, mean_n_sig=mu, bkg_kwargs=bkg_kwargs,
-                     sig_kwargs=sig_kwargs, ppbar=pbar, tl=tl)['ts']))
+                     rss=rss,
+                     n=100,
+                     mean_n_sig=mu,
+                     bkg_kwargs=bkg_kwargs,
+                     sig_kwargs=sig_kwargs,
+                     ppbar=pbar,
+                     tl=tl)['ts']))
             (p, p_sigma) = calculate_pval_from_trials(ts_vals, c)
             n_total_generated_trials += 100
         logger.debug(
             'mu: %g, n_trials: %d, p: %g, p_sigma: %g',
             mu, ts_vals.size, p, p_sigma)
         p_vals[idx] = p
 
-        if(pbar is not None):
+        if pbar is not None:
             pbar.increment()
 
     # Make a mu(p) spline via interp1d.
-    # The interp1d function requires unique x values. So we need to sort the
-    # p_vals in increasing order and mask out repeating p values.
-    p_mu_vals = np.array(sorted(zip(p_vals, mu_vals)), dtype=np.float64)
-    p_vals = p_mu_vals[:,0]
-    unique_pval_mask = np.concatenate(([True], np.invert(
-        p_vals[1:] <= p_vals[:-1])))
-    p_vals = p_vals[unique_pval_mask]
-    mu_vals = p_mu_vals[:,1][unique_pval_mask]
-
-    spline = interp1d(p_vals, mu_vals, kind=kind, copy=False,
+    spline = make_spline_1d(
+        p_vals,
+        mu_vals,
+        kind=kind,
+        copy=False,
         assume_sorted=True)
 
-    if(pbar is not None):
+    if pbar is not None:
         pbar.finish()
 
     return spline
 
 
-def create_trial_data_file(
-        ana, rss, n_trials, mean_n_sig=0, mean_n_sig_null=0,
-        mean_n_bkg_list=None, bkg_kwargs=None, sig_kwargs=None,
-        pathfilename=None, ncpu=None, ppbar=None, tl=None):
+def create_trial_data_file(  # noqa: C901
+        ana,
+        rss,
+        n_trials,
+        mean_n_sig=0,
+        mean_n_sig_null=0,
+        mean_n_bkg_list=None,
+        bkg_kwargs=None,
+        sig_kwargs=None,
+        pathfilename=None,
+        ncpu=None,
+        ppbar=None,
+        tl=None):
     """Creates and fills a trial data file with `n_trials` generated trials for
     each mean number of injected signal events from `ns_min` up to `ns_max` for
     a given analysis.
 
     Parameters
     ----------
     ana : instance of Analysis
         The Analysis instance to use for the trial generation.
-    rss : RandomStateService
+    rss : instance of RandomStateService
         The RandomStateService instance to use for generating random
         numbers.
     n_trials : int
         The number of trials to perform for each hypothesis test.
     mean_n_sig : ndarray of float | float | 2- or 3-element sequence of float
         The array of mean number of injected signal events (MNOISEs) for which
         to generate trials. If this argument is not a ndarray, an array of
         MNOISEs is generated based on this argument.
         If a single float is given, only this given MNOISEs are injected.
         If a 2-element sequence of floats is given, it specifies the range of
         MNOISEs with a step size of one.
         If a 3-element sequence of floats is given, it specifies the range plus
         the step size of the MNOISEs.
-    mean_n_sig_null : ndarray of float | float | 2- or 3-element sequence of
-                      float
+    mean_n_sig_null : ndarray of float | float | 2- or 3-element sequence of float
         The array of the fixed mean number of signal events (FMNOSEs) for the
         null-hypothesis for which to generate trials. If this argument is not a
         ndarray, an array of FMNOSEs is generated based on this argument.
         If a single float is given, only this given FMNOSEs are used.
         If a 2-element sequence of floats is given, it specifies the range of
         FMNOSEs with a step size of one.
         If a 3-element sequence of floats is given, it specifies the range plus
@@ -1136,105 +1296,126 @@
         trials.
     mean_n_sig_null : 1d ndarray
         The array holding the fixed mean number of signal events for the
         null-hypothesis used to generate the trials.
     trial_data : structured numpy ndarray
         The generated trial data.
     """
-    n_trials = int_cast(n_trials,
+    n_trials = int_cast(
+        n_trials,
         'The n_trials argument must be castable to type int!')
 
-    if(not isinstance(mean_n_sig, np.ndarray)):
-        if(not issequence(mean_n_sig)):
-            mean_n_sig = float_cast(mean_n_sig,
+    if not isinstance(mean_n_sig, np.ndarray):
+        if not issequence(mean_n_sig):
+            mean_n_sig = float_cast(
+                mean_n_sig,
                 'The mean_n_sig argument must be castable to type float!')
             mean_n_sig_min = mean_n_sig
             mean_n_sig_max = mean_n_sig
             mean_n_sig_step = 1
         else:
-            mean_n_sig = float_cast(mean_n_sig,
+            mean_n_sig = float_cast(
+                mean_n_sig,
                 'The sequence elements of the mean_n_sig argument must be '
                 'castable to float values!')
-            if(len(mean_n_sig) == 2):
+            if len(mean_n_sig) == 2:
                 (mean_n_sig_min, mean_n_sig_max) = mean_n_sig
                 mean_n_sig_step = 1
-            elif(len(mean_n_sig) == 3):
+            elif len(mean_n_sig) == 3:
                 (mean_n_sig_min, mean_n_sig_max, mean_n_sig_step) = mean_n_sig
 
         mean_n_sig = np.arange(
             mean_n_sig_min, mean_n_sig_max+1, mean_n_sig_step,
             dtype=np.float64)
 
-    if(not isinstance(mean_n_sig_null, np.ndarray)):
-        if(not issequence(mean_n_sig_null)):
-            mean_n_sig_null = float_cast(mean_n_sig_null,
+    if not isinstance(mean_n_sig_null, np.ndarray):
+        if not issequence(mean_n_sig_null):
+            mean_n_sig_null = float_cast(
+                mean_n_sig_null,
                 'The mean_n_sig_null argument must be castable to type float!')
             mean_n_sig_null_min = mean_n_sig_null
             mean_n_sig_null_max = mean_n_sig_null
             mean_n_sig_null_step = 1
         else:
-            mean_n_sig_null = float_cast(mean_n_sig_null,
+            mean_n_sig_null = float_cast(
+                mean_n_sig_null,
                 'The sequence elements of the mean_n_sig_null argument must '
                 'be castable to float values!')
-            if(len(mean_n_sig_null) == 2):
+            if len(mean_n_sig_null) == 2:
                 (mean_n_sig_null_min, mean_n_sig_null_max) = mean_n_sig_null
                 mean_n_sig_null_step = 1
-            elif(len(mean_n_sig_null) == 3):
+            elif len(mean_n_sig_null) == 3:
                 (mean_n_sig_null_min, mean_n_sig_null_max,
                  mean_n_sig_null_step) = mean_n_sig_null
 
         mean_n_sig_null = np.arange(
             mean_n_sig_null_min, mean_n_sig_null_max+1, mean_n_sig_null_step,
             dtype=np.float64)
 
     pbar = ProgressBar(
         len(mean_n_sig)*len(mean_n_sig_null), parent=ppbar).start()
     trial_data = None
     for (mean_n_sig_, mean_n_sig_null_) in itertools.product(
             mean_n_sig, mean_n_sig_null):
 
         trials = ana.do_trials(
-            rss, n=n_trials, mean_n_bkg_list=mean_n_bkg_list,
-            mean_n_sig=mean_n_sig_, mean_n_sig_0=mean_n_sig_null_,
-            bkg_kwargs=bkg_kwargs, sig_kwargs=sig_kwargs, ncpu=ncpu, tl=tl,
+            rss=rss,
+            n=n_trials,
+            mean_n_bkg_list=mean_n_bkg_list,
+            mean_n_sig=mean_n_sig_,
+            mean_n_sig_0=mean_n_sig_null_,
+            bkg_kwargs=bkg_kwargs,
+            sig_kwargs=sig_kwargs,
+            ncpu=ncpu,
+            tl=tl,
             ppbar=pbar)
 
-        if(trial_data is None):
+        if trial_data is None:
             trial_data = trials
         else:
             trial_data = np_rfn.stack_arrays(
-                [trial_data, trials], usemask=False, asrecarray=True)
+                [trial_data, trials],
+                usemask=False,
+                asrecarray=True)
 
         pbar.increment()
     pbar.finish()
 
-    if(trial_data is None):
-        raise RuntimeError('No trials have been generated! Check your '
-            'generation boundaries!')
+    if trial_data is None:
+        raise RuntimeError(
+            'No trials have been generated! Check your generation boundaries!')
 
-    if(pathfilename is not None):
+    if pathfilename is not None:
         # Save the trial data to file.
         makedirs(os.path.dirname(pathfilename), exist_ok=True)
         np.save(pathfilename, trial_data)
 
     return (rss.seed, mean_n_sig, mean_n_sig_null, trial_data)
 
 
 def extend_trial_data_file(
-        ana, rss, n_trials, trial_data, mean_n_sig=0, mean_n_sig_null=0,
-        mean_n_bkg_list=None, bkg_kwargs=None, sig_kwargs=None,
-        pathfilename=None, **kwargs):
+        ana,
+        rss,
+        n_trials,
+        trial_data,
+        mean_n_sig=0,
+        mean_n_sig_null=0,
+        mean_n_bkg_list=None,
+        bkg_kwargs=None,
+        sig_kwargs=None,
+        pathfilename=None,
+        **kwargs):
     """Appends to the trial data file `n_trials` generated trials for each
     mean number of injected signal events up to `ns_max` for a given analysis.
 
     Parameters
     ----------
-    ana : Analysis
+    ana : instance of Analysis
         The Analysis instance to use for sensitivity estimation.
-    rss : RandomStateService
+    rss : instance of RandomStateService
         The RandomStateService instance to use for generating random
         numbers.
     n_trials : int
         The number of trials the trial data file needs to be extended by.
     trial_data : structured numpy ndarray
         The structured numpy ndarray holding the trials.
     mean_n_sig : ndarray of float | float | 2- or 3-element sequence of float
@@ -1242,16 +1423,15 @@
         to generate trials. If this argument is not a ndarray, an array of
         MNOISEs is generated based on this argument.
         If a single float is given, only this given MNOISEs are injected.
         If a 2-element sequence of floats is given, it specifies the range of
         MNOISEs with a step size of one.
         If a 3-element sequence of floats is given, it specifies the range plus
         the step size of the MNOISEs.
-    mean_n_sig_null : ndarray of float | float | 2- or 3-element sequence of
-                      float
+    mean_n_sig_null : ndarray of float | float | 2- or 3-element sequence of float
         The array of the fixed mean number of signal events (FMNOSEs) for the
         null-hypothesis for which to generate trials. If this argument is not a
         ndarray, an array of FMNOSEs is generated based on this argument.
         If a single float is given, only this given FMNOSEs are used.
         If a 2-element sequence of floats is given, it specifies the range of
         FMNOSEs with a step size of one.
         If a 3-element sequence of floats is given, it specifies the range plus
@@ -1276,98 +1456,112 @@
     -------
     trial_data :
         Trial data file extended by the required number of trials for each
         mean number of injected signal events..
     """
     # Use unique seed to generate non identical trials.
     if rss.seed in trial_data['seed']:
-        seed = next(i for i, e in
-                    enumerate(sorted(np.unique(trial_data['seed'])) +
-                                [None], 1) if i != e)
+        seed = next(
+            i
+            for (i, e) in enumerate(
+                sorted(np.unique(trial_data['seed'])) + [None], 1)
+            if i != e)
         rss.reseed(seed)
 
     (seed, mean_n_sig, mean_n_sig_null, trials) = create_trial_data_file(
         ana=ana,
         rss=rss,
         n_trials=n_trials,
         mean_n_sig=mean_n_sig,
+        mean_n_sig_null=mean_n_sig_null,
+        mean_n_bkg_list=mean_n_bkg_list,
+        bkg_kwargs=bkg_kwargs,
+        sig_kwargs=sig_kwargs,
         **kwargs
     )
     trial_data = np_rfn.stack_arrays(
         [trial_data, trials],
         usemask=False,
         asrecarray=True)
 
-    if(pathfilename is not None):
+    if pathfilename is not None:
         # Save the trial data to file.
         makedirs(os.path.dirname(pathfilename), exist_ok=True)
         np.save(pathfilename, trial_data)
 
     return trial_data
 
+
 def calculate_upper_limit_distribution(
-        analysis, rss, pathfilename, N_bkg=5000, n_bins=100):
+        ana,
+        rss,
+        pathfilename,
+        n_bkg=5000,
+        n_bins=100):
     """Function to calculate upper limit distribution. It loads the trial data
     file containing test statistic distribution and calculates 10 percentile
     value for each mean number of injected signal event. Then it finds upper
     limit values which correspond to generated background trials test statistic
     values by linearly interpolated curve of 10 percentile values distribution.
 
     Parameters
     ----------
-    analysis : Analysis
+    ana : instance of Analysis
         The Analysis instance to use for sensitivity estimation.
-    rss : RandomStateService
+    rss : instance of RandomStateService
         The RandomStateService instance to use for generating random
         numbers.
     pathfilename : string
         Trial data file path including the filename.
-    N_bkg : int, optional
+    n_bkg : int, optional
         Number of times to perform background analysis trial.
     n_bins : int, optional
         Number of returned test statistic histograms bins.
 
     Returns
     -------
     result : dict
         Result dictionary which contains the following fields:
 
-        - ul : list of float
+        ul : list of float
             List of upper limit values.
-        - mean : float
+        mean : float
             Mean of upper limit values.
-        - median : float
+        median : float
             Median of upper limit values.
-        - var : float
+        var : float
             Variance of upper limit values.
-        - ts_hist : numpy ndarray
+        ts_hist : numpy ndarray
             2D array of test statistic histograms calculated by axis 1.
-        - extent : list of float
+        extent : list of float
             Test statistic histogram boundaries.
-        - q_values : list of float
+        q_values : list of float
             `q` percentile values of test statistic for different injected
             events means.
     """
     # Load trial data file.
     trial_data = NPYFileLoader(pathfilename).load_data()
     ns_max = max(trial_data['sig_mean']) + 1
     ts_bins_range = (min(trial_data['TS']), max(trial_data['TS']))
 
-    q = 10 # Upper limit criterion.
+    q = 10  # Upper limit criterion.
     trial_data_q_values = np.empty((ns_max,))
     trial_data_ts_hist = np.empty((ns_max, n_bins))
     for ns in range(ns_max):
         trial_data_q_values[ns] = np.percentile(
             trial_data['TS'][trial_data['sig_mean'] == ns], q)
         (trial_data_ts_hist[ns, :], bin_edges) = np.histogram(
             trial_data['TS'][trial_data['sig_mean'] == ns],
             bins=n_bins, range=ts_bins_range)
 
     ts_inv_f = interp1d(trial_data_q_values, range(ns_max), kind='linear')
-    ts_bkg = analysis.do_trials(rss, N_bkg, sig_mean=0)['TS']
+    ts_bkg = ana.do_trials(
+        rss=rss,
+        n=n_bkg,
+        mean_n_sig=0)['TS']
 
     # Cut away lower background test statistic values than the minimal
     # `ts_inv_f` interpolation boundary.
     ts_bkg = ts_bkg[ts_bkg >= min(trial_data_q_values)]
 
     ul_list = map(ts_inv_f, ts_bkg)
     ul_mean = np.mean(ul_list)
```

### Comparing `skyllh-23.1.1/skyllh/core/binning.py` & `skyllh-23.2.0/skyllh/core/binning.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # -*- coding: utf-8 -*-
 
 import numpy as np
 
-from scipy.linalg import solve
-
-from skyllh.core.py import classname
+from skyllh.core.py import (
+    classname,
+)
 
 
 def get_bincenters_from_binedges(edges):
     """Calculates the bin center values from the given bin edge values.
 
     Parameters
     ----------
@@ -18,14 +18,15 @@
     Returns
     -------
     bincenters : 1D numpy ndarray
         The (n,)-shaped 1D ndarray holding the bin center values.
     """
     return 0.5*(edges[:-1] + edges[1:])
 
+
 def get_binedges_from_bincenters(centers):
     """Calculates the bin edges from the given bin center values. The bin center
     values must be evenly spaced.
 
     Parameters
     ----------
     centers : 1D numpy ndarray
@@ -43,14 +44,15 @@
 
     edges = np.zeros((len(centers)+1,), dtype=np.double)
     edges[:-1] = centers - d/2
     edges[-1] = centers[-1] + d/2
 
     return edges
 
+
 def get_bin_indices_from_lower_and_upper_binedges(le, ue, values):
     """Returns the bin indices for the given values which must fall into bins
     defined by the given lower and upper bin edges.
 
     Note: The upper edge is not included in the bin.
 
     Parameters
@@ -82,75 +84,85 @@
         invalid_values = values[values >= ue[-1]]
         raise ValueError(
             '{} values ({}) are larger or equal than the largest bin edge '
             '({})!'.format(
                 len(invalid_values), str(invalid_values), ue[-1]))
 
     m = (
-        (values[:,np.newaxis] >= le[np.newaxis,:]) &
-        (values[:,np.newaxis] < ue[np.newaxis,:])
+        (values[:, np.newaxis] >= le[np.newaxis, :]) &
+        (values[:, np.newaxis] < ue[np.newaxis, :])
     )
     idxs = np.nonzero(m)[1]
 
     return idxs
 
 
 class BinningDefinition(object):
     """The BinningDefinition class provides a structure to hold histogram
     binning definitions for an analyis.
     """
-    def __init__(self, name, binedges):
+    def __init__(
+            self,
+            name,
+            binedges):
         """Creates a new binning definition object.
 
         Parameters
         ----------
         name : str
             The name of the binning definition.
-        binedges : sequence
+        binedges : sequence of float
             The sequence of the bin edges, which should be used for the binning.
         """
         self.name = name
         self.binedges = binedges
 
     def __str__(self):
         """Pretty string representation.
         """
-        s = '%s: %s\n'%(classname(self), self._name)
+        s = f'{classname(self)}: {self._name}\n'
         s += str(self._binedges)
         return s
 
     def __eq__(self, other):
         """Checks if object ``other`` is equal to this BinningDefinition object.
         """
-        if(not isinstance(other, BinningDefinition)):
-            raise TypeError('The other object in the equal comparison must be '
-                'an instance of BinningDefinition!')
-        if(self.name != other.name):
+        if not isinstance(other, BinningDefinition):
+            raise TypeError(
+                'The other object in the equal comparison must be an instance '
+                'of BinningDefinition! '
+                f'Its current type is {classname(other)}.')
+        if self.name != other.name:
             return False
-        if(np.any(self.binedges != other.binedges)):
+        if np.any(self.binedges != other.binedges):
             return False
+
         return True
 
     @property
     def name(self):
         """The name of the binning setting. This must be an unique name
         for all the different binning settings used within a season.
         """
         return self._name
+
     @name.setter
     def name(self, name):
-        if(not isinstance(name, str)):
-            raise TypeError("The name must be of type 'str'!")
+        if not isinstance(name, str):
+            raise TypeError(
+                'The name must be of type str! '
+                f'Its current type is {classname(name)}.')
         self._name = name
 
     @property
     def binedges(self):
         """The numpy.ndarray holding the bin edges.
         """
         return self._binedges
+
     @binedges.setter
     def binedges(self, arr):
         arr = np.atleast_1d(arr)
         self._binedges = np.array(arr, dtype=np.float64)
 
     @property
     def nbins(self):
@@ -184,21 +196,22 @@
 
     @property
     def range(self):
         """The tuple (lower_edge, upper_edge) of the binning.
         """
         return (self.lower_edge, self.upper_edge)
 
-    def any_data_out_of_binning_range(self, data):
-        """Checks if any of the given data is outside of the binning range.
+    def any_data_out_of_range(self, data):
+        """Checks if any of the given data is outside the range of this binning
+        definition.
 
         Parameters
         ----------
-        data : 1d ndarray
-            The array with the data values to check.
+        data : instance of ndarray
+            The 1D ndarray with the data values to check.
 
         Returns
         -------
         outofrange : bool
             True if any data value is outside the binning range.
             False otherwise.
         """
@@ -211,43 +224,66 @@
         """
         idx = np.digitize(value, self._binedges) - 1
 
         bin_width = self.binwidths[idx]
 
         return bin_width
 
+    def get_out_of_range_data(self, data):
+        """Returns the data values which are outside the range of this binning
+        definition.
+
+        Parameters
+        ----------
+        data : instance of numpy.ndarray
+            The 1D ndarray with the data values to check.
+
+        Returns
+        -------
+        oor_data : instance of numpy.ndarray
+            The 1D ndarray with data outside the range of this binning
+            definition.
+        """
+        oor_mask = (
+            (data < self.lower_edge) |
+            (data > self.upper_edge)
+        )
+        oor_data = data[oor_mask]
+
+        return oor_data
+
     def get_subset(self, lower_edge, upper_edge):
         """Creates a new BinningDefinition instance which contains only a subset
         of the bins of this BinningDefinition instance. The range of the subset
         is given by a lower and upper edge value.
 
         Parameters
         ----------
         lower_edge : float
             The lower edge value of the subset.
         upper_edge : float
             The upper edge value of the subset.
 
         Returns
         -------
-        new_binning : BinningDefinition instance
-            The new BinningDefinition instance holding the binning subset.
+        binning : instance of BinningDefinition
+            The new instance of BinningDefinition holding the binning subset.
         """
 
         idxs = np.indices((len(self._binedges),))[0]
         m = (self._binedges >= lower_edge) & (self._binedges <= upper_edge)
 
         idx_lower = np.min(idxs[m])
         # Include the lower edge of the bin the lower_edge value falls into.
-        if(self._binedges[idx_lower] > lower_edge):
+        if self._binedges[idx_lower] > lower_edge:
             idx_lower -= 1
 
         idx_upper = np.max(idxs[m])
         # Include the upper edge of the bin the upper_edge value falls into.
-        if(self._binedges[idx_upper] < upper_edge):
+        if self._binedges[idx_upper] < upper_edge:
             idx_upper += 1
 
         new_binedges = self._binedges[idx_lower:idx_upper+1]
 
         return BinningDefinition(self._name, new_binedges)
 
 
@@ -258,16 +294,15 @@
     This class defines the property ``binnings``, which is a list of
     BinningDefinition objects.
 
     This class provides the method ``has_same_binning_as(obj)`` to determine if
     a given object (that also uses binning) has the same binning.
     """
     def __init__(self, *args, **kwargs):
-        # Make sure that multiple inheritance can be used.
-        super(UsesBinning, self).__init__(*args, **kwargs)
+        super().__init__(*args, **kwargs)
 
         # Define the list of binning definition objects and a name->list_index
         # mapping for faster access.
         self._binnings = []
         self._binning_name2idx = {}
 
     @property
@@ -284,52 +319,58 @@
         return len(self._binnings)
 
     def has_same_binning_as(self, obj):
         """Checks if this object has the same binning as the given object.
 
         Parameters
         ----------
-        obj : class instance derived from UsesBinning
+        obj : instance of UsesBinning
             The object that should be checked for same binning.
 
         Returns
         -------
         check : bool
             True if ``obj`` uses the same binning, False otherwise.
         """
-        if(not isinstance(obj, UsesBinning)):
-            raise TypeError('The obj argument must be an instance of '
-                'UsesBinning!')
+        if not isinstance(obj, UsesBinning):
+            raise TypeError(
+                'The obj argument must be an instance of UsesBinning! '
+                f'Its current type is {classname(obj)}.')
 
         for (self_binning, obj_binning) in zip(self.binnings, obj.binnings):
-            if(not (self_binning == obj_binning)):
+            if self_binning != obj_binning:
                 return False
+
         return True
 
     def add_binning(self, binning, name=None):
         """Adds the given binning definition to the list of binnings.
 
         Parameters
         ----------
-        binning : BinningDefinition
+        binning : instance of BinningDefinition
             The binning definition to add.
         name : str | (default) None
             The name of the binning. If not None and it's different to the
             name of the given binning definition, a copy of the
             BinningDefinition object is made and the new name is set.
         """
-        if(not isinstance(binning, BinningDefinition)):
-            raise TypeError('The binning argument must be an instance of '
-                'BinningDefinition!')
+        if not isinstance(binning, BinningDefinition):
+            raise TypeError(
+                'The binning argument must be an instance of '
+                'BinningDefinition! '
+                f'Its current type is {classname(binning)}.')
 
         # Create a copy of the BinningDefinition object if the name differs.
-        if(name is not None):
-            if(not isinstance(name, str)):
-                raise TypeError('The name argument must be of type str!')
-            if(name != binning.name):
+        if name is not None:
+            if not isinstance(name, str):
+                raise TypeError(
+                    'The name argument must be of type str! '
+                    f'Its current type is {classname(name)}.')
+            if name != binning.name:
                 binning = BinningDefinition(name, binning.binedges)
 
         self._binnings.append(binning)
         self._binning_name2idx[binning.name] = len(self._binnings)-1
 
     def get_binning(self, name):
         """Retrieves the binning definition of the given name.
@@ -338,21 +379,23 @@
         ----------
         name : str | int
             The name of the binning definition. A string specifies the name and
             an integer the dimension index.
 
         Returns
         -------
-        binning : BinningDefinition
+        binning : instance of BinningDefinition
             The binning definition of the given name.
         """
-        if(isinstance(name, str)):
-            if(name not in self._binning_name2idx):
-                raise KeyError('The binning definition "%s" is not defined!'%(
-                    name))
+        if isinstance(name, str):
+            if name not in self._binning_name2idx:
+                raise KeyError(
+                    f'The binning definition "{name}" is not defined!')
             binning = self._binnings[self._binning_name2idx[name]]
-        elif(isinstance(name, int)):
+        elif isinstance(name, int):
             binning = self._binnings[name]
         else:
-            raise TypeError('The name argument must be of type str or int!')
+            raise TypeError(
+                'The name argument must be of type str or int! '
+                f'Its current type is {classname(name)}.')
 
         return binning
```

### Comparing `skyllh-23.1.1/skyllh/core/dataset.py` & `skyllh-23.2.0/skyllh/core/dataset.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,115 +1,159 @@
 # -*- coding: utf-8 -*-
 
+from copy import (
+    deepcopy,
+)
 import os
 import os.path
 import numpy as np
-from copy import deepcopy
 
-from skyllh.core.binning import BinningDefinition
-from skyllh.core.config import CFG
-from skyllh.core.livetime import Livetime
-from skyllh.core.progressbar import ProgressBar
+from skyllh.core import (
+    display,
+)
+from skyllh.core.binning import (
+    BinningDefinition,
+)
+from skyllh.core.config import (
+    HasConfig,
+)
+from skyllh.core.datafields import (
+    DataFields,
+    DataFieldStages as DFS,
+)
+from skyllh.core.display import (
+    ANSIColors,
+)
+from skyllh.core.livetime import (
+    Livetime,
+)
+from skyllh.core.progressbar import (
+    ProgressBar,
+)
 from skyllh.core.py import (
+    classname,
     float_cast,
     issequence,
     issequenceof,
     list_of_cast,
-    str_cast
+    str_cast,
 )
-from skyllh.core import display
-from skyllh.core.display import ANSIColors
 from skyllh.core.storage import (
     DataFieldRecordArray,
-    create_FileLoader
+    create_FileLoader,
+)
+from skyllh.core.timing import (
+    TaskTimer,
 )
-from skyllh.core.timing import TaskTimer
 
 
-class Dataset(object):
+class Dataset(
+        HasConfig,
+):
     """The Dataset class describes a set of self-consistent experimental and
     simulated detector data. Usually this is for a certain time period, i.e.
     a season.
 
     Independet data sets of the same kind, e.g. event selection, can be joined
     through a DatasetCollection object.
     """
     @staticmethod
-    def get_combined_exp_pathfilenames(datasets):
+    def get_combined_exp_pathfilenames(
+            datasets):
         """Creates the combined list of exp pathfilenames of all the given
         datasets.
 
         Parameters
         ----------
         datasets : sequence of Dataset
             The sequence of Dataset instances.
 
         Returns
         -------
         exp_pathfilenames : list
             The combined list of exp pathfilenames.
         """
-        if(not issequenceof(datasets, Dataset)):
-            raise TypeError('The datasets argument must be a sequence of Dataset instances!')
+        if not issequenceof(datasets, Dataset):
+            raise TypeError(
+                'The datasets argument must be a sequence of Dataset '
+                'instances!')
 
         exp_pathfilenames = []
         for ds in datasets:
             exp_pathfilenames += ds.exp_pathfilename_list
 
         return exp_pathfilenames
 
     @staticmethod
-    def get_combined_mc_pathfilenames(datasets):
+    def get_combined_mc_pathfilenames(
+            datasets):
         """Creates the combined list of mc pathfilenames of all the given
         datasets.
 
         Parameters
         ----------
         datasets : sequence of Dataset
             The sequence of Dataset instances.
 
         Returns
         -------
         mc_pathfilenames : list
             The combined list of mc pathfilenames.
         """
-        if(not issequenceof(datasets, Dataset)):
-            raise TypeError('The datasets argument must be a sequence of Dataset instances!')
+        if not issequenceof(datasets, Dataset):
+            raise TypeError(
+                'The datasets argument must be a sequence of Dataset '
+                'instances!')
 
         mc_pathfilenames = []
         for ds in datasets:
             mc_pathfilenames += ds.mc_pathfilename_list
 
         return mc_pathfilenames
 
     @staticmethod
-    def get_combined_livetime(datasets):
+    def get_combined_livetime(
+            datasets):
         """Sums the live-time of all the given datasets.
 
         Parameters
         ----------
         datasets : sequence of Dataset
             The sequence of Dataset instances.
 
         Returns
         -------
         livetime : float
             The sum of all the individual live-times.
         """
-        if(not issequenceof(datasets, Dataset)):
-            raise TypeError('The datasets argument must be a sequence of Dataset instances!')
+        if not issequenceof(datasets, Dataset):
+            raise TypeError(
+                'The datasets argument must be a sequence of Dataset '
+                'instances!')
 
-        livetime = np.sum([ ds.livetime for ds in datasets ])
+        livetime = np.sum([
+            ds.livetime
+            for ds in datasets
+        ])
 
         return livetime
 
     def __init__(
-            self, name, exp_pathfilenames, mc_pathfilenames, livetime,
-            default_sub_path_fmt, version, verqualifiers=None,
-            base_path=None, sub_path_fmt=None):
+            self,
+            name,
+            exp_pathfilenames,
+            mc_pathfilenames,
+            livetime,
+            default_sub_path_fmt,
+            version,
+            verqualifiers=None,
+            base_path=None,
+            sub_path_fmt=None,
+            **kwargs,
+    ):
         """Creates a new dataset object that describes a self-consistent set of
         data.
 
         Parameters
         ----------
         name : str
             The name of the dataset.
@@ -134,33 +178,34 @@
             If specified, this dictionary specifies version qualifiers. These
             can be interpreted as subversions of the dataset. The format of the
             dictionary must be 'qualifier (str): version (int)'.
         base_path : str | None
             The user-defined base path of the data set.
             Usually, this is the path of the location of the data directory.
             If set to ``None`` the configured repository base path
-            ``CFG['repository']['base_path']`` is used.
+            ``Config['repository']['base_path']`` is used.
         sub_path_fmt : str | None
             The user-defined format of the sub path of the data set.
             If set to ``None``, the ``default_sub_path_fmt`` will be used.
         """
+        super().__init__(**kwargs)
+
         self.name = name
         self.exp_pathfilename_list = exp_pathfilenames
         self.mc_pathfilename_list = mc_pathfilenames
         self.livetime = livetime
         self.default_sub_path_fmt = default_sub_path_fmt
         self.version = version
         self.verqualifiers = verqualifiers
         self.base_path = base_path
         self.sub_path_fmt = sub_path_fmt
 
         self.description = ''
 
-        self._loading_extra_exp_field_name_list = list()
-        self._loading_extra_mc_field_name_list = list()
+        self._datafields = dict()
 
         self._exp_field_name_renaming_dict = dict()
         self._mc_field_name_renaming_dict = dict()
 
         self._data_preparation_functions = list()
         self._binning_definitions = dict()
         self._aux_data_definitions = dict()
@@ -168,47 +213,66 @@
 
     @property
     def name(self):
         """The name of the dataset. This must be an unique identifier among
         all the different datasets.
         """
         return self._name
+
     @name.setter
     def name(self, name):
         self._name = name
 
     @property
     def description(self):
         """The (longer) description of the dataset.
         """
         return self._description
+
     @description.setter
     def description(self, description):
-        if(not isinstance(description, str)):
-            raise TypeError('The description of the dataset must be of '
-                'type str!')
+        if not isinstance(description, str):
+            raise TypeError(
+                'The description of the dataset must be of type str!')
         self._description = description
 
     @property
+    def datafields(self):
+        """The dictionary holding the names and stages of required data fields
+        specific for this dataset.
+        """
+        return self._datafields
+
+    @datafields.setter
+    def datafields(self, fields):
+        if not isinstance(fields, dict):
+            raise TypeError(
+                'The datafields property must be a dictionary! '
+                f'Its current type is "{classname(fields)}"!')
+        self._datafields = fields
+
+    @property
     def exp_pathfilename_list(self):
         """The list of file names of the data files that store the experimental
         data for this dataset.
         If a file name is given with a relative path, it will be relative to the
         root_dir property of this Dataset instance.
         """
         return self._exp_pathfilename_list
+
     @exp_pathfilename_list.setter
     def exp_pathfilename_list(self, pathfilenames):
-        if(pathfilenames is None):
+        if pathfilenames is None:
             pathfilenames = []
-        if(isinstance(pathfilenames, str)):
+        if isinstance(pathfilenames, str):
             pathfilenames = [pathfilenames]
-        if(not issequenceof(pathfilenames, str)):
-            raise TypeError('The exp_pathfilename_list property must be of '
-                'type str or a sequence of str!')
+        if not issequenceof(pathfilenames, str):
+            raise TypeError(
+                'The exp_pathfilename_list property must be of type str or a '
+                'sequence of str!')
         self._exp_pathfilename_list = list(pathfilenames)
 
     @property
     def exp_abs_pathfilename_list(self):
         """(read-only) The list of absolute path file names of the experimental
         data files.
         """
@@ -218,23 +282,25 @@
     def mc_pathfilename_list(self):
         """The list of file names of the data files that store the monte-carlo
         data for this dataset.
         If a file name is given with a relative path, it will be relative to the
         root_dir property of this Dataset instance.
         """
         return self._mc_pathfilename_list
+
     @mc_pathfilename_list.setter
     def mc_pathfilename_list(self, pathfilenames):
-        if(pathfilenames is None):
+        if pathfilenames is None:
             pathfilenames = []
-        if(isinstance(pathfilenames, str)):
+        if isinstance(pathfilenames, str):
             pathfilenames = [pathfilenames]
-        if(not issequenceof(pathfilenames, str)):
-            raise TypeError('The mc_pathfilename_list property must be of '
-                'type str or a sequence of str!')
+        if not issequenceof(pathfilenames, str):
+            raise TypeError(
+                'The mc_pathfilename_list property must be of type str or a '
+                'sequence of str!')
         self._mc_pathfilename_list = list(pathfilenames)
 
     @property
     def mc_abs_pathfilename_list(self):
         """(read-only) The list of absolute path file names of the monte-carlo
         data files.
         """
@@ -242,197 +308,181 @@
 
     @property
     def livetime(self):
         """The integrated live-time in days of the dataset. This can be None in
         cases where the livetime is retrieved directly from the data files.
         """
         return self._lifetime
+
     @livetime.setter
     def livetime(self, lt):
-        if(lt is not None):
-            lt = float_cast(lt,
+        if lt is not None:
+            lt = float_cast(
+                lt,
                 'The lifetime property of the dataset must be castable to '
                 'type float!')
         self._lifetime = lt
 
     @property
     def version(self):
         """The main version (int) of the dataset.
         """
         return self._version
+
     @version.setter
     def version(self, version):
-        if(not isinstance(version, int)):
-            raise TypeError('The version of the dataset must be of type int!')
+        if not isinstance(version, int):
+            raise TypeError(
+                'The version of the dataset must be of type int!')
         self._version = version
 
     @property
     def verqualifiers(self):
         """The dictionary holding the version qualifiers, i.e. sub-version
         qualifiers. If set to None, an empty dictionary will be used.
         The dictionary must have the type form of str:int.
         """
         return self._verqualifiers
+
     @verqualifiers.setter
     def verqualifiers(self, verqualifiers):
-        if(verqualifiers is None):
+        if verqualifiers is None:
             verqualifiers = dict()
-        if(not isinstance(verqualifiers, dict)):
+        if not isinstance(verqualifiers, dict):
             raise TypeError('The version qualifiers must be of type dict!')
         # Check if the dictionary has format str:int.
-        for (q,v) in verqualifiers.items():
-            if(not isinstance(q, str)):
-                raise TypeError('The version qualifier "%s" must be of type str!'%(q))
-            if(not isinstance(v, int)):
-                raise TypeError('The version for the qualifier "%s" must be of type int!'%(q))
+        for (q, v) in verqualifiers.items():
+            if not isinstance(q, str):
+                raise TypeError(
+                    f'The version qualifier "{q}" must be of type str!')
+            if not isinstance(v, int):
+                raise TypeError(
+                    f'The version for the qualifier "{q}" must be of type int!')
         # We need to take a deep copy in order to make sure that two datasets
         # don't share the same version qualifier dictionary.
         self._verqualifiers = deepcopy(verqualifiers)
 
     @property
     def base_path(self):
         """The base path of the data set. This can be ``None``.
         """
         return self._base_path
+
     @base_path.setter
     def base_path(self, path):
-        if(path is not None):
-            path = str_cast(path, 'The base_path property must be castable to '
-                'type str!')
-            if(not os.path.isabs(path)):
-                raise ValueError('The base_path property must be an absolute '
-                    'path!')
+        if path is not None:
+            path = str_cast(
+                path,
+                'The base_path property must be castable to type str!')
+            if not os.path.isabs(path):
+                raise ValueError(
+                    'The base_path property must be an absolute path!')
         self._base_path = path
 
     @property
     def default_sub_path_fmt(self):
         """The default format of the sub path of the data set. This must be a
         string that can be formatted via the ``format`` method of the ``str``
         class.
         """
         return self._default_sub_path_fmt
+
     @default_sub_path_fmt.setter
     def default_sub_path_fmt(self, fmt):
-        fmt = str_cast(fmt, 'The default_sub_path_fmt property must be '
-            'castable to type str!')
+        fmt = str_cast(
+            fmt,
+            'The default_sub_path_fmt property must be castable to type str!')
         self._default_sub_path_fmt = fmt
 
     @property
     def sub_path_fmt(self):
         """The format of the sub path of the data set. This must be a string
         that can be formatted via the ``format`` method of the ``str`` class.
         If set to ``None``, this property will return the
         ``default_sub_path_fmt`` property.
         """
-        if(self._sub_path_fmt is None):
+        if self._sub_path_fmt is None:
             return self._default_sub_path_fmt
         return self._sub_path_fmt
+
     @sub_path_fmt.setter
     def sub_path_fmt(self, fmt):
-        if(fmt is not None):
-            fmt = str_cast(fmt, 'The sub_path_fmt property must be None, or '
-                'castable to type str!')
+        if fmt is not None:
+            fmt = str_cast(
+                fmt,
+                'The sub_path_fmt property must be None, or castable to type '
+                'str!')
         self._sub_path_fmt = fmt
 
     @property
     def root_dir(self):
         """(read-only) The root directory to use when data files are specified
         with relative paths. It is constructed from the ``base_path`` and the
         ``sub_path_fmt`` properties via the ``generate_data_file_root_dir``
         function.
         """
         return generate_data_file_root_dir(
-            default_base_path=CFG['repository']['base_path'],
+            default_base_path=self._cfg['repository']['base_path'],
             default_sub_path_fmt=self._default_sub_path_fmt,
             version=self._version,
             verqualifiers=self._verqualifiers,
             base_path=self._base_path,
             sub_path_fmt=self._sub_path_fmt)
 
     @property
-    def loading_extra_exp_field_name_list(self):
-        """The list of extra field names that should get loaded when loading
-        experimental data. These should only be field names that are required
-        during the data preparation of this specific data set.
-        """
-        return self._loading_extra_exp_field_name_list
-    @loading_extra_exp_field_name_list.setter
-    def loading_extra_exp_field_name_list(self, fieldnames):
-        if(isinstance(fieldnames, str)):
-            fieldnames = [ fieldnames ]
-        elif(not issequenceof(fieldnames, str)):
-            raise TypeError('The loading_extra_exp_field_name_list property '
-                'must be an instance of str or a sequence of str type '
-                'instances!')
-        self._loading_extra_exp_field_name_list = list(fieldnames)
-
-    @property
-    def loading_extra_mc_field_name_list(self):
-        """The list of extra field names that should get loaded when loading
-        monte-carlo data. These should only be field names that are required
-        during the data preparation of this specific data set.
-        """
-        return self._loading_extra_mc_field_name_list
-    @loading_extra_mc_field_name_list.setter
-    def loading_extra_mc_field_name_list(self, fieldnames):
-        if(isinstance(fieldnames, str)):
-            fieldnames = [ fieldnames ]
-        elif(not issequenceof(fieldnames, str)):
-            raise TypeError('The loading_extra_mc_field_name_list property '
-                'must be an instance of str or a sequence of str type '
-                'instances!')
-        self._loading_extra_mc_field_name_list = list(fieldnames)
-
-    @property
     def exp_field_name_renaming_dict(self):
         """The dictionary specifying the field names of the experimental data
         which need to get renamed just after loading the data. The dictionary
         values are the new names.
         """
         return self._exp_field_name_renaming_dict
+
     @exp_field_name_renaming_dict.setter
     def exp_field_name_renaming_dict(self, d):
-        if(not isinstance(d, dict)):
-            raise TypeError('The exp_field_name_renaming_dict property must '
-                'be an instance of dict!')
+        if not isinstance(d, dict):
+            raise TypeError(
+                'The exp_field_name_renaming_dict property must be an instance '
+                'of dict!')
         self._exp_field_name_renaming_dict = d
 
     @property
     def mc_field_name_renaming_dict(self):
         """The dictionary specifying the field names of the monte-carlo data
         which need to get renamed just after loading the data. The dictionary
         values are the new names.
         """
         return self._mc_field_name_renaming_dict
+
     @mc_field_name_renaming_dict.setter
     def mc_field_name_renaming_dict(self, d):
-        if(not isinstance(d, dict)):
-            raise TypeError('The mc_field_name_renaming_dict property must '
-                'be an instance of dict!')
+        if not isinstance(d, dict):
+            raise TypeError(
+                'The mc_field_name_renaming_dict property must be an instance '
+                'of dict!')
         self._mc_field_name_renaming_dict = d
 
     @property
     def exists(self):
         """(read-only) Flag if all the data files of this data set exists. It is
         ``True`` if all data files exist and ``False`` otherwise.
         """
         for pathfilename in (self.exp_abs_pathfilename_list +
                              self.mc_abs_pathfilename_list):
-            if(not os.path.exists(pathfilename)):
+            if not os.path.exists(pathfilename):
                 return False
         return True
 
     @property
     def version_str(self):
         """The version string of the dataset. This combines all the version
         information about the dataset.
         """
-        s = '%03d'%(self._version)
-        for (q,v) in self._verqualifiers.items():
-            s += q+'%02d'%(v)
+        s = f'{self._version:03d}'
+        for (q, v) in self._verqualifiers.items():
+            s += f'{q}{v:02d}'
         return s
 
     @property
     def data_preparation_functions(self):
         """The list of callback functions that will be called to prepare the
         data (experimental and monte-carlo).
         """
@@ -448,15 +498,15 @@
             The fully qualified path and filename of the file.
 
         Returns
         -------
         s : str
             The generated string.
         """
-        if(os.path.exists(pathfilename)):
+        if os.path.exists(pathfilename):
             s = '['+ANSIColors.OKGREEN+'FOUND'+ANSIColors.ENDC+']'
         else:
             s = '['+ANSIColors.FAIL+'NOT FOUND'+ANSIColors.ENDC+']'
         s += ' ' + pathfilename
         return s
 
     def __gt__(self, ds):
@@ -473,85 +523,85 @@
         -------
         bool
             True, if this dataset is newer than the reference dataset.
             False, if this dataset is as new or older than the reference
             dataset.
         """
         # Datasets of different names cannot be compared usefully.
-        if(self._name != ds._name):
+        if self._name != ds._name:
             return False
 
         # Larger main version numbers indicate newer datasets.
-        if(self._version > ds._version):
+        if self._version > ds._version:
             return True
 
         # Look for version qualifiers that make this dataset older than the
         # reference dataset.
         qs1 = self._verqualifiers.keys()
         qs2 = ds._verqualifiers.keys()
 
         # If a qualifier of self is also specified for ds, the version number
         # of the self qualifier must be larger than the version number of the ds
         # qualifier, in order to consider self as newer dataset.
         # If a qualifier is present in self but not in ds, self is considered
         # newer.
         for q in qs1:
-            if(q in qs2 and qs1[q] <= qs2[q]):
+            if q in qs2 and qs1[q] <= qs2[q]:
                 return False
         # If there is a qualifier in ds but not in self, self is considered
         # older.
         for q in qs2:
-            if(q not in qs1):
+            if q not in qs1:
                 return False
 
         return True
 
-    def __str__(self):
+    def __str__(self):  # noqa: C901
         """Implementation of the pretty string representation of the Dataset
         object.
         """
-        s = 'Dataset "%s": v%s\n'%(self.name, self.version_str)
+        s = f'Dataset "{self.name}": v{self.version_str}\n'
 
         s1 = ''
 
-        if(self.livetime is None):
+        if self.livetime is None:
             s1 += '{ livetime = UNDEFINED }'
         else:
-            s1 += '{ livetime = %.3f days }'%(self.livetime)
+            s1 += '{ 'f'livetime = {self.livetime:.3f} days'' }'
         s1 += '\n'
 
-        if(self.description != ''):
+        if self.description != '':
             s1 += 'Description:\n' + self.description + '\n'
 
         s1 += 'Experimental data:\n'
         s2 = ''
         for (idx, pathfilename) in enumerate(self.exp_abs_pathfilename_list):
-            if(idx > 0):
+            if idx > 0:
                 s2 += '\n'
             s2 += self._gen_datafile_pathfilename_entry(pathfilename)
         s1 += display.add_leading_text_line_padding(
             display.INDENTATION_WIDTH, s2)
         s1 += '\n'
 
         s1 += 'MC data:\n'
         s2 = ''
         for (idx, pathfilename) in enumerate(self.mc_abs_pathfilename_list):
-            if(idx > 0):
+            if idx > 0:
                 s2 += '\n'
             s2 += self._gen_datafile_pathfilename_entry(pathfilename)
         s1 += display.add_leading_text_line_padding(
             display.INDENTATION_WIDTH, s2)
         s1 += '\n'
 
-        if(len(self._aux_data_definitions) > 0):
+        if len(self._aux_data_definitions) > 0:
             s1 += 'Auxiliary data:\n'
             s2 = ''
-            for (idx,(name, pathfilename_list)) in enumerate(
-                self._aux_data_definitions.items()):
-                if(idx > 0):
+            for (idx, (name, pathfilename_list)) in enumerate(
+                    self._aux_data_definitions.items()):
+                if idx > 0:
                     s2 += '\n'
 
                 s2 += name+':'
                 s3 = ''
                 pathfilename_list = self.get_abs_pathfilename_list(
                     pathfilename_list)
                 for pathfilename in pathfilename_list:
@@ -562,15 +612,18 @@
                 display.INDENTATION_WIDTH, s2)
 
         s += display.add_leading_text_line_padding(
             display.INDENTATION_WIDTH, s1)
 
         return s
 
-    def get_abs_pathfilename_list(self, pathfilename_list):
+    def get_abs_pathfilename_list(
+            self,
+            pathfilename_list,
+    ):
         """Returns a list where each entry of the given pathfilename_list is
         an absolute path. Relative paths will be prefixed with the root_dir
         property of this Dataset instance.
 
         Parameters
         ----------
         pathfilename_list : sequence of str
@@ -581,24 +634,27 @@
         abs_pathfilename_list : list of str
             The list of file names with absolute paths.
         """
         root_dir = self.root_dir
 
         abs_pathfilename_list = []
         for pathfilename in pathfilename_list:
-            if(os.path.isabs(pathfilename)):
+            if os.path.isabs(pathfilename):
                 abs_pathfilename_list.append(
                     pathfilename)
             else:
                 abs_pathfilename_list.append(
                     os.path.join(root_dir, pathfilename))
 
         return abs_pathfilename_list
 
-    def update_version_qualifiers(self, verqualifiers):
+    def update_version_qualifiers(
+            self,
+            verqualifiers,
+    ):
         """Updates the version qualifiers of the dataset. The update can only
         be done by increasing the version qualifier integer or by adding new
         version qualifiers.
 
         Parameters
         ----------
         verqualifiers : dict
@@ -609,37 +665,44 @@
         ValueError
             If the integer number of an existing version qualifier is not larger
             than the old one.
         """
         got_new_verqualifiers = False
         verqualifiers_keys = verqualifiers.keys()
         self_verqualifiers_keys = self._verqualifiers.keys()
-        if(len(verqualifiers_keys) > len(self_verqualifiers_keys)):
+        if len(verqualifiers_keys) > len(self_verqualifiers_keys):
             # New version qualifiers must be a subset of the old version
             # qualifiers.
             for q in self_verqualifiers_keys:
-                if(not q in verqualifiers_keys):
-                    raise ValueError('The version qualifier {} has been '
-                        'dropped!'.format(q))
+                if q not in verqualifiers_keys:
+                    raise ValueError(
+                        f'The version qualifier {q} has been dropped!')
             got_new_verqualifiers = True
 
         existing_verqualifiers_incremented = False
         for q in verqualifiers:
-            if((q in self._verqualifiers) and
-               (verqualifiers[q] > self._verqualifiers[q])):
+            if (q in self._verqualifiers) and\
+               (verqualifiers[q] > self._verqualifiers[q]):
                 existing_verqualifiers_incremented = True
             self._verqualifiers[q] = verqualifiers[q]
 
-        if(not (got_new_verqualifiers or existing_verqualifiers_incremented)):
-            raise ValueError('Version qualifier values did not increment and '
-                'no new version qualifiers were added!')
+        if not (got_new_verqualifiers or existing_verqualifiers_incremented):
+            raise ValueError(
+                'Version qualifier values did not increment and no new version '
+                'qualifiers were added!')
 
     def load_data(
-            self, keep_fields=None, livetime=None, dtc_dict=None,
-            dtc_except_fields=None, efficiency_mode=None, tl=None):
+            self,
+            keep_fields=None,
+            livetime=None,
+            dtc_dict=None,
+            dtc_except_fields=None,
+            efficiency_mode=None,
+            tl=None,
+    ):
         """Loads the data, which is described by the dataset.
 
         Note: This does not call the ``prepare_data`` method! It only loads
               the data as the method names says.
 
         Parameters
         ----------
@@ -658,65 +721,76 @@
         dtc_except_fields : str | sequence of str | None
             The sequence of field names whose data type should not get
             converted.
         efficiency_mode : str | None
             The efficiency mode the data should get loaded with. Possible values
             are:
 
-                - 'memory':
+                ``'memory'``
                     The data will be load in a memory efficient way. This will
                     require more time, because all data records of a file will
                     be loaded sequentially.
-                - 'time'
+                ``'time'``
                     The data will be loaded in a time efficient way. This will
                     require more memory, because each data file gets loaded in
                     memory at once.
 
             The default value is ``'time'``. If set to ``None``, the default
             value will be used.
-        tl : TimeLord instance | None
+        tl : instance of TimeLord | None
             The TimeLord instance to use to time the data loading procedure.
 
         Returns
         -------
-        data : DatasetData
-            A DatasetData instance holding the experimental and monte-carlo
+        data : instance of DatasetData
+            A instance of DatasetData holding the experimental and monte-carlo
             data.
         """
-        def _conv_new2orig_field_names(new_field_names, orig2new_renaming_dict):
+        def _conv_new2orig_field_names(
+                new_field_names,
+                orig2new_renaming_dict,
+        ):
             """Converts the given ``new_field_names`` into their original name
             given the original-to-new field name renaming dictionary.
             """
-            if(new_field_names is None):
+            if new_field_names is None:
                 return None
 
-            new2orig_renaming_dict = dict()
-            for (k,v) in orig2new_renaming_dict.items():
-                new2orig_renaming_dict[v] = k
+            new2orig_renaming_dict = {
+                v: k
+                for (k, v) in orig2new_renaming_dict.items()
+            }
 
             orig_field_names = [
                 new2orig_renaming_dict.get(new_field_name, new_field_name)
-                    for new_field_name in new_field_names
+                for new_field_name in new_field_names
             ]
 
             return orig_field_names
 
-        if(keep_fields is None):
+        if keep_fields is None:
             keep_fields = []
 
+        datafields = {**self._cfg['datafields'], **self._datafields}
+
         # Load the experimental data if there is any.
-        if(len(self._exp_pathfilename_list) > 0):
+        if len(self._exp_pathfilename_list) > 0:
             with TaskTimer(tl, 'Loading exp data from disk.'):
                 fileloader_exp = create_FileLoader(
                     self.exp_abs_pathfilename_list)
                 # Create the list of field names that should get kept.
                 keep_fields_exp = list(set(
                     _conv_new2orig_field_names(
-                        CFG['dataset']['analysis_required_exp_field_names'] +
-                        self._loading_extra_exp_field_name_list +
+                        DataFields.get_joint_names(
+                            datafields=datafields,
+                            stages=(
+                                DFS.DATAPREPARATION_EXP |
+                                DFS.ANALYSIS_EXP
+                            )
+                        ) +
                         keep_fields,
                         self._exp_field_name_renaming_dict
                     )
                 ))
 
                 data_exp = fileloader_exp.load_data(
                     keep_fields=keep_fields_exp,
@@ -726,106 +800,134 @@
                         self._exp_field_name_renaming_dict),
                     efficiency_mode=efficiency_mode)
                 data_exp.rename_fields(self._exp_field_name_renaming_dict)
         else:
             data_exp = None
 
         # Load the monte-carlo data if there is any.
-        if(len(self._mc_pathfilename_list) > 0):
+        if len(self._mc_pathfilename_list) > 0:
             with TaskTimer(tl, 'Loading mc data from disk.'):
                 fileloader_mc = create_FileLoader(
                     self.mc_abs_pathfilename_list)
                 # Determine `keep_fields_mc` for the generic case, where MC
                 # field names are an union of exp and mc field names.
                 # But the renaming dictionary can differ for exp and MC fields.
                 keep_fields_mc = list(set(
                     _conv_new2orig_field_names(
-                        CFG['dataset']['analysis_required_exp_field_names'] +
-                        self._loading_extra_exp_field_name_list +
+                        DataFields.get_joint_names(
+                            datafields=datafields,
+                            stages=(
+                                DFS.DATAPREPARATION_EXP |
+                                DFS.ANALYSIS_EXP
+                            )
+                        ) +
                         keep_fields,
-                        self._exp_field_name_renaming_dict) +
+                        self._exp_field_name_renaming_dict
+                    ) +
                     _conv_new2orig_field_names(
-                        CFG['dataset']['analysis_required_exp_field_names'] +
-                        self._loading_extra_exp_field_name_list +
-                        CFG['dataset']['analysis_required_mc_field_names'] +
-                        self._loading_extra_mc_field_name_list +
+                        DataFields.get_joint_names(
+                            datafields=datafields,
+                            stages=(
+                                DFS.DATAPREPARATION_EXP |
+                                DFS.ANALYSIS_EXP |
+                                DFS.DATAPREPARATION_MC |
+                                DFS.ANALYSIS_MC
+                            )
+                        ) +
                         keep_fields,
-                        self._mc_field_name_renaming_dict)
+                        self._mc_field_name_renaming_dict
+                    )
                 ))
                 data_mc = fileloader_mc.load_data(
                     keep_fields=keep_fields_mc,
                     dtype_convertions=dtc_dict,
                     dtype_convertion_except_fields=_conv_new2orig_field_names(
                         dtc_except_fields,
                         self._mc_field_name_renaming_dict),
                     efficiency_mode=efficiency_mode)
                 data_mc.rename_fields(self._mc_field_name_renaming_dict)
         else:
             data_mc = None
 
-        if(livetime is None):
+        if livetime is None:
             livetime = self.livetime
 
-        data = DatasetData(data_exp, data_mc, livetime)
+        data = DatasetData(
+            data_exp=data_exp,
+            data_mc=data_mc,
+            livetime=livetime)
 
         return data
 
-    def load_aux_data(self, name, tl=None):
+    def load_aux_data(
+            self,
+            name,
+            tl=None,
+    ):
         """Loads the auxiliary data for the given auxiliary data definition.
 
         Parameters
         ----------
         name : str
             The name of the auxiliary data.
-        tl : TimeLord instance | None
+        tl : instance of TimeLord | None
             The TimeLord instance to use to time the data loading procedure.
 
         Returns
         -------
         data : unspecified
             The loaded auxiliary data.
         """
-        name = str_cast(name,
+        name = str_cast(
+            name,
             'The name argument must be castable to type str!')
 
         # Check if the data was defined in memory.
-        if(name in self._aux_data):
-            with TaskTimer(tl, 'Loaded aux data "%s" from memory.'%(name)):
+        if name in self._aux_data:
+            with TaskTimer(tl, f'Loaded aux data "{name}" from memory.'):
                 data = self._aux_data[name]
             return data
 
-        if(name not in self._aux_data_definitions):
-            raise KeyError('The auxiliary data named "%s" does not exist!'%(
-                name))
+        if name not in self._aux_data_definitions:
+            raise KeyError(
+                f'The auxiliary data named "{name}" does not exist!')
 
         aux_pathfilename_list = self._aux_data_definitions[name]
-        with TaskTimer(tl, 'Loaded aux data "%s" from disk.'%(name)):
+        with TaskTimer(tl, f'Loaded aux data "{name}" from disk.'):
             fileloader_aux = create_FileLoader(self.get_abs_pathfilename_list(
                 aux_pathfilename_list))
             data = fileloader_aux.load_data()
 
         return data
 
-    def add_data_preparation(self, func):
+    def add_data_preparation(
+            self,
+            func,
+    ):
         """Adds the given data preparation function to the dataset.
 
         Parameters
         ----------
         func : callable
             The object with call signature __call__(data) that will prepare
             the data after it was loaded. The argument 'data' is a DatasetData
             instance holding the experimental and monte-carlo data. The function
             must alter the properties of the DatasetData instance.
 
         """
-        if(not callable(func)):
-            raise TypeError('The argument "func" must be a callable object with call signature __call__(data)!')
+        if not callable(func):
+            raise TypeError(
+                'The argument "func" must be a callable object with call '
+                'signature __call__(data)!')
         self._data_preparation_functions.append(func)
 
-    def remove_data_preparation(self, key=-1):
+    def remove_data_preparation(
+            self,
+            key=-1,
+    ):
         """Removes a data preparation function from the dataset.
 
         Parameters
         ----------
         key : str, int, optional
             The name or the index of the data preparation function that should
             be removed. Default value is ``-1``, i.e. the last added function.
@@ -835,53 +937,65 @@
         TypeError
             If the type of the key argument is invalid.
         IndexError
             If the given key is out of range.
         KeyError
             If the data preparation function cannot be found.
         """
-        if(isinstance(key, int)):
+        if isinstance(key, int):
             n = len(self._data_preparation_functions)
-            if((key < -n) or (key >= n)):
-                raise IndexError('The given index (%d) for the data '
-                    'preparation function is out of range (%d,%d)!'%(
-                        key, -n, n-1))
+            if (key < -n) or (key >= n):
+                raise IndexError(
+                    f'The given index ({key}) for the data preparation '
+                    f'function is out of range ({-n},{n-1})!')
             del self._data_preparation_functions[key]
             return
-        elif(isinstance(key, str)):
-            for (i,func) in enumerate(self._data_preparation_functions):
-                if(func.__name__ == key):
+        elif isinstance(key, str):
+            for (i, func) in enumerate(self._data_preparation_functions):
+                if func.__name__ == key:
                     del self._data_preparation_functions[i]
                     return
-            raise KeyError('The data preparation function "%s" was not found '
-                'in the dataset "%s"!'%(key, self._name))
+            raise KeyError(
+                f'The data preparation function "{key}" was not found in the '
+                f'dataset "{self._name}"!')
 
-        TypeError('The key argument must be an instance of int or str!')
+        TypeError(
+            'The key argument must be an instance of int or str!')
 
-    def prepare_data(self, data, tl=None):
+    def prepare_data(
+            self,
+            data,
+            tl=None,
+    ):
         """Prepares the data by calling the data preparation callback functions
         of this dataset.
 
         Parameters
         ----------
-        data : DatasetData instance
-            The DatasetData instance holding the data.
-        tl : TimeLord instance | None
-            The TimeLord instance that should be used to time the data
+        data : instance of DatasetData
+            The instance of DatasetData holding the data.
+        tl : instance of TimeLord | None
+            The instance TimeLord that should be used to time the data
             preparation.
         """
         for data_prep_func in self._data_preparation_functions:
-            task = 'Preparing data of dataset "'+self.name+'" by '\
-                '"'+data_prep_func.__name__+'".'
-            with TaskTimer(tl, task):
+            with TaskTimer(
+                    tl,
+                    f'Preparing data of dataset "{self.name}" by '
+                    f'"{data_prep_func.__name__}".'):
                 data_prep_func(data)
 
     def load_and_prepare_data(
-            self, livetime=None, keep_fields=None, compress=False,
-            efficiency_mode=None, tl=None):
+            self,
+            livetime=None,
+            keep_fields=None,
+            compress=False,
+            efficiency_mode=None,
+            tl=None,
+    ):
         """Loads and prepares the experimental and monte-carlo data of this
         dataset by calling its ``load_data`` and ``prepare_data`` methods.
         After loading the data it drops all unnecessary data fields if they are
         not listed in ``keep_fields``.
         In the end it asserts the data format of the experimental and
         monte-carlo data.
 
@@ -901,282 +1015,368 @@
             The only field, which will not get converted is the 'mcweight'
             field, in order to ensure reliable calculations.
             Default is False.
         efficiency_mode : str | None
             The efficiency mode the data should get loaded with. Possible values
             are:
 
-                - 'memory':
+                ``'memory'``
                     The data will be load in a memory efficient way. This will
                     require more time, because all data records of a file will
                     be loaded sequentially.
-                - 'time'
+                ``'time'``
                     The data will be loaded in a time efficient way. This will
                     require more memory, because each data file gets loaded in
                     memory at once.
 
             The default value is ``'time'``. If set to ``None``, the default
             value will be used.
-        tl : TimeLord instance | None
-            The TimeLord instance that should be used to time the data loading
-            and preparation.
+        tl : instance of TimeLord | None
+            The instance of TimeLord that should be used to time the data
+            loading and preparation.
 
         Returns
         -------
-        data : DatasetData
-            The DatasetData instance holding the experimental and monte-carlo
+        data : instance of DatasetData
+            The instance of DatasetData holding the experimental and monte-carlo
             data.
         """
-        if(keep_fields is None):
+        if keep_fields is None:
             keep_fields = list()
-        elif(not issequenceof(keep_fields, str)):
-            raise TypeError('The keep_fields argument must be None, or a '
-                'sequence of str!')
+        elif not issequenceof(keep_fields, str):
+            raise TypeError(
+                'The keep_fields argument must be None, or a sequence of str!')
         keep_fields = list(keep_fields)
 
         dtc_dict = None
         dtc_except_fields = None
-        if(compress):
-            dtc_dict = { np.dtype(np.float64): np.dtype(np.float32) }
-            dtc_except_fields = [ 'mcweight' ]
+        if compress:
+            dtc_dict = {np.dtype(np.float64): np.dtype(np.float32)}
+            dtc_except_fields = ['mcweight']
 
         data = self.load_data(
             keep_fields=keep_fields,
             livetime=livetime,
             dtc_dict=dtc_dict,
             dtc_except_fields=dtc_except_fields,
             efficiency_mode=efficiency_mode,
             tl=tl)
 
         self.prepare_data(data, tl=tl)
 
         # Drop unrequired data fields.
-        if(data.exp is not None):
+        if data.exp is not None:
             with TaskTimer(tl, 'Cleaning exp data.'):
                 keep_fields_exp = (
-                    CFG['dataset']['analysis_required_exp_field_names'] +
+                    DataFields.get_joint_names(
+                        datafields=self._cfg['datafields'],
+                        stages=(
+                            DFS.ANALYSIS_EXP
+                        )
+                    ) +
                     keep_fields
                 )
                 data.exp.tidy_up(keep_fields=keep_fields_exp)
 
-        if(data.mc is not None):
+        if data.mc is not None:
             with TaskTimer(tl, 'Cleaning MC data.'):
                 keep_fields_mc = (
-                    CFG['dataset']['analysis_required_exp_field_names'] +
-                    CFG['dataset']['analysis_required_mc_field_names'] +
+                    DataFields.get_joint_names(
+                        datafields=self._cfg['datafields'],
+                        stages=(
+                            DFS.ANALYSIS_EXP |
+                            DFS.ANALYSIS_MC
+                        )
+                    ) +
                     keep_fields
                 )
                 data.mc.tidy_up(keep_fields=keep_fields_mc)
 
         with TaskTimer(tl, 'Asserting data format.'):
             assert_data_format(self, data)
 
         return data
 
-    def add_binning_definition(self, binning):
+    def add_binning_definition(
+            self,
+            binning,
+    ):
         """Adds a binning setting to this dataset.
 
         Parameters
         ----------
         binning : BinningDefinition
             The BinningDefinition object holding the binning information.
         """
-        if(not isinstance(binning, BinningDefinition)):
-            raise TypeError('The "binning" argument must be of type '
-                'BinningDefinition!')
-        if(binning.name in self._binning_definitions):
-            raise KeyError('The binning definition "%s" is already defined for '
-                'dataset "%s"!'%(binning.name, self._name))
+        if not isinstance(binning, BinningDefinition):
+            raise TypeError(
+                'The "binning" argument must be of type BinningDefinition!')
+        if binning.name in self._binning_definitions:
+            raise KeyError(
+                f'The binning definition "{binning.name}" is already defined '
+                f'for dataset "{self._name}"!')
 
         self._binning_definitions[binning.name] = binning
 
-    def get_binning_definition(self, name):
+    def get_binning_definition(
+            self,
+            name,
+    ):
         """Gets the BinningDefinition object for the given binning name.
 
         Parameters
         ----------
         name : str
             The name of the binning definition.
 
         Returns
         -------
-        binning_definition : BinningDefinition instance
-            The requested BinningDefinition instance.
+        binning_definition : instance of BinningDefinition
+            The requested instance of BinningDefinition.
         """
-        if(name not in self._binning_definitions):
-            raise KeyError('The given binning name "%s" has not been added to '
-                'the dataset yet!'%(name))
+        if name not in self._binning_definitions:
+            raise KeyError(
+                f'The given binning name "{name}" has not been added to the '
+                'dataset yet!')
         return self._binning_definitions[name]
 
-    def remove_binning_definition(self, name):
+    def remove_binning_definition(
+            self,
+            name,
+    ):
         """Removes the BinningDefinition object from the dataset.
 
         Parameters
         ----------
         name : str
             The name of the binning definition.
 
         """
-        if(name not in self._binning_definitions):
+        if name not in self._binning_definitions:
             raise KeyError(
                 f'The given binning name "{name}" does not exist in the '
-                f'dataset "{self.name}", nothing to remove!'
-            )
+                f'dataset "{self.name}", nothing to remove!')
 
         self._binning_definitions.pop(name)
 
-    def has_binning_definition(self, name):
+    def has_binning_definition(
+            self,
+            name,
+    ):
         """Checks if the dataset has a defined binning definition with the given
         name.
 
         Parameters
         ----------
         name : str
             The name of the binning definition.
 
         Returns
         -------
         check : bool
             True if the binning definition exists, False otherwise.
         """
-        if(name in self._binning_definitions):
+        if name in self._binning_definitions:
             return True
         return False
 
-    def define_binning(self, name, binedges):
+    def define_binning(
+            self,
+            name,
+            binedges,
+    ):
         """Defines a binning for ``name``, and adds it as binning definition.
 
         Parameters
         ----------
         name : str
             The name of the binning setting.
         binedges : sequence
             The sequence of the bin edges, which should be used for this binning
             definition.
 
         Returns
         -------
-        binning : BinningDefinition
-            The BinningDefinition object which was created and added to this
-            season.
+        binning : instance of BinningDefinition
+            The instance of BinningDefinition which was created and added to
+            this dataset.
         """
         binning = BinningDefinition(name, binedges)
         self.add_binning_definition(binning)
         return binning
 
     def replace_binning_definition(self, binning):
         """Replaces an already defined binning definition of this dataset by
         the given binning definition.
 
         Parameters
         ----------
-        binning : BinningDefinition instance
-            The instance of BinningDefinition that will replace the data set's
+        binning : instance of BinningDefinition
+            The instance of BinningDefinition that will replace the dataset's
             BinningDefinition instance of the same name.
         """
-        if(not isinstance(binning, BinningDefinition)):
-            raise TypeError('The "binning" argument must be of type '
-                'BinningDefinition!')
-        if(binning.name not in self._binning_definitions):
-            raise KeyError('The given binning definition "%s" has not been '
-                'added to the dataset yet!'%(binning.name))
+        if not isinstance(binning, BinningDefinition):
+            raise TypeError(
+                'The "binning" argument must be of type BinningDefinition!')
+        if binning.name not in self._binning_definitions:
+            raise KeyError(
+                f'The given binning definition "{binning.name}" has not been '
+                'added to the dataset yet!')
 
         self._binning_definitions[binning.name] = binning
 
-    def add_aux_data_definition(self, name, pathfilenames):
+    def add_aux_data_definition(
+            self,
+            name,
+            pathfilenames,
+    ):
         """Adds the given data files as auxiliary data definition to the
         dataset.
 
         Parameters
         ----------
         name : str
-            The name of the auxiliary data. The name is used as identifier for
-            the data within SkyLLH.
+            The name of the auxiliary data definition. The name is used as
+            identifier for the data within SkyLLH.
         pathfilenames : str | sequence of str
             The file name(s) (including paths) of the data file(s).
         """
-        name = str_cast(name,
-            'The name argument must be castable to type str!')
-        pathfilenames = list_of_cast(str, pathfilenames,
+        name = str_cast(
+            name,
+            'The name argument must be castable to type str! '
+            f'Its current type is {classname(name)}.')
+
+        pathfilenames = list_of_cast(
+            str,
+            pathfilenames,
             'The pathfilenames argument must be of type str or a sequence '
-            'of str!')
+            f'of str! Its current type is {classname(pathfilenames)}.')
 
-        if(name in self._aux_data_definitions):
-            raise KeyError('The auxiliary data definition "%s" is already '
-                'defined for dataset "%s"!'%(name, self.name))
+        if name in self._aux_data_definitions:
+            raise KeyError(
+                f'The auxiliary data definition "{name}" is already defined '
+                f'for dataset "{self.name}"!')
 
         self._aux_data_definitions[name] = pathfilenames
 
-    def get_aux_data_definition(self, name):
+    def get_aux_data_definition(
+            self,
+            name,
+    ):
         """Returns the auxiliary data definition from the dataset.
 
         Parameters
         ----------
         name : str
-            The name of the auxiliary data.
+            The name of the auxiliary data definition.
 
         Raises
         ------
         KeyError
             If auxiliary data with the given name does not exist.
 
         Returns
         -------
         aux_data_definition : list of str
-            The locations (pathfilenames) of the files defined in the auxiliary data
-                    as auxiliary data definition.
+            The locations (pathfilenames) of the files defined in the auxiliary
+            data as auxiliary data definition.
         """
-
-        if(not name in self._aux_data_definitions):
-            raise KeyError('The auxiliary data definition "{}" does not '
-                'exist in dataset "{}"!'.format(name, self.name))
+        if name not in self._aux_data_definitions:
+            raise KeyError(
+                f'The auxiliary data definition "{name}" does not exist in '
+                f'dataset "{self.name}"!')
 
         return self._aux_data_definitions[name]
 
-    def remove_aux_data_definition(self, name):
+    def set_aux_data_definition(
+            self,
+            name,
+            pathfilenames,
+    ):
+        """Sets the files of the auxiliary data definition, which has the given
+        name.
+
+        Parameters
+        ----------
+        name : str
+            The name of the auxiliary data definition.
+        pathfilenames : str | sequence of str
+            The file name(s) (including paths) of the data file(s).
+        """
+        name = str_cast(
+            name,
+            'The name argument must be castable to type str! '
+            f'Its current type is {classname(name)}.')
+
+        pathfilenames = list_of_cast(
+            str,
+            pathfilenames,
+            'The pathfilenames argument must be of type str or a sequence '
+            f'of str! Its current type is {classname(pathfilenames)}.')
+
+        if name not in self._aux_data_definitions:
+            raise KeyError(
+                f'The auxiliary data definition "{name}" is not defined '
+                f'for dataset "{self.name}"! Use add_aux_data_definition '
+                'instead!')
+
+        self._aux_data_definitions[name] = pathfilenames
+
+    def remove_aux_data_definition(
+            self,
+            name,
+    ):
         """Removes the auxiliary data definition from the dataset.
 
         Parameters
         ----------
         name : str
             The name of the dataset that should get removed.
         """
-        if(name not in self._aux_data_definitions):
+        if name not in self._aux_data_definitions:
             raise KeyError(
                 f'The auxiliary data definition "{name}" does not exist in '
-                f'dataset "{self.name}", nothing to remove!'
-            )
+                f'dataset "{self.name}", nothing to remove!')
 
         self._aux_data_definitions.pop(name)
 
-    def add_aux_data(self, name, data):
+    def add_aux_data(
+            self,
+            name,
+            data,
+    ):
         """Adds the given data as auxiliary data to this data set.
 
         Parameters
         ----------
         name : str
             The name under which the auxiliary data will be stored.
         data : unspecified
             The data that should get stored. This can be of any data type.
 
         Raises
         ------
         KeyError
             If auxiliary data is already stored under the given name.
         """
-        name = str_cast(name,
+        name = str_cast(
+            name,
             'The name argument must be castable to type str!')
 
-        if(name in self._aux_data):
-            raise KeyError('The auxiliary data "%s" is already defined for '
-                'dataset "%s"!'%(name, self.name))
+        if name in self._aux_data:
+            raise KeyError(
+                f'The auxiliary data "{name}" is already defined for dataset '
+                f'"{self.name}"!')
 
         self._aux_data[name] = data
 
-    def get_aux_data(self, name):
+    def get_aux_data(
+            self,
+            name,
+    ):
         """Retrieves the auxiliary data that is stored in this data set under
         the given name.
 
         Parameters
         ----------
         name : str
             The name under which the auxiliary data is stored.
@@ -1187,48 +1387,56 @@
             The retrieved auxiliary data.
 
         Raises
         ------
         KeyError
             If no auxiliary data is stored with the given name.
         """
-        name = str_cast(name,
+        name = str_cast(
+            name,
             'The name argument must be castable to type str!')
 
-        if(name not in self._aux_data):
-            raise KeyError('The auxiliary data "%s" is not defined for '
-                'dataset "%s"!'%(name, self.name))
+        if name not in self._aux_data:
+            raise KeyError(
+                f'The auxiliary data "{name}" is not defined for dataset '
+                f'"{self.name}"!')
 
         return self._aux_data[name]
 
-    def remove_aux_data(self, name):
+    def remove_aux_data(
+            self,
+            name,
+    ):
         """Removes the auxiliary data that is stored in this data set under
         the given name.
 
         Parameters
         ----------
         name : str
             The name of the dataset that should get removed.
         """
-        if(name not in self._aux_data):
+        if name not in self._aux_data:
             raise KeyError(
                 f'The auxiliary data "{name}" is not defined for dataset '
-                f'"{self.name}", nothing to remove!'
-            )
+                f'"{self.name}", nothing to remove!')
 
         self._aux_data.pop(name)
 
 
-class DatasetCollection(object):
+class DatasetCollection(
+        object):
     """The DatasetCollection class describes a collection of different datasets.
 
     New datasets can be added via the add-assign operator (+=), which calls
     the ``add_datasets`` method.
     """
-    def __init__(self, name, description=''):
+    def __init__(
+            self,
+            name,
+            description=''):
         """Creates a new DatasetCollection instance.
 
         Parameters
         ----------
         name : str
             The name of the collection.
         description : str
@@ -1240,29 +1448,34 @@
         self._datasets = dict()
 
     @property
     def name(self):
         """The name (str) of the dataset collection.
         """
         return self._name
+
     @name.setter
     def name(self, name):
-        if(not isinstance(name, str)):
-            raise TypeError('The name of the dataset collection must be of type str!')
+        if not isinstance(name, str):
+            raise TypeError(
+                'The name of the dataset collection must be of type str!')
         self._name = name
 
     @property
     def description(self):
         """The (longer) description of the dataset collection.
         """
         return self._description
+
     @description.setter
     def description(self, description):
-        if(not isinstance(description, str)):
-            raise TypeError('The description of the dataset collection must be of type str!')
+        if not isinstance(description, str):
+            raise TypeError(
+                'The description of the dataset collection must be of type '
+                'str!')
         self._description = description
 
     @property
     def dataset_names(self):
         """The list of names of the assigned datasets.
         """
         return sorted(self._datasets.keys())
@@ -1283,81 +1496,94 @@
         ds_name = list(self._datasets.keys())[0]
         return self._datasets[ds_name].verqualifiers
 
     def __iadd__(self, ds):
         """Implementation of the ``self += dataset`` operation to add a
         Dataset object to this dataset collection.
         """
-        if(not isinstance(ds, Dataset)):
-            raise TypeError('The dataset object must be a subclass of Dataset!')
+        if not isinstance(ds, Dataset):
+            raise TypeError(
+                'The dataset object must be a subclass of Dataset!')
 
         self.add_datasets(ds)
 
         return self
 
     def __str__(self):
         """Implementation of the pretty string representation of the
         DatasetCollection instance. It shows the available datasets.
         """
-        lines  = 'DatasetCollection "%s"\n'%(self.name)
+        lines = f'DatasetCollection "{self.name}"\n'
         lines += "-"*display.PAGE_WIDTH + "\n"
         lines += "Description:\n" + self.description + "\n"
         lines += "Available datasets:\n"
 
         for name in self.dataset_names:
             lines += '\n'
-            lines += display.add_leading_text_line_padding(2, str(self._datasets[name]))
+            lines += display.add_leading_text_line_padding(
+                2, str(self._datasets[name]))
 
         return lines
 
-    def add_datasets(self, datasets):
+    def add_datasets(
+            self,
+            datasets,
+    ):
         """Adds the given Dataset object(s) to this dataset collection.
 
         Parameters
         ----------
-        datasets : Dataset | sequence of Dataset
-            The Dataset object or the sequence of Dataset objects that should be
-            added to the dataset collection.
+        datasets : instance of Dataset | sequence of instance of Dataset
+            The instance of Dataset or the sequence of instance of Dataset that
+            should be added to the dataset collection.
 
         Returns
         -------
-        self : DatasetCollection
-            This DatasetCollection object in order to be able to chain several
-            add_dataset calls.
+        self : instance of DatasetCollection
+            This instance of DatasetCollection in order to be able to chain
+            several ``add_datasets`` calls.
         """
-        if(not issequence(datasets)):
+        if not issequence(datasets):
             datasets = [datasets]
 
         for dataset in datasets:
-            if(not isinstance(dataset, Dataset)):
-                raise TypeError('The dataset object must be a sub-class of '
-                    'Dataset!')
-
-            if(dataset.name in self._datasets):
-                raise KeyError('Dataset "%s" already exists!'%(dataset.name))
+            if not isinstance(dataset, Dataset):
+                raise TypeError(
+                    'The dataset object must be a sub-class of Dataset!')
+
+            if dataset.name in self._datasets:
+                raise KeyError(
+                    f'Dataset "{dataset.name}" already exists!')
 
             self._datasets[dataset.name] = dataset
 
         return self
 
-    def remove_dataset(self, name):
+    def remove_dataset(
+            self,
+            name,
+    ):
         """Removes the given dataset from the collection.
 
         Parameters
         ----------
         name : str
             The name of the dataset that should get removed.
         """
-        if(name not in self._datasets):
-            raise KeyError('Dataset "%s" is not part of the dataset '
-                'collection "%s", nothing to remove!'%(name, self.name))
+        if name not in self._datasets:
+            raise KeyError(
+                f'Dataset "{name}" is not part of the dataset collection '
+                f'"{self.name}", nothing to remove!')
 
         self._datasets.pop(name)
 
-    def get_dataset(self, name):
+    def get_dataset(
+            self,
+            name,
+    ):
         """Retrieves a Dataset object from this dataset collection.
 
         Parameters
         ----------
         name : str
             The name of the dataset.
 
@@ -1368,23 +1594,27 @@
 
         Raises
         ------
         KeyError
             If the data set of the given name is not present in this data set
             collection.
         """
-        if(name not in self._datasets):
+        if name not in self._datasets:
             ds_names = '", "'.join(self.dataset_names)
             ds_names = '"'+ds_names+'"'
-            raise KeyError('The dataset "%s" is not part of the dataset '
-                'collection "%s"! Possible dataset names are: %s!'%(
-                    name, self.name, ds_names))
+            raise KeyError(
+                f'The dataset "{name}" is not part of the dataset collection '
+                f'"{self.name}"! Possible dataset names are: {ds_names}!')
+
         return self._datasets[name]
 
-    def get_datasets(self, names):
+    def get_datasets(
+            self,
+            names,
+    ):
         """Retrieves a list of Dataset objects from this dataset collection.
 
         Parameters
         ----------
         names : str | sequence of str
             The name or sequence of names of the datasets to retrieve.
 
@@ -1395,55 +1625,66 @@
 
         Raises
         ------
         KeyError
             If one of the requested data sets is not present in this data set
             collection.
         """
-        if(not issequence(names)):
+        if not issequence(names):
             names = [names]
-        if(not issequenceof(names, str)):
-            raise TypeError('The names argument must be an instance of str or '
-                'a sequence of str instances!')
+        if not issequenceof(names, str):
+            raise TypeError(
+                'The names argument must be an instance of str or a sequence '
+                'of str instances!')
 
         datasets = []
         for name in names:
             datasets.append(self.get_dataset(name))
 
         return datasets
 
-    def set_exp_field_name_renaming_dict(self, d):
+    def set_exp_field_name_renaming_dict(
+            self,
+            d,
+    ):
         """Sets the dictionary with the data field names of the experimental
         data that needs to be renamed just after loading the data. The
         dictionary will be set to all added data sets.
 
         Parameters
         ----------
         d : dict
             The dictionary with the old field names as keys and the new field
             names as values.
         """
         for (dsname, dataset) in self._datasets.items():
             dataset.exp_field_name_renaming_dict = d
 
-    def set_mc_field_name_renaming_dict(self, d):
+    def set_mc_field_name_renaming_dict(
+            self,
+            d,
+    ):
         """Sets the dictionary with the data field names of the monte-carlo
         data that needs to be renamed just after loading the data. The
         dictionary will be set to all added data sets.
 
         Parameters
         ----------
         d : dict
             The dictionary with the old field names as keys and the new field
             names as values.
         """
         for (dsname, dataset) in self._datasets.items():
             dataset.mc_field_name_renaming_dict = d
 
-    def set_dataset_prop(self, name, value):
+    def set_dataset_prop(
+            self,
+            name,
+            value,
+    ):
         """Sets the given property to the given name for all data sets of this
         data set collection.
 
         Parameters
         ----------
         name : str
             The name of the property.
@@ -1452,49 +1693,60 @@
 
         Raises
         ------
         KeyError
             If the given property does not exist in the data sets.
         """
         for (dsname, dataset) in self._datasets.items():
-            if(not hasattr(dataset, name)):
-                raise KeyError('The data set "%s" does not have a property '
-                    'named "%s"!'%(dsname, name))
+            if not hasattr(dataset, name):
+                raise KeyError(
+                    f'The dataset "{dsname}" does not have a property named '
+                    f'"{name}"!')
             setattr(dataset, name, value)
 
-    def define_binning(self, name, binedges):
+    def define_binning(
+            self,
+            name,
+            binedges,
+    ):
         """Defines a binning definition and adds it to all the datasets of this
         dataset collection.
 
         Parameters
         ----------
         name : str
             The name of the binning definition.
         binedges : sequence
             The sequence of the bin edges, that should be used for the binning.
         """
-        for (dsname, dataset) in self._datasets.items():
+        for dataset in self._datasets.values():
             dataset.define_binning(name, binedges)
 
-    def add_data_preparation(self, func):
+    def add_data_preparation(
+            self,
+            func,
+    ):
         """Adds the data preparation function to all the datasets of this
         dataset collection.
 
         Parameters
         ----------
         func : callable
-            The object with call signature __call__(data) that will prepare
+            The object with call signature ``__call__(data)`` that will prepare
             the data after it was loaded. The argument 'data' is the DatasetData
             instance holding the experimental and monte-carlo data.
             This function must alter the properties of the DatasetData instance.
         """
-        for (dsname, dataset) in self._datasets.items():
+        for dataset in self._datasets.values():
             dataset.add_data_preparation(func)
 
-    def remove_data_preparation(self, key=-1):
+    def remove_data_preparation(
+            self,
+            key=-1,
+    ):
         """Removes data preparation function from all the datasets of this
         dataset collection.
 
         Parameters
         ----------
         key : str, int, optional
             The name or the index of the data preparation function that should
@@ -1505,231 +1757,299 @@
         TypeError
             If the type of the key argument is invalid.
         IndexError
             If the given key is out of range.
         KeyError
             If the data preparation function cannot be found.
         """
-        for (dsname, dataset) in self._datasets.items():
+        for dataset in self._datasets.values():
             dataset.remove_data_preparation(key=key)
 
-    def update_version_qualifiers(self, verqualifiers):
+    def update_version_qualifiers(
+            self,
+            verqualifiers,
+    ):
         """Updates the version qualifiers of all datasets of this dataset
         collection.
         """
-        for (dsname, dataset) in self._datasets.items():
+        for dataset in self._datasets.values():
             dataset.update_version_qualifiers(verqualifiers)
 
-    def load_data(self, livetime=None, tl=None, ppbar=None):
+    def load_data(
+            self,
+            livetime=None,
+            tl=None,
+            ppbar=None,
+            **kwargs,
+    ):
         """Loads the data of all data sets of this data set collection.
 
         Parameters
         ----------
         livetime : float | dict of str => float | None
             If not None, uses this livetime (in days) as livetime for (all) the
             DatasetData instances, otherwise uses the live time from the Dataset
             instance. If a dictionary of data set names and floats is given, it
             defines the livetime for the individual data sets.
-        tl : TimeLord instance | None
-            The TimeLord instance that should be used to time the data load
+        tl : instance of TimeLord | None
+            The instance of TimeLord that should be used to time the data load
             operation.
         ppbar : instance of ProgressBar | None
             The optional parent progress bar.
+        **kwargs
+            Additional keyword arguments are passed to the
+            :meth:`~skyllh.core.dataset.Dataset.load_data` method of the
+            individual datasets.
 
         Returns
         -------
         data_dict : dictionary str => instance of DatasetData
             The dictionary with the DatasetData instance holding the data of
             an individual data set as value and the data set's name as key.
         """
-        if(not isinstance(livetime, dict)):
+        if not isinstance(livetime, dict):
             livetime_dict = dict()
             for (dsname, dataset) in self._datasets.items():
                 livetime_dict[dsname] = livetime
             livetime = livetime_dict
 
-        if(len(livetime) != len(self._datasets)):
-            raise ValueError('The livetime argument must be None, a single '
-                'float, or a dictionary with %d str:float entries! Currently '
-                'the dictionary has %d entries.'%(
-                    len(self._datasets), len(livetime)))
+        if len(livetime) != len(self._datasets):
+            raise ValueError(
+                'The livetime argument must be None, a single float, or a '
+                f'dictionary with {len(self._datasets)} str:float entries! '
+                f'Currently the dictionary has {len(livetime)} entries.')
 
         pbar = ProgressBar(len(self._datasets), parent=ppbar).start()
         data_dict = dict()
         for (dsname, dataset) in self._datasets.items():
             data_dict[dsname] = dataset.load_data(
-                livetime=livetime[dsname], tl=tl)
+                livetime=livetime[dsname],
+                tl=tl,
+                **kwargs)
             pbar.increment()
         pbar.finish()
 
         return data_dict
 
 
-class DatasetData(object):
+class DatasetData(
+        object):
     """This class provides the container for the actual experimental and
-    monto-carlo data. It also holds a reference to the Dataset instance, which
-    holds the data's meta information.
+    monto-carlo data.
     """
-    def __init__(self, data_exp, data_mc, livetime):
+    def __init__(
+            self,
+            data_exp,
+            data_mc,
+            livetime,
+            **kwargs,
+    ):
         """Creates a new DatasetData instance.
 
         Parameters
         ----------
         data_exp : instance of DataFieldRecordArray | None
             The instance of DataFieldRecordArray holding the experimental data.
             This can be None for a MC-only study.
         data_mc : instance of DataFieldRecordArray
             The instance of DataFieldRecordArray holding the monto-carlo data.
         livetime : float
             The integrated livetime in days of the data.
         """
-        super(DatasetData, self).__init__()
+        super().__init__(**kwargs)
 
         self.exp = data_exp
         self.mc = data_mc
         self.livetime = livetime
 
     @property
     def exp(self):
         """The DataFieldRecordArray instance holding the experimental data.
         This is None, if there is no experimental data available.
         """
         return self._exp
+
     @exp.setter
     def exp(self, data):
-        if(data is not None):
-            if(not isinstance(data, DataFieldRecordArray)):
-                raise TypeError('The exp property must be an instance of '
+        if data is not None:
+            if not isinstance(data, DataFieldRecordArray):
+                raise TypeError(
+                    'The exp property must be an instance of '
                     'DataFieldRecordArray!')
         self._exp = data
 
     @property
     def mc(self):
         """The DataFieldRecordArray instance holding the monte-carlo data.
         This is None, if there is no monte-carlo data available.
         """
         return self._mc
+
     @mc.setter
     def mc(self, data):
-        if(data is not None):
-            if(not isinstance(data, DataFieldRecordArray)):
-                raise TypeError('The mc property must be an instance of '
+        if data is not None:
+            if not isinstance(data, DataFieldRecordArray):
+                raise TypeError(
+                    'The mc property must be an instance of '
                     'DataFieldRecordArray!')
         self._mc = data
 
     @property
     def livetime(self):
         """The integrated livetime in days of the data.
         This is None, if there is no live-time provided.
         """
         return self._livetime
+
     @livetime.setter
     def livetime(self, lt):
-        if(lt is not None):
-            lt = float_cast(lt,
+        if lt is not None:
+            lt = float_cast(
+                lt,
                 'The livetime property must be castable to type float!')
         self._livetime = lt
 
     @property
     def exp_field_names(self):
         """(read-only) The list of field names present in the experimental data.
         This is an empty list if there is no experimental data available.
         """
-        if(self._exp is None):
+        if self._exp is None:
             return []
         return self._exp.field_name_list
 
     @property
     def mc_field_names(self):
         """(read-only) The list of field names present in the monte-carlo data.
         """
         return self._mc.field_name_list
 
 
-def assert_data_format(dataset, data):
+def assert_data_format(
+        dataset,
+        data,
+):
     """Checks the format of the experimental and monte-carlo data.
 
+    Parameters
+    ----------
+    dataset : instance of Dataset
+        The instance of Dataset describing the dataset and holding the local
+        configuration.
+    data : instance of DatasetData
+        The instance of DatasetData holding the actual experimental and
+        simulation data of the data set.
+
     Raises
     ------
     KeyError
         If a required data field is missing.
     """
+    cfg = dataset.cfg
+
     def _get_missing_keys(keys, required_keys):
         missing_keys = []
         for reqkey in required_keys:
-            if(reqkey not in keys):
+            if reqkey not in keys:
                 missing_keys.append(reqkey)
         return missing_keys
 
-    if(data.exp is not None):
-        # Check experimental data keys.
+    if data.exp is not None:
         missing_exp_keys = _get_missing_keys(
             data.exp.field_name_list,
-            CFG['dataset']['analysis_required_exp_field_names'])
-        if(len(missing_exp_keys) != 0):
-            raise KeyError('The following data fields are missing for the '
-                'experimental data of dataset "%s": '%(dataset.name)+
+            DataFields.get_joint_names(
+                datafields=cfg['datafields'],
+                stages=(
+                    DFS.ANALYSIS_EXP
+                )
+            )
+        )
+        if len(missing_exp_keys) != 0:
+            raise KeyError(
+                'The following data fields are missing for the experimental '
+                f'data of dataset "{dataset.name}": '
                 ', '.join(missing_exp_keys))
 
-    if(data.mc is not None):
-        # Check monte-carlo data keys.
+    if data.mc is not None:
         missing_mc_keys = _get_missing_keys(
             data.mc.field_name_list,
-            CFG['dataset']['analysis_required_exp_field_names'] +
-            CFG['dataset']['analysis_required_mc_field_names'])
-        if(len(missing_mc_keys) != 0):
-            raise KeyError('The following data fields are missing for the '
-                'monte-carlo data of dataset "%s": '%(dataset.name)+
+            DataFields.get_joint_names(
+                datafields=cfg['datafields'],
+                stages=(
+                    DFS.ANALYSIS_EXP |
+                    DFS.ANALYSIS_MC
+                )
+            )
+        )
+        if len(missing_mc_keys) != 0:
+            raise KeyError(
+                'The following data fields are missing for the monte-carlo '
+                f'data of dataset "{dataset.name}": '
                 ', '.join(missing_mc_keys))
 
-    if(data.livetime is None):
-        raise ValueError('No livetime was specified for dataset "{}"!'.format(
-            dataset.name))
+    if data.livetime is None:
+        raise ValueError(
+            f'No livetime was specified for dataset "{dataset.name}"!')
 
 
-def remove_events(data_exp, mjds):
+def remove_events(
+        data_exp,
+        mjds,
+):
     """Utility function to remove events having the specified MJD time stamps.
 
     Parameters
     ----------
-    data_exp : numpy record ndarray
-        The numpy record ndarray holding the experimental data events.
+    data_exp : instance of DataFieldRecordArray
+        The instance of DataFieldRecordArray holding the experimental data
+        events.
     mjds : float | array of floats
         The MJD time stamps of the events, that should get removed from the
         experimental data array.
 
     Returns
     -------
-    data_exp : numpy record ndarray
-        The array holding the experimental data events with the specified events
-        removed.
+    data_exp : instance of DataFieldRecordArray
+        The instance of DataFieldRecordArray holding the experimental data
+        events with the specified events removed.
     """
     mjds = np.atleast_1d(mjds)
 
+    mask = np.zeros((len(data_exp)), dtype=np.bool_)
     for time in mjds:
-        mask = data_exp['time'] == time
-        if(np.sum(mask) > 1):
-            raise LookupError('The MJD time stamp %f is not unique!'%(time))
-        data_exp = data_exp[~mask]
+        m = data_exp['time'] == time
+        if np.count_nonzero(m) > 1:
+            raise LookupError(
+                f'The MJD time stamp {time} is not unique!')
+        mask |= m
+    data_exp = data_exp[~mask]
 
     return data_exp
 
+
 def generate_data_file_root_dir(
-    default_base_path, default_sub_path_fmt,
-    version, verqualifiers,
-    base_path=None, sub_path_fmt=None
+        default_base_path,
+        default_sub_path_fmt,
+        version,
+        verqualifiers,
+        base_path=None,
+        sub_path_fmt=None,
 ):
     """Generates the root directory of the data files based on the given base
     path and sub path format. If base_path is None, default_base_path is used.
     If sub_path_fmt is None, default_sub_path_fmt is used.
 
-    The default_sub_path_fmt and sub_path_fmt arguments can contain the
+    The ``default_sub_path_fmt`` and ``sub_path_fmt`` arguments can contain the
     following wildcards:
-        - '{version:d}'
-        - '{<verqualifiers_key>:d}'
+
+        ``{version:d}``
+            The version integer number of the dataset.
+        ``{<verqualifiers_key>:d}``
+            The integer number of the specific version qualifier
+            ``'verqualifiers_key'``.
 
     Parameters
     ----------
     default_base_path : str
         The default base path if base_path is None.
     default_sub_path_fmt : str
         The default sub path format if sub_path_fmt is None.
@@ -1743,66 +2063,83 @@
         The user-specified sub path format.
 
     Returns
     -------
     root_dir : str
         The generated root directory of the data files.
     """
-    if(base_path is None):
-        if(default_base_path is None):
-            raise ValueError('The default_base_path argument must not be None, '
-                'when the base_path argument is set to None!')
+    if base_path is None:
+        if default_base_path is None:
+            raise ValueError(
+                'The default_base_path argument must not be None, when the '
+                'base_path argument is set to None!')
         base_path = default_base_path
 
-    if(sub_path_fmt is None):
+    if sub_path_fmt is None:
         sub_path_fmt = default_sub_path_fmt
 
-    fmtdict = dict( [('version', version)] + list(verqualifiers.items()) )
+    fmtdict = dict(
+        [('version', version)] + list(verqualifiers.items())
+    )
     sub_path = sub_path_fmt.format(**fmtdict)
 
     root_dir = os.path.join(base_path, sub_path)
 
     return root_dir
 
-def get_data_subset(data, livetime, t_start, t_end):
-    """Gets DatasetData and Livetime objects with data subsets between the given
-    time range from t_start to t_end.
+
+def get_data_subset(
+        data,
+        livetime,
+        t_start,
+        t_stop,
+):
+    """Gets instance of DatasetData and instance of Livetime with data subsets
+    between the given time range from ``t_start`` to ``t_stop``.
 
     Parameters
     ----------
     data : DatasetData
         The DatasetData object.
     livetime : Livetime
         The Livetime object.
     t_start : float
         The MJD start time of the time range to consider.
     t_end : float
         The MJD end time of the time range to consider.
 
     Returns
     -------
-    dataset_data_subset : DatasetData
-        DatasetData object with subset of the data between the given time range
-        from t_start to t_end.
-    livetime_subset : Livetime
-        Livetime object with subset of the data between the given time range
-        from t_start to t_end.
+    data_subset : instance of DatasetData
+        The instance of DatasetData with subset of the data between the given
+        time range from ``t_start`` to ``t_stop``.
+    livetime_subset : instance of Livetime
+        The instance of Livetime for a subset of the data between the given
+        time range from ``t_start`` to ``t_stop``.
     """
-    if(not isinstance(data, DatasetData)):
-        raise TypeError('The "data" argument must be of type DatasetData!')
-    if(not isinstance(livetime, Livetime)):
-        raise TypeError('The "livetime" argument must be of type Livetime!')
-
-    exp_slice = np.logical_and(data.exp['time'] >= t_start,
-                               data.exp['time'] < t_end)
-    mc_slice = np.logical_and(data.mc['time'] >= t_start,
-                              data.mc['time'] < t_end)
+    if not isinstance(data, DatasetData):
+        raise TypeError(
+            'The "data" argument must be of type DatasetData!')
+    if not isinstance(livetime, Livetime):
+        raise TypeError(
+            'The "livetime" argument must be of type Livetime!')
+
+    exp_slice = np.logical_and(
+        data.exp['time'] >= t_start,
+        data.exp['time'] < t_stop)
+    mc_slice = np.logical_and(
+        data.mc['time'] >= t_start,
+        data.mc['time'] < t_stop)
 
     data_exp = data.exp[exp_slice]
     data_mc = data.mc[mc_slice]
 
-    uptime_mjd_intervals_arr = livetime.get_ontime_intervals_between(t_start, t_end)
+    uptime_mjd_intervals_arr = livetime.get_uptime_intervals_between(
+        t_start, t_stop)
     livetime_subset = Livetime(uptime_mjd_intervals_arr)
 
-    dataset_data_subset = DatasetData(data_exp, data_mc, livetime_subset.livetime)
+    data_subset = DatasetData(
+        data_exp=data_exp,
+        data_mc=data_mc,
+        livetime=livetime_subset.livetime)
 
-    return (dataset_data_subset, livetime_subset)
+    return (data_subset, livetime_subset)
```

### Comparing `skyllh-23.1.1/skyllh/core/detsigyield.py` & `skyllh-23.2.0/skyllh/core/detsigyield.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,343 +1,362 @@
 # -*- coding: utf-8 -*-
 
 import abc
-import numpy as np
 
-from astropy import units
-
-from skyllh.core.py import issequenceofsubclass
-from skyllh.core.dataset import Dataset, DatasetData
-from skyllh.core.livetime import Livetime
-from skyllh.physics.source import SourceModel
-from skyllh.physics.flux import FluxModel
-
-
-def get_integrated_livetime_in_days(livetime):
-    """Gets the integrated live-time in days from the given livetime argument.
-
-    Parameters
-    ----------
-    livetime : float | Livetime instance
-        The live-time in days as float, or an instance of Livetime.
-
-    Returns
-    -------
-    livetime_days : float
-        The integrated live-time in days.
-    """
-    livetime_days = livetime
-    if(isinstance(livetime, Livetime)):
-        livetime_days = livetime.livetime
-    return livetime_days
-
-
-class DetSigYield(object, metaclass=abc.ABCMeta):
+from skyllh.core.config import (
+    Config,
+    HasConfig,
+)
+from skyllh.core.dataset import (
+    Dataset,
+    DatasetData,
+)
+from skyllh.core.flux_model import (
+    FluxModel,
+)
+from skyllh.core.livetime import (
+    Livetime,
+)
+from skyllh.core.progressbar import (
+    ProgressBar,
+)
+from skyllh.core.py import (
+    classname,
+    issequence,
+    issequenceof,
+)
+from skyllh.core.types import (
+    SourceHypoGroup_t,
+)
+
+
+class DetSigYield(
+        object,
+        metaclass=abc.ABCMeta,
+):
     """This is the abstract base class for a detector signal yield.
 
-    The detector signal yield, Y_s(x_s,p_s), is defined as the expected mean
-    number of signal events detected by the detector from a given source at
-    position x_s with flux fit parameters p_s.
+    The detector signal yield, Y_s(p_s), is defined as the expected mean
+    number of signal events detected by the detector from a given source with
+    source parameters p_s.
 
     To construct a detector signal yield object, four ingredients are
     needed: the dataset holding the monte-carlo data events, a signal flux
-    model, the live time, and an implementation method that knows howto contruct
+    model, the livetime, and a builder instance that knows how to construct
     the actual detector yield in an efficient way.
     In general, the implementation method depends on the detector, the source,
     the flux model with its flux model's signal parameters, and the dataset.
     Hence, for a given detector, source, flux model, and dataset, an appropriate
     implementation method needs to be chosen.
     """
-
-    def __init__(self, implmethod, dataset, fluxmodel, livetime):
+    def __init__(
+            self,
+            param_names,
+            dataset,
+            fluxmodel,
+            livetime,
+            **kwargs,
+    ):
         """Constructs a new detector signal yield object. It takes
         the monte-carlo data events, a flux model of the signal, and the live
         time to compute the detector signal yield.
 
         Parameters
         ----------
-        implmethod : instance of DetSigYieldImplMethod
-            The implementation method to use for constructing and receiving
-            the detector signal yield. The appropriate method depends on
-            the used flux model.
+        param_names : sequence of str
+            The sequence of parameter names this detector signal yield depends
+            on. These are either fixed or floating parameters.
         dataset : Dataset instance
             The Dataset instance holding the monte-carlo event data.
         fluxmodel : FluxModel
             The flux model instance. Must be an instance of FluxModel.
         livetime : float | Livetime
             The live-time in days to use for the detector signal yield.
         """
-        super(DetSigYield, self).__init__()
+        super().__init__(**kwargs)
 
-        self.implmethod = implmethod
+        self.param_names = param_names
         self.dataset = dataset
         self.fluxmodel = fluxmodel
         self.livetime = livetime
 
     @property
-    def implmethod(self):
-        return self._implmethod
-    @implmethod.setter
-    def implmethod(self, method):
-        if(not isinstance(method, DetSigYieldImplMethod)):
-            raise TypeError('The implmethod property must be an instance of '
-                'DetSigYieldImplMethod!')
-        self._implmethod = method
+    def param_names(self):
+        """The tuple of parameter names this detector signal yield instance
+        is a function of.
+        """
+        return self._param_names
+
+    @param_names.setter
+    def param_names(self, names):
+        if not issequence(names):
+            names = [names]
+        if not issequenceof(names, str):
+            raise TypeError(
+                'The param_names property must be a sequence of str '
+                'instances! '
+                f'Its current type is {classname(names)}.')
+        self._param_names = tuple(names)
 
     @property
     def dataset(self):
         """The Dataset instance, for which this detector signal yield is made
         for.
         """
         return self._dataset
+
     @dataset.setter
     def dataset(self, ds):
-        if(not isinstance(ds, Dataset)):
-            raise TypeError('The dataset property must be an instance of '
-                'Dataset!')
+        if not isinstance(ds, Dataset):
+            raise TypeError(
+                'The dataset property must be an instance of Dataset! '
+                f'Its current type is {classname(ds)}.')
         self._dataset = ds
 
     @property
     def fluxmodel(self):
         """The flux model, which should be used to calculate the detector
         signal yield.
         """
         return self._fluxmodel
+
     @fluxmodel.setter
     def fluxmodel(self, model):
-        if(not isinstance(model, FluxModel)):
-           raise TypeError('The fluxmodel property must be an instance of '
-               'FluxModel!')
+        if not isinstance(model, FluxModel):
+            raise TypeError(
+                'The fluxmodel property must be an instance of FluxModel! '
+                f'Its current type is {classname(model)}.')
         self._fluxmodel = model
 
     @property
     def livetime(self):
         """The live-time in days.
         """
         return self._livetime
+
     @livetime.setter
     def livetime(self, lt):
-        if(not (isinstance(lt, float) or isinstance(lt, Livetime))):
-            raise TypeError('The livetime property must be of type float or '
-                'an instance of Livetime!')
+        if not (isinstance(lt, float) or isinstance(lt, Livetime)):
+            raise TypeError(
+                'The livetime property must be of type float or an instance '
+                'of Livetime! '
+                f'Its current type is {classname(lt)}.')
         self._livetime = lt
 
-    @property
-    def n_fitparams(self):
-        """(read-only) The number of fit parameters this detector signal yield
-        depends on.
-        """
-        return self._implmethod.n_fitparams
-
-    @property
-    def fitparam_names(self):
-        """(read-only) The list of fit parameter names this detector signal
-        yield depends on.
-        """
-        return self._implmethod.fitparam_names
-
-    def source_to_array(self, source):
-        """Converts the (sequence of) source(s) into a numpy record array needed
-        for the __call__ method. This convertion is intrinsic to the
-        implementation method.
+    @abc.abstractmethod
+    def sources_to_recarray(
+            self,
+            sources,
+    ):
+        """This method is supposed to convert a (list of) source model(s) into
+        a numpy record array that is understood by the detector signal yield
+        class.
+        This is for efficiency reasons only. This way the user code can
+        pre-convert the list of sources into a numpy record array and cache the
+        array.
+        The fields of the array are detector signal yield implementation
+        dependent, i.e. what kind of sources: point-like source, or extended
+        source for instance. Because the sources usually don't change their
+        position in the sky, this has to be done only once.
 
         Parameters
         ----------
-        source : SourceModel | sequence of SourceModel
-            The source model containing the spatial information of the source.
+        sources : SourceModel | sequence of SourceModel
+            The source model(s) containing the information of the source(s).
 
         Returns
         -------
-        arr : numpy record ndarray
-            The generated numpy record ndarray holding the spatial information
-            for each source.
+        recarr : numpy record ndarray
+            The generated (N_sources,)-shaped 1D numpy record ndarray holding
+            the information for each source.
         """
-        return self._implmethod.source_to_array(source)
+        pass
 
     @abc.abstractmethod
-    def __call__(self, src, src_flux_params):
+    def __call__(
+            self,
+            src_recarray,
+            src_params_recarray,
+    ):
         """Abstract method to retrieve the detector signal yield for the given
-        sources and source flux parameters.
+        sources and source parameter values.
 
         Parameters
         ----------
-        src : numpy record ndarray
-            The numpy record array containing the spatial information of the
-            signal sources. The required fields of this record array are
-            implementation method dependent. In the most generic case for a
-            point-like source, it must contain the following three fields:
-            ra, dec, time.
-        src_flux_params : numpy record ndarray
-            The numpy record ndarray containing the flux parameters of the
-            sources. The flux parameters can be different for the different
+        src_recarray : (N_sources,)-shaped numpy record ndarray
+            The numpy record array containing the information of the sources.
+            The required fields of this record array are implementation
+            dependent. In the most generic case for a point-like source, it
+            must contain the following three fields: ra, dec.
+        src_params_recarray : (N_sources,)-shaped numpy record ndarray
+            The numpy record ndarray containing the parameter values of the
+            sources. The parameter values can be different for the different
             sources.
+            The record array must contain two fields for each source parameter,
+            one named <name> with the source's local parameter name
+            holding the source's local parameter value, and one named
+            <name:gpidx> holding the global parameter index plus one for each
+            source value. For values mapping to non-fit parameters, the index
+            should be negative.
 
         Returns
         -------
         detsigyield : (N_sources,)-shaped 1D ndarray of float
             The array with the mean number of signal in the detector for each
             given source.
-        grads : None | (N_sources,N_fitparams)-shaped 2D ndarray
-            The gradient of the detector signal yield w.r.t. each fit
-            parameter for each source. If the detector signal yield depends
-            on no fit parameter, None is returned.
+        grads : dict
+            The dictionary holding the gradient values for each global fit
+            parameter. The key is the global fit parameter index and the value
+            is the (N_sources,)-shaped numpy ndarray holding the gradient value
+            dY_k/dp_s.
         """
         pass
 
 
-class DetSigYieldImplMethod(object, metaclass=abc.ABCMeta):
-    """Abstract base class for an implementation method of a detector signal
-    yield. Via the ``construct_detsigyield`` method it creates a DetSigYield
-    instance holding the internal objects to calculate the detector signal
-    yield.
+class DetSigYieldBuilder(
+        HasConfig,
+        metaclass=abc.ABCMeta,
+):
+    """Abstract base class for a builder of a detector signal yield. Via the
+    ``construct_detsigyield`` method it creates a DetSigYield instance holding
+    the internal objects to calculate the detector signal yield.
     """
 
-    def __init__(self, **kwargs):
-        super(DetSigYieldImplMethod, self).__init__(**kwargs)
-
-        self.supported_sourcemodels = ()
-        self.supported_fluxmodels = ()
-
-    @property
-    def supported_sourcemodels(self):
-        """The tuple with the SourceModel classes, which are supported by this
-        detector signal yield implementation method.
-        """
-        return self._supported_sourcemodels
-    @supported_sourcemodels.setter
-    def supported_sourcemodels(self, models):
-        if(not isinstance(models, tuple)):
-            raise TypeError('The supported_sourcemodels property must be of '
-                'type tuple!')
-        if(not issequenceofsubclass(models, SourceModel)):
-            raise TypeError('The supported_sourcemodels property must be a '
-                'sequence of SourceModel classes!')
-        self._supported_sourcemodels = models
-
-    @property
-    def supported_fluxmodels(self):
-        """The tuple with the FluxModel classes, which are supported by this
-        detector signal yield implementation method.
-        """
-        return self._supported_fluxmodels
-    @supported_fluxmodels.setter
-    def supported_fluxmodels(self, models):
-        if(not isinstance(models, tuple)):
-            raise TypeError('The supported_fluxmodels property must be of '
-                'type tuple!')
-        if(not issequenceofsubclass(models, FluxModel)):
-            raise TypeError('The supported_fluxmodels property must be a '
-                'sequence of FluxModel instances!')
-        self._supported_fluxmodels = models
-
-    @property
-    def n_signal_fitparams(self):
-        """(read-only) The number of signal fit parameters the detector signal
-        yield depends on.
-        """
-        return len(self._get_signal_fitparam_names())
-
-    @property
-    def signal_fitparam_names(self):
-        """(read-only) The list of fit parameter names the detector signal
-        yield depends on. An empty list indicates that it does not depend
-        on any fit parameter.
-        """
-        return self._get_signal_fitparam_names()
-
-    def _get_signal_fitparam_names(self):
-        """This method must be re-implemented by the derived class and needs to
-        return the list of fit parameter names, this detector signal yield
-        is a function of. If it returns an empty list, the detector signal
-        yield is independent of any fit parameters.
+    def __init__(
+            self,
+            **kwargs,
+    ):
+        """Constructor.
+        """
+        super().__init__(
+            **kwargs)
+
+    def assert_types_of_construct_detsigyield_arguments(
+            self,
+            dataset,
+            data,
+            shgs,
+            ppbar,
+    ):
+
+        """Checks the types of the arguments for the ``construct_detsigyield``
+        method. It raises errors if the arguments have the wrong type.
+        """
+        if not isinstance(dataset, Dataset):
+            raise TypeError(
+                'The dataset argument must be an instance of Dataset! '
+                f'Its current type is {classname(dataset)}.')
+
+        if not isinstance(data, DatasetData):
+            raise TypeError(
+                'The data argument must be an instance of DatasetData! '
+                f'Its current type is {classname(data)}.')
+
+        if (not isinstance(shgs, SourceHypoGroup_t)) and\
+           (not issequenceof(shgs, SourceHypoGroup_t)):
+            raise TypeError(
+                'The shgs argument must be an instance of SourceHypoGroup '
+                'or a sequence of SourceHypoGroup instances!'
+                f'Its current type is {classname(shgs)}.')
+
+        if ppbar is not None:
+            if not isinstance(ppbar, ProgressBar):
+                raise TypeError(
+                    'The ppbar argument must be an instance of ProgressBar! '
+                    f'Its current type is {classname(ppbar)}.')
+
+    def get_detsigyield_construction_factory(self):
+        """This method is supposed to return a callable with the call-signature
+
+        .. code::
+
+            __call__(
+                dataset,
+                data,
+                shgs,
+                ppbar,
+            )
+
+
+        to construct several DetSigYield instances, one for each provided
+        source hypo group (i.e. sources and fluxmodel).
+        The return value of this callable must be a sequence of DetSigYield
+        instances of the same length as the sequence of ``shgs``.
 
         Returns
         -------
-        list of str
-            The list of the fit parameter names, this detector signal yield
-            is a function of. By default this method returns an empty list
-            indicating that the detector signal yield depends on no fit
-            parameter.
-        """
-        return []
-
-    def supports_sourcemodel(self, sourcemodel):
-        """Checks if the given source model is supported by this detected signal
-        yield implementation method.
-        """
-        for ssm in self._supported_sourcemodels:
-            if(isinstance(sourcemodel, ssm)):
-                return True
-        return False
-
-    def supports_fluxmodel(self, fluxmodel):
-        """Checks if the given flux model is supported by this detector signal
-        yield implementation method.
-        """
-        for sfm in self._supported_fluxmodels:
-            if(isinstance(fluxmodel, sfm)):
-                return True
-        return False
+        factory : callable | None
+            This default implementation returns ``None``, indicating that a
+            factory is not supported by this builder.
+        """
+        return None
 
     @abc.abstractmethod
-    def construct_detsigyield(self, dataset, data, fluxmodel, livetime):
+    def construct_detsigyield(
+            self,
+            dataset,
+            data,
+            shg,
+            ppbar=None,
+    ):
         """Abstract method to construct the DetSigYield instance.
         This method must be called by the derived class method implementation
         to ensure the compatibility check of the given flux model with the
         supported flux models.
 
         Parameters
         ----------
-        dataset : Dataset
-            The Dataset instance holding possible dataset specific settings.
-        data : DatasetData
-            The DatasetData instance holding the monte-carlo event data.
-        fluxmodel : FluxModel
-            The flux model instance. Must be an instance of FluxModel.
-        livetime : float | Livetime
-            The live-time in days to use for the detector signal yield.
+        dataset : instance of Dataset
+            The instance of Dataset holding possible dataset specific settings.
+        data : instance of DatasetData
+            The instance of DatasetData holding the monte-carlo event data.
+        shg : instance of SourceHypoGroup
+            The instance of SourceHypoGroup (i.e. sources and flux model) for
+            which the detector signal yield should be constructed.
+        ppbar : instance of ProgressBar | None
+            The instance of ProgressBar of the optional parent progress bar.
 
         Returns
         -------
-        detsigyield : DetSigYield instance
+        detsigyield : instance of DetSigYield
             An instance derived from DetSigYield.
         """
-        if(not isinstance(dataset, Dataset)):
-            raise TypeError('The dataset argument must be an instance of '
-                'Dataset!')
-        if(not isinstance(data, DatasetData)):
-            raise TypeError('The data argument must be an instance of '
-                'DatasetData!')
-        if(not self.supports_fluxmodel(fluxmodel)):
-            raise TypeError('The DetSigYieldImplMethod "%s" does not support '
-                'the flux model "%s"!'%(
-                    self.__class__.__name__,
-                    fluxmodel.__class__.__name__))
-        if((not isinstance(livetime, float)) and
-           (not isinstance(livetime, Livetime))):
-            raise TypeError('The livetime argument must be an instance of '
-                'float or Livetime!')
+        pass
 
-    @abc.abstractmethod
-    def source_to_array(self, source):
-        """This method is supposed to convert a (list of) source model(s) into
-        a numpy record array that is understood by the implementation method.
-        This is for efficiency reasons only. This way the user code can
-        pre-convert the list of sources into a numpy record array and cache the
-        array.
-        The fields of the array are detector signal yield implementation
-        dependent, i.e. what kind of sources: point-like source, or extended
-        source for instance. Because the sources usually don't change their
-        position in the sky, this has to be done only once.
+
+class NullDetSigYieldBuilder(
+        DetSigYieldBuilder):
+    """This class provides a dummy detector signal yield builder, which can
+    be used for testing purposes, when an actual builder is not required.
+    """
+    def __init__(
+            self,
+            cfg=None,
+            **kwargs,
+    ):
+        """Creates a new instance of NullDetSigYieldBuilder.
 
         Parameters
         ----------
-        source : SourceModel | sequence of SourceModel
-            The source model containing the spatial information of the source.
-
-        Returns
-        -------
-        arr : numpy record ndarray
-            The generated numpy record ndarray holding the spatial information
-            for each source.
-        """
-        pass
+        cfg : instance of Config | None
+            The instance of Config holding the local configuration. Since this
+            detector signal yield builder does nothing, this argument is
+            optional. If not provided the default configuration is used.
+        """
+        if cfg is None:
+            cfg = Config()
+
+        super().__init__(
+            cfg=cfg,
+            **kwargs)
+
+    def construct_detsigyield(
+            self,
+            *args,
+            **kwargs,
+    ):
+        """Since this is a dummy detector signal yield builder, calling this
+        method will raise a NotImplementedError!
+        """
+        raise NotImplementedError(
+            f'The {classname(self)} detector signal yield builder cannot '
+            'actually build a DetSigYield instance!')
```

### Comparing `skyllh-23.1.1/skyllh/core/display.py` & `skyllh-23.2.0/skyllh/core/display.py`

 * *Files 2% similar despite different names*

```diff
@@ -36,8 +36,8 @@
         The text with new line characters for each line.
 
     Returns
     -------
     padded_text : str
         The text where each line is padded with the given number of whitespaces.
     """
-    return '\n'.join([ ' '*padwidth + line for line in text.split('\n') ])
+    return '\n'.join([' '*padwidth + line for line in text.split('\n')])
```

### Comparing `skyllh-23.1.1/skyllh/core/expectation_maximization.py` & `skyllh-23.2.0/skyllh/core/expectation_maximization.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,68 +1,75 @@
 import numpy as np
 from scipy.stats import norm
 
-from skyllh.core.analysis import TimeIntegratedMultiDatasetSingleSourceAnalysis
-from skyllh.core.backgroundpdf import BackgroundUniformTimePDF
-from skyllh.core.pdf import TimePDF
-from skyllh.core.pdfratio import SigOverBkgPDFRatio
-from skyllh.core.random import RandomStateService
-from skyllh.core.signalpdf import (
-    SignalBoxTimePDF,
-    SignalGaussTimePDF,
-)
 
-
-def expectation_em(ns, mu, sigma, t, sob):
-    """
-    Expectation step of expectation maximization.
+def em_expectation_step(
+        ns,
+        mu,
+        sigma,
+        t,
+        sob,
+):
+    """Expectation step of expectation maximization algorithm.
 
     Parameters
     ----------
-    ns : float | 1d ndarray of float
-        The number of signal neutrinos, as weight for the gaussian flare.
-    mu : float | 1d ndarray of float
-        The mean time of the gaussian flare.
-    sigma: float | 1d ndarray of float
-        Sigma of the gaussian flare.
-    t : 1d ndarray of float
-        Times of the events.
-    sob : 1d ndarray of float
-        The signal over background values of events, or weights of events
+    ns : instance of ndarray
+        The (n_flares,)-shaped numpy ndarray holding the number of signal
+        neutrinos, as weight for each gaussian flare.
+    mu : instance of ndarray
+        The (n_flares,)-shaped numpy ndarray holding the mean for each gaussian
+        flare.
+    sigma: instance of ndarray
+        The (n_flares,)-shaped numpy ndarray holding the sigma for each gaussian
+        flare.
+    t : instance of ndarray
+        The (n_events,)-shaped numpy ndarray holding the time of each event.
+    sob : instance of ndarray
+        The (n_events,)-shaped numpy ndarray holding the signal-over-background
+        values of each event.
 
     Returns
     -------
-    expectation : list of 1d ndarray of float
-        Weighted "responsibility" function of each event to belong to the flare.
-    sum_log_denom : float
-        Sum of log of denominators.
+    expectations : instane of ndarray
+        The (n_flares, n_events)-shaped numpy ndarray holding the expectation
+        of each flare and event.
+    llh : float
+        The log-likelihood value, which is the sum of log of the signal and
+        background expectations.
     """
-    ns = np.atleast_1d(ns)
-    mu = np.atleast_1d(mu)
-    sigma = np.atleast_1d(sigma)
+    n_flares = len(ns)
 
     b_term = (1 - np.cos(10 / 180 * np.pi)) / 2
     N = len(t)
-    e_sig = []
-    for i in range(len(ns)):
-        e_sig.append(norm(loc=mu[i], scale=sigma[i]).pdf(t) * sob * ns[i])
-    e_bg = (N - np.sum(ns)) / (np.max(t) - np.min(t)) / b_term
-    denom = sum(e_sig) + e_bg
-
-    return [e / denom for e in e_sig], np.sum(np.log(denom))
-
-
-def maximization_em(e_sig, t):
-    """
-    Maximization step of expectation maximization.
+    e_sig = np.empty((n_flares, N), dtype=np.float64)
+    for i in range(n_flares):
+        e_sig[i] = norm(loc=mu[i], scale=sigma[i]).pdf(t)
+        e_sig[i] *= sob
+        e_sig[i] *= ns[i]
+    e_bkg = (N - np.sum(ns)) / (np.max(t) - np.min(t)) / b_term
+    denom = np.sum(e_sig, axis=0) + e_bkg
+
+    expectations = e_sig / denom
+    llh = np.sum(np.log(denom))
+
+    return (expectations, llh)
+
+
+def em_maximization_step(
+        e,
+        t,
+):
+    """The maximization step of the expectation maximization algorithm.
 
     Parameters
     ----------
-    e_sig : list of 1d ndarray of float
-        The weights for each event from the expectation step.
+    e : instance of ndarray
+        The (n_flares, n_events)-shaped numpy ndarray holding the expectation
+        for each event and flare.
     t : 1d ndarray of float
         The times of each event.
 
     Returns
     -------
     mu : list of float
         Best fit mean time of the gaussian flare.
@@ -70,83 +77,105 @@
         Best fit sigma of the gaussian flare.
     ns : list of float
         Best fit number of signal neutrinos, as weight for the gaussian flare.
     """
     mu = []
     sigma = []
     ns = []
-    for i in range(len(e_sig)):
-        mu.append(np.average(t, weights=e_sig[i]))
-        sigma.append(np.sqrt(np.average(np.square(t - mu[i]), weights=e_sig[i])))
-        ns.append(np.sum(e_sig[i]))
+    for i in range(e.shape[0]):
+        mu.append(np.average(t, weights=e[i]))
+        sigma.append(np.sqrt(np.average(np.square(t - mu[i]), weights=e[i])))
+        ns.append(np.sum(e[i]))
     sigma = [max(1, s) for s in sigma]
 
-    return mu, sigma, ns
+    return (mu, sigma, ns)
 
 
-def em_fit(x, weights, n=1, tol=1.e-200, iter_max=500, weight_thresh=0, initial_width=5000,
-            remove_x=None):
-    """Run expectation maximization.
-    
+def em_fit(
+        x,
+        weights,
+        n=1,
+        tol=1.e-200,
+        iter_max=500,
+        weight_thresh=0,
+        initial_width=5000,
+        remove_x=None,
+):
+    """Perform the expectation maximization fit.
+
     Parameters
     ----------
-    x : array[float]
-        Quantity to run EM on (e.g. the time if EM should find time flares)
-    weights :
-        weights for each event (e.g. the signal over background ratio)
-    fitparams : dict
-        Dictionary with value for gamma, e.g. {'gamma': 2}.
+    x : array of float
+        The quantity to run EM on (e.g. the time if EM should find time flares).
+    weights : array of float
+        The weights for each x value (e.g. the signal over background ratio).
     n : int
         How many Gaussians flares we are looking for.
     tol : float
-        the stopping criteria for expectation maximization. This is the difference in the normalized likelihood over the
-        last 20 iterations.
+        The stopping criteria for the expectation maximization. This is the
+        difference in the normalized likelihood over the last 20 iterations.
     iter_max : int
-        The maximum number of iterations, even if stopping criteria tolerance (`tol`) is not yet reached.
+        The maximum number of iterations, even if stopping criteria tolerance
+        (``tol``) is not yet reached.
     weight_thresh : float
-        Set a minimum threshold for event weights. Events with smaller weights will be removed.
+        Set a minimum threshold for event weights. Events with smaller weights
+        will be removed.
     initial_width : float
-        Starting width for the gaussian flare in days.
+        The starting width for the gaussian flare in days.
     remove_x : float | None
         Specific x of event that should be removed.
-    
+
     Returns
     -------
-    Mean, width, normalization factor
+    mu : list of float
+        The list of size ``n`` with the determined mean values.
+    sigma : list of float
+        The list of size ``n`` with the standard deviation values.
+    ns : list of float
+        The list of size ``n`` with the normalization factor values.
     """
-
-    if weight_thresh > 0: # remove events below threshold
+    if weight_thresh > 0:
+        # Remove events below threshold.
         for i in range(len(weights)):
             mask = weights > weight_thresh
             weights[i] = weights[i][mask]
             x[i] = x[i][mask]
 
-    # in case, remove event
     if remove_x is not None:
+        # Remove data point.
         mask = x == remove_x
         weights = weights[~mask]
         x = x[~mask]
 
-    # expectation maximization
+    # Do the expectation maximization.
     mu = np.linspace(x[0], x[-1], n+2)[1:-1]
-    sigma = np.ones(n) * initial_width
-    ns = np.ones(n) * 10
+    sigma = np.full((n,), initial_width)
+    ns = np.full((n,), 10)
+
     llh_diff = 100
     llh_old = 0
     llh_diff_list = [100] * 20
 
+    # Run until convergence or maximum number of iterations is reached.
     iteration = 0
-
-    while iteration < iter_max and llh_diff > tol: # run until convergence or maximum number of iterations
+    while (iteration < iter_max) and (llh_diff > tol):
         iteration += 1
 
-        e, logllh = expectation_em(ns, mu, sigma, x, weights)
+        (e, llh_new) = em_expectation_step(
+            ns=ns,
+            mu=mu,
+            sigma=sigma,
+            t=x,
+            sob=weights)
 
-        llh_new = np.sum(logllh)
         tmp_diff = np.abs(llh_old - llh_new) / llh_new
         llh_diff_list = llh_diff_list[:-1]
         llh_diff_list.insert(0, tmp_diff)
         llh_diff = np.max(llh_diff_list)
+
         llh_old = llh_new
-        mu, sigma, ns = maximization_em(e, x)
 
-    return mu, sigma, ns
+        (mu, sigma, ns) = em_maximization_step(
+            e=e,
+            t=x)
+
+    return (mu, sigma, ns)
```

### Comparing `skyllh-23.1.1/skyllh/core/interpolate.py` & `skyllh-23.2.0/skyllh/i3/pdfratio.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,467 +1,545 @@
 # -*- coding: utf-8 -*-
 
-"""This module provides functionality for interpolation.
-"""
+import scipy.interpolate
 
-import abc
 import numpy as np
 
-from skyllh.core.parameters import (
-    ParameterGrid,
-    ParameterGridSet
+from numpy.lib.recfunctions import (
+    repack_fields,
 )
-from skyllh.core.py import classname
 
+from skyllh.core.multiproc import (
+    IsParallelizable,
+    parallelize,
+)
+from skyllh.core.pdfratio import (
+    SigSetOverBkgPDFRatio,
+)
+from skyllh.core.pdfratio_fill import (
+    MostSignalLikePDFRatioFillMethod,
+    PDFRatioFillMethod,
+)
+from skyllh.core.py import (
+    make_dict_hash,
+)
 
-class GridManifoldInterpolationMethod(object, metaclass=abc.ABCMeta):
-    """This is an abstract base class for implementing a method to interpolate
-    a manifold that is defined on a grid of parameter values. In general the
-    number of parameters can be arbitrary and hence the manifold's
-    dimensionality can be arbitrary, too. However, in practice the interpolation
-    on a multi-dimensional manifold can be rather difficult.
-    Nevertheless, we provide this interface to allow for manifold grids with
-    different dimensionality.
-    """
 
-    def __init__(self, f, param_grid_set):
-        """Constructor for a GridManifoldInterpolationMethod object.
-        It must be called by the derived class.
+class SplinedI3EnergySigSetOverBkgPDFRatio(
+        SigSetOverBkgPDFRatio,
+        IsParallelizable):
+    """This class implements a splined signal over background PDF ratio for
+    enegry PDFs of type I3EnergyPDF.
+    It takes an instance, which is derived from PDFSet, and which is derived
+    from IsSignalPDF, as signal PDF. Furthermore, it takes an instance, which
+    is derived from I3EnergyPDF and IsBackgroundPDF, as background PDF, and
+    creates a spline for the ratio of the signal and background PDFs for a grid
+    of different discrete energy signal parameters, which are defined by the
+    signal PDF set.
+    """
+    def __init__(
+            self,
+            sig_pdf_set,
+            bkg_pdf,
+            fillmethod=None,
+            interpolmethod_cls=None,
+            ncpu=None,
+            ppbar=None,
+            **kwargs,
+    ):
+        """Creates a new IceCube signal-over-background energy PDF ratio spline
+        instance.
 
         Parameters
         ----------
-        f : callable R^d -> R
-            The function that takes d parameters as input and returns the
-            value of the d-dimensional manifold at this point for each given
-            event.
-            The call signature of f must be:
-
-                ``__call__(tdm, gridparams, eventdata)``
-
-            where ``tdm`` is the TrialDataManager instance holding the trial
-            data, ``gridparams`` is the dictionary with the parameter values
-            on the grid, and ``eventdata`` is a 2-dimensional (N,V)-shaped numpy
-            ndarray holding the event data, where N is the number of events, and
-            V the dimensionality of the event data.
-        param_grid_set : ParameterGrid instance | ParameterGridSet instance
-            The set of d parameter grids. This defines the grid of the
-            manifold.
-        """
-        super(GridManifoldInterpolationMethod, self).__init__()
-
-        self.f = f
-        self.param_grid_set = param_grid_set
+        sig_pdf_set : class instance derived from PDFSet (for PDF type
+                       I3EnergyPDF), IsSignalPDF, and UsesBinning
+            The PDF set, which provides signal energy PDFs for a set of
+            discrete signal parameters.
+        bkg_pdf : class instance derived from I3EnergyPDF, and
+                        IsBackgroundPDF
+            The background energy PDF object.
+        fillmethod : instance of PDFRatioFillMethod | None
+            An instance of class derived from PDFRatioFillMethod that implements
+            the desired ratio fill method.
+            If set to None (default), the default ratio fill method
+            MostSignalLikePDFRatioFillMethod will be used.
+        interpolmethod_cls : class of GridManifoldInterpolationMethod
+            The class implementing the parameter interpolation method for
+            the PDF ratio manifold grid.
+        ncpu : int | None
+            The number of CPUs to use to create the ratio splines for the
+            different sets of signal parameters.
+        ppbar : ProgressBar instance | None
+            The instance of ProgressBar of the optional parent progress bar.
+
+        Raises
+        ------
+        ValueError
+            If the signal and background PDFs use different binning.
+        """
+        super().__init__(
+            sig_pdf_set=sig_pdf_set,
+            bkg_pdf=bkg_pdf,
+            interpolmethod_cls=interpolmethod_cls,
+            ncpu=ncpu,
+            **kwargs)
+
+        # Define the default ratio fill method.
+        if fillmethod is None:
+            fillmethod = MostSignalLikePDFRatioFillMethod()
+        self.fillmethod = fillmethod
+
+        # Ensure same binning of signal and background PDFs.
+        for sig_pdf in self._sig_pdf_set.values():
+            if not sig_pdf.has_same_binning_as(self._bkg_pdf):
+                raise ValueError(
+                    'At least one signal PDF does not have the same binning '
+                    'as the background PDF!')
+
+        def create_log_ratio_spline(
+                sig_pdf_set,
+                bkg_pdf,
+                fillmethod,
+                gridparams):
+            """Creates the signal/background ratio spline for the given signal
+            parameters.
+
+            Returns
+            -------
+            log_ratio_spline : instance of RegularGridInterpolator
+                The spline of the logarithmic PDF ratio values.
+            """
+            # Get the signal PDF for the given signal parameters.
+            sig_pdf = sig_pdf_set[gridparams]
+
+            # Create the ratio array with the same shape than the background pdf
+            # histogram.
+            ratio = np.ones_like(bkg_pdf.hist, dtype=np.float64)
+
+            # Fill the ratio array.
+            ratio = fillmethod(
+                ratio,
+                sig_pdf.hist,
+                bkg_pdf.hist,
+                sig_pdf.hist_mask_mc_covered,
+                sig_pdf.hist_mask_mc_covered_zero_physics,
+                bkg_pdf.hist_mask_mc_covered,
+                bkg_pdf.hist_mask_mc_covered_zero_physics)
+
+            # Define the grid points for the spline. In general, we use the bin
+            # centers of the binning, but for the first and last point of each
+            # dimension we use the lower and upper bin edge, respectively, to
+            # ensure full coverage of the spline across the binning range.
+            points_list = []
+            for binning in sig_pdf.binnings:
+                points = binning.bincenters
+                (points[0], points[-1]) = (
+                    binning.lower_edge, binning.upper_edge)
+                points_list.append(points)
+
+            # Create the spline for the ratio values.
+            log_ratio_spline = scipy.interpolate.RegularGridInterpolator(
+                tuple(points_list),
+                np.log(ratio),
+                method='linear',
+                bounds_error=False,
+                fill_value=0.)
+
+            return log_ratio_spline
+
+        # Get the list of parameter permutations on the grid for which we
+        # need to create PDF ratio splines.
+        gridparams_list = self._sig_pdf_set.gridparams_list
+
+        args_list = [
+            ((self._sig_pdf_set,
+              self._bkg_pdf,
+              self._fillmethod,
+              gridparams),
+             {})
+            for gridparams in gridparams_list
+        ]
+
+        log_ratio_spline_list = parallelize(
+            func=create_log_ratio_spline,
+            args_list=args_list,
+            ncpu=self.ncpu,
+            ppbar=ppbar)
+
+        # Save all the log_ratio splines in a dictionary.
+        self._gridparams_hash_log_ratio_spline_dict = dict()
+        for (gridparams, log_ratio_spline) in zip(gridparams_list,
+                                                  log_ratio_spline_list):
+            gridparams_hash = make_dict_hash(gridparams)
+            self._gridparams_hash_log_ratio_spline_dict[gridparams_hash] =\
+                log_ratio_spline
+
+        # Save the list of data field names.
+        self._data_field_names = [
+            binning.name
+            for binning in self._bkg_pdf.binnings
+        ]
+
+        # Construct the instance for the parameter interpolation method.
+        self._interpolmethod = self._interpolmethod_cls(
+            func=self._evaluate_splines,
+            param_grid_set=sig_pdf_set.param_grid_set)
+
+        # Save the parameter names needed for the interpolation for later usage.
+        self._interpol_param_names = \
+            self._sig_pdf_set.param_grid_set.params_name_list
+
+        # Create cache variable for the last ratio values and gradients in order
+        # to avoid the recalculation of the ratio value when the
+        # ``get_gradient`` method is called (usually after the ``get_ratio``
+        # method was called).
+        self._cache = self._create_cache(
+            trial_data_state_id=None,
+            interpol_params_recarray=None,
+            ratio=None,
+            grads=None
+        )
 
     @property
-    def f(self):
-        """The R^d -> R manifold function.
-        """
-        return self._f
-    @f.setter
-    def f(self, func):
-        if(not callable(func)):
-            raise TypeError('The f property must be a callable object!')
-        self._f = func
-
-    @property
-    def param_grid_set(self):
-        """The ParameterGridSet object defining the set of d parameter grids.
-        This defines the grid of the manifold.
-        """
-        return self._param_grid_set
-    @param_grid_set.setter
-    def param_grid_set(self, obj):
-        if(isinstance(obj, ParameterGrid)):
-            obj = ParameterGridSet([obj])
-        elif(not isinstance(obj, ParameterGridSet)):
-            raise TypeError('The param_grid_set property must be an instance '
-                'of ParameterGridSet!')
-        self._param_grid_set = obj
-
-    @property
-    def ndim(self):
-        """(read-only) The dimensionality of the manifold.
-        """
-        return len(self._param_grid_set)
-
-    @abc.abstractmethod
-    def get_value_and_gradients(self, tdm, eventdata, params):
-        """Retrieves the interpolated value of the manifold at the d-dimensional
-        point ``params`` for all given events, along with the d gradients,
-        i.e. partial derivatives.
+    def fillmethod(self):
+        """The PDFRatioFillMethod object, which should be used for filling the
+        PDF ratio bins.
+        """
+        return self._fillmethod
+
+    @fillmethod.setter
+    def fillmethod(self, obj):
+        if not isinstance(obj, PDFRatioFillMethod):
+            raise TypeError(
+                'The fillmethod property must be an instance of '
+                'PDFRatioFillMethod!')
+        self._fillmethod = obj
+
+    def _create_cache(
+            self,
+            trial_data_state_id,
+            interpol_params_recarray,
+            ratio,
+            grads):
+        """Creates a cache dictionary holding cache data.
 
         Parameters
         ----------
-        tdm : TrialDataManager
-            The TrialDataManager instance holding the trial data.
-        eventdata : numpy (N_events,V)-shaped 2D ndarray
-            The 2D (N_events,V)-shaped numpy ndarray holding the event data,
-            where N_events is the number of events, and V the dimensionality of
-            the event data.
-        params : dict
-            The dictionary with the parameter values, defining the point on the
-            manifold for which the value should get calculated.
+        trial_data_state_id : int | None
+            The trial data state ID of the TrialDataManager.
+        interpol_params_recarray : instance of numpy record ndarray | None
+            The numpy record ndarray of length N_sources holding the parameter
+            names and values necessary for the interpolation for all sources.
+        ratio : instance of numpy ndarray
+            The (N_values,)-shaped numpy ndarray holding the PDF ratio values
+            for all sources and trial events.
+        grads : instance of numpy ndarray
+            The (D,N_values)-shaped numpy ndarray holding the gradients for each
+            PDF ratio value w.r.t. each interpolation parameter.
+        """
+        cache = {
+            'trial_data_state_id': trial_data_state_id,
+            'interpol_params_recarray': interpol_params_recarray,
+            'ratio': ratio,
+            'grads': grads
+        }
 
-        Returns
-        -------
-        value : (N,) ndarray of float
-            The interpolated manifold value for the N given events.
-        gradients : (D,N) ndarray of float
-            The D manifold gradients for the N given events, where D is the
-            number of parameters. The order of the D parameters is defined
-            by the ParameterGridSet that has been provided at construction time
-            of this interpolation method object.
+        return cache
+
+    def _is_cached(self, trial_data_state_id, interpol_params_recarray):
+        """Checks if the ratio and gradients for the given set of interpolation
+        parameters are already cached.
         """
-        pass
+        if self._cache['trial_data_state_id'] is None:
+            return False
 
+        if self._cache['trial_data_state_id'] != trial_data_state_id:
+            return False
 
-class NullGridManifoldInterpolationMethod(GridManifoldInterpolationMethod):
-    """This grid manifold interpolation method performes no interpolation. When
-    the ``get_value_and_gradients`` method is called, it rounds the parameter
-    values to their nearest grid point values. All gradients are set to zero.
-    """
-    def __init__(self, f, param_grid_set):
-        """Creates a new NullGridManifoldInterpolationMethod instance.
+        if not np.all(
+                self._cache['interpol_params_recarray'] ==
+                interpol_params_recarray):
+            return False
 
-        Parameters
-        ----------
-        f : callable R^D -> R
-            The function that takes the parameter grid value as input and
-            returns the value of the n-dimensional manifold at this point for
-            each given event.
-
-                ``__call__(tdm, gridparams, eventdata)``
-
-            where ``gridparams`` is the dictionary with the parameter names and
-            values on the grid, and ``eventdata`` is a 2-dimensional
-            (N,V)-shaped numpy ndarray holding the event data, where N is the
-            number of events, and V the dimensionality of the event data.
-            The return value of ``f`` must be a (N,)-shaped 1d ndarray of float.
-        param_grid_set : ParameterGrid instance | ParameterGridSet instance
-            The set of parameter grids. This defines the grid of the
-            D-dimensional manifold.
-        """
-        super(NullGridManifoldInterpolationMethod, self).__init__(
-            f, param_grid_set)
+        return True
 
-    def get_value_and_gradients(self, tdm, eventdata, params):
-        """Calculates the non-interpolated manifold value and its gradient
-        (zero) for each given event at the point ``params``.
-        By definition the D values of ``params`` must coincide with the
-        parameter grid values.
+    def _get_spline_for_param_values(self, interpol_param_values):
+        """Retrieves the spline for a given set of parameter values.
 
         Parameters
         ----------
-        tdm : TrialDataManager
-            The TrialDataManager instance holding the trial data.
-        eventdata : numpy (N_events,V)-shaped 2D ndarray
-            The 2D (N_events,V)-shaped numpy ndarray holding the event data,
-            where N_events is the number of events, and V the dimensionality of
-            the event data.
-        params : dict
-            The dictionary with the D parameter values, defining the point on
-            the manifold for which the value should get calculated.
+        interpol_param_values : instance of numpy ndarray
+            The (N_interpol_params,)-shaped numpy ndarray holding the values of
+            the interpolation parameters.
 
         Returns
         -------
-        value : (N,) ndarray of float
-            The interpolated manifold value for the N given events.
-        gradients : (D,N) ndarray of float
-            The D manifold gradients for the N given events, where D is the
-            number of parameters.
-            By definition, all gradients are zero.
+        spline : instance of scipy.interpolate.RegularGridInterpolator
+            The requested spline instance.
         """
-        # Round the given parameter values to their nearest grid values.
-        gridparams = dict()
-        for (pname,pvalue) in params.items():
-            p_grid = self._param_grid_set[pname]
-            gridparams[pname] = p_grid.round_to_nearest_grid_point(pvalue)
-
-        value = self._f(tdm, gridparams, eventdata)
-        gradients = np.zeros(
-            (len(params), tdm.n_selected_events), dtype=np.float64)
-
-        return (value, gradients)
-
-
-class Linear1DGridManifoldInterpolationMethod(GridManifoldInterpolationMethod):
-    """This grid manifold interpolation method interpolates the 1-dimensional
-    grid manifold using a line.
-    """
-    def __init__(self, f, param_grid_set):
-        """Creates a new Linear1DGridManifoldInterpolationMethod instance.
+        gridparams = dict(
+            zip(self._interpol_param_names, interpol_param_values))
+        gridparams_hash = make_dict_hash(gridparams)
+
+        spline = self._gridparams_hash_log_ratio_spline_dict[gridparams_hash]
+
+        return spline
+
+    def _evaluate_splines(
+            self,
+            tdm,
+            eventdata,
+            gridparams_recarray,
+            n_values):
+        """For each set of parameter values given by ``gridparams_recarray``,
+        the spline is retrieved and evaluated for the events suitable for that
+        source model.
 
         Parameters
         ----------
-        f : callable R -> R
-            The function that takes the parameter grid value as input and
-            returns the value of the 1-dimensional manifold at this point for
-            each given event.
-
-                ``__call__(tdm, gridparams, eventdata)``
-
-            where ``gridparams`` is the dictionary with the parameter names and
-            values on the grid, and ``eventdata`` is a 2-dimensional
-            (N,V)-shaped numpy ndarray holding the event data, where N is the
-            number of events, and V the dimensionality of the event data.
-            The return value of ``f`` must be a (N,)-shaped 1d ndarray of float.
-        param_grid_set : ParameterGrid instance | ParameterGridSet instance
-            The set of parameter grids. This defines the grid of the
-            1-dimensional manifold. By definition, only the first parameter grid
-            is considered.
+        tdm : instance of TrialDataManager
+            The TrialDataManager instance holding the trial data and the event
+            mapping to the sources via the ``src_evt_idx`` property.
+        eventdata : instance of numpy ndarray
+            The (N_events,V)-shaped numpy ndarray holding the event data, where
+            N_events is the number of events, and V the dimensionality of the
+            event data.
+        gridparams_recarray : instance of numpy structured ndarray
+            The numpy structured ndarray of length N_sources with the parameter
+            names and values needed for the interpolation on the grid for all
+            sources. If the length of this record array is 1, the set of
+            parameters will be used for all sources.
+        n_values : int
+            The size of the output array.
+
+        Returns
+        -------
+        values : instance of ndarray
+            The (N_values,)-shaped numpy ndarray holding the values for each set
+            of parameter values of the ``gridparams_recarray``. The length of
+            the array depends on the ``src_evt_idx`` property of the
+            TrialDataManager. In the worst case it is
+            ``N_sources * N_selected_events``.
         """
-        super(Linear1DGridManifoldInterpolationMethod, self).__init__(
-            f, param_grid_set)
+        (src_idxs, evt_idxs) = tdm.src_evt_idxs
 
-        if(len(self._param_grid_set) != 1):
-            raise ValueError('The %s class supports only 1D grid manifolds. '
-                'The param_grid_set argument must contain 1 ParameterGrid '
-                'instance! Currently it has %d!'%(
-                    classname(self), len(self._param_grid_set)))
-        self.p_grid = self._param_grid_set[0]
-
-        # Create a cache for the line parameterization for the last
-        # manifold grid point for the different events.
-        self._create_cache(None, np.array([]), np.array([]))
-        self._cache_tdm_trial_data_state_id = None
-
-    def _create_cache(self, x0, m, b):
-        """Creates a cache for the line parameterization for the last manifold
-        grid point for the nevents different events.
+        # Check for special case when a single set of parameters are provided.
+        if len(gridparams_recarray) == 1:
+            # We got a single parameter set. We will use it for all sources.
+            spline = self._get_spline_for_param_values(gridparams_recarray[0])
 
-        Parameters
-        ----------
-        x0 : float | None
-            The parameter grid value for the lower point of the grid manifold
-            used to estimate the line.
-        m : 1d ndarray
-            The slope of the line for each event.
-        b : 1d ndarray
-            The offset coefficient of the line for each event.
-        """
-        self._cache = {
-            'x0': x0,
-            'm': m,
-            'b': b
-        }
+            eventdata = np.take(eventdata, evt_idxs, axis=0)
+            values = spline(eventdata)
 
-    def get_value_and_gradients(self, tdm, eventdata, params):
-        """Calculates the interpolated manifold value and its gradient for each
-        given event at the point ``params``.
+            return values
 
-        Parameters
-        ----------
-        tdm : TrialDataManager
-            The TrialDataManager instance holding the trial data.
-        eventdata : numpy (N_events,V)-shaped 2D ndarray
-            The 2D (N_events,V)-shaped numpy ndarray holding the event data,
-            where N_events is the number of events, and V the dimensionality of
-            the event data.
-        params : dict
-            The dictionary with the parameter values, defining the point on the
-            manifold for which the value should get calculated.
+        values = np.empty(n_values, dtype=np.float64)
 
-        Returns
-        -------
-        value : (N,) ndarray of float
-            The interpolated manifold value for the N given events.
-        gradients : (D,N) ndarray of float
-            The D manifold gradients for the N given events, where D is the
-            number of parameters.
-        """
-        (xname, x) = tuple(params.items())[0]
+        v_start = 0
+        for (sidx, param_values) in enumerate(gridparams_recarray):
+            spline = self._get_spline_for_param_values(param_values)
 
-        self__p_grid = self.p_grid
+            # Select the eventdata that belongs to the current source.
+            m = src_idxs == sidx
+            src_eventdata = np.take(eventdata, evt_idxs[m], axis=0)
 
-        # Determine the nearest grid point that is lower than x and use that as
-        # x0.
-        x0 = self__p_grid.round_to_lower_grid_point(x)
-
-        # Check if the line parametrization for x0 is already cached.
-        self__cache = self._cache
-
-        tdm_trial_data_state_id = tdm.trial_data_state_id
-        cache_tdm_trial_data_state_id = self._cache_tdm_trial_data_state_id
-
-        if((self__cache['x0'] == x0) and
-           (tdm.n_selected_events == len(self__cache['m'])) and
-           (cache_tdm_trial_data_state_id is not None) and
-           (cache_tdm_trial_data_state_id == tdm_trial_data_state_id)
-          ):
-            m = self__cache['m']
-            b = self__cache['b']
-        else:
-            # Calculate the line parametrization for all the given events.
-            self__f = self.f
-
-            # Calculate the upper grid point of x.
-            x1 = self__p_grid.round_to_upper_grid_point(x)
-
-            # Check if x was on a grid point. In that case x0 and x1 are equal.
-            # The value will be of that grid point x0, but the gradient is
-            # calculated based on the two neighboring grid points of x0.
-            if(x1 == x0):
-                value = self__f(tdm, {xname:x0}, eventdata)
-                x0 = self__p_grid.round_to_nearest_grid_point(
-                    x0 - self__p_grid.delta)
-                x1 = self__p_grid.round_to_nearest_grid_point(
-                    x1 + self__p_grid.delta)
-
-                M0 = self__f(tdm, {xname:x0}, eventdata)
-                M1 = self__f(tdm, {xname:x1}, eventdata)
-                m = (M1 - M0) / (x1 - x0)
-                return (value, np.atleast_2d(m))
-
-            M0 = self__f(tdm, {xname:x0}, eventdata)
-            M1 = self__f(tdm, {xname:x1}, eventdata)
-
-            m = (M1 - M0) / (x1 - x0)
-            b = M0 - m*x0
-
-            # Cache the line parametrization.
-            self._create_cache(x0, m, b)
-            self._cache_tdm_trial_data_state_id = tdm_trial_data_state_id
-
-        # Calculate the interpolated manifold value. The gradient is m.
-        value = m*x + b
-
-        return (value, np.atleast_2d(m))
-
-
-class Parabola1DGridManifoldInterpolationMethod(GridManifoldInterpolationMethod):
-    """This grid manifold interpolation method interpolates the 1-dimensional
-    grid manifold using a parabola.
-    """
-    def __init__(self, f, param_grid_set):
-        """Creates a new Parabola1DGridManifoldInterpolationMethod instance.
+            n = src_eventdata.shape[0]
+            sl = slice(v_start, v_start+n)
+            values[sl] = spline(src_eventdata)
 
-        Parameters
-        ----------
-        f : callable R -> R
-            The function that takes the parameter grid value as input and
-            returns the value of the 1-dimensional manifold at this point for
-            each given event.
-            The call signature of f must be:
-
-                ``__call__(tdm, gridparams, eventdata)``
-
-            where ``gridparams`` is the dictionary with the parameter names and
-            values on the grid, and ``eventdata`` is a 2-dimensional
-            (N,V)-shaped numpy ndarray holding the event data, where N is the
-            number of events, and V the dimensionality of the event data.
-        param_grid_set : instance of ParameterGridSet
-            The set of parameter grids. This defines the grid of the
-            1-dimensional manifold. By definition, only the first parameter grid
-            is considered.
-        """
-        super(Parabola1DGridManifoldInterpolationMethod, self).__init__(
-            f, param_grid_set)
+            v_start += n
+
+        return values
 
-        if(len(self._param_grid_set) != 1):
-            raise ValueError('The %s class supports only 1D grid manifolds. '
-                'The param_grid_set argument must contain 1 ParameterGrid '
-                'instance! Currently it has %d!'%(
-                    classname(self), len(self._param_grid_set)))
-        self._p_grid = self._param_grid_set[0]
-
-        # Create a cache for the parabola parameterization for the last
-        # manifold grid point for the different events.
-        self._create_cache(None, np.array([]), np.array([]), np.array([]))
-        self._cache_tdm_trial_data_state_id = None
-
-    def _create_cache(self, x1, M1, a, b):
-        """Creates a cache for the parabola parameterization for the last
-        manifold grid point for the nevents different events.
+    def _create_interpol_params_recarray(self, src_params_recarray):
+        """Creates the params_recarray needed for the interpolation. It selects
+        The interpolation parameters from the ``params_recarray`` argument.
+        If all parameters have the same value for all sources, the length will
+        be 1.
 
         Parameters
         ----------
-        x1 : float | None
-            The parameter grid value for middle point of the grid manifold used
-            to estimate the parabola.
-        M1 : 1d ndarray
-            The grid manifold value for each event of the middle point (x1,).
-        a : 1d ndarray
-            The parabola coefficient ``a`` for each event.
-        b : 1d ndarray
-            The parabola coefficient ``b`` for each event.
-        """
-        self._cache = {
-            'x1': x1,
-            'M1': M1,
-            'a': a,
-            'b': b
-        }
+        src_params_recarray : instance of numpy record ndarray
+            The numpy record ndarray of length N_sources holding all local
+            parameter names and values.
 
-    def get_value_and_gradients(self, tdm, eventdata, params):
-        """Calculates the interpolated manifold value and its gradient for each
-        given event at the point ``params``.
+        Returns
+        -------
+        interpol_params_recarray : instance of numpy record ndarray
+            The numpy record ndarray of length N_sources or 1 holding only the
+            parameters needed for the interpolation.
+        """
+        interpol_params_recarray = repack_fields(
+            src_params_recarray[self._interpol_param_names])
+
+        all_params_are_equal_for_all_sources = True
+        for pname in self._interpol_param_names:
+            if not np.all(
+                    np.isclose(np.diff(interpol_params_recarray[pname]), 0)):
+                all_params_are_equal_for_all_sources = False
+                break
+        if all_params_are_equal_for_all_sources:
+            return interpol_params_recarray[:1]
+
+        return interpol_params_recarray
+
+    def _calculate_ratio_and_grads(
+            self,
+            tdm,
+            interpol_params_recarray):
+        """Calculates the ratio values and ratio gradients for all the sources
+        and trial events given the source parameter values.
+        The result is stored in the class member variable ``_cache``.
 
         Parameters
         ----------
-        tdm : TrialDataManager
+        tdm : instance of TrialDataManager
             The TrialDataManager instance holding the trial data.
-        eventdata : numpy (N_events,V)-shaped 2D ndarray
-            The 2D (N_events,V)-shaped numpy ndarray holding the event data,
-            where N_events is the number of events, and V the dimensionality of
-            the event data.
-        params : dict
-            The dictionary with the parameter values, defining the point on the
-            manifold for which the value should get calculated.
+        interpol_params_recarray : instance of numpy record ndarray
+            The numpy record ndarray of length N_sources holding the parameter
+            names and values for all sources.
+            It must contain only the parameters necessary for the interpolation.
+        """
+        # Create a 2D event data array holding only the needed event data fields
+        # for the PDF ratio spline evaluation.
+        eventdata = np.vstack([tdm[fn] for fn in self._data_field_names]).T
+
+        (ratio, grads) = self._interpolmethod(
+            tdm=tdm,
+            eventdata=eventdata,
+            params_recarray=interpol_params_recarray)
+
+        # The interpolation works on the logarithm of the ratio spline, hence
+        # we need to transform it using the exp function, and we need to account
+        # for the exp function in the gradients.
+        ratio = np.exp(ratio)
+        grads = ratio * grads
+
+        # Cache the value and the gradients.
+        self._cache = self._create_cache(
+            trial_data_state_id=tdm.trial_data_state_id,
+            interpol_params_recarray=interpol_params_recarray,
+            ratio=ratio,
+            grads=grads
+        )
+
+    def get_ratio(
+            self,
+            tdm,
+            src_params_recarray,
+            tl=None):
+        """Retrieves the PDF ratio values for each given trial event data, given
+        the given set of fit parameters. This method is called during the
+        likelihood maximization process.
+        For computational efficiency reasons, the gradients are calculated as
+        well and will be cached.
+
+        Parameters
+        ----------
+        tdm : instance of TrialDataManager
+            The TrialDataManager instance holding the trial event data for which
+            the PDF ratio values should get calculated.
+        src_params_recarray : instance of numpy record ndarray | None
+            The (N_sources,)-shaped numpy record ndarray holding the parameter
+            names and values of the sources. See the
+            :meth:`skyllh.core.parameters.ParameterModelMapper.create_src_params_recarray`
+            for more information.
+        tl : instance of TimeLord | None
+            The optional TimeLord instance that should be used to measure
+            timing information.
 
         Returns
         -------
-        value : (N,) ndarray of float
-            The interpolated manifold value for the N given events.
-        gradients : (D,N) ndarray of float
-            The D manifold gradients for the N given events, where D is the
-            number of parameters.
-        """
-        (xname, x) = tuple(params.items())[0]
+        ratio : instance of numpy ndarray
+            The (N_values,)-shaped numpy ndarray of float holding the PDF ratio
+            value for each source and trial event.
+        """
+        # Select only the parameters necessary for the interpolation.
+        interpol_params_recarray = self._create_interpol_params_recarray(
+            src_params_recarray)
+
+        # Check if the ratio values are already cached.
+        if self._is_cached(
+               trial_data_state_id=tdm.trial_data_state_id,
+               interpol_params_recarray=interpol_params_recarray):
+            return self._cache['ratio']
+
+        self._calculate_ratio_and_grads(
+            tdm=tdm,
+            interpol_params_recarray=interpol_params_recarray)
+
+        return self._cache['ratio']
+
+    def get_gradient(
+            self,
+            tdm,
+            src_params_recarray,
+            fitparam_id,
+            tl=None):
+        """Retrieves the PDF ratio gradient for the given fit parameter
+        ``fitparam_id``.
 
-        # Create local variable name alias to avoid Python dot lookups.
-        self__p_grid = self._p_grid
-        self__p_grid__round_to_nearest_grid_point = \
-            self__p_grid.round_to_nearest_grid_point
-        self__cache = self._cache
-
-        tdm_trial_data_state_id = tdm.trial_data_state_id
-        cache_tdm_trial_data_state_id = self._cache_tdm_trial_data_state_id
-
-        # Determine the nearest grid point x1.
-        x1 = self__p_grid__round_to_nearest_grid_point(x)
-
-        # Check if the parabola parametrization for x1 is already cached.
-        if((self__cache['x1'] == x1) and
-           (tdm.n_selected_events == len(self__cache['M1'])) and
-           (cache_tdm_trial_data_state_id is not None) and
-           (cache_tdm_trial_data_state_id == tdm_trial_data_state_id)
-          ):
-            M1 = self__cache['M1']
-            a = self__cache['a']
-            b = self__cache['b']
-        else:
-            dx = self__p_grid.delta
-
-            # Calculate the neighboring grid points to x1: x0 and x2.
-            x0 = self__p_grid__round_to_nearest_grid_point(x1 - dx)
-            x2 = self__p_grid__round_to_nearest_grid_point(x1 + dx)
-
-            # Parameterize the parabola with parameters a, b, and M1.
-            self__f = self.f
-            M0 = self__f(tdm, {xname:x0}, eventdata)
-            M1 = self__f(tdm, {xname:x1}, eventdata)
-            M2 = self__f(tdm, {xname:x2}, eventdata)
-
-            a = 0.5*(M0 - 2.*M1 + M2) / dx**2
-            b = 0.5*(M2 - M0) / dx
-
-            # Cache the parabola parametrization.
-            self._create_cache(x1, M1, a, b)
-            self._cache_tdm_trial_data_state_id = tdm_trial_data_state_id
-
-        # Calculate the interpolated manifold value.
-        value = a * (x - x1)**2 + b * (x - x1) + M1
-        # Calculate the gradient of the manifold.
-        gradients = 2. * a * (x - x1) + b
+        Parameters
+        ----------
+        tdm : instance of TrialDataManager
+            The TrialDataManager instance holding the trial event data for which
+            the PDF ratio gradient values should get calculated.
+        src_params_recarray : instance of numpy record ndarray | None
+            The (N_sources,)-shaped numpy record ndarray holding the local
+            parameter names and values of all sources. See the
+            :meth:`skyllh.core.parameters.ParameterModelMapper.create_src_params_recarray`
+            method for more information.
+        fitparam_id : int
+            The ID of the global fit parameter for which the gradient should
+            get calculated.
+        tl : instance of TimeLord | None
+            The optional TimeLord instance that should be used to measure
+            timing information.
 
-        return (value, np.atleast_2d(gradients))
+        Returns
+        -------
+        grad : instance of ndarray
+            The (N_values,)-shaped numpy ndarray holding the gradient values
+            for all sources and trial events w.r.t. the given global fit
+            parameter.
+        """
+        # Select only the parameters necessary for the interpolation.
+        interpol_params_recarray = self._create_interpol_params_recarray(
+            src_params_recarray)
+
+        # Calculate the gradients if necessary.
+        if not self._is_cached(
+            trial_data_state_id=tdm.trial_data_state_id,
+            interpol_params_recarray=interpol_params_recarray
+        ):
+            self._calculate_ratio_and_grads(
+                tdm=tdm,
+                interpol_params_recarray=interpol_params_recarray)
+
+        tdm_n_sources = tdm.n_sources
+
+        grad = np.zeros((tdm.get_n_values(),), dtype=np.float64)
+
+        # Loop through the parameters of the signal PDF set and match them with
+        # the global fit parameter.
+        for (pidx, pname) in enumerate(
+                self._sig_pdf_set.param_grid_set.params_name_list):
+            if pname not in src_params_recarray.dtype.fields:
+                continue
+            p_gpidxs = src_params_recarray[f'{pname}:gpidx']
+            src_mask = p_gpidxs == (fitparam_id + 1)
+            n_sources = np.count_nonzero(src_mask)
+            if n_sources == 0:
+                continue
+            if n_sources == tdm_n_sources:
+                # This parameter applies to all sources, hence to all values,
+                # and hence it's the only local parameter contributing to the
+                # global parameter fitparam_id.
+                return self._cache['grads'][pidx]
+
+            # The current parameter does not apply to all sources.
+            # Create a values mask that matches a given source mask.
+            values_mask = tdm.get_values_mask_for_source_mask(src_mask)
+            grad[values_mask] = self._cache['grads'][pidx][values_mask]
 
+        return grad
```

### Comparing `skyllh-23.1.1/skyllh/core/livetime.py` & `skyllh-23.2.0/skyllh/core/livetime.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,21 +1,52 @@
 # -*- coding: utf-8 -*-
 
 """The livetime module provides general functionality for detector up-time.
 """
+
 import numpy as np
 
-from skyllh.core.random import RandomStateService
-from skyllh.core.py import issequence, classname
+from skyllh.core.py import (
+    classname,
+    issequence,
+)
+
 
-class Livetime(object):
+class Livetime(
+        object):
     """The ``Livetime`` class defines an interface to query the up-time of the
     detector.
     """
-    def __init__(self, uptime_mjd_intervals_arr):
+
+    @staticmethod
+    def get_integrated_livetime(livetime):
+        """Gets the integrated live-time from the given livetime argument, which
+        is either a scalar value or an instance of Livetime.
+
+        Parameters
+        ----------
+        livetime : float | Livetime instance
+            The live-time in days as float, or an instance of Livetime.
+
+        Returns
+        -------
+        intgrated_livetime : float
+            The integrated live-time.
+        """
+        intgrated_livetime = livetime
+
+        if isinstance(livetime, Livetime):
+            intgrated_livetime = livetime.livetime
+
+        return intgrated_livetime
+
+    def __init__(
+            self,
+            uptime_mjd_intervals_arr,
+            **kwargs):
         """Creates a new Livetime object from a (N,2)-shaped ndarray holding
         the uptime intervals.
 
         Parameters
         ----------
         uptime_mjd_intervals_arr : (N,2)-shaped ndarray
             The (N,2)-shaped ndarray holding the start and end times of each
@@ -28,53 +59,83 @@
 
             Note 3: The intervals must not overlap.
 
             The integrity of the internal mjd interval array will be ensured by
             the property setter method of ``uptime_mjd_intervals_arr`` by
             calling the ``assert_mjd_intervals_integrity`` method.
         """
-        # The internal Nx2 numpy ndarray holding the MJD intervals when the
-        # detector was taking data.
+        super().__init__(**kwargs)
+
         self.uptime_mjd_intervals_arr = uptime_mjd_intervals_arr
 
-    def assert_mjd_intervals_integrity(self):
-        """Checks if the internal MJD interval array conforms with all its
+    def assert_mjd_intervals_integrity(
+            self,
+            arr):
+        """Checks if the given MJD interval array conforms with all its
         data requirements.
 
-        Raises TypeError if the data array is not a float64 array.
-        Raises ValueError if the data integrity is broken.
-        """
-        if(not isinstance(self._uptime_mjd_intervals_arr, np.ndarray)):
-            raise TypeError('The internal MJD interval array must be of type ndarray!')
+        Parameters
+        ----------
+        arr : instance of numpy ndarray
+            The (N,2)-shaped numpy ndarray holding the up-time intervals.
 
-        if(self._uptime_mjd_intervals_arr.dtype != np.float64):
-            raise TypeError('The type of the internal MJD interval array is not float64!')
+        Raises
+        ------
+        TypeError
+            If the data array is not a float64 array.
+        ValueError
+            If the data integrity is broken.
+        """
+        if not isinstance(arr, np.ndarray):
+            raise TypeError(
+                'The internal MJD interval array must be of type ndarray! '
+                'Its current type is '
+                f'{classname(arr)}!')
+
+        if arr.dtype != np.float64:
+            raise TypeError(
+                'The type of the internal MJD interval array is not float64!')
 
         # Check the shape of the array.
-        if(self._uptime_mjd_intervals_arr.ndim != 2):
-            raise ValueError('The dimensionality of the internel MJD interval array must be 2!')
-        if(self._uptime_mjd_intervals_arr.shape[1] != 2):
-            raise ValueError('The length of the second axis of the internal MJD interval array must be 2!')
+        if arr.ndim != 2:
+            raise ValueError(
+                'The dimensionality of the internel MJD interval array must '
+                'be 2! Its current dimensionality is '
+                f'{arr.ndim}!')
+        if arr.shape[1] != 2:
+            raise ValueError(
+                'The length of the second axis of the internal MJD interval '
+                'array must be 2! Its current length is '
+                f'{arr.shape[1]}!')
 
-        bins = self._onoff_intervals
         # Check if the bin edges are monotonically non decreasing.
-        if(not np.all(np.diff(bins) >= 0)):
-            raise ValueError('The interval edges of the internal MJD interval array are not monotonically non decreasing!')
+        diff = np.diff(arr.flat)
+        if not np.all(diff >= 0):
+            info = ''
+            for i in range(len(diff)-1):
+                if diff[i] < 0:
+                    info += f'i={int(i/2)}: {arr[int(i/2)]}\n'
+                    info += f'i={int(i/2)+1}: {arr[int(i/2)+1]}\n'
+            raise ValueError(
+                'The interval edges of the internal MJD interval array are not '
+                'monotonically non-decreasing!\n'
+                f'{info}')
 
     @property
     def uptime_mjd_intervals_arr(self):
         """The Nx2 numpy ndarray holding the up-time intervals of the detector.
         The first and second elements of the second axis is the start and stop
         time of the up-time interval, respectively.
         """
         return self._uptime_mjd_intervals_arr
+
     @uptime_mjd_intervals_arr.setter
     def uptime_mjd_intervals_arr(self, arr):
+        self.assert_mjd_intervals_integrity(arr)
         self._uptime_mjd_intervals_arr = arr
-        self.assert_mjd_intervals_integrity()
 
     @property
     def n_uptime_mjd_intervals(self):
         """The number of on-time intervals defined.
         """
         return self._uptime_mjd_intervals_arr.shape[0]
 
@@ -87,44 +148,53 @@
 
     @property
     def time_window(self):
         """(read-only) The two-element tuple holding the time window which is
         spanned by all the MJD uptime intervals.
         By definition this included possible detector down-time periods.
         """
-        return (self._uptime_mjd_intervals_arr[0,0],
-                self._uptime_mjd_intervals_arr[-1,1])
+        return (self._uptime_mjd_intervals_arr[0, 0],
+                self._uptime_mjd_intervals_arr[-1, 1])
 
     @property
     def time_start(self):
-        """(read-only) The start of the detector live-time.
-        """
-        return self._uptime_mjd_intervals_arr[0,0]
-
-    @property
-    def time_end(self):
-        """(read-only) The end of the detector live-time.
+        """(read-only) The start time of the detector live-time.
         """
-        return self._uptime_mjd_intervals_arr[-1,1]
+        return self._uptime_mjd_intervals_arr[0, 0]
 
     @property
-    def _onoff_intervals(self):
-        """A view on the mjd intervals where each time is a lower bin edge.
-        Hence, odd array elements (bins) are on-time intervals, and even array
-        elements are off-time intervals.
+    def time_stop(self):
+        """(read-only) The stop time of the detector live-time.
         """
-        return np.reshape(self._uptime_mjd_intervals_arr, (self._uptime_mjd_intervals_arr.size,))
+        return self._uptime_mjd_intervals_arr[-1, 1]
 
     def __str__(self):
         """Pretty string representation of the Livetime class instance.
         """
-        s = '%s(time_window=(%.6f, %.6f))'%(
-            classname(self), self.time_window[0], self.time_window[1])
+        s = (f'{classname(self)}(time_window=('
+             f'{self.time_window[0]:.6f}, {self.time_window[1]:.6f}))')
         return s
 
+    def _get_onoff_intervals(self):
+        """A view on the uptime intervals where each time is a lower bin edge.
+        Hence, odd array elements (bins) are on-time intervals, and even array
+        elements are off-time intervals.
+
+        Returns
+        -------
+        onoff_intervals : instance of numpy ndarray
+            The (n_uptime_intervals*2,)-shaped numpy ndarray holding the time
+            edges of the uptime intervals.
+        """
+        onoff_intervals = np.reshape(
+            self._uptime_mjd_intervals_arr,
+            (self._uptime_mjd_intervals_arr.size,))
+
+        return onoff_intervals
+
     def _get_onoff_interval_indices(self, mjds):
         """Retrieves the indices of the on-time and off-time intervals, which
         correspond to the given MJD values.
 
         Odd indices correspond to detector on-time intervals and even indices
         to detector off-time intervals.
 
@@ -145,30 +215,22 @@
             given MJD values.
         """
         # Get the interval indices.
         # Note: For MJD values outside the total interval range, the np.digitize
         # function will return either 0, or len(bins). Since, there is always
         # an even amount of intervals edges, and 0 is also an 'even' number,
         # those MJDs will correspond to off-time automatically.
-        idxs = np.digitize(mjds, self._onoff_intervals)
+        idxs = np.digitize(mjds, self._get_onoff_intervals())
 
         return idxs
 
-    def load_from_ontime_mjd_intervals(self, intervals):
-        """Loads the internal MJD uptime intervals from the given interval
-        array.
-
-        Parameters
-        ----------
-        intervals : Nx2 ndarray holding the MJD edges of the on-time intervals.
-
-        """
-        self._uptime_mjd_intervals = intervals
-
-    def get_ontime_intervals_between(self, t_start, t_end):
+    def get_uptime_intervals_between(
+            self,
+            t_start,
+            t_end):
         """Creates a (N,2)-shaped ndarray holding the on-time detector intervals
         between the given time range from t_start to t_end.
 
         Parameters
         ----------
         t_start : float
             The MJD start time of the time range to consider. This might be the
@@ -178,59 +240,62 @@
             upper bound of the last on-time interval.
 
         Returns
         -------
         ontime_intervals : (N,2)-shaped ndarray
             The (N,2)-shaped ndarray holding the on-time detector intervals.
         """
-        onoff_intervals = self._onoff_intervals
+        onoff_intervals = self._get_onoff_intervals()
 
-        (t_start_idx, t_end_idx) = self._get_onoff_interval_indices((t_start, t_end))
-        if(t_start_idx % 2 == 0):
+        (t_start_idx, t_end_idx) = self._get_onoff_interval_indices(
+            (t_start, t_end))
+        if t_start_idx % 2 == 0:
             # t_start is during off-time. Use the next on-time lower edge as
             # first on-time edge.
             t_start = onoff_intervals[t_start_idx]
         else:
             t_start_idx -= 1
-        if(t_end_idx % 2 == 0):
+        if t_end_idx % 2 == 0:
             # t_end is during off-time. Use the previous on-time upper edge as
             # the last on-time edge.
             t_end = onoff_intervals[t_end_idx-1]
         else:
             t_end_idx += 1
 
         # The t_start_idx and t_end_idx variables hold even indices.
         N_ontime_intervals = int((t_end_idx - t_start_idx)/2)
 
         ontime_intervals_flat = np.empty(
             (N_ontime_intervals*2,), dtype=np.float64)
         # Set the first and last on-time interval edges.
         ontime_intervals_flat[0] = t_start
         ontime_intervals_flat[-1] = t_end
-        if(N_ontime_intervals > 1):
+        if N_ontime_intervals > 1:
             # Fill also the interval edges of the intermediate on-time bins.
             ontime_intervals_flat[1:-1] = onoff_intervals[t_start_idx+1:t_end_idx-1]
 
-        ontime_intervals = np.reshape(ontime_intervals_flat, (N_ontime_intervals,2))
+        ontime_intervals = np.reshape(
+            ontime_intervals_flat,
+            (N_ontime_intervals, 2))
 
         return ontime_intervals
 
-    def get_ontime_upto(self, mjd):
-        """Calculates the cumulative detector on-time up to the given time.
+    def get_livetime_upto(self, mjd):
+        """Calculates the cumulative detector livetime up to the given time.
 
         Parameters
         ----------
         mjd : float | array of floats
-            The time in MJD up to which the detector on-time should be
+            The time in MJD up to which the detector livetime should be
             calculated.
 
         Returns
         -------
-        ontimes : float | ndarray of floats
-            The ndarray holding the cumulative detector on-time corresponding
+        livetimes : float | ndarray of floats
+            The ndarray holding the cumulative detector livetime corresponding
             to the the given MJD times.
         """
         mjds = np.atleast_1d(mjd)
 
         onoff_idxs = self._get_onoff_interval_indices(mjds)
 
         # Create a mask for all the odd indices, i.e. the MJDs falling into an
@@ -244,25 +309,32 @@
         # At this point, there could be indices of value -1 from MJD values
         # prior to the first on-time interval. So we just move all the indices
         # by one.
         idxs += 1
 
         # Create a cumulative on-time array with a leading 0 element for MJDs
         # prior to the first on-time interval.
-        ontime_bins = np.diff(self._uptime_mjd_intervals_arr).reshape((self.n_uptime_mjd_intervals,))
+        ontime_bins = np.diff(self._uptime_mjd_intervals_arr).reshape(
+            (self.n_uptime_mjd_intervals,))
         cum_ontime_bins = np.array([0], dtype=np.float64)
         cum_ontime_bins = np.append(cum_ontime_bins, np.cumsum(ontime_bins))
 
         # For odd (on-time) mjds, use the cumulative value of the previous bin
         # and add the part of the interval bin up to the mjd value.
-        ontimes = np.where(odd_idxs_mask, cum_ontime_bins[idxs-1] + mjds - self._onoff_intervals[onoff_idxs-1], cum_ontime_bins[idxs])
+        livetimes = np.where(
+            odd_idxs_mask,
+            cum_ontime_bins[idxs-1]
+            + mjds
+            - self._get_onoff_intervals()[onoff_idxs-1],
+            cum_ontime_bins[idxs])
 
-        if(not issequence(mjd)):
-            return np.asscalar(ontimes)
-        return ontimes
+        if not issequence(mjd):
+            return np.asscalar(livetimes)
+
+        return livetimes
 
     def is_on(self, mjd):
         """Checks if the detector is on at the given MJD time. MJD times
         outside any live-time interval will be masked as False.
 
         Parameters
         ----------
@@ -282,36 +354,62 @@
 
         # Mask odd indices as on-time (True) MJD values and even indices as
         # off-time (False).
         is_on = np.array(onoff_idxs & 0x1, dtype=np.bool_)
 
         return is_on
 
-    def draw_ontimes(self, rss, size):
+    def draw_ontimes(
+            self,
+            rss,
+            size,
+            t_min=None,
+            t_max=None):
         """Draws random MJD times based on the detector on-time intervals.
 
         Parameters
         ----------
         rss : RandomStateService
             The skyllh RandomStateService instance to use for drawing random
             numbers from.
         size : int
             The number of random MJD times to generate.
+        t_min : float
+            The optional minimal time to consider. If set to ``None``, the
+            start time of this Livetime instance will be used.
+        t_max : float
+            The optional maximal time to consider. If set to ``None``, the
+            end time of this Livetime instance will be used.
 
         Returns
         -------
         ontimes : ndarray
             The 1d array holding the generated MJD times.
         """
+        uptime_intervals_arr = self._uptime_mjd_intervals_arr
+
+        if t_min is not None or t_max is not None:
+            if t_min is None:
+                t_min = self.time_start
+            if t_max is None:
+                t_max = self.time_stop
+
+            uptime_intervals_arr = self.get_uptime_intervals_between(
+                t_min, t_max)
+
+        onoff_intervals = np.reshape(
+            uptime_intervals_arr,
+            (uptime_intervals_arr.size,))
+
         # Create bin array with only on-time bins. We have to mask out the
         # off-time bins.
-        ontime_bins = np.diff(self._onoff_intervals)
+        ontime_bins = np.diff(onoff_intervals)
         mask = np.invert(
             np.array(
-                np.linspace(0, ontime_bins.size-1, ontime_bins.size)%2,
+                np.linspace(0, ontime_bins.size-1, ontime_bins.size) % 2,
                 dtype=np.bool_))
         ontime_bins = ontime_bins[mask]
 
         # Create the cumulative array of the on-time bins.
         cum_ontime_bins = np.array([0], dtype=np.float64)
         cum_ontime_bins = np.append(cum_ontime_bins, np.cumsum(ontime_bins))
 
@@ -323,12 +421,12 @@
         # L = \sum (u_i - l_i)
 
         x = rss.random.uniform(0, 1, size)
         # Get the sum L of all the on-time intervals.
         L = cum_ontime_bins[-1]
         w = x*L
         idxs = np.digitize(w, cum_ontime_bins)
-        l = self._uptime_mjd_intervals_arr[:,0]
+        lower = uptime_intervals_arr[:, 0]
         y = w - cum_ontime_bins[idxs-1]
-        ontimes = l[idxs-1] + y
+        ontimes = lower[idxs-1] + y
 
         return ontimes
```

### Comparing `skyllh-23.1.1/skyllh/core/llhratio.py` & `skyllh-23.2.0/skyllh/core/llhratio.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,1486 +1,1024 @@
 # -*- coding: utf-8 -*-
 
 """The llhratio module provides classes implementing the log-likelihood ratio
 functions. In general these should be detector independent, because they
 implement the pure math of the log-likelihood ratio function.
 """
 
-from __future__ import division
-
 import abc
 import numpy as np
 
-from skyllh.core.config import CFG
-from skyllh.core.debugging import get_logger
-from skyllh.core.py import (
-    classname,
-    int_cast,
-    issequence,
-    issequenceof,
-    float_cast
+from skyllh.core.config import (
+    HasConfig,
+)
+from skyllh.core.debugging import (
+    get_logger,
 )
-from skyllh.core.source_hypothesis import SourceHypoGroupManager
-from skyllh.core.trialdata import TrialDataManager
-from skyllh.core.detsigyield import DetSigYield
 from skyllh.core.minimizer import (
     Minimizer,
     NR1dNsMinimizerImpl,
-    NRNsScan2dMinimizerImpl
+    NRNsScan2dMinimizerImpl,
 )
 from skyllh.core.parameters import (
-    SourceFitParameterMapper,
-    SingleSourceFitParameterMapper,
-    MultiSourceFitParameterMapper
+    ParameterModelMapper,
 )
 from skyllh.core.pdfratio import (
     PDFRatio,
-    SingleSourcePDFRatioArrayArithmetic
 )
-from skyllh.core.timing import TaskTimer
-from skyllh.physics.source import SourceModel
+from skyllh.core.py import (
+    classname,
+    issequenceof,
+    float_cast,
+)
+from skyllh.core.services import (
+    DatasetSignalWeightFactorsService,
+    SrcDetSigYieldWeightsService,
+)
+from skyllh.core.source_hypo_grouping import (
+    SourceHypoGroupManager,
+)
+from skyllh.core.timing import (
+    TaskTimer,
+)
+from skyllh.core.trialdata import (
+    TrialDataManager,
+)
 
 
 logger = get_logger(__name__)
 
 
-class LLHRatio(object, metaclass=abc.ABCMeta):
+class LLHRatio(
+        HasConfig,
+        metaclass=abc.ABCMeta,
+):
     """Abstract base class for a log-likelihood (LLH) ratio function.
     """
 
-    def __init__(self, minimizer):
+    def __init__(
+            self,
+            pmm,
+            minimizer,
+            **kwargs):
         """Creates a new LLH ratio function instance.
 
         Parameters
         ----------
+        pmm : instance of ParameterModelMapper
+            The instance of ParameterModelMapper providing the mapping of
+            global parameters to local parameters of individual models.
         minimizer : instance of Minimizer
             The Minimizer instance that should be used to minimize the negative
             of this log-likelihood ratio function.
         """
-        super(LLHRatio, self).__init__()
+        super().__init__(**kwargs)
 
+        self.pmm = pmm
         self.minimizer = minimizer
 
     @property
+    def pmm(self):
+        """The ParameterModelMapper instance providing the mapping of
+        global floating parameters to individual models.
+        """
+        return self._pmm
+
+    @pmm.setter
+    def pmm(self, mapper):
+        if not isinstance(mapper, ParameterModelMapper):
+            raise TypeError(
+                'The pmm property must be an instance of '
+                'ParameterModelMapper! '
+                f'Its current type is {classname(mapper)}.')
+        self._pmm = mapper
+
+    @property
     def minimizer(self):
         """The Minimizer instance used to minimize the negative of the
         log-likelihood ratio function.
         """
         return self._minimizer
+
     @minimizer.setter
     def minimizer(self, minimizer):
-        if(not isinstance(minimizer, Minimizer)):
-            raise TypeError('The minimizer property must be an instance '
-                'of Minimizer!')
+        if not isinstance(minimizer, Minimizer):
+            raise TypeError(
+                'The minimizer property must be an instance of Minimizer! '
+                f'Its current type is {classname(minimizer)}.')
         self._minimizer = minimizer
 
-    def initialize_for_new_trial(self, tl=None):
+    @abc.abstractmethod
+    def initialize_for_new_trial(
+            self,
+            tl=None,
+            **kwargs):
         """This method will be called by the Analysis class after new trial data
         has been initialized to the trial data manager. Derived classes can make
         use of this call hook to perform LLHRatio specific trial initialization.
 
         Parameters
         ----------
-        tl : TimeLord | None
-            The optional TimeLord instance to use for timing measurements.
+        tl : instance of TimeLord | None
+            The optional instance of TimeLord to use for timing measurements.
         """
         pass
 
     @abc.abstractmethod
-    def evaluate(self, fitparam_values, tl=None):
+    def evaluate(
+            self,
+            fitparam_values,
+            src_params_recarray=None,
+            tl=None):
         """This method evaluates the LLH ratio function for the given set of
         fit parameter values.
 
         Parameters
         ----------
-        fitparam_values : numpy 1D ndarray
-            The ndarray holding the current values of the (global) fit
-            parameters.
-        tl : TimeLord | None
-            The optional TimeLord instance to use for measuring timeing.
+        fitparam_values : instance of numpy ndarray
+            The (N_fitparams,)-shaped numpy 1D ndarray holding the current
+            values of the global fit parameters.
+        src_params_recarray : instance of numpy record ndarray | None
+            The numpy record ndarray of length N_sources holding the parameter
+            names and values of all sources. If set to ``None`` it will be
+            created automatically from the ``fitparam_values`` array.
+            See the documentation of the
+            :meth:`skyllh.core.parameters.ParameterModelMapper.create_src_params_recarray`
+            method for more information about this array.
+        tl : instance of TimeLord | None
+            The optional instance of TimeLord to use for measuring timing.
 
         Returns
         -------
         log_lambda : float
             The calculated log-lambda value.
-        grads : (N_fitparams,)-shaped 1D ndarray
-            The ndarray holding the gradient value for each (global) fit
-            parameter.
+        grads : instance of numpy ndarray
+            The (N_fitparams,)-shaped 1D numpy ndarray holding the gradient
+            value for each global fit parameter.
         """
         pass
 
-    def maximize(self, rss, fitparamset, tl=None):
+    def maximize(
+            self,
+            rss,
+            tl=None):
         """Maximize the log-likelihood ratio function, by using the ``evaluate``
         method.
 
         Parameters
         ----------
-        rss : RandomStateService instance
-            The RandomStateService instance to draw random numbers from.
+        rss : instance of RandomStateService
+            The instance of RandomStateService to draw random numbers from.
             This is needed to generate random parameter initial values.
-        fitparamset : FitParameterSet instance
-            The instance of FitParameterSet holding the global fit parameter
-            definitions used in the maximization process.
-        tl : TimeLord instance | None
-            The optional TimeLord instance that should be used to time the
+        tl : instance of TimeLord | None
+            The optional instance of TimeLord that should be used to time the
             maximization process.
 
         Returns
         -------
         log_lambda_max : float
             The (maximum) value of the log-likelihood ratio (log_lambda)
             function for the best fit parameter values.
-        fitparam_values : (N_fitparam)-shaped 1D ndarray
-            The ndarray holding the global fit parameter values.
+        fitparam_values : instance of numpy ndarray
+            The (N_fitparam,)-shaped 1D numpy ndarray holding the values of the
+            global fit parameters.
         status : dict
             The dictionary with status information about the maximization
             process, i.e. from the minimizer.
         """
-        tracing = CFG['debugging']['enable_tracing']
+        tracing = self._cfg['debugging']['enable_tracing']
 
         # Define the negative llhratio function, that will get minimized.
         self_evaluate = self.evaluate
+
         def negative_llhratio_func(fitparam_values, func_stats, tl=None):
+            src_params_recarray = self._pmm.create_src_params_recarray(
+                fitparam_values)
+
             func_stats['n_calls'] += 1
             with TaskTimer(tl, 'Evaluate llh-ratio function.'):
-                (f, grads) = self_evaluate(fitparam_values, tl=tl)
-                if(tracing): logger.debug(
-                    'LLH-ratio func value f={:g}, grads={}'.format(
-                        f, str(grads)))
+                (f, grads) = self_evaluate(
+                    fitparam_values=fitparam_values,
+                    src_params_recarray=src_params_recarray,
+                    tl=tl)
+                if tracing:
+                    logger.debug(
+                        f'LLH-ratio func value f={f:g}, grads={str(grads)}')
             return (-f, -grads)
 
-        minimize_kwargs = {'func_provides_grads': True}
-
-        func_stats = {'n_calls': 0}
+        minimize_kwargs = {
+            'func_provides_grads': True
+        }
+
+        func_stats = {
+            'n_calls': 0
+        }
         with TaskTimer(tl, 'Minimize -llhratio function.'):
             (fitparam_values, fmin, status) = self._minimizer.minimize(
-                rss, fitparamset, negative_llhratio_func, args=(func_stats,tl),
+                rss=rss,
+                paramset=self._pmm.global_paramset,
+                func=negative_llhratio_func,
+                args=(func_stats, tl),
                 kwargs=minimize_kwargs)
         log_lambda_max = -fmin
         status['n_llhratio_func_calls'] = func_stats['n_calls']
 
         logger.debug(
-            'Maximized LLH ratio function with {:d} calls'.format(
-                status['n_llhratio_func_calls']))
+            f'Maximized LLH ratio function with '
+            f'{status["n_llhratio_func_calls"]:d} calls')
 
         return (log_lambda_max, fitparam_values, status)
 
 
-class TCLLHRatio(LLHRatio, metaclass=abc.ABCMeta):
+class TCLLHRatio(
+        LLHRatio,
+        metaclass=abc.ABCMeta):
     """Abstract base class for a log-likelihood (LLH) ratio function with two
     components, i.e. signal and background.
     """
 
-    def __init__(self, minimizer, mean_n_sig_0):
+    def __init__(
+            self,
+            pmm,
+            minimizer,
+            mean_n_sig_0,
+            **kwargs):
         """Creates a new two-component LLH ratio function instance.
 
         Parameters
         ----------
+        pmm : instance of ParameterModelMapper
+            The instance of ParameterModelMapper providing the mapping of
+            global floating parameters to individual models.
+        minimizer : instance of Minimizer
+            The Minimizer instance that should be used to minimize the negative
+            of this log-likelihood ratio function.
         mean_n_sig_0 : float
             The fixed mean number of signal events for the null-hypothesis.
         """
-        super(TCLLHRatio, self).__init__(minimizer)
+        super().__init__(
+            pmm=pmm,
+            minimizer=minimizer,
+            **kwargs)
 
         self.mean_n_sig_0 = mean_n_sig_0
 
     @property
     def mean_n_sig_0(self):
         """The parameter value for the mean number of signal events of the
         null-hypothesis.
         """
         return self._mean_n_sig_0
+
     @mean_n_sig_0.setter
     def mean_n_sig_0(self, v):
-        v = float_cast(v, 'The mean_n_sig_0 property must be castable to a '
-            'float value!')
+        v = float_cast(
+            v,
+            'The mean_n_sig_0 property must be castable to a float value!')
         self._mean_n_sig_0 = v
 
     @abc.abstractmethod
-    def calculate_ns_grad2(self, fitparam_values):
+    def calculate_ns_grad2(
+            self,
+            ns,
+            ns_pidx,
+            src_params_recarray,
+            tl=None,
+            **kwargs):
         """This method is supposed to calculate the second derivative of the
         log-likelihood ratio function w.r.t. the fit parameter ns, the number
         of signal events in the data set.
 
         Parameters
         ----------
-        fitparam_values : numpy (N_fitparams,)-shaped 1D ndarray
-            The ndarray holding the current values of the fit parameters.
-            By definition, the first element is the fit parameter for the number
-            of signal events, ns.
+        fitparam_values : instance of numpy ndarray
+            The (N_fitparams,)-shaped 1D numpy ndarray holding the current
+            values of the global fit parameters.
+        ns_pidx : int
+            The index of the global ns fit parameter.
+        src_params_recarray : instance of numpy record ndarray
+            The numpy record ndarray of length N_sources holding the parameter
+            names and values of all sources.
+            See the documentation of the
+            :meth:`skyllh.core.parameters.ParameterModelMapper.create_src_params_recarray`
+            method for more information about this array.
+        tl : instance of TimeLord | None
+            The optional instance of TimeLord that should be used for timing
+            measurements.
 
         Returns
         -------
         nsgrad2 : float
             The second derivative w.r.t. ns of the log-likelihood ratio function
             for the given fit parameter values.
         """
         pass
 
-    def maximize(self, rss, fitparamset, tl=None):
+    def maximize_with_1d_newton_rapson_minimizer(
+            self,
+            rss,
+            tl=None):
         """Maximizes this log-likelihood ratio function, by minimizing its
-        negative.
-        This method has a special implementation when a 1D Newton-Rapson
-        minimizer is used. In that case only the first and second derivative
-        of the log-likelihood ratio function is calculated.
+        negative using a 1D Newton-Rapson minimizer.
 
         Parameters
         ----------
-        rss : RandomStateService instance
-            The RandomStateService instance that should be used to draw random
-            numbers from. It is used by the minimizer to generate random
+        rss : instance of RandomStateService
+            The instance of RandomStateService that should be used to draw
+            random numbers from. It is used by the minimizer to generate random
             fit parameter initial values.
-        fitparamset : FitParameterSet instance
-            The instance of FitParameterSet holding the global fit parameter
-            definitions used in the maximization process.
-        tl : TimeLord instance | None
-            The optional TimeLord instance that should be used to time the
+        tl : instance of TimeLord | None
+            The optional instance of TimeLord that should be used to time the
             maximization of the LLH-ratio function.
 
         Returns
         -------
         log_lambda_max : float
             The (maximum) value of the log-likelihood ratio (log_lambda)
             function for the best fit parameter values.
-        fitparam_values : (N_fitparam)-shaped 1D ndarray
-            The ndarray holding the global fit parameter values.
+        fitparam_values : instance of numpy ndarray
+            The (N_fitparam,)-shaped 1D numpy ndarray holding the global
+            fit parameter values.
         status : dict
             The dictionary with status information about the maximization
             process, i.e. from the minimizer.
         """
-        if(isinstance(self._minimizer.minimizer_impl, NR1dNsMinimizerImpl) or
-           isinstance(self._minimizer.minimizer_impl, NRNsScan2dMinimizerImpl)):
-            # Define the negative llhratio function, that will get minimized
-            # when using the Newton-Rapson 1D minimizer for llhratio functions
-            # depending solely on ns.
-            self__evaluate = self.evaluate
-            self__calculate_ns_grad2 = self.calculate_ns_grad2
-            def negative_llhratio_func_nr1d_ns(fitparam_values, tl):
-                with TaskTimer(tl, 'Evaluate llh-ratio function.'):
-                    (f, grads) = self__evaluate(fitparam_values, tl=tl)
-                with TaskTimer(tl, 'Calculate 2nd derivative of llh-ratio '
-                        'function w.r.t. ns'):
-                    grad2_ns = self__calculate_ns_grad2(fitparam_values)
+        # Define the negative llhratio function, that will get minimized
+        # when using the Newton-Rapson 1D minimizer for llhratio functions
+        # depending solely on ns.
+        self__evaluate = self.evaluate
+        self__calculate_ns_grad2 = self.calculate_ns_grad2
+
+        ns_pidx = self._pmm.get_gflp_idx(name='ns')
+
+        def negative_llhratio_func_nr1d_ns(fitparam_values, tl):
+            ns = fitparam_values[ns_pidx]
+            src_params_recarray = self._pmm.create_src_params_recarray(
+                fitparam_values)
+            with TaskTimer(
+                    tl,
+                    'Evaluate llh-ratio function.'):
+                (f, grads) = self__evaluate(
+                    fitparam_values=fitparam_values,
+                    src_params_recarray=src_params_recarray,
+                    tl=tl)
+            with TaskTimer(
+                    tl,
+                    'Calculate 2nd derivative of llh-ratio function w.r.t. ns'):
+                grad2_ns = self__calculate_ns_grad2(
+                    ns=ns,
+                    ns_pidx=ns_pidx,
+                    src_params_recarray=src_params_recarray,
+                    tl=tl)
+
+            return (-f, -grads[ns_pidx], -grad2_ns)
+
+        (fitparam_values, fmin, status) = self._minimizer.minimize(
+            rss=rss,
+            paramset=self._pmm.global_paramset,
+            func=negative_llhratio_func_nr1d_ns,
+            args=(tl,))
+        log_lambda_max = -fmin
 
-                return (-f, -grads[0], -grad2_ns)
+        return (log_lambda_max, fitparam_values, status)
 
-            (fitparam_values, fmin, status) = self._minimizer.minimize(
-                rss, fitparamset, negative_llhratio_func_nr1d_ns, args=(tl,))
-            log_lambda_max = -fmin
+    def maximize(
+            self,
+            rss,
+            tl=None):
+        """Maximizes this log-likelihood ratio function, by minimizing its
+        negative.
+        This method has a special implementation when a 1D Newton-Rapson
+        minimizer is used. In that case only the first and second derivative
+        of the log-likelihood ratio function is calculated.
 
-            return (log_lambda_max, fitparam_values, status)
+        Parameters
+        ----------
+        rss : instance of RandomStateService
+            The instance of RandomStateService that should be used to draw
+            random numbers from. It is used by the minimizer to generate random
+            fit parameter initial values.
+        tl : instance of TimeLord | None
+            The optional instance of TimeLord that should be used to time the
+            maximization of the LLH-ratio function.
 
-        return super(TCLLHRatio, self).maximize(rss, fitparamset, tl=tl)
+        Returns
+        -------
+        log_lambda_max : float
+            The (maximum) value of the log-likelihood ratio (log_lambda)
+            function for the best fit parameter values.
+        fitparam_values : instance of numpy ndarray
+            The (N_fitparam,)-shaped 1D numpy ndarray holding the global
+            fit parameter values.
+        status : dict
+            The dictionary with status information about the maximization
+            process, i.e. from the minimizer.
+        """
+        if isinstance(self._minimizer.minimizer_impl, NR1dNsMinimizerImpl) or\
+           isinstance(self._minimizer.minimizer_impl, NRNsScan2dMinimizerImpl):
+            return self.maximize_with_1d_newton_rapson_minimizer(
+                rss=rss,
+                tl=tl)
+
+        return super().maximize(
+            rss=rss,
+            tl=tl)
 
 
-class SingleDatasetTCLLHRatio(TCLLHRatio, metaclass=abc.ABCMeta):
+class SingleDatasetTCLLHRatio(
+        TCLLHRatio,
+        metaclass=abc.ABCMeta):
     """Abstract base class for a log-likelihood (LLH) ratio function with two
     components, i.e. signal and background, for a single data set.
     """
 
     def __init__(
-            self, minimizer, src_hypo_group_manager, src_fitparam_mapper, tdm,
-            mean_n_sig_0):
+            self,
+            pmm,
+            minimizer,
+            shg_mgr,
+            tdm,
+            mean_n_sig_0,
+            **kwargs):
         """Creates a new two-component LLH ratio function instance for a single
         data set.
 
         Parameters
         ----------
+        pmm : instance of ParameterModelMapper
+            The instance of ParameterModelMapper providing the mapping of
+            global floating parameters to individual models.
         minimizer : instance of Minimizer
             The Minimizer instance that should be used to minimize the negative
             of this log-likelihood ratio function.
-        src_hypo_group_manager : SourceHypoGroupManager instance
+        shg_mgr : SourceHypoGroupManager instance
             The SourceHypoGroupManager instance that defines the source
-            hypotheses.
-        src_fitparam_mapper : SourceFitParameterMapper
-            The instance of SourceFitParameterMapper defining the global fit
-            parameters and their mapping to the source fit parameters.
-            The order of the fit parameters defines the order of the fit values
-            during the maximization process of the log-likelihood-ratio
-            function. The names of the source fit parameters must coincide with
-            the signal fit parameter names of the PDF instances.
+            hypothesis groups.
         tdm : instance of TrialDataManager
             The instance of TrialDataManager that holds the trial event data and
             additional data fields for this LLH ratio function.
         mean_n_sig_0 : float
             The fixed mean number of signal events for the null-hypothesis.
         """
-        super(SingleDatasetTCLLHRatio, self).__init__(
-            minimizer, mean_n_sig_0)
+        super().__init__(
+            pmm=pmm,
+            minimizer=minimizer,
+            mean_n_sig_0=mean_n_sig_0,
+            **kwargs)
 
-        self.src_hypo_group_manager = src_hypo_group_manager
-        self.src_fitparam_mapper = src_fitparam_mapper
+        self.shg_mgr = shg_mgr
         self.tdm = tdm
 
         # Calculate the data fields that solely depend on source parameters.
-        self._tdm.calculate_source_data_fields(src_hypo_group_manager)
+        self._tdm.calculate_source_data_fields(
+            shg_mgr=self._shg_mgr,
+            pmm=self._pmm)
 
     @property
-    def src_hypo_group_manager(self):
+    def shg_mgr(self):
         """The SourceHypoGroupManager instance that defines the source
-        hypotheses.
+        hypothesis groups.
         """
-        return self._src_hypo_group_manager
-    @src_hypo_group_manager.setter
-    def src_hypo_group_manager(self, manager):
-        if(not isinstance(manager, SourceHypoGroupManager)):
-            raise TypeError('The src_hypo_group_manager property must be an '
-                'instance of SourceHypoGroupManager!')
-        self._src_hypo_group_manager = manager
+        return self._shg_mgr
 
-    @property
-    def src_fitparam_mapper(self):
-        """The SourceFitParameterMapper instance defining the global fit
-        parameters and their mapping to the source fit parameters.
-        """
-        return self._src_fitparam_mapper
-    @src_fitparam_mapper.setter
-    def src_fitparam_mapper(self, mapper):
-        if(not isinstance(mapper, SourceFitParameterMapper)):
-            raise TypeError('The src_fitparam_mapper property must be an '
-                'instance of SourceFitParameterMapper!')
-        self._src_fitparam_mapper = mapper
+    @shg_mgr.setter
+    def shg_mgr(self, mgr):
+        if not isinstance(mgr, SourceHypoGroupManager):
+            raise TypeError(
+                'The shg_mgr property must be an instance of '
+                'SourceHypoGroupManager! '
+                f'Its current type is {classname(mgr)}.')
+        self._shg_mgr = mgr
 
     @property
     def tdm(self):
-        """The TrialDataManager instance that holds the trial event data and
+        """The instance of TrialDataManager that holds the trial event data and
         additional data fields for this LLH ratio function.
         """
         return self._tdm
+
     @tdm.setter
-    def tdm(self, manager):
-        if(not isinstance(manager, TrialDataManager)):
-            raise TypeError('The tdm property must be an instance of '
-                'TrialDataManager!')
-        self._tdm = manager
+    def tdm(self, mgr):
+        if not isinstance(mgr, TrialDataManager):
+            raise TypeError(
+                'The tdm property must be an instance of TrialDataManager! '
+                f'Its current type is {classname(mgr)}.')
+        self._tdm = mgr
 
-    def change_source_hypo_group_manager(self, src_hypo_group_manager):
+    def change_shg_mgr(self, shg_mgr):
         """Changes the source hypothesis group manager of this two-component LLH
         ratio function.
 
         Parameters
         ----------
-        src_hypo_group_manager : SourceHypoGroupManager instance
-            The new SourceHypoGroupManager instance.
+        shg_mgr : instance of SourceHypoGroupManager
+            The new instance of SourceHypoGroupManager.
         """
-        self.src_hypo_group_manager = src_hypo_group_manager
-        self._tdm.change_source_hypo_group_manager(src_hypo_group_manager)
+        self.shg_mgr = shg_mgr
+
+        self._tdm.change_shg_mgr(
+            shg_mgr=shg_mgr,
+            pmm=self._pmm)
 
 
-class ZeroSigH0SingleDatasetTCLLHRatio(SingleDatasetTCLLHRatio):
+class ZeroSigH0SingleDatasetTCLLHRatio(
+        SingleDatasetTCLLHRatio):
     """This class implements a two-component (TC) log-likelihood (LLH) ratio
-    function for a single data assuming zero signal for the null-hypothesis.
-    The log-likelihood-ratio function uses a list of independent PDF ratio
-    instances.
+    function for a single dataset assuming zero signal for the null-hypothesis.
     """
     # The (1 + alpha)-threshold float value for which the log-likelihood ratio
     # function of a single event should get approximated by a Taylor expansion.
     # This is to prevent a divergence of the log-function for each event, where
     # (1 + alpha_i) < (1 + alpha).
     # This setting is implemented as a class type member instead of a class
     # instance member, because it is supposed to be the same for all instances.
     _one_plus_alpha = 1e-3
 
     def __init__(
-            self, minimizer, src_hypo_group_manager, src_fitparam_mapper, tdm,
-            pdfratios):
+            self,
+            pmm,
+            minimizer,
+            shg_mgr,
+            tdm,
+            pdfratio,
+            **kwargs):
         """Constructor of the two-component log-likelihood ratio function.
 
         Parameters
         ----------
+        pmm : instance of ParameterModelMapper
+            The instance of ParameterModelMapper providing the mapping of
+            global floating parameters to individual models.
         minimizer : instance of Minimizer
             The Minimizer instance that should be used to minimize the negative
             of this log-likelihood ratio function.
-        src_hypo_group_manager : SourceHypoGroupManager instance
+        shg_mgr : instance of SourceHypoGroupManager
             The SourceHypoGroupManager instance that defines the source
-            hypotheses.
-        src_fitparam_mapper : SourceFitParameterMapper
-            The instance of SourceFitParameterMapper defining the global fit
-            parameters and their mapping to the source fit parameters.
-            The order of the fit parameters defines the order of the fit values
-            during the maximization process of the log-likelihood-ratio
-            function. The names of the source fit parameters must coincide with
-            the signal fit parameter names of the PDF ratio instances.
+            hypothesis groups.
         tdm : instance of TrialDataManager
             The instance of TrialDataManager that holds the trial event data and
             additional data fields for this LLH ratio function.
-        pdfratios : sequence of PDFRatio
-            The sequence of PDFRatio instances. A PDFRatio instance might depend
+        pdfratio : instance of PDFRatio
+            The instance of PDFRatio. A PDFRatio instance might depend
             on none, one, or several fit parameters.
         """
-        super(ZeroSigH0SingleDatasetTCLLHRatio, self).__init__(
-            minimizer, src_hypo_group_manager, src_fitparam_mapper, tdm,
-            mean_n_sig_0=0)
+        super().__init__(
+            pmm=pmm,
+            minimizer=minimizer,
+            shg_mgr=shg_mgr,
+            tdm=tdm,
+            mean_n_sig_0=0,
+            **kwargs)
 
-        self.pdfratio_list = pdfratios
+        self.pdfratio = pdfratio
 
-        # Define cache variables for evaluate method to store values needed for
-        # a possible calculation of the second derivative w.r.t. ns of the
+        # Define cache variable for the evaluate method to store values needed
+        # for a possible calculation of the second derivative w.r.t. ns of the
         # log-likelihood ratio function.
-        self._cache_fitparam_values = None
         self._cache_nsgrad_i = None
 
     @SingleDatasetTCLLHRatio.mean_n_sig_0.setter
     def mean_n_sig_0(self, v):
         SingleDatasetTCLLHRatio.mean_n_sig_0.fset(self, v)
-        if(self._mean_n_sig_0 != 0):
-            raise ValueError('The %s class is only valid for '
-                'mean_n_sig_0 = 0!'%(classname(self)))
+        if self._mean_n_sig_0 != 0:
+            raise ValueError(
+                f'The {classname(self)} class is only valid for '
+                f'mean_n_sig_0 = 0!')
 
     @property
-    def pdfratio_list(self):
-        """The list of PDFRatio instances.
+    def pdfratio(self):
+        """The instance of PDFRatio.
         """
-        return self._pdfratio_list
-    @pdfratio_list.setter
-    def pdfratio_list(self, seq):
-        if(not issequenceof(seq, PDFRatio)):
-            raise TypeError('The pdfratio_list property must be a sequence of '
-                'PDFRatio instances!')
-        self._pdfratio_list = list(seq)
+        return self._pdfratio
+
+    @pdfratio.setter
+    def pdfratio(self, r):
+        if not isinstance(r, PDFRatio):
+            raise TypeError(
+                'The pdfratio property must be an instance of PDFRatio! '
+                f'Its current type is {classname(r)}.')
+        self._pdfratio = r
+
+    def initialize_for_new_trial(
+            self,
+            tl=None,
+            **kwargs):
+        """Initializes the log-likelihood ratio function for a new trial.
+        It calls the
+        :meth:`~skyllh.core.pdfratio.PDFRatio.initialize_for_new_trial` method
+        of the :class:`~skyllh.core.pdfratio.PDFRatio` class.
 
-    def calculate_log_lambda_and_grads(self, fitparam_values, N, ns, Xi, dXi_ps):
+        Parameters
+        ----------
+        tl : instance of TimeLord
+            The optional instance of TimeLord to measure timing information.
+        """
+        self._pdfratio.initialize_for_new_trial(
+            tdm=self._tdm,
+            tl=tl,
+            **kwargs)
+
+    def calculate_log_lambda_and_grads(
+            self,
+            N,
+            ns,
+            ns_pidx,
+            p_mask,
+            Xi,
+            dXi_dp):
         """Calculates the log(Lambda) value and its gradient for each global fit
         parameter. This calculation is source and detector independent.
 
         Parameters
         ----------
-        fitparam_values : numpy (N_fitparams+1)-shaped 1D ndarray
-            The ndarray holding the current values of the fit parameters.
-            By definition, the first element is the fit parameter for the number
-            of signal events, ns.
+        fitparam_values : instance of numpy ndarray
+            The (N_fitparams,)-shaped ndarray holding the current values of the
+            global fit parameters.
             These numbers are used as cache key to validate the ``nsgrad_i``
             values for the given fit parameter values for a possible later
             calculation of the second derivative w.r.t. ns of the log-likelihood
             ratio function.
         N : int
             The total number of events.
         ns : float
-            The current fit parameter value for ns.
-        Xi : numpy (n_selected_events,)-shaped 1D ndarray
-            The X value of each selected event.
-        dXi_ps : numpy (N_fitparams,n_selected_events)-shaped 2D ndarray
-            The derivative value for each fit parameter ps of each event's X
-            value.
+            The value of the global fit paramater ns.
+        ns_pidx : int
+            The index of the global fit parameter ns.
+        p_mask : instance of numpy ndarray
+            The (N_fitparam,)-shaped numpy ndarray of bool selecting all global
+            fit parameters, except ns.
+        Xi : instance of numpy ndarray
+            The (n_selected_events,)-shaped 1D numpy ndarray holding the X value
+            of each selected event.
+        dXi_dp : instance of numpy ndarray
+            The (n_selected_events, N_fitparams-1,)-shaped 2D ndarray holding
+            the derivative value for each fit parameter p (i.e. except ns) of
+            each event's X value.
 
         Returns
         -------
         log_lambda : float
             The value of the log-likelihood ratio function.
-        grads : 1D numpy (N_fitparams+1,)-shaped ndarray
-            The gradient value of log_lambda for each fit parameter.
-            The first element is the gradient for ns.
+        grads : instance of numpy ndarray
+            The (N_fitparams,)-shaped numpy ndarray holding the gradient value
+            of log_lambda for each fit parameter.
         """
-        tracing = CFG['debugging']['enable_tracing']
+        tracing = self._cfg['debugging']['enable_tracing']
 
         # Get the number of selected events.
         Nprime = len(Xi)
 
-        if(tracing):
+        if tracing:
             logger.debug(
-                'N={:d}, Nprime={:d}'.format(
-                    N, Nprime))
+                f'N={N:d}, Nprime={Nprime:d}')
 
         one_plus_alpha = ZeroSigH0SingleDatasetTCLLHRatio._one_plus_alpha
 
         alpha = one_plus_alpha - 1
         alpha_i = ns*Xi
 
         # Create a mask for events which have a stable non-diverging
         # log-function argument, and an inverted mask thereof.
-        stablemask = alpha_i > alpha
-        unstablemask = ~stablemask
-        if(tracing):
+        m_stable = alpha_i > alpha
+        m_unstable = ~m_stable
+
+        if tracing:
             logger.debug(
                 '# of events doing Taylor expansion for (unstable events): '
-                '{:d}'.format(
-                    np.count_nonzero(unstablemask)))
+                f'{np.count_nonzero(m_unstable):d}')
 
         # Allocate memory for the log_lambda_i values.
         log_lambda_i = np.empty_like(alpha_i, dtype=np.float64)
 
         # Calculate the log_lambda_i value for the numerical stable events.
-        log_lambda_i[stablemask] = np.log1p(alpha_i[stablemask])
+        np.log1p(alpha_i, where=m_stable, out=log_lambda_i)
+
         # Calculate the log_lambda_i value for the numerical unstable events.
-        tildealpha_i = (alpha_i[unstablemask] - alpha) / one_plus_alpha
-        log_lambda_i[unstablemask] = np.log1p(alpha) + tildealpha_i - 0.5*tildealpha_i**2
+        tildealpha_i = (alpha_i[m_unstable] - alpha) / one_plus_alpha
+        log_lambda_i[m_unstable] =\
+            np.log1p(alpha) + tildealpha_i - 0.5 * tildealpha_i**2
 
         # Calculate the log_lambda value and account for pure background events.
         log_lambda = np.sum(log_lambda_i) + (N - Nprime)*np.log1p(-ns/N)
 
         # Calculate the gradient for each fit parameter.
-        grads = np.empty((dXi_ps.shape[0]+1,), dtype=np.float64)
+        grads = np.empty((dXi_dp.shape[1]+1,), dtype=np.float64)
 
         # Pre-calculate value that is used twice for the gradients of the
         # numerical stable events.
-        one_over_one_plus_alpha_i_stablemask = 1 / (1 + alpha_i[stablemask])
+        one_over_one_plus_alpha_i_stablemask = 1 / (1 + alpha_i[m_stable])
 
         # For ns.
         nsgrad_i = np.empty_like(alpha_i, dtype=np.float64)
-        nsgrad_i[stablemask] = Xi[stablemask] * one_over_one_plus_alpha_i_stablemask
-        nsgrad_i[unstablemask] = (1 - tildealpha_i)*Xi[unstablemask] / one_plus_alpha
+        nsgrad_i[m_stable] =\
+            Xi[m_stable] * one_over_one_plus_alpha_i_stablemask
+        nsgrad_i[m_unstable] =\
+            (1 - tildealpha_i) * Xi[m_unstable] / one_plus_alpha
+
         # Cache the nsgrad_i values for a possible later calculation of the
         # second derivative w.r.t. ns of the log-likelihood ratio function.
-        # Note: We create a copy of the fitparam_values array here to make sure
-        #       that the values don't get changed outside this method before the
-        #       calculate_ns_grad2 method is called.
-        self._cache_fitparam_values = fitparam_values.copy()
         self._cache_nsgrad_i = nsgrad_i
+
         # Calculate the first derivative w.r.t. ns.
-        grads[0] = np.sum(nsgrad_i) - (N - Nprime)/(N - ns)
+        grads[ns_pidx] = np.sum(nsgrad_i) - (N - Nprime) / (N - ns)
+
+        # Now for each other fit parameter.
 
-        # For each other fit parameter.
         # For all numerical stable events.
-        grads[1:] = np.sum(ns * one_over_one_plus_alpha_i_stablemask * dXi_ps[:,stablemask], axis=1)
+        grads[p_mask] = np.sum(
+            ns * one_over_one_plus_alpha_i_stablemask[:, np.newaxis] *
+            dXi_dp[m_stable],
+            axis=0)
+
         # For all numerical unstable events.
-        grads[1:] += np.sum(ns*(1 - tildealpha_i)*dXi_ps[:,unstablemask] / one_plus_alpha, axis=1)
+        grads[p_mask] += np.sum(
+            ns * (1 - tildealpha_i[:, np.newaxis]) * dXi_dp[m_unstable] /
+            one_plus_alpha,
+            axis=0)
 
         return (log_lambda, grads)
 
-    def calculate_ns_grad2(self, fitparam_values):
+    def calculate_ns_grad2(
+            self,
+            ns,
+            ns_pidx=None,
+            src_params_recarray=None,
+            tl=None):
         """Calculates the second derivative w.r.t. ns of the log-likelihood
         ratio function.
         This method tries to use cached values for the first derivative
         w.r.t. ns of the log-likelihood ratio function for the individual
         events. If cached values don't exist or do not match the given fit
         parameter values, they will get calculated automatically by calling the
         evaluate method with the given fit parameter values.
 
         Parameters
         ----------
         fitparam_values : numpy (N_fitparams+1)-shaped 1D ndarray
-            The ndarray holding the current values of the fit parameters.
-            By definition, the first element is the fit parameter for the number
-            of signal events, ns.
+            The ndarray holding the current values of the global fit
+            parameters.
+        ns : float
+            The value of the global fit paramater ns.
+        ns_pidx : int
+            The parameter index of the global fit parameter ns.
+            For this particular class this is an ignored interface parameter.
+        src_params_recarray : instance of numpy record ndarray
+            The numpy record ndarray of length N_sources holding the parameter
+            names and values of all sources.
+            See the documentation of the
+            :meth:`skyllh.core.parameters.ParameterModelMapper.create_src_params_recarray`
+            method for more information about this array.
+            For this particular class this is an ignored interface parameter.
+        tl : instance of TimeLord | None
+            The optional instance of TimeLord that should be used for timing
+            measurements.
 
         Returns
         -------
         nsgrad2 : float
             The second derivative w.r.t. ns of the log-likelihood ratio function
             for the given fit parameter values.
         """
-        # Check if the cached nsgrad_i values match the given fitparam_values.
-        if((self._cache_fitparam_values is None) or
-           (not np.all(self._cache_fitparam_values == fitparam_values))):
-            # Calculate the cache values by evaluating the log-likelihood ratio
-            # function.
-            self.evaluate(fitparam_values)
+        if self._cache_nsgrad_i is None:
+            raise RuntimeError(
+                'The evaluate method needs to be called before the '
+                'calculate_ns_grad2 method can be called!')
 
-        ns = fitparam_values[0]
         Nprime = self._tdm.n_selected_events
         N = Nprime + self._tdm.n_pure_bkg_events
 
         nsgrad2 = -np.sum(self._cache_nsgrad_i**2) - (N - Nprime)/(N - ns)**2
 
         return nsgrad2
 
-
-class SingleSourceZeroSigH0SingleDatasetTCLLHRatio(
-        ZeroSigH0SingleDatasetTCLLHRatio):
-    """This class implements a 2-component, i.e. signal and background,
-    log-likelihood ratio function for a single data set. The
-    log-likelihood-ratio function assumes a zero signal null-hypothesis and uses
-    a list of independent PDFRatio instances assuming a single source.
-    """
-    def __init__(
-            self, minimizer, src_hypo_group_manager, src_fitparam_mapper, tdm,
-            pdfratios):
-        """Constructor for creating a 2-component, i.e. signal and background,
-        log-likelihood ratio function assuming a single source.
-
-        Parameters
-        ----------
-        minimizer : instance of Minimizer
-            The Minimizer instance that should be used to minimize the negative
-            of this log-likelihood ratio function.
-        src_hypo_group_manager : SourceHypoGroupManager instance
-            The SourceHypoGroupManager instance that defines the source
-            hypotheses.
-        src_fitparam_mapper : SingleSourceFitParameterMapper
-            The instance of SingleSourceFitParameterMapper defining the global
-            fit parameters and their mapping to the source fit parameters.
-            The order of the fit parameters defines the order of the fit values
-            during the maximization process.
-            The names of the source fit parameters must coincide with the signal
-            fit parameter names of the PDF ratio objects.
-        tdm : instance of TrialDataManager
-            The instance of TrialDataManager that holds the trial event data and
-            additional data fields for this LLH ratio function.
-        pdfratios : list of PDFRatio
-            The list of PDFRatio instances. A PDFRatio instance might depend on
-            none, one, or several fit parameters.
-        """
-        if(not isinstance(src_fitparam_mapper, SingleSourceFitParameterMapper)):
-            raise TypeError('The src_fitparam_mapper argument must be an '
-                'instance of SingleSourceFitParameterMapper!')
-
-        super(SingleSourceZeroSigH0SingleDatasetTCLLHRatio, self).__init__(
-            minimizer, src_hypo_group_manager, src_fitparam_mapper, tdm,
-            pdfratios)
-
-        # Construct a PDFRatio array arithmetic object specialized for a single
-        # source. This will pre-calculate the PDF ratio values for all PDF ratio
-        # instances, which do not depend on any fit parameters.
-        self._pdfratioarray = SingleSourcePDFRatioArrayArithmetic(
-            self._pdfratio_list,
-            self._src_fitparam_mapper.fitparamset.fitparam_list)
-
-    def initialize_for_new_trial(self, tl=None):
-        """Initializes the log-likelihood ratio function for a new trial.
-
-        Parameters
-        ----------
-        tl : TimeLord | None
-            The optional TimeLord instance that should be used for timing
-            measurements.
-        """
-        self._pdfratioarray.initialize_for_new_trial(self._tdm)
-
-    def evaluate(self, fitparam_values, tl=None):
+    def evaluate(
+            self,
+            fitparam_values,
+            src_params_recarray=None,
+            tl=None):
         """Evaluates the log-likelihood ratio function for the given set of
         data events.
 
         Parameters
         ----------
-        fitparam_values : numpy (N_fitparams+1)-shaped 1D ndarray
-            The ndarray holding the current values of the fit parameters.
-            By definition, the first element is the fit parameter for the number
-            of signal events, ns.
-        tl : TimeLord instance | None
-            The optional TimeLord instance to measure the timing of evaluating
-            the LLH ratio function.
+        fitparam_values : instance of numpy ndarray
+            The (N_fitparams,)-shaped 1D ndarray holding the current values of
+            the global fit parameters.
+        src_params_recarray : instance of numpy structured ndarray | None
+            The numpy record ndarray of length N_sources holding the local
+            parameter names and values of all sources.
+            If it is ``None``, it will be generated automatically from the
+            ``fitparam_values`` argument using the
+            :class:`~skyllh.core.parameters.ParameterModelMapper` instance.
+        tl : instance of TimeLord | None
+            The optional instance of TimeLord to measure the timing of
+            evaluating the LLH ratio function.
 
         Returns
         -------
         log_lambda : float
             The calculated log-lambda value.
-        grads : (N_fitparams+1,)-shaped 1D ndarray
-            The ndarray holding the gradient value of log_lambda for each fit
-            parameter and ns.
-            The first element is the gradient for ns.
-        """
-        tracing = CFG['debugging']['enable_tracing']
+        grads : instance of numpy ndarray
+            The (N_fitparams,)-shaped 1D numpy ndarray holding the gradient
+            value for each global fit parameter.
+        """
+        tracing = self._cfg['debugging']['enable_tracing']
+
+        if src_params_recarray is None:
+            src_params_recarray = self._pmm.create_src_params_recarray(
+                gflp_values=fitparam_values
+            )
 
-        # Define local variables to avoid (.)-lookup procedure.
         tdm = self._tdm
-        pdfratioarray = self._pdfratioarray
 
-        ns = fitparam_values[0]
+        ns_pidx = self._pmm.get_gflp_idx('ns')
+
+        ns = fitparam_values[ns_pidx]
 
         N = tdm.n_events
 
-        # Create the fitparams dictionary with the fit parameter names and
-        # values.
-        with TaskTimer(tl, 'Create fitparams dictionary.'):
-            fitparams = self._src_fitparam_mapper.get_src_fitparams(
-                fitparam_values[1:])
-
-        # Calculate the data fields that depend on fit parameter values.
-        with TaskTimer(tl, 'Calc fit param dep data fields.'):
-            tdm.calculate_fitparam_data_fields(
-                self._src_hypo_group_manager, fitparams)
-
-        # Calculate the PDF ratio values of all PDF ratio objects, which depend
-        # on any fit parameter.
-        with TaskTimer(tl, 'Calc pdfratio values.'):
-            pdfratioarray.calculate_pdfratio_values(tdm, fitparams, tl=tl)
-
-        # Calculate the product of all the PDF ratio values for each (selected)
-        # event.
-        with TaskTimer(tl, 'Calc pdfratio value product Ri'):
-            Ri = pdfratioarray.get_ratio_product()
+        # Calculate the data fields that depend on global fit parameters.
+        if tdm.has_global_fitparam_data_fields:
+            with TaskTimer(
+                    tl,
+                    'Calculate global fit parameter dependent data fields.'):
+                # Create the global_fitparams dictionary with the global fit
+                # parameter names and values.
+                global_fitparams = self._pmm.get_global_floating_params_dict(
+                    gflp_values=fitparam_values)
+                tdm.calculate_global_fitparam_data_fields(
+                    shg_mgr=self._shg_mgr,
+                    pmm=self._pmm,
+                    global_fitparams=global_fitparams)
+
+        # Calculate the PDF ratio values for each selected event.
+        with TaskTimer(tl, 'Calc pdfratio value Ri'):
+            Ri = self._pdfratio.get_ratio(
+                tdm=tdm,
+                src_params_recarray=src_params_recarray,
+                tl=tl)
 
-        # Calculate Xi for each (selected) event.
+        # Calculate Xi for each selected event.
         Xi = (Ri - 1.) / N
-        if(tracing):
-            logger.debug('dtype(Xi)={:s}'.format(str(Xi.dtype)))
 
-        # Calculate the gradients of Xi for each fit parameter (without ns).
-        dXi_ps = np.empty((len(fitparam_values)-1,len(Xi)), dtype=np.float64)
-        for (idx, fitparam_value) in enumerate(fitparam_values[1:]):
-            fitparam_name = self._src_fitparam_mapper.get_src_fitparam_name(idx)
-
-            dRi = np.zeros((len(Xi),), dtype=np.float64)
-            for (num_k) in np.arange(len(pdfratioarray._pdfratio_list)):
-                # Get the PDFRatio instance from which we need the derivative from.
-                pdfratio = pdfratioarray.get_pdfratio(num_k)
-                # Calculate the derivative of Ri.
-                dRi += pdfratio.get_gradient(tdm, fitparams, fitparam_name) * pdfratioarray.get_ratio_product(excluded_idx=num_k)
+        n_fitparams = len(fitparam_values)
 
-            # Calculate the derivative of Xi w.r.t. the fit parameter.
-            dXi_ps[idx] = dRi / N
+        # Calculate the gradients of Xi for each fit parameter (without ns).
+        dXi_dp = np.empty(
+            (Xi.shape[0], n_fitparams-1),
+            dtype=np.float64)
+
+        # Create a mask that selects all fit parameters except ns.
+        p_mask = np.ones((n_fitparams,), dtype=np.bool_)
+        p_mask[ns_pidx] = False
+
+        # Loop over the global fit parameters and calculate the derivative of
+        # Xi w.r.t. each fit paramater.
+        fitparam_ids = np.arange(n_fitparams)
+        for (idx, fitparam_id) in enumerate(fitparam_ids[p_mask]):
+            dRi = self._pdfratio.get_gradient(
+                tdm=tdm,
+                src_params_recarray=src_params_recarray,
+                fitparam_id=fitparam_id,
+                tl=tl)
+
+            # Calculate the derivative of Xi w.r.t. the global fit parameter
+            # with ID fitparam_id.
+            dXi_dp[:, idx] = dRi / N
 
-        if(tracing):
+        if tracing:
             logger.debug(
-                '{:s}.evaluate: N={:d}, Nprime={:d}, ns={:.3f}, '.format(
-                    classname(self), N, len(Xi), ns))
+                f'{classname(self)}.evaluate: N={N}, Nprime={len(Xi)}, '
+                f'ns={ns:.3f}')
 
         with TaskTimer(tl, 'Calc logLamds and grads'):
             (log_lambda, grads) = self.calculate_log_lambda_and_grads(
-                fitparam_values, N, ns, Xi, dXi_ps)
+                N=N,
+                ns=ns,
+                ns_pidx=ns_pidx,
+                p_mask=p_mask,
+                Xi=Xi,
+                dXi_dp=dXi_dp)
 
         return (log_lambda, grads)
 
 
-class MultiSourceZeroSigH0SingleDatasetTCLLHRatio(
-        SingleSourceZeroSigH0SingleDatasetTCLLHRatio):
-    """This class implements a 2-component, i.e. signal and background,
-    log-likelihood ratio function for a single data set assuming zero signal for
-    the null-hypothesis. It uses a list of independent PDFRatio instances
-    assuming multiple sources (stacking).
-    """
-    def __init__(
-            self, minimizer, src_hypo_group_manager, src_fitparam_mapper, tdm,
-            pdfratios, detsigyields):
-        """Constructor for creating a 2-component, i.e. signal and background,
-        log-likelihood ratio function assuming a single source.
-
-        Parameters
-        ----------
-        minimizer : instance of Minimizer
-            The Minimizer instance that should be used to minimize the negative
-            of this log-likelihood ratio function.
-        src_hypo_group_manager : SourceHypoGroupManager instance
-            The SourceHypoGroupManager instance that defines the source
-            hypotheses.
-        src_fitparam_mapper : SingleSourceFitParameterMapper
-            The instance of SingleSourceFitParameterMapper defining the global
-            fit parameters and their mapping to the source fit parameters.
-            The order of the fit parameters defines the order of the fit values
-            during the maximization process.
-            The names of the source fit parameters must coincide with the signal
-            fit parameter names of the PDF ratio objects.
-        tdm : instance of TrialDataManager
-            The instance of TrialDataManager that holds the trial event data and
-            additional data fields for this LLH ratio function.
-        pdfratios : list of PDFRatio
-            The list of PDFRatio instances. A PDFRatio instance might depend on
-            none, one, or several fit parameters.
-        detsigyields : (N_source_hypo_groups,)-shaped 1D ndarray of DetSigYield
-                instances
-            The collection of DetSigYield instances for each source hypothesis
-            group.
-        """
-        if(not isinstance(src_fitparam_mapper, SingleSourceFitParameterMapper)):
-            raise TypeError('The src_fitparam_mapper argument must be an '
-                'instance of SingleSourceFitParameterMapper!')
-
-        super(MultiSourceZeroSigH0SingleDatasetTCLLHRatio, self).__init__(
-            minimizer, src_hypo_group_manager, src_fitparam_mapper, tdm,
-            pdfratios)
-
-        # Construct a PDFRatio array arithmetic object specialized for a single
-        # source. This will pre-calculate the PDF ratio values for all PDF ratio
-        # instances, which do not depend on any fit parameters.
-        self._pdfratioarray = SingleSourcePDFRatioArrayArithmetic(
-            self._pdfratio_list,
-            self._src_fitparam_mapper.fitparamset.fitparam_list)
-
-        self._calc_source_weights = MultiPointSourcesRelSourceWeights(
-            src_hypo_group_manager, src_fitparam_mapper, detsigyields)
-
-    def evaluate(self, fitparam_values, tl=None):
-        """Evaluates the log-likelihood ratio function for the given set of
-        data events.
-
-        Parameters
-        ----------
-        fitparam_values : numpy (N_fitparams+1)-shaped 1D ndarray
-            The ndarray holding the current values of the fit parameters.
-            By definition, the first element is the fit parameter for the number
-            of signal events, ns.
-        tl : TimeLord instance | None
-            The optional TimeLord instance to measure the timing of evaluating
-            the LLH ratio function.
-
-        Returns
-        -------
-        log_lambda : float
-            The calculated log-lambda value.
-        grads : (N_fitparams+1,)-shaped 1D ndarray
-            The ndarray holding the gradient value of log_lambda for each fit
-            parameter and ns.
-            The first element is the gradient for ns.
-        """
-        _src_w, _src_w_grads = self._calc_source_weights(
-                fitparam_values)
-        self._tdm.get_data('src_array')['src_w'] = _src_w
-        if _src_w_grads is not None:
-            self._tdm.get_data('src_array')['src_w_grad'] = _src_w_grads.flatten()
-        else:
-            self._tdm.get_data('src_array')['src_w_grad'] = np.zeros_like(_src_w)
-
-        (log_lambda, grads) = super(
-            MultiSourceZeroSigH0SingleDatasetTCLLHRatio, self).evaluate(
-                fitparam_values, tl)
-
-        return (log_lambda, grads)
-
-
-class DatasetSignalWeights(object, metaclass=abc.ABCMeta):
-    """Abstract base class for a dataset signal weight calculator class.
-    """
-    def __init__(
-            self, src_hypo_group_manager, src_fitparam_mapper, detsigyields):
-        """Base class constructor.
-
-        Parameters
-        ----------
-        src_hypo_group_manager : SourceHypoGroupManager instance
-            The instance of the SourceHypoGroupManager managing the source
-            hypothesis groups.
-        src_fitparam_mapper : SourceFitParameterMapper
-            The SourceFitParameterMapper instance that defines the global fit
-            parameters and their mapping to the source fit parameters.
-        detsigyields : 2D (N_source_hypo_groups,N_datasets)-shaped ndarray of
-                     DetSigYield instances
-            The collection of DetSigYield instances for each
-            dataset and source group combination. The detector signal yield
-            instances are used to calculate the dataset signal weight factors.
-            The order must follow the definition order of the log-likelihood
-            ratio functions, i.e. datasets, and the definition order of the
-            source hypothesis groups.
-        """
-        self.src_hypo_group_manager = src_hypo_group_manager
-        self.src_fitparam_mapper = src_fitparam_mapper
-        self.detsigyield_arr = detsigyields
-
-        if(self._detsigyield_arr.shape[0] != self._src_hypo_group_manager.n_src_hypo_groups):
-            raise ValueError('The detsigyields array must have the same number '
-                'of source hypothesis groups as the source hypothesis group '
-                'manager defines!')
-
-        # Pre-convert the source list of each source hypothesis group into a
-        # source array needed for the detector signal yield evaluation.
-        # Since all the detector signal yield instances must be of the same
-        # kind for each dataset, we can just use the one of the first dataset of
-        # each source hypothesis group.
-        self._src_arr_list = self._create_src_arr_list(
-            self._src_hypo_group_manager, self._detsigyield_arr)
-
-    def _create_src_arr_list(self, src_hypo_group_manager, detsigyield_arr):
-        """Pre-convert the source list of each source hypothesis group into a
-        source array needed for the detector signal yield evaluation.
-        Since all the detector signal yield instances must be of the same
-        kind for each dataset, we can just use the one of the first dataset of
-        each source hypothesis group.
-
-        Parameters
-        ----------
-        src_hypo_group_manager : SourceHypoGroupManager instance
-            The SourceHypoGroupManager instance defining the sources.
-
-        detsigyield_arr : 2D (N_source_hypo_groups,N_datasets)-shaped ndarray of
-                        DetSigYield instances
-            The collection of DetSigYield instances for each dataset and source
-            group combination.
-        Returns
-        -------
-        src_arr_list : list of numpy record ndarrays
-            The list of the source numpy record ndarrays, one for each source
-            hypothesis group, which is needed by the detector signal yield
-            instance.
-        """
-        src_arr_list = []
-        for (gidx, src_hypo_group) in enumerate(src_hypo_group_manager.src_hypo_group_list):
-            src_arr_list.append(
-                detsigyield_arr[gidx,0].source_to_array(src_hypo_group.source_list)
-            )
-
-        return src_arr_list
-
-    @property
-    def src_hypo_group_manager(self):
-        """The instance of SourceHypoGroupManager, which defines the source
-        hypothesis groups.
-        """
-        return self._src_hypo_group_manager
-    @src_hypo_group_manager.setter
-    def src_hypo_group_manager(self, manager):
-        if(not isinstance(manager, SourceHypoGroupManager)):
-            raise TypeError('The src_hypo_group_manager property must be an '
-                'instance of SourceHypoGroupManager!')
-        self._src_hypo_group_manager = manager
-
-    @property
-    def src_fitparam_mapper(self):
-        """The SourceFitParameterMapper instance defining the global fit
-        parameters and their mapping to the source fit parameters.
-        """
-        return self._src_fitparam_mapper
-    @src_fitparam_mapper.setter
-    def src_fitparam_mapper(self, mapper):
-        if(not isinstance(mapper, SourceFitParameterMapper)):
-            raise TypeError('The src_fitparam_mapper property must be an '
-                'instance of SourceFitParameterMapper!')
-        self._src_fitparam_mapper = mapper
-
-    @property
-    def detsigyield_arr(self):
-        """The 2D (N_source_hypo_groups,N_datasets)-shaped ndarray of
-        DetSigYield instances.
-        """
-        return self._detsigyield_arr
-    @detsigyield_arr.setter
-    def detsigyield_arr(self, detsigyields):
-        if(not isinstance(detsigyields, np.ndarray)):
-            raise TypeError('The detsigyield_arr property must be an instance '
-                'of numpy.ndarray!')
-        if(detsigyields.ndim != 2):
-            raise ValueError('The detsigyield_arr property must be a '
-                'numpy.ndarray with 2 dimensions!')
-        if(not issequenceof(detsigyields.flat, DetSigYield)):
-            raise TypeError('The detsigyield_arr property must contain '
-                'DetSigYield instances, one for each source hypothesis group '
-                'and dataset combination!')
-        self._detsigyield_arr = detsigyields
-
-    @property
-    def n_datasets(self):
-        """(read-only) The number of datasets this DatasetSignalWeights instance
-        is for.
-        """
-        return self._detsigyield_arr.shape[1]
-
-    def change_source_hypo_group_manager(self, src_hypo_group_manager):
-        """Changes the SourceHypoGroupManager instance of this
-        DatasetSignalWeights instance. This will also recreate the internal
-        source numpy record arrays needed for the detector signal efficiency
-        calculation.
-
-        Parameters
-        ----------
-        src_hypo_group_manager : SourceHypoGroupManager instance
-            The new SourceHypoGroupManager instance, that should be used for
-            this dataset signal weights instance.
-        """
-        self.src_hypo_group_manager = src_hypo_group_manager
-        self._src_arr_list = self._create_src_arr_list(
-            self._src_hypo_group_manager, self._detsigyield_arr)
-
-    @abc.abstractmethod
-    def __call__(self, fitparam_values):
-        """This method is supposed to calculate the dataset signal weights and
-        their gradients.
-
-        Parameters
-        ----------
-        fitparam_values : (N_fitparams+1,)-shaped 1D numpy ndarray
-            The ndarray holding the current values of the fit parameters.
-            The first element of that array is, by definition, the number of
-            signal events, ns.
-
-        Returns
-        -------
-        f : (N_datasets,)-shaped 1D ndarray
-            The dataset signal weight factor for each dataset.
-        f_grads : (N_datasets,N_fitparams)-shaped 2D ndarray
-            The gradients of the dataset signal weight factors, one for each
-            fit parameter.
-        """
-        pass
-
-
-class SingleSourceDatasetSignalWeights(DatasetSignalWeights):
-    """This class calculates the dataset signal weight factors for each dataset
-    assuming a single source.
-    """
-    def __init__(
-            self, src_hypo_group_manager, src_fitparam_mapper, detsigyields):
-        """Constructs a new DatasetSignalWeights instance assuming a single
-        source.
-
-        Parameters
-        ----------
-        src_hypo_group_manager : SourceHypoGroupManager instance
-            The instance of the SourceHypoGroupManager managing the source
-            hypothesis groups.
-        src_fitparam_mapper : SingleSourceFitParameterMapper
-            The instance of SingleSourceFitParameterMapper defining the global
-            fit parameters and their mapping to the source fit parameters.
-        detsigyields : 2D (N_source_hypo_groups,N_datasets)-shaped ndarray of
-                     DetSigYield instances
-            The collection of DetSigYield instances for each
-            dataset and source group combination. The detector signal yield
-            instances are used to calculate the dataset signal weight factors.
-            The order must follow the definition order of the log-likelihood
-            ratio functions, i.e. datasets, and the definition order of the
-            source hypothesis groups.
-        """
-
-        if(not isinstance(src_fitparam_mapper, SingleSourceFitParameterMapper)):
-            raise TypeError('The src_fitparam_mapper argument must be an '
-                'instance of SingleSourceFitParameterMapper!')
-
-        # Convert sequence into a 2D numpy array.
-        detsigyields = np.atleast_2d(detsigyields)
-
-        super(SingleSourceDatasetSignalWeights, self).__init__(
-            src_hypo_group_manager, src_fitparam_mapper, detsigyields)
-
-    def __call__(self, fitparam_values):
-        """Calculates the dataset signal weight and its fit parameter gradients
-        for each dataset.
-
-        Parameters
-        ----------
-        fitparam_values : (N_fitparams+1,)-shaped 1D numpy ndarray
-            The ndarray holding the current values of the fit parameters.
-            The first element of that array is, by definition, the number of
-            signal events, ns.
-
-        Returns
-        -------
-        f : (N_datasets,)-shaped 1D ndarray
-            The dataset signal weight factor for each dataset.
-        f_grads : (N_datasets,N_fitparams)-shaped 2D ndarray | None
-            The gradients of the dataset signal weight factors, one for each
-            fit parameter. None is returned if there are no fit parameters
-            beside ns.
-        """
-        fitparams_arr = self._src_fitparam_mapper.get_fitparams_array(fitparam_values[1:])
-
-        N_datasets = self.n_datasets
-        N_fitparams = self._src_fitparam_mapper.n_global_fitparams
-
-        Y = np.empty((N_datasets,), dtype=np.float64)
-        if(N_fitparams > 0):
-            Y_grads = np.empty((N_datasets, N_fitparams), dtype=np.float64)
-
-        # Loop over the detector signal efficiency instances for the first and
-        # only source hypothesis group.
-        for (j, detsigyield) in enumerate(self._detsigyield_arr[0]):
-            (Yj, Yj_grads) = detsigyield(self._src_arr_list[0], fitparams_arr)
-            # Store the detector signal yield and its fit parameter
-            # gradients for the first and only source (element 0).
-            Y[j] = Yj[0]
-            if(N_fitparams > 0):
-                if Yj_grads is None:
-                    Y_grads[j] = np.zeros_like(Yj[0])
-                else:
-                    Y_grads[j] = Yj_grads[0]
-
-        # sumj_Y is a scalar.
-        sumj_Y = np.sum(Y, axis=0)
-
-        # f is a (N_datasets,)-shaped 1D ndarray.
-        f = Y/sumj_Y
-
-        # f_grads is a (N_datasets, N_fitparams)-shaped 2D ndarray.
-        if(N_fitparams > 0):
-            # sumj_Y_grads is a (N_fitparams,)-shaped 1D array.
-            sumj_Y_grads = np.sum(Y_grads, axis=0)
-            f_grads = (Y_grads*sumj_Y - Y[...,np.newaxis]*sumj_Y_grads) / sumj_Y**2
-        else:
-            f_grads = None
-
-        return (f, f_grads)
-
-
-class MultiSourceDatasetSignalWeights(SingleSourceDatasetSignalWeights):
-    """This class calculates the dataset signal weight factors for each dataset
-    assuming multiple sources.
-    """
-    def __init__(
-            self, src_hypo_group_manager, src_fitparam_mapper, detsigyields):
-        """Constructs a new DatasetSignalWeights instance assuming multiple
-        sources.
-
-        Parameters
-        ----------
-        src_hypo_group_manager : SourceHypoGroupManager instance
-            The instance of the SourceHypoGroupManager managing the source
-            hypothesis groups.
-        src_fitparam_mapper : SingleSourceFitParameterMapper
-            The instance of SingleSourceFitParameterMapper defining the global
-            fit parameters and their mapping to the source fit parameters.
-        detsigyields : 2D (N_source_hypo_groups,N_datasets)-shaped ndarray of
-                     DetSigYield instances
-            The collection of DetSigYield instances for each
-            dataset and source group combination. The detector signal yield
-            instances are used to calculate the dataset signal weight factors.
-            The order must follow the definition order of the log-likelihood
-            ratio functions, i.e. datasets, and the definition order of the
-            source hypothesis groups.
-        """
-
-        if(not isinstance(src_fitparam_mapper, SingleSourceFitParameterMapper)):
-            raise TypeError('The src_fitparam_mapper argument must be an '
-                'instance of SingleSourceFitParameterMapper!')
-
-        super(MultiSourceDatasetSignalWeights, self).__init__(
-            src_hypo_group_manager, src_fitparam_mapper, detsigyields)
-
-    def __call__(self, fitparam_values):
-        """Calculates the dataset signal weight and its fit parameter gradients
-        for each dataset.
-
-        Parameters
-        ----------
-        fitparam_values : (N_fitparams+1,)-shaped 1D numpy ndarray
-            The ndarray holding the current values of the fit parameters.
-            The first element of that array is, by definition, the number of
-            signal events, ns.
-
-        Returns
-        -------
-        f : (N_datasets,)-shaped 1D ndarray
-            The dataset signal weight factor for each dataset.
-        f_grads : (N_datasets,N_fitparams)-shaped 2D ndarray | None
-            The gradients of the dataset signal weight factors, one for each
-            fit parameter. None is returned if there are no fit parameters
-            beside ns.
-        """
-        fitparams_arr = self._src_fitparam_mapper.get_fitparams_array(fitparam_values[1:])
-
-        N_datasets = self.n_datasets
-        N_fitparams = self._src_fitparam_mapper.n_global_fitparams
-
-        Y = np.empty((N_datasets, len(self._src_arr_list[0])), dtype=np.float64)
-        if(N_fitparams > 0):
-            Y_grads = np.empty((N_datasets, len(self._src_arr_list[0]), N_fitparams), dtype=np.float64)
-
-        # Loop over the detector signal efficiency instances for the first and
-        # only source hypothesis group.
-        for (k, detsigyield_k) in enumerate(self._detsigyield_arr):
-            for (j, detsigyield) in enumerate(detsigyield_k):
-                (Yj, Yj_grads) = detsigyield(self._src_arr_list[k], fitparams_arr)
-                # Store the detector signal yield and its fit parameter
-                # gradients for the first and only source (element 0).
-                Y[j] = Yj
-                if(N_fitparams > 0):
-                    Y_grads[j] = Yj_grads.T
-
-        sum_Y = np.sum(Y)
-
-        # f is a (N_datasets,)-shaped 1D ndarray.
-        f = np.sum(Y, axis=1) / sum_Y
-
-        # f_grads is a (N_datasets, N_fitparams)-shaped 2D ndarray.
-        if(N_fitparams > 0):
-            # sum_Y_grads is a (N_datasets, N_fitparams,)-shaped 2D array.
-            sum_Y_grads = np.sum(Y_grads, axis=1)
-            f_grads = (sum_Y_grads*sum_Y - (f*sum_Y)[...,np.newaxis]*np.sum(sum_Y_grads, axis=0)) / sum_Y**2
-        else:
-            f_grads = None
-
-        return (f, f_grads)
-
-
-class SourceWeights(object, metaclass=abc.ABCMeta):
-    """Abstract base class for a source weight calculator class.
-    """
-    def __init__(
-            self, src_hypo_group_manager, src_fitparam_mapper, detsigyields):
-        """Constructs a new SourceWeights instance.
-
-        Parameters
-        ----------
-        src_hypo_group_manager : SourceHypoGroupManager instance
-            The instance of the SourceHypoGroupManager managing the source
-            hypothesis groups.
-        src_fitparam_mapper : SourceFitParameterMapper
-            The SourceFitParameterMapper instance that defines the global fit
-            parameters and their mapping to the source fit parameters.
-        detsigyields : (N_source_hypo_groups,)-shaped 1D ndarray of DetSigYield
-                instances
-            The collection of DetSigYield instances for each source hypothesis
-            group.
-        """
-        self.src_hypo_group_manager = src_hypo_group_manager
-        self.src_fitparam_mapper = src_fitparam_mapper
-        self.detsigyield_arr = np.atleast_1d(detsigyields)
-
-        if(self._detsigyield_arr.shape[0] != self._src_hypo_group_manager.n_src_hypo_groups):
-            raise ValueError('The detsigyields array must have the same number '
-                'of source hypothesis groups as the source hypothesis group '
-                'manager defines!')
-
-        # Pre-convert the source list of each source hypothesis group into a
-        # source array needed for the detector signal yield evaluation.
-        # Since all the detector signal yield instances must be of the same
-        # kind for each dataset, we can just use the one of the first dataset of
-        # each source hypothesis group.
-        self._src_arr_list = self._create_src_arr_list(
-            self._src_hypo_group_manager, self._detsigyield_arr)
-
-    def _create_src_arr_list(self, src_hypo_group_manager, detsigyield_arr):
-        """Pre-convert the source list of each source hypothesis group into a
-        source array needed for the detector signal yield evaluation.
-        Since all the detector signal yield instances must be of the same
-        kind for each dataset, we can just use the one of the first dataset of
-        each source hypothesis group.
-
-        Parameters
-        ----------
-        src_hypo_group_manager : SourceHypoGroupManager instance
-            The SourceHypoGroupManager instance defining the sources.
-
-        detsigyield_arr : (N_source_hypo_groups,)-shaped 1D ndarray of
-                DetSigYield instances
-            The collection of DetSigYield instances for each source hypothesis
-            group.
-        Returns
-        -------
-        src_arr_list : list of numpy record ndarrays
-            The list of the source numpy record ndarrays, one for each source
-            hypothesis group, which is needed by the detector signal yield
-            instance.
-        """
-        src_arr_list = []
-        for (gidx, src_hypo_group) in enumerate(src_hypo_group_manager.src_hypo_group_list):
-            src_arr_list.append(
-                detsigyield_arr[gidx].source_to_array(src_hypo_group.source_list)
-            )
-
-        return src_arr_list
-
-    @property
-    def src_hypo_group_manager(self):
-        """The instance of SourceHypoGroupManager, which defines the source
-        hypothesis groups.
-        """
-        return self._src_hypo_group_manager
-    @src_hypo_group_manager.setter
-    def src_hypo_group_manager(self, manager):
-        if(not isinstance(manager, SourceHypoGroupManager)):
-            raise TypeError('The src_hypo_group_manager property must be an '
-                'instance of SourceHypoGroupManager!')
-        self._src_hypo_group_manager = manager
-
-    @property
-    def src_fitparam_mapper(self):
-        """The SourceFitParameterMapper instance defining the global fit
-        parameters and their mapping to the source fit parameters.
-        """
-        return self._src_fitparam_mapper
-    @src_fitparam_mapper.setter
-    def src_fitparam_mapper(self, mapper):
-        if(not isinstance(mapper, SourceFitParameterMapper)):
-            raise TypeError('The src_fitparam_mapper property must be an '
-                'instance of SourceFitParameterMapper!')
-        self._src_fitparam_mapper = mapper
-
-    @property
-    def detsigyield_arr(self):
-        """The (N_source_hypo_groups,)-shaped 1D ndarray of DetSigYield
-        instances.
-        """
-        return self._detsigyield_arr
-    @detsigyield_arr.setter
-    def detsigyield_arr(self, detsigyields):
-        if(not isinstance(detsigyields, np.ndarray)):
-            raise TypeError('The detsigyield_arr property must be an instance '
-                'of numpy.ndarray!')
-        if(detsigyields.ndim != 1):
-            raise ValueError('The detsigyield_arr property must be a '
-                'numpy.ndarray with 1 dimensions!')
-        if(not issequenceof(detsigyields.flat, DetSigYield)):
-            raise TypeError('The detsigyield_arr property must contain '
-                'DetSigYield instances, one for each source hypothesis group!')
-        self._detsigyield_arr = detsigyields
-
-    def change_source_hypo_group_manager(self, src_hypo_group_manager):
-        """Changes the SourceHypoGroupManager instance of this
-        DatasetSignalWeights instance. This will also recreate the internal
-        source numpy record arrays needed for the detector signal efficiency
-        calculation.
-
-        Parameters
-        ----------
-        src_hypo_group_manager : SourceHypoGroupManager instance
-            The new SourceHypoGroupManager instance, that should be used for
-            this dataset signal weights instance.
-        """
-        self.src_hypo_group_manager = src_hypo_group_manager
-        self._src_arr_list = self._create_src_arr_list(
-            self._src_hypo_group_manager, self._detsigyield_arr)
-
-    @abc.abstractmethod
-    def __call__(self, fitparam_values):
-        """This method is supposed to calculate source weights and
-        their gradients.
-
-        Parameters
-        ----------
-        fitparam_values : (N_fitparams+1,)-shaped 1D numpy ndarray
-            The ndarray holding the current values of the fit parameters.
-            The first element of that array is, by definition, the number of
-            signal events, ns.
-
-        Returns
-        -------
-        f : (N_sources,)-shaped 1D ndarray
-            The source weight factor for each source.
-        f_grads : (N_sources,)-shaped 1D ndarray | None
-            The gradients of the source weight factors. None is returned if
-            there are no fit parameters beside ns.
-        """
-        pass
-
-
-class MultiPointSourcesRelSourceWeights(SourceWeights):
-    """This class calculates the relative source weights for a group of point
-    sources.
-    """
-    def __init__(
-            self, src_hypo_group_manager, src_fitparam_mapper, detsigyields):
-        """Constructs a new MultiPointSourcesRelSourceWeights instance assuming
-        multiple sources.
-
-        Parameters
-        ----------
-        src_hypo_group_manager : SourceHypoGroupManager instance
-            The instance of the SourceHypoGroupManager managing the source
-            hypothesis groups.
-        src_fitparam_mapper : SingleSourceFitParameterMapper
-            The instance of SingleSourceFitParameterMapper defining the global
-            fit parameters and their mapping to the source fit parameters.
-        detsigyields : (N_source_hypo_groups,)-shaped 1D ndarray of
-                DetSigYield instances
-            The collection of DetSigYield instances for each source hypothesis
-            group.
-        """
-
-        if(not isinstance(src_fitparam_mapper, SingleSourceFitParameterMapper)):
-            raise TypeError('The src_fitparam_mapper argument must be an '
-                'instance of SingleSourceFitParameterMapper!')
-
-        super(MultiPointSourcesRelSourceWeights, self).__init__(
-            src_hypo_group_manager, src_fitparam_mapper, detsigyields)
-
-    def __call__(self, fitparam_values):
-        """Calculates the source weights and its fit parameter gradients
-        for each source.
-
-        Parameters
-        ----------
-        fitparam_values : (N_fitparams+1,)-shaped 1D numpy ndarray
-            The ndarray holding the current values of the fit parameters.
-            The first element of that array is, by definition, the number of
-            signal events, ns.
-
-        Returns
-        -------
-        f : (N_sources,)-shaped 1D ndarray
-            The source weight factor for each source.
-        f_grads : (N_sources,)-shaped 1D ndarray | None
-            The gradients of the source weight factors. None is returned if
-            there are no fit parameters beside ns.
-        """
-        fitparams_arr = self._src_fitparam_mapper.get_fitparams_array(fitparam_values[1:])
-
-        N_fitparams = self._src_fitparam_mapper.n_global_fitparams
-
-        Y = []
-        Y_grads = []
-
-        # Loop over detector signal efficiency instances for each source
-        # hypothesis group in source hypothesis group manager.
-        for (g, detsigyield) in enumerate(self._detsigyield_arr):
-            (Yg, Yg_grads) = detsigyield(self._src_arr_list[g], fitparams_arr)
-
-            # Store the detector signal yield and its fit parameter
-            # gradients for all sources.
-            Y.append(Yg)
-            if(N_fitparams > 0):
-                Y_grads.append(Yg_grads.T)
-
-        Y = np.array(Y)
-        sum_Y = np.sum(Y)
-
-        # f is a (N_sources,)-shaped 1D ndarray.
-        f = Y / sum_Y
-
-        # Flatten the array so that each relative weight corresponds to specific
-        # source.
-        f = f.flatten()
-
-        if(N_fitparams > 0):
-            Y_grads = np.concatenate(Y_grads)
-
-            # Sum over fit parameter gradients axis.
-            # f_grads is a (N_sources,)-shaped 1D ndarray.
-            f_grads = np.sum(Y_grads, axis=1) / sum_Y
-        else:
-            f_grads = None
-
-        return (f, f_grads)
-
-
-class MultiDatasetTCLLHRatio(TCLLHRatio):
+class MultiDatasetTCLLHRatio(
+        TCLLHRatio):
     """This class describes a two-component log-likelihood ratio function for
     multiple datasets. The final log-likelihood ratio value is the sum of the
     individual log-likelihood ratio values.
 
     The different datasets contribute according to their dataset signal weight
     factor, f_j(p_s), which depends on possible signal fit parameters. By
     definition the signal fit parameters are assumed to be the same for each
     dataset.
 
     By mathematical definition this class is suitable for single and multi
     source hypotheses.
     """
-    def __init__(self, minimizer, dataset_signal_weights, llhratios):
+    def __init__(
+            self,
+            pmm,
+            minimizer,
+            src_detsigyield_weights_service,
+            ds_sig_weight_factors_service,
+            llhratio_list,
+            **kwargs):
         """Creates a new composite two-component log-likelihood ratio function.
 
         Parameters
         ----------
+        pmm : instance of ParameterModelMapper
+            The instance of ParameterModelMapper providing the mapping of
+            global floating parameters to individual models.
         minimizer : instance of Minimizer
             The Minimizer instance that should be used to minimize the negative
             of this log-likelihood ratio function.
-        dataset_signal_weights : DatasetSignalWeights
-            An instance of DatasetSignalWeights, which calculates the relative
-            dataset weight factors.
-        llhratios : sequence of SingleDatasetTCLLHRatio instances
-            The sequence of the two-component log-likelihood ratio functions,
+        src_detsigyield_weights_service : instance of SrcDetSigYieldWeightsService
+            An instance of SrcDetSigYieldWeightsService, which provides the
+            roduct of the source weights with the detector signal yield.
+        ds_sig_weight_factors_service : instance of DatasetSignalWeightFactorsService
+            An instance of DatasetSignalWeightFactorsService, which provides
+            the relative dataset signal weight factors.
+        llhratio_list : list of instance of SingleDatasetTCLLHRatio
+            The list of the two-component log-likelihood ratio functions,
             one for each dataset.
         """
-        self.dataset_signal_weights = dataset_signal_weights
-        self.llhratio_list = llhratios
-
-        super(MultiDatasetTCLLHRatio, self).__init__(
-            minimizer, self._llhratio_list[0].mean_n_sig_0)
-
-        # Check if the number of datasets the DatasetSignalWeights instance is
-        # made for equals the number of log-likelihood ratio functions.
-        if(self.dataset_signal_weights.n_datasets != len(self._llhratio_list)):
-            raise ValueError('The number of datasets the DatasetSignalWeights '
+        if not issequenceof(llhratio_list, SingleDatasetTCLLHRatio):
+            raise TypeError(
+                'The llhratio_list argument must be a sequence of '
+                'SingleDatasetTCLLHRatio instances! '
+                f'Its current type is {classname(llhratio_list)}.')
+        self._llhratio_list = list(llhratio_list)
+
+        super().__init__(
+            pmm=pmm,
+            minimizer=minimizer,
+            mean_n_sig_0=self._llhratio_list[0].mean_n_sig_0,
+            **kwargs)
+
+        self.src_detsigyield_weights_service = src_detsigyield_weights_service
+        self.ds_sig_weight_factors_service = ds_sig_weight_factors_service
+
+        if (
+            self.ds_sig_weight_factors_service.n_datasets
+            != len(self._llhratio_list)
+           ):
+            raise ValueError(
+                'The number of datasets the DatasetSignalWeightFactorsService '
                 'instance is made for must be equal to the number of '
                 'log-likelihood ratio functions!')
 
-        # Define cache variable for the dataset signal weight factors, which
-        # will be needed when calculating the second derivative w.r.t. ns of the
-        # log-likelihood ratio function.
-        self._cache_fitparam_values_ns = None
-        self._cache_f = None
+    @property
+    def src_detsigyield_weights_service(self):
+        """The instance of SrcDetSigYieldWeightsService, which provides the
+        product of the source weights with the detector signal yield.
+        """
+        return self._src_detsigyield_weights_service
+
+    @src_detsigyield_weights_service.setter
+    def src_detsigyield_weights_service(self, service):
+        if not isinstance(service, SrcDetSigYieldWeightsService):
+            raise TypeError(
+                'The src_detsigyield_weights_service property must be an '
+                'instance of SrcDetSigYieldWeightsService! '
+                f'Its current type is {classname(service)}.')
+        self._src_detsigyield_weights_service = service
 
     @property
-    def dataset_signal_weights(self):
-        """The DatasetSignalWeights instance that provides the relative dataset
-        weight factors.
-        """
-        return self._dataset_signal_weights
-    @dataset_signal_weights.setter
-    def dataset_signal_weights(self, obj):
-        if(not isinstance(obj, DatasetSignalWeights)):
-            raise TypeError('The dataset_signal_weights property must be an '
-                'instance of DatasetSignalWeights!')
-        self._dataset_signal_weights = obj
+    def ds_sig_weight_factors_service(self):
+        """The instance of DatasetSignalWeightFactorsService that provides the
+        relative dataset signal weight factors.
+        """
+        return self._ds_sig_weight_factors_service
+
+    @ds_sig_weight_factors_service.setter
+    def ds_sig_weight_factors_service(self, service):
+        if not isinstance(service, DatasetSignalWeightFactorsService):
+            raise TypeError(
+                'The ds_sig_weight_factors_service property must be an '
+                'instance of DatasetSignalWeightFactorsService! '
+                f'Its current type is {classname(service)}.')
+        self._ds_sig_weight_factors_service = service
 
     @property
     def llhratio_list(self):
         """(read-only) The list of TCLLHRatio instances, which are part of this
         composite log-likelihood-ratio function.
         """
         return self._llhratio_list
+
     @llhratio_list.setter
     def llhratio_list(self, llhratios):
-        if(not issequenceof(llhratios, SingleDatasetTCLLHRatio)):
-            raise TypeError('The llhratio_list property must be a sequence of '
-                'SingleDatasetTCLLHRatio instances!')
+        if not issequenceof(llhratios, SingleDatasetTCLLHRatio):
+            raise TypeError(
+                'The llhratio_list property must be a sequence of '
+                'SingleDatasetTCLLHRatio instances! '
+                f'Its current type is {classname(llhratios)}.')
         self._llhratio_list = list(llhratios)
 
     @property
     def n_selected_events(self):
         """(read-only) The sum of selected events of each individual
         log-likelihood ratio function.
         """
@@ -1491,303 +1029,422 @@
 
     @TCLLHRatio.mean_n_sig_0.setter
     def mean_n_sig_0(self, v):
         TCLLHRatio.mean_n_sig_0.fset(self, v)
         for llhratio in self._llhratio_list:
             llhratio.mean_n_sig_0 = self._mean_n_sig_0
 
-    def change_source_hypo_group_manager(self, src_hypo_group_manager):
+    def change_shg_mgr(self, shg_mgr):
         """Changes the source hypo group manager of all objects of this LLH
-        ratio function, hence, calling the `change_source_hypo_group_manager`
-        method of all TCLLHRatio objects of this LLHRatio instance.
+        ratio function, hence, calling the ``change_shg_mgr``
+        method of all TCLLHRatio instances of this LLHRatio instance.
+
+        Parameters
+        ----------
+        shg_mgr : instance of SourceHypoGroupManager
+            The instance of SourceHypoGroupManager that defines the groups of
+            source hypotheses.
         """
-        # Change the source hypo group manager of the DatasetSignalWeights
-        # instance.
-        self._dataset_signal_weights.change_source_hypo_group_manager(
-            src_hypo_group_manager)
+        self._src_detsigyield_weights_service.change_shg_mgr(
+            shg_mgr=shg_mgr)
+
+        self._ds_sig_weight_factors_service.change_shg_mgr(
+            shg_mgr=shg_mgr)
 
         for llhratio in self._llhratio_list:
-            llhratio.change_source_hypo_group_manager(src_hypo_group_manager)
+            llhratio.change_shg_mgr(
+                shg_mgr=shg_mgr)
 
-    def initialize_for_new_trial(self, tl=None):
+    def initialize_for_new_trial(
+            self,
+            tl=None,
+            **kwargs):
         """Initializes the log-likelihood-ratio function for a new trial.
+        It calls the
+        :meth:`~skyllh.core.llhratio.LLHRatio.initialize_for_new_trial` method
+        of the :class:`~skyllh.core.llhratio.LLHRatio` class of each individual
+        log-likelihood ratio function.
+
+        Parameters
+        ----------
+        tl : instance of TimeLord
+            The optional instance of TimeLord to measure timing information.
         """
         for llhratio in self._llhratio_list:
-            llhratio.initialize_for_new_trial(tl=tl)
-
-    def evaluate(self, fitparam_values, tl=None):
+            llhratio.initialize_for_new_trial(
+                tl=tl,
+                **kwargs)
+
+    def evaluate(
+            self,
+            fitparam_values,
+            src_params_recarray=None,
+            tl=None):
         """Evaluates the composite log-likelihood-ratio function and returns its
         value and global fit parameter gradients.
 
         Parameters
         ----------
-        fitparam_values : (N_fitparams)-shaped numpy 1D ndarray
-            The ndarray holding the current values of the global fit parameters.
-            The first element of that array is, by definition, the number of
-            signal events, ns.
+        fitparam_values : instance of numpy ndarray
+            The (N_fitparams,)-shaped numpy 1D ndarray holding the current
+            values of the global fit parameters.
+        src_params_recarray : instance of numpy record ndarray | None
+            The numpy record ndarray of length N_sources holding the parameter
+            names and values of all sources.
+            See the documentation of the
+            :meth:`skyllh.core.parameters.ParameterModelMapper.create_src_params_recarray`
+            method for more information about this array.
+            It case it is ``None``, it will be created automatically from the
+            ``fitparam_values`` argument using the
+            :class:`~skyllh.core.parameters.ParameterModelMapper` instance.
+        tl : instance of TimeLord | None
+            The optional instance of TimeLord that should be used for timing
+            measurements.
 
         Returns
         -------
         log_lambda : float
             The calculated log-lambda value of the composite
             log-likelihood-ratio function.
-        grads : (N_fitparams,)-shaped 1D ndarray
-            The ndarray holding the gradient value of the composite
-            log-likelihood-ratio function for ns and each global fit parameter.
-            By definition the first element is the gradient for ns.
+        grads : instance of numpy ndarray
+            The (N_fitparams,)-shaped 1D ndarray holding the gradient value of
+            the composite log-likelihood-ratio function for each global fit
+            parameter.
         """
-        tracing = CFG['debugging']['enable_tracing']
+        tracing = self._cfg['debugging']['enable_tracing']
 
-        ns = fitparam_values[0]
-        if(tracing):
+        if src_params_recarray is None:
+            src_params_recarray = self._pmm.create_src_params_recarray(
+                gflp_values=fitparam_values
+            )
+
+        n_fitparams = len(fitparam_values)
+
+        ns_pidx = self._pmm.get_gflp_idx('ns')
+
+        ns = fitparam_values[ns_pidx]
+        if tracing:
             logger.debug(
-                '{:s}.evaluate: ns={:.3f}'.format(
-                    classname(self), ns))
+                f'{classname(self)}.evaluate: ns={ns:.3f}')
+
+        # We need to calculate the source detsigyield weights and the dataset
+        # signal weight factors.
+        self._src_detsigyield_weights_service.calculate(
+            src_params_recarray=src_params_recarray)
+        self._ds_sig_weight_factors_service.calculate()
 
         # Get the dataset signal weights and their gradients.
         # f is a (N_datasets,)-shaped 1D ndarray.
-        # f_grads is a (N_datasets,N_fitparams)-shaped 2D ndarray.
-        (f, f_grads) = self._dataset_signal_weights(fitparam_values)
-
-        # Cache f for possible later calculation of the second derivative w.r.t.
-        # ns of the log-likelihood ratio function.
-        self._cache_fitparam_values_ns = ns
-        self._cache_f = f
+        # f_grads is a dictionary holding (N_datasets,)-shaped 1D ndarrays for
+        # each global fit parameter.
+        (f, f_grads_dict) = self._ds_sig_weight_factors_service.get_weights()
+
+        # Convert the f_grads dictionary into a (N_datasets,N_fitparams)
+        f_grads = np.zeros((len(f), n_fitparams), dtype=np.float64)
+        for pidx in f_grads_dict.keys():
+            f_grads[:, pidx] = f_grads_dict[pidx]
 
         nsf = ns * f
 
         # Calculate the composite log-likelihood-ratio function value and the
         # gradient of the composite log-likelihood ratio function for each
         # global fit parameter.
         log_lambda = 0
 
         # Allocate an array for the gradients of the composite log-likelihood
         # function. It is always at least one element long, i.e. the gradient
         # for ns.
-        grads = np.zeros((len(fitparam_values),), dtype=np.float64)
+        grads = np.zeros((n_fitparams,), dtype=np.float64)
 
         # Create an array holding the fit parameter values for a particular
         # llh ratio function. Since we need to adjust ns with nsj it's more
         # efficient to create this array once and use it within the for loop
         # over the llh ratio functions.
-        llhratio_fitparam_values = np.empty(
-            (len(fitparam_values),), dtype=np.float64)
+        llhratio_fitparam_values = fitparam_values.copy()
+
+        pmask = np.ones((n_fitparams,), dtype=np.bool_)
+        pmask[ns_pidx] = False
+
         # Loop over the llh ratio functions.
         for (j, llhratio) in enumerate(self._llhratio_list):
-            if(tracing):
+            if tracing:
                 logger.debug(
-                    'nsf[j={:d}] = {:.3f}'.format(
-                        j, nsf[j]))
-            llhratio_fitparam_values[0] = nsf[j]
-            llhratio_fitparam_values[1:] = fitparam_values[1:]
+                    f'nsf[j={j}] = {nsf[j]:.3f}')
+
+            llhratio_fitparam_values[ns_pidx] = nsf[j]
+
             (log_lambda_j, grads_j) = llhratio.evaluate(
-                llhratio_fitparam_values, tl=tl)
+                fitparam_values=llhratio_fitparam_values,
+                src_params_recarray=src_params_recarray,
+                tl=tl)
             log_lambda += log_lambda_j
+
             # Gradient for ns.
-            grads[0] += grads_j[0] * f[j]
+            grads[ns_pidx] += grads_j[ns_pidx] * f[j]
+
             # Gradient for each global fit parameter, if there are any.
-            if(len(grads) > 1):
-                grads[1:] += grads_j[0] * ns * f_grads[j] + grads_j[1:]
+            if len(grads) > 1:
+                ns_summand = grads_j[ns_pidx] * ns * f_grads[j][pmask]
+                grads[pmask] += ns_summand + grads_j[pmask]
 
         return (log_lambda, grads)
 
-    def calculate_ns_grad2(self, fitparam_values):
+    def calculate_ns_grad2(
+            self,
+            ns,
+            ns_pidx,
+            src_params_recarray,
+            tl=None):
         """Calculates the second derivative w.r.t. ns of the log-likelihood
         ratio function.
-        This method tries to use cached values for the dataset signal weight
-        factors. If cached values don't exist or do not match the given fit
-        parameter values, they will get calculated automatically by calling the
-        evaluate method with the given fit parameter values.
+
+        Note::
+
+            This method takes the dataset signal weight factors from the dataset
+            signal weight factors service. Hence, the service needs to be
+            updated before calling this method.
 
         Parameters
         ----------
-        fitparam_values : numpy (N_fitparams+1)-shaped 1D ndarray
-            The ndarray holding the current values of the fit parameters.
-            By definition, the first element is the fit parameter for the number
-            of signal events, ns.
+        fitparam_values : instance of numpy ndarray
+            The (N_fitparams,)-shaped 1D ndarray holding the current values of
+            the global fit parameters.
+        ns : float
+            The value of the global fit paramater ns.
+        ns_pidx : int
+            The index of the global parameter ns.
+        src_params_recarray : instance of numpy record ndarray
+            The numpy record ndarray of length N_sources holding the parameter
+            names and values of all sources.
+            See the documentation of the
+            :meth:`skyllh.core.parameters.ParameterModelMapper.create_src_params_recarray`
+            method for more information about this array.
+        tl : instance of TimeLord | None
+            The optional instance of TimeLord that should be used for timing
+            measurements.
 
         Returns
         -------
         nsgrad2 : float
             The second derivative w.r.t. ns of the log-likelihood ratio function
             for the given fit parameter values.
         """
-        ns = fitparam_values[0]
-
-        # Check if the cached fit parameters match the given ones. The ns value
-        # is special to the multi-dataset LLH ratio function, but all the other
-        # fit parameters are shared by all the LLH ratio functions of the
-        # different datasets. So those we just query from the first LLH ratio
-        # function.
-        if((self._cache_fitparam_values_ns is None) or
-           (self._cache_fitparam_values_ns != ns) or
-           (not np.all(self._llhratio_list[0]._cache_fitparam_values[1:] == fitparam_values[1:]))):
-            self.evaluate(fitparam_values)
+        (f, f_grads_dict) = self._ds_sig_weight_factors_service.get_weights()
 
-        nsf = ns * self._cache_f
+        nsf = ns * f
 
         nsgrad2j = np.empty((len(self._llhratio_list),), dtype=np.float64)
-        # Loop over the llh ratio functions and their second derivative.
-        llhratio_fitparam_values = np.empty(
-            (len(fitparam_values),), dtype=np.float64)
+
+        # Loop over the llh ratio functions and calculate their second
+        # derivative.
         for (j, llhratio) in enumerate(self._llhratio_list):
-            llhratio_fitparam_values[0] = nsf[j]
-            llhratio_fitparam_values[1:] = fitparam_values[1:]
-            nsgrad2j[j] = llhratio.calculate_ns_grad2(llhratio_fitparam_values)
+            nsgrad2j[j] = llhratio.calculate_ns_grad2(
+                ns=nsf[j],
+                ns_pidx=ns_pidx,
+                src_params_recarray=src_params_recarray,
+                tl=tl)
 
-        nsgrad2 = np.sum(nsgrad2j * self._cache_f**2)
+        nsgrad2 = np.sum(nsgrad2j * f**2)
 
         return nsgrad2
 
 
-class NsProfileMultiDatasetTCLLHRatio(TCLLHRatio):
+class NsProfileMultiDatasetTCLLHRatio(
+        TCLLHRatio):
     r"""This class implements a profile log-likelihood ratio function that has
     only ns as fit parameter. It uses a MultiDatasetTCLLHRatio instance as
     log-likelihood function. Hence, mathematically it is
 
     .. math::
 
-        \Lambda(n_s) = \frac{L(n_s)}{L(n_s=n_{s,0})},
+        \Lambda(n_{\mathrm{s}}) = \frac{L(n_{\mathrm{s}})}{L(n_{\mathrm{s}}=n_{\mathrm{s},0})},
 
-    where :math:`n_{s,0}` is the fixed mean number of signal events for the
-    null-hypothesis.
+    where :math:`n_{\mathrm{s},0}` is the fixed mean number of signal events for
+    the null-hypothesis.
     """
-    def __init__(self, minimizer, mean_n_sig_0, llhratio):
-        """Creates a new ns-profile log-likelihood-ratio function with a
-        null-hypothesis where ns is fixed to `mean_n_sig_0`.
+    def __init__(
+            self,
+            pmm,
+            minimizer,
+            mean_n_sig_0,
+            llhratio,
+            **kwargs):
+        r"""Creates a new ns-profile log-likelihood-ratio function with a
+        null-hypothesis where :math:`n_{\mathrm{s}}` is fixed to
+        ``mean_n_sig_0``.
 
         Parameters
         ----------
+        pmm : instance of ParameterModelMapper
+            The instance of ParameterModelMapper providing the mapping of
+            global parameters to local parameters of individual models.
         minimizer : instance of Minimizer
             The Minimizer instance that should be used to minimize the negative
             of this log-likelihood ratio function.
         mean_n_sig_0 : float
             The fixed parameter value for the mean number of signal events of
             the null-hypothesis.
         llhratio : instance of MultiDatasetTCLLHRatio
             The instance of MultiDatasetTCLLHRatio, which should be used as
             log-likelihood function.
         """
-        super(NsProfileMultiDatasetTCLLHRatio, self).__init__(
-            minimizer, mean_n_sig_0)
+        super().__init__(
+            pmm=pmm,
+            minimizer=minimizer,
+            mean_n_sig_0=mean_n_sig_0,
+            **kwargs)
 
         self.llhratio = llhratio
 
-        # Check that the given log-likelihood-ratio function has no fit
-        # parameters, i.e. only ns in the end.
-        for sub_llhratio in llhratio.llhratio_list:
-            n_global_fitparams = sub_llhratio.src_fitparam_mapper.n_global_fitparams
-            if(n_global_fitparams != 0):
-                raise ValueError('The log-likelihood-ratio functions of the '
-                    'MultiDatasetTCLLHRatio instance must have no global fit '
-                    'parameters, i.e. only ns in the end! Currently it has %d '
-                    'global fit parameters'%(n_global_fitparams))
+        if self._pmm.n_global_floating_params != 1:
+            raise ValueError(
+                'The log-likelihood-ratio function implemented by '
+                f'{classname(self)} provides functionality only for LLH '
+                'function with a single global fit parameter! Currently there '
+                f'are {pmm.n_global_floating_params} global fit parameters '
+                'defined!')
 
         # Define a member to hold the constant null-hypothesis log-likelihood
         # function value for ns=mean_n_sig_0.
         self._logL_0 = None
 
     @property
     def llhratio(self):
         """The instance of MultiDatasetTCLLHRatio, which should be used as
         log-likelihood function.
         """
         return self._llhratio
+
     @llhratio.setter
     def llhratio(self, obj):
-        if(not isinstance(obj, MultiDatasetTCLLHRatio)):
-            raise TypeError('The llhratio property must be an instance of '
-                'MultiDatasetTCLLHRatio!')
+        if not isinstance(obj, MultiDatasetTCLLHRatio):
+            raise TypeError(
+                'The llhratio property must be an instance of '
+                'MultiDatasetTCLLHRatio! '
+                f'Its current type is {classname(obj)}.')
         self._llhratio = obj
 
-    def change_source_hypo_group_manager(self, src_hypo_group_manager):
+    def change_shg_mgr(
+            self,
+            shg_mgr):
         """Changes the source hypo group manager of all objects of this LLH
-        ratio function, hence, calling the `change_source_hypo_group_manager`
+        ratio function, hence, calling the ``change_shg_mgr``
         method of the underlaying MultiDatasetTCLLHRatio instance of this
         LLHRatio instance.
+
+        Parameters
+        ----------
+        shg_mgr : instance of SourceHypoGroupManager
+            The new instance of SourceHypoGroupManager.
         """
-        self._llhratio.change_source_hypo_group_manager(src_hypo_group_manager)
+        self._llhratio.change_shg_mgr(shg_mgr=shg_mgr)
 
-    def initialize_for_new_trial(self, tl=None):
+    def initialize_for_new_trial(
+            self,
+            tl=None,
+            **kwargs):
         """Initializes the log-likelihood-ratio function for a new trial.
 
         Parameters
         ----------
-        tl : TimeLord | None
-            The optional TimeLord instance that should be used for timing
+        tl : instance of TimeLord | None
+            The optional instance of TimeLord that should be used for timing
             measurements.
         """
-        self._llhratio.initialize_for_new_trial(tl=tl)
+        self._llhratio.initialize_for_new_trial(
+            tl=tl,
+            **kwargs)
 
         # Compute the constant log-likelihood function value for the
         # null-hypothesis.
         fitparam_values_0 = np.array([self._mean_n_sig_0], dtype=np.float64)
-        (self._logL_0, grads_0) = self._llhratio.evaluate(fitparam_values_0)
-
-    def evaluate(self, fitparam_values):
+        (self._logL_0, grads_0) = self._llhratio.evaluate(
+            fitparam_values=fitparam_values_0,
+            tl=tl)
+
+    def evaluate(
+            self,
+            fitparam_values,
+            src_params_recarray=None,
+            tl=None):
         """Evaluates the log-likelihood-ratio function and returns its value and
         global fit parameter gradients.
 
         Parameters
         ----------
-        fitparam_values : (N_fitparams)-shaped numpy 1D ndarray
-            The ndarray holding the current values of the global fit parameters.
-            The first element of that array is, by definition, the number of
-            signal events, ns.
+        fitparam_values : instance of numpy ndarray
+            The (1,)-shaped numpy 1D ndarray holding the current
+            values of the global fit parameters.
+            By definition of this LLH ratio function, it must contain the single
+            fit paramater value for ns.
+        src_params_recarray : instance of numpy record ndarray
+            The numpy record ndarray of length N_sources holding the parameter
+            names and values of all sources.
+            See the documentation of the
+            :meth:`skyllh.core.parameters.ParameterModelMapper.create_src_params_recarray`
+            method for more information about this array.
+        tl : instance of TimeLord | None
+            The optional instance of TimeLord that should be used for timing
+            measurements.
 
         Returns
         -------
         log_lambda : float
             The calculated log-lambda value of this log-likelihood-ratio
             function.
-        grads : (N_fitparams,)-shaped 1D ndarray
+        grads : (1,)-shaped 1D ndarray
             The ndarray holding the gradient value of this log-likelihood-ratio
             for ns.
-            By definition the first element is the gradient for ns.
         """
-        (logL, grads) = self._llhratio.evaluate(fitparam_values)
+        (logL, grads) = self._llhratio.evaluate(
+            fitparam_values=fitparam_values,
+            src_params_recarray=src_params_recarray,
+            tl=tl)
 
-        return (logL - self._logL_0, grads)
+        log_lambda = logL - self._logL_0
 
-    def calculate_ns_grad2(self, fitparam_values):
+        return (log_lambda, grads)
+
+    def calculate_ns_grad2(
+            self,
+            ns,
+            ns_pidx,
+            src_params_recarray,
+            tl=None):
         """Calculates the second derivative w.r.t. ns of the log-likelihood
         ratio function.
 
         Parameters
         ----------
-        fitparam_values : numpy (N_fitparams+1)-shaped 1D ndarray
-            The ndarray holding the current values of the fit parameters.
-            By definition, the first element is the fit parameter for the number
-            of signal events, ns.
+        ns : float
+            The value of the global fit paramater ns.
+        ns_pidx : int
+            The index of the global fit paramater ns. By definition this must
+            be ``0``.
+        src_params_recarray : instance of numpy record ndarray
+            The numpy record ndarray of length N_sources holding the parameter
+            names and values of all sources.
+            See the documentation of the
+            :meth:`skyllh.core.parameters.ParameterModelMapper.create_src_params_recarray`
+            method for more information about this array.
+        tl : instance of TimeLord | None
+            The optional instance of TimeLord that should be used for timing
+            measurements.
 
         Returns
         -------
         nsgrad2 : float
             The second derivative w.r.t. ns of the log-likelihood ratio function
             for the given fit parameter values.
         """
-        return self._llhratio.calculate_ns_grad2(fitparam_values)
-
-#class NestedProfileLLHRatio(LLHRatio, metaclass=abc.ABCMeta):
-    #r"""This class provides the abstract base class for a nested profile
-    #log-likelihood ratio function, which is, by definition, of the form
-
-    #.. math::
-
-        #\Lambda = \frac{\sup_{\Theta_0} L(\theta|D)}{\sup_{\Theta} L(\theta|D)}
-
-    #where :math:`\theta` are the possible fit parameters, and :math:`\Theta`
-    #and :math:`\Theta_0` are the total and nested fit parameter spaces,
-    #respectively.
-    #"""
-
-    #def __init__(self, ):
-        #super(NestedProfileLLHRatio, self).__init__()
-
-
-#class MultiDatasetNestedProfileLLHRatio(NestedProfileLLHRatio):
-    #"""This class provides a nested profile log-likelihood ratio function for
-    #multiple data sets.
-    #"""
-    #def __init__(self):
-        #super(MultiDatasetNestedProfileLLHRatio, self).__init__()
+        if ns_pidx != 0:
+            raise ValueError(
+                'The value of the ns_pidx argument must be 0! '
+                f'Its current value is {ns_pidx}.')
+
+        nsgrad2 = self._llhratio.calculate_ns_grad2(
+            ns=ns,
+            ns_pidx=ns_pidx,
+            src_params_recarray=src_params_recarray,
+            tl=tl)
 
+        return nsgrad2
```

### Comparing `skyllh-23.1.1/skyllh/core/minimizer.py` & `skyllh-23.2.0/skyllh/core/minimizer.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,34 +1,51 @@
 """
 The minimizer module provides functionality for the minimization process of
 a function.
 """
 import abc
 import logging
-import numpy as np
 import scipy.optimize
-from typing import Optional, Dict, Any, List
 
-from skyllh.core.parameters import FitParameterSet
-from skyllh.core.py import classname
+import numpy as np
+
+from skyllh.core.config import (
+    HasConfig,
+)
+from skyllh.core.parameters import (
+    ParameterSet,
+)
+from skyllh.core.py import (
+    classname,
+)
 
 
 logger = logging.getLogger(__name__)
 
 
-class MinimizerImpl(object, metaclass=abc.ABCMeta):
+class MinimizerImpl(
+        HasConfig,
+        metaclass=abc.ABCMeta,
+):
     """Abstract base class for a minimizer implementation. It defines the
     interface between the implementation and the Minimizer class.
     """
 
-    def __init__(self):
-        super(MinimizerImpl, self).__init__()
+    def __init__(self, **kwargs):
+        super().__init__(**kwargs)
 
     @abc.abstractmethod
-    def minimize(self, initials, bounds, func, func_args=None, **kwargs):
+    def minimize(
+            self,
+            initials,
+            bounds,
+            func,
+            func_args=None,
+            **kwargs,
+    ):
         """This method is supposed to minimize the given function with the given
         initials.
 
         Parameters
         ----------
         initials : 1D (N_fitparams)-shaped numpy ndarray
             The ndarray holding the initial values of all the fit parameters.
@@ -116,22 +133,46 @@
         repeatable : bool
             The flag if the minimization process can be repeated to obtain a
             better minimum.
         """
         pass
 
 
-class ScipyMinimizerImpl(MinimizerImpl):
+class ScipyMinimizerImpl(
+        MinimizerImpl
+):
     """Wrapper for `scipy.optimize.minimize`"""
 
-    def __init__(self, method: str) -> None:
-        super().__init__()
+    def __init__(
+            self,
+            method: str,
+            **kwargs,
+    ) -> None:
+        """Creates a new instance of ScipyMinimizerImpl.
+
+        Parameters
+        ----------
+        method : str
+            The minimizer method to use. See the documentation for the method
+            argument of the :func:`scipy.optimize.minimize` function for
+            possible values.
+        """
+        super().__init__(
+            **kwargs)
+
         self._method = method
 
-    def minimize(self, initials, bounds, func, func_args=None, **kwargs):
+    def minimize(
+            self,
+            initials,
+            bounds,
+            func,
+            func_args=None,
+            **kwargs,
+    ):
         """Minimizes the given function ``func`` with the given initial function
         argument values ``initials``.
 
         Parameters
         ----------
         initials : 1D numpy ndarray
             The ndarray holding the initial values of all the fit parameters.
@@ -150,79 +191,80 @@
             ``func_provides_grads`` keyword argument option is set to True.
             If set to False, ``func`` must return only the function value.
         func_args : sequence | None
             Optional sequence of arguments for ``func``.
 
         Additional Keyword Arguments
         ----------------------------
+
         Additional keyword arguments include options for this minimizer
         implementation. Possible options are:
 
             func_provides_grads : bool
                 Flag if the function ``func`` also returns its gradients.
                 Default is ``True``.
 
+
         Any additional keyword arguments are passed on to the underlaying
         :func:`scipy.optimize.minimize` minimization function.
 
         Returns
         -------
-        xmin : 1D ndarray
-            The array containing the function arguments at the function's
+        xmin : instance of numpy.ndarray
+            The 1D array containing the function arguments at the function's
             minimum.
         fmin : float
             The function value at its minimum.
-        res : scipy.optimize.OptimizeResult
-            Scipy OptimizeResult
+        res : instance of scipy.optimize.OptimizeResult
+            The scipy OptimizeResult.
         """
 
         method_supports_bounds = False
 
-        constraints: Optional[List[Dict[str, Any]]]
+        # constraints: List[Dict[str, Any]]
         constraints = None
 
         # Check if method allows for bounds
-        if(self._method in ["L-BFGS-B", "TNC", "SLSQP"]):
+        if self._method in ["L-BFGS-B", "TNC", "SLSQP"]:
             method_supports_bounds = True
-        elif(self._method == "COBYLA"):
+        elif self._method == "COBYLA":
             # COBYLA doesn't allow for bounds, but we can convert bounds
             # to a linear constraint
 
             constraints = []
             for (bound_num, bound) in enumerate(bounds):
                 lower, upper = bound
                 lc = {"type": "ineq",
                       "fun": lambda x, lb=lower, i=bound_num: x[i] - lb}
                 uc = {"type": "ineq",
                       "fun": lambda x, ub=upper, i=bound_num: ub - x[i]}
                 constraints.append(lc)
                 constraints.append(uc)
             bounds = None
 
-        if((bounds is not None) and (not method_supports_bounds)):
+        if (bounds is not None) and (not method_supports_bounds):
             logger.warn(
-                "Selected minimization method ({}) does not "
-                "support bounds. Continue at your own risk.".format(
-                    self._method))
+                f'Selected minimization method "{self._method}" does not '
+                'support bounds. Continue at your own risk!')
             bounds = None
 
-        if(func_args is None):
+        if func_args is None:
             func_args = tuple()
-        if(kwargs is None):
+        if kwargs is None:
             kwargs = {}
 
         func_provides_grads = kwargs.pop('func_provides_grads', True)
 
         res = scipy.optimize.minimize(
             func,
             initials,
             bounds=bounds,
             constraints=constraints,
             args=func_args,
-            jac = func_provides_grads,
+            jac=func_provides_grads,
             **kwargs)
 
         return (res.x, res.fun, res)
 
     def get_niter(self, status):
         """Returns the number of iterations needed to find the minimum.
 
@@ -275,41 +317,56 @@
         repeatable : bool
             The flag if the minimization process can be repeated to obtain a
             better minimum.
         """
         return False
 
 
-class LBFGSMinimizerImpl(MinimizerImpl):
+class LBFGSMinimizerImpl(
+        MinimizerImpl
+):
     """The LBFGSMinimizerImpl class provides the minimizer implementation for
     L-BFG-S minimizer used from the :mod:`scipy.optimize` module.
     """
 
-    def __init__(self, ftol=1e-6, pgtol=1e-5, maxls=100):
+    def __init__(
+            self,
+            ftol=1e-6,
+            pgtol=1e-5,
+            maxls=100,
+            **kwargs,
+    ):
         """Creates a new L-BGF-S minimizer instance to minimize the given
         likelihood function with its given partial derivatives.
 
         Parameters
         ----------
         ftol : float
             The function value tolerance.
         pgtol : float
             The gradient value tolerance.
         maxls : int
             The maximum number of line search steps for an interation.
         """
-        super(LBFGSMinimizerImpl, self).__init__()
+        super().__init__(**kwargs)
 
         self._ftol = ftol
         self._pgtol = pgtol
         self._maxls = maxls
 
         self._fmin_l_bfgs_b = scipy.optimize.fmin_l_bfgs_b
 
-    def minimize(self, initials, bounds, func, func_args=None, **kwargs):
+    def minimize(
+            self,
+            initials,
+            bounds,
+            func,
+            func_args=None,
+            **kwargs,
+    ):
         """Minimizes the given function ``func`` with the given initial function
         argument values ``initials``.
 
         Parameters
         ----------
         initials : 1D numpy ndarray
             The ndarray holding the initial values of all the fit parameters.
@@ -335,14 +392,15 @@
         Additional keyword arguments include options for this minimizer
         implementation. Possible options are:
 
             func_provides_grads : bool
                 Flag if the function ``func`` also returns its gradients.
                 Default is ``True``.
 
+
         Any additional keyword arguments are passed on to the underlaying
         :func:`scipy.optimize.fmin_l_bfgs_b` minimization function.
 
         Returns
         -------
         xmin : 1D ndarray
             The array containing the function arguments at the function's
@@ -357,39 +415,42 @@
                 The number of iterations needed to find the minimum.
             warnflag : int
                 The warning flag indicating if the minimization did converge.
                 The possible values are:
 
                     0: The minimization converged.
         """
-        if(func_args is None):
+        if func_args is None:
             func_args = tuple()
-        if(kwargs is None):
+        if kwargs is None:
             kwargs = {}
 
-        if('factr' not in kwargs):
+        if 'factr' not in kwargs:
             kwargs['factr'] = self._ftol / np.finfo(float).eps
-        if('pgtol' not in kwargs):
+        if 'pgtol' not in kwargs:
             kwargs['pgtol'] = self._pgtol
-        if('maxls' not in kwargs):
+        if 'maxls' not in kwargs:
             kwargs['maxls'] = self._maxls
 
         func_provides_grads = kwargs.pop('func_provides_grads', True)
 
         (xmin, fmin, status) = self._fmin_l_bfgs_b(
             func, initials,
             bounds=bounds,
             args=func_args,
             approx_grad=not func_provides_grads,
             **kwargs
         )
 
         return (xmin, fmin, status)
 
-    def get_niter(self, status):
+    def get_niter(
+            self,
+            status,
+    ):
         """Returns the number of iterations needed to find the minimum.
 
         Parameters
         ----------
         status : dict
             The dictionary with the status information about the minimization
             process.
@@ -397,15 +458,18 @@
         Returns
         -------
         niter : int
             The number of iterations needed to find the minimum.
         """
         return status['nit']
 
-    def has_converged(self, status):
+    def has_converged(
+            self,
+            status,
+    ):
         """Analyzes the status information dictionary if the minimization
         process has converged. By definition the minimization process has
         converged if ``status['warnflag']`` equals 0.
 
         Parameters
         ----------
         status : dict
@@ -413,19 +477,22 @@
             process.
 
         Returns
         -------
         converged : bool
             The flag if the minimization has converged (True), or not (False).
         """
-        if(status['warnflag'] == 0):
+        if status['warnflag'] == 0:
             return True
         return False
 
-    def is_repeatable(self, status):
+    def is_repeatable(
+            self,
+            status,
+    ):
         """Checks if the minimization process can be repeated to get a better
         result. It's repeatable if
 
             `status['warnflag'] == 2 and 'FACTR' in str(status['task'])`
 
         Parameters
         ----------
@@ -435,50 +502,64 @@
 
         Returns
         -------
         repeatable : bool
             The flag if the minimization process can be repeated to obtain a
             better minimum.
         """
-        if(status['warnflag'] == 2):
+        if status['warnflag'] == 2:
             task = str(status['task'])
-            if('FACTR' in task):
+            if 'FACTR' in task:
                 return True
-            if('ABNORMAL_TERMINATION_IN_LNSRCH' in task):
+            if 'ABNORMAL_TERMINATION_IN_LNSRCH' in task:
                 # This is causes most probably by starting the minimization at
                 # a parameter boundary.
                 return True
         return False
 
 
-class NR1dNsMinimizerImpl(MinimizerImpl):
+class NR1dNsMinimizerImpl(
+        MinimizerImpl
+):
     """The NR1dNsMinimizerImpl class provides a minimizer implementation for the
     Newton-Raphson method for finding the minimum of a one-dimensional R1->R1
     function, i.e. a function that depends solely on one parameter, the number of
     signal events ns.
     """
 
-    def __init__(self, ns_tol=1e-3, max_steps=100):
+    def __init__(
+            self,
+            ns_tol=1e-3,
+            max_steps=100,
+            **kwargs,
+    ):
         """Creates a new NRNs minimizer instance to minimize the given
         likelihood function with its given partial derivatives.
 
         Parameters
         ----------
         ns_tol : float
             The tolerance / precision for the ns parameter value.
         max_steps : int
             The maximum number of NR steps. If max_step is reached,
             the fit is considered NOT converged.
         """
-        super(NR1dNsMinimizerImpl, self).__init__()
+        super().__init__(**kwargs)
 
         self.ns_tol = ns_tol
         self.max_steps = max_steps
 
-    def minimize(self, initials, bounds, func, func_args=None, **kwargs):
+    def minimize(  # noqa: C901
+            self,
+            initials,
+            bounds,
+            func,
+            func_args=None,
+            **kwargs,
+    ):
         """Minimizes the given function ``func`` with the given initial function
         argument values ``initials``. This minimizer implementation will only
         vary the first parameter. All other parameters will be set to their
         initial value.
 
         Parameters
         ----------
@@ -533,96 +614,103 @@
                     1: The minimization did NOT converge within self.max_steps
                        number of steps
 
             warnreason: str
                 The description for the set warn flag.
 
         """
-        if(func_args is None):
+        if func_args is None:
             func_args = tuple()
 
         (ns_min, ns_max) = bounds[0]
-        if(ns_min > initials[0]):
-            raise ValueError('The initial value for ns (%g) must be equal or '
-                             'greater than the minimum bound value for ns (%g)' % (
-                                 initials[0], ns_min))
+        if ns_min > initials[0]:
+            raise ValueError(
+                f'The initial value for ns ({initials[0]:g}) must be equal or '
+                f'greater than the minimum bound value for ns ({ns_min:g})')
 
         ns_tol = self.ns_tol
 
         niter = 0
         x = np.copy(initials).astype(np.float64)
         ns = x[0]
 
         # Initialize stepsize to be larger than ns tolerance.
         # Also initialize first derivative to large value.
         # Want to perform at least one NR iteration.
         step = ns_tol + 1
         fprime = 1000
         # NR does not guarantee convergence, thus limit iterations.
         max_steps = self.max_steps
-        status = {'warnflag': 0, 'warnreason': ''}
+        status = {
+            'warnflag': 0,
+            'warnreason': '',
+        }
         f = None
         at_boundary = False
 
         # We do the minimization process while the precision of ns is not
         # reached yet or the function is still rising or falling fast, i.e. the
         # minimum is in a deep well.
         # In case the optimum is found outside the bounds on ns the best fit
         # will be set to the boundary value and the fit considered converged.
-        while( ((ns_tol < np.fabs(step)) or (np.fabs(fprime) > 1.e-1)) and (niter < max_steps) ):
+        while ((ns_tol < np.fabs(step)) or (np.fabs(fprime) > 1.e-1)) and\
+              (niter < max_steps):
 
             x[0] = ns
             (f, fprime, fprimeprime) = func(x, *func_args)
             step = -fprime / fprimeprime
 
             # Exit optimization if ns is at boundary but next step would be outside.
-            if((ns == ns_min and step < 0.0) or (ns == ns_max and step > 0.0)):
+            if (ns == ns_min and step < 0.0) or (ns == ns_max and step > 0.0):
                 at_boundary = True
 
-                if(ns == ns_min):
+                if ns == ns_min:
                     status['warnflag'] = -2
-                    status['warnreason'] = ('Function minimum is below the '
-                        'minimum bound of the parameter '
-                        'value. Convergence forced at boundary.')
-                elif(ns == ns_max):
+                    status['warnreason'] = (
+                        'Function minimum is below the minimum bound of the '
+                        'parameter value. Convergence forced at boundary.')
+                elif ns == ns_max:
                     status['warnflag'] = -1
-                    status['warnreason'] = ('Function minimum is above the '
-                        'maximum bound of the parameter '
-                        'value. Convergence forced at boundary.')
+                    status['warnreason'] = (
+                        'Function minimum is above the maximum bound of the '
+                        'parameter value. Convergence forced at boundary.')
                 break
 
             # Always perform step in ns as it improves the solution.
             ns += step
 
             # Do not allow ns outside boundaries.
-            if(ns < ns_min):
+            if ns < ns_min:
                 ns = ns_min
-            elif(ns > ns_max):
+            elif ns > ns_max:
                 ns = ns_max
 
             # Increase counter since a step was taken.
             niter += 1
 
         x[0] = ns
         # Once converged evaluate function at minimum value unless
         # Convergence was forced at boundary
         # in which case function value is already known.
-        if(not at_boundary):
+        if not at_boundary:
             (f, fprime, fprimeprime) = func(x, *func_args)
 
-        if(niter == max_steps):
+        if niter == max_steps:
             status['warnflag'] = 1
-            status['warnreason'] = ('NR optimization did not converge within {} '
-                                    'NR steps.'.format(niter))
+            status['warnreason'] = (
+                f'NR optimization did not converge within {niter} NR steps.')
 
         status['niter'] = niter
         status['last_nr_step'] = step
         return (x, f, status)
 
-    def get_niter(self, status):
+    def get_niter(
+            self,
+            status,
+    ):
         """Returns the number of iterations needed to find the minimum.
 
         Parameters
         ----------
         status : dict
             The dictionary with the status information about the minimization
             process.
@@ -630,15 +718,18 @@
         Returns
         -------
         niter : int
             The number of iterations needed to find the minimum.
         """
         return status['niter']
 
-    def has_converged(self, status):
+    def has_converged(
+            self,
+            status,
+    ):
         """Analyzes the status information dictionary if the minimization
         process has converged. By definition the minimization process has
         converged if ``status['warnflag']`` is smaller or equal to 0.
 
         Parameters
         ----------
         status : dict
@@ -646,48 +737,65 @@
             process.
 
         Returns
         -------
         converged : bool
             The flag if the minimization has converged (True), or not (False).
         """
-        if(status['warnflag'] <= 0):
+        if status['warnflag'] <= 0:
             return True
 
         return False
 
     def is_repeatable(self, status):
         """Checks if the minimization process can be repeated to get a better
         result. By definition of this minimization method, this method will
         always return ``False``.
         """
         return False
 
 
-class NRNsScan2dMinimizerImpl(NR1dNsMinimizerImpl):
+class NRNsScan2dMinimizerImpl(
+        NR1dNsMinimizerImpl
+):
     """The NRNsScan2dMinimizerImpl class provides a minimizer implementation for
     the R2->R1 function where the first dimension is minimized using the
     Newton-Raphson minimization method and the second dimension is scanned.
     """
 
-    def __init__(self, p2_scan_step, ns_tol=1e-3):
+    def __init__(
+            self,
+            p2_scan_step,
+            ns_tol=1e-3,
+            **kwargs,
+    ):
         """Creates a new minimizer implementation instance.
 
         Parameters
         ----------
         p2_scan_step : float
             The step size for the scan of the second parameter of the function
             to minimize.
         ns_tol : float
             The tolerance / precision for the ns parameter value.
         """
-        super().__init__(ns_tol=ns_tol)
+        super().__init__(
+            ns_tol=ns_tol,
+            **kwargs)
+
         self.p2_scan_step = p2_scan_step
 
-    def minimize(self, initials, bounds, func, func_args=None, **kwargs):
+    def minimize(
+            self,
+            initials,
+            bounds,
+            func,
+            func_args=None,
+            **kwargs,
+    ):
         """Minimizes the given function ``func`` with the given initial function
         argument values ``initials``. This minimizer implementation will only
         vary the first two parameters. The first parameter is the number of
         signal events, ns, and it is minimized using the Newton-Rapson method.
         The second parameter is scanned through its boundaries in step sizes of
         ``p2_scan_step``.
 
@@ -749,104 +857,122 @@
                 The description for the set warn flag.
         """
         p2_low = bounds[1][0]
         p2_high = bounds[1][1]
         p2_scan_values = np.linspace(
             p2_low, p2_high, int((p2_high-p2_low)/self.p2_scan_step)+1)
 
-        logger.debug('Minimize func by scanning 2nd parameter in {:d} steps '
-                     'with a step size of {:g}'.format(
-                         len(p2_scan_values), np.mean(np.diff(p2_scan_values))))
+        logger.debug(
+            'Minimize func by scanning 2nd parameter in '
+            f'{len(p2_scan_values):d} steps with a step size of '
+            f'{np.mean(np.diff(p2_scan_values)):g}')
 
         niter_total = 0
         best_xmin = None
         best_fmin = None
         best_status = None
         for p2_value in p2_scan_values:
             initials[1] = p2_value
             (xmin, fmin, status) = super().minimize(
                 initials, bounds, func, func_args, **kwargs)
             niter_total += status['niter']
-            if((best_fmin is None) or (fmin < best_fmin)):
+            if (best_fmin is None) or (fmin < best_fmin):
                 best_xmin = xmin
                 best_fmin = fmin
                 best_status = status
 
         best_status['p2_n_steps'] = len(p2_scan_values)
         best_status['niter'] = niter_total
 
         return (best_xmin, best_fmin, best_status)
 
 
-class Minimizer(object):
+class Minimizer(
+        object
+):
     """The Minimizer class provides the general interface for minimizing a
     function. The class takes an instance of MinimizerImpl for a specific
     minimizer implementation.
     """
 
-    def __init__(self, minimizer_impl, max_repetitions=100):
+    def __init__(
+            self,
+            minimizer_impl,
+            max_repetitions=100,
+            **kwargs,
+    ):
         """Creates a new Minimizer instance.
 
         Parameters
         ----------
         minimizer_impl : instance of MinimizerImpl
             The minimizer implementation for a specific minimizer algorithm.
         max_repetitions : int
             In case the minimization process did not converge at the first time
             this option specifies the maximum number of repetitions with
             different initials.
         """
+        super().__init__(**kwargs)
+
         self.minimizer_impl = minimizer_impl
         self.max_repetitions = max_repetitions
 
     @property
     def minimizer_impl(self):
         """The instance of MinimizerImpl, which provides the implementation of
         the minimizer.
         """
         return self._minimizer_impl
 
     @minimizer_impl.setter
     def minimizer_impl(self, impl):
-        if(not isinstance(impl, MinimizerImpl)):
-            raise TypeError('The minimizer_impl property must be an instance '
-                            'of MinimizerImpl!')
+        if not isinstance(impl, MinimizerImpl):
+            raise TypeError(
+                'The minimizer_impl property must be an instance of '
+                'MinimizerImpl!')
         self._minimizer_impl = impl
 
     @property
     def max_repetitions(self):
         """In case the minimization process did not converge at the first time
         this option specifies the maximum number of repetitions with
         different initials.
         """
         return self._max_repetitions
 
     @max_repetitions.setter
     def max_repetitions(self, n):
-        if(not isinstance(n, int)):
-            raise TypeError('The maximal repetitions property must be of type '
-                            'int!')
+        if not isinstance(n, int):
+            raise TypeError(
+                'The maximal repetitions property must be of type int!')
         self._max_repetitions = n
 
-    def minimize(self, rss, fitparamset, func, args=None, kwargs=None):
+    def minimize(
+            self,
+            rss,
+            paramset,
+            func,
+            args=None,
+            kwargs=None,
+    ):
         """Minimizes the the given function ``func`` by calling the ``minimize``
         method of the minimizer implementation.
 
         After the minimization process it calls the ``has_converged`` and
         ``is_repeatable`` methods of the minimizer implementation to determine
         if an additional minimization attempt has to be performed.
         This is repeated until the minimization process did converge or if the
         maximal number of repetitions has occurred.
 
         Parameters
         ----------
         rss : RandomStateService instance
             The RandomStateService instance to draw random numbers from.
-        fitparamset : instance of FitParameterSet
-            The set of FitParameter instances defining fit parameters of the
+        paramset : instance of ParameterSet
+            The ParameterSet instances holding the floating parameters of the
             function ``func``.
         func : callable ``f(x, *args)``
             The function to be minimized. It must have the call signature
 
                 ``__call__(x, *args)``
 
             The return value of ``func`` is minimizer implementation dependent.
@@ -863,70 +989,70 @@
             a minimum.
         fmin : float
             The function value at its minimum.
         status : dict
             The status dictionary with information about the minimization
             process.
         """
-        if(not isinstance(fitparamset, FitParameterSet)):
-            raise TypeError('The fitparamset argument must be an instance of '
-                            'FitParameterSet!')
+        if not isinstance(paramset, ParameterSet):
+            raise TypeError(
+                'The paramset argument must be an instance of ParameterSet!')
 
-        if(kwargs is None):
+        if kwargs is None:
             kwargs = dict()
 
-        bounds = fitparamset.bounds
-        initials = fitparamset.initials
-        logger.debug('Do function minimization: initials: {}'.format(initials))
+        bounds = paramset.floating_param_bounds
+        initials = paramset.floating_param_initials
+        logger.debug(f'Doing function minimization: initials: {initials}.')
 
         (xmin, fmin, status) = self._minimizer_impl.minimize(
             initials, bounds, func, args, **kwargs)
 
         reps = 0
-        while((not self._minimizer_impl.has_converged(status)) and
-              self._minimizer_impl.is_repeatable(status) and
-              reps < self._max_repetitions
-              ):
+        while (not self._minimizer_impl.has_converged(status)) and\
+              (self._minimizer_impl.is_repeatable(status)) and\
+              (reps < self._max_repetitions):
             # The minimizer did not converge at the first time, but it is
             # possible to repeat the minimization process with different
             # initials to obtain a better result.
 
             # Create a new set of random parameter initials based on the
             # parameter bounds.
-            initials = fitparamset.generate_random_initials(rss)
+            initials = paramset.generate_random_floating_param_initials(
+                rss=rss)
 
             logger.debug(
                 'Previous rep ({}) status={}, new initials={}'.format(
                     reps, str(status), str(initials)))
 
             # Repeat the minimization process.
             (xmin, fmin, status) = self._minimizer_impl.minimize(
                 initials, bounds, func, args, **kwargs)
 
             reps += 1
 
         # Store the number of repetitions in the status dictionary.
         status['skyllh_minimizer_n_reps'] = reps
 
-        if(not self._minimizer_impl.has_converged(status)):
+        if not self._minimizer_impl.has_converged(status):
             raise ValueError(
-                'The minimizer did not converge after %d '
-                'repetitions! The maximum number of repetitions is %d. '
-                'The status dictionary is "%s".' % (
-                    reps, self._max_repetitions, str(status)))
+                f'The minimizer did not converge after {reps:d} repetitions! '
+                'The maximum number of repetitions is '
+                f'{self._max_repetitions:d}. The status dictionary is '
+                f'"{str(status)}".')
 
         # Check if any fit value is outside its bounds due to rounding errors by
         # the minimizer. If so, set those fit values to their respective bound
         # value and re-evaluate the function with the corrected fit values.
         condmin = xmin < bounds[:, 0]
         condmax = xmin > bounds[:, 1]
-        if(np.any(condmin) or np.any(condmax)):
+        if np.any(condmin) or np.any(condmax):
             xmin = np.where(condmin, bounds[:, 0], xmin)
             xmin = np.where(condmax, bounds[:, 1], xmin)
-            if(args is None):
+            if args is None:
                 args = tuple()
             (fmin, grads) = func(xmin, *args)
 
         logger.debug(
             '%s (%s): Minimized function: %d iterations, %d repetitions, '
             'xmin=%s' % (
                 classname(self), classname(self._minimizer_impl),
```

### Comparing `skyllh-23.1.1/skyllh/core/minimizers/iminuit.py` & `skyllh-23.2.0/skyllh/core/minimizers/iminuit.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,124 +1,150 @@
 """
-The minimizers/iminuit module provides a SkyLLH interface to the iminuit minimizer.
+The minimizers/iminuit module provides a SkyLLH interface to the iminuit
+minimizer.
 """
-import numpy as np
 
 import iminuit
 
+import numpy as np
+
+from skyllh.core.config import (
+    HasConfig,
+)
 from skyllh.core.debugging import (
     get_logger,
-    is_tracing_enabled
 )
-from skyllh.core.minimizer import MinimizerImpl
+from skyllh.core.minimizer import (
+    MinimizerImpl,
+)
 
 
 logger = get_logger(__name__)
 
 
-class FuncWithGradsFunctor(object):
+class FuncWithGradsFunctor(
+        HasConfig,
+):
     """Helper class to evaluate the LLH function that returns the function value
     and its gradients in two seperate calls, one for the LLH function value and
     one for its gradient values.
     """
 
-    def __init__(self, func, func_args=None):
-        """
-        Initializes a new functor instance for the given function ``func``.
+    def __init__(
+            self,
+            func,
+            func_args=None,
+            **kwargs,
+    ):
+        """Initializes a new functor instance for the given function ``func``.
 
         Parameters
         ----------
         func : callable
             The function with call signature
 
                 ``__call__(x, *args)``
 
             returning the a two-element tuple (f, grads).
         func_args : tuple | None
             The optional positional arguments for the function ``func``.
         """
-        super().__init__()
+        super().__init__(
+            **kwargs)
 
-        if(func_args is None):
+        if func_args is None:
             func_args = tuple()
 
         self._func = func
         self._func_args = func_args
 
-        self._tracing = is_tracing_enabled()
+        self._tracing = self._cfg.is_tracing_enabled
 
         self._cache_x = None
         self._cache_f = None
         self._cache_grads = None
 
     def get_f(self, x):
         tracing = self._tracing
 
-        if(self._cache_x is None):
+        if self._cache_x is None:
             self._cache_x = np.copy(x)
         else:
-            if(np.all(x == self._cache_x)):
-                if(tracing):
+            if np.all(x == self._cache_x):
+                if tracing:
                     logger.debug(
                         f'call_func(x={x}): Return cached f={self._cache_f}')
                 return self._cache_f
             else:
                 np.copyto(self._cache_x, x)
 
         (self._cache_f, self._cache_grads) = self._func(
             x, *self._func_args)
 
-        if(tracing):
+        if tracing:
             logger.debug(
                 f'call_func(x={x}): Return calculated f={self._cache_f}')
         return self._cache_f
 
     def get_grads(self, x):
         tracing = self._tracing
 
-        if(self._cache_x is None):
+        if self._cache_x is None:
             self._cache_x = np.copy(x)
         else:
-            if(np.all(x == self._cache_x)):
-                if(tracing):
+            if np.all(x == self._cache_x):
+                if tracing:
                     logger.debug(
                         f'call_grads(x={x}): Return cached '
                         f'grads={self._cache_grads}')
                 return self._cache_grads
             else:
                 np.copyto(self._cache_x, x)
 
         (self._cache_f, self._cache_grads) = self._func(
             x, *self._func_args)
 
-        if(tracing):
+        if tracing:
             logger.debug(
                 f'call_grads(x={x}): Return calculated '
                 f'grads={self._cache_grads}')
         return self._cache_grads
 
 
-class IMinuitMinimizerImpl(MinimizerImpl):
+class IMinuitMinimizerImpl(
+        MinimizerImpl,
+):
     """The SkyLLH minimizer implementation that utilizes the iminuit minimizer.
     """
 
-    def __init__(self, ftol=1e-6):
+    def __init__(
+            self,
+            ftol=1e-6,
+            **kwargs,
+    ):
         """Creates a new IMinuit minimizer instance to minimize a given
         function.
 
         Parameters
         ----------
         ftol : float
             The function value tolerance as absolute value.
         """
-        super().__init__()
+        super().__init__(**kwargs)
 
         self._ftol = ftol
 
-    def minimize(self, initials, bounds, func, func_args=None, **kwargs):
+    def minimize(
+            self,
+            initials,
+            bounds,
+            func,
+            func_args=None,
+            **kwargs,
+    ):
         """Minimizes the given function ``func`` with the given initial function
         argument values ``initials`` and within the given parameter bounds
         ``bounds``.
 
         Parameters
         ----------
         initials : 1D numpy ndarray
@@ -145,38 +171,40 @@
         Additional keyword arguments include options for this minimizer
         implementation. Possible options are:
 
             func_provides_grads : bool
                 Flag if the function ``func`` also returns its gradients.
                 Default is ``True``.
 
+
         Any additional keyword arguments are passed on to the underlaying
         :func:`iminuit.minimize` minimization function.
 
         Returns
         -------
         xmin : 1D ndarray
             The array containing the function arguments at the function's
             minimum.
         fmin : float
             The function value at its minimum.
         res : iminuit.OptimizeResult
             The iminuit OptimizeResult dictionary with additional information.
         """
-        if(func_args is None):
+        if func_args is None:
             func_args = tuple()
-        if(kwargs is None):
+        if kwargs is None:
             kwargs = dict()
 
         func_provides_grads = kwargs.pop('func_provides_grads', True)
 
-        if(func_provides_grads):
+        if func_provides_grads:
             # The function func returns the function value and its gradients,
             # so we need to use the FuncWithGradsFunctor helper class.
             functor = FuncWithGradsFunctor(
+                cfg=self._cfg,
                 func=func,
                 func_args=func_args)
 
             res = iminuit.minimize(
                 fun=functor.get_f,
                 x0=initials,
                 bounds=bounds,
```

### Comparing `skyllh-23.1.1/skyllh/core/model.py` & `skyllh-23.2.0/skyllh/core/model.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,129 +1,153 @@
 # -*- coding: utf-8 -*-
-# Author: Martin Wolf <mail@martin-wolf.org>
+# Author: Dr. Martin Wolf <mail@martin-wolf.org>
 
 """This module defines the base class for any model class used in SkyLLH.
 """
 
 from skyllh.core.py import (
     NamedObjectCollection,
-    issequence,
-    str_cast
+    issequenceof,
+    str_cast,
+    typename,
 )
 
-class Model(object):
+
+class Model(
+        object):
     """This class provides a base class for all model classes used in SkyLLH.
     Models could be for instance source models or background models.
     """
-    def __init__(self, name=None):
+    def __init__(
+            self,
+            name=None,
+            **kwargs):
         """Creates a new Model instance.
 
         Parameters
         ----------
         name : str | None
             The name of the model. If set to `None`, the id of the object is
             taken as name.
         """
-        super(Model, self).__init__()
+        super().__init__(
+            **kwargs)
 
-        if(name is None):
+        if name is None:
             name = self.id
 
         self.name = name
 
     @property
     def name(self):
         """The name of the model.
         """
         return self._name
+
     @name.setter
     def name(self, name):
-        name = str_cast(name, 'The name property must be castable to type str!')
+        name = str_cast(
+            name,
+            'The name property must be castable to type str!')
         self._name = name
 
     @property
     def id(self):
         """(read-only) The ID of the model. It's an integer generated with
         Python's `id` function. Hence, it's related to the memory address
         of the object.
         """
         return id(self)
 
 
-class ModelCollection(NamedObjectCollection):
+class ModelCollection(
+        NamedObjectCollection):
     """This class describes a collection of Model instances. It can be
     used to group several models into a single object.
     """
     @staticmethod
-    def cast(obj, errmsg=None):
+    def cast(
+            obj,
+            errmsg=None,
+            **kwargs):
         """Casts the given object to a ModelCollection object.
         If the cast fails, a TypeError with the given error message is raised.
 
         Parameters
         ----------
         obj : Model instance | sequence of Model instances |
                 ModelCollection | None
             The object that should be casted to ModelCollection.
             If set to None, an empty ModelCollection is created.
         errmsg : str | None
             The error message if the cast fails.
             If set to None, a generic error message will be used.
 
+        Additional keyword arguments
+        ----------------------------
+        Additional keyword arguments are passed to the constructor of the
+        ModelCollection class.
+
         Raises
         ------
         TypeError
-            If the cast fails.
+            If the cast failed.
 
         Returns
         -------
-        modelcollection : instance of ModelCollection
+        model_collection : instance of ModelCollection
             The created ModelCollection instance. If `obj` is already a
             ModelCollection instance, it will be returned.
         """
-        if(obj is None):
-            obj = ModelCollection(models=None, model_type=Model)
-            return obj
+        if obj is None:
+            return ModelCollection(
+                models=None, model_type=Model, **kwargs)
+
+        if isinstance(obj, Model):
+            return ModelCollection(
+                models=[obj], model_type=Model, **kwargs)
 
-        if(isinstance(obj, Model)):
-            obj = ModelCollection(models=[obj], model_type=Model)
+        if isinstance(obj, ModelCollection):
             return obj
 
-        if(isinstance(obj, ModelCollection)):
-            return obj
-
-        if(issequence(obj)):
-            obj = ModelCollection(models=obj, model_type=Model)
-            return obj
-
-        if(errmsg is None):
-            errmsg = 'Cast of object "%s" to ModelCollection failed!'%(str(obj))
+        if issequenceof(obj, Model):
+            return ModelCollection(
+                models=obj, model_type=Model, **kwargs)
+
+        if errmsg is None:
+            errmsg = (f'Cast of object "{str(obj)}" of type '
+                      f'"{typename(type(obj))}" to ModelCollection failed!')
         raise TypeError(errmsg)
 
-    def __init__(self, models=None, model_type=None):
+    def __init__(
+            self,
+            models=None,
+            model_type=None,
+            **kwargs):
         """Creates a new Model collection. The type of the model instances this
         collection holds can be restricted, by setting the model_type argument.
 
         Parameters
         ----------
         models : sequence of model_type instances | None
             The sequence of models this collection should be initalized with.
         model_type : type | None
             The type of the model. It must be a subclass of class ``Model``.
             If set to None (default), Model will be used.
         """
-        if(model_type is None):
+        if model_type is None:
             model_type = Model
+        if not issubclass(model_type, Model):
+            raise TypeError(
+                'The model_type argument must be a subclass of Model!')
 
-        if(not issubclass(model_type, Model)):
-            raise TypeError('The model_type argument must be a subclass of '
-                'class Model!')
-
-        super(ModelCollection, self).__init__(
+        super().__init__(
             objs=models,
-            obj_type=model_type)
+            obj_type=model_type,
+            **kwargs)
 
     @property
     def model_type(self):
         """(read-only) The type of the model.
         """
         return self.obj_type
 
@@ -132,18 +156,20 @@
         """(read-only) The list of models of type `model_type`.
         """
         return self.objects
 
 
 class DetectorModel(Model):
     """This class provides a base class for a detector model. It can be used
-    in combination with the ModelParameterMapper class.
+    in combination with the ParameterModelMapper class.
     """
-    def __init__(self, name):
+    def __init__(self, name, **kwargs):
         """Creates a new DetectorModel instance.
 
         Parameters
         ----------
         name : str
             The name of the detector model.
         """
-        super().__init__(name=name)
+        super().__init__(
+            name=name,
+            **kwargs)
```

### Comparing `skyllh-23.1.1/skyllh/core/multiproc.py` & `skyllh-23.2.0/skyllh/core/multiproc.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,64 +1,84 @@
 # -*- coding: utf-8 -*-
 
 import logging
-import numpy as np
-import multiprocessing as mp
-
-try:
-    # For Python 3.
-    from logging.handlers import QueueHandler
-except ImportError:
-    # For Python 2.
-    from skyllh.core.debugging import QueueHandler
-
-try:
-    # For Python 3.
-    import queue
-except ImportError:
-    # For Python 2.
-    import Queue as queue
-
+import queue
 import time
 
-from skyllh.core.config import CFG
-from skyllh.core.progressbar import ProgressBar
-from skyllh.core.random import RandomStateService
-from skyllh.core.timing import TimeLord
-
+import multiprocessing as mp
+import numpy as np
 
-def get_ncpu(local_ncpu):
+from logging.handlers import (
+    QueueHandler,
+)
+
+from skyllh.core.config import (
+    HasConfig,
+)
+from skyllh.core.progressbar import (
+    ProgressBar,
+)
+from skyllh.core.py import (
+    classname,
+)
+from skyllh.core.random import (
+    RandomStateService,
+)
+from skyllh.core.timing import (
+    TimeLord,
+)
+
+
+def get_ncpu(
+        cfg,
+        local_ncpu,
+):
     """Determines the number of CPUs to use for functions that support
     multi-processing.
 
     Parameters
     ----------
+    cfg : instance of Config
+        The instance of Config holding the local configuration.
     local_ncpu : int | None
         The local setting of the number of CPUs to use.
 
     Returns
     -------
     ncpu : int
         The number of CPUs to use by functions that allow multi-processing.
         If ``local_ncpu`` is set to None, the global NCPU setting is returned.
         If the global NCPU setting is None as well, the default value 1 is
         returned.
     """
     ncpu = local_ncpu
-    if(ncpu is None):
-        ncpu = CFG['multiproc']['ncpu']
-    if(ncpu is None):
+    if ncpu is None:
+        ncpu = cfg['multiproc']['ncpu']
+    if ncpu is None:
         ncpu = 1
-    if(not isinstance(ncpu, int)):
-        raise TypeError('The ncpu setting must be of type int!')
-    if(ncpu < 1):
-        raise ValueError('The ncpu setting must be >= 1!')
+
+    if not isinstance(ncpu, int):
+        raise TypeError(
+            'The ncpu setting must be of type int!')
+
+    if ncpu < 1:
+        raise ValueError(
+            'The ncpu setting must be >= 1!')
+
     return ncpu
 
-def parallelize(func, args_list, ncpu, rss=None, tl=None, ppbar=None):
+
+def parallelize(  # noqa: C901
+        func,
+        args_list,
+        ncpu,
+        rss=None,
+        tl=None,
+        ppbar=None,
+):
     """Parallelizes the execution of the given function for different arguments.
 
     Parameters
     ----------
     func : callable
         The function which should be called with different arguments, which are
         given through the args_list argument. If the `rss` argument is not None,
@@ -82,16 +102,24 @@
     -------
     result_list : list
         The list of the result values of ``func``, where each element of that
         list corresponds to the arguments element in ``args_list``.
     """
     # Define a wrapper function for the multiprocessing module that evaluates
     # ``func`` for a subset of `args_list` on a worker process.
-    def worker_wrapper(func, sub_args_list, pid, rqueue, lqueue,
-                       squeue=None, rss=None, tl=None):
+    def worker_wrapper(
+            func,
+            sub_args_list,
+            pid,
+            rqueue,
+            lqueue,
+            squeue=None,
+            rss=None,
+            tl=None,
+    ):
         """Wrapper function for the multiprocessing module that evaluates
         ``func`` for the subset ``sub_args_list`` of ``args_list`` on a worker
         process.
 
         Parameters
         ----------
         func : callable
@@ -125,33 +153,40 @@
         """
         # Get the `QueueHandler` and update its log records queue.
         logger = logging.getLogger('skyllh')
         queue_handler = list(logger.handlers)[0]
         queue_handler.queue = lqueue
 
         result_list = []
-        for (task_idx, (args,kwargs)) in enumerate(sub_args_list):
-            if(rss is not None):
+        for (task_idx, (args, kwargs)) in enumerate(sub_args_list):
+            if rss is not None:
                 kwargs['rss'] = rss
-            if(tl is not None):
+            if tl is not None:
                 kwargs['tl'] = tl
             result_list.append(func(*args, **kwargs))
 
-            if(squeue is not None):
+            if squeue is not None:
                 squeue.put((pid, task_idx))
 
         rqueue.put((pid, result_list, tl))
 
         # Put None object as the last log records queue item.
         lqueue.put_nowait(None)
 
     # Define a wrapper function that evaluates ``func`` for a subset of
     # `args_list` on the master process.
     def master_wrapper(
-            pbar, sarr, func, sub_args_list, squeue=None, rss=None, tl=None):
+            pbar,
+            sarr,
+            func,
+            sub_args_list,
+            squeue=None,
+            rss=None,
+            tl=None,
+    ):
         """This is the wrapper function for the master process.
 
         Parameters
         ----------
         pbar : instance of ProgressBar | None
             The instance of ProgressBar that should be used to display the
             progress if the current session is interactive.
@@ -180,88 +215,93 @@
             The RandomStateService instance to use for generating random
             numbers.
         tl : instance of TimeLord | None
             The instance of TimeLord that should be used to time individual
             tasks.
         """
         result_list = []
-        for (master_task_idx, (args,kwargs)) in enumerate(sub_args_list):
-            if(rss is not None):
+        for (master_task_idx, (args, kwargs)) in enumerate(sub_args_list):
+            if rss is not None:
                 kwargs['rss'] = rss
-            if(tl is not None):
+            if tl is not None:
                 kwargs['tl'] = tl
             result_list.append(func(*args, **kwargs))
 
             # Skip the rest, if we are not in an interactive session, hence
             # there is not progress bar.
-            if(not pbar.gets_shown):
+            if not pbar.is_shown:
                 continue
 
             sarr[0]['n_finished_tasks'] = master_task_idx + 1
 
             # Get possible status information from the worker processes.
-            if(squeue is not None):
+            if squeue is not None:
                 while not squeue.empty():
                     (pid, worker_task_idx) = squeue.get()
                     sarr[pid]['n_finished_tasks'] = worker_task_idx + 1
 
             # Calculate the total number of finished tasks.
             n_finished_tasks = np.sum(sarr['n_finished_tasks'])
             pbar.update(n_finished_tasks)
 
         return result_list
 
     # Create the progress bar if we are in an interactive session.
     pbar = ProgressBar(maxval=len(args_list), parent=ppbar).start()
 
     # Return result list if only one CPU is used.
-    if(ncpu == 1):
+    if ncpu == 1:
         sarr = np.zeros((1,), dtype=[('n_finished_tasks', np.int64)])
         result_list = master_wrapper(
             pbar, sarr, func, args_list, squeue=None, rss=rss, tl=tl)
 
         pbar.finish()
 
         return result_list
 
     # Multiple CPUs are used. Split the work across multiple processes.
     # We will use our own process (pid = 0) as a worker too.
     rqueue = mp.Queue()
     squeue = None
-    if(pbar.gets_shown):
+    if pbar.is_shown:
         squeue = mp.Queue()
-    sub_args_list_list = np.array_split(args_list, ncpu)
+
+    sub_args_list_list = np.array_split(np.array(args_list, dtype=object), ncpu)
 
     # Create a multiprocessing queue for each worker process.
     # Prepend it with None to be able to use `pid` as the list index.
     lqueue_list = [None] + [mp.Queue() for i in range(ncpu-1)]
 
     # Create a list of RandomStateService for each process if rss argument is
     # set.
     rss_list = [rss]
-    if(rss is None):
+    if rss is None:
         rss_list += [None]*(ncpu-1)
     else:
-        if(not isinstance(rss, RandomStateService)):
-            raise TypeError('The rss argument must be an instance of '
-                'RandomStateService!')
-        rss_list.extend([RandomStateService(seed=rss.random.randint(0, 2**32))
-            for i in range(1, ncpu)])
+        if not isinstance(rss, RandomStateService):
+            raise TypeError(
+                'The rss argument must be an instance of RandomStateService!')
+        rss_list.extend([
+            RandomStateService(seed=rss.random.randint(0, 2**32))
+            for i in range(1, ncpu)
+        ])
 
     # Create a list of TimeLord instances, one for each process if tl argument
     # is set.
     tl_list = [tl]
-    if(tl is None):
+    if tl is None:
         tl_list += [None]*(ncpu-1)
     else:
-        if(not isinstance(tl, TimeLord)):
-            raise TypeError('The tl argument must be an instance of '
-                'TimeLord!')
-        tl_list.extend([TimeLord()
-            for i in range(1, ncpu)])
+        if not isinstance(tl, TimeLord):
+            raise TypeError(
+                'The tl argument must be an instance of TimeLord!')
+        tl_list.extend([
+            TimeLord()
+            for i in range(1, ncpu)
+        ])
 
     # Replace all existing main process handlers with the `QueueHandler`.
     # This allows storing all the log record generated by worker processes at
     # separate `multiprocessing.Queue` instances. After creating
     # worker processes revert handlers to the initial state.
     logger = logging.getLogger('skyllh')
     orig_handlers = list(logger.handlers)
@@ -305,32 +345,34 @@
         while (result_received is False) and (proc_died is False):
             try:
                 (pid, result_list, proc_tl) = rqueue.get(block=False)
                 result_received = True
             except queue.Empty:
                 # If this exception is raised, either the child process isn't
                 # finished yet, or it dies due to an exception.
-                if(proc.exitcode is None):
+                if proc.exitcode is None:
                     # Child process hasn't finish yet.
                     # We'll wait a short moment.
                     time.sleep(0.01)
-                elif(proc.exitcode != 0):
+                elif proc.exitcode != 0:
                     proc_died = True
-        if(proc_died):
-            raise RuntimeError('Child process %d did not return with 0! '
-                'Exit code was %d.'%(proc.pid, proc.exitcode))
+        if proc_died:
+            raise RuntimeError(
+                f'Child process {proc.pid} did not return with 0! '
+                f'Exit code was {proc.exitcode}.')
 
         pid_result_list_map[pid] = result_list
-        if(tl is not None):
+        if tl is not None:
             tl.join(proc_tl)
-        logger.debug('Beginning of worker process (pid=%d) log records.', pid)
+        logger.debug(
+            f'Beginning of worker process (pid={pid}) log records.')
         lqueue_end = False
         while not lqueue_end:
             record = lqueue_list[pid].get()
-            if record == None:
+            if record is None:
                 lqueue_end = True
             else:
                 lqueue_logger = logging.getLogger(record.name)
                 lqueue_logger.handle(record)
         logger.debug('Ending of worker process (pid=%d) log records.', pid)
 
     # Join all the processes.
@@ -343,31 +385,48 @@
         result_list += pid_result_list_map[pid]
 
     pbar.finish()
 
     return result_list
 
 
-class IsParallelizable(object):
+class IsParallelizable(
+        object,
+):
     """Classifier class defining the ncpu property. Classes that derive from
     this class indicate, that they can make use of multi-processing on several
     CPUs at the same time.
     """
-    def __init__(self, ncpu=None, *args, **kwargs):
-        super(IsParallelizable, self).__init__(*args, **kwargs)
+
+    def __init__(
+            self,
+            *args,
+            ncpu=None,
+            **kwargs,
+    ):
+        super().__init__(*args, **kwargs)
+
+        if not isinstance(self, HasConfig):
+            raise TypeError(
+                f'The class "{classname(self)}" is not derived from '
+                'skyllh.core.config.HasConfig!')
+
         self.ncpu = ncpu
 
     @property
     def ncpu(self):
         """The number (int) of CPUs to utilize. It calls the ``get_ncpu``
         utility function with this property as argument. Hence, if this property
         is set to None, the global NCPU setting will take precedence.
         """
-        return get_ncpu(self._ncpu)
+        return get_ncpu(self._cfg, self._ncpu)
+
     @ncpu.setter
     def ncpu(self, n):
-        if(n is not None):
-            if(not isinstance(n, int)):
-                raise TypeError('The ncpu property must be of type int!')
-            if(n < 1):
-                raise ValueError('The ncpu property must be >= 1!')
+        if n is not None:
+            if not isinstance(n, int):
+                raise TypeError(
+                    'The ncpu property must be of type int!')
+            if n < 1:
+                raise ValueError(
+                    'The ncpu property must be >= 1!')
         self._ncpu = n
```

### Comparing `skyllh-23.1.1/skyllh/core/optimize.py` & `skyllh-23.2.0/skyllh/core/event_selection.py`

 * *Files 20% similar despite different names*

```diff
@@ -4,201 +4,433 @@
 import inspect
 import numpy as np
 import scipy.sparse
 
 from skyllh.core.py import (
     classname,
     float_cast,
-    issequenceof
+    issequenceof,
+)
+from skyllh.core.source_hypo_grouping import (
+    SourceHypoGroupManager,
+)
+from skyllh.core.source_model import (
+    SourceModel,
+)
+from skyllh.core.timing import (
+    TaskTimer,
+)
+from skyllh.core.utils.coords import (
+    angular_separation,
 )
-from skyllh.core.source_hypothesis import SourceHypoGroupManager
-from skyllh.core.timing import TaskTimer
-from skyllh.physics.source import SourceModel
 
 
-class EventSelectionMethod(object, metaclass=abc.ABCMeta):
+class EventSelectionMethod(
+        object,
+        metaclass=abc.ABCMeta):
     """This is the abstract base class for all event selection method classes.
     The idea is to pre-select only events that contribute to the likelihood
     function, i.e. are more signal than background like. The different methods
     are implemented through derived classes of this base class.
     """
 
-    def __init__(self, src_hypo_group_manager):
+    def __init__(
+            self,
+            shg_mgr,
+            **kwargs):
         """Creates a new event selection method instance.
 
         Parameters
         ----------
-        src_hypo_group_manager : SourceHypoGroupManager instance
+        shg_mgr : instance of SourceHypoGroupManager | None
             The instance of SourceHypoGroupManager that defines the list of
             sources, i.e. the list of SourceModel instances.
+            It can be ``None`` if the event selection method does not depend on
+            the sources.
         """
-        super(EventSelectionMethod, self).__init__()
+        super().__init__(
+            **kwargs)
 
-        self.src_hypo_group_manager = src_hypo_group_manager
+        self._src_arr = None
 
-        # The _src_arr variable holds a numpy record array with the necessary
-        # source information needed for the event selection method.
-        self._src_arr = self.source_to_array(
-            self._src_hypo_group_manager.source_list)
+        self._shg_mgr = shg_mgr
+        if self._shg_mgr is not None:
+            if not isinstance(self._shg_mgr, SourceHypoGroupManager):
+                raise TypeError(
+                    'The shg_mgr argument must be None or an instance of '
+                    'SourceHypoGroupManager! '
+                    f'Its current type is {classname(self._shg_mgr)}.')
+
+            # The _src_arr variable holds a numpy record array with the
+            # necessary source information needed for the event selection
+            # method.
+            self._src_arr = self.sources_to_array(
+                sources=self._shg_mgr.source_list)
 
     @property
-    def src_hypo_group_manager(self):
-        """The SourceHypoGroupManager instance, which defines the list of
-        sources.
-        """
-        return self._src_hypo_group_manager
-    @src_hypo_group_manager.setter
-    def src_hypo_group_manager(self, manager):
-        if(not isinstance(manager, SourceHypoGroupManager)):
-            raise TypeError(
-                'The src_hypo_group_manager property must be an instance of '
-                'SourceHypoGroupManager!')
-        self._src_hypo_group_manager = manager
+    def shg_mgr(self):
+        """(read-only) The instance of SourceHypoGroupManager, which defines the
+        list of sources.
+        """
+        return self._shg_mgr
+
+    def __and__(self, other):
+        """Implements the AND operator (&) for creating an event selection
+        method, which is the intersection of this event selection method and
+        another one using the expression ``intersection = self & other``.
+
+        Parameters
+        ----------
+        other : instance of EventSelectionMethod
+            The instance of EventSelectionMethod that is the other event
+            selection method.
 
-    def change_source_hypo_group_manager(self, src_hypo_group_manager):
+        Returns
+        -------
+        intersection : instance of IntersectionEventSelectionMethod
+            The instance of IntersectionEventSelectionMethod that creates the
+            intersection of this event selection method and the other.
+        """
+        return IntersectionEventSelectionMethod(self, other)
+
+    def change_shg_mgr(self, shg_mgr):
         """Changes the SourceHypoGroupManager instance of the event selection
         method. This will also recreate the internal source numpy record array.
 
         Parameters
         ----------
-        src_hypo_group_manager : SourceHypoGroupManager instance
+        shg_mgr : instance of SourceHypoGroupManager | None
             The new SourceHypoGroupManager instance, that should be used for
             this event selection method.
+            It can be ``None`` if the event selection method does not depend on
+            the sources.
         """
-        self.src_hypo_group_manager = src_hypo_group_manager
-        self._src_arr = self.source_to_array(self._src_hypo_group_manager.source_list)
+        self._shg_mgr = shg_mgr
+        self._src_arr = None
 
-    @abc.abstractmethod
-    def source_to_array(self, sources):
+        if self._shg_mgr is not None:
+            if not isinstance(self._shg_mgr, SourceHypoGroupManager):
+                raise TypeError(
+                    'The shg_mgr argument must be None or an instance of '
+                    'SourceHypoGroupManager! '
+                    f'Its current type is {classname(self._shg_mgr)}.')
+
+            self._src_arr = self.sources_to_array(
+                sources=self._shg_mgr.source_list)
+
+    def sources_to_array(self, sources):
         """This method is supposed to convert a sequence of SourceModel
         instances into a structured numpy ndarray with the source information
         in a format that is best understood by the actual event selection
         method.
 
         Parameters
         ----------
         sources : sequence of SourceModel
             The sequence of source models containing the necessary information
             of the source.
 
         Returns
         -------
-        arr : numpy record ndarray
+        arr : numpy record ndarray | None
             The generated numpy record ndarray holding the necessary information
             for each source.
+            By default ``None`` is returned.
         """
-        pass
+        return None
 
     @abc.abstractmethod
-    def select_events(self, events, ret_src_ev_idxs=False, tl=None):
+    def select_events(
+            self,
+            events,
+            src_evt_idxs=None,
+            ret_original_evt_idxs=False,
+            tl=None):
         """This method selects the events, which will contribute to the
         log-likelihood ratio function.
 
         Parameters
         ----------
         events : instance of DataFieldRecordArray
+            The instance of DataFieldRecordArray of length N_events, holding the
+            events.
+        src_evt_idxs : 2-tuple of 1d ndarrays of ints | None
+            The 2-element tuple holding the two 1d ndarrays of int of length
+            N_values, specifying to which sources the given events belong to.
+        ret_original_evt_idxs : bool
+            Flag if the original indices of the selected events should get
+            returned as well.
+        tl : instance of TimeLord | None
+            The optional instance of TimeLord that should be used to collect
+            timing information about this method.
+
+        Returns
+        -------
+        selected_events : instance of DataFieldRecordArray
+            The instance of DataFieldRecordArray of length N_selected_events,
+            holding the selected events, i.e. a subset of the ``events``
+            argument.
+        (src_idxs, evt_idxs) : 1d ndarrays of ints
+            The two 1d ndarrays of int of length N_values, holding the indices
+            of the sources and the selected events.
+        original_evt_idxs : 1d ndarray of ints
+            The (N_selected_events,)-shaped numpy ndarray holding the original
+            indices of the selected events, if ``ret_original_evt_idxs`` is set
+            to ``True``.
+        """
+        pass
+
+
+class IntersectionEventSelectionMethod(
+        EventSelectionMethod):
+    """This class provides an event selection method for the intersection of two
+    event selection methods. It can be created using the ``&`` operator:
+    ``evt_sel_method1 & evt_sel_method2``.
+    """
+    def __init__(
+            self,
+            evt_sel_method1,
+            evt_sel_method2,
+            **kwargs):
+        """Creates a compounded event selection method of two given event
+        selection methods.
+
+        Parameters
+        ----------
+        evt_sel_method1 : instance of EventSelectionMethod
+            The instance of EventSelectionMethod for the first event selection
+            method.
+        evt_sel_method2 : instance of EventSelectionMethod
+            The instance of EventSelectionMethod for the second event selection
+            method.
+        """
+        super().__init__(
+            shg_mgr=None,
+            **kwargs)
+
+        self.evt_sel_method1 = evt_sel_method1
+        self.evt_sel_method2 = evt_sel_method2
+
+    @property
+    def evt_sel_method1(self):
+        """The instance of EventSelectionMethod for the first event selection
+        method.
+        """
+        return self._evt_sel_method1
+
+    @evt_sel_method1.setter
+    def evt_sel_method1(self, method):
+        if not isinstance(method, EventSelectionMethod):
+            raise TypeError(
+                'The evt_sel_method1 property must be an instance of '
+                'EventSelectionMethod!'
+                f'Its current type is {classname(method)}.')
+        self._evt_sel_method1 = method
+
+    @property
+    def evt_sel_method2(self):
+        """The instance of EventSelectionMethod for the second event selection
+        method.
+        """
+        return self._evt_sel_method2
+
+    @evt_sel_method2.setter
+    def evt_sel_method2(self, method):
+        if not isinstance(method, EventSelectionMethod):
+            raise TypeError(
+                'The evt_sel_method2 property must be an instance of '
+                'EventSelectionMethod!'
+                f'Its current type is {classname(method)}.')
+        self._evt_sel_method2 = method
+
+    def change_shg_mgr(self, shg_mgr):
+        """Changes the SourceHypoGroupManager instance of the event selection
+        method. This will call the ``change_shg_mgr`` of the individual event
+        selection methods.
+
+        Parameters
+        ----------
+        shg_mgr : instance of SourceHypoGroupManager | None
+            The new SourceHypoGroupManager instance, that should be used for
+            this event selection method.
+            It can be ``None`` if the event selection method does not depend on
+            the sources.
+        """
+        self._evt_sel_method1.change_shg_mgr(shg_mgr=shg_mgr)
+        self._evt_sel_method2.change_shg_mgr(shg_mgr=shg_mgr)
+
+    def select_events(
+            self,
+            events,
+            src_evt_idxs=None,
+            ret_original_evt_idxs=False,
+            tl=None):
+        """Selects events by calling the ``select_events`` methods of the
+        individual event selection methods.
+
+        Parameters
+        ----------
+        events : instance of DataFieldRecordArray
             The instance of DataFieldRecordArray holding the events.
-        ret_src_ev_idxs : bool
-            Flag if also the indices of the selected events should get
-            returned as a (src_idxs, ev_idxs) tuple of 1d ndarrays.
-            Default is False.
+        src_evt_idxs : 2-tuple of 1d ndarrays of ints | None
+            The 2-element tuple holding the two 1d ndarrays of int of length
+            N_values, specifying to which sources the given events belong to.
+        ret_original_evt_idxs : bool
+            Flag if the original indices of the selected events should get
+            returned as well.
         tl : instance of TimeLord | None
             The optional instance of TimeLord that should be used to collect
             timing information about this method.
 
         Returns
         -------
         selected_events : DataFieldRecordArray
             The instance of DataFieldRecordArray holding the selected events,
             i.e. a subset of the `events` argument.
-        (src_idxs, ev_idxs) : 1d ndarrays of ints | None
-            The indices of sources and selected events, in case
-            `ret_src_ev_idxs` is set to True. Returns None, in case
-            `ret_src_ev_idxs` is set to False.
-        """
-        pass
+        (src_idxs, evt_idxs) : 1d ndarrays of ints
+            The indices of the sources and the selected events.
+        original_evt_idxs : 1d ndarray of ints
+            The (N_selected_events,)-shaped numpy ndarray holding the original
+            indices of the selected events, if ``ret_original_evt_idxs`` is set
+            to ``True``.
+        """
+        if ret_original_evt_idxs:
+            (events, src_evt_idxs, org_evt_idxs1) =\
+                self._evt_sel_method1.select_events(
+                    events=events,
+                    src_evt_idxs=src_evt_idxs,
+                    ret_original_evt_idxs=True)
+
+            (events, src_evt_idxs, org_evt_idxs2) =\
+                self._evt_sel_method2.select_events(
+                    events=events,
+                    src_evt_idxs=src_evt_idxs,
+                    ret_original_evt_idxs=True)
+
+            org_evt_idxs = np.take(org_evt_idxs1, org_evt_idxs2)
+
+            return (events, src_evt_idxs, org_evt_idxs)
+
+        (events, src_evt_idxs) = self._evt_sel_method1.select_events(
+            events=events,
+            src_evt_idxs=src_evt_idxs)
+
+        (events, src_evt_idxs) = self._evt_sel_method2.select_events(
+            events=events,
+            src_evt_idxs=src_evt_idxs)
 
+        return (events, src_evt_idxs)
 
-class AllEventSelectionMethod(EventSelectionMethod):
+
+class AllEventSelectionMethod(
+        EventSelectionMethod):
     """This event selection method selects all events.
     """
-    def __init__(self, src_hypo_group_manager):
+    def __init__(self, shg_mgr):
         """Creates a new event selection method instance.
 
         Parameters
         ----------
-        src_hypo_group_manager : SourceHypoGroupManager instance
+        shg_mgr : instance of SourceHypoGroupManager
             The instance of SourceHypoGroupManager that defines the list of
             sources, i.e. the list of SourceModel instances. For this particular
             event selection method it has no meaning, but it is an interface
             parameter.
         """
-        super(AllEventSelectionMethod, self).__init__(
-            src_hypo_group_manager)
+        super().__init__(
+            shg_mgr=shg_mgr)
+
+    def sources_to_array(self, sources):
+        """Creates the source array from the given list of sources. This event
+        selection method does not depend on the sources. Hence, ``None`` is
+        returned.
 
-    def source_to_array(self, sources):
+        Returns
+        -------
+        arr : None
+            The generated numpy record ndarray holding the necessary information
+            for each source. Since this event selection method does not depend
+            on any source, ``None`` is returned.
+        """
         return None
 
-    def select_events(self, events, ret_src_ev_idxs=False, tl=None):
+    def select_events(
+            self,
+            events,
+            src_evt_idxs=None,
+            ret_original_evt_idxs=False,
+            tl=None):
         """Selects all of the given events. Hence, the returned event array is
         the same as the given array.
 
         Parameters
         ----------
         events : instance of DataFieldRecordArray
             The instance of DataFieldRecordArray holding the events, for which
             the selection method should get applied.
-        ret_src_ev_idxs : bool
-            Flag if also the indices of the selected events should get
-            returned as a (src_idxs, ev_idxs) tuple of 1d ndarrays.
-            Default is False.
+        src_evt_idxs : 2-tuple of 1d ndarrays of ints | None
+            The 2-element tuple holding the two 1d ndarrays of int of length
+            N_values, specifying to which sources the given events belong to.
+        ret_original_evt_idxs : bool
+            Flag if the original indices of the selected events should get
+            returned as well.
         tl : instance of TimeLord | None
             The optional instance of TimeLord that should be used to collect
             timing information about this method.
 
         Returns
         -------
         selected_events : DataFieldRecordArray
             The instance of DataFieldRecordArray holding the selected events,
             i.e. a subset of the `events` argument.
-        (src_idxs, ev_idxs) : 1d ndarrays of ints | None
-            The indices of sources and selected events, in case
-            `ret_src_ev_idxs` is set to True. Returns None, in case
-            `ret_src_ev_idxs` is set to False.
-        """
-        if(ret_src_ev_idxs):
-            # Calculate events indices.
-            with TaskTimer(tl, 'ESM: Calculate indices of selected events.'):
-                n_sources = self.src_hypo_group_manager.n_sources
-                src_idxs = np.repeat(np.arange(n_sources), len(events.indices))
-                ev_idxs = np.tile(events.indices, n_sources)
-
-            return (events, (src_idxs, ev_idxs))
-
-        return (events, None)
-
-
-class SpatialEventSelectionMethod(EventSelectionMethod, metaclass=abc.ABCMeta):
-    """This class defines the base class for all spatial event selection
-    methods.
+        (src_idxs, evt_idxs) : 1d ndarrays of ints
+            The indices of sources and the selected events.
+        original_evt_idxs : 1d ndarray of ints
+            The (N_selected_events,)-shaped numpy ndarray holding the original
+            indices of the selected events, if ``ret_original_evt_idxs`` is set
+            to ``True``.
+        """
+        with TaskTimer(tl, 'ESM: Calculate indices of selected events.'):
+            if src_evt_idxs is None:
+                n_sources = self.shg_mgr.n_sources
+                src_idxs = np.repeat(np.arange(n_sources), len(events))
+                evt_idxs = np.tile(events.indices, n_sources)
+            else:
+                (src_idxs, evt_idxs) = src_evt_idxs
+
+        if ret_original_evt_idxs:
+            return (events, (src_idxs, evt_idxs), events.indices)
+
+        return (events, (src_idxs, evt_idxs))
+
+
+class SpatialEventSelectionMethod(
+        EventSelectionMethod,
+        metaclass=abc.ABCMeta):
+    """This abstract base class defines the base class for all spatial event
+    selection methods.
     """
 
-    def __init__(self, src_hypo_group_manager):
+    def __init__(
+            self,
+            shg_mgr,
+            **kwargs):
         """Creates a new event selection method instance.
 
         Parameters
         ----------
-        src_hypo_group_manager : SourceHypoGroupManager instance
+        shg_mgr : instance of SourceHypoGroupManager
             The instance of SourceHypoGroupManager that defines the list of
             sources, i.e. the list of SourceModel instances.
         """
-        super(SpatialEventSelectionMethod, self).__init__(
-            src_hypo_group_manager)
+        super().__init__(
+            shg_mgr=shg_mgr,
+            **kwargs)
 
-    def source_to_array(self, sources):
+    def sources_to_array(self, sources):
         """Converts the given sequence of SourceModel instances into a
         structured numpy ndarray holding the necessary source information needed
         for this event selection method.
 
         Parameters
         ----------
         sources : sequence of SourceModel
@@ -207,217 +439,239 @@
 
         Returns
         -------
         arr : numpy record ndarray
             The generated numpy record ndarray holding the necessary information
             for each source. It contains the following data fields: 'ra', 'dec'.
         """
-        if(not issequenceof(sources, SourceModel)):
-            raise TypeError('The sources argument must be a sequence of '
-                'SourceModel instances!')
+        if not issequenceof(sources, SourceModel):
+            raise TypeError(
+                'The sources argument must be a sequence of SourceModel '
+                'instances! '
+                f'Its current type is {classname(sources)}.')
 
         arr = np.empty(
             (len(sources),),
-            dtype=[('ra', np.float64), ('dec', np.float64)],
+            dtype=[
+                ('ra', np.float64),
+                ('dec', np.float64)
+            ],
             order='F')
 
         for (i, src) in enumerate(sources):
-            arr['ra'][i] = src.loc.ra
-            arr['dec'][i] = src.loc.dec
+            arr['ra'][i] = src.ra
+            arr['dec'][i] = src.dec
 
         return arr
 
 
-class DecBandEventSectionMethod(SpatialEventSelectionMethod):
+class DecBandEventSectionMethod(
+        SpatialEventSelectionMethod):
     """This event selection method selects events within a declination band
     around a list of point-like source positions.
     """
-    def __init__(self, src_hypo_group_manager, delta_angle):
+    def __init__(
+            self,
+            shg_mgr,
+            delta_angle):
         """Creates and configures a spatial declination band event selection
         method object.
 
         Parameters
         ----------
-        src_hypo_group_manager : SourceHypoGroupManager instance
+        shg_mgr : instance of SourceHypoGroupManager
             The instance of SourceHypoGroupManager that defines the list of
             sources, i.e. the list of SourceModel instances.
         delta_angle : float
             The half-opening angle around the source in declination for which
             events should get selected.
         """
-        super(DecBandEventSectionMethod, self).__init__(
-            src_hypo_group_manager)
+        super().__init__(
+            shg_mgr=shg_mgr)
 
         self.delta_angle = delta_angle
 
     @property
     def delta_angle(self):
         """The half-opening angle around the source in declination and
         right-ascention for which events should get selected.
         """
         return self._delta_angle
+
     @delta_angle.setter
     def delta_angle(self, angle):
-        angle = float_cast(angle, 'The delta_angle property must be castable '
-            'to type float!')
+        angle = float_cast(
+            angle,
+            'The delta_angle property must be castable to type float!')
         self._delta_angle = angle
 
     def select_events(
-            self, events, ret_src_ev_idxs=False,
-            ret_mask_idxs=False, tl=None):
+            self,
+            events,
+            src_evt_idxs=None,
+            ret_original_evt_idxs=False,
+            tl=None):
         """Selects the events within the declination band.
 
         Parameters
         ----------
         events : instance of DataFieldRecordArray
             The instance of DataFieldRecordArray that holds the event data.
             The following data fields must exist:
 
-            - 'dec' : float
-                The declination of the event.
-        ret_src_ev_idxs : bool
-            Flag if also the indices of the selected events should get
-            returned as a (src_idxs, ev_idxs) tuple of 1d ndarrays.
-            Default is False.
-        ret_mask_idxs : bool
-            Flag if also the indices of the selected events mask should get
-            returned as a mask_idxs 1d ndarray.
-            Default is False.
+                ``'dec'`` : float
+                    The declination of the event.
+
+        src_evt_idxs : 2-tuple of 1d ndarrays of ints | None
+            The 2-element tuple holding the two 1d ndarrays of int of length
+            N_values, specifying to which sources the given events belong to.
+        ret_original_evt_idxs : bool
+            Flag if the original indices of the selected events should get
+            returned as well.
         tl : instance of TimeLord | None
             The optional instance of TimeLord that should be used to collect
             timing information about this method.
 
         Returns
         -------
         selected_events : instance of DataFieldRecordArray
             The instance of DataFieldRecordArray holding only the selected
             events.
-        idxs: where idxs is one of the following:
-            - (src_idxs, ev_idxs) : 1d ndarrays of ints
-                The indices of sources and selected events, in case
-                `ret_src_ev_idxs` is set to True.
-            - mask_idxs : 1d ndarrays of ints
-                The indices of selected events mask, in case
-                `ret_mask_idxs` is set to True.
-            - None
-                In case both `ret_src_ev_idxs` and `ret_mask_idxs` are set to
-                False.
+        (src_idxs, evt_idxs) : 1d ndarrays of ints
+            The indices of sources and the selected events.
+        original_evt_idxs : 1d ndarray of ints
+            The (N_selected_events,)-shaped numpy ndarray holding the original
+            indices of the selected events, if ``ret_original_evt_idxs`` is set
+            to ``True``.
         """
         delta_angle = self._delta_angle
         src_arr = self._src_arr
 
         # Calculates the minus and plus declination around each source and
         # bound it to -90deg and +90deg, respectively.
         src_dec_minus = np.maximum(-np.pi/2, src_arr['dec'] - delta_angle)
         src_dec_plus = np.minimum(src_arr['dec'] + delta_angle, np.pi/2)
 
         # Determine the mask for the events which fall inside the declination
         # window.
         # mask_dec is a (N_sources,N_events)-shaped ndarray.
         with TaskTimer(tl, 'ESM-DecBand: Calculate mask_dec.'):
-            mask_dec = ((events['dec'] > src_dec_minus[:,np.newaxis]) &
-                        (events['dec'] < src_dec_plus[:,np.newaxis]))
+            mask_dec = (
+                (events['dec'] > src_dec_minus[:, np.newaxis]) &
+                (events['dec'] < src_dec_plus[:, np.newaxis])
+            )
 
         # Determine the mask for the events that fall inside at least one
         # source declination band.
         # mask is a (N_events,)-shaped ndarray.
         with TaskTimer(tl, 'ESM-DecBand: Calculate mask.'):
             mask = np.any(mask_dec, axis=0)
 
         # Reduce the events according to the mask.
         with TaskTimer(tl, 'ESM-DecBand: Create selected_events.'):
             # Using an integer indices array for data selection is several
             # factors faster than using a boolean array.
-            mask_idxs = events.indices[mask]
-            selected_events = events[mask_idxs]
+            selected_events_idxs = events.indices[mask]
+            selected_events = events[selected_events_idxs]
 
-        if(ret_src_ev_idxs and ret_mask_idxs):
-            raise ValueError(
-                'Only one of `ret_src_ev_idxs` and `ret_mask_idxs` can be set '
-                'to True.')
-        elif(ret_src_ev_idxs):
-            # Get selected events indices.
-            idxs = np.argwhere(mask_dec[:, mask])
-            src_idxs = idxs[:, 0]
-            ev_idxs = idxs[:, 1]
-            return (selected_events, (src_idxs, ev_idxs))
-        elif(ret_mask_idxs):
-            return (selected_events, mask_idxs)
+        # Get selected events indices.
+        idxs = np.argwhere(mask_dec[:, mask])
+        src_idxs = idxs[:, 0]
+        evt_idxs = idxs[:, 1]
 
-        return (selected_events, None)
+        if ret_original_evt_idxs:
+            return (selected_events, (src_idxs, evt_idxs), selected_events_idxs)
 
+        return (selected_events, (src_idxs, evt_idxs))
 
-class RABandEventSectionMethod(SpatialEventSelectionMethod):
+
+class RABandEventSectionMethod(
+        SpatialEventSelectionMethod):
     """This event selection method selects events within a right-ascension band
     around a list of point-like source positions.
     """
-    def __init__(self, src_hypo_group_manager, delta_angle):
+    def __init__(
+            self,
+            shg_mgr,
+            delta_angle):
         """Creates and configures a right-ascension band event selection
         method object.
 
         Parameters
         ----------
-        src_hypo_group_manager : SourceHypoGroupManager instance
+        shg_mgr : instance of SourceHypoGroupManager
             The instance of SourceHypoGroupManager that defines the list of
             sources, i.e. the list of SourceModel instances.
         delta_angle : float
             The half-opening angle around the source in right-ascension for
             which events should get selected.
         """
-        super(RABandEventSectionMethod, self).__init__(
-            src_hypo_group_manager)
+        super().__init__(
+            shg_mgr=shg_mgr)
 
         self.delta_angle = delta_angle
 
     @property
     def delta_angle(self):
         """The half-opening angle around the source in declination and
         right-ascention for which events should get selected.
         """
         return self._delta_angle
+
     @delta_angle.setter
     def delta_angle(self, angle):
-        angle = float_cast(angle,
+        angle = float_cast(
+            angle,
             'The delta_angle property must be castable to type float!')
         self._delta_angle = angle
 
-    def select_events(self, events, ret_src_ev_idxs=False, tl=None):
+    def select_events(
+            self,
+            events,
+            src_evt_idxs=None,
+            ret_original_evt_idxs=False,
+            tl=None):
         """Selects the events within the right-ascention band.
 
         The solid angle dOmega = dRA * dSinDec = dRA * dDec * cos(dec) is a
         function of declination, i.e. for a constant dOmega, the right-ascension
         value has to change with declination.
 
         Parameters
         ----------
         events : instance of DataFieldRecordArray
             The instance of DataFieldRecordArray that holds the event data.
             The following data fields must exist:
 
-            - 'ra' : float
+            ``'ra'`` : float
                 The right-ascention of the event.
-            - 'dec' : float
+            ``'dec'`` : float
                 The declination of the event.
-        ret_src_ev_idxs : bool
-            Flag if also the indices of the selected events should get
-            returned as a (src_idxs, ev_idxs) tuple of 1d ndarrays.
-            Default is False.
+
+        src_evt_idxs : 2-tuple of 1d ndarrays of ints | None
+            The 2-element tuple holding the two 1d ndarrays of int of length
+            N_values, specifying to which sources the given events belong to.
+        ret_original_evt_idxs : bool
+            Flag if the original indices of the selected events should get
+            returned as well.
         tl : instance of TimeLord | None
             The optional instance of TimeLord that should be used to collect
             timing information about this method.
 
         Returns
         -------
         selected_events : instance of DataFieldRecordArray
             The instance of DataFieldRecordArray holding only the selected
             events.
-        (src_idxs, ev_idxs) : 1d ndarrays of ints | None
-            The indices of sources and selected events, in case
-            `ret_src_ev_idxs` is set to True. Returns None, in case
-            `ret_src_ev_idxs` is set to False.
+        (src_idxs, evt_idxs) : 1d ndarrays of ints
+            The indices of the sources and the selected events.
+        original_evt_idxs : 1d ndarray of ints
+            The (N_selected_events,)-shaped numpy ndarray holding the original
+            indices of the selected events, if ``ret_original_evt_idxs`` is set
+            to ``True``.
         """
         delta_angle = self._delta_angle
         src_arr = self._src_arr
 
         # Get the minus and plus declination around the sources.
         src_dec_minus = np.maximum(-np.pi/2, src_arr['dec'] - delta_angle)
         src_dec_plus = np.minimum(src_arr['dec'] + delta_angle, np.pi/2)
@@ -436,164 +690,194 @@
 
         # Calculate the right-ascension distance of the events w.r.t. the
         # source. We make sure to use the smaller distance on the circle, thus
         # the maximal distance is 180deg, i.e. pi.
         # ra_dist is a (N_sources,N_events)-shaped 2D ndarray.
         with TaskTimer(tl, 'ESM-RaBand: Calculate ra_dist.'):
             ra_dist = np.fabs(
-                np.mod(events['ra'] - src_arr['ra'][:,np.newaxis] + np.pi, 2*np.pi) - np.pi)
+                np.mod(
+                    events['ra'] - src_arr['ra'][:, np.newaxis] + np.pi,
+                    2*np.pi) - np.pi)
 
         # Determine the mask for the events which fall inside the
         # right-ascention window.
         # mask_ra is a (N_sources,N_events)-shaped ndarray.
         with TaskTimer(tl, 'ESM-RaBand: Calculate mask_ra.'):
-            mask_ra = ra_dist < dRA_half[:,np.newaxis]
+            mask_ra = ra_dist < dRA_half[:, np.newaxis]
 
         # Determine the mask for the events that fall inside at least one
         # source sky window.
         # mask is a (N_events,)-shaped ndarray.
         with TaskTimer(tl, 'ESM-RaBand: Calculate mask.'):
             mask = np.any(mask_ra, axis=0)
 
         # Reduce the events according to the mask.
         with TaskTimer(tl, 'ESM-RaBand: Create selected_events.'):
             # Using an integer indices array for data selection is several
             # factors faster than using a boolean array.
-            selected_events = events[events.indices[mask]]
+            selected_events_idxs = events.indices[mask]
+            selected_events = events[selected_events_idxs]
 
-        if(ret_src_ev_idxs):
-            # Get selected events indices.
-            idxs = np.argwhere(mask_ra[:, mask])
-            src_idxs = idxs[:, 0]
-            ev_idxs = idxs[:, 1]
+        # Get selected events indices.
+        idxs = np.argwhere(mask_ra[:, mask])
+        src_idxs = idxs[:, 0]
+        evt_idxs = idxs[:, 1]
 
-            return (selected_events, (src_idxs, ev_idxs))
+        if ret_original_evt_idxs:
+            return (selected_events, (src_idxs, evt_idxs), selected_events_idxs)
 
-        return (selected_events, None)
+        return (selected_events, (src_idxs, evt_idxs))
 
 
-class SpatialBoxEventSelectionMethod(SpatialEventSelectionMethod):
+class SpatialBoxEventSelectionMethod(
+        SpatialEventSelectionMethod):
     """This event selection method selects events within a spatial box in
     right-ascention and declination around a list of point-like source
     positions.
     """
-    def __init__(self, src_hypo_group_manager, delta_angle):
+    def __init__(
+            self,
+            shg_mgr,
+            delta_angle):
         """Creates and configures a spatial box event selection method object.
 
         Parameters
         ----------
-        src_hypo_group_manager : SourceHypoGroupManager instance
+        shg_mgr : instance of SourceHypoGroupManager
             The instance of SourceHypoGroupManager that defines the list of
             sources, i.e. the list of SourceModel instances.
         delta_angle : float
             The half-opening angle around the source for which events should
             get selected.
         """
-        super(SpatialBoxEventSelectionMethod, self).__init__(
-            src_hypo_group_manager)
+        super().__init__(
+            shg_mgr=shg_mgr)
 
         self.delta_angle = delta_angle
 
     @property
     def delta_angle(self):
         """The half-opening angle around the source in declination and
         right-ascention for which events should get selected.
         """
         return self._delta_angle
+
     @delta_angle.setter
     def delta_angle(self, angle):
-        angle = float_cast(angle,
+        angle = float_cast(
+            angle,
             'The delta_angle property must be castable to type float!')
         self._delta_angle = angle
 
-    def select_events(self, events, ret_src_ev_idxs=False, tl=None):
+    def select_events(
+            self,
+            events,
+            src_evt_idxs=None,
+            ret_original_evt_idxs=False,
+            tl=None):
         """Selects the events within the spatial box in right-ascention and
         declination.
 
         The solid angle dOmega = dRA * dSinDec = dRA * dDec * cos(dec) is a
         function of declination, i.e. for a constant dOmega, the right-ascension
         value has to change with declination.
 
         Parameters
         ----------
         events : instance of DataFieldRecordArray
             The instance of DataFieldRecordArray that holds the event data.
             The following data fields must exist:
 
-            - 'ra' : float
+            ``'ra'`` : float
                 The right-ascention of the event.
-            - 'dec' : float
+            ``'dec'`` : float
                 The declination of the event.
-        ret_src_ev_idxs : bool
-            Flag if also the indices of the selected events should get
-            returned as a (src_idxs, ev_idxs) tuple of 1d ndarrays.
-            Default is False.
+
+        src_evt_idxs : 2-tuple of 1d ndarrays of ints | None
+            The 2-element tuple holding the two 1d ndarrays of int of length
+            N_values, specifying to which sources the given events belong to.
+        ret_original_evt_idxs : bool
+            Flag if the original indices of the selected events should get
+            returned as well.
         tl : instance of TimeLord | None
             The optional instance of TimeLord that should be used to collect
             timing information about this method.
 
         Returns
         -------
         selected_events : instance of DataFieldRecordArray
             The instance of DataFieldRecordArray holding only the selected
             events.
-        (src_idxs, ev_idxs) : 1d ndarrays of ints | None
-            The indices of sources and selected events, in case
-            `ret_src_ev_idxs` is set to True. Returns None, in case
-            `ret_src_ev_idxs` is set to False.
+        (src_idxs, evt_idxs) : 1d ndarrays of ints | None
+            The indices of sources and the selected events.
+        original_evt_idxs : 1d ndarray of ints
+            The (N_selected_events,)-shaped numpy ndarray holding the original
+            indices of the selected events, if ``ret_original_evt_idxs`` is set
+            to ``True``.
         """
         delta_angle = self._delta_angle
         src_arr = self._src_arr
+        n_sources = len(src_arr)
+
+        srcs_ra = src_arr['ra']
+        srcs_dec = src_arr['dec']
 
         # Get the minus and plus declination around the sources.
-        src_dec_minus = np.maximum(-np.pi/2, src_arr['dec'] - delta_angle)
-        src_dec_plus = np.minimum(src_arr['dec'] + delta_angle, np.pi/2)
+        src_dec_minus = np.maximum(-np.pi/2, srcs_dec - delta_angle)
+        src_dec_plus = np.minimum(srcs_dec + delta_angle, np.pi/2)
 
         # Calculate the cosine factor for the largest declination distance from
         # the source. We use np.amin here because smaller cosine values are
         # larger angles.
         # cosfact is a (N_sources,)-shaped ndarray.
         cosfact = np.amin(np.cos([src_dec_minus, src_dec_plus]), axis=0)
 
         # Calculate delta RA, which is a function of declination.
         # dRA is a (N_sources,)-shaped ndarray.
         dRA_half = np.amin(
-            [np.repeat(2*np.pi, len(src_arr['ra'])),
+            [np.repeat(2*np.pi, n_sources),
              np.fabs(delta_angle / cosfact)], axis=0)
 
         # Determine the mask for the events which fall inside the
         # right-ascention window.
         # mask_ra is a (N_sources,N_events)-shaped ndarray.
         with TaskTimer(tl, 'ESM: Calculate mask_ra.'):
-            nsrc = len(src_arr['ra'])
-            # Fill in batch sizes of 200 maximum to save memory.
-            batch_size=200
-            if nsrc > batch_size:
-                mask_ra = np.zeros((nsrc, len(events['ra'])), dtype=bool)
-                n_batches = int(np.ceil(nsrc / float(batch_size)))
+            evts_ra = events['ra']
+            # Fill in batch sizes of 128 maximum to save memory.
+            batch_size = 128
+            if n_sources > batch_size:
+                mask_ra = np.zeros((n_sources, len(evts_ra)), dtype=bool)
+                n_batches = int(np.ceil(n_sources / float(batch_size)))
                 for bi in range(n_batches):
-                    if not (bi == n_batches-1):
-                        mask_ra[bi*batch_size : (bi+1)*batch_size,...] = (np.fabs(
-                            np.mod(events['ra'] - src_arr['ra'][bi*batch_size : (bi+1)*batch_size][:,np.newaxis] + np.pi, 2*np.pi) -
-                            np.pi) < dRA_half[ bi*batch_size : (bi+1)*batch_size ][:,np.newaxis])
+                    if bi == n_batches-1:
+                        # We got the last batch of sources.
+                        srcs_slice = slice(bi*batch_size, None)
                     else:
-                        mask_ra[bi*batch_size : ,...] = (np.fabs(
-                            np.mod(events['ra'] - src_arr['ra'][bi*batch_size:][:,np.newaxis] + np.pi, 2*np.pi) -
-                            np.pi) < dRA_half[bi*batch_size:][:,np.newaxis])
+                        srcs_slice = slice(bi*batch_size, (bi+1)*batch_size)
 
+                    ra_diff = np.fabs(
+                        evts_ra - srcs_ra[srcs_slice][:, np.newaxis])
+                    ra_mod = np.where(
+                        ra_diff >= np.pi, 2*np.pi - ra_diff, ra_diff)
+                    mask_ra[srcs_slice, :] = (
+                        ra_mod < dRA_half[srcs_slice][:, np.newaxis]
+                    )
             else:
-                mask_ra = np.fabs(
-                    np.mod(events['ra'] - src_arr['ra'][:,np.newaxis] + np.pi, 2*np.pi) - np.pi) < dRA_half[:,np.newaxis]
+                ra_diff = np.fabs(evts_ra - srcs_ra[:, np.newaxis])
+                ra_mod = np.where(ra_diff >= np.pi, 2*np.pi-ra_diff, ra_diff)
+                mask_ra = ra_mod < dRA_half[:, np.newaxis]
 
         # Determine the mask for the events which fall inside the declination
         # window.
         # mask_dec is a (N_sources,N_events)-shaped ndarray.
         with TaskTimer(tl, 'ESM: Calculate mask_dec.'):
-            mask_dec = ((events['dec'] > src_dec_minus[:,np.newaxis]) &
-                        (events['dec'] < src_dec_plus[:,np.newaxis]))
+            mask_dec = (
+                (events['dec'] > src_dec_minus[:, np.newaxis]) &
+                (events['dec'] < src_dec_plus[:, np.newaxis])
+            )
 
         # Determine the mask for the events which fall inside the
         # right-ascension and declination window.
         # mask_sky is a (N_sources,N_events)-shaped ndarray.
         with TaskTimer(tl, 'ESM: Calculate mask_sky.'):
             mask_sky = mask_ra & mask_dec
             del mask_ra
@@ -605,330 +889,287 @@
         with TaskTimer(tl, 'ESM: Calculate mask.'):
             mask = np.any(mask_sky, axis=0)
 
         # Reduce the events according to the mask.
         with TaskTimer(tl, 'ESM: Create selected_events.'):
             # Using an integer indices array for data selection is several
             # factors faster than using a boolean array.
-            selected_events = events[events.indices[mask]]
+            selected_events_idxs = events.indices[mask]
+            selected_events = events[selected_events_idxs]
 
-        if(ret_src_ev_idxs):
-            # Get selected events indices.
-            idxs = np.argwhere(mask_sky[:, mask])
-            src_idxs = idxs[:, 0]
-            ev_idxs = idxs[:, 1]
+        # Get selected events indices.
+        idxs = np.argwhere(mask_sky[:, mask])
+        src_idxs = idxs[:, 0]
+        evt_idxs = idxs[:, 1]
 
-            return (selected_events, (src_idxs, ev_idxs))
+        if ret_original_evt_idxs:
+            return (selected_events, (src_idxs, evt_idxs), selected_events_idxs)
 
-        return (selected_events, None)
+        return (selected_events, (src_idxs, evt_idxs))
 
 
-class PsiFuncEventSelectionMethod(EventSelectionMethod):
+class PsiFuncEventSelectionMethod(
+        EventSelectionMethod):
     """This event selection method selects events whose psi value, i.e. the
     great circle distance of the event to the source, is smaller than the value
     of the provided function.
     """
-    def __init__(self, src_hypo_group_manager, psi_name, func, axis_name_list):
+    def __init__(
+            self,
+            shg_mgr,
+            psi_name,
+            func,
+            axis_name_list):
         """Creates a new PsiFuncEventSelectionMethod instance.
 
         Parameters
         ----------
-        src_hypo_group_manager : SourceHypoGroupManager instance
+        shg_mgr : instance of SourceHypoGroupManager
             The instance of SourceHypoGroupManager that defines the list of
             sources, i.e. the list of SourceModel instances.
         psi_name : str
             The name of the data field that provides the psi value of the event.
         func : callable
             The function that should get evaluated for each event. The call
-            signature must be ``func(*axis_data)``, where ``*axis_data`` is the
-            event data of each required axis. The number of axes must match the
-            provided axis names through the ``axis_name_list``.
+            signature must be
+
+                ``func(*axis_data)``,
+
+            where ``*axis_data`` is the event data of each required axis. The
+            number of axes must match the provided axis names through the
+            ``axis_name_list``.
         axis_name_list : list of str
             The list of data field names for each axis of the function ``func``.
             All field names must be valid field names of the trial data's
             DataFieldRecordArray instance.
         """
-        super(PsiFuncEventSelectionMethod, self).__init__(
-            src_hypo_group_manager)
+        super().__init__(
+            shg_mgr=shg_mgr)
 
         self.psi_name = psi_name
         self.func = func
         self.axis_name_list = axis_name_list
 
-        if(not (len(inspect.signature(self._func).parameters) >=
-                len(self._axis_name_list))):
+        n_func_args = len(inspect.signature(self._func).parameters)
+        if n_func_args < len(self._axis_name_list):
             raise TypeError(
                 'The func argument must be a callable instance with at least '
-                '%d arguments!'%(
-                    len(self._axis_name_list)))
+                f'{len(self._axis_name_list)} arguments! Its current number '
+                f'of arguments is {n_func_args}.')
 
-        n_sources = self.src_hypo_group_manager.n_sources
-        if(n_sources != 1):
+        n_sources = self.shg_mgr.n_sources
+        if n_sources != 1:
             raise ValueError(
                 'The `PsiFuncEventSelectionMethod.select_events` currently '
-                f'supports only one source. It was called with {n_sources} '
-                'sources.'
-            )
+                'supports only a single source. It was called with '
+                f'{n_sources} sources.')
 
     @property
     def psi_name(self):
         """The name of the data field that provides the psi value of the event.
         """
         return self._psi_name
+
     @psi_name.setter
     def psi_name(self, name):
-        if(not isinstance(name, str)):
+        if not isinstance(name, str):
             raise TypeError(
-                'The psi_name property must be an instance of type str!')
+                'The psi_name property must be an instance of type str! '
+                f'Its current type is {classname(name)}.')
         self._psi_name = name
 
     @property
     def func(self):
         """The function that should get evaluated for each event. The call
         signature must be ``func(*axis_data)``, where ``*axis_data`` is the
         event data of each required axis. The number of axes must match the
         provided axis names through the ``axis_name_list`` property.
         """
         return self._func
+
     @func.setter
     def func(self, f):
-        if(not callable(f)):
+        if not callable(f):
             raise TypeError(
-                'The func property must be a callable instance!')
+                'The func property must be a callable instance! '
+                f'Its current type is {classname(f)}.')
         self._func = f
 
     @property
     def axis_name_list(self):
         """The list of data field names for each axis of the function defined
         through the ``func`` property.
         """
         return self._axis_name_list
+
     @axis_name_list.setter
     def axis_name_list(self, names):
-        if(not issequenceof(names, str)):
+        if not issequenceof(names, str):
             raise TypeError(
                 'The axis_name_list property must be a sequence of str '
-                'instances!')
+                'instances! '
+                f'Its current type is {classname(names)}.')
         self._axis_name_list = list(names)
 
-    def source_to_array(self, sources):
-        """Converts the given sequence of SourceModel instances into a
-        structured numpy ndarray holding the necessary source information needed
-        for this event selection method.
-
-        Parameters
-        ----------
-        sources : sequence of SourceModel
-            The sequence of source models containing the necessary information
-            of the source.
-
-        Returns
-        -------
-        arr : None
-            Because this event selection method does not depend directly on the
-            source (only indirectly through the psi values), no source array
-            is required.
-        """
-        return None
-
-    def select_events(self, events, ret_src_ev_idxs=False, tl=None):
+    def select_events(
+            self,
+            events,
+            src_evt_idxs=None,
+            ret_original_evt_idxs=False,
+            tl=None):
         """Selects the events whose psi value is smaller than the value of the
         predefined function.
 
         Parameters
         ----------
         events : instance of DataFieldRecordArray
             The instance of DataFieldRecordArray that holds the event data.
             The following data fields must exist:
 
-            - <psi_name> : float
+            <psi_name> : float
                 The great circle distance of the event with the source.
-            - <*axis_name_list> : float
+            <axis_name(s)> : float
                 The name of the axis required for the function ``func`` to be
                 evaluated.
 
-        ret_src_ev_idxs : bool
-            Flag if also the indices of the selected events should get
-            returned as a (src_idxs, ev_idxs) tuple of 1d ndarrays.
-            Default is False.
+        src_evt_idxs : 2-tuple of 1d ndarrays of ints | None
+            The 2-element tuple holding the two 1d ndarrays of int of length
+            N_values, specifying to which sources the given events belong to.
+        ret_original_evt_idxs : bool
+            Flag if the original indices of the selected events should get
+            returned as well.
         tl : instance of TimeLord | None
             The optional instance of TimeLord that should be used to collect
             timing information about this method.
 
         Returns
         -------
         selected_events : instance of DataFieldRecordArray
             The instance of DataFieldRecordArray holding only the selected
             events.
-        (src_idxs, ev_idxs) : 1d ndarrays of ints | None
-            The indices of sources and selected events, in case
-            `ret_src_ev_idxs` is set to True. Returns None, in case
-            `ret_src_ev_idxs` is set to False.
+        (src_idxs, evt_idxs) : 1d ndarrays of ints
+            The indices of the sources and the selected events.
+        original_evt_idxs : 1d ndarray of ints
+            The (N_selected_events,)-shaped numpy ndarray holding the original
+            indices of the selected events, if ``ret_original_evt_idxs`` is set
+            to ``True``.
         """
         cls_name = classname(self)
 
-        with TaskTimer(tl, '%s: Get psi values.'%(cls_name)):
+        with TaskTimer(tl, f'{cls_name}: Get psi values.'):
             psi = events[self._psi_name]
 
-        with TaskTimer(tl, '%s: Get axis data values.'%(cls_name)):
-            func_args = [ events[axis] for axis in self._axis_name_list ]
+        with TaskTimer(tl, f'{cls_name}: Get axis data values.'):
+            func_args = [events[axis] for axis in self._axis_name_list]
 
-        with TaskTimer(tl, '%s: Creating mask.'%(cls_name)):
+        with TaskTimer(tl, f'{cls_name}: Creating mask.'):
             mask = psi < self._func(*func_args)
 
-        with TaskTimer(tl, '%s: Create selected_events.'%(cls_name)):
+        with TaskTimer(tl, f'{cls_name}: Create selected_events.'):
             # Using an integer indices array for data selection is several
             # factors faster than using a boolean array.
-            selected_events = events[events.indices[mask]]
+            selected_events_idxs = events.indices[mask]
+            selected_events = events[selected_events_idxs]
+
+        # Get selected events indices.
+        idxs = np.argwhere(np.atleast_2d(mask))
+        src_idxs = idxs[:, 0]
+        evt_idxs = idxs[:, 1]
 
-        if(ret_src_ev_idxs):
-            # Get selected events indices.
-            idxs =  np.argwhere(np.atleast_2d(mask))
-            src_idxs = idxs[:, 0]
-            ev_idxs = idxs[:, 1]
-            return (selected_events, (src_idxs, ev_idxs))
+        if ret_original_evt_idxs:
+            return (selected_events, (src_idxs, evt_idxs), selected_events_idxs)
 
-        return (selected_events, None)
+        return (selected_events, (src_idxs, evt_idxs))
 
 
-class SpatialBoxAndPsiFuncEventSelectionMethod(SpatialBoxEventSelectionMethod):
+class AngErrOfPsiEventSelectionMethod(
+        SpatialEventSelectionMethod):
     """This event selection method selects events within a spatial box in
     right-ascention and declination around a list of point-like source
     positions and performs an additional selection of events whose ang_err value
     is larger than the value of the provided function at a given psi value.
     """
-    def __init__(self, src_hypo_group_manager, delta_angle, psi_name, func,
-                 axis_name_list, psi_floor=None):
+    def __init__(
+            self,
+            shg_mgr,
+            func,
+            psi_floor=None,
+            **kwargs):
         """Creates and configures a spatial box and psi func event selection
         method object.
 
         Parameters
         ----------
-        src_hypo_group_manager : SourceHypoGroupManager instance
+        shg_mgr : instance of SourceHypoGroupManager
             The instance of SourceHypoGroupManager that defines the list of
             sources, i.e. the list of SourceModel instances.
         delta_angle : float
             The half-opening angle around the source for which events should
             get selected.
-        psi_name : str
+        psi_name : str | None
             The name of the data field that provides the psi value of the event.
+            If set to ``None``, the psi value will be calculated automatically.
         func : callable
             The function that should get evaluated for each event. The call
-            signature must be ``func(*axis_data)``, where ``*axis_data`` is the
-            event data of each required axis. The number of axes must match the
-            provided axis names through the ``axis_name_list``.
-        axis_name_list : list of str
-            The list of data field names for each axis of the function ``func``.
-            All field names must be valid field names of the trial data's
-            DataFieldRecordArray instance.
+            signature must be
+
+                ``func(psi)``,
+
+            where ``psi`` is the opening angle between the source and the event.
         psi_floor : float | None
             The psi func event selection is excluded for events having psi value
-            below the `psi_floor`. If None, set it to default 5 degrees.
+            below the ``psi_floor``. If None, set it to default 5 degrees.
         """
-        super(SpatialBoxAndPsiFuncEventSelectionMethod, self).__init__(
-            src_hypo_group_manager, delta_angle)
+        super().__init__(
+            shg_mgr=shg_mgr,
+            **kwargs)
 
-        self.psi_name = psi_name
         self.func = func
-        self.axis_name_list = axis_name_list
 
-        if(psi_floor is None):
+        if psi_floor is None:
             psi_floor = np.deg2rad(5)
         self.psi_floor = psi_floor
 
-        if(not (len(inspect.signature(self._func).parameters) >=
-                len(self._axis_name_list))):
-            raise TypeError(
-                'The func argument must be a callable instance with at least '
-                '%d arguments!'%(
-                    len(self._axis_name_list)))
-
-    @property
-    def psi_name(self):
-        """The name of the data field that provides the psi value of the event.
-        """
-        return self._psi_name
-    @psi_name.setter
-    def psi_name(self, name):
-        if(not isinstance(name, str)):
-            raise TypeError(
-                'The psi_name property must be an instance of type str!')
-        self._psi_name = name
-
     @property
     def func(self):
         """The function that should get evaluated for each event. The call
         signature must be ``func(*axis_data)``, where ``*axis_data`` is the
         event data of each required axis. The number of axes must match the
         provided axis names through the ``axis_name_list`` property.
         """
         return self._func
+
     @func.setter
     def func(self, f):
-        if(not callable(f)):
+        if not callable(f):
             raise TypeError(
-                'The func property must be a callable instance!')
+                'The func property must be a callable instance! '
+                f'Its current type is {classname(f)}.')
         self._func = f
 
     @property
-    def axis_name_list(self):
-        """The list of data field names for each axis of the function defined
-        through the ``func`` property.
-        """
-        return self._axis_name_list
-    @axis_name_list.setter
-    def axis_name_list(self, names):
-        if(not issequenceof(names, str)):
-            raise TypeError(
-                'The axis_name_list property must be a sequence of str '
-                'instances!')
-        self._axis_name_list = list(names)
-
-    @property
     def psi_floor(self):
         """The psi func event selection is excluded for events having psi value
         below the `psi_floor`.
         """
         return self._psi_floor
+
     @psi_floor.setter
     def psi_floor(self, psi):
-        psi = float_cast(psi, 'The psi_floor property must be castable '
-            'to type float!')
+        psi = float_cast(
+            psi,
+            'The psi_floor property must be castable to type float!')
         self._psi_floor = psi
 
-    def _get_psi(self, events, idxs):
-        """Function to calculate the the opening angle between the source
-        position and the event's reconstructed position.
-        """
-        ra = events['ra']
-        dec = events['dec']
-
-        src_idxs, ev_idxs = idxs
-        src_ra = self._src_arr['ra'][src_idxs]
-        src_dec = self._src_arr['dec'][src_idxs]
-
-        delta_dec = np.abs(np.take(dec, ev_idxs) - src_dec)
-        delta_ra = np.abs(np.take(ra, ev_idxs) - src_ra)
-        x = (np.sin(delta_dec / 2.))**2. + np.cos(np.take(dec, ev_idxs)) *\
-            np.cos(src_dec) * (np.sin(delta_ra / 2.))**2.
-
-        # Handle possible floating precision errors.
-        x[x < 0.] = 0.
-        x[x > 1.] = 1.
-
-        psi = (2.0*np.arcsin(np.sqrt(x)))
-        # Floor psi values below the first bin location in spatial KDE PDF.
-        # Flooring at the boundary (1e-6) requires a regeneration of the
-        # spatial KDE splines.
-        floor = 10**(-5.95442953)
-        psi = np.where(psi < floor, floor, psi)
-
-        return psi
-
-    def select_events(self, events, ret_src_ev_idxs=False, tl=None):
+    def select_events(
+            self,
+            events,
+            src_evt_idxs=None,
+            ret_original_evt_idxs=False,
+            tl=None):
         """Selects the events within the spatial box in right-ascention and
         declination and performs an additional selection of events whose ang_err
         value is larger than the value of the provided function at a given psi
         value.
 
         The solid angle dOmega = dRA * dSinDec = dRA * dDec * cos(dec) is a
         function of declination, i.e. for a constant dOmega, the right-ascension
@@ -936,137 +1177,82 @@
 
         Parameters
         ----------
         events : instance of DataFieldRecordArray
             The instance of DataFieldRecordArray that holds the event data.
             The following data fields must exist:
 
-            - 'ra' : float
+            ``'ra'`` : float
                 The right-ascention of the event.
-            - 'dec' : float
+            ``'dec'`` : float
                 The declination of the event.
-        ret_src_ev_idxs : bool
-            Flag if also the indices of the selected events should get
-            returned as a (src_idxs, ev_idxs) tuple of 1d ndarrays.
-            Default is False.
+
+        src_evt_idxs : 2-tuple of 1d ndarrays of ints | None
+            The 2-element tuple holding the two 1d ndarrays of int of length
+            N_values, specifying to which sources the given events belong to.
+            If set to ``None`` all given events will be considered to for all
+            sources.
+        ret_original_evt_idxs : bool
+            Flag if the original indices of the selected events should get
+            returned as well.
         tl : instance of TimeLord | None
             The optional instance of TimeLord that should be used to collect
             timing information about this method.
 
         Returns
         -------
         selected_events : instance of DataFieldRecordArray
             The instance of DataFieldRecordArray holding only the selected
             events.
-        (src_idxs, ev_idxs) : 1d ndarrays of ints | None
-            The indices of sources and selected events, in case
-            `ret_src_ev_idxs` is set to True. Returns None, in case
-            `ret_src_ev_idxs` is set to False.
-        """
-        delta_angle = self._delta_angle
-        src_arr = self._src_arr
-
-        # Get the minus and plus declination around the sources.
-        src_dec_minus = np.maximum(-np.pi/2, src_arr['dec'] - delta_angle)
-        src_dec_plus = np.minimum(src_arr['dec'] + delta_angle, np.pi/2)
-
-        # Calculate the cosine factor for the largest declination distance from
-        # the source. We use np.amin here because smaller cosine values are
-        # larger angles.
-        # cosfact is a (N_sources,)-shaped ndarray.
-        cosfact = np.amin(np.cos([src_dec_minus, src_dec_plus]), axis=0)
-
-        # Calculate delta RA, which is a function of declination.
-        # dRA is a (N_sources,)-shaped ndarray.
-        dRA_half = np.amin(
-            [np.repeat(2*np.pi, len(src_arr['ra'])),
-             np.fabs(delta_angle / cosfact)], axis=0)
-
-        # Determine the mask for the events which fall inside the
-        # right-ascention window.
-        # mask_ra is a (N_sources,N_events)-shaped ndarray.
-        with TaskTimer(tl, 'ESM: Calculate mask_ra.'):
-            nsrc = len(src_arr['ra'])
-
-            # Fill in batch sizes of 200 maximum to save memory.
-            batch_size=200
-            if nsrc > batch_size:
-                mask_ra = np.zeros((nsrc, len(events['ra'])), dtype=bool)
-                n_batches = int(np.ceil(nsrc / float(batch_size)))
-                for bi in range(n_batches):
-                    if not (bi == n_batches-1):
-                        mask_ra[bi*batch_size : (bi+1)*batch_size,...] = (np.fabs(
-                            np.mod(events['ra'] - src_arr['ra'][bi*batch_size : (bi+1)*batch_size][:,np.newaxis] + np.pi, 2*np.pi) -
-                            np.pi) < dRA_half[ bi*batch_size : (bi+1)*batch_size ][:,np.newaxis])
-                    else:
-                        mask_ra[bi*batch_size : ,...] = (np.fabs(
-                            np.mod(events['ra'] - src_arr['ra'][bi*batch_size:][:,np.newaxis] + np.pi, 2*np.pi) -
-                            np.pi) < dRA_half[bi*batch_size:][:,np.newaxis])
-
-            else:
-                mask_ra = np.fabs(
-                    np.mod(events['ra'] - src_arr['ra'][:,np.newaxis] + np.pi, 2*np.pi) - np.pi) < dRA_half[:,np.newaxis]
-
-        # Determine the mask for the events which fall inside the declination
-        # window.
-        # mask_dec is a (N_sources,N_events)-shaped ndarray.
-        with TaskTimer(tl, 'ESM: Calculate mask_dec.'):
-            mask_dec = ((events['dec'] > src_dec_minus[:,np.newaxis]) &
-                        (events['dec'] < src_dec_plus[:,np.newaxis]))
-
-        # Determine the mask for the events which fall inside the
-        # right-ascension and declination window.
-        # mask_sky is a (N_sources,N_events)-shaped ndarray.
-        with TaskTimer(tl, 'ESM: Calculate mask_sky.'):
-            mask_sky = mask_ra & mask_dec
-            del mask_ra
-            del mask_dec
-
-        # Determine the mask for the events that fall inside at least one
-        # source sky window.
-        # mask is a (N_events,)-shaped ndarray.
-        with TaskTimer(tl, 'ESM: Calculate mask.'):
-            mask = np.any(mask_sky, axis=0)
-
-        # Reduce the events according to the mask.
-        with TaskTimer(tl, 'ESM: Create selected_events.'):
-            # Get selected events indices.
-            idxs = np.argwhere(mask_sky[:, mask])
-            src_idxs = idxs[:, 0]
-            ev_idxs = idxs[:, 1]
-
-            # Using an integer indices array for data selection is several
-            # factors faster than using a boolean array.
-            selected_events = events[events.indices[mask]]
-
-        # Perform an additional selection based on psi values.
-        with TaskTimer(tl, 'ESM: Get psi values.'):
-            psi = self._get_psi(selected_events, (src_idxs, ev_idxs))
+        (src_idxs, evt_idxs) : 1d ndarrays of ints
+            The indices of the sources and the selected events.
+        original_evt_idxs : 1d ndarray of ints
+            The (N_selected_events,)-shaped numpy ndarray holding the original
+            indices of the selected events, if ``ret_original_evt_idxs`` is set
+            to ``True``.
+        """
+        if src_evt_idxs is None:
+            n_sources = len(self._src_arr)
+            n_events = len(events)
+            src_idxs = np.repeat(np.arange(n_sources), n_events)
+            evt_idxs = np.tile(np.arange(n_events), n_sources)
+        else:
+            (src_idxs, evt_idxs) = src_evt_idxs
+
+        # Perform selection based on psi values.
+        with TaskTimer(tl, 'ESM: Calculate psi values.'):
+            psi = angular_separation(
+                ra1=np.take(self._src_arr['ra'], src_idxs),
+                dec1=np.take(self._src_arr['dec'], src_idxs),
+                ra2=np.take(events['ra'], evt_idxs),
+                dec2=np.take(events['dec'], evt_idxs),
+            )
 
         with TaskTimer(tl, 'ESM: Create mask_psi.'):
             mask_psi = (
-                (self._func(psi) <= selected_events['ang_err'][ev_idxs])
-                | (psi < self.psi_floor)
+                (events['ang_err'][evt_idxs] >= self._func(psi)) |
+                (psi < self.psi_floor)
             )
 
-        with TaskTimer(tl, 'ESM: Create final_selected_events.'):
+        with TaskTimer(tl, 'ESM: Create selected_events.'):
             # Have to define the shape argument in order to not truncate
             # the mask in case last events are not selected.
-            final_mask_sky = scipy.sparse.csr_matrix(
-                (mask_psi, (src_idxs, ev_idxs)),
-                shape=(len(src_arr['ra']), len(selected_events))
+            mask_sky = scipy.sparse.csr_matrix(
+                (mask_psi, (src_idxs, evt_idxs)),
+                shape=(len(self._src_arr), len(events))
             ).toarray()
-            final_mask = np.any(final_mask_sky, axis=0)
+            mask = np.any(mask_sky, axis=0)
 
             # Using an integer indices array for data selection is several
             # factors faster than using a boolean array.
-            final_selected_events = selected_events[selected_events.indices[final_mask]]
+            selected_events_idxs = events.indices[mask]
+            selected_events = events[selected_events_idxs]
 
-        if(ret_src_ev_idxs):
-            # Get final selected events indices.
-            idxs = np.argwhere(final_mask_sky[:, final_mask])
-            src_idxs = idxs[:, 0]
-            ev_idxs = idxs[:, 1]
+        # Get final selected events indices.
+        idxs = np.argwhere(mask_sky[:, mask])
+        src_idxs = idxs[:, 0]
+        evt_idxs = idxs[:, 1]
 
-            return (final_selected_events, (src_idxs, ev_idxs))
+        if ret_original_evt_idxs:
+            return (selected_events, (src_idxs, evt_idxs), selected_events_idxs)
 
-        return (final_selected_events, None)
+        return (selected_events, (src_idxs, evt_idxs))
```

### Comparing `skyllh-23.1.1/skyllh/core/parameters.py` & `skyllh-23.2.0/skyllh/core/parameters.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,35 +1,40 @@
 # -*- coding: utf-8 -*-
 
-import abc
 import itertools
 import numpy as np
 from copy import deepcopy
 
-from skyllh.physics.source import (
-    SourceCollection,
-    SourceModel
+from skyllh.core import (
+    display,
+)
+from skyllh.core.model import (
+    Model,
+    ModelCollection,
 )
-from skyllh.core import display
-from skyllh.core.model import ModelCollection
 from skyllh.core.py import (
     NamedObjectCollection,
     bool_cast,
     classname,
-    const,
     float_cast,
     get_number_of_float_decimals,
+    int_cast,
     issequence,
     issequenceof,
-    str_cast
 )
-from skyllh.core.random import RandomStateService
+from skyllh.core.source_model import (
+    SourceModel,
+)
 
 
-def make_linear_parameter_grid_1d(name, low, high, delta):
+def make_linear_parameter_grid_1d(
+        name,
+        low,
+        high,
+        delta):
     """Utility function to create a ParameterGrid object for a 1-dimensional
     linear parameter grid.
 
     Parameters
     ----------
     name : str
         The name of the parameter.
@@ -45,41 +50,14 @@
     -------
     obj : ParameterGrid
         The ParameterGrid object holding the discrete parameter grid values.
     """
     grid = np.arange(low, high+delta, delta)
     return ParameterGrid(name, grid, delta)
 
-def make_params_hash(params):
-    """Utility function to create a hash value for a given parameter dictionary.
-
-    Parameters
-    ----------
-    params : dict | None
-        The dictionary holding the parameter (name: value) pairs.
-        If set to None, an empty dictionary is used.
-
-    Returns
-    -------
-    hash : int
-        The hash of the parameter dictionary.
-    """
-    if(params is None):
-        params = {}
-
-    if(not isinstance(params, dict)):
-        raise TypeError('The params argument must be of type dict!')
-
-    # A note on the ordering of Python dictionary items: The items are ordered
-    # internally according to the hash value of their keys. Hence, if we don't
-    # insert more dictionary items, the order of the items won't change. Thus,
-    # we can just take the items list and make a tuple to create a hash of it.
-    # The hash will be the same for two dictionaries having the same items.
-    return hash(tuple(params.items()))
-
 
 class Parameter(object):
     """This class describes a parameter of a mathematical function, like a PDF,
     or source flux function. A parameter has a name, a value range, and an
     initial value. Furthermore, it has a flag that determines whether this
     parameter has a fixed value or not.
     """
@@ -102,16 +80,16 @@
             Flag if the value of this parameter is mutable (False), or not
             (True). If set to `True`, the value of the parameter will always be
             the `initial` value.
             If set to None, the parameter will be mutable if valmin and valmax
             were specified. Otherwise, the parameter is fixed.
             The default is None.
         """
-        if(isfixed is None):
-            if((valmin is not None) and (valmax is not None)):
+        if isfixed is None:
+            if (valmin is not None) and (valmax is not None):
                 isfixed = False
             else:
                 isfixed = True
 
         self.name = name
         self.initial = initial
         self.isfixed = isfixed
@@ -120,81 +98,99 @@
         self.value = self._initial
 
     @property
     def name(self):
         """The name of the parameter.
         """
         return self._name
+
     @name.setter
     def name(self, name):
-        if(not isinstance(name, str)):
-            raise TypeError('The name property must be of type str!')
+        if not isinstance(name, str):
+            raise TypeError(
+                'The "name" property must be of type str! '
+                f'Its current type is {classname(name)}.')
         self._name = name
 
     @property
     def initial(self):
         """The initial value of the parameter.
         """
         return self._initial
+
     @initial.setter
     def initial(self, v):
-        v = float_cast(v, 'The initial property must be castable to type '
-            'float!')
+        v = float_cast(
+            v,
+            'The "initial" property must be castable to type float!')
         self._initial = v
 
     @property
     def isfixed(self):
         """The flag if the parameter is mutable (False) or not (True).
         """
         return self._isfixed
+
     @isfixed.setter
     def isfixed(self, b):
-        b = bool_cast(b, 'The isfixed property must be castable to type bool!')
+        b = bool_cast(
+            b,
+            'The "isfixed" property must be castable to type bool!')
         self._isfixed = b
 
     @property
     def valmin(self):
         """The minimum bound value of the parameter.
         """
         return self._valmin
+
     @valmin.setter
     def valmin(self, v):
-        v = float_cast(v, 'The valmin property must be castable to type float!',
+        v = float_cast(
+            v,
+            'The "valmin" property must be castable to type float!',
             allow_None=True)
         self._valmin = v
 
     @property
     def valmax(self):
         """The maximum bound value of the parameter.
         """
         return self._valmax
+
     @valmax.setter
     def valmax(self, v):
-        v = float_cast(v, 'The valmax property must be castable to type float!',
+        v = float_cast(
+            v,
+            'The "valmax" property must be castable to type float!',
             allow_None=True)
         self._valmax = v
 
     @property
     def value(self):
         """The current value of the parameter.
         """
         return self._value
+
     @value.setter
     def value(self, v):
-        v = float_cast(v, 'The value property must be castable to type float!')
-        if(self._isfixed):
-            if(v != self._initial):
-                raise ValueError('The value (%f) of the fixed parameter "%s" '
-                    'must to equal to the parameter\'s initial value (%f)!'%(
-                    v, self.name, self._initial))
+        v = float_cast(
+            v,
+            'The "value" property must be castable to type float!')
+        if self._isfixed:
+            if v != self._initial:
+                raise ValueError(
+                    f'The value ({v}) of the fixed parameter "{self._name}" '
+                    'must be equal to the parameter\'s initial value '
+                    f'({self._initial})!')
         else:
-            if((v < self._valmin) or (v > self._valmax)):
-                raise ValueError('The value (%f) of parameter "%s" must be '
-                    'within the range (%f, %f)!'%(
-                    v, self._name, self._valmin, self._valmax))
+            if (v < self._valmin) or (v > self._valmax):
+                raise ValueError(
+                    f'The value ({v}) of parameter "{self._name}" must be '
+                    f'within the range [{self._valmin:g}, {self._valmax:g}]!')
         self._value = v
 
     def __eq__(self, other):
         """Implements the equal comparison operator (==).
         By definition two parameters are equal if there property values are
         equal.
 
@@ -206,43 +202,43 @@
 
         Returns
         -------
         cmp : bool
             True, if this Parameter instance and the other Parameter instance
             have the same property values.
         """
-        if((self.name != other.name) or
-           (self.value != other.value) or
-           (self.isfixed != other.isfixed)):
+        if (self.name != other.name) or\
+           (self.value != other.value) or\
+           (self.isfixed != other.isfixed):
             return False
 
         # If both parameters are floating parameters, also their initial, min,
         # and max values must match.
-        if(not self.isfixed):
-            if((self.initial != other.initial) or
-               (self.valmin != other.valmin) or
-               (self.valmax != other.valmax)):
+        if not self.isfixed:
+            if (self.initial != other.initial) or\
+               (self.valmin != other.valmin) or\
+               (self.valmax != other.valmax):
                 return False
 
         return True
 
     def __str__(self):
         """Creates and returns a pretty string representation of this Parameter
         instance.
         """
         indstr = ' ' * display.INDENTATION_WIDTH
 
-        s = 'Parameter: %s = %.3f '%(self._name, self._value)
+        s = f'Parameter: {self._name} = {self._value:g} '
 
-        if(self.isfixed):
+        if self.isfixed:
             s += '[fixed]'
         else:
             s += '[floating] {\n'
-            s += indstr + 'initial: %.3f\n'%(self._initial)
-            s += indstr + 'range: (%.3f, %.3f)\n'%(self._valmin, self._valmax)
+            s += indstr + f'initial: {self._initial:g}\n'
+            s += indstr + f'range: ({self._valmin:g}, {self._valmax:g})\n'
             s += '}'
 
         return s
 
     def as_linear_grid(self, delta):
         """Creates a ParameterGrid instance with a linear grid with constant
         grid value distances delta.
@@ -259,22 +255,29 @@
             The ParameterGrid instance holding the grid values.
 
         Raises
         ------
         ValueError
             If this Parameter instance represents a fixed parameter.
         """
-        if(self.isfixed):
-            raise ValueError('Cannot create a linear grid from the fixed '
-                'parameter "%s". The parameter must be floating!'%(self.name))
+        if self.isfixed:
+            raise ValueError(
+                'Cannot create a linear grid from the fixed '
+                f'parameter "{self._name}". The parameter must be floating!')
+
+        delta = float_cast(
+            delta,
+            'The delta argument must be castable to type float!')
 
-        delta = float_cast(delta, 'The delta argument must be castable to type '
-            'float!')
         grid = make_linear_parameter_grid_1d(
-            self._name, self._valmin, self._valmax, delta)
+            name=self._name,
+            low=self._valmin,
+            high=self._valmax,
+            delta=delta)
+
         return grid
 
     def change_fixed_value(self, value):
         """Changes the value of this fixed parameter to the given value.
 
         Parameters
         ----------
@@ -287,17 +290,17 @@
             The parameter's new value.
 
         Raises
         ------
         ValueError
             If this parameter is not a fixed parameter.
         """
-        if(not self._isfixed):
-            raise ValueError('The parameter "%s" is not a fixed parameter!'%(
-                self.name))
+        if not self._isfixed:
+            raise ValueError(
+                f'The parameter "{self._name}" is not a fixed parameter!')
 
         self.initial = value
         self.value = value
 
     def make_fixed(self, initial=None):
         """Fixes this parameter to the given initial value.
 
@@ -311,25 +314,25 @@
         -------
         value : float
             The parameter's new value.
         """
         self._isfixed = True
 
         # If no new initial value is given, use the current value.
-        if(initial is None):
+        if initial is None:
             self._initial = self._value
             return self._value
 
         self.initial = initial
         self._value = self._initial
 
         # Undefine the valmin and valmax values if the parameter's new value is
         # outside the valmin and valmax range.
-        if((self._valmin is not None) and (self._valmax is not None) and
-           ((self._value < self._valmin) or (self._value > self._valmax))):
+        if (self._valmin is not None) and (self._valmax is not None) and\
+           ((self._value < self._valmin) or (self._value > self._valmax)):
             self._valmin = None
             self._valmax = None
 
         return self._value
 
     def make_floating(self, initial=None, valmin=None, valmax=None):
         """Defines this parameter as floating with the given initial, minimal,
@@ -356,39 +359,42 @@
 
         Raises
         ------
         ValueError
             If valmin is set to None and this parameter has no valmin defined.
             If valmax is set to None and this parameter has no valmax defined.
         """
-        if(initial is None):
+        if initial is None:
             initial = self._value
-        if(valmin is None):
-            if(self._valmin is None):
-                raise ValueError('The current minimal value of parameter "%s" '
+        if valmin is None:
+            if self._valmin is None:
+                raise ValueError(
+                    f'The current minimal value of parameter "{self._name}" '
                     'is not set. So it must be defined through the valmin '
-                    'argument!'%(self._name))
+                    'argument!')
             valmin = self._valmin
-        if(valmax is None):
-            if(self._valmax is None):
-                raise ValueError('The current maximal value of parameter "%s" '
+        if valmax is None:
+            if self._valmax is None:
+                raise ValueError(
+                    f'The current maximal value of parameter "{self._name}" '
                     'is not set. So it must be defined through the valmax '
-                    'argument!'%(self._name))
+                    'argument!')
             valmax = self._valmax
 
         self._isfixed = False
         self.initial = initial
         self.valmin = valmin
         self.valmax = valmax
         self.value = self._initial
 
         return self._value
 
 
-class ParameterSet(object):
+class ParameterSet(
+        object):
     """This class holds a set of Parameter instances.
     """
     @staticmethod
     def union(*paramsets):
         """Creates a ParameterSet instance that is the union of the given
         ParameterSet instances.
 
@@ -399,25 +405,27 @@
 
         Returns
         -------
         paramset : ParameterSet instance
             The newly created ParameterSet instance that holds the union of the
             parameters provided by all the ParameterSet instances.
         """
-        if(not issequenceof(paramsets, ParameterSet)):
-            raise TypeError('The arguments of the union static function must '
-                'be instances of ParameterSet!')
-        if(not len(paramsets) >= 1):
-            raise ValueError('At least 1 ParameterSet instance must be '
-                'provided to the union static function!')
+        if not issequenceof(paramsets, ParameterSet):
+            raise TypeError(
+                'The arguments of the union static function must be instances '
+                'of ParameterSet!')
+        if len(paramsets) == 0:
+            raise ValueError(
+                'At least 1 ParameterSet instance must be provided to the '
+                'union static function!')
 
         paramset = ParameterSet(params=paramsets[0])
         for paramset_i in paramsets[1:]:
             for param in paramset_i._params:
-                if(not paramset.has_param(param)):
+                if not paramset.has_param(param):
                     paramset.add_param(param)
 
         return paramset
 
     def __init__(self, params=None):
         """Constructs a new ParameterSet instance.
 
@@ -446,58 +454,91 @@
         self._floating_param_name_to_idx = dict()
 
         # Define a (n_fixed_params,)-shaped ndarray holding the values of the
         # fixed parameters. This is for optimization purpose only.
         self._fixed_param_values = np.empty((0,), dtype=np.float64)
 
         # Add the initial Parameter instances.
-        if(params is not None):
-            if(isinstance(params, Parameter)):
+        if params is not None:
+            if isinstance(params, Parameter):
                 params = [params]
-            if(not issequenceof(params, Parameter)):
-                raise TypeError('The params argument must be None, an instance '
-                    'of Parameter, or a sequence of Parameter instances!')
+            if not issequenceof(params, Parameter):
+                raise TypeError(
+                    'The params argument must be None, an instance of '
+                    'Parameter, or a sequence of Parameter instances!')
             for param in params:
                 self.add_param(param)
 
     @property
     def params(self):
         """(read-only) The 1D ndarray holding the Parameter instances.
         """
         return self._params
 
     @property
+    def params_name_list(self):
+        """(read-only) The list of str holding the names of all the parameters.
+        """
+        return self._fixed_param_name_list + self._floating_param_name_list
+
+    @property
     def fixed_params(self):
         """(read-only) The 1D ndarray holding the Parameter instances, whose
         values are fixed.
         """
         return self._params[self._params_fixed_mask]
 
     @property
+    def fixed_params_name_list(self):
+        """(read-only) The list of the fixed parameter names.
+        """
+        return self._fixed_param_name_list
+
+    @property
     def fixed_params_mask(self):
         """(read-only) The 1D ndarray holding the mask for the fixed parameters
         of this parameter set.
         """
         return self._params_fixed_mask
 
     @property
+    def fixed_params_idxs(self):
+        """The numpy ndarray holding the indices of the fixed parameters.
+        """
+        idxs = np.argwhere(self._params_fixed_mask).flatten()
+        return idxs
+
+    @property
     def floating_params(self):
         """(read-only) The 1D ndarray holding the Parameter instances,
         whose values are floating.
         """
         return self._params[np.invert(self._params_fixed_mask)]
 
     @property
+    def floating_params_name_list(self):
+        """(read-only) The list of the floating parameter names.
+        """
+        return self._floating_param_name_list
+
+    @property
     def floating_params_mask(self):
         """(read-only) The 1D ndarray holding the mask for the floating
         parameters of this parameter set.
         """
         return np.invert(self._params_fixed_mask)
 
     @property
+    def floating_params_idxs(self):
+        """The numpy ndarray holding the indices of the floating parameters.
+        """
+        idxs = np.argwhere(self.floating_params_mask).flatten()
+        return idxs
+
+    @property
     def n_params(self):
         """(read-only) The number of parameters this ParameterSet has.
         """
         return len(self._params)
 
     @property
     def n_fixed_params(self):
@@ -510,55 +551,68 @@
     def n_floating_params(self):
         """(read-only) The number of floating parameters defined in this
         parameter set.
         """
         return len(self._floating_param_name_list)
 
     @property
-    def fixed_param_name_list(self):
-        """(read-only) The list of the fixed parameter names.
-        """
-        return self._fixed_param_name_list
-
-    @property
-    def floating_param_name_list(self):
-        """(read-only) The list of the floating parameter names.
-        """
-        return self._floating_param_name_list
-
-    @property
     def fixed_param_values(self):
         """(read-only) The (n_fixed_params,)-shaped ndarray holding values of
         the fixed parameters.
         """
         return self._fixed_param_values
 
     @property
     def floating_param_initials(self):
         """(read-only) The 1D (n_floating_params,)-shaped ndarray holding the
         initial values of all the global floating parameters.
         """
         floating_params = self.floating_params
-        if(len(floating_params) == 0):
+
+        if len(floating_params) == 0:
             return np.empty((0,), dtype=np.float64)
-        return np.array(
-            [ param.initial
-             for param in floating_params ], dtype=np.float64)
+
+        initials = np.array(
+            [param.initial for param in floating_params],
+            dtype=np.float64)
+
+        return initials
 
     @property
     def floating_param_bounds(self):
         """(read-only) The 2D (n_floating_params,2)-shaped ndarray holding the
         boundaries for all the floating parameters.
         """
         floating_params = self.floating_params
-        if(len(floating_params) == 0):
-            return np.empty((0,2), dtype=np.float64)
-        return np.array(
-            [ (param.valmin, param.valmax)
-             for param in floating_params ], dtype=np.float64)
+
+        if len(floating_params) == 0:
+            return np.empty((0, 2), dtype=np.float64)
+
+        bounds = np.array(
+            [(param.valmin, param.valmax) for param in floating_params],
+            dtype=np.float64)
+
+        return bounds
+
+    def __contains__(self, param_name):
+        """Implements the ``param_name in self`` expression. It calls the
+        :meth:`has_param` method of this class.
+
+        Parameters
+        ----------
+        param_name : str
+            The name of the parameter.
+
+        Returns
+        -------
+        check : bool
+            Returns ``True`` if the given parameter is part of this ParameterSet
+            instance, ``False`` otherwise.
+        """
+        return self.has_param(param_name)
 
     def __iter__(self):
         """Returns an iterator over the Parameter instances of this ParameterSet
         instance.
         """
         return iter(self._params)
 
@@ -567,17 +621,17 @@
         """
         return len(self._params)
 
     def __str__(self):
         """Creates and returns a pretty string representation of this
         ParameterSet instance.
         """
-        s = '%s: %d parameters (%d floating, %d fixed) {'%(
-            classname(self), self.n_params, self.n_floating_params,
-            self.n_fixed_params)
+        s = (f'{classname(self)}: {self.n_params} parameters '
+             f'({self.n_floating_params} floating, '
+             f'{self.n_fixed_params} fixed) ''{')
         for param in self._params:
             s += '\n'
             s += display.add_leading_text_line_padding(
                 display.INDENTATION_WIDTH, str(param))
         s += '\n}'
         return s
 
@@ -618,14 +672,41 @@
         ------
         KeyError
             If the given parameter is not part of the set of floating
             parameters.
         """
         return self._floating_param_name_to_idx[param_name]
 
+    def generate_random_floating_param_initials(self, rss):
+        """Generates a set of random initials for all floating parameters.
+        A new random initial is defined as
+
+            lower_bound + RAND * (upper_bound - lower_bound),
+
+        where RAND is a uniform random variable between 0 and 1.
+
+        Parameters
+        ----------
+        rss : RandomStateService instance
+            The RandomStateService instance that should be used for drawing
+            random numbers from.
+
+        Returns
+        -------
+        ri : (N_floating_params,)-shaped numpy ndarray
+            The numpy 1D ndarray holding the generated random initial values.
+        """
+        vb = self.floating_param_bounds
+
+        # Do random_initial = lower_bound + RAND * (upper_bound - lower_bound).
+        ri = (vb[:, 0] +
+              rss.random.uniform(size=vb.shape[0])*(vb[:, 1] - vb[:, 0]))
+
+        return ri
+
     def has_fixed_param(self, param_name):
         """Checks if this ParameterSet instance has a fixed parameter named
         ``param_name``.
 
         Parameters
         ----------
         param_name : str
@@ -674,107 +755,109 @@
         self._fixed_param_name_list = []
         self._floating_param_name_list = []
         self._fixed_param_name_to_idx = dict()
         self._floating_param_name_to_idx = dict()
         self._fixed_param_values = np.empty((0,), dtype=np.float64)
         for (pidx, param) in enumerate(self._params):
             pname = param.name
-            if(pname in fix_params_keys):
+            if pname in fix_params_keys:
                 # The parameter of name `pname` should get fixed.
-                if(param.isfixed is True):
-                    raise ValueError('The parameter "%s" is already a fixed '
-                        'parameter!'%(pname))
+                if param.isfixed is True:
+                    raise ValueError(
+                        f'The parameter "{pname}" is already a fixed '
+                        'parameter!')
                 initial = fix_params[pname]
                 param.make_fixed(initial)
                 self._params_fixed_mask[pidx] = True
-                self._fixed_param_name_list += [ pname ]
+                self._fixed_param_name_list += [pname]
                 self._fixed_param_values = np.concatenate(
                     (self._fixed_param_values, [param.value]))
                 self._fixed_param_name_to_idx[pname] = len(
                     self._fixed_param_name_list) - 1
             else:
-                if(param.isfixed):
-                    self._fixed_param_name_list += [ pname ]
+                if param.isfixed:
+                    self._fixed_param_name_list += [pname]
                     self._fixed_param_values = np.concatenate(
                         (self._fixed_param_values, [param.value]))
                     self._fixed_param_name_to_idx[pname] = len(
                         self._fixed_param_name_list) - 1
                 else:
-                    self._floating_param_name_list += [ pname ]
+                    self._floating_param_name_list += [pname]
                     self._floating_param_name_to_idx[pname] = len(
                         self._floating_param_name_list) - 1
 
     def make_params_floating(self, float_params):
         """Makes the given parameters floating with the given initial value and
         within the given bounds.
 
         Parameters
         ----------
         float_params : dict
             The dictionary defining the parameters that should get set to be
             floating. The format of a dictionary's entry can be one of the
             following formats:
 
-                - None
+                ``None``
                     The parameter's initial, minimal and maximal value should be
                     taken from the parameter's current settings.
-                - initial : float
+                initial : float
                     The parameter's initial value should be set to the given
                     value. The minimal and maximal values of the parameter will
                     be taken from the parameter's current settings.
-                - (initial, valmin, valmax)
+                (initial, valmin, valmax)
                     The parameter's initial value, minimal and maximal value
                     should be set to the given values. If `initial` is set to
                     `None`, the parameter's current value will be used as
                     initial value.
 
         Raises
         ------
         ValueError
             If one of the given parameters is already a floating parameter.
         """
         def _parse_float_param_dict_entry(e):
             """Parses the given float_param dictionary entry into initial,
             valmin, and valmax values.
             """
-            if(e is None):
+            if e is None:
                 return (None, None, None)
-            if(issequence(e)):
+            if issequence(e):
                 return (e[0], e[1], e[2])
             return (e, None, None)
 
         float_params_keys = float_params.keys()
         self._fixed_param_name_list = []
         self._floating_param_name_list = []
         self._fixed_param_name_to_idx = dict()
         self._floating_param_name_to_idx = dict()
         self._fixed_param_values = np.empty((0,), dtype=np.float64)
         for (pidx, param) in enumerate(self._params):
             pname = param.name
-            if(pname in float_params_keys):
+            if pname in float_params_keys:
                 # The parameter of name `pname` should get set floating.
-                if(param.isfixed is False):
-                    raise ValueError('The parameter "%s" is already a floating '
-                        'parameter!'%(pname))
+                if param.isfixed is False:
+                    raise ValueError(
+                        f'The parameter "{pname}" is already a floating '
+                        'parameter!')
                 (initial, valmin, valmax) = _parse_float_param_dict_entry(
                     float_params[pname])
                 param.make_floating(initial, valmin, valmax)
                 self._params_fixed_mask[pidx] = False
-                self._floating_param_name_list += [ pname ]
+                self._floating_param_name_list += [pname]
                 self._floating_param_name_to_idx[pname] = len(
                     self._floating_param_name_list) - 1
             else:
-                if(param.isfixed):
-                    self._fixed_param_name_list += [ pname ]
+                if param.isfixed:
+                    self._fixed_param_name_list += [pname]
                     self._fixed_param_values = np.concatenate(
                         (self._fixed_param_values, [param.value]))
                     self._fixed_param_name_to_idx[pname] = len(
                         self._fixed_param_name_list) - 1
                 else:
-                    self._floating_param_name_list += [ pname ]
+                    self._floating_param_name_list += [pname]
                     self._floating_param_name_to_idx[pname] = len(
                         self._floating_param_name_list) - 1
 
     def update_fixed_param_value_cache(self):
         """Updates the internal cache of the fixed parameter values. This method
         has to be called whenever the values of the fixed Parameter instances
         change.
@@ -814,53 +897,57 @@
         ------
         TypeError
             If param is not an instance of Parameter.
         KeyError
             If given parameter is already present in the set. The check is
             performed based on the parameter name.
         """
-        if(not isinstance(param, Parameter)):
-            raise TypeError('The param argument must be an instance of '
-                'Parameter!')
-
-        if(self.has_param(param)):
-            raise KeyError('The parameter named "%s" was already added to the '
-                'parameter set!'%(param.name))
+        if not isinstance(param, Parameter):
+            raise TypeError(
+                'The param argument must be an instance of Parameter! '
+                f'Its current type is {classname(param)}.')
+
+        if self.has_param(param):
+            raise KeyError(
+                f'The parameter named "{param.name}" was already added to the '
+                'parameter set!')
 
         param_fixed_mask = True if param.isfixed else False
 
-        if(atfront):
+        if atfront:
             # Add parameter at front of parameter list.
             self._params = np.concatenate(
                 ([param], self._params))
             self._params_fixed_mask = np.concatenate(
                 ([param_fixed_mask], self._params_fixed_mask))
-            if(param.isfixed):
+            if param.isfixed:
                 self._fixed_param_name_list = (
                     [param.name] + self._fixed_param_name_list)
                 self._fixed_param_values = np.concatenate(
                     ([param.value], self._fixed_param_values))
                 # Shift the index of all fixed parameters.
-                self._fixed_param_name_to_idx = dict([ (k,v+1)
-                    for (k,v) in self._fixed_param_name_to_idx.items() ])
+                self._fixed_param_name_to_idx = dict(
+                    [(k, v+1)
+                     for (k, v) in self._fixed_param_name_to_idx.items()])
                 self._fixed_param_name_to_idx[param.name] = 0
             else:
                 self._floating_param_name_list = (
                     [param.name] + self._floating_param_name_list)
                 # Shift the index of all floating parameters.
-                self._floating_param_name_to_idx = dict([ (k,v+1)
-                    for (k,v) in self._floating_param_name_to_idx.items() ])
+                self._floating_param_name_to_idx = dict(
+                    [(k, v+1)
+                     for (k, v) in self._floating_param_name_to_idx.items()])
                 self._floating_param_name_to_idx[param.name] = 0
         else:
             # Add parameter at back of parameter list.
             self._params = np.concatenate(
                 (self._params, [param]))
             self._params_fixed_mask = np.concatenate(
                 (self._params_fixed_mask, [param_fixed_mask]))
-            if(param.isfixed):
+            if param.isfixed:
                 self._fixed_param_name_list = (
                     self._fixed_param_name_list + [param.name])
                 self._fixed_param_values = np.concatenate(
                     (self._fixed_param_values, [param.value]))
                 self._fixed_param_name_to_idx[param.name] = len(
                     self._fixed_param_name_list) - 1
             else:
@@ -882,230 +969,65 @@
 
         Returns
         -------
         check : bool
             ``True`` if the given parameter is present in this parameter set,
             ``False`` otherwise.
         """
-        if((param.name in self._floating_param_name_list) or
-           (param.name in self._fixed_param_name_list)):
+        if (param.name in self._floating_param_name_list) or\
+           (param.name in self._fixed_param_name_list):
             return True
 
         return False
 
-    def floating_param_values_to_dict(self, floating_param_values):
+    def get_params_dict(self, floating_param_values):
         """Converts the given floating parameter values into a dictionary with
         the floating parameter names and values and also adds the fixed
         parameter names and their values to this dictionary.
 
         Parameters
         ----------
         floating_param_values : 1D ndarray
             The ndarray holding the values of the floating parameters in the
             order that the floating parameters are defined.
 
         Returns
         -------
-        param_dict : dict
+        params_dict : dict
             The dictionary with the floating and fixed parameter names and
             values.
         """
-        param_dict = dict(
+        params_dict = dict(
             list(zip(self._floating_param_name_list, floating_param_values)) +
-            list(zip(self._fixed_param_name_list, self._fixed_param_values)))
-
-        return param_dict
-
-
-class ParameterSetArray(object):
-    """This class provides a data holder for an array of ParameterSet instances.
-    Given an array of global floating parameter values, it can split that array
-    into floating parameter value sub arrays, one for each ParameterSet instance
-    of this ParameterSetArray instance. This functionality is required in
-    order to be able to map the global floating parameter values from the
-    minimizer to their parameter names.
-    """
-    def __init__(self, paramsets):
-        """Creates a new ParameterSetArray instance, which will hold a list of
-        constant ParameterSet instances.
-
-        Parameters
-        ----------
-        paramsets : const instance of ParameterSet | sequence of const instances
-                of ParameterSet
-            The sequence of constant ParameterSet instances holding the global
-            parameters.
-
-        Raises
-        ------
-        TypeError
-            If the given paramsets argument ist not a sequence of constant
-            instances of ParameterSet.
-        """
-        super(ParameterSetArray, self).__init__()
-
-        if(isinstance(paramsets, ParameterSet)):
-            paramsets = [paramsets]
-        if(not issequenceof(paramsets, ParameterSet, const)):
-            raise TypeError('The paramsets argument must be a constant '
-                'instance of ParameterSet or a sequence of constant '
-                'ParameterSet instances!')
-        self._paramset_list = list(paramsets)
-
-        # Calculate the total number of parameters hold by this
-        # ParameterSetArray instance.
-        self._n_params = np.sum([paramset.n_params
-            for paramset in self._paramset_list])
-
-        # Calculate the total number of fixed parameters hold by this
-        # ParameterSetArray instance.
-        self._n_fixed_params = np.sum([paramset.n_fixed_params
-            for paramset in self._paramset_list])
-
-        # Calculate the total number of floating parameters hold by this
-        # ParameterSetArray instance.
-        self._n_floating_params = np.sum([paramset.n_floating_params
-            for paramset in self._paramset_list])
-
-        # Determine the array of initial values of all floating parameters.
-        self._floating_param_initials = np.concatenate([
-            paramset.floating_param_initials
-            for paramset in self._paramset_list])
-
-        # Determine the array of bounds of all floating parameters.
-        self._floating_param_bounds = np.concatenate([
-            paramset.floating_param_bounds
-            for paramset in self._paramset_list])
-
-    @property
-    def paramset_list(self):
-        """(read-only) The list of ParameterSet instances holding the global
-        parameters.
-        """
-        return self._paramset_list
-
-    @property
-    def n_params(self):
-        """(read-only) The total number of parameters hold by this
-        ParameterSetArray instance.
-        """
-        return self._n_params
-
-    @property
-    def n_fixed_params(self):
-        """(read-only) The total number of fixed parameters hold by this
-        ParameterSetArray instance.
-        """
-        return self._n_fixed_params
-
-    @property
-    def n_floating_params(self):
-        """(read-only) The total number of floating parameters hold by this
-        ParameterSetArray instance.
-        """
-        return self._n_floating_params
-
-    @property
-    def floating_param_initials(self):
-        """(read-only) The 1D (n_floating_params,)-shaped ndarray holding the
-        initial values of all the floating parameters.
-        """
-        return self._floating_param_initials
-
-    @property
-    def floating_param_bounds(self):
-        """(read-only) The 2D (n_floating_params,2)-shaped ndarray holding the
-        boundaries for all the floating parameters.
-        """
-        return self._floating_param_bounds
-
-    def __str__(self):
-        """Creates and returns a pretty string representation of this
-        ParameterSetArray instance.
-        """
-        s = '%s: %d parameters (%d floating, %d fixed) {\n'%(
-            classname(self), self.n_params, self.n_floating_params,
-            self.n_fixed_params)
-
-        for (idx,paramset) in enumerate(self._paramset_list):
-            if(idx > 0):
-                s += '\n'
-            s += display.add_leading_text_line_padding(
-                display.INDENTATION_WIDTH,
-                str(paramset))
-
-        s += '\n}'
-
-        return s
-
-    def generate_random_initials(self, rss):
-        """Generates a set of random initials for all global floating
-        parameters.
-        A new random initial is defined as
-
-            lower_bound + RAND * (upper_bound - lower_bound),
-
-        where RAND is a uniform random variable between 0 and 1.
-
-        Parameters
-        ----------
-        rss : RandomStateService instance
-            The RandomStateService instance that should be used for drawing
-            random numbers from.
-        """
-        vb = self.floating_param_bounds
-        # Do random_initial = lower_bound + RAND * (upper_bound - lower_bound)
-        ri = vb[:,0] + rss.random.uniform(size=vb.shape[0])*(vb[:,1] - vb[:,0])
+            list(zip(self._fixed_param_name_list, self._fixed_param_values))
+        )
 
-        return ri
+        return params_dict
 
-    def split_floating_param_values(self, floating_param_values):
-        """Splits the given floating parameter values into their specific
-        ParameterSet part.
+    def get_floating_params_dict(self, floating_param_values):
+        """Converts the given floating parameter values into a dictionary with
+        the floating parameter names and values.
 
         Parameters
         ----------
-        floating_param_values : (n_floating_params,)-shaped 1D ndarray
-            The ndarray holding the values of all the floating parameters for
-            all ParameterSet instances. The order must match the order of
-            ParameterSet instances and their order of floating parameters.
+        floating_param_values : 1D ndarray
+            The ndarray holding the values of the floating parameters in the
+            order that the floating parameters are defined.
 
         Returns
         -------
-        floating_param_values_list : list of (n_floating_params,)-shaped 1D
-                ndarray
-            The list of ndarray objects, where each ndarray holds only the
-            floating values of the particular ParameterSet instance. The order
-            matches the order of ParameterSet instances defined for this
-            ParameterSetArray.
+        params_dict : dict
+            The dictionary with the floating and fixed parameter names and
+            values.
         """
-        if(len(floating_param_values) != self.n_floating_params):
-            raise ValueError('The number of given floating parameter values '
-                '(%d) does not match the total number of defined floating '
-                'parameters (%d)!'%(len(floating_param_values),
-                self.n_floating_params))
-
-        floating_param_values_list = []
-
-        offset = 0
-        for paramset in self._paramset_list:
-            n_floating_params = paramset.n_floating_params
-            floating_param_values_list.append(floating_param_values[
-                offset:offset+n_floating_params])
-            offset += n_floating_params
+        params_dict = dict(
+            list(zip(self._floating_param_name_list, floating_param_values))
+        )
 
-        return floating_param_values_list
-
-    def update_fixed_param_value_cache(self):
-        """Updates the internal cache of the fixed parameter values. This method
-        has to be called whenever the values of the fixed Parameter instances
-        change.
-        """
-        for paramset in self._paramset_list:
-            paramset.update_fixed_param_value_cache()
+        return params_dict
 
 
 class ParameterGrid(object):
     """This class provides a data holder for a parameter that has a set of
     discrete values on a grid. Thus, the parameter has a value grid.
     This class represents a one-dimensional grid.
     """
@@ -1127,23 +1049,72 @@
             The maximal number of decimals is 16.
             If set to None, the number of decimals will be the maximum of the
             number of decimals of the first grid value and the number of
             decimals of the delta value.
 
         Returns
         -------
-        param_grid : ParameterGrid instance
+        param_grid : instance of ParameterGrid
             The created ParameterGrid instance.
         """
         return ParameterGrid(
             name=binning.name,
             grid=binning.binedges,
             delta=delta,
             decimals=decimals)
 
+    @staticmethod
+    def from_range(name, start, stop, delta, decimals=None):
+        """Creates a ParameterGrid instance from a range definition. The stop
+        value will be the last grid point.
+
+        Parameters
+        ----------
+        name : str
+            The name of the parameter grid.
+        start : float
+            The start value of the range.
+        stop : float
+            The end value of the range.
+        delta : float
+            The width between the grid values.
+        decimals : int | None
+            The number of decimals the grid values should get rounded to.
+            The maximal number of decimals is 16.
+            If set to None, the number of decimals will be the maximum of the
+            number of decimals of the first grid value and the number of
+            decimals of the delta value.
+
+        Returns
+        -------
+        param_grid : instance of ParameterGrid
+            The created ParameterGrid instance.
+        """
+        start = float_cast(
+            start,
+            'The start argument must be castable to type float!')
+        stop = float_cast(
+            stop,
+            'The stop argument must be castable to type float!')
+        delta = float_cast(
+            delta,
+            'The delta argument must be castable to type float!')
+        decimals = int_cast(
+            decimals,
+            'The decimals argument must be castable to type int!',
+            allow_None=True)
+
+        grid = np.arange(start, stop+delta, delta)
+
+        return ParameterGrid(
+            name=name,
+            grid=grid,
+            delta=delta,
+            decimals=decimals)
+
     def __init__(self, name, grid, delta=None, decimals=None):
         """Creates a new parameter grid.
 
         Parameters
         ----------
         name : str
             The name of the parameter.
@@ -1157,121 +1128,123 @@
         decimals : int | None
             The number of decimals the grid values should get rounded to.
             The maximal number of decimals is 16.
             If set to None, the number of decimals will be the maximum of the
             number of decimals of the first grid value and the number of
             decimals of the delta value.
         """
-        if(delta is None):
+        if delta is None:
             # We need to take the mean of all the "equal" differences in order
             # to smooth out unlucky rounding issues of a particular difference.
             delta = np.mean(np.diff(grid))
 
-        delta = float_cast(delta, 'The delta argument must be castable to '
-            'type float!')
+        delta = float_cast(
+            delta,
+            'The delta argument must be castable to type float!')
         self._delta = np.float64(delta)
 
         # Determine the number of decimals of delta.
-        if(decimals is None):
+        if decimals is None:
             decimals_value = get_number_of_float_decimals(grid[0])
             decimals_delta = get_number_of_float_decimals(delta)
             decimals = int(np.max((decimals_value, decimals_delta)))
-        if(not isinstance(decimals, int)):
-            raise TypeError('The decimals argument must be an instance of '
-                'type int!')
-        if(decimals > 16):
-            raise ValueError('The maximal number of decimals is 16! Maybe you '
-                'should consider log-space!?')
+        if not isinstance(decimals, int):
+            raise TypeError(
+                'The decimals argument must be an instance of type int!')
+        if decimals > 16:
+            raise ValueError(
+                'The maximal number of decimals is 16! Maybe you should '
+                'consider log-space!?')
 
         self.name = name
         self._decimals = decimals
         self._delta = np.around(self._delta, self._decimals)
         self.lower_bound = grid[0]
 
         # Setting the grid, will automatically round the grid values to their
         # next nearest grid value. Hence, we need to set the grid property after
         # setting the delta and offser properties.
         self.grid = grid
 
+    def __str__(self):
+        """Pretty string representation.
+        """
+        return '{:s} = {:s}, decimals = {:d}'.format(
+            self._name, str(self._grid), self._decimals)
+
     @property
     def name(self):
         """The name of the parameter.
         """
         return self._name
+
     @name.setter
     def name(self, name):
-        if(not isinstance(name, str)):
-            raise TypeError('The name property must be of type str!')
+        if not isinstance(name, str):
+            raise TypeError(
+                'The name property must be of type str!')
         self._name = name
 
     @property
     def decimals(self):
         """(read-only) The number of significant decimals of the grid values.
         """
         return self._decimals
 
     @property
     def grid(self):
         """The numpy.ndarray with the grid values of the parameter.
         """
         return self._grid
+
     @grid.setter
     def grid(self, arr):
-        if(not issequence(arr)):
-            raise TypeError('The grid property must be a sequence!')
-        if(not isinstance(arr, np.ndarray)):
+        if not issequence(arr):
+            raise TypeError(
+                'The grid property must be a sequence!')
+        if not isinstance(arr, np.ndarray):
             arr = np.array(arr, dtype=np.float64)
-        if(arr.ndim != 1):
-            raise ValueError('The grid property must be a 1D numpy.ndarray!')
+        if arr.ndim != 1:
+            raise ValueError(
+                'The grid property must be a 1D numpy.ndarray!')
         self._grid = self.round_to_nearest_grid_point(arr)
 
     @property
     def delta(self):
         """(read-only) The width (float) between the grid values.
         """
         return self._delta
 
     @property
     def lower_bound(self):
         """The lower bound of the parameter grid.
         """
         return self._lower_bound
+
     @lower_bound.setter
     def lower_bound(self, v):
-        v = float_cast(v, 'The lower_bound property must be castable to type '
-            'float!')
+        v = float_cast(
+            v,
+            'The lower_bound property must be castable to type float!')
         self._lower_bound = np.around(np.float64(v), self._decimals)
 
     @property
     def ndim(self):
         """The dimensionality of the parameter grid.
         """
         return self._grid.ndim
 
     def _calc_floatD_and_intD(self, value):
         """Calculates the number of delta intervals of the given values counted
         from the lower bound of the grid. It returns its float and integer
         representation.
-
-        Raises
-        ------
-        ValueError
-            If one of the values are below or above the grid range.
         """
         value = np.atleast_1d(value).astype(np.float64)
 
-        if(hasattr(self, '_grid')):
-            m = (value >= self._lower_bound) & (value <= self._grid[-1])
-            if(not np.all(m)):
-                raise ValueError('The following values are outside the range '
-                    'of the parameter grid "%s": %s'%(
-                        self.name,
-                        ','.join(str(v) for v in value[np.invert(m)])))
-
-        floatD = value/self._delta - self._lower_bound/self._delta
+        floatD = (value - self._lower_bound)/self._delta
         floatD = np.around(floatD, 9)
         intD = floatD.astype(np.int64)
 
         return (floatD, intD)
 
     def add_extra_lower_and_upper_bin(self):
         """Adds an extra lower and upper bin to this parameter grid. This is
@@ -1307,16 +1280,17 @@
         """
         scalar_input = np.isscalar(value)
 
         (floatD, intD) = self._calc_floatD_and_intD(value)
         gp = self._lower_bound + (np.around(floatD % 1, 0) + intD)*self._delta
         gp = np.around(gp, self._decimals)
 
-        if(scalar_input):
+        if scalar_input:
             return gp.item()
+
         return gp
 
     def round_to_lower_grid_point(self, value):
         """Rounds the given value to the nearest grid point that is lower than
         the given value.
 
         Note: If the given value is a grid point, that grid point will be
@@ -1334,16 +1308,17 @@
         """
         scalar_input = np.isscalar(value)
 
         (floatD, intD) = self._calc_floatD_and_intD(value)
         gp = self._lower_bound + intD*self._delta
         gp = np.around(gp, self._decimals)
 
-        if(scalar_input):
+        if scalar_input:
             return gp.item()
+
         return gp
 
     def round_to_upper_grid_point(self, value):
         """Rounds the given value to the nearest grid point that is larger than
         the given value.
 
         Note: If the given value is a grid point, the next grid point will be
@@ -1361,62 +1336,69 @@
         """
         scalar_input = np.isscalar(value)
 
         (floatD, intD) = self._calc_floatD_and_intD(value)
         gp = self._lower_bound + (intD + 1)*self._delta
         gp = np.around(gp, self._decimals)
 
-        if(scalar_input):
+        if scalar_input:
             return gp.item()
+
         return gp
 
 
-class ParameterGridSet(NamedObjectCollection):
+class ParameterGridSet(
+        NamedObjectCollection):
     """Describes a set of parameter grids.
     """
-    def __init__(self, param_grids=None):
+    def __init__(
+            self,
+            param_grids=None,
+            **kwargs):
         """Constructs a new ParameterGridSet object.
 
         Parameters
         ----------
-        param_grids : sequence of ParameterGrid instances |
-                ParameterGrid instance | None
-            The ParameterGrid instances this ParameterGridSet instance should
+        param_grids : sequence of instance of ParameterGrid | instance of ParameterGrid | None
+            The ParameterGrid instances this instance of ParameterGridSet should
             get initialized with.
         """
-        super(ParameterGridSet, self).__init__(
-            objs=param_grids, obj_type=ParameterGrid)
+        super().__init__(
+            objs=param_grids,
+            obj_type=ParameterGrid,
+            **kwargs)
 
     @property
     def ndim(self):
         """The dimensionality of this parameter grid set. By definition it's the
         number of parameters of the set.
         """
         return len(self)
 
     @property
-    def parameter_names(self):
+    def params_name_list(self):
         """(read-only) The list of the parameter names.
         """
-        return [ paramgrid.name for paramgrid in self.objects ]
+        return self.name_list
 
     @property
     def parameter_permutation_dict_list(self):
         """(read-only) The list of parameter dictionaries constructed from all
         permutations of all the parameter values.
         """
-        # Get the list of parameter names.
-        param_names = [ paramgrid.name for paramgrid in self.objects ]
-        # Get the list of parameter grids, in same order than the parameter
-        # names.
-        param_grids = [ paramgrid.grid for paramgrid in self.objects ]
-
-        dict_list = [ dict([ (p_i, t_i)
-                            for (p_i, t_i) in zip(param_names, tup) ])
-                     for tup in itertools.product(*param_grids) ]
+        param_grids = [paramgrid.grid for paramgrid in self.objects]
+
+        dict_list = [
+            dict([
+                (p_i, t_i)
+                for (p_i, t_i) in zip(self.name_list, tup)
+            ])
+            for tup in itertools.product(*param_grids)
+        ]
+
         return dict_list
 
     def add_extra_lower_and_upper_bin(self):
         """Adds an extra lower and upper bin to all the parameter grids. This is
         usefull when interpolation or gradient methods require an extra bin on
         each side of the grid.
         """
@@ -1426,79 +1408,124 @@
     def copy(self):
         """Copies this ParameterGridSet object and returns the copy.
         """
         copy = deepcopy(self)
         return copy
 
 
-class ModelParameterMapper(object, metaclass=abc.ABCMeta):
-    """This abstract base class defines the interface of a model parameter
-    mapper. A model parameter mapper provides the functionality to map a global
+class ParameterModelMapper(
+        object):
+    """This class provides the parameter to model mapper.
+    The parameter to model mapper provides the functionality to map a global
     parameter, usually a fit parameter, to a local parameter of a model, e.g.
     to a source, or a background model parameter.
     """
 
-    def __init__(self, name, models):
+    @staticmethod
+    def is_global_fitparam_a_local_param(
+            fitparam_id,
+            params_recarray,
+            local_param_names):
+        """Determines if the given global fit parameter is a local parameter of
+        the given list of local parameter names.
+
+        Parameters
+        ----------
+        fitparam_id : int
+            The ID of the global fit parameter.
+        params_recarray : instance of numpy record ndarray
+            The (N_models,)-shaped numpy record ndarray holding the local
+            parameter names and values of the models. See the
+            :meth:`skyllh.core.parameters.ParameterModelMapper.create_src_params_recarray`
+            method for the format of this record array.
+        local_param_names : list of str
+            The list of local parameters.
+
+        Returns
+        -------
+        check : bool
+            ``True`` if the global fit parameter translates to a local parameter
+            contained in the ``local_param_names`` list, ``False`` otherwise.
+        """
+        for pname in local_param_names:
+            if pname not in params_recarray.dtype.fields:
+                continue
+            if np.any(params_recarray[f'{pname}:gpidx'] == fitparam_id + 1):
+                return True
+
+        return False
+
+    @staticmethod
+    def is_local_param_a_fitparam(
+            local_param_name,
+            params_recarray):
+        """Checks if the given local parameter is a (partly) a fit parameter.
+
+        Parameters
+        ----------
+        local_param_name : str
+            The name of the local parameter.
+        params_recarray : instance of numpy record ndarray
+            The (N_models,)-shaped numpy record ndarray holding the local
+            parameter names and values of the models. See the
+            :meth:`skyllh.core.parameters.ParameterModelMapper.create_src_params_recarray`
+            method for the format of this record array.
+
+        Returns
+        -------
+        check : bool
+            ``True`` if the given local parameter is (partly) a fit parameter.
+        """
+        if np.any(params_recarray[f'{local_param_name}:gpidx'] > 0):
+            return True
+
+        return False
+
+    def __init__(self, models, **kwargs):
         """Constructor of the parameter mapper.
 
         Parameters
         ----------
-        name : str
-            The name of the model parameter mapper. In practice this is a
-            representative name for the set of global parameters this model
-            parameter mapper holds. For a two-component signal-background
-            likelihood model, "signal", or "background" could be useful names.
-        models : sequence of Model instances.
+        models : sequence of instance of Model.
             The sequence of Model instances the parameter mapper can map global
             parameters to.
         """
-        super(ModelParameterMapper, self).__init__()
+        super().__init__(**kwargs)
 
-        self.name = name
-        self.models = models
+        models = ModelCollection.cast(
+            models,
+            'The models property must be castable to an instance of '
+            'ModelCollection!')
+        self._models = models
 
         # Create the parameter set for the global parameters.
         self._global_paramset = ParameterSet()
 
-        # Define a (n_global_params,)-shaped numpy ndarray of str objects that
-        # will hold the local parameter names of the global parameters as
-        # defined by the models.
-        # The local model parameter names are the names used by the internal
-        # math objects, like PDFs. Thus, the global parameter names can be
-        # aliases of such local model parameter names.
-        self._model_param_names = np.empty((0,), dtype=np.object_)
-
-        # (N_params, N_models) shaped boolean ndarray defining what global
-        # parameter maps to which model.
-        self._global_param_2_model_mask = np.zeros(
-            (0, len(self._models)), dtype=np.bool_)
+        # Define the attribute holding the boolean mask of the models that are
+        # source models.
+        self._source_model_mask = np.array(
+            [isinstance(model, SourceModel) for model in self._models],
+            dtype=bool)
 
-    @property
-    def name(self):
-        """The name of this ModelParameterMapper instance. In practice this is
-        a representative name for the set of global parameters this mapper
-        holds.
-        """
-        return self._name
-    @name.setter
-    def name(self, name):
-        name = str_cast(name, 'The name property must be castable to type str!')
-        self._name = name
+        # Define a (n_models, n_global_params)-shaped numpy ndarray of str
+        # objects that will hold the local model parameter names of the global
+        # parameters.
+        # The local model parameter names are the names used by the internal
+        # math objects, like PDFs. Thus, they can be aliases for the global
+        # parameter names. Entries set to None, will indicate masked-out
+        # global parameters.
+        self._model_param_names = np.empty(
+            (len(self._models), 0), dtype=np.object_)
 
     @property
     def models(self):
-        """The ModelCollection instance defining the models the mapper can
-        map global parameters to.
+        """(read-only) The ModelCollection instance defining the models the
+        mapper can map global parameters to.
         """
         return self._models
-    @models.setter
-    def models(self, obj):
-        obj = ModelCollection.cast(obj, 'The models property must '
-            'be castable to an instance of ModelCollection!')
-        self._models = obj
 
     @property
     def global_paramset(self):
         """(read-only) The ParameterSet instance holding the list of global
         parameters.
         """
         return self._global_paramset
@@ -1523,1046 +1550,528 @@
 
     @property
     def n_global_floating_params(self):
         """(read-only) The number of defined global floating parameters.
         """
         return self._global_paramset.n_floating_params
 
+    @property
+    def n_sources(self):
+        """(read-only) The number of source models the mapper knows about.
+        """
+        return np.count_nonzero(self._source_model_mask)
+
+    @property
+    def unique_model_param_names(self):
+        """(read-only) The unique parameters names of all the models.
+        """
+        m = self._model_param_names != np.array(None)
+        return np.unique(self._model_param_names[m])
+
+    @property
+    def unique_source_param_names(self):
+        """(read-only) The unique parameter names of the sources.
+        """
+        src_param_names = self._model_param_names[self._source_model_mask, ...]
+        m = src_param_names != np.array(None)
+        return np.unique(src_param_names[m])
+
     def __str__(self):
-        """Generates and returns a pretty string representation of this model
-        parameter mapper.
+        """Generates and returns a pretty string representation of this
+        parameter model mapper.
         """
         n_global_params = self.n_global_params
 
         # Determine the number of models that have global parameters assigned.
-        # Remember self._global_param_2_model_mask is a
-        # (n_global_params, n_models)-shaped 2D ndarray.
-        n_models = np.sum(np.sum(self._global_param_2_model_mask, axis=0) > 0)
+        # Remember self._model_param_names is a (n_models, n_global_params)-
+        # shaped 2D ndarray.
+        n_models = self.n_models
+        n_sources = self.n_sources
 
-        s = classname(self) + ' "%s": '%(self._name)
-        s += '%d global parameter'%(n_global_params)
+        s = f'{classname(self)}: '
+        s += f'{n_global_params} global parameter'
         s += '' if n_global_params == 1 else 's'
         s += ', '
-        s += '%d model'%(n_models)
+        s += f'{n_models} model'
         s += '' if n_models == 1 else 's'
+        s += f' ({n_sources} source'
+        s += '' if n_sources == 1 else 's'
+        s += ')'
 
-        if(n_global_params == 0):
+        if n_global_params == 0:
             return s
 
         s1 = 'Parameters:'
         s += '\n' + display.add_leading_text_line_padding(
             display.INDENTATION_WIDTH, s1)
-        for (pidx,param) in enumerate(self._global_paramset.params):
-            model_names = [ self._models[model_idx].name
-                for model_idx in np.nonzero(
-                    self._global_param_2_model_mask[pidx])[0]
-            ]
-            if(param.isfixed):
-                pstate = 'fixed (%.3f)'%(
-                    param.initial)
+        for (pidx, p) in enumerate(self._global_paramset.params):
+            if p.isfixed:
+                pstate = (
+                    f'fixed ({p.initial:g})'
+                )
             else:
-                pstate = 'floating (%.3f <= %.3f <= %.3f)'%(
-                    param.valmin, param.initial, param.valmax)
-            ps = '\n%s [%s] --> %s\n'%(
-                param.name, pstate, self._model_param_names[pidx])
+                pstate = (
+                    f'floating ({p.valmin:g} <= {p.initial:g} <= {p.valmax:g})'
+                )
+            ps = f'\n{p.name} [{pstate}]\n'
+
             ps1 = 'in models:\n'
-            ps1 += '- '
-            ps1 += '\n- '.join(model_names)
+            for (midx, mpname) in enumerate(self._model_param_names[:, pidx]):
+                if mpname is not None:
+                    ps1 += '- ' + self._models[midx].name + ': ' + mpname + "\n"
+
             ps += display.add_leading_text_line_padding(
                 display.INDENTATION_WIDTH, ps1)
             s += display.add_leading_text_line_padding(
                 2*display.INDENTATION_WIDTH, ps)
 
         return s
 
-    def finalize(self):
-        """Finalizes this ModelParameterMapper instance by declaring its
-        ParameterSet instance as constant. No new global parameters can be added
-        after calling this method.
-        """
-        self._global_paramset = const(self._global_paramset)
-
-    @abc.abstractmethod
-    def def_param(self, param, model_param_name=None, models=None):
-        """This method is supposed to add the given Parameter instance to the
-        parameter mapper and maps the global parameter to the given sequence of
-        models the parameter mapper knows about.
-
-        Parameters
-        ----------
-        param : instance of Parameter
-            The global parameter which should get mapped to one or more models.
-        model_param_name : str | None
-            The name of the parameter of the model. Hence, the global
-            parameter name can be different to the parameter name of the model.
-            If `None`, the name of the global parameter will be used as model
-            parameter name.
-        models : sequence of Model instances
-            The sequence of Model instances the parameter should get
-            mapped to. The instances in the sequence must match Model instances
-            specified at construction of this mapper.
-        """
-        pass
-
-    @abc.abstractmethod
-    def get_model_param_dict(
-            self, global_floating_param_values, model_idx=None):
-        """This method is supposed to create a dictionary with the fixed and
-        floating parameter names and their values for the given model.
+    def get_model_param_name(self, model_idx, gp_idx):
+        """Retrieves the local parameter name of a given model and global
+        parameter index.
 
         Parameters
         ----------
-        global_floating_param_values : 1D ndarray instance
-            The ndarray instance holding the current values of the global
-            floating parameters.
-        model_idx : int | None
-            The index of the model as it was defined at construction
-            time of this ModelParameterMapper instance.
+        model_idx : int
+            The index of the model.
+        gp_idx : int
+            The index of the global parameter.
 
         Returns
         -------
-        model_param_dict : dict
-            The dictionary holding the fixed and floating parameter names and
-            values of the specified model.
+        param_name : str | None
+            The name of the local model parameter. It is ``None``, if the given
+            global parameter is not mapped to the given model.
         """
-        pass
+        param_name = self._model_param_names[model_idx, gp_idx]
 
+        return param_name
 
-class SingleModelParameterMapper(ModelParameterMapper):
-    """This class provides a model parameter mapper for a single model, like a
-    single source, or a single background model.
-    """
-    def __init__(self, name, model):
-        """Constructs a new model parameter mapper for a single model.
+    def get_gflp_idx(self, name):
+        """Gets the index of the global floating parameter of the given name.
 
         Parameters
         ----------
         name : str
-            The name of the model parameter mapper. In practice this is a
-            representative name for the set of global parameters this model
-            parameter mapper holds. For a two-component signal-background
-            likelihood model, "signal", or "background" could be useful names.
-        model : instance of Model
-            The instance of Model the parameter mapper can map global
-            parameters to.
+            The global floating parameter's name.
+
+        Returns
+        -------
+        idx : int
+            The index of the global floating parameter.
         """
-        super(SingleModelParameterMapper, self).__init__(
-            name=name, models=model)
+        return self._global_paramset.get_floating_pidx(
+            param_name=name)
 
-    def def_param(self, param, model_param_name=None):
-        """Adds the given Parameter instance to the parameter mapper.
+    def get_model_idx_by_name(self, name):
+        """Determines the index within this ParameterModelMapper instance of
+        the model with the given name.
 
         Parameters
         ----------
-        param : instance of Parameter
-            The global parameter which should get mapped to the single model.
-        model_param_name : str | None
-            The parameter name of the model. Hence, the global parameter name
-            can be different to the parameter name of the model.
-            If set to `None`, the name of the global parameter will be used as
-            model parameter name.
+        name : str
+            The model's name.
 
         Returns
         -------
-        self : SingleModelParameterMapper
-            The instance of this SingleModelParameterMapper, so that several
-            `def_param` calls can be concatenated.
+        model_idx : int
+            The model's index within this ParameterModelMapper instance.
 
         Raises
         ------
         KeyError
-            If there is already a model parameter with the given name defined.
+            If there is no model of the given name.
         """
-        if(model_param_name is None):
-            model_param_name = param.name
-        if(not isinstance(model_param_name, str)):
-            raise TypeError('The model_param_name argument must be None or of '
-                'type str!')
-
-        if(model_param_name in self._model_param_names):
-            raise KeyError('There is already a global parameter defined for '
-                'the model parameter name "%s"!'%(model_param_name))
+        for (model_idx, model) in enumerate(self._models):
+            if model.name == name:
+                return model_idx
 
-        self._global_paramset.add_param(param)
-        self._model_param_names = np.concatenate(
-            (self._model_param_names,[model_param_name]))
+        raise KeyError(
+            f'The model with name "{name}" does not exist within the '
+            'ParameterModelMapper instance!')
 
-        mask = np.ones((1,), dtype=np.bool_)
-        self._global_param_2_model_mask = np.vstack(
-            (self._global_param_2_model_mask, mask))
-
-        return self
-
-    def get_model_param_dict(
-            self, global_floating_param_values, model_idx=None):
-        """Creates a dictionary with the fixed and floating parameter names and
-        their values for the single model.
+    def get_src_model_idxs(self, sources=None):
+        """Creates a numpy ndarray holding the indices of the requested source
+        models.
 
         Parameters
         ----------
-        global_floating_param_values : 1D ndarray instance
-            The ndarray instance holding the current values of the global
-            floating parameters. The values must be in the same order as the
-            floating parameters were defined.
-        model_idx : None
-            The index of the model as it was defined at construction
-            time of this ModelParameterMapper instance. Since this is a
-            ModelParameterMapper for a single model, this argument is
-            ignored.
+        sources : instance of SourceModel | sequence of SourceModel | None
+            The requested sequence of source models.
+            If set to ``None``, all source models will be requested.
 
         Returns
         -------
-        model_param_dict : dict
-            The dictionary holding the fixed and floating parameter names and
-            values of the single model.
+        src_model_idxs : numpy ndarray
+            The (N_sources,)-shaped 1D ndarray holding the indices of the
+            requested source models.
         """
-        # Create the list of parameter names such that floating parameters are
-        # before the fixed parameters.
-        model_param_names = np.concatenate(
-            (self._model_param_names[self._global_paramset.floating_params_mask],
-             self._model_param_names[self._global_paramset.fixed_params_mask]))
-
-        # Create a 1D (n_global_params,)-shaped ndarray holding the values of
-        # the floating and fixed parameters. Since we only have a single model,
-        # these values coincide with the parameter values of the single model.
-        model_param_values = np.concatenate((
-            global_floating_param_values,
-            self._global_paramset.fixed_param_values
-        ))
-        if(len(model_param_values) != len(self._model_param_names)):
-            raise ValueError('The number of parameter values (%d) does not '
-                'equal the number of parameter names (%d) for model "%s"!'%
-                (len(model_param_values), len(self._model_param_names),
-                 self._models[0].name))
+        # Get the model indices of all the source models.
+        src_model_idxs = np.arange(self.n_models)[self._source_model_mask]
 
-        model_param_dict = dict(
-            zip(model_param_names, model_param_values))
+        if sources is None:
+            return src_model_idxs
 
-        return model_param_dict
+        # Select only the source models of interest.
+        if isinstance(sources, SourceModel):
+            sources = [sources]
+        if not issequenceof(sources, SourceModel):
+            raise TypeError(
+                'The sources argument must be None, an instance of '
+                'SourceModel, or a sequence of SourceModel! '
+                f'Its type is {classname(sources)}')
 
+        src_selection_mask = np.zeros((len(src_model_idxs),), dtype=bool)
+        for smidx in src_model_idxs:
+            src = self._models[smidx]
+            if src in sources:
+                src_selection_mask[smidx] = True
 
-class MultiModelParameterMapper(ModelParameterMapper):
-    """This class provides a model parameter mapper for multiple models, like
-    multiple sources, or multiple background models.
-    """
-    def __init__(self, name, models):
-        """Constructs a new multi model parameter mapper for mapping global
-        parameters to the given models.
+        src_model_idxs = src_model_idxs[src_selection_mask]
 
-        Parameters
-        ----------
-        name : str
-            The name of the model parameter mapper. In practice this is a
-            representative name for the set of global parameters this model
-            parameter mapper holds. For a two-component signal-background
-            likelihood model, "signal", or "background" could be useful names.
-        models : sequence of Model instances.
-            The sequence of Model instances the parameter mapper can
-            map global parameters to.
-        """
-        super(MultiModelParameterMapper, self).__init__(
-            name=name, models=models)
-
-    def def_param(self, param, model_param_name=None, models=None):
-        """Adds the given Parameter instance to this parameter mapper and maps
-        the parameter to the given sequence of models this model parameter
-        mapper knows about.
+        return src_model_idxs
+
+    def map_param(self, param, models=None, model_param_names=None):
+        """Maps the given instance of Parameter to the given sequence of models
+        this parameter model mapper knows about. Aliases for the given parameter
+        can be specified for each individual model.
 
         Parameters
         ----------
         param : instance of Parameter
-            The global parameter which should get mapped to one or multiple
-            models.
-        model_param_name : str | None
-            The parameter name of the models. The parameter name of the models
-            must be the same for all the models this global parameter should get
-            mapped to. The global parameter name can be different to the
-            parameter name of the models.
-            If set to `None`, the name of the global parameter will be used as
-            model parameter name.
-        models : sequence of Model instances | None
-            The sequence of Model instances the parameter should get mapped to.
-            The instances in the sequence must match Model instances specified
-            at construction of this mapper.
-            If set to `None` the global parameter will be mapped to all known
-            models.
+            The global parameter which should get mapped to one or more models.
+        models : sequence of Model instances
+            The sequence of Model instances the parameter should get
+            mapped to. The instances in the sequence must match Model instances
+            specified at construction of this mapper.
+        model_param_names : str | sequence of str |  None
+            The name of the parameter of the model. Hence, the global
+            parameter name can be different to the parameter name of the model.
+            If `None`, the name of the global parameter will be used as model
+            parameter name for all models.
 
         Returns
         -------
-        self : MultiModelParameterMapper
-            The instance of this MultiModelParameterMapper, so that several
-            `def_param` calls can be concatenated.
+        self : ParameterModelMapper
+            The instance of this ParameterModelMapper, so that several
+            `map_param` calls can be concatenated.
 
         Raises
         ------
         KeyError
             If there is already a model parameter of the same name defined for
             any of the given to-be-applied models.
         """
-        if(model_param_name is None):
-            model_param_name = param.name
-        if(not isinstance(model_param_name, str)):
-            raise TypeError('The model_param_name argument must be None or of '
-                'type str!')
+        if model_param_names is None:
+            model_param_names = np.array([param.name]*len(self._models))
+        if isinstance(model_param_names, str):
+            model_param_names = np.array([model_param_names]*len(self._models))
+        if not issequenceof(model_param_names, str):
+            raise TypeError(
+                'The model_param_names argument must be None, an instance of '
+                'str, or a sequence of instances of str!')
 
-        if(models is None):
+        if models is None:
             models = self._models
-        models = ModelCollection.cast(models,
+        models = ModelCollection.cast(
+            models,
             'The models argument must be castable to an instance of '
             'ModelCollection!')
         # Make sure that the user did not provide an empty sequence.
-        if(len(models) == 0):
-            raise ValueError('The sequence of models, to which the parameter '
-                'maps, cannot be empty!')
+        if len(models) == 0:
+            raise ValueError(
+                'The sequence of models, to which the parameter maps, cannot '
+                'be empty!')
 
         # Get the list of model indices to which the parameter maps.
         mask = np.zeros((self.n_models,), dtype=np.bool_)
-        for ((midx,model), applied_model) in itertools.product(
+        for ((midx, model), applied_model) in itertools.product(
                 enumerate(self._models), models):
-            if(applied_model.id == model.id):
+            if applied_model.id == model.id:
                 mask[midx] = True
 
         # Check that the model parameter name is not already defined for any of
         # the given to-be-mapped models.
-        model_indices = np.arange(self.n_models)[mask]
-        for midx in model_indices:
-            param_mask = self._global_param_2_model_mask[:,midx]
-            if(model_param_name in self._model_param_names[param_mask]):
-                raise KeyError('The model parameter "%s" is already defined '
-                    'for model "%s"!'%(model_param_name,
-                    self._models[midx].name))
+        for midx in np.arange(self.n_models)[mask]:
+            mpnames = self._model_param_names[midx][
+                self._model_param_names[midx] != np.array(None)]
+            if model_param_names[midx] in mpnames:
+                raise KeyError(
+                    f'The model parameter "{model_param_names[midx]}" is '
+                    f'already defined for model "{self._models[midx].name}"!')
 
         self._global_paramset.add_param(param)
-        self._model_param_names = np.concatenate(
-            (self._model_param_names, [model_param_name]))
 
-        self._global_param_2_model_mask = np.vstack(
-            (self._global_param_2_model_mask, mask))
+        entry = np.where(mask, model_param_names, None)
+        self._model_param_names = np.hstack(
+            (self._model_param_names, entry[np.newaxis, :].T))
 
         return self
 
-    def get_model_param_dict(
-            self, global_floating_param_values, model_idx):
+    def create_model_params_dict(self, gflp_values, model):
         """Creates a dictionary with the fixed and floating parameter names and
         their values for the given model.
 
         Parameters
         ----------
-        global_floating_param_values : 1D ndarray instance
+        gflp_values : 1D ndarray of float
             The ndarray instance holding the current values of the global
             floating parameters.
-        model_idx : int
+        model : instance of Model | str | int
             The index of the model as it was defined at construction
-            time of this ModelParameterMapper instance.
+            time of this ParameterModelMapper instance.
 
         Returns
         -------
         model_param_dict : dict
             The dictionary holding the fixed and floating parameter names and
             values of the specified model.
         """
+        gflp_values = np.atleast_1d(gflp_values)
+
+        if isinstance(model, str):
+            midx = self.get_model_idx_by_name(name=model)
+        elif isinstance(model, Model):
+            midx = self.get_model_idx_by_name(name=model.name)
+        else:
+            midx = int_cast(
+                model,
+                'The model argument must be an instance of Model, str, or '
+                'castable to int!')
+            if midx < 0 or midx >= len(self._models):
+                raise IndexError(
+                    f'The model index {midx} is out of range '
+                    f'[0,{len(self._models)-1}]!')
+
         # Get the model parameter mask that masks the global parameters for
         # the requested model.
-        model_mask = self._global_param_2_model_mask[:,model_idx]
+        m_gp_mask = self._model_param_names[midx] != np.array(None)
 
-        # Create the array of parameter names that belong to the requested
-        # model, where floating parameters are before the fixed parameters.
-        model_param_names = np.concatenate(
-            (self._model_param_names[
-                self._global_paramset.floating_params_mask & model_mask],
-             self._model_param_names[
-                self._global_paramset.fixed_params_mask & model_mask]
-            ))
+        _model_param_names = self._model_param_names
+        _global_paramset = self._global_paramset
+        gflp_mask = _global_paramset.floating_params_mask
+        gfxp_mask = _global_paramset.fixed_params_mask
+
+        # Create the array of local parameter names that belong to the
+        # requested model, where the floating parameters are before the fixed
+        # parameters.
+        model_param_names = np.concatenate((
+            _model_param_names[
+                midx,
+                gflp_mask & m_gp_mask],
+            _model_param_names[
+                midx,
+                gfxp_mask & m_gp_mask]
+        ))
 
         # Create the array of parameter values that belong to the requested
         # model, where floating parameters are before the fixed parameters.
         model_param_values = np.concatenate((
-            global_floating_param_values[
-                model_mask[self._global_paramset.floating_params_mask]],
-            self._global_paramset.fixed_param_values[
-                model_mask[self._global_paramset.fixed_params_mask]]
+            gflp_values[m_gp_mask[gflp_mask]],
+            _global_paramset.fixed_param_values[m_gp_mask[gfxp_mask]]
         ))
 
         model_param_dict = dict(
             zip(model_param_names, model_param_values))
 
         return model_param_dict
 
-
-class HypoParameterDefinition(NamedObjectCollection):
-    """This class provides a data holder for a list of model parameter mappers,
-    where each parameter mapper defines a set of global parameters for the
-    likelihood function, and their mapping to local model parameters.
-    In addition this class provides a method to create a copy of itself, where
-    floating parameters can get fixed to a certain values.
-    """
-    def __init__(self, model_param_mappers):
-        """Creates a new instance of HypoParameterDefinition with the given list
-        of ModelParameterMapper instances.
-
-        Parameters
-        ----------
-        model_param_mappers : instance of ModelParameterMapper | sequence of
-                ModelParameterMapper instances
-            The list of ModelParameterMapper instances defining the global
-            parameters and their mapping to local parameters of individual
-            models.
-        """
-        super(HypoParameterDefinition, self).__init__(
-            model_param_mappers, obj_type=ModelParameterMapper)
-
-        # Finalize all ModelParameterMapper instances, hence no parameters can
-        # be added anymore.
-        for mapper in self._objects:
-            mapper.finalize()
-
-    @property
-    def model_param_mapper_list(self):
-        """(read-only) The list of ModelParameterMapper instances defining the
-        global parameters and their mapping to the individual local model
-        parameters.
-        """
-        return self._objects
-
-    def __str__(self):
-        """Creates a pretty string representation of this
-        HypoParameterDefinition instance.
-        """
-        s = '%s:\n'%(classname(self))
-
-        for (idx, mapper) in enumerate(self._objects):
-            if(idx > 0):
-                s += '\n'
-            s1 = str(mapper)
-            s += display.add_leading_text_line_padding(
-                display.INDENTATION_WIDTH, s1)
-
-        return s
-
-    def copy(self, fix_params=None):
-        """Creates a deep copy of this HypoParameterDefinition instance and
-        fixes the given global parameters to the given values.
-
-        Parameters
-        ----------
-        fix_params : dict | None
-            The dictionary defining the global parameters that should get fixed
-            in the copy.
-
-        Returns
-        -------
-        copy : instance of HypoParameterDefinition
-            The copy of this HypoParameterDefinition instance with the given
-            global parameters fixed to the given values.
-        """
-        copy = deepcopy(self)
-
-        if(fix_params is not None):
-            if(not isinstance(fix_params, dict)):
-                raise TypeError('The fix_params argument must be of type dict!')
-
-            for mp_mapper in copy.model_param_mapper_list:
-                mp_mapper.global_paramset.make_params_fixed(fix_params)
-
-        return copy
-
-    def create_ParameterSetArray(self):
-        """Creates a ParameterSetArray instance for all the ModelParameterMapper
-        instances of this HypoParameterDefinition instance.
-
-        Returns
-        -------
-        paramsetarray : ParameterSetArray
-            The instance of ParameterSetArray holding references to the
-            ParameterSet instances of all the ModelParameterMapper instances of
-            this HypoParameterDefinition instance.
-        """
-        paramsetarray = ParameterSetArray(
-            [mpmapper.global_paramset
-             for mpmapper in self._objects])
-        return paramsetarray
-
-
-class FitParameter(object):
-    """This class is DEPRECATED! Use class Parameter instead!
-
-    This class describes a single fit parameter. A fit parameter has a name,
-    a value range, an initial value, and a current value. The current value will
-    be updated in the fitting process.
-    """
-    def __init__(self, name, valmin, valmax, initial):
-        """Creates a new fit parameter object.
-
-        Parameters
-        ----------
-        name : str
-            The name of the fit parameter.
-        valmin : float
-            The minimal bound value of the fit parameter.
-        valmax : float
-            The maximal bound value of the fit parameter.
-        initial : float
-            The (initial) value (guess) of the parameter, which will be used as
-            start point for the fitting procedure.
-        """
-        self.name = name
-        self.valmin = valmin
-        self.valmax = valmax
-        self.initial = initial
-
-        self.value = self.initial
-
-    @property
-    def name(self):
-        """The name of the fit parameter.
-        """
-        return self._name
-    @name.setter
-    def name(self, name):
-        if(not isinstance(name, str)):
-            raise TypeError('The name property must be of type str!')
-        self._name = name
-
-    @property
-    def valmin(self):
-        """The minimal bound value of the fit parameter.
-        """
-        return self._valmin
-    @valmin.setter
-    def valmin(self, v):
-        v = float_cast(v, 'The valmin property must castable to type float!')
-        self._valmin = v
-
-    @property
-    def valmax(self):
-        """The maximal bound value of the fit parameter.
-        """
-        return self._valmax
-    @valmax.setter
-    def valmax(self, v):
-        v = float_cast(v, 'The valmax property must be castable to type float!')
-        self._valmax = v
-
-    @property
-    def initial(self):
-        """The initial value of the fit parameter.
-        """
-        return self._initial
-    @initial.setter
-    def initial(self, v):
-        v = float_cast(v, 'The initial property must be castable to type float!')
-        self._initial = v
-
-    def as_linear_grid(self, delta):
-        """Creates a ParameterGrid instance with a linear grid with constant
-        grid value distances delta.
-
-        Parameters
-        ----------
-        delta : float
-            The constant distance between the grid values. By definition this
-            defines also the precision of the parameter values.
-
-        Returns
-        -------
-        grid : ParameterGrid instance
-            The ParameterGrid instance holding the grid values.
-        """
-        delta = float_cast(
-            delta, 'The delta argument must be castable to type float!')
-        grid = make_linear_parameter_grid_1d(
-            self._name, self._valmin, self._valmax, delta)
-        return grid
-
-
-class FitParameterSet(object):
-    """This class is DEPRECATED, use ParameterSet instead!
-
-    This class describes a set of FitParameter instances.
-    """
-    def __init__(self):
-        """Constructs a fit parameter set instance.
-        """
-        # Define the list of fit parameters.
-        # Define the (N_fitparams,)-shaped numpy array of FitParameter objects.
-        self._fitparams = np.empty((0,), dtype=np.object_)
-        # Define a list for the fit parameter names. This is for optimization
-        # purpose only.
-        self._fitparam_name_list = []
-
-    @property
-    def fitparams(self):
-        """The 1D ndarray holding the FitParameter instances.
-        """
-        return self._fitparams
-
-    @property
-    def fitparam_list(self):
-        """(read-only) The list of the global FitParameter instances.
-        """
-        return list(self._fitparams)
-
-    @property
-    def fitparam_name_list(self):
-        """(read-only) The list of the fit parameter names.
-        """
-        return self._fitparam_name_list
-
-    @property
-    def initials(self):
-        """(read-only) The 1D ndarray holding the initial values of all the
-        global fit parameters.
-        """
-        return np.array([ fitparam.initial
-                         for fitparam in self._fitparams ], dtype=np.float64)
-
-    @property
-    def bounds(self):
-        """(read-only) The 2D (N_fitparams,2)-shaped ndarray holding the
-        boundaries for all the global fit parameters.
-        """
-        return np.array([ (fitparam.valmin, fitparam.valmax)
-                         for fitparam in self._fitparams ], dtype=np.float64)
-
-    def copy(self):
-        """Creates a deep copy of this FitParameterSet instance.
-
-        Returns
-        -------
-        copy : FitParameterSet instance
-            The copied instance of this FitParameterSet instance.
-        """
-        copy = deepcopy(self)
-        return copy
-
-    def add_fitparam(self, fitparam, atfront=False):
-        """Adds the given FitParameter instance to the list of fit parameters.
-
-        Parameters
-        ----------
-        fitparam : instance of FitParameter
-            The fit parameter, which should get added.
-        atfront : bool
-            Flag if the fit parameter should be added at the front of the
-            parameter list. If set to False (default), it will be added at the
-            back.
-        """
-        if(not isinstance(fitparam, FitParameter)):
-            raise TypeError('The fitparam argument must be an instance of FitParameter!')
-
-        if(atfront):
-            # Add fit parameter at front of list.
-            self._fitparams = np.concatenate(([fitparam], self._fitparams))
-            self._fitparam_name_list = [fitparam.name] + self._fitparam_name_list
+    def create_src_params_recarray(
+            self,
+            gflp_values=None,
+            sources=None):
+        """Creates a numpy record ndarray with a field for each local source
+        parameter name and parameter's value. In addition each parameter field
+        ``<name>`` has a field named ``<<name>:gpidx>`` which holds the index
+        plus one of the corresponding global parameter for each source value.
+        For values mapping to fixed parameters, the index is negative. Local
+        parameter values that do not apply to a particular source are set to
+        NaN. The parameter index in such cases is undefined.
+        In addition to the parameter fields, the field ``:model_idx`` holds the
+        index of the model for which the local parameter values apply.
+
+        Parameters
+        ----------
+        gflp_values : numpy ndarray | None
+            The (N_global_floating_param,)-shaped 1D ndarray holding the global
+            floating parameter values. The order must match the order of
+            parameter definition in this ParameterModelMapper instance.
+            If set to ``None``, the value ``numpy.nan`` will be used as
+            parameter value for floating parameters.
+        sources : SourceModel | sequence of SourceModel | ndarray of int32 | None
+            The sources which should be considered.
+            If a ndarray of type int is provides, it must contain the global
+            source indices.
+            If set to ``None``, all sources are considered.
+
+        Returns
+        -------
+        recarray : numpy structured ndarray
+            The (N_sources,)-shaped numpy structured ndarray holding the local
+            parameter names and their values for each requested source.
+            It contains the following fields:
+
+                :model_idx
+                    The field holding the index of the model to which the set
+                    of local parameters apply.
+                <name>
+                    The field holding the value for the local parameter <name>.
+                    Not all local parameters apply to all sources.
+                    Example: "gamma".
+                <name>:gpidx
+                    The field holding the global parameter index plus one for
+                    the local parameter <name>. Example: "gamma:gpidx". Indices
+                    for values mapping to fixed parameters are negative.
+        """
+        if gflp_values is None:
+            gflp_values = np.full((self.n_global_floating_params,), np.nan)
+
+        gflp_values = np.atleast_1d(gflp_values)
+
+        # Check input.
+        n_global_floating_params = self.n_global_floating_params
+        if len(gflp_values) != n_global_floating_params:
+            raise ValueError(
+                f'The gflp_values argument is of length '
+                f'{len(gflp_values)}, but must be of length '
+                f'{n_global_floating_params}!')
+
+        if isinstance(sources, np.ndarray) and sources.dtype == np.int32:
+            # The sources are already specified in terms of their source
+            # indices.
+            smidxs = sources
         else:
-            # Add fit parameter at back of list.
-            self._fitparams = np.concatenate((self._fitparams, [fitparam]))
-            self._fitparam_name_list = self._fitparam_name_list + [fitparam.name]
-
-    def fitparam_values_to_dict(self, fitparam_values):
-        """Converts the given fit parameter values into a dictionary with the
-        fit parameter names and values.
-
-        Parameters
-        ----------
-        fitparam_values : 1D ndarray
-            The ndarray holding the fit parameter values in the order that the
-            fit parameters are defined.
-
-        Returns
-        -------
-        fitparam_dict : dict
-            The dictionary with the fit parameter names and values.
-        """
-        fitparam_dict = dict(zip(self._fitparam_name_list, fitparam_values))
-        return fitparam_dict
-
-    def fitparam_dict_to_values(self, fitparam_dict):
-        """Converts the given fit parameter dictionary into a 1D ndarray holding
-        the fit parameter values in the order the fit parameters are defined.
-
-        Parameters
-        ----------
-        fitparam_dict : dict
-            The dictionary with the fit parameter names and values.
-
-        Returns
-        -------
-        fitparam_values : 1D ndarray
-            The ndarray holding the fit parameter values in the order that the
-            fit parameters are defined.
-        """
-        fitparam_values = np.empty_like(self._fitparams, dtype=np.float64)
-        for (i, fitparam) in enumerate(self._fitparams):
-            fitparam_values[i] = fitparam_dict[fitparam.name]
-        return fitparam_values
-
-    def generate_random_initials(self, rss):
-        """Generates a set of random initials for all global fit parameters.
-        A new random initial is defined as
-
-            lower_bound + RAND * (upper_bound - lower_bound),
-
-        where RAND is a uniform random variable between 0 and 1.
-
-        Parameters
-        ----------
-        rss : RandomStateService instance
-            The RandomStateService instance that should be used for drawing
-            random numbers from.
-        """
-        vb = self.bounds
-        # Do random_initial = lower_bound + RAND * (upper_bound - lower_bound)
-        ri = vb[:,0] + rss.random.uniform(size=vb.shape[0])*(vb[:,1] - vb[:,0])
-
-        return ri
-
-
-class SourceFitParameterMapper(object, metaclass=abc.ABCMeta):
-    """This abstract base class defines the interface of the source fit
-    parameter mapper. This mapper provides the functionality to map a global fit
-    parameter to a source fit parameter.
-    """
-
-    def __init__(self):
-        """Constructor of the source fit parameter mapper.
-        """
-        self._fitparamset = FitParameterSet()
-
-        # Define the list of source parameter names, which map to the fit
-        # parameters.
-        # Define the (N_fitparams,)-shaped numpy array of str objects.
-        self._src_param_names = np.empty((0,), dtype=np.object_)
-
-    @property
-    def fitparamset(self):
-        """(read-only) The FitParameterSet instance holding the list of global
-        fit parameters.
-        """
-        return self._fitparamset
-
-    @property
-    def n_global_fitparams(self):
-        """(read-only) The number of defined global fit parameters.
-        """
-        return len(self._fitparamset.fitparams)
-
-    def get_src_fitparam_name(self, fitparam_idx):
-        """Returns the name of the source fit parameter for the given global fit
-        parameter index.
-
-        Parameters
-        ----------
-        fitparam_idx : int
-            The index of the global fit parameter.
-
-        Returns
-        -------
-        src_fitparam_name : str
-            The name of the source fit parameter.
-        """
-        return self._src_param_names[fitparam_idx]
+            # Get the source indices of the requested sources.
+            smidxs = self.get_src_model_idxs(sources=sources)
 
-    @abc.abstractmethod
-    def def_fit_parameter(self, fit_param, src_param_name=None, sources=None):
-        """This method is supposed to define a new fit parameter that maps to a
-        given source fit parameter for a list of sources. If no list of sources
-        is given, it maps to all sources.
-
-        Parameters
-        ----------
-        fit_param : FitParameter
-            The FitParameter instance defining the fit parameter.
-        src_param_name : str | None
-            The name of the source parameter. It must match the name of a source
-            model property. If set to None (default) the name of the fit
-            parameter will be used.
-        sources : sequence of SourceModel | None
-            The sequence of SourceModel instances for which the fit parameter
-            applies. If None (the default) is specified, the fit parameter will
-            apply to all sources.
-        """
-        pass
-
-    @abc.abstractmethod
-    def get_src_fitparams(self, fitparam_values, src_idx=0):
-        """This method is supposed to create a dictionary of source fit
-        parameter names and values for the requested source based on the given
-        fit parameter values.
-
-        Parameters
-        ----------
-        fitparam_values : 1D ndarray
-            The array holding the current global fit parameter values.
-        src_idx : int
-            The index of the source for which the parameters should get
-            retrieved.
-
-        Returns
-        -------
-        src_fitparams : dict
-            The dictionary holding the translated source parameters that are
-            beeing fitted.
-        """
-        pass
-
-    @abc.abstractmethod
-    def get_fitparams_array(self, fitparam_values):
-        """This method is supposed to create a numpy record ndarray holding the
-        unique source fit parameter names as key and their value for each
-        source. The returned array must be (N_sources,)-shaped.
-
-        Parameters
-        ----------
-        fitparam_values : 1D ndarray
-            The array holding the current global fit parameter values.
-
-        Returns
-        -------
-        fitparams_arr : (N_sources,)-shaped numpy record ndarray | None
-            The numpy record ndarray holding the fit parameter names as keys
-            and their value for each source in each row.
-            None must be returned if no global fit parameters were defined.
-        """
-        pass
-
-
-class SingleSourceFitParameterMapper(SourceFitParameterMapper):
-    """This class provides the functionality to map the global fit parameters to
-    the source fit parameters of the single source. This class assumes a single
-    source, hence the mapping can be performed faster than in the multi-source
-    case.
-    """
-    def __init__(self):
-        """Constructs a new source fit parameter mapper for a single source.
-        """
-        super(SingleSourceFitParameterMapper, self).__init__()
-
-    def def_fit_parameter(self, fitparam, src_param_name=None):
-        """Define a new fit parameter that maps to a given source fit parameter.
-
-        Parameters
-        ----------
-        fitparam : FitParameter
-            The FitParameter instance defining the fit parameter.
-        src_param_name : str | None
-            The name of the source parameter. It must match the name of a source
-            model property. If set to None (default) the name of the fit
-            parameter will be used.
-        """
-        self._fitparamset.add_fitparam(fitparam)
-
-        if(src_param_name is None):
-            src_param_name = fitparam.name
-        if(not isinstance(src_param_name, str)):
-            raise TypeError('The src_param_name argument must be of type str!')
+        # Create the output record array with nan as default value.
+        dtype = [(':model_idx', np.int32)]
+        for name in self.unique_source_param_names:
+            dtype += [(name, np.float64), (f'{name}:gpidx', np.int32)]
+
+        recarray = np.zeros(
+            (len(smidxs),),
+            dtype=dtype)
+        for name in self.unique_source_param_names:
+            recarray[name] = np.nan
+
+        recarray[':model_idx'] = smidxs
+
+        # Loop over the requested sources.
+        _model_param_names = self._model_param_names
+        _global_paramset = self._global_paramset
+        gflp_mask = _global_paramset.floating_params_mask
+        gfxp_mask = _global_paramset.fixed_params_mask
+        for (i, smidx) in enumerate(smidxs):
+            # Get the mask that selects the global parameters for the requested
+            # source.
+            src_gp_mask = _model_param_names[smidx] != np.array(None)
 
-        # Append the source parameter name to the internal array.
-        self._src_param_names = np.concatenate((self._src_param_names, [src_param_name]))
+            # Create the array of local parameter names that belong to the
+            # requested model, where the floating parameters are before the
+            # fixed parameters.
+            model_param_names = np.concatenate((
+                _model_param_names[smidx, gflp_mask & src_gp_mask],
+                _model_param_names[smidx, gfxp_mask & src_gp_mask]
+            ))
 
-    def get_src_fitparams(self, fitparam_values):
-        """Create a dictionary of source fit parameter names and values based on
-        the given fit parameter values.
+            # Create the array of local parameter values that belong to the
+            # requested model, where the floating parameters are before the
+            # fixed parameters.
+            model_param_values = np.concatenate((
+                gflp_values[
+                    src_gp_mask[gflp_mask]],
+                _global_paramset.fixed_param_values[
+                    src_gp_mask[gfxp_mask]]
+            ))
 
-        Parameters
-        ----------
-        fitparam_values : 1D ndarray
-            The array holding the current global fit parameter values.
+            # Create the array of the global parameter indices.
+            gpidxs = np.arange(len(_global_paramset))
+            model_gp_idxs = np.concatenate((
+                gpidxs[gflp_mask & src_gp_mask] + 1,
+                -gpidxs[gfxp_mask & src_gp_mask] - 1,
+            ))
 
-        Returns
-        -------
-        src_fitparams : dict
-            The dictionary holding the translated source parameters that are
-            beeing fitted.
-            An empty dictionary is returned if no fit parameters were defined.
-        """
-        src_fitparams = dict(zip(self._src_param_names, fitparam_values))
+            # Loop over the local parameters of the source and fill the
+            # params record array.
+            for (name, value, gpidx) in zip(
+                    model_param_names, model_param_values, model_gp_idxs):
+                recarray[name][i] = value
+                recarray[f'{name}:gpidx'][i] = gpidx
 
-        return src_fitparams
+        return recarray
 
-    def get_fitparams_array(self, fitparam_values):
-        """Creates a numpy record ndarray holding the fit parameters names as
-        key and their value for each source. The returned array is (1,)-shaped
-        since there is only one source defined for this mapper class.
+    def create_global_params_dict(self, gflp_values):
+        """Converts the given global floating parameter values into a dictionary
+        holding the names and values of all floating and fixed parameters.
 
         Parameters
         ----------
-        fitparam_values : 1D ndarray
-            The array holding the current global fit parameter values.
+        gflp_values : numpy ndarray
+            The (n_global_floating_params,)-shaped 1D numpy ndarray holding the
+            values of the global floating parameters.
 
         Returns
         -------
-        fitparams_arr : (1,)-shaped numpy record ndarray | None
-            The numpy record ndarray holding the fit parameter names as keys
-            and their value for the one single source.
-            None is returned if no fit parameters were defined.
-        """
-        if(self.n_global_fitparams == 0):
-            return None
-
-        fitparams_arr = np.array([tuple(fitparam_values)],
-                                 dtype=[ (name, np.float64)
-                                        for name in self._src_param_names ])
-        return fitparams_arr
-
-
-class MultiSourceFitParameterMapper(SourceFitParameterMapper):
-    """This class provides the functionality to map the global fit parameters to
-    the source fit parameters of the sources.
-    Sometimes it's necessary to define a global fit parameter, which relates to
-    a source model fit parameter for a set of sources, while another global fit
-    parameter relates to the same source model fit parameter, but for another
-    set of sources.
-
-    At construction time this manager takes the collection of sources. Each
-    source gets an index, which is defined as the position of the source within
-    the collection.
-    """
-    def __init__(self, sources):
-        """Constructs a new source fit parameter mapper for multiple sources.
-
-        Parameters
-        ----------
-        sources : sequence of SourceModel
-            The sequence of SourceModel instances defining the list of sources.
-        """
-        super(MultiSourceFitParameterMapper, self).__init__()
-
-        self.sources = sources
-
-        # (N_fitparams, N_sources) shaped boolean ndarray defining what fit
-        # parameter applies to which source.
-        self._fit_param_2_src_mask = np.zeros(
-            (0, len(self.sources)), dtype=np.bool_)
-
-        # Define an array, which will hold the unique source parameter names.
-        self._unique_src_param_names = np.empty((0,), dtype=np.object_)
-
-    @property
-    def sources(self):
-        """The SourceCollection defining the sources.
+        params_dict : dict
+            The dictionary holding the parameter name and values of all
+            floating and fixed parameters.
         """
-        return self._sources
-    @sources.setter
-    def sources(self, obj):
-        obj = SourceCollection.cast(obj, 'The sources property must be castable to an instance of SourceCollection!')
-        self._sources = obj
+        params_dict = self._global_paramset.get_params_dict(
+            floating_param_values=gflp_values)
 
-    @property
-    def N_sources(self):
-        """(read-only) The number of sources.
-        """
-        return len(self._sources)
-
-    def def_fit_parameter(self, fitparam, src_param_name=None, sources=None):
-        """Defines a new fit parameter that maps to a given source parameter
-        for a list of sources. If no list of sources is given, it maps to all
-        sources.
-
-        Parameters
-        ----------
-        fitparam : FitParameter
-            The FitParameter instance defining the fit parameter.
-        src_param_name : str | None
-            The name of the source parameter. It must match the name of a source
-            model property. If set to None (default) the name of the fit
-            parameter will be used.
-        sources : SourceCollection | None
-            The instance of SourceCollection with the sources for which the fit
-            parameter applies. If None (the default) is specified, the fit
-            parameter will apply to all sources.
-        """
-        self._fitparamset.add_fitparam(fitparam)
-
-        if(src_param_name is None):
-            src_param_name = fitparam.name
-        if(not isinstance(src_param_name, str)):
-            raise TypeError('The src_param_name argument must be of type str!')
-
-        if(sources is None):
-            sources = self.sources
-        sources = SourceCollection.cast(sources,
-            'The sources argument must be castable to an instance of SourceCollection!')
+        return params_dict
 
-        # Append the source parameter name to the internal array and keep track
-        # of the unique names.
-        self._src_param_names = np.concatenate((self._src_param_names, [src_param_name]))
-        self._unique_src_param_names = np.unique(self._src_param_names)
-
-        # Get the list of source indices for which the fit parameter applies.
-        mask = np.zeros((len(self.sources),), dtype=np.bool_)
-        for ((idx,src), applied_src) in itertools.product(enumerate(self.sources), sources):
-            if(applied_src.id == src.id):
-                mask[idx] = True
-        self._fit_param_2_src_mask = np.vstack((self._fit_param_2_src_mask, mask))
-
-    def get_src_fitparams(self, fitparam_values, src_idx):
-        """Constructs a dictionary with the source parameters that are beeing
-        fitted. As values the given global fit parameter values will be used.
-        Hence, this method translates the global fit parameter values into the
-        source parameters.
+    def create_global_floating_params_dict(self, gflp_values):
+        """Converts the given global floating parameter values into a dictionary
+        holding the names and values of all floating parameters.
 
         Parameters
         ----------
-        fitparam_values : 1D ndarray
-            The array holding the current global fit parameter values.
-        src_idx : int
-            The index of the source for which the parameters should get
-            retieved.
+        gflp_values : numpy ndarray
+            The (n_global_floating_params,)-shaped 1D numpy ndarray holding the
+            values of the global floating parameters.
 
         Returns
         -------
-        src_fitparams : dict
-            The dictionary holding the translated source parameters that are
-            beeing fitted.
+        params_dict : dict
+            The dictionary holding the parameter name and values of all
+            floating parameters.
         """
-        # Get the mask of global fit parameters that apply to the requested
-        # source.
-        fp_mask = self._fit_param_2_src_mask[:,src_idx]
-
-        # Get the source parameter names and values.
-        src_param_names = self._src_param_names[fp_mask]
-        src_param_values = fitparam_values[fp_mask]
-
-        src_fitparams = dict(zip(src_param_names, src_param_values))
+        params_dict = self._global_paramset.get_floating_params_dict(
+            floating_param_values=gflp_values)
 
-        return src_fitparams
+        return params_dict
 
-    def get_fitparams_array(self, fitparam_values):
-        """Creates a numpy record ndarray holding the fit parameters names as
-        key and their value for each source. The returned array is
-        (N_sources,)-shaped.
+    def get_local_param_is_global_floating_param_mask(
+            self,
+            local_param_names,
+    ):
+        """Checks which local parameter name is mapped to a global floating
+        parameter.
 
         Parameters
         ----------
-        fitparam_values : 1D ndarray
-            The array holding the current global fit parameter values.
+        local_param_names : sequence of str
+            The sequence of the local parameter names to test.
 
         Returns
         -------
-        fitparams_arr : (N_sources,)-shaped numpy record ndarray | None
-            The numpy record ndarray holding the unique source fit parameter
-            names as keys and their value for each source per row.
-            None is returned if no fit parameters were defined.
+        mask : instance of ndarray
+            The (N_local_param_names,)-shaped numpy ndarray holding the mask
+            for each local parameter name if it is mapped to a global floating
+            parameter.
         """
-        if(self.n_global_fitparams == 0):
-            return None
-
-        fitparams_arr = np.empty((self.N_sources,),
-                                 dtype=[ (name, np.float64)
-                                         for name in self._unique_src_param_names ])
-
-        for src_idx in range(self.N_sources):
-            # Get the mask of global fit parameters that apply to the requested
-            # source.
-            fp_mask = self._fit_param_2_src_mask[:,src_idx]
-
-            # Get the source parameter names and values.
-            src_param_names = self._src_param_names[fp_mask]
-            src_param_values = fitparam_values[fp_mask]
+        mask = np.zeros(len(local_param_names), dtype=np.bool_)
 
-            # Fill the fit params array.
-            for (name, value) in zip(src_param_names, src_param_values):
-                fitparams_arr[name][src_idx] = value
+        global_floating_params_idxs = self._global_paramset.floating_params_idxs
 
-        return fitparams_arr
+        # Get the global parameter indices for each local parameter name.
+        for (local_param_idx, local_param_name) in enumerate(local_param_names):
+            gpidxs = np.unique(
+                np.nonzero(self._model_param_names == local_param_name)[1]
+            )
+            if np.any(np.isin(gpidxs, global_floating_params_idxs)):
+                mask[local_param_idx] = True
 
+        return mask
```

### Comparing `skyllh-23.1.1/skyllh/core/pdfratio.py` & `skyllh-23.2.0/skyllh/core/pdfratio.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,897 +1,1075 @@
 # -*- coding: utf-8 -*-
 
 import abc
-import itertools
+
 import numpy as np
 
-from skyllh.core.py import (
-    classname,
-    float_cast,
-    issequenceof,
-    typename
-)
-from skyllh.core.parameters import (
-    FitParameter,
-    make_params_hash
+from skyllh.core.config import (
+    HasConfig,
 )
 from skyllh.core.interpolate import (
     GridManifoldInterpolationMethod,
-    Parabola1DGridManifoldInterpolationMethod
+    Parabola1DGridManifoldInterpolationMethod,
+)
+from skyllh.core.parameters import (
+    ParameterModelMapper,
 )
 from skyllh.core.pdf import (
-    PDF,
     PDFSet,
     IsBackgroundPDF,
     IsSignalPDF,
-    SpatialPDF,
-    TimePDF
 )
-from skyllh.core.timing import TaskTimer
+from skyllh.core.py import (
+    classname,
+    float_cast,
+    int_cast,
+    issequence,
+    issequenceof,
+)
+from skyllh.core.services import (
+    SrcDetSigYieldWeightsService,
+)
+from skyllh.core.timing import (
+    TaskTimer,
+)
 
 
-class PDFRatio(object, metaclass=abc.ABCMeta):
-    """Abstract base class for a PDF ratio class. It defines the interface
-    of a PDF ratio class.
+class PDFRatio(
+        HasConfig,
+        metaclass=abc.ABCMeta,
+):
+    """Abstract base class for a signal over background PDF ratio class.
+    It defines the interface of a signal over background PDF ratio class.
     """
 
-    def __init__(self, pdf_type, *args, **kwargs):
-        """Constructor for a PDF ratio class.
+    def __init__(
+            self,
+            sig_param_names=None,
+            bkg_param_names=None,
+            **kwargs,
+    ):
+        """Creates a new PDFRatio instance.
 
         Parameters
         ----------
-        pdf_type : type
-            The Python type of the PDF object the PDF ratio is made for.
+        sig_param_names : sequence of str | str | None
+            The sequence of signal parameter names this PDFRatio instance is a
+            function of.
+        bkg_param_names : sequence of str | str | None
+            The sequence of background parameter names this PDFRatio instance
+            is a function of.
         """
-        super(PDFRatio, self).__init__(*args, **kwargs)
+        super().__init__(**kwargs)
 
-        self._pdf_type = pdf_type
+        self.sig_param_names = sig_param_names
+        self.bkg_param_names = bkg_param_names
 
     @property
-    def n_fitparams(self):
-        """(read-only) The number of fit parameters the PDF ratio depends on.
-        This is the sum of signal and background fit parameters. At the moment
-        only signal fit parameters are supported, so this property is equivalent
-        to the n_signal_fitparams property. But this might change in the future.
+    def n_params(self):
+        """(read-only) The number of parameters the PDF ratio depends on.
+        This is the sum of signal and background parameters.
         """
-        return self.n_signal_fitparams
+        return self.n_sig_params + self.n_bkg_params
 
     @property
-    def fitparam_names(self):
-        """(read-only) The list of fit parameter names this PDF ratio is a
-        function of.
-        This is the superset of signal and background fit parameter names.
-        At the moment only signal fit parameters are supported, so this property
-        is equivalent to the signal_fitparam_names property. But this might
-        change in the future.
+    def param_names(self):
+        """(read-only) The list of parameter names this PDF ratio is a
+        function of. This is the superset of signal and background parameter
+        names.
         """
-        return self.signal_fitparam_names
+        return list(
+            set(list(self._sig_param_names) + list(self._bkg_param_names)))
 
     @property
-    def n_signal_fitparams(self):
-        """(read-only) The number of signal fit parameters the PDF ratio depends
+    def n_sig_params(self):
+        """(read-only) The number of signal parameters the PDF ratio depends
         on.
         """
-        return len(self._get_signal_fitparam_names())
+        return len(self._sig_param_names)
 
     @property
-    def signal_fitparam_names(self):
-        """(read-only) The list of signal fit parameter names this PDF ratio is
-        a function of.
+    def n_bkg_params(self):
+        """(read-only) The number of background parameters the PDF ratio depends
+        on.
         """
-        return self._get_signal_fitparam_names()
+        return len(self._bkg_param_names)
 
     @property
-    def pdf_type(self):
-        """(read-only) The Python type of the PDF object for which the PDF
-        ratio is made for.
+    def sig_param_names(self):
+        """The list of signal parameter names this PDF ratio is a function of.
         """
-        return self._pdf_type
+        return self._sig_param_names
 
-    def _get_signal_fitparam_names(self):
-        """This method must be re-implemented by the derived class and needs to
-        return the list of signal fit parameter names, this PDF ratio is a
-        function of. If it returns an empty list, the PDF ratio is independent
-        of any signal fit parameters.
+    @sig_param_names.setter
+    def sig_param_names(self, names):
+        if names is None:
+            names = []
+        if not issequence(names):
+            names = [names]
+        if not issequenceof(names, str):
+            raise TypeError(
+                'The sig_param_names property must be a sequence of str '
+                'instances!')
+        self._sig_param_names = names
 
-        Returns
-        -------
-        list of str
-            The list of the signal fit parameter names, this PDF ratio is a
-            function of. By default this method returns an empty list indicating
-            that the PDF ratio depends on no signal parameter.
+    @property
+    def bkg_param_names(self):
+        """The list of background parameter names this PDF ratio is a function
+        of.
+        """
+        return self._bkg_param_names
+
+    @bkg_param_names.setter
+    def bkg_param_names(self, names):
+        if names is None:
+            names = []
+        if not issequence(names):
+            names = [names]
+        if not issequenceof(names, str):
+            raise TypeError(
+                'The bkg_param_names property must be a sequence of str '
+                'instances!')
+        self._bkg_param_names = names
+
+    @abc.abstractmethod
+    def initialize_for_new_trial(
+            self,
+            tdm,
+            tl=None,
+            **kwargs,
+    ):
+        """Initializes the PDFRatio instance for a new trial. This method can
+        be utilized to pre-calculate PDFRatio values that do not depend on any
+        fit parameters.
+
+        Parameters
+        ----------
+        tdm : instance of TrialDataManager
+            The instance of TrialDataManager that holds the trial data.
+        tl : instance of TimeLord
+            The optional instance of TimeLord to measure timing information.
         """
-        return []
+        pass
 
     @abc.abstractmethod
-    def get_ratio(self, tdm, params=None, tl=None):
-        """Retrieves the PDF ratio value for each given trial data event, given
-        the given set of fit parameters. This method is called during the
-        likelihood maximization process.
+    def get_ratio(
+            self,
+            tdm,
+            src_params_recarray,
+            tl=None,
+    ):
+        """Retrieves the PDF ratio value for each given trial data events (and
+        sources), given the given set of parameters.
 
         Parameters
         ----------
         tdm : instance of TrialDataManager
             The TrialDataManager instance holding the trial data events for
             which the PDF ratio values should get calculated.
-        params : dict | None
-            The dictionary with the parameter name-value pairs.
-            It can be ``None``, if the PDF ratio does not depend on any
-            parameters.
-        tl : TimeLord instance | None
+        src_params_recarray : instance of numpy record ndarray | None
+            The (N_sources,)-shaped numpy record ndarray holding the parameter
+            names and values of the sources.
+            See the documentation of the
+            :meth:`skyllh.core.parameters.ParameterModelMapper.create_src_params_recarray`
+            method for more information.
+        tl : instance of TimeLord | None
             The optional TimeLord instance that should be used to measure
             timing information.
 
         Returns
         -------
-        ratios : (N_events,)-shaped 1d numpy ndarray of float
-            The PDF ratio value for each trial event.
+        ratios : instance of ndarray
+            The (N_values,)-shaped 1d numpy ndarray of float holding the PDF
+            ratio value for each trial event and source.
         """
         pass
 
     @abc.abstractmethod
-    def get_gradient(self, tdm, params, fitparam_name):
-        """Retrieves the PDF ratio gradient for the parameter ``fitparam_name``
-        for each given trial event, given the given set of fit parameters.
-        This method is called during the likelihood maximization process.
+    def get_gradient(
+            self,
+            tdm,
+            src_params_recarray,
+            fitparam_id,
+            tl=None,
+    ):
+        """Retrieves the PDF ratio gradient for the global fit parameter
+        ``fitparam_id`` for each trial data event and source, given the given
+        set of parameters ``src_params_recarray`` for each source.
 
         Parameters
         ----------
         tdm : instance of TrialDataManager
             The TrialDataManager instance holding the trial data events for
-            which the PDF ratio values should get calculated.
-        params : dict
-            The dictionary with the parameter names and values.
-        fitparam_name : str
-            The name of the fit parameter for which the gradient should
+            which the PDF ratio gradient values should get calculated.
+        src_params_recarray : instance of numpy structured ndarray
+            The (N_sources,)-shaped numpy structured ndarray holding the
+            parameter names and values of the sources.
+            See the documentation of the
+            :meth:`skyllh.core.parameters.ParameterModelMapper.create_src_params_recarray`
+            method for more information.
+        fitparam_id : int
+            The ID of the global fit parameter for which the gradient should
             get calculated.
+        tl : instance of TimeLord | None
+            The optional TimeLord instance that should be used to measure
+            timing information.
 
         Returns
         -------
-        gradient : (N_events,)-shaped 1d numpy ndarray of float
-            The PDF ratio gradient value for each trial event.
+        gradient : instance of ndarray | 0
+            The (N_values,)-shaped 1d numpy ndarray of float holding the PDF
+            ratio gradient value for each source and trial data event.
+            If the PDF ratio does not depend on the given global fit parameter,
+            0 should be returned.
         """
         pass
 
+    def __mul__(self, other):
+        """Implements the mathematical operation ``new = self * other``, where
+        ``other`` is an instance of PDFRatio. It creates an instance of
+        PDFRatioProduct holding the two PDFRatio instances.
+        """
+        return PDFRatioProduct(self, other, cfg=self.cfg)
 
-class SingleSourcePDFRatioArrayArithmetic(object):
-    """This class provides arithmetic methods for arrays of PDFRatio instances.
-    It has methods to calculate the product of the ratio values for a given set
-    of PDFRatio objects. This class assumes a single source.
-
-    The rational is that in the calculation of the derivates of the
-    log-likelihood-ratio function for a given fit parameter, the product of the
-    PDF ratio values of the PDF ratio objects which do not depend on that fit
-    parameter is needed.
+
+class PDFRatioProduct(
+        PDFRatio,
+):
+    """This is the mathematical product of two PDFRatio instances, which is a
+    PDFRatio instance again.
     """
-    def __init__(self, pdfratios, fitparams):
-        """Constructs a PDFRatio array arithmetic object assuming a single
-        source.
+    def __init__(
+            self,
+            pdfratio1,
+            pdfratio2,
+            **kwargs,
+    ):
+        """Creates a new PDFRatioProduct instance representing the product of
+        two PDFRatio instances.
+        """
+        self.pdfratio1 = pdfratio1
+        self.pdfratio2 = pdfratio2
+
+        sig_param_names = set(
+            list(pdfratio1.sig_param_names) + list(pdfratio2.sig_param_names))
+        bkg_param_names = set(
+            list(pdfratio1.bkg_param_names) + list(pdfratio2.bkg_param_names))
+
+        super().__init__(
+            sig_param_names=sig_param_names,
+            bkg_param_names=bkg_param_names,
+            **kwargs)
+
+    @property
+    def pdfratio1(self):
+        """The first PDFRatio instance in the muliplication
+        ``pdfratio1 * pdfratio2``.
+        """
+        return self._pdfratio1
+
+    @pdfratio1.setter
+    def pdfratio1(self, pdfratio):
+        if not isinstance(pdfratio, PDFRatio):
+            raise TypeError(
+                'The pdfratio1 property must be an instance of PDFRatio!')
+        self._pdfratio1 = pdfratio
+
+    @property
+    def pdfratio2(self):
+        """The second PDFRatio instance in the muliplication
+        ``pdfratio1 * pdfratio2``.
+        """
+        return self._pdfratio2
+
+    @pdfratio2.setter
+    def pdfratio2(self, pdfratio):
+        if not isinstance(pdfratio, PDFRatio):
+            raise TypeError(
+                'The pdfratio2 property must be an instance of PDFRatio!')
+        self._pdfratio2 = pdfratio
+
+    def initialize_for_new_trial(
+            self,
+            **kwargs):
+        """Initializes the PDFRatioProduct instance for a new trial.
+        It calls the
+        :meth:`~skyllh.core.pdfratio.PDFRatio.initialize_for_new_trial` method
+        of each of the two :class:`~skyllh.core.pdfratio.PDFRatio` instances.
+        """
+        self._pdfratio1.initialize_for_new_trial(**kwargs)
+        self._pdfratio2.initialize_for_new_trial(**kwargs)
+
+    def get_ratio(
+            self,
+            tdm,
+            src_params_recarray,
+            tl=None,
+    ):
+        """Retrieves the PDF ratio product value for each trial data
+        event and source, given the given set of parameters for all sources.
 
         Parameters
         ----------
-        pdfratios : list of PDFRatio
-            The list of PDFRatio instances.
-        fitparams : list of FitParameter
-            The list of fit parameters. The order must match the fit parameter
-            order of the minimizer.
-        """
-        super(SingleSourcePDFRatioArrayArithmetic, self).__init__()
-
-        self.pdfratio_list = pdfratios
-        self.fitparam_list = fitparams
+        tdm : instance of TrialDataManager
+            The TrialDataManager instance holding the trial data events for
+            which the PDF ratio values should get calculated.
+        src_params_recarray : instance of numpy record ndarray
+            The (N_sources,)-shaped numpy record ndarray holding the parameter
+            names and values of the sources.
+            See the documentation of the
+            :meth:`skyllh.core.parameters.ParameterModelMapper.create_src_params_recarray`
+            method for more information.
+        tl : TimeLord instance | None
+            The optional TimeLord instance that should be used to measure
+            timing information.
 
-        # The ``_ratio_values`` member variable will hold a
-        # (N_pdfratios,N_events)-shaped array holding the PDF ratio values of
-        # each PDF ratio object for each event. It will be created by the
-        # ``initialize_for_new_trial`` method.
-        self._ratio_values = None
+        Returns
+        -------
+        ratios : instance of ndarray
+            The (N_values,)-shaped 1d numpy ndarray of float holding the product
+            of the PDF ratio values for each trial event and source.
+            The PDF ratio product value for each trial event.
+        """
+        r1 = self._pdfratio1.get_ratio(
+            tdm=tdm,
+            src_params_recarray=src_params_recarray,
+            tl=tl)
+
+        r2 = self._pdfratio2.get_ratio(
+            tdm=tdm,
+            src_params_recarray=src_params_recarray,
+            tl=tl)
+
+        return r1 * r2
+
+    def get_gradient(
+            self,
+            tdm,
+            src_params_recarray,
+            fitparam_id,
+            tl=None,
+    ):
+        """Retrieves the PDF ratio product gradient for the global fit parameter
+        with parameter ID ``fitparam_id`` for each trial data event and source,
+        given the set of parameters ``src_params_recarray`` for all sources.
 
-        # Create a mapping of fit parameter index to pdfratio index. We
-        # initialize the mapping with -1 first in order to be able to check in
-        # the end if all fit parameters found a PDF ratio object.
-        self._fitparam_idx_2_pdfratio_idx = np.repeat(
-            np.array([-1], dtype=np.int64), len(self._fitparam_list))
-        for ((fpidx, fitparam), (pridx, pdfratio)) in itertools.product(
-                enumerate(self._fitparam_list), enumerate(self.pdfratio_list)):
-            if(fitparam.name in pdfratio.fitparam_names):
-                self._fitparam_idx_2_pdfratio_idx[fpidx] = pridx
-        check_mask = (self._fitparam_idx_2_pdfratio_idx == -1)
-        if(np.any(check_mask)):
-            raise KeyError('%d fit parameters are not defined in any of the '
-                'PDF ratio instances!'%(np.sum(check_mask)))
+        Parameters
+        ----------
+        tdm : instance of TrialDataManager
+            The TrialDataManager instance holding the trial data events for
+            which the PDF ratio values should get calculated.
+        src_params_recarray : instance of numpy record ndarray | None
+            The (N_sources,)-shaped numpy record ndarray holding the parameter
+            names and values of the sources.
+            See the documentation of the
+            :meth:`skyllh.core.parameters.ParameterModelMapper.create_src_params_recarray`
+            method for more information.
+        fitparam_id : int
+            The ID of the global fit parameter for which the gradient should
+            get calculated.
+        tl : instance of TimeLord | None
+            The optional TimeLord instance that should be used to measure
+            timing information.
 
-        # Create the list of indices of the PDFRatio instances, which depend on
-        # at least one fit parameter.
-        self._var_pdfratio_indices = np.unique(self._fitparam_idx_2_pdfratio_idx)
+        Returns
+        -------
+        gradient : instance of ndarray | 0
+            The (N_values,)-shaped 1d numpy ndarray of float holding the PDF
+            ratio gradient value for each trial event and source. If none of the
+            two PDFRatio instances depend on the given global fit parameter, the
+            scalar value ``0`` is returned.
+        """
+        r1_depends_on_fitparam =\
+            ParameterModelMapper.is_global_fitparam_a_local_param(
+                fitparam_id=fitparam_id,
+                params_recarray=src_params_recarray,
+                local_param_names=self._pdfratio1.param_names)
+
+        r2_depends_on_fitparam =\
+            ParameterModelMapper.is_global_fitparam_a_local_param(
+                fitparam_id=fitparam_id,
+                params_recarray=src_params_recarray,
+                local_param_names=self._pdfratio2.param_names)
+
+        if r1_depends_on_fitparam:
+            r2 = self._pdfratio2.get_ratio(
+                tdm=tdm,
+                src_params_recarray=src_params_recarray,
+                tl=tl)
+
+            r1_grad = self._pdfratio1.get_gradient(
+                tdm=tdm,
+                src_params_recarray=src_params_recarray,
+                fitparam_id=fitparam_id,
+                tl=tl)
+
+        if r2_depends_on_fitparam:
+            r1 = self._pdfratio1.get_ratio(
+                tdm=tdm,
+                src_params_recarray=src_params_recarray,
+                tl=tl)
+
+            r2_grad = self._pdfratio2.get_gradient(
+                tdm=tdm,
+                src_params_recarray=src_params_recarray,
+                fitparam_id=fitparam_id,
+                tl=tl)
+
+        if r1_depends_on_fitparam and r2_depends_on_fitparam:
+            gradient = r1 * r2_grad
+            gradient += r1_grad * r2
+        elif r1_depends_on_fitparam:
+            gradient = r1_grad * r2
+        elif r2_depends_on_fitparam:
+            gradient = r1 * r2_grad
+        else:
+            gradient = 0
+
+        return gradient
+
+
+class SourceWeightedPDFRatio(
+        PDFRatio):
+    r"""This class provides the calculation of a source weighted PDF ratio for
+    multiple sources:
+
+    .. math::
+
+        \mathcal{R}_i(\vec{p}_{\mathrm{s}}) = \frac{1}{A(\vec{p}_{\mathrm{s}})}
+            \sum_{k=1}^{K} a_k(\vec{p}_{\mathrm{s}_k}) \mathcal{R}_{i,k}
+            (\vec{p}_{\mathrm{s}_k})
 
-    def _precompute_static_pdfratio_values(self, tdm):
-        """Pre-compute the PDF ratio values for the PDF ratios that do not
-        depend on any fit parameters.
+    """
+    def __init__(
+            self,
+            dataset_idx,
+            src_detsigyield_weights_service,
+            pdfratio,
+            **kwargs):
+        """Creates a new SourceWeightedPDFRatio instance.
 
         Parameters
         ----------
-        tdm : instance of TrialDataManager
-            The instance of TrialDataManager that holds the trial event data for
-            which the PDF ratio values should get calculated.
-        """
-        for (i, pdfratio) in enumerate(self._pdfratio_list):
-            if(pdfratio.n_fitparams == 0):
-                # The PDFRatio does not depend on any fit parameters. So we
-                # pre-calculate the PDF ratio values for all the events. Since
-                # the get_ratio method of the PDFRatio class might return a 2D
-                # (N_sources, N_events)-shaped array, and we assume a single
-                # source, we need to reshape the array, which does not involve
-                # any data copying.
-                self._ratio_values[i] = np.reshape(
-                    pdfratio.get_ratio(tdm), (tdm.n_selected_events,))
+        dataset_idx : int
+            The index of the dataset. It is used to access the source detector
+            signal yield weight.
+        src_detsigyield_weights_service : instance of SrcDetSigYieldWeightsService
+            The instance of SrcDetSigYieldWeightsService providing the source
+            detector signal yield weights, i.e. the product of the theoretical
+            source weight with the detector signal yield.
+        pdfratio : instance of PDFRatio
+            The instance of PDFRatio providing the PDF ratio values and
+            derivatives.
+        """
+        if not isinstance(pdfratio, PDFRatio):
+            raise TypeError(
+                'The pdfratio argument must be an instance of PDFRatio! '
+                f'Its current type is {classname(pdfratio)}.')
+        self._pdfratio = pdfratio
+
+        super().__init__(
+            sig_param_names=self._pdfratio.sig_param_names,
+            bkg_param_names=self._pdfratio.bkg_param_names,
+            **kwargs)
+
+        self._dataset_idx = int_cast(
+            dataset_idx,
+            'The dataset_idx argument must be castable to type int!')
+
+        if not isinstance(
+                src_detsigyield_weights_service,
+                SrcDetSigYieldWeightsService):
+            raise TypeError(
+                'The src_detsigyield_weights_service argument must be an '
+                'instance of type SrcDetSigYieldWeightsService! '
+                'Its current type is '
+                f'{classname(src_detsigyield_weights_service)}.')
+        self._src_detsigyield_weights_service = src_detsigyield_weights_service
+
+        self._cache_R_ik = None
+        self._cache_R_i = None
 
     @property
-    def pdfratio_list(self):
-        """The list of PDFRatio objects.
+    def dataset_idx(self):
+        """(read-only) The index of the dataset for which this
+        SourceWeightedPDFRatio instance is made.
         """
-        return self._pdfratio_list
-    @pdfratio_list.setter
-    def pdfratio_list(self, seq):
-        if(not issequenceof(seq, PDFRatio)):
-            raise TypeError('The pdfratio_list property must be a sequence of '
-                'PDFRatio instances!')
-        self._pdfratio_list = list(seq)
+        return self._dataset_idx
 
     @property
-    def fitparam_list(self):
-        """The list of FitParameter instances.
+    def src_detsigyield_weights_service(self):
+        """(read-only) The instance of SrcDetSigYieldWeightsService providing
+        the source detector signal yield weights, i.e. the product of the
+        theoretical source weight with the detector signal yield.
         """
-        return self._fitparam_list
-    @fitparam_list.setter
-    def fitparam_list(self, seq):
-        if(not issequenceof(seq, FitParameter)):
-            raise TypeError('The fitparam_list property must be a sequence of '
-                'FitParameter instances!')
-        self._fitparam_list = list(seq)
+        return self._src_detsigyield_weights_service
 
-    def initialize_for_new_trial(self, tdm):
-        """Initializes the PDFRatio array arithmetic for a new trial. For a new
-        trial the data events change, hence we need to recompute the PDF ratio
-        values of the fit parameter independent PDFRatio instances.
+    @property
+    def pdfratio(self):
+        """(read-only) The PDFRatio instance that is used to calculate the
+        source weighted PDF ratio.
+        """
+        return self._pdfratio
+
+    def initialize_for_new_trial(
+            self,
+            tdm,
+            tl=None,
+            **kwargs):
+        """Initializes the PDFRatio instance for a new trial. It calls the
+        :meth:`~skyllh.core.pdfratio.PDFRatio.initialize_for_new_trial` method
+        of the :class:`~skyllh.core.pdfratio.PDFRatio` instance.
 
         Parameters
         ----------
         tdm : instance of TrialDataManager
-            The instance of TrialDataManager that holds the trial event data for
-            that this PDFRatioArrayArithmetic instance should get initialized.
-        """
-        n_events_old = 0
-        if(self._ratio_values is not None):
-            n_events_old = self._ratio_values.shape[1]
-
-        # If the amount of events have changed, we need a new array holding the
-        # ratio values.
-        if(n_events_old != tdm.n_selected_events):
-            # Create a (N_pdfratios,N_events)-shaped array to hold the PDF ratio
-            # values of each PDF ratio object for each event.
-            self._ratio_values = np.empty(
-                (len(self._pdfratio_list), tdm.n_selected_events),
-                dtype=np.float64)
-
-        self._precompute_static_pdfratio_values(tdm)
-
-    def get_pdfratio(self, idx):
-        """Returns the PDFRatio instance that corresponds to the given fit
-        parameter index.
-
-        Parameters
-        ----------
-        fitparam_idx : int
-            The index of the fit parameter.
-
-        Returns
-        -------
-        pdfratio : PDFRatio
-            The PDFRatio instance which corresponds to the given fit parameter
-            index.
-        """
-        return self._pdfratio_list[idx]
-
-    def calculate_pdfratio_values(self, tdm, fitparams, tl=None):
-        """Calculates the PDF ratio values for the PDF ratio objects which
-        depend on fit parameters.
+            The instance of TrialDataManager that holds the trial data.
+        tl : instance of TimeLord
+            The optional instance of TimeLord to measure timing information.
+        """
+        self._pdfratio.initialize_for_new_trial(
+            tdm=tdm,
+            tl=tl,
+            **kwargs)
+
+    def get_ratio(
+            self,
+            tdm,
+            src_params_recarray,
+            tl=None):
+        """Retrieves the PDF ratio value for each given trial data events (and
+        sources), given the given set of parameters.
+
+        Note:
+
+            This method uses the source detector signal yield weights service.
+            Hence, the
+            :meth:`skyllh.core.weights.SrcDetSigYieldWeightsService.calculate`
+            method needs to be called prior to calling this method.
 
         Parameters
         ----------
         tdm : instance of TrialDataManager
-            The instance of TrialDataManager that holds the trial event data for
+            The TrialDataManager instance holding the trial data events for
             which the PDF ratio values should get calculated.
-        fitparams : dict
-            The dictionary with the fit parameter name-value pairs.
-        tl : TimeLord instance | None
+        src_params_recarray : instance of numpy record ndarray
+            The (N_sources,)-shaped numpy record ndarray holding the parameter
+            names and values of the sources.
+            See the documentation of the
+            :meth:`skyllh.core.parameters.ParameterModelMapper.create_src_params_recarray`
+            method for more information.
+        tl : instance of TimeLord | None
             The optional TimeLord instance that should be used to measure
             timing information.
-        """
-        for (i, _pdfratio_i) in enumerate(self._pdfratio_list):
-            # Since the get_ratio method of the PDFRatio class might return a 2D
-            # (N_sources, N_events)-shaped array, and we assume a single source,
-            # we need to reshape the array, which does not involve any data
-            # copying.
-            self._ratio_values[i] = np.reshape(
-                _pdfratio_i.get_ratio(tdm, fitparams, tl=tl),
-                (tdm.n_selected_events,))
-
-    def get_ratio_product(self, excluded_idx=None):
-        """Calculates the product of the of the PDF ratio values of each event,
-        but excludes the PDF ratio values that correspond to the given excluded
-        fit parameter index. This is useful for calculating the derivates of
-        the log-likelihood ratio function.
-
-        Parameters
-        ----------
-        excluded_fitparam_idx : int | None
-            The index of the fit parameter whose PDF ratio values should get
-            excluded from the product. If None, the product over all PDF ratio
-            values will be computed.
 
         Returns
         -------
-        product : 1D (N_events,)-shaped ndarray
-            The product of the PDF ratio values for each event.
-        """
-        if(excluded_idx is None):
-            return np.prod(self._ratio_values, axis=0)
+        ratios : instance of ndarray
+            The (N_selected_events,)-shaped 1d numpy ndarray of float holding
+            the PDF ratio value for each selected trial data event.
+        """
+        (a_jk, a_jk_grads) = self._src_detsigyield_weights_service.get_weights()
+        a_k = a_jk[self._dataset_idx]
+
+        n_sources = len(a_k)
+        n_sel_events = tdm.n_selected_events
+
+        A = np.sum(a_k)
+
+        R_ik = self._pdfratio.get_ratio(
+            tdm=tdm,
+            src_params_recarray=src_params_recarray,
+            tl=tl)
+        # The R_ik ndarray is (N_values,)-shaped.
+
+        R_i = np.zeros((n_sel_events,), dtype=np.double)
+
+        (src_idxs, evt_idxs) = tdm.src_evt_idxs
+        for k in range(n_sources):
+            src_mask = src_idxs == k
+            R_i[evt_idxs[src_mask]] += R_ik[src_mask] * a_k[k]
+        R_i /= A
+
+        self._cache_R_ik = R_ik
+        self._cache_R_i = R_i
+
+        return R_i
+
+    def get_gradient(
+            self,
+            tdm,
+            src_params_recarray,
+            fitparam_id,
+            tl=None):
+        """Retrieves the PDF ratio gradient for the parameter ``fitparam_id``
+        for each trial data event, given the given set of parameters
+        ``src_params_recarray`` for each source.
 
-        # Get the index of the PDF ratio object that corresponds to the excluded
-        # fit parameter.
-        #excluded_pdfratio_idx = self._fitparam_idx_2_pdfratio_idx[excluded_fitparam_idx]
-        pdfratio_indices = list(range(self._ratio_values.shape[0]))
-        pdfratio_indices.pop(excluded_idx)
-        return np.prod(self._ratio_values[pdfratio_indices], axis=0)
-
-
-class PDFRatioFillMethod(object, metaclass=abc.ABCMeta):
-    """Abstract base class to implement a PDF ratio fill method. It can happen,
-    that there are empty background bins but where signal could possibly be.
-    A PDFRatioFillMethod implements what happens in such cases.
-    """
-
-    def __init__(self, *args, **kwargs):
-        super(PDFRatioFillMethod, self).__init__(*args, **kwargs)
+        Note:
 
-    @abc.abstractmethod
-    def fill_ratios(self, ratios, sig_prob_h, bkg_prob_h,
-                    sig_mask_mc_covered, sig_mask_mc_covered_zero_physics,
-                    bkg_mask_mc_covered, bkg_mask_mc_covered_zero_physics):
-        """The fill_ratios method is supposed to fill the ratio bins (array)
-        with the signal / background division values. For bins (array elements),
-        where the division is undefined, e.g. due to zero background, the fill
-        method decides how to fill those bins.
-
-        Note: Bins which have neither signal monte-carlo nor background
-              monte-carlo coverage, are undefined about their signal-ness or
-              background-ness by construction.
+            This method requires that the get_ratio method has been called prior
+            to calling this method.
 
         Parameters
         ----------
-        ratios : ndarray of float
-            The multi-dimensional array for the final ratio bins. The shape is
-            the same as the sig_h and bkg_h ndarrays.
-        sig_prob_h : ndarray of float
-            The multi-dimensional array (histogram) holding the signal
-            probabilities.
-        bkg_prob_h : ndarray of float
-            The multi-dimensional array (histogram) holding the background
-            probabilities.
-        sig_mask_mc_covered : ndarray of bool
-            The mask array indicating which array elements of sig_prob_h have
-            monte-carlo coverage.
-        sig_mask_mc_covered_zero_physics : ndarray of bool
-            The mask array indicating which array elements of sig_prob_h have
-            monte-carlo coverage but don't have physics contribution.
-        bkg_mask_mc_covered : ndarray of bool
-            The mask array indicating which array elements of bkg_prob_h have
-            monte-carlo coverage.
-            In case of experimental data as background, this mask indicate where
-            (experimental data) background is available.
-        bkg_mask_mc_covered_zero_physics : ndarray of bool
-            The mask array ndicating which array elements of bkg_prob_h have
-            monte-carlo coverage but don't have physics contribution.
-            In case of experimental data as background, this mask contains only
-            False entries.
+        tdm : instance of TrialDataManager
+            The TrialDataManager instance holding the trial data events for
+            which the PDF ratio gradient values should get calculated.
+        src_params_recarray : instance of numpy record ndarray
+            The (N_sources,)-shaped numpy record ndarray holding the parameter
+            names and values of the sources.
+            See the documentation of the
+            :meth:`skyllh.core.parameters.ParameterModelMapper.create_src_params_recarray`
+            method for more information.
+        fitparam_id : int
+            The ID of the global fit parameter for which the gradient should
+            get calculated.
+        tl : instance of TimeLord | None
+            The optional TimeLord instance that should be used to measure
+            timing information.
 
         Returns
         -------
-        ratios : ndarray
-            The array holding the final ratio values.
-        """
-        return ratios
-
-class Skylab2SkylabPDFRatioFillMethod(PDFRatioFillMethod):
-    """This PDF ratio fill method implements the exact same fill method as in
-    the skylab2 software named "skylab". It exists just for comparsion and
-    backward compatibility reasons. In general, it should not be used, because
-    it does not distinguish between bins with MC converage and physics model
-    contribution, and those with MC coverage and no physics model contribution!
-    """
-    def __init__(self):
-        super(Skylab2SkylabPDFRatioFillMethod, self).__init__()
-        self.signallike_percentile = 99.
-
-    def fill_ratios(self, ratio, sig_prob_h, bkg_prob_h,
-                    sig_mask_mc_covered, sig_mask_mc_covered_zero_physics,
-                    bkg_mask_mc_covered, bkg_mask_mc_covered_zero_physics):
-        """Fills the ratio array.
-        """
-        # Check if we have predicted background for the entire background MC
-        # range.
-        if(np.any(bkg_mask_mc_covered_zero_physics)):
-            raise ValueError('Some of the background bins have MC coverage but no physics background prediction. I don\'t know what to do in this case!')
-
-        sig_domain = sig_prob_h > 0
-        bkg_domain = bkg_prob_h > 0
-
-        ratio[sig_domain & bkg_domain] = sig_prob_h[sig_domain & bkg_domain] / bkg_prob_h[sig_domain & bkg_domain]
-
-        ratio_value = np.percentile(ratio[ratio > 1.], self.signallike_percentile)
-        np.copyto(ratio, ratio_value, where=sig_domain & ~bkg_domain)
-
-        return ratio
-
-class MostSignalLikePDFRatioFillMethod(PDFRatioFillMethod):
-    """PDF ratio fill method to set the PDF ratio to the most signal like PDF
-    ratio for bins, where there is signal MC coverage but no background (MC)
-    coverage.
-    """
-    def __init__(self, signallike_percentile=99.):
-        """Creates the PDF ratio fill method object for filling PDF ratio bins,
-        where there is signal MC coverage but no background (MC) coverage
-        with the most signal-like ratio value.
-
-        Parameters
-        ----------
-        signallike_percentile : float in range [0., 100.], default 99.
-            The percentile of signal-like ratios, which should be taken as the
-            ratio value for ratios with no background probability.
-        """
-        super(MostSignalLikePDFRatioFillMethod, self).__init__()
-
-        self.signallike_percentile = signallike_percentile
-
-    @property
-    def signallike_percentile(self):
-        """The percentile of signal-like ratios, which should be taken as the
-        ratio value for ratios with no background probability. This percentile
-        must be given as a float value in the range [0, 100] inclusively.
-        """
-        return self._signallike_percentile
-    @signallike_percentile.setter
-    def signallike_percentile(self, value):
-        if(not isinstance(value, float)):
-            raise TypeError('The signallike_percentile property must be of type float!')
-        if(value < 0. or value > 100.):
-            raise ValueError('The value of the signallike_percentile property must be in the range [0, 100]!')
-        self._signallike_percentile = value
+        gradient : instance of ndarray | 0
+            The (N_selected_events,)-shaped 1d numpy ndarray of float holding
+            the PDF ratio gradient value for each trial data event. If the PDF
+            ratio does not depend on the given global fit parameter, 0 will
+            be returned.
+        """
+        (a_jk, a_jk_grads) = self._src_detsigyield_weights_service.get_weights()
+
+        a_k = a_jk[self._dataset_idx]
+        A = np.sum(a_k)
+
+        n_sources = a_jk.shape[1]
+        n_sel_events = tdm.n_selected_events
+
+        if fitparam_id not in a_jk_grads:
+            a_k_grad = 0
+            dAdp = 0
+        else:
+            a_k_grad = a_jk_grads[fitparam_id][self._dataset_idx]
+            dAdp = np.sum(a_k_grad)
+
+        R_ik_grad = self._pdfratio.get_gradient(
+            tdm=tdm,
+            src_params_recarray=src_params_recarray,
+            fitparam_id=fitparam_id)
+        # R_ik_grad is a (N_values,)-shaped ndarray or 0.
 
-    def fill_ratios(self, ratio, sig_prob_h, bkg_prob_h,
-                    sig_mask_mc_covered, sig_mask_mc_covered_zero_physics,
-                    bkg_mask_mc_covered, bkg_mask_mc_covered_zero_physics):
-        """Fills the ratio array.
-        """
-        # Check if we have predicted background for the entire background MC
-        # range.
-        if(np.any(bkg_mask_mc_covered_zero_physics)):
-            raise ValueError('Some of the background bins have MC coverage but no physics background prediction. I don\'t know what to do in this case!')
-
-        # Fill the bins where we have signal and background MC coverage.
-        mask_sig_and_bkg_mc_covered = sig_mask_mc_covered & bkg_mask_mc_covered
-        ratio[mask_sig_and_bkg_mc_covered] = sig_prob_h[mask_sig_and_bkg_mc_covered] / bkg_prob_h[mask_sig_and_bkg_mc_covered]
-
-        # Calculate the ratio value, which should be used for ratio bins, where
-        # we have signal MC coverage but no background MC coverage.
-        ratio_value = np.percentile(ratio[ratio > 1.], self.signallike_percentile)
-        mask_sig_but_notbkg_mc_covered = sig_mask_mc_covered & ~bkg_mask_mc_covered
-        np.copyto(ratio, ratio_value, where=mask_sig_but_notbkg_mc_covered)
-
-        return ratio
-
-
-class MinBackgroundLikePDFRatioFillMethod(PDFRatioFillMethod):
-    """PDF ratio fill method to set the PDF ratio to the minimal background like
-    value for bins, where there is signal MC coverage but no background (MC)
-    coverage.
-    """
-    def __init__(self):
-        """Creates the PDF ratio fill method object for filling PDF ratio bins,
-        where there is signal MC coverage but no background (MC) coverage
-        with the minimal background-like ratio value.
-        """
-        super(MinBackgroundLikePDFRatioFillMethod, self).__init__()
+        if (type(a_k_grad) == int) and (a_k_grad == 0) and\
+           (type(R_ik_grad) == int) and (R_ik_grad == 0):
+            return 0
 
-    def fill_ratios(self, ratio, sig_prob_h, bkg_prob_h,
-                    sig_mask_mc_covered, sig_mask_mc_covered_zero_physics,
-                    bkg_mask_mc_covered, bkg_mask_mc_covered_zero_physics):
-        """Fills the ratio array.
-        """
-        # Check if we have predicted background for the entire background MC
-        # range.
-        if(np.any(bkg_mask_mc_covered_zero_physics)):
-            raise ValueError('Some of the background bins have MC coverage but no physics background prediction. I don\'t know what to do in this case!')
+        R_i_grad = -self._cache_R_i * dAdp
 
-        # Fill the bins where we have signal and background MC coverage.
-        mask_sig_and_bkg_mc_covered = sig_mask_mc_covered & bkg_mask_mc_covered
-        ratio[mask_sig_and_bkg_mc_covered] = sig_prob_h[mask_sig_and_bkg_mc_covered] / bkg_prob_h[mask_sig_and_bkg_mc_covered]
+        src_sum_i = np.zeros((n_sel_events,), dtype=np.double)
 
-        # Calculate the minimal background-like value.
-        min_bkg_prob = np.min(bkg_prob_h[bkg_mask_mc_covered])
+        (src_idxs, evt_idxs) = tdm.src_evt_idxs
+        for k in range(n_sources):
+            src_mask = src_idxs == k
+            src_evt_idxs = evt_idxs[src_mask]
+            if isinstance(a_k_grad, np.ndarray):
+                src_sum_i[src_evt_idxs] +=\
+                    a_k_grad[k] * self._cache_R_ik[src_mask]
+            if isinstance(R_ik_grad, np.ndarray):
+                src_sum_i[src_evt_idxs] +=\
+                    a_k[k] * R_ik_grad[src_mask]
 
-        # Set the ratio using the minimal background probability where we
-        # have signal MC coverage but no background (MC) coverage.
-        mask_sig_but_notbkg_mc_covered = sig_mask_mc_covered & ~bkg_mask_mc_covered
-        ratio[mask_sig_but_notbkg_mc_covered] = sig_prob_h[mask_sig_but_notbkg_mc_covered] / min_bkg_prob
+        R_i_grad += src_sum_i
+        R_i_grad /= A
 
-        return ratio
+        return R_i_grad
 
 
-class SigOverBkgPDFRatio(PDFRatio):
+class SigOverBkgPDFRatio(
+        PDFRatio):
     """This class implements a generic signal-over-background PDF ratio for a
     signal and a background PDF instance.
     It takes a signal PDF of type *pdf_type* and a background PDF of type
     *pdf_type* and calculates the PDF ratio.
     """
-    def __init__(self, sig_pdf, bkg_pdf, pdf_type=None, same_axes=True,
-        zero_bkg_ratio_value=1., *args, **kwargs):
+    def __init__(
+            self,
+            sig_pdf,
+            bkg_pdf,
+            same_axes=True,
+            zero_bkg_ratio_value=1.,
+            **kwargs):
         """Creates a new signal-over-background PDF ratio instance.
 
         Parameters
         ----------
         sig_pdf : class instance derived from `pdf_type`, IsSignalPDF
             The instance of the signal PDF.
         bkg_pdf : class instance derived from `pdf_type`, IsBackgroundPDF
             The instance of the background PDF.
-        pdf_type : type | None
-            The python type of the PDF object for which the PDF ratio is for.
-            If set to None, the default class ``PDF`` will be used.
         same_axes : bool
             Flag if the signal and background PDFs are supposed to have the
             same axes. Default is True.
         zero_bkg_ratio_value : float
             The value of the PDF ratio to take when the background PDF value
             is zero. This is to avoid division by zero. Default is 1.
         """
-        if(pdf_type is None):
-            pdf_type = PDF
-
-        super(SigOverBkgPDFRatio, self).__init__(
-            pdf_type=pdf_type, *args, **kwargs)
+        super().__init__(
+            sig_param_names=sig_pdf.param_set.params_name_list,
+            bkg_param_names=bkg_pdf.param_set.params_name_list,
+            **kwargs)
 
         self.sig_pdf = sig_pdf
         self.bkg_pdf = bkg_pdf
 
         # Check that the PDF axes ranges are the same for the signal and
         # background PDFs.
-        if(same_axes and (not sig_pdf.axes.is_same_as(bkg_pdf.axes))):
-            raise ValueError('The signal and background PDFs do not have the '
-                'same axes.')
+        if same_axes and not sig_pdf.axes.is_same_as(bkg_pdf.axes):
+            raise ValueError(
+                'The signal and background PDFs do not have the same axes!')
 
         self.zero_bkg_ratio_value = zero_bkg_ratio_value
 
         # Define cache member variables to calculate gradients efficiently.
-        self._cache_trial_data_state_id = None
-        self._cache_params_hash = None
-        self._cache_sigprob = None
-        self._cache_bkgprob = None
-        self._cache_siggrads = None
-        self._cache_bkggrads = None
+        self._cache_sig_pd = None
+        self._cache_bkg_pd = None
+        self._cache_sig_grads = None
+        self._cache_bkg_grads = None
 
     @property
     def sig_pdf(self):
         """The signal PDF object used to create the PDF ratio.
         """
         return self._sig_pdf
+
     @sig_pdf.setter
     def sig_pdf(self, pdf):
-        if(not isinstance(pdf, self.pdf_type)):
-            raise TypeError('The sig_pdf property must be an instance of '
-                '%s!'%(typename(self.pdf_type)))
-        if(not isinstance(pdf, IsSignalPDF)):
-            raise TypeError('The sig_pdf property must be an instance of '
-                'IsSignalPDF!')
+        if not isinstance(pdf, IsSignalPDF):
+            raise TypeError(
+                'The sig_pdf property must be an instance of IsSignalPDF! '
+                f'Its type is "{classname(pdf)}".')
         self._sig_pdf = pdf
 
     @property
     def bkg_pdf(self):
         """The background PDF object used to create the PDF ratio.
         """
         return self._bkg_pdf
+
     @bkg_pdf.setter
     def bkg_pdf(self, pdf):
-        if(not isinstance(pdf, self.pdf_type)):
-            raise TypeError('The bkg_pdf property must be an instance of '
-                '%s!'%(typename(self.pdf_type)))
-        if(not isinstance(pdf, IsBackgroundPDF)):
-            raise TypeError('The bkg_pdf property must be an instance of '
-                'IsBackgroundPDF!')
+        if not isinstance(pdf, IsBackgroundPDF):
+            raise TypeError(
+                'The bkg_pdf property must be an instance of IsBackgroundPDF! '
+                f'Its type is "{classname(pdf)}".')
         self._bkg_pdf = pdf
 
     @property
     def zero_bkg_ratio_value(self):
         """The value of the PDF ratio to take when the background PDF value
         is zero. This is to avoid division by zero.
         """
         return self._zero_bkg_ratio_value
+
     @zero_bkg_ratio_value.setter
     def zero_bkg_ratio_value(self, v):
-        v = float_cast(v, 'The zero_bkg_ratio_value must be castable into a '
-            'float!')
+        v = float_cast(
+            v,
+            'The zero_bkg_ratio_value must be castable to type float!')
         self._zero_bkg_ratio_value = v
 
-    def _get_signal_fitparam_names(self):
-        """Returns the list of fit parameter names the signal PDF depends on.
-        """
-        return self._sig_pdf.param_set.floating_param_name_list
-
-    def get_ratio(self, tdm, params=None, tl=None):
+    def initialize_for_new_trial(
+            self,
+            tdm,
+            tl=None,
+            **kwargs):
+        """Initializes the PDFRatio instance for a new trial. It calls the
+        :meth:`~skyllh.core.pdf.PDF.assert_is_valid_for_trial_data` of the
+        signal and background :class:`~skyllh.core.pdf.PDF` instances.
+        """
+        self._sig_pdf.initialize_for_new_trial(
+            tdm=tdm,
+            tl=tl,
+            **kwargs)
+        self._sig_pdf.assert_is_valid_for_trial_data(
+            tdm=tdm,
+            tl=tl,
+            **kwargs)
+
+        self._bkg_pdf.initialize_for_new_trial(
+            tdm=tdm,
+            tl=tl,
+            **kwargs)
+        self._bkg_pdf.assert_is_valid_for_trial_data(
+            tdm=tdm,
+            tl=tl,
+            **kwargs)
+
+    def get_ratio(
+            self,
+            tdm,
+            src_params_recarray,
+            tl=None):
         """Calculates the PDF ratio for the given trial events.
 
         Parameters
         ----------
         tdm : instance of TrialDataManager
             The TrialDataManager instance holding the trial data events for
             which the PDF ratio values should be calculated.
-        params : dict | None
-            The dictionary holding the parameter names and values for which the
-            probability ratio should get calculated.
-            This can be ``None``, if the signal and background PDFs do not
-            depend on any parameters.
-        tl : TimeLord instance | None
+        src_params_recarray : instance of numpy record ndarray
+            The (N_sources,)-shaped numpy record ndarray holding the local
+            parameter names and values of the sources.
+            See the
+            :meth:`skyllh.core.parameters.ParameterModelMapper.create_src_params_recarray`
+            method for more information.
+        tl : instance of TimeLord | None
             The optional TimeLord instance that should be used to measure
             timing information.
 
         Returns
         -------
-        ratios : (N_events)-shaped numpy ndarray
-            The ndarray holding the probability ratio for each event (and each
-            source). The dimensionality of the returned ndarray depends on the
-            dimensionality of the probability ndarray returned by the
-            ``get_prob`` method of the signal PDF object.
-        """
-        with TaskTimer(tl, 'Get sig prob.'):
-            (sigprob, self._cache_siggrads) = self._sig_pdf.get_prob(
-                tdm, params, tl=tl)
-        with TaskTimer(tl, 'Get bkg prob.'):
-            (bkgprob, self._cache_bkggrads) = self._bkg_pdf.get_prob(
-                tdm, params, tl=tl)
-
-        with TaskTimer(tl, 'Calc PDF ratios.'):
-            # Select only the events, where background pdf is greater than zero.
-            m = (bkgprob > 0)
-            ratios = np.full_like(sigprob, self._zero_bkg_ratio_value)
-            ratios[m] = sigprob[m] / bkgprob[m]
-
-        # Store the current state of parameter values and trial data, so that
-        # the get_gradient method can verify the consistency of the signal and
-        # background probabilities and gradients.
-        self._cache_trial_data_state_id = tdm.trial_data_state_id
-        self._cache_params_hash = make_params_hash(params)
-        self._cache_sigprob = sigprob
-        self._cache_bkgprob = bkgprob
+        ratios : instance of ndarray
+            The (N_values,)-shaped numpy ndarray holding the probability density
+            ratio for each event and source.
+        """
+        with TaskTimer(tl, 'Get sig probability densities and grads.'):
+            (self._cache_sig_pd, self._cache_sig_grads) = self._sig_pdf.get_pd(
+                tdm=tdm,
+                params_recarray=src_params_recarray,
+                tl=tl)
+        with TaskTimer(tl, 'Get bkg probability densities and grads.'):
+            (self._cache_bkg_pd, self._cache_bkg_grads) = self._bkg_pdf.get_pd(
+                tdm=tdm,
+                params_recarray=None,
+                tl=tl)
+
+        with TaskTimer(tl, 'Calculate PDF ratios.'):
+            # Select only the events, where the background pdf is greater than
+            # zero.
+            ratios = np.full_like(self._cache_sig_pd, self._zero_bkg_ratio_value)
+            m = (self._cache_bkg_pd > 0)
+            (m, bkg_pd) = tdm.broadcast_selected_events_arrays_to_values_arrays(
+                (m, self._cache_bkg_pd))
+            np.divide(
+                self._cache_sig_pd,
+                bkg_pd,
+                where=m,
+                out=ratios)
 
         return ratios
 
-    def get_gradient(self, tdm, params, fitparam_name):
-        """Retrieves the gradient of the PDF ratio w.r.t. the given fit
-        parameter. This method must be called after the ``get_ratio`` method.
+    def get_gradient(
+            self,
+            tdm,
+            src_params_recarray,
+            fitparam_id,
+            tl=None):
+        """Retrieves the gradient of the PDF ratio w.r.t. the given parameter.
+
+        Note:
+
+            This method uses cached values from the
+            :meth:`~skyllh.core.pdfratio.SigOverBkgPDFRatio.get_ratio` method.
+            Hence, that method needs to be called prior to this method.
 
         Parameters
         ----------
-        tdm : TrialDataManager instance
-            The instance of TrialDataManager that should be used to get the
-            trial data from.
-        params : dict
-            The dictionary with the parameter names and values.
-        fitparam_name : str
-            The name of the fit parameter for which the gradient should
+        tdm : instance of TrialDataManager
+            The instance of TrialDataManager holding the trial data.
+        src_params_recarray : instance of numpy record ndarray | None
+            The (N_models,)-shaped numpy record ndarray holding the parameter
+            names and values of the models.
+            See :meth:`skyllh.core.pdf.PDF.get_pd` for more information.
+            This can be ``None``, if the signal and background PDFs do not
+            depend on any parameters.
+        fitparam_id : int
+            The ID of the global fit parameter for which the gradient should
             get calculated.
+        tl : instance of TimeLord | None
+            The optional TimeLord instance that should be used to measure
+            timing information.
 
         Returns
         -------
-        gradient : (N_events,)-shaped 1d numpy ndarray of float
-            The PDF ratio gradient value for each trial event.
+        grad : instance of ndarray
+            The (N_values,)-shaped 1d numpy ndarray of float holding the PDF
+            ratio gradient value for each source and trial event.
         """
-        if((tdm.trial_data_state_id != self._cache_trial_data_state_id) or
-           (make_params_hash(params) != self._cache_params_hash)):
-            raise RuntimeError('The get_ratio method must be called prior to '
-                'the get_gradient method!')
-
         # Create the 1D return array for the gradient.
-        grad = np.zeros((tdm.n_selected_events,), dtype=np.float64)
+        grad = np.zeros_like(self._cache_sig_pd, dtype=np.float64)
 
-        # Calculate the gradient for the given fit parameter.
+        # Calculate the gradient for the given parameter.
         # There are four cases:
-        #   1) Neither the signal nor the background PDF depend on the fit
+        #   1) Neither the signal nor the background PDF depend on the
         #      parameter.
-        #   2) Only the signal PDF depends on the fit parameter.
-        #   3) Only the background PDF depends on the fit parameter.
-        #   4) Both, the signal and the background PDF depend on the fit
+        #   2) Only the signal PDF depends on the parameter.
+        #   3) Only the background PDF depends on the parameter.
+        #   4) Both, the signal and the background PDF depend on the
         #      parameter.
-        sig_pdf_param_set = self._sig_pdf.param_set
-        bkg_pdf_param_set = self._bkg_pdf.param_set
+        sig_dep = fitparam_id in self._cache_sig_grads
+        bkg_dep = fitparam_id in self._cache_bkg_grads
 
-        sig_dep = sig_pdf_param_set.has_floating_param(fitparam_name)
-        bkg_dep = bkg_pdf_param_set.has_floating_param(fitparam_name)
+        if (not sig_dep) and (not bkg_dep):
+            # Case 1. Return zeros.
+            return grad
+
+        m = self._cache_bkg_pd > 0
+        b = self._cache_bkg_pd
+
+        (m, b) = tdm.broadcast_selected_events_arrays_to_values_arrays(
+            (m, b))
 
-        if(sig_dep and (not bkg_dep)):
+        if sig_dep and not bkg_dep:
             # Case 2, which should be the most common case.
-            # Get the signal grad idx for that fit parameter.
-            sig_pidx = sig_pdf_param_set.get_floating_pidx(fitparam_name)
-            bkgprob = self._cache_bkgprob
-            m = bkgprob > 0
-            grad[m] = self._cache_siggrads[sig_pidx][m] / bkgprob[m]
-            return grad
-        if((not sig_dep) and (not bkg_dep)):
-            # Case 1. Returns zeros.
+            grad[m] = self._cache_sig_grads[fitparam_id][m] / b[m]
             return grad
 
-        if(sig_dep and bkg_dep):
+        bgrad = self._cache_bkg_grads[fitparam_id]
+        (bgrad,) = tdm.broadcast_selected_events_arrays_to_values_arrays(
+            (bgrad,))
+
+        if sig_dep and bkg_dep:
             # Case 4.
-            sig_pidx = sig_pdf_param_set.get_floating_pidx(fitparam_name)
-            bkg_pidx = bkg_pdf_param_set.get_floating_pidx(fitparam_name)
-            m = self._cache_bkgprob > 0
-            s = self._cache_sigprob[m]
-            b = self._cache_bkgprob[m]
-            sgrad = self._cache_siggrads[sig_pidx][m]
-            bgrad = self._cache_bkggrads[bkg_pidx][m]
+            s = self._cache_sig_pd
+            sgrad = self._cache_sig_grads[fitparam_id]
+
             # Make use of quotient rule of differentiation.
-            grad[m] = (sgrad * b - bgrad * s) / b**2
+            grad[m] = (sgrad[m] * b[m] - bgrad[m] * s[m]) / b[m]**2
             return grad
 
         # Case 3.
-        bkg_pidx = bkg_pdf_param_set.get_floating_pidx(fitparam_name)
-        bkgprob = self._cache_bkgprob
-        m = bkgprob > 0
-        grad[m] = (-self._cache_sigprob[m] / bkgprob[m]**2 *
-            self._cache_bkggrads[bkg_pidx][m])
-        return grad
-
-
-class SpatialSigOverBkgPDFRatio(SigOverBkgPDFRatio):
-    """This class implements a signal-over-background PDF ratio for spatial
-    PDFs. It takes a signal PDF of type SpatialPDF and a background PDF of type
-    SpatialPDF and calculates the PDF ratio.
-    """
-    def __init__(self, sig_pdf, bkg_pdf, *args, **kwargs):
-        """Creates a new signal-over-background PDF ratio instance for spatial
-        PDFs.
+        grad[m] = -self._cache_sig_pd[m] / b[m]**2 * bgrad[m]
 
-        Parameters
-        ----------
-        sig_pdf : class instance derived from SpatialPDF, IsSignalPDF
-            The instance of the spatial signal PDF.
-        bkg_pdf : class instance derived from SpatialPDF, IsBackgroundPDF
-            The instance of the spatial background PDF.
-        """
-        super(SpatialSigOverBkgPDFRatio, self).__init__(pdf_type=SpatialPDF,
-            sig_pdf=sig_pdf, bkg_pdf=bkg_pdf, *args, **kwargs)
-
-        # Make sure that the PDFs have two dimensions, i.e. RA and Dec.
-        if(not sig_pdf.ndim == 2):
-            raise ValueError('The spatial signal PDF must have two dimensions! '
-                'Currently it has %d!'%(sig_pdf.ndim))
+        return grad
 
 
-class SigSetOverBkgPDFRatio(PDFRatio):
-    """Class for a PDF ratio class that takes a PDFSet of PDF type
-    *pdf_type* as signal PDF and a PDF of type *pdf_type* as background PDF.
-    The signal PDF depends on signal fit parameters and a interpolation method
-    defines how the PDF ratio gets interpolated between the fit parameter
+class SigSetOverBkgPDFRatio(
+        PDFRatio):
+    """Class for a PDF ratio class that takes a PDFSet as signal PDF and a PDF
+    as background PDF.
+    The signal PDF depends on signal parameters and an interpolation method
+    defines how the PDF ratio gets interpolated between the parameter grid
     values.
     """
-    def __init__(self, pdf_type, signalpdfset, backgroundpdf,
-                 interpolmethod=None, *args, **kwargs):
+    def __init__(
+            self,
+            sig_pdf_set,
+            bkg_pdf,
+            interpolmethod_cls=None,
+            **kwargs):
         """Constructor called by creating an instance of a class which is
         derived from this PDFRatio class.
 
         Parameters
         ----------
-        pdf_type : type
-            The Python type of the PDF object for which the PDF ratio is for.
-        signalpdfset : class instance derived from PDFSet (for PDF type
-                       ``pdf_type``), and IsSignalPDF
+        sig_pdf_set : instance of PDFSet and instance of IsSignalPDF
             The PDF set, which provides signal PDFs for a set of
-            discrete signal fit parameters.
-        backgroundpdf : class instance derived from ``pdf_type``, and
-                        IsBackgroundPDF
+            discrete signal parameter values.
+        bkg_pdf : instance of PDF and instance of IsBackgroundPDF
             The background PDF instance.
-        interpolmethod : class of GridManifoldInterpolationMethod | None
-            The class implementing the fit parameter interpolation method for
-            the PDF ratio manifold grid.
-            If set to None (default), the
-            Parabola1DGridManifoldInterpolationMethod will be used for
-            1-dimensional parameter manifolds.
-        """
-        # Call super to allow for multiple class inheritance.
-        super(SigSetOverBkgPDFRatio, self).__init__(pdf_type, *args, **kwargs)
-
-        self.signalpdfset = signalpdfset
-        self.backgroundpdf = backgroundpdf
-
-        # Define the default fit parameter interpolation method. The default
-        # depends on the dimensionality of the fit parameter manifold.
-        if(interpolmethod is None):
-            ndim = signalpdfset.fitparams_grid_set.ndim
-            if(ndim == 1):
-                interpolmethod = Parabola1DGridManifoldInterpolationMethod
-            else:
-                raise ValueError('There is no default fit parameter manifold grid interpolation method available for %d dimensions!'%(ndim))
-        self.interpolmethod = interpolmethod
+        interpolmethod_cls : class of GridManifoldInterpolationMethod | None
+            The class implementing the parameter interpolation method for
+            the PDF ratio manifold grid. If set to ``None`` (default), the
+            :class:`skyllh.core.interpolate.Parabola1DGridManifoldInterpolationMethod`
+            will be used for 1-dimensional parameter manifolds.
+        """
+        super().__init__(
+            sig_param_names=sig_pdf_set.param_grid_set.params_name_list,
+            bkg_param_names=bkg_pdf.param_set.params_name_list,
+            **kwargs)
 
-        # Generate the list of signal fit parameter names once here.
-        self._cache_signal_fitparam_name_list = self.signal_fitparam_names
+        self.sig_pdf_set = sig_pdf_set
+        self.bkg_pdf = bkg_pdf
+
+        # Define the default parameter interpolation method. The default
+        # depends on the dimensionality of the parameter manifold.
+        if interpolmethod_cls is None:
+            ndim = self._sig_pdf_set.param_grid_set.ndim
+            if ndim == 1:
+                interpolmethod_cls = Parabola1DGridManifoldInterpolationMethod
+            else:
+                raise ValueError(
+                    'There is no default parameter manifold grid '
+                    f'interpolation method class available for {ndim} '
+                    'dimensions!')
+        self.interpolmethod_cls = interpolmethod_cls
 
     @property
-    def backgroundpdf(self):
-        """The background PDF object, derived from ``pdf_type`` and
-        IsBackgroundPDF.
+    def bkg_pdf(self):
+        """The background PDF instance, derived from IsBackgroundPDF.
         """
-        return self._bkgpdf
-    @backgroundpdf.setter
-    def backgroundpdf(self, pdf):
-        if(not (isinstance(pdf, self.pdf_type) and isinstance(pdf, IsBackgroundPDF))):
-            raise TypeError('The backgroundpdf property must be an object which is derived from %s and IsBackgroundPDF!'%(typename(self.pdf_type)))
-        self._bkgpdf = pdf
+        return self._bkg_pdf
+
+    @bkg_pdf.setter
+    def bkg_pdf(self, pdf):
+        if not isinstance(pdf, IsBackgroundPDF):
+            raise TypeError(
+                'The bkg_pdf property must be an instance derived from '
+                'IsBackgroundPDF! '
+                f'Its current type is {classname(pdf)}.')
+        self._bkg_pdf = pdf
 
     @property
-    def signalpdfset(self):
-        """The signal PDFSet object for ``pdf_type`` PDF objects.
+    def sig_pdf_set(self):
+        """The signal PDFSet instance, derived from IsSignalPDF.
         """
-        return self._sigpdfset
-    @signalpdfset.setter
-    def signalpdfset(self, pdfset):
-        if(not (isinstance(pdfset, PDFSet) and isinstance(pdfset, IsSignalPDF) and issubclass(pdfset.pdf_type, self.pdf_type))):
-            raise TypeError('The signalpdfset property must be an object which is derived from PDFSet and IsSignalPDF and whose pdf_type property is a subclass of %s!'%(typename(self.pdf_type)))
-        self._sigpdfset = pdfset
+        return self._sig_pdf_set
+
+    @sig_pdf_set.setter
+    def sig_pdf_set(self, pdfset):
+        if not (isinstance(pdfset, PDFSet) and
+                isinstance(pdfset, IsSignalPDF)):
+            raise TypeError(
+                'The sig_pdf_set property must be a class instance which is '
+                'derived from PDFSet and IsSignalPDF! '
+                f'Its current type is {classname(pdfset)}.')
+        self._sig_pdf_set = pdfset
 
     @property
-    def interpolmethod(self):
+    def interpolmethod_cls(self):
         """The class derived from GridManifoldInterpolationMethod
-        implementing the interpolation of the fit parameter manifold.
-        """
-        return self._interpolmethod
-    @interpolmethod.setter
-    def interpolmethod(self, cls):
-        if(not issubclass(cls, GridManifoldInterpolationMethod)):
-            raise TypeError('The interpolmethod property must be a sub-class '
-                'of GridManifoldInterpolationMethod!')
-        self._interpolmethod = cls
-
-    def _get_signal_fitparam_names(self):
-        """Returns the list of signal fit parameter names this PDF ratio is a
-        function of. The list is taken from the fit parameter grid set of the
-        signal PDFSet object. By construction this parameter grid set defines
-        the signal fit parameters.
+        implementing the interpolation of the parameter manifold.
         """
-        return self._sigpdfset.fitparams_grid_set.parameter_names
+        return self._interpolmethod_cls
 
-    def convert_signal_fitparam_name_into_index(self, signal_fitparam_name):
-        """Converts the given signal fit parameter name into the parameter
-        index, i.e. the position of parameter in the signal parameter grid set.
+    @interpolmethod_cls.setter
+    def interpolmethod_cls(self, cls):
+        if not issubclass(cls, GridManifoldInterpolationMethod):
+            raise TypeError(
+                'The interpolmethod_cls property must be a sub-class '
+                'of GridManifoldInterpolationMethod! '
+                f'Its current type is {classname(cls)}.')
+        self._interpolmethod_cls = cls
+
+    def initialize_for_new_trial(
+            self,
+            tdm,
+            tl=None,
+            **kwargs):
+        """Initializes the PDFRatio instance for a new trial. It calls the
+        :meth:`~skyllh.core.pdf.PDF.assert_is_valid_for_trial_data` of the
+        signal :class:`~skyllh.core.pdf.PDFSet` instance and the background
+        :class:`~skyllh.core.pdf.PDF` instance.
 
         Parameters
         ----------
-        signal_fitparam_name : str
-            The name of the signal fit parameter.
-
-        Returns
-        -------
-        index : int
-            The index of the signal fit parameter.
-        """
-        # If there is only one signal fit parameter, we just return index 0.
-        if(len(self._cache_signal_fitparam_name_list) == 1):
-            return 0
-
-        # At this point we have to loop through the list and do name
-        # comparisons.
-        for (index, name) in enumerate(self._cache_signal_fitparam_name_list):
-            if(name == signal_fitparam_name):
-                return index
-
-        # At this point there is no parameter defined.
-        raise KeyError('The PDF ratio "%s" has no signal fit parameter named '
-            '"%s"!'%(classname(self), signal_fitparam_name))
+        tdm : instance of TrialDataManager
+            The instance of TrialDataManager holding the trial data.
+        tl : instance of TimeLord | None
+            The optional instance of TimeLord for measuring timing information.
+        """
+        self._sig_pdf_set.initialize_for_new_trial(
+            tdm=tdm,
+            tl=tl,
+            **kwargs)
+        self._sig_pdf_set.assert_is_valid_for_trial_data(
+            tdm=tdm,
+            tl=tl,
+            **kwargs)
+
+        self._bkg_pdf.initialize_for_new_trial(
+            tdm=tdm,
+            tl=tl,
+            **kwargs)
+        self._bkg_pdf.assert_is_valid_for_trial_data(
+            tdm=tdm,
+            tl=tl,
+            **kwargs)
```

### Comparing `skyllh-23.1.1/skyllh/core/py.py` & `skyllh-23.2.0/skyllh/core/py.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,27 +1,32 @@
 # -*- coding: utf-8 -*-
 
-from __future__ import division
-
 import abc
 import copy
+import functools
 import inspect
 import numpy as np
 import sys
 
+from collections import OrderedDict
+
+from skyllh.core.display import INDENTATION_WIDTH
 
-class PyQualifier(object, metaclass=abc.ABCMeta):
+
+class PyQualifier(
+        object,
+        metaclass=abc.ABCMeta):
     """This is the abstract base class for any Python qualifier class.
     An object can get qualified by calling a PyQualifier instance with that
     object. The PyQualifier class will be added to the ``__pyqualifiers__``
     attribute of the object.
     """
 
-    def __init__(self):
-        super(PyQualifier, self).__init__()
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
 
     def __call__(self, obj):
         """Declares the given Python object to be qualified with the
         PyQualifier class of `self`. It adds the class of `self` to the
         `__pyqualifiers__` tuple of the given object.
 
         Parameters
@@ -31,15 +36,15 @@
 
         Returns
         -------
         obj : object
             The given object, but modified to be declared for this Python
             qualifier.
         """
-        if(not hasattr(obj, '__pyqualifiers__')):
+        if not hasattr(obj, '__pyqualifiers__'):
             setattr(obj, '__pyqualifiers__', ())
 
         obj.__pyqualifiers__ += (self.__class__,)
 
         return obj
 
     def check(self, obj):
@@ -53,45 +58,105 @@
 
         Returns
         -------
         check : bool
             The check result. `True` if the object is declared for this Python
             qualifier, and `False` otherwise.
         """
-        if(not hasattr(obj, '__pyqualifiers__')):
+        if not hasattr(obj, '__pyqualifiers__'):
             return False
 
-        if(self.__class__ in obj.__pyqualifiers__):
+        if self.__class__ in obj.__pyqualifiers__:
             return True
 
         return False
 
-class ConstPyQualifier(PyQualifier):
+
+class ConstPyQualifier(
+        PyQualifier):
     """This class defines a PyQualifier for constant Python objects.
     """
-    def __init__(self):
-        super(ConstPyQualifier, self).__init__()
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+
 
 const = ConstPyQualifier()
 
 
+def get_class_of_func(f):
+    """Determines the class object that defined the given method or function
+    ``f``.
+
+    Parameters
+    ----------
+    f : function | method
+        The function or method whose parent class should be determined.
+
+    Returns
+    -------
+    cls : class | None
+        The class object which defines the given function or method. ``None``
+        is returned when no class could be determined.
+    """
+    if isinstance(f, functools.partial):
+        return get_class_of_func(f.func)
+
+    if inspect.ismethod(f) or\
+        ((inspect.isbuiltin(f)) and
+         (getattr(f, '__self__', None) is not None) and
+         (getattr(f.__self__, '__class__', None))):
+        for cls in inspect.getmro(f.__self__.__class__):
+            if hasattr(cls, '__dict__') and (f.__name__ in cls.__dict__):
+                return cls
+        # Fall back to normal function evaluation.
+        f = getattr(f, '__func__', f)
+
+    if inspect.isfunction(f):
+        cls = getattr(
+            inspect.getmodule(f),
+            f.__qualname__.split('.<locals>', 1)[0].rsplit('.', 1)[0],
+            None)
+        if isinstance(cls, type):
+            return cls
+
+    # Handle special descriptor objects.
+    cls = getattr(f, '__objclass__', None)
+    return cls
+
+
 def typename(t):
     """Returns the name of the given type ``t``.
     """
     return t.__name__
 
+
 def classname(obj):
     """Returns the name of the class of the class instance ``obj``.
     """
     return typename(type(obj))
 
+
 def module_classname(obj):
-    """Returns the module and class name of the class instance ``obj``.
+    """Returns the module and class name of the given instance ``obj``.
     """
-    return '{}.{}'.format(obj.__module__, classname(obj))
+    return f'{obj.__module__}.{classname(obj)}'
+
+
+def module_class_method_name(obj, meth_name):
+    """Returns the module, class, and method name of the given instance ``obj``.
+
+    Parameters
+    ----------
+    obj : instance of object
+        The object instance.
+    meth_name : str
+        The name of the method.
+    """
+    return f'{module_classname(obj)}.{meth_name}'
+
 
 def get_byte_size_prefix(size):
     """Determines the biggest size prefix for the given size in bytes such that
     the new size is still greater one.
 
     Parameters
     ----------
@@ -106,23 +171,24 @@
         The biggest byte size prefix.
     """
     prefix_factor_list = [
         ('', 1), ('K', 1024), ('M', 1024**2), ('G', 1024**3), ('T', 1024**4)]
 
     prefix_idx = 0
     for (prefix, factor) in prefix_factor_list[1:]:
-        if(size / factor < 1):
+        if size / factor < 1:
             break
         prefix_idx += 1
 
     (prefix, factor) = prefix_factor_list[prefix_idx]
     newsize = size / factor
 
     return (newsize, prefix)
 
+
 def getsizeof(objects):
     """Determines the size in bytes the given objects have in memory.
     If an object is a sequence, the size of the elements of the sequence will
     be estimated as well and added to the result. This does not account for the
     multiple occurence of the same object.
 
     Parameters
@@ -130,57 +196,62 @@
     objects : sequence of instances of object | instance of object.
 
     Returns
     -------
     memsize : int
         The memory size in bytes of the given objects.
     """
-    if(not issequence(objects)):
+    if not issequence(objects):
         objects = [objects]
 
     memsize = 0
     for obj in objects:
-        if(issequence(obj)):
+        if issequence(obj):
             memsize += getsizeof(obj)
         else:
             memsize += sys.getsizeof(obj)
 
     return memsize
 
+
 def issequence(obj):
     """Checks if the given object ``obj`` is a sequence or not. The definition of
     a sequence in this case is, that the function ``len`` is defined for the
     object.
 
     .. note::
 
-        A str object is NOT considered as a sequence!
+        A str object is NOT considered a sequence!
 
-    :return True: If the given object is a sequence.
-    :return False: If the given object is a str object or not a sequence.
+    Returns
+    -------
+    check : bool
+        ``True`` if the given object is a sequence. ``False`` if the given
+        object is an instance of str or not a sequence.
 
     """
-    if(isinstance(obj, str)):
+    if isinstance(obj, str):
         return False
 
     try:
         len(obj)
     except TypeError:
         return False
 
     return True
 
+
 def issequenceof(obj, T, pyqualifiers=None):
     """Checks if the given object ``obj`` is a sequence with items being
     instances of type ``T``, possibly qualified with the given Python
     qualifiers.
 
     Parameters
     ----------
-    obj : object
+    obj : instance of object
         The Python object to check.
     T : type | tuple of types
         The type each item of the sequence should be. If a tuple of types is
         given, each item can be one of the given types.
     pyqualifiers : instance of PyQualifier |
             sequence of instances of PyQualifier | None
         One or more instances of PyQualifier. Each instance acts as a filter
@@ -189,41 +260,60 @@
         returns `False`. If set to `one`, no filters are applied.
 
     Returns
     -------
     check : bool
         The result of the check.
     """
-    if(pyqualifiers is None):
+    if pyqualifiers is None:
         pyqualifiers = tuple()
-    elif(not issequence(pyqualifiers)):
+    elif not issequence(pyqualifiers):
         pyqualifiers = (pyqualifiers,)
 
-    if(not issequence(obj)):
+    if not issequence(obj):
         return False
+
     for item in obj:
-        if(not isinstance(item, T)):
+        if not isinstance(item, T):
             return False
         for pyqualifier in pyqualifiers:
-            if(not pyqualifier.check(item)):
+            if not pyqualifier.check(item):
                 return False
 
     return True
 
+
 def issequenceofsubclass(obj, T):
     """Checks if the given object ``obj`` is a sequence with items being
     sub-classes of class T.
+
+    Parameters
+    ----------
+    obj : instance of object
+        The object to check.
+    T : class
+        The base class of the items of the given object.
+
+    Returns
+    -------
+    check : bool
+        ``True`` if the given object is a sequence of instances which are
+        sub-classes of class ``T``. ``False`` if ``obj`` is not a sequence or
+        any item is not a sub-class of class ``T``.
     """
-    if(not issequence(obj)):
+    if not issequence(obj):
         return False
+
     for item in obj:
-        if(not issubclass(item, T)):
+        if not issubclass(item, T):
             return False
+
     return True
 
+
 def isproperty(obj, name):
     """Checks if the given attribute is of type property. The attribute must
     exist in ``obj``.
 
     Parameters
     ----------
     obj : object
@@ -240,14 +330,15 @@
     ------
     AttributeError
         If the given attribute is not an attribute of the class of ``obj``.
     """
     attr = type(obj).__class__.__getattribute__(type(obj), name)
     return isinstance(attr, property)
 
+
 def func_has_n_args(func, n):
     """Checks if the given function `func` has `n` arguments.
 
     Parameters
     ----------
     func : callable
         The function to check.
@@ -258,39 +349,43 @@
     -------
     check : bool
         True if the given function has `n` arguments. False otherwise.
     """
     check = (len(inspect.signature(func).parameters) == n)
     return check
 
+
 def bool_cast(v, errmsg):
     """Casts the given value to a boolean value. If the cast is impossible, a
     TypeError is raised with the given error message.
     """
     try:
         v = bool(v)
-    except:
+    except Exception:
         raise TypeError(errmsg)
+
     return v
 
+
 def int_cast(v, errmsg, allow_None=False):
     """Casts the given value to an integer value. If the cast is impossible, a
     TypeError is raised with the given error message. If `allow_None` is set to
     `True` the value `v` can also be `None`.
     """
-    if(allow_None and v is None):
+    if allow_None and v is None:
         return v
 
     try:
         v = int(v)
-    except:
+    except Exception:
         raise TypeError(errmsg)
 
     return v
 
+
 def float_cast(v, errmsg, allow_None=False):
     """Casts the given value to a float. If the cast is impossible, a TypeError
     is raised with the given error message. If `allow_None` is set to `True`
     the value `v` can also be `None`.
 
     Parameters
     ----------
@@ -307,55 +402,65 @@
     -------
     float | None | list of float and or None
         The float / ``None`` value casted from ``v``. If a sequence of objects
         was provided, a list of casted values is returned.
     """
     # Define cast function for a single object.
     def _obj_float_cast(v, errmsg, allow_None):
-        if(allow_None and v is None):
+        if allow_None and v is None:
             return v
 
         try:
             v = float(v)
-        except:
+        except Exception:
             raise TypeError(errmsg)
 
         return v
 
-    if(issequence(v)):
+    if issequence(v):
         float_list = []
         for el_v in v:
             float_list.append(_obj_float_cast(el_v, errmsg, allow_None))
         return float_list
 
     return _obj_float_cast(v, errmsg, allow_None)
 
-def str_cast(v, errmsg):
+
+def str_cast(v, errmsg, allow_None=False):
     """Casts the given value to a str object.
     If the cast is impossible, a TypeError is raised with the given error
     message.
     """
+    if allow_None and v is None:
+        return v
+
     try:
         v = str(v)
-    except:
+    except Exception:
         raise TypeError(errmsg)
+
     return v
 
+
 def list_of_cast(t, v, errmsg):
     """Casts the given value `v` to a list of items of type `t`.
     If the cast is impossible, a TypeError is raised with the given error
     message.
     """
-    if(isinstance(v, t)):
+    if isinstance(v, t):
         v = [v]
-    if(not issequenceof(v, t)):
+
+    if not issequenceof(v, t):
         raise TypeError(errmsg)
+
     v = list(v)
+
     return v
 
+
 def get_smallest_numpy_int_type(values):
     """Returns the smallest numpy integer type that can represent the given
     integer values.
 
     Parameters
     ----------
     values : int | sequence of int
@@ -368,25 +473,27 @@
         The smallest numpy integer type that can represent the given values.
     """
     values = np.atleast_1d(values)
 
     vmin = np.min(values)
     vmax = np.max(values)
 
-    if(vmin < 0):
+    if vmin < 0:
         types = [np.int8, np.int16, np.int32, np.int64]
     else:
         types = [np.uint8, np.uint16, np.uint32, np.uint64]
 
     for inttype in types:
         ii = np.iinfo(inttype)
-        if(vmin >= ii.min and vmax <= ii.max):
+        if vmin >= ii.min and vmax <= ii.max:
             return inttype
 
-    raise ValueError("No integer type spans [%d, %d]!"%(vmin, vmax))
+    raise ValueError(
+        f'No integer type spans [{vmin}, {vmax}]!')
+
 
 def get_number_of_float_decimals(value):
     """Determines the number of significant decimals the given float number has.
     The maximum number of supported decimals is 16.
 
     Parameters
     ----------
@@ -394,24 +501,64 @@
         The float value whose number of significant decimals should get
         determined.
 
     Returns
     -------
     decimals : int
         The number of decimals of value which are non-zero.
+
+    Raises
+    ------
+    ValueError
+        If a nan value was provided.
     """
+    if np.isnan(value):
+        raise ValueError(
+            'The provided value is nan!')
+
     val_str = '{:.16f}'.format(value)
     (val_num_str, val_decs_str) = val_str.split('.', 1)
     for idx in range(len(val_decs_str)-1, -1, -1):
-        if(int(val_decs_str[idx]) != 0):
+        if int(val_decs_str[idx]) != 0:
             return idx+1
+
     return 0
 
 
-class ObjectCollection(object):
+def make_dict_hash(d):
+    """Creates a hash value for the given dictionary.
+
+    Parameters
+    ----------
+    d : dict | None
+        The dictionary holding (name: value) pairs.
+        If set to None, an empty dictionary is used.
+
+    Returns
+    -------
+    hash : int
+        The hash of the dictionary.
+    """
+    if d is None:
+        d = {}
+
+    if not isinstance(d, dict):
+        raise TypeError(
+            'The d argument must be of type dict!')
+
+    # A note on the ordering of Python dictionary items: The items are ordered
+    # internally according to the hash value of their keys. Hence, if we don't
+    # insert more dictionary items, the order of the items won't change. Thus,
+    # we can just take the items list and make a tuple to create a hash of it.
+    # The hash will be the same for two dictionaries having the same items.
+    return hash(tuple(d.items()))
+
+
+class ObjectCollection(
+        object):
     """This class provides a collection of objects of a specific type. Objects
     can be added to the collection via the ``add`` method or can be removed
     from the collection via the ``pop`` method. The objects of another object
     collection can be added to this object collection via the ``add`` method as
     well.
     """
     def __init__(self, objs=None, obj_type=None):
@@ -419,30 +566,38 @@
         derived class.
 
         Parameters
         ----------
         objs : instance of obj_type | sequence of obj_type instances | None
             The sequence of objects of type ``obj_type`` with which this
             collection should get initialized with.
-        obj_type : type
+        obj_type : type | None
             The type of the objects, which can be added to the collection.
+            If set to None, the type will be determined from the given objects.
+            If no objects are given, the object type will be `object`.
         """
-        if(obj_type is None):
+        if obj_type is None:
             obj_type = object
-        if(not issubclass(obj_type, object)):
+            if objs is not None:
+                if issequence(objs) and len(objs) > 0:
+                    obj_type = type(objs[0])
+                else:
+                    obj_type = type(objs)
+
+        if not issubclass(obj_type, object):
             raise TypeError(
                 'The obj_type argument must be a subclass of object!')
 
         self._obj_type = obj_type
         self._objects = []
 
         # Add given list of objects.
-        if(objs is not None):
-            if(not issequence(objs)):
-                objs = [ objs ]
+        if objs is not None:
+            if not issequence(objs):
+                objs = [objs]
             for obj in objs:
                 self.add(obj)
 
     @property
     def obj_type(self):
         """(read-only) The object type.
         """
@@ -485,53 +640,62 @@
         oc = self.copy()
         oc.add(other)
         return oc
 
     def __str__(self):
         """Pretty string representation of this object collection.
         """
-        return classname(self)+ ': ' + str(self._objects)
+        obj_str = ",\n".join([
+            ' '*INDENTATION_WIDTH + str(obj) for obj in self._objects
+        ])
+        return classname(self) + ": {\n" + obj_str + "\n}"
 
     def copy(self):
         """Creates a copy of this ObjectCollection. The objects of the
         collection are not copied!
         """
         oc = ObjectCollection(self._obj_type)
         oc._objects = copy.copy(self._objects)
         return oc
 
     def add(self, obj):
-        """Adds the given object to this object collection.
+        """Adds the given object, sequence of objects, or object collection to
+        this object collection.
 
         Parameters
         ----------
-        obj : obj_type instance | ObjectCollection of obj_type
+        obj : obj_type instance | sequence of obj_type |
+              ObjectCollection of obj_type
             An instance of ``obj_type`` that should be added to the collection.
             If given an ObjectCollection for objects of type obj_type, it will
             add all objects of the given collection to this collection.
 
         Returns
         -------
         self : ObjectCollection
             The instance of this ObjectCollection, in order to be able to chain
             several ``add`` calls.
         """
-        if(isinstance(obj, ObjectCollection)):
-            if(typename(obj.obj_type) != typename(self._obj_type)):
-                raise TypeError('Cannot add objects from ObjectCollection for '
-                    'objects of type "%s" to this ObjectCollection for objects '
-                    'of type "%s"!'%(
-                        typename(obj.obj_type), typename(self._obj_type)))
+        if issequence(obj) and not isinstance(obj, ObjectCollection):
+            obj = ObjectCollection(obj)
+
+        if isinstance(obj, ObjectCollection):
+            if not issubclass(obj.obj_type, self.obj_type):
+                raise TypeError(
+                    'Cannot add objects from ObjectCollection for objects of '
+                    f'type "{typename(obj.obj_type)}" to this ObjectCollection '
+                    f'for objects of type "{typename(self._obj_type)}"!')
             self._objects.extend(obj.objects)
             return self
 
-        if(not isinstance(obj, self._obj_type)):
-            raise TypeError('The object of type "%s" cannot be added to the '
-                'object collection for objects of type "%s"!'%(
-                    classname(obj), typename(self._obj_type)))
+        if not isinstance(obj, self._obj_type):
+            raise TypeError(
+                f'The object of type "{classname(obj)}" cannot be added to the '
+                'object collection for objects of type '
+                f'"{typename(self._obj_type)}"!')
 
         self._objects.append(obj)
         return self
     __iadd__ = add
 
     def index(self, obj):
         """Gets the index of the given object instance within this object
@@ -560,43 +724,101 @@
             last object is used.
 
         Returns
         -------
         obj : obj_type
             The removed object.
         """
-        if(index is None):
-            index = len(self._objects)-1
+        if index is None:
+            index = len(self._objects) - 1
         obj = self._objects.pop(index)
         return obj
 
 
 class NamedObjectCollection(ObjectCollection):
     """This class provides a collection of objects, which have a name. Access
     via the object name is efficient because the index of each object is
     tracked w.r.t. its name.
     """
-    def __init__(self, objs=None, obj_type=None):
+    def __init__(self, objs=None, obj_type=None, **kwargs):
         """Creates a new NamedObjectCollection instance. Must be called by the
         derived class.
 
         Parameters
         ----------
         objs : instance of obj_type | sequence of instances of obj_type | None
-            The sequence of objects of type ``obj_type`` with which this collection
-            should get initialized with.
+            The sequence of objects of type ``obj_type`` with which this
+            collection should get initialized with.
         obj_type : type
             The type of the objects, which can be added to the collection.
             This type must have an attribute named ``name``.
         """
-        self._obj_name_to_idx = dict()
+        self._obj_name_to_idx = OrderedDict()
 
-        super(NamedObjectCollection, self).__init__(
+        # The ObjectCollection class will call the add method to add individual
+        # objects. This will update the _obj_name_to_idx attribute.
+        super().__init__(
             objs=objs,
-            obj_type=obj_type)
+            obj_type=obj_type,
+            **kwargs)
+
+        if not hasattr(self.obj_type, 'name'):
+            raise TypeError(
+                f'The object type "{typename(self.obj_type)}" has no '
+                'attribute named "name"!')
+
+    @property
+    def name_list(self):
+        """(read-only) The list of the names of all the objects of this
+        NamedObjectCollection instance.
+        The order of this list of names is preserved to the order objects were
+        added to this collection.
+        """
+        return list(self._obj_name_to_idx.keys())
+
+    def _create_obj_name_to_idx_dict(self, start=None, end=None):
+        """Creates the dictionary {obj.name: index} for object in the interval
+        [`start`, `end`).
+
+        Parameters
+        ----------
+        start : int | None
+            The object start index position, which is inclusive.
+        end : int | None
+            The object end index position, which is exclusive.
+
+        Returns
+        -------
+        obj_name_to_idx : dict
+            The dictionary {obj.name: index}.
+        """
+        if start is None:
+            start = 0
+
+        return OrderedDict([
+            (o.name, start+idx)
+            for (idx, o) in enumerate(self._objects[start:end])
+        ])
+
+    def __contains__(self, name):
+        """Returns ``True`` if an object of the given name exists in this
+        NamedObjectCollection instance, ``False`` otherwise.
+
+        Parameters
+        ----------
+        name : str
+            The name of the object.
+
+        Returns
+        -------
+        check : bool
+            ``True`` if an object of name ``name`` exists in this
+            NamedObjectCollection instance, ``False`` otherwise.
+        """
+        return name in self._obj_name_to_idx
 
     def __getitem__(self, key):
         """Returns an object based on its name or index.
 
         Parameters
         ----------
         key : str | int
@@ -609,17 +831,17 @@
             The requested object.
 
         Raises
         ------
         KeyError
             If the given object is not found within this object collection.
         """
-        if(isinstance(key, str)):
-            key = self.index_by_name(key)
-        return super(NamedObjectCollection, self).__getitem__(key)
+        if isinstance(key, str):
+            key = self.get_index_by_name(key)
+        return super().__getitem__(key)
 
     def add(self, obj):
         """Adds the given object to this named object collection.
 
         Parameters
         ----------
         obj : obj_type instance | NamedObjectCollection of obj_type
@@ -631,28 +853,25 @@
 
         Returns
         -------
         self : NamedObjectCollection
             The instance of this NamedObjectCollection, in order to be able to
             chain several ``add`` calls.
         """
-        super(NamedObjectCollection, self).add(obj)
+        n_objs = len(self)
 
-        if(isinstance(obj, NamedObjectCollection)):
-            # Several objects have been added, so we recreate the name to index
-            # dictionary.
-            self._obj_name_to_idx = dict([
-                (o.name,idx) for (idx,o) in enumerate(self._objects) ])
-        else:
-            # Only a single object was added at the end.
-            self._obj_name_to_idx[obj.name] = len(self) - 1
+        super().add(obj)
+
+        self._obj_name_to_idx.update(
+            self._create_obj_name_to_idx_dict(n_objs))
 
         return self
+    __iadd__ = add
 
-    def index_by_name(self, name):
+    def get_index_by_name(self, name):
         """Gets the index of the object with the given name within this named
         object collection.
 
         Parameters
         ----------
         name : str
             The name of the object whose index should get retrieved.
@@ -676,18 +895,17 @@
             If a str instance is given, it specifies the name of the object.
 
         Returns
         -------
         obj : obj_type instance
             The removed object.
         """
-        if(isinstance(index, str)):
+        if isinstance(index, str):
             # Get the index of the object given its name.
-            index = self.index_by_name(index)
+            index = self.get_index_by_name(index)
 
-        obj = super(NamedObjectCollection, self).pop(index)
+        obj = super().pop(index)
 
         # Recreate the object name to index dictionary.
-        self._obj_name_to_idx = dict([
-            (o.name,idx) for (idx,o) in enumerate(self._objects) ])
+        self._obj_name_to_idx = self._create_obj_name_to_idx_dict()
 
         return obj
```

### Comparing `skyllh-23.1.1/skyllh/core/random.py` & `skyllh-23.2.0/skyllh/core/random.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,58 +1,72 @@
 # -*- coding: utf-8 -*-
 
 import numpy as np
 
 from skyllh.core.py import int_cast
 
-class RandomStateService(object):
+
+class RandomStateService(
+        object):
     """The RandomStateService class provides a container for a
     numpy.random.RandomState object, initialized with a given seed. This service
     can then be passed to any function or method within skyllh that requires a
     random number generator.
     """
-    def __init__(self, seed=None):
+    def __init__(
+            self,
+            seed=None,
+            **kwargs,
+    ):
         """Creates a new random state service. The ``random`` property can then
         be used to draw random numbers.
 
         Parameters
         ----------
         seed : int | None
             The seed to use. If None, the random number generator will be seeded
             randomly. See the numpy documentation for numpy.random.RandomState
             what that means.
         """
-        self._seed = int_cast(seed, 'The seed argument must be None, or '
-            'castable to type int!', allow_None=True)
+        super().__init__(**kwargs)
+
+        self._seed = int_cast(
+            seed,
+            'The seed argument must be None, or castable to type int!',
+            allow_None=True)
         self.random = np.random.RandomState(self._seed)
 
     @property
     def seed(self):
         """(read-only) The seed (int) of the random number generator.
         None, if not set. To change the seed, use the `reseed` method.
         """
         return self._seed
 
     @property
     def random(self):
         """The numpy.random.RandomState object.
         """
         return self._random
+
     @random.setter
     def random(self, random):
-        if(not isinstance(random, np.random.RandomState)):
-            raise TypeError('The random property must be of type numpy.random.RandomState!')
+        if not isinstance(random, np.random.RandomState):
+            raise TypeError(
+                'The random property must be of type numpy.random.RandomState!')
         self._random = random
 
     def reseed(self, seed):
         """Reseeds the random number generator with the given seed.
 
         Parameters
         ----------
         seed : int | None
             The seed to use. If None, the random number generator will be seeded
             randomly. See the numpy documentation for numpy.random.RandomState
             what that means.
         """
-        self._seed = int_cast(seed, 'The seed argument must be None or '
-            'castable to type int!', allow_None=True)
+        self._seed = int_cast(
+            seed,
+            'The seed argument must be None or castable to type int!',
+            allow_None=True)
         self.random.seed(self._seed)
```

### Comparing `skyllh-23.1.1/skyllh/core/scrambling.py` & `skyllh-23.2.0/skyllh/core/scrambling.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,236 +1,314 @@
 # -*- coding: utf-8 -*-
 
 import abc
 
 import numpy as np
 
-from skyllh.core.times import TimeGenerator
+from skyllh.core.times import (
+    TimeGenerator,
+)
 
 
-class DataScramblingMethod(object, metaclass=abc.ABCMeta):
-    """Base class (type) for implementing a data scrambling method.
+class DataScramblingMethod(
+        object,
+        metaclass=abc.ABCMeta,
+):
+    """Base class for implementing a data scrambling method.
     """
 
-    def __init__(self):
-        super(DataScramblingMethod, self).__init__()
+    def __init__(self, **kwargs):
+        super().__init__(
+            **kwargs)
 
     @abc.abstractmethod
-    def scramble(self, rss, data):
+    def scramble(
+            self,
+            rss,
+            dataset,
+            data,
+    ):
         """The scramble method implements the actual scrambling of the given
         data, which is method dependent. The scrambling must be performed
         in-place, i.e. it alters the data inside the given data array.
 
         Parameters
         ----------
-        rss : RandomStateService
+        rss : instance of RandomStateService
             The random state service providing the random number
             generator (RNG).
+        dataset : instance of Dataset
+            The instance of Dataset for which the data should get scrambled.
         data : instance of DataFieldRecordArray
             The DataFieldRecordArray containing the to be scrambled data.
 
         Returns
         -------
-        data : DataFieldRecordArray
+        data : instance of DataFieldRecordArray
             The given DataFieldRecordArray holding the scrambled data.
         """
         pass
 
 
-class UniformRAScramblingMethod(DataScramblingMethod):
+class UniformRAScramblingMethod(
+        DataScramblingMethod,
+):
     r"""The UniformRAScramblingMethod method performs right-ascention scrambling
     uniformly within a given RA range. By default it's (0, 2\pi).
 
-    Note: This alters only the ``ra`` values of the data!
+    :note::
+
+        This alters only the ``ra`` values of the data!
+
     """
-    def __init__(self, ra_range=None):
+    def __init__(
+            self,
+            ra_range=None,
+            **kwargs,
+    ):
         r"""Initializes a new RAScramblingMethod instance.
 
         Parameters
         ----------
         ra_range : tuple | None
             The two-element tuple holding the range in radians within the RA
             values should get drawn from. If set to None, the default (0, 2\pi)
             will be used.
         """
-        super(UniformRAScramblingMethod, self).__init__()
+        super().__init__(
+            **kwargs)
 
         self.ra_range = ra_range
 
     @property
     def ra_range(self):
         """The two-element tuple holding the range within the RA values
         should get drawn from.
         """
         return self._ra_range
+
     @ra_range.setter
     def ra_range(self, ra_range):
-        if(ra_range is None):
+        if ra_range is None:
             ra_range = (0, 2*np.pi)
-        if(not isinstance(ra_range, tuple)):
-            raise TypeError('The ra_range property must be a tuple!')
-        if(len(ra_range) != 2):
-            raise ValueError('The ra_range tuple must contain 2 elements!')
+        if not isinstance(ra_range, tuple):
+            raise TypeError(
+                'The ra_range property must be a tuple!')
+        if len(ra_range) != 2:
+            raise ValueError(
+                'The ra_range tuple must contain 2 elements!')
         self._ra_range = ra_range
 
-    def scramble(self, rss, data):
+    def scramble(
+            self,
+            rss,
+            dataset,
+            data,
+    ):
         """Scrambles the given data uniformly in right-ascention.
 
         Parameters
         ----------
-        rss : RandomStateService
+        rss : instance of RandomStateService
             The random state service providing the random number
             generator (RNG).
+        dataset : instance of Dataset
+            The instance of Dataset for which the data should get scrambled.
         data : instance of DataFieldRecordArray
             The DataFieldRecordArray instance containing the to be scrambled
             data.
 
         Returns
         -------
-        data : DataFieldRecordArray
+        data : instance of DataFieldRecordArray
             The given DataFieldRecordArray holding the scrambled data.
         """
         dt = data['ra'].dtype
+
         data['ra'] = rss.random.uniform(
-            *self.ra_range, size=len(data)).astype(dt)
+            *self.ra_range, size=len(data)).astype(dt, copy=False)
+
         return data
 
 
-class TimeScramblingMethod(DataScramblingMethod):
+class TimeScramblingMethod(
+        DataScramblingMethod):
     """The TimeScramblingMethod class provides a data scrambling method to
     perform data coordinate scrambling based on a generated time. It draws a
     random time from a time generator and transforms the horizontal (local)
     coordinates into equatorial coordinates using a specified transformation
     function.
     """
-    def __init__(self, timegen, hor_to_equ_transform):
+    def __init__(
+            self,
+            timegen,
+            hor_to_equ_transform,
+            **kwargs,
+    ):
         """Initializes a new time scramling method instance.
 
         Parameters
         ----------
-        timegen : TimeGenerator
+        timegen : instance of TimeGenerator
             The time generator that should be used to generate random MJD times.
         hor_to_equ_transform : callable
             The transformation function to transform coordinates from the
             horizontal system into the equatorial system.
 
             The call signature must be:
 
                 __call__(azi, zen, mjd)
 
             The return signature must be: (ra, dec)
 
         """
-        super(TimeScramblingMethod, self).__init__()
+        super().__init__(
+            **kwargs)
 
         self.timegen = timegen
         self.hor_to_equ_transform = hor_to_equ_transform
 
     @property
     def timegen(self):
         """The TimeGenerator instance that should be used to generate random MJD
         times.
         """
         return self._timegen
+
     @timegen.setter
     def timegen(self, timegen):
-        if(not isinstance(timegen, TimeGenerator)):
-            raise TypeError('The timegen property must be an instance of TimeGenerator!')
+        if not isinstance(timegen, TimeGenerator):
+            raise TypeError(
+                'The timegen property must be an instance of TimeGenerator!')
         self._timegen = timegen
 
     @property
     def hor_to_equ_transform(self):
         """The transformation function to transform coordinates from the
         horizontal system into the equatorial system.
         """
         return self._hor_to_equ_transform
+
     @hor_to_equ_transform.setter
     def hor_to_equ_transform(self, transform):
-        if(not callable(transform)):
-            raise TypeError('The hor_to_equ_transform property must be a callable object!')
+        if not callable(transform):
+            raise TypeError(
+                'The hor_to_equ_transform property must be a callable object!')
         self._hor_to_equ_transform = transform
 
-    def scramble(self, rss, data):
+    def scramble(
+            self,
+            rss,
+            dataset,
+            data,
+    ):
         """Scrambles the given data based on random MJD times, which are
         generated from a TimeGenerator instance. The event's right-ascention and
         declination coordinates are calculated via a horizontal-to-equatorial
         coordinate transformation and the generated MJD time of the event.
 
         Parameters
         ----------
-        rss : RandomStateService
+        rss : instance of RandomStateService
             The random state service providing the random number
             generator (RNG).
+        dataset : instance of Dataset
+            The instance of Dataset for which the data should get scrambled.
         data : instance of DataFieldRecordArray
             The DataFieldRecordArray instance containing the to be scrambled
             data.
 
         Returns
         -------
-        data : DataFieldRecordArray
+        data : instance of DataFieldRecordArray
             The given DataFieldRecordArray holding the scrambled data.
         """
         mjds = self.timegen.generate_times(rss, len(data))
+
         data['time'] = mjds
+
         (data['ra'], data['dec']) = self.hor_to_equ_transform(
             data['azi'], data['zen'], mjds)
+
         return data
 
 
-class DataScrambler(object):
-    def __init__(self, method):
+class DataScrambler(
+        object,
+):
+    def __init__(
+            self,
+            method,
+            **kwargs,
+    ):
         """Creates a data scrambler instance with a given defined scrambling
         method.
 
         Parameters
         ----------
-        method : DataScramblingMethod
+        method : instance of DataScramblingMethod
             The instance of DataScramblingMethod that defines the method of
             the data scrambling.
         """
+        super().__init__(
+            **kwargs)
+
         self.method = method
 
     @property
     def method(self):
         """The underlaying scrambling method that should be used to scramble
         the data. This must be an instance of the DataScramblingMethod class.
         """
         return self._method
+
     @method.setter
     def method(self, method):
-        if(not isinstance(method, DataScramblingMethod)):
-            raise TypeError('The data scrambling method must be an instance '
-                'of DataScramblingMethod!')
+        if not isinstance(method, DataScramblingMethod):
+            raise TypeError(
+                'The data scrambling method must be an instance of '
+                'DataScramblingMethod!')
         self._method = method
 
-    def scramble_data(self, rss, data, copy=False):
+    def scramble_data(
+            self,
+            rss,
+            dataset,
+            data,
+            copy=False,
+    ):
         """Scrambles the given data by calling the scramble method of the
         scrambling method class, that was configured for the data scrambler.
         If the ``inplace_scrambling`` property is set to False, a copy of the
         data is created before the scrambling is performed.
 
         Parameters
         ----------
-        rss : RandomStateService
+        rss : instance of RandomStateService
             The random state service providing the random number generator
             (RNG).
+        dataset : instance of Dataset
+            The instance of Dataset for which the data should get scrambled.
         data : instance of DataFieldRecordArray
-            The DataFieldRecordArray instance holding the data, which should get
-            scrambled.
+            The instance of DataFieldRecordArray holding the data, which should
+            get scrambled.
         copy : bool
             Flag if a copy of the given data should be made before scrambling
             the data. The default is False.
 
         Returns
         -------
-        data : DataFieldRecordArray
+        data : instance of DataFieldRecordArray
             The given DataFieldRecordArray instance with the scrambled data.
             If the ``inplace_scrambling`` property is set to True, this output
             array is the same array as the input array, otherwise it's a new
             array.
         """
-        if(copy):
+        if copy:
             data = data.copy()
 
-        data = self._method.scramble(rss, data)
+        data = self._method.scramble(
+            rss=rss,
+            dataset=dataset,
+            data=data)
 
         return data
```

### Comparing `skyllh-23.1.1/skyllh/core/session.py` & `skyllh-23.2.0/skyllh/core/session.py`

 * *Files 1% similar despite different names*

```diff
@@ -5,38 +5,42 @@
 """The session module provides global settings for session handling.
 """
 
 # By default SkyLLH will not be in interactive session, i.e. will be in batch
 # mode. Hence, progress bars will not be displayed to not screw up the output.
 IS_INTERACTIVE_SESSION = False
 
+
 def enable_interactive_session():
     """Enables interactive session mode.
     """
     global IS_INTERACTIVE_SESSION
 
     IS_INTERACTIVE_SESSION = True
 
+
 def disable_interactive_session():
     """Disables interactive session mode.
     """
     global IS_INTERACTIVE_SESSION
 
     IS_INTERACTIVE_SESSION = False
 
+
 def is_interactive_session():
     """Checks whether the current session is interactive (True) or not (False).
 
     Returns
     -------
     check : bool
         True if the current SkyLLH session is interactive, False otherwise.
     """
     return IS_INTERACTIVE_SESSION
 
+
 def is_python_interpreter_in_interactive_mode():
     """Checks if the Python interpreter is in interactive mode.
 
     Returns
     -------
     check : bool
         True if the Python interpreter is in interactive mode, False otherwise.
```

### Comparing `skyllh-23.1.1/skyllh/core/signal_generation.py` & `skyllh-23.2.0/skyllh/core/signal_generation.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,85 +1,111 @@
 # -*- coding: utf-8 -*-
 
 import abc
 
 from skyllh.core.py import (
     issequence,
-    float_cast
+    float_cast,
 )
 
-class SignalGenerationMethod(object, metaclass=abc.ABCMeta):
+
+class SignalGenerationMethod(
+        object,
+        metaclass=abc.ABCMeta
+):
     """This is a base class for a source and detector specific signal generation
     method, that calculates the source flux for a given monte-carlo event, which
     is needed to calculate the MC event weights for the signal generator.
     """
 
-    def __init__(self, energy_range):
+    def __init__(
+            self,
+            energy_range,
+            **kwargs,
+    ):
         """Constructs a new signal generation method instance.
 
         Parameters
         ----------
         energy_range : 2-element tuple of float | None
             The energy range from which to take MC events into account for
             signal event generation.
             If set to None, the entire energy range [0, +inf] is used.
         """
-        super(SignalGenerationMethod, self).__init__()
+        super().__init__(**kwargs)
 
         self.energy_range = energy_range
 
     @property
     def energy_range(self):
         """The 2-element tuple of floats holding the energy range from which to
         take MC events into account for signal event generation.
         """
         return self._energy_range
+
     @energy_range.setter
     def energy_range(self, r):
-        if(r is not None):
-            if(not issequence(r)):
-                raise TypeError('The energy_range property must be a sequence!')
-            if(len(r) != 2):
-                raise ValueError('The energy_range property must be a sequence '
-                    'of 2 elements!')
+        if r is not None:
+            if not issequence(r):
+                raise TypeError(
+                    'The energy_range property must be a sequence!')
+            if len(r) != 2:
+                raise ValueError(
+                    'The energy_range property must be a sequence of 2 '
+                    'elements!')
             r = tuple(
-                (float_cast(r[0], 'The first element of the energy_range '
-                                 'sequence must be castable to type float!'),
-                float_cast(r[1], 'The second element of the energy_range '
-                                 'sequence must be castable to type float!'))
+                float_cast(
+                    r[0],
+                    'The first element of the energy_range '
+                    'sequence must be castable to type float!'),
+                float_cast(
+                    r[1],
+                    'The second element of the energy_range '
+                    'sequence must be castable to type float!')
             )
         self._energy_range = r
 
     @abc.abstractmethod
-    def calc_source_signal_mc_event_flux(self, data_mc, src_hypo_group):
+    def calc_source_signal_mc_event_flux(
+            self,
+            data_mc,
+            shg,
+    ):
         """This method is supposed to calculate the signal flux of each given
         MC event for each source hypothesis of the given source hypothesis
         group.
 
         Parameters
         ----------
         data_mc : numpy record ndarray
             The numpy record array holding all the MC events.
-        src_hypo_group : SourceHypoGroup instance
-            The source hypothesis group, which defines the list of sources, and
-            their flux model.
+        shg : instance of SourceHypoGroup
+            The source hypothesis group instance, which defines the list of
+            sources, and their flux model.
 
         Returns
         -------
-        flux_list : list of 2-element tuples
-            The list of 2-element tuples with one tuple for each source. Each
-            tuple must be made of two 1D ndarrays of size
-            N_selected_signal_events, where the first array contains the global
-            MC data event indices and the second array the flux of each selected
-            signal event.
+        ev_idx_arr : ndarray
+            The (N_selected_signal_events,)-shaped 1D ndarray holding the index
+            of the MC event.
+        shg_src_idx_arr : ndarray
+            The (N_selected_signal_events,)-shaped 1D ndarray holding the index
+            of the source within the given source hypothesis group for each
+            signal candidate event.
+        flux_arr : ndarray
+            The (N_selected_signal_events,)-shaped 1D ndarray holding the flux
+            value of each signal candidate event.
         """
         pass
 
     def signal_event_post_sampling_processing(
-        self, shg, shg_sig_events_meta, shg_sig_events
+        self,
+        shg,
+        shg_sig_events_meta,
+        shg_sig_events,
     ):
         """This method should be reimplemented by the derived class if there
         is some processing needed after the MC signal events have been sampled
         from the global MC data.
 
         Parameters
         ----------
@@ -88,16 +114,17 @@
             locations.
         shg_sig_events_meta : numpy record ndarray
             The numpy record ndarray holding meta information about the
             generated signal events for the given source hypothesis group.
             The length of this array must be the same as shg_sig_events.
             It needs to contain the following data fields:
 
-            - 'shg_src_idx' : int
-                The source index within the source hypothesis group.
+                shg_src_idx : int
+                    The source index within the source hypothesis group.
+
         shg_sig_events : numpy record ndarray
             The numpy record ndarray holding the generated signal events for
             the given source hypothesis group and in the format of the original
             MC events.
 
         Returns
         -------
```

### Comparing `skyllh-23.1.1/skyllh/core/signal_generator.py` & `skyllh-23.2.0/skyllh/core/services.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,449 +1,634 @@
 # -*- coding: utf-8 -*-
 
-import abc
-import itertools
+from collections import (
+    defaultdict,
+)
 import numpy as np
 
+from skyllh.core.dataset import (
+    Dataset,
+    DatasetData,
+)
+from skyllh.core.progressbar import (
+    ProgressBar,
+)
 from skyllh.core.py import (
+    classname,
     issequenceof,
-    float_cast,
-    int_cast,
-    get_smallest_numpy_int_type
 )
-from skyllh.core.dataset import Dataset, DatasetData
-from skyllh.core.source_hypothesis import SourceHypoGroupManager
-from skyllh.core.storage import DataFieldRecordArray
-from skyllh.physics.flux import (
-    get_conversion_factor_to_internal_flux_unit
+from skyllh.core.source_hypo_grouping import (
+    SourceHypoGroupManager,
 )
 
 
-class SignalGeneratorBase(object, metaclass=abc.ABCMeta):
-    """This is the abstract base class for all signal generator classes in
-    SkyLLH. It defines the interface for signal generators.
+class DetSigYieldService(
+        object):
+    """This class provides a service to build and hold detector signal yield
+    instances for multiple datasets and source hypothesis groups.
     """
-    def __init__(self, src_hypo_group_manager, dataset_list, data_list,
-                 *args, **kwargs):
-        """Constructs a new signal generator instance.
 
-        Parameters
-        ----------
-        src_hypo_group_manager : SourceHypoGroupManager instance
-            The SourceHypoGroupManager instance defining the source hypothesis
-            groups.
-        dataset_list : list of Dataset instances
-            The list of Dataset instances for which signal events should get
-            generated for.
-        data_list : list of DatasetData instances
-            The list of DatasetData instances holding the actual data of each
-            dataset. The order must match the order of ``dataset_list``.
+    def __init__(
+            self,
+            shg_mgr,
+            dataset_list,
+            data_list,
+            ppbar=None,
+            **kwargs):
+        """Creates a new DetSigYieldService instance.
         """
-        super().__init__(*args, **kwargs)
+        super().__init__(
+            **kwargs)
+
+        self._set_shg_mgr(shg_mgr)
 
-        self.src_hypo_group_manager = src_hypo_group_manager
         self.dataset_list = dataset_list
         self.data_list = data_list
 
+        self._arr = self.construct_detsigyield_array(
+            ppbar=ppbar)
+
     @property
-    def src_hypo_group_manager(self):
-        """The SourceHypoGroupManager instance defining the source hypothesis
-        groups.
-        """
-        return self._src_hypo_group_manager
-    @src_hypo_group_manager.setter
-    def src_hypo_group_manager(self, manager):
-        if(not isinstance(manager, SourceHypoGroupManager)):
-            raise TypeError(
-                'The src_hypo_group_manager property must be an instance of '
-                'SourceHypoGroupManager!')
-        self._src_hypo_group_manager = manager
+    def shg_mgr(self):
+        """(read-only) The instance of SourceHypoGroupManager providing the list
+        of source hypothesis groups.
+        """
+        return self._shg_mgr
 
     @property
     def dataset_list(self):
-        """The list of Dataset instances for which signal events should get
-        generated for.
+        """The list of instance of Dataset for which the detector signal yields
+        should be built.
         """
         return self._dataset_list
+
     @dataset_list.setter
     def dataset_list(self, datasets):
-        if(not issequenceof(datasets, Dataset)):
+        if not issequenceof(datasets, Dataset):
             raise TypeError(
                 'The dataset_list property must be a sequence of Dataset '
-                'instances!')
+                'instances! '
+                f'Its current type is {classname(datasets)}!')
         self._dataset_list = list(datasets)
 
     @property
     def data_list(self):
-        """The list of DatasetData instances holding the actual data of each
-        dataset. The order must match the order of the ``dataset_list``
-        property.
+        """The list of instance of DatasetData for which the detector signal
+        yields should be built.
         """
         return self._data_list
+
     @data_list.setter
     def data_list(self, datas):
-        if(not issequenceof(datas, DatasetData)):
+        if not issequenceof(datas, DatasetData):
             raise TypeError(
                 'The data_list property must be a sequence of DatasetData '
-                'instances!')
-        self._data_list = datas
+                'instances! '
+                f'Its current type is {classname(datas)}!')
+        self._data_list = list(datas)
 
-    def change_source_hypo_group_manager(self, src_hypo_group_manager):
-        """Changes the source hypothesis group manager. Derived classes can
-        reimplement this method but this method of the base class must still be
-        called by the derived class.
-        """
-        self.src_hypo_group_manager = src_hypo_group_manager
-
-    @abc.abstractmethod
-    def generate_signal_events(self, rss, mean, poisson=True):
-        """This abstract method must be implemented by the derived class to
-        generate a given number of signal events.
+    @property
+    def arr(self):
+        """(read-only) The (N_datasets, N_source_hypo_groups)-shaped numpy
+        ndarray of object holding the constructed DetSigYield instances.
+        """
+        return self._arr
+
+    @property
+    def n_datasets(self):
+        """(read-only) The number of datasets this service was created for.
+        """
+        return self._arr.shape[0]
+
+    @property
+    def n_shgs(self):
+        """(read-only) The number of source hypothesis groups this service was
+        created for.
+        """
+        return self._arr.shape[1]
+
+    def _set_shg_mgr(self, mgr):
+        """Sets the internal member variable to the given instance of
+        SourceHypoGroupManager.
+        """
+        if not isinstance(mgr, SourceHypoGroupManager):
+            raise TypeError(
+                'The shg_mgr argument must be an instance of '
+                'SourceHypoGroupManager! '
+                f'Its current type is {classname(mgr)}!')
+
+        self._shg_mgr = mgr
+
+    def change_shg_mgr(
+            self,
+            shg_mgr,
+            ppbar=None,
+    ):
+        """Changes the instance of SourceHypoGroupManager of this service. This
+        will also rebuild the detector signal yields.
+        """
+        self._set_shg_mgr(shg_mgr)
+
+        self._arr = self.construct_detsigyield_array(
+            ppbar=ppbar)
+
+    def get_builder_to_shgidxs_dict(
+            self,
+            ds_idx,
+    ):
+        """Creates a dictionary with the builder instance as key and the list of
+        source hypo group indices to which the builder applies as value.
+        Hence, SHGs using the same builder instance can be grouped for
+        DetSigYield construction.
 
         Parameters
         ----------
-        rss : instance of RandomStateService
-            The instance of RandomStateService providing the random number
-            generator state.
-        mean : float
-            The mean number of signal events. If the ``poisson`` argument is set
-            to True, the actual number of generated signal events will be drawn
-            from a Poisson distribution with this given mean value of signal
-            events.
-        poisson : bool
-            If set to True, the actual number of generated signal events will
-            be drawn from a Poisson distribution with the given mean value of
-            signal events.
-            If set to False, the argument ``mean`` specifies the actual number
-            of generated signal events.
+        ds_idx : int
+            The index of the dataset for which the same builders apply.
 
         Returns
         -------
-        n_signal : int
-            The number of generated signal events.
-        signal_events_dict : dict of DataFieldRecordArray
-            The dictionary holding the DataFieldRecordArray instancs with the
-            generated signal events. Each key of this dictionary represents the
-            dataset index for which the signal events have been generated.
-        """
-        pass
-
-
-class SignalGenerator(SignalGeneratorBase):
-    """This is the general signal generator class. It does not depend on the
-    detector or source hypothesis, because these dependencies are factored out
-    into the signal generation method. In fact the construction within this
-    class depends on the construction of the signal generation method. In case
-    of multiple sources the handling here is very suboptimal. Therefore the
-    MultiSourceSignalGenerator should be used instead!
-    """
-    def __init__(self, src_hypo_group_manager, dataset_list, data_list,
-                 *args, **kwargs):
-        """Constructs a new signal generator instance.
+        builder_shgidxs_dict : dict
+            The dictionary with the builder instance as key and the list of
+            source hypo group indices to which the builder applies as value.
+        """
+        n_datasets = len(self._dataset_list)
+
+        if ds_idx < 0 or ds_idx >= n_datasets:
+            raise ValueError(
+                f'The dataset index {ds_idx} must be within the range '
+                f'[0,{n_datasets-1}]!')
+
+        builder_shgidxs_dict = defaultdict(list)
+        for (g, shg) in enumerate(self._shg_mgr.shg_list):
+
+            builder_list = shg.detsigyield_builder_list
+            if (len(builder_list) != 1) and (len(builder_list) != n_datasets):
+                raise ValueError(
+                    'The number of detector signal yield builders '
+                    f'({len(builder_list)}) is not 1 and does not '
+                    f'match the number of datasets ({n_datasets}) for the '
+                    f'{g}th source hypothesis group!')
+
+            builder = (
+                builder_list[0] if len(builder_list) == 1 else
+                builder_list[ds_idx]
+            )
+
+            builder_shgidxs_dict[builder].append(g)
+
+        return builder_shgidxs_dict
+
+    def construct_detsigyield_array(
+            self,
+            ppbar=None,
+    ):
+        """Creates a (N_datasets, N_source_hypo_groups)-shaped numpy ndarray of
+        object holding the constructed DetSigYield instances.
+
+        If the same DetSigYieldBuilder class is used for all source hypotheses
+        of a particular dataset, the
+        :meth:`~skyllh.core.detsigyield.DetSigYieldBuilder.construct_detsigyields`
+        method is called with different flux models to optimize the construction
+        of the detector signal yield functions.
 
         Parameters
         ----------
-        src_hypo_group_manager : SourceHypoGroupManager instance
-            The SourceHypoGroupManager instance defining the source groups with
-            their spectra.
-        dataset_list : list of Dataset instances
-            The list of Dataset instances for which signal events should get
-            generated for.
-        data_list : list of DatasetData instances
-            The list of DatasetData instances holding the actual data of each
-            dataset. The order must match the order of ``dataset_list``.
-        kwargs
-            A typical keyword argument is the instance of MultiDatasetTCLLHRatio.
-        """
-        super().__init__(
-            *args,
-            src_hypo_group_manager=src_hypo_group_manager,
-            dataset_list=dataset_list,
-            data_list=data_list,
-            **kwargs)
+        ppbar : instance of ProgressBar | None
+            The instance of ProgressBar of the optional parent progress bar.
 
-        self._construct_signal_candidates()
-
-    def _construct_signal_candidates(self):
-        """Constructs an array holding pointer information of signal candidate
-        events pointing into the real MC dataset(s).
+        Returns
+        -------
+        detsigyield_arr : instance of numpy.ndarray
+            The (N_datasets, N_source_hypo_groups)-shaped numpy ndarray of
+            object holding the constructed DetSigYield instances.
         """
         n_datasets = len(self._dataset_list)
-        n_sources = self._src_hypo_group_manager.n_sources
-        shg_list = self._src_hypo_group_manager.src_hypo_group_list
-        sig_candidates_dtype = [
-            ('ds_idx', get_smallest_numpy_int_type((0, n_datasets))),
-            ('ev_idx', get_smallest_numpy_int_type(
-                [0]+[len(data.mc) for data in self._data_list])),
-            ('shg_idx', get_smallest_numpy_int_type((0, n_sources))),
-            ('shg_src_idx', get_smallest_numpy_int_type(
-                [0]+[shg.n_sources for shg in shg_list])),
-            ('weight', np.float64)
-        ]
-        self._sig_candidates = np.empty(
-            (0,), dtype=sig_candidates_dtype, order='F')
 
-        # Go through the source hypothesis groups to get the signal event
-        # candidates.
-        for ((shg_idx,shg), (j,(ds,data))) in itertools.product(
-            enumerate(shg_list), enumerate(zip(self._dataset_list, self._data_list))):
-            sig_gen_method = shg.sig_gen_method
-            if(sig_gen_method is None):
-                raise ValueError('No signal generation method has been '
-                    'specified for the %dth source hypothesis group!'%(shg_idx))
-            data_mc = data.mc
-            (ev_indices_list, flux_list) = sig_gen_method.calc_source_signal_mc_event_flux(
-                data_mc, shg
-            )
-            for (k, (ev_indices, flux)) in enumerate(zip(ev_indices_list, flux_list)):
-                ev = data_mc[ev_indices]
-                # The weight of the event specifies the number of signal events
-                # this one event corresponds to for the given reference flux.
-                # [weight] = GeV cm^2 sr * s * 1/(GeV cm^2 s sr)
-                weight = ev['mcweight'] * data.livetime * 86400 * flux
-
-                sig_candidates = np.empty(
-                    (len(ev_indices),), dtype=sig_candidates_dtype, order='F'
-                )
-                sig_candidates['ds_idx'] = j
-                sig_candidates['ev_idx'] = ev_indices
-                sig_candidates['shg_idx'] = shg_idx
-                sig_candidates['shg_src_idx'] = k
-                sig_candidates['weight'] = weight
-
-                self._sig_candidates = np.append(self._sig_candidates, sig_candidates)
-
-        # Normalize the signal candidate weights.
-        self._sig_candidates_weight_sum = np.sum(self._sig_candidates['weight'])
-        self._sig_candidates['weight'] /= self._sig_candidates_weight_sum
-
-    def change_source_hypo_group_manager(self, src_hypo_group_manager):
-        """Recreates the signal candidates with the changed source hypothesis
-        group manager.
-        """
-        super().change_source_hypo_group_manager(src_hypo_group_manager)
-
-        self._construct_signal_candidates()
-
-    def mu2flux(self, mu, per_source=False):
-        """Translate the mean number of signal events `mu` into the
-        corresponding flux. The unit of the returned flux is 1/(GeV cm^2 s).
+        detsigyield_arr = np.empty(
+            (n_datasets,
+             self._shg_mgr.n_src_hypo_groups),
+            dtype=object
+        )
+
+        pbar = ProgressBar(
+            self._shg_mgr.n_src_hypo_groups * n_datasets,
+            parent=ppbar).start()
+
+        shg_list = self.shg_mgr.shg_list
+
+        for (j, (dataset, data)) in enumerate(zip(self._dataset_list,
+                                                  self._data_list)):
+
+            builder_to_shgidxs_dict = self.get_builder_to_shgidxs_dict(ds_idx=j)
+
+            for (builder, shgidxs) in builder_to_shgidxs_dict.items():
+                factory = builder.get_detsigyield_construction_factory()
+                if factory is None:
+                    # The builder does not provide a factory for DetSigYield
+                    # instance construction. So we have to construct the
+                    # detector signal yields one by one for each SHG.
+                    for g in shgidxs:
+                        shg = shg_list[g]
+
+                        detsigyield = builder.construct_detsigyield(
+                            dataset=dataset,
+                            data=data,
+                            shg=shg,
+                            ppbar=pbar)
+
+                        detsigyield_arr[j, g] = detsigyield
+
+                        pbar.increment()
+                else:
+                    # The builder provides a factory for the construction of
+                    # several DetSigYield instances simultaniously, one for each
+                    # flux model.
+                    shgs = [
+                        shg_list[g]
+                        for g in shgidxs
+                    ]
+
+                    detsigyields = factory(
+                        dataset=dataset,
+                        data=data,
+                        shgs=shgs,
+                        ppbar=pbar)
+
+                    for (i, g) in enumerate(shgidxs):
+                        detsigyield_arr[j, g] = detsigyields[i]
+
+                    pbar.increment(len(detsigyields))
+
+        pbar.finish()
+
+        return detsigyield_arr
+
+
+class SrcDetSigYieldWeightsService(
+        object):
+    r"""This class provides a service for the source detector signal yield
+    weights, which are the product of the source weights with the detector
+    signal yield, denoted with :math:`a_{j,k}(\vec{p}_{\mathrm{s}_k})` in the
+    math formalism documentation.
+
+    .. math::
+
+        a_{j,k}(\vec{p}_{\mathrm{s}_k}) = W_k
+            \mathcal{Y}_{\mathrm{s}_{j,k}}(\vec{p}_{\mathrm{s}_k})
+
+    The service has a method to calculate the weights and a method to retrieve
+    the weights. The weights are stored internally.
+    """
+
+    @staticmethod
+    def create_src_recarray_list_list(
+            detsigyield_service,
+    ):
+        """Creates a list of numpy record ndarrays, one for each source
+        hypothesis group suited for evaluating the detector signal yield
+        instance of that source hypothesis group.
 
         Parameters
         ----------
-        mu : float
-            The mean number of expected signal events for which to get the flux.
-        per_source : bool
-            Flag if the flux should be returned for each source individually
-            (True), or as the sum of all these fluxes (False). The default is
-            False.
+        detsigyield_service : instance of DetSigYieldService
+            The instance of DetSigYieldService providing the
+            (N_datasets, N_source_hypo_groups)-shaped 2D ndarray of
+            DetSigYield instances, one for each dataset and source hypothesis
+            group combination.
 
         Returns
         -------
-        mu_flux : float | (n_sources,)-shaped numpy ndarray
-            The total flux for all sources (if `per_source = False`) that would
-            correspond to the given mean number of detected signal events `mu`.
-            If `per_source` is set to True, a numpy ndarray is returned that
-            contains the flux for each individual source.
-        """
-        # Calculate the expected mean number of signal events for each source
-        # of the source hypo group manager. For each source we can calculate the
-        # flux that would correspond to the given mean number of signal events
-        # `mu`. The total flux for all sources is then just the sum.
-
-        # The ref_N variable describes how many total signal events are expected
-        # on average for the reference fluxes.
-        ref_N = self._sig_candidates_weight_sum
-
-        # The mu_fluxes array is the flux of each source for mu mean detected
-        # signal events.
-        n_sources = self._src_hypo_group_manager.n_sources
-        mu_fluxes = np.empty((n_sources,), dtype=np.float64)
-
-        shg_list = self._src_hypo_group_manager.src_hypo_group_list
-        mu_fluxes_idx_offset = 0
-        for (shg_idx,shg) in enumerate(shg_list):
-            fluxmodel = shg.fluxmodel
-            # Calculate conversion factor from the flux model unit into the
-            # internal flux unit GeV^-1 cm^-2 s^-1.
-            toGeVcm2s = get_conversion_factor_to_internal_flux_unit(fluxmodel)
-            for k in range(shg.n_sources):
-                mask = ((self._sig_candidates['shg_idx'] == shg_idx) &
-                        (self._sig_candidates['shg_src_idx'] == k))
-                ref_N_k = np.sum(self._sig_candidates[mask]['weight']) * ref_N
-                mu_flux_k = mu / ref_N * (ref_N_k / ref_N) * fluxmodel.Phi0*toGeVcm2s
-                mu_fluxes[mu_fluxes_idx_offset + k] = mu_flux_k
-            mu_fluxes_idx_offset += shg.n_sources
-
-        if(per_source):
-            return mu_fluxes
-
-        mu_flux = np.sum(mu_fluxes)
-        return mu_flux
-
-    def generate_signal_events(self, rss, mean, poisson=True):
-        """Generates a given number of signal events from the signal candidate
-        monte-carlo events.
+        src_recarray_list_list : list of list of numpy record ndarrays
+            The (N_datasets,N_source_hypo_groups)-shaped list of list of the
+            source numpy record ndarrays, one for each dataset and source
+            hypothesis group combination, which is needed for
+            evaluating a particular detector signal yield instance.
+        """
+        n_datasets = detsigyield_service.n_datasets
+        n_shgs = detsigyield_service.n_shgs
+        shg_list = detsigyield_service.shg_mgr.shg_list
+
+        src_recarray_list_list = []
+        for ds_idx in range(n_datasets):
+            src_recarray_list = []
+            for shg_idx in range(n_shgs):
+                shg = shg_list[shg_idx]
+                src_recarray_list.append(
+                    detsigyield_service.arr[ds_idx][shg_idx].sources_to_recarray(
+                        shg.source_list))
+
+            src_recarray_list_list.append(src_recarray_list)
+
+        return src_recarray_list_list
+
+    @staticmethod
+    def create_src_weight_array_list(
+            shg_mgr,
+    ):
+        """Creates a list of numpy 1D ndarrays holding the source weights, one
+        for each source hypothesis group.
 
         Parameters
         ----------
-        rss : instance of RandomStateService
-            The instance of RandomStateService providing the random number
-            generator state.
-        mean : float
-            The mean number of signal events. If the ``poisson`` argument is set
-            to True, the actual number of generated signal events will be drawn
-            from a Poisson distribution with this given mean value of signal
-            events.
-        poisson : bool
-            If set to True, the actual number of generated signal events will
-            be drawn from a Poisson distribution with the given mean value of
-            signal events.
-            If set to False, the argument ``mean`` specifies the actual number
-            of generated signal events.
+        shg_mgr : instance of SourceHypoGroupManager
+            The instance of SourceHypoGroupManager defining the source
+            hypothesis groups with their sources.
 
         Returns
         -------
-        n_signal : int
-            The number of generated signal events.
-        signal_events_dict : dict of DataFieldRecordArray
-            The dictionary holding the DataFieldRecordArray instancs with the
-            generated signal events. Each key of this dictionary represents the
-            dataset index for which the signal events have been generated.
-        """
-        if(poisson):
-            mean = rss.random.poisson(float_cast(
-                mean, 'The mean argument must be castable to type of float!'))
-
-        n_signal = int_cast(
-            mean, 'The mean argument must be castable to type of int!')
-
-        # Draw n_signal signal candidates according to their weight.
-        sig_events_meta = rss.random.choice(
-            self._sig_candidates,
-            size=n_signal,
-            p=self._sig_candidates['weight']
-        )
-        # Get the list of unique dataset and source hypothesis group indices of
-        # the drawn signal events.
-        # Note: This code does not assume the same format for each of the
-        #       individual MC dataset numpy record arrays, thus might be a bit
-        #       slower. If one could assume the same MC dataset format, one
-        #       could gather all the MC events of all the datasets first and do
-        #       the signal event post processing for all datasets at once.
-        signal_events_dict = dict()
-        ds_idxs = np.unique(sig_events_meta['ds_idx'])
-        for ds_idx in ds_idxs:
-            mc = self._data_list[ds_idx].mc
-            ds_mask = sig_events_meta['ds_idx'] == ds_idx
-            n_sig_events_ds = np.count_nonzero(ds_mask)
-
-            data = dict(
-                [(fname, np.empty(
-                    (n_sig_events_ds,),
-                    dtype=mc.get_field_dtype(fname))
-                 ) for fname in mc.field_name_list])
-            sig_events = DataFieldRecordArray(data, copy=False)
-
-            fill_start_idx = 0
-            # Get the list of unique source hypothesis group indices for the
-            # current dataset.
-            shg_idxs = np.unique(sig_events_meta[ds_mask]['shg_idx'])
-            for shg_idx in shg_idxs:
-                shg = self._src_hypo_group_manager.src_hypo_group_list[shg_idx]
-                shg_mask = sig_events_meta['shg_idx'] == shg_idx
-                # Get the MC events for the drawn signal events.
-                ds_shg_mask = ds_mask & shg_mask
-                shg_sig_events_meta = sig_events_meta[ds_shg_mask]
-                n_shg_sig_events = len(shg_sig_events_meta)
-                ev_idx = shg_sig_events_meta['ev_idx']
-                # Get the signal MC events of the current dataset and source
-                # hypothesis group.
-                shg_sig_events = mc.get_selection(ev_idx)
-
-                # Do the signal event post sampling processing.
-                shg_sig_events = shg.sig_gen_method.signal_event_post_sampling_processing(
-                    shg, shg_sig_events_meta, shg_sig_events)
-
-                indices = np.indices((n_shg_sig_events,))[0] + fill_start_idx
-                sig_events.set_selection(indices, shg_sig_events)
-
-                #sig_events[fill_start_idx:fill_start_idx+n_shg_sig_events] = shg_sig_events
-                fill_start_idx += n_shg_sig_events
+        src_weight_array_list : list of numpy 1D ndarrays
+            The list of 1D numpy ndarrays holding the source weights, one for
+            each source hypothesis group.
+        """
+        src_weight_array_list = [
+            np.array([src.weight for src in shg.source_list])
+            for shg in shg_mgr.shg_list
+        ]
+        return src_weight_array_list
+
+    def __init__(
+            self,
+            detsigyield_service,
+            **kwargs,
+    ):
+        """Creates a new SrcDetSigYieldWeightsService instance.
+
+        Parameters
+        ----------
+        detsigyield_service : instance of DetSigYieldService
+            The instance of DetSigYieldService providing the
+            (N_datasets, N_source_hypo_groups)-shaped array of DetSigYield
+            instances, one instance for each combination of dataset and source
+            hypothesis group.
+        """
+        super().__init__(
+            **kwargs)
+
+        self.detsigyield_service = detsigyield_service
+
+        # Create the list of list of source record arrays for each combination
+        # of dataset and source hypothesis group.
+        self._src_recarray_list_list = type(self).create_src_recarray_list_list(
+            detsigyield_service=self._detsigyield_service)
+
+        # Create the list of 1D ndarrays holding the source weights for each
+        # source hypothesis group.
+        self._src_weight_array_list = type(self).create_src_weight_array_list(
+            shg_mgr=self._detsigyield_service.shg_mgr)
+
+        self._a_jk = None
+        self._a_jk_grads = None
+
+    @property
+    def shg_mgr(self):
+        """(read-only) The instance of SourceHypoGroupManager defining the
+        source hypothesis groups.
+        """
+        return self._detsigyield_service.shg_mgr
 
-            signal_events_dict[ds_idx] = sig_events
+    @property
+    def detsigyield_service(self):
+        """The instance of DetSigYieldService providing the
+        (N_datasets, N_source_hypo_groups)-shaped array of DetSigYield
+        instances.
+        """
+        return self._detsigyield_service
+
+    @detsigyield_service.setter
+    def detsigyield_service(self, service):
+        if not isinstance(service, DetSigYieldService):
+            raise TypeError(
+                'The detsigyield_service property must be an instance of '
+                'DetSigYieldService! '
+                f'Its current type is {classname(service)}!')
+        self._detsigyield_service = service
+
+    @property
+    def detsigyield_arr(self):
+        """(read-only) The (N_datasets, N_source_hypo_groups)-shaped 2D numpy
+        ndarray holding the DetSigYield instances for each source hypothesis
+        group.
+        """
+        return self._detsigyield_service.arr
+
+    @property
+    def n_datasets(self):
+        """(read-only) The number of datasets this service was created for.
+        """
+        return self._detsigyield_service.n_datasets
+
+    @property
+    def n_shgs(self):
+        """(read-only) The number of source hypothesis groups this service was
+        created for.
+        """
+        return self._detsigyield_service.n_shgs
+
+    @property
+    def src_recarray_list_list(self):
+        """(read-only) The (N_datasets,N_source_hypo_groups)-shaped list of list
+        of the source numpy record ndarrays, one for each dataset and source
+        hypothesis group combination, which is needed for evaluating a
+        particular detector signal yield instance.
+        """
+        return self._src_recarray_list_list
+
+    def change_shg_mgr(
+            self,
+            shg_mgr,
+    ):
+        """Re-creates the internal source numpy record arrays needed for the
+        detector signal yield calculation.
+
+        Parameters
+        ----------
+        shg_mgr : instance of SourceHypoGroupManager
+            The new SourceHypoGroupManager instance.
+        """
+        if id(shg_mgr) != id(self._detsigyield_service.shg_mgr):
+            raise ValueError(
+                'The provides instance of SourceHypoGroupManager does not '
+                'match the instance of the detector signal yield service!')
+
+        self._src_recarray_list_list = type(self).create_src_recarray_list_list(
+            detsigyield_service=self._detsigyield_service)
+
+        self._src_weight_array_list = type(self).create_src_weight_array_list(
+            shg_mgr=self._detsigyield_service.shg_mgr)
+
+    def calculate(
+            self,
+            src_params_recarray):
+        """Calculates the source detector signal yield weights for each source
+        and their derivative w.r.t. each global floating parameter. The result
+        is stored internally as:
+
+            a_jk : instance of ndarray
+                The (N_datasets,N_sources)-shaped numpy ndarray holding the
+                source detector signal yield weight for each combination of
+                dataset and source.
+            a_jk_grads : dict
+                The dictionary holding the (N_datasets,N_sources)-shaped numpy
+                ndarray with the derivatives w.r.t. the global fit parameter
+                the SrcDetSigYieldWeightsService depend on. The dictionary's key
+                is the index of the global fit parameter.
+
+        Parameters
+        ----------
+        src_params_recarray : instance of numpy record ndarray
+            The numpy record ndarray of length N_sources holding the local
+            source parameters. See the documentation of
+            :meth:`skyllh.core.parameters.ParameterModelMapper.create_src_params_recarray`
+            for more information about this record array.
+        """
+        n_datasets = self.n_datasets
 
-        return (n_signal, signal_events_dict)
+        shg_mgr = self._detsigyield_service.shg_mgr
 
+        self._a_jk = np.empty(
+            (n_datasets, shg_mgr.n_sources,),
+            dtype=np.double)
 
-class MultiSourceSignalGenerator(SignalGenerator):
-    """More optimal signal generator for multiple sources.
+        self._a_jk_grads = defaultdict(
+            lambda: np.zeros(
+                (n_datasets, shg_mgr.n_sources),
+                dtype=np.double))
+
+        sidx = 0
+        for (shg_idx, (shg, src_weights)) in enumerate(zip(
+                shg_mgr.shg_list, self._src_weight_array_list)):
+
+            shg_n_src = shg.n_sources
+
+            shg_src_slice = slice(sidx, sidx+shg_n_src)
+
+            shg_src_params_recarray = src_params_recarray[shg_src_slice]
+
+            for ds_idx in range(n_datasets):
+                detsigyield = self._detsigyield_service.arr[ds_idx, shg_idx]
+                src_recarray = self._src_recarray_list_list[ds_idx][shg_idx]
+
+                (Yg, Yg_grads) = detsigyield(
+                    src_recarray=src_recarray,
+                    src_params_recarray=shg_src_params_recarray)
+
+                self._a_jk[ds_idx][shg_src_slice] = src_weights * Yg
+
+                for gpidx in Yg_grads.keys():
+                    self._a_jk_grads[gpidx][ds_idx, shg_src_slice] =\
+                        src_weights * Yg_grads[gpidx]
+
+            sidx += shg_n_src
+
+    def get_weights(self):
+        """Returns the source detector signal yield weights and their
+        derivatives w.r.t. the global fit parameters.
+
+        Returns
+        -------
+        a_jk : instance of ndarray
+            The (N_datasets, N_sources)-shaped numpy ndarray holding the
+            source detector signal yield weight for each combination of
+            dataset and source.
+        a_jk_grads : dict
+            The dictionary holding the (N_datasets, N_sources)-shaped numpy
+            ndarray with the derivatives w.r.t. the global fit parameter
+            the SrcDetSigYieldWeightsService depend on. The dictionary's key
+            is the index of the global fit parameter.
+        """
+        return (self._a_jk, self._a_jk_grads)
+
+
+class DatasetSignalWeightFactorsService(
+        object):
+    r"""This class provides a service to calculates the dataset signal weight
+    factors, :math:`f_j(\vec{p}_\mathrm{s})`, for each dataset.
+    It utilizes the source detector signal yield weights
+    :math:`a_{j,k}(\vec{p}_{\mathrm{s}_k})`, provided by the
+    :class:`~SrcDetSigYieldWeightsService` class.
     """
-    def __init__(self, src_hypo_group_manager, dataset_list, data_list,
-                **kwargs):
-        """Constructs a new signal generator instance.
+
+    def __init__(
+            self,
+            src_detsigyield_weights_service):
+        r"""Creates a new DatasetSignalWeightFactors instance.
 
         Parameters
         ----------
-        src_hypo_group_manager : SourceHypoGroupManager instance
-            The SourceHypoGroupManager instance defining the source groups with
-            their spectra.
-        dataset_list : list of Dataset instances
-            The list of Dataset instances for which signal events should get
-            generated for.
-        data_list : list of DatasetData instances
-            The list of DatasetData instances holding the actual data of each
-            dataset. The order must match the order of ``dataset_list``.
-        kwargs
-            A typical keyword argument is the instance of MultiDatasetTCLLHRatio.
-        """
-        super(MultiSourceSignalGenerator, self).__init__(
-            src_hypo_group_manager, dataset_list, data_list, **kwargs)
-
-    def _construct_signal_candidates(self):
-        """Constructs an array holding pointer information of signal candidate
-        events pointing into the real MC dataset(s).
+        src_detsigyield_weights_service : instance of SrcDetSigYieldWeightsService
+            The instance of SrcDetSigYieldWeightsService providing the source
+            detector signal yield weights
+            :math:`a_{j,k}(\vec{p}_{\mathrm{s}_k})`.
         """
-        n_datasets = len(self._dataset_list)
-        n_sources = self._src_hypo_group_manager.n_sources
-        shg_list = self._src_hypo_group_manager.src_hypo_group_list
-        sig_candidates_dtype = [
-            ('ds_idx', get_smallest_numpy_int_type((0, n_datasets))),
-            ('ev_idx', get_smallest_numpy_int_type(
-                [0]+[len(data.mc) for data in self._data_list])),
-            ('shg_idx', get_smallest_numpy_int_type((0, n_sources))),
-            ('shg_src_idx', get_smallest_numpy_int_type(
-                [0]+[shg.n_sources for shg in shg_list])),
-            ('weight', np.float64)
-        ]
-        self._sig_candidates = np.empty(
-            (0,), dtype=sig_candidates_dtype, order='F')
+        self.src_detsigyield_weights_service = src_detsigyield_weights_service
 
-        # Go through the source hypothesis groups to get the signal event
-        # candidates.
-        for ((shg_idx, shg), (j, (ds, data))) in itertools.product(
-                enumerate(shg_list),
-                enumerate(zip(self._dataset_list, self._data_list))):
-            sig_gen_method = shg.sig_gen_method
-            if(sig_gen_method is None):
-                raise ValueError(
-                    'No signal generation method has been specified '
-                    'for the %dth source hypothesis group!' % (shg_idx))
-            data_mc = data.mc
-            (ev_indices, src_indices, flux) = sig_gen_method.calc_source_signal_mc_event_flux(
-                data_mc, shg)
-
-            sig_candidates = np.empty(
-                    (len(ev_indices),), dtype=sig_candidates_dtype, order='F'
-                )
-            sig_candidates['ds_idx'] = j
-            sig_candidates['ev_idx'] = ev_indices
-            sig_candidates['shg_idx'] = shg_idx
-            sig_candidates['shg_src_idx'] = src_indices
-            sig_candidates['weight'] = data_mc[ev_indices]['mcweight'] * data.livetime * 86400 * flux
-
-            self._sig_candidates = np.append(self._sig_candidates, sig_candidates)
-            del sig_candidates
-
-        # Normalize the signal candidate weights.
-        self._sig_candidates_weight_sum = np.sum(self._sig_candidates['weight'])
-        self._sig_candidates['weight'] /= self._sig_candidates_weight_sum
+    @property
+    def src_detsigyield_weights_service(self):
+        r"""The instance of SrcDetSigYieldWeightsService providing the source
+        detector signal yield weights :math:`a_{j,k}(\vec{p}_{\mathrm{s}_k})`.
+        """
+        return self._src_detsigyield_weights_service
+
+    @src_detsigyield_weights_service.setter
+    def src_detsigyield_weights_service(self, service):
+        if not isinstance(service, SrcDetSigYieldWeightsService):
+            raise TypeError(
+                'The src_detsigyield_weights_service property must be an '
+                'instance of SrcDetSigYieldWeightsService!')
+        self._src_detsigyield_weights_service = service
+
+    @property
+    def n_datasets(self):
+        """(read-only) The number of datasets.
+        """
+        return self._src_detsigyield_weights_service.n_datasets
+
+    def calculate(self):
+        r"""Calculates the dataset signal weight factors,
+        :math:`f_j(\vec{p}_\mathrm{s})`. The result is stored internally as:
+
+            f_j : instance of ndarray
+                The (N_datasets,)-shaped 1D numpy ndarray holding the dataset
+                signal weight factor for each dataset.
+            f_j_grads : dict
+                The dictionary holding the (N_datasets,)-shaped numpy
+                ndarray with the derivatives w.r.t. the global fit parameter
+                the DatasetSignalWeightFactorsService depend on.
+                The dictionary's key is the index of the global fit parameter.
+        """
+        (a_jk, a_jk_grads) = self._src_detsigyield_weights_service.get_weights()
+
+        a_j = np.sum(a_jk, axis=1)
+        a = np.sum(a_jk)
+
+        self._f_j = a_j / a
+
+        # Calculate the derivative of f_j w.r.t. all floating parameters present
+        # in the a_jk_grads using the quotient rule of differentation.
+        self._f_j_grads = dict()
+        for gpidx in a_jk_grads.keys():
+            # a is a scalar.
+            # a_j is a (N_datasets)-shaped ndarray.
+            # a_jk_grads is a dict of length N_gfl_params with values of
+            #    (N_datasets,N_sources)-shaped ndarray.
+            # a_j_grads is a (N_datasets,)-shaped ndarray.
+            # a_grads is a scalar.
+            a_j_grads = np.sum(a_jk_grads[gpidx], axis=1)
+            a_grads = np.sum(a_jk_grads[gpidx])
+            self._f_j_grads[gpidx] = (a_j_grads * a - a_j * a_grads) / a**2
+
+    def get_weights(self):
+        """Returns the
+
+        Returns
+        -------
+        f_j : instance of ndarray
+            The (N_datasets,)-shaped 1D numpy ndarray holding the dataset
+            signal weight factor for each dataset.
+        f_j_grads : dict
+            The dictionary holding the (N_datasets,)-shaped numpy
+            ndarray with the derivatives w.r.t. the global fit parameter
+            the DatasetSignalWeightFactorsService depend on.
+            The dictionary's key is the index of the global fit parameter.
+        """
+        return (self._f_j, self._f_j_grads)
```

### Comparing `skyllh-23.1.1/skyllh/core/smoothing.py` & `skyllh-23.2.0/skyllh/core/smoothing.py`

 * *Files 14% similar despite different names*

```diff
@@ -8,20 +8,24 @@
 
 from skyllh.core.py import issequenceof
 
 # Define a constant that can be used when specifying a histogram axis as
 # unsmooth, i.e. no smoothing should be applied along that axis.
 UNSMOOTH_AXIS = np.ones(1)
 
-class HistSmoothingMethod(object, metaclass=abc.ABCMeta):
+
+class HistSmoothingMethod(
+        object,
+        metaclass=abc.ABCMeta,
+):
     """Abstract base class for implementing a histogram smoothing method.
     """
 
-    def __init__(self):
-        super(HistSmoothingMethod, self).__init__()
+    def __init__(self, **kwargs):
+        super().__init__(**kwargs)
 
     @abc.abstractmethod
     def smooth(self, h):
         """This method is supposed to smooth the given histogram h.
 
         Parameters
         ----------
@@ -32,19 +36,21 @@
         -------
         smoothed_h : N-dimensional ndarray
             The array holding the smoothed histogram bin values.
         """
         pass
 
 
-class NoHistSmoothingMethod(HistSmoothingMethod):
+class NoHistSmoothingMethod(
+        HistSmoothingMethod,
+):
     """This class implements a no-shoothing histogram method.
     """
-    def __init__(self):
-        super(NoHistSmoothingMethod, self).__init__()
+    def __init__(self, **kwargs):
+        super().__init__(**kwargs)
 
     def smooth(self, h):
         """Does not perform any smoothing and just returns the input histogram.
 
         Parameters
         ----------
         h : N-dimensional ndarray
@@ -54,133 +60,176 @@
         -------
         h : N-dimensional ndarray
             The input histogram array.
         """
         return h
 
 
-class NeighboringBinHistSmoothingMethod(HistSmoothingMethod):
-    """This class implements
-    """
-    def __init__(self, axis_kernel_arrays):
+class NeighboringBinHistSmoothingMethod(
+        HistSmoothingMethod,
+):
+    """This class implements a smoothing algorithm that smoothes a histogram
+    based on the neighboring bins.
+    """
+    def __init__(
+            self,
+            axis_kernel_arrays,
+            **kwargs,
+    ):
         """Constructs a new neighboring bin histogram smoothing method.
 
         Parameters
         ----------
         axis_kernel_arrays: sequence of 1D ndarrays
             The sequence of smoothing kernel arrays, one for each axis. If an
             axis should not get smoothed, the UNSMOOTH_AXIS constant should be
             used for that axis' smoothing kernel array.
         """
-        super(NeighboringBinHistSmoothingMethod, self).__init__()
+        super().__init__(**kwargs)
 
-        if(not issequenceof(axis_kernel_arrays, np.ndarray)):
-            raise TypeError('The axis_kernel_arrays argument must be a sequence of numpy.ndarray instances!')
+        if not issequenceof(axis_kernel_arrays, np.ndarray):
+            raise TypeError(
+                'The axis_kernel_arrays argument must be a sequence of '
+                'numpy.ndarray instances!')
 
         self._ndim = len(axis_kernel_arrays)
 
         # Construct the smoothing kernel k used by the smooth method.
         # k is a N-dimensional ndarray. It defines which neighboring bin values
         # of the histogram will contribute how much to the central bin value.
-        self._k = np.product(np.meshgrid(*axis_kernel_arrays, indexing='ij'), axis=0)
+        self._k = np.product(
+            np.meshgrid(*axis_kernel_arrays, indexing='ij'), axis=0)
 
     @property
     def ndim(self):
         """(read-only) The dimensionality of the histogram this smoothing
         instances is made for.
         """
         return self._ndim
 
     def smooth(self, h):
-        """Smoothes the given histogram array h with the internal kernel array k. Both arrays must have the same dimensionality. The shape
+        """Smoothes the given histogram array h with the internal kernel array
+        k. Both arrays must have the same dimensionality. The shape
         values of k must be smaller than or equal to the shape values of h.
 
         Parameters
         ----------
         h : N-dimensional ndarray
             The ndarray holding histogram bin values.
 
         Returns
         -------
         smoothed_h : N-dimensional ndarray.
         """
-        if(h.ndim != self._ndim):
-            raise ValueError('The ndarrays of argument h and k must have the same dimensionality! Currently they are %d and %d, respectively.'%(h.ndim, self._ndim))
+        if h.ndim != self._ndim:
+            raise ValueError(
+                'The ndarrays of argument h and k must have the same '
+                f'dimensionality! Currently they are {h.ndim:d} and '
+                f'{self._ndim:d}, respectively.')
         for d in range(h.ndim):
-            if(self._k.shape[d] > h.shape[d]):
-                raise ValueError('The shape value (%d) of dimension %d of ndarray k must be smaller than or equal to the shape value (%d) of dimension %d of ndarray h!'%(self._k.shape[d], d, h.shape[d], d))
+            if self._k.shape[d] > h.shape[d]:
+                raise ValueError(
+                    f'The shape value ({self._k.shape[d]:d}) of dimension '
+                    f'{d:d} of ndarray k must be smaller than or equal to the '
+                    f'shape value ({h.shape[d]:d}) of dimension {d:d} of '
+                    'ndarray h!')
 
         norm = scipy.signal.convolve(np.ones_like(h), self._k, mode="same")
         smoothed_h = scipy.signal.convolve(h, self._k, mode="same") / norm
 
         return smoothed_h
 
 
-class SmoothingFilter(object):
+class SmoothingFilter(
+        object):
     """This class provides a base class for a histogram smoothing filter. It
     provides an axis kernel array that defines how many neighboring bins of a
     histogram bin should be used to smooth that histogram bin.
     """
-    def __init__(self, axis_kernel_array):
-        super(SmoothingFilter, self).__init__()
+    def __init__(
+            self,
+            axis_kernel_array,
+            **kwargs):
+        super().__init__(**kwargs)
 
         self.axis_kernel_array = axis_kernel_array
 
     @property
     def axis_kernel_array(self):
         """The kernel array for a histogram axis.
         """
         return self._axis_kernel_array
+
     @axis_kernel_array.setter
     def axis_kernel_array(self, arr):
-        if(not isinstance(arr, np.ndarray)):
-            raise TypeError('The axis_kernel_array property must be an instance of numpy.ndarray!')
+        if not isinstance(arr, np.ndarray):
+            raise TypeError(
+                'The axis_kernel_array property must be an instance of '
+                'numpy.ndarray!')
         self._axis_kernel_array = arr
 
 
-class BlockSmoothingFilter(SmoothingFilter):
+class BlockSmoothingFilter(
+        SmoothingFilter,
+):
     """This class defines the histogram smoothing filter for smoothing a
     histogram via a block kernel function. The half-width of that
     block is specified via the nbins argument.
     """
-    def __init__(self, nbins):
-        """
+    def __init__(
+            self,
+            nbins,
+            **kwargs
+    ):
+        """Creates a new BlockSmoothingFilter instance.
+
         Parameters
         ----------
         nbins : int
             The number of neighboring bins into one direction of a histogram
             bin, which should be used to smooth that histogram bin.
         """
-        if(not isinstance(nbins, int)):
-            raise TypeError('The nbins argument must be of type int!')
-        if(nbins <= 0):
-            raise ValueError('The nbins argument must be greater zero!')
+        if not isinstance(nbins, int):
+            raise TypeError(
+                'The nbins argument must be of type int!')
+        if nbins <= 0:
+            raise ValueError(
+                'The nbins argument must be greater zero!')
 
         arr = np.ones(2*nbins + 1, dtype=np.float64)
 
-        super(BlockSmoothingFilter, self).__init__(arr)
+        super().__init__(arr, **kwargs)
 
 
-class GaussianSmoothingFilter(SmoothingFilter):
+class GaussianSmoothingFilter(
+        SmoothingFilter,
+):
     """This class defines the histogram smoothing filter for smoothing a
     histogram via a Gaussian kernel function. The width of that Gaussian is
     approximately one standard deviation, spread over nbins on each side of the
     central histogram bin.
     """
-    def __init__(self, nbins):
-        """
+    def __init__(
+            self,
+            nbins,
+            **kwargs,
+    ):
+        """Creates a new GaussianSmoothingFilter instance.
+
         Parameters
         ----------
         nbins : int
             The number of neighboring bins into one direction of a histogram
             bin, which should be used to smooth that histogram bin.
         """
-        if(not isinstance(nbins, int)):
-            raise TypeError('The nbins argument must be of type int!')
-        if(nbins <= 0):
-            raise ValueError('The nbins argument must be greater zero!')
+        if not isinstance(nbins, int):
+            raise TypeError(
+                'The nbins argument must be of type int!')
+        if nbins <= 0:
+            raise ValueError(
+                'The nbins argument must be greater zero!')
 
         val = 1.6635
         r = np.linspace(-val, val, 2*nbins + 1)
         arr = scipy.stats.norm.pdf(r)
 
-        super(GaussianSmoothingFilter, self).__init__(arr)
+        super().__init__(arr, **kwargs)
```

### Comparing `skyllh-23.1.1/skyllh/core/storage.py` & `skyllh-23.2.0/skyllh/i3/detsigyield.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,1200 +1,1125 @@
 # -*- coding: utf-8 -*-
 
+"""This module contains classes for IceCube specific detector signal yields,
+for a variation of source model and flux model combinations.
+"""
+
 import abc
-import copy
-import pickle
-import os.path
+from astropy import units
 import numpy as np
-import sys
 
+import scipy.interpolate
+
+from skyllh.core import (
+    multiproc,
+)
 from skyllh.core.py import (
     classname,
-    get_byte_size_prefix,
-    getsizeof,
-    issequence,
-    issequenceof
+    issequenceof,
+)
+from skyllh.core.binning import (
+    BinningDefinition,
+)
+from skyllh.core.parameters import (
+    ParameterGrid,
+)
+from skyllh.core.detsigyield import (
+    DetSigYield,
+    DetSigYieldBuilder,
+)
+from skyllh.core.livetime import (
+    Livetime,
+)
+from skyllh.core.source_model import (
+    PointLikeSource,
 )
-from skyllh.core import display as dsp
-
 
-# Define a file loader registry that holds the FileLoader classes for different
-# file formats.
-_FILE_LOADER_REG = dict()
 
-def register_FileLoader(formats, fileloader_cls):
-    """Registers the given file formats (file extensions) to the given
-    FileLoader class.
+class I3DetSigYield(
+        DetSigYield,
+        metaclass=abc.ABCMeta):
+    """Abstract base class for all IceCube specific detector signal yield
+    classes. It assumes that sin(dec) binning is required for calculating the
+    detector effective area and hence the detector signal yield.
+    """
 
-    Parameters
-    ----------
-    formats : str | list of str
-        The list of file name extensions that should be mapped to the FileLoader
+    def __init__(
+            self,
+            param_names,
+            dataset,
+            fluxmodel,
+            livetime,
+            sin_dec_binning,
+            **kwargs):
+        """Constructor of the IceCube specific detector signal yield base
         class.
-    fileloader_cls : FileLoader
-        The subclass of FileLoader that should be used for the given file
-        formats.
-    """
-    if(isinstance(formats, str)):
-        formats = [ formats ]
-    if(not issequence(formats)):
-        raise TypeError('The "formats" argument must be a sequence!')
-    if(not issubclass(fileloader_cls, FileLoader)):
-        raise TypeError('The "fileloader_cls" argument must be a subclass of FileLoader!')
-
-    for fmt in formats:
-        if(fmt in _FILE_LOADER_REG.keys()):
-            raise KeyError('The format "%s" is already registered!'%(fmt))
-        _FILE_LOADER_REG[fmt] = fileloader_cls
-
-def create_FileLoader(pathfilenames, **kwargs):
-    """Creates the appropriate FileLoader object for the given file names.
-    It looks up the FileLoader class from the FileLoader registry for the
-    file name extension of the first file name in the given list.
-
-    Parameters
-    ----------
-    pathfilenames : str | sequence of str
-        The sequence of fully qualified file names of the files that should be
-        loaded.
-
-    Additional Parameters
-    ---------------------
-    Additional parameters will be passed to the constructor method of the
-    chosen FileLoader class.
-
-    Returns
-    -------
-    fileloader : FileLoader
-        The appropiate FileLoader instance for the given type of data files.
-    """
-    if(isinstance(pathfilenames, str)):
-        pathfilenames = [pathfilenames]
-    if(not issequenceof(pathfilenames, str)):
-        raise TypeError('The pathfilenames argument must be a sequence of str!')
-
-    # Sort the file names extensions with shorter extensions before longer ones
-    # to support a format that is sub-string of another format.
-    formats = sorted(_FILE_LOADER_REG.keys())
-    for fmt in formats:
-        l = len(fmt)
-        if(pathfilenames[0][-l:].lower() == fmt.lower()):
-            cls = _FILE_LOADER_REG[fmt]
-            return cls(pathfilenames, **kwargs)
-
-    raise RuntimeError('No FileLoader class is suitable to load the data file '
-        '"%s"!'%(pathfilenames[0]))
-
-def assert_file_exists(pathfilename):
-    """Checks if the given file exists and raises a RuntimeError if it does
-    not exist.
-    """
-    if(not os.path.isfile(pathfilename)):
-        raise RuntimeError('The data file "%s" does not exist!'%(pathfilename))
+
+        Parameters
+        ----------
+        param_names : sequence of str
+            The sequence of parameter names this detector signal yield depends
+            on. These are either fixed or floating parameters.
+        dataset : Dataset instance
+            The Dataset instance holding the monte-carlo event data.
+        fluxmodel : FluxModel
+            The flux model instance. Must be an instance of FluxModel.
+        livetime : float | Livetime instance
+            The live-time.
+        sin_dec_binning : BinningDefinition instance
+            The BinningDefinition instance defining the sin(dec) binning.
+        """
+        super().__init__(
+            param_names=param_names,
+            dataset=dataset,
+            fluxmodel=fluxmodel,
+            livetime=livetime,
+            **kwargs)
+
+        self.sin_dec_binning = sin_dec_binning
+
+    @property
+    def sin_dec_binning(self):
+        """The BinningDefinition instance defining the sin(dec) binning.
+        """
+        return self._sin_dec_binning
+
+    @sin_dec_binning.setter
+    def sin_dec_binning(self, bd):
+        if not isinstance(bd, BinningDefinition):
+            raise TypeError(
+                'The sin_dec_binning property must be an instance of '
+                f'BinningDefinition! Its current type is {classname(bd)}.')
+        self._sin_dec_binning = bd
 
 
-class FileLoader(object, metaclass=abc.ABCMeta):
-    """Abstract base class for a FileLoader class.
+class I3DetSigYieldBuilder(
+        DetSigYieldBuilder,
+        metaclass=abc.ABCMeta):
+    """Abstract base class for an IceCube specific detector signal yield
+    builder class.
     """
-    def __init__(self, pathfilenames, **kwargs):
-        """Initializes a new FileLoader instance.
+
+    def __init__(
+            self,
+            sin_dec_binning=None,
+            **kwargs,
+    ):
+        """Constructor of the IceCube specific detector signal yield
+        builder class.
 
         Parameters
         ----------
-        pathfilenames : str | sequence of str
-            The sequence of fully qualified file names of the data files that
-            need to be loaded.
+        sin_dec_binning : BinningDefinition instance
+            The instance of BinningDefinition defining the sin(dec) binning.
         """
-        super(FileLoader, self).__init__(**kwargs)
+        super().__init__(**kwargs)
 
-        self.pathfilename_list = pathfilenames
+        self.sin_dec_binning = sin_dec_binning
 
     @property
-    def pathfilename_list(self):
-        """The list of fully qualified file names of the data files.
-        """
-        return self._pathfilename_list
-    @pathfilename_list.setter
-    def pathfilename_list(self, pathfilenames):
-        if(isinstance(pathfilenames, str)):
-            pathfilenames = [ pathfilenames ]
-        if(not issequence(pathfilenames)):
-            raise TypeError('The pathfilename_list property must be of type '
-                'str or a sequence of type str!')
-        self._pathfilename_list = list(pathfilenames)
-
-    @abc.abstractmethod
-    def load_data(self, **kwargs):
-        """This method is supposed to load the data from the file.
-        """
-        pass
-
-
-class NPYFileLoader(FileLoader):
-    """The NPYFileLoader class provides the data loading functionality for
-    numpy data files containing numpy arrays. It uses the ``numpy.load``
-    function for loading the data and the numpy.append function to concatenate
-    several data files.
+    def sin_dec_binning(self):
+        """The BinningDefinition instance for the sin(dec) binning that should
+        be used for computing the sin(dec) dependency of the detector signal
+        yield. If None, the binning is supposed to be taken from the Dataset's
+        binning definitions.
+        """
+        return self._sin_dec_binning
+
+    @sin_dec_binning.setter
+    def sin_dec_binning(self, binning):
+        if (binning is not None) and\
+           (not isinstance(binning, BinningDefinition)):
+            raise TypeError(
+                'The sin_dec_binning property must be None, or an instance of '
+                'BinningDefinition! '
+                f'Its current type is "{classname(binning)}".')
+        self._sin_dec_binning = binning
+
+    def get_sin_dec_binning(self, dataset):
+        """Gets the sin(dec) binning definition either as setting from this
+        detector signal yield implementation method itself, or from the
+        given dataset.
+        """
+        sin_dec_binning = self.sin_dec_binning
+        if sin_dec_binning is None:
+            if not dataset.has_binning_definition('sin_dec'):
+                raise KeyError(
+                    'No binning definition named "sin_dec" is defined in the '
+                    f'dataset "{dataset.name}" and no user defined binning '
+                    'definition was provided to this detector signal yield '
+                    f'builder "{classname(self)}"!')
+            sin_dec_binning = dataset.get_binning_definition('sin_dec')
+        return sin_dec_binning
+
+
+class PointLikeSourceI3DetSigYield(
+        I3DetSigYield):
+    """Abstract base class for all IceCube specific detector signal yield
+    classes for point-like sources.
     """
-    def __init__(self, pathfilenames, **kwargs):
-        super(NPYFileLoader, self).__init__(pathfilenames)
 
-    def _load_file_memory_efficiently(
-            self, pathfilename, keep_fields, dtype_convertions,
-            dtype_convertion_except_fields):
-        """Loads a single file in a memory efficient way.
+    def __init__(
+            self,
+            param_names,
+            dataset,
+            fluxmodel,
+            livetime,
+            sin_dec_binning,
+            **kwargs):
+        """Constructor of the IceCube specific detector signal yield base
+        class for point-like sources.
 
         Parameters
         ----------
-        pathfilename : str
-            The fully qualified file name of the to-be-loaded file.
-
-        Returns
-        -------
-        data : DataFieldRecordArray instance
-            An instance of DataFieldRecordArray holding the data.
-        """
-        assert_file_exists(pathfilename)
-        # Create a memory map into the data file. This loads the data only when
-        # accessing the data.
-        mmap_ndarray = np.load(pathfilename, mmap_mode='r')
-        field_names = mmap_ndarray.dtype.names
-        fname_to_fidx = dict(
-            [ (fname,idx) for (idx,fname) in enumerate(field_names) ])
-        dt_fields = mmap_ndarray.dtype.fields
-        n_rows = mmap_ndarray.shape[0]
-
-        data = dict()
-
-        # Create empty arrays for each column of length n_rows.
-        for fname in field_names:
-            # Ignore fields that should not get kept.
-            if((keep_fields is not None) and (fname not in keep_fields)):
-                continue
-
-            # Get the original data type of the field.
-            dt = dt_fields[fname][0]
-            # Convert the data type if requested.
-            if((fname not in dtype_convertion_except_fields) and
-               (dt in dtype_convertions)):
-                dt = dtype_convertions[dt]
-
-            data[fname] = np.empty((n_rows,), dtype=dt)
-
-        # Loop through the rows of the recarray.
-        bs = 4096
-        for ridx in range(0, n_rows):
-            row = mmap_ndarray[ridx]
-            for fname in data.keys():
-                fidx = fname_to_fidx[fname]
-                data[fname][ridx] = row[fidx]
-
-            # Reopen the data file after each given blocksize.
-            if(ridx % bs == 0):
-                del mmap_ndarray
-                mmap_ndarray = np.load(pathfilename, mmap_mode='r')
-
-        # Close the memory map file.
-        del mmap_ndarray
-
-        # Create a DataFieldRecordArray out of the dictionary.
-        data = DataFieldRecordArray(data, copy=False)
-
-        return data
-
-    def _load_file_time_efficiently(
-            self, pathfilename, keep_fields, dtype_convertions,
-            dtype_convertion_except_fields):
-        """Loads a single file in a time efficient way. This will load the data
-        column-wise.
-        """
-        assert_file_exists(pathfilename)
-        # Create a memory map into the data file. This loads the data only when
-        # accessing the data.
-        mmap_ndarray = np.load(pathfilename, mmap_mode='r')
-
-        # Create a DataFieldRecordArray out of the memory mapped file. We need
-        # to copy the data, otherwise we get read-only numpy arrays.
-        data = DataFieldRecordArray(
-            mmap_ndarray,
-            keep_fields=keep_fields,
-            dtype_convertions=dtype_convertions,
-            dtype_convertion_except_fields=dtype_convertion_except_fields,
-            copy=True)
-
-        # Close the memory map file.
-        del mmap_ndarray
-
-        return data
-
-    def load_data(
-            self, keep_fields=None, dtype_convertions=None,
-            dtype_convertion_except_fields=None, efficiency_mode=None):
-        """Loads the data from the files specified through their fully qualified
-        file names.
+        param_names : sequence of str
+            The sequence of parameter names this detector signal yield depends
+            on. These are either fixed or floating parameters.
+        implmethod : instance of DetSigYieldImplMethod
+            The implementation method to use for constructing and receiving
+            the detector signal yield. The appropriate method depends on
+            the used flux model.
+        dataset : Dataset instance
+            The Dataset instance holding the monte-carlo event data.
+        fluxmodel : FluxModel
+            The flux model instance. Must be an instance of FluxModel.
+        sin_dec_binning : BinningDefinition instance
+            The BinningDefinition instance defining the sin(dec) binning.
+        """
+        super().__init__(
+            param_names=param_names,
+            dataset=dataset,
+            fluxmodel=fluxmodel,
+            livetime=livetime,
+            sin_dec_binning=sin_dec_binning,
+            **kwargs)
+
+    def sources_to_recarray(self, sources):
+        """Converts the sequence of PointLikeSource sources into a numpy record
+        array holding the information of the sources needed for the
+        detector signal yield calculation.
 
         Parameters
         ----------
-        keep_fields : str | sequence of str | None
-            Load the data into memory only for these data fields. If set to
-            ``None``, all in-file-present data fields are loaded into memory.
-        dtype_convertions : dict | None
-            If not None, this dictionary defines how data fields of specific
-            data types get converted into the specified data types.
-            This can be used to use less memory.
-        dtype_convertion_except_fields : str | sequence of str | None
-            The sequence of field names whose data type should not get
-            converted.
-        efficiency_mode : str | None
-            The efficiency mode the data should get loaded with. Possible values
-            are:
-
-                - 'memory':
-                    The data will be load in a memory efficient way. This will
-                    require more time, because all data records of a file will
-                    be loaded sequentially.
-                - 'time'
-                    The data will be loaded in a time efficient way. This will
-                    require more memory, because each data file gets loaded in
-                    memory at once.
-
-            The default value is ``'time'``. If set to ``None``, the default
-            value will be used.
+        sources : SourceModel | sequence of SourceModel
+            The source model(s) containing the information of the source(s).
 
         Returns
         -------
-        data : DataFieldRecordArray
-            The DataFieldRecordArray holding the loaded data.
-
-        Raises
-        ------
-        RuntimeError if a file does not exist.
-        """
-        if(keep_fields is not None):
-            if(isinstance(keep_fields, str)):
-                keep_fields = [ keep_fields ]
-            elif(not issequenceof(keep_fields, str)):
-                raise TypeError('The keep_fields argument must be None, an '
-                    'instance of type str, or a sequence of instances of '
-                    'type str!')
-
-        if(dtype_convertions is None):
-            dtype_convertions = dict()
-        elif(not isinstance(dtype_convertions, dict)):
-            raise TypeError('The dtype_convertions argument must be None, '
-                'or an instance of dict!')
-
-        if(dtype_convertion_except_fields is None):
-            dtype_convertion_except_fields = []
-        elif(isinstance(dtype_convertion_except_fields, str)):
-            dtype_convertion_except_fields = [ dtype_convertion_except_fields ]
-        elif(not issequenceof(dtype_convertion_except_fields, str)):
-            raise TypeError('The dtype_convertion_except_fields argument '
-                'must be a sequence of str instances.')
-
-        efficiency_mode2func = {
-            'memory': self._load_file_memory_efficiently,
-            'time': self._load_file_time_efficiently
-        }
-        if(efficiency_mode is None):
-            efficiency_mode = 'time'
-        if(not isinstance(efficiency_mode, str)):
-            raise TypeError('The efficiency_mode argument must be an instance '
-                'of type str!')
-        if(efficiency_mode not in efficiency_mode2func):
-            raise ValueError('The efficiency_mode argument value must be one '
-                'of %s!'%(', '.join(efficiency_mode2func.keys())))
-        load_file_func = efficiency_mode2func[efficiency_mode]
-
-        # Load the first data file.
-        data = load_file_func(
-            self._pathfilename_list[0],
-            keep_fields=keep_fields,
-            dtype_convertions=dtype_convertions,
-            dtype_convertion_except_fields=dtype_convertion_except_fields
-        )
-
-        # Load possible subsequent data files by appending to the first data.
-        for i in range(1, len(self._pathfilename_list)):
-            data.append(load_file_func(
-                self._pathfilename_list[i],
-                keep_fields=keep_fields,
-                dtype_convertions=dtype_convertions,
-                dtype_convertion_except_fields=dtype_convertion_except_fields
-            ))
-
-        return data
-
+        recarr : numpy record ndarray
+            The generated (N_sources,)-shaped 1D numpy record ndarray holding
+            the information for each source.
+        """
+        if isinstance(sources, PointLikeSource):
+            sources = [sources]
+        if not issequenceof(sources, PointLikeSource):
+            raise TypeError(
+                'The sources argument must be an instance or a sequence of '
+                'instances of PointLikeSource!')
 
-class PKLFileLoader(FileLoader):
-    """The PKLFileLoader class provides the data loading functionality for
-    pickled Python data files containing Python data structures. It uses the
-    `pickle.load` function for loading the data from the file.
+        recarr = np.empty((len(sources),), dtype=[('dec', np.float64)])
+        for (i, src) in enumerate(sources):
+            recarr['dec'][i] = src.dec
+
+        return recarr
+
+
+class PointLikeSourceI3DetSigYieldBuilder(
+        I3DetSigYieldBuilder,
+        metaclass=abc.ABCMeta,
+):
+    """Abstract base class for all IceCube specific detector signal yield
+    builders for point-like sources. All IceCube detector signal
+    yield builders require a sin(dec) binning definition for
+    the effective area. By default it is taken from the binning definitions
+    stored in the dataset, but a user-defined sin(dec) binning can be specified
+    if needed.
     """
-    def __init__(self, pathfilenames, pkl_encoding=None, **kwargs):
-        """Creates a new file loader instance for a pickled data file.
+
+    def __init__(
+            self,
+            sin_dec_binning=None,
+            **kwargs,
+    ):
+        """Initializes a new detector signal yield builder object.
 
         Parameters
         ----------
-        pathfilenames : str | sequence of str
-            The sequence of fully qualified file names of the data files that
-            need to be loaded.
-        pkl_encoding : str | None
-            The encoding of the pickled data files. If None, the default
-            encodings 'ASCII' and 'latin1' will be tried to load the data.
+        sin_dec_binning : BinningDefinition | None
+            The BinningDefinition instance defining the sin(dec) binning that
+            should be used to compute the sin(dec) dependency of the detector
+            effective area. If set to None, the binning will be taken from the
+            Dataset binning definitions.
         """
-        super(PKLFileLoader, self).__init__(pathfilenames)
+        super().__init__(
+            sin_dec_binning=sin_dec_binning,
+            **kwargs)
 
-        self.pkl_encoding = pkl_encoding
 
-    @property
-    def pkl_encoding(self):
-        """The encoding of the pickled data files. Can be None.
-        If None, the default encodings 'ASCII' and 'latin1' will be tried to
-        load the data.
-        """
-        return self._pkl_encoding
-    @pkl_encoding.setter
-    def pkl_encoding(self, encoding):
-        if(encoding is not None):
-            if(not isinstance(encoding, str)):
-                raise TypeError('The pkl_encoding property must be None or of '
-                    'type str!')
-        self._pkl_encoding = encoding
-
-    def load_data(self, **kwargs):
-        """Loads the data from the files specified through their fully qualified
-        file names.
-
-        Returns
-        -------
-        data : Python object | list of Python objects
-            The de-pickled Python object. If more than one file was specified,
-            this is a list of Python objects, i.e. one object for each file.
-            The file <-> object mapping order is preserved.
-
-        Raises
-        ------
-        RuntimeError if a file does not exist.
-        """
-        # Define the possible encodings of the pickled files.
-        encodings = ['ASCII', 'latin1']
-        if(self._pkl_encoding is not None):
-            encodings = [self._pkl_encoding] + encodings
-
-        data = []
-        for pathfilename in self.pathfilename_list:
-            assert_file_exists(pathfilename)
-            with open(pathfilename, 'rb') as ifile:
-                enc_idx = 0
-                load_ok = False
-                obj = None
-                while (not load_ok) and (enc_idx < len(encodings)):
-                    try:
-                        encoding = encodings[enc_idx]
-                        obj = pickle.load(ifile, encoding=encoding)
-                    except UnicodeDecodeError:
-                        enc_idx += 1
-                        # Move the file pointer back to the beginning of the
-                        # file.
-                        ifile.seek(0)
-                    else:
-                        load_ok = True
-                if(obj is None):
-                    raise RuntimeError('The file "%s" could not get unpickled! '
-                        'No correct encoding available!'%(pathfilename))
-                data.append(obj)
-
-        if(len(data) == 1):
-            data = data[0]
-
-        return data
-
-
-class TextFileLoader(FileLoader):
-    """The TextFileLoader class provides the data loading functionality for
-    data text files where values are stored in a comma, or whitespace, separated
-    format. It uses the numpy.loadtxt function to load the data. It reads the
-    first line of the text file for a table header.
+class FixedFluxPointLikeSourceI3DetSigYield(
+        PointLikeSourceI3DetSigYield):
+    """The detector signal yield class for a point-source with a fixed flux.
     """
-    def __init__(self, pathfilenames, header_comment='#', header_separator=None,
+    def __init__(
+            self,
+            param_names,
+            dataset,
+            fluxmodel,
+            livetime,
+            sin_dec_binning,
+            log_spl_sinDec,
             **kwargs):
-        """Creates a new file loader instance for a text data file.
+        """Constructs an IceCube detector signal yield instance for a
+        point-like source with a fixed flux.
 
         Parameters
         ----------
-        pathfilenames : str | sequence of str
-            The sequence of fully qualified file names of the data files that
-            need to be loaded.
-        header_comment : str
-            The character that defines a comment line in the text file.
-        header_separator : str | None
-            The separator of the header field names. If None, it assumes
-            whitespaces.
-        """
-        super().__init__(pathfilenames, **kwargs)
-
-        self.header_comment = header_comment
-        self.header_separator = header_separator
+        param_names : sequence of str
+            The sequence of parameter names this detector signal yield depends
+            on. These are either fixed or floating parameters.
+        dataset : Dataset instance
+            The instance of Dataset holding the monte-carlo data this detector
+            signal yield is made for.
+        fluxmodel : FluxModel instance
+            The instance of FluxModel with fixed parameters this detector signal
+            yield is made for.
+        livetime : float | Livetime instance
+            The livetime in days or an instance of Livetime.
+        sin_dec_binning : BinningDefinition instance
+            The binning definition for sin(dec).
+        log_spl_sinDec : scipy.interpolate.InterpolatedUnivariateSpline
+            The spline instance representing the log value of the detector
+            signal yield as a function of sin(dec).
+        """
+        super().__init__(
+            param_names=param_names,
+            dataset=dataset,
+            fluxmodel=fluxmodel,
+            livetime=livetime,
+            sin_dec_binning=sin_dec_binning,
+            **kwargs)
 
-    @property
-    def header_comment(self):
-        """The character that defines a comment line in the text file.
-        """
-        return self._header_comment
-    @header_comment.setter
-    def header_comment(self, s):
-        if(not isinstance(s, str)):
-            raise TypeError('The header_comment property must be of type str!')
-        self._header_comment = s
+        self.log_spl_sinDec = log_spl_sinDec
 
     @property
-    def header_separator(self):
-        """The separator of the header field names. If None, it assumes
-        whitespaces.
-        """
-        return self._header_separator
-    @header_separator.setter
-    def header_separator(self, s):
-        if(s is not None):
-            if(not isinstance(s, str)):
-                raise TypeError('The header_separator property must be None or '
-                    'of type str!')
-        self._header_separator = s
-
-    def _extract_column_names(self, line):
-        """Tries to extract the column names of the data table based on the
-        given line.
-
-        Parameters
-        ----------
-        line : str
-            The text line containing the column names.
-
-        Returns
-        -------
-        names : list of str | None
-            The column names.
-            It returns None, if the column names cannot be extracted.
-        """
-        # Remove possible new-line character and leading white-spaces.
-        line = line.strip()
-        # Check if the line is a comment line.
-        if(line[0:len(self._header_comment)] != self._header_comment):
-            return None
-        # Remove the leading comment character(s).
-        line = line.strip(self._header_comment)
-        # Remove possible leading whitespace characters.
-        line = line.strip()
-        # Split the line into the column names.
-        names = line.split(self._header_separator)
-        # Remove possible whitespaces of column names.
-        names = [ n.strip() for n in names ]
-
-        if(len(names) == 0):
-            return None
-
-        return names
-
-    def _load_file(self, pathfilename, keep_fields, dtype_convertions,
-            dtype_convertion_except_fields):
-        """Loads the given file.
-
-        Parameters
-        ----------
-        pathfilename : str
-            The fully qualified file name of the data file that
-            need to be loaded.
-        keep_fields : str | sequence of str | None
-            Load the data into memory only for these data fields. If set to
-            ``None``, all in-file-present data fields are loaded into memory.
-        dtype_convertions : dict | None
-            If not None, this dictionary defines how data fields of specific
-            data types get converted into the specified data types.
-            This can be used to use less memory.
-        dtype_convertion_except_fields : str | sequence of str | None
-            The sequence of field names whose data type should not get
-            converted.
-
-        Returns
-        -------
-        data : DataFieldRecordArray instance
-            The DataFieldRecordArray instance holding the loaded data.
-        """
-        assert_file_exists(pathfilename)
+    def log_spl_sinDec(self):
+        """The :class:`scipy.interpolate.InterpolatedUnivariateSpline` instance
+        representing the spline for the log value of the detector signal
+        yield as a function of sin(dec).
+        """
+        return self._log_spl_sinDec
+
+    @log_spl_sinDec.setter
+    def log_spl_sinDec(self, spl):
+        if not isinstance(spl, scipy.interpolate.InterpolatedUnivariateSpline):
+            raise TypeError(
+                'The log_spl_sinDec property must be an instance '
+                'of scipy.interpolate.InterpolatedUnivariateSpline!')
+        self._log_spl_sinDec = spl
 
-        with open(pathfilename, 'r') as ifile:
-            line = ifile.readline()
-            column_names = self._extract_column_names(line)
-            if(column_names is None):
-                raise ValueError('The data text file "{}" does not contain a '
-                    'readable table header as first line!'.format(pathfilename))
-            usecols = None
-            dtype = [(n,np.float64) for n in column_names]
-            if(keep_fields is not None):
-                # Select only the given columns.
-                usecols = []
-                dtype = []
-                for (idx,name) in enumerate(column_names):
-                    if(name in keep_fields):
-                        usecols.append(idx)
-                        dtype.append((name,np.float64))
-                usecols = tuple(usecols)
-            if(len(dtype) == 0):
-                raise ValueError('No data columns were selected to be loaded!')
-
-            data_ndarray = np.loadtxt(ifile,
-                dtype=dtype,
-                comments=self._header_comment,
-                usecols=usecols)
-
-        data = DataFieldRecordArray(
-            data_ndarray,
-            keep_fields=keep_fields,
-            dtype_convertions=dtype_convertions,
-            dtype_convertion_except_fields=dtype_convertion_except_fields,
-            copy=False)
-
-        return data
-
-    def load_data(self, keep_fields=None, dtype_convertions=None,
-            dtype_convertion_except_fields=None, **kwargs):
-        """Loads the data from the data files specified through their fully
-        qualified file names.
+    def __call__(self, src_recarray, src_params_recarray=None):
+        """Retrieves the detector signal yield for the list of given sources.
 
         Parameters
         ----------
-        keep_fields : str | sequence of str | None
-            Load the data into memory only for these data fields. If set to
-            ``None``, all in-file-present data fields are loaded into memory.
-        dtype_convertions : dict | None
-            If not None, this dictionary defines how data fields of specific
-            data types get converted into the specified data types.
-            This can be used to use less memory.
-        dtype_convertion_except_fields : str | sequence of str | None
-            The sequence of field names whose data type should not get
-            converted.
+        src_recarray : numpy record ndarray
+            The numpy record ndarray with the field ``dec`` holding the
+            declination of the source.
+        src_params_recarray : None
+            Unused interface argument, because this detector signal yield does
+            not depend on any source parameters.
 
         Returns
         -------
-        data : DataFieldRecordArray
-            The DataFieldRecordArray holding the loaded data.
-
-        Raises
-        ------
-        RuntimeError
-            If a file does not exist.
-        ValueError
-            If the table header cannot be read.
-        """
-        if(keep_fields is not None):
-            if(isinstance(keep_fields, str)):
-                keep_fields = [ keep_fields ]
-            elif(not issequenceof(keep_fields, str)):
-                raise TypeError('The keep_fields argument must be None, an '
-                    'instance of type str, or a sequence of instances of '
-                    'type str!')
-
-        if(dtype_convertions is None):
-            dtype_convertions = dict()
-        elif(not isinstance(dtype_convertions, dict)):
-            raise TypeError('The dtype_convertions argument must be None, '
-                'or an instance of dict!')
-
-        if(dtype_convertion_except_fields is None):
-            dtype_convertion_except_fields = []
-        elif(isinstance(dtype_convertion_except_fields, str)):
-            dtype_convertion_except_fields = [ dtype_convertion_except_fields ]
-        elif(not issequenceof(dtype_convertion_except_fields, str)):
-            raise TypeError('The dtype_convertion_except_fields argument '
-                'must be a sequence of str instances.')
-
-        # Load the first data file.
-        data = self._load_file(
-            self._pathfilename_list[0],
-            keep_fields=keep_fields,
-            dtype_convertions=dtype_convertions,
-            dtype_convertion_except_fields=dtype_convertion_except_fields
+        values : numpy 1d ndarray
+            The array with the detector signal yield for each source.
+        grads : dict
+            This detector signal yield does not depend on any parameters.
+            So there are no gradients and the dictionary is empty.
+        """
+        src_dec = np.atleast_1d(src_recarray['dec'])
+
+        # Create results array.
+        values = np.zeros_like(src_dec, dtype=np.float64)
+
+        # Create mask for all source declinations which are inside the
+        # declination range.
+        mask = (
+            (np.sin(src_dec) >= self._sin_dec_binning.lower_edge) &
+            (np.sin(src_dec) <= self._sin_dec_binning.upper_edge)
         )
 
-        # Load possible subsequent data files by appending to the first data.
-        for i in range(1, len(self._pathfilename_list)):
-            data.append(self._load_file(
-                self._pathfilename_list[i],
-                keep_fields=keep_fields,
-                dtype_convertions=dtype_convertions,
-                dtype_convertion_except_fields=dtype_convertion_except_fields
-            ))
+        values[mask] = np.exp(self._log_spl_sinDec(np.sin(src_dec[mask])))
 
-        return data
+        return (values, {})
 
 
-class DataFieldRecordArray(object):
-    """The DataFieldRecordArray class provides a data container similar to a numpy
-    record ndarray. But the data fields are stored as individual numpy ndarray
-    objects. Hence, access of single data fields is much faster compared to
-    access on the record ndarray.
+class FixedFluxPointLikeSourceI3DetSigYieldBuilder(
+        PointLikeSourceI3DetSigYieldBuilder,
+        multiproc.IsParallelizable,
+):
+    """This detector signal yield builder constructs a
+    detector signal yield for a fixed flux model, assuming a point-like
+    source. This means that the detector signal yield does not depend on
+    any source parameters, hence it is only dependent on the detector
+    effective area.
+    It constructs a one-dimensional spline function in sin(dec), using a
+    :class:`scipy.interpolate.InterpolatedUnivariateSpline`.
+
+    This detector signal yield builder works with all flux models.
+
+    It is tailored to the IceCube detector at the South Pole, where the
+    effective area depends soley on the zenith angle, and hence on the
+    declination, of the source.
     """
-    def __init__(self, data, keep_fields=None, dtype_convertions=None,
-            dtype_convertion_except_fields=None, copy=True):
-        """Creates a DataFieldRecordArray from the given numpy record ndarray.
+
+    def __init__(
+            self,
+            sin_dec_binning=None,
+            spline_order_sinDec=2,
+            **kwargs,
+    ):
+        """Creates a new IceCube detector signal yield builder object for a
+        fixed flux model. It requires a sinDec binning definition to compute
+        the sin(dec) dependency of the detector effective area.
+        The construct_detsigyield class method of this builder will create a
+        spline function of a given order in logarithmic space of the
+        effective area.
 
         Parameters
         ----------
-        data : numpy record ndarray | dict | DataFieldRecordArray | None
-            The numpy record ndarray that needs to get transformed into the
-            DataFieldRecordArray instance. Alternative a dictionary with field
-            names as keys and numpy ndarrays as values can be provided. If an
-            instance of DataFieldRecordArray is provided, the new
-            DataFieldRecordArray gets constructed from the copy of the data of
-            the provided DataFieldRecordArray instance.
-            If set to `None`, the DataFieldRecordArray instance is initialized
-            with no data and the length of the array is set to 0.
-        keep_fields : str | sequence of str | None
-            If not None (default), this specifies the data fields that should
-            get kept from the given data. Otherwise all data fields get kept.
-        dtype_convertions : dict | None
-            If not None, this dictionary defines how data fields of specific
-            data types get converted into the specified data types.
-            This can be used to use less memory.
-        dtype_convertion_except_fields : str | sequence of str | None
-            The sequence of field names whose data type should not get
-            converted.
-        copy : bool
-            Flag if the input data should get copied. Default is True. If a
-            DataFieldRecordArray instance is provided, this option is set to
-            `True` automatically.
-        """
-        self._data_fields = dict()
-        self._len = None
-
-        if(data is None):
-            data = dict()
-
-        if(keep_fields is not None):
-            if(isinstance(keep_fields, str)):
-                keep_fields = [ keep_fields ]
-            elif(not issequenceof(keep_fields, str)):
-                raise TypeError('The keep_fields argument must be None, an '
-                    'instance of type str, or a sequence of instances of '
-                    'type str!')
-
-        if(dtype_convertions is None):
-            dtype_convertions = dict()
-        elif(not isinstance(dtype_convertions, dict)):
-            raise TypeError('The dtype_convertions argument must be None, '
-                'or an instance of dict!')
-
-        if(dtype_convertion_except_fields is None):
-            dtype_convertion_except_fields = []
-        elif(isinstance(dtype_convertion_except_fields, str)):
-            dtype_convertion_except_fields = [ dtype_convertion_except_fields ]
-        elif(not issequenceof(dtype_convertion_except_fields, str)):
-            raise TypeError('The dtype_convertion_except_fields argument '
-                'must be a sequence of str instances.')
-
-        if(isinstance(data, np.ndarray)):
-            field_names = data.dtype.names
-            fname2dtype = dict(
-                [(k,v[0]) for (k,v) in data.dtype.fields.items() ])
-            length = data.shape[0]
-        elif(isinstance(data, dict)):
-            field_names = list(data.keys())
-            fname2dtype = dict(
-                [ (fname, data[fname].dtype) for fname in field_names ])
-            length = 0
-            if(len(field_names) > 0):
-                length = data[field_names[0]].shape[0]
-        elif(isinstance(data, DataFieldRecordArray)):
-            field_names = data.field_name_list
-            fname2dtype = dict(
-                [ (fname, data[fname].dtype) for fname in field_names ])
-            length = len(data)
-            copy = True
-        else:
-            raise TypeError('The data argument must be an instance of ndarray, '
-                'dict, or DataFieldRecordArray!')
-
-        for fname in field_names:
-            # Ignore fields that should not get kept.
-            if((keep_fields is not None) and (fname not in keep_fields)):
-                continue
-
-            copy_field = copy
-            dt = fname2dtype[fname]
-            if((fname not in dtype_convertion_except_fields) and
-               (dt in dtype_convertions)):
-                dt = dtype_convertions[dt]
-                # If a data type convertion is needed, the data of the field
-                # needs to get copied.
-                copy_field = True
-
-            if(copy_field is True):
-                # Create a ndarray with the final data type and then assign the
-                # values from the data, which technically is a copy.
-                field_arr = np.empty((length,), dtype=dt)
-                field_arr[:] = data[fname]
-            else:
-                field_arr = data[fname]
-            if(self._len is None):
-                self._len = len(field_arr)
-            elif(len(field_arr) != self._len):
-                raise ValueError('All field arrays must have the same length. '
-                    'Field "%s" has length %d, but must be %d!'%(
-                    fname, len(field_arr), self._len))
-
-            self._data_fields[fname] = field_arr
-
-        if(self._len is None):
-            # The DataFieldRecordArray is initialized with no fields, i.e. also
-            # also no data.
-            self._len = 0
-
-        self._field_name_list = list(self._data_fields.keys())
-        self._indices = None
-
-    def __contains__(self, name):
-        """Checks if the given field exists in this DataFieldRecordArray
-        instance.
+        sin_dec_binning : BinningDefinition | None
+            The BinningDefinition instance which defines the sin(dec) binning.
+            If set to None, the binning will be taken from the Dataset binning
+            definitions.
+        spline_order_sinDec : int
+            The order of the spline function for the logarithmic values of the
+            detector signal yield along the sin(dec) axis.
+            The default is 2.
+        """
+        super().__init__(
+            sin_dec_binning=sin_dec_binning,
+            **kwargs)
+
+        self.spline_order_sinDec = spline_order_sinDec
+
+    @property
+    def spline_order_sinDec(self):
+        """The order (int) of the logarithmic spline function, that splines the
+        detector signal yield, along the sin(dec) axis.
+        """
+        return self._spline_order_sinDec
+
+    @spline_order_sinDec.setter
+    def spline_order_sinDec(self, order):
+        if not isinstance(order, int):
+            raise TypeError(
+                'The spline_order_sinDec property must be of type int! '
+                f'Its current type is {classname(order)}.')
+        self._spline_order_sinDec = order
+
+    def _create_hist(
+            self,
+            data_sin_true_dec,
+            data_true_energy,
+            sin_dec_binning,
+            weights,
+            fluxmodel,
+            to_internal_flux_unit_factor,
+    ):
+        """Creates a histogram of the detector signal yield with the
+        given sin(dec) binning for the given flux model.
 
         Parameters
         ----------
-        name : str
-            The name of the field.
+        data_sin_true_dec : instance of numpy.ndarray
+            The (N_data,)-shaped numpy.ndarray holding the sin(true_dec) values
+            of the monte-carlo events.
+        data_true_energy : instance of numpy.ndarray
+            The (N_data,)-shaped numpy.ndarray holding the true energy of the
+            monte-carlo events.
+        sin_dec_binning : instance of BinningDefinition
+            The sin(dec) binning definition to use for the histogram.
+        weights : 1d ndarray
+            The (N_data,)-shaped numpy.ndarray holding the weight factor of
+            each monte-carlo event where only the flux value needs to be
+            multiplied with in order to get the detector signal yield.
+        fluxmodel : instance of FluxModel
+            The flux model to get the flux values from.
+        to_internal_flux_unit_factor : float
+            The conversion factor to convert the flux unit into the internal
+            flux unit.
 
         Returns
         -------
-        check : bool
-            True, if the given field exists in this DataFieldRecordArray
-            instance, False otherwise.
-        """
-        return (name in self._data_fields)
+        hist : instance of numpy.ndarray
+            The (N_sin_dec_bins,)-shaped numpy.ndarray containing the histogram
+            values.
+        """
+        weights = (
+            weights *
+            fluxmodel(E=data_true_energy).squeeze() *
+            to_internal_flux_unit_factor
+        )
 
-    def __getitem__(self, name):
-        """Implements data field value access.
+        (hist, _) = np.histogram(
+            data_sin_true_dec,
+            bins=sin_dec_binning.binedges,
+            weights=weights,
+            density=False)
+
+        # Normalize by solid angle of each bin which is
+        # 2*\pi*(\Delta sin(\delta)).
+        hist /= (2.*np.pi * np.diff(sin_dec_binning.binedges))
+
+        return hist
+
+    def _create_detsigyield_from_hist(
+            self,
+            hist,
+            sin_dec_binning,
+            **kwargs,
+    ):
+        """Create a single instance of FixedFluxPointLikeSourceI3DetSigYield
+        from the given histogram.
 
         Parameters
         ----------
-        name : str | numpy ndarray of int or bool
-            The name of the data field. If a numpy ndarray is given, it must
-            contain the indices for which to retrieve a data selection of the
-            entire DataFieldRecordArray. A numpy ndarray of bools can be given
-            as well to define a mask.
-
-        Raises
-        ------
-        KeyError
-            If the given data field does not exist.
+        hist : instance of numpy.ndarray
+            The (N_sin_dec_bins,)-shaped numpy.ndarray holding the normalized
+            histogram of the detector signal yield.
+        sin_dec_binning : instance of BinningDefinition
+            The sin(dec) binning definition to use for the histogram.
+        **kwargs
+            Additional keyword arguments are passed to the constructor of the
+            FixedFluxPointLikeSourceI3DetSigYield class.
 
         Returns
         -------
-        data : numpy ndarray | DataFieldRecordArray
-            The requested field data or a DataFieldRecordArray holding the
-            requested selection of the entire data.
-        """
-        if(isinstance(name, np.ndarray)):
-            return self.get_selection(name)
-
-        if(name not in self._data_fields):
-            raise KeyError('The data field "%s" is not present in the '
-                'DataFieldRecordArray instance.'%(name))
-
-        return self._data_fields[name]
-
-    def __setitem__(self, name, arr):
-        """Implements data field value assigment. If values are assigned to a
-        data field that does not exist yet, it  will be added via the
-        ``append_field`` method.
+        detsigyield : instance of FixedFluxPointLikeSourceI3DetSigYield
+            The instance of FixedFluxPointLikeSourceI3DetSigYield for the given
+            flux model.
+        """
+        # Create spline in ln(hist) at the histogram's bin centers.
+        log_spl_sinDec = scipy.interpolate.InterpolatedUnivariateSpline(
+            sin_dec_binning.bincenters,
+            np.log(hist),
+            k=self.spline_order_sinDec)
+
+        detsigyield = FixedFluxPointLikeSourceI3DetSigYield(
+            param_names=[],
+            sin_dec_binning=sin_dec_binning,
+            log_spl_sinDec=log_spl_sinDec,
+            **kwargs)
+
+        return detsigyield
+
+    def construct_detsigyields(
+            self,
+            dataset,
+            data,
+            shgs,
+            ppbar=None,
+    ):
+        """Constructs a set of FixedFluxPointLikeSourceI3DetSigYield instances,
+        one for each provided fluxmodel.
 
         Parameters
         ----------
-        name : str | numpy ndarray of int or bool
-            The name of the data field, or a numpy ndarray holding the indices
-            or mask of a selection of this DataFieldRecordArray.
-        arr : numpy ndarray | instance of DataFieldRecordArray
-            The numpy ndarray holding the field values. It must be of the same
-            length as this DataFieldRecordArray. If `name` is a numpy ndarray,
-            `arr` must be a DataFieldRecordArray.
-
-        Raises
-        ------
-        ValueError
-            If the given data array is not of the same length as this
-            DataFieldRecordArray instance.
-        """
-        if(isinstance(name, np.ndarray)):
-            self.set_selection(name, arr)
-            return
-
-        # Check if a new field is supposed to be added.
-        if(name not in self):
-            self.append_field(name, arr)
-            return
-
-        # We set a particular already existing data field.
-        if(len(arr) != self._len):
-            raise ValueError('The length of the to-be-set data (%d) must '
-                'match the length (%d) of the DataFieldRecordArray instance!'%(
-                len(arr), self._len))
-
-        if(not isinstance(arr, np.ndarray)):
-            raise TypeError(
-                'When setting a field directly, the data must be provided as a '
-                'numpy ndarray!')
-
-        self._data_fields[name] = arr
-
-    def __len__(self):
-        return self._len
-
-    def __sizeof__(self):
-        """Calculates the size in bytes of this DataFieldRecordArray instance
-        in memory.
+        dataset : instance of Dataset
+            The instance of Dataset holding meta information about the data.
+        data : instance of DatasetData
+            The instance of DatasetData holding the monte-carlo event data.
+            The numpy record ndarray holding the monte-carlo event data must
+            contain the following data fields:
+
+            - 'true_dec' : float
+                The true declination of the data event.
+            - 'true_energy' : float
+                The true energy value of the data event.
+            - 'mcweight' : float
+                The monte-carlo weight of the data event in the unit
+                GeV cm^2 sr.
+
+        shgs : sequence of instance of SourceHypoGroup
+            The sequence of instance of SourceHypoGroup specifying the
+            source hypothesis groups (i.e. flux model) for which the detector
+            signal yields should get constructed.
+        ppbar : instance of ProgressBar | None
+            The optional instance of ProgressBar of the parent progress bar.
 
         Returns
         -------
-        memsize : int
-            The memory size in bytes that this DataFieldRecordArray instance
-            has.
-        """
-        memsize = getsizeof([
-            self._data_fields,
-            self._len,
-            self._field_name_list,
-            self._indices
-        ])
-        return memsize
-
-    def __str__(self):
-        """Creates a pretty informative string representation of this
-        DataFieldRecordArray instance.
-        """
-        (size, prefix) = get_byte_size_prefix(sys.getsizeof(self))
-
-        max_field_name_len = np.max(
-            [len(fname) for fname in self._field_name_list])
-
-        # Generates a pretty string representation of the given field name.
-        def _pretty_str_field(name):
-            field = self._data_fields[name]
-            s = '%s: {dtype: %s, vmin: % .3e, vmax: % .3e}'%(
-                name.ljust(max_field_name_len), str(field.dtype), np.min(field),
-                np.max(field))
-            return s
-
-        indent_str = ' '*dsp.INDENTATION_WIDTH
-        s = '%s: %d fields, %d entries, %.0f %sbytes '%(
-            classname(self), len(self._field_name_list), self.__len__(),
-            np.round(size, 0), prefix)
-        if(len(self._field_name_list) > 0):
-            s += '\n' + indent_str + 'fields = {'
-            s += '\n' + indent_str*2 + _pretty_str_field(self._field_name_list[0])
-            for fname in self._field_name_list[1:]:
-                s += '\n' + indent_str*2 + _pretty_str_field(fname)
-            s += '\n' + indent_str + '}'
-        return s
+        detsigyields : list of instance of FixedFluxPointLikeSourceI3DetSigYield
+            The list of instance of FixedFluxPointLikeSourceI3DetSigYield
+            providing the detector signal yield function for a point-like source
+            with each of the given fixed flux models.
+        """
+        self.assert_types_of_construct_detsigyield_arguments(
+            dataset=dataset,
+            data=data,
+            shgs=shgs,
+            ppbar=ppbar)
+
+        # Calculate conversion factor from the flux model unit into the
+        # internal flux unit (usually GeV^-1 cm^-2 s^-1).
+        to_internal_flux_unit_factors = [
+            shg.fluxmodel.to_internal_flux_unit()
+            for shg in shgs
+        ]
 
-    @property
-    def field_name_list(self):
-        """(read-only) The list of the field names of this DataFieldRecordArray.
-        """
-        return self._field_name_list
+        to_internal_time_unit_factor = self._cfg.to_internal_time_unit(
+            time_unit=units.day
+        )
 
-    @property
-    def indices(self):
-        """(read-only) The numpy ndarray holding the indices of this
-        DataFieldRecordArray.
-        """
-        if(self._indices is None):
-            self._indices = np.indices((self._len,))[0]
-        return self._indices
-
-    def append(self, arr):
-        """Appends the given DataFieldRecordArray to this DataFieldRecordArray
-        instance.
+        # Get integrated live-time in days.
+        livetime_days = Livetime.get_integrated_livetime(data.livetime)
 
-        Parameters
-        ----------
-        arr : instance of DataFieldRecordArray
-            The instance of DataFieldRecordArray that should get appended to
-            this DataFieldRecordArray. It must contain the same data fields.
-            Additional data fields are ignored.
-        """
-        if(not isinstance(arr, DataFieldRecordArray)):
-            raise TypeError('The arr argument must be an instance of '
-                'DataFieldRecordArray!')
-
-        for fname in self._field_name_list:
-            self._data_fields[fname] = np.append(
-                self._data_fields[fname], arr[fname])
+        # Get the sin(dec) binning definition either as setting from this
+        # implementation method, or from the dataset.
+        sin_dec_binning = self.get_sin_dec_binning(dataset)
+
+        data_sin_true_dec = np.sin(data.mc['true_dec'])
+
+        # Generate a list of indices that would sort the data according to the
+        # sin(true_dec) values. We will sort the MC data according to it,
+        # because the histogram creation is much faster (2x) when the
+        # to-be-histogrammed values are already sorted.
+        sorted_idxs = np.argsort(data_sin_true_dec)
+
+        data_sin_true_dec = np.take(data_sin_true_dec, sorted_idxs)
+        data_true_energy = np.take(data.mc['true_energy'], sorted_idxs)
+        mc_weight = np.take(data.mc['mcweight'], sorted_idxs)
+
+        weights = (
+            mc_weight *
+            livetime_days*to_internal_time_unit_factor
+        )
+
+        args_list = [
+            ((), dict(
+                data_sin_true_dec=data_sin_true_dec,
+                data_true_energy=data_true_energy,
+                sin_dec_binning=sin_dec_binning,
+                weights=weights,
+                fluxmodel=shg.fluxmodel,
+                to_internal_flux_unit_factor=to_internal_flux_unit_factor,
+            ))
+            for (shg, to_internal_flux_unit_factor) in zip(
+                shgs, to_internal_flux_unit_factors)
+        ]
+
+        hists = multiproc.parallelize(
+            func=self._create_hist,
+            args_list=args_list,
+            ncpu=multiproc.get_ncpu(
+                cfg=self._cfg,
+                local_ncpu=self.ncpu),
+            ppbar=ppbar,
+        )
 
-        self._len += len(arr)
-        self._indices = None
+        detsigyields = [
+            self._create_detsigyield_from_hist(
+                hist=hist,
+                sin_dec_binning=sin_dec_binning,
+                dataset=dataset,
+                livetime=data.livetime,
+                fluxmodel=shg.fluxmodel,
+            )
+            for (hist, shg) in zip(hists, shgs)
+        ]
+
+        return detsigyields
+
+    def construct_detsigyield(
+            self,
+            dataset,
+            data,
+            shg,
+            ppbar=None,
+    ):
+        """Constructs a detector signal yield log spline function for the
+        given fixed flux model.
 
-    def append_field(self, name, data):
-        """Appends a field and its data to this DataFieldRecordArray instance.
+        This method calls the :meth:`construct_detsigyiels` method of this
+        class.
 
         Parameters
         ----------
-        name : str
-            The name of the new data field.
-        data : numpy ndarray
-            The numpy ndarray holding the data.
-
-        Raises
-        ------
-        KeyError
-            If the given data field name already exists in this
-            DataFieldRecordArray instance.
-        ValueError
-            If the length of the data array does not equal to the length of the
-            data of this DataFieldRecordArray instance.
-        TypeError
-            If the arguments are of the wrong type.
-        """
-        if(not isinstance(name, str)):
-            raise TypeError(
-                'The name argument must be an instance of str!')
-        if(not isinstance(data, np.ndarray)):
-            raise TypeError(
-                'The data argument must be an instance of ndarray!')
-        if(name in self._data_fields):
-            raise KeyError(
-                'The data field "%s" already exists in this %s instance!'%(
-                    name, classname(self)))
-        #if(len(data) != self._len):
-        #    raise ValueError(
-        #        'The length of the given data is %d, but must be %d!'%(
-        #            len(data), self._len))
-
-        self._data_fields[name] = data
-        self._field_name_list.append(name)
-
-    def as_numpy_record_array(self):
-        """Creates a numpy record ndarray instance holding the data of this
-        DataFieldRecordArray instance.
+        dataset : instance of Dataset
+            The instance of Dataset holding meta information about the data.
+        data : instance of DatasetData
+            The instance of DatasetData holding the monte-carlo event data.
+            The numpy record ndarray holding the monte-carlo event data must
+            contain the following data fields:
+
+            - 'true_dec' : float
+                The true declination of the data event.
+            - 'true_energy' : float
+                The true energy value of the data event.
+            - 'mcweight' : float
+                The monte-carlo weight of the data event in the unit
+                GeV cm^2 sr.
+
+        shg : instance of SourceHypoGroup
+            The instance of SourceHypoGroup (i.e. sources and flux model) for
+            which the detector signal yield should get constructed.
+        ppbar : instance of ProgressBar | None
+            The optional instance of ProgressBar of the parent progress bar.
 
         Returns
         -------
-        arr : numpy record ndarray
-            The numpy recarray ndarray holding the data of this
-            DataFieldRecordArray instance.
-        """
-        dt = np.dtype(
-            [(name, self._data_fields[name].dtype)
-             for name in self.field_name_list])
-
-        arr = np.empty((len(self),), dtype=dt)
-        for name in self.field_name_list:
-            arr[name] = self[name]
-
-        return arr
-
-    def copy(self, keep_fields=None):
-        """Creates a new DataFieldRecordArray that is a copy of this
-        DataFieldRecordArray instance.
-
-        Parameters
-        ----------
-        keep_fields : str | sequence of str | None
-            If not None (default), this specifies the data fields that should
-            get kept from this DataFieldRecordArray. Otherwise all data fields
-            get kept.
-        """
-        return DataFieldRecordArray(self, keep_fields=keep_fields)
+        detsigyield : instance of FixedFluxPointLikeSourceI3DetSigYield
+            The instance of FixedFluxPointLikeSourceI3DetSigYield providing the
+            detector signal yield function for a point-like source with a
+            fixed flux.
+        """
+        detsigyield = self.construct_detsigyields(
+            dataset=dataset,
+            data=data,
+            shgs=[shg],
+            ppbar=ppbar,
+        )[0]
+
+        return detsigyield
+
+    def get_detsigyield_construction_factory(self):
+        """Returns the factory callable for constructing a set of instance of
+        FixedFluxPointLikeSourceI3DetSigYield.
 
-    def remove_field(self, name):
-        """Removes the given field from this array.
-
-        Parameters
-        ----------
-        name : str
-            The name of the data field that is to be removed.
-        """
-        self._data_fields.pop(name)
-        self._field_name_list.remove(name)
+        Returns
+        -------
+        factory : callable
+            The factory callable for constructing a set of instance of
+            FixedFluxPointLikeSourceI3DetSigYield.
+        """
+        factory = self.construct_detsigyields
+        return factory
 
-    def get_field_dtype(self, name):
-        """Returns the numpy dtype object of the given data field.
-        """
-        return self._data_fields[name].dtype
 
-    def set_field_dtype(self, name, dt):
-        """Sets the data type of the given field.
+class SingleParamFluxPointLikeSourceI3DetSigYield(
+        PointLikeSourceI3DetSigYield):
+    """The detector signal yield class for a flux that depends on a single
+    source parameter.
+    """
+    def __init__(
+            self,
+            param_name,
+            dataset,
+            fluxmodel,
+            livetime,
+            sin_dec_binning,
+            log_spl_sinDec_param,
+            **kwargs):
+        """Constructs the detector signal yield instance.
 
         Parameters
         ----------
-        name : str
-            The name of the data field.
-        dt : numpy.dtype
-            The dtype instance defining the new data type.
-        """
-        if(name not in self):
-            raise KeyError(
-                f'The data field "{name}" does not exist in this '
-                 'DataFieldRecordArray!')
-        if(not isinstance(dt, np.dtype)):
-            raise TypeError(
-                'The dt argument must be an instance of type numpy.dtype!')
+        param_name : str
+            The parameter name this detector signal yield depends
+            on. These are either fixed or floating parameter.
+        dataset : Dataset instance
+            The Dataset instance holding the monte-carlo event data.
+        fluxmodel : FluxModel
+            The flux model instance. Must be an instance of FluxModel.
+        livetime : float | Livetime instance
+            The live-time.
+        sin_dec_binning : BinningDefinition instance
+            The BinningDefinition instance defining the sin(dec) binning.
+        log_spl_sinDec_param : scipy.interpolate.RectBivariateSpline instance
+            The 2D spline in sin(dec) and the parameter this detector signal
+            yield depends on.
+        """
+        super().__init__(
+            param_names=[param_name],
+            dataset=dataset,
+            fluxmodel=fluxmodel,
+            livetime=livetime,
+            sin_dec_binning=sin_dec_binning,
+            **kwargs)
 
-        self._data_fields[name] = self._data_fields[name].astype(dt, copy=False)
+        self.log_spl_sinDec_param = log_spl_sinDec_param
 
-    def convert_dtypes(self, convertions, except_fields=None):
-        """Converts the data type of the data fields of this
-        DataFieldRecordArray. This method can be used to compress the data.
-
-        Parameters
-        ----------
-        convertions : dict of `old_dtype` -> `new_dtype`
-            The dictionary with the old dtype as key and the new dtype as value.
-        except_fields : sequence of str | None
-            The sequence of field names, which should not get converted.
-        """
-        if(not isinstance(convertions, dict)):
-            raise TypeError('The convertions argument must be an instance of '
-                'dict!')
-
-        if(except_fields is None):
-            except_fields = []
-        if(not issequenceof(except_fields, str)):
-            raise TypeError('The except_fields argument must be a sequence '
-                'of str!')
-
-        _data_fields = self._data_fields
-        for fname in self._field_name_list:
-            if(fname in except_fields):
-                continue
-            old_dtype = _data_fields[fname].dtype
-            if(old_dtype in convertions):
-                new_dtype = convertions[old_dtype]
-                _data_fields[fname] = _data_fields[fname].astype(new_dtype)
-
-    def get_selection(self, indices):
-        """Creates an DataFieldRecordArray that contains a selection of the data
-        of this DataFieldRecordArray instance.
+    @property
+    def log_spl_sinDec_param(self):
+        """The :class:`scipy.interpolate.RectBivariateSpline` instance
+        representing the spline for the log value of the detector signal
+        yield as a function of sin(dec) and the floating parameter.
+        """
+        return self._log_spl_sinDec_param
+
+    @log_spl_sinDec_param.setter
+    def log_spl_sinDec_param(self, spl):
+        if not isinstance(spl, scipy.interpolate.RectBivariateSpline):
+            raise TypeError(
+                'The log_spl_sinDec_param property must be an instance of '
+                'scipy.interpolate.RectBivariateSpline! '
+                f'Its current type is {classname(spl)}.')
+        self._log_spl_sinDec_param = spl
+
+    def __call__(self, src_recarray, src_params_recarray):
+        """Retrieves the detector signal yield for the given list of
+        sources and their flux parameters.
 
         Parameters
         ----------
-        indices : (N,)-shaped numpy ndarray of int or bool
-            The numpy ndarray holding the indices for which to select the data.
+        src_recarray : numpy record ndarray
+            The numpy record ndarray with the field ``dec`` holding the
+            declination of the source.
+        src_params_recarray : (N_sources,)-shaped numpy record ndarray
+            The numpy record ndarray containing the parameter values of the
+            sources. The parameter values can be different for the different
+            sources.
+            The record array needs to contain two fields for each source
+            parameter, one named <name> with the source's local parameter name
+            holding the source's local parameter value, and one named
+            <name:gpidx> holding the global parameter index plus one for each
+            source value. For values mapping to non-fit parameters, the index
+            should be negative.
 
         Returns
         -------
-        data_field_array : DataFieldRecordArray
-            The DataFieldRecordArray that contains the selection of the
-            original DataFieldRecordArray. The selection data is a copy of the
-            original data.
-        """
-        data = dict()
-        for fname in self._field_name_list:
-            # Get the data selection from the original data. This creates a
-            # copy.
-            data[fname] = self._data_fields[fname][indices]
-        return DataFieldRecordArray(data, copy=False)
-
-    def set_selection(self, indices, arr):
-        """Sets a selection of the data of this DataFieldRecordArray instance
-        to the data given in arr.
-
-        Parameters
-        ----------
-        indices : (N,)-shaped numpy ndarray of int or bool
-            The numpy ndarray holding the indices or mask for which to set the
-            data.
-        arr : instance of DataFieldRecordArray
-            The instance of DataFieldRecordArray holding the selection data.
-            It must have the same fields defined as this DataFieldRecordArray
-            instance.
-        """
-        if(not isinstance(arr, DataFieldRecordArray)):
-            raise TypeError('The arr argument must be an instance of '
-                'DataFieldRecordArray!')
-
-        for fname in self._field_name_list:
-            self._data_fields[fname][indices] = arr[fname]
-
-    def rename_fields(self, convertions, must_exist=False):
-        """Renames the given fields of this array.
+        values : numpy (N_sources,)-shaped 1D ndarray
+            The array with the detector signal yield for each source.
+        grads : dict
+            The dictionary holding the gradient values for each global floating
+            parameter. The key is the global floating parameter index and the
+            value is the (N_sources,)-shaped numpy ndarray holding the gradient
+            value dY_k/dp_s.
+        """
+        local_param_name = self.param_names[0]
+
+        src_dec = np.atleast_1d(src_recarray['dec'])
+        src_param = src_params_recarray[local_param_name]
+        src_param_gp_idxs = src_params_recarray[f'{local_param_name}:gpidx']
+
+        n_sources = len(src_dec)
+
+        # Check for correct input format.
+        if not (len(src_param) == n_sources and
+                len(src_param_gp_idxs) == n_sources):
+            raise RuntimeError(
+                f'The length ({len(src_param)}) of the array for the '
+                f'source parameter "{local_param_name}" does not match the '
+                f'number of sources ({n_sources})!')
+
+        # Calculate the detector signal yield only for the sources for
+        # which we actually have detector acceptance. For the other sources,
+        # the detector signal yield is zero.
+        src_mask = (np.sin(src_dec) >= self._sin_dec_binning.lower_edge) &\
+                   (np.sin(src_dec) <= self._sin_dec_binning.upper_edge)
+
+        values = np.zeros((n_sources,), dtype=np.float64)
+        values[src_mask] = np.exp(self._log_spl_sinDec_param(
+            np.sin(src_dec[src_mask]), src_param[src_mask], grid=False))
+
+        # Determine the number of global parameters the local parameter is
+        # made of.
+        gfp_idxs = np.unique(src_param_gp_idxs)
+        gfp_idxs = gfp_idxs[gfp_idxs > 0] - 1
+
+        # Calculate the gradients for each global fit parameter.
+        grads = dict()
+        for gfp_idx in gfp_idxs:
+            # Create the gradient array of shape (n_sources,). This could be
+            # a masked array to save memory, when there are many sources and
+            # global fit parameters.
+            grads[gfp_idx] = np.zeros((n_sources,), dtype=np.float64)
+
+            # Create a mask to select the sources that depend on the global
+            # fit parameter with index gfp_idx.
+            gfp_src_mask = (src_param_gp_idxs == gfp_idx+1)
+
+            # m is a (n_sources,)-shaped ndarray, which selects only sources
+            # that have detector exceptance and depend on the global fit
+            # parameter gfp_idx.
+            m = src_mask & gfp_src_mask
+
+            grads[gfp_idx][m] = values[m] * self._log_spl_sinDec_param(
+                np.sin(src_dec[m]), src_param[m], grid=False, dy=1)
+
+        return (values, grads)
+
+
+class SingleParamFluxPointLikeSourceI3DetSigYieldBuilder(
+        PointLikeSourceI3DetSigYieldBuilder,
+        multiproc.IsParallelizable,
+):
+    """This detector signal yield builder constructs a
+    detector signal yield for a variable flux model with a single parameter,
+    assuming a point-like source.
+    It constructs a two-dimensional spline function in sin(dec) and the
+    parameter, using a :class:`scipy.interpolate.RectBivariateSpline`.
+    Hence, the detector signal yield can vary with the declination and the
+    parameter of the flux model.
+
+    It is tailored to the IceCube detector at the South Pole, where the
+    effective area depends soley on the zenith angle, and hence on the
+    declination, of the source.
+    """
+    def __init__(
+            self,
+            param_grid,
+            sin_dec_binning=None,
+            spline_order_sinDec=2,
+            spline_order_param=2,
+            ncpu=None,
+            **kwargs,
+    ):
+        """Creates a new IceCube detector signal yield builder instance for a
+        flux model with a single parameter.
+        It requires a sinDec binning definition to compute the sin(dec)
+        dependency of the detector effective area, and a parameter grid to
+        compute the parameter dependency of the detector signal yield.
 
         Parameters
         ----------
-        convertions : dict of `old_name` -> `new_name`
-            The dictionary holding the old and new names of the data fields.
-        must_exist : bool
-            Flag if the given fields must exist. If set to ``True`` and a field
-            does not exist, a KeyError is raised.
-
-        Raises
-        ------
-        KeyError
-            If ``must_exist`` is set to ``True`` and a given field does not
-            exist.
-        """
-        for (old_fname, new_fname) in convertions.items():
-            if(old_fname in self.field_name_list):
-                self._data_fields[new_fname] = self._data_fields.pop(old_fname)
-            elif(must_exist is True):
-                raise KeyError('The required field "%s" does not exist!'%(
-                    old_fname))
-
-        self._field_name_list = list(self._data_fields.keys())
-
-    def tidy_up(self, keep_fields):
-        """Removes all fields that are not specified through the keep_fields
-        argument.
+        param_grid : instance of ParameterGrid
+            The instance of ParameterGrid which defines the grid of the
+            parameter values. The name of the parameter is defined via the name
+            property of the ParameterGrid instance.
+        sin_dec_binning : instance of BinningDefinition | None
+            The instance of BinningDefinition which defines the sin(dec)
+            binning. If set to None, the sin(dec) binning will be taken from the
+            dataset's binning definitions.
+        spline_order_sinDec : int
+            The order of the spline function for the logarithmic values of the
+            detector signal yield along the sin(dec) axis.
+            The default is 2.
+        spline_order_gamma : int
+            The order of the spline function for the logarithmic values of the
+            detector signal yield along the gamma axis.
+            The default is 2.
+        ncpu : int | None
+            The number of CPUs to utilize. If set to ``None``, global setting
+            will take place.
+        """
+        super().__init__(
+            sin_dec_binning,
+            ncpu=ncpu,
+            **kwargs)
+
+        self.param_grid = param_grid
+        self.spline_order_sinDec = spline_order_sinDec
+        self.spline_order_param = spline_order_param
 
-        Parameters
-        ----------
-        keep_fields : str | sequence of str
-            The field name(s), that should not be removed.
+    @property
+    def param_grid(self):
+        """The ParameterGrid instance for the parameter grid that should be used
+        for computing the parameter dependency of the detector signal yield.
+        """
+        return self._param_grid
+
+    @param_grid.setter
+    def param_grid(self, grid):
+        if not isinstance(grid, ParameterGrid):
+            raise TypeError(
+                'The param_grid property must be an instance of '
+                'ParameterGrid! '
+                f'Its current type is {classname(grid)}.')
+        self._param_grid = grid
 
-        Raises
-        ------
-        TypeError
-            If keep_fields is not an instance of str or a sequence of str
-            instances.
-        """
-        if(isinstance(keep_fields, str)):
-            keep_fields = [ keep_fields ]
-        if(not issequenceof(keep_fields, str)):
-            raise TypeError('The keep_fields argument must be a sequence of '
-                'str!')
-
-        # We need to make a copy of the field_name_list because that list will
-        # get changed by the `remove_field` method.
-        field_name_list = copy.copy(self._field_name_list)
-        for fname in field_name_list:
-            if(fname not in keep_fields):
-                self.remove_field(fname)
+    @property
+    def spline_order_sinDec(self):
+        """The order (int) of the logarithmic spline function, that splines the
+        detector signal yield, along the sin(dec) axis.
+        """
+        return self._spline_order_sinDec
+
+    @spline_order_sinDec.setter
+    def spline_order_sinDec(self, order):
+        if not isinstance(order, int):
+            raise TypeError(
+                'The spline_order_sinDec property must be of type int! '
+                f'Its current type is {classname(order)}.')
+        self._spline_order_sinDec = order
 
-    def sort_by_field(self, name):
-        """Sorts the data along the given field name in ascending order.
+    @property
+    def spline_order_param(self):
+        """The order (int) of the logarithmic spline function, that splines the
+        detector signal yield, along the parameter axis.
+        """
+        return self._spline_order_param
+
+    @spline_order_param.setter
+    def spline_order_param(self, order):
+        if not isinstance(order, int):
+            raise TypeError(
+                'The spline_order_param property must be of type int! '
+                f'Its current type is {classname(order)}.')
+        self._spline_order_param = order
+
+    def construct_detsigyield(
+            self,
+            dataset,
+            data,
+            shg,
+            ppbar=None,
+    ):
+        """Constructs a detector signal yield 2-dimensional log spline
+        function for the given flux model with varying parameter values.
 
         Parameters
         ----------
-        name : str
-            The name of the field along the events should get sorted.
+        dataset : instance of Dataset
+            The instance of Dataset holding the sin(dec) binning definition.
+        data : instance of DatasetData
+            The instance of DatasetData holding the monte-carlo event data.
+            The numpy record array for the monte-carlo data of the dataset must
+            contain the following data fields:
+
+            ``'true_dec'`` : float
+                The true declination of the data event.
+            ``'mcweight'`` : float
+                The monte-carlo weight of the data event in the unit
+                GeV cm^2 sr.
+            ``'true_energy'`` : float
+                The true energy value of the data event.
+
+        shg : instance of SourceHypoGroup
+            The instance of SourceHypoGroup for which the detector signal yield
+            should get constructed.
+        ppbar : instance of ProgressBar | None
+            The instance of ProgressBar of the optional parent progress bar.
 
         Returns
         -------
-        sorted_idxs : (n_events,)-shaped numpy ndarray
-            The numpy ndarray holding the indices of the sorted array.
+        detsigyield : instance of SingleParamFluxPointLikeSourceI3DetSigYield
+            The I3DetSigYield instance for a point-like source with a flux model
+            of a single parameter.
+        """
+        self.assert_types_of_construct_detsigyield_arguments(
+            dataset=dataset,
+            data=data,
+            shgs=shg,
+            ppbar=ppbar)
+
+        # Get integrated live-time in days.
+        livetime_days = Livetime.get_integrated_livetime(data.livetime)
+
+        # Get the sin(dec) binning definition either as setting from this
+        # implementation method, or from the dataset.
+        sin_dec_binning = self.get_sin_dec_binning(dataset)
+
+        # Calculate conversion factor from the flux model unit into the internal
+        # flux unit GeV^-1 cm^-2 s^-1.
+        to_internal_flux_unit_factor = shg.fluxmodel.to_internal_flux_unit()
 
-        Raises
-        ------
-        KeyError
-            If the given data field does not exist.
-        """
-        if(name not in self._data_fields):
-            raise KeyError('The data field "{}" does not exist in this '
-                'DataFieldRecordArray instance!'.format(name))
-
-        sorted_idxs = np.argsort(self._data_fields[name])
+        to_internal_time_unit_factor = self._cfg.to_internal_time_unit(
+            time_unit=units.day
+        )
 
-        for fname in self._field_name_list:
-            self._data_fields[fname] = self._data_fields[fname][sorted_idxs]
+        # Define a function that creates a detector signal yield histogram
+        # along sin(dec) for a given flux model, i.e. for given spectral index,
+        # gamma.
+        def _create_hist(
+                data_sin_true_dec,
+                data_true_energy,
+                sin_dec_binning,
+                weights,
+                fluxmodel,
+                to_internal_flux_unit_factor,
+        ):
+            """Creates a histogram of the detector signal yield with the
+            given sin(dec) binning.
+
+            Parameters
+            ----------
+            data_sin_true_dec : 1d ndarray
+                The sin(true_dec) values of the monte-carlo events.
+            data_true_energy : 1d ndarray
+                The true energy of the monte-carlo events.
+            sin_dec_binning : BinningDefinition
+                The sin(dec) binning definition to use for the histogram.
+            weights : 1d ndarray
+                The weight factors of each monte-carlo event where only the
+                flux value needs to be multiplied with in order to get the
+                detector signal yield.
+            fluxmodel : FluxModel
+                The flux model to get the flux values from.
+            to_internal_flux_unit_factor : float
+                The conversion factor to convert the flux unit into the internal
+                flux unit.
+
+            Returns
+            -------
+            h : 1d ndarray
+                The numpy array containing the histogram values.
+            """
+            weights = (
+                weights *
+                fluxmodel(E=data_true_energy).squeeze() *
+                to_internal_flux_unit_factor
+            )
+
+            (h, edges) = np.histogram(
+                data_sin_true_dec,
+                bins=sin_dec_binning.binedges,
+                weights=weights,
+                density=False)
+
+            return h
+
+        data_sin_true_dec = np.sin(data.mc['true_dec'])
+
+        # Generate a list of indices that would sort the data according to the
+        # sin(true_dec) values. We will sort the MC data according to it,
+        # because the histogram creation is much faster (2x) when the
+        # to-be-histogrammed values are already sorted.
+        sorted_idxs = np.argsort(data_sin_true_dec)
+
+        data_sin_true_dec = np.take(data_sin_true_dec, sorted_idxs)
+        data_true_energy = np.take(data.mc['true_energy'], sorted_idxs)
+
+        weights = (
+            np.take(data.mc['mcweight'], sorted_idxs) *
+            livetime_days*to_internal_time_unit_factor
+        )
 
-        return sorted_idxs
+        # Make a copy of the parameter grid and extend the grid by one bin on
+        # each side.
+        param_grid = self._param_grid.copy()
+        param_grid.add_extra_lower_and_upper_bin()
+
+        # Construct the arguments for the hist function to be used in the
+        # multiproc.parallelize function.
+        args_list = [
+            (
+                (
+                    data_sin_true_dec,
+                    data_true_energy,
+                    sin_dec_binning,
+                    weights,
+                    shg.fluxmodel.copy({param_grid.name: param_val}),
+                    to_internal_flux_unit_factor,
+                ),
+                {}
+            )
+            for param_val in param_grid.grid
+        ]
+        h = np.vstack(
+            multiproc.parallelize(
+                _create_hist, args_list, self.ncpu, ppbar=ppbar)).T
+
+        # Normalize by solid angle of each bin along the sin(dec) axis.
+        # The solid angle is given by 2*\pi*(\Delta sin(\delta)).
+        h /= (2.*np.pi * np.diff(sin_dec_binning.binedges)).reshape(
+            (sin_dec_binning.nbins, 1))
+
+        # Create the 2D spline.
+        log_spl_sinDec_param = scipy.interpolate.RectBivariateSpline(
+            sin_dec_binning.bincenters,
+            param_grid.grid,
+            np.log(h),
+            kx=self.spline_order_sinDec,
+            ky=self.spline_order_param,
+            s=0)
+
+        detsigyield = SingleParamFluxPointLikeSourceI3DetSigYield(
+            param_name=self._param_grid.name,
+            dataset=dataset,
+            fluxmodel=shg.fluxmodel,
+            livetime=data.livetime,
+            sin_dec_binning=sin_dec_binning,
+            log_spl_sinDec_param=log_spl_sinDec_param)
 
-register_FileLoader(['.npy'], NPYFileLoader)
-register_FileLoader(['.pkl'], PKLFileLoader)
-register_FileLoader(['.csv'], TextFileLoader)
+        return detsigyield
```

### Comparing `skyllh-23.1.1/skyllh/core/times.py` & `skyllh-23.2.0/skyllh/core/times.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,23 +1,30 @@
 # -*- coding: utf-8 -*-
 
 import abc
 
 from skyllh.core.livetime import Livetime
 
 
-class TimeGenerationMethod(object, metaclass=abc.ABCMeta):
+class TimeGenerationMethod(
+        object,
+        metaclass=abc.ABCMeta,
+):
     """Base class (type) for implementing a method to generate times.
     """
 
-    def __init__(self):
-        pass
+    def __init__(self, **kwargs):
+        super().__init__(**kwargs)
 
     @abc.abstractmethod
-    def generate_times(self, rss, size):
+    def generate_times(
+            self,
+            rss,
+            size,
+    ):
         """The ``generate_times`` method implements the actual generation of
         times, which is method dependent.
 
         Parameters
         ----------
         rss : instance of RandomStateService
             The random state service providing the random number
@@ -29,42 +36,53 @@
         -------
         times : ndarray
             The 1d numpy ndarray holding the generated times.
         """
         pass
 
 
-class LivetimeTimeGenerationMethod(TimeGenerationMethod):
+class LivetimeTimeGenerationMethod(
+        TimeGenerationMethod,
+):
     """The LivetimeTimeGenerationMethod provides the method to generate times
     from a Livetime object. It will uniformely generate times that will coincide
     with the on-time intervals of the detector, by calling the `draw_ontimes`
     method of the Livetime class.
     """
-    def __init__(self, livetime):
+    def __init__(self, livetime, **kwargs):
         """Creates a new LivetimeTimeGeneration instance.
 
         Parameters
         ----------
         livetime : Livetime
             The Livetime instance that should be used to generate times from.
         """
+        super().__init__(**kwargs)
+
         self.livetime = livetime
 
     @property
     def livetime(self):
         """The Livetime instance used to draw times from.
         """
         return self._livetime
+
     @livetime.setter
     def livetime(self, livetime):
-        if(not isinstance(livetime, Livetime)):
-            raise TypeError('The livetime property must be an instance of Livetime!')
+        if not isinstance(livetime, Livetime):
+            raise TypeError(
+                'The livetime property must be an instance of Livetime!')
         self._livetime = livetime
 
-    def generate_times(self, rss, size):
+    def generate_times(
+            self,
+            rss,
+            size,
+            **kwargs,
+    ):
         """Generates `size` MJD times according to the detector on-times
         provided by the Livetime instance.
 
         Parameters
         ----------
         rss : instance of RandomStateService
             The random state service providing the random number
@@ -73,53 +91,75 @@
             The number of times that should get generated.
 
         Returns
         -------
         times : ndarray
             The 1d (`size`,)-shaped numpy ndarray holding the generated times.
         """
-        return self.livetime.draw_ontimes(rss, size)
+        times = self._livetime.draw_ontimes(
+            rss=rss,
+            size=size,
+            **kwargs)
 
+        return times
 
-class TimeGenerator(object):
+
+class TimeGenerator(
+        object):
     def __init__(self, method):
         """Creates a time generator instance with a given defined time
         generation method.
 
         Parameters
         ----------
-        method : TimeGenerationMethod
+        method : instance of TimeGenerationMethod
             The instance of TimeGenerationMethod that defines the method of
             generating times.
         """
         self.method = method
 
     @property
     def method(self):
         """The TimeGenerationMethod object that should be used to generate
         the times.
         """
         return self._method
+
     @method.setter
     def method(self, method):
-        if(not isinstance(method, TimeGenerationMethod)):
-            raise TypeError('The time generation method must be an instance of TimeGenerationMethod!')
+        if not isinstance(method, TimeGenerationMethod):
+            raise TypeError(
+                'The time generation method must be an instance of '
+                'TimeGenerationMethod!')
         self._method = method
 
-    def generate_times(self, rss, size):
+    def generate_times(
+            self,
+            rss,
+            size,
+            **kwargs,
+    ):
         """Generates ``size`` amount of times by calling the ``generate_times``
         method of the TimeGenerationMethod class.
 
         Parameters
         ----------
-        rss : RandomStateService
-            The random state service providing the random number
-            generator (RNG).
+        rss : instance of RandomStateService
+            The random state service providing the random number generator
+            (RNG).
         size : int
             The number of time that should get generated.
+        **kwargs
+            Additional keyword arguments are passed to the ``generate_times``
+            method of the TimeGenerationMethod class.
 
         Returns
         -------
         times : ndarray
-            The 1d (`size`,)-shaped ndarray holding the generated times.
+            The 1d (``size``,)-shaped ndarray holding the generated times.
         """
-        return self._method.generate_times(rss, size)
+        times = self._method.generate_times(
+            rss=rss,
+            size=size,
+            **kwargs)
+
+        return times
```

### Comparing `skyllh-23.1.1/skyllh/core/timing.py` & `skyllh-23.2.0/skyllh/core/timing.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,143 +1,160 @@
 # -*- coding: utf-8 -*-
 
-import numpy as np
-import time
-
-from skyllh.core import display
-from skyllh.core.py import classname
-
 """The timing module provides code execution timing functionalities. The
 TimeLord class keeps track of execution times of specific code segments,
 called "tasks". The TaskTimer class can be used within a `with`
 statement to time the execution of the code within the `with` block.
 """
 
-class TaskRecord(object):
-    def __init__(self, name, tstart, tend):
+import numpy as np
+import time
+
+from skyllh.core import (
+    display,
+)
+from skyllh.core.py import (
+    classname,
+)
+
+
+class TaskRecord(
+        object):
+    def __init__(
+            self,
+            name,
+            start_times,
+            end_times):
         """Creates a new TaskRecord instance.
 
         Parameters
         ----------
         name : str
             The name of the task.
-        tstart : float | 1d ndarray of float
-            The start time(s) of the task in seconds.
-        tend : float | 1d ndarray of float
-            The end time(s) of the task in seconds.
+        start_times : list of float
+            The start times of the task in seconds.
+        end_times : list of float
+            The end times of the task in seconds.
         """
         self.name = name
 
-        tstart = np.atleast_1d(tstart)
-        tend = np.atleast_1d(tend)
+        if len(start_times) != len(end_times):
+            raise ValueError(
+                'The number of start and end time stamps must be equal!')
 
-        if(len(tstart) != len(tend)):
-            raise ValueError('The number of start and end time stamps must '
-                'be equal!')
-
-        self._tstart_list = list(tstart)
-        self._tend_list = list(tend)
+        self._start_times = start_times
+        self._end_times = end_times
 
     @property
     def tstart(self):
         """(read-only) The time stamps the execution of this task started.
         """
-        return self._tstart_list
+        return self._start_times
 
     @property
     def tend(self):
         """(read-only) The time stamps the execution of this task was stopped.
         """
-        return self._tend_list
+        return self._end_times
 
     @property
     def duration(self):
         """(read-only) The total duration (without time overlap) the task was
         executed.
         """
         # Create a (2,Niter)-shaped 2D ndarray holding the start and end time
         # stamps of the task executions. This array gets then sorted by the
         # start time stamps.
-        arr = np.sort(np.vstack((self._tstart_list, self._tend_list)), axis=1)
+        arr = np.sort(
+            np.vstack((self._start_times, self._end_times)),
+            axis=1)
 
-        d = arr[1,0] - arr[0,0]
-        last_tend = arr[1,0]
+        d = arr[1, 0] - arr[0, 0]
+        last_tend = arr[1, 0]
         for idx in range(1, arr.shape[1]):
-            tstart = arr[0,idx]
-            tend = arr[1,idx]
-            if(tend <= last_tend):
+            tstart = arr[0, idx]
+            tend = arr[1, idx]
+            if tend <= last_tend:
                 continue
-            if(tstart <= last_tend and tend > last_tend):
+            if tstart <= last_tend and tend > last_tend:
                 d += tend - last_tend
-            elif(tstart >= last_tend):
+            elif tstart >= last_tend:
                 d += tend - tstart
             last_tend = tend
 
         return d
 
     @property
     def niter(self):
         """(read-only) The number of times this task was executed.
         """
-        return len(self._tstart_list)
+        return len(self._start_times)
 
     def join(self, tr):
         """Joins this TaskRecord with the given TaskRecord instance.
 
         Parameters
         ----------
-        tr : TaskRecord
+        tr : instance of TaskRecord
             The instance of TaskRecord that should be joined with this
             TaskRecord instance.
         """
-        self._tstart_list.extend(tr._tstart_list)
-        self._tend_list.extend(tr._tend_list)
+        self._start_times.extend(tr._start_times)
+        self._end_times.extend(tr._end_times)
 
 
-class TimeLord(object):
-    def __init__(self):
+class TimeLord(
+        object):
+    def __init__(
+            self):
         self._task_records = []
         self._task_records_name_idx_map = {}
 
     @property
     def task_name_list(self):
         """(read-only) The list of task names.
         """
         return list(self._task_records_name_idx_map.keys())
 
-    def add_task_record(self, tr):
+    def add_task_record(
+            self,
+            tr):
         """Adds a given task record to the internal list of task records.
         """
         tname = tr.name
 
-        if(self.has_task_record(tname)):
+        if self.has_task_record(tname):
             # The TaskRecord already exists. Update the task record.
             self_tr = self.get_task_record(tname)
             self_tr.join(tr)
             return
 
         self._task_records.append(tr)
         self._task_records_name_idx_map[tr.name] = len(self._task_records)-1
 
-    def get_task_record(self, name):
+    def get_task_record(
+            self,
+            name):
         """Retrieves a task record of the given name.
 
         Parameters
         ----------
         name : str
             The name of the task record.
 
         Returns
         -------
-        task_record : TaskRecord
+        task_record : instance of TaskRecord
             The instance of TaskRecord with the requested name.
         """
         return self._task_records[self._task_records_name_idx_map[name]]
 
-    def has_task_record(self, name):
+    def has_task_record(
+            self,
+            name):
         """Checks if this TimeLord instance has a task record of the given name.
 
         Parameters
         ----------
         name : str
             The name of the task record.
 
@@ -145,27 +162,29 @@
         -------
         check : bool
             ``True`` if this TimeLord instance has a task record of the given
             name, and ``False`` otherwise.
         """
         return name in self._task_records_name_idx_map
 
-    def join(self, tl):
+    def join(
+            self,
+            tl):
         """Joins a given TimeLord instance with this TimeLord instance. Tasks
         of the same name will be updated and new tasks will be added.
 
         Parameters
         ----------
-        tl : TimeLord instance
+        tl : instance of TimeLord
             The instance of TimeLord whos tasks should be joined with the tasks
             of this TimeLord instance.
         """
         for tname in tl.task_name_list:
             other_tr = tl.get_task_record(tname)
-            if(self.has_task_record(tname)):
+            if self.has_task_record(tname):
                 # Update the task record.
                 tr = self.get_task_record(tname)
                 tr.join(other_tr)
             else:
                 # Add a new task record.
                 self.add_task_record(other_tr)
 
@@ -177,40 +196,47 @@
     def __str__(self):
         """Generates a pretty string for this time lord.
         """
         task_name_list = self.task_name_list
         n_tasks = len(task_name_list)
 
         s = f'{classname(self)}: Executed tasks:'
-        if(n_tasks == 0):
+        if n_tasks == 0:
             s += ' None.'
             return s
 
-        task_name_len_list = [ len(task_name) for task_name in task_name_list ]
+        task_name_len_list = [
+            len(task_name)
+            for task_name in task_name_list
+        ]
         max_task_name_len = np.minimum(
             np.max(task_name_len_list), display.PAGE_WIDTH-25)
 
         for i in range(n_tasks):
             tr = self._task_records[i]
             task_name = tr.name[0:max_task_name_len]
             t = tr.duration / tr.niter
             line = '\n[{task_name:'+str(max_task_name_len)+'s}] {t:7.{p}{c}} '\
                 'sec/iter ({niter:d})'
             s += line.format(
                 task_name=task_name,
                 t=t,
-                p=1 if t>1e3 or t<1e-3 else 3,
+                p=1 if t > 1e3 or t < 1e-3 else 3,
                 c='e' if t > 1e3 or t < 1e-3 else 'f',
                 niter=tr.niter)
 
         return s
 
 
-class TaskTimer(object):
-    def __init__(self, time_lord, name):
+class TaskTimer(
+        object):
+    def __init__(
+            self,
+            time_lord,
+            name):
         """
         Parameters
         ----------
         time_lord : instance of TimeLord
             The TimeLord instance that keeps track of the recorded tasks.
         name : str
             The name of the task.
@@ -223,31 +249,35 @@
 
     @property
     def time_lord(self):
         """The TimeLord instance that keeps track of the recorded tasks. This
         can be None, which means that the task should not get recorded.
         """
         return self._time_lord
+
     @time_lord.setter
     def time_lord(self, lord):
-        if(lord is not None):
-            if(not isinstance(lord, TimeLord)):
-                raise TypeError('The time_lord property must be None or an '
-                    'instance of TimeLord!')
+        if lord is not None:
+            if not isinstance(lord, TimeLord):
+                raise TypeError(
+                    'The time_lord property must be None or an instance of '
+                    'TimeLord!')
         self._time_lord = lord
 
     @property
     def name(self):
         """The name if the task.
         """
         return self._name
+
     @name.setter
     def name(self, name):
-        if(not isinstance(name, str)):
-            raise TypeError('The name property must be an instance of str!')
+        if not isinstance(name, str):
+            raise TypeError(
+                'The name property must be an instance of str!')
         self._name = name
 
     @property
     def duration(self):
         """The duration in seconds the task was executed.
         """
         return (self._end - self._start)
@@ -259,12 +289,15 @@
         return self
 
     def __exit__(self, exc_type, exc_value, traceback):
         """This gets executed when exiting the `with` block.
         """
         self._end = time.process_time()
 
-        if(self._time_lord is None):
+        if self._time_lord is None:
             return
 
-        self._time_lord.add_task_record(TaskRecord(
-            self._name, self._start, self._end))
+        self._time_lord.add_task_record(
+            TaskRecord(
+                name=self._name,
+                start_times=[self._start],
+                end_times=[self._end]))
```

### Comparing `skyllh-23.1.1/skyllh/core/trialdata.py` & `skyllh-23.2.0/skyllh/core/trialdata.py`

 * *Files 25% similar despite different names*

```diff
@@ -6,320 +6,404 @@
 The rational behind this manager is to compute data fields only once, which can
 then be used by different analysis objects, like PDF objects.
 """
 
 from collections import OrderedDict
 import numpy as np
 
-from skyllh.core.debugging import get_logger
+from skyllh.core.debugging import (
+    get_logger,
+)
 from skyllh.core import display as dsp
 from skyllh.core.py import (
     classname,
     func_has_n_args,
     int_cast,
     issequenceof,
-    typename
 )
-from skyllh.core.storage import DataFieldRecordArray
+from skyllh.core.storage import (
+    DataFieldRecordArray,
+)
 
 
 logger = get_logger(__name__)
 
 
 class DataField(object):
     """This class defines a data field and its calculation that is used by an
     Analysis class instance. The calculation is defined through an external
     function.
     """
     def __init__(
-            self, name, func, fitparam_names=None, dt=None):
+            self,
+            name,
+            func,
+            global_fitparam_names=None,
+            dt=None,
+            is_src_field=False,
+            is_srcevt_data=False,
+            **kwargs):
         """Creates a new instance of DataField that might depend on fit
         parameters.
 
         Parameters
         ----------
         name : str
             The name of the data field. It serves as the identifier for the
             data field.
         func : callable
             The function that calculates the values of this data field. The call
             signature must be
-            `__call__(tdm, src_hypo_group_manager, fitparams)`,
-            where `tdm` is the TrialDataManager instance holding the event data,
-            `src_hypo_group_manager` is the SourceHypoGroupManager instance, and
-            `fitparams` is the dictionary with the current fit parameter names
-            and values. If the data field depends solely on source parameters,
-            the call signature must be `__call__(tdm, src_hypo_group_manager)`
-            instead.
-        fitparam_names : sequence of str | None
-            The sequence of str instances specifying the names of the fit
+
+                __call__(tdm, shg_mgr, pmm, global_fitparams_dict=None)
+
+            where ``tdm`` is the instance of TrialDataManager holding the trial
+            event data, ``shg_mgr`` is the instance of SourceHypoGroupManager,
+            ``pmm`` is the instance of ParameterModelMapper, and
+            ``global_fitparams_dict`` is the dictionary with the current global
+            fit parameter names and values.
+        global_fitparam_names : str | sequence of str | None
+            The sequence of str instances specifying the names of the global fit
             parameters this data field depends on. If set to None, the data
             field does not depend on any fit parameters.
         dt : numpy dtype | str | None
             If specified it defines the data type this data field should have.
             If a str instance is given, it defines the name of the data field
             whose data type should be taken for this data field.
+        is_src_field : bool
+            Flag if this data field is a source data field (``True``) and values
+            should be stored within this DataField instance, instead of the
+            events DataFieldRecordArray instance of the TrialDataManager
+            (``False``).
+        is_srcevt_data : bool
+            Flag if the data field will hold source-event data, i.e. data of
+            length N_values. In that case the data cannot be stored within the
+            events attribute of the TrialDataManager, but must be stored in the
+            values attribute of this DataField instance.
         """
-        super(DataField, self).__init__()
+        super().__init__(**kwargs)
 
         self.name = name
         self.func = func
 
-        if(fitparam_names is None):
-            fitparam_names = []
-        if(not issequenceof(fitparam_names, str)):
-            raise TypeError('The fitparam_names argument must be None or a '
-                'sequence of str instances!')
-        self._fitparam_name_list = list(fitparam_names)
+        if global_fitparam_names is None:
+            global_fitparam_names = []
+        if isinstance(global_fitparam_names, str):
+            global_fitparam_names = [global_fitparam_names]
+        if not issequenceof(global_fitparam_names, str):
+            raise TypeError(
+                'The global_fitparam_names argument must be None or a sequence '
+                'of str instances! It is of type '
+                f'{classname(global_fitparam_names)}!')
+        self._global_fitparam_name_list = list(global_fitparam_names)
 
         self.dt = dt
 
         # Define the list of fit parameter values for which the fit parameter
         # depend data field values have been calculated for.
-        self._fitparam_value_list = [None]*len(self._fitparam_name_list)
+        self._global_fitparam_value_list = [None] *\
+            len(self._global_fitparam_name_list)
+
+        self._is_srcevt_data = is_srcevt_data
 
         # Define the member variable that holds the numpy ndarray with the data
         # field values.
         self._values = None
 
         # Define the most efficient `calculate` method for this kind of data
         # field.
-        if(func_has_n_args(self._func, 2)):
+        if is_src_field:
             self.calculate = self._calc_source_values
-        elif(len(self._fitparam_name_list) == 0):
+        elif len(self._global_fitparam_name_list) == 0:
             self.calculate = self._calc_static_values
         else:
-            self.calculate = self._calc_fitparam_dependent_values
+            self.calculate = self._calc_global_fitparam_dependent_values
 
     @property
     def name(self):
         """The name of the data field.
         """
         return self._name
+
     @name.setter
     def name(self, name):
-        if(not isinstance(name, str)):
-            raise TypeError('The name property must be an instance of str!')
+        if not isinstance(name, str):
+            raise TypeError(
+                'The name property must be an instance of str!'
+                f'It is of type {classname(name)}!')
         self._name = name
 
     @property
     def func(self):
         """The function that calculates the data field values.
         """
         return self._func
+
     @func.setter
     def func(self, f):
-        if(not callable(f)):
-            raise TypeError('The func property must be a callable object!')
-        if((not func_has_n_args(f, 2)) and
-           (not func_has_n_args(f, 3))):
-            raise TypeError('The func property must be a function with 2 or 3 '
-                'arguments!')
+        if not callable(f):
+            raise TypeError(
+                'The func property must be a callable object!')
+        if (not func_has_n_args(f, 3)) and\
+           (not func_has_n_args(f, 4)):
+            raise TypeError(
+                'The func property must be a function with 3 or 4 arguments!')
         self._func = f
 
     @property
     def dt(self):
         """The numpy dtype object defining the data type of this data field.
         A str instance defines the name of the data field whose data type should
         be taken for this data field.
         It is None, if there is no explicit data type defined for this data
         field.
         """
         return self._dt
+
     @dt.setter
     def dt(self, obj):
-        if(obj is not None):
-            if((not isinstance(obj, np.dtype)) and
-               (not isinstance(obj, str))):
+        if obj is not None:
+            if (not isinstance(obj, np.dtype)) and\
+               (not isinstance(obj, str)):
                 raise TypeError(
                     'The dt property must be None, an instance of numpy.dtype, '
-                    'or an instance of str! Currently it is of type %s.'%(
-                        str(type(obj))))
+                    'or an instance of str! Currently it is of type '
+                    f'{classname(obj)}.')
         self._dt = obj
 
     @property
+    def is_srcevt_data(self):
+        """(read-only) Flag if the data field contains source-event data, i.e.
+        is of length N_values.
+        """
+        return self._is_srcevt_data
+
+    @property
     def values(self):
         """(read-only) The calculated data values of the data field.
         """
         return self._values
 
     def __str__(self):
         """Pretty string representation of this DataField instance.
         """
         dtype = 'None'
         vmin = np.nan
         vmax = np.nan
-        if(self._values is not None):
+
+        if self._values is not None:
             dtype = str(self._values.dtype)
-            try:
-                vmin = np.min(self._values)
-            except:
-                pass
-            try:
-                vmax = np.max(self._values)
-            except:
-                pass
-        s = '{}: {}: '.format(classname(self), self.name)
-        s +='{dtype: '
-        s += '{}, vmin: {: .3e}, vmax: {: .3e}'.format(
-            dtype, vmin, vmax)
+            vmin = np.min(self._values)
+            vmax = np.max(self._values)
+
+        s = f'{classname(self)}: {self.name}: '
+        s += '{dtype: '
+        s += f'{dtype}, vmin: {vmin: .3e}, vmax: {vmax: .3e}'
         s += '}'
 
         return s
 
     def _get_desired_dtype(self, tdm):
-        """Retrieves the data type this field should have. It's None, if no
+        """Retrieves the data type this field should have. It's ``None``, if no
         data type was defined for this data field.
         """
-        if(self._dt is not None):
-            if(isinstance(self._dt, str)):
+        if self._dt is not None:
+            if isinstance(self._dt, str):
+                # The _dt attribute defines the name of the data field whose
+                # data type should be used.
                 self._dt = tdm.get_dtype(self._dt)
         return self._dt
 
     def _convert_to_desired_dtype(self, tdm, values):
         """Converts the data type of the given values array to the given data
         type.
         """
         dt = self._get_desired_dtype(tdm)
-        if(dt is not None):
+        if dt is not None:
             values = values.astype(dt, copy=False)
         return values
 
     def _calc_source_values(
-            self, tdm, src_hypo_group_manager, fitparams):
+            self,
+            tdm,
+            shg_mgr,
+            pmm):
         """Calculates the data field values utilizing the defined external
         function. The data field values solely depend on fixed source
         parameters.
-        """
-        self._values = self._func(tdm, src_hypo_group_manager)
-        if(not isinstance(self._values, np.ndarray)):
+
+        Parameters
+        ----------
+        tdm : instance of TrialDataManager
+            The TrialDataManager instance this data field is part of and is
+            holding the event data.
+        shg_mgr : instance of SourceHypoGroupManager
+            The instance of SourceHypoGroupManager, which defines the source
+            hypothesis groups.
+        pmm : instance of ParameterModelMapper
+            The instance of ParameterModelMapper, which defines the global
+            parameters and their mapping to local source parameters.
+        """
+        self._values = self._func(
+            tdm=tdm,
+            shg_mgr=shg_mgr,
+            pmm=pmm)
+
+        if not isinstance(self._values, np.ndarray):
             raise TypeError(
-                'The calculation function for the data field "%s" must '
-                'return an instance of numpy.ndarray! '
-                'Currently it is of type "%s".'%(
-                    self._name, typename(type(self._values))))
+                f'The calculation function for the data field "{self._name}" '
+                'must return an instance of numpy.ndarray! '
+                f'Currently it is of type "{classname(self._values)}".')
 
         # Convert the data type.
         self._values = self._convert_to_desired_dtype(tdm, self._values)
 
     def _calc_static_values(
-            self, tdm, src_hypo_group_manager, fitparams):
+            self,
+            tdm,
+            shg_mgr,
+            pmm):
         """Calculates the data field values utilizing the defined external
         function, that are static and only depend on source parameters.
 
         Parameters
         ----------
         tdm : instance of TrialDataManager
             The TrialDataManager instance this data field is part of and is
             holding the event data.
-        src_hypo_group_manager : instance of SourceHypoGroupManager
-            The instance of SourceHypoGroupManager, which defines the groups of
-            source hypotheses.
-        fitparams : dict
-            The dictionary holding the current fit parameter names and values.
-            By definition this dictionary is empty.
-        """
-        values = self._func(tdm, src_hypo_group_manager, fitparams)
-        if(not isinstance(values, np.ndarray)):
+        shg_mgr : instance of SourceHypoGroupManager
+            The instance of SourceHypoGroupManager, which defines the source
+            hypothesis groups.
+        pmm : instance of ParameterModelMapper
+            The instance of ParameterModelMapper, which defines the global
+            parameters and their mapping to local source parameters.
+        """
+        values = self._func(
+            tdm=tdm,
+            shg_mgr=shg_mgr,
+            pmm=pmm)
+
+        if not isinstance(values, np.ndarray):
             raise TypeError(
-                'The calculation function for the data field "%s" must '
-                'return an instance of numpy.ndarray! '
-                'Currently it is of type "%s".'%(
-                    self._name, typename(type(values))))
+                f'The calculation function for the data field "{self._name}" '
+                'must return an instance of numpy.ndarray! '
+                f'Currently it is of type "{classname(values)}".')
 
         # Convert the data type.
         values = self._convert_to_desired_dtype(tdm, values)
 
-        # Set the data values. This will add the data field to the
-        # DataFieldRecordArray if it does not exist yet.
-        tdm.events[self._name] = values
+        if self._is_srcevt_data:
+            n_values = tdm.get_n_values()
+            if values.shape[0] != n_values:
+                raise ValueError(
+                    'The calculation function for the data field '
+                    f'"{self._name}" must return a numpy ndarray of shape '
+                    f'({n_values},), but the shape is {values.shape}!')
+            self._values = values
+        else:
+            # Set the data values. This will add the data field to the
+            # DataFieldRecordArray if it does not exist yet.
+            tdm.events[self._name] = values
 
-    def _calc_fitparam_dependent_values(
-            self, tdm, src_hypo_group_manager, fitparams):
+    def _calc_global_fitparam_dependent_values(
+            self,
+            tdm,
+            shg_mgr,
+            pmm,
+            global_fitparams_dict):
         """Calculate data field values utilizing the defined external
         function, that depend on fit parameter values. We check if the fit
         parameter values have changed.
 
         Parameters
         ----------
         tdm : instance of TrialDataManager
             The TrialDataManager instance this data field is part of and is
             holding the event data.
-        src_hypo_group_manager : instance of SourceHypoGroupManager
-            The instance of SourceHypoGroupManager, which defines the groups of
-            source hypotheses.
-        fitparams : dict
-            The dictionary holding the current fit parameter names and values.
+        shg_mgr : instance of SourceHypoGroupManager
+            The instance of SourceHypoGroupManager, which defines the source
+            hypothesis groups.
+        pmm : instance of ParameterModelMapper
+            The instance of ParameterModelMapper defining the mapping of the
+            global parameters to local source parameters.
+        global_fitparams_dict : dict
+            The dictionary holding the current global fit parameter names and
+            values.
         """
-        if(self._name not in tdm.events):
-            # It's the first time this method is called, so we need to calculate
-            # the data field values for sure.
-            values = self._func(tdm, src_hypo_group_manager, fitparams)
-            if(not isinstance(values, np.ndarray)):
-                raise TypeError(
-                    'The calculation function for the data field "%s" must '
-                    'return an instance of numpy.ndarray! '
-                    'Currently it is of type "%s".'%(
-                        self._name, typename(type(values))))
+        # Determine if we need to calculate the values.
+        calc_values = False
 
-            # Convert the data type.
-            values = self._convert_to_desired_dtype(tdm, values)
+        if self._name not in tdm.events:
+            calc_values = True
+        else:
+            for (idx, name) in enumerate(self._global_fitparam_name_list):
+                if global_fitparams_dict[name] !=\
+                   self._global_fitparam_value_list[idx]:
+                    calc_values = True
+                    break
 
-            # Set the data values. This will add the data field to the
-            # DataFieldRecordArray if it does not exist yet.
-            tdm.events[self._name] = values
+        if not calc_values:
+            return
 
-            # We store the fit parameter values for which the field values were
-            # calculated for. So they have to get recalculated only when the
-            # fit parameter values the field depends on change.
-            self._fitparam_value_list = [
-                fitparams[name] for name in self._fitparam_name_list
-            ]
+        values = self._func(
+            tdm=tdm,
+            shg_mgr=shg_mgr,
+            pmm=pmm,
+            global_fitparams_dict=global_fitparams_dict)
 
-            return
+        if not isinstance(values, np.ndarray):
+            raise TypeError(
+                'The calculation function for the data field '
+                f'"{self._name}" must return an instance of numpy.ndarray! '
+                f'Currently it is of type "{classname(values)}".')
 
-        for (idx, fitparam_name) in enumerate(self._fitparam_name_list):
-            if(fitparams[fitparam_name] != self._fitparam_value_list[idx]):
-                # This current fit parameter value has changed. So we need to
-                # re-calculate the data field values.
-                values = self._func(tdm, src_hypo_group_manager, fitparams)
-
-                # Convert the data type.
-                values = self._convert_to_desired_dtype(tdm, values)
-
-                # Set the data values.
-                tdm.events[self._name] = values
-
-                # Store the new fit parameter values.
-                self._fitparam_value_list = [
-                    fitparams[name] for name in self._fitparam_name_list
-                ]
+        # Convert the data type.
+        values = self._convert_to_desired_dtype(tdm, values)
 
-                break
+        if self._is_srcevt_data:
+            n_values = tdm.get_n_values()
+            if values.shape[0] != n_values:
+                raise ValueError(
+                    'The calculation function for the data field '
+                    f'"{self._name}" must return a numpy ndarray of shape '
+                    f'({n_values},), but the shape is {values.shape}!')
+            self._values = values
+        else:
+            # Set the data values. This will add the data field to the
+            # DataFieldRecordArray if it does not exist yet.
+            tdm.events[self._name] = values
+
+        # We store the global fit parameter values for which the field values
+        # were calculated. So they have to get recalculated only when the
+        # global fit parameter values, the field depends on, change.
+        self._global_fitparam_value_list = [
+            global_fitparams_dict[name]
+            for name in self._global_fitparam_name_list
+        ]
 
 
 class TrialDataManager(object):
     """The TrialDataManager class manages the event data for an analysis trial.
     It provides possible additional data fields and their calculation.
-    New data fields can be defined via the `add_data_field` method.
+    New data fields can be defined via the :py:meth:`add_data_field` method.
     Whenever a new trial is being initialized the data fields get re-calculated.
     The data trial manager is provided to the PDF evaluation method.
     Hence, data fields are calculated only once.
     """
-    def __init__(self, index_field_name=None):
+    def __init__(self, index_field_name=None, **kwargs):
         """Creates a new TrialDataManager instance.
 
         Parameters
         ----------
         index_field_name : str | None
             The name of the field that should be used as primary index field.
             If provided, the events will be sorted along this data field. This
             might be useful for run-time performance.
         """
-        super(TrialDataManager, self).__init__()
+        super().__init__(**kwargs)
 
         self.index_field_name = index_field_name
 
         # Define the list of data fields that depend only on the source
         # parameters.
         self._source_data_fields_dict = OrderedDict()
 
@@ -328,68 +412,92 @@
         self._pre_evt_sel_static_data_fields_dict = OrderedDict()
 
         # Define the list of data fields that are static, i.e. don't depend on
         # any fit parameters. These fields have to be calculated only once when
         # a new evaluation data is available.
         self._static_data_fields_dict = OrderedDict()
 
-        # Define the list of data fields that depend on fit parameters. These
-        # data fields have to be re-calculated whenever a fit parameter value
-        # changes.
-        self._fitparam_data_fields_dict = OrderedDict()
+        # Define the list of data fields that depend on global fit parameters.
+        # These data fields have to be re-calculated whenever a global fit
+        # parameter value changes.
+        self._global_fitparam_data_fields_dict = OrderedDict()
+
+        # Define the member variable that will hold the number of sources.
+        self._n_sources = None
+
+        # Define the member variable that will hold the total number of events
+        # of the dataset this TrialDataManager belongs to.
+        self._n_events = None
 
         # Define the member variable that will hold the raw events for which the
         # data fields get calculated.
         self._events = None
 
         # Define the member variable that holds the source to event index
         # mapping.
-        self._src_ev_idxs = None
+        self._src_evt_idxs = None
 
         # We store an integer number for the trial data state and increase it
         # whenever the state of the trial data changed. This way other code,
         # e.g. PDFs, can determine when the data changed and internal caches
         # must be flushed.
         self._trial_data_state_id = -1
 
     @property
     def index_field_name(self):
         """The name of the primary index data field. If not None, events will
         be sorted by this data field.
         """
         return self._index_field_name
+
     @index_field_name.setter
     def index_field_name(self, name):
-        if(name is not None):
-            if(not isinstance(name, str)):
+        if name is not None:
+            if not isinstance(name, str):
                 raise TypeError(
                     'The index_field_name property must be an instance of '
-                    'type str!')
+                    f'type str! It is of type {classname(name)}!')
         self._index_field_name = name
 
     @property
     def events(self):
         """The DataFieldRecordArray instance holding the data events, which
         should get evaluated.
         """
         return self._events
+
     @events.setter
     def events(self, arr):
-        if(not isinstance(arr, DataFieldRecordArray)):
+        if not isinstance(arr, DataFieldRecordArray):
             raise TypeError(
                 'The events property must be an instance of '
-                'DataFieldRecordArray!')
+                f'DataFieldRecordArray! It is of type {classname(arr)}!')
         self._events = arr
 
     @property
+    def has_global_fitparam_data_fields(self):
+        """(read-only) ``True`` if the TrialDataManager has global fit parameter
+        data fields defined, ``False`` otherwise.
+        """
+        return len(self._global_fitparam_data_fields_dict) > 0
+
+    @property
+    def n_sources(self):
+        """(read-only) The number of sources. This information is taken from
+        the source hypo group manager when a new trial is initialized.
+        """
+        return self._n_sources
+
+    @property
     def n_events(self):
         """The total number of events of the dataset this trial data manager
         corresponds to.
         """
         return self._n_events
+
     @n_events.setter
     def n_events(self, n):
         self._n_events = int_cast(
             n, 'The n_events property must be castable to type int!')
 
     @property
     def n_selected_events(self):
@@ -402,19 +510,20 @@
         """(read-only) The number of pure background events, which are not part
         of the trial data, but must be considered for the test-statistic value.
         It is the difference of n_events and n_selected_events.
         """
         return self._n_events - len(self._events)
 
     @property
-    def src_ev_idxs(self):
-        """(read-only) The 2-tuple holding the source index and event index
-        1d ndarray arrays.
+    def src_evt_idxs(self):
+        """(read-only) The 2-tuple holding the source indices and event indices
+        1d ndarray arrays. This can be ``None``, indicating that all trial data
+        events should be considered for all sources.
         """
-        return self._src_ev_idxs
+        return self._src_evt_idxs
 
     @property
     def trial_data_state_id(self):
         """(read-only) The integer ID number of the trial data. This ID number
         can be used to determine when the trial data has changed its state.
         """
         return self._trial_data_state_id
@@ -430,359 +539,613 @@
         Returns
         -------
         check : bool
             True if the data field is defined in this data field manager,
             False otherwise.
         """
         # Check if the data field is part of the original trial data.
-        if((self._events is not None) and
-           (name in self._events.field_name_list)):
+        if (self._events is not None) and\
+           (name in self._events.field_name_list):
             return True
 
         # Check if the data field is a user defined data field.
-        if((name in self._source_data_fields_dict) or
-           (name in self._pre_evt_sel_static_data_fields_dict) or
-           (name in self._static_data_fields_dict) or
-           (name in self._fitparam_data_fields_dict)):
+        if (name in self._source_data_fields_dict) or\
+           (name in self._pre_evt_sel_static_data_fields_dict) or\
+           (name in self._static_data_fields_dict) or\
+           (name in self._global_fitparam_data_fields_dict):
             return True
 
         return False
 
+    def __getitem__(self, name):
+        """Implements the evaluation of ``self[name]`` to access data fields.
+        This method calls the :meth:`get_data` method of this class.
+        """
+        return self.get_data(name)
+
     def __str__(self):
         """Implements pretty string representation of this TrialDataManager
         instance.
         """
         s = classname(self)+':\n'
         s1 = 'Base data fields:\n'
         s2 = str(self._events)
         s1 += dsp.add_leading_text_line_padding(dsp.INDENTATION_WIDTH, s2)
         s += dsp.add_leading_text_line_padding(dsp.INDENTATION_WIDTH, s1)
         s += '\n'
 
         s1 = 'Source data fields:\n'
-        s2 = ''
-        for (idx, dfield) in enumerate(self._source_data_fields_dict):
-            if(idx > 0):
-                s2 += '\n'
-            s2 += str(dfield)
-        if(s2 == ''):
+        s2 = '\n'.join(
+            [
+                str(df)
+                for (_, df) in self._source_data_fields_dict.items()
+            ]
+        )
+        if s2 == '':
             s2 = 'None'
         s1 += dsp.add_leading_text_line_padding(dsp.INDENTATION_WIDTH, s2)
         s += dsp.add_leading_text_line_padding(dsp.INDENTATION_WIDTH, s1)
         s += '\n'
 
         s1 = 'Pre-event-selection static data fields:\n'
-        s2 = ''
-        for (idx, dfield) in enumerate(self._pre_evt_sel_static_data_fields_dict):
-            if(idx > 0):
-                s2 += '\n'
-            s2 += str(dfield)
+        s2 = '\n'.join(
+            [
+                str(df)
+                for (_, df) in self._pre_evt_sel_static_data_fields_dict.items()
+            ]
+        )
+        if s2 == '':
+            s2 = 'None'
         s1 += dsp.add_leading_text_line_padding(dsp.INDENTATION_WIDTH, s2)
         s += dsp.add_leading_text_line_padding(dsp.INDENTATION_WIDTH, s1)
         s += '\n'
 
         s1 = 'Static data fields:\n'
-        s2 = ''
-        for (idx, dfield) in enumerate(self._static_data_fields_dict):
-            if(idx > 0):
-                s2 += '\n'
-            s2 += str(dfield)
+        s2 = '\n'.join(
+            [
+                str(df)
+                for (_, df) in self._static_data_fields_dict.items()
+            ]
+        )
+        if s2 == '':
+            s2 = 'None'
         s1 += dsp.add_leading_text_line_padding(dsp.INDENTATION_WIDTH, s2)
         s += dsp.add_leading_text_line_padding(dsp.INDENTATION_WIDTH, s1)
         s += '\n'
 
-        s1 = 'Fitparam data fields:\n'
-        s2 = ''
-        for (idx, dfield) in enumerate(self._fitparam_data_fields_dict):
-            if(idx > 0):
-                s2 += '\n'
-            s2 += str(dfield)
-        if(s2 == ''):
+        s1 = 'Global fitparam data fields:\n'
+        s2 = '\n'.join(
+            [
+                str(df)
+                for (_, df) in self._global_fitparam_data_fields_dict.items()
+            ]
+        )
+        if s2 == '':
             s2 = 'None'
         s1 += dsp.add_leading_text_line_padding(dsp.INDENTATION_WIDTH, s2)
         s += dsp.add_leading_text_line_padding(dsp.INDENTATION_WIDTH, s1)
 
         return s
 
-    def change_source_hypo_group_manager(self, src_hypo_group_manager):
-        """Recalculate the source data fields.
+    def broadcast_sources_array_to_values_array(
+            self,
+            arr):
+        """Broadcasts the given 1d numpy ndarray of length 1 or N_sources to a
+        numpy ndarray of length N_values.
 
         Parameters
         ----------
-        src_hypo_group_manager : instance of SourceHypoGroupManager
-            The SourceHypoGroupManager manager that defines the groups of
-            source hypotheses.
+        arr : instance of ndarray
+            The (N_sources,)- or (1,)-shaped numpy ndarray holding values for
+            each source.
+
+        Returns
+        -------
+        out_arr : instance of ndarray
+            The (N_values,)-shaped numpy ndarray holding the source values
+            broadcasted to each event value.
         """
-        self.calculate_source_data_fields(src_hypo_group_manager)
+        arr_dtype = arr.dtype
+        n_values = self.get_n_values()
+
+        if len(arr) == 1:
+            return np.full((n_values,), arr[0], dtype=arr_dtype)
+
+        if len(arr) != self.n_sources:
+            raise ValueError(
+                f'The length of arr ({len(arr)}) must be 1 or equal to the '
+                f'number of sources ({self.n_sources})!')
+
+        out_arr = np.empty(
+            (n_values,),
+            dtype=arr.dtype)
+
+        src_idxs = self.src_evt_idxs[0]
+        v_start = 0
+        for (src_idx, src_value) in enumerate(arr):
+            n = np.count_nonzero(src_idxs == src_idx)
+            # n = len(evt_idxs[src_idxs == src_idx])
+            out_arr[v_start:v_start+n] = np.full(
+                (n,), src_value, dtype=arr_dtype)
+            v_start += n
+
+        return out_arr
+
+    def broadcast_sources_arrays_to_values_arrays(
+            self,
+            arrays):
+        """Broadcasts the 1d numpy ndarrays to the values array.
+
+        Parameters
+        ----------
+        arrays : sequence of numpy 1d ndarrays
+            The sequence of (N_sources,)-shaped numpy ndarrays holding the
+            parameter values.
+
+        Returns
+        -------
+        out_arrays : list of numpy 1d ndarrays
+            The list of (N_values,)-shaped numpy ndarrays holding the
+            broadcasted array values.
+        """
+        out_arrays = [
+            self.broadcast_sources_array_to_values_array(arr)
+            for arr in arrays
+        ]
+
+        return out_arrays
+
+    def broadcast_selected_events_arrays_to_values_arrays(
+            self,
+            arrays):
+        """Broadcasts the given arrays of length N_selected_events to arrays
+        of length N_values.
+
+        Parameters
+        ----------
+        arrays : sequence of instance of ndarray
+            The sequence of instance of ndarray with the arrays to be
+            broadcasted.
+
+        Returns
+        -------
+        out_arrays : list of instance of ndarray
+            The list of broadcasted numpy ndarray instances.
+        """
+        evt_idxs = self._src_evt_idxs[1]
+        out_arrays = [
+            np.take(arr, evt_idxs)
+            for arr in arrays
+        ]
+
+        return out_arrays
+
+    def change_shg_mgr(self, shg_mgr, pmm):
+        """This method is called when the source hypothesis group manager has
+        changed. Hence, the source data fields need to get recalculated.
+
+        After calling this method, a new trial should be initialized via the
+        :meth:`initialize_trial` method!
+
+        Parameters
+        ----------
+        shg_mgr : instance of SourceHypoGroupManager
+            The instance of SourceHypoGroupManager that defines the source
+            hypothesis groups.
+        pmm : instance of ParameterModelMapper
+            The instance of ParameterModelMapper that defines the global
+            parameters and their mapping to local source parameter.
+        """
+        self.calculate_source_data_fields(
+            shg_mgr=shg_mgr,
+            pmm=pmm)
 
     def initialize_trial(
-            self, src_hypo_group_manager, events, n_events=None,
-            evt_sel_method=None, store_src_ev_idxs=False, tl=None):
+            self,
+            shg_mgr,
+            pmm,
+            events,
+            n_events=None,
+            evt_sel_method=None,
+            tl=None):
         """Initializes the trial data manager for a new trial. It sets the raw
         events, calculates pre-event-selection data fields, performs a possible
         event selection and calculates the static data fields for the left-over
         events.
 
         Parameters
         ----------
-        src_hypo_group_manager : SourceHypoGroupManager instance
+        shg_mgr : instance of SourceHypoGroupManager
             The instance of SourceHypoGroupManager that defines the source
             hypothesis groups.
+        pmm : instance of ParameterModelMapper
+            The instance of ParameterModelMapper, that defines the global
+            parameters and their mapping to local source parameters.
         events : DataFieldRecordArray instance
             The DataFieldRecordArray instance holding the entire raw events.
         n_events : int | None
             The total number of events of the data set this trial data manager
             corresponds to.
             If None, the number of events is taken from the number of events
             present in the ``events`` array.
-        evt_sel_method : EventSelectionMethod | None
+        evt_sel_method : instance of EventSelectionMethod | None
             The optional event selection method that should be used to select
             potential signal events.
-        store_src_ev_idxs : bool
-            If the evt_sel_method is not None, it determines if source and
-            event indices of the selected events should get calculated and
-            stored.
-        tl : TimeLord | None
+        tl : instance of TimeLord | None
             The optional TimeLord instance that should be used for timing
             measurements.
         """
         # Set the events property, so that the calculation functions of the data
         # fields can access them.
         self.events = events
-        self._src_ev_idxs = None
+        self._src_evt_idxs = None
 
-        if(n_events is None):
+        # Save the number of sources.
+        self._n_sources = shg_mgr.n_sources
+
+        if n_events is None:
             n_events = len(self._events)
         self.n_events = n_events
 
         # Calculate pre-event-selection data fields that are required by the
         # event selection method.
-        self.calculate_pre_evt_sel_static_data_fields(src_hypo_group_manager)
+        self.calculate_pre_evt_sel_static_data_fields(
+            shg_mgr=shg_mgr,
+            pmm=pmm)
 
-        if(evt_sel_method is not None):
+        if evt_sel_method is not None:
             logger.debug(
                 f'Performing event selection method '
                 f'"{classname(evt_sel_method)}".')
-            (selected_events, src_ev_idxs) = evt_sel_method.select_events(
-                self._events, tl=tl, ret_src_ev_idxs=store_src_ev_idxs)
+            (selected_events, src_evt_idxs) = evt_sel_method.select_events(
+                events=self._events,
+                tl=tl)
             logger.debug(
                 f'Selected {len(selected_events)} out of {len(self._events)} '
                 'events.')
             self.events = selected_events
-            self._src_ev_idxs = src_ev_idxs
+            self._src_evt_idxs = src_evt_idxs
 
         # Sort the events by the index field, if a field was provided.
-        if(self._index_field_name is not None):
+        if self._index_field_name is not None:
             logger.debug(
                 f'Sorting events in index field "{self._index_field_name}"')
             sorted_idxs = self._events.sort_by_field(self._index_field_name)
             # If event indices are stored, we need to re-assign also those event
             # indices according to the new order.
-            if self._src_ev_idxs is not None:
-                self._src_ev_idxs[1] = sorted_idxs[self._src_ev_idxs[1]]
+            if self._src_evt_idxs is not None:
+                self._src_evt_idxs[1] = np.take(
+                    sorted_idxs, self._src_evt_idxs[1])
+
+        # Create the src_evt_idxs property data in case it was not provided by
+        # the event selection. In that case all events are selected for all
+        # sources. This simplifies the implementations of the PDFs.
+        if self._src_evt_idxs is None:
+            self._src_evt_idxs = (
+                np.repeat(np.arange(self.n_sources), self.n_selected_events),
+                np.tile(np.arange(self.n_selected_events), self.n_sources)
+            )
 
         # Now calculate all the static data fields. This will increment the
         # trial data state ID.
-        self.calculate_static_data_fields(src_hypo_group_manager)
+        self.calculate_static_data_fields(
+            shg_mgr=shg_mgr,
+            pmm=pmm)
+
+    def get_n_values(self):
+        """Returns the expected size of the values array after a PDF
+        evaluation, which will include PDF values for all trial data events and
+        all sources.
+
+        Returns
+        -------
+        n : int
+            The length of the expected values array after a PDF evaluation.
+        """
+        return len(self._src_evt_idxs[0])
+
+    def get_values_mask_for_source_mask(self, src_mask):
+        """Creates a boolean mask for the values array where entries belonging
+        to the sources given by the source mask are selected.
+
+        Parameters
+        ----------
+        src_mask : instance of numpy ndarray
+            The (N_sources,)-shaped numpy ndarray holding the boolean selection
+            of the sources.
+
+        Returns
+        -------
+        values_mask : instance of numpy ndarray
+            The (N_values,)-shaped numpy ndarray holding the boolean selection
+            of the values.
+        """
+        tdm_src_idxs = self.src_evt_idxs[0]
+        src_idxs = np.arange(self.n_sources)[src_mask]
 
-    def add_source_data_field(self, name, func, dt=None):
+        values_mask = np.zeros((self.get_n_values(),), dtype=np.bool_)
+
+        def make_values_mask(src_idx):
+            global values_mask
+            values_mask |= tdm_src_idxs == src_idx
+
+        np.vectorize(make_values_mask)(src_idxs)
+
+        return values_mask
+
+    def add_source_data_field(
+            self,
+            name,
+            func,
+            dt=None):
         """Adds a new data field to the manager. The data field must depend
         solely on source parameters.
 
         Parameters
         ----------
         name : str
             The name of the data field. It serves as the identifier for the
             data field.
         func : callable
             The function that calculates the data field values. The call
             signature must be
-            `__call__(tdm, src_hypo_group_manager, fitparams)`, where
-            `tdm` is the TrialDataManager instance holding the event data,
-            `src_hypo_group_manager` is the SourceHypoGroupManager instance,
-            and `fitparams` is an unused interface argument.
+
+                __call__(tdm, shg_mgr, pmm)
+
+            where ``tdm`` is the TrialDataManager instance holding the event
+            data, ``shg_mgr`` is the instance of SourceHypoGroupManager,
+            and ``pmm`` is the instance of ParameterModelMapper.
         dt : numpy dtype | str | None
             If specified it defines the data type this data field should have.
             If a str instance is given, it defines the name of the data field
             whose data type should be taken for the data field.
         """
-        if(name in self):
+        if name in self:
             raise KeyError(
-                'The data field "%s" is already defined!'%(name))
+                f'The data field "{name}" is already defined!')
 
-        data_field = DataField(name, func, dt=dt)
+        data_field = DataField(
+            name=name,
+            func=func,
+            dt=dt,
+            is_src_field=True)
 
         self._source_data_fields_dict[name] = data_field
 
     def add_data_field(
-            self, name, func, fitparam_names=None, dt=None, pre_evt_sel=False):
+            self,
+            name,
+            func,
+            global_fitparam_names=None,
+            dt=None,
+            pre_evt_sel=False,
+            is_srcevt_data=False):
         """Adds a new data field to the manager.
 
         Parameters
         ----------
         name : str
             The name of the data field. It serves as the identifier for the
             data field.
         func : callable
             The function that calculates the data field values. The call
             signature must be
-            `__call__(tdm, src_hypo_group_manager, fitparams)`, where
-            `tdm` is the TrialDataManager instance holding the event data,
-            `src_hypo_group_manager` is the SourceHypoGroupManager instance,
-            and `fitparams` is the dictionary with the current fit parameter
-            names and values.
-        fitparam_names : sequence of str | None
-            The sequence of str instances specifying the names of the fit
-            parameters this data field depends on. If set to None, it means that
-            the data field does not depend on any fit parameters.
+
+                __call__(tdm, shg_mgr, pmm, global_fitparams_dict=None)
+
+            where ``tdm`` is the TrialDataManager instance holding the trial
+            event data, ``shg_mgr`` is the instance of SourceHypoGroupManager,
+            ``pmm`` is the instance of ParameterModelMapper, and
+            ``global_fitparams_dict`` is the dictionary with the current global
+            fit parameter names and values.
+            The shape of the returned array must be (N_selected_events,).
+        global_fitparam_names : str | sequence of str | None
+            The sequence of str instances specifying the names of the global fit
+            parameters this data field depends on. If set to ``None``, it means
+            that the data field does not depend on any fit parameters.
         dt : numpy dtype | str | None
             If specified it defines the data type this data field should have.
             If a str instance is given, it defines the name of the data field
             whose data type should be taken for the data field.
         pre_evt_sel : bool
             Flag if this data field should get calculated before potential
             signal events get selected (True), or afterwards (False).
             Default is False.
+        is_srcevt_data : bool
+            Flag if this data field contains source-event data, hence the length
+            of the data array will be N_values.
+            Default is False.
         """
-        if(name in self):
+        if name in self:
             raise KeyError(
-                'The data field "%s" is already defined!'%(name))
-
-        if(pre_evt_sel and (fitparam_names is not None)):
-            raise ValueError(
-                f'The pre-event-selection data field "{name}" must not depend '
-                 'on fit parameters!')
+                'The data field "{name}" is already defined!')
 
-        data_field = DataField(name, func, fitparam_names, dt=dt)
+        if pre_evt_sel:
+            if global_fitparam_names is not None:
+                raise ValueError(
+                    f'The pre-event-selection data field "{name}" must not '
+                    'depend on global fit parameters!')
+
+            if is_srcevt_data:
+                raise ValueError(
+                    'By definition the pre-event-selection data field '
+                    f'"{name}" cannot hold source-event data! The '
+                    'is_srcevt_data argument must be set to False!')
+
+        data_field = DataField(
+            name=name,
+            func=func,
+            global_fitparam_names=global_fitparam_names,
+            dt=dt,
+            is_src_field=False,
+            is_srcevt_data=is_srcevt_data)
 
-        if(pre_evt_sel):
+        if pre_evt_sel:
             self._pre_evt_sel_static_data_fields_dict[name] = data_field
-        elif(fitparam_names is None):
+        elif global_fitparam_names is None:
             self._static_data_fields_dict[name] = data_field
         else:
-            self._fitparam_data_fields_dict[name] = data_field
+            self._global_fitparam_data_fields_dict[name] = data_field
 
-    def calculate_source_data_fields(self, src_hypo_group_manager):
+    def calculate_source_data_fields(
+            self,
+            shg_mgr,
+            pmm):
         """Calculates the data values of the data fields that solely depend on
         source parameters.
 
         Parameters
         ----------
-        src_hypo_group_manager : instance of SourceHypoGroupManager
+        shg_mgr : instance of SourceHypoGroupManager
             The instance of SourceHypoGroupManager, which defines the groups of
             source hypotheses.
+        pmm : instance of ParameterModelMapper
+            The instance of ParameterModelMapper, that defines the global
+            parameters and their mapping to local source parameters.
         """
-        if(len(self._source_data_fields_dict) == 0):
+        if len(self._source_data_fields_dict) == 0:
             return
 
-        fitparams = None
         for (name, dfield) in self._source_data_fields_dict.items():
-            dfield.calculate(self, src_hypo_group_manager, fitparams)
+            dfield.calculate(
+                tdm=self,
+                shg_mgr=shg_mgr,
+                pmm=pmm)
 
         self._trial_data_state_id += 1
 
-    def calculate_pre_evt_sel_static_data_fields(self, src_hypo_group_manager):
+    def calculate_pre_evt_sel_static_data_fields(
+            self,
+            shg_mgr,
+            pmm):
         """Calculates the data values of the data fields that should be
         available for the event selection method and do not depend on any fit
         parameters.
 
         Parameters
         ----------
-        src_hypo_group_manager : instance of SourceHypoGroupManager
+        shg_mgr : instance of SourceHypoGroupManager
             The instance of SourceHypoGroupManager, which defines the groups of
             source hypotheses.
+        pmm : instance of ParameterModelMapper
+            The instance of ParameterModelMapper, that defines the global
+            parameters and their mapping to local source parameters.
         """
-        if(len(self._pre_evt_sel_static_data_fields_dict) == 0):
+        if len(self._pre_evt_sel_static_data_fields_dict) == 0:
             return
 
-        fitparams = dict()
         for (name, dfield) in self._pre_evt_sel_static_data_fields_dict.items():
-            dfield.calculate(self, src_hypo_group_manager, fitparams)
+            dfield.calculate(
+                tdm=self,
+                shg_mgr=shg_mgr,
+                pmm=pmm)
 
         self._trial_data_state_id += 1
 
-    def calculate_static_data_fields(self, src_hypo_group_manager):
+    def calculate_static_data_fields(
+            self,
+            shg_mgr,
+            pmm):
         """Calculates the data values of the data fields that do not depend on
         any source or fit parameters.
 
         Parameters
         ----------
-        src_hypo_group_manager : instance of SourceHypoGroupManager
+        shg_mgr : instance of SourceHypoGroupManager
             The instance of SourceHypoGroupManager, which defines the groups of
             source hypotheses.
+        pmm : instance of ParameterModelMapper
+            The instance of ParameterModelMapper, that defines the global
+            parameters and their mapping to local source parameters.
         """
-        if(len(self._static_data_fields_dict) == 0):
+        if len(self._static_data_fields_dict) == 0:
             return
 
-        fitparams = dict()
         for (name, dfield) in self._static_data_fields_dict.items():
-            dfield.calculate(self, src_hypo_group_manager, fitparams)
+            dfield.calculate(
+                tdm=self,
+                shg_mgr=shg_mgr,
+                pmm=pmm)
 
         self._trial_data_state_id += 1
 
-    def calculate_fitparam_data_fields(self, src_hypo_group_manager, fitparams):
-        """Calculates the data values of the data fields that depend on fit
-        parameter values.
+    def calculate_global_fitparam_data_fields(
+            self,
+            shg_mgr,
+            pmm,
+            global_fitparams_dict):
+        """Calculates the data values of the data fields that depend on global
+        fit parameter values.
 
         Parameters
         ----------
-        src_hypo_group_manager : instance of SourceHypoGroupManager
+        shg_mgr : instance of SourceHypoGroupManager
             The instance of SourceHypoGroupManager, which defines the groups of
             source hypotheses.
-        fitparams : dict
-            The dictionary holding the fit parameter names and values.
+        pmm : instance of ParameterModelMapper
+            The instance of ParameterModelMapper, that defines the global
+            parameters and their mapping to local source parameters.
+        global_fitparams_dict : dict
+            The dictionary holding the current global fit parameter names and
+            values.
         """
-        if(len(self._fitparam_data_fields_dict) == 0):
+        if len(self._global_fitparam_data_fields_dict) == 0:
             return
 
-        for (name, dfield) in self._fitparam_data_fields_dict.items():
-            dfield.calculate(self, src_hypo_group_manager, fitparams)
+        for (name, dfield) in self._global_fitparam_data_fields_dict.items():
+            dfield.calculate(
+                tdm=self,
+                shg_mgr=shg_mgr,
+                pmm=pmm,
+                global_fitparams_dict=global_fitparams_dict)
 
         self._trial_data_state_id += 1
 
     def get_data(self, name):
         """Gets the data for the given data field name. The data is stored
-        either in the raw events record ndarray or in one of the additional
-        defined data fields. Data from the raw events record ndarray is
-        prefered.
+        either in the raw events DataFieldRecordArray or in one of the
+        additional defined data fields. Data from the raw events
+        DataFieldRecordArray is prefered.
 
         Parameters
         ----------
         name : str
             The name of the data field for which to retrieve the data.
 
         Returns
         -------
-        data : numpy ndarray
-            The data of the requested data field.
+        data : instance of numpy ndarray
+            The numpy ndarray holding the data of the requested data field.
+            The length of the array is either N_sources, N_selected_events, or
+            N_values.
 
         Raises
         ------
         KeyError
             If the given data field is not defined.
         """
-        if((self._events is not None) and
-           (name in self._events.field_name_list)):
+        # Data fields which are static or depend on global fit parameters are
+        # stored within the _events DataFieldRecordArray if they do not contain
+        # source-event data. For all other cases, the data is stored in the
+        # .values attribute of the DataField class instance.
+        if self._events is not None and\
+           name in self._events.field_name_list:
             return self._events[name]
 
-        if(name in self._source_data_fields_dict):
-            data = self._source_data_fields_dict[name].values
+        if name in self._source_data_fields_dict:
+            return self._source_data_fields_dict[name].values
 
-            # Broadcast the value of an one-element 1D ndarray to the length
-            # of the number of events. Note: Make sure that we don't broadcast
-            # recarrays.
-            if(self._events is not None):
-                if((len(data) == 1) and (data.ndim == 1) and
-                   (data.dtype.fields is None)):
-                    data = np.repeat(data, len(self._events))
-        else:
-            raise KeyError(
-                f'The data field "{name}" is not defined!')
+        if name in self._static_data_fields_dict:
+            return self._static_data_fields_dict[name].values
 
-        return data
+        if name in self._global_fitparam_data_fields_dict:
+            return self._global_fitparam_data_fields_dict[name].values
+
+        raise KeyError(
+            f'The data field "{name}" is not defined!')
 
     def get_dtype(self, name):
         """Gets the data type of the given data field.
 
         Parameters
         ----------
         name : str
@@ -797,7 +1160,71 @@
         ------
         KeyError
             If the given data field is not defined.
         """
         dt = self.get_data(name).dtype
 
         return dt
+
+    def is_event_data_field(self, name):
+        """Checks if the given data field is an events data field, i.e. its
+        length is N_selected_events.
+
+        Parameters
+        ----------
+        name : str
+            The name of the data field.
+
+        Returns
+        -------
+        check : bool
+            ``True`` if the given data field contains event data, ``False``
+            otherwise.
+        """
+        if self._events is not None and\
+           name in self._events.field_name_list:
+            return True
+
+        return False
+
+    def is_source_data_field(self, name):
+        """Checks if the given data field is a source data field, i.e. its
+        length is N_sources.
+
+        Parameters
+        ----------
+        name : str
+            The name of the data field.
+
+        Returns
+        -------
+        check : bool
+            ``True`` if the given data field contains source data, ``False``
+            otherwise.
+        """
+        if name in self._source_data_fields_dict:
+            return True
+
+        return False
+
+    def is_srcevt_data_field(self, name):
+        """Checks if the given data field is a source-event data field, i.e. its
+        length is N_values.
+
+        Parameters
+        ----------
+        name : str
+            The name of the data field.
+
+        Returns
+        -------
+        check : bool
+            ``True`` if the given data field contains source-event data,
+            ``False`` otherwise.
+        """
+        if name in self._static_data_fields_dict:
+            return self._static_data_fields_dict[name].is_srcevt_data
+
+        if name in self._global_fitparam_data_fields_dict:
+            return self._global_fitparam_data_fields_dict[name].is_srcevt_data
+
+        return False
```

### Comparing `skyllh-23.1.1/skyllh/core/utils/multidimgridpdf.py` & `skyllh-23.2.0/skyllh/core/utils/multidimgridpdf.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,214 +1,275 @@
 # -*- coding: utf-8 -*-
 
 """This module contains utility functions for creating and managing
 MultiDimGridPDF instances.
 """
 
 import numpy as np
-import os
 
-from skyllh.core.binning import BinningDefinition
-from skyllh.core.pdf import MultiDimGridPDF
-from skyllh.core.signalpdf import SignalMultiDimGridPDF
-from skyllh.core.backgroundpdf import BackgroundMultiDimGridPDF
+from skyllh.core.binning import (
+    BinningDefinition,
+)
+from skyllh.core.pdf import (
+    MultiDimGridPDF,
+)
+from skyllh.core.py import (
+    classname,
+)
 
 
-def kde_pdf_sig_spatial_norm_factor_func(pdf, tdm, fitparams, eventdata):
-    """This is the standard normalization factor function for the spatial signal
-    MultiDimGridPDF, which is created from KDE PDF values.
+def get_kde_pdf_sig_spatial_norm_factor_func(
+        log10_psi_name='log10_psi'):
+    """Returns the standard normalization factor function for the spatial
+    signal MultiDimGridPDF, which is created from KDE PDF values.
     It can be used for the ``norm_factor_func`` argument of the
+    ``create_MultiDimGridPDF_from_photosplinetable`` and
     ``create_MultiDimGridPDF_from_kde_pdf`` function.
+
+    Parameters
+    ----------
+    log10_psi_name : str
+        The name of the event data field for the log10(psi) values.
     """
-    log10_psi_idx = pdf._axes.axis_name_list.index('log10_psi')
-    # psi = tdm.get_data('psi')
-    # Convert to psi.
-    psi = 10**eventdata[:, log10_psi_idx]
-    norm = 1. / (2 * np.pi * np.log(10) * psi * np.sin(psi))
-    return norm
+    def kde_pdf_sig_spatial_norm_factor_func(
+            pdf,
+            tdm,
+            params_recarray,
+            eventdata,
+            evt_mask=None):
+
+        log10_psi_idx = pdf._axes.get_index_by_name(log10_psi_name)
+
+        if evt_mask is None:
+            psi = 10**eventdata[:, log10_psi_idx]
+        else:
+            psi = 10**eventdata[:, log10_psi_idx][evt_mask]
+
+        norm = 1. / (2 * np.pi * np.log(10) * psi * np.sin(psi))
+
+        return norm
 
+    return kde_pdf_sig_spatial_norm_factor_func
 
-def kde_pdf_bkg_norm_factor_func(pdf, tdm, fitparams, eventdata):
-    """This is the standard normalization factor function for the background
+
+def get_kde_pdf_bkg_norm_factor_func():
+    """Returns the standard normalization factor function for the background
     MultiDimGridPDF, which is created from KDE PDF values.
     It can be used for the ``norm_factor_func`` argument of the
+    ``create_MultiDimGridPDF_from_photosplinetable`` and
     ``create_MultiDimGridPDF_from_kde_pdf`` function.
     """
-    return 1. / (2 * np.pi)
+    def kde_pdf_bkg_norm_factor_func(
+            pdf,
+            tdm,
+            params_recarray,
+            eventdata,
+            evt_mask=None):
+
+        return 1. / (2 * np.pi)
+
+    return kde_pdf_bkg_norm_factor_func
 
 
 def create_MultiDimGridPDF_from_photosplinetable(
-        ds, data, info_key, splinetable_key, norm_factor_func=None,
-        kind=None, tl=None):
+        multidimgridpdf_cls,
+        pmm,
+        ds,
+        data,
+        info_key,
+        splinetable_key,
+        kde_pdf_axis_name_map_key='KDE_PDF_axis_name_map',
+        norm_factor_func=None,
+        cache_pd_values=False,
+        tl=None,
+        **kwargs,
+):
     """
-    Creates a MultiDimGridPDF instance with pdf values taken from photospline pdf,
-    a spline interpolation of KDE PDF values stored in a splinetable on disk.
+    Creates a MultiDimGridPDF instance with pdf values taken from a photospline
+    pdf, i.e. a spline interpolation of KDE PDF values stored in a splinetable
+    on disk.
 
     Parameters
     ----------
-    ds : Dataset instance
-        The Dataset instance the PDF applies to.
-    data : DatasetData instance
-        The DatasetData instance that holds the auxiliary data of the data set.
+    multidimgridpdf_cls : subclass of MultiDimGridPDF
+        The MultiDimGridPDF class, which should be used.
+    pmm : instance of ParameterModelMapper
+        The instance of ParameterModelMapper, which defines the mapping of
+        global parameters to local model parameters.
+    ds : instance of Dataset
+        The instance of Dataset the PDF applies to.
+    data : instance of DatasetData
+        The instance of DatasetData that holds the experimental and monte-carlo
+        data of the dataset.
     info_key : str
         The auxiliary data name for the file containing PDF information.
     splinetable_key : str
-        The auxiliary data name for the name of the splinetablefile.
+        The auxiliary data name for the name of the file containing the
+        photospline spline table.
+    kde_pdf_axis_name_map_key : str
+        The auxiliary data name for the KDE PDF axis name map.
     norm_factor_func : callable | None
-        The normalization factor function. It must have the following call
-        signature:
-            __call__(pdf, tdm, fitparams)
-    kind : str | None
-        The kind of PDF to create. This is either ``'sig'`` for a
-        SignalMultiDimGridPDF or ``'bkg'`` for a BackgroundMultiDimGridPDF
-        instance. If set to None, a MultiDimGridPDF instance is created.
-    tl : TimeLord instance | None
-        The optional TimeLord instance to use for measuring timing information.
+        The function that calculates a possible required normalization
+        factor for the PDF value based on the event properties.
+        For more information about this argument see the documentation of the
+        :meth:`skyllh.core.pdf.MultiDimGridPDF.__init__` method.
+    cache_pd_values : bool
+        Flag if the probability density values should get cached by the
+        MultiDimGridPDF class.
+    tl : instance of TimeLord | None
+        The optional instance of TimeLord to use for measuring timing
+        information.
 
     Returns
     -------
-    pdf : SignalMultiDimGridPDF instance | BackgroundMultiDimGridPDF instance |
-          MultiDimGridPDF instance
-        The created PDF instance. Depending on the ``kind`` argument, this is
-        a SignalMultiDimGridPDF, a BackgroundMultiDimGridPDF, or a
-        MultiDimGridPDF instance.
+    pdf : instance of ``multidimgridpdf_cls``
+        The created PDF instance of MultiDimGridPDF.
     """
-
-    if(kind is None):
-        pdf_type = MultiDimGridPDF
-    elif(kind == 'sig'):
-        pdf_type = SignalMultiDimGridPDF
-    elif(kind == 'bkg'):
-        pdf_type = BackgroundMultiDimGridPDF
-    else:
-        raise ValueError('The kind argument must be None, "sig", or "bkg"! '
-                         'Currently it is '+str(kind)+'!')
+    if not issubclass(multidimgridpdf_cls, MultiDimGridPDF):
+        raise TypeError(
+            'The multidimgridpdf_cls argument must be a subclass of '
+            'MultiDimGridPDF! '
+            f'Its current type is {classname(multidimgridpdf_cls)}.')
 
     # Load the PDF data from the auxilary files.
     num_dict = ds.load_aux_data(info_key, tl=tl)
 
-    kde_pdf_axis_name_map = ds.load_aux_data('KDE_PDF_axis_name_map', tl=tl)
+    kde_pdf_axis_name_map = ds.load_aux_data(kde_pdf_axis_name_map_key, tl=tl)
     kde_pdf_axis_name_map_inv = dict(
         [(v, k) for (k, v) in kde_pdf_axis_name_map.items()])
     for var in num_dict['vars']:
-        if(var not in kde_pdf_axis_name_map_inv):
+        if var not in kde_pdf_axis_name_map_inv:
             kde_pdf_axis_name_map_inv[var] = var
 
     if 'bin_centers' in num_dict:
         bin_centers_key = 'bin_centers'
     elif 'bins' in num_dict:
         bin_centers_key = 'bins'
     else:
         raise KeyError(
-            "The PDF information file is missing 'bin_centers' or 'bins' key.")
+            'The PDF information file is missing "bin_centers" or "bins" key!')
 
     axis_binnings = [
         BinningDefinition(
-            kde_pdf_axis_name_map_inv[var], num_dict[bin_centers_key][idx])
+            name=kde_pdf_axis_name_map_inv[var],
+            binedges=num_dict[bin_centers_key][idx])
         for (idx, var) in enumerate(num_dict['vars'])
     ]
 
     # Getting the name of the splinetable file
-    splinetable_file_list = ds.get_aux_data_definition(splinetable_key)
-    # This is a list with only one element.
-    splinetable_file = os.path.join(ds.root_dir, splinetable_file_list[0])
+    splinetable_file = ds.get_abs_pathfilename_list(
+        ds.get_aux_data_definition(splinetable_key))[0]
 
-    pdf = pdf_type(
-        axis_binnings,
+    pdf = multidimgridpdf_cls(
+        pmm=pmm,
+        axis_binnings=axis_binnings,
         path_to_pdf_splinetable=splinetable_file,
-        pdf_grid_data=None,
-        norm_factor_func=norm_factor_func)
+        norm_factor_func=norm_factor_func,
+        cache_pd_values=cache_pd_values,
+        **kwargs)
 
     return pdf
 
 
-def create_MultiDimGridPDF_from_kde_pdf(
-        ds, data, numerator_key, denumerator_key=None, norm_factor_func=None,
-        kind=None, tl=None):
+def create_MultiDimGridPDF_from_kde_pdf(  # noqa: C901
+        multidimgridpdf_cls,
+        pmm,
+        ds,
+        data,
+        numerator_key,
+        denumerator_key=None,
+        kde_pdf_axis_name_map_key='KDE_PDF_axis_name_map',
+        norm_factor_func=None,
+        cache_pd_values=False,
+        tl=None,
+        **kwargs,
+):
     """Creates a MultiDimGridPDF instance with pdf values taken from KDE PDF
     values stored in the dataset's auxiliary data.
 
     Parameters
     ----------
-    ds : Dataset instance
-        The Dataset instance the PDF applies to.
-    data : DatasetData instance
-        The DatasetData instance that holds the auxiliary data of the data set.
+    multidimgridpdf_cls : subclass of MultiDimGridPDF
+        The MultiDimGridPDF class, which should be used.
+    pmm : instance of ParameterModelMapper
+        The instance of ParameterModelMapper, which defines the mapping of
+        global parameters to local model parameters.
+    ds : instance of Dataset
+        The instance of Dataset the PDF applies to.
+    data : instance of DatasetData
+        The instance of DatasetData that holds the auxiliary data of the
+        dataset.
     numerator_key : str
         The auxiliary data name for the PDF numerator array.
     denumerator_key : str | None
         The auxiliary data name for the PDF denumerator array.
-        This can be None, if no denumerator array is required.
+        This can be ``None``, if no denumerator array is required.
+    kde_pdf_axis_name_map_key : str
+        The auxiliary data name for the KDE PDF axis name map.
     norm_factor_func : callable | None
-        The normalization factor function. It must have the following call
-        signature:
-            __call__(pdf, tdm, fitparams)
-    kind : str | None
-        The kind of PDF to create. This is either ``'sig'`` for a
-        SignalMultiDimGridPDF or ``'bkg'`` for a BackgroundMultiDimGridPDF
-        instance. If set to None, a MultiDimGridPDF instance is created.
-    tl : TimeLord instance | None
-        The optional TimeLord instance to use for measuring timing information.
+        The function that calculates a possible required normalization
+        factor for the PDF value based on the event properties.
+        For more information about this argument see the documentation of the
+        :meth:`skyllh.core.pdf.MultiDimGridPDF.__init__` method.
+    cache_pd_values : bool
+        Flag if the probability density values should get cached by the
+        MultiDimGridPDF class.
+    tl : instance of TimeLord | None
+        The optional instance of TimeLord to use for measuring timing
+        information.
 
     Returns
     -------
-    pdf : SignalMultiDimGridPDF instance | BackgroundMultiDimGridPDF instance |
-          MultiDimGridPDF instance
-        The created PDF instance. Depending on the ``kind`` argument, this is
-        a SignalMultiDimGridPDF, a BackgroundMultiDimGridPDF, or a
-        MultiDimGridPDF instance.
+    pdf : instance of ``multidimgridpdf_cls``
+        The created PDF instance of MultiDimGridPDF.
     """
-    if(kind is None):
-        pdf_type = MultiDimGridPDF
-    elif(kind == 'sig'):
-        pdf_type = SignalMultiDimGridPDF
-    elif(kind == 'bkg'):
-        pdf_type = BackgroundMultiDimGridPDF
-    else:
-        raise ValueError('The kind argument must be None, "sig", or "bkg"! '
-                         'Currently it is '+str(kind)+'!')
+    if not issubclass(multidimgridpdf_cls, MultiDimGridPDF):
+        raise TypeError(
+            'The multidimgridpdf_cls argument must be a subclass of '
+            'MultiDimGridPDF! '
+            f'Its current type is {classname(multidimgridpdf_cls)}.')
 
     # Load the PDF data from the auxilary files.
     num_dict = ds.load_aux_data(numerator_key, tl=tl)
 
     denum_dict = None
-    if(denumerator_key is not None):
+    if denumerator_key is not None:
         denum_dict = ds.load_aux_data(denumerator_key, tl=tl)
 
-    kde_pdf_axis_name_map = ds.load_aux_data('KDE_PDF_axis_name_map', tl=tl)
+    kde_pdf_axis_name_map = ds.load_aux_data(kde_pdf_axis_name_map_key, tl=tl)
     kde_pdf_axis_name_map_inv = dict(
         [(v, k) for (k, v) in kde_pdf_axis_name_map.items()])
     for var in num_dict['vars']:
-        if(var not in kde_pdf_axis_name_map_inv):
+        if var not in kde_pdf_axis_name_map_inv:
             kde_pdf_axis_name_map_inv[var] = var
 
     if 'bin_centers' in num_dict:
         bin_centers_key = 'bin_centers'
     elif 'bins' in num_dict:
         bin_centers_key = 'bins'
     else:
         raise KeyError(
-            "The PDF information file is missing 'bin_centers' or 'bins' key.")
+            'The PDF information file is missing "bin_centers" or "bins" key!')
 
     axis_binnings = [
         BinningDefinition(
             kde_pdf_axis_name_map_inv[var], num_dict[bin_centers_key][idx])
         for (idx, var) in enumerate(num_dict['vars'])
     ]
 
     vals = num_dict['pdf_vals']
-    if(denum_dict is not None):
+    if denum_dict is not None:
         # A denumerator is required, so we need to divide the numerator pdf
         # values by the denumerator pdf values, by preserving the correct axis
         # order.
         # Construct the slicing selector for the denumerator pdf values array to
         # match the axis order of the numerator pdf values array.
         selector = []
         for var in num_dict['vars']:
-            if(var in denum_dict['vars']):
+            if var in denum_dict['vars']:
                 # The variable is present in both pdf value arrays. So select
                 # all values of that dimension.
                 selector.append(slice(None, None))
             else:
                 # The variable is not present in the normalization pdf value
                 # array, so we need to add a dimension for that variable.
                 selector.append(np.newaxis)
@@ -220,14 +281,16 @@
         vals = out
 
     # Set infinite values to NaN.
     vals[np.isinf(vals)] = np.nan
     # Set NaN values to 0.
     vals[np.isnan(vals)] = 0
 
-    pdf = pdf_type(
-        axis_binnings,
-        path_to_pdf_splinetable=None,
+    pdf = multidimgridpdf_cls(
+        pmm=pmm,
+        axis_binnings=axis_binnings,
         pdf_grid_data=vals,
-        norm_factor_func=norm_factor_func)
+        norm_factor_func=norm_factor_func,
+        cache_pd_values=cache_pd_values,
+        **kwargs)
 
     return pdf
```

### Comparing `skyllh-23.1.1/skyllh/core/utils/trials.py` & `skyllh-23.2.0/skyllh/core/utils/trials.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,29 +1,31 @@
 # -*- coding: utf-8 -*-
 
 """This module contains utility functions related analysis trials.
 """
 
-import numpy as np
 import pickle
 
-from skyllh.core.timing import TaskTimer
+from skyllh.core.timing import (
+    TaskTimer,
+)
 
 
 def create_pseudo_data_file(
         ana,
         rss,
         filename,
         mean_n_bkg_list=None,
         mean_n_sig=0,
         bkg_kwargs=None,
         sig_kwargs=None,
         tl=None
-    ):
-    """Creates a pickle file that contains the pseudo data for a single trial.
+):
+    """Creates a pickle file that contains the pseudo data for a single trial
+    by generating background and signal events.
 
     Parameters
     ----------
     ana : Analysis
         The Analysis instance that should be used to generate the pseudo data.
     rss : RandomStateService
         The RandomStateService instance to use for generating random numbers.
@@ -47,37 +49,37 @@
         of the `SignalGenerator` class. An usual keyword argument is
         `poisson`.
     tl : TimeLord | None
         The instance of TimeLord that should be used to time individual tasks.
 
     """
     (n_bkg_events_list, bkg_events_list) = ana.generate_background_events(
-        rss = rss,
-        mean_n_bkg_list = mean_n_bkg_list,
-        bkg_kwargs = bkg_kwargs,
-        tl = tl
+        rss=rss,
+        mean_n_bkg_list=mean_n_bkg_list,
+        bkg_kwargs=bkg_kwargs,
+        tl=tl
     )
 
     (n_sig, n_sig_events_list, sig_events_list) = ana.generate_signal_events(
-        rss = rss,
-        mean_n_sig = mean_n_sig,
-        sig_kwargs = sig_kwargs,
-        tl = tl
+        rss=rss,
+        mean_n_sig=mean_n_sig,
+        sig_kwargs=sig_kwargs,
+        tl=tl
     )
 
     trial_data = dict(
-        mean_n_bkg_list = mean_n_bkg_list,
-        mean_n_sig = mean_n_sig,
-        bkg_kwargs = bkg_kwargs,
-        sig_kwargs = sig_kwargs,
-        n_sig = n_sig,
-        n_bkg_events_list = n_bkg_events_list,
-        n_sig_events_list = n_sig_events_list,
-        bkg_events_list = bkg_events_list,
-        sig_events_list = sig_events_list
+        mean_n_bkg_list=mean_n_bkg_list,
+        mean_n_sig=mean_n_sig,
+        bkg_kwargs=bkg_kwargs,
+        sig_kwargs=sig_kwargs,
+        n_sig=n_sig,
+        n_bkg_events_list=n_bkg_events_list,
+        n_sig_events_list=n_sig_events_list,
+        bkg_events_list=bkg_events_list,
+        sig_events_list=sig_events_list
     )
 
     with TaskTimer(tl, 'Writing pseudo data to file.'):
         with open(filename, 'wb') as fp:
             pickle.dump(trial_data, fp)
 
 
@@ -102,15 +104,15 @@
         The total number of background events for each data set of the
         pseudo data.
     n_sig_events_list : list of int
         The total number of signal events for each data set of the pseudo data.
     bkg_events_list : list of DataFieldRecordArray instances
         The list of DataFieldRecordArray instances containing the background
         pseudo data events for each data set.
-    sig_events_list : list of DataFieldRecordArray instances or None
+    sig_events_list : list of DataFieldRecordArray instances | None
         The list of DataFieldRecordArray instances containing the signal
         pseudo data events for each data set. If a particular dataset has
         no signal events, the entry for that dataset can be None.
     """
     with TaskTimer(tl, 'Loading pseudo data from file.'):
         with open(filename, 'rb') as fp:
             trial_data = pickle.load(fp)
```

### Comparing `skyllh-23.1.1/skyllh/datasets/i3/PublicData_10y_ps.py` & `skyllh-23.2.0/skyllh/datasets/i3/PublicData_10y_ps.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,27 +1,37 @@
 # -*- coding: utf-8 -*-
 # Author: Dr. Martin Wolf <mail@martin-wolf.org>
 
 import numpy as np
 
-from skyllh.core.dataset import DatasetCollection
-from skyllh.i3.dataset import I3Dataset
+from skyllh.core.dataset import (
+    DatasetCollection,
+)
+from skyllh.i3.dataset import (
+    I3Dataset,
+)
 
 
-def create_dataset_collection(base_path=None, sub_path_fmt=None):
+def create_dataset_collection(
+        cfg,
+        base_path=None,
+        sub_path_fmt=None,
+):
     """Defines the dataset collection for IceCube's 10-year
     point-source public data, which is available at
     http://icecube.wisc.edu/data-releases/20210126_PS-IC40-IC86_VII.zip
 
     Parameters
     ----------
+    cfg : instance of Config
+        The instance of Config holding the local configuration.
     base_path : str | None
         The base path of the data files. The actual path of a data file is
         assumed to be of the structure <base_path>/<sub_path>/<file_name>.
-        If None, use the default path CFG['repository']['base_path'].
+        If None, use the default path ``cfg['repository']['base_path']``.
     sub_path_fmt : str | None
         The sub path format of the data files of the public data sample.
         If None, use the default sub path format
         'icecube_10year_ps'.
 
     Returns
     -------
@@ -35,15 +45,15 @@
     # Define the default sub path format.
     default_sub_path_fmt = 'icecube_10year_ps'
 
     # We create a dataset collection that will hold the individual seasonal
     # public data datasets (all of the same version!).
     dsc = DatasetCollection('Public Data 10-year point-source')
 
-    dsc.description = """
+    dsc.description = r"""
     The events contained in this release correspond to the IceCube's
     time-integrated point source search with 10 years of data [2]. Please refer
     to the description of the sample and known changes in the text at [1].
 
     The data contained in this release of IceCubes point source sample shows
     evidence of a cumulative excess of events from four sources (NGC 1068,
     TXS 0506+056, PKS 1424+240, and GB6 J1542+6129) from a catalogue of 110
@@ -71,17 +81,17 @@
     sample over the old sample, these results do not supercede those in [4].
 
     This release contains data beginning in 2008 (IC40) until the spring of 2018
     (IC86-2017). This release duplicates and supplants previously released data
     from 2012 and earlier. Events from this release cannot be combined with any
     other releases
 
-    -----------------------------------------
+    --------------------------
     # Experimental data events
-    -----------------------------------------
+    --------------------------
     The "events" folder contains the events observed in the 10 year sample of
     IceCube's point source neutrino selection. Each file corresponds to a single
     season of IceCube datataking, including roughly one year of data. For each
     event, reconstructed particle information is included.
 
     - MJD: The MJD time (ut1) of the event interaction given to 1e-8 days,
     corresponding to roughly millisecond precision.
@@ -110,40 +120,40 @@
     scrambling events in time, then using the local coordinates and time to
     calculate new RA and Dec values.
 
     Note that during the preparation of this data release, one duplicated event
     was discovered in the IC86-2015 season. This event has not contributed to
     any significant excesses.
 
-    -----------------------------------------
+    -----------------
     # Detector uptime
-    -----------------------------------------
+    -----------------
     In order to properly account for detector uptime, IceCube maintains
     "good run lists". These contain information about "good runs", periods of
     datataking useful for analysis. Data may be marked unusable for various
     reasons, including major construction or upgrade work, calibration runs, or
     other anomalies. The "uptime" folder contains lists of the good runs for
     each season.
 
     - MJD_start[days], MJD_stop[days]: The start and end times for each good run
 
-    -----------------------------------------
+    -------------------------------
     # Instrument response functions
-    -----------------------------------------
+    -------------------------------
     In order to best model the response of the IceCube detector to a given
     signal, Monte Carlo simulations are produced for each detector
     configuration. Events are sampled from these simulations to model the
     response of point sources from an arbitrary source and spectrum.
 
     We provide several binned responses for the detector in the "irfs" folder
     of this data release.
 
-    ------------------
+    -----------------
     # Effective Areas
-    ------------------
+    -----------------
     The effective area is a property of the detector and selection which, when
     convolved with a flux model, gives the expected rate of events in the
     detector. Here we release the muon neutrino effective areas for each season
     of data.
 
     The effective areas are averaged over bins using simulated muon neutrino
     events ranging from 100 GeV to 100 PeV. Because the response varies widely
@@ -167,17 +177,17 @@
 
     - Dec_nu_min[deg], Dec_nu_max[deg]: The minimum and maximum of the
     declination of the neutrino origin. Again, note that this is the true
     direction of the neutrino and not the reconstructed muon direction.
 
     - A_Eff[cm^2]: The average effective area across a bin.
 
-    ------------------
+    -------------------
     # Smearing Matrices
-    ------------------
+    -------------------
     IceCube has a nontrivial smearing matrix with correlations between the
     directional uncertainty, the point spread function, and the reconstructed
     muon energy. To provide the most complete set of information, we include
     tables of these responses for each season from IC40 through IC86-2012.
     Seasons after IC86-2012 reuse that season's response functions.
 
     The included smearing matrices take the form of 5D tables mapping a
@@ -217,17 +227,17 @@
     The errors are calibrated so that they provide correct coverage for an
     E^{-2} power law flux. This sample assumes a lower limit on the estimated
     angular uncertainty of 0.2 degrees.
 
     - Fractional_Counts: The fraction of simulated events falling within each
     5D bin relative to all events in the (E_nu, Dec_nu) bin.
 
-    -----------------------------------------
+    ------------
     # References
-    -----------------------------------------
+    ------------
     [1] IceCube Data for Neutrino Point-Source Searches: Years 2008-2018,
         [ArXiv link](https://arxiv.org/abs/2101.09836)
     [2] Time-integrated Neutrino Source Searches with 10 years of IceCube Data,
         Phys. Rev. Lett. 124, 051103 (2020)
     [3] All-sky search for time-integrated neutrino emission from astrophysical
         sources with 7 years of IceCube data,
         Astrophys. J., 835 (2017) no. 2, 151
@@ -235,47 +245,48 @@
         the IceCube-170922A alert,
         Science 361, 147-151 (2018)
     [5] Energy Reconstruction Methods in the IceCube Neutrino Telescope,
         JINST 9 (2014), P03009
     [6] Methods for point source analysis in high energy neutrino telescopes,
         Astropart.Phys.29:299-305,2008
 
-    -----------------------------------------
+    -------------
     # Last Update
-    -----------------------------------------
+    -------------
     28 January 2021
     """
 
     # Define the common keyword arguments for all data sets.
     ds_kwargs = dict(
-        livetime = None,
-        version = version,
-        verqualifiers = verqualifiers,
-        base_path = base_path,
-        default_sub_path_fmt = default_sub_path_fmt,
-        sub_path_fmt = sub_path_fmt
+        cfg=cfg,
+        livetime=None,
+        version=version,
+        verqualifiers=verqualifiers,
+        base_path=base_path,
+        default_sub_path_fmt=default_sub_path_fmt,
+        sub_path_fmt=sub_path_fmt,
     )
 
     grl_field_name_renaming_dict = {
         'MJD_start[days]': 'start',
-        'MJD_stop[days]': 'stop'
+        'MJD_stop[days]': 'stop',
     }
 
     # Define the datasets for the different seasons.
     # For the declination and energy binning we use the same binning as was
     # used in the original point-source analysis using the PointSourceTracks
     # dataset.
 
     # ---------- IC40 ----------------------------------------------------------
     IC40 = I3Dataset(
-        name = 'IC40',
-        exp_pathfilenames = 'events/IC40_exp.csv',
-        mc_pathfilenames = None,
-        grl_pathfilenames = 'uptime/IC40_exp.csv',
-        **ds_kwargs
+        name='IC40',
+        exp_pathfilenames='events/IC40_exp.csv',
+        mc_pathfilenames=None,
+        grl_pathfilenames='uptime/IC40_exp.csv',
+        **ds_kwargs,
     )
     IC40.grl_field_name_renaming_dict = grl_field_name_renaming_dict
     IC40.add_aux_data_definition(
         'eff_area_datafile', 'irfs/IC40_effectiveArea.csv')
     IC40.add_aux_data_definition(
         'smearing_datafile', 'irfs/IC40_smearing.csv')
 
@@ -287,19 +298,19 @@
     IC40.define_binning('sin_dec', sin_dec_bins)
 
     energy_bins = np.arange(2., 9.5 + 0.01, 0.125)
     IC40.define_binning('log_energy', energy_bins)
 
     # ---------- IC59 ----------------------------------------------------------
     IC59 = I3Dataset(
-        name = 'IC59',
-        exp_pathfilenames = 'events/IC59_exp.csv',
-        mc_pathfilenames = None,
-        grl_pathfilenames = 'uptime/IC59_exp.csv',
-        **ds_kwargs
+        name='IC59',
+        exp_pathfilenames='events/IC59_exp.csv',
+        mc_pathfilenames=None,
+        grl_pathfilenames='uptime/IC59_exp.csv',
+        **ds_kwargs,
     )
     IC59.grl_field_name_renaming_dict = grl_field_name_renaming_dict
     IC59.add_aux_data_definition(
         'eff_area_datafile', 'irfs/IC59_effectiveArea.csv')
     IC59.add_aux_data_definition(
         'smearing_datafile', 'irfs/IC59_smearing.csv')
 
@@ -312,69 +323,69 @@
     IC59.define_binning('sin_dec', sin_dec_bins)
 
     energy_bins = np.arange(2., 9.5 + 0.01, 0.125)
     IC59.define_binning('log_energy', energy_bins)
 
     # ---------- IC79 ----------------------------------------------------------
     IC79 = I3Dataset(
-        name = 'IC79',
-        exp_pathfilenames = 'events/IC79_exp.csv',
-        mc_pathfilenames = None,
-        grl_pathfilenames = 'uptime/IC79_exp.csv',
-        **ds_kwargs
+        name='IC79',
+        exp_pathfilenames='events/IC79_exp.csv',
+        mc_pathfilenames=None,
+        grl_pathfilenames='uptime/IC79_exp.csv',
+        **ds_kwargs,
     )
     IC79.grl_field_name_renaming_dict = grl_field_name_renaming_dict
     IC79.add_aux_data_definition(
         'eff_area_datafile', 'irfs/IC79_effectiveArea.csv')
     IC79.add_aux_data_definition(
         'smearing_datafile', 'irfs/IC79_smearing.csv')
 
     sin_dec_bins = np.unique(np.concatenate([
-                        np.linspace(-1., -0.75, 10 + 1),
-                        np.linspace(-0.75, 0., 15 + 1),
-                        np.linspace(0., 1., 20 + 1)
-                        ]))
+        np.linspace(-1., -0.75, 10 + 1),
+        np.linspace(-0.75, 0., 15 + 1),
+        np.linspace(0., 1., 20 + 1),
+    ]))
     IC79.define_binning('sin_dec', sin_dec_bins)
 
     energy_bins = np.arange(2., 9.5 + 0.01, 0.125)
     IC79.define_binning('log_energy', energy_bins)
 
     # ---------- IC86-I --------------------------------------------------------
     IC86_I = I3Dataset(
-        name = 'IC86_I',
-        exp_pathfilenames = 'events/IC86_I_exp.csv',
-        mc_pathfilenames = None,
-        grl_pathfilenames = 'uptime/IC86_I_exp.csv',
-        **ds_kwargs
+        name='IC86_I',
+        exp_pathfilenames='events/IC86_I_exp.csv',
+        mc_pathfilenames=None,
+        grl_pathfilenames='uptime/IC86_I_exp.csv',
+        **ds_kwargs,
     )
     IC86_I.grl_field_name_renaming_dict = grl_field_name_renaming_dict
     IC86_I.add_aux_data_definition(
         'eff_area_datafile', 'irfs/IC86_I_effectiveArea.csv')
     IC86_I.add_aux_data_definition(
         'smearing_datafile', 'irfs/IC86_I_smearing.csv')
 
-    b = np.sin(np.radians(-5.)) # North/South transition boundary.
+    b = np.sin(np.radians(-5.))  # North/South transition boundary.
     sin_dec_bins = np.unique(np.concatenate([
         np.linspace(-1., -0.2, 10 + 1),
         np.linspace(-0.2, b, 4 + 1),
         np.linspace(b, 0.2, 5 + 1),
         np.linspace(0.2, 1., 10),
     ]))
     IC86_I.define_binning('sin_dec', sin_dec_bins)
 
     energy_bins = np.arange(1., 10.5 + 0.01, 0.125)
     IC86_I.define_binning('log_energy', energy_bins)
 
     # ---------- IC86-II -------------------------------------------------------
     IC86_II = I3Dataset(
-        name = 'IC86_II',
-        exp_pathfilenames = 'events/IC86_II_exp.csv',
-        mc_pathfilenames = None,
-        grl_pathfilenames = 'uptime/IC86_II_exp.csv',
-        **ds_kwargs
+        name='IC86_II',
+        exp_pathfilenames='events/IC86_II_exp.csv',
+        mc_pathfilenames=None,
+        grl_pathfilenames='uptime/IC86_II_exp.csv',
+        **ds_kwargs,
     )
     IC86_II.grl_field_name_renaming_dict = grl_field_name_renaming_dict
     IC86_II.add_aux_data_definition(
         'eff_area_datafile', 'irfs/IC86_II_effectiveArea.csv')
     IC86_II.add_aux_data_definition(
         'smearing_datafile', 'irfs/IC86_II_smearing.csv')
 
@@ -387,95 +398,95 @@
     IC86_II.define_binning('sin_dec', sin_dec_bins)
 
     energy_bins = np.arange(0.5, 9.5 + 0.01, 0.125)
     IC86_II.define_binning('log_energy', energy_bins)
 
     # ---------- IC86-III ------------------------------------------------------
     IC86_III = I3Dataset(
-        name = 'IC86_III',
-        exp_pathfilenames = 'events/IC86_III_exp.csv',
-        mc_pathfilenames = None,
-        grl_pathfilenames = 'uptime/IC86_III_exp.csv',
-        **ds_kwargs
+        name='IC86_III',
+        exp_pathfilenames='events/IC86_III_exp.csv',
+        mc_pathfilenames=None,
+        grl_pathfilenames='uptime/IC86_III_exp.csv',
+        **ds_kwargs,
     )
     IC86_III.grl_field_name_renaming_dict = grl_field_name_renaming_dict
     IC86_III.add_aux_data_definition(
         'eff_area_datafile', 'irfs/IC86_II_effectiveArea.csv')
     IC86_III.add_aux_data_definition(
         'smearing_datafile', 'irfs/IC86_II_smearing.csv')
 
     IC86_III.add_binning_definition(
         IC86_II.get_binning_definition('sin_dec'))
     IC86_III.add_binning_definition(
         IC86_II.get_binning_definition('log_energy'))
 
     # ---------- IC86-IV -------------------------------------------------------
     IC86_IV = I3Dataset(
-        name = 'IC86_IV',
-        exp_pathfilenames = 'events/IC86_IV_exp.csv',
-        mc_pathfilenames = None,
-        grl_pathfilenames = 'uptime/IC86_IV_exp.csv',
-        **ds_kwargs
+        name='IC86_IV',
+        exp_pathfilenames='events/IC86_IV_exp.csv',
+        mc_pathfilenames=None,
+        grl_pathfilenames='uptime/IC86_IV_exp.csv',
+        **ds_kwargs,
     )
     IC86_IV.grl_field_name_renaming_dict = grl_field_name_renaming_dict
     IC86_IV.add_aux_data_definition(
         'eff_area_datafile', 'irfs/IC86_II_effectiveArea.csv')
     IC86_IV.add_aux_data_definition(
         'smearing_datafile', 'irfs/IC86_II_smearing.csv')
 
     IC86_IV.add_binning_definition(
         IC86_II.get_binning_definition('sin_dec'))
     IC86_IV.add_binning_definition(
         IC86_II.get_binning_definition('log_energy'))
 
     # ---------- IC86-V --------------------------------------------------------
     IC86_V = I3Dataset(
-        name = 'IC86_V',
-        exp_pathfilenames = 'events/IC86_V_exp.csv',
-        mc_pathfilenames = None,
-        grl_pathfilenames = 'uptime/IC86_V_exp.csv',
-        **ds_kwargs
+        name='IC86_V',
+        exp_pathfilenames='events/IC86_V_exp.csv',
+        mc_pathfilenames=None,
+        grl_pathfilenames='uptime/IC86_V_exp.csv',
+        **ds_kwargs,
     )
     IC86_V.grl_field_name_renaming_dict = grl_field_name_renaming_dict
     IC86_V.add_aux_data_definition(
         'eff_area_datafile', 'irfs/IC86_II_effectiveArea.csv')
     IC86_V.add_aux_data_definition(
         'smearing_datafile', 'irfs/IC86_II_smearing.csv')
 
     IC86_V.add_binning_definition(
         IC86_II.get_binning_definition('sin_dec'))
     IC86_V.add_binning_definition(
         IC86_II.get_binning_definition('log_energy'))
 
     # ---------- IC86-VI -------------------------------------------------------
     IC86_VI = I3Dataset(
-        name = 'IC86_VI',
-        exp_pathfilenames = 'events/IC86_VI_exp.csv',
-        mc_pathfilenames = None,
-        grl_pathfilenames = 'uptime/IC86_VI_exp.csv',
-        **ds_kwargs
+        name='IC86_VI',
+        exp_pathfilenames='events/IC86_VI_exp.csv',
+        mc_pathfilenames=None,
+        grl_pathfilenames='uptime/IC86_VI_exp.csv',
+        **ds_kwargs,
     )
     IC86_VI.grl_field_name_renaming_dict = grl_field_name_renaming_dict
     IC86_VI.add_aux_data_definition(
         'eff_area_datafile', 'irfs/IC86_II_effectiveArea.csv')
     IC86_VI.add_aux_data_definition(
         'smearing_datafile', 'irfs/IC86_II_smearing.csv')
 
     IC86_VI.add_binning_definition(
         IC86_II.get_binning_definition('sin_dec'))
     IC86_VI.add_binning_definition(
         IC86_II.get_binning_definition('log_energy'))
 
     # ---------- IC86-VII ------------------------------------------------------
     IC86_VII = I3Dataset(
-        name = 'IC86_VII',
-        exp_pathfilenames = 'events/IC86_VII_exp.csv',
-        mc_pathfilenames = None,
-        grl_pathfilenames = 'uptime/IC86_VII_exp.csv',
-        **ds_kwargs
+        name='IC86_VII',
+        exp_pathfilenames='events/IC86_VII_exp.csv',
+        mc_pathfilenames=None,
+        grl_pathfilenames='uptime/IC86_VII_exp.csv',
+        **ds_kwargs,
     )
     IC86_VII.grl_field_name_renaming_dict = grl_field_name_renaming_dict
     IC86_VII.add_aux_data_definition(
         'eff_area_datafile', 'irfs/IC86_II_effectiveArea.csv')
     IC86_VII.add_aux_data_definition(
         'smearing_datafile', 'irfs/IC86_II_smearing.csv')
 
@@ -490,19 +501,19 @@
         IC86_III,
         IC86_IV,
         IC86_V,
         IC86_VI,
         IC86_VII,
     ]
     IC86_II_VII = I3Dataset(
-        name = 'IC86_II-VII',
-        exp_pathfilenames = I3Dataset.get_combined_exp_pathfilenames(ds_list),
-        mc_pathfilenames = None,
-        grl_pathfilenames = I3Dataset.get_combined_grl_pathfilenames(ds_list),
-        **ds_kwargs
+        name='IC86_II-VII',
+        exp_pathfilenames=I3Dataset.get_combined_exp_pathfilenames(ds_list),
+        mc_pathfilenames=None,
+        grl_pathfilenames=I3Dataset.get_combined_grl_pathfilenames(ds_list),
+        **ds_kwargs,
     )
     IC86_II_VII.grl_field_name_renaming_dict = grl_field_name_renaming_dict
     IC86_II_VII.add_aux_data_definition(
         'eff_area_datafile',
         IC86_II.get_aux_data_definition('eff_area_datafile'))
 
     IC86_II_VII.add_aux_data_definition(
@@ -510,38 +521,38 @@
         IC86_II.get_aux_data_definition('smearing_datafile'))
 
     IC86_II_VII.add_binning_definition(
         IC86_II.get_binning_definition('sin_dec'))
     IC86_II_VII.add_binning_definition(
         IC86_II.get_binning_definition('log_energy'))
 
-    #---------------------------------------------------------------------------
+    # --------------------------------------------------------------------------
 
     dsc.add_datasets((
         IC40,
         IC59,
         IC79,
         IC86_I,
         IC86_II,
         IC86_III,
         IC86_IV,
         IC86_V,
         IC86_VI,
         IC86_VII,
-        IC86_II_VII
+        IC86_II_VII,
     ))
 
     dsc.set_exp_field_name_renaming_dict({
         'MJD[days]':    'time',
         'log10(E/GeV)': 'log_energy',
         'AngErr[deg]':  'ang_err',
         'RA[deg]':      'ra',
         'Dec[deg]':     'dec',
         'Azimuth[deg]': 'azi',
-        'Zenith[deg]':  'zen'
+        'Zenith[deg]':  'zen',
     })
 
     def add_run_number(data):
         exp = data.exp
         exp.append_field('run', np.repeat(0, len(exp)))
 
     def convert_deg2rad(data):
```

### Comparing `skyllh-23.1.1/skyllh/datasets/i3/PublicData_10y_ps_wMC.py` & `skyllh-23.2.0/skyllh/datasets/i3/PublicData_10y_ps_wMC.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,27 +1,37 @@
 # -*- coding: utf-8 -*-
 # Author: Dr. Martin Wolf <mail@martin-wolf.org>
 
 import numpy as np
 
-from skyllh.core.dataset import DatasetCollection
-from skyllh.i3.dataset import I3Dataset
+from skyllh.core.dataset import (
+    DatasetCollection,
+)
+from skyllh.i3.dataset import (
+    I3Dataset,
+)
 
 
-def create_dataset_collection(base_path=None, sub_path_fmt=None):
+def create_dataset_collection(
+        cfg,
+        base_path=None,
+        sub_path_fmt=None,
+):
     """Defines the dataset collection for IceCube's 10-year
     point-source public data, which is available at
     http://icecube.wisc.edu/data-releases/20210126_PS-IC40-IC86_VII.zip
 
     Parameters
     ----------
+    cfg : instance of Config
+        The instance of Config holding the local configuration.
     base_path : str | None
         The base path of the data files. The actual path of a data file is
         assumed to be of the structure <base_path>/<sub_path>/<file_name>.
-        If None, use the default path CFG['repository']['base_path'].
+        If None, use the default path ``cfg['repository']['base_path']``.
     sub_path_fmt : str | None
         The sub path format of the data files of the public data sample.
         If None, use the default sub path format
         'icecube_10year_ps'.
 
     Returns
     -------
@@ -35,15 +45,15 @@
     # Define the default sub path format.
     default_sub_path_fmt = 'icecube_10year_ps'
 
     # We create a dataset collection that will hold the individual seasonal
     # public data datasets (all of the same version!).
     dsc = DatasetCollection('Public Data 10-year point-source')
 
-    dsc.description = """
+    dsc.description = r"""
     The events contained in this release correspond to the IceCube's
     time-integrated point source search with 10 years of data [2]. Please refer
     to the description of the sample and known changes in the text at [1].
 
     The data contained in this release of IceCubes point source sample shows
     evidence of a cumulative excess of events from four sources (NGC 1068,
     TXS 0506+056, PKS 1424+240, and GB6 J1542+6129) from a catalogue of 110
@@ -71,17 +81,17 @@
     sample over the old sample, these results do not supercede those in [4].
 
     This release contains data beginning in 2008 (IC40) until the spring of 2018
     (IC86-2017). This release duplicates and supplants previously released data
     from 2012 and earlier. Events from this release cannot be combined with any
     other releases
 
-    -----------------------------------------
+    --------------------------
     # Experimental data events
-    -----------------------------------------
+    --------------------------
     The "events" folder contains the events observed in the 10 year sample of
     IceCube's point source neutrino selection. Each file corresponds to a single
     season of IceCube datataking, including roughly one year of data. For each
     event, reconstructed particle information is included.
 
     - MJD: The MJD time (ut1) of the event interaction given to 1e-8 days,
     corresponding to roughly millisecond precision.
@@ -110,40 +120,40 @@
     scrambling events in time, then using the local coordinates and time to
     calculate new RA and Dec values.
 
     Note that during the preparation of this data release, one duplicated event
     was discovered in the IC86-2015 season. This event has not contributed to
     any significant excesses.
 
-    -----------------------------------------
+    -----------------
     # Detector uptime
-    -----------------------------------------
+    -----------------
     In order to properly account for detector uptime, IceCube maintains
     "good run lists". These contain information about "good runs", periods of
     datataking useful for analysis. Data may be marked unusable for various
     reasons, including major construction or upgrade work, calibration runs, or
     other anomalies. The "uptime" folder contains lists of the good runs for
     each season.
 
     - MJD_start[days], MJD_stop[days]: The start and end times for each good run
 
-    -----------------------------------------
+    -------------------------------
     # Instrument response functions
-    -----------------------------------------
+    -------------------------------
     In order to best model the response of the IceCube detector to a given
     signal, Monte Carlo simulations are produced for each detector
     configuration. Events are sampled from these simulations to model the
     response of point sources from an arbitrary source and spectrum.
 
     We provide several binned responses for the detector in the "irfs" folder
     of this data release.
 
-    ------------------
+    -----------------
     # Effective Areas
-    ------------------
+    -----------------
     The effective area is a property of the detector and selection which, when
     convolved with a flux model, gives the expected rate of events in the
     detector. Here we release the muon neutrino effective areas for each season
     of data.
 
     The effective areas are averaged over bins using simulated muon neutrino
     events ranging from 100 GeV to 100 PeV. Because the response varies widely
@@ -167,17 +177,17 @@
 
     - Dec_nu_min[deg], Dec_nu_max[deg]: The minimum and maximum of the
     declination of the neutrino origin. Again, note that this is the true
     direction of the neutrino and not the reconstructed muon direction.
 
     - A_Eff[cm^2]: The average effective area across a bin.
 
-    ------------------
+    -------------------
     # Smearing Matrices
-    ------------------
+    -------------------
     IceCube has a nontrivial smearing matrix with correlations between the
     directional uncertainty, the point spread function, and the reconstructed
     muon energy. To provide the most complete set of information, we include
     tables of these responses for each season from IC40 through IC86-2012.
     Seasons after IC86-2012 reuse that season's response functions.
 
     The included smearing matrices take the form of 5D tables mapping a
@@ -217,17 +227,17 @@
     The errors are calibrated so that they provide correct coverage for an
     E^{-2} power law flux. This sample assumes a lower limit on the estimated
     angular uncertainty of 0.2 degrees.
 
     - Fractional_Counts: The fraction of simulated events falling within each
     5D bin relative to all events in the (E_nu, Dec_nu) bin.
 
-    -----------------------------------------
+    ------------
     # References
-    -----------------------------------------
+    ------------
     [1] IceCube Data for Neutrino Point-Source Searches: Years 2008-2018,
         [ArXiv link](https://arxiv.org/abs/2101.09836)
     [2] Time-integrated Neutrino Source Searches with 10 years of IceCube Data,
         Phys. Rev. Lett. 124, 051103 (2020)
     [3] All-sky search for time-integrated neutrino emission from astrophysical
         sources with 7 years of IceCube data,
         Astrophys. J., 835 (2017) no. 2, 151
@@ -235,47 +245,48 @@
         the IceCube-170922A alert,
         Science 361, 147-151 (2018)
     [5] Energy Reconstruction Methods in the IceCube Neutrino Telescope,
         JINST 9 (2014), P03009
     [6] Methods for point source analysis in high energy neutrino telescopes,
         Astropart.Phys.29:299-305,2008
 
-    -----------------------------------------
+    -------------
     # Last Update
-    -----------------------------------------
+    -------------
     28 January 2021
     """
 
     # Define the common keyword arguments for all data sets.
     ds_kwargs = dict(
-        livetime = None,
-        version = version,
-        verqualifiers = verqualifiers,
-        base_path = base_path,
-        default_sub_path_fmt = default_sub_path_fmt,
-        sub_path_fmt = sub_path_fmt
+        cfg=cfg,
+        livetime=None,
+        version=version,
+        verqualifiers=verqualifiers,
+        base_path=base_path,
+        default_sub_path_fmt=default_sub_path_fmt,
+        sub_path_fmt=sub_path_fmt,
     )
 
     grl_field_name_renaming_dict = {
         'MJD_start[days]': 'start',
-        'MJD_stop[days]': 'stop'
+        'MJD_stop[days]': 'stop',
     }
 
     # Define the datasets for the different seasons.
     # For the declination and energy binning we use the same binning as was
     # used in the original point-source analysis using the PointSourceTracks
     # dataset.
 
     # ---------- IC40 ----------------------------------------------------------
     IC40 = I3Dataset(
-        name = 'IC40',
-        exp_pathfilenames = 'events/IC40_exp.csv',
-        mc_pathfilenames = 'sim/IC40_MC.npy',
-        grl_pathfilenames = 'uptime/IC40_exp.csv',
-        **ds_kwargs
+        name='IC40',
+        exp_pathfilenames='events/IC40_exp.csv',
+        mc_pathfilenames='sim/IC40_MC.npy',
+        grl_pathfilenames='uptime/IC40_exp.csv',
+        **ds_kwargs,
     )
     IC40.grl_field_name_renaming_dict = grl_field_name_renaming_dict
     IC40.add_aux_data_definition(
         'eff_area_datafile', 'irfs/IC40_effectiveArea.csv')
     IC40.add_aux_data_definition(
         'smearing_datafile', 'irfs/IC40_smearing.csv')
 
@@ -287,19 +298,19 @@
     IC40.define_binning('sin_dec', sin_dec_bins)
 
     energy_bins = np.arange(2., 9.5 + 0.01, 0.125)
     IC40.define_binning('log_energy', energy_bins)
 
     # ---------- IC59 ----------------------------------------------------------
     IC59 = I3Dataset(
-        name = 'IC59',
-        exp_pathfilenames = 'events/IC59_exp.csv',
-        mc_pathfilenames = 'sim/IC59_MC.npy',
-        grl_pathfilenames = 'uptime/IC59_exp.csv',
-        **ds_kwargs
+        name='IC59',
+        exp_pathfilenames='events/IC59_exp.csv',
+        mc_pathfilenames='sim/IC59_MC.npy',
+        grl_pathfilenames='uptime/IC59_exp.csv',
+        **ds_kwargs,
     )
     IC59.grl_field_name_renaming_dict = grl_field_name_renaming_dict
     IC59.add_aux_data_definition(
         'eff_area_datafile', 'irfs/IC59_effectiveArea.csv')
     IC59.add_aux_data_definition(
         'smearing_datafile', 'irfs/IC59_smearing.csv')
 
@@ -312,69 +323,69 @@
     IC59.define_binning('sin_dec', sin_dec_bins)
 
     energy_bins = np.arange(2., 9.5 + 0.01, 0.125)
     IC59.define_binning('log_energy', energy_bins)
 
     # ---------- IC79 ----------------------------------------------------------
     IC79 = I3Dataset(
-        name = 'IC79',
-        exp_pathfilenames = 'events/IC79_exp.csv',
-        mc_pathfilenames = 'sim/IC79_MC.npy',
-        grl_pathfilenames = 'uptime/IC79_exp.csv',
-        **ds_kwargs
+        name='IC79',
+        exp_pathfilenames='events/IC79_exp.csv',
+        mc_pathfilenames='sim/IC79_MC.npy',
+        grl_pathfilenames='uptime/IC79_exp.csv',
+        **ds_kwargs,
     )
     IC79.grl_field_name_renaming_dict = grl_field_name_renaming_dict
     IC79.add_aux_data_definition(
         'eff_area_datafile', 'irfs/IC79_effectiveArea.csv')
     IC79.add_aux_data_definition(
         'smearing_datafile', 'irfs/IC79_smearing.csv')
 
     sin_dec_bins = np.unique(np.concatenate([
-                        np.linspace(-1., -0.75, 10 + 1),
-                        np.linspace(-0.75, 0., 15 + 1),
-                        np.linspace(0., 1., 20 + 1)
-                        ]))
+        np.linspace(-1., -0.75, 10 + 1),
+        np.linspace(-0.75, 0., 15 + 1),
+        np.linspace(0., 1., 20 + 1),
+    ]))
     IC79.define_binning('sin_dec', sin_dec_bins)
 
     energy_bins = np.arange(2., 9.5 + 0.01, 0.125)
     IC79.define_binning('log_energy', energy_bins)
 
     # ---------- IC86-I --------------------------------------------------------
     IC86_I = I3Dataset(
-        name = 'IC86_I',
-        exp_pathfilenames = 'events/IC86_I_exp.csv',
-        mc_pathfilenames = 'sim/IC86_I_MC.npy',
-        grl_pathfilenames = 'uptime/IC86_I_exp.csv',
-        **ds_kwargs
+        name='IC86_I',
+        exp_pathfilenames='events/IC86_I_exp.csv',
+        mc_pathfilenames='sim/IC86_I_MC.npy',
+        grl_pathfilenames='uptime/IC86_I_exp.csv',
+        **ds_kwargs,
     )
     IC86_I.grl_field_name_renaming_dict = grl_field_name_renaming_dict
     IC86_I.add_aux_data_definition(
         'eff_area_datafile', 'irfs/IC86_I_effectiveArea.csv')
     IC86_I.add_aux_data_definition(
         'smearing_datafile', 'irfs/IC86_I_smearing.csv')
 
-    b = np.sin(np.radians(-5.)) # North/South transition boundary.
+    b = np.sin(np.radians(-5.))  # North/South transition boundary.
     sin_dec_bins = np.unique(np.concatenate([
         np.linspace(-1., -0.2, 10 + 1),
         np.linspace(-0.2, b, 4 + 1),
         np.linspace(b, 0.2, 5 + 1),
         np.linspace(0.2, 1., 10),
     ]))
     IC86_I.define_binning('sin_dec', sin_dec_bins)
 
     energy_bins = np.arange(1., 10.5 + 0.01, 0.125)
     IC86_I.define_binning('log_energy', energy_bins)
 
     # ---------- IC86-II -------------------------------------------------------
     IC86_II = I3Dataset(
-        name = 'IC86_II',
-        exp_pathfilenames = 'events/IC86_II_exp.csv',
-        mc_pathfilenames = 'sim/IC86_II-VII_MC.npy',
-        grl_pathfilenames = 'uptime/IC86_II_exp.csv',
-        **ds_kwargs
+        name='IC86_II',
+        exp_pathfilenames='events/IC86_II_exp.csv',
+        mc_pathfilenames='sim/IC86_II-VII_MC.npy',
+        grl_pathfilenames='uptime/IC86_II_exp.csv',
+        **ds_kwargs,
     )
     IC86_II.grl_field_name_renaming_dict = grl_field_name_renaming_dict
     IC86_II.add_aux_data_definition(
         'eff_area_datafile', 'irfs/IC86_II_effectiveArea.csv')
     IC86_II.add_aux_data_definition(
         'smearing_datafile', 'irfs/IC86_II_smearing.csv')
 
@@ -387,95 +398,95 @@
     IC86_II.define_binning('sin_dec', sin_dec_bins)
 
     energy_bins = np.arange(0.5, 9.5 + 0.01, 0.125)
     IC86_II.define_binning('log_energy', energy_bins)
 
     # ---------- IC86-III ------------------------------------------------------
     IC86_III = I3Dataset(
-        name = 'IC86_III',
-        exp_pathfilenames = 'events/IC86_III_exp.csv',
-        mc_pathfilenames = 'sim/IC86_II-VII_MC.npy',
-        grl_pathfilenames = 'uptime/IC86_III_exp.csv',
-        **ds_kwargs
+        name='IC86_III',
+        exp_pathfilenames='events/IC86_III_exp.csv',
+        mc_pathfilenames='sim/IC86_II-VII_MC.npy',
+        grl_pathfilenames='uptime/IC86_III_exp.csv',
+        **ds_kwargs,
     )
     IC86_III.grl_field_name_renaming_dict = grl_field_name_renaming_dict
     IC86_III.add_aux_data_definition(
         'eff_area_datafile', 'irfs/IC86_II_effectiveArea.csv')
     IC86_III.add_aux_data_definition(
         'smearing_datafile', 'irfs/IC86_II_smearing.csv')
 
     IC86_III.add_binning_definition(
         IC86_II.get_binning_definition('sin_dec'))
     IC86_III.add_binning_definition(
         IC86_II.get_binning_definition('log_energy'))
 
     # ---------- IC86-IV -------------------------------------------------------
     IC86_IV = I3Dataset(
-        name = 'IC86_IV',
-        exp_pathfilenames = 'events/IC86_IV_exp.csv',
-        mc_pathfilenames = 'sim/IC86_II-VII_MC.npy',
-        grl_pathfilenames = 'uptime/IC86_IV_exp.csv',
-        **ds_kwargs
+        name='IC86_IV',
+        exp_pathfilenames='events/IC86_IV_exp.csv',
+        mc_pathfilenames='sim/IC86_II-VII_MC.npy',
+        grl_pathfilenames='uptime/IC86_IV_exp.csv',
+        **ds_kwargs,
     )
     IC86_IV.grl_field_name_renaming_dict = grl_field_name_renaming_dict
     IC86_IV.add_aux_data_definition(
         'eff_area_datafile', 'irfs/IC86_II_effectiveArea.csv')
     IC86_IV.add_aux_data_definition(
         'smearing_datafile', 'irfs/IC86_II_smearing.csv')
 
     IC86_IV.add_binning_definition(
         IC86_II.get_binning_definition('sin_dec'))
     IC86_IV.add_binning_definition(
         IC86_II.get_binning_definition('log_energy'))
 
     # ---------- IC86-V --------------------------------------------------------
     IC86_V = I3Dataset(
-        name = 'IC86_V',
-        exp_pathfilenames = 'events/IC86_V_exp.csv',
-        mc_pathfilenames = 'sim/IC86_II-VII_MC.npy',
-        grl_pathfilenames = 'uptime/IC86_V_exp.csv',
-        **ds_kwargs
+        name='IC86_V',
+        exp_pathfilenames='events/IC86_V_exp.csv',
+        mc_pathfilenames='sim/IC86_II-VII_MC.npy',
+        grl_pathfilenames='uptime/IC86_V_exp.csv',
+        **ds_kwargs,
     )
     IC86_V.grl_field_name_renaming_dict = grl_field_name_renaming_dict
     IC86_V.add_aux_data_definition(
         'eff_area_datafile', 'irfs/IC86_II_effectiveArea.csv')
     IC86_V.add_aux_data_definition(
         'smearing_datafile', 'irfs/IC86_II_smearing.csv')
 
     IC86_V.add_binning_definition(
         IC86_II.get_binning_definition('sin_dec'))
     IC86_V.add_binning_definition(
         IC86_II.get_binning_definition('log_energy'))
 
     # ---------- IC86-VI -------------------------------------------------------
     IC86_VI = I3Dataset(
-        name = 'IC86_VI',
-        exp_pathfilenames = 'events/IC86_VI_exp.csv',
-        mc_pathfilenames = 'sim/IC86_II-VII_MC.npy',
-        grl_pathfilenames = 'uptime/IC86_VI_exp.csv',
-        **ds_kwargs
+        name='IC86_VI',
+        exp_pathfilenames='events/IC86_VI_exp.csv',
+        mc_pathfilenames='sim/IC86_II-VII_MC.npy',
+        grl_pathfilenames='uptime/IC86_VI_exp.csv',
+        **ds_kwargs,
     )
     IC86_VI.grl_field_name_renaming_dict = grl_field_name_renaming_dict
     IC86_VI.add_aux_data_definition(
         'eff_area_datafile', 'irfs/IC86_II_effectiveArea.csv')
     IC86_VI.add_aux_data_definition(
         'smearing_datafile', 'irfs/IC86_II_smearing.csv')
 
     IC86_VI.add_binning_definition(
         IC86_II.get_binning_definition('sin_dec'))
     IC86_VI.add_binning_definition(
         IC86_II.get_binning_definition('log_energy'))
 
     # ---------- IC86-VII ------------------------------------------------------
     IC86_VII = I3Dataset(
-        name = 'IC86_VII',
-        exp_pathfilenames = 'events/IC86_VII_exp.csv',
-        mc_pathfilenames = 'sim/IC86_II-VII_MC.npy',
-        grl_pathfilenames = 'uptime/IC86_VII_exp.csv',
-        **ds_kwargs
+        name='IC86_VII',
+        exp_pathfilenames='events/IC86_VII_exp.csv',
+        mc_pathfilenames='sim/IC86_II-VII_MC.npy',
+        grl_pathfilenames='uptime/IC86_VII_exp.csv',
+        **ds_kwargs,
     )
     IC86_VII.grl_field_name_renaming_dict = grl_field_name_renaming_dict
     IC86_VII.add_aux_data_definition(
         'eff_area_datafile', 'irfs/IC86_II_effectiveArea.csv')
     IC86_VII.add_aux_data_definition(
         'smearing_datafile', 'irfs/IC86_II_smearing.csv')
 
@@ -490,19 +501,19 @@
         IC86_III,
         IC86_IV,
         IC86_V,
         IC86_VI,
         IC86_VII,
     ]
     IC86_II_VII = I3Dataset(
-        name = 'IC86_II-VII',
-        exp_pathfilenames = I3Dataset.get_combined_exp_pathfilenames(ds_list),
-        mc_pathfilenames = IC86_II.mc_pathfilename_list,
-        grl_pathfilenames = I3Dataset.get_combined_grl_pathfilenames(ds_list),
-        **ds_kwargs
+        name='IC86_II-VII',
+        exp_pathfilenames=I3Dataset.get_combined_exp_pathfilenames(ds_list),
+        mc_pathfilenames=IC86_II.mc_pathfilename_list,
+        grl_pathfilenames=I3Dataset.get_combined_grl_pathfilenames(ds_list),
+        **ds_kwargs,
     )
     IC86_II_VII.grl_field_name_renaming_dict = grl_field_name_renaming_dict
     IC86_II_VII.add_aux_data_definition(
         'eff_area_datafile',
         IC86_II.get_aux_data_definition('eff_area_datafile'))
 
     IC86_II_VII.add_aux_data_definition(
@@ -510,38 +521,38 @@
         IC86_II.get_aux_data_definition('smearing_datafile'))
 
     IC86_II_VII.add_binning_definition(
         IC86_II.get_binning_definition('sin_dec'))
     IC86_II_VII.add_binning_definition(
         IC86_II.get_binning_definition('log_energy'))
 
-    #---------------------------------------------------------------------------
+    # --------------------------------------------------------------------------
 
     dsc.add_datasets((
         IC40,
         IC59,
         IC79,
         IC86_I,
         IC86_II,
         IC86_III,
         IC86_IV,
         IC86_V,
         IC86_VI,
         IC86_VII,
-        IC86_II_VII
+        IC86_II_VII,
     ))
 
     dsc.set_exp_field_name_renaming_dict({
         'MJD[days]':    'time',
         'log10(E/GeV)': 'log_energy',
         'AngErr[deg]':  'ang_err',
         'RA[deg]':      'ra',
         'Dec[deg]':     'dec',
         'Azimuth[deg]': 'azi',
-        'Zenith[deg]':  'zen'
+        'Zenith[deg]':  'zen',
     })
 
     def add_run_number(data):
         exp = data.exp
         mc = data.mc
         exp.append_field('run', np.repeat(0, len(exp)))
         mc.append_field('run', np.repeat(0, len(mc)))
```

### Comparing `skyllh-23.1.1/skyllh/i3/background_generation.py` & `skyllh-23.2.0/skyllh/i3/background_generation.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,49 +1,72 @@
 # -*- coding: utf-8 -*-
 
-from skyllh.core.background_generation import BackgroundGenerationMethod
-from skyllh.core.scrambling import DataScrambler
+from skyllh.core.background_generation import (
+    BackgroundGenerationMethod,
+)
+from skyllh.core.py import (
+    classname,
+)
+from skyllh.core.scrambling import (
+    DataScrambler,
+)
 
 
-class FixedScrambledExpDataI3BkgGenMethod(BackgroundGenerationMethod):
+class FixedScrambledExpDataI3BkgGenMethod(
+        BackgroundGenerationMethod,
+):
     """This class implements the background event generation method for the
     IceCube detector using scrambled experimental data as background hypothesis
     with a fixed number of background events equal to the number of events in
     the dataset. This background generation method is the one used in SkyLab.
     """
-    def __init__(self, data_scrambler):
+    def __init__(
+            self,
+            data_scrambler,
+            **kwargs,
+    ):
         """Creates a new background generation method instance to generate
         background events from scrambled experimental data with a fixed number
         of events equal to the number of events in the dataset.
 
         Parameters
         ----------
         data_scrambler : instance of DataScrambler
             The DataScrambler instance to use to generate scrambled experimental
             data.
         """
-        super(FixedScrambledExpDataI3BkgGenMethod, self).__init__()
+        super().__init__(**kwargs)
 
         self.data_scrambler = data_scrambler
 
     @property
     def data_scrambler(self):
         """The DataScrambler instance that implements the data scrambling.
         """
         return self._data_scrambler
+
     @data_scrambler.setter
     def data_scrambler(self, scrambler):
-        if(not isinstance(scrambler, DataScrambler)):
-            raise TypeError('The data_scrambler property must be an instance '
-                'of DataScrambler!')
+        if not isinstance(scrambler, DataScrambler):
+            raise TypeError(
+                'The data_scrambler property must be an instance of '
+                'DataScrambler! '
+                f'Its current type is {classname(scrambler)}!')
         self._data_scrambler = scrambler
 
-    def generate_events(self, rss, dataset, data, **kwargs):
+    def generate_events(
+            self,
+            rss,
+            dataset,
+            data,
+            **kwargs,
+    ):
         """Generates background events from the given data, by scrambling the
-        data. The number of events is equal to the size of the given dataset.
+        experimental data. The number of events is equal to the size of the
+        given dataset.
 
         Parameters
         ----------
         rss : instance of RandomStateService
             The instance of RandomStateService that should be used to generate
             random numbers from. It is used to scramble the experimental data.
         dataset : instance of Dataset
@@ -57,11 +80,14 @@
         -------
         n_bkg : int
             The number of generated background events.
         bkg_events : instance of DataFieldRecordArray
             The instance of DataFieldRecordArray holding the generated
             background events.
         """
-        # Scramble the experimental data events, but make a copy first.
-        bkg_events = self._data_scrambler.scramble_data(rss, data.exp.copy())
+        bkg_events = self._data_scrambler.scramble_data(
+            rss=rss,
+            dataset=dataset,
+            data=data.exp,
+            copy=True)
 
         return (len(bkg_events), bkg_events)
```

### Comparing `skyllh-23.1.1/skyllh/i3/coords.py` & `skyllh-23.2.0/skyllh/core/tool.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,65 +1,90 @@
 # -*- coding: utf-8 -*-
 
-"""IceCube specific coordinate utility functions.
+"""The tool module provides functionality to interface with an optional external
+python package (tool). The tool can be imported dynamically at run-time when
+needed.
 """
 
-import numpy as np
+import importlib
+import importlib.util
+import sys
 
-def azi_to_ra_transform(azi, mjd):
-    """Rotates the given IceCube azimuth angles into right-ascention angles for
-    the given MJD times. This function is IceCube specific and assumes that the
-    detector is located excently at the South Pole and neglects all astronomical
-    effects like Earth's precession.
+from skyllh.core.py import (
+    get_class_of_func,
+)
+
+
+def is_available(name):
+    """Checks if the given Python package is available for import.
 
     Parameters
     ----------
-    azi : ndarray
-        The array with the azimuth angles.
-    mjd : ndarray
-        The array with the MJD times for each azimuth angle.
+    name : str
+        The name of the Python package.
+
+    Returns
+    -------
+    check : bool
+        ``True`` if the given Python package is available, ``False`` otherwise.
+
+    Raises
+    ------
+    ModuleNotFoundError
+        If the package is not a Python package, i.e. lacks a __path__ attribute.
     """
-    _sidereal_length = 0.997269566 # sidereal day = length * solar day
-    _sidereal_offset = 2.54199002505 # RA = offset + 2pi * (MJD/sidereal_length)%1 - azimuth
-    sidereal_day_residuals = ((mjd/_sidereal_length)%1)
-    ra = _sidereal_offset + 2 * np.pi * sidereal_day_residuals - azi
-    ra = np.mod(ra, 2*np.pi)
-    return ra
+    # Check if module is already imported.
+    if name in sys.modules:
+        return True
+
+    spec = importlib.util.find_spec(name)
+    if spec is not None:
+        return True
+
+    return False
 
-def ra_to_azi_transform(ra, mjd):
-    """Rotates the given right-ascention angles to local IceCube azimuth angles.
+
+def get(name):
+    """Returns the module object of the given tool. This will import the Python
+    package if it was not yet imported.
 
     Parameters
     ----------
-    ra : ndarray
-        The array with the right-ascention angles.
-    mjd : ndarray
-        The array with the MJD times for each right-ascention angle.
+    name : str
+        The name of the Python package.
 
+    Returns
+    -------
+    module : Python module
+        The (imported) Python module object.
     """
-    # Use the azi_to_ra_transform function because it is symmetric.
-    return azi_to_ra_transform(ra, mjd)
+    if name in sys.modules:
+        return sys.modules[name]
+
+    module = importlib.import_module(name)
+    return module
 
-def hor_to_equ_transform(azi, zen, mjd):
-    """Transforms the coordinate from the horizontal system (azimuth, zenith)
-    into the equatorial system (right-ascention, declination) for detector at
-    the South Pole and neglecting all astronomical effects like Earth
-    precession.
+
+def requires(*tools):
+    """This is decorator function that can be used whenever a function requires
+    optional tools.
 
     Parameters
     ----------
-    azi : ndarray
-        The azimuth angle.
-    zen : ndarray
-        The zenith angle.
-    mjd : ndarray
-        The time in MJD.
+    *tools : sequence of str
+        The name of the required Python packages.
 
-    Returns
-    -------
-    (ra, dec) : (ndarray, ndarray)
-        The two-element tuple with the arrays of right-ascention and
-        declination.
+    Raises
+    ------
+    ModuleNotFoundError
+        If any of the specified tools is not available.
     """
-    ra = azi_to_ra_transform(azi, mjd)
-    dec = np.pi - zen
-    return (ra, dec)
+    def decorator(f):
+        def wrapper(*args, **kwargs):
+            for tool in tools:
+                if not is_available(tool):
+                    raise ModuleNotFoundError(
+                        f'The Python module "{tool}" is not available, but is '
+                        f'required by "{get_class_of_func(f)}.{f.__name__}"!')
+            return f(*args, **kwargs)
+        return wrapper
+    return decorator
```

### Comparing `skyllh-23.1.1/skyllh/i3/dataset.py` & `skyllh-23.2.0/skyllh/i3/dataset.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,34 +1,39 @@
 # -*- coding: utf-8 -*-
 
-import numpy as np
 import os.path
 
-from skyllh.core import display
-from skyllh.core.py import (
-    issequenceof,
-    module_classname
+import numpy as np
+
+from skyllh.core import (
+    display,
 )
 from skyllh.core.dataset import (
     Dataset,
-    DatasetData
+    DatasetData,
+)
+from skyllh.core.debugging import (
+    get_logger,
+)
+from skyllh.core.py import (
+    issequenceof,
+    module_classname,
 )
-from skyllh.core.debugging import get_logger
 from skyllh.core.storage import (
     DataFieldRecordArray,
-    create_FileLoader
+    create_FileLoader,
+)
+from skyllh.core.timing import (
+    TaskTimer,
 )
-from skyllh.core.timing import TaskTimer
-
-# Load the IceCube specific config defaults.
-# This will change the skyllh.core.config.CFG dictionary.
-from skyllh.i3 import config
 
 
-class I3Dataset(Dataset):
+class I3Dataset(
+        Dataset,
+):
     """The I3Dataset class is an IceCube specific Dataset class that adds
     IceCube specific properties to the Dataset class. These additional
     properties are:
 
         * good-run-list (GRL)
 
     """
@@ -43,57 +48,70 @@
             The sequence of I3Dataset instances.
 
         Returns
         -------
         grl_pathfilenames : list
             The combined list of grl pathfilenames.
         """
-        if(not issequenceof(datasets, I3Dataset)):
-            raise TypeError('The datasets argument must be a sequence of I3Dataset instances!')
+        if not issequenceof(datasets, I3Dataset):
+            raise TypeError(
+                'The datasets argument must be a sequence of I3Dataset '
+                'instances!')
 
         grl_pathfilenames = []
         for ds in datasets:
             grl_pathfilenames += ds.grl_pathfilename_list
 
         return grl_pathfilenames
 
-    def __init__(self, grl_pathfilenames=None, *args, **kwargs):
+    def __init__(
+            self,
+            livetime=None,
+            grl_pathfilenames=None,
+            **kwargs,
+    ):
         """Creates a new IceCube specific dataset, that also can hold a list
         of GRL data files.
 
         Parameters
         ----------
+        livetime : float | None
+            The live-time of the dataset in days. It can be ``None``, if
+            good-run-list data files are provided.
         grl_pathfilenames : str | sequence of str
-
+            The sequence of pathfilenames pointing to the good-run-list (GRL)
+            data files.
         """
-        super(I3Dataset, self).__init__(*args, **kwargs)
+        super().__init__(
+            livetime=livetime,
+            **kwargs)
 
         self._logger = get_logger(module_classname(self))
 
         self.grl_pathfilename_list = grl_pathfilenames
 
         self.grl_field_name_renaming_dict = dict()
 
     @property
     def grl_pathfilename_list(self):
-        """The list of file names of the good-run-list data files for this
-        dataset.
-        If a file name is given with a relative path, it will be relative to the
-        root_dir property of this Dataset instance.
+        """The list of file names of the good-run-list (GRL) data files for this
+        dataset. If a file name is given with a relative path, it will be
+        relative to the ``root_dir`` property of this Dataset instance.
         """
         return self._grl_pathfilename_list
+
     @grl_pathfilename_list.setter
     def grl_pathfilename_list(self, pathfilenames):
-        if(pathfilenames is None):
+        if pathfilenames is None:
             pathfilenames = []
-        if(isinstance(pathfilenames, str)):
+        if isinstance(pathfilenames, str):
             pathfilenames = [pathfilenames]
-        if(not issequenceof(pathfilenames, str)):
-            raise TypeError('The grl_pathfilename_list property must be a '
-                'sequence of str!')
+        if not issequenceof(pathfilenames, str):
+            raise TypeError(
+                'The grl_pathfilename_list property must be a sequence of str!')
         self._grl_pathfilename_list = list(pathfilenames)
 
     @property
     def grl_abs_pathfilename_list(self):
         """(read-only) The list of absolute path file names of the good-run-list
         data files.
         """
@@ -102,48 +120,50 @@
     @property
     def grl_field_name_renaming_dict(self):
         """The dictionary specifying the field names of the good-run-list data
         which need to get renamed just after loading the data. The dictionary
         keys are the old names and their values are the new names.
         """
         return self._grl_field_name_renaming_dict
+
     @grl_field_name_renaming_dict.setter
     def grl_field_name_renaming_dict(self, d):
-        if(not isinstance(d, dict)):
-            raise TypeError('The grl_field_name_renaming_dict property must '
-                'be an instance of dict!')
+        if not isinstance(d, dict):
+            raise TypeError(
+                'The grl_field_name_renaming_dict property must be an '
+                'instance of dict!')
         self._grl_field_name_renaming_dict = d
 
     @property
     def exists(self):
         """(read-only) Flag if all the data files of this data set exists. It is
         ``True`` if all data files exist and ``False`` otherwise.
         """
-        if(not super(I3Dataset,self).exists):
+        if not super().exists:
             return False
 
         for pathfilename in self.grl_abs_pathfilename_list:
-            if(not os.path.exists(pathfilename)):
+            if not os.path.exists(pathfilename):
                 return False
 
         return True
 
     def __str__(self):
         """Implementation of the pretty string representation of the I3Dataset
         object.
         """
-        s = super(I3Dataset, self).__str__()
+        s = super().__str__()
         s += '\n'
 
         s1 = ''
         s1 += 'GRL data:\n'
         s2 = ''
-        if(len(self._grl_pathfilename_list) > 0):
+        if len(self._grl_pathfilename_list) > 0:
             for (idx, pathfilename) in enumerate(self.grl_abs_pathfilename_list):
-                if(idx > 0):
+                if idx > 0:
                     s2 += '\n'
                 s2 += self._gen_datafile_pathfilename_entry(pathfilename)
         else:
             s2 += 'None'
         s1 += display.add_leading_text_line_padding(
             display.INDENTATION_WIDTH, s2)
 
@@ -196,14 +216,17 @@
         with TaskTimer(tl, 'Loading grl data from disk.'):
             fileloader_grl = create_FileLoader(
                 self.grl_abs_pathfilename_list)
             grl_data = fileloader_grl.load_data(
                 efficiency_mode=efficiency_mode)
             grl_data.rename_fields(self._grl_field_name_renaming_dict)
 
+        with TaskTimer(tl, 'Sort grl data according to start time'):
+            grl_data.sort_by_field(name='start')
+
         return grl_data
 
     def load_data(
             self, keep_fields=None, livetime=None, dtc_dict=None,
             dtc_except_fields=None, efficiency_mode=None, tl=None):
         """Loads the data, which is described by the dataset. If a good-run-list
         (GRL) is provided for this dataset, only experimental data will be
@@ -251,15 +274,15 @@
         data : instance of DatasetData
             A DatasetData instance holding the experimental and monte-carlo
             data of this data set.
         """
         # Load the good-run-list (GRL) data if it is provided for this dataset,
         # and calculate the livetime based on the GRL.
         data_grl = None
-        if(len(self._grl_pathfilename_list) > 0):
+        if len(self._grl_pathfilename_list) > 0:
             data_grl = self.load_grl(
                 efficiency_mode=efficiency_mode,
                 tl=tl)
 
         # Load all the defined data.
         data = I3DatasetData(
             super(I3Dataset, self).load_data(
@@ -269,15 +292,19 @@
                 dtc_except_fields=dtc_except_fields,
                 efficiency_mode=efficiency_mode,
                 tl=tl),
             data_grl)
 
         return data
 
-    def prepare_data(self, data, tl=None):
+    def prepare_data(  # noqa: C901
+            self,
+            data,
+            tl=None
+    ):
         """Prepares the data for IceCube by pre-calculating the following
         experimental data fields:
 
         - sin_dec: float
             The sin value of the declination coordinate.
 
         and monte-carlo data fields:
@@ -289,67 +316,78 @@
         ----------
         data : DatasetData instance
             The DatasetData instance holding the data as numpy record ndarray.
         tl : TimeLord instance | None
             The TimeLord instance that should be used to time the data
             preparation.
         """
+        # Set the livetime of the dataset from the GRL data when no livetime
+        # was specified previously.
+        if data.livetime is None and data.grl is not None:
+            if 'start' not in data.grl:
+                raise KeyError(
+                    f'The GRL data for dataset "{self.name}" has no data '
+                    'field named "start"!')
+            if 'stop' not in data.grl:
+                raise KeyError(
+                    f'The GRL data for dataset "{self.name}" has no data '
+                    'field named "stop"!')
+            data.livetime = np.sum(data.grl['stop'] - data.grl['start'])
+
         # Execute all the data preparation functions for this dataset.
-        super(I3Dataset, self).prepare_data(data, tl=tl)
+        super().prepare_data(
+            data=data,
+            tl=tl)
 
-        if(data.exp is not None):
+        if data.exp is not None:
             # Append sin(dec) data field to the experimental data.
             task = 'Appending IceCube-specific data fields to exp data.'
             with TaskTimer(tl, task):
                 if 'sin_dec' not in data.exp.field_name_list:
                     data.exp.append_field(
                         'sin_dec', np.sin(data.exp['dec']))
 
-        if(data.mc is not None):
+        if data.mc is not None:
             # Append sin(dec) and sin(true_dec) to the MC data.
             task = 'Appending IceCube-specific data fields to MC data.'
             with TaskTimer(tl, task):
                 if 'sin_dec' not in data.mc.field_name_list:
                     data.mc.append_field(
                         'sin_dec', np.sin(data.mc['dec']))
                 if 'sin_true_dec' not in data.mc.field_name_list:
                     data.mc.append_field(
                         'sin_true_dec', np.sin(data.mc['true_dec']))
 
-        # Set the livetime of the dataset from the GRL data when no livetime
-        # was specified previously.
-        if(data.livetime is None and data.grl is not None):
-            if('start' not in data.grl):
-                raise KeyError('The GRL data for dataset "{}" has no data '
-                    'field named "start"!'.format(self.name))
-            if('stop' not in data.grl):
-                raise KeyError('The GRL data for dataset "{}" has no data '
-                    'field named "stop"!'.format(self.name))
-            data.livetime = np.sum(data.grl['stop'] - data.grl['start'])
-
         # Select only the experimental data which fits the good-run-list for
         # this dataset.
-        if data.grl is not None:
+        if (data.grl is not None) and (data.exp is not None):
             # Select based on run information.
-            if (('run' in data.grl) and
-                ('run' in data.exp)):
-                task = 'Selected only the experimental data that matches the '\
-                    'run information in the GRL for dataset "%s".'%(self.name)
+            if ('run' in data.grl) and ('run' in data.exp):
+                task = (
+                    'Select only the experimental data that matches the run '
+                    f'information in the GRL for dataset "{self.name}".')
                 with TaskTimer(tl, task):
                     runs = np.unique(data.grl['run'])
                     mask = np.isin(data.exp['run'], runs)
-                    data.exp = data.exp[mask]
+
+                    if np.any(~mask):
+                        n_cut_runs = np.count_nonzero(~mask)
+                        self._logger.info(
+                            f'Cutting {n_cut_runs} runs from dataset '
+                            f'{self.name} due to GRL run information.')
+                        data.exp = data.exp[mask]
 
             # Select based on detector on-time information.
-            if (('start' in data.grl) and
-                ('stop' in data.grl) and
-                ('time' in data.exp)):
-                task = 'Selected only the experimental data that matches the '\
-                    'detector\'s on-time information in the GRL for dataset '\
-                    '"%s".'%(self.name)
+            if ('start' in data.grl) and\
+               ('stop' in data.grl) and\
+               ('time' in data.exp):
+                task = (
+                    'Select only the experimental data that matches the '
+                    'detector\'s on-time information in the GRL for dataset '
+                    f'"{self.name}".')
                 with TaskTimer(tl, task):
                     mask = np.zeros((len(data.exp),), dtype=np.bool_)
                     for (start, stop) in zip(data.grl['start'],
                                              data.grl['stop']):
                         mask |= (
                             (data.exp['time'] >= start) &
                             (data.exp['time'] <= stop)
@@ -360,43 +398,53 @@
                         self._logger.info(
                             f'Cutting {n_cut_evts} events from dataset '
                             f'{self.name} due to GRL on-time window '
                             'information.')
                         data.exp = data.exp[mask]
 
 
-class I3DatasetData(DatasetData):
+class I3DatasetData(
+        DatasetData,
+):
     """The class provides the container for the loaded experimental and
     monto-carlo data of a data set. It's the IceCube specific class that also
     holds the good-run-list (GRL) data.
     """
-    def __init__(self, data, data_grl):
+    def __init__(
+            self,
+            data,
+            data_grl,
+    ):
         """Constructs a new I3DatasetData instance.
 
         Parameters
         ----------
-        data : DatasetData instance
-            The DatasetData instance holding the experimental and monte-carlo
+        data : instance of DatasetData
+            The instance of DatasetData holding the experimental and monte-carlo
             data.
-        data_grl : DataFieldRecordArray instance | None
-            The DataFieldRecordArray instance holding the good-run-list data
+        data_grl : instance of DataFieldRecordArray | None
+            The instance of DataFieldRecordArray holding the good-run-list data
             of the dataset. This can be None, if no GRL data is available.
         """
-        super(I3DatasetData, self).__init__(
-            data._exp, data._mc, data._livetime)
+        super().__init__(
+            data_exp=data._exp,
+            data_mc=data._mc,
+            livetime=data._livetime)
 
         self.grl = data_grl
 
     @property
     def grl(self):
         """The DataFieldRecordArray instance holding the good-run-list (GRL)
         data of the IceCube data set. It is None, if there is no GRL data
         available for this IceCube data set.
         """
         return self._grl
+
     @grl.setter
     def grl(self, data):
-        if(data is not None):
-            if(not isinstance(data, DataFieldRecordArray)):
-                raise TypeError('The grl property must be an instance of '
+        if data is not None:
+            if not isinstance(data, DataFieldRecordArray):
+                raise TypeError(
+                    'The grl property must be an instance of '
                     'DataFieldRecordArray!')
         self._grl = data
```

### Comparing `skyllh-23.1.1/skyllh/i3/detsigyield.py` & `skyllh-23.2.0/skyllh/core/interpolate.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,680 +1,622 @@
 # -*- coding: utf-8 -*-
 
-"""This module contains classes for IceCube specific detector signal yields,
-for a variation of source model and flux model combinations.
+"""This module provides functionality for interpolation.
 """
 
 import abc
 import numpy as np
 
-import scipy.interpolate
-
-from skyllh.core import multiproc
-from skyllh.core.py import issequenceof
-from skyllh.core.binning import BinningDefinition
-from skyllh.core.parameters import ParameterGrid
-from skyllh.core.detsigyield import (
-    DetSigYield,
-    DetSigYieldImplMethod,
-    get_integrated_livetime_in_days
+from skyllh.core.parameters import (
+    ParameterGrid,
+    ParameterGridSet
 )
-from skyllh.core.livetime import Livetime
-from skyllh.physics.source import PointLikeSource
-from skyllh.physics.flux import (
-    FluxModel,
-    PowerLawFlux,
-    get_conversion_factor_to_internal_flux_unit
+from skyllh.core.py import (
+    classname,
 )
 
 
-class I3DetSigYield(DetSigYield, metaclass=abc.ABCMeta):
-    """Abstract base class for all IceCube specific detector signal yield
-    classes. It assumes that sin(dec) binning is required for calculating the
-    detector effective area and hence the detector signal yield.
+class GridManifoldInterpolationMethod(
+        object,
+        metaclass=abc.ABCMeta):
+    """This is an abstract base class for implementing a method to interpolate
+    a manifold that is defined on a grid of parameter values. In general the
+    number of parameters can be arbitrary and hence the manifold's
+    dimensionality can be arbitrary, too. However, in practice the interpolation
+    on a multi-dimensional manifold can be rather difficult.
+    Nevertheless, we provide this interface to allow for manifold grids with
+    different dimensionality.
     """
 
-    def __init__(self, implmethod, dataset, fluxmodel, livetime, sin_dec_binning):
-        """Constructor of the IceCube specific detector signal yield base
-        class.
+    def __init__(self, func, param_grid_set, **kwargs):
+        """Constructor for a GridManifoldInterpolationMethod object.
+        It must be called by the derived class.
+
+        Parameters
+        ----------
+        func : callable R^D -> R
+            The function that takes D parameter grid values as input and returns
+            the value of the D-dimensional manifold at this point for each given
+            trial event and source.
+            The call signature of func must be:
+
+                ``__call__(tdm, eventdata, gridparams_recarray, n_values)``
+
+            The arguments are as follows:
+
+                tdm : instance of TrialDataManager
+                    The TrialDataManager instance holding the trial event data.
+                eventdata : instance of numpy ndarray
+                    A two-dimensional (N_events,V)-shaped numpy ndarray holding
+                    the event data, where N_events is the number of trial
+                    events, and V the dimensionality of the event data.
+                gridparams_recarray : instance of numpy record ndarray
+                    The numpy record ndarray of length ``len(src_idxs)`` with
+                    the D parameter names and values on the grid for all
+                    sources.
+                n_values : int
+                    The length of the output numpy ndarray of shape (n_values,).
+
+            The return value of ``func`` should be the (n_values,)-shaped
+            one-dimensional ndarray holding the values for each set of parameter
+            values of the sources given via the ``gridparams_recarray``.
+            The length of the array, i.e. n_values, depends on the
+            ``src_evt_idx`` property of the TrialDataManager. In the worst case
+            n_values is N_sources * N_events.
+        param_grid_set : instance of ParameterGrid | instance of ParameterGridSet
+            The set of D parameter grids. This defines the grid of the
+            manifold.
         """
-        super(I3DetSigYield, self).__init__(implmethod, dataset, fluxmodel, livetime)
+        super().__init__(**kwargs)
 
-        self.sin_dec_binning = sin_dec_binning
+        self.func = func
+        self.param_grid_set = param_grid_set
 
     @property
-    def sin_dec_binning(self):
-        """The BinningDefinition instance defining the sin(dec) binning
-        definition.
-        """
-        return self._sin_dec_binning
-    @sin_dec_binning.setter
-    def sin_dec_binning(self, bd):
-        if(not isinstance(bd, BinningDefinition)):
-            raise TypeError('The sin_dec_binning property must be an instance '
-                'of BinningDefinition!')
-        self._sin_dec_binning = bd
-
-
-class I3DetSigYieldImplMethod(DetSigYieldImplMethod, metaclass=abc.ABCMeta):
-    """Abstract base class for an IceCube specific detector signal yield
-    implementation method class.
-    """
-
-    def __init__(self, sin_dec_binning=None, **kwargs):
-        """Constructor of the IceCube specific detector signal yield
-        implementation base class.
-
-        Parameters
-        ----------
-        sin_dec_binning : BinningDefinition instance
-            The instance of BinningDefinition defining the binning of sin(dec).
+    def func(self):
+        """The R^d -> R manifold function.
         """
-        super(I3DetSigYieldImplMethod, self).__init__(**kwargs)
+        return self._func
 
-        self.sin_dec_binning = sin_dec_binning
+    @func.setter
+    def func(self, f):
+        if not callable(f):
+            raise TypeError(
+                'The func property must be a callable object!')
+        self._func = f
 
     @property
-    def sin_dec_binning(self):
-        """The BinningDefinition instance for the sin(dec) binning that should
-        be used for computing the sin(dec) dependency of the detector signal
-        yield. If None, the binning is supposed to be taken from the Dataset's
-        binning definitions.
-        """
-        return self._sin_dec_binning
-    @sin_dec_binning.setter
-    def sin_dec_binning(self, binning):
-        if((binning is not None) and
-           (not isinstance(binning, BinningDefinition))):
-            raise TypeError('The sin_dec_binning property must be None, or '
-                'an instance of BinningDefinition!')
-        self._sin_dec_binning = binning
-
-    def get_sin_dec_binning(self, dataset):
-        """Gets the sin(dec) binning definition either as setting from this
-        detector signal yield implementation method itself, or from the
-        given dataset.
-        """
-        sin_dec_binning = self.sin_dec_binning
-        if(sin_dec_binning is None):
-            if(not dataset.has_binning_definition('sin_dec')):
-                raise KeyError('No binning definition named "sin_dec" is '
-                    'defined in the dataset and no user defined binning '
-                    'definition was provided to this detector signal yield '
-                    'implementation method!')
-            sin_dec_binning = dataset.get_binning_definition('sin_dec')
-        return sin_dec_binning
-
-
-class PointLikeSourceI3DetSigYieldImplMethod(
-        I3DetSigYieldImplMethod, metaclass=abc.ABCMeta):
-    """Abstract base class for all IceCube specific detector signal yield
-    implementation methods for a point-like source. All IceCube detector signal
-    yield implementation methods require a sinDec binning definition for
-    the effective area. By default it is taken from the binning definitios
-    stored in the dataset, but a user-defined sinDec binning can be specified
-    if needed.
-    """
+    def param_grid_set(self):
+        """The ParameterGridSet instance defining the set of d parameter grids.
+        This defines the grid of the manifold.
+        """
+        return self._param_grid_set
+
+    @param_grid_set.setter
+    def param_grid_set(self, obj):
+        if isinstance(obj, ParameterGrid):
+            obj = ParameterGridSet([obj])
+        elif not isinstance(obj, ParameterGridSet):
+            raise TypeError(
+                'The param_grid_set property must be an instance '
+                'of ParameterGrid or ParameterGridSet!')
+        self._param_grid_set = obj
 
-    def __init__(self, sin_dec_binning=None, **kwargs):
-        """Initializes a new detector signal yield implementation method
-        object.
+    @property
+    def ndim(self):
+        """(read-only) The dimensionality of the manifold.
+        """
+        return len(self._param_grid_set)
 
-        Parameters
-        ----------
-        sin_dec_binning : BinningDefinition | None
-            The BinningDefinition instance defining the sin(dec) binning that
-            should be used to compute the sin(dec) dependency of the detector
-            effective area. If set to None, the binning will be taken from the
-            Dataset binning definitions.
-        """
-        super(PointLikeSourceI3DetSigYieldImplMethod, self).__init__(
-            sin_dec_binning, **kwargs)
-
-        # Define the supported source models.
-        self.supported_sourcemodels = (PointLikeSource,)
-
-    def source_to_array(self, sources):
-        """Converts the sequence of PointLikeSource sources into a numpy record
-        array holding the spatial information of the sources needed for the
-        detector signal yield calculation.
+    @abc.abstractmethod
+    def __call__(self, tdm, eventdata, params_recarray):
+        """Retrieves the interpolated value of the manifold at the D-dimensional
+        point ``params_recarray`` for all given events and sources, along with
+        the D gradients, i.e. partial derivatives.
 
         Parameters
         ----------
-        sources : SourceModel | sequence of SourceModel
-            The source model containing the spatial information of the source.
+        tdm : instance of TrialDataManager
+            The TrialDataManager instance holding the trial data.
+        eventdata : numpy ndarray
+            The 2D (N_events,V)-shaped numpy ndarray holding the event data,
+            where N_events is the number of trial events, and V the
+            dimensionality of the event data.
+        params_recarray : instance of numpy record ndarray
+            The numpy record ndarray holding the N_sources set of parameter
+            names and values, that define the point (for each source) on the
+            manifold for which the value should get calculated for each event.
 
         Returns
         -------
-        arr : numpy record ndarray
-            The generated numpy record ndarray holding the spatial information
-            for each source.
-        """
-        if(isinstance(sources, PointLikeSource)):
-            sources = [ sources ]
-        if(not issequenceof(sources, PointLikeSource)):
-            raise TypeError('The source argument must be an instance of PointLikeSource!')
-
-        arr = np.empty((len(sources),), dtype=[('dec', np.float64)])
-        for (i, src) in enumerate(sources):
-            arr['dec'][i] = src.dec
-
-        return arr
-
-
-class FixedFluxPointLikeSourceI3DetSigYield(I3DetSigYield):
-    """The detector signal yield class for the
-    FixedFluxPointLikeSourceI3DetSigYieldImplMethod detector signal yield
-    implementation method.
+        values : ndarray of float
+            The (N,)-shaped numpy ndarray holding the interpolated manifold
+            values for the given events and sources.
+        grads : ndarray of float
+            The (D,N)-shaped numpy ndarray holding the D manifold gradients for
+            the N given values, where D is the number of parameters.
+            The order of the D parameters is defined by the ParameterGridSet
+            that has been provided at construction time of this interpolation
+            method object.
+        """
+        pass
+
+
+class NullGridManifoldInterpolationMethod(
+        GridManifoldInterpolationMethod):
+    """This grid manifold interpolation method performes no interpolation. When
+    the
+    :meth:`~skyllh.core.interpolate.NullGridManifoldInterpolationMethod.__call__`
+    method is called, it rounds the parameter values to their nearest grid
+    point values. All gradients are set to zero.
     """
-    def __init__(self, implmethod, dataset, fluxmodel, livetime, sin_dec_binning, log_spl_sinDec):
-        """Constructs an IceCube detector signal yield instance for a
-        point-like source with a fixed flux.
+    def __init__(
+            self,
+            func,
+            param_grid_set,
+            **kwargs):
+        """Creates a new NullGridManifoldInterpolationMethod instance.
 
         Parameters
         ----------
-        implmethod : FixedFluxPointLikeSourceI3DetSigYieldImplMethod instance
-            The instance of the detector signal yield implementation
-            method.
-        dataset : Dataset instance
-            The instance of Dataset holding the monte-carlo data this detector
-            signal yield is made for.
-        fluxmodel : FluxModel instance
-            The instance of FluxModel with fixed parameters this detector signal
-            yield is made for.
-        livetime : float | Livetime instance
-            The livetime in days or an instance of Livetime.
-        sin_dec_binning : BinningDefinition instance
-            The binning definition for sin(dec).
-        log_spl_sinDec : scipy.interpolate.InterpolatedUnivariateSpline
-            The spline instance representing the log value of the detector
-            signal yield as a function of sin(dec).
-        """
-        if(not isinstance(implmethod, FixedFluxPointLikeSourceI3DetSigYieldImplMethod)):
-            raise TypeError('The implmethod argument must be an instance of '
-                'FixedFluxPointLikeSourceI3DetSigYieldImplMethod!')
-
-        super(FixedFluxPointLikeSourceI3DetSigYield, self).__init__(
-            implmethod, dataset, fluxmodel, livetime, sin_dec_binning)
-
-        self.log_spl_sinDec = log_spl_sinDec
-
-    @property
-    def log_spl_sinDec(self):
-        """The :class:`scipy.interpolate.InterpolatedUnivariateSpline` instance
-        representing the spline for the log value of the detector signal
-        yield as a function of sin(dec).
-        """
-        return self._log_spl_sinDec
-    @log_spl_sinDec.setter
-    def log_spl_sinDec(self, spl):
-        if(not isinstance(spl, scipy.interpolate.InterpolatedUnivariateSpline)):
-            raise TypeError('The log_spl_sinDec property must be an instance '
-                'of scipy.interpolate.InterpolatedUnivariateSpline!')
-        self._log_spl_sinDec = spl
-
-    def __call__(self, src, src_flux_params=None):
-        """Retrieves the detector signal yield for the list of given sources.
+        func : callable R^d -> R
+            The function that takes d parameter grid values as input and returns
+            the value of the d-dimensional manifold at this point for each given
+            trial event and source.
+            See the documentation of the
+            :class:`~skyllh.core.interpolate.GridManifoldInterpolationMethod`
+            class for more details.
+        param_grid_set : instance of ParameterGrid | instance of ParameterGridSet
+            The set of d parameter grids. This defines the grid of the
+            manifold.
+        """
+        super().__init__(
+            func=func,
+            param_grid_set=param_grid_set,
+            **kwargs)
+
+    def __call__(self, tdm, eventdata, params_recarray):
+        """Calculates the non-interpolated manifold value and its gradient
+        (zero) for each given event and source at the points given by
+        ``params_recarray``.
 
         Parameters
         ----------
-        src : numpy record ndarray
-            The numpy record ndarray with the field ``dec`` holding the
-            declination of the source.
-        src_flux_params : None
-            Unused interface argument, because this implementation does not
-            depend on any source flux fit parameters.
+        tdm : instance of TrialDataManager
+            The TrialDataManager instance holding the trial data.
+        eventdata : instance of numpy.ndarray
+            The (N_events,V)-shaped numpy ndarray holding the event data,
+            where N_events is the number of events, and V the dimensionality of
+            the event data.
+        params_recarray : instance of numpy.ndarray
+            The structured numpy ndarray of length N_sources holding the
+            parameter names and values of the sources, defining the point on the
+            manifold for which the values should get calculated.
 
         Returns
         -------
-        values : numpy 1d ndarray
-            The array with the detector signal yield for each source.
-        grads : None
-            Because with this implementation the detector signal yield
-            does not depend on any fit parameters. So there are no gradients
-            and None is returned.
-        """
-        src_dec = np.atleast_1d(src['dec'])
-
-        # Create results array.
-        values = np.zeros_like(src_dec, dtype=np.float64)
-
-        # Create mask for all source declinations which are inside the
-        # declination range.
-        mask = (np.sin(src_dec) >= self._sin_dec_binning.lower_edge)\
-              &(np.sin(src_dec) <= self._sin_dec_binning.upper_edge)
-
-        values[mask] = np.exp(self._log_spl_sinDec(np.sin(src_dec[mask])))
-
-        return (values, None)
-
-
-class FixedFluxPointLikeSourceI3DetSigYieldImplMethod(
-    PointLikeSourceI3DetSigYieldImplMethod):
-    """This detector signal yield implementation method constructs a
-    detector signal yield for a fixed flux model, assuming a point-like
-    source. This means that the detector signal yield does not depend on
-    any source flux parameters, hence it is only dependent on the detector
-    effective area.
-    It constructs a one-dimensional spline function in sin(dec), using a
-    :class:`scipy.interpolate.InterpolatedUnivariateSpline`.
-
-    This detector signal yield implementation method works with all flux
-    models.
-
-    It is tailored to the IceCube detector at the South Pole, where the
-    effective area depends soley on the zenith angle, and hence on the
-    declination, of the source.
+        values : instance of numpy.ndarray
+            The (N,)-shaped numpy ndarray holding the interpolated manifold
+            values for the given events and sources.
+        grads : instance of numpy.ndarray
+            The (D,N)-shaped ndarray of float holding the D manifold gradients
+            for the N values, where D is the number of parameters of the
+            manifold.
+            By definition, all gradients are zero.
+        """
+        # Round the given parameter values to their nearest grid values.
+        gridparams_recarray_dtype = [
+            (p_grid.name, np.float64)
+            for p_grid in self._param_grid_set
+        ]
+
+        gridparams_recarray = np.empty(
+            params_recarray.shape,
+            dtype=gridparams_recarray_dtype)
+
+        for p_grid in self._param_grid_set:
+            pname = p_grid.name
+            pvalues = params_recarray[pname]
+            gridparams_recarray[pname] = p_grid.round_to_nearest_grid_point(
+                pvalues)
+
+        values = self._func(
+            tdm=tdm,
+            eventdata=eventdata,
+            gridparams_recarray=gridparams_recarray,
+            n_values=tdm.get_n_values())
+
+        grads = np.zeros(
+            (len(self.param_grid_set), len(values)),
+            dtype=np.float64)
+
+        return (values, grads)
+
+
+class Linear1DGridManifoldInterpolationMethod(
+        GridManifoldInterpolationMethod):
+    """This grid manifold interpolation method interpolates the 1-dimensional
+    grid manifold using a line.
     """
-    def __init__(self, sin_dec_binning=None, spline_order_sinDec=2, **kwargs):
-        """Creates a new IceCube detector signal yield implementation
-        method object for a fixed flux model. It requires a sinDec binning
-        definition to compute the sin(dec) dependency of the detector effective
-        area. The construct class method of this implementation method will
-        create a spline function of a given order in logarithmic space of the
-        effective area.
+    def __init__(
+            self,
+            func,
+            param_grid_set,
+            **kwargs):
+        """Creates a new Linear1DGridManifoldInterpolationMethod instance.
 
         Parameters
         ----------
-        sin_dec_binning : BinningDefinition | None
-            The BinningDefinition instance which defines the sin(dec) binning.
-            If set to None, the binning will be taken from the Dataset binning
-            definitions.
-        spline_order_sinDec : int
-            The order of the spline function for the logarithmic values of the
-            detector signal yield along the sin(dec) axis.
-            The default is 2.
-        """
-        super(FixedFluxPointLikeSourceI3DetSigYieldImplMethod, self).__init__(
-            sin_dec_binning, **kwargs)
-
-        self.supported_fluxmodels = (FluxModel,)
-
-        self.spline_order_sinDec = spline_order_sinDec
-
-    @property
-    def spline_order_sinDec(self):
-        """The order (int) of the logarithmic spline function, that splines the
-        detector signal yield, along the sin(dec) axis.
-        """
-        return self._spline_order_sinDec
-    @spline_order_sinDec.setter
-    def spline_order_sinDec(self, order):
-        if(not isinstance(order, int)):
-            raise TypeError('The spline_order_sinDec property must be of '
-                'type int!')
-        self._spline_order_sinDec = order
-
-    def construct_detsigyield(self, dataset, data, fluxmodel, livetime, ppbar=None):
-        """Constructs a detector signal yield log spline function for the
-        given fixed flux model.
+        func : callable R -> R
+            The function that takes the parameter grid value as input and
+            returns the value of the 1-dimensional manifold at this point for
+            each given source and trial event.
+            See the documentation of the
+            :class:`~skyllh.core.interpolate.GridManifoldInterpolationMethod`
+            class for more details.
+        param_grid_set : instance of ParameterGrid | instance of ParameterGridSet
+            The one parameter grid. This defines the grid of the manifold.
+        """
+        super().__init__(
+            func=func,
+            param_grid_set=param_grid_set,
+            **kwargs)
+
+        if len(self._param_grid_set) != 1:
+            raise ValueError(
+                f'The {classname(self)} class supports only 1D grid manifolds. '
+                'The param_grid_set argument must contain 1 ParameterGrid '
+                f'instance! Currently it has {len(self._param_grid_set)}!')
+        self._p_grid = self._param_grid_set[0]
+
+        # Create a cache for the line parameterization for the last
+        # manifold grid point for the different events.
+        self._cache = self._create_cache(
+            trial_data_state_id=None,
+            x0=None,
+            m=None,
+            b=None)
+
+    def _create_cache(self, trial_data_state_id, x0, m, b):
+        """Creates a cache for the line parameterization for the last manifold
+        grid point for the nevents different events.
 
         Parameters
         ----------
-        dataset : Dataset instance
-            The Dataset instance holding meta information about the data.
-        data : DatasetData instance
-            The DatasetData instance holding the monte-carlo event data.
-            The numpy record ndarray holding the monte-carlo event data must
-            contain the following data fields:
-
-            - 'true_dec' : float
-                The true declination of the data event.
-            - 'true_energy' : float
-                The true energy value of the data event.
-            - 'mcweight' : float
-                The monte-carlo weight of the data event in the unit
-                GeV cm^2 sr.
-
-        fluxmodel : FluxModel
-            The flux model instance. Must be an instance of FluxModel.
-        livetime : float | Livetime
-            The live-time in days to use for the detector signal yield.
-        ppbar : ProgressBar instance | None
-            The instance of ProgressBar of the optional parent progress bar.
+        trial_data_state_id : int | None
+            The trial data state id of the TrialDataManager.
+        x0 : instance of ndarray | None
+            The (N_sources,)-shaped numpy ndarray holding the parameter grid
+            value of the lower point of the grid manifold for each source used
+            to estimate the line.
+        m : instance of ndarray | None
+            The (N_values,)-shaped numpy ndarray holding the slope of the line
+            for each trial event and source.
+        b : instance of ndarray | None
+            The (N_values,)-shaped numpy ndarray holding the offset coefficient
+            of the line for each trial event and source.
+        """
+        cache = {
+            'trial_data_state_id': trial_data_state_id,
+            'x0': x0,
+            'm': m,
+            'b': b
+        }
+
+        return cache
+
+    def _is_cached(self, trial_data_state_id, x0):
+        """Checks if the given line parametrization are already cached for the
+        given x0 values.
 
         Returns
         -------
-        detsigyield : FixedFluxPointLikeSourceI3DetSigYield instance
-            The DetSigYield instance for point-like source with a fixed flux.
-        """
-        # Check data types of the input arguments.
-        super(FixedFluxPointLikeSourceI3DetSigYieldImplMethod, self).construct_detsigyield(
-            dataset, data, fluxmodel, livetime)
-
-        # Get integrated live-time in days.
-        livetime_days = get_integrated_livetime_in_days(livetime)
-
-        # Get the sin(dec) binning definition either as setting from this
-        # implementation method, or from the dataset.
-        sin_dec_binning = self.get_sin_dec_binning(dataset)
-
-        # Calculate conversion factor from the flux model unit into the internal
-        # flux unit GeV^-1 cm^-2 s^-1.
-        toGeVcm2s = get_conversion_factor_to_internal_flux_unit(fluxmodel)
-
-        # Calculate the detector signal yield contribution of each event.
-        # The unit of mcweight is assumed to be GeV cm^2 sr.
-        w = data.mc["mcweight"] * fluxmodel(data.mc["true_energy"])*toGeVcm2s * livetime_days * 86400.
-
-        # Create a histogram along sin(true_dec).
-        (h, bins) = np.histogram(np.sin(data.mc["true_dec"]),
-                                 weights = w,
-                                 bins = sin_dec_binning.binedges,
-                                 density = False)
-
-        # Normalize by solid angle of each bin which is
-        # 2*\pi*(\Delta sin(\delta)).
-        h /= (2.*np.pi * np.diff(sin_dec_binning.binedges))
-
-        # Create spline in ln(h) at the histogram's bin centers.
-        log_spl_sinDec = scipy.interpolate.InterpolatedUnivariateSpline(
-            sin_dec_binning.bincenters, np.log(h), k=self.spline_order_sinDec)
-
-        detsigyield = FixedFluxPointLikeSourceI3DetSigYield(
-            self, dataset, fluxmodel, livetime, sin_dec_binning, log_spl_sinDec)
-
-        return detsigyield
-
-
-class PowerLawFluxPointLikeSourceI3DetSigYield(I3DetSigYield):
-    """The detector signal yield class for the
-    PowerLawFluxPointLikeSourceI3DetSigYieldImplMethod detector signal yield
-    implementation method.
-    """
-    def __init__(self, implmethod, dataset, fluxmodel, livetime,
-                 sin_dec_binning, log_spl_sinDec_gamma):
-        """Constructs the detector signal yield instance.
-
-        """
-        if(not isinstance(implmethod, PowerLawFluxPointLikeSourceI3DetSigYieldImplMethod)):
-            raise TypeError('The implmethod argument must be an instance of '
-                'PowerLawFluxPointLikeSourceI3DetSigYieldImplMethod!')
-
-        super(PowerLawFluxPointLikeSourceI3DetSigYield, self).__init__(
-            implmethod, dataset, fluxmodel, livetime, sin_dec_binning)
-
-        self.log_spl_sinDec_gamma = log_spl_sinDec_gamma
-
-    @property
-    def log_spl_sinDec_gamma(self):
-        """The :class:`scipy.interpolate.RectBivariateSpline` instance
-        representing the spline for the log value of the detector signal
-        yield as a function of sin(dec) and gamma.
-        """
-        return self._log_spl_sinDec_gamma
-    @log_spl_sinDec_gamma.setter
-    def log_spl_sinDec_gamma(self, spl):
-        if(not isinstance(spl, scipy.interpolate.RectBivariateSpline)):
-            raise TypeError('The log_spl_sinDec_gamma property must be an '
-                'instance of scipy.interpolate.RectBivariateSpline!')
-        self._log_spl_sinDec_gamma = spl
-
-    def __call__(self, src, src_flux_params):
-        """Retrieves the detector signal yield for the given list of
-        sources and their flux parameters.
+        check : bool
+            ``True`` if the line parametrization for x0 is already cached,
+            ``False`` otherwise.
+        """
+        self__cache = self._cache
+        if (self__cache['trial_data_state_id'] is not None) and\
+           (self__cache['trial_data_state_id'] == trial_data_state_id) and\
+           (np.all(np.isclose(self__cache['x0'], x0))):
+            return True
+
+        return False
+
+    def __call__(self, tdm, eventdata, params_recarray):
+        """Calculates the interpolated manifold value and its gradient for each
+        given source and trial event at the point ``params_recarray``.
 
         Parameters
         ----------
-        src : numpy record ndarray
-            The numpy record ndarray with the field ``dec`` holding the
-            declination of the source.
-        src_flux_params : (N_sources,)-shaped numpy record ndarray
-            The numpy record ndarray containing the flux parameter ``gamma`` for
-            the sources. ``gamma`` can be different for the different sources.
+        tdm : instance of TrialDataManager
+            The TrialDataManager instance holding the trial data.
+        eventdata : instance of numpy ndarray
+            The (N_events,V)-shaped numpy ndarray holding the event data,
+            where N_events is the number of events, and V the dimensionality of
+            the event data.
+        params_recarray : numpy record ndarray
+            The numpy record ndarray of length N_sources holding the parameter
+            names and values for each source, defining the point on the manifold
+            for which the value should get calculated.
+            This record ndarray can be of length 1. In that case the single set
+            of parameters is used for all sources.
 
         Returns
         -------
-        values : numpy (N_sources,)-shaped 1D ndarray
-            The array with the detector signal yield for each source.
-        grads : numpy (N_sources,N_fitparams)-shaped 2D ndarray
-            The array containing the gradient values for each source and fit
-            parameter. Since, this implementation depends on only one fit
-            parameter, i.e. gamma, the array is (N_sources,1)-shaped.
-        """
-        src_dec = np.atleast_1d(src['dec'])
-        if src_flux_params is None:
-            # Gamma is not a fit parameter. So we take it from the
-            # initial flux model.
-            src_gamma = np.array([self.fluxmodel.gamma], dtype=np.double)
-        else:
-            src_gamma = src_flux_params['gamma']
-
-        # Create results array.
-        values = np.zeros_like(src_dec, dtype=np.float64)
-        grads = np.zeros_like(src_dec, dtype=np.float64)
-
-        # Calculate the detector signal yield only for the sources for
-        # which we actually have detector acceptance. For the other sources,
-        # the detector signal yield is zero.
-        mask = (np.sin(src_dec) >= self._sin_dec_binning.lower_edge)\
-              &(np.sin(src_dec) <= self._sin_dec_binning.upper_edge)
-
-        if len(src_gamma) == len(src_dec):
-            src_gamma = src_gamma[mask]
-        else:
-            src_gamma = src_gamma[0]
-
-        values[mask] = np.exp(self._log_spl_sinDec_gamma(
-            np.sin(src_dec[mask]), src_gamma, grid=False))
-        grads[mask] = values[mask] * self._log_spl_sinDec_gamma(
-            np.sin(src_dec[mask]), src_gamma, grid=False, dy=1)
-
-        return (values, np.atleast_2d(grads))
-
-
-class PowerLawFluxPointLikeSourceI3DetSigYieldImplMethod(
-    PointLikeSourceI3DetSigYieldImplMethod, multiproc.IsParallelizable):
-    """This detector signal yield implementation method constructs a
-    detector signal yield for a variable power law flux model, which has
-    the spectral index gamma as fit parameter, assuming a point-like source.
-    It constructs a two-dimensional spline function in sin(dec) and gamma, using
-    a :class:`scipy.interpolate.RectBivariateSpline`. Hence, the detector signal yield
-    can vary with the declination and the spectral index, gamma, of the source.
-
-    This detector signal yield implementation method works with a
-    PowerLawFlux flux model.
-
-    It is tailored to the IceCube detector at the South Pole, where the
-    effective area depends soley on the zenith angle, and hence on the
-    declination, of the source.
+        values : instance of numpy ndarray
+            The (N_values,)-shaped numpy ndarray of float holding the
+            interpolated manifold values for all sources and trial events.
+        grads :  ndarray of float
+            The (D,N_values)-shaped numpy ndarray of float holding the D
+            manifold gradients for the N_values values for all sources and trial
+            events, where D is the number of interpolation parameters.
+        """
+        xname = self._p_grid.name
+
+        x = params_recarray[xname]
+
+        # Determine the nearest grid point that is lower than x and use that as
+        # x0.
+        x0 = self._p_grid.round_to_lower_grid_point(x)
+
+        # Check if the line parametrization for x0 is already cached.
+        if self._is_cached(tdm.trial_data_state_id, x0):
+            m = self._cache['m']
+            b = self._cache['b']
+
+            (x,) = tdm.broadcast_sources_arrays_to_values_arrays((x,))
+
+            values = m*x + b
+
+            return (values, np.atleast_2d(m))
+
+        # The line parametrization is not cached.
+        # Calculate the line parametrization for all the given events.
+        self__func = self._func
+
+        x1 = self._p_grid.round_to_upper_grid_point(x)
+
+        n_values = tdm.get_n_values()
+
+        values = np.empty((n_values,), dtype=np.float64)
+        m = np.empty((n_values,), dtype=np.float64)
+
+        gridparams_recarray = np.array(
+            x0,
+            dtype=[(xname, np.float64)])
+        M0 = self__func(
+            tdm=tdm,
+            eventdata=eventdata,
+            gridparams_recarray=gridparams_recarray,
+            n_values=n_values)
+
+        gridparams_recarray = np.array(
+            x1,
+            dtype=[(xname, np.float64)])
+        M1 = self__func(
+            tdm=tdm,
+            eventdata=eventdata,
+            gridparams_recarray=gridparams_recarray,
+            n_values=n_values)
+
+        # Broadcast x0 and x1 to the values array.
+        (x, v_x0, v_x1) = tdm.broadcast_sources_arrays_to_values_arrays(
+            (x, x0, x1))
+
+        m = (M1 - M0) / (v_x1 - v_x0)
+        b = M0 - m*v_x0
+
+        # Cache the line parametrization.
+        self._cache = self._create_cache(
+            trial_data_state_id=tdm.trial_data_state_id,
+            x0=x0,
+            m=m,
+            b=b)
+
+        # Calculate the interpolated manifold values. The gradient is m.
+        values = m*x + b
+
+        return (values, np.atleast_2d(m))
+
+
+class Parabola1DGridManifoldInterpolationMethod(
+        GridManifoldInterpolationMethod):
+    """This grid manifold interpolation method interpolates the 1-dimensional
+    grid manifold using a parabola.
     """
     def __init__(
-            self, gamma_grid, sin_dec_binning=None, spline_order_sinDec=2,
-            spline_order_gamma=2, ncpu=None):
-        """Creates a new IceCube detector signal yield implementation
-        method object for a power law flux model. It requires a sinDec binning
-        definition to compute the sin(dec) dependency of the detector effective
-        area, and a gamma parameter grid to compute the gamma dependency of the
-        detector signal yield.
+            self,
+            func,
+            param_grid_set,
+            **kwargs):
+        """Creates a new Parabola1DGridManifoldInterpolationMethod instance.
 
         Parameters
         ----------
-        gamma_grid : ParameterGrid instance
-            The ParameterGrid instance which defines the grid of gamma values.
-        sin_dec_binning : BinningDefinition | None
-            The BinningDefinition instance which defines the sin(dec) binning.
-            If set to None, the sin(dec) binning will be taken from the
-            dataset's binning definitions.
-        spline_order_sinDec : int
-            The order of the spline function for the logarithmic values of the
-            detector signal yield along the sin(dec) axis.
-            The default is 2.
-        spline_order_gamma : int
-            The order of the spline function for the logarithmic values of the
-            detector signal yield along the gamma axis.
-            The default is 2.
-        ncpu : int | None
-            The number of CPUs to utilize. Global setting will take place if
-            not specified, i.e. set to None.
-        """
-        super(PowerLawFluxPointLikeSourceI3DetSigYieldImplMethod, self).__init__(
-            sin_dec_binning, ncpu=ncpu)
-
-        self.supported_fluxmodels = (PowerLawFlux,)
-
-        self.gamma_grid = gamma_grid
-        self.spline_order_sinDec = spline_order_sinDec
-        self.spline_order_gamma = spline_order_gamma
-
-    @property
-    def gamma_grid(self):
-        """The ParameterGrid instance for the gamma grid that should be used for
-        computing the gamma dependency of the detector signal yield.
-        """
-        return self._gamma_grid
-    @gamma_grid.setter
-    def gamma_grid(self, grid):
-        if(not isinstance(grid, ParameterGrid)):
-            raise TypeError('The gamma_grid property must be an instance of '
-                'ParameterGrid!')
-        self._gamma_grid = grid
+        func : callable R -> R
+            The function that takes the parameter grid value as input and
+            returns the value of the 1-dimensional manifold at this point for
+            each given source and trial event.
+            See the documentation of the
+            :class:`~skyllh.core.interpolate.GridManifoldInterpolationMethod`
+            class for more details.
+        param_grid_set : instance of ParameterGrid | instance of ParameterGridSet
+            The one parameter grid. This defines the grid of the manifold.
+        """
+        super().__init__(
+            func=func,
+            param_grid_set=param_grid_set,
+            **kwargs)
+
+        if len(self._param_grid_set) != 1:
+            raise ValueError(
+                f'The {classname(self)} class supports only 1D grid manifolds. '
+                'The param_grid_set argument must contain 1 ParameterGrid '
+                f'instance! Currently it has {len(self._param_grid_set)}!')
+        self._p_grid = self._param_grid_set[0]
+
+        # Create a cache for the parabola parameterization for the last
+        # manifold grid point for the different events.
+        self._cache = self._create_cache(
+            trial_data_state_id=None,
+            x1=None,
+            M1=None,
+            a=None,
+            b=None)
+
+    def _create_cache(
+            self,
+            trial_data_state_id,
+            x1,
+            M1,
+            a,
+            b):
+        """Creates a cache for the parabola parameterization for the last
+        manifold grid point for the nevents different events.
 
-    @property
-    def spline_order_sinDec(self):
-        """The order (int) of the logarithmic spline function, that splines the
-        detector signal yield, along the sin(dec) axis.
-        """
-        return self._spline_order_sinDec
-    @spline_order_sinDec.setter
-    def spline_order_sinDec(self, order):
-        if(not isinstance(order, int)):
-            raise TypeError('The spline_order_sinDec property must be of '
-                'type int!')
-        self._spline_order_sinDec = order
+        Parameters
+        ----------
+        trial_data_state_id : int | None
+            The trial data state ID of the TrialDataManager.
+        x1 : instance of numpy ndarray | None
+            The (N_sources,)-shaped numpy ndarray of float holding the parameter
+            grid value for the middle point of the grid manifold for all sources
+            used to estimate the parabola.
+        M1 : instance of numpy ndarray
+            The (N_values,)-shaped numpy ndarray of float holding the grid
+            manifold value for each source and trial event of the middle point
+            (x1,).
+        a : instance of numpy ndarray
+            The (N_values,)-shaped numpy ndarray of float holding the parabola
+            coefficient ``a`` for each source and trial event.
+        b : instance of numpy ndarray
+            The (N_values,)-shaped numpy ndarray of float holding the parabola
+            coefficient ``b`` for each source and trial event.
 
-    @property
-    def spline_order_gamma(self):
-        """The order (int) of the logarithmic spline function, that splines the
-        detector signal yield, along the gamma axis.
-        """
-        return self._spline_order_gamma
-    @spline_order_gamma.setter
-    def spline_order_gamma(self, order):
-        if(not isinstance(order, int)):
-            raise TypeError('The spline_order_gamma property must be of '
-                'type int!')
-        self._spline_order_gamma = order
-
-    def _get_signal_fitparam_names(self):
-        """The list of signal fit parameter names the detector signal yield
-        depends on.
-        """
-        return ['gamma']
-
-    def construct_detsigyield(
-            self, dataset, data, fluxmodel, livetime, ppbar=None):
-        """Constructs a detector signal yield 2-dimensional log spline
-        function for the given power law flux model with varying gamma values.
+        Returns
+        -------
+        cache : dict
+            The dictionary holding the cache data.
+        """
+        cache = {
+            'trial_data_state_id': trial_data_state_id,
+            'x1': x1,
+            'M1': M1,
+            'a': a,
+            'b': b
+        }
+
+        return cache
+
+    def _is_cached(self, trial_data_state_id, x1):
+        """Checks if the parabola parametrization is already cached for the
+        given x1 values.
+        """
+        self__cache = self._cache
+        if (self__cache['trial_data_state_id'] is not None) and\
+           (self__cache['trial_data_state_id'] == trial_data_state_id):
+            if np.any(np.not_equal(self__cache['x1'], x1)):
+                return False
+            return True
+
+        return False
+
+    def __call__(self, tdm, eventdata, params_recarray):
+        """Calculates the interpolated manifold value and its gradient for each
+        given source and trial event at the point ``params_recarray``.
 
         Parameters
         ----------
-        dataset : Dataset instance
-            The Dataset instance holding the sin(dec) binning definition.
-        data : DatasetData instance
-            The DatasetData instance holding the monte-carlo event data.
-            The numpy record array for the monte-carlo data of the dataset must
-            contain the following data fields:
-
-            - 'true_dec' : float
-                The true declination of the data event.
-            - 'mcweight' : float
-                The monte-carlo weight of the data event in the unit
-                GeV cm^2 sr.
-            - 'true_energy' : float
-                The true energy value of the data event.
-
-        fluxmodel : FluxModel
-            The flux model instance. Must be an instance of FluxModel.
-        livetime : float | Livetime instance
-            The live-time in days or an instance of Livetime to use for the
-            detector signal yield.
-        ppbar : ProgressBar instance | None
-            The instance of ProgressBar of the optional parent progress bar.
+        tdm : instance of TrialDataManager
+            The TrialDataManager instance holding the trial data.
+        eventdata : instance of numpy ndarray
+            The (N_events,V)-shaped numpy ndarray holding the event data,
+            where N_events is the number of events, and V the dimensionality of
+            the event data.
+        params_recarray : numpy record ndarray
+            The numpy record ndarray of length N_sources holding the parameter
+            names and values for each source, defining the point on the manifold
+            for which the value should get calculated.
+            This record ndarray can be of length 1. In that case the single set
+            of parameters is used for all sources.
 
         Returns
         -------
-        detsigyield : PowerLawFluxPointLikeSourceI3DetSigYield instance
-            The DetSigYield instance for a point-like source with a power law
-            flux with variable gamma parameter.
-        """
-        # Check for the correct data types of the input arguments.
-        super(PowerLawFluxPointLikeSourceI3DetSigYieldImplMethod, self).construct_detsigyield(
-            dataset, data, fluxmodel, livetime)
-
-        # Get integrated live-time in days.
-        livetime_days = get_integrated_livetime_in_days(livetime)
-
-        # Get the sin(dec) binning definition either as setting from this
-        # implementation method, or from the dataset.
-        sin_dec_binning = self.get_sin_dec_binning(dataset)
-
-        # Calculate conversion factor from the flux model unit into the internal
-        # flux unit GeV^-1 cm^-2 s^-1.
-        toGeVcm2s = get_conversion_factor_to_internal_flux_unit(fluxmodel)
-
-        # Define a function that creates a detector signal yield histogram
-        # along sin(dec) for a given flux model, i.e. for given spectral index,
-        # gamma.
-        def hist(data_sin_true_dec, data_true_energy, sin_dec_binning, weights, fluxmodel):
-            """Creates a histogram of the detector signal yield with the
-            given sin(dec) binning.
-
-            Parameters
-            ----------
-            data_sin_true_dec : 1d ndarray
-                The sin(true_dec) values of the monte-carlo events.
-            data_true_energy : 1d ndarray
-                The true energy of the monte-carlo events.
-            sin_dec_binning : BinningDefinition
-                The sin(dec) binning definition to use for the histogram.
-            weights : 1d ndarray
-                The weight factors of each monte-carlo event where only the
-                flux value needs to be multiplied with in order to get the
-                detector signal yield.
-            fluxmodel : FluxModel
-                The flux model to get the flux values from.
-
-            Returns
-            -------
-            h : 1d ndarray
-                The numpy array containing the histogram values.
-            """
-            (h, edges) = np.histogram(data_sin_true_dec,
-                                      bins = sin_dec_binning.binedges,
-                                      weights = weights * fluxmodel(data_true_energy),
-                                      density = False)
-            return h
-
-        data_sin_true_dec = np.sin(data.mc["true_dec"])
-        weights = data.mc["mcweight"] * toGeVcm2s * livetime_days * 86400.
-
-        # Make a copy of the gamma grid and extend the grid by one bin on each
-        # side.
-        gamma_grid = self._gamma_grid.copy()
-        gamma_grid.add_extra_lower_and_upper_bin()
-
-        # Construct the arguments for the hist function to be used in the
-        # multiproc.parallelize function.
-        args_list = [ ((data_sin_true_dec,
-                        data.mc['true_energy'],
-                        sin_dec_binning,
-                        weights,
-                        fluxmodel.copy({'gamma':gamma})), {})
-                     for gamma in gamma_grid.grid ]
-        h = np.vstack(
-            multiproc.parallelize(
-                hist, args_list, self.ncpu, ppbar=ppbar)).T
-
-        # Normalize by solid angle of each bin along the sin(dec) axis.
-        # The solid angle is given by 2*\pi*(\Delta sin(\delta))
-        h /= (2.*np.pi * np.diff(sin_dec_binning.binedges)).reshape(
-            (sin_dec_binning.nbins,1))
-
-        log_spl_sinDec_gamma = scipy.interpolate.RectBivariateSpline(
-            sin_dec_binning.bincenters, gamma_grid.grid, np.log(h),
-            kx = self.spline_order_sinDec, ky = self.spline_order_gamma, s = 0)
+        values : (N_values,) ndarray of float
+            The interpolated manifold value for the N given events.
+        grads : (D,N_values) ndarray of float
+            The D manifold gradients for the N given events, where D is the
+            number of parameters.
+        """
+        xname = self._p_grid.name
+
+        x = params_recarray[xname]
+
+        # Determine the nearest grid point x1.
+        x1 = self._p_grid.round_to_nearest_grid_point(x)
+
+        # Check if the parabola parametrization for x1 is already cached.
+        if self._is_cached(tdm.trial_data_state_id, x1):
+            M1 = self._cache['M1']
+            a = self._cache['a']
+            b = self._cache['b']
+        else:
+            dx = self._p_grid.delta
 
-        detsigyield = PowerLawFluxPointLikeSourceI3DetSigYield(
-            self, dataset, fluxmodel, livetime, sin_dec_binning, log_spl_sinDec_gamma)
+            # Calculate the neighboring grid points to x1: x0 and x2.
+            x0 = self._p_grid.round_to_nearest_grid_point(x1 - dx)
+            x2 = self._p_grid.round_to_nearest_grid_point(x1 + dx)
+
+            # Parameterize the parabola with parameters a, b, and M1.
+            self__func = self._func
+
+            n_values = tdm.get_n_values()
+
+            gridparams_recarray = np.array(
+                x0,
+                dtype=[(xname, np.float64)])
+            M0 = self__func(
+                tdm=tdm,
+                eventdata=eventdata,
+                gridparams_recarray=gridparams_recarray,
+                n_values=n_values)
+
+            gridparams_recarray = np.array(
+                x1,
+                dtype=[(xname, np.float64)])
+            M1 = self__func(
+                tdm=tdm,
+                eventdata=eventdata,
+                gridparams_recarray=gridparams_recarray,
+                n_values=n_values)
+
+            gridparams_recarray = np.array(
+                x2,
+                dtype=[(xname, np.float64)])
+            M2 = self__func(
+                tdm=tdm,
+                eventdata=eventdata,
+                gridparams_recarray=gridparams_recarray,
+                n_values=n_values)
+
+            a = 0.5*(M0 - 2.*M1 + M2) / dx**2
+            b = 0.5*(M2 - M0) / dx
+
+            # Cache the parabola parametrization.
+            self._cache = self._create_cache(
+                trial_data_state_id=tdm.trial_data_state_id,
+                x1=x1,
+                M1=M1,
+                a=a,
+                b=b)
+
+        # Broadcast x, x1, and (x-x1) to the values array.
+        (x, x1, x_minus_x1) = tdm.broadcast_sources_arrays_to_values_arrays(
+            (x, x1, x-x1))
+
+        # Calculate the interpolated manifold values.
+        values = a * x_minus_x1**2 + b * x_minus_x1 + M1
+        # Calculate the gradient of the manifold for all values.
+        grads = 2. * a * x_minus_x1 + b
 
-        return detsigyield
+        return (values, np.atleast_2d(grads))
```

### Comparing `skyllh-23.1.1/skyllh/i3/livetime.py` & `skyllh-23.2.0/skyllh/i3/livetime.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,66 +1,123 @@
 # -*- coding: utf-8 -*-
 
 import numpy as np
 
-from skyllh.core.livetime import Livetime
-from skyllh.core import storage
-from skyllh.i3.dataset import I3Dataset
+from skyllh.core.livetime import (
+    Livetime,
+)
+from skyllh.core.storage import (
+    create_FileLoader,
+)
+from skyllh.i3.dataset import (
+    I3Dataset,
+)
 
-class I3Livetime(Livetime):
+
+class I3Livetime(
+        Livetime):
     """The I3Livetime class provides the functionality to load a Livetime object
     from a good-run-list data file.
     """
+
+    @classmethod
+    def from_grl_data(cls, grl_data):
+        """Creates an I3LiveTime instance from the given good-run-list (GRL)
+        data.
+
+        Parameters
+        ----------
+        grl_data : instance of numpy structured ndarray.
+            The numpy structured ndarray of length N_runs holding the start end
+            end times of the good runs. The following fields need to exist:
+
+            start : float
+                The MJD of the run start.
+            end : float
+                The MJD of the run stop.
+
+        Returns
+        -------
+        livetime : instance of I3Livetime
+            The created instance of I3Livetime for the provided GRL data.
+        """
+        uptime_mjd_intervals_arr = np.hstack((
+            grl_data['start'].reshape((len(grl_data), 1)),
+            grl_data['stop'].reshape((len(grl_data), 1))
+        ))
+
+        livetime = cls(
+            uptime_mjd_intervals_arr=uptime_mjd_intervals_arr)
+
+        return livetime
+
     @staticmethod
-    def from_GRL_files(pathfilenames):
+    def from_grl_files(
+            pathfilenames):
         """Loads an I3Livetime instance from the given good-run-list (GRL) data
         file. The data file needs to contain the following data fields:
 
-            - start : float
+            start : float
                 The MJD of the run start.
-            - stop : float
+            stop : float
                 The MJD of the run stop.
 
         Parameters
         ----------
         pathfilenames : str | list of str
             The list of fully qualified file names of the GRL data files.
 
         Returns
         -------
-        livetime : I3Livetime instance
-            The created I3Livetime instance for the provided GRL data.
+        livetime : instance of I3Livetime
+            The created instance of I3Livetime for the provided GRL data.
         """
-        grl_data = storage.create_FileLoader(pathfilenames).load_data()
+        grl_data = create_FileLoader(pathfilenames).load_data()
 
         uptime_mjd_intervals_arr = np.hstack((
-            grl_data['start'].reshape((len(grl_data),1)),
-            grl_data['stop'].reshape((len(grl_data),1))
+            grl_data['start'].reshape((len(grl_data), 1)),
+            grl_data['stop'].reshape((len(grl_data), 1))
         ))
 
-        return I3Livetime(uptime_mjd_intervals_arr)
+        livetime = I3Livetime(
+            uptime_mjd_intervals_arr=uptime_mjd_intervals_arr)
+
+        return livetime
 
     @staticmethod
     def from_I3Dataset(ds):
         """Loads an I3Livetime instance from a given I3Dataset instance, which
         must have a good-run-list (GRL) files defined.
 
         Parameters
         ----------
         ds : I3Dataset instance
             The instance of I3Dataset which defined the good-run-list (GRL)
             files for the dataset.
 
         Returns
         -------
-        livetime : I3Livetime instance
-            The created I3Livetime instance for the GRL data from the provided
-            dataset.
+        livetime : instance of I3Livetime
+            The created instance of I3Livetime for the GRL data from the
+            provided dataset.
         """
-        if(not isinstance(ds, I3Dataset)):
-            raise TypeError('The ds argument must be an instance of I3Dataset!')
-        if(len(ds.grl_pathfilename_list) == 0):
-            raise ValueError('No GRL files have been defined for the given dataset!')
-        return I3Livetime.from_GRL_files(ds.grl_pathfilename_list)
-
-    def __init__(self, uptime_mjd_intervals_arr):
-        super(I3Livetime, self).__init__(uptime_mjd_intervals_arr)
+        if not isinstance(ds, I3Dataset):
+            raise TypeError(
+                'The ds argument must be an instance of I3Dataset!')
+        if len(ds.grl_pathfilename_list) == 0:
+            raise ValueError(
+                'No GRL files have been defined for the given dataset!')
+
+        livetime = I3Livetime.from_grl_files(
+            pathfilenames=ds.grl_pathfilename_list)
+
+        return livetime
+
+    def __init__(
+            self,
+            *args,
+            **kwargs):
+        """Creates a new instance of I3Livetime.
+        """
+        super().__init__(
+            *args,
+            **kwargs)
```

### Comparing `skyllh-23.1.1/skyllh/i3/pdf.py` & `skyllh-23.2.0/skyllh/i3/pdf.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,150 +1,195 @@
 # -*- coding: utf-8 -*-
 
 import numpy as np
 
-from skyllh.core.binning import UsesBinning
+from skyllh.core.binning import (
+    UsesBinning,
+)
+from skyllh.core.debugging import (
+    get_logger,
+)
 from skyllh.core.pdf import (
     EnergyPDF,
-    PDFAxis
+    PDFAxis,
+)
+from skyllh.core.py import (
+    classname,
 )
 from skyllh.core.smoothing import (
     UNSMOOTH_AXIS,
     SmoothingFilter,
     HistSmoothingMethod,
     NoHistSmoothingMethod,
-    NeighboringBinHistSmoothingMethod
+    NeighboringBinHistSmoothingMethod,
+)
+from skyllh.core.timing import (
+    TaskTimer,
 )
-from skyllh.core.timing import TaskTimer
 
+logger = get_logger(__name__)
 
-class I3EnergyPDF(EnergyPDF, UsesBinning):
+
+class I3EnergyPDF(
+        EnergyPDF,
+        UsesBinning,
+):
     """This is the base class for all IceCube specific energy PDF models.
     IceCube energy PDFs depend soley on the energy and the
     zenith angle, and hence, on the declination of the event.
 
     The IceCube energy PDF is modeled as a 1d histogram in energy,
     but for different sin(declination) bins, hence, stored as a 2d histogram.
     """
-    def __init__(self, data_logE, data_sinDec, data_mcweight, data_physicsweight,
-                 logE_binning, sinDec_binning, smoothing_filter):
+    def __init__(
+            self,
+            pmm,
+            data_log10_energy,
+            data_sin_dec,
+            data_mcweight,
+            data_physicsweight,
+            log10_energy_binning,
+            sin_dec_binning,
+            smoothing_filter,
+            **kwargs,
+    ):
         """Creates a new IceCube energy PDF object.
 
         Parameters
         ----------
-        data_logE : 1d ndarray
+        pmm : instance of ParameterModelMapper | None
+            The instance of ParameterModelMapper defining the global parameters
+            and their mapping to local model/source parameters.
+            It can be ``None``, if the PDF does not depend on any parameters.
+        data_log10_energy : 1d ndarray
             The array holding the log10(E) values of the events.
-        data_sinDec : 1d ndarray
+        data_sin_dec : 1d ndarray
             The array holding the sin(dec) values of the events.
         data_mcweight : 1d ndarray
             The array holding the monte-carlo weights of the events.
             The final data weight will be the product of data_mcweight and
             data_physicsweight.
         data_physicsweight : 1d ndarray
             The array holding the physics weights of the events.
             The final data weight will be the product of data_mcweight and
             data_physicsweight.
-        logE_binning : BinningDefinition
-            The binning definition for the log(E) axis.
-        sinDec_binning : BinningDefinition
+        log10_energy_binning : instance of BinningDefinition
+            The binning definition for the log10(E) axis.
+        sin_dec_binning : instance of BinningDefinition
             The binning definition for the sin(declination) axis.
-        smoothing_filter : SmoothingFilter instance | None
+        smoothing_filter : instance of SmoothingFilter | None
             The smoothing filter to use for smoothing the energy histogram.
-            If None, no smoothing will be applied.
+            If ``None``, no smoothing will be applied.
         """
-        super(I3EnergyPDF, self).__init__()
-
-        #self.logger = logging.getLogger(__name__)
+        super().__init__(
+            pmm=pmm,
+            **kwargs)
 
         # Define the PDF axes.
-        self.add_axis(PDFAxis(name='log_energy',
-                              vmin=logE_binning.lower_edge,
-                              vmax=logE_binning.upper_edge))
-        self.add_axis(PDFAxis(name='sin_dec',
-                              vmin=sinDec_binning.lower_edge,
-                              vmax=sinDec_binning.upper_edge))
+        self.add_axis(
+            PDFAxis(
+                name='log_energy',
+                vmin=log10_energy_binning.lower_edge,
+                vmax=log10_energy_binning.upper_edge))
+        self.add_axis(
+            PDFAxis(
+                name='sin_dec',
+                vmin=sin_dec_binning.lower_edge,
+                vmax=sin_dec_binning.upper_edge))
 
-        self.add_binning(logE_binning, 'log_energy')
-        self.add_binning(sinDec_binning, 'sin_dec')
+        self.add_binning(log10_energy_binning, 'log_energy')
+        self.add_binning(sin_dec_binning, 'sin_dec')
 
         # Create the smoothing method instance tailored to the energy PDF.
-        # We will smooth only the first axis (logE).
-        if((smoothing_filter is not None) and
-           (not isinstance(smoothing_filter, SmoothingFilter))):
-            raise TypeError('The smoothing_filter argument must be None or an instance of SmoothingFilter!')
-        if(smoothing_filter is None):
+        # We will smooth only the first axis (log10(E)).
+        if (smoothing_filter is not None) and\
+           (not isinstance(smoothing_filter, SmoothingFilter)):
+            raise TypeError(
+                'The smoothing_filter argument must be None or an instance of '
+                f'SmoothingFilter! It is of type {classname(smoothing_filter)}')
+        if smoothing_filter is None:
             self.hist_smoothing_method = NoHistSmoothingMethod()
         else:
             self.hist_smoothing_method = NeighboringBinHistSmoothingMethod(
                 (smoothing_filter.axis_kernel_array, UNSMOOTH_AXIS))
 
         # We have to figure out, which histogram bins are zero due to no
         # monte-carlo coverage, and which due to zero physics model
         # contribution.
 
         # Create a 2D histogram with only the MC events to determine the MC
         # coverage.
-        (h, bins_logE, bins_sinDec) = np.histogram2d(data_logE, data_sinDec,
-            bins = [logE_binning.binedges, sinDec_binning.binedges],
-            range = [logE_binning.range, sinDec_binning.range],
-            normed = False)
+        (h, bins_logE, bins_sinDec) = np.histogram2d(
+            data_log10_energy,
+            data_sin_dec,
+            bins=[log10_energy_binning.binedges, sin_dec_binning.binedges],
+            range=[log10_energy_binning.range, sin_dec_binning.range],
+            density=False)
         h = self._hist_smoothing_method.smooth(h)
         self._hist_mask_mc_covered = h > 0
 
         # Select the events which have MC coverage but zero physics
         # contribution, i.e. the physics model predicts zero contribution.
         mask = data_physicsweight == 0.
 
         # Create a 2D histogram with only the MC events that have zero physics
         # contribution. Note: By construction the zero physics contribution bins
         # are a subset of the MC covered bins.
-        (h, bins_logE, bins_sinDec) = np.histogram2d(data_logE[mask], data_sinDec[mask],
-            bins = [logE_binning.binedges, sinDec_binning.binedges],
-            range = [logE_binning.range, sinDec_binning.range],
-            normed = False)
+        (h, bins_logE, bins_sinDec) = np.histogram2d(
+            data_log10_energy[mask],
+            data_sin_dec[mask],
+            bins=[log10_energy_binning.binedges, sin_dec_binning.binedges],
+            range=[log10_energy_binning.range, sin_dec_binning.range],
+            density=False)
         h = self._hist_smoothing_method.smooth(h)
         self._hist_mask_mc_covered_zero_physics = h > 0
 
         # Create a 2D histogram with only the data which has physics
         # contribution. We will do the normalization along the logE
         # axis manually.
         data_weights = data_mcweight[~mask] * data_physicsweight[~mask]
-        (h, bins_logE, bins_sinDec) = np.histogram2d(data_logE[~mask], data_sinDec[~mask],
-            bins = [logE_binning.binedges, sinDec_binning.binedges],
-            weights = data_weights,
-            range = [logE_binning.range, sinDec_binning.range],
-            normed = False)
+        (h, bins_logE, bins_sinDec) = np.histogram2d(
+            data_log10_energy[~mask],
+            data_sin_dec[~mask],
+            bins=[log10_energy_binning.binedges, sin_dec_binning.binedges],
+            weights=data_weights,
+            range=[log10_energy_binning.range, sin_dec_binning.range],
+            density=False)
 
         # Calculate the normalization for each logE bin. Hence we need to sum
         # over the logE bins (axis 0) for each sin(dec) bin and need to divide
         # by the logE bin widths along the sin(dec) bins. The result array norm
         # is a 2D array of the same shape as h.
-        norms = np.sum(h, axis=(0,))[np.newaxis,...] * np.diff(logE_binning.binedges)[...,np.newaxis]
+        norms = np.sum(h, axis=(0,))[np.newaxis, ...] *\
+            np.diff(log10_energy_binning.binedges)[..., np.newaxis]
         h /= norms
         h = self._hist_smoothing_method.smooth(h)
 
-        self._hist_logE_sinDec = h
+        self._hist_log10_energy_sin_dec = h
 
     @property
     def hist_smoothing_method(self):
         """The HistSmoothingMethod instance defining the smoothing filter of the
         energy PDF histogram.
         """
         return self._hist_smoothing_method
+
     @hist_smoothing_method.setter
     def hist_smoothing_method(self, method):
-        if(not isinstance(method, HistSmoothingMethod)):
-            raise TypeError('The hist_smoothing_method property must be an instance of HistSmoothingMethod!')
+        if not isinstance(method, HistSmoothingMethod):
+            raise TypeError(
+                'The hist_smoothing_method property must be an instance of '
+                f'HistSmoothingMethod! It is of type {classname(method)}')
         self._hist_smoothing_method = method
 
     @property
     def hist(self):
         """(read-only) The 2D logE-sinDec histogram array.
         """
-        return self._hist_logE_sinDec
+        return self._hist_log10_energy_sin_dec
 
     @property
     def hist_mask_mc_covered(self):
         """(read-only) The boolean ndarray holding the mask of the 2D histogram
         bins for which there is monte-carlo coverage.
         """
         return self._hist_mask_mc_covered
@@ -159,82 +204,110 @@
 
     @property
     def hist_mask_mc_covered_with_physics(self):
         """(read-only) The boolean ndarray holding the mask of the 2D histogram
         bins for which there is monte-carlo coverage and has physics
         contribution.
         """
-        return self._hist_mask_mc_covered & ~self._hist_mask_mc_covered_zero_physics
-
-    def assert_is_valid_for_exp_data(self, data_exp):
-        """Checks if this energy PDF is valid for all the given experimental
-        data.
-        It checks if all the data is within the logE and sin(dec) binning range.
+        mask = (
+            self._hist_mask_mc_covered & ~self._hist_mask_mc_covered_zero_physics
+        )
+        return mask
+
+    def assert_is_valid_for_trial_data(
+            self,
+            tdm,
+            tl=None,
+            **kwargs):
+        """Checks if this energy PDF is valid for all the given trial events.
+        It checks if all the data is within the log10(E) and sin(dec) binning
+        range.
 
         Parameters
         ----------
-        data_exp : numpy record ndarray
-            The array holding the experimental data. The following data fields
-            must exist:
-
-            - 'log_energy' : float
-                The logarithm of the energy value of the data event.
-            - 'dec' : float
+        tdm : instance of TrialDataManager
+            The instance of TrialDataManager holding the trial data events.
+            The following data fields must exist:
+
+            log_energy : float
+                The base-10 logarithm of the energy value of the data event.
+            dec : float
                 The declination of the data event.
 
+        tl : instance of TimeLord | None
+            The optional instance of TimeLord for measuring timing information.
+
         Raises
         ------
         ValueError
-            If some of the data is outside the logE or sin(dec) binning range.
+            If some of the data is outside the log10(E) or sin(dec) binning
+            range.
         """
-        logE_binning = self.get_binning('log_energy')
-        sinDec_binning = self.get_binning('sin_dec')
+        log10_energy_binning = self.get_binning('log_energy')
+        sin_dec_binning = self.get_binning('sin_dec')
 
-        exp_logE = data_exp['log_energy']
-        exp_sinDec = np.sin(data_exp['dec'])
+        data_log10_energy = tdm['log_energy']
+        data_sin_dec = np.sin(tdm['dec'])
 
-        # Check if all the data is within the binning range.
-        #if(logE_binning.any_data_out_of_binning_range(exp_logE)):
-            #self.logger.warning('Some data is outside the logE range (%.3f, %.3f)', logE_binning.lower_edge, logE_binning.upper_edge)
-        #if(sinDec_binning.any_data_out_of_binning_range(exp_sinDec)):
-            #self.logger.warning('Some data is outside the sin(dec) range (%.3f, %.3f)', sinDec_binning.lower_edge, sinDec_binning.upper_edge)
+        if log10_energy_binning.any_data_out_of_range(data_log10_energy):
+            oor_data = log10_energy_binning.get_out_of_range_data(
+                data_log10_energy)
+            raise ValueError(
+                'Some data is outside the log10(E) range '
+                f'({log10_energy_binning.lower_edge:.3f},'
+                f' {log10_energy_binning.upper_edge:.3f})! '
+                f'The following data values are out of range: {oor_data}')
+
+        if sin_dec_binning.any_data_out_of_range(data_sin_dec):
+            oor_data = sin_dec_binning.get_out_of_range_data(
+                data_sin_dec)
+            raise ValueError(
+                'Some data is outside the sin(dec) range '
+                f'({sin_dec_binning.lower_edge:.3f},'
+                f' {sin_dec_binning.upper_edge:.3f})! '
+                f'The following data values are out of range: {oor_data}')
 
-    def get_prob(self, tdm, fitparams=None, tl=None):
-        """Calculates the energy probability (in logE) of each event.
+    def get_pd(self, tdm, params_recarray=None, tl=None):
+        """Calculates the energy probability density of each event.
 
         Parameters
         ----------
         tdm : instance of TrialDataManager
             The TrialDataManager instance holding the data events for which the
-            probability should be calculated for. The following data fields must
-            exist:
+            probability density should be calculated.
+            The following data fields must exist:
 
-            - 'log_energy' : float
-                The logarithm of the energy value of the event.
-            - 'sin_dec' : float
+            log_energy : float
+                The base-10 logarithm of the energy value of the event.
+            sin_dec : float
                 The sin(declination) value of the event.
 
-        fitparams : None
+        params_recarray : None
             Unused interface parameter.
         tl : TimeLord instance | None
             The optional TimeLord instance that should be used to measure
             timing information.
 
         Returns
         -------
-        prob : 1D (N_events,) shaped ndarray
-            The array with the energy probability for each event.
+        pd : instance of ndarray
+            The 1D (N_events,)-shaped numpy ndarray with the energy probability
+            density for each event.
+        grads : dict
+            The dictionary holding the gradients of the probability density
+            w.r.t. each fit parameter. The key of the dictionary is the id
+            of the global fit parameter. Because this energy PDF does not depend
+            on any fit parameters, an empty dictionary is returned.
         """
-        get_data = tdm.get_data
-
-        logE_binning = self.get_binning('log_energy')
-        sinDec_binning = self.get_binning('sin_dec')
-
-        logE_idx = np.digitize(
-            get_data('log_energy'), logE_binning.binedges) - 1
-        sinDec_idx = np.digitize(
-            get_data('sin_dec'), sinDec_binning.binedges) - 1
+        log10_energy_binning = self.get_binning('log_energy')
+        sin_dec_binning = self.get_binning('sin_dec')
 
-        with TaskTimer(tl, 'Evaluating logE-sinDec histogram.'):
-            prob = self._hist_logE_sinDec[(logE_idx,sinDec_idx)]
+        log10_energy_idx = np.digitize(
+            tdm['log_energy'], log10_energy_binning.binedges) - 1
+        sin_dec_idx = np.digitize(
+            tdm['sin_dec'], sin_dec_binning.binedges) - 1
+
+        with TaskTimer(tl, 'Evaluating log10_energy-sin_dec histogram.'):
+            pd = self._hist_log10_energy_sin_dec[
+                (log10_energy_idx, sin_dec_idx)]
 
-        return prob
+        return (pd, dict())
```

### Comparing `skyllh-23.1.1/skyllh/i3/pdfratio.py` & `skyllh-23.2.0/skyllh/core/config.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,262 +1,433 @@
 # -*- coding: utf-8 -*-
 
-import abc
-import numpy as np
-import scipy.interpolate
-
-from skyllh.core.parameters import make_params_hash
-from skyllh.core.multiproc import IsParallelizable, parallelize
-from skyllh.core.pdfratio import SigSetOverBkgPDFRatio, PDFRatioFillMethod, MostSignalLikePDFRatioFillMethod
-
-from skyllh.i3.pdf import I3EnergyPDF
-
-
-class I3EnergySigSetOverBkgPDFRatioSpline(SigSetOverBkgPDFRatio, IsParallelizable):
-    """This class implements a signal over background PDF ratio spline for
-    I3EnergyPDF enegry PDFs. It takes an object, which is derived from PDFSet
-    for I3EnergyPDF PDF types, and which is derived from IsSignalPDF, as signal
-    PDF. Furthermore, it takes an object, which is derived from I3EnergyPDF and
-    IsBackgroundPDF, as background PDF, and creates a spline for the ratio of
-    the signal and background PDFs for a grid of different discrete energy
-    signal fit parameters, which are defined by the signal PDF set.
+"""This file contains the global configuration dictionary, together with some
+convenience utility functions to set different configuration settings.
+"""
+
+import os.path
+import sys
+
+from astropy import (
+    units,
+)
+
+from typing import (
+    Any,
+    Dict,
+)
+
+from skyllh.core import (
+    tool,
+)
+from skyllh.core.datafields import (
+    DataFieldStages as DFS,
+)
+from skyllh.core.py import (
+    classname,
+)
+
+
+_BASECONFIG = {
+    'multiproc': {
+        # The number of CPUs to use for functions that allow multi-processing.
+        # If this setting is set to an int value in the range [1, N] this
+        # setting will be used if a function's local ncpu setting is not
+        # specified.
+        'ncpu': None,
+    },
+    'debugging': {
+        # The default log format.
+        'log_format': (
+            '%(asctime)s %(processName)s %(name)s %(levelname)s: '
+            '%(message)s'),
+        # Flag if detailed debug log messages, i.e. trace log messages, should
+        # get generated. This is good for debugging but bad for performance.
+        'enable_tracing': False,
+    },
+    'project': {
+        # The project's working directory.
+        'working_directory': '.',
+    },
+    'repository': {
+        # A base path of repository datasets.
+        'base_path': None,
+    },
+    'units': {
+        # Definition of the internal units to use. These must match with the
+        # units of the monto-carlo data files.
+        'internal': {
+            'angle': units.radian,
+            'energy': units.GeV,
+            'length': units.cm,
+            'time': units.s,
+        },
+        'defaults': {
+            # Definition of default units used for fluxes.
+            'fluxes': {
+                'angle': units.radian,
+                'energy': units.GeV,
+                'length': units.cm,
+                'time': units.s,
+            }
+        }
+    },
+    'datafields': {
+        'run': DFS.ANALYSIS_EXP,
+        'ra': DFS.ANALYSIS_EXP,
+        'dec': DFS.ANALYSIS_EXP,
+        'ang_err': DFS.ANALYSIS_EXP,
+        'time': DFS.ANALYSIS_EXP,
+        'log_energy': DFS.ANALYSIS_EXP,
+        'true_ra': DFS.ANALYSIS_MC,
+        'true_dec': DFS.ANALYSIS_MC,
+        'true_energy': DFS.ANALYSIS_MC,
+        'mcweight': DFS.ANALYSIS_MC,
+    },
+    # Flag if specific calculations in the core module can be cached.
+    'caching': {
+        'pdf': {
+            'MultiDimGridPDF': False,
+        }
+    }
+}
+
+
+class Config(
+        dict,
+):
+    """This class, derived from dict, holds the a local configuration state.
     """
+
     def __init__(
-            self, signalpdfset, backgroundpdf,
-            fillmethod=None, interpolmethod=None, ncpu=None, ppbar=None):
-        """Creates a new IceCube signal-over-background energy PDF ratio object.
+            self,
+    ) -> None:
+        """Initializes a new Config instance holding the base configuration.
+        """
+        super().__init__(_BASECONFIG)
+
+    @classmethod
+    @tool.requires('yaml')
+    def from_yaml(
+            cls,
+            pathfilename: str,
+    ):
+        """Creates a new instance of Config holding the base configuration and
+        updated by the configuration items contained in the yaml file using the
+        :meth:`dict.update` method.
+
+        Parameters
+        ----------
+        pathfilename: str | None
+            Path and filename to the yaml file containg the to-be-updated
+            configuration items.
+            If set to ``None``, nothing is done.
+
+        Returns
+        -------
+        cfg : instance of Config
+            The instance of Config holding the base configuration and updated by
+            the configuration given in the yaml file.
+        """
+        cfg = cls()
+
+        if pathfilename is None:
+            return cfg
+
+        yaml = tool.get('yaml')
+
+        user_config_dict = yaml.load(
+            open(pathfilename),
+            Loader=yaml.SafeLoader)
+        cfg.update(user_config_dict)
+
+        return cfg
+
+    @classmethod
+    def from_dict(
+            cls,
+            user_dict: Dict[str, Any],
+    ):
+        """Creates a new instance of Config holding the base configuration and
+        updated by the given configuration dictionary using the
+        :meth:`dict.update` method.
 
         Parameters
         ----------
-        signalpdfset : class instance derived from PDFSet (for PDF type
-                       I3EnergyPDF), IsSignalPDF, and UsesBinning
-            The PDF set, which provides signal energy PDFs for a set of
-            discrete signal parameters.
-        backgroundpdf : class instance derived from I3EnergyPDF, and
-                        IsBackgroundPDF
-            The background energy PDF object.
-        fillmethod : instance of PDFRatioFillMethod | None
-            An instance of class derived from PDFRatioFillMethod that implements
-            the desired ratio fill method.
-            If set to None (default), the default ratio fill method
-            MostSignalLikePDFRatioFillMethod will be used.
-        interpolmethod : class of GridManifoldInterpolationMethod
-            The class implementing the fit parameter interpolation method for
-            the PDF ratio manifold grid.
-        ncpu : int | None
-            The number of CPUs to use to create the ratio splines for the
-            different sets of signal parameters.
-        ppbar : ProgressBar instance | None
-            The instance of ProgressBar of the optional parent progress bar.
-
-        Raises
-        ------
-        ValueError
-            If the signal and background PDFs use different binning.
-        """
-        super(I3EnergySigSetOverBkgPDFRatioSpline, self).__init__(
-            pdf_type=I3EnergyPDF,
-            signalpdfset=signalpdfset, backgroundpdf=backgroundpdf,
-            interpolmethod=interpolmethod,
-            ncpu=ncpu)
-
-        # Define the default ratio fill method.
-        if(fillmethod is None):
-            fillmethod = MostSignalLikePDFRatioFillMethod()
-        self.fillmethod = fillmethod
-
-        # Ensure same binning of signal and background PDFs.
-        for (sigpdf_hash, sigpdf) in self.signalpdfset.items():
-            if(not sigpdf.has_same_binning_as(self.backgroundpdf)):
-                raise ValueError('At least one signal PDF does not have the same binning as the background PDF!')
-
-        def create_log_ratio_spline(sigpdfset, bkgpdf, fillmethod, gridfitparams):
-            """Creates the signal/background ratio spline for the given signal
-            parameters.
-
-            Returns
-            -------
-            log_ratio_spline : RegularGridInterpolator
-                The spline of the logarithmic PDF ratio values.
-            """
-            # Get the signal PDF for the given signal parameters.
-            sigpdf = sigpdfset.get_pdf(gridfitparams)
-
-            # Create the ratio array with the same shape than the background pdf
-            # histogram.
-            ratio = np.ones_like(bkgpdf.hist, dtype=np.float64)
-
-            # Fill the ratio array.
-            ratio = fillmethod.fill_ratios(ratio,
-                sigpdf.hist, bkgpdf.hist,
-                sigpdf.hist_mask_mc_covered, sigpdf.hist_mask_mc_covered_zero_physics,
-                bkgpdf.hist_mask_mc_covered, bkgpdf.hist_mask_mc_covered_zero_physics)
-
-            # Define the grid points for the spline. In general, we use the bin
-            # centers of the binning, but for the first and last point of each
-            # dimension we use the lower and upper bin edge, respectively, to
-            # ensure full coverage of the spline across the binning range.
-            points_list = []
-            for binning in sigpdf.binnings:
-                points = binning.bincenters
-                (points[0], points[-1]) = (binning.lower_edge, binning.upper_edge)
-                points_list.append(points)
-
-            # Create the spline for the ratio values.
-            log_ratio_spline = scipy.interpolate.RegularGridInterpolator(
-                tuple(points_list),
-                np.log(ratio),
-                method='linear',
-                bounds_error=False,
-                fill_value=0.)
-
-            return log_ratio_spline
-
-        # Get the list of fit parameter permutations on the grid for which we
-        # need to create PDF ratio arrays.
-        gridfitparams_list = self.signalpdfset.gridfitparams_list
-
-        args_list = [ ((signalpdfset, backgroundpdf, self.fillmethod, gridfitparams),{})
-                     for gridfitparams in gridfitparams_list ]
-
-        log_ratio_spline_list = parallelize(
-            create_log_ratio_spline, args_list, self.ncpu, ppbar=ppbar)
-
-        # Save all the log_ratio splines in a dictionary.
-        self._gridfitparams_hash_log_ratio_spline_dict = dict()
-        for (gridfitparams, log_ratio_spline) in zip(gridfitparams_list, log_ratio_spline_list):
-            gridfitparams_hash = make_params_hash(gridfitparams)
-            self._gridfitparams_hash_log_ratio_spline_dict[gridfitparams_hash] = log_ratio_spline
-
-        # Save the list of data field names.
-        self._data_field_names = [ binning.name
-                                  for binning in self.backgroundpdf.binnings ]
-
-        # Construct the instance for the fit parameter interpolation method.
-        self._interpolmethod_instance = self.interpolmethod(self._get_spline_value, signalpdfset.fitparams_grid_set)
-
-        # Create cache variables for the last ratio value and gradients in order
-        # to avoid the recalculation of the ratio value when the
-        # ``get_gradient`` method is called (usually after the ``get_ratio``
-        # method was called).
-        self._cache_fitparams_hash = None
-        self._cache_ratio = None
-        self._cache_gradients = None
+        user_dict: dict
+            The dictionary containg the to-be-updated configuration items.
+
+        Returns
+        -------
+        cfg : instance of Config
+            The instance of Config holding the base configuration and updated by
+            the given configuration dictionary.
+        """
+        cfg = cls()
+
+        cfg.update(user_dict)
+
+        return cfg
 
     @property
-    def fillmethod(self):
-        """The PDFRatioFillMethod object, which should be used for filling the
-        PDF ratio bins.
-        """
-        return self._fillmethod
-    @fillmethod.setter
-    def fillmethod(self, obj):
-        if(not isinstance(obj, PDFRatioFillMethod)):
-            raise TypeError('The fillmethod property must be an instance of PDFRatioFillMethod!')
-        self._fillmethod = obj
-
-    def _get_spline_value(self, tdm, gridfitparams, eventdata):
-        """Selects the spline object for the given fit parameter grid point and
-        evaluates the spline for all the given events.
-        """
-        # Get the spline object for the given fit parameter grid values.
-        gridfitparams_hash = make_params_hash(gridfitparams)
-        spline = self._gridfitparams_hash_log_ratio_spline_dict[gridfitparams_hash]
-
-        # Evaluate the spline.
-        value = spline(eventdata)
-
-        return value
-
-    def _is_cached(self, tdm, fitparams_hash):
-        """Checks if the ratio and gradients for the given set of fit parameters
-        are already cached.
-        """
-        if((self._cache_fitparams_hash == fitparams_hash) and
-           (len(self._cache_ratio) == tdm.n_selected_events)
-          ):
-            return True
-        return False
-
-    def _calculate_ratio_and_gradients(self, tdm, fitparams, fitparams_hash):
-        """Calculates the ratio values and ratio gradients for all the events
-        given the fit parameters. It caches the results.
-        """
-        get_data = tdm.get_data
-
-        # Create a 2D event data array holding only the needed event data fields
-        # for the PDF ratio spline evaluation.
-        eventdata = np.vstack([get_data(fn) for fn in self._data_field_names]).T
-
-        (ratio, gradients) = self._interpolmethod_instance.get_value_and_gradients(
-            tdm, eventdata, fitparams)
-        # The interpolation works on the logarithm of the ratio spline, hence
-        # we need to transform it using the exp function, and we need to account
-        # for the exp function in the gradients.
-        ratio = np.exp(ratio)
-        gradients = ratio * gradients
-
-        # Cache the value and the gradients.
-        self._cache_fitparams_hash = fitparams_hash
-        self._cache_ratio = ratio
-        self._cache_gradients = gradients
-
-    def get_ratio(self, tdm, fitparams, tl=None):
-        """Retrieves the PDF ratio values for each given trial event data, given
-        the given set of fit parameters. This method is called during the
-        likelihood maximization process.
-        For computational efficiency reasons, the gradients are calculated as
-        well and will be cached.
+    def is_tracing_enabled(self):
+        """``True``, if tracing mode is enabled, ``False`` otherwise.
+        """
+        return self['debugging']['enable_tracing']
+
+    def disable_tracing(
+            self,
+    ):
+        """Disables the tracing mode of SkyLLH.
+
+        Returns
+        -------
+        self : instance of Config
+            The updated instance of Config.
+        """
+        self['debugging']['enable_tracing'] = False
+
+        return self
+
+    def enable_tracing(
+            self,
+    ):
+        """Enables the tracing mode of SkyLLH.
+
+        Returns
+        -------
+        self : instance of Config
+            The updated instance of Config.
+        """
+        self['debugging']['enable_tracing'] = True
+
+        return self
+
+    def get_wd(
+            self,
+    ):
+        """Retrieves the absolut path to the working directoy as configured in
+        this configuration.
+
+        Returns
+        -------
+        wd : str
+            The absolut path to the project's working directory.
+        """
+        wd = os.path.abspath(self['project']['working_directory'])
+
+        return wd
+
+    def set_enable_tracing(
+            self,
+            flag,
+    ):
+        """Sets the setting for tracing.
 
         Parameters
         ----------
-        tdm : instance of TrialDataManager
-            The TrialDataManager instance holding the trial event data for which
-            the PDF ratio values should get calculated.
-        fitparams : dict
-            The dictionary with the fit parameter values.
-        tl : TimeLord instance | None
-            The optional TimeLord instance that should be used to measure
-            timing information.
+        flag : bool
+            The flag if tracing should be enabled (``True``) or disabled
+            (``False``).
 
         Returns
         -------
-        ratio : 1d ndarray of float
-            The PDF ratio value for each given event.
+        self : instance of Config
+            The updated instance of Config.
         """
-        fitparams_hash = make_params_hash(fitparams)
+        self['debugging']['enable_tracing'] = flag
 
-        # Check if the ratio value is already cached.
-        if(self._is_cached(tdm, fitparams_hash)):
-            return self._cache_ratio
+        return self
 
-        self._calculate_ratio_and_gradients(tdm, fitparams, fitparams_hash)
+    def set_internal_units(
+            self,
+            angle_unit=None,
+            energy_unit=None,
+            length_unit=None,
+            time_unit=None,
+    ):
+        """Sets the units used internally to compute quantities. These units
+        must match the units used in the monte-carlo files.
 
-        return self._cache_ratio
+        Parameters
+        ----------
+        angle_unit : instance of astropy.units.UnitBase | None
+            The internal unit that should be used for angles.
+            If set to ``None``, the unit is not changed.
+        energy_unit : instance of astropy.units.UnitBase | None
+            The internal unit that should be used for energy.
+            If set to ``None``, the unit is not changed.
+        length_unit : instance of astropy.units.UnitBase | None
+            The internal unit that should be used for length.
+            If set to ``None``, the unit is not changed.
+        time_unit : instance of astropy.units.UnitBase | None
+            The internal unit that should be used for time.
+            If set to ``None``, the unit is not changed.
 
-    def get_gradient(self, tdm, fitparams, fitparam_name):
-        """Retrieves the PDF ratio gradient for the pidx'th fit parameter.
+        Returns
+        -------
+        self : instance of Config
+            The updated instance of Config.
+        """
+        if angle_unit is not None:
+            if not isinstance(angle_unit, units.UnitBase):
+                raise TypeError(
+                    'The angle_unit argument must be an instance of '
+                    'astropy.units.UnitBase!')
+            self['units']['internal']['angle'] = angle_unit
+
+        if energy_unit is not None:
+            if not isinstance(energy_unit, units.UnitBase):
+                raise TypeError(
+                    'The energy_unit argument must be an instance of '
+                    'astropy.units.UnitBase!')
+            self['units']['internal']['energy'] = energy_unit
+
+        if length_unit is not None:
+            if not isinstance(length_unit, units.UnitBase):
+                raise TypeError(
+                    'The length_unit argument must be an instance of '
+                    'astropy.units.UnitBase!')
+            self['units']['internal']['length'] = length_unit
+
+        if time_unit is not None:
+            if not isinstance(time_unit, units.UnitBase):
+                raise TypeError(
+                    'The time_unit argument must be an instance of '
+                    'astropy.units.UnitBase!')
+            self['units']['internal']['time'] = time_unit
+
+        return self
+
+    def set_ncpu(
+            self,
+            ncpu,
+    ):
+        """Sets the global setting for the number of CPUs to use, when
+        parallelization is available.
+
+        Parameters
+        ----------
+        ncpu : int
+            The number of CPUs.
+
+        Returns
+        -------
+        self : instance of Config
+            The updated instance of Config.
+        """
+        self['multiproc']['ncpu'] = ncpu
+
+        return self
+
+    def set_wd(
+            self,
+            path=None,
+    ):
+        """Sets the project's working directory configuration variable and adds
+        it to the Python path variable.
+
+        Parameters
+        ----------
+        cfg : instance of Config
+            The instance of Config holding the local configuration.
+        path : str | None
+            The path of the project's working directory. This can be a path
+            relative to the path given by ``os.path.getcwd``, the current
+            working directory of the program.
+            If set to ``None``, the path is taken from the working directory
+            setting of the given configuration.
+
+        Returns
+        -------
+        wd : str
+            The absolut path to the project's working directory.
+        """
+        if path is None:
+            path = self['project']['working_directory']
+
+        if self['project']['working_directory'] in sys.path:
+            sys.path.remove(self['project']['working_directory'])
+
+        wd = os.path.abspath(path)
+        self['project']['working_directory'] = wd
+        sys.path.insert(0, wd)
+
+        return wd
+
+    def to_internal_time_unit(
+            self,
+            time_unit,
+    ):
+        """Calculates the conversion factor from the given time unit to the
+        internal time unit specified by this local configuration.
+
+        Parameters
+        ----------
+        time_unit : instance of astropy.units.UnitBase
+            The time unit from which to convert to the internal time unit.
+        """
+        internal_time_unit = self['units']['internal']['time']
+        factor = time_unit.to(internal_time_unit)
+
+        return factor
+
+    def wd_filename(self, filename):
+        """Generates the fully qualified file name under the project's working
+        directory of the given file.
 
         Parameters
         ----------
-        tdm : instance of TrialDataManager
-            The TrialDataManager instance holding the trial event data for which
-            the PDF ratio gradient values should get calculated.
-        fitparams : dict
-            The dictionary with the fit parameter values.
-        fitparam_name : str
-            The name of the fit parameter for which the gradient should get
-            calculated.
-        """
-        fitparams_hash = make_params_hash(fitparams)
-
-        # Convert the fit parameter name into the local fit parameter index.
-        pidx = self.convert_signal_fitparam_name_into_index(fitparam_name)
-
-        # Check if the gradients have been calculated already.
-        if(self._is_cached(tdm, fitparams_hash)):
-            return self._cache_gradients[pidx]
+        filename : str
+            The name of the file for which to generate the working directory
+            path file name.
 
-        # The gradients have not been calculated yet.
-        self._calculate_ratio_and_gradients(tdm, fitparams, fitparams_hash)
+        Returns
+        -------
+        pathfilename : str
+            The generated fully qualified path file name of ``filename`` with
+            the project's working directory prefixed.
+        """
+        pathfilename = os.path.join(self.get_wd(), filename)
+
+        return pathfilename
+
+
+class HasConfig(
+        object,
+):
+    """Classifier class defining the cfg property. Classes that derive from
+    this class indicate, that they hold an instance of Config.
+    """
+
+    def __init__(
+            self,
+            cfg,
+            *args,
+            **kwargs,
+    ):
+        """Creates a new instance having the property ``cfg``.
+
+        Parameters
+        ----------
+        cfg : instance of Config
+            The instance of Config holding the local configuration.
+        """
+        super().__init__(
+            *args,
+            **kwargs)
+
+        self.cfg = cfg
+
+    @property
+    def cfg(self):
+        """The instance of Config holding the local configuration.
+        """
+        return self._cfg
 
-        return self._cache_gradients[pidx]
+    @cfg.setter
+    def cfg(self, c):
+        if not isinstance(c, Config):
+            raise TypeError(
+                'The cfg property must be an instance of Config! '
+                f'Currently its type is {classname(c)}!')
+        self._cfg = c
```

### Comparing `skyllh-23.1.1/skyllh/i3/scrambling.py` & `skyllh-23.2.0/skyllh/i3/scrambling.py`

 * *Files 6% similar despite different names*

```diff
@@ -2,47 +2,64 @@
 
 import numpy as np
 
 from skyllh.core.scrambling import (
     DataScramblingMethod,
     TimeScramblingMethod,
 )
-from skyllh.i3.coords import (
+
+from skyllh.i3.utils.coords import (
     azi_to_ra_transform,
     hor_to_equ_transform,
 )
 
 
-class I3TimeScramblingMethod(TimeScramblingMethod):
+class I3TimeScramblingMethod(
+        TimeScramblingMethod,
+):
     """The I3TimeScramblingMethod class provides a data scrambling method to
     perform time scrambling of the data,
     by drawing a MJD time from a given time generator.
     """
-    def __init__(self, timegen):
+    def __init__(
+            self,
+            timegen,
+            **kwargs,
+    ):
         """Initializes a new I3 time scrambling instance.
 
         Parameters
         ----------
         timegen : TimeGenerator
             The time generator that should be used to generate random MJD times.
         """
-        super(I3TimeScramblingMethod, self).__init__(timegen, hor_to_equ_transform)
+        super().__init__(
+            timegen=timegen,
+            hor_to_equ_transform=hor_to_equ_transform,
+            **kwargs)
 
     # We override the scramble method because for IceCube we only need to change
     # the ``ra`` field.
-    def scramble(self, rss, data):
+    def scramble(
+            self,
+            rss,
+            dataset,
+            data,
+    ):
         """Draws a time from the time generator and calculates the right
         ascention coordinate from the azimuth angle according to the time.
         Sets the values of the ``time`` and ``ra`` keys of data.
 
         Parameters
         ----------
         rss : RandomStateService
             The random state service providing the random number
             generator (RNG).
+        dataset : instance of Dataset
+            The instance of Dataset for which the data should get scrambled.
         data : DataFieldRecordArray instance
             The DataFieldRecordArray instance containing the to be scrambled
             data.
 
         Returns
         -------
         data : numpy record ndarray
@@ -52,20 +69,26 @@
 
         data['time'] = mjds
         data['ra'] = azi_to_ra_transform(data['azi'], mjds)
 
         return data
 
 
-class I3SeasonalVariationTimeScramblingMethod(DataScramblingMethod):
+class I3SeasonalVariationTimeScramblingMethod(
+        DataScramblingMethod,
+):
     """The I3SeasonalVariationTimeScramblingMethod class provides a data
     scrambling method to perform data coordinate scrambling based on a generated
     time, which follows seasonal variations within the experimental data.
     """
-    def __init__(self, data, **kwargs):
+    def __init__(
+            self,
+            data,
+            **kwargs,
+    ):
         """Initializes a new seasonal time scrambling instance.
 
         Parameters
         ----------
         data : instance of I3DatasetData
             The instance of I3DatasetData holding the experimental data and
             good-run-list information.
@@ -80,24 +103,31 @@
                 zip(data.grl['start'], data.grl['stop'])):
             mask = (data.exp['time'] >= start) & (data.exp['time'] < stop)
             self.run_weights[i] = len(data.exp[mask]) / n_events
         self.run_weights /= np.sum(self.run_weights)
 
         self.grl = data.grl
 
-    def scramble(self, rss, data):
+    def scramble(
+            self,
+            rss,
+            dataset,
+            data,
+    ):
         """Scrambles the given data based on random MJD times, which are
         generated uniformely within the data runs, where the data runs are
         weighted based on their amount of events compared to the total events.
 
         Parameters
         ----------
         rss : instance of RandomStateService
             The random state service providing the random number
             generator (RNG).
+        dataset : instance of Dataset
+            The instance of Dataset for which the data should get scrambled.
         data : instance of DataFieldRecordArray
             The DataFieldRecordArray instance containing the to be scrambled
             data.
 
         Returns
         -------
         data : instance of DataFieldRecordArray
```

### Comparing `skyllh-23.1.1/skyllh/i3/signal_generation.py` & `skyllh-23.2.0/skyllh/i3/signal_generation.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,19 +1,25 @@
 # -*- coding: utf-8 -*-
 
 import numpy as np
 
 from skyllh.core.py import (
     get_smallest_numpy_int_type,
-    float_cast
+    float_cast,
+    int_cast,
+)
+from skyllh.core.utils.coords import (
+    rotate_spherical_vector,
+)
+from skyllh.core.signal_generation import (
+    SignalGenerationMethod,
+)
+from skyllh.core.source_model import (
+    PointLikeSource,
 )
-from skyllh.core.coords import rotate_spherical_vector
-from skyllh.core.signal_generation import SignalGenerationMethod
-from skyllh.physics.source import PointLikeSource
-from skyllh.physics.flux import get_conversion_factor_to_internal_flux_unit
 
 
 def source_sin_dec_shift_linear(x, w, L, U):
     """Calculates the shift of the sine of the source declination, in order to
     allow the construction of the source sine declination band with
     sin(dec_src) +/- w. This shift function, S(x), is implemented as a line
     with the following points:
@@ -77,28 +83,31 @@
         that ``sin(dec_src) + S`` is the new sin(dec) of the source, and
         ``sin(dec_src) + S +/- w`` is always within the sin(dec) range [L, U].
     """
     # TODO: Make sure that this function does what it's supposed to do
     x = np.atleast_1d(x)
 
     m = w / (x - 0.5*(L+U))**3
-    S = m * np.power(x-0.5*(L+U),3)
+    S = m * np.power(x-0.5*(L+U), 3)
 
     return S
 
 
 class PointLikeSourceI3SignalGenerationMethod(SignalGenerationMethod):
-    """This class provides a signal generation method for a point-like source
+    """This class provides a signal generation method for point-like sources
     seen in the IceCube detector.
     """
+
     def __init__(
         self,
         src_sin_dec_half_bandwidth=np.sin(np.radians(1)),
         src_sin_dec_shift_func=None,
-        energy_range=None
+        energy_range=None,
+        src_batch_size=128,
+        **kwargs
     ):
         """Constructs a new signal generation method instance for a point-like
         source detected with IceCube.
 
         Parameters
         ----------
         src_sin_dec_half_bandwidth : float
@@ -109,50 +118,75 @@
             constructing the source declination bands from where to draw
             monte-carlo events from. If set to None, the default function
             ``source_sin_dec_shift_linear`` will be used.
         energy_range : 2-element tuple of float | None
             The energy range from which to take MC events into account for
             signal event generation.
             If set to None, the entire energy range [0, +inf] is used.
+        src_batch_size : int
+            The source processing batch size used for the signal event flux
+            calculation.
         """
-        super(PointLikeSourceI3SignalGenerationMethod, self).__init__(
-            energy_range=energy_range)
+        super().__init__(
+            energy_range=energy_range,
+            **kwargs)
 
         self.src_sin_dec_half_bandwidth = src_sin_dec_half_bandwidth
 
-        if(src_sin_dec_shift_func is None):
+        if src_sin_dec_shift_func is None:
             src_sin_dec_shift_func = source_sin_dec_shift_linear
         self.src_sin_dec_shift_func = src_sin_dec_shift_func
 
+        self.src_batch_size = src_batch_size
+
     @property
     def src_sin_dec_half_bandwidth(self):
         """The half-width of the sin(dec) band to take MC events from around a
         source.
         """
         return self._src_sin_dec_half_bandwidth
+
     @src_sin_dec_half_bandwidth.setter
     def src_sin_dec_half_bandwidth(self, v):
-        v = float_cast(v, 'The src_sin_dec_half_bandwidth property must be '
-            'castable to a float type!')
+        v = float_cast(
+            v,
+            'The src_sin_dec_half_bandwidth property must be castable to type '
+            'float!')
         self._src_sin_dec_half_bandwidth = v
 
     @property
     def src_sin_dec_shift_func(self):
         """The function that provides the source sin(dec) shift needed for
         constructing the source declination bands from where to draw
         monte-carlo events from.
         """
         return self._src_sin_dec_shift_func
+
     @src_sin_dec_shift_func.setter
     def src_sin_dec_shift_func(self, func):
-        if(not callable(func)):
-            raise TypeError('The src_sin_dec_shift_func property must be a '
-                'callable object!')
+        if not callable(func):
+            raise TypeError(
+                'The src_sin_dec_shift_func property must be a callable '
+                'object!')
         self._src_sin_dec_shift_func = func
 
+    @property
+    def src_batch_size(self):
+        """The source processing batch size used for the signal event flux
+        calculation.
+        """
+        return self._src_batch_size
+
+    @src_batch_size.setter
+    def src_batch_size(self, v):
+        v = int_cast(
+            v,
+            'The src_batch_size property must be castable to type int!')
+        self._src_batch_size = v
+
     def _get_src_dec_bands(self, src_dec, max_sin_dec_range):
         """Calculates the minimum and maximum sin(dec) values for each source
         to use with a specified maximal sin(dec) range, which should get
         determined from the available MC data itself.
 
         Parameters
         ----------
@@ -178,99 +212,137 @@
         src_sin_dec += self._src_sin_dec_shift_func(
             src_sin_dec, self._src_sin_dec_half_bandwidth, *max_sin_dec_range)
 
         src_sin_dec_band_min = src_sin_dec - self._src_sin_dec_half_bandwidth
         src_sin_dec_band_max = src_sin_dec + self._src_sin_dec_half_bandwidth
 
         # Calculate the solid angle of the declination band.
-        src_dec_band_omega = 2*np.pi*(src_sin_dec_band_max - src_sin_dec_band_min)
+        src_dec_band_omega = (
+            2 * np.pi * (src_sin_dec_band_max - src_sin_dec_band_min)
+        )
 
         return (src_sin_dec_band_min, src_sin_dec_band_max, src_dec_band_omega)
 
     def calc_source_signal_mc_event_flux(self, data_mc, shg):
         """Calculates the signal flux of each given MC event for each source
-        hypothesis of the given source hypothesis group. The unit of the signal
-        flux must be 1/(GeV cm^2 s sr).
+        hypothesis of the given source hypothesis group.
 
         Parameters
         ----------
         data_mc : numpy record ndarray
             The numpy record array holding the MC events of a dataset.
         shg : SourceHypoGroup instance
             The source hypothesis group, which defines the list of sources, and
             their flux model.
 
         Returns
         -------
-        indices_list : list of 1D ndarrays
-            The list of event indices arrays specifying which MC events have
-            been selected as signal candidate events for each source of the
-            given source hypothesis group. Hence, the length of that list is the
-            number of sources of the source hypothesis group. The length of the
-            different 1D ndarrays is variable and depends on the source.
-        flux_list : list of 1D ndarrays
-            The list of 1D ndarrays holding the flux value of the selected
-            signal candidate events. One array for each source of the given
-            source hypothesis group. Hence, the length of that list is the
-            number of sources of the source hypothesis group. The length of the
-            different 1D ndarrays is variable and depends on the source.
+        ev_idx_arr : ndarray
+            The (N_selected_signal_events,)-shaped 1D ndarray holding the index
+            of the MC event.
+        shg_src_idx_arr : ndarray
+            The (N_selected_signal_events,)-shaped 1D ndarray holding the index
+            of the source within the given source hypothesis group for each
+            signal candidate event.
+        flux_arr : ndarray
+            The (N_selected_signal_events,)-shaped 1D ndarray holding the flux
+            value of each signal candidate event.
         """
         indices = np.arange(
             0, len(data_mc),
             dtype=get_smallest_numpy_int_type((0, len(data_mc)))
         )
         n_sources = shg.n_sources
 
         # Get 1D array of source declination.
         src_dec = np.empty((n_sources,), dtype=np.float64)
         for (k, source) in enumerate(shg.source_list):
-            if(not isinstance(source, PointLikeSource)):
-                raise TypeError('The source instance must be an instance of '
+            if not isinstance(source, PointLikeSource):
+                raise TypeError(
+                    'The source instance must be an instance of '
                     'PointLikeSource!')
             src_dec[k] = source.dec
 
         data_mc_sin_true_dec = data_mc['sin_true_dec']
         data_mc_true_energy = data_mc['true_energy']
 
         # Calculate the source declination bands and their solid angle.
         max_sin_dec_range = (
             np.min(data_mc_sin_true_dec),
             np.max(data_mc_sin_true_dec)
         )
-        (src_sin_dec_band_min, src_sin_dec_band_max, src_dec_band_omega) = self._get_src_dec_bands(src_dec, max_sin_dec_range)
+        (src_sin_dec_band_min, src_sin_dec_band_max, src_dec_band_omega) =\
+            self._get_src_dec_bands(src_dec, max_sin_dec_range)
 
-        # Get the flux model of this source hypo group.
+        # Get the flux model of this source hypo group (SHG).
         fluxmodel = shg.fluxmodel
 
+        # Get the theoretical weights of all the sources of this SHG.
+        src_weights = shg.get_source_weights()
+
         # Calculate conversion factor from the flux model unit into the internal
-        # flux unit GeV^-1 cm^-2 s^-1.
-        toGeVcm2s = get_conversion_factor_to_internal_flux_unit(fluxmodel)
+        # flux unit.
+        to_internal_flux_unit =\
+            fluxmodel.to_internal_flux_unit()
 
         # Select the events that belong to a given source.
-        indices_list = []
-        flux_list = []
+        ev_idx_arr = np.empty(
+            (0,),
+            dtype=get_smallest_numpy_int_type((0, len(data_mc))))
+        shg_src_idx_arr = np.empty(
+            (0,),
+            dtype=get_smallest_numpy_int_type((0, n_sources)))
+        flux_arr = np.empty(
+            (0,),
+            dtype=np.float32)
 
-        for k in range(n_sources):
-            # Create the sin(true_dec) range event mask for the source.
-            src_event_mask = (
-                (data_mc_sin_true_dec >= src_sin_dec_band_min[k]) &
-                (data_mc_sin_true_dec <= src_sin_dec_band_max[k])
+        src_batch_size = self._src_batch_size
+        n_batches = int(np.ceil(n_sources / src_batch_size))
+
+        for bi in range(n_batches):
+            src_start = bi*src_batch_size
+            src_end = np.min([(bi+1)*src_batch_size, n_sources])
+            bs = src_end - src_start
+
+            src_slice = slice(src_start, src_end)
+
+            # Create an event mask of shape (N_sources,N_events).
+            ev_mask = np.logical_and(
+                (data_mc_sin_true_dec >=
+                    src_sin_dec_band_min[src_slice][:, np.newaxis]),
+                (data_mc_sin_true_dec <=
+                    src_sin_dec_band_max[src_slice][:, np.newaxis])
             )
-            # Apply energy range cut if an energy range is defined.
-            if(self.energy_range is not None):
-                src_event_mask &= (
-                    (data_mc_true_energy >= self.energy_range[0]) &
+
+            if self.energy_range is not None:
+                ev_mask &= np.logical_and(
+                    (data_mc_true_energy >= self.energy_range[0]),
                     (data_mc_true_energy <= self.energy_range[1])
                 )
 
-            indices_list.append(indices[src_event_mask])
-            flux = fluxmodel(data_mc_true_energy[src_event_mask])*toGeVcm2s / src_dec_band_omega[k]
-            flux_list.append(flux)
+            ev_idxs = np.tile(indices, bs)[ev_mask.ravel()]
+            shg_src_idxs = bi*src_batch_size + np.repeat(
+                np.arange(bs),
+                ev_mask.sum(axis=1)
+            )
+            del ev_mask
+
+            flux = (
+                fluxmodel(E=data_mc_true_energy[ev_idxs]).squeeze() *
+                to_internal_flux_unit /
+                src_dec_band_omega[shg_src_idxs]
+            )
+            if src_weights is not None:
+                flux *= src_weights[shg_src_idxs]
 
-        return (indices_list, flux_list)
+            ev_idx_arr = np.append(ev_idx_arr, ev_idxs)
+            shg_src_idx_arr = np.append(shg_src_idx_arr, shg_src_idxs)
+            flux_arr = np.append(flux_arr, flux)
+
+        return (ev_idx_arr, shg_src_idx_arr, flux_arr)
 
     def signal_event_post_sampling_processing(
         self, shg, shg_sig_events_meta, shg_sig_events
     ):
         """Rotates the generated signal events to their source location for a
         given source hypothesis group.
 
@@ -319,168 +391,7 @@
             shg_src_sig_events['ra'] = ra
             shg_src_sig_events['dec'] = dec
             shg_src_sig_events['sin_dec'] = np.sin(dec)
 
             shg_sig_events[shg_src_mask] = shg_src_sig_events
 
         return shg_sig_events
-
-
-class MultiPointLikeSourceI3SignalGenerationMethod(
-        PointLikeSourceI3SignalGenerationMethod):
-    """This class provides a signal generation method for a multiple point-like
-    sources seen in the IceCube detector.
-    """
-    def __init__(
-        self,
-        src_sin_dec_half_bandwidth=np.sin(np.radians(1)),
-        src_sin_dec_shift_func=None,
-        energy_range=None,
-        batch_size=200
-    ):
-        """Constructs a new signal generation method instance for a point-like
-        source detected with IceCube.
-
-        Parameters
-        ----------
-        src_sin_dec_half_bandwidth : float
-            The half-width of the sin(dec) band to take MC events from around a
-            source. The default is sin(1deg), i.e. a 1deg half-bandwidth.
-        src_sin_dec_shift_func : callable | None
-            The function that provides the source sin(dec) shift needed for
-            constructing the source declination bands from where to draw
-            monte-carlo events from. If set to None, the default function
-            ``source_sin_dec_shift_linear`` will be used.
-        energy_range : 2-element tuple of float | None
-            The energy range from which to take MC events into account for
-            signal event generation.
-            If set to None, the entire energy range [0, +inf] is used.
-        batch_size : int, optional
-            Batch size for signal generation.
-        """
-        super(MultiPointLikeSourceI3SignalGenerationMethod, self).__init__(
-                    src_sin_dec_half_bandwidth=src_sin_dec_half_bandwidth,
-                    src_sin_dec_shift_func=src_sin_dec_shift_func,
-                    energy_range=energy_range
-                    )
-        self.batch_size = batch_size
-
-    def calc_source_signal_mc_event_flux(self, data_mc, shg):
-        """Calculates the signal flux of each given MC event for each source
-        hypothesis of the given source hypothesis group. The unit of the signal
-        flux must be 1/(GeV cm^2 s sr).
-
-        Parameters
-        ----------
-        data_mc : numpy record ndarray
-            The numpy record array holding the MC events of a dataset.
-        shg : SourceHypoGroup instance
-            The source hypothesis group, which defines the list of sources, and
-            their flux model.
-
-        Returns
-        -------
-        ev_indices : 1D ndarray
-            Event indices array specifying which MC events have been selected as
-            signal candidate events for each source of the given source
-            hypothesis group. The length of the 1D ndarray is variable and
-            depends on the source.
-        src_indices : 1D ndarray
-            Source indices array specifying which source corresponds to the
-            event in ev_indices array.
-        flux_list : list of 1D ndarrays
-            The list of 1D ndarrays holding the flux value of the selected
-            signal candidate events. One array for each source of the given
-            source hypothesis group. Hence, the length of that list is the
-            number of sources of the source hypothesis group. The length of the
-            different 1D ndarrays is variable and depends on the source.
-        """
-        indices = np.arange(
-            0, len(data_mc),
-            dtype=get_smallest_numpy_int_type((0, len(data_mc)))
-        )
-        n_sources = shg.n_sources
-
-        # Get 1D array of source declination.
-        src_dec = np.empty((n_sources,), dtype=np.float64)
-        for (k, source) in enumerate(shg.source_list):
-            if(not isinstance(source, PointLikeSource)):
-                raise TypeError(
-                    'The source instance must be an instance of '
-                    'PointLikeSource!')
-            src_dec[k] = source.dec
-
-        data_mc_sin_true_dec = data_mc['sin_true_dec']
-        data_mc_true_energy = data_mc['true_energy']
-
-        # Calculate the source declination bands and their solid angle.
-        max_sin_dec_range = (
-            np.min(data_mc_sin_true_dec),
-            np.max(data_mc_sin_true_dec)
-        )
-        (src_sin_dec_band_min, src_sin_dec_band_max, src_dec_band_omega) = self._get_src_dec_bands(src_dec, max_sin_dec_range)
-
-        # Get the flux model and source weights of this source hypo group.
-        fluxmodel = shg.fluxmodel
-        src_weights = shg.source_weights
-
-        # Calculate conversion factor from the flux model unit into the internal
-        # flux unit GeV^-1 cm^-2 s^-1.
-        toGeVcm2s = get_conversion_factor_to_internal_flux_unit(fluxmodel)
-
-        # Select the events that belong to a given source.
-        ev_indices = np.empty(
-            (0,), dtype=get_smallest_numpy_int_type((0, len(data_mc))))
-        src_indices = np.empty(
-            (0,), dtype=get_smallest_numpy_int_type((0, n_sources)))
-        flux = np.empty((0,), dtype='float32')
-
-        n_batches = int(np.ceil(n_sources / float(self.batch_size)))
-
-        for bi in range(n_batches):
-            if(bi != n_batches-1):
-                band_mask = np.logical_and(
-                            (data_mc_sin_true_dec >=
-                                src_sin_dec_band_min[bi*self.batch_size:(bi+1)*self.batch_size][:, np.newaxis]),
-                            (data_mc_sin_true_dec <=
-                                src_sin_dec_band_max[bi*self.batch_size:(bi+1)*self.batch_size][:, np.newaxis])
-                            )
-                if(self.energy_range is not None):
-                    band_mask &= np.logical_and(
-                        (data_mc_true_energy >= self.energy_range[0]),
-                        (data_mc_true_energy <= self.energy_range[1]))
-
-                ev_indi = np.tile(indices, self.batch_size)[band_mask.ravel()]
-                src_indi = bi*self.batch_size + np.repeat(
-                    np.arange(self.batch_size),
-                    band_mask.sum(axis=1)
-                )
-                del band_mask
-            else:
-                n_final_batch = int(n_sources - bi*self.batch_size)
-                band_mask = np.logical_and(
-                            (data_mc_sin_true_dec >=
-                                src_sin_dec_band_min[bi*self.batch_size:][:, np.newaxis]),
-                            (data_mc_sin_true_dec <=
-                                src_sin_dec_band_max[bi*self.batch_size:][:, np.newaxis])
-                            )
-                if(self.energy_range is not None):
-                    band_mask &= np.logical_and(
-                        (data_mc_true_energy >= self.energy_range[0]),
-                        (data_mc_true_energy <= self.energy_range[1]))
-
-                ev_indi = np.tile(indices, n_final_batch)[band_mask.ravel()]
-                src_indi = bi*self.batch_size + np.repeat(
-                    np.arange(n_final_batch),
-                    band_mask.sum(axis=1)
-                )
-                del band_mask
-
-            if(src_weights is None):
-                fluxi = fluxmodel(data_mc_true_energy[ev_indi])*toGeVcm2s / src_dec_band_omega[src_indi]
-            else:
-                fluxi = src_weights[src_indi]*fluxmodel(data_mc_true_energy[ev_indi])*toGeVcm2s / src_dec_band_omega[src_indi]
-
-            ev_indices = np.append(ev_indices, ev_indi)
-            src_indices = np.append(src_indices, src_indi)
-            flux = np.append(flux, fluxi)
-        return (ev_indices, src_indices, flux)
```

### Comparing `skyllh-23.1.1/skyllh/i3/signalpdf.py` & `skyllh-23.2.0/skyllh/i3/signalpdf.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,237 +1,258 @@
 # -*- coding: utf-8 -*-
 
 import numpy as np
 
-from skyllh.core.binning import BinningDefinition
+from skyllh.core.binning import (
+    BinningDefinition,
+)
+from skyllh.core.flux_model import (
+    FluxModel,
+)
 from skyllh.core.multiproc import (
     IsParallelizable,
-    parallelize
+    parallelize,
 )
 from skyllh.core.parameters import (
     ParameterGrid,
-    ParameterGridSet
+    ParameterGridSet,
 )
-from skyllh.core.smoothing import SmoothingFilter
 from skyllh.core.pdf import (
+    PDF,
     PDFSet,
-    IsSignalPDF
+    IsSignalPDF,
+)
+from skyllh.core.py import (
+    classname,
+)
+from skyllh.core.smoothing import (
+    SmoothingFilter,
+)
+from skyllh.i3.pdf import (
+    I3EnergyPDF,
 )
-from skyllh.physics.flux import FluxModel
-from skyllh.physics.source import PointLikeSource
-from skyllh.i3.pdf import I3EnergyPDF
 
 
-class SignalI3EnergyPDFSet(PDFSet, IsSignalPDF, IsParallelizable):
+class SignalI3EnergyPDFSet(
+        PDFSet,
+        IsSignalPDF,
+        PDF,
+        IsParallelizable,
+):
     """This is the signal energy PDF for IceCube. It creates a set of
     I3EnergyPDF objects for a discrete set of energy signal parameters. Energy
-    signal parameters are the parameters that influence the source flux model.
+    signal parameters influence the source's flux model.
     """
-    def __init__(self, data_mc, logE_binning, sinDec_binning, fluxmodel,
-                 fitparam_grid_set, smoothing_filter=None, ncpu=None,
-                 ppbar=None):
+    def __init__(
+            self,
+            cfg,
+            data_mc,
+            log10_energy_binning,
+            sin_dec_binning,
+            fluxmodel,
+            param_grid_set,
+            smoothing_filter=None,
+            ncpu=None,
+            ppbar=None,
+            **kwargs,
+    ):
         """Creates a new IceCube energy signal PDF for a given flux model and
-        a set of fit parameter grids for the flux model.
+        a set of parameter grids for the flux model.
         It creates a set of I3EnergyPDF objects for each signal parameter value
-        permutation and stores it inside the ``_params_hash_I3EnergyPDF_dict``
-        dictionary, where the hash of the fit parameters dictionary is the key.
+        permutation and stores it in an internal dictionary, where the hash of
+        the parameters dictionary is the key.
 
         Parameters
         ----------
+        cfg : instance of Config
+            The instance of Config holding the local configuration.
         data_mc : instance of DataFieldRecordArray
-            The array holding the monte-carlo data. The following data fields
-            must exist:
+            The instance of DataFieldRecordArray holding the monte-carlo data.
+            The following data fields must exist:
 
-            - 'true_energy' : float
+            true_energy : float
                 The true energy value of the data event.
-            - 'log_energy' : float
-                The logarithm of the reconstructed energy value of the data
-                event.
-            - 'dec' : float
+            log_energy : float
+                The base10-logarithm of the reconstructed energy value of the
+                data event.
+            sin_dec : float
                 The declination of the data event.
-            - 'mcweight' : float
+            mcweight : float
                 The monte-carlo weight value of the data events in unit
                 GeV cm^2 sr.
 
-        logE_binning : BinningDefinition
-            The binning definition for the binning in log10(E).
-        sinDec_binning : BinningDefinition
-            The binning definition for the sin(declination).
-        fluxmodel : FluxModel
+        log10_energy_binning : instance of BinningDefinition
+            The binning definition for the reconstructed energy binning in
+            log10(E).
+        sin_dec_binning : instance of BinningDefinition
+            The binning definition for the binning in sin(declination).
+        fluxmodel : instance of FluxModel
             The flux model to use to create the signal energy PDF.
-        fitparam_grid_set : ParameterGridSet | ParameterGrid
-            The set of parameter grids. A ParameterGrid object for each
-            energy fit parameter, for which an I3EnergyPDF object needs to be
+        param_grid_set : instance of ParameterGridSet |
+                         instance of ParameterGrid
+            The set of parameter grids. A ParameterGrid instance for each
+            energy parameter, for which an I3EnergyPDF object needs to be
             created.
-        smoothing_filter : SmoothingFilter instance | None
+        smoothing_filter : instance of SmoothingFilter | None
             The smoothing filter to use for smoothing the energy histogram.
-            If None, no smoothing will be applied.
-        ncpu : int | None (default)
+            If ``None``, no smoothing will be applied.
+        ncpu : int | None
             The number of CPUs to use to create the different I3EnergyPDF
-            objects for the different fit parameter grid values.
-        ppbar : ProgressBar instance | None
+            instances for the different parameter grid values.
+            If set to ``None``, the configured default number of CPUs will be
+            used.
+        ppbar : instance of ProgressBar | None
             The instance of ProgressBar of the optional parent progress bar.
         """
-        if(isinstance(fitparam_grid_set, ParameterGrid)):
-            fitparam_grid_set = ParameterGridSet([fitparam_grid_set])
-        if(not isinstance(fitparam_grid_set, ParameterGridSet)):
-            raise TypeError('The fitparam_grid_set argument must be an '
-                'instance of ParameterGrid or ParameterGridSet!')
+        if isinstance(param_grid_set, ParameterGrid):
+            param_grid_set = ParameterGridSet([param_grid_set])
+        if not isinstance(param_grid_set, ParameterGridSet):
+            raise TypeError(
+                'The param_grid_set argument must be an instance of '
+                'ParameterGrid or ParameterGridSet! But its type is '
+                f'{classname(param_grid_set)}!')
 
-        # We need to extend the fit parameter grids on the lower and upper end
+        # We need to extend the parameter grids on the lower and upper end
         # by one bin to allow for the calculation of the interpolation. But we
         # will do this on a copy of the object.
-        fitparam_grid_set = fitparam_grid_set.copy()
-        fitparam_grid_set.add_extra_lower_and_upper_bin()
+        param_grid_set = param_grid_set.copy()
+        param_grid_set.add_extra_lower_and_upper_bin()
 
-        super(SignalI3EnergyPDFSet, self).__init__(pdf_type=I3EnergyPDF,
-            fitparams_grid_set=fitparam_grid_set, ncpu=ncpu)
+        super().__init__(
+            cfg=cfg,
+            param_grid_set=param_grid_set,
+            ncpu=ncpu,
+            **kwargs)
+
+        if not isinstance(log10_energy_binning, BinningDefinition):
+            raise TypeError(
+                'The log10_energy_binning argument must be an instance of '
+                'BinningDefinition! '
+                f'Its type is {classname(log10_energy_binning)}!')
+        if not isinstance(sin_dec_binning, BinningDefinition):
+            raise TypeError(
+                'The sin_dec_binning argument must be an instance '
+                'of BinningDefinition! '
+                f'Its type is {classname(sin_dec_binning)}!')
+        if not isinstance(fluxmodel, FluxModel):
+            raise TypeError(
+                'The fluxmodel argument must be an instance of FluxModel! '
+                f'Its type is {classname(fluxmodel)}!')
+        if (smoothing_filter is not None) and\
+           (not isinstance(smoothing_filter, SmoothingFilter)):
+            raise TypeError(
+                'The smoothing_filter argument must be None or '
+                'an instance of SmoothingFilter! '
+                f'Its type is {classname(smoothing_filter)}!')
 
-        if(not isinstance(logE_binning, BinningDefinition)):
-            raise TypeError('The logE_binning argument must be an instance of '
-                'BinningDefinition!')
-        if(not isinstance(sinDec_binning, BinningDefinition)):
-            raise TypeError('The sinDec_binning argument must be an instance '
-                'of BinningDefinition!')
-        if(not isinstance(fluxmodel, FluxModel)):
-            raise TypeError('The fluxmodel argument must be an instance of '
-                'FluxModel!')
-        if((smoothing_filter is not None) and
-           (not isinstance(smoothing_filter, SmoothingFilter))):
-            raise TypeError('The smoothing_filter argument must be None or '
-                'an instance of SmoothingFilter!')
-
-        # Create I3EnergyPDF objects for all permutations of the fit parameter
+        # Create I3EnergyPDF objects for all permutations of the parameter
         # grid values.
         def create_I3EnergyPDF(
-            data_logE, data_sinDec, data_mcweight, data_true_energy,
-            logE_binning, sinDec_binning, smoothing_filter, fluxmodel,
-            gridfitparams):
+                cfg,
+                data_log10_energy,
+                data_sin_dec,
+                data_mcweight,
+                data_true_energy,
+                log10_energy_binning,
+                sin_dec_binning,
+                smoothing_filter,
+                fluxmodel,
+                flux_unit_conv_factor,
+                gridparams,
+        ):
             """Creates an I3EnergyPDF object for the given flux model and flux
             parameters.
 
             Parameters
             ----------
-            data_logE : 1d ndarray
-                The logarithm of the reconstructed energy value of the data
-                events.
-            data_sinDec : 1d ndarray
+            cfg : instance of Config
+                The instance of Config holding the local configuration.
+            data_log10_energy : 1d ndarray
+                The base-10 logarithm of the reconstructed energy value of the
+                data events.
+            data_sin_dec : 1d ndarray
                 The sin(dec) value of the the data events.
             data_mcweight : 1d ndarray
                 The monte-carlo weight value of the data events.
             data_true_energy : 1d ndarray
                 The true energy value of the data events.
-            logE_binning : BinningDefinition
+            log10_energy_binning : instance of BinningDefinition
                 The binning definition for the binning in log10(E).
-            sinDec_binning : BinningDefinition
+            sin_dec_binning : instance of BinningDefinition
                 The binning definition for the sin(declination).
-            smoothing_filter : SmoothingFilter instance | None
+            smoothing_filter : instance of SmoothingFilter | None
                 The smoothing filter to use for smoothing the energy histogram.
-                If None, no smoothing will be applied.
-            fluxmodel : FluxModel
+                If ``None``, no smoothing will be applied.
+            fluxmodel : instance of FluxModel
                 The flux model to use to create the signal event weights.
-            gridfitparams : dict
+            flux_unit_conv_factor : float
+                The factor to convert the flux unit into the internal flux unit.
+            gridparams : dict
                 The dictionary holding the specific signal flux parameters.
 
             Returns
             -------
-            i3energypdf : I3EnergyPDF
-                The created I3EnergyPDF object for the given flux model and flux
-                parameters.
+            i3energypdf : instance of I3EnergyPDF
+                The created I3EnergyPDF instance for the given flux model and
+                flux parameters.
             """
             # Create a copy of the FluxModel with the given flux parameters.
             # The copy is needed to not interfer with other CPU processes.
-            myfluxmodel = fluxmodel.copy(newprop=gridfitparams)
+            myfluxmodel = fluxmodel.copy(newparams=gridparams)
 
             # Calculate the signal energy weight of the event. Note, that
             # because we create a normalized PDF, we can ignore all constants.
             # So we don't have to convert the flux unit into the internally used
             # flux unit.
-            data_physicsweight = myfluxmodel(data_true_energy)
+            data_physicsweight = np.squeeze(myfluxmodel(E=data_true_energy))
+            data_physicsweight *= flux_unit_conv_factor
 
             i3energypdf = I3EnergyPDF(
-                data_logE, data_sinDec, data_mcweight, data_physicsweight,
-                logE_binning, sinDec_binning, smoothing_filter)
+                cfg=cfg,
+                pmm=None,
+                data_log10_energy=data_log10_energy,
+                data_sin_dec=data_sin_dec,
+                data_mcweight=data_mcweight,
+                data_physicsweight=data_physicsweight,
+                log10_energy_binning=log10_energy_binning,
+                sin_dec_binning=sin_dec_binning,
+                smoothing_filter=smoothing_filter)
 
             return i3energypdf
 
-        data_logE = data_mc['log_energy']
-        data_sinDec = np.sin(data_mc['dec'])
+        data_log10_energy = data_mc['log_energy']
+        data_sin_dec = data_mc['sin_dec']
         data_mcweight = data_mc['mcweight']
         data_true_energy = data_mc['true_energy']
 
-        args_list = [ ((data_logE, data_sinDec, data_mcweight, data_true_energy,
-                        logE_binning, sinDec_binning, smoothing_filter,
-                        fluxmodel, gridfitparams), {})
-                     for gridfitparams in self.gridfitparams_list ]
+        flux_unit_conv_factor =\
+            fluxmodel.to_internal_flux_unit()
+
+        args_list = [
+            (
+                (cfg,
+                 data_log10_energy,
+                 data_sin_dec,
+                 data_mcweight,
+                 data_true_energy,
+                 log10_energy_binning,
+                 sin_dec_binning,
+                 smoothing_filter,
+                 fluxmodel,
+                 flux_unit_conv_factor,
+                 gridparams),
+                {}
+            )
+            for gridparams in self.gridparams_list
+        ]
 
         i3energypdf_list = parallelize(
-            create_I3EnergyPDF, args_list, self.ncpu, ppbar=ppbar)
+            func=create_I3EnergyPDF,
+            args_list=args_list,
+            ncpu=self.ncpu,
+            ppbar=ppbar)
 
-        # Save all the I3EnergyPDF objects in the IsSignalPDF PDF registry with
+        # Save all the I3EnergyPDF instances in the PDFSet registry with
         # the hash of the individual parameters as key.
-        for (gridfitparams, i3energypdf) in zip(self.gridfitparams_list, i3energypdf_list):
-            self.add_pdf(i3energypdf, gridfitparams)
-
-    def assert_is_valid_for_exp_data(self, data_exp):
-        """Checks if this signal energy PDF is valid for all the given
-        experimental data.
-        It checks if all the data is within the logE and sin(dec) binning range.
-
-        Parameters
-        ----------
-        data_exp : numpy record ndarray
-            The array holding the experimental data. The following data fields
-            must exist:
-
-            - 'log_energy' : float
-                The logarithm of the energy value of the data event.
-            - 'dec' : float
-                The declination of the data event.
-
-        Raises
-        ------
-        ValueError
-            If some of the data is outside the logE or sin(dec) binning range.
-        """
-        # Since we use the same binning for all the I3EnergyPDF objects, we
-        # can just use an arbitrary object to verify the data.
-        self.get_pdf(self.pdf_keys[0]).assert_is_valid_for_exp_data(data_exp)
-
-    def get_prob(self, tdm, gridfitparams):
-        """Calculates the signal energy probability (in logE) of each event for
-        a given set of signal fit parameters on a grid.
-
-        Parameters
-        ----------
-        tdm : instance of TrialDataManager
-            The TrialDataManager instance holding the data events for which the
-            probability should be calculated for. The following data fields must
-            exist:
-
-            - 'log_energy' : float
-                The logarithm of the energy value of the event.
-            - 'sin_dec' : float
-                The sin(declination) value of the event.
-
-        gridfitparams : dict
-            The dictionary holding the signal parameter values for which the
-            signal energy probability should be calculated. Note, that the
-            parameter values must match a set of parameter grid values for which
-            an I3EnergyPDF object has been created at construction time of this
-            SignalI3EnergyPDF object. There is no interpolation method defined
-            at this point to allow for arbitrary parameter values!
-
-        Returns
-        -------
-        prob : 1d ndarray
-            The array with the signal energy probability for each event.
-
-        Raises
-        ------
-        KeyError
-            If no energy PDF can be found for the given signal parameter values.
-        """
-        i3energypdf = self.get_pdf(gridfitparams)
-
-        prob = i3energypdf.get_prob(tdm)
-        return prob
+        for (gridparams, i3energypdf) in zip(self.gridparams_list,
+                                             i3energypdf_list):
+            self.add_pdf(i3energypdf, gridparams)
```

### Comparing `skyllh-23.1.1/skyllh/i3/utils/sensitivity.py` & `skyllh-23.2.0/skyllh/i3/utils/analysis.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,22 @@
 # -*- coding: utf-8 -*-
 
 import logging
 import numpy as np
 
-from skyllh.core.analysis_utils import estimate_sensitivity, estimate_discovery_potential
-from skyllh.core.progressbar import ProgressBar
-from skyllh.physics.source import PointLikeSource
+from skyllh.core.utils.analysis import (
+    estimate_discovery_potential,
+    estimate_sensitivity,
+)
+from skyllh.core.progressbar import (
+    ProgressBar,
+)
+from skyllh.core.source_model import (
+    PointLikeSource,
+)
 
 
 def generate_ps_sin_dec_h0_ts_values(
         ana, rss, sin_dec_min, sin_dec_max, sin_dec_step, n_bkg_trials=10000,
         n_iter=1, bkg_kwargs=None, ppbar=None):
     """Generates sets of null-hypothesis, i.e. background-only trial data
     events, test-statistic values for the given point-source analysis for a
@@ -71,19 +78,19 @@
         '%d times each, that is %d trials in total',
         n_bkg_trials, len(sin_dec_arr), n_iter,
         n_bkg_trials*len(sin_dec_arr)*n_iter)
 
     pbar_iter = ProgressBar(n_iter, parent=ppbar).start()
     for iter_idx in range(n_iter):
         pbar_sin_dec = ProgressBar(len(sin_dec_arr), parent=pbar_iter).start()
-        for (sin_dec_idx,sin_dec) in enumerate(sin_dec_arr):
+        for (sin_dec_idx, sin_dec) in enumerate(sin_dec_arr):
             source = PointLikeSource(np.pi, np.arcsin(sin_dec))
             ana.change_source(source)
 
-            h0_ts_vals_arr[sin_dec_idx,iter_idx] = ana.do_trials(
+            h0_ts_vals_arr[sin_dec_idx, iter_idx] = ana.do_trials(
                 rss, n_bkg_trials, mean_n_sig=0, bkg_kwargs=bkg_kwargs,
                 ppbar=pbar_sin_dec)['ts']
 
             pbar_sin_dec.increment()
         pbar_sin_dec.finish()
 
         pbar_iter.increment()
@@ -176,77 +183,77 @@
     mu_max_arr = np.repeat(mu_max, len(sin_dec_arr))
 
     mean_ns_arr = np.empty((len(sin_dec_arr), n_iter))
     mean_ns_err_arr = np.empty((len(sin_dec_arr), n_iter))
     flux_scaling_arr = np.empty((len(sin_dec_arr), n_iter))
 
     pbar_sin_dec = ProgressBar(n_iter, parent=ppbar).start()
-    for (sin_dec_idx,sin_dec) in enumerate(sin_dec_arr):
+    for (sin_dec_idx, sin_dec) in enumerate(sin_dec_arr):
         logger.debug(
             'Estimate point-source sensitivity for sin(dec) = %g, %d times',
             sin_dec, n_iter)
         source = PointLikeSource(np.pi, np.arcsin(sin_dec))
         ana.change_source(source)
 
         pbar_iter = ProgressBar(len(sin_dec_arr), parent=pbar_sin_dec).start()
         for iter_idx in range(n_iter):
-            h0_ts_vals = h0_ts_vals_arr[sin_dec_idx,iter_idx]
+            h0_ts_vals = h0_ts_vals_arr[sin_dec_idx, iter_idx]
 
             mu_min = mu_min_arr[sin_dec_idx]
             mu_max = mu_max_arr[sin_dec_idx]
 
             (mean_ns, mean_ns_err) = estimate_sensitivity(
-                ana, rss, mu_range=(mu_min,mu_max), eps_p=eps_p,
+                ana, rss, mu_range=(mu_min, mu_max), eps_p=eps_p,
                 h0_ts_vals=h0_ts_vals, bkg_kwargs=bkg_kwargs,
                 sig_kwargs=sig_kwargs, ppbar=pbar_iter)
 
-            mean_ns_arr[sin_dec_idx,iter_idx] = mean_ns
-            mean_ns_err_arr[sin_dec_idx,iter_idx] = mean_ns_err
-            flux_scaling_arr[sin_dec_idx,iter_idx] = ana.calculate_fluxmodel_scaling_factor(
+            mean_ns_arr[sin_dec_idx, iter_idx] = mean_ns
+            mean_ns_err_arr[sin_dec_idx, iter_idx] = mean_ns_err
+            flux_scaling_arr[sin_dec_idx, iter_idx] = ana.calculate_fluxmodel_scaling_factor(
                 mean_ns=mean_ns, fitparam_values=np.array(fitparam_values))
 
             # A new iteration is done, update the mu range using the previous
             # results.
-            mu_min_arr = np.mean(mean_ns_arr[:,0:iter_idx+1]*0.8, axis=1)
-            mu_max_arr = np.mean(mean_ns_arr[:,0:iter_idx+1]*1.2, axis=1)
+            mu_min_arr = np.mean(mean_ns_arr[:, 0:iter_idx+1]*0.8, axis=1)
+            mu_max_arr = np.mean(mean_ns_arr[:, 0:iter_idx+1]*1.2, axis=1)
 
             rss.reseed(rss.seed+1)
 
             pbar_iter.increment()
         pbar_iter.finish()
 
         # It could happen that the first estimation wasn't very accurate due to
         # the unknown seed range for mu. We check for that by calculating the
         # variance of the further iterations and check if the first estimation
         # deviates more than 2 times that variance. If so, recalculate the first
         # estimation.
-        if(n_iter >= 5):
-            mean_ns = mean_ns_arr[sin_dec_idx,0]
-            mean_ns_mean = np.mean(mean_ns_arr[sin_dec_idx,1:])
-            mean_ns_std = np.std(mean_ns_arr[sin_dec_idx,1:])
-            if(np.abs(mean_ns - mean_ns_mean) >= 2*mean_ns_std):
+        if n_iter >= 5:
+            mean_ns = mean_ns_arr[sin_dec_idx, 0]
+            mean_ns_mean = np.mean(mean_ns_arr[sin_dec_idx, 1:])
+            mean_ns_std = np.std(mean_ns_arr[sin_dec_idx, 1:])
+            if np.abs(mean_ns - mean_ns_mean) >= 2*mean_ns_std:
                 logger.debug(
                     'Detected unprecise estimate for first iteration (mu=%g) '
                     'for sin(dec)=%g: (|%g - %g| >= 2*%g). Recalculating ...',
                     mean_ns, sin_dec, mean_ns, mean_ns_mean, mean_ns_std)
                 iter_idx = 0
 
-                h0_ts_vals = h0_ts_vals_arr[sin_dec_idx,iter_idx]
+                h0_ts_vals = h0_ts_vals_arr[sin_dec_idx, iter_idx]
 
                 mu_min = mu_min_arr[sin_dec_idx]
                 mu_max = mu_max_arr[sin_dec_idx]
 
                 (mean_ns, mean_ns_err) = estimate_sensitivity(
-                    ana, rss, mu_range=(mu_min,mu_max), eps_p=eps_p,
+                    ana, rss, mu_range=(mu_min, mu_max), eps_p=eps_p,
                     h0_ts_vals=h0_ts_vals, bkg_kwargs=bkg_kwargs,
                     sig_kwargs=sig_kwargs, ppbar=pbar_sin_dec)
 
-                mean_ns_arr[sin_dec_idx,iter_idx] = mean_ns
-                mean_ns_err_arr[sin_dec_idx,iter_idx] = mean_ns_err
-                flux_scaling_arr[sin_dec_idx,iter_idx] =   ana.calculate_fluxmodel_scaling_factor(
+                mean_ns_arr[sin_dec_idx, iter_idx] = mean_ns
+                mean_ns_err_arr[sin_dec_idx, iter_idx] = mean_ns_err
+                flux_scaling_arr[sin_dec_idx, iter_idx] = ana.calculate_fluxmodel_scaling_factor(
                     mean_ns=mean_ns, fitparam_values=np.array(fitparam_values))
 
         pbar_sin_dec.increment()
     pbar_sin_dec.finish()
 
     return (sin_dec_arr, mean_ns_arr, mean_ns_err_arr, flux_scaling_arr)
 
@@ -338,40 +345,40 @@
     mean_ns_arr = np.empty((len(sin_dec_arr), n_iter))
     mean_ns_err_arr = np.empty((len(sin_dec_arr), n_iter))
     flux_scaling_arr = np.empty((len(sin_dec_arr), n_iter))
 
     pbar_iter = ProgressBar(n_iter, parent=ppbar).start()
     for iter_idx in range(n_iter):
         pbar = ProgressBar(len(sin_dec_arr), parent=pbar_iter).start()
-        for (sin_dec_idx,sin_dec) in enumerate(sin_dec_arr):
+        for (sin_dec_idx, sin_dec) in enumerate(sin_dec_arr):
             source = PointLikeSource(np.pi, np.arcsin(sin_dec))
             ana.change_source(source)
 
-            h0_ts_vals = h0_ts_vals_arr[sin_dec_idx,iter_idx]
+            h0_ts_vals = h0_ts_vals_arr[sin_dec_idx, iter_idx]
 
             mu_min = mu_min_arr[sin_dec_idx]
             mu_max = mu_max_arr[sin_dec_idx]
 
             (mean_ns, mean_ns_err) = estimate_discovery_potential(
                 ana, rss, h0_ts_quantile=h0_ts_quantile,
-                mu_range=(mu_min,mu_max), eps_p=eps_p,
+                mu_range=(mu_min, mu_max), eps_p=eps_p,
                 h0_ts_vals=h0_ts_vals, bkg_kwargs=bkg_kwargs,
                 sig_kwargs=sig_kwargs, ppbar=pbar, **kwargs)
 
-            mean_ns_arr[sin_dec_idx,iter_idx] = mean_ns
-            mean_ns_err_arr[sin_dec_idx,iter_idx] = mean_ns_err
-            flux_scaling_arr[sin_dec_idx,iter_idx] = ana.calculate_fluxmodel_scaling_factor(
+            mean_ns_arr[sin_dec_idx, iter_idx] = mean_ns
+            mean_ns_err_arr[sin_dec_idx, iter_idx] = mean_ns_err
+            flux_scaling_arr[sin_dec_idx, iter_idx] = ana.calculate_fluxmodel_scaling_factor(
                 mean_ns=mean_ns, fitparam_values=np.array(fitparam_values))
 
             pbar.increment()
         pbar.finish()
 
         # One iteration is done, update the mu range using the previous results.
-        mu_min_arr = np.mean(mean_ns_arr[:,0:iter_idx+1]*0.8, axis=1)
-        mu_max_arr = np.mean(mean_ns_arr[:,0:iter_idx+1]*1.2, axis=1)
+        mu_min_arr = np.mean(mean_ns_arr[:, 0:iter_idx+1]*0.8, axis=1)
+        mu_max_arr = np.mean(mean_ns_arr[:, 0:iter_idx+1]*1.2, axis=1)
 
         pbar_iter.increment()
 
         rss.reseed(rss.seed+1)
     pbar_iter.finish()
 
     return (sin_dec_arr, mean_ns_arr, mean_ns_err_arr, flux_scaling_arr)
```

### Comparing `skyllh-23.1.1/skyllh/physics/flux_model.py` & `skyllh-23.2.0/skyllh/core/flux_model.py`

 * *Files 18% similar despite different names*

```diff
@@ -2,286 +2,370 @@
 
 r"""The `flux_model` module contains classes for different flux models. The
 class for the most generic flux model is `FluxModel`, which is an abstract base
 class. It describes a mathematical function for the differential flux:
 
 .. math::
 
-    d^4\Phi_S(\alpha,\delta,E,t | \vec{x}_s,\vec{p}_s) / (dA d\Omega dE dt)
-"""
+    \frac{d^4\Phi(\alpha,\delta,E,t | \vec{p}_{\mathrm{s}})}{\mathrm{d}A
+    \mathrm{d}\Omega \mathrm{d}E \mathrm{d}t}
 
-from __future__ import division
+"""
 
 import abc
+from astropy import (
+    units,
+)
 import numpy as np
+from scipy.integrate import (
+    quad,
+)
+import scipy.special
 import scipy.stats
 
-from astropy import units
-from copy import deepcopy
-
-from skyllh.core.config import CFG
-from skyllh.core.math import MathFunction
-from skyllh.core.model import Model
+from skyllh.core import (
+    tool,
+)
+from skyllh.core.config import (
+    Config,
+    HasConfig,
+)
+from skyllh.core.math import (
+    MathFunction,
+)
+from skyllh.core.model import (
+    Model,
+)
 from skyllh.core.py import (
     classname,
-    isproperty,
-    issequence,
-    issequenceof,
-    float_cast
+    float_cast,
+)
+from skyllh.core.source_model import (
+    IsPointlike,
 )
 
 
-class FluxProfile(MathFunction, metaclass=abc.ABCMeta):
+class FluxProfile(
+        MathFunction,
+        HasConfig,
+        metaclass=abc.ABCMeta):
     """The abstract base class for a flux profile math function.
     """
-
-    def __init__(self):
-        super(FluxProfile, self).__init__()
+    def __init__(
+            self,
+            **kwargs,
+    ):
+        """Creates a new FluxProfile instance.
+        """
+        super().__init__(**kwargs)
 
 
-class SpatialFluxProfile(FluxProfile, metaclass=abc.ABCMeta):
+class SpatialFluxProfile(
+        FluxProfile,
+        metaclass=abc.ABCMeta):
     """The abstract base class for a spatial flux profile function.
     """
-
     def __init__(
-            self, angle_unit=None):
+            self,
+            angle_unit=None,
+            **kwargs):
         """Creates a new SpatialFluxProfile instance.
 
         Parameters
         ----------
         angle_unit : instance of astropy.units.UnitBase | None
             The used unit for angles.
             If set to ``one``, the configured default angle unit for fluxes is
             used.
         """
-        super(SpatialFluxProfile, self).__init__()
+        super().__init__(
+            **kwargs)
 
         self.angle_unit = angle_unit
 
     @property
     def angle_unit(self):
         """The set unit of angle used for this spatial flux profile.
         If set to ``one`` the configured default angle unit for fluxes is used.
         """
         return self._angle_unit
+
     @angle_unit.setter
     def angle_unit(self, unit):
-        if(unit is None):
-            unit = CFG['units']['defaults']['fluxes']['angle']
-        if(not isinstance(unit, units.UnitBase)):
-            raise TypeError('The property angle_unit must be of type '
+        if unit is None:
+            unit = self._cfg['units']['defaults']['fluxes']['angle']
+        if not isinstance(unit, units.UnitBase):
+            raise TypeError(
+                'The property angle_unit must be of type '
                 'astropy.units.UnitBase!')
         self._angle_unit = unit
 
     @abc.abstractmethod
-    def __call__(self, alpha, delta, unit=None):
+    def __call__(
+            self,
+            ra,
+            dec,
+            unit=None):
         """This method is supposed to return the spatial profile value for the
         given celestrial coordinates.
 
         Parameters
         ----------
-        alpha : float | 1d numpy ndarray of float
+        ra : float | 1d numpy ndarray of float
             The right-ascention coordinate.
-        delta : float | 1d numpy ndarray of float
+        dec : float | 1d numpy ndarray of float
             The declination coordinate.
         unit : instance of astropy.units.UnitBase | None
             The unit of the given celestrial angles.
             If ``None``, the set angle unit of this SpatialFluxProfile is
             assumed.
 
         Returns
         -------
         values : 1D numpy ndarray
             The spatial profile values.
         """
         pass
 
 
-class UnitySpatialFluxProfile(SpatialFluxProfile):
+class UnitySpatialFluxProfile(
+        SpatialFluxProfile):
     """Spatial flux profile for the constant profile function 1 for any spatial
     coordinates.
     """
-    def __init__(self, angle_unit=None):
+    def __init__(
+            self,
+            angle_unit=None,
+            **kwargs,
+    ):
         """Creates a new UnitySpatialFluxProfile instance.
 
         Parameters
         ----------
         angle_unit : instance of astropy.units.UnitBase | None
             The used unit for angles.
             If set to ``one``, the configured default angle unit for fluxes is
             used.
         """
-        super(UnitySpatialFluxProfile, self).__init__(
-            angle_unit=angle_unit)
+        super().__init__(
+            angle_unit=angle_unit,
+            **kwargs)
 
     @property
     def math_function_str(self):
+        """(read-only) The string representation of the mathematical function of
+        this spatial flux profile instance.
+        """
         return '1'
 
-    def __call__(self, alpha, delta, unit=None):
-        """Returns 1 as numpy ndarray in same shape as alpha and delta.
+    def __call__(
+            self,
+            ra,
+            dec,
+            unit=None):
+        """Returns 1 as numpy ndarray in same shape as ra and dec.
 
         Parameters
         ----------
-        alpha : float | 1d numpy ndarray of float
+        ra : float | 1d numpy ndarray of float
             The right-ascention coordinate.
-        delta : float | 1d numpy ndarray of float
+        dec : float | 1d numpy ndarray of float
             The declination coordinate.
         unit : instance of astropy.units.UnitBase | None
             The unit of the given celestrial angles.
             By the definition of this class this argument is ignored.
 
         Returns
         -------
         values : 1D numpy ndarray
-            1 in same shape as alpha and delta.
+            1 in same shape as ra and dec.
         """
-        (alpha, delta) = np.atleast_1d(alpha, delta)
-        if(alpha.shape != delta.shape):
-            raise ValueError('The alpha and delta arguments must be of the '
-                'same shape!')
+        (ra, dec) = np.atleast_1d(ra, dec)
+        if ra.shape != dec.shape:
+            raise ValueError(
+                'The ra and dec arguments must be of the same shape!')
 
-        return np.ones_like(alpha)
+        return np.ones_like(ra)
 
 
-class PointSpatialFluxProfile(SpatialFluxProfile):
+class PointSpatialFluxProfile(
+        SpatialFluxProfile):
     """Spatial flux profile for a delta function at the celestrical coordinate
-    (alpha_s, delta_s).
+    (ra, dec).
     """
-    def __init__(self, alpha_s, delta_s, angle_unit=None):
-        """Creates a new spatial flux profile for a point.
+    def __init__(
+            self,
+            ra,
+            dec,
+            angle_unit=None,
+            **kwargs,
+    ):
+        """Creates a new spatial flux profile for a point at equatorial
+        coordinate (ra, dec).
 
         Parameters
         ----------
-        alpha_s : float
-            The right-ascention of the point-like source.
-        delta_s : float
-            The declination of the point-like source.
+        ra : float | None
+            The right-ascention of the point.
+            In case it is None, the evaluation of this spatial flux profile will
+            return zero, unless evaluated for ra=None.
+        dec : float | None
+            The declination of the point.
+            In case it is None, the evaluation of this spatial flux profile will
+            return zero, unless evaluated for dec=None.
         angle_unit : instance of astropy.units.UnitBase | None
             The used unit for angles.
             If set to ``one``, the configured default angle unit for fluxes is
             used.
         """
-        super(PointSpatialFluxProfile, self).__init__(
-            angle_unit=angle_unit)
+        super().__init__(
+            angle_unit=angle_unit,
+            **kwargs)
 
-        self.alpha_s = alpha_s
-        self.delta_s = delta_s
+        self.ra = ra
+        self.dec = dec
 
         # Define the names of the parameters, which can be updated.
-        self.param_names = ('alpha_s', 'delta_s')
+        self.param_names = ('ra', 'dec')
 
     @property
-    def alpha_s(self):
-        """The right-ascention of the point-like source.
+    def ra(self):
+        """The right-ascention of the point.
         The unit is the set angle unit of this SpatialFluxProfile instance.
         """
-        return self._alpha_s
-    @alpha_s.setter
-    def alpha_s(self, v):
-        v = float_cast(v,
-            'The alpha_s property must be castable to type float!')
-        self._alpha_s = v
+        return self._ra
+
+    @ra.setter
+    def ra(self, v):
+        v = float_cast(
+            v,
+            'The ra property must be castable to type float!',
+            allow_None=True)
+        self._ra = v
 
     @property
-    def delta_s(self):
-        """The declination of the point-like source.
+    def dec(self):
+        """The declination of the point.
         The unit is the set angle unit of this SpatialFluxProfile instance.
         """
-        return self._delta_s
-    @delta_s.setter
-    def delta_s(self, v):
-        v = float_cast(v,
-            'The delta_s property must be castable to type float!')
-        self._delta_s = v
+        return self._dec
+
+    @dec.setter
+    def dec(self, v):
+        v = float_cast(
+            v,
+            'The dec property must be castable to type float!',
+            allow_None=True)
+        self._dec = v
 
     @property
     def math_function_str(self):
         """(read-only) The string representation of the mathematical function of
-        this spatial flux profile instance.
+        this spatial flux profile instance. It is None, if the right-ascention
+        or declination property is set to None.
         """
-        return 'delta(alpha-%g%s)*delta(delta-%g%s)'%(
-            self._alpha_s, self._angle_unit.to_string(), self._delta_s,
-            self._angle_unit.to_string())
+        if (self._ra is None) or (self._dec is None):
+            return None
 
-    def __call__(self, alpha, delta, unit=None):
-        """Returns a numpy ndarray in same shape as alpha and delta with 1 if
-        `alpha` equals `alpha_s` and `delta` equals `delta_s`, and 0 otherwise.
+        s = (f'delta(ra-{self._ra:g}{self._angle_unit})*'
+             f'delta(dec-{self._dec:g}{self._angle_unit})')
+
+        return s
+
+    def __call__(
+            self,
+            ra,
+            dec,
+            unit=None):
+        """Returns a numpy ndarray in same shape as ra and dec with 1 if
+        `ra` equals `self.ra` and `dec` equals `self.dec`, and 0 otherwise.
 
         Parameters
         ----------
-        alpha : float | 1d numpy ndarray of float
+        ra : float | 1d numpy ndarray of float
             The right-ascention coordinate at which to evaluate the spatial flux
             profile. The unit must be the internally used angle unit.
-        delta : float | 1d numpy ndarray of float
+        dec : float | 1d numpy ndarray of float
             The declination coordinate at which to evaluate the spatial flux
             profile. The unit must be the internally used angle unit.
         unit : instance of astropy.units.UnitBase | None
             The unit of the given celestrial angles.
             If set to ``None``, the set angle unit of this SpatialFluxProfile
             instance is assumed.
 
         Returns
         -------
         value : 1D numpy ndarray of int8
-            A numpy ndarray in same shape as alpha and delta with 1 if `alpha`
-            equals `alpha_s` and `delta` equals `delta_s`, and 0 otherwise.
+            A numpy ndarray in same shape as ra and dec with 1 if `ra`
+            equals `self.ra` and `dec` equals `self.dec`, and 0 otherwise.
         """
-        (alpha, delta) = np.atleast_1d(alpha, delta)
-        if(alpha.shape != delta.shape):
-            raise ValueError('The alpha and delta arguments must be of the '
-                'same shape!')
+        (ra, dec) = np.atleast_1d(ra, dec)
+        if ra.shape != dec.shape:
+            raise ValueError(
+                'The ra and dec arguments must be of the same shape!')
 
-        if((unit is not None) and (unit != self._angle_unit)):
+        if (unit is not None) and (unit != self._angle_unit):
             angle_unit_conv_factor = unit.to(self._angle_unit)
-            alpha = alpha * angle_unit_conv_factor
-            delta = delta * angle_unit_conv_factor
+            ra = ra * angle_unit_conv_factor
+            dec = dec * angle_unit_conv_factor
 
-        value = ((alpha == self._alpha_s) &
-                 (delta == self._delta_s)).astype(np.int8, copy=False)
+        value = (
+            (ra == self._ra) &
+            (dec == self._dec)
+        ).astype(np.int8, copy=False)
 
         return value
 
 
-class EnergyFluxProfile(FluxProfile, metaclass=abc.ABCMeta):
+class EnergyFluxProfile(
+        FluxProfile,
+        metaclass=abc.ABCMeta):
     """The abstract base class for an energy flux profile function.
     """
-
-    def __init__(self, energy_unit=None):
+    def __init__(
+            self,
+            energy_unit=None,
+            **kwargs):
         """Creates a new energy flux profile with a given energy unit to be used
         for flux calculation.
 
         Parameters
         ----------
         energy_unit : instance of astropy.units.UnitBase | None
             The used unit for energy.
             If set to ``None``, the configured default energy unit for fluxes is
             used.
         """
-        super(EnergyFluxProfile, self).__init__()
+        super().__init__(
+            **kwargs)
 
         # Set the energy unit.
         self.energy_unit = energy_unit
 
     @property
     def energy_unit(self):
         """The unit of energy used for the flux profile calculation.
         """
         return self._energy_unit
+
     @energy_unit.setter
     def energy_unit(self, unit):
-        if(unit is None):
-            unit = CFG['units']['defaults']['fluxes']['energy']
-        if(not isinstance(unit, units.UnitBase)):
-            raise TypeError('The property energy_unit must be of type '
+        if unit is None:
+            unit = self._cfg['units']['defaults']['fluxes']['energy']
+        if not isinstance(unit, units.UnitBase):
+            raise TypeError(
+                'The property energy_unit must be of type '
                 'astropy.units.UnitBase!')
         self._energy_unit = unit
 
     @abc.abstractmethod
-    def __call__(self, E, unit=None):
+    def __call__(
+            self,
+            E,
+            unit=None):
         """This method is supposed to return the energy profile value for the
         given energy value.
 
         Parameters
         ----------
         E : float | 1d numpy ndarray of float
             The energy value for which to retrieve the energy profile value.
@@ -293,39 +377,95 @@
         Returns
         -------
         values : 1D numpy ndarray of float
             The energy profile values for the given energies.
         """
         pass
 
+    def get_integral(
+            self,
+            E1,
+            E2,
+            unit=None,
+    ):
+        """This is the default implementation for calculating the integral value
+        of this energy flux profile in the range ``[E1, E2]``.
+
+        .. note::
+
+            This implementation utilizes the ``scipy.integrate.quad`` function
+            to perform a generic numeric integration. Hence, this implementation
+            is slow and should be reimplemented by the derived class if an
+            analytic integral form is available.
+
+        Parameters
+        ----------
+        E1 : float | 1d numpy ndarray of float
+            The lower energy bound of the integration.
+        E2 : float | 1d numpy ndarray of float
+            The upper energy bound of the integration.
+        unit : instance of astropy.units.UnitBase | None
+            The unit of the given energies.
+            If set to ``None``, the set energy unit of this EnergyFluxProfile
+            instance is assumed.
+
+        Returns
+        -------
+        integral : instance of ndarray
+            The (n,)-shaped numpy ndarray holding the integral values of the
+            given integral ranges.
+        """
+        E1 = np.atleast_1d(E1)
+        E2 = np.atleast_1d(E2)
+
+        if (unit is not None) and (unit != self._energy_unit):
+            time_unit_conv_factor = unit.to(self._energy_unit)
+            E1 = E1 * time_unit_conv_factor
+            E2 = E2 * time_unit_conv_factor
+
+        integral = np.empty((len(E1),), dtype=np.float64)
+
+        for (i, (E1_i, E2_i)) in enumerate(zip(E1, E2)):
+            integral[i] = quad(self, E1_i, E2_i, full_output=True)[0]
+
+        return integral
+
 
-class UnityEnergyFluxProfile(EnergyFluxProfile):
+class UnityEnergyFluxProfile(
+        EnergyFluxProfile):
     """Energy flux profile for the constant function 1.
     """
-    def __init__(self, energy_unit=None):
+    def __init__(
+            self,
+            energy_unit=None,
+            **kwargs):
         """Creates a new UnityEnergyFluxProfile instance.
 
         Parameters
         ----------
         energy_unit : instance of astropy.units.UnitBase | None
             The used unit for energy.
             If set to ``None``, the configured default energy unit for fluxes is
             used.
         """
-        super(UnityEnergyFluxProfile, self).__init__(
-            energy_unit=energy_unit)
+        super().__init__(
+            energy_unit=energy_unit,
+            **kwargs)
 
     @property
     def math_function_str(self):
-        """The string representation of the mathematical function of this energy
-        flux profile.
+        """(read-only) The string representation of the mathematical function of
+        this energy flux profile.
         """
         return '1'
 
-    def __call__(self, E, unit=None):
+    def __call__(
+            self,
+            E,
+            unit=None):
         """Returns 1 as numpy ndarray in some shape as E.
 
         Parameters
         ----------
         E : float | 1D numpy ndarray of float
             The energy value for which to retrieve the energy profile value.
         unit : instance of astropy.units.UnitBase | None
@@ -339,77 +479,133 @@
         """
         E = np.atleast_1d(E)
 
         values = np.ones_like(E, dtype=np.int8)
 
         return values
 
+    def get_integral(
+            self,
+            E1,
+            E2,
+            unit=None):
+        """Computes the integral of this energy flux profile in the range
+        [``E1``, ``E2``], which by definition is ``E2 - E1``.
 
-class PowerLawEnergyFluxProfile(EnergyFluxProfile):
-    """Energy flux profile for a power law profile with a reference energy
+        Parameters
+        ----------
+        E1 : float | 1d numpy ndarray of float
+            The lower energy bound of the integration.
+        E2 : float | 1d numpy ndarray of float
+            The upper energy bound of the integration.
+        unit : instance of astropy.units.UnitBase | None
+            The unit of the given energies.
+            If set to ``None``, the set energy unit of this EnergyFluxProfile
+            instance is assumed.
+
+        Returns
+        -------
+        integral : 1d ndarray of float
+            The integral values of the given integral ranges.
+        """
+        E1 = np.atleast_1d(E1)
+        E2 = np.atleast_1d(E2)
+
+        if (unit is not None) and (unit != self._energy_unit):
+            time_unit_conv_factor = unit.to(self._energy_unit)
+            E1 = E1 * time_unit_conv_factor
+            E2 = E2 * time_unit_conv_factor
+
+        integral = E2 - E1
+
+        return integral
+
+
+class PowerLawEnergyFluxProfile(
+        EnergyFluxProfile,
+):
+    r"""Energy flux profile for a power law profile with a reference energy
     ``E0`` and a spectral index ``gamma``.
 
     .. math::
+
         (E / E_0)^{-\gamma}
+
     """
-    def __init__(self, E0, gamma, energy_unit=None):
+    def __init__(
+            self,
+            E0,
+            gamma,
+            energy_unit=None,
+            **kwargs):
         """Creates a new power law flux profile with the reference energy ``E0``
         and spectral index ``gamma``.
 
         Parameters
         ----------
         E0 : castable to float
             The reference energy.
         gamma : castable to float
             The spectral index.
         energy_unit : instance of astropy.units.UnitBase | None
             The used unit for energy.
             If set to ``None``, the configured default energy unit for fluxes is
             used.
         """
-        super(PowerLawEnergyFluxProfile, self).__init__(
-            energy_unit=energy_unit)
+        super().__init__(
+            energy_unit=energy_unit,
+            **kwargs)
 
         self.E0 = E0
         self.gamma = gamma
 
-        # Define the parameters which can be set via the `set_parameters`
+        # Define the parameters which can be set via the `set_params`
         # method.
-        self.parameter_names = ('E0', 'gamma',)
+        self.param_names = ('E0', 'gamma',)
 
     @property
     def E0(self):
         """The reference energy in the set energy unit of this EnergyFluxProfile
         instance.
         """
         return self._E0
+
     @E0.setter
     def E0(self, v):
-        v = float_cast(v,
+        v = float_cast(
+            v,
             'Property E0 must be castable to type float!')
         self._E0 = v
 
     @property
     def gamma(self):
         """The spectral index.
         """
         return self._gamma
+
     @gamma.setter
     def gamma(self, v):
-        v = float_cast(v,
+        v = float_cast(
+            v,
             'Property gamma must be castable to type float!')
         self._gamma = v
 
     @property
     def math_function_str(self):
-        """The string representation of this EnergyFluxProfile instance.
+        """(read-only) The string representation of this energy flux profile
+        instance.
         """
-        return '(E / (%g %s))^-%g'%(self._E0, self._energy_unit, self._gamma)
+        s = f'(E / ({self._E0:g} {self._energy_unit}))^-{self._gamma:g}'
 
-    def __call__(self, E, unit=None):
+        return s
+
+    def __call__(
+            self,
+            E,
+            unit=None):
         """Returns the power law values for the given energies as numpy ndarray
         in same shape as E.
 
         Parameters
         ----------
         E : float | 1D numpy ndarray of float
             The energy value for which to retrieve the energy profile value.
@@ -421,116 +617,483 @@
         Returns
         -------
         values : 1D numpy ndarray of float
             The energy profile values for the given energies.
         """
         E = np.atleast_1d(E)
 
-        if((unit is not None) and (unit != self._energy_unit)):
-            energy_unit_conv_factor = unit.to(self._energy_unit)
-            E = E * energy_unit_conv_factor
+        if (unit is not None) and (unit != self._energy_unit):
+            E = E * unit.to(self._energy_unit)
 
         value = np.power(E / self._E0, -self._gamma)
 
         return value
 
+    def get_integral(
+            self,
+            E1,
+            E2,
+            unit=None):
+        """Computes the integral value of this power-law energy flux profile in
+        the range ``[E1, E2]``.
 
-class TimeFluxProfile(FluxProfile, metaclass=abc.ABCMeta):
-    """The abstract base class for a time flux profile function.
+        Parameters
+        ----------
+        E1 : float | 1d numpy ndarray of float
+            The lower energy bound of the integration.
+        E2 : float | 1d numpy ndarray of float
+            The upper energy bound of the integration.
+        unit : instance of astropy.units.UnitBase | None
+            The unit of the given energies.
+            If set to ``None``, the set energy unit of this EnergyFluxProfile
+            instance is assumed.
+
+        Returns
+        -------
+        integral : 1d ndarray of float
+            The integral values of the given integral ranges.
+        """
+        E1 = np.atleast_1d(E1)
+        E2 = np.atleast_1d(E2)
+
+        if (unit is not None) and (unit != self._energy_unit):
+            time_unit_conv_factor = unit.to(self._energy_unit)
+            E1 = E1 * time_unit_conv_factor
+            E2 = E2 * time_unit_conv_factor
+
+        gamma = self._gamma
+
+        # Handle special case for gamma = 1.
+        if gamma == 1:
+            integral = self._E0 * np.log(E2/E1)
+            return integral
+
+        integral = (
+            np.power(self._E0, gamma) / (1-gamma) *
+            (np.power(E2, 1-gamma) - np.power(E1, 1-gamma))
+        )
+
+        return integral
+
+
+class CutoffPowerLawEnergyFluxProfile(
+        PowerLawEnergyFluxProfile,
+):
+    r"""Cut-off power law energy flux profile of the form
+
+    .. math::
+
+        (E / E_0)^{-\gamma} \exp(-E/E_{\mathrm{cut}})
+
+    """
+    def __init__(
+            self,
+            E0,
+            gamma,
+            Ecut,
+            energy_unit=None,
+            **kwargs,
+    ):
+        """Creates a new cut-off power law flux profile with the reference
+        energy ``E0``, spectral index ``gamma``, and cut-off energy ``Ecut``.
+
+        Parameters
+        ----------
+        E0 : castable to float
+            The reference energy.
+        gamma : castable to float
+            The spectral index.
+        Ecut : castable to float
+            The cut-off energy.
+        energy_unit : instance of astropy.units.UnitBase | None
+            The used unit for energy.
+            If set to ``None``, the configured default energy unit for fluxes is
+            used.
+        """
+        super().__init__(
+            E0=E0,
+            gamma=gamma,
+            energy_unit=energy_unit,
+            **kwargs)
+
+        self.Ecut = Ecut
+
+    @property
+    def Ecut(self):
+        """The energy cut value.
+        """
+        return self._Ecut
+
+    @Ecut.setter
+    def Ecut(self, v):
+        v = float_cast(
+            v,
+            'The Property Ecut  must be castable to type float!')
+        self._Ecut = v
+
+    @property
+    def math_function_str(self):
+        """(read-only) The string representation of this energy flux profile
+        instance.
+        """
+        s = (f'(E / ({self._E0:g} {self._energy_unit}))^-{self._gamma:g} '
+             f'exp(-E / ({self._Ecut:g} {self._energy_unit}))')
+
+        return s
+
+    def __call__(
+            self,
+            E,
+            unit=None,
+    ):
+        """Returns the cut-off power law values for the given energies as
+        numpy ndarray in the same shape as E.
+
+        Parameters
+        ----------
+        E : float | instance of numpy ndarray
+            The energy value(s) for which to retrieve the energy profile value.
+        unit : instance of astropy.units.UnitBase | None
+            The unit of the given energies.
+            If set to ``None``, the set energy unit of this EnergyFluxProfile
+            instance is assumed.
+
+        Returns
+        -------
+        values : instance of numpy ndarray
+            The energy profile values for the given energies.
+        """
+        E = np.atleast_1d(E)
+
+        if (unit is not None) and (unit != self._energy_unit):
+            E = E * unit.to(self._energy_unit)
+
+        values = super().__call__(E=E, unit=None)
+        values *= np.exp(-E / self._Ecut)
+
+        return values
+
+
+class LogParabolaPowerLawEnergyFluxProfile(
+        PowerLawEnergyFluxProfile,
+):
+    r"""This class provides an energy flux profile for a power-law with a
+    spectral index that varies as a log parabola in energy of the form
+
+    .. math::
+
+        \frac{E}{E_0}^{-\left(\alpha + \beta\log(\frac{E}{E_0})\right)}
+
+    """
+    def __init__(
+            self,
+            E0,
+            alpha,
+            beta,
+            energy_unit=None,
+            **kwargs,
+    ):
+        """
+        Parameters
+        ----------
+        E0 : castable to float
+            The reference energy.
+        alpha : float
+            The alpha parameter of the log-parabola spectral index.
+        beta : float
+            The beta parameter of the log-parabola spectral index.
+        energy_unit : instance of astropy.units.UnitBase | None
+            The used unit for energy.
+            If set to ``None``, the configured default energy unit for fluxes is
+            used.
+        """
+        super().__init__(
+            E0=E0,
+            gamma=np.nan,
+            energy_unit=energy_unit,
+            **kwargs)
+
+        self.alpha = alpha
+        self.beta = beta
+
+    @property
+    def alpha(self):
+        """The alpha parameter of the log-parabola spectral index.
+        """
+        return self._alpha
+
+    @alpha.setter
+    def alpha(self, v):
+        v = float_cast(
+            v,
+            'Property alpha must be castable to type float!')
+        self._alpha = v
+
+    @property
+    def beta(self):
+        """The beta parameter of the log-parabola spectral index.
+        """
+        return self._beta
+
+    @beta.setter
+    def beta(self, v):
+        v = float_cast(
+            v,
+            'Property beta must be castable to type float!')
+        self._beta = v
+
+    @property
+    def math_function_str(self):
+        """(read-only) The string representation of this energy flux profile
+        instance.
+        """
+        s_E0 = f'{self._E0:g} {self._energy_unit}'
+        s = (
+            f'(E / {s_E0})'
+            f'^(-({self._alpha:g} + {self._beta:g} log(E / {s_E0})))'
+        )
+
+        return s
+
+    def __call__(
+            self,
+            E,
+            unit=None,
+    ):
+        """Returns the log-parabola power-law values for the given energies as
+        numpy ndarray in the same shape as E.
+
+        Parameters
+        ----------
+        E : float | instance of numpy ndarray
+            The energy value(s) for which to retrieve the energy profile value.
+        unit : instance of astropy.units.UnitBase | None
+            The unit of the given energies.
+            If set to ``None``, the set energy unit of this EnergyFluxProfile
+            instance is assumed.
+
+        Returns
+        -------
+        values : instance of numpy ndarray
+            The energy profile values for the given energies.
+        """
+        E = np.atleast_1d(E)
+
+        if (unit is not None) and (unit != self._energy_unit):
+            E = E * unit.to(self._energy_unit)
+
+        values = np.power(
+            E / self._E0,
+            -self._alpha - self._beta * np.log(E / self._E0)
+        )
+
+        return values
+
+
+class PhotosplineEnergyFluxProfile(
+        EnergyFluxProfile,
+):
+    """The abstract base class for an energy flux profile based on a
+    photospline.
     """
+    @tool.requires('photospline')
+    def __init__(
+            self,
+            splinetable,
+            crit_log10_energy_lower,
+            crit_log10_energy_upper,
+            energy_unit=None,
+            **kwargs,
+    ):
+        """Creates a new instance of PhotosplineEnergyFluxProfile.
+
+        Parameters
+        ----------
+        splinetable : instance of photospline.SplineTable
+            The instance of photospline.SplineTable representing the energy flux
+            profile as a spline.
+        crit_log10_energy_lower : float
+            The lower edge of the spline's supported energy range in log10(E).
+        crit_log10_energy_upper : float
+            The upper edge of the spline's supported energy range in log10(E).
+        energy_unit : instance of astropy.units.UnitBase | None
+            The used unit for energy.
+            If set to ``None``, the configured default energy unit for fluxes is
+            used.
+        """
+        super().__init__(
+            energy_unit=energy_unit,
+            **kwargs)
+
+        self.photospline = tool.get('photospline')
+
+        self.splinetable = splinetable
+        self.crit_log10_energy_lower = crit_log10_energy_lower
+        self.crit_log10_energy_upper = crit_log10_energy_upper
+
+    @property
+    def splinetable(self):
+        """The instance of photospline.SplineTable that describes the neutrino
+        energy flux profile as function of neutrino energy via B-spline
+        interpolation.
+        """
+        return self._splinetable
+
+    @splinetable.setter
+    def splinetable(self, table):
+        if not isinstance(table, self.photospline.SplineTable):
+            raise TypeError(
+                'The splinetable property must be an instance of '
+                'photospline.SplineTable! '
+                f'Its current type is {classname(table)}!')
+        self._splinetable = table
+
+    @property
+    def crit_log10_energy_lower(self):
+        """The lower energy bound of the spline's support.
+        """
+        return self._crit_log10_energy_lower
+
+    @crit_log10_energy_lower.setter
+    def crit_log10_energy_lower(self, v):
+        v = float_cast(
+            v,
+            'The property crit_log10_energy_lower must be castable to type '
+            'float!')
+        self._crit_log10_energy_lower = v
+
+    @property
+    def crit_log10_energy_upper(self):
+        """The upper energy bound of the spline's support.
+        """
+        return self._crit_log10_energy_upper
+
+    @crit_log10_energy_upper.setter
+    def crit_log10_energy_upper(self, v):
+        v = float_cast(
+            v,
+            'The property crit_log10_energy_upper must be castable to type '
+            'float!')
+        self._crit_log10_energy_upper = v
+
 
-    def __init__(self, t_start=-np.inf, t_end=np.inf, time_unit=None):
+class TimeFluxProfile(
+        FluxProfile,
+        metaclass=abc.ABCMeta,
+):
+    """The abstract base class for a time flux profile function.
+    """
+    def __init__(
+            self,
+            t_start=-np.inf,
+            t_stop=np.inf,
+            time_unit=None,
+            **kwargs):
         """Creates a new time flux profile instance.
 
         Parameters
         ----------
         t_start : float
             The start time of the time profile.
             If set to -inf, it means, that the profile starts at the beginning
             of the entire time-span of the dataset.
-        t_end : float
-            The end time of the time profile.
+        t_stop : float
+            The stop time of the time profile.
             If set to +inf, it means, that the profile ends at the end of the
             entire time-span of the dataset.
         time_unit : instance of astropy.units.UnitBase | None
             The used unit for time.
             If set to ``None``, the configured default time unit for fluxes is
             used.
         """
-        super(TimeFluxProfile, self).__init__()
+        super().__init__(
+            **kwargs)
 
         self.time_unit = time_unit
 
         self.t_start = t_start
-        self.t_end = t_end
+        self.t_stop = t_stop
 
-        # Define the parameters which can be set via the `set_parameters`
+        # Define the parameters which can be set via the `set_params`
         # method.
-        self.parameter_names = ('t_start', 't_end')
+        self.param_names = ('t_start', 't_stop')
 
     @property
     def t_start(self):
         """The start time of the time profile. Can be -inf which means, that
         the profile starts at the beginning of the entire dataset.
         """
         return self._t_start
+
     @t_start.setter
     def t_start(self, t):
-        t = float_cast(t,
-            'The t_start property must be castable to type float!')
+        t = float_cast(
+            t,
+            'The t_start property must be castable to type float! '
+            f'Its current type is {classname(t)}!')
         self._t_start = t
 
     @property
-    def t_end(self):
-        """The end time of the time profile. Can be +inf which means, that
+    def t_stop(self):
+        """The stop time of the time profile. Can be +inf which means, that
         the profile ends at the end of the entire dataset.
         """
-        return self._t_end
-    @t_end.setter
-    def t_end(self, t):
-        t = float_cast(t,
-            'The t_end property must be castable to type float!')
-        self._t_end = t
+        return self._t_stop
+
+    @t_stop.setter
+    def t_stop(self, t):
+        t = float_cast(
+            t,
+            'The t_stop property must be castable to type float! '
+            f'Its current type is {classname(t)}!')
+        self._t_stop = t
 
     @property
     def duration(self):
         """(read-only) The duration of the time profile.
         """
-        return self._t_end - self._t_start
+        return self._t_stop - self._t_start
 
     @property
     def time_unit(self):
         """The unit of time used for the flux profile calculation.
         """
         return self._time_unit
+
     @time_unit.setter
     def time_unit(self, unit):
-        if(unit is None):
-            unit = CFG['units']['defaults']['fluxes']['time']
-        if(not isinstance(unit, units.UnitBase)):
-            raise TypeError('The property time_unit must be of type '
-                'astropy.units.UnitBase!')
+        if unit is None:
+            unit = self._cfg['units']['defaults']['fluxes']['time']
+        if not isinstance(unit, units.UnitBase):
+            raise TypeError(
+                'The property time_unit must be of type '
+                'astropy.units.UnitBase! '
+                f'Its current type is {classname(unit)}!')
         self._time_unit = unit
 
     def get_total_integral(self):
         """Calculates the total integral of the time profile from t_start to
-        t_end.
+        t_stop.
 
         Returns
         -------
         integral : float
             The integral value of the entire time profile.
             The value is in the set time unit of this TimeFluxProfile instance.
         """
-        integral = self.get_integral(self._t_start, self._t_end)
+        integral = self.get_integral(self._t_start, self._t_stop).squeeze()
 
         return integral
 
     @abc.abstractmethod
-    def __call__(self, t, unit=None):
+    def __call__(
+            self,
+            t,
+            unit=None,
+    ):
         """This method is supposed to return the time profile value for the
         given times.
 
         Parameters
         ----------
         t : float | 1D numpy ndarray of float
             The time(s) for which to get the time flux profile values.
@@ -543,15 +1106,19 @@
         -------
         values : 1D numpy ndarray of float
             The time profile values.
         """
         pass
 
     @abc.abstractmethod
-    def move(self, dt, unit=None):
+    def move(
+            self,
+            dt,
+            unit=None,
+    ):
         """Abstract method to move the time profile by the given amount of time.
 
         Parameters
         ----------
         dt : float
             The time difference of how far to move the time profile in time.
             This can be a positive or negative time shift value.
@@ -559,15 +1126,20 @@
             The unit of the given time difference.
             If set to ``one``, the set time unit of this TimeFluxProfile
             instance is assumed.
         """
         pass
 
     @abc.abstractmethod
-    def get_integral(self, t1, t2, unit=None):
+    def get_integral(
+            self,
+            t1,
+            t2,
+            unit=None,
+    ):
         """This method is supposed to calculate the integral of the time profile
         from time ``t1`` to time ``t2``.
 
         Parameters
         ----------
         t1 : float | array of float
             The start time of the integration.
@@ -583,26 +1155,46 @@
         integral : array of float
             The integral value(s) of the time profile. The values are in the
             set time unit of this TimeFluxProfile instance.
         """
         pass
 
 
-class UnityTimeFluxProfile(TimeFluxProfile):
+class UnityTimeFluxProfile(
+        TimeFluxProfile,
+):
     """Time flux profile for the constant profile function ``1``.
     """
-    def __init__(self, time_unit=None):
-        super(UnityTimeFluxProfile, self).__init__(
-            time_unit=time_unit)
+    def __init__(
+            self,
+            time_unit=None,
+            **kwargs,
+    ):
+        """Creates a new instance of UnityTimeFluxProfile.
+
+        Parameters
+        ----------
+        time_unit : instance of astropy.units.UnitBase | None
+            The used unit for time.
+            If set to ``None``, the configured default time unit for fluxes is
+            used.
+        """
+        super().__init__(
+            time_unit=time_unit,
+            **kwargs)
 
     @property
     def math_function_str(self):
         return '1'
 
-    def __call__(self, t, unit=None):
+    def __call__(
+            self,
+            t,
+            unit=None,
+    ):
         """Returns 1 as numpy ndarray in same shape as t.
 
         Parameters
         ----------
         t : float | 1D numpy ndarray of float
             The time(s) for which to get the time flux profile values.
         unit : instance of astropy.units.UnitBase | None
@@ -616,15 +1208,19 @@
         """
         t = np.atleast_1d(t)
 
         values = np.ones_like(t, dtype=np.int8)
 
         return values
 
-    def move(self, dt, unit=None):
+    def move(
+            self,
+            dt,
+            unit=None,
+    ):
         """Moves the time profile by the given amount of time. By definition
         this method does nothing, because the profile is 1 over the entire
         dataset time range.
 
         Parameters
         ----------
         dt : float
@@ -633,15 +1229,20 @@
         unit : instance of astropy.units.UnitBase | None
             The unit of the given time difference.
             If set to ``None``, the set time unit of this TimeFluxProfile
             instance is assumed.
         """
         pass
 
-    def get_integral(self, t1, t2, unit=None):
+    def get_integral(
+            self,
+            t1,
+            t2,
+            unit=None,
+    ):
         """Calculates the integral of the time profile from time t1 to time t2.
 
         Parameters
         ----------
         t1 : float | array of float
             The start time of the integration.
         t2 : float | array of float
@@ -653,91 +1254,155 @@
 
         Returns
         -------
         integral : array of float
             The integral value(s) of the time profile. The values are in the
             set time unit of this TimeFluxProfile instance.
         """
-        if((unit is not None) and (unit != self._time_unit)):
+        if (unit is not None) and (unit != self._time_unit):
             time_unit_conv_factor = unit.to(self._time_unit)
             t1 = t1 * time_unit_conv_factor
             t2 = t2 * time_unit_conv_factor
 
         integral = t2 - t1
 
         return integral
 
 
-class BoxTimeFluxProfile(TimeFluxProfile):
+class BoxTimeFluxProfile(
+        TimeFluxProfile,
+):
     """This class describes a box-shaped time flux profile.
     It has the following parameters:
 
         t0 : float
             The mid time of the box profile.
         tw : float
             The width of the box profile.
 
     The box is centered at ``t0`` and extends to +/-``tw``/2 around ``t0``.
     """
-    def __init__(self, t0, tw, time_unit=None):
+
+    @classmethod
+    def from_start_and_stop_time(
+            cls,
+            start,
+            stop,
+            time_unit=None,
+            **kwargs,
+    ):
+        """Constructs a BoxTimeFluxProfile instance from the given start and
+        stop time.
+
+        Parameters
+        ----------
+        start : float
+            The start time of the box profile.
+        stop : float
+            The stop time of the box profile.
+        time_unit : instance of astropy.units.UnitBase | None
+            The used unit for time.
+            If set to ``None``, the configured default time unit for fluxes is
+            used.
+        **kwargs
+            Additional keyword arguments, which are passed to the constructor
+            of the :class:`BoxTimeFluxProfile` class.
+
+        Returns
+        -------
+        profile : instance of BoxTimeFluxProfile
+            The newly created instance of BoxTimeFluxProfile.
+        """
+        t0 = 0.5*(start + stop)
+        tw = stop - start
+
+        profile = cls(
+            t0=t0,
+            tw=tw,
+            time_unit=time_unit,
+            **kwargs)
+
+        return profile
+
+    def __init__(
+            self,
+            t0,
+            tw,
+            time_unit=None,
+            **kwargs,
+    ):
         """Creates a new box-shaped time profile instance.
 
         Parameters
         ----------
         t0 : float
             The mid time of the box profile.
         tw : float
             The width of the box profile.
         time_unit : instance of astropy.units.UnitBase | None
             The used unit for time.
             If set to ``None``, the configured default time unit for fluxes is
             used.
         """
         t_start = t0 - tw/2.
-        t_end = t0 + tw/2.
+        t_stop = t0 + tw/2.
 
-        super(BoxTimeFluxProfile, self).__init__(
-            t_start=t_start, t_end=t_end, time_unit=time_unit)
+        super().__init__(
+            t_start=t_start,
+            t_stop=t_stop,
+            time_unit=time_unit,
+            **kwargs)
 
-        # Define the parameters which can be set via the `set_parameters`
+        # Define the parameters which can be set via the `set_params`
         # method.
-        self.parameter_names = ('t0', 'tw')
+        self.param_names = ('t0', 'tw')
 
     @property
     def t0(self):
         """The time of the mid point of the box.
         The value is in the set time unit of this TimeFluxProfile instance.
         """
-        return 0.5*(self._t_start + self._t_end)
+        return 0.5*(self._t_start + self._t_stop)
+
     @t0.setter
     def t0(self, t):
         old_t0 = self.t0
         dt = t - old_t0
         self.move(dt)
 
     @property
     def tw(self):
         """The time width of the box.
         The value is in the set time unit of this TimeFluxProfile instance.
         """
-        return self._t_end - self._t_start
+        return self._t_stop - self._t_start
+
     @tw.setter
     def tw(self, w):
         t0 = self.t0
         self._t_start = t0 - 0.5*w
-        self._t_end = t0 + 0.5*w
+        self._t_stop = t0 + 0.5*w
 
     @property
     def math_function_str(self):
+        """The string representation of the mathematical function of this
+        TimeFluxProfile instance.
+        """
         t0 = self.t0
         tw = self.tw
-        return '1 for t in [%g-%g/2; %g+%g/2], 0 otherwise'%(
-            t0, tw, t0, tw)
 
-    def __call__(self, t, unit=None):
+        s = f'1 for t in [{t0:g}-{tw:g}/2; {t0:g}+{tw:g}/2], 0 otherwise'
+
+        return s
+
+    def __call__(
+            self,
+            t,
+            unit=None,
+    ):
         """Returns 1 for all t within the interval [t0-tw/2; t0+tw/2], and 0
         otherwise.
 
         Parameters
         ----------
         t : float | 1D numpy ndarray of float
             The time(s) for which to get the time flux profile values.
@@ -749,44 +1414,94 @@
         Returns
         -------
         values : 1D numpy ndarray of int8
             The value(s) of the time flux profile for the given time(s).
         """
         t = np.atleast_1d(t)
 
-        if((unit is not None) and (unit != self._time_unit)):
-            time_unit_conv_factor = unit.to(self._time_unit)
-            t = t * time_unit_conv_factor
+        if (unit is not None) and (unit != self._time_unit):
+            t = t * unit.to(self._time_unit)
 
         values = np.zeros((t.shape[0],), dtype=np.int8)
-        m = (t >= self._t_start) & (t <= self._t_end)
+        m = (t >= self._t_start) & (t <= self._t_stop)
+        values[m] = 1
+
+        return values
+
+    def cdf(
+            self,
+            t,
+            unit=None,
+    ):
+        """Calculates the cumulative distribution function value for the given
+        time values ``t``.
+
+        Parameters
+        ----------
+        t : float | instance of numpy ndarray
+            The (N_times,)-shaped numpy ndarray holding the time values for
+            which to calculate the CDF values.
+        unit : instance of astropy.units.UnitBase | None
+            The unit of the given times.
+            If set to ``None``, the set time unit of this TimeFluxProfile is
+            assumed.
+
+        Returns
+        -------
+        values : instance of numpy ndarray
+            The (N_times,)-shaped numpy ndarray holding the cumulative
+            distribution function values for each time ``t``.
+        """
+        t = np.atleast_1d(t)
+
+        if (unit is not None) and (unit != self._time_unit):
+            t = t * unit.to(self._time_unit)
+
+        t_start = self._t_start
+        t_stop = self._t_stop
+
+        values = np.zeros(t.size, dtype=np.float64)
+
+        m = (t_start <= t) & (t <= t_stop)
+        values[m] = (t[m] - t_start) / (t_stop - t_start)
+
+        m = (t > t_stop)
         values[m] = 1
 
         return values
 
-    def move(self, dt, unit=None):
+    def move(
+            self,
+            dt,
+            unit=None,
+    ):
         """Moves the box-shaped time profile by the time difference dt.
 
         Parameters
         ----------
         dt : float
             The time difference of how far to move the time profile in time.
             This can be a positive or negative time shift value.
         unit : instance of astropy.units.UnitBase | None
             The unit of ``dt``.
             If set to ``None``, the set time unit of this TimeFluxProfile
             instance is assumed.
         """
-        if((unit is not None) and (unit != self._time_unit)):
+        if (unit is not None) and (unit != self._time_unit):
             dt = dt * unit.to(self._time_unit)
 
         self._t_start += dt
-        self._t_end += dt
+        self._t_stop += dt
 
-    def get_integral(self, t1, t2, unit=None):
+    def get_integral(
+            self,
+            t1,
+            t2,
+            unit=None,
+    ):
         """Calculates the integral of the box-shaped time flux profile from
         time t1 to time t2.
 
         Parameters
         ----------
         t1 : float | array of float
             The start time(s) of the integration.
@@ -802,43 +1517,52 @@
         integral : array of float
             The integral value(s). The values are in the set time unit of this
             TimeFluxProfile instance.
         """
         t1 = np.atleast_1d(t1)
         t2 = np.atleast_1d(t2)
 
-        if((unit is not None) and (unit != self._time_unit)):
+        if (unit is not None) and (unit != self._time_unit):
             time_unit_conv_factor = unit.to(self._time_unit)
             t1 = t1 * time_unit_conv_factor
             t2 = t2 * time_unit_conv_factor
 
         integral = np.zeros((t1.shape[0],), dtype=np.float64)
 
-        m = (t2 >= self._t_start) & (t1 <= self._t_end)
+        m = (t2 >= self._t_start) & (t1 <= self._t_stop)
         N = np.count_nonzero(m)
 
         t1 = np.max(np.vstack((t1[m], np.repeat(self._t_start, N))).T, axis=1)
-        t2 = np.min(np.vstack((t2[m], np.repeat(self._t_end, N))).T, axis=1)
+        t2 = np.min(np.vstack((t2[m], np.repeat(self._t_stop, N))).T, axis=1)
 
         integral[m] = t2 - t1
 
         return integral
 
 
-class GaussianTimeFluxProfile(TimeFluxProfile):
+class GaussianTimeFluxProfile(
+        TimeFluxProfile,
+):
     """This class describes a gaussian-shaped time flux profile.
     It has the following parameters:
 
         t0 : float
             The mid time of the gaussian profile.
         sigma_t : float
             The one-sigma width of the gaussian profile.
     """
-    def __init__(self, t0, sigma_t, tol=1e-12, time_unit=None):
-        """Creates a new gaussian-shaped time profile instance.
+
+    def __init__(
+            self,
+            t0,
+            sigma_t,
+            tol=1e-12,
+            time_unit=None,
+            **kwargs):
+        """Creates a new gaussian-shaped time flux profile instance.
 
         Parameters
         ----------
         t0 : float
             The mid time of the gaussian profile.
         sigma_t : float
             The one-sigma width of the gaussian profile.
@@ -848,58 +1572,72 @@
         time_unit : instance of astropy.units.UnitBase | None
             The used unit for time.
             If set to ``None``, the configured default time unit for fluxes is
             used.
         """
         # Calculate the start and end time of the gaussian profile, such that
         # at those times the gaussian values obey the given tolerance.
-        dt = np.sqrt(-2*sigma_t*sigma_t*np.log(np.sqrt(2*np.pi)*sigma_t*tol))
+        dt = np.sqrt(-2 * sigma_t**2 * np.log(tol))
         t_start = t0 - dt
-        t_end = t0 + dt
+        t_stop = t0 + dt
 
-        # A Gaussian profile extends to +/- infinity by definition.
-        super(GaussianTimeFluxProfile, self).__init__(
-            t_start=t_start, t_end=t_end, time_unit=time_unit)
+        super().__init__(
+            t_start=t_start,
+            t_stop=t_stop,
+            time_unit=time_unit,
+            **kwargs)
 
         self.t0 = t0
         self.sigma_t = sigma_t
 
-        # Define the parameters which can be set via the `set_parameters`
+        # Define the parameters which can be set via the `set_params`
         # method.
-        self.parameter_names = ('t0', 'sigma_t')
+        self.param_names = ('t0', 'sigma_t')
+
+    @property
+    def math_function_str(self):
+        return 'exp(-(t-t0)^2/(2 sigma_t^2))'
 
     @property
     def t0(self):
         """The time of the mid point of the gaussian profile.
         The unit of the value is the set time unit of this TimeFluxProfile
         instance.
         """
-        return 0.5*(self._t_start + self._t_end)
+        return 0.5*(self._t_start + self._t_stop)
+
     @t0.setter
     def t0(self, t):
-        t = float_cast(t,
+        t = float_cast(
+            t,
             'The t0 property must be castable to type float!')
         old_t0 = self.t0
         dt = t - old_t0
         self.move(dt)
 
     @property
     def sigma_t(self):
         """The one-sigma width of the gaussian profile.
         The unit of the value is the set time unit of this TimeFluxProfile
         instance.
         """
         return self._sigma_t
+
     @sigma_t.setter
     def sigma_t(self, sigma):
-        sigma = float_cast(sigma,
-            'The sigma property must be castable to type float!')
+        sigma = float_cast(
+            sigma,
+            'The sigma_t property must be castable to type float!')
         self._sigma_t = sigma
 
-    def __call__(self, t, unit=None):
+    def __call__(
+            self,
+            t,
+            unit=None,
+    ):
         """Returns the gaussian profile value for the given time ``t``.
 
         Parameters
         ----------
         t : float | 1D numpy ndarray of float
             The time(s) for which to get the time flux profile values.
         unit : instance of astropy.units.UnitBase | None
@@ -910,47 +1648,103 @@
         Returns
         -------
         values : 1D numpy ndarray of float
             The value(s) of the time flux profile for the given time(s).
         """
         t = np.atleast_1d(t)
 
-        if((unit is not None) and (unit != self._time_unit)):
+        if (unit is not None) and (unit != self._time_unit):
             time_unit_conv_factor = unit.to(self._time_unit)
             t = t * time_unit_conv_factor
 
+        m = (t >= self.t_start) & (t < self.t_stop)
+
         s = self._sigma_t
         twossq = 2*s*s
-        t0 = 0.5*(self._t_end + self._t_start)
-        dt = t - t0
+        t0 = 0.5*(self._t_stop + self._t_start)
+        dt = t[m] - t0
 
-        values = 1/(np.sqrt(np.pi*twossq)) * np.exp(-dt*dt/twossq)
+        values = np.zeros_like(t)
+        values[m] = np.exp(-dt*dt/twossq)
 
         return values
 
-    def move(self, dt, unit=None):
+    def cdf(
+            self,
+            t,
+            unit=None,
+    ):
+        """Calculates the cumulative distribution function values for the given
+        time values ``t``.
+
+        Parameters
+        ----------
+        t : float | instance of numpy ndarray
+            The (N_times,)-shaped numpy ndarray holding the time values for
+            which to calculate the CDF values.
+        unit : instance of astropy.units.UnitBase | None
+            The unit of the given times.
+            If set to ``None``, the set time unit of this TimeFluxProfile is
+            assumed.
+
+        Returns
+        -------
+        values : instance of numpy ndarray
+            The (N_times,)-shaped numpy ndarray holding the cumulative
+            distribution function values for each time ``t``.
+        """
+        t = np.atleast_1d(t)
+
+        if (unit is not None) and (unit != self._time_unit):
+            t = t * unit.to(self._time_unit)
+
+        t_start = self._t_start
+        t_stop = self._t_stop
+
+        values = np.zeros(t.size, dtype=np.float64)
+
+        m = (t_start <= t) & (t <= t_stop)
+        values[m] = (
+            self.get_integral(t1=t_start, t2=t[m]) / self.get_total_integral()
+        )
+
+        m = (t > t_stop)
+        values[m] = 1
+
+        return values
+
+    def move(
+            self,
+            dt,
+            unit=None,
+    ):
         """Moves the gaussian time profile by the given amount of time.
 
         Parameters
         ----------
         dt : float
             The time difference of how far to move the time profile in time.
             This can be a positive or negative time shift value.
         unit : instance of astropy.units.UnitBase | None
             The unit of the given time difference.
             If set to ``None``, the set time unit of this TimeFluxProfile is
             assumed.
         """
-        if((unit is not None) and (unit != self._time_unit)):
+        if (unit is not None) and (unit != self._time_unit):
             dt = dt * unit.to(self._time_unit)
 
         self._t_start += dt
-        self._t_end += dt
+        self._t_stop += dt
 
-    def get_integral(self, t1, t2, unit=None):
+    def get_integral(
+            self,
+            t1,
+            t2,
+            unit=None,
+    ):
         """Calculates the integral of the gaussian time profile from time ``t1``
         to time ``t2``.
 
         Parameters
         ----------
         t1 : float | array of float
             The start time(s) of the integration.
@@ -963,43 +1757,73 @@
 
         Returns
         -------
         integral : array of float
             The integral value(s). The values are in the set time unit of
             this TimeFluxProfile instance.
         """
-        if((unit is not None) and (unit != self._time_unit)):
+        if (unit is not None) and (unit != self._time_unit):
             time_unit_conv_factor = unit.to(self._time_unit)
             t1 = t1 * time_unit_conv_factor
             t2 = t2 * time_unit_conv_factor
 
-        t0 = 0.5*(self._t_end + self._t_start)
+        t0 = 0.5*(self._t_stop + self._t_start)
         sigma_t = self._sigma_t
 
-        c1 = scipy.stats.norm.cdf(t1, loc=t0, scale=sigma_t)
-        c2 = scipy.stats.norm.cdf(t2, loc=t0, scale=sigma_t)
+        c1 = np.sqrt(np.pi/2) * sigma_t
+        c2 = np.sqrt(2) * sigma_t
+        i1 = c1 * scipy.special.erf((t1 - t0)/c2)
+        i2 = c1 * scipy.special.erf((t2 - t0)/c2)
 
-        integral = c2 - c1
+        integral = i2 - i1
 
         return integral
 
 
-class FluxModel(MathFunction, Model, metaclass=abc.ABCMeta):
-    r"""Abstract base class for all flux models
-    :math:`\Phi_S(\alpha,\delta,E,t | \vec{x}_s,\vec{p}_s)`.
+class FluxModel(
+        MathFunction,
+        HasConfig,
+        Model,
+        metaclass=abc.ABCMeta,
+):
+    r"""Abstract base class for all flux models of the form
+
+    .. math::
+
+        \Phi_S(\alpha,\delta,E,t | \vec{x}_s,\vec{p}_s).
 
     This base class defines the units used for the flux calculation. The unit
     of the flux is ([angle]^{-2} [energy]^{-1} [length]^{-2} [time]^{-1}).
 
     At this point the functional form of the flux model is not yet defined.
     """
+    @staticmethod
+    def get_default_units(cfg):
+        """Returns the configured default units for flux models.
+
+        Parameters
+        ----------
+        cfg : instance of Config
+            The instance of Config holding the local configuration.
+
+        Returns
+        -------
+        units_dict : dict
+            The dictionary holding the configured default units used for flux
+            models.
+        """
+        return cfg['units']['defaults']['fluxes']
 
     def __init__(
-            self, angle_unit=None, energy_unit=None, length_unit=None,
-            time_unit=None, **kwargs):
+            self,
+            angle_unit=None,
+            energy_unit=None,
+            length_unit=None,
+            time_unit=None,
+            **kwargs):
         """Creates a new FluxModel instance and defines the user-defined units.
 
         Parameters
         ----------
         angle_unit : instance of astropy.units.UnitBase | None
             The used unit for angles.
             If set to ``None``, the configured default angle unit for fluxes is
@@ -1013,111 +1837,142 @@
             If set to ``None``, the configured default length unit for fluxes is
             used.
         time_unit : instance of astropy.units.UnitBase | None
             The used unit for time.
             If set to ``None``, the configured default time unit for fluxes is
             used.
         """
-        super(FluxModel, self).__init__(**kwargs)
+        super().__init__(
+            **kwargs)
 
         # Define the units.
         self.angle_unit = angle_unit
         self.energy_unit = energy_unit
         self.length_unit = length_unit
         self.time_unit = time_unit
 
     @property
     def angle_unit(self):
         """The unit of angle used for the flux calculation.
         """
         return self._angle_unit
+
     @angle_unit.setter
     def angle_unit(self, unit):
-        if(unit is None):
-            unit = CFG['units']['defaults']['fluxes']['angle']
-        if(not isinstance(unit, units.UnitBase)):
-            raise TypeError('The property angle_unit must be of type '
+        if unit is None:
+            unit = self._cfg['units']['defaults']['fluxes']['angle']
+        if not isinstance(unit, units.UnitBase):
+            raise TypeError(
+                'The property angle_unit must be of type '
                 'astropy.units.UnitBase!')
         self._angle_unit = unit
 
     @property
     def energy_unit(self):
         """The unit of energy used for the flux calculation.
         """
         return self._energy_unit
+
     @energy_unit.setter
     def energy_unit(self, unit):
-        if(unit is None):
-            unit = CFG['units']['defaults']['fluxes']['energy']
-        if(not isinstance(unit, units.UnitBase)):
-            raise TypeError('The property energy_unit must be of type '
+        if unit is None:
+            unit = self._cfg['units']['defaults']['fluxes']['energy']
+        if not isinstance(unit, units.UnitBase):
+            raise TypeError(
+                'The property energy_unit must be of type '
                 'astropy.units.UnitBase!')
         self._energy_unit = unit
 
     @property
     def length_unit(self):
         """The unit of length used for the flux calculation.
         """
         return self._length_unit
+
     @length_unit.setter
     def length_unit(self, unit):
-        if(unit is None):
-            unit = CFG['units']['defaults']['fluxes']['length']
-        if(not isinstance(unit, units.UnitBase)):
-            raise TypeError('The property length_unit must be of type '
+        if unit is None:
+            unit = self._cfg['units']['defaults']['fluxes']['length']
+        if not isinstance(unit, units.UnitBase):
+            raise TypeError(
+                'The property length_unit must be of type '
                 'astropy.units.UnitBase!')
         self._length_unit = unit
 
     @property
     def time_unit(self):
         """The unit of time used for the flux calculation.
         """
         return self._time_unit
+
     @time_unit.setter
     def time_unit(self, unit):
-        if(unit is None):
-            unit = CFG['units']['defaults']['fluxes']['time']
-        if(not isinstance(unit, units.UnitBase)):
-            raise TypeError('The property time_unit must be of type '
+        if unit is None:
+            unit = self._cfg['units']['defaults']['fluxes']['time']
+        if not isinstance(unit, units.UnitBase):
+            raise TypeError(
+                'The property time_unit must be of type '
                 'astropy.units.UnitBase!')
         self._time_unit = unit
 
     @property
     def unit_str(self):
         """The string representation of the flux unit.
         """
-        return '1/(%s %s %s^2 %s)'%(
-            self.energy_unit.to_string(), (self.angle_unit**2).to_string(),
-            self.length_unit.to_string(), self.time_unit.to_string())
+        if self.angle_unit == units.radian:
+            angle_unit_sq = units.steradian
+        else:
+            angle_unit_sq = self.angle_unit**2
+
+        s = (f'({self.energy_unit.to_string()}'
+             f' {angle_unit_sq.to_string()}'
+             f' {self.length_unit.to_string()}^2'
+             f' {self.time_unit.to_string()})^-1')
+
+        return s
 
     @property
     def unit_latex_str(self):
         """The latex string representation of the flux unit.
         """
-        return r'%s$^{-1}$ %s$^{-1}$ %s$^{-2}$ %s$^{-1}$'%(
-            self.energy_unit.to_string(), (self.angle_unit**2).to_string(),
-            self.length_unit.to_string(), self.time_unit.to_string())
+        if self.angle_unit == units.radian:
+            angle_unit_sq = units.steradian
+        else:
+            angle_unit_sq = self.angle_unit**2
+
+        s = (f'{self.energy_unit.to_string()}''$^{-1}$ '
+             f'{angle_unit_sq.to_string()}''$^{-1}$ '
+             f'{self.length_unit.to_string()}''$^{-2}$ '
+             f'{self.time_unit.to_string()}''$^{-1}$')
+
+        return s
 
     def __str__(self):
         """Pretty string representation of this class.
         """
-        return self.math_function_str + ' ' + self.unit_str
+        return f'{self.math_function_str} {self.unit_str}'
 
     @abc.abstractmethod
     def __call__(
-            self, alpha, delta, E, t,
-            angle_unit=None, energy_unit=None, time_unit=None):
+            self,
+            ra=None,
+            dec=None,
+            E=None,
+            t=None,
+            angle_unit=None,
+            energy_unit=None,
+            time_unit=None):
         """The call operator to retrieve a flux value for a given celestrial
         position, energy, and observation time.
 
         Parameters
         ----------
-        alpha : float | (Ncoord,)-shaped 1D numpy ndarray of float
+        ra : float | (Ncoord,)-shaped 1D numpy ndarray of float
             The right-ascention coordinate for which to retrieve the flux value.
-        delta : float | (Ncoord,)-shaped 1D numpy ndarray of float
+        dec : float | (Ncoord,)-shaped 1D numpy ndarray of float
             The declination coordinate for which to retrieve the flux value.
         E : float | (Nenergy,)-shaped 1D numpy ndarray of float
             The energy for which to retrieve the flux value.
         t : float | (Ntime,)-shaped 1D numpy ndarray of float
             The observation time for which to retrieve the flux value.
         angle_unit : instance of astropy.units.UnitBase | None
             The unit of the given angles.
@@ -1133,210 +1988,325 @@
         -------
         flux : (Ncoord,Nenergy,Ntime)-shaped ndarray of float
             The flux values are in unit of the set flux model units
             [energy]^{-1} [angle]^{-2} [length]^{-2} [time]^{-1}.
         """
         pass
 
+    def to_internal_flux_unit(self):
+        """Calculates the conversion factor to convert the flux unit of this
+        flux model instance to the SkyLLH internally used flux unit.
 
-class FactorizedFluxModel(FluxModel):
+        Returns
+        -------
+        factor : float
+            The conversion factor.
+        """
+        self_flux_unit = 1 / (
+            self.angle_unit**2 *
+            self.energy_unit *
+            self.length_unit**2 *
+            self.time_unit)
+
+        internal_units = self._cfg['units']['internal']
+        internal_flux_unit = 1 / (
+            internal_units['angle']**2 *
+            internal_units['energy'] *
+            internal_units['length']**2 *
+            internal_units['time'])
+
+        factor = (self_flux_unit).to(internal_flux_unit).value
+
+        return factor
+
+
+class NullFluxModel(
+        FluxModel,
+):
+    """This class provides a dummy flux model class, which can be used for
+    testing purposes, in cases where an actual flux model is not required but
+    the framework interface requires one.
+    """
+    def __init__(
+            self,
+            *args,
+            cfg=None,
+            **kwargs,
+    ):
+        """Creates a new instance of NullFluxModel.
+
+        Parameters
+        ----------
+        cfg : instance of Config | None
+            The instance of Config holding the local configuration. Since this
+            flux model does nothing, this argument is optional. If not provided
+            the default configuration is used.
+        """
+        if cfg is None:
+            cfg = Config()
+
+        super().__init__(
+            *args,
+            cfg=cfg,
+            **kwargs)
+
+    def math_function_str(self):
+        """Since this is a dummy flux model, calling this method will raise a
+        NotImplementedError.
+        """
+        raise NotImplementedError(
+            f'The {classname(self)} flux model is a dummy flux model which has '
+            'no math function prepresentation!')
+
+    def __call__(self, *args, **kwargs):
+        """Since this is a dummy flux model, calling this method will raise a
+        NotImplementedError.
+        """
+        raise NotImplementedError(
+            f'The {classname(self)} flux model is a dummy flux model and '
+            'cannot be called!')
+
+
+class FactorizedFluxModel(
+        FluxModel,
+):
     r"""This class describes a flux model where the spatial, energy, and time
     profiles of the source factorize. That means the flux can be written as:
 
     .. math::
 
-        \Phi_S(\alpha,\delta,E,t | \vec{x}_s,\vec{p}_s) =
+        \Phi(\alpha,\delta,E,t | \vec{p}_\mathrm{s}) =
             \Phi_0
-            \Psi_{\mathrm{S}}(\alpha,\delta|\vec{p}_s)
-            \epsilon_{\mathrm{S}}(E|\vec{p}_s)
-            T_{\mathrm{S}}(t|\vec{p}_s)
+            \Psi(\alpha,\delta|\vec{p}_\mathrm{s})
+            \epsilon(E|\vec{p}_\mathrm{s})
+            T(t|\vec{p}_\mathrm{s})
 
     where, :math:`\Phi_0` is the normalization constant of the flux, and
-    :math:`\Psi_{\mathrm{S}}`, :math:`\epsilon_{\mathrm{S}}`, and
-    :math:`T_{\mathrm{S}}` are the spatial, energy, and time profiles of the
-    flux, respectively.
+    :math:`\Psi`, :math:`\epsilon`, and :math:`T` are the spatial, energy, and
+    time profiles of the flux given the source parameters
+    :math:`\vec{p}_\mathrm{s}`, respectively.
     """
     def __init__(
-            self, Phi0, spatial_profile, energy_profile, time_profile,
-            length_unit=None, **kwargs):
+            self,
+            Phi0,
+            spatial_profile,
+            energy_profile,
+            time_profile,
+            length_unit=None,
+            **kwargs,
+    ):
         """Creates a new factorized flux model.
 
         Parameters
         ----------
         Phi0 : float
             The flux normalization constant.
-        spatial_profile : SpatialFluxProfile instance | None
+        spatial_profile : instance of SpatialFluxProfile | None
             The SpatialFluxProfile instance providing the spatial profile
             function of the flux.
             If set to None, an instance of UnitySpatialFluxProfile will be used,
             which represents the constant function 1.
-        energy_profile : EnergyFluxProfile instance | None
+        energy_profile : instance of EnergyFluxProfile | None
             The EnergyFluxProfile instance providing the energy profile
             function of the flux.
             If set to None, an instance of UnityEnergyFluxProfile will be used,
             which represents the constant function 1.
-        time_profile : TimeFluxProfile instance | None
+        time_profile : instance of TimeFluxProfile | None
             The TimeFluxProfile instance providing the time profile function
             of the flux.
             If set to None, an instance of UnityTimeFluxProfile will be used,
             which represents the constant function 1.
         length_unit : instance of astropy.units.UnitBase | None
             The used unit for length.
             If set to ``None``, the configured default length unit for fluxes is
             used.
         """
+        cfg = kwargs.get('cfg')
+
         self.Phi0 = Phi0
+
+        if spatial_profile is None:
+            spatial_profile = UnitySpatialFluxProfile(
+                cfg=cfg)
         self.spatial_profile = spatial_profile
+
+        if energy_profile is None:
+            energy_profile = UnityEnergyFluxProfile(
+                cfg=cfg)
         self.energy_profile = energy_profile
+
+        if time_profile is None:
+            time_profile = UnityTimeFluxProfile(
+                cfg=cfg)
         self.time_profile = time_profile
 
         # The base class will set the default (internally used) flux unit, which
         # will be set automatically to the particular profile.
-        super(FactorizedFluxModel, self).__init__(
-            angle_unit=spatial_profile.angle_unit,
-            energy_unit=energy_profile.energy_unit,
-            time_unit=time_profile.time_unit,
+        super().__init__(
+            angle_unit=self._spatial_profile.angle_unit,
+            energy_unit=self._energy_profile.energy_unit,
+            time_unit=self._time_profile.time_unit,
             length_unit=length_unit,
-            **kwargs
+            **kwargs,
         )
 
-        # Define the parameters which can be set via the `set_parameters`
+        # Define the parameters which can be set via the `set_params`
         # method.
-        self.parameter_names = ('Phi0',)
+        self.param_names = ('Phi0',)
 
     @property
     def Phi0(self):
         """The flux normalization constant.
         The unit of this normalization constant is
         ([angle]^{-2} [energy]^{-1} [length]^{-2} [time]^{-1}).
         """
         return self._Phi0
+
     @Phi0.setter
     def Phi0(self, v):
-        v = float_cast(v,
+        v = float_cast(
+            v,
             'The Phi0 property must be castable to type float!')
         self._Phi0 = v
 
     @property
     def spatial_profile(self):
         """Instance of SpatialFluxProfile describing the spatial profile of the
         flux.
         """
         return self._spatial_profile
+
     @spatial_profile.setter
     def spatial_profile(self, profile):
-        if(profile is None):
-            profile = UnitySpatialFluxProfile()
-        if(not isinstance(profile, SpatialFluxProfile)):
-            raise TypeError('The spatial_profile property must be None, or an '
+        if not isinstance(profile, SpatialFluxProfile):
+            raise TypeError(
+                'The spatial_profile property must be None, or an '
                 'instance of SpatialFluxProfile!')
         self._spatial_profile = profile
 
     @property
     def energy_profile(self):
         """Instance of EnergyFluxProfile describing the energy profile of the
         flux.
         """
         return self._energy_profile
+
     @energy_profile.setter
     def energy_profile(self, profile):
-        if(profile is None):
-            profile = UnityEnergyFluxProfile()
-        if(not isinstance(profile, EnergyFluxProfile)):
-            raise TypeError('The energy_profile property must be None, or an '
+        if not isinstance(profile, EnergyFluxProfile):
+            raise TypeError(
+                'The energy_profile property must be None, or an '
                 'instance of EnergyFluxProfile!')
         self._energy_profile = profile
 
     @property
     def time_profile(self):
         """Instance of TimeFluxProfile describing the time profile of the flux.
         """
         return self._time_profile
+
     @time_profile.setter
     def time_profile(self, profile):
-        if(profile is None):
-            profile = UnityTimeFluxProfile()
-        if(not isinstance(profile, TimeFluxProfile)):
-            raise TypeError('The time_profile property must be None, or an '
+        if not isinstance(profile, TimeFluxProfile):
+            raise TypeError(
+                'The time_profile property must be None, or an '
                 'instance of TimeFluxProfile!')
         self._time_profile = profile
 
     @property
     def math_function_str(self):
         """The string showing the mathematical function of the flux.
         """
-        return '%.3e * %s * %s * %s * %s'%(
-            self._Phi0,
-            self.unit_str,
-            self._spatial_profile.math_function_str,
-            self._energy_profile.math_function_str,
-            self._time_profile.math_function_str
-        )
+        s = f'{self._Phi0:.3e}'
+
+        spatial_str = self._spatial_profile.math_function_str
+        if spatial_str is not None:
+            s += f' * {spatial_str}'
+        energy_str = self._energy_profile.math_function_str
+        if energy_str is not None:
+            s += f' * {energy_str}'
+
+        time_str = self._time_profile.math_function_str
+        if time_str is not None:
+            s += f' * {time_str}'
+
+        return s
 
     @property
     def angle_unit(self):
         """The unit of angle used for the flux calculation. The unit is
         taken and set from and to the set spatial flux profile, respectively.
         """
         return self._spatial_profile.angle_unit
+
     @angle_unit.setter
     def angle_unit(self, unit):
         self._spatial_profile.angle_unit = unit
 
     @property
     def energy_unit(self):
         """The unit of energy used for the flux calculation. The unit is
         taken and set from and to the set energy flux profile, respectively.
         """
         return self._energy_profile.energy_unit
+
     @energy_unit.setter
     def energy_unit(self, unit):
         self._energy_profile.energy_unit = unit
 
     @property
     def time_unit(self):
         """The unit of time used for the flux calculation. The unit is
         taken and set from and to the set time flux profile, respectively.
         """
         return self._time_profile.time_unit
+
     @time_unit.setter
     def time_unit(self, unit):
         self._time_profile.time_unit = unit
 
     @property
-    def parameter_names(self):
+    def param_names(self):
         """The tuple holding the names of the math function's parameters. This
         is the total set of parameter names for all flux profiles of this
         FactorizedFluxModel instance.
         """
-        pnames = list(self._parameter_names)
-        pnames += self._spatial_profile.parameter_names
-        pnames += self._energy_profile.parameter_names
-        pnames += self._time_profile.parameter_names
+        pnames = list(super(FactorizedFluxModel, type(self)).param_names)
+        pnames += self._spatial_profile.param_names
+        pnames += self._energy_profile.param_names
+        pnames += self._time_profile.param_names
 
         return tuple(pnames)
-    @parameter_names.setter
-    def parameter_names(self, names):
-        super(FactorizedFluxModel, self.__class__).parameter_names.fset(self, names)
+
+    @param_names.setter
+    def param_names(self, names):
+        super(FactorizedFluxModel, type(self)).param_names.fset(self, names)
 
     def __call__(
-            self, alpha, delta, E, t,
-            angle_unit=None, energy_unit=None, time_unit=None):
+            self,
+            ra=None,
+            dec=None,
+            E=None,
+            t=None,
+            angle_unit=None,
+            energy_unit=None,
+            time_unit=None,
+    ):
         """Calculates the flux values for the given celestrial positions,
         energies, and observation times.
 
         Parameters
         ----------
-        alpha : float | (Ncoord,)-shaped 1d numpy ndarray of float
+        ra: float | (Ncoord,)-shaped 1d numpy ndarray of float | None
             The right-ascention coordinate for which to retrieve the flux value.
-        delta : float | (Ncoord,)-shaped 1d numpy ndarray of float
+        dec : float | (Ncoord,)-shaped 1d numpy ndarray of float | None
             The declination coordinate for which to retrieve the flux value.
-        E : float | (Nenergy,)-shaped 1d numpy ndarray of float
+        E : float | (Nenergy,)-shaped 1d numpy ndarray of float | None
             The energy for which to retrieve the flux value.
-        t : float | (Ntime,)-shaped 1d numpy ndarray of float
+        t : float | (Ntime,)-shaped 1d numpy ndarray of float | None
             The observation time for which to retrieve the flux value.
         angle_unit : instance of astropy.units.UnitBase | None
             The unit of the given angles.
             If ``None``, the set angle unit of the spatial flux profile is
             assumed.
         energy_unit : instance of astropy.units.UnitBase | None
             The unit of the given energies.
@@ -1349,31 +2319,67 @@
 
         Returns
         -------
         flux : (Ncoord,Nenergy,Ntime)-shaped ndarray of float
             The flux values are in unit
             [energy]^{-1} [angle]^{-2} [length]^{-2} [time]^{-1}.
         """
-        spatial_profile_values = self._spatial_profile(
-            alpha, delta, unit=angle_unit)
-        energy_profile_values = self._energy_profile(
-            E, unit=energy_unit)
-        time_profile_values = self._time_profile(
-            t, unit=time_unit)
+        if (ra is not None) and (dec is not None):
+            spatial_profile_values = self._spatial_profile(
+                ra, dec, unit=angle_unit)
+        else:
+            spatial_profile_values = np.array([1])
+
+        if E is not None:
+            energy_profile_values = self._energy_profile(
+                E, unit=energy_unit)
+        else:
+            energy_profile_values = np.array([1])
+
+        if t is not None:
+            time_profile_values = self._time_profile(
+                t, unit=time_unit)
+        else:
+            time_profile_values = np.array([1])
 
         flux = (
             self._Phi0 *
-            spatial_profile_values[:,np.newaxis,np.newaxis] *
-            energy_profile_values[np.newaxis,:,np.newaxis] *
-            time_profile_values[np.newaxis,np.newaxis,:]
+            spatial_profile_values[:, np.newaxis, np.newaxis] *
+            energy_profile_values[np.newaxis, :, np.newaxis] *
+            time_profile_values[np.newaxis, np.newaxis, :]
         )
 
         return flux
 
-    def set_parameters(self, pdict):
+    def get_param(self, name):
+        """Retrieves the value of the given parameter. It returns ``np.nan`` if
+        the parameter does not exist.
+
+        Parameters
+        ----------
+        name : str
+            The name of the parameter.
+
+        Returns
+        -------
+        value : float | np.nan
+            The value of the parameter.
+        """
+        for obj in (
+                super(),
+                self._spatial_profile,
+                self._energy_profile,
+                self._time_profile):
+            value = obj.get_param(name=name)
+            if not np.isnan(value):
+                return value
+
+        return np.nan
+
+    def set_params(self, pdict):
         """Sets the parameters of the flux model. For this factorized flux model
         it means that it sets the parameters of the spatial, energy, and time
         profiles.
 
         Parameters
         ----------
         pdict : dict
@@ -1382,187 +2388,169 @@
         Returns
         -------
         updated : bool
             Flag if parameter values were actually updated.
         """
         updated = False
 
-        updated |= super(FactorizedFluxModel, self).set_parameters(pdict)
+        updated |= super().set_params(pdict)
 
-        updated |= self._spatial_profile.set_parameters(pdict)
-        updated |= self._energy_profile.set_parameters(pdict)
-        updated |= self._time_profile.set_parameters(pdict)
+        updated |= self._spatial_profile.set_params(pdict)
+        updated |= self._energy_profile.set_params(pdict)
+        updated |= self._time_profile.set_params(pdict)
 
         return updated
 
 
-class IsPointlikeSource(object):
-    """This is a classifier class that can be used by other classes to indicate
-    that the specific class describes a point-like source.
-    """
-    def __init__(
-            self, ra_func_instance=None, get_ra_func=None, set_ra_func=None,
-            dec_func_instance=None, get_dec_func=None, set_dec_func=None,
-            **kwargs):
-        """Constructor method. Gets called when the an instance of a class is
-        created which derives from this IsPointlikeSource class.
-
-
-        """
-        super(IsPointlikeSource, self).__init__(**kwargs)
-
-        self._ra_func_instance = ra_func_instance
-        self._get_ra_func = get_ra_func
-        self._set_ra_func = set_ra_func
-
-        self._dec_func_instance = dec_func_instance
-        self._get_dec_func = get_dec_func
-        self._set_dec_func = set_dec_func
-
-    @property
-    def ra(self):
-        """The right-ascention coordinate of the point-like source.
-        """
-        return self._get_ra_func(self._ra_func_instance)
-    @ra.setter
-    def ra(self, v):
-        self._set_ra_func(self._ra_func_instance, v)
-
-    @property
-    def dec(self):
-        """The declination coordinate of the point-like source.
-        """
-        return self._get_dec_func(self._dec_func_instance)
-    @dec.setter
-    def dec(self, v):
-        self._set_dec_func(self._dec_func_instance, v)
-
-
-class PointlikeSourceFFM(FactorizedFluxModel, IsPointlikeSource):
+class PointlikeFFM(
+        FactorizedFluxModel,
+        IsPointlike,
+):
     """This class describes a factorized flux model (FFM), where the spatial
     profile is modeled as a point. This class provides the base class for a flux
     model of a point-like source.
     """
     def __init__(
-            self, alpha_s, delta_s, Phi0, energy_profile, time_profile,
-            angle_unit=None, length_unit=None):
+            self,
+            Phi0,
+            energy_profile,
+            time_profile,
+            ra=None,
+            dec=None,
+            angle_unit=None,
+            length_unit=None,
+            **kwargs,
+    ):
         """Creates a new factorized flux model for a point-like source.
 
         Parameters
         ----------
-        alpha_s : float
-            The right-ascention of the point-like source.
-        delta_s : float
-            The declination of the point-like source.
         Phi0 : float
             The flux normalization constant in unit of flux.
-        energy_profile : EnergyFluxProfile instance | None
+        energy_profile : instance of EnergyFluxProfile | None
             The EnergyFluxProfile instance providing the energy profile
             function of the flux.
             If set to None, an instance of UnityEnergyFluxProfile will be used,
             which represents the constant function 1.
-        time_profile : TimeFluxProfile instance | None
+        time_profile : instance of TimeFluxProfile | None
             The TimeFluxProfile instance providing the time profile function
             of the flux.
             If set to None, an instance of UnityTimeFluxProfile will be used,
             which represents the constant function 1.
+        ra : float | None
+            The right-ascention of the point.
+        dec : float | None
+            The declination of the point.
         angle_unit : instance of astropy.units.UnitBase | None
             The unit for angles used for the flux unit.
             If set to ``None``, the configured internal angle unit is used.
         length_unit : instance of astropy.units.UnitBase | None
             The unit for length used for the flux unit.
             If set to ``None``, the configured internal length unit is used.
         """
-        spatial_profile=PointSpatialFluxProfile(
-            alpha_s, delta_s, angle_unit=angle_unit)
+        spatial_profile = PointSpatialFluxProfile(
+            cfg=kwargs.get('cfg'),
+            ra=ra,
+            dec=dec,
+            angle_unit=angle_unit)
 
-        super(PointlikeSourceFFM, self).__init__(
+        super().__init__(
             Phi0=Phi0,
             spatial_profile=spatial_profile,
             energy_profile=energy_profile,
             time_profile=time_profile,
             length_unit=length_unit,
             ra_func_instance=spatial_profile,
-            get_ra_func=spatial_profile.__class__.alpha_s.fget,
-            set_ra_func=spatial_profile.__class__.alpha_s.fset,
+            get_ra_func=type(spatial_profile).ra.fget,
+            set_ra_func=type(spatial_profile).ra.fset,
             dec_func_instance=spatial_profile,
-            get_dec_func=spatial_profile.__class__.delta_s.fget,
-            set_dec_func=spatial_profile.__class__.delta_s.fset
-        )
+            get_dec_func=type(spatial_profile).dec.fget,
+            set_dec_func=type(spatial_profile).dec.fset,
+            **kwargs)
+
+    @property
+    def unit_str(self):
+        """The string representation of the flux unit.
+        """
+        # Note:
+        #    For a point-like differential flux, there is no solid-angle
+        #    element.
+        s = (f'({self.energy_unit.to_string()}'
+             f' {self.length_unit.to_string()}^2'
+             f' {self.time_unit.to_string()})^-1')
+
+        return s
+
+    @property
+    def unit_latex_str(self):
+        """The latex string representation of the flux unit.
+        """
+        # Note:
+        #    For a point-like differential flux, there is no solid-angle
+        #    element.
+        s = (f'{self.energy_unit.to_string()}''$^{-1}$ '
+             f'{self.length_unit.to_string()}''$^{-2}$ '
+             f'{self.time_unit.to_string()}''$^{-1}$')
+
+        return s
 
 
-class SteadyPointlikeSourceFFM(PointlikeSourceFFM):
+class SteadyPointlikeFFM(
+        PointlikeFFM,
+):
     """This class describes a factorized flux model (FFM), where the spatial
     profile is modeled as a point and the time profile as constant 1. It is
-    derived from the ``PointlikeSourceFFM`` class.
+    derived from the ``PointlikeFFM`` class.
     """
     def __init__(
-            self, alpha_s, delta_s, Phi0, energy_profile,
-            angle_unit=None, length_unit=None, time_unit=None):
+            self,
+            Phi0,
+            energy_profile,
+            ra=None,
+            dec=None,
+            angle_unit=None,
+            length_unit=None,
+            time_unit=None,
+            **kwargs,
+    ):
         """Creates a new factorized flux model for a point-like source with no
         time dependance.
 
         Parameters
         ----------
-        alpha_s : float
-            The right-ascention of the point-like source.
-        delta_s : float
-            The declination of the point-like source.
         Phi0 : float
             The flux normalization constant.
-        energy_profile : EnergyFluxProfile instance | None
+        energy_profile : instance of EnergyFluxProfile | None
             The EnergyFluxProfile instance providing the energy profile
             function of the flux.
             If set to None, an instance of UnityEnergyFluxProfile will be used,
             which represents the constant function 1.
+        ra : float | None
+            The right-ascention of the point.
+        dec : float | None
+            The declination of the point.
+        angle_unit : instance of astropy.units.UnitBase | None
+            The unit for angles used for the flux unit.
+            If set to ``None``, the configured default angle unit for fluxes
+            is used.
+        length_unit : instance of astropy.units.UnitBase | None
+            The unit for length used for the flux unit.
+            If set to ``None``, the configured default length unit for fluxes
+            is used.
+        time_unit : instance of astropy.units.UnitBase | None
+            The used unit for time.
+            If set to ``None``, the configured default time unit for fluxes
+            is used.
         """
-        super(SteadyPointlikeSourceFFM, self).__init__(
-            alpha_s=alpha_s,
-            delta_s=delta_s,
+        time_profile = UnityTimeFluxProfile(
+            cfg=kwargs.get('cfg'),
+            time_unit=time_unit)
+
+        super().__init__(
             Phi0=Phi0,
+            ra=ra,
+            dec=dec,
             energy_profile=energy_profile,
-            time_profile=UnityTimeFluxProfile(time_unit=time_unit),
+            time_profile=time_profile,
             angle_unit=angle_unit,
-            length_unit=length_unit
-        )
-
-    def __call__(
-            self, alpha, delta, E,
-            angle_unit=None, energy_unit=None):
-        """Calculates the flux values for the given celestrial positions, and
-        energies.
-
-        Parameters
-        ----------
-        alpha : float | (Ncoord,)-shaped 1d numpy ndarray of float
-            The right-ascention coordinate for which to retrieve the flux value.
-        delta : float | (Ncoord,)-shaped 1d numpy ndarray of float
-            The declination coordinate for which to retrieve the flux value.
-        E : float | (Nenergy,)-shaped 1d numpy ndarray of float
-            The energy for which to retrieve the flux value.
-        angle_unit : instance of astropy.units.UnitBase | None
-            The unit of the given angles.
-            If ``None``, the set angle unit of the spatial flux profile is
-            assumed.
-        energy_unit : instance of astropy.units.UnitBase | None
-            The unit of the given energies.
-            If ``None``, the set energy unit of the energy flux profile is
-            assumed.
-
-        Returns
-        -------
-        flux : (Ncoord,Nenergy)-shaped ndarray of float
-            The flux values are in unit
-            [energy]^{-1} [angle]^{-2} [length]^{-2} [time]^{-1}.
-        """
-        spatial_profile_values = self._spatial_profile(
-            alpha, delta, unit=angle_unit)
-        energy_profile_values = self._energy_profile(
-            E, unit=energy_unit)
-
-        flux = (
-            self._Phi0 *
-            spatial_profile_values[:,np.newaxis] *
-            energy_profile_values[np.newaxis,:]
-        )
-
-        return flux
+            length_unit=length_unit,
+            **kwargs)
```

### Comparing `skyllh-23.1.1/skyllh/physics/source.py` & `skyllh-23.2.0/skyllh/core/source_model.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,302 +1,326 @@
 # -*- coding: utf-8 -*-
 
-"""The ``source`` module contains the base class ``SourceModel`` for modelling a
-source in the sky. What kind of properties this source has is modeled by a
-derived class. The most common one is the PointLikeSource source model for a
-point-like source at a given location in the sky with a given flux model.
+"""The :mod:`~skyllh.core.model` module contains the base class ``SourceModel``
+for modelling a source in the sky. What kind of properties this source has is
+modeled by a derived class. The most common one is the PointLikeSource source
+model for a point-like source at a given location in the sky.
 """
 
 import numpy as np
 
+from skyllh.core.model import (
+    Model,
+    ModelCollection,
+)
 from skyllh.core.py import (
-    ObjectCollection,
     classname,
     float_cast,
-    issequence
+    issequenceof,
+    str_cast,
+    typename,
 )
 
 
-class SourceLocation(object):
-    """Stores the location of a source, i.e. right-ascention and declination.
-    """
-    def __init__(self, ra, dec):
-        self.ra = ra
-        self.dec = dec
-
-    @property
-    def ra(self):
-        """The right-ascention angle in radian of the source position.
-        """
-        return self._ra
-    @ra.setter
-    def ra(self, v):
-        v = float_cast(v, 'The ra property must be castable to type float!')
-        self._ra = v
-
-    @property
-    def dec(self):
-        """The declination angle in radian of the source position.
-        """
-        return self._dec
-    @dec.setter
-    def dec(self, v):
-        v = float_cast(v, 'The dec property must be castable to type float!')
-        self._dec = v
-
-class SourceWeights(object):
-    """Stores the relative weights of a source, i.e. weights and gradients.
-       There are two weights that should be included. one is the detector weight,
-       which is declination dependent, and the other is a hypothesis weight, and that
-       is provided by the user.
+class SourceModel(
+        Model,
+):
+    """The base class for all source models in SkyLLH. A source can have a
+    relative weight w.r.t. other sources.
     """
-    def __init__(self, src_w=None, src_w_grad=None, src_w_W=None):
-        self.src_w      = src_w
-        self.src_w_grad = src_w_grad
-        self.src_w_W    = src_w_W
-    @property
-    def src_w(self):
-        """The relative weight of the source(s).
-        """
-        return self._src_w
-    @src_w.setter
-    def src_w(self, v):
-        v = float_cast(v, 'The src_w property must be castable to type float!')
-        self._src_w = v
+    def __init__(
+            self,
+            name=None,
+            classification=None,
+            weight=None,
+            **kwargs):
+        """Creates a new source model instance.
 
-    @property
-    def src_w_grad(self):
-        """The relative weight gradients of the source(s).
-        """
-        return self._src_w_grad
-    @src_w_grad.setter
-    def src_w_grad(self, v):
-        v = float_cast(v, 'The src_w_grad property must be castable to type float!')
-        self._src_w_grad = v
-
-    @property
-    def src_w_W(self):
-        """The hypothesis weight of the source(s).
-        """
-        return self._src_w_W
-    @src_w_W.setter
-    def src_w_W(self, v):
-        v = float_cast(v, 'The src_w_W property must be castable to type float!')
-        self._src_w_W = v
-
-
-class SourceModel(object):
-    """The base class for all source models in Skyllh. Each source has a central
-    location given by a right-ascention and declination location.
-    """
-    def __init__(self, ra, dec, src_w=None, src_w_grad=None, src_w_W=None):
-        self.loc = SourceLocation(ra, dec)
-        src_w = np.ones_like(self.loc.ra, dtype=np.float64)
-        src_w_grad = np.zeros_like(self.loc.ra, dtype=np.float64)
-
-        if (src_w_W is None):
-            src_w_W = np.ones_like(self.loc.ra, dtype=np.float64)
-
-        self.weight = SourceWeights(src_w, src_w_grad, src_w_W)
-
-    @property
-    def loc(self):
-        """The location of the source.
-        """
-        return self._loc
-    @loc.setter
-    def loc(self, srcloc):
-        if(not isinstance(srcloc, SourceLocation)):
-            raise TypeError('The loc property must be an instance of SourceLocation!')
-        self._loc = srcloc
+        Parameters
+        ----------
+        name : str | None
+            The name of the source model.
+        classification : str | None
+            The astronomical classification of the source.
+        weight : float | None
+            The relative weight of the source w.r.t. other sources.
+            If set to None, unity will be used.
+        """
+        super().__init__(
+            name=name,
+            **kwargs)
+
+        self.classification = classification
+        self.weight = weight
+
+    @property
+    def classification(self):
+        """The astronomical classification of the source.
+        """
+        return self._classification
+
+    @classification.setter
+    def classification(self, c):
+        self._classification = str_cast(
+            c,
+            'The classification property must be castable to type str!',
+            allow_None=True)
 
     @property
     def weight(self):
         """The weight of the source.
         """
         return self._weight
-    @weight.setter
-    def weight(self, w_src):
-        if(not isinstance(w_src, SourceWeights)):
-            raise TypeError('The weight property must be an instance of SourceWeights!')
-        self._weight = w_src
 
-    @property
-    def id(self):
-        """(read-only) The ID of the source. It's an integer generated with the
-        id() function. Hence, it's related to the memory address of the object.
-        """
-        return id(self)
+    @weight.setter
+    def weight(self, w):
+        if w is None:
+            w = 1.
+        w = float_cast(
+            w,
+            'The weight property must be castable to type float!')
+        self._weight = w
 
 
-class SourceCollection(ObjectCollection):
-    """This class describes a collection of sources. It can be used to group
-    sources into a single object, for instance for a stacking analysis.
+class SourceModelCollection(
+        ModelCollection,
+):
+    """This class describes a collection of source models. It can be used to
+    group sources into a single object, for instance for a stacking analysis.
     """
     @staticmethod
-    def cast(obj, errmsg):
-        """Casts the given object to a SourceCollection object. If the cast
+    def cast(
+            obj,
+            errmsg=None,
+            **kwargs):
+        """Casts the given object to a SourceModelCollection object. If the cast
         fails, a TypeError with the given error message is raised.
 
         Parameters
         ----------
-        obj : SourceModel | sequence of SourceModel | SourceCollection
-            The object that should be casted to SourceCollection.
-        errmsg : str
+        obj : SourceModel | sequence of SourceModel | SourceModelCollection |
+                None
+            The object that should be casted to SourceModelCollection.
+            If set to None, an empty SourceModelCollection is created.
+        errmsg : str | None
             The error message if the cast fails.
+            If set to None, a generic error message will be used.
+
+        Additional keyword arguments
+        ----------------------------
+        Additional keyword arguments are passed to the constructor of the
+        SourceModelCollection class.
 
         Raises
         ------
         TypeError
-            If the cast fails.
+            If the cast failed.
         """
-        if(isinstance(obj, SourceModel)):
-            obj = SourceCollection(SourceModel, [obj])
-        if(not isinstance(obj, SourceCollection)):
-            if(issequence(obj)):
-                obj = SourceCollection(SourceModel, obj)
-            else:
-                raise TypeError(errmsg)
-        return obj
-
-    def __init__(self, source_type=None, sources=None):
+        if obj is None:
+            return SourceModelCollection(
+                sources=None, source_type=SourceModel, **kwargs)
+
+        if isinstance(obj, SourceModel):
+            return SourceModelCollection(
+                sources=[obj], source_type=SourceModel, **kwargs)
+
+        if isinstance(obj, SourceModelCollection):
+            return obj
+
+        if issequenceof(obj, SourceModel):
+            return SourceModelCollection(
+                sources=obj, source_type=SourceModel, **kwargs)
+
+        if errmsg is None:
+            errmsg = (f'Cast of object "{str(obj)}" of type '
+                      f'"{typename(obj)}" to SourceModelCollection failed!')
+        raise TypeError(errmsg)
+
+    def __init__(
+            self,
+            sources=None,
+            source_type=None,
+            **kwargs):
         """Creates a new source collection.
 
         Parameters
         ----------
-        source_type : type | None
-            The type of the source. If set to None (default), SourceModel will
-            be used.
         sources : sequence of source_type instances | None
             The sequence of sources this collection should be initalized with.
+            If set to None, an empty SourceModelCollection instance is created.
+        source_type : type | None
+            The type of the source.
+            If set to None (default), SourceModel will be used.
         """
-        if(source_type is None):
+        if source_type is None:
             source_type = SourceModel
-        super(SourceCollection, self).__init__(sources, obj_type=source_type)
+
+        super().__init__(
+            models=sources,
+            model_type=source_type,
+            **kwargs)
 
     @property
     def source_type(self):
         """(read-only) The type of the source model.
+        This property is an alias for the `obj_type` property.
         """
-        return self.obj_type
+        return self.model_type
 
     @property
     def sources(self):
         """(read-only) The list of sources of type ``source_type``.
         """
-        return self.objects
+        return self.models
 
 
-class Catalog(SourceCollection):
-    """This class describes a catalog of sources. It is derived from
-    SourceCollection. A catalog has a name.
+class IsPointlike(
+        object):
+    """This is a classifier class that can be used by other classes to indicate
+    that the specific class describes a point-like object.
     """
-    def __init__(self, name, source_type=None, sources=None):
-        """Creates a new source catalog.
+    def __init__(
+            self,
+            ra_func_instance=None,
+            get_ra_func=None,
+            set_ra_func=None,
+            dec_func_instance=None,
+            get_dec_func=None,
+            set_dec_func=None,
+            **kwargs):
+        """Constructor method. Gets called when the an instance of a class is
+        created which derives from this IsPointlike class.
 
         Parameters
         ----------
-        name : str
-            The name of the catalog.
-        source_type : type | None
-            The type of the source. If set to None (default), the default type
-            defined by SourceCollection will be used.
-        sources : sequence of source_type | None
-            The sequence of sources this catalog should be initalized with.
-        """
-        super(Catalog, self).__init__(source_type=source_type, sources=sources)
-        self.name = name
-
-    @property
-    def name(self):
-        """The name of the catalog.
-        """
-        return self._name
-    @name.setter
-    def name(self, name):
-        if(not isinstance(name, str)):
-            raise TypeError('The name property must be of type str!')
-        self._name = name
-
-    def as_source_collection(self):
-        """Creates a SourceCollection object for this catalog and returns it.
-        """
-        source_collection = SourceCollection(source_type=self.source_type, sources=self.sources)
-        return source_collection
-
-
-class PointLikeSource(SourceModel):
-    """The PointLikeSource class is a source model for a point-like source
-    object in the sky at a given location (right-ascention and declination).
-    """
-    def __init__(self, ra, dec, src_w=None, src_w_grad=None, src_w_W=None):
-        super(PointLikeSource, self).__init__(ra, dec, src_w, src_w_grad, src_w_W)
+        ra_func_instance : object
+            The instance object the right-ascention property's getter and setter
+            functions are defined in.
+        get_ra_func : callable
+            The callable object of the getter function of the right-ascention
+            property. It must have the call signature
+            `__call__(ra_func_instance)`.
+        set_ra_func : callable
+            The callable object of the setter function of the right-ascention
+            property. It must have the call signature
+            `__call__(ra_func_instance, value)`.
+        dec_func_instance : object
+            The instance object the declination property's getter and setter
+            functions are defined in.
+        get_dec_func : object
+            The callable object of the getter function of the declination
+            property. It must have the call signature
+            `__call__(dec_func_instance)`.
+        set_dec_func : object
+            The callable object of the setter function of the declination
+            property. It must have the call signature
+            `__call__(dec_func_instance, value)`.
+        """
+        super().__init__(**kwargs)
+
+        self._ra_func_instance = ra_func_instance
+        self._get_ra_func = get_ra_func
+        self._set_ra_func = set_ra_func
+
+        self._dec_func_instance = dec_func_instance
+        self._get_dec_func = get_dec_func
+        self._set_dec_func = set_dec_func
 
     @property
     def ra(self):
-        """(read-only) The right-ascention angle in radian of the source
-        position.
-        This is a short-cut for `self.loc.ra`.
+        """The right-ascention coordinate of the point-like source.
         """
-        return self._loc._ra
+        return self._get_ra_func(self._ra_func_instance)
+
+    @ra.setter
+    def ra(self, v):
+        v = float_cast(
+            v,
+            'The ra property must be castable to type float!')
+        self._set_ra_func(self._ra_func_instance, v)
 
     @property
     def dec(self):
-        """(read-only) The declination angle in radian of the source position.
-        This is a short-cut for `self.loc.dec`.
+        """The declination coordinate of the point-like source.
         """
-        return self._loc._dec
+        return self._get_dec_func(self._dec_func_instance)
 
-    def __str__(self):
-        """Pretty string representation of this class instance.
-        """
-        s = classname(self) + ': { ra=%.3f deg, dec=%.3f deg }'%(
-            np.rad2deg(self.ra), np.rad2deg(self.dec))
-        return s
+    @dec.setter
+    def dec(self, v):
+        v = float_cast(
+            v,
+            'The dec property must be castable to type float!')
+        self._set_dec_func(self._dec_func_instance, v)
 
 
-class PointLikeSourceCollection(SourceCollection):
-    """Describes a collection of point-like sources.
+class PointLikeSource(
+        SourceModel,
+        IsPointlike):
+    """The PointLikeSource class is a source model for a point-like source
+    object in the sky at a given location (right-ascention and declination).
     """
-    def __init__(self, sources=None):
-        """Creates a new collection of PointLikeSource objects.
+    def __init__(
+            self,
+            ra,
+            dec,
+            name=None,
+            weight=None,
+            **kwargs):
+        """Creates a new PointLikeSource instance for defining a point-like
+        source.
 
         Parameters
         ----------
-        sources : sequence of PointLikeSource instances | None
-            The sequence of PointLikeSource objects this collection should be
-            initalized with.
-        """
-        super(PointLikeSourceCollection, self).__init__(
-            source_type=PointLikeSource, sources=sources)
+        ra : float
+            The right-ascention coordinate of the source in radians.
+        dec : float
+            The declination coordinate of the source in radians.
+        name : str | None
+            The name of the source.
+        weight : float | None
+            The relative weight of the source w.r.t. other sources.
+            If set to None, unity will be used.
+        """
+        super().__init__(
+            name=name,
+            weight=weight,
+            ra_func_instance=self,
+            get_ra_func=type(self)._get_ra,
+            set_ra_func=type(self)._set_ra,
+            dec_func_instance=self,
+            get_dec_func=type(self)._get_dec,
+            set_dec_func=type(self)._set_dec,
+            **kwargs,
+        )
 
-    @property
-    def ra(self):
-        """(read-only) The ndarray with the right-ascention of all the sources.
-        """
-        return np.array([ src.ra for src in self ])
+        self.ra = ra
+        self.dec = dec
 
-    @property
-    def dec(self):
-        """(read-only) The ndarray with the declination of all the sources.
-        """
-        return np.array([ src.dec for src in self ])
+    def _get_ra(self):
+        return self._ra
 
+    def _set_ra(self, ra):
+        self._ra = ra
 
-class PointLikeSourceCatalog(Catalog):
-    """Describes a catalog of point-like sources. The difference to a
-    PointLikeSourceCollection is the additional properties of a catalog, e.g.
-    the name.
-    """
-    def __init__(self, name, sources=None):
-        """Creates a new point source catalog of the given name.
+    def _get_dec(self):
+        return self._dec
 
-        Parameters
-        ----------
-        name : str
-            The name of the point-like source catalog.
-        sources : sequence of PointLikeSource instances | None
-            The sequence of PointLikeSource instances this catalog should be
-            initalized with.
+    def _set_dec(self, dec):
+        self._dec = dec
+
+    def __str__(self):
+        """Pretty string representation.
         """
-        super(PointLikeSourceCatalog, self).__init__(
-            name=name, source_type=PointLikeSource, sources=sources)
+        c = ''
+        if self.classification is not None:
+            c = f', classification={self.classification}'
+
+        s = (
+            f'{classname(self)}: "{self.name}": '
+            '{ '
+            f'ra={np.rad2deg(self.ra):.3f} deg, '
+            f'dec={np.rad2deg(self.dec):.3f} deg'
+            f'{c}'
+            ' }'
+        )
+
+        return s
```

### Comparing `skyllh-23.1.1/skyllh/plotting/core/pdfratio.py` & `skyllh-23.2.0/skyllh/plotting/core/pdfratio.py`

 * *Files 8% similar despite different names*

```diff
@@ -6,22 +6,21 @@
 import numpy as np
 import itertools
 
 from matplotlib.axes import Axes
 from matplotlib.colors import LogNorm
 
 from skyllh.core.py import classname
-from skyllh.core.source_hypothesis import SourceHypoGroupManager
 from skyllh.core.storage import DataFieldRecordArray
 from skyllh.core.trialdata import TrialDataManager
-from skyllh.core.pdfratio import SpatialSigOverBkgPDFRatio
+from skyllh.core.pdfratio import SigOverBkgPDFRatio
 
 
-class SpatialSigOverBkgPDFRatioPlotter(object):
-    """Plotter class to plot a SpatialSigOverBkgPDFRatio object.
+class SigOverBkgPDFRatioPlotter(object):
+    """Plotter class to plot a SigOverBkgPDFRatio object.
     """
     def __init__(self, tdm, pdfratio):
         """Creates a new plotter object for plotting a
         SpatialSigOverBkgPDFRatio object.
 
         Parameters
         ----------
@@ -35,34 +34,44 @@
         self.pdfratio = pdfratio
 
     @property
     def pdfratio(self):
         """The PDF ratio object to plot.
         """
         return self._pdfratio
+
     @pdfratio.setter
     def pdfratio(self, pdfratio):
-        if(not isinstance(pdfratio, SpatialSigOverBkgPDFRatio)):
-            raise TypeError('The pdfratio property must be an object of instance SpatialSigOverBkgPDFRatio!')
+        if not isinstance(pdfratio, SigOverBkgPDFRatio):
+            raise TypeError(
+                'The pdfratio property must be an instance of '
+                'SigOverBkgPDFRatio!')
         self._pdfratio = pdfratio
 
     @property
     def tdm(self):
         """The TrialDataManager that provides the data for the PDF evaluation.
         """
         return self._tdm
+
     @tdm.setter
     def tdm(self, obj):
-        if(not isinstance(obj, TrialDataManager)):
-            raise TypeError('The tdm property must be an instance of '
-                'TrialDataManager!')
+        if not isinstance(obj, TrialDataManager):
+            raise TypeError(
+                'The tdm property must be an instance of TrialDataManager!')
         self._tdm = obj
 
-    def plot(self, src_hypo_group_manager, axes, source_idx=None, log=True,
-            **kwargs):
+    def plot(
+            self,
+            src_hypo_group_manager,
+            axes,
+            source_idx=None,
+            log=True,
+            **kwargs
+    ):
         """Plots the spatial PDF ratio. If the signal PDF depends on the source,
         source_idx specifies the index of the source for which the PDF should
         get plotted.
 
         Parameters
         ----------
         src_hypo_group_manager : instance of SourceHypoGroupManager
@@ -82,73 +91,78 @@
         function.
 
         Returns
         -------
         img : instance of mpl.AxesImage
             The AxesImage instance showing the PDF ratio image.
         """
-        if(not isinstance(axes, Axes)):
-            raise TypeError('The axes argument must be an instance of matplotlib.axes.Axes!')
+        if not isinstance(axes, Axes):
+            raise TypeError(
+                'The axes argument must be an instance of '
+                'matplotlib.axes.Axes!')
 
-        if(source_idx is None):
+        if source_idx is None:
             source_idx = 0
 
         # Define the binning for ra, dec, and sin_dec.
         delta_ra_deg = 0.5
         delta_dec_deg = 0.5
 
-        raaxis = self._pdfratio.signalpdf.axes.get_axis('ra')
-        decaxis = self._pdfratio.signalpdf.axes.get_axis('dec')
+        raaxis = self._pdfratio.signalpdf.axes['ra']
+        decaxis = self._pdfratio.signalpdf.axes['dec']
 
         # Create a grid of ratio in right-ascention and declination and fill it
         # with PDF ratio values from events that fall into these bins.
         # Use a binning for 1/2 degree.
         rabins = int(np.ceil(raaxis.length / np.deg2rad(delta_ra_deg)))
         decbins = int(np.ceil(decaxis.length / np.deg2rad(delta_dec_deg)))
 
-        ratios = np.zeros((rabins,decbins), dtype=np.float64)
+        ratios = np.zeros((rabins, decbins), dtype=np.float64)
 
         ra_binedges = np.linspace(raaxis.vmin, raaxis.vmax, rabins+1)
         ra_bincenters = 0.5*(ra_binedges[:-1] + ra_binedges[1:])
 
         dec_binedges = np.linspace(decaxis.vmin, decaxis.vmax, decbins+1)
         dec_bincenters = 0.5*(dec_binedges[:-1] + dec_binedges[1:])
 
         # Generate events that fall into the ratio bins.
         events = DataFieldRecordArray(
             np.zeros(
                 (ratios.size,),
-                dtype=[('ira', np.int64), ('ra', np.float64),
-                       ('idec', np.int64), ('dec', np.float64),
+                dtype=[('ira', np.int64),
+                       ('ra', np.float64),
+                       ('idec', np.int64),
+                       ('dec', np.float64),
                        ('sin_dec', np.float64),
                        ('ang_err', np.float64)]))
-        for (i, ((ira,ra),(idec,dec))) in enumerate(itertools.product(
+        for (i, ((ira, ra), (idec, dec))) in enumerate(itertools.product(
                                                 enumerate(ra_bincenters),
                                                 enumerate(dec_bincenters))):
             events['ira'][i] = ira
             events['ra'][i] = ra
             events['idec'][i] = idec
             events['dec'][i] = dec
             events['sin_dec'][i] = np.sin(dec)
             events['ang_err'][i] = np.deg2rad(0.5)
 
         self._tdm.initialize_for_new_trial(src_hypo_group_manager, events)
 
         event_ratios = self._pdfratio.get_ratio(self._tdm)
 
         # Select only the ratios for the requested source.
-        if(event_ratios.ndim == 2):
+        if event_ratios.ndim == 2:
             event_ratios = event_ratios[source_idx]
 
-        ratios[events['ira'],events['idec']] = event_ratios
+        ratios[events['ira'], events['idec']] = event_ratios
 
         (left, right, bottom, top) = (raaxis.vmin, raaxis.vmax,
                                       decaxis.vmin, decaxis.vmax)
         norm = LogNorm() if log else None
-        img = axes.imshow(ratios.T,
+        img = axes.imshow(
+            ratios.T,
             extent=(left, right, bottom, top),
             origin='lower',
             norm=norm,
             interpolation='none', **kwargs)
         axes.set_xlabel(raaxis.name)
         axes.set_ylabel(decaxis.name)
         axes.set_title(classname(self._pdfratio))
```

### Comparing `skyllh-23.1.1/skyllh/plotting/core/signalpdf.py` & `skyllh-23.2.0/skyllh/plotting/core/signalpdf.py`

 * *Files 5% similar despite different names*

```diff
@@ -2,69 +2,96 @@
 
 import numpy as np
 import itertools
 
 from matplotlib.axes import Axes
 from matplotlib.colors import LogNorm
 
-from skyllh.core.py import classname
-from skyllh.core.source_hypothesis import SourceHypoGroupManager
-from skyllh.core.storage import DataFieldRecordArray
-from skyllh.core.trialdata import TrialDataManager
 from skyllh.core.pdf import (
     IsSignalPDF,
-    SpatialPDF
+    SpatialPDF,
+)
+from skyllh.core.py import (
+    classname,
+)
+from skyllh.core.source_hypo_grouping import (
+    SourceHypoGroupManager,
+)
+from skyllh.core.storage import (
+    DataFieldRecordArray,
+)
+from skyllh.core.trialdata import (
+    TrialDataManager,
 )
 
 
-class SignalSpatialPDFPlotter(object):
+class SignalSpatialPDFPlotter(
+        object,
+):
     """Plotter class to plot spatial signal PDF object.
     """
-    def __init__(self, tdm, pdf):
+    def __init__(
+            self,
+            tdm,
+            pdf,
+            **kwargs,
+    ):
         """Creates a new plotter object for plotting a spatial signal PDF
         object.
 
         Parameters
         ----------
         tdm : instance of TrialDataManager
             The instance of TrialDataManager that provides the data for the
             PDF evaluation.
         pdf : class instance derived from SpatialPDF and IsSignalPDF
             The PDF object to plot.
         """
+        super().__init__(**kwargs)
         self.tdm = tdm
         self.pdf = pdf
 
     @property
     def pdf(self):
         """The PDF object to plot.
         """
         return self._pdf
+
     @pdf.setter
     def pdf(self, pdf):
-        if(not isinstance(pdf, SpatialPDF)):
-            raise TypeError('The pdf property must be an object of instance SpatialPDF!')
-        if(not isinstance(pdf, IsSignalPDF)):
-            raise TypeError('The pdf property must be an object of instance IsSignalPDF!')
+        if not isinstance(pdf, SpatialPDF):
+            raise TypeError(
+                'The pdf property must be an object of instance SpatialPDF!')
+        if not isinstance(pdf, IsSignalPDF):
+            raise TypeError(
+                'The pdf property must be an object of instance IsSignalPDF!')
         self._pdf = pdf
 
     @property
     def tdm(self):
         """The TrialDataManager that provides the data for the PDF evaluation.
         """
         return self._tdm
+
     @tdm.setter
     def tdm(self, obj):
-        if(not isinstance(obj, TrialDataManager)):
-            raise TypeError('The tdm property must be an instance of '
-                'TrialDataManager!')
+        if not isinstance(obj, TrialDataManager):
+            raise TypeError(
+                'The tdm property must be an instance of TrialDataManager!')
         self._tdm = obj
 
-    def plot(self, src_hypo_group_manager, axes, source_idx=None, sin_dec=True,
-             log=True, **kwargs):
+    def plot(
+            self,
+            src_hypo_group_manager,
+            axes,
+            source_idx=None,
+            sin_dec=True,
+            log=True,
+            **kwargs,
+    ):
         """Plots the signal spatial PDF for the specified source.
 
         Parameters
         ----------
         axes : mpl.axes.Axes
             The matplotlib Axes object on which the PDF ratio should get drawn
             to.
@@ -82,87 +109,98 @@
         function.
 
         Returns
         -------
         img : instance of mpl.AxesImage
             The AxesImage instance showing the PDF ratio image.
         """
-        if(not isinstance(src_hypo_group_manager, SourceHypoGroupManager)):
-            raise TypeError('The src_hypo_group_manager argument must be an '
+        if not isinstance(src_hypo_group_manager, SourceHypoGroupManager):
+            raise TypeError(
+                'The src_hypo_group_manager argument must be an '
                 'instance of SourceHypoGroupManager!')
-        if(not isinstance(axes, Axes)):
-            raise TypeError('The axes argument must be an instance of '
+        if not isinstance(axes, Axes):
+            raise TypeError(
+                'The axes argument must be an instance of '
                 'matplotlib.axes.Axes!')
 
-        if(source_idx is None):
+        if source_idx is None:
             source_idx = 0
 
         # Define the binning for ra, dec, and sin_dec.
         delta_ra_deg = 0.5
         delta_dec_deg = 0.5
         delta_sin_dec = 0.01
         # Define the event spatial uncertainty.
         sigma_deg = 0.5
 
         # Create a grid of signal probabilities in right-ascention and
         # declination/sin(declination) and fill it with probabilities from
         # events that fall into these bins.
-        raaxis = self.pdf.axes.get_axis('ra')
+        raaxis = self.pdf.axes['ra']
         rabins = int(np.ceil(raaxis.length / np.deg2rad(delta_ra_deg)))
         ra_binedges = np.linspace(raaxis.vmin, raaxis.vmax, rabins+1)
         ra_bincenters = 0.5*(ra_binedges[:-1] + ra_binedges[1:])
 
-        decaxis = self.pdf.axes.get_axis('dec')
-        if(sin_dec is True):
+        decaxis = self.pdf.axes['dec']
+        if sin_dec is True:
             (dec_min, dec_max) = (np.sin(decaxis.vmin), np.sin(decaxis.vmax))
             decbins = int(np.ceil((dec_max-dec_min) / delta_sin_dec))
         else:
             (dec_min, dec_max) = (decaxis.vmin, decaxis.vmax)
             decbins = int(np.ceil(decaxis.length / np.deg2rad(delta_dec_deg)))
         dec_binedges = np.linspace(dec_min, dec_max, decbins+1)
         dec_bincenters = 0.5*(dec_binedges[:-1] + dec_binedges[1:])
 
-        probs = np.zeros((rabins,decbins), dtype=np.float64)
+        probs = np.zeros((rabins, decbins), dtype=np.float64)
 
         # Generate events that fall into the probability bins.
-        events = DataFieldRecordArray(np.zeros((probs.size,),
-            dtype=[('ira', np.int64), ('ra', np.float64),
-                   ('idec', np.int64), ('dec', np.float64),
-                   ('ang_err', np.float64)]))
-        for (i, ((ira,ra),(idec,dec))) in enumerate(itertools.product(
+        events = DataFieldRecordArray(
+            np.zeros(
+                (probs.size,),
+                dtype=[
+                    ('ira', np.int64), ('ra', np.float64),
+                    ('idec', np.int64), ('dec', np.float64),
+                    ('ang_err', np.float64)
+                ]))
+        for (i, ((ira, ra), (idec, dec))) in enumerate(itertools.product(
                 enumerate(ra_bincenters),
                 enumerate(dec_bincenters))):
             events['ira'][i] = ira
             events['ra'][i] = ra
             events['idec'][i] = idec
-            if(sin_dec is True):
+            if sin_dec is True:
                 events['dec'][i] = np.arcsin(dec)
             else:
                 events['dec'][i] = dec
             events['ang_err'][i] = np.deg2rad(sigma_deg)
 
         self._tdm.initialize_for_new_trial(src_hypo_group_manager, events)
 
         event_probs = self._pdf.get_prob(self._tdm)
 
         # Select only the probabilities for the requested source.
-        if(event_probs.ndim == 2):
+        if event_probs.ndim == 2:
             event_probs = event_probs[source_idx]
 
         # Fill the probs grid array.
-        probs[events['ira'],events['idec']] = event_probs
+        probs[events['ira'], events['idec']] = event_probs
 
         (left, right, bottom, top) = (raaxis.vmin, raaxis.vmax,
                                       dec_min, dec_max)
         norm = None
-        if(log):
+        if log:
             norm = LogNorm()
-        img = axes.imshow(probs.T, extent=(left, right, bottom, top),
-            origin='lower', norm=norm, interpolation='none', **kwargs)
+        img = axes.imshow(
+            probs.T,
+            extent=(left, right, bottom, top),
+            origin='lower',
+            norm=norm,
+            interpolation='none',
+            **kwargs)
         axes.set_xlabel(raaxis.name)
-        if(sin_dec is True):
+        if sin_dec is True:
             axes.set_ylabel('sin('+decaxis.name+')')
         else:
             axes.set_ylabel(decaxis.name)
         axes.set_title(classname(self._pdf))
 
         return img
```

### Comparing `skyllh-23.1.1/skyllh/plotting/i3/backgroundpdf.py` & `skyllh-23.2.0/skyllh/plotting/i3/backgroundpdf.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,21 +1,36 @@
 # -*- coding: utf-8 -*-
 
 """Plotting module to plot IceCube specific background PDF objects.
 """
-import numpy as np
 
-from matplotlib.axes import Axes
-from matplotlib.colors import LogNorm
+import numpy as np
 
-from skyllh.core.py import classname
-from skyllh.core.source_hypothesis import SourceHypoGroupManager
-from skyllh.core.storage import DataFieldRecordArray
-from skyllh.core.trialdata import TrialDataManager
-from skyllh.i3.backgroundpdf import BackgroundI3SpatialPDF
+from matplotlib.axes import (
+    Axes,
+)
+from matplotlib.colors import (
+    LogNorm,
+)
+
+from skyllh.core.py import (
+    classname,
+)
+from skyllh.core.source_hypo_grouping import (
+    SourceHypoGroupManager,
+)
+from skyllh.core.storage import (
+    DataFieldRecordArray,
+)
+from skyllh.core.trialdata import (
+    TrialDataManager,
+)
+from skyllh.i3.backgroundpdf import (
+    BackgroundI3SpatialPDF,
+)
 
 
 class BackgroundI3SpatialPDFPlotter(object):
     """Plotter class to plot an BackgroundI3SpatialPDF object.
     """
     def __init__(self, tdm, pdf):
         """Creates a new plotter object for plotting an BackgroundI3SpatialPDF
@@ -33,30 +48,34 @@
         self.pdf = pdf
 
     @property
     def pdf(self):
         """The PDF object to plot.
         """
         return self._pdf
+
     @pdf.setter
     def pdf(self, pdf):
-        if(not isinstance(pdf, BackgroundI3SpatialPDF)):
-            raise TypeError('The pdf property must be an object of instance BackgroundI3SpatialPDF!')
+        if not isinstance(pdf, BackgroundI3SpatialPDF):
+            raise TypeError(
+                'The pdf property must be an object of instance '
+                'BackgroundI3SpatialPDF!')
         self._pdf = pdf
 
     @property
     def tdm(self):
         """The TrialDataManager that provides the data for the PDF evaluation.
         """
         return self._tdm
+
     @tdm.setter
     def tdm(self, obj):
-        if(not isinstance(obj, TrialDataManager)):
-            raise TypeError('The tdm property must be an instance of '
-                'TrialDataManager!')
+        if not isinstance(obj, TrialDataManager):
+            raise TypeError(
+                'The tdm property must be an instance of TrialDataManager!')
         self._tdm = obj
 
     def plot(self, src_hypo_group_manager, axes):
         """Plots the spatial PDF. It uses the sin(dec) binning of the PDF to
         propperly represent the resolution of the PDF in the drawing.
 
         Parameters
@@ -68,44 +87,51 @@
             The matplotlib Axes object on which the PDF should get drawn to.
 
         Returns
         -------
         img : instance of mpl.AxesImage
             The AxesImage instance showing the PDF image.
         """
-        if(not isinstance(src_hypo_group_manager, SourceHypoGroupManager)):
-            raise TypeError('The src_hypo_group_manager argument must be an '
+        if not isinstance(src_hypo_group_manager, SourceHypoGroupManager):
+            raise TypeError(
+                'The src_hypo_group_manager argument must be an '
                 'instance of SourceHypoGroupManager!')
-        if(not isinstance(axes, Axes)):
-            raise TypeError('The axes argument must be an instance of '
+        if not isinstance(axes, Axes):
+            raise TypeError(
+                'The axes argument must be an instance of '
                 'matplotlib.axes.Axes!')
 
         # By construction the BackgroundI3SpatialPDF does not depend on
         # right-ascention. Hence, we only need a single bin for the
         # right-ascention.
         sin_dec_binning = self.pdf.get_binning('sin_dec')
         pdfprobs = np.zeros((1, sin_dec_binning.nbins))
 
         sin_dec_points = sin_dec_binning.bincenters
-        events = DataFieldRecordArray(np.zeros((pdfprobs.size,),
-                          dtype=[('sin_dec', np.float64)]))
+        events = DataFieldRecordArray(np.zeros(
+            (pdfprobs.size,),
+            dtype=[('sin_dec', np.float64)]))
         for (i, sin_dec) in enumerate(sin_dec_points):
             events['sin_dec'][i] = sin_dec
 
         self._tdm.initialize_for_new_trial(src_hypo_group_manager, events)
 
         event_probs = self._pdf.get_prob(self._tdm)
 
         for i in range(len(events)):
-            pdfprobs[0,i] = event_probs[i]
+            pdfprobs[0, i] = event_probs[i]
 
-        ra_axis = self.pdf.axes.get_axis('ra')
+        ra_axis = self.pdf.axes['ra']
         (left, right, bottom, top) = (
             ra_axis.vmin, ra_axis.vmax,
             sin_dec_binning.lower_edge, sin_dec_binning.upper_edge)
-        img = axes.imshow(pdfprobs.T, extent=(left, right, bottom, top), origin='lower',
-                    norm=LogNorm(), interpolation='none')
+        img = axes.imshow(
+            pdfprobs.T,
+            extent=(left, right, bottom, top),
+            origin='lower',
+            norm=LogNorm(),
+            interpolation='none')
         axes.set_xlabel('ra')
         axes.set_ylabel('sin_dec')
         axes.set_title(classname(self.pdf))
 
         return img
```

### Comparing `skyllh-23.1.1/skyllh/plotting/i3/pdf.py` & `skyllh-23.2.0/skyllh/plotting/i3/pdf.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,23 +1,38 @@
 # -*- coding: utf-8 -*-
 
 """Plotting module to plot IceCube specific PDF objects.
 """
 
-import numpy as np
 import itertools
 
-from matplotlib.axes import Axes
-from matplotlib.colors import LogNorm
+import numpy as np
 
-from skyllh.core.py import classname
-from skyllh.core.source_hypothesis import SourceHypoGroupManager
-from skyllh.core.storage import DataFieldRecordArray
-from skyllh.core.trialdata import TrialDataManager
-from skyllh.i3.pdf import I3EnergyPDF
+from matplotlib.axes import (
+    Axes,
+)
+from matplotlib.colors import (
+    LogNorm,
+)
+
+from skyllh.core.py import (
+    classname,
+)
+from skyllh.core.source_hypo_grouping import (
+    SourceHypoGroupManager,
+)
+from skyllh.core.storage import (
+    DataFieldRecordArray,
+)
+from skyllh.core.trialdata import (
+    TrialDataManager,
+)
+from skyllh.i3.pdf import (
+    I3EnergyPDF,
+)
 
 
 class I3EnergyPDFPlotter(object):
     """Plotter class to plot an I3EnergyPDF object.
     """
     def __init__(self, tdm, pdf):
         """Creates a new plotter object for plotting an I3EnergyPDF object.
@@ -34,31 +49,33 @@
         self.pdf = pdf
 
     @property
     def pdf(self):
         """The PDF object to plot.
         """
         return self._pdf
+
     @pdf.setter
     def pdf(self, obj):
-        if(not isinstance(obj, I3EnergyPDF)):
-            raise TypeError('The pdf property must be an object of instance '
-                'I3EnergyPDF!')
+        if not isinstance(obj, I3EnergyPDF):
+            raise TypeError(
+                'The pdf property must be an object of instance I3EnergyPDF!')
         self._pdf = obj
 
     @property
     def tdm(self):
         """The TrialDataManager that provides the data for the PDF evaluation.
         """
         return self._tdm
+
     @tdm.setter
     def tdm(self, obj):
-        if(not isinstance(obj, TrialDataManager)):
-            raise TypeError('The tdm property must be an instance of '
-                'TrialDataManager!')
+        if not isinstance(obj, TrialDataManager):
+            raise TypeError(
+                'The tdm property must be an instance of TrialDataManager!')
         self._tdm = obj
 
     def plot(self, src_hypo_group_manager, axes, **kwargs):
         """Plots the PDF object.
 
         Parameters
         ----------
@@ -77,43 +94,51 @@
         function.
 
         Returns
         -------
         img : instance of mpl.AxesImage
             The AxesImage instance showing the PDF ratio image.
         """
-        if(not isinstance(src_hypo_group_manager, SourceHypoGroupManager)):
-            raise TypeError('The src_hypo_group_manager argument must be an '
-                'instance of SourceHypoGroupManager!')
-        if(not isinstance(axes, Axes)):
-            raise TypeError('The axes argument must be an instance of '
+        if not isinstance(src_hypo_group_manager, SourceHypoGroupManager):
+            raise TypeError(
+                'The src_hypo_group_manager argument must be an instance of '
+                'SourceHypoGroupManager!')
+        if not isinstance(axes, Axes):
+            raise TypeError(
+                'The axes argument must be an instance of '
                 'matplotlib.axes.Axes!')
 
         # The I3EnergyPDF object has two axes, one for log10_energy and sin_dec.
         (xbinning, ybinning) = self._pdf.binnings
 
         pdf_values = np.zeros((xbinning.nbins, ybinning.nbins), dtype=np.float64)
-        events = DataFieldRecordArray(np.zeros((pdf_values.size,),
+        events = DataFieldRecordArray(np.zeros(
+            (pdf_values.size,),
             dtype=[('ix', np.int64), (xbinning.name, np.float64),
                    ('iy', np.int64), (ybinning.name, np.float64)]))
-        for (i, ((ix,x),(iy,y))) in enumerate(itertools.product(
+        for (i, ((ix, x), (iy, y))) in enumerate(itertools.product(
                 enumerate(xbinning.bincenters),
                 enumerate(ybinning.bincenters))):
             events['ix'][i] = ix
             events[xbinning.name][i] = x
             events['iy'][i] = iy
             events[ybinning.name][i] = y
 
         self._tdm.initialize_for_new_trial(src_hypo_group_manager, events)
 
         event_pdf_values = self._pdf.get_prob(self._tdm)
-        pdf_values[events['ix'],events['iy']] = event_pdf_values
+        pdf_values[events['ix'], events['iy']] = event_pdf_values
 
         (left, right, bottom, top) = (xbinning.lower_edge, xbinning.upper_edge,
                                       ybinning.lower_edge, ybinning.upper_edge)
-        img = axes.imshow(pdf_values.T, extent=(left, right, bottom, top),
-            origin='lower', norm=LogNorm(), interpolation='none', **kwargs)
+        img = axes.imshow(
+            pdf_values.T,
+            extent=(left, right, bottom, top),
+            origin='lower',
+            norm=LogNorm(),
+            interpolation='none',
+            **kwargs)
         axes.set_xlabel(xbinning.name)
         axes.set_ylabel(ybinning.name)
         axes.set_title(classname(self._pdf))
 
         return img
```

### Comparing `skyllh-23.1.1/skyllh/plotting/i3/pdfratio.py` & `skyllh-23.2.0/skyllh/plotting/i3/pdfratio.py`

 * *Files 6% similar despite different names*

```diff
@@ -6,21 +6,23 @@
 import numpy as np
 import itertools
 
 from matplotlib.axes import Axes
 from matplotlib.colors import LogNorm
 
 from skyllh.core.py import classname
-from skyllh.core.source_hypothesis import SourceHypoGroupManager
+from skyllh.core.source_hypo_grouping import (
+    SourceHypoGroupManager,
+)
 from skyllh.core.storage import DataFieldRecordArray
 from skyllh.core.trialdata import TrialDataManager
-from skyllh.i3.pdfratio import I3EnergySigSetOverBkgPDFRatioSpline
+from skyllh.i3.pdfratio import SplinedI3EnergySigSetOverBkgPDFRatio
 
 
-class I3EnergySigSetOverBkgPDFRatioSplinePlotter(object):
+class SplinedI3EnergySigSetOverBkgPDFRatioPlotter(object):
     """Plotter class to plot an I3EnergySigSetOverBkgPDFRatioSpline object.
     """
     def __init__(self, tdm, pdfratio):
         """Creates a new plotter object for plotting an
         I3EnergySigSetOverBkgPDFRatioSpline object.
 
         Parameters
@@ -35,30 +37,34 @@
         self.pdfratio = pdfratio
 
     @property
     def pdfratio(self):
         """The PDF ratio object to plot.
         """
         return self._pdfratio
+
     @pdfratio.setter
     def pdfratio(self, pdfratio):
-        if(not isinstance(pdfratio, I3EnergySigSetOverBkgPDFRatioSpline)):
-            raise TypeError('The pdfratio property must be an object of instance I3EnergySigSetOverBkgPDFRatioSpline!')
+        if not isinstance(pdfratio, SplinedI3EnergySigSetOverBkgPDFRatio):
+            raise TypeError(
+                'The pdfratio property must be an instance of '
+                'SplinedI3EnergySigSetOverBkgPDFRatio!')
         self._pdfratio = pdfratio
 
     @property
     def tdm(self):
         """The TrialDataManager that provides the data for the PDF evaluation.
         """
         return self._tdm
+
     @tdm.setter
     def tdm(self, obj):
-        if(not isinstance(obj, TrialDataManager)):
-            raise TypeError('The tdm property must be an instance of '
-                'TrialDataManager!')
+        if not isinstance(obj, TrialDataManager):
+            raise TypeError(
+                'The tdm property must be an instance of TrialDataManager!')
         self._tdm = obj
 
     def plot(self, src_hypo_group_manager, axes, fitparams, **kwargs):
         """Plots the PDF ratio for the given set of fit paramater values.
 
         Parameters
         ----------
@@ -77,51 +83,59 @@
         function.
 
         Returns
         -------
         img : instance of mpl.AxesImage
             The AxesImage instance showing the PDF ratio image.
         """
-        if(not isinstance(src_hypo_group_manager, SourceHypoGroupManager)):
-            raise TypeError('The src_hypo_group_manager argument must be an '
+        if not isinstance(src_hypo_group_manager, SourceHypoGroupManager):
+            raise TypeError(
+                'The src_hypo_group_manager argument must be an '
                 'instance of SourceHypoGroupManager!')
-        if(not isinstance(axes, Axes)):
-            raise TypeError('The axes argument must be an instance of '
+        if not isinstance(axes, Axes):
+            raise TypeError(
+                'The axes argument must be an instance of '
                 'matplotlib.axes.Axes!')
-        if(not isinstance(fitparams, dict)):
-            raise TypeError('The fitparams argument must be an instance of '
-                'dict!')
+        if not isinstance(fitparams, dict):
+            raise TypeError(
+                'The fitparams argument must be an instance of dict!')
 
         # Get the binning for the axes. We use the background PDF to get it
         # from. By construction, all PDFs use the same binning. We know that
         # the PDFs are 2-dimensional.
         (xbinning, ybinning) = self._pdfratio.backgroundpdf.binnings
 
         # Create a 2D array with the ratio values. We put one event into each
         # bin.
         ratios = np.zeros((xbinning.nbins, ybinning.nbins), dtype=np.float64)
-        events = DataFieldRecordArray(np.zeros((ratios.size,),
+        events = DataFieldRecordArray(np.zeros(
+            (ratios.size,),
             dtype=[('ix', np.int64), (xbinning.name, np.float64),
                    ('iy', np.int64), (ybinning.name, np.float64)]))
-        for (i, ((ix,x),(iy,y))) in enumerate(itertools.product(
+        for (i, ((ix, x), (iy, y))) in enumerate(itertools.product(
                 enumerate(xbinning.bincenters),
                 enumerate(ybinning.bincenters))):
             events['ix'][i] = ix
             events[xbinning.name][i] = x
             events['iy'][i] = iy
             events[ybinning.name][i] = y
 
         self._tdm.initialize_for_new_trial(src_hypo_group_manager, events)
 
         event_ratios = self.pdfratio.get_ratio(self._tdm, fitparams)
         for i in range(len(events)):
-            ratios[events['ix'][i],events['iy'][i]] = event_ratios[i]
+            ratios[events['ix'][i], events['iy'][i]] = event_ratios[i]
 
         (left, right, bottom, top) = (xbinning.lower_edge, xbinning.upper_edge,
                                       ybinning.lower_edge, ybinning.upper_edge)
-        img = axes.imshow(ratios.T, extent=(left, right, bottom, top),
-            origin='lower', norm=LogNorm(), interpolation='none', **kwargs)
+        img = axes.imshow(
+            ratios.T,
+            extent=(left, right, bottom, top),
+            origin='lower',
+            norm=LogNorm(),
+            interpolation='none',
+            **kwargs)
         axes.set_xlabel(xbinning.name)
         axes.set_ylabel(ybinning.name)
         axes.set_title(classname(self._pdfratio))
 
         return img
```

### Comparing `skyllh-23.1.1/skyllh/plotting/utils/trials.py` & `skyllh-23.2.0/skyllh/plotting/utils/trials.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,34 +1,38 @@
 # -*- coding: utf-8 -*-
 # Author: Dr. Martin Wolf <mail@martin-wolf.org>
 
+import matplotlib as mpl
 import numpy as np
 
-import matplotlib as mpl
-from matplotlib import pyplot as plt
-from matplotlib import text as mpl_text
-from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable
+from matplotlib import (
+    pyplot as plt,
+)
+from mpl_toolkits.axes_grid1.axes_divider import (
+    make_axes_locatable,
+)
 
 
-def plot_ns_fit_vs_mean_ns_inj(
+def plot_ns_fit_vs_mean_ns_inj(  # noqa: C901
         trials,
         mean_n_sig_key='mean_n_sig',
         ns_fit_key='ns',
         rethist=False,
         title='',
         figsize=None,
         line_color=None,
         axis_fontsize=16,
         title_fontsize=16,
         tick_fontsize=16,
         xlabel=None,
         ylabel=None,
         ylim=None,
-        ratio_ylim=None):
-    """Creates a 2D histogram plot showing the fit number of signal events vs.
+        ratio_ylim=None,
+):
+    r"""Creates a 2D histogram plot showing the fit number of signal events vs.
     the mean number of injected signal events.
 
     Parameters
     ----------
     trials : numpy record array
         The record array holding the results of the trials.
     mean_n_sig_key : str
@@ -84,23 +88,23 @@
         This will only be returned, when the ``rethist`` argument was set to
         ``True``.
     yedges : 1d ndarray
         The histogram y-axis bin edges.
         This will only be returned, when the ``rethist`` argument was set to
         ``True``.
     """
-    if(figsize is None):
-        figsize = (12,10)
-    if(line_color is None):
+    if figsize is None:
+        figsize = (12, 10)
+    if line_color is None:
         line_color = '#E37222'
-    if(xlabel is None):
+    if xlabel is None:
         xlabel = r'<n>_{\mathrm{sig,inj}}'
-    if(ylabel is None):
+    if ylabel is None:
         ylabel = r'n_\mathrm{sig,fit}'
-    if(ratio_ylim is None):
+    if ratio_ylim is None:
         ratio_ylim = (-100, 100)
 
     # Create the x-axis binning.
     mean_n_sig_min = np.min(trials[mean_n_sig_key])
     mean_n_sig_max = np.max(trials[mean_n_sig_key])
     mean_n_sig_step = np.diff(np.sort(np.unique(trials[mean_n_sig_key])))[0]
     x_bins = np.arange(
@@ -119,24 +123,26 @@
     # Also calculate the median and upper and lower 68% quantile of ns_fit.
     hist_weights = np.ones_like(trials[mean_n_sig_key])
     (mean_n_sig, n_trials) = np.unique(
         trials[mean_n_sig_key], return_counts=True)
     ns_fit_median = np.empty_like(mean_n_sig, dtype=np.float64)
     ns_fit_uq = np.empty_like(mean_n_sig, dtype=np.float64)
     ns_fit_lq = np.empty_like(mean_n_sig, dtype=np.float64)
-    for (idx, (mean_n_sig_,n_trials_)) in enumerate(zip(mean_n_sig, n_trials)):
+    for (idx, (mean_n_sig_, n_trials_)) in enumerate(zip(mean_n_sig, n_trials)):
         m = trials[mean_n_sig_key] == mean_n_sig_
         hist_weights[m] /= n_trials_
         ns_fit_median[idx] = np.median(trials[m][ns_fit_key])
         ns_fit_uq[idx] = np.percentile(trials[m][ns_fit_key], 84.1)
         ns_fit_lq[idx] = np.percentile(trials[m][ns_fit_key], 15.9)
 
     # Create two Axes objects, one for the histogram and one for the ratio.
     (fig, ax) = plt.subplots(
-        2, 1, gridspec_kw={'height_ratios': [3,1]}, sharex=True,
+        2, 1,
+        gridspec_kw={'height_ratios': [3, 1]},
+        sharex=True,
         figsize=figsize)
 
     # Add an axes above the main axes for the colorbar.
     ax_divider = make_axes_locatable(ax[0])
     cax = ax_divider.append_axes("top", size="7%", pad="2%")
 
     # Create and plot the 2D histogram.
@@ -148,36 +154,45 @@
         cmap=plt.get_cmap('GnBu'))
 
     ax[0].set_ylabel('$'+ylabel+'$', fontsize=axis_fontsize)
 
     # Add the diagonal expectation line.
     ax[0].plot(
         x_bins, x_bins,
-        color='black', alpha=0.4, linestyle='-', linewidth=2)
+        color='black',
+        alpha=0.4,
+        linestyle='-',
+        linewidth=2)
 
     # Plot the lower quantile.
     ax[0].plot(
         mean_n_sig, ns_fit_lq,
-         color=line_color, linestyle='-.', linewidth=2)
+        color=line_color,
+        linestyle='-.',
+        linewidth=2)
 
     # Plot the median fitted ns.
     ax[0].plot(
         mean_n_sig, ns_fit_median,
-        color=line_color, linestyle='-', linewidth=2,
+        color=line_color,
+        linestyle='-',
+        linewidth=2,
         label=r'median')
 
     # Plot the upper quantile.
     ax[0].plot(
         mean_n_sig, ns_fit_uq,
-         color=line_color, linestyle='-.', linewidth=2,
-         label=r'$1\sigma$')
+        color=line_color,
+        linestyle='-.',
+        linewidth=2,
+        label=r'$1\sigma$')
 
     ax[0].legend()
 
-    if(ylim is not None):
+    if ylim is not None:
         ax[0].set_ylim(ylim)
 
     # Create the color bar.
     cb = fig.colorbar(image, cax=cax, orientation='horizontal')
     cb.ax.xaxis.set_ticks_position('top')
     cb.ax.text(
         0.5, 1, title, horizontalalignment='center', verticalalignment='bottom',
@@ -194,50 +209,52 @@
         (ns_fit_uq[m]-mean_n_sig[m])/mean_n_sig[m]*100,
         (ns_fit_lq[m]-mean_n_sig[m])/mean_n_sig[m]*100,
         alpha=0.2, color='gray', label=r'$1\sigma$')
 
     ax[1].legend()
 
     ax[1].set_xlabel('$'+xlabel+'$', fontsize=axis_fontsize)
-    ratio_ylabel = r'$\frac{%s - %s}{%s}$'%(ylabel, xlabel, xlabel)+' [%]'
+    ratio_ylabel = r'$\frac{%s - %s}{%s}$' % (ylabel, xlabel, xlabel)+' [%]'
     ax[1].set_ylabel(ratio_ylabel, fontsize=axis_fontsize)
     ax[1].set_xlim(x_bins[0], x_bins[-1])
     ax[1].set_ylim(ratio_ylim)
 
     # Set the font size of the tick labels.
     for tick in ax[0].yaxis.get_major_ticks():
         tick.label.set_fontsize(tick_fontsize)
     for tick in ax[1].xaxis.get_major_ticks():
         tick.label.set_fontsize(tick_fontsize)
     for tick in ax[1].yaxis.get_major_ticks():
         tick.label.set_fontsize(tick_fontsize)
 
     plt.tight_layout()
 
-    if(rethist):
+    if rethist:
         return (fig, hist, xedges, yedges)
 
     return fig
 
-def plot_gamma_fit_vs_mean_ns_inj(
+
+def plot_gamma_fit_vs_mean_ns_inj(  # noqa: C901
         trials,
         gamma_inj=2,
         mean_n_sig_key='mean_n_sig',
         gamma_fit_key='gamma',
         rethist=False,
         title='',
         figsize=None,
         line_color=None,
         axis_fontsize=16,
         title_fontsize=16,
         tick_fontsize=16,
         xlabel=None,
         ylabel=None,
-        ratio_ylim=None):
-    """Creates a 2D histogram plot showing the fit spectral index gamma vs.
+        ratio_ylim=None,
+):
+    r"""Creates a 2D histogram plot showing the fit spectral index gamma vs.
     the mean number of injected signal events.
 
     Parameters
     ----------
     trials : numpy record array
         The record array holding the results of the trials.
     gamma_inj : float
@@ -293,23 +310,23 @@
         This will only be returned, when the ``rethist`` argument was set to
         ``True``.
     yedges : 1d ndarray
         The histogram y-axis bin edges.
         This will only be returned, when the ``rethist`` argument was set to
         ``True``.
     """
-    if(figsize is None):
-        figsize = (12,10)
-    if(line_color is None):
+    if figsize is None:
+        figsize = (12, 10)
+    if line_color is None:
         line_color = '#E37222'
-    if(xlabel is None):
+    if xlabel is None:
         xlabel = r'<n>_{\mathrm{sig,inj}}'
-    if(ylabel is None):
+    if ylabel is None:
         ylabel = r'\gamma_\mathrm{fit}'
-    if(ratio_ylim is None):
+    if ratio_ylim is None:
         ratio_ylim = (-100, 100)
 
     # Create the x-axis binning.
     mean_n_sig_min = np.min(trials[mean_n_sig_key])
     mean_n_sig_max = np.max(trials[mean_n_sig_key])
     mean_n_sig_step = np.diff(np.sort(np.unique(trials[mean_n_sig_key])))[0]
     x_bins = np.arange(
@@ -328,24 +345,26 @@
     # Also calculate the median and upper and lower 68% quantile of gamma_fit.
     hist_weights = np.ones_like(trials[mean_n_sig_key])
     (mean_n_sig, n_trials) = np.unique(
         trials[mean_n_sig_key], return_counts=True)
     gamma_fit_median = np.empty_like(mean_n_sig, dtype=np.float64)
     gamma_fit_uq = np.empty_like(mean_n_sig, dtype=np.float64)
     gamma_fit_lq = np.empty_like(mean_n_sig, dtype=np.float64)
-    for (idx, (mean_n_sig_,n_trials_)) in enumerate(zip(mean_n_sig, n_trials)):
+    for (idx, (mean_n_sig_, n_trials_)) in enumerate(zip(mean_n_sig, n_trials)):
         m = trials[mean_n_sig_key] == mean_n_sig_
         hist_weights[m] /= n_trials_
         gamma_fit_median[idx] = np.median(trials[m][gamma_fit_key])
         gamma_fit_uq[idx] = np.percentile(trials[m][gamma_fit_key], 84.1)
         gamma_fit_lq[idx] = np.percentile(trials[m][gamma_fit_key], 15.9)
 
     # Create two Axes objects, one for the histogram and one for the ratio.
     (fig, ax) = plt.subplots(
-        2, 1, gridspec_kw={'height_ratios': [3,1]}, sharex=True,
+        2, 1,
+        gridspec_kw={'height_ratios': [3, 1]},
+        sharex=True,
         figsize=figsize)
 
     # Add an axes above the main axes for the colorbar.
     ax_divider = make_axes_locatable(ax[0])
     cax = ax_divider.append_axes('top', size='7%', pad='2%')
 
     # Create and plot the 2D histogram.
@@ -357,55 +376,69 @@
         cmap=plt.get_cmap('GnBu'))
 
     ax[0].set_ylabel('$'+ylabel+'$', fontsize=axis_fontsize)
 
     # Add the horizontal expectation line.
     ax[0].hlines(
         gamma_inj, x_bins[0], x_bins[-1],
-        color='black', alpha=0.4, linestyle='-', linewidth=2)
+        color='black',
+        alpha=0.4,
+        linestyle='-',
+        linewidth=2)
 
     # Plot the upper quantile curve.
     ax[0].plot(
         mean_n_sig, gamma_fit_uq,
-        color=line_color, linestyle='-.', linewidth=2)
+        color=line_color,
+        linestyle='-.',
+        linewidth=2)
 
     # Plot the median fitted gamma.
     ax[0].plot(
         mean_n_sig, gamma_fit_median,
-        color=line_color, linestyle='-', linewidth=2)
+        color=line_color,
+        linestyle='-',
+        linewidth=2)
 
     # Plot the lower quantile curve.
     ax[0].plot(
         mean_n_sig, gamma_fit_lq,
-        color=line_color, linestyle='-.', linewidth=2)
+        color=line_color,
+        linestyle='-.',
+        linewidth=2)
 
     # Create the color bar.
     cb = fig.colorbar(image, cax=cax, orientation='horizontal')
     cb.ax.xaxis.set_ticks_position('top')
     cb.ax.text(
-        0.5, 1, title, horizontalalignment='center', verticalalignment='bottom',
-        transform=fig.transFigure, fontsize=title_fontsize)
+        0.5, 1, title,
+        horizontalalignment='center',
+        verticalalignment='bottom',
+        transform=fig.transFigure,
+        fontsize=title_fontsize)
 
     # Plot the ratio.
     ax[1].hlines(0, x_bins[0], x_bins[-1])
     m = mean_n_sig != 0
     ax[1].plot(
         mean_n_sig[m], (gamma_fit_median[m]-gamma_inj)/gamma_inj*100,
         linewidth=2)
     ax[1].fill_between(
         mean_n_sig[m],
         (gamma_fit_uq[m]-gamma_inj)/gamma_inj*100,
         (gamma_fit_lq[m]-gamma_inj)/gamma_inj*100,
-        alpha=0.2, color='gray', label=r'$1\sigma$')
+        alpha=0.2,
+        color='gray',
+        label=r'$1\sigma$')
 
     ax[1].legend()
 
     ax[1].set_xlabel('$'+xlabel+'$', fontsize=axis_fontsize)
     gamma_inj_label = r'\gamma_{\mathrm{inj}}'
-    ratio_ylabel = r'$\frac{<%s> - %s}{%s}$'%(
+    ratio_ylabel = r'$\frac{<%s> - %s}{%s}$' % (
         ylabel, gamma_inj_label, gamma_inj_label)+' [%]'
     ax[1].set_ylabel(ratio_ylabel, fontsize=axis_fontsize)
     ax[1].set_xlim(x_bins[0], x_bins[-1])
     ax[1].set_ylim(ratio_ylim)
 
     # Set the font size of the tick labels.
     for tick in ax[0].yaxis.get_major_ticks():
@@ -413,11 +446,11 @@
     for tick in ax[1].xaxis.get_major_ticks():
         tick.label.set_fontsize(tick_fontsize)
     for tick in ax[1].yaxis.get_major_ticks():
         tick.label.set_fontsize(tick_fontsize)
 
     plt.tight_layout()
 
-    if(rethist):
+    if rethist:
         return (fig, hist, xedges, yedges)
 
     return fig
```

### Comparing `skyllh-23.1.1/skyllh.egg-info/PKG-INFO` & `skyllh-23.2.0/skyllh.egg-info/PKG-INFO`

 * *Files 14% similar despite different names*

```diff
@@ -1,24 +1,24 @@
 Metadata-Version: 2.1
 Name: skyllh
-Version: 23.1.1
+Version: 23.2.0
 Summary: The SkyLLH framework is an open-source Python3-based package licensed under the GPLv3 license. It provides a modular framework for implementing custom likelihood functions and executing log-likelihood ratio hypothesis tests. The idea is to provide a class structure tied to the mathematical objects of the likelihood functions.
 Home-page: https://github.com/icecube/skyllh
 Author: Martin Wolf
 Author-email: martin.wolf@icecube.wisc.edu
 License: GPL-3+
 Project-URL: Bug Tracker, https://github.com/icecube/skyllh/issues
 Project-URL: Documentation, https://icecube.github.io/skyllh
 Project-URL: Source Code, https://github.com/icecube/skyllh
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: Environment :: Console
 Classifier: Intended Audience :: Science/Research
 Classifier: License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)
 Classifier: Operating System :: POSIX
-Classifier: Programming Language :: Python :: 3.6
+Classifier: Programming Language :: Python :: 3.8
 Classifier: Topic :: Scientific/Engineering :: Physics
 Description-Content-Type: text/markdown
 License-File: LICENSE.txt
 
 # SkyLLH
 
 [![Tests](https://github.com/icecube/skyllh/actions/workflows/pythonpackage.yml/badge.svg)](#)
@@ -36,20 +36,20 @@
 The latest `skyllh` release can be installed from [PyPI](https://pypi.org/project/skyllh/) repository:
 ```bash
 pip install skyllh
 ```
 
 The current development version can be installed using pip:
 ```bash
-pip install git+https://github.com/icecube/skyllh.git#egg=skyllh 
+pip install git+https://github.com/icecube/skyllh.git#egg=skyllh
 ```
 
 Optionally, the editable package version with a specified reference can be installed by:
 ```bash
-pip install -e git+https://github.com/icecube/skyllh.git@[ref]#egg=skyllh 
+pip install -e git+https://github.com/icecube/skyllh.git@[ref]#egg=skyllh
 ```
 where
 - `-e` is an editable flag
 - `[ref]` is an optional argument containing a specific commit hash, branch name or tag
 
 ## Cloning from GitHub
 
@@ -58,10 +58,19 @@
 ```python
 import sys
 
 sys.path.insert(0, '/path/to/skyllh')
 sys.path.insert(0, '/path/to/i3skyllh')  # optional
 ```
 
+# Publications
+
+Several publications about the SkyLLH software are available:
+
+- IceCube Collaboration, T. Kontrimas, M. Wolf, et al. PoS ICRC2021 (2022) 1073
+  [DOI](http://doi.org/10.22323/1.395.1073)
+- IceCube Collaboration, M. Wolf, et al. PoS ICRC2019 (2020) 1035
+  [DOI](https://doi.org/10.22323/1.358.1035)
+
 # i3skyllh
 
 The [`i3skyllh`](https://github.com/icecube/i3skyllh) package provides complementary pre-defined common analyses and datasets for the [IceCube Neutrino Observatory](https://icecube.wisc.edu) detector in a private [repository](https://github.com/icecube/i3skyllh).
```

### Comparing `skyllh-23.1.1/skyllh.egg-info/SOURCES.txt` & `skyllh-23.2.0/skyllh.egg-info/SOURCES.txt`

 * *Files 18% similar despite different names*

```diff
@@ -29,90 +29,92 @@
 skyllh/cluster/__init__.py
 skyllh/cluster/commands.py
 skyllh/cluster/compute_node.py
 skyllh/cluster/master_node.py
 skyllh/cluster/srvclt.py
 skyllh/core/__init__.py
 skyllh/core/analysis.py
-skyllh/core/analysis_utils.py
 skyllh/core/background_generation.py
 skyllh/core/background_generator.py
 skyllh/core/backgroundpdf.py
 skyllh/core/binning.py
+skyllh/core/catalog.py
 skyllh/core/config.py
-skyllh/core/coords.py
+skyllh/core/datafields.py
 skyllh/core/dataset.py
 skyllh/core/debugging.py
 skyllh/core/detsigyield.py
 skyllh/core/display.py
+skyllh/core/event_selection.py
 skyllh/core/expectation_maximization.py
+skyllh/core/flux_model.py
 skyllh/core/interpolate.py
 skyllh/core/livetime.py
 skyllh/core/llhratio.py
 skyllh/core/math.py
 skyllh/core/minimizer.py
 skyllh/core/model.py
 skyllh/core/multiproc.py
-skyllh/core/optimize.py
 skyllh/core/parameters.py
 skyllh/core/pdf.py
 skyllh/core/pdfratio.py
+skyllh/core/pdfratio_fill.py
 skyllh/core/progressbar.py
 skyllh/core/py.py
 skyllh/core/random.py
 skyllh/core/scrambling.py
+skyllh/core/services.py
 skyllh/core/session.py
 skyllh/core/signal_generation.py
 skyllh/core/signal_generator.py
 skyllh/core/signalpdf.py
 skyllh/core/smoothing.py
-skyllh/core/source_hypo_group.py
-skyllh/core/source_hypothesis.py
+skyllh/core/source_hypo_grouping.py
+skyllh/core/source_model.py
 skyllh/core/storage.py
 skyllh/core/test_statistic.py
 skyllh/core/times.py
 skyllh/core/timing.py
+skyllh/core/tool.py
 skyllh/core/trialdata.py
+skyllh/core/types.py
 skyllh/core/minimizers/__init__.py
 skyllh/core/minimizers/iminuit.py
 skyllh/core/utils/__init__.py
+skyllh/core/utils/analysis.py
+skyllh/core/utils/coords.py
+skyllh/core/utils/flux_model.py
 skyllh/core/utils/multidimgridpdf.py
-skyllh/core/utils/ndphotosplinepdf.py
+skyllh/core/utils/spline.py
 skyllh/core/utils/trials.py
 skyllh/datasets/__init__.py
 skyllh/datasets/i3/PublicData_10y_ps.py
 skyllh/datasets/i3/PublicData_10y_ps_wMC.py
-skyllh/datasets/i3/PublicData_10y_ps_wMCEq.py
 skyllh/datasets/i3/__init__.py
 skyllh/i3/__init__.py
 skyllh/i3/background_generation.py
 skyllh/i3/backgroundpdf.py
 skyllh/i3/config.py
-skyllh/i3/coords.py
 skyllh/i3/dataset.py
 skyllh/i3/detsigyield.py
 skyllh/i3/livetime.py
 skyllh/i3/pdf.py
 skyllh/i3/pdfratio.py
 skyllh/i3/scrambling.py
 skyllh/i3/signal_generation.py
 skyllh/i3/signalpdf.py
 skyllh/i3/utils/__init__.py
-skyllh/i3/utils/sensitivity.py
-skyllh/physics/__init__.py
-skyllh/physics/flux.py
-skyllh/physics/flux_model.py
-skyllh/physics/model.py
-skyllh/physics/source.py
-skyllh/physics/time_profile.py
+skyllh/i3/utils/analysis.py
+skyllh/i3/utils/coords.py
 skyllh/plotting/__init__.py
 skyllh/plotting/core/__init__.py
 skyllh/plotting/core/pdfratio.py
 skyllh/plotting/core/signalpdf.py
 skyllh/plotting/i3/__init__.py
 skyllh/plotting/i3/backgroundpdf.py
 skyllh/plotting/i3/pdf.py
 skyllh/plotting/i3/pdfratio.py
 skyllh/plotting/utils/__init__.py
 skyllh/plotting/utils/trials.py
-skyllh/utils/__init__.py
-skyllh/utils/spline.py
+skyllh/scripting/__init__.py
+skyllh/scripting/argparser.py
+skyllh/scripting/logging.py
```

### Comparing `skyllh-23.1.1/versioneer.py` & `skyllh-23.2.0/versioneer.py`

 * *Files identical despite different names*

