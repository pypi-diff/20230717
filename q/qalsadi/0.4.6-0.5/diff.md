# Comparing `tmp/qalsadi-0.4.6-py3-none-any.whl.zip` & `tmp/qalsadi-0.5-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,37 +1,41 @@
-Zip file size: 256592 bytes, number of entries: 35
+Zip file size: 264267 bytes, number of entries: 39
 -rw-rw-r--  2.0 unx       21 b- defN 18-Apr-27 22:02 qalsadi/__init__.py
--rw-rw-r--  2.0 unx    30065 b- defN 20-Aug-31 15:53 qalsadi/analex.py
+-rw-r--r--  2.0 unx    30401 b- defN 23-Jul-14 08:09 qalsadi/analex.py
 -rw-rw-r--  2.0 unx      908 b- defN 20-Apr-02 18:06 qalsadi/analex_const.py
--rw-rw-r--  2.0 unx     4630 b- defN 20-Aug-12 20:37 qalsadi/cache.py
--rw-rw-r--  2.0 unx     4783 b- defN 20-Aug-13 00:57 qalsadi/cache_codernity.py
--rw-rw-r--  2.0 unx     3121 b- defN 20-Aug-13 16:14 qalsadi/cache_pickledb.py
+-rw-r--r--  2.0 unx     2163 b- defN 23-Jul-09 14:07 qalsadi/cache.py
+-rw-r--r--  2.0 unx     4454 b- defN 23-Jul-14 08:09 qalsadi/cache_codernity.py
+-rw-r--r--  2.0 unx     1692 b- defN 23-Jul-14 08:09 qalsadi/cache_factory.py
+-rw-r--r--  2.0 unx     2946 b- defN 23-Jul-09 14:29 qalsadi/cache_pickle.py
+-rw-r--r--  2.0 unx     3225 b- defN 23-Jul-09 13:18 qalsadi/cache_pickledb.py
 -rw-rw-r--  2.0 unx      822 b- defN 20-Aug-13 00:05 qalsadi/coder.py
 -rw-rw-r--  2.0 unx     2912 b- defN 20-Jun-15 21:25 qalsadi/custom_dictionary.py
 -rw-rw-r--  2.0 unx    18038 b- defN 20-Apr-02 18:09 qalsadi/dictword.py
 -rw-rw-r--  2.0 unx     6930 b- defN 20-Jan-24 10:13 qalsadi/disambig.py
 -rw-rw-r--  2.0 unx     3612 b- defN 17-Dec-10 13:25 qalsadi/disambig_const.py
 -rw-rw-r--  2.0 unx     4466 b- defN 19-Feb-03 23:43 qalsadi/genAffixes.py
 -rw-rw-r--  2.0 unx     4484 b- defN 19-Feb-05 19:29 qalsadi/genalex.py
--rw-r--r--  2.0 unx     5620 b- defN 21-Jul-02 19:06 qalsadi/lemmatizer.py
+-rw-r--r--  2.0 unx     5661 b- defN 23-Jul-12 17:15 qalsadi/lemmatizer.py
 -rw-rw-r--  2.0 unx    21906 b- defN 19-Feb-03 22:47 qalsadi/noun_affixer.py
--rw-rw-r--  2.0 unx     3738 b- defN 19-Nov-09 20:26 qalsadi/print_debug.py
--rw-rw-r--  2.0 unx    38275 b- defN 20-Aug-12 01:54 qalsadi/stem_noun.py
+-rw-r--r--  2.0 unx     3631 b- defN 23-Jul-12 06:30 qalsadi/print_debug.py
+-rw-r--r--  2.0 unx    37961 b- defN 23-Jul-09 11:25 qalsadi/stem_noun.py
 -rw-rw-r--  2.0 unx    36411 b- defN 20-Jan-26 08:31 qalsadi/stem_noun_const.py
 -rw-rw-r--  2.0 unx     1814 b- defN 17-Dec-10 14:12 qalsadi/stem_pounct_const.py
--rw-rw-r--  2.0 unx    25693 b- defN 19-Nov-09 21:03 qalsadi/stem_stop.py
--rw-rw-r--  2.0 unx    26991 b- defN 20-Jan-26 11:55 qalsadi/stem_stopwords_const.py
--rw-rw-r--  2.0 unx    19287 b- defN 19-Nov-09 21:02 qalsadi/stem_unknown.py
+-rw-r--r--  2.0 unx    26198 b- defN 23-Jul-08 12:20 qalsadi/stem_stop.py
+-rw-r--r--  2.0 unx    27054 b- defN 23-Jan-20 14:23 qalsadi/stem_stopwords_const.py
+-rw-r--r--  2.0 unx    20183 b- defN 23-Jul-09 11:06 qalsadi/stem_unknown.py
 -rw-rw-r--  2.0 unx    30128 b- defN 20-Jan-26 08:36 qalsadi/stem_unknown_const.py
--rw-r--r--  2.0 unx    31754 b- defN 21-Dec-27 21:36 qalsadi/stem_verb.py
+-rw-r--r--  2.0 unx    33106 b- defN 23-Jul-09 10:44 qalsadi/stem_verb.py
 -rw-rw-r--  2.0 unx    45768 b- defN 20-Apr-02 18:25 qalsadi/stem_verb_const.py
 -rw-rw-r--  2.0 unx    22962 b- defN 20-Apr-03 16:39 qalsadi/stemmedaffix.py
 -rw-r--r--  2.0 unx    34467 b- defN 22-Dec-29 09:56 qalsadi/stemmedword.py
--rw-r--r--  2.0 unx    21605 b- defN 22-Dec-29 10:21 qalsadi/stemnode.py
+-rw-r--r--  2.0 unx    22078 b- defN 23-Jul-14 08:09 qalsadi/stemnode.py
+-rw-r--r--  2.0 unx    12784 b- defN 23-Jan-20 14:43 qalsadi/stop_affixer.py
 -rw-rw-r--  2.0 unx  4235391 b- defN 17-Dec-10 15:20 qalsadi/stopwords.py
--rw-rw-r--  2.0 unx     9281 b- defN 19-Nov-09 12:53 qalsadi/wordcase.py
+-rw-r--r--  2.0 unx     3100 b- defN 23-Jan-09 22:32 qalsadi/templater.py
+-rw-r--r--  2.0 unx     9590 b- defN 23-Jul-12 22:58 qalsadi/wordcase.py
 -rw-rw-r--  2.0 unx    20480 b- defN 19-Feb-18 17:10 qalsadi/data/custom_dictionary.sqlite
--rw-r--r--  2.0 unx     9413 b- defN 23-Jan-09 21:48 qalsadi-0.4.6.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jan-09 21:48 qalsadi-0.4.6.dist-info/WHEEL
--rw-rw-r--  2.0 unx        8 b- defN 23-Jan-09 21:48 qalsadi-0.4.6.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     2779 b- defN 23-Jan-09 21:48 qalsadi-0.4.6.dist-info/RECORD
-35 files, 4728655 bytes uncompressed, 252252 bytes compressed:  94.7%
+-rw-r--r--  2.0 unx    12904 b- defN 23-Jul-17 06:39 qalsadi-0.5.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jul-17 06:39 qalsadi-0.5.dist-info/WHEEL
+-rw-rw-r--  2.0 unx        8 b- defN 23-Jul-17 06:39 qalsadi-0.5.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     3091 b- defN 23-Jul-17 06:39 qalsadi-0.5.dist-info/RECORD
+39 files, 4753842 bytes uncompressed, 259459 bytes compressed:  94.6%
```

## zipnote {}

```diff
@@ -9,14 +9,20 @@
 
 Filename: qalsadi/cache.py
 Comment: 
 
 Filename: qalsadi/cache_codernity.py
 Comment: 
 
+Filename: qalsadi/cache_factory.py
+Comment: 
+
+Filename: qalsadi/cache_pickle.py
+Comment: 
+
 Filename: qalsadi/cache_pickledb.py
 Comment: 
 
 Filename: qalsadi/coder.py
 Comment: 
 
 Filename: qalsadi/custom_dictionary.py
@@ -78,29 +84,35 @@
 
 Filename: qalsadi/stemmedword.py
 Comment: 
 
 Filename: qalsadi/stemnode.py
 Comment: 
 
+Filename: qalsadi/stop_affixer.py
+Comment: 
+
 Filename: qalsadi/stopwords.py
 Comment: 
 
+Filename: qalsadi/templater.py
+Comment: 
+
 Filename: qalsadi/wordcase.py
 Comment: 
 
 Filename: qalsadi/data/custom_dictionary.sqlite
 Comment: 
 
-Filename: qalsadi-0.4.6.dist-info/METADATA
+Filename: qalsadi-0.5.dist-info/METADATA
 Comment: 
 
-Filename: qalsadi-0.4.6.dist-info/WHEEL
+Filename: qalsadi-0.5.dist-info/WHEEL
 Comment: 
 
-Filename: qalsadi-0.4.6.dist-info/top_level.txt
+Filename: qalsadi-0.5.dist-info/top_level.txt
 Comment: 
 
-Filename: qalsadi-0.4.6.dist-info/RECORD
+Filename: qalsadi-0.5.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## qalsadi/analex.py

```diff
@@ -12,27 +12,27 @@
 # Licence:     GPL
 #-----------------------------------------------------------------------
 """
     Arabic text morphological analyzer.
     Provides routins  to alanyze text.
     Can treat text as verbs or as nouns.
 """
-from __future__ import (
-    absolute_import,
-    print_function,
-    unicode_literals,
-    #~ division,
-    )
+# ~ from __future__ import (
+    # ~ absolute_import,
+    # ~ print_function,
+    # ~ unicode_literals,
+    # ~ #~ division,
+    # ~ )
 if __name__ == "__main__":
     import sys
     sys.path.append('..')
-try:
-    unicode
-except NameError:
-    from six import text_type as unicode
+# try:
+#     unicode
+# except NameError:
+#     from six import text_type as unicode
 import re
 import pyarabic.araby as araby 
 from pyarabic.arabrepr import arepr
 #~ import qalsadi.analex_const as analex_const  # special constant for analex
 #~ import qalsadi.stem_noun as stem_noun  # noun stemming
 #~ import qalsadi.stem_verb as stem_verb  # verb stemming
 #~ import qalsadi.stem_unknown as stem_unknown  # unknown word stemming
@@ -364,27 +364,41 @@
             return "stop"
         for c in word:
             if c in u"إة":
                 return "nonverb"
             if c in araby.TANWIN:
                 return "nonverb"
         return ""
+    def light_tag2(self, word):
+        """
+        tag words as verbs or nouns according to some features
+        Some letters are forbiden in some types like TehMarbuta in verbs
+        """
+        if stopwords.is_stop(word):
+            return "stop"
+        for c in word:
+            if c in u"إة":
+                return "nonverb"
+            if c in araby.TANWIN:
+                return "nonverb"
+        return ""
         
     def check_word(self, word, guessedtag=""):
         """
         Analyze one word morphologically as verbs
         @param word: the input word.
         @type word: unicode.
         @return: list of dictionaries of analyzed words with tags.
         @rtype: list.
         """
 
         word = araby.strip_tatweel(word)
         word_vocalised = word
         word_nm = araby.strip_tashkeel(word)
+        word_nm_shadda = araby.strip_harakat(word)
         # get analysed details from cache if used
         if self.allow_cache_use and self.cache.is_already_checked(word_nm):
             #~ print (u"'%s'"%word).encode('utf8'), 'found'
             resulted_data = self.cache.get_checked(word_nm)
         else:
             resulted_data = []
             # if word is a pounctuation
@@ -392,38 +406,47 @@
             # Done: if the word is a stop word we have  some problems,
             # the stop word can also be another normal word (verb or noun),
             # we must consider it in future works
             # if word is stopword allow stop words analysis
             if araby.is_arabicword(word_nm):
                 if self.light_tag(word) == "stop":                
                     resulted_data += self.check_word_as_stopword(word_nm)
+                if self.allow_tag_guessing:
+                    # 2nd method
+                    #if word is a possible verb or just a stop word
+                    # مشكلة بعض الكلمات المستبعدة تعتبر أفعلا أو اسماء
+                    if  self.tagger.has_verb_tag(guessedtag) or \
+                    self.tagger.is_stopword_tag(guessedtag):
+                        resulted_data += self.check_word_as_verb(word_nm)
+                    #if word is noun
+                    if self.tagger.has_noun_tag(guessedtag) or \
+                    self.tagger.is_stopword_tag(guessedtag):
+                        resulted_data += self.check_word_as_noun(word_nm)
+                else:
+                    #if word is verb
+                    if self.light_tag(word) != "nonverb":
+                        resulted_data += self.check_word_as_verb(word_nm)
+                    #if word is noun
+                    if self.light_tag(word) != "nonnoun":                
+                        resulted_data += self.check_word_as_noun(word_nm)
 
-                #if word is verb
-                # مشكلة بعض الكلمات المستبعدة تعتبر أفعلا أو اسماء
-                #~if  self.tagger.has_verb_tag(guessedtag) or \
-                #~self.tagger.is_stopword_tag(guessedtag):
-                #~resulted_data += self.check_word_as_verb(word_nm)
-                if self.light_tag(word) != "nonverb":
-                    resulted_data += self.check_word_as_verb(word_nm)
-                #print "is verb", rabti, len(resulted_data)
-                #if word is noun
-                #~if self.tagger.has_noun_tag(guessedtag) or \
-                #~self.tagger.is_stopword_tag(guessedtag):
-                #~resulted_data += self.check_word_as_noun(word_nm)
-                if self.light_tag(word) != "nonnoun":                
-                    resulted_data += self.check_word_as_noun(word_nm)
             if len(resulted_data) == 0:
                 #print (u"1 _unknown %s-%s"%(word, word_nm)).encode('utf8')
                 #check the word as unkonwn
                 resulted_data += self.check_word_as_unknown(word_nm)
-                #check if the word is nomralized and solution are equivalent
-            resulted_data = self.check_normalized(word_vocalised, resulted_data)
+            
+            # ------- Filters used to reduce cases 
+            # 1- with normalized letters
+            # 2- with given Shadda
+            # 3- vocalized like
+            #check if the word is nomralized and solution are equivalent
+            resulted_data = self.check_normalized(word_nm, resulted_data)
             #check if the word is shadda like
             
-            resulted_data = self.check_shadda(word_vocalised, resulted_data, self.fully_vocalized_input)
+            resulted_data = self.check_shadda(word_nm_shadda, resulted_data, self.fully_vocalized_input)
 
             # add word frequency information in tags
             resulted_data = self.add_word_frequency(resulted_data)
 
             # add the stemmed words details into Cache
             data_list_to_serialize = [w.__dict__ for w in resulted_data]
             if self.allow_cache_use:
@@ -639,79 +662,68 @@
         @type noun: unicode.
         @return: list of dictionaries of analyzed words with tags.
         @rtype: list.
         """
         return self.unknownstemmer.stemming_noun(noun)
 
     @staticmethod
-    def check_shadda(word_vocalised, resulted_data, fully_vocalized_input=False):
+    def check_shadda(word_nm_shadda, resulted_data, fully_vocalized_input=False):
         """
         if the entred word is like the found word in dictionary,
         to treat some normalized cases,
         the analyzer return the vocalized like words.
         This function treat the Shadda case.
-        @param word_vocalised: the input word.
-        @type word_vocalised: unicode.
+        @param word_nm_shadda: a word without harakat, but shadda
+        @type word_nm_shadda: unicode
         @param resulted_data: the founded resulat from dictionary.
         @type resulted_data: list of dict.
         @param fully_vocalized_input: if the two words must resect the shadda and vocalized.
         @type fully_vocalized_input: Boolean, default is False.
         @return: list of dictionaries of analyzed words with tags.
         @rtype: list.
         """
-        #~return filter(lambda item: araby.shaddalike(word_vocalised,
-        #~item.__dict__.get('vocalized', '')), resulted_data)
-        #~x for x in [1, 1, 2] if x == 1
-        #~ return [
-        #~ x for x in resulted_data
-        #~ if araby.shaddalike(word_vocalised, x.__dict__.get('vocalized', '')) ]
         if fully_vocalized_input:
-            return [x for x in resulted_data if araby.strip_harakat(word_vocalised) == 
-        #~ araby.strip_harakat(x.__dict__.get('vocalized', ''))]
-        araby.strip_harakat(x.vocalized)]
+            # strip harakat keep shadda
+            # input word can be vocalized
+            # The output word vocalized
+            return [x for x in resulted_data 
+                    if araby.strip_harakat(word_nm_shadda) == araby.strip_harakat(x.vocalized)]
         else:
-            return [
-            x for x in resulted_data
-            #~ if araby.shaddalike(word_vocalised, x.__dict__.get('vocalized', ''))
-            if araby.shaddalike(word_vocalised, x.vocalized)
-        ]
+            return [x for x in resulted_data
+            if araby.shaddalike(word_nm_shadda, x.vocalized)]
         
 
     #~ @staticmethod
-    def check_normalized(self, word_vocalised, resulted_data):
+    def check_normalized(self, word_nm, resulted_data):
         """
         If the entred word is like the found word in dictionary,
         to treat some normalized cases,
         the analyzer return the vocalized like words
         ُIf the word is ذئب, the normalized form is ذءب,
         which can give from dictionary ذئبـ ذؤب.
         this function filter normalized resulted word according
         the given word, and give ذئب.
-        @param word_vocalised: the input word.
-        @type word_vocalised: unicode.
+        @param word_nm the input word.
+        @type word_nm: unicode.
+        @param word_unvocalised: the input word unvocalized.
+        @type word_unvocalised: unicode.
         @param resulted_data: the founded resulat from dictionary.
         @type resulted_data: list of dict.
         @return: list of dictionaries of analyzed words with tags.
         @rtype: list.
         """
+        return [d for d in resulted_data if d.unvocalized == word_nm]
         #print word_vocalised.encode('utf8')
-        filtred_data = []
-        inputword = araby.strip_tashkeel(word_vocalised)
-        for item in resulted_data:
-            #~ if 'vocalized' in item.__dict__: 
-            if hasattr(item, 'vocalized'): 
-                #~ if 'vocalized' in item :
-                outputword = araby.strip_tashkeel(item.vocalized)
-                if self.debug:
-                    print(u'\t'.join([inputword, outputword]).encode('utf8'))
-                if inputword == outputword:
-                    #item['tags'] += ':a'
-                    filtred_data.append(item)
-                #~ filtred_data.append(item)
-        return filtred_data
+        # ~ filtred_data = []
+        # ~ for item in resulted_data:
+            # ~ outputword = item.unvocalized
+            # ~ inputword = item.word_nm
+            # ~ if inputword == outputword:
+                # ~ filtred_data.append(item)
+        # ~ return filtred_data
 
     @staticmethod
     def check_partial_vocalized(word_vocalised, resulted_data):
         """
         if the entred word is vocalized fully or partially,
         the analyzer return the vocalized like words
         This function treat the partial vocalized case.
```

## qalsadi/cache.py

```diff
@@ -8,126 +8,57 @@
 # Author:      Taha Zerrouki (taha.zerrouki[at]gmail.com)
 #
 # Created:     31-10-2011
 # Copyright:   (c) Taha Zerrouki 2011
 # Licence:     GPL
 #-------------------------------------------------------------------------------
 """Cache Module for analex"""
-#~ import sys
-
-    
-#~ from hashlib import md5
-#~ import os
-#~ try:
-    #~ unicode
-#~ except NameError:
-    #~ from six import text_type as unicode
-#~ if __name__ == "__main__":
-    #~ sys.path.append('..')
-#~ if sys.version_info[0] < 3:
-    #~ from CodernityDB.database import Database
-    #~ from CodernityDB.hash_index import HashIndex
-#~ else:
-    #~ from codernitydb3.database import Database
-    #~ from codernitydb3.hash_index import HashIndex
-    
-#~ class WithAIndex(HashIndex):
-    #~ """ hache with Index Class """
-    #~ def __init__(self, *args, **kwargs):
-        #~ """init"""
-        #~ kwargs['key_format'] = '32s'
-        #~ super(WithAIndex, self).__init__(*args, **kwargs)
-
-    #~ def make_key_value(self, data):
-        #~ """make a key value from ``data`` """
-        #~ a_val = data.get("a")
-        #~ if a_val:
-            #~ if not isinstance(a_val, unicode):
-                #~ a_val = unicode(a_val)
-            #~ return md5(a_val.encode('utf8')).hexdigest(), {}
-        #~ return None
-
-    #~ def make_key(self, key):
-        #~ """make a ``key`` """
-        #~ if not isinstance(key, unicode):
-            #~ key = unicode(key)
-        #~ return md5(key.encode('utf8')).hexdigest()
-
 
 class Cache(object):
     """
         cache for word morphological analysis
     """
-    #~ DB_PATH = os.path.join(os.path.expanduser('~'), '.qalsadiCache')
-
     def __init__(self, dp_path = False):
         """
         Create Analex Cache
         """
         self.cache = {
             'checkedWords': {},
             'FreqWords': {
                 'noun': {},
                 'verb': {},
                 'stopword': {}
             },
         }
         self.db = False
-        #~ if not dp_path:
-            #~ dp_path = self.DB_PATH
-        #~ else:
-            #~ dp_path = os.path.join(os.path.dirname(dp_path), '.qalsadiCache')
-        #~ # sys.stderr.write("Tahahahah\n"+" "+ dp_path +" "+str(type(dp_path)))
-        #~ # self.db = Database("/tmp/QC")
-        #~ try:
-            #~ self.db = Database(str(dp_path))
-            #~ if not self.db.exists():
-                #~ self.db.create()
-                #~ x_ind = WithAIndex(self.db.path, 'a')
-                #~ self.db.add_index(x_ind)
-            #~ else:
-                #~ self.db.open()
-        #~ except:
-            #~ self.db = None
+
     def __del__(self):
         """
         Delete instance and clear cache
 
         """
         self.cache = None
-        #~ if self.db:
-            #~ self.db.close()
+
 
     def is_already_checked(self, word):
         """ return if ``word`` is already cached"""
         return  word in self.cache["checkedWords"]
-        #~ try:
-            #~ return bool(self.db.get('a', word))
-        #~ except:
-            #~ return False
-        #~ except: return False;
+
 
     def get_checked(self, word):
         """ return checked ``word`` form cache"""
 
         return self.cache["checkedWords"].get(word, {})
 
-        #~ xxx = self.db.get('a', word, with_doc=True)
-        #~ yyy = xxx.get('doc', False)
-        #~ if yyy:
-            #~ return yyy.get('d', [])
-        #~ else: return []
 
     def add_checked(self, word, data):
         """ add checked ``word`` form cache"""
         if not word in self.cache["checkedWords"]:
             self.cache["checkedWords"][word] = data
-        #~ idata = {"a": word, 'd': data}
-        #~ if self.db:
-            #~ self.db.insert(idata)
+
 
     def exists_cache_freq(self, word, wordtype):
         """ return if word exists in freq cache"""
         return word in self.cache['FreqWords']
 
     def get_freq(self, originalword, wordtype):
         """ return  ``word`` frequency form cache"""
```

## qalsadi/cache_codernity.py

```diff
@@ -16,28 +16,15 @@
 
     
 from hashlib import md5
 import os
 from codernitydb3.database import Database
 from codernitydb3.hash_index import HashIndex
     
-#~ class WithAIndex(HashIndex):
 
-    #~ def __init__(self, *args, **kwargs):
-        #~ kwargs['key_format'] = '32s'
-        #~ super(WithAIndex, self).__init__(*args, **kwargs)
-
-    #~ def make_key_value(self, data):
-        #~ a_val = data.get("a")
-        #~ if a_val is not None:
-            #~ return a_val, None
-        #~ return None
-
-    #~ def make_key(self, key):
-        #~ return key
 class WithAIndex(HashIndex):
     def __init__(self, *args, **kwargs):
         # kwargs['entry_line_format'] = '<32s32sIIcI'
         #~ kwargs['key_format'] = '16s'
         kwargs['key_format'] = '32s'
         kwargs['hash_lim'] = 8 * 1024
         super(WithAIndex, self).__init__(*args, **kwargs)
@@ -63,25 +50,25 @@
     """
         cache for word morphological analysis
     """
     def __init__(self, dp_path = False):
         """
         Create Analex Cache
         """
-        DB_PATH = os.path.join(os.path.expanduser('~'), '.qalsadiCache')
+        DB_PATH_DEFAULT = os.path.join(os.path.expanduser('~'), '.qalsadiCache')
         self.cache = {
             'checkedWords': {},
             'FreqWords': {
                 'noun': {},
                 'verb': {},
                 'stopword': {}
             },
         }
         if not dp_path:
-            dp_path = self.DB_PATH
+            dp_path = DB_PATH_DEFAULT
         else:
             dp_path = os.path.join(os.path.dirname(dp_path), '.qalsadiCache')
         #~ sys.stderr.write("Tahahahah\n"+" "+ dp_path +" "+str(type(dp_path)))
         #~ self.db = Database("/tmp/QC")
         try:
         #~ if True:
             self.db = Database(str(dp_path))
@@ -89,24 +76,25 @@
                 self.db.create()
                 x_ind = WithAIndex(self.db.path, "a")
                 self.db.add_index(x_ind)
             else:
                 self.db.open()
         except:
         #~ else:
-            print("Can't Open data base")
+            print(__file__," Error: Can't Open data base ", dp_path)
             self.db = None
     def __del__(self):
         """
         Delete instance and clear cache
 
         """
         self.cache = None
-        if self.db:
-            self.db.close()
+        if hasattr(self, "db"):
+            if self.db:
+                self.db.close()
 
     def is_already_checked(self, word):
         """ return if ``word`` is already cached"""
         try:
             return bool(self.db.get('a', word))
         except:
             return False
```

## qalsadi/cache_pickledb.py

```diff
@@ -36,27 +36,29 @@
                 'stopword': {}
             },
         }
         if not dp_path:
             dp_path = DB_PATH
         else:
             dp_path = os.path.join(os.path.dirname(dp_path), '.qalsadiCache.pickledb')
-        #~ self.db =  pickledb.load(dp_path, False)
         try:
-            self.db =  pickledb.load(dp_path, False)
+            self.db =  pickledb.load(open(dp_path,"wb"), False)
         except:
-            print("Can't Open data base")
+            print(__file__," Error: Can't Open data base ", dp_path)
+            
             self.db = None
+        else:
+            print(__file__," Success: Open data base ", dp_path)
     def __del__(self):
         """
         Delete instance and clear cache
 
         """
         self.cache = None
-        if self.db:
+        if hasattr(self, "db") and self.db:
             self.db.dump()
 
     def is_already_checked(self, word):
         """ return if ``word`` is already cached"""
         try:
             return bool(self.db.get(word))
         except:
```

## qalsadi/lemmatizer.py

```diff
@@ -160,24 +160,27 @@
         if lemmas:
             return lemmas[0]
         else:
             if return_pos:
                 return ()
             else:
                 return ""
+
+       
             
 
 def mainly():
     """
     main test
     """
     # #test syn
     text = u"إلى البيت"
     result = []
     lemmer = Lemmatizer()
     result = lemmer.analyze_text(text)
+    result = lemmer.morph(text)
     # the result contains objects
     pprint.pprint(result)
 
 if __name__ == "__main__":
     mainly()
```

## qalsadi/print_debug.py

```diff
@@ -17,23 +17,17 @@
 #  
 #  You should have received a copy of the GNU General Public License
 #  along with this program; if not, write to the Free Software
 #  Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston,
 #  MA 02110-1301, USA.
 #  
 # 
-from __future__ import (
-    absolute_import,
-    print_function,
-    unicode_literals,
-    #~ division,
-    )
 
 from builtins import str
-from future.utils import python_2_unicode_compatible
+# ~ from future.utils import python_2_unicode_compatible
 import sys
 
 
 from pyarabic.arabrepr import arepr
 def print_md_table(myDict, colList=None):
    """ Pretty print a list of dictionaries (myDict) as a dynamically sized table.
    If column names (colList) aren't specified, they will show in random order.
```

## qalsadi/stem_noun.py

```diff
@@ -10,29 +10,18 @@
 # Created:     31-10-2011
 # Copyright:   (c) Taha Zerrouki 2011
 # Licence:     GPL
 #-------------------------------------------------------------------------
 """
     Arabic noun stemmer
 """
-#~ from __future__ import (
-    #~ absolute_import,
-    #~ print_function,
-    #~ unicode_literals,
-    #~ )
 import re
-if __name__ == '__main__':
-    import sys
-    sys.path.append('../support')
-    sys.path.append('support')
-    sys.path.append('..')
 import pyarabic.araby as ar
 from pyarabic.arabrepr import arepr
 import tashaphyne.stemming
-#~ import tashaphyne.normalize
 import alyahmor.aly_stem_noun_const as SNC
 import alyahmor.noun_affixer
 import arramooz.arabicdictionary as arabicdictionary
 from .print_debug import print_table
 from . import custom_dictionary
 from . import wordcase
 
@@ -63,14 +52,15 @@
         # costum noun dictionary
         self.custom_noun_dictionary = custom_dictionary.custom_dictionary("nouns")
 
         # allow to print internal results.
         self.cache_dict_search = {}
         self.cache_affixes_verification = {}
         self.noun_cache = {}
+        self.noun_vocalize_cache = {}
         self.debug = debug
         self.error_code = ""
         
     def get_error_code(self,):
         """
         Return error code when word is not recognized
         """
@@ -107,14 +97,16 @@
             return None
         debug = self.debug
         #~list_found = []
         detailed_result = []
         noun_list = [
             noun_in,
         ] + self.get_noun_variants(noun_in)
+        noun_list = list(set(noun_list))
+        
         word_segmented_list = []
         for noun in noun_list:
             list_seg_comp = self.comp_stemmer.segment(noun)
             # filter
             list_seg_comp = self.verify_affix(noun, list_seg_comp,
                                          SNC.COMP_NOUN_AFFIXES)
             # treat multi vocalization enclitic
@@ -288,37 +280,26 @@
                     voc_affix_case = [ vac for vac in voc_affix_case if vac]
                     detailed_result.append(
                         wordcase.WordCase({
                             'word':
                             noun_in,
                             'affix': (word_seg['pro'], '', word_seg['suf_voc'],
                                       word_seg['enc_voc']),
-                            'stem':
-                            word_seg['stem_conj'],
+                            'stem':  word_seg['stem_conj'],
                             'root':ar.normalize_hamza(word_seg['noun_tuple'].get('root','')),
-                            'original':
-                            word_seg['noun_tuple']['vocalized'],  #original,
-                            'vocalized':
-                            vocalized,
-                            'semivocalized':
-                            semi_vocalized,
-                            'tags':
-                            u':'.join(voc_affix_case),
-                            'type':
-                            u':'.join(['Noun', word_seg['noun_tuple']['wordtype']]),
-                            'number':
-                            word_seg['noun_tuple']['number'],
-                            'gender':
-                            word_seg['noun_tuple']['gender'],
-                            'freq':
-                            'freqnoun',  # to note the frequency type
-                            'originaltags':
-                            u':'.join(original_tags),
-                            'syntax':
-                            '',
+                            'original': word_seg['noun_tuple']['vocalized'],  #original,
+                            'vocalized': vocalized,
+                            'semivocalized': semi_vocalized,
+                            'tags':  u':'.join(voc_affix_case),
+                            'type':  u':'.join(['Noun', word_seg['noun_tuple']['wordtype']]),
+                            'number': word_seg['noun_tuple']['number'],
+                            'gender':  word_seg['noun_tuple']['gender'],
+                            'freq': 'freqnoun',  # to note the frequency type
+                            'originaltags':  u':'.join(original_tags),
+                            'syntax': '',
                         }))
         if not detailed_result:
             self.set_error_code("Forms are not generated")
 
         if debug: print("after generate result")
         if debug: print(len(detailed_result))
         #~ if debug: print repr(detailed_result).replace('},','},\n').decode("unicode-escape")
@@ -630,16 +611,18 @@
         @param suffix: second level suffix.
         @type suffix: unicode.
         @param enclitic: first level suffix.
         @type enclitic: unicode.
         @return: vocalized word.
         @rtype: unicode.
         """
-        return self.generator.vocalize(noun, proclitic, suffix, enclitic)
-        
+        key = "-".join([noun, proclitic, suffix, enclitic])
+        if not key in self.noun_vocalize_cache:
+            self.noun_vocalize_cache[key] = self.generator.vocalize(noun, proclitic, suffix, enclitic)
+        return self.noun_vocalize_cache[key] 
         
         # proclitic have only an uniq vocalization in arabic
         proclitic_voc = SNC.COMP_PREFIX_LIST_TAGS[proclitic]["vocalized"][0]
         # enclitic can be variant according to suffix
         #print (u"vocalize: '%s' '%s'"%(enclitic, noun)).encode('utf8')
         enclitic_voc = SNC.COMP_SUFFIX_LIST_TAGS[enclitic]["vocalized"][0]
         enclitic_voc, encl_voc_non_inflect = self.get_enclitic_variant(
```

## qalsadi/stem_stop.py

```diff
@@ -18,14 +18,18 @@
 import pyarabic.araby as araby
 import tashaphyne.stemming
 import tashaphyne.normalize
 from . import stem_stopwords_const as ssconst
 import arramooz.stopwordsdictionaryclass as stopwordsdictionaryclass
 from . import wordcase
 
+from alyahmor import stopword_affixer  as stop_affixer
+# ~ from . import stop_affixer
+
+
 
 class StopWordStemmer:
     """
         Arabic stop stemmer
     """
 
     def __init__(self, debug=False):
@@ -36,14 +40,17 @@
         self.comp_stemmer.set_suffix_list(ssconst.COMP_SUFFIX_LIST)
         # create a stemmer object for stemming conjugated verb
         self.conj_stemmer = tashaphyne.stemming.ArabicLightStemmer()
         # configure the stemmer object
         self.conj_stemmer.set_prefix_list(ssconst.CONJ_PREFIX_LIST)
         self.conj_stemmer.set_suffix_list(ssconst.CONJ_SUFFIX_LIST)
 
+        # generator 
+        self.generator = stop_affixer.stopword_affixer()
+        
         # enable the last mark (Harakat Al-I3rab)
         self.allow_syntax_lastmark = True
 
         # stop dictionary
         #~self.stop_dictionary = stopwordsdictionaryclass.StopWordsDictionary("stopwords")
         self.stop_dictionary = stopwordsdictionaryclass.StopWordsDictionary(
             "classedstopwords")
@@ -172,17 +179,18 @@
                                   +ssconst.CONJ_SUFFIX_LIST_TAGS[vocalized_suffix]['tags']
                         ## verify compatibility between procletics and affix
                         valid = self.validate_tags(stop_tuple, affix_tags, procletic, encletic_nm)
                         compatible = self.is_compatible_proaffix_affix(
                             stop_tuple, procletic, vocalized_encletic,
                             vocalized_suffix)
                         if valid and compatible:
-                            vocalized, semi_vocalized = self.vocalize(
+                            vocalized_list = self.vocalize(
                                 original, procletic, vocalized_suffix,
                                 vocalized_encletic)
+                            vocalized, semi_vocalized =   vocalized_list[0][0],  vocalized_list[0][1]                              
                             vocalized = self.ajust_vocalization(vocalized)
                             #ToDo:
                             # if the stop word is inflected or not
                             is_inflected = u"مبني" if stop_tuple[
                                 'is_inflected'] == 0 else u"معرب"
                             #add some tags from dictionary entry as
                             # use action and object_type
@@ -203,15 +211,15 @@
                                     'stem':
                                     stem_conj,
                                     'original':
                                     original,
                                     'vocalized':
                                     vocalized,
                                     'semivocalized':
-                                    semi_vocalized,
+                                    semi_vocalized,                                     
                                     'tags':
                                     u':'.join(affix_tags),
                                     'type':
                                     u':'.join(
                                         ['STOPWORD', stop_tuple['word_type']]),
                                     'freq':
                                     'freqstopword',  # to note the frequency type
@@ -453,14 +461,19 @@
         @param suffix: second level suffix.
         @type suffix: unicode.
         @param enclitic: first level suffix.
         @type enclitic: unicode.
         @return: vocalized word.
         @rtype: unicode.
         """
+        #TODO: fix an error
+        # The generator generate more than one case
+        return self.generator.vocalize(stop, proclitic, suffix, enclitic)
+        
+        
         # procletic have only an uniq vocalization in arabic
         proclitic_voc = ssconst.COMP_PREFIX_LIST_TAGS[proclitic]["vocalized"][0]
         # enclitic can have many vocalization in arabic
         # like heh => عليهِ سواهُ
         # in this stage we consider only one,
         # the second situation is ajusted by vocalize_ajust
         enclitic_voc = ssconst.COMP_SUFFIX_LIST_TAGS[enclitic]["vocalized"][0]
```

## qalsadi/stem_stopwords_const.py

```diff
@@ -953,14 +953,16 @@
 
 AJUSTMENT = {
     u"عَلَيْهُ": u"عَلَيْهِ",
     u"عَلَيهُ": u"عَلَيْهِ",
     u"فِيهُ": u"فِيهِ",
     u"أَبِيهُ": u"أَبِيهِ",
     u"إِلَيْهُ": u"إِلَيْهِ",
+    u"وَإِلَيْهُ": u"وَإِلَيْهِ",
+        
     u"إِلَيْهُمْ": u"إِلَيْهِمْ",
     u"إِلَيْهُمَا": u"إِلَيْهِمَا",
     u"عَنِي": u"عَنِّي",
     u"مِنِي": u"مِنِّي",
     u'لِهُ': u'لَهُ',
     u'لِهَا': u'لَهَا',
     u'لِهُمَا': u'لَهُمَا',
```

## qalsadi/stem_unknown.py

```diff
@@ -87,14 +87,17 @@
         self.conj_stemmer.set_prefix_list(snconst.CONJ_PREFIX_LIST)
         self.conj_stemmer.set_suffix_list(snconst.CONJ_SUFFIX_LIST)
         #word frequency dictionary
         self.wordfreq = wordfreqdictionaryclass.WordFreqDictionary(
             'wordfreq', wordfreqdictionaryclass.WORDFREQ_DICTIONARY_INDEX)
         # use the word frequency dictionary as a dictionary for unkonwn words
         self.noun_dictionary = self.wordfreq
+        
+        self.noun_cache = {}
+        self.noun_vocalize_cache = {}
 
         self.debug = debug
 
     def stemming_noun(self, noun):
         """
         Analyze word morphologically as noun
         @param noun: the input noun.
@@ -212,32 +215,31 @@
             # search the noun in the dictionary
             # we can return the tashkeel
             infnoun_form_list = []
             for infnoun in possible_noun_list:
                 # get the noun and get all its forms from the dict
                 # if the noun has plural suffix, don't look up
                 # in broken plural dictionary
-                infnoun_foundlist = self.noun_dictionary.lookup(
-                    infnoun, 'unknown')
+                infnoun_foundlist = self.lookup_dict(infnoun)
                 infnoun_form_list += infnoun_foundlist
             for noun_tuple in infnoun_form_list:
                 # noun_tuple = self.noun_dictionary.getEntryById(id)
                 infnoun = noun_tuple['vocalized']
                 original_tags = ()
                 #~original = noun_tuple['vocalized']
                 wordtype = noun_tuple['word_type']
-                vocalized = vocalize(infnoun, procletic, prefix_conj, suffix_conj, encletic)
+                vocalized = self.vocalize(infnoun, procletic, prefix_conj, suffix_conj, encletic)
                 #print "v", vocalized.encode('utf8')
                 detailed_result.append(wordcase.WordCase({
                     'word':noun,
                     'affix': (procletic, prefix_conj, suffix_conj, encletic),
                     'stem':stem_conj,
                     'original':infnoun, #original,
                     'vocalized':vocalized,
-                    'semivocalized':vocalized,
+                    'semivocalized':vocalized,                     
                     'tags':u':'.join(snconst.COMP_PREFIX_LIST_TAGS[procletic]['tags']\
                     +snconst.COMP_SUFFIX_LIST_TAGS[encletic]['tags']+\
                     snconst.CONJ_SUFFIX_LIST_TAGS[suffix_conj]['tags']),
                     'type':u':'.join(['Noun', wordtype]), #'Noun',
                     'freq':noun_tuple['freq'],
                     'originaltags':u':'.join(original_tags),
                     'syntax':'',
@@ -248,66 +250,84 @@
     def set_debug(self, debug):
         """
         Set the debug attribute to allow printing internal analysis results.
         @param debug: the debug value.
         @type debug: True/False.
         """
         self.debug = debug
+        
+    def lookup_dict(self, word):
+        """
+        lookup for word in dict
+        """
+        if word in self.noun_cache:
+            return self.noun_cache[word]
+        else:
+            result = self.noun_dictionary.lookup(word, 'unknown')
+            # ~ result +=  self.custom_noun_dictionary.lookup(word)
+            self.noun_cache[word] = result
+        return result        
 
 
-def vocalize(noun, proclitic, prefix, suffix, enclitic):
-    """
-    Join the  noun and its affixes, and get the vocalized form
-    @param noun: noun found in dictionary.
-    @type noun: unicode.
-    @param proclitic: first level prefix.
-    @type proclitic: unicode.
-    @param prefix: second level suffix.
-    @type prefix: unicode.
-    @param suffix: second level suffix.
-    @type suffix: unicode.
-    @param enclitic: first level suffix.
-    @type enclitic: unicode.
-    @return: vocalized word.
-    @rtype: unicode.
-    """
-    enclitic_voc = snconst.COMP_SUFFIX_LIST_TAGS[enclitic]["vocalized"][0]
-    proclitic_voc = snconst.COMP_PREFIX_LIST_TAGS[proclitic]["vocalized"][0]
-    suffix_voc = suffix
-    #adjust some some harakat
-
-    #strip last if tanwin or harakat
-    if noun[-1:] in araby.HARAKAT:
-        noun = noun[:-1]
-    #completate the dictionary word vocalization
-    # this allow to avoid some missed harakat before ALEF
-    # in the dictionary form of word, all alefat are preceded by Fatha
-    #~noun = araby.complet
-    #~ print "stem_unknown.vocalize; before", noun.encode('utf8');
-    noun = noun.replace(araby.ALEF, araby.FATHA + araby.ALEF)
-    #~ print "stem_unknown.vocalize; 2", noun.encode('utf8');
-
-    noun = noun.replace(araby.ALEF_MAKSURA, araby.FATHA + araby.ALEF_MAKSURA)
-    noun = re.sub(u"(%s)+" % araby.FATHA, araby.FATHA, noun)
-
-    # remove initial fatha if alef is the first letter
-    noun = re.sub(u"^(%s)+" % araby.FATHA, "", noun)
-    #~ print "stem_unknown.vocalize; 3", noun.encode('utf8');
-
-    #add shadda if the first letter is sunny and the prefix
-    #ends by al definition
-    if proclitic.endswith(araby.ALEF + araby.LAM) and araby.is_sun(noun[0]):
-        noun = u''.join([noun[0], araby.SHADDA, noun[1:]])
-        #strip the Skun from the lam
-        if proclitic_voc.endswith(araby.SUKUN):
-            proclitic_voc = proclitic_voc[:-1]
-    noun = get_word_variant(noun, suffix)
-    noun = get_word_variant(noun, enclitic)
-    suffix_voc = get_suffix_variant(noun, suffix_voc, enclitic)
-    return ''.join([proclitic_voc, prefix, noun, suffix_voc, enclitic_voc])
+    def vocalize(self, noun, proclitic, prefix, suffix, enclitic):
+        """
+        Join the  noun and its affixes, and get the vocalized form
+        @param noun: noun found in dictionary.
+        @type noun: unicode.
+        @param proclitic: first level prefix.
+        @type proclitic: unicode.
+        @param prefix: second level suffix.
+        @type prefix: unicode.
+        @param suffix: second level suffix.
+        @type suffix: unicode.
+        @param enclitic: first level suffix.
+        @type enclitic: unicode.
+        @return: vocalized word.
+        @rtype: unicode.
+        """
+        
+        key = "-".join([noun, proclitic, prefix, suffix, enclitic])
+        if key in self.noun_vocalize_cache:
+            return self.noun_vocalize_cache[key]
+        enclitic_voc = snconst.COMP_SUFFIX_LIST_TAGS[enclitic]["vocalized"][0]
+        proclitic_voc = snconst.COMP_PREFIX_LIST_TAGS[proclitic]["vocalized"][0]
+        suffix_voc = suffix
+        #adjust some some harakat
+
+        #strip last if tanwin or harakat
+        if noun[-1:] in araby.HARAKAT:
+            noun = noun[:-1]
+        #completate the dictionary word vocalization
+        # this allow to avoid some missed harakat before ALEF
+        # in the dictionary form of word, all alefat are preceded by Fatha
+        #~noun = araby.complet
+        #~ print "stem_unknown.vocalize; before", noun.encode('utf8');
+        noun = noun.replace(araby.ALEF, araby.FATHA + araby.ALEF)
+        #~ print "stem_unknown.vocalize; 2", noun.encode('utf8');
+
+        noun = noun.replace(araby.ALEF_MAKSURA, araby.FATHA + araby.ALEF_MAKSURA)
+        noun = re.sub(u"(%s)+" % araby.FATHA, araby.FATHA, noun)
+
+        # remove initial fatha if alef is the first letter
+        noun = re.sub(u"^(%s)+" % araby.FATHA, "", noun)
+        #~ print "stem_unknown.vocalize; 3", noun.encode('utf8');
+
+        #add shadda if the first letter is sunny and the prefix
+        #ends by al definition
+        if proclitic.endswith(araby.ALEF + araby.LAM) and araby.is_sun(noun[0]):
+            noun = u''.join([noun[0], araby.SHADDA, noun[1:]])
+            #strip the Skun from the lam
+            if proclitic_voc.endswith(araby.SUKUN):
+                proclitic_voc = proclitic_voc[:-1]
+        noun = get_word_variant(noun, suffix)
+        noun = get_word_variant(noun, enclitic)
+        suffix_voc = get_suffix_variant(noun, suffix_voc, enclitic)
+        noun_conj =  ''.join([proclitic_voc, prefix, noun, suffix_voc, enclitic_voc])
+        self.noun_vocalize_cache[key] = noun_conj
+        return noun_conj
 
 
 def is_compatible_proaffix_affix(procletic, encletic, suffix):
     """
     Verify if proaffixes (sytaxic affixes) are compatable
     with affixes (conjugation)
     @param procletic: first level prefix.
```

## qalsadi/stem_verb.py

```diff
@@ -10,41 +10,25 @@
 # Created:     31-10-2011
 # Copyright:   (c) Taha Zerrouki 2011
 # Licence:     GPL
 #-----------------------------------------------------------------------
 """
     Arabic verb stemmer
 """
-from __future__ import (
-    absolute_import,
-    print_function,
-    #~ unicode_literals,
-    #~ division,
-    )
-#~ import re
-if __name__ == '__main__':
-    import sys
-    sys.path.append('../support')
-    sys.path.append('support')
-    sys.path.append('..')
 import pyarabic.araby as ar
 from pyarabic.arabrepr import arepr
 import tashaphyne.stemming
-#~ import stem_verb_const as SVC
 import alyahmor.aly_stem_verb_const as SVC
 import alyahmor.verb_affixer
-#~import analex_const
 import libqutrub.classverb
 import arramooz.arabicdictionary as arabicdictionary
 from .print_debug import print_table
 from . import custom_dictionary
 from . import wordcase
 
-#~ import  stemmedword
-
 
 class VerbStemmer:
     """
         Arabic verb stemmer
     """
 
     def __init__(self, debug=False):
@@ -69,26 +53,29 @@
 
         # To show statistics about verbs
         #~statistics = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0,
         #~10:0, 11:0, 12:0, 13:0, 14:0, 15:0, 16:0, 17:0, 18:0, 19:0, 20:0,
         #~}
         # affixes compatibility
         self.compatibility_cache = {}
-        #~ self.verb_dict_cache = {}
+        self.verb_dict_cache = {}
 
         self.debug = debug
-        self.cache_verb = {'verb': {}}
+        # ~ self.cache_verb = {'verb': {}}
 
         self.verb_dictionary = arabicdictionary.ArabicDictionary("verbs")
         
         # costum verb dictionary
         self.custom_verb_dictionary = custom_dictionary.custom_dictionary("verbs")        
 
         self.verb_stamp_pat = SVC.VERB_STAMP_PAT
+        self.stamp_cache = {}
         self.verb_cache = {}
+        self.verbclass_cache = {}
+        self.stripped_words_cache = {}
         self.verb_conj_cache = {}
 
         self.error_code = ""
     def get_error_code(self,):
         """
         Return error code when word is not recognized
         """
@@ -118,17 +105,19 @@
         """
         stamp = self.verb_dictionary.word_stamp(word)
         stamp = stamp.replace(ar.TEH,"")
         # a verb stamp can't more than 4 letters
         # لا يمكن للفعل أن يكون فيه أكثر من أربعة حروف أصلية
         if len(stamp) > 4:
             return False
-        result = self.verb_dictionary.exists_as_stamp(word)
-        result +=  self.custom_verb_dictionary.exists_as_stamp(word)
-        return result            
+        if stamp not in self.stamp_cache:
+            result = self.verb_dictionary.exists_as_stamp(word)
+            result +=  self.custom_verb_dictionary.exists_as_stamp(word)
+            self.stamp_cache[stamp] = result
+        return self.stamp_cache.get(stamp, False)           
 
     def stemming_verb(self, verb_in):
         """
         Stemming verb
         @param verb_in: given verb
         @type verb_in: unicode
         @return : stemmed words
@@ -322,15 +311,15 @@
                 detailed_result.append(wordcase.WordCase({
                     'word':word_seg['verb'],
                     'affix': (word_seg['pro'], word_seg['prefix'], word_seg['suffix'], word_seg['enc']),
                     'stem':word_seg['stem_conj'],
                     'root':ar.normalize_hamza(word_seg.get('root','')),
                     'original':conj['verb'],
                     'vocalized':vocalized,
-                    'semivocalized':semivocalized,
+                    'semivocalized':semivocalized,                
                     'tags':tags,#\
                     'type':tag_type,
                     'number': conj['pronoun_tags'].get('number', ''),
                     'gender': conj['pronoun_tags'].get('gender', ''),
                     'person': conj['pronoun_tags'].get('person', ''),
                     'tense2': conj['tense_tags'].get('tense', ''),
                     'voice': conj['tense_tags'].get('voice', ''),
@@ -364,17 +353,17 @@
         @type verb; unicode
         @param transitive: tranitive or intransitive
         @type transitive: boolean
         @return : list of infinitive verbs
         @rtype: list of unicode
         """
         # verb key
-        #~ verb_key = u":".join([verb, str(transitive)])
-        #~ if verb_key in self.verb_dict_cache:
-        #~ return self.verb_dict_cache[verb_key]
+        verb_key = u":".join([verb, str(transitive)])
+        if verb_key in self.verb_dict_cache:
+            return self.verb_dict_cache[verb_key]
         # a solution by using verbs stamps
         liste = []
         #~ print (u"* ".join([verb,])).encode('utf8')
         #~ verb_id_list = self.verb_dictionary.lookup_by_stamp(verb)
         verb_id_list = self.lookup_by_stamp(verb)
 
         if len(verb_id_list):
@@ -392,15 +381,15 @@
         #~don't accepot
         listetemp = liste
         liste = []
         for item in listetemp:
             #~ #print item['transitive'], transitive
             if item['transitive'] in (u'y', 1) or not transitive:
                 liste.append(item)
-        #~ self.verb_dict_cache[verb_key] = liste
+        self.verb_dict_cache[verb_key] = liste
         return liste
 
     def set_debug(self, debug):
         """
         Set the debug attribute to allow printing internal analysis results.
         @param debug: the debug value.
         @type debug: True/False.
@@ -560,14 +549,42 @@
                    or pronoun in SVC.EXTERNAL_SUFFIX_TABLE.get(enclitic, ''))):
             self.compatibility_cache[comp_key] = True
             return True
 
         else:
             self.compatibility_cache[comp_key] = False
             return False
+            
+    def _get_verbclass(self, infinitive_verb, transitive,future_type):
+        """
+        return the verb class, used to reduce verb class init
+        """
+        key = "-".join([infinitive_verb, future_type, str(transitive)])
+        if key in self.verbclass_cache:
+            return self.verbclass_cache[key]
+        else:
+            vbc = libqutrub.classverb.VerbClass(infinitive_verb, transitive,
+                                            future_type)
+            self.verbclass_cache[key]= vbc
+            return vbc
+            
+            
+    def _get_conjugation(self, verb_object, tense, pronoun):
+        """
+        return the conjugation from a verb
+        """
+
+        # if the verb is already conjuged return the conjugation
+        # else conjugate and return
+        conj = verb_object.conj_display.get_conj(tense, pronoun)
+        if not conj:
+            conj = verb_object.conjugate_tense_for_pronoun(tense, pronoun)
+        conj_nm = self._strip_tashkeel(conj)
+        return conj, conj_nm
+
 
     def __generate_possible_conjug(self,
                                    infinitive_verb,
                                    unstemed_verb,
                                    affix,
                                    future_type=ar.FATHA,
                                    extern_prefix="-",
@@ -578,16 +595,16 @@
         """
         ##    future_type = FATHA
         #~ transitive = True
         list_correct_conj = []
         if infinitive_verb == "" or unstemed_verb == "" or affix == "":
             return set()
         #~ infinitive_verb = infinitive_verb.replace(ar.ALEF_MADDA, ar.HAMZA+ ar.ALEF)
-        vbc = libqutrub.classverb.VerbClass(infinitive_verb, transitive,
-                                            future_type)
+        
+        vbc = self._get_verbclass(infinitive_verb, transitive,future_type)
         # الألف ليست جزءا من السابقة، لأنها تستعمل لمنع الابتداء بساكن
         # وتصريف الفعل في الامر يولده
         if affix.startswith(ar.ALEF):
             affix = affix[1:]
         # get all tenses to conjugate the verb one time
         tenses = []
         if affix in SVC.TABLE_AFFIX:
@@ -601,18 +618,20 @@
                 pronoun = pair[1]
                 test = self.__check_clitic_tense(extern_prefix, extern_suffix,
                                                  tense, pronoun, transitive)
                 #~ print "stem_verb 529", (u", ".join([extern_prefix, extern_suffix,
                 #~ tense, pronoun, str(transitive), "test",str(test)])).encode('utf8')
                 if test:
 
-                    conj_vocalized = vbc.conjugate_tense_for_pronoun(
-                        tense, pronoun)
-                    #strip all marks and shadda
-                    conj_nm = ar.strip_tashkeel(conj_vocalized)
+                    
+                    conj_vocalized, conj_nm  = self._get_conjugation(vbc, tense, pronoun)
+                    # ~ conj_vocalized = vbc.conjugate_tense_for_pronoun(
+                        # ~ tense, pronoun)                        
+                    # ~ #strip all marks and shadda
+                    # ~ conj_nm = self._strip_tashkeel(conj_vocalized)
                     if conj_nm == unstemed_verb:
                         list_correct_conj.append({
                             'verb':
                             infinitive_verb,
                             'tense':
                             tense,
                             'pronoun':
@@ -731,16 +750,23 @@
         if enclitic and verb.endswith(ar.ALEF_MAKSURA):
             verb = verb[:-1] + ar.ALEF
 
         vocalized = ''.join([proclitic_voc, verb, enclitic_voc])
         semivocalized = ''.join(
             [proclitic_voc, ar.strip_lastharaka(verb), enclitic_voc])
         return (vocalized, semivocalized)
-
-
+    
+    def _strip_tashkeel(self, word):
+        """
+        reduce amount of calculate strip tashkeel
+        """
+        if not word in self.stripped_words_cache:
+            self.stripped_words_cache[word] = ar.strip_tashkeel(word)
+        return self.stripped_words_cache[word]
+        
 def mainly():
     """
     Test main"""
     #ToDo: use the full dictionary of arramooz
     wordlist = [
         #~ u'يضرب',
         u'يضربه',
```

## qalsadi/stemnode.py

```diff
@@ -14,14 +14,16 @@
 stemNode represents the regrouped data resulted from the 
 morpholocigal analysis
 """
 from collections import Counter
 
 
 import pyarabic.araby as araby
+from .wordcase import  WordCase 
+from .stemmedword import StemmedWord
 
 def ispunct(word):
     return word in u'!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~،؟'
 #@deprecated_func
 def most_frequent2(List): 
     return max(set(List), key = List.count) 
     
@@ -39,28 +41,44 @@
     def __init__(self, case_list, vocalized_lemma=False):
         """
         Create the stemNode  from a list of StemmedSynword cases
         """
         # option to handle vocalized lemmas
         self.vocalized_lemma = vocalized_lemma
         self.case_count = len(case_list)
+        
+        # convert the case list into StemmedSynword
+        tmp_case_list = []
+        for case in case_list:
+            if isinstance(case, WordCase):
+                tmp_case_list.append(StemmedWord(case))
+            else:
+                tmp_case_list.append(case)
+        case_list = tmp_case_list
+                
         #~""" the number of syntaxtical cases """
         #~ print("case_list", len(case_list))
         self.word = ''
         #~""" The unstemmed word """
         self.previous_nodes = {}
         # the  syntaxical previous nodes 
         self.next_nodes = {}
         # the  syntaxical next nodes
         self.vocalizeds = []
         if case_list:        
             self.vocalizeds = [case.get_vocalized() for case in case_list]
             self.vocalizeds = list(set(self.vocalizeds))
             self.vocalizeds.sort()
             
+        self.tags = []
+        if case_list:        
+            self.tags = [case.get_tags()+":"+case.get_type() for case in case_list]
+            self.tags = list(set(self.tags))
+            self.tags.sort()
+            
         # the  affixs  list
         self.affixes = []
         if case_list:        
             self.affixes = [case.get_affix() for case in case_list]
             self.affixes = list(set(self.affixes))
             self.affixes.sort()
             
@@ -342,51 +360,57 @@
             lemmas = [l for l in originals]
         else:
             lemmas = [araby.strip_tashkeel(l) for l in originals]
             
         lemmas = list(set(lemmas))
         return lemmas
 
-    def get_lemma(self,  pos ="", return_pos=False):
+    def get_lemma(self,  pos = "", return_pos=False):
         """
         Get a lemma of the input word, you can select a POS tag (n,v,s)
         @return: the given lemmas list.
         @rtype: unicode string
         """
         # our strategy to select a lemma from many lemmas
         # if one return it
         # if it's a punct return it directly
 
+        lemma = ""
+        lemma_type = ""
         if self.lemmas.get("pounct", []):
-            return most_frequent(self.lemmas["pounct"])
-        # strategy to select lemmas
-        word_type_strategy = ["stopword", "noun", "verb", "all"]
-        
-        if pos:
-            pos = pos.lower()
-            if pos in ("s", "stop_words", "stop_word"):
-                pos = "stopword"
-            elif pos in ("n",):
-                pos = "noun"
-            elif pos in ("p", "punct",):
-                pos = "pounct"
-            elif pos in ("v", ):
-                pos = "verb"
-            else:
-                pos = "all"            
-            word_type_strategy = [pos,]
-        # select according to defined strategy
-        for  word_type in word_type_strategy:
-            if self.lemmas.get(word_type, []):
-                if not return_pos:
-                    return most_frequent(self.lemmas[word_type])
+                lemma  = most_frequent(self.lemmas["pounct"])
+                lemma_type = "pounct"
+        else:
+            # strategy to select lemmas
+            word_type_strategy = ["stopword", "noun", "verb", "all"]
+            
+            if pos:
+                pos = pos.lower()
+                if pos in ("s", "stop_words", "stop_word"):
+                    pos = "stopword"
+                elif pos in ("n",):
+                    pos = "noun"
+                elif pos in ("p", "punct",):
+                    pos = "pounct"
+                elif pos in ("v", ):
+                    pos = "verb"
                 else:
-                    # return lemma and its POS
-                    return (most_frequent(self.lemmas[word_type]), word_type)
-        return ""
+                    pos = "all"
+
+                word_type_strategy = [pos,]
+            # select according to defined strategy
+            for  word_type in word_type_strategy:
+                if self.lemmas.get(word_type, []):
+                     lemma =  most_frequent(self.lemmas[word_type])
+                     lemma_type = word_type
+                     break
+        if not return_pos:
+            return lemma
+        else:
+            return (lemma,lemma_type)
         
     
     def get_affixes(self, ):
         """
         Get the affixes of the input word
         @return: the given affixes.
         @rtype: unicode string
@@ -404,15 +428,23 @@
                 
     def get_vocalizeds(self, ):
         """
         Get the vocalized forms of the input word
         @return: the given vocalizeds.
         @rtype: list of unicode string
         """
-        return self.vocalizeds        
+        return self.vocalizeds 
+               
+    def get_tags(self, ):
+        """
+        Get the tags of the input word
+        @return: the tags list.
+        @rtype: list of unicode string
+        """
+        return self.tags      
 
     def get_chosen_indexes(self, ):
         """
         Get the chosen_indexes forms of the input word
         @return: the given chosen_indexes.
         @rtype: unicode string
         """
@@ -590,27 +622,14 @@
         """
         The syn node is break, if it hasn't any syntaxique or semantique 
         relation with the previous word
         """
         #~ return not self.syn_previous and not self.sem_previous #or self.get_break_type() == "break"
         return self.get_break_type() in ("break", "mostBreak")
 
-    #~ def is_next_break(self,):
-        #~ """
-        #~ The syn node is next break, if it hasn't any syntaxique or semantique 
-        #~ relation with the next word
-        #~ """
-        #~ if not(self.syn_nexts or self.sem_nexts):
-            #~ return True
-        #~ else:
-        #~ # or only the relation is tanwin
-            #~ for key in self.syn_nexts:
-                #~ if self.syn_nexts[k] != syn_const.TanwinRelation:
-                    #~ return False
-        #~ return False
                 
     def __repr__(self):
         text = u"\n'%s':%s, [%s-%s]{V:%d, N:%d, S:%d} " % (
             self.__dict__['word'], u', '.join(self.originals), 
             self.get_word_type(), self.get_break_type(), self.count["verb"], 
             self.count["noun"], self.count["stopword"], )
         text += repr(self.syntax_mark)
```

## qalsadi/wordcase.py

```diff
@@ -12,33 +12,37 @@
 #-------------------------------------------------------------------------------
 """
 wordCase represents the data resulted from the morpholocigal analysis
 """
 if __name__ == "__main__":
     import sys
     sys.path.append('..')
-#~import pyarabic.araby as araby
+import pyarabic.araby as araby
 import pyarabic.arabrepr as arabrepr
 arabicRepr = arabrepr.ArabicRepr()
 
 #~import analex_const
 
 
 class WordCase:
     """
     wordCase represents the data resulted from the morpholocigal analysis
     """
 
     def __init__(self, result_dict=None):
         self.word = u"",
         #~"""input word"""
+        self.word_nm = u"",
+        #~"""input word with marks"""
         self.vocalized = u"",
         #~"""vocalized form of the input word """
         self.semivocalized = u"",
         #~"""vocalized form without inflection mark """
+        self.unvocalized = u"",
+        #~"""unvocalized form"""
         self.tags = u"",
         #~"""tags of affixes and tags extracted form lexical dictionary"""
         self.affix_key = u'-'
         #~affixTags = u""
         #~"""tags of affixes"""
         # stemmed word attributes
         self.stem = u"",
@@ -77,14 +81,18 @@
             self.need = result_dict.get('need', u'')
             self.number = result_dict.get('number', u'')
             self.gender = result_dict.get('gender', u'')
             self.person = result_dict.get('person', u'')
             self.voice = result_dict.get('voice', u'')
             self.mood = result_dict.get('mood', u'')
             self.transitive = result_dict.get('transitive', False)
+            
+            # calculated attributes
+            self.unvocalized = araby.strip_tashkeel(self.vocalized)
+            # ~ self.word_nm = araby.strip_tashkeel(self.word)
 
     ######################################################################
     #{ Attribut Functions
     ######################################################################
     def get(self, key, default=u''):
         """
         get item by []
```

## Comparing `qalsadi-0.4.6.dist-info/RECORD` & `qalsadi-0.5.dist-info/RECORD`

 * *Files 14% similar despite different names*

```diff
@@ -1,35 +1,39 @@
 qalsadi/__init__.py,sha256=cosJijQbRz_b9i5guO3_sOR3_4V7EHW6hquxIo2JqGE,21
-qalsadi/analex.py,sha256=IE8OHBJX_FRqAn2pRbrt0Bjm1XdSeTgNWefzY3Zob40,30065
+qalsadi/analex.py,sha256=ybuGACj31Ua5o2EYmFKqhS5f-pKIvdE8JKIuZ0UNnuY,30401
 qalsadi/analex_const.py,sha256=kqWK_CWC-Uh9ZDiVNefcrSM7ukVg1588epvX0mjbDIQ,908
-qalsadi/cache.py,sha256=QzCX0F7E494VvFaHyxtfUpcZX3qBCKpCxFOAEMNnyUw,4630
-qalsadi/cache_codernity.py,sha256=aqy3DCNna_7584OTiFSQWVcFOBrmuXHV_Bx9A7umBtc,4783
-qalsadi/cache_pickledb.py,sha256=0GorR_Sqy4P56p3_GrffHDnwP8h43ZVRs_iiQAtvPfU,3121
+qalsadi/cache.py,sha256=weaUKsvl7snSQnQNXdC8on5fI7Z2JFnH1fHYUneVUuI,2163
+qalsadi/cache_codernity.py,sha256=kydPh71XusLOAC7vhiCAZWCJVjfRdRkpYwXg891PyWw,4454
+qalsadi/cache_factory.py,sha256=h_x7UhToHA0XMsGjREdKMJ9WgY7WSFn1TFLbivQNEG4,1692
+qalsadi/cache_pickle.py,sha256=XHEeFWaYyXZeK0RGzU8yjq_POpEmZupf-M1-wyNCli4,2946
+qalsadi/cache_pickledb.py,sha256=iQAsxxS68s8zdK9MmX1gDBiI8NZMHu05lXWtumQ0s9k,3225
 qalsadi/coder.py,sha256=1mkquOUH0qVrfJQDiE38xcR0o7PoEzaX46YIMVes9bo,822
 qalsadi/custom_dictionary.py,sha256=B0yPyaUzF5rtkH4dGPhCv9iSE_LoTWA0u8NEaVL85FU,2912
 qalsadi/dictword.py,sha256=f_w2SnGnVfPUQjwXZz6OSbaOdf7AKHHdnIsUXjRXcA8,18038
 qalsadi/disambig.py,sha256=2pz9Yi_iXyVeMEegFxbmE9v_V85CZgg-HcC_FKe-iNY,6930
 qalsadi/disambig_const.py,sha256=vKJwAkiAetQJZ5q2KoBJGBNuTR2VwJkOZPOwlzLWoBc,3612
 qalsadi/genAffixes.py,sha256=92K1AJLpSE13uw-okRXXlyEDQst1Hqt6IJUsvVOpNAk,4466
 qalsadi/genalex.py,sha256=asj45_RkamgObYPNGEgSiQKf1avkmfO2BKtbMpO9UYo,4484
-qalsadi/lemmatizer.py,sha256=zNjvAOKZifYoxLUdsXL-F3o_ztAn_0zdNhyeSRE0eJs,5620
+qalsadi/lemmatizer.py,sha256=Di_e04Wb6AlHKtjMFaGK8-V1n3ocGDVcuE_hUicfAsw,5661
 qalsadi/noun_affixer.py,sha256=_QOXNOcO6hNUf2PlKrU0e075pwAbuLjcH4wWu3PEpyg,21906
-qalsadi/print_debug.py,sha256=cibHS_ejU_FvtqqoQhp3y9gqfjuSilOqCjXxJkNbK4g,3738
-qalsadi/stem_noun.py,sha256=l4QsDPnMPLJDUAXdQmmw8njNKNm8k2ucVtkY-bqpwsI,38275
+qalsadi/print_debug.py,sha256=aLZWwZUUZ2eAQ2TXcvuK7p0WIV36nIj1fAw9vYdM2jE,3631
+qalsadi/stem_noun.py,sha256=CayOBn93o9_hQsFf2C-tS1V_QoTG3NUfr3ktcr2ZIWo,37961
 qalsadi/stem_noun_const.py,sha256=O_EQvsG4nJsuUMxQ5-7ghVcjRtteVWOnxHlOXXDpaDA,36411
 qalsadi/stem_pounct_const.py,sha256=PhUAlJuO8UEtV-fv-z6gd5lHkNFP5dS_ECEq_fTigQw,1814
-qalsadi/stem_stop.py,sha256=9IhjQu04irj68g9wr0XEuUo-dSM2P77DAVpHgcjibYg,25693
-qalsadi/stem_stopwords_const.py,sha256=V0XF4YZ2x91RabJ7EOMd6pG0r20b86sM4cdKTksBwEM,26991
-qalsadi/stem_unknown.py,sha256=I9UJote0g2Q0W9zX2sg9SvNxNEeHCS7txoTeBwQlsUw,19287
+qalsadi/stem_stop.py,sha256=2lRVCoUjgWE7guYdsBLKMMshFZsPy-JyVS8eXmsU2Tw,26198
+qalsadi/stem_stopwords_const.py,sha256=3cQnxZ7DwQlTjdGvyCG3JRoQqlB49AHGvN6eaa9Mrsw,27054
+qalsadi/stem_unknown.py,sha256=9nz2FvIk10GkfTGg1LhbCEeTcwG6qiSsgR0Qu7hIWts,20183
 qalsadi/stem_unknown_const.py,sha256=kp6Pe0e5C8n0KP7xB0w0KfVrvI7clPbl_LATg745tio,30128
-qalsadi/stem_verb.py,sha256=7RTF184WV5wrEkoPvUDwGr-UxEeRUgZN3OZrjLsC2nc,31754
+qalsadi/stem_verb.py,sha256=gn8bIkxQQ-JaQuS5icHPCctN82n28kuDSDs7v5XqQjg,33106
 qalsadi/stem_verb_const.py,sha256=hk_3VKBrhq-zlv_-WZ0R-Fw1tfJjc6PMfU3ON2tIUi0,45768
 qalsadi/stemmedaffix.py,sha256=p7982i0tMuh9B7qOsrevURkvzo2kTZLLb_7b3kKyFhE,22962
 qalsadi/stemmedword.py,sha256=ShwfgIiID8bhizPDBUqJ_lzvkrC9mLNzePaCQ7X33pQ,34467
-qalsadi/stemnode.py,sha256=gQhoWGGNgyb1UfB1M9pVye8rOBNlJe7BTOVfl1S1Xrk,21605
+qalsadi/stemnode.py,sha256=dbJ95BOLiHqYTpOkFZ0Q32mX7TxgswWWPQRrP8_c9EU,22078
+qalsadi/stop_affixer.py,sha256=IJuHU0jYdMCbK2IOnufSUEPQQh2NY-R2XYkxda5_5Po,12784
 qalsadi/stopwords.py,sha256=TRGg50nWdxVHGvIunQQeb3VZsDqnVUMA3_dsCSiAHB8,4235391
-qalsadi/wordcase.py,sha256=-pvRlItuNP1iKspmmSXBcMLvDoUrz8xuDGaZ2xJV0MY,9281
+qalsadi/templater.py,sha256=tkYc-j2g87MkjlyNU_7WBw-m-8zUpbFRoAXIslAHEnM,3100
+qalsadi/wordcase.py,sha256=KxvCgO3DI63sNc6_edcPL1WgTfi9w-J5E9SD5pp2_Ik,9590
 qalsadi/data/custom_dictionary.sqlite,sha256=aJxC9bPAm3KXJv7HwsZeQlila_ri4B1bi9Xd6m5hjZg,20480
-qalsadi-0.4.6.dist-info/METADATA,sha256=1fDsb5_vDZ9_hM6LDlBqAJUBkAjxSUryw9lnmz_WhmQ,9413
-qalsadi-0.4.6.dist-info/WHEEL,sha256=g4nMs7d-Xl9-xC9XovUrsDHGXt-FT0E17Yqo92DEfvY,92
-qalsadi-0.4.6.dist-info/top_level.txt,sha256=DsyTLhhIvafN5fjiuRJWV-gspFFLQxlHuf6KExJ5CYI,8
-qalsadi-0.4.6.dist-info/RECORD,,
+qalsadi-0.5.dist-info/METADATA,sha256=fJxb5TO73_69giPx-UHsXt7HIRnNN7sFtnQ4itDu5Y4,12904
+qalsadi-0.5.dist-info/WHEEL,sha256=g4nMs7d-Xl9-xC9XovUrsDHGXt-FT0E17Yqo92DEfvY,92
+qalsadi-0.5.dist-info/top_level.txt,sha256=DsyTLhhIvafN5fjiuRJWV-gspFFLQxlHuf6KExJ5CYI,8
+qalsadi-0.5.dist-info/RECORD,,
```

