# Comparing `tmp/prompt4all-0.0.2-py3-none-any.whl.zip` & `tmp/prompt4all-0.0.4-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,20 +1,30 @@
-Zip file size: 43919 bytes, number of entries: 18
--rw-rw-rw-  2.0 fat      731 b- defN 23-Jun-18 18:28 prompt4all/__init__.py
--rw-rw-rw-  2.0 fat    32290 b- defN 23-Jun-18 16:53 prompt4all/app.py
--rw-rw-rw-  2.0 fat     7678 b- defN 23-Jun-18 11:23 prompt4all/gradio_chatbot_patch.py
+Zip file size: 55358 bytes, number of entries: 28
+-rw-rw-rw-  2.0 fat      706 b- defN 23-Jul-13 18:07 prompt4all/__init__.py
+-rw-rw-rw-  2.0 fat    71722 b- defN 23-Jul-13 19:10 prompt4all/app.py
+-rw-rw-rw-  2.0 fat    10797 b- defN 23-Jul-05 17:41 prompt4all/context.py
 -rw-rw-rw-  2.0 fat    10299 b- defN 23-May-13 01:35 prompt4all/gradio_css.py
--rw-rw-rw-  2.0 fat    32290 b- defN 23-Jun-18 16:53 prompt4all/gradio_streamimg_chatbot.py
--rw-rw-rw-  2.0 fat    18666 b- defN 23-Jun-18 17:18 prompt4all/theme.py
+-rw-rw-rw-  2.0 fat    18697 b- defN 23-Jun-19 03:42 prompt4all/theme.py
 -rw-rw-rw-  2.0 fat        2 b- defN 23-Jun-01 05:13 prompt4all/api/__init__.py
--rw-rw-rw-  2.0 fat    27719 b- defN 23-Jun-18 17:29 prompt4all/api/base_api.py
+-rw-rw-rw-  2.0 fat    29292 b- defN 23-Jul-11 14:39 prompt4all/api/base_api.py
 -rw-rw-rw-  2.0 fat      227 b- defN 23-Apr-12 16:29 prompt4all/api/context_type.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Jun-01 05:14 prompt4all/utils/__init__.py
--rw-rw-rw-  2.0 fat     9100 b- defN 23-Jun-18 16:09 prompt4all/utils/chatgpt_utils.py
--rw-rw-rw-  2.0 fat      754 b- defN 23-Jun-18 16:34 prompt4all/utils/regex_utils.py
+-rw-rw-rw-  2.0 fat     2967 b- defN 23-Jul-13 07:52 prompt4all/prompts/incremental_rolling_summary.md
+-rw-rw-rw-  2.0 fat     1085 b- defN 23-Jul-08 13:54 prompt4all/prompts/meeting_minutes_summary.md
+-rw-rw-rw-  2.0 fat     3184 b- defN 23-Jul-13 18:20 prompt4all/prompts/mindmap_summary.md
+-rw-rw-rw-  2.0 fat     2206 b- defN 23-Jul-06 16:49 prompt4all/prompts/parallel_chunks_summary.md
+-rw-rw-rw-  2.0 fat     3024 b- defN 23-Jul-13 16:50 prompt4all/prompts/rolling_summary.md
+-rw-rw-rw-  2.0 fat     1394 b- defN 23-Jul-13 16:50 prompt4all/prompts/summary_final_cleansing.md
+-rw-rw-rw-  2.0 fat     2229 b- defN 23-Jul-08 11:24 prompt4all/prompts/topic_driven_summary.md
+-rw-rw-rw-  2.0 fat       30 b- defN 23-Jul-08 12:06 prompt4all/utils/__init__.py
+-rw-rw-rw-  2.0 fat     8686 b- defN 23-Jul-13 10:00 prompt4all/utils/chatgpt_utils.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jun-26 23:24 prompt4all/utils/io_utils.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jun-23 12:07 prompt4all/utils/pdf_utils.py
+-rw-rw-rw-  2.0 fat     3081 b- defN 23-Jul-13 07:22 prompt4all/utils/regex_utils.py
+-rw-rw-rw-  2.0 fat     7224 b- defN 23-Jul-13 10:02 prompt4all/utils/summary_utils.py
 -rw-rw-rw-  2.0 fat     1822 b- defN 23-Jun-14 14:34 prompt4all/utils/tokens_utils.py
--rw-rw-rw-  2.0 fat     1087 b- defN 23-Jun-18 18:28 prompt4all-0.0.2.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     4272 b- defN 23-Jun-18 18:28 prompt4all-0.0.2.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-Jun-18 18:28 prompt4all-0.0.2.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       11 b- defN 23-Jun-18 18:28 prompt4all-0.0.2.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     1498 b- defN 23-Jun-18 18:28 prompt4all-0.0.2.dist-info/RECORD
-18 files, 148538 bytes uncompressed, 41461 bytes compressed:  72.1%
+-rw-rw-rw-  2.0 fat     7361 b- defN 23-Jul-11 23:30 prompt4all/utils/whisper_utils.py
+-rw-rw-rw-  2.0 fat     1087 b- defN 23-Jul-17 10:49 prompt4all-0.0.4.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     4587 b- defN 23-Jul-17 10:49 prompt4all-0.0.4.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-Jul-17 10:49 prompt4all-0.0.4.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       11 b- defN 23-Jul-17 10:49 prompt4all-0.0.4.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     2436 b- defN 23-Jul-17 10:49 prompt4all-0.0.4.dist-info/RECORD
+28 files, 194248 bytes uncompressed, 51396 bytes compressed:  73.5%
```

## zipnote {}

```diff
@@ -1,55 +1,85 @@
 Filename: prompt4all/__init__.py
 Comment: 
 
 Filename: prompt4all/app.py
 Comment: 
 
-Filename: prompt4all/gradio_chatbot_patch.py
+Filename: prompt4all/context.py
 Comment: 
 
 Filename: prompt4all/gradio_css.py
 Comment: 
 
-Filename: prompt4all/gradio_streamimg_chatbot.py
-Comment: 
-
 Filename: prompt4all/theme.py
 Comment: 
 
 Filename: prompt4all/api/__init__.py
 Comment: 
 
 Filename: prompt4all/api/base_api.py
 Comment: 
 
 Filename: prompt4all/api/context_type.py
 Comment: 
 
+Filename: prompt4all/prompts/incremental_rolling_summary.md
+Comment: 
+
+Filename: prompt4all/prompts/meeting_minutes_summary.md
+Comment: 
+
+Filename: prompt4all/prompts/mindmap_summary.md
+Comment: 
+
+Filename: prompt4all/prompts/parallel_chunks_summary.md
+Comment: 
+
+Filename: prompt4all/prompts/rolling_summary.md
+Comment: 
+
+Filename: prompt4all/prompts/summary_final_cleansing.md
+Comment: 
+
+Filename: prompt4all/prompts/topic_driven_summary.md
+Comment: 
+
 Filename: prompt4all/utils/__init__.py
 Comment: 
 
 Filename: prompt4all/utils/chatgpt_utils.py
 Comment: 
 
+Filename: prompt4all/utils/io_utils.py
+Comment: 
+
+Filename: prompt4all/utils/pdf_utils.py
+Comment: 
+
 Filename: prompt4all/utils/regex_utils.py
 Comment: 
 
+Filename: prompt4all/utils/summary_utils.py
+Comment: 
+
 Filename: prompt4all/utils/tokens_utils.py
 Comment: 
 
-Filename: prompt4all-0.0.2.dist-info/LICENSE
+Filename: prompt4all/utils/whisper_utils.py
+Comment: 
+
+Filename: prompt4all-0.0.4.dist-info/LICENSE
 Comment: 
 
-Filename: prompt4all-0.0.2.dist-info/METADATA
+Filename: prompt4all-0.0.4.dist-info/METADATA
 Comment: 
 
-Filename: prompt4all-0.0.2.dist-info/WHEEL
+Filename: prompt4all-0.0.4.dist-info/WHEEL
 Comment: 
 
-Filename: prompt4all-0.0.2.dist-info/top_level.txt
+Filename: prompt4all-0.0.4.dist-info/top_level.txt
 Comment: 
 
-Filename: prompt4all-0.0.2.dist-info/RECORD
+Filename: prompt4all-0.0.4.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## prompt4all/__init__.py

```diff
@@ -11,21 +11,21 @@
 if sys.getdefaultencoding() != defaultencoding:
     reload(sys)
     sys.setdefaultencoding(defaultencoding)
 
 PACKAGE_ROOT = os.path.dirname(__file__)
 PROJECT_ROOT = os.path.dirname(PACKAGE_ROOT)
 
-__version__ = '0.0.2'
+__version__ = '0.0.4'
 stderr.write('prompt4all {0}\n'.format(__version__))
 
 from prompt4all import api
 from prompt4all import utils
-#from . import gradio_chatbot_patch
-#from . import theme
+from prompt4all import prompts
+
 import threading
 import random
 import glob
 from tqdm import tqdm
 import numpy as np
```

## prompt4all/app.py

```diff
@@ -1,31 +1,45 @@
 # -*- coding: utf-8-sig -*-
 import json
 import os
+import datetime
+import time
+import string
+import builtins
 import regex
+from pathlib import Path
 import gradio as gr
 import openai
 import copy
 import requests
 import asyncio
 import time
 import nest_asyncio
 import openai_async
+import whisper
+import math
+import numpy as np
+from pydub import AudioSegment
+from datetime import datetime, timedelta
+from queue import Queue
 nest_asyncio.apply()
 from collections import OrderedDict
 from datetime import datetime
 from prompt4all.utils.chatgpt_utils import *
 from prompt4all.utils.regex_utils import *
 import prompt4all.api.context_type as ContextType
 from prompt4all.api.base_api import *
 from prompt4all.utils.tokens_utils import *
-#from gradio_chatbot_patch import Chatbot as grChatbot
-# from gradio_css import code_highlight_css
+from prompt4all.utils.summary_utils import *
 from prompt4all.theme import adjust_theme, advanced_css
+from prompt4all.utils.whisper_utils import *
+from prompt4all import context
+from prompt4all.context import *
 
+cxt=context._context()
 os.environ['no_proxy'] = '*'
 
 # 設置您的OpenAI API金鑰
 # 請將您的金鑰值寫入至環境變數"OPENAI_API_KEY"中
 # os.environ['OPENAI_API_KEY']=#'你的金鑰值'
 if "OPENAI_API_KEY" not in os.environ:
     print("OPENAI_API_KEY  is not exists!")
@@ -63,15 +77,15 @@
     while True:
         try:
             chat, answer, full_history = next(streaming_chat)
             yield chat, full_history, full_history
         except StopIteration:
             break
         except Exception as e:
-            raise ValueError(e)
+            raise gr.Error(str(e))
 
 
 def nlu_api(text_input):
     # 創建與API的對話
 
     text_inputs = text_input.split('\n')
     _parameters = copy.deepcopy(baseChatGpt.API_PARAMETERS)
@@ -153,156 +167,756 @@
     :param system_prompt:
     :param text_input:
     :return:
     """
     partial_words = ''
     token_counter = 0
     context_type = ContextType.skip
+    passage = "輸入文字內容:\"\"\"\n{0}\n\"\"\"\n".format(text_input)
     conversation = [
         {
             "role": "system",
             "content": system_prompt
         },
         {
             "role": "user",
-            "content": "#zh-TW \n輸入文字內容:\"\"\"\n{0}\n\"\"\"".format(text_input)
+            "content": passage
         }
     ]
-    payload = baseChatGpt.parameters2payload(baseChatGpt.API_MODEL, conversation, baseChatGpt.API_PARAMETERS,stream=False)
+    _parameters = copy.deepcopy(summaryChatGpt.API_PARAMETERS)
+    _parameters['temperature'] = 0.001
+    _parameters['presence_penalty'] = 1.2
+    payload = summaryChatGpt.parameters2payload(summaryChatGpt.API_MODEL, conversation, _parameters,stream=False)
 
     response =await asyncio.to_thread(
         requests.post,
-        baseChatGpt.BASE_URL, headers=baseChatGpt.API_HEADERS, json=payload,stream=False
+        summaryChatGpt.BASE_URL, headers=summaryChatGpt.API_HEADERS, json=payload,stream=False
     )
 
-
-
     try:
         # 解析返回的JSON結果
         this_choice = json.loads(response.content.decode())['choices'][0]
         print(this_choice)
         summary =this_choice["message"]
         total_tokens = response.json()["usage"]['completion_tokens']
         summary['total_tokens'] = total_tokens
         return summary
     except Exception as e:
-        return str(response.json()) + "\n" + str(e)
+        raise gr.Error(str(response.json()) + "\n" + str(e))
 
-
-async def rolling_summary(large_inputs, full_history, is_parallel):
-    _parameters = copy.deepcopy(baseChatGpt.API_PARAMETERS)
+async def rolling_summary(large_inputs, full_history, summary_method,summary_options):
+    _parameters = copy.deepcopy(summaryChatGpt.API_PARAMETERS)
     _parameters['temperature'] = 0.001
+    _parameters['presence_penalty'] = 1.2
     large_inputs = large_inputs.split('\n') if isinstance(large_inputs, str) else large_inputs
+    large_inputs_tokens=builtins.sum([estimate_used_tokens(w)+1 for w in large_inputs])
     large_inputs_bk = copy.deepcopy(large_inputs)
     st = datetime.now()
-    if is_parallel:
-        _system_prompt = "#你是一個萬能文字助手，你擅長使用繁體中文以條列式的格式來整理逐字稿、會議記錄以及長文本文件，你懂得如何將「輸入文字內容」視狀況修正同音錯字，去除口語贅字後，尤其是涉及[人名、公司機構名稱、事物名稱、地點、時間、數量、知識點、事實、數據集、url]這些資訊時，在保持原意不變的前提下進行摘要，請基於縮排或標號(\"-\")來表達摘要資訊間的階層性與相關性，輸出結果應該是「摘要清單」，不解釋原因。請以繁體中文書寫。 #zh-tw"
-        this_system_tokens = estimate_used_tokens(_system_prompt) + estimate_used_tokens('system',
-                                                                                         model_name=baseChatGpt.API_MODEL) + 4
-        available_tokens = (baseChatGpt.MAX_TOKENS - this_system_tokens - 4 - 2) * 0.66
-        keep_summary = True
-        text_dict = OrderedDict()
-        tasks = []
-        n = 0
+
+    is_final_stage = False
+    keep_summary = True
+    summary_repository = OrderedDict()
+    cleansed_summary = []
+    mindmap_history = "```  \nmindmap  \nroot((摘要心智圖)) \n```"
+    mindmap_head = '# 摘要心智圖'
+    meeting_minutes=''
+    meeting_head = '# 會議記錄'
+    topic_shortcuts=''
+    topic_head = '# 主題重點'
+
+    if summary_method==0:
+        _system_prompt =open("prompt4all/prompts/rolling_summary.md", encoding="utf-8").read()
+        _final_prompt = open("prompt4all/prompts/summary_final_cleansing.md", encoding="utf-8").read()
+
+        summary_history = '空的清單'
+        this_system_tokens = estimate_used_tokens(_system_prompt) + estimate_used_tokens('system',model_name=summaryChatGpt.API_MODEL) + 4
+        this_final_tokens = estimate_used_tokens(_final_prompt) + estimate_used_tokens('system',model_name=summaryChatGpt.API_MODEL) + 4
+        available_tokens = summaryChatGpt.MAX_TOKENS - this_system_tokens - 4 - 2
+        this_summary_tokens = estimate_used_tokens(summary_history)
+        this_available_tokens= (available_tokens - 2*this_summary_tokens)*0.667 - 100
+
+
+        partial_words = ''
+
+        cnt = 0
+        unchanged_summary=[]
         while keep_summary:
-            partial_words, large_inputs = get_next_paragraph(large_inputs, available_tokens)
+            summary_size_ratio=2*this_summary_tokens/available_tokens
+            print('summary_size_ratio:{0:.2%}'.format(summary_size_ratio))
+            #摘要量過大需要縮減
+            if summary_size_ratio>0.4 and not is_final_stage:
+                content=summary_repository[cnt]
+                this_tokens=builtins.sum([estimate_used_tokens(c)+1 for c in content ])
+                part1,part2=split_summary(content,int(this_tokens*0.667))
+                summary_history='\n'.join(part2)
+                unchanged_summary.extend(part1)
+                this_summary_tokens = estimate_used_tokens(summary_history)
+                this_available_tokens = (available_tokens - 2 * this_summary_tokens) * 0.667 - 100
+                new_summary_size_ratio = 2 * this_summary_tokens / available_tokens
+                print('summary_size_ratio  {0:.2%}=>{1:.2%}'.format(summary_size_ratio, new_summary_size_ratio))
+
+            try:
+                this_available_tokens = (available_tokens - 2*this_summary_tokens)*0.667 - 100 if not is_final_stage else  (available_tokens )//2 - 100
+                # get tokens
+                if len(large_inputs) == 0 :
+                    if is_final_stage:
+                        break
+                    else:
+                        is_final_stage=True
+                        keep_summary = False
+                        available_tokens = summaryChatGpt.MAX_TOKENS - this_final_tokens - 4 - 2
+                        this_summary_tokens = 0
+                        this_available_tokens = (available_tokens )//2 - 100
+                        large_inputs=copy.deepcopy(unchanged_summary)
+                        large_inputs.extend(summary_history.split('\n'))
+                        keep_summary = True
+
+
+                if not is_final_stage:
+                    partial_words, large_inputs = get_next_paragraph(large_inputs, this_available_tokens)
+                    remain_tokens=builtins.sum([estimate_used_tokens(w)+1 for w in large_inputs])
+                    print('partial_words:{0} large_inputs:{1}'.format(builtins.sum([estimate_used_tokens(w)+1 for w in partial_words]),remain_tokens ))
 
-            if len(large_inputs) == 0 or len(partial_words) == 0:
-                keep_summary = False
+                else:
 
+                    partial_words, large_inputs = get_next_paragraph(large_inputs, this_available_tokens)
+                    remain_tokens = builtins.sum([estimate_used_tokens(w) + 1 for w in large_inputs])
+                    if remain_tokens<50:
+                        partial_words.extend(large_inputs)
+                        remain_tokens=0
+                        large_inputs=[]
+                    print('partial_words:{0} large_inputs:{1}'.format(builtins.sum([estimate_used_tokens(w) + 1 for w in partial_words]), remain_tokens))
+                    if len(large_inputs)==0:
+                        keep_summary=False
+
+                passage = "累積摘要清單:\n\n\"\"\"\n\n{0}\n\n\"\"\"\n\n輸入文字內容:\n\n\"\"\"\n\n{1}\n\n\"\"\"\n\n".format(summary_history,'\n'.join(partial_words))
+                passage_final="摘要清單:\n\n\"\"\"\n\n{0}\n\n\"\"\"\n\n標號起始數字:{1}\n".format('\n'.join(partial_words), get_last_ordered_index(cleansed_summary))
+
+                conversation = [
+                    {
+                        "role": "system",
+                        "content": _system_prompt if not is_final_stage else _final_prompt
+                    },
+                    {
+                        "role": "user",
+                        "content": passage if not is_final_stage else passage_final
+                    }
+                ]
+                print(conversation)
+
+                _max_tokens=builtins.min(summaryChatGpt.MAX_TOKENS,estimate_used_tokens(str(conversation))+estimate_used_tokens('\n'.join(partial_words))*(0.3 if not is_final_stage else 1))
+                _parameters['max_tokens']=_max_tokens
+
+                streaming_answer = summaryChatGpt.post_and_get_streaming_answer(conversation, _parameters, full_history)
+                answer = ''
+                answer_head =  """  \n## 第{0}部分摘要 {1:.2%}  \n\n\n""".format(cnt + 1,float(large_inputs_tokens-remain_tokens)/large_inputs_tokens).replace('\n\n\n','\n{0} \n') if not is_final_stage else """  \n## 最終版摘要  \n{0} \n"""
+
+                while True:
+                    try:
+                        answer, full_history = next(streaming_answer)
+                        if not is_final_stage:
+                            yield answer_head.format(text2markdown(('\n'.join(unchanged_summary) if len(unchanged_summary)>0 else '')+'  \n'+'  \n'.join(get_rolling_summary_results(answer)))), full_history
+                        else:
+                            yield answer_head.format(text2markdown(('\n'.join(cleansed_summary) if len(cleansed_summary)>0 else '')+'  \n'+'  \n'.join(get_rolling_summary_results(answer)))), full_history
+                    except StopIteration:
+                        break
+                print(answer_head.format(answer))
+                print('\n\n')
+                if not is_final_stage :
+                    summary_repository[cnt+1]=get_rolling_summary_results(answer)
+                    summary_history ='\n'.join(summary_repository[cnt+1])
+                    this_summary_tokens = estimate_used_tokens(summary_history)
+                    this_available_tokens = (available_tokens - 2*this_summary_tokens) * 0.667 - 100
 
-            text_dict[n] = OrderedDict()
-            text_dict[n]['text'] = '\n'.join(partial_words)
-            tasks.append(summarize_text('\n'.join(partial_words), _system_prompt))
-            time.sleep(0.5)
-        print('預計切成{0}塊'.format(len(tasks)))
-        return_values = await asyncio.gather(*tasks)
-        print(datetime.now() - st)
-        print(return_values)
+                    cnt += 1
+                else:
+                    cleansed_summary.extend(get_rolling_summary_results(answer))
+                    this_available_tokens =(available_tokens )//2 - 100
 
-        yield aggregate_summary(return_values), full_history
+                yield answer_head.format(text2markdown('\n'.join(unchanged_summary)+'\n'+summary_history)) if not is_final_stage else answer_head.format(text2markdown('\n'.join(cleansed_summary))) , full_history
 
-    else:
+            except Exception as e:
+                PrintException()
+                raise gr.Error(str(e))
+
+    elif summary_method==1:
+        _system_prompt =open("prompt4all/prompts/incremental_rolling_summary.md", encoding="utf-8").read()
+        _final_prompt = open("prompt4all/prompts/summary_final_cleansing.md", encoding="utf-8").read()
 
-        _system_prompt = "你是一個萬能文字助手，你擅長使用繁體中文以條列式的格式來整理逐字稿以及會議記錄，你懂得如何基於滾動式摘要，將「輸入文字內容」視狀況修正同音錯字，去除口語贅字後，比對之前的「累積摘要清單」，若是有新增資訊，尤其是涉及[人名、公司機構名稱、事物名稱、地點、時間、數量、知識點、事實、數據集、url]這些資訊時，在保持原意不變的前提下，提煉為新的摘要內容並將其append至「累積摘要清單」中,請基於縮排或標號來表達摘要資訊間的階層性與相關性，所有已存在於「累積摘要清單」內的資訊在新的內容加入後可以視狀況作微調的二次摘要，但是原有的資訊不應該因此而丟失。輸出結果應該是「累積摘要清單」，不解釋原因。請以繁體中文書寫。"
-        this_system_tokens = estimate_used_tokens(_system_prompt) + estimate_used_tokens('system',
-                                                                                         model_name=baseChatGpt.API_MODEL) + 4
-        available_tokens = baseChatGpt.MAX_TOKENS * 0.75 - this_system_tokens - 4 - 2
         summary_history = '空的清單'
+        this_system_tokens = estimate_used_tokens(_system_prompt) + estimate_used_tokens('system',model_name=summaryChatGpt.API_MODEL) + 4
+        this_final_tokens = estimate_used_tokens(_final_prompt) + estimate_used_tokens('system',model_name=summaryChatGpt.API_MODEL) + 4
+        available_tokens = summaryChatGpt.MAX_TOKENS - this_system_tokens - 4 - 2
+        this_summary_tokens = estimate_used_tokens(summary_history)
+        this_available_tokens= (available_tokens - 2*this_summary_tokens)*0.667 - 100
+
+
         partial_words = ''
 
-        keep_summary = True
         cnt = 0
+        unchanged_summary=[]
+        while keep_summary:
+            summary_size_ratio=2*this_summary_tokens/available_tokens
+            print('summary_size_ratio:{0:.2%}'.format(summary_size_ratio))
+            #摘要量過大需要縮減
+            if summary_size_ratio>0.4 and not is_final_stage:
+                content=summary_repository[cnt]
+                this_tokens=builtins.sum([estimate_used_tokens(c)+1 for c in content ])
+                part1,part2=split_summary(content,int(this_tokens*0.667))
+                summary_history='\n'.join(part2)
+                unchanged_summary.extend(part1)
+                this_summary_tokens = estimate_used_tokens(summary_history)
+                this_available_tokens = (available_tokens - 2 * this_summary_tokens) * 0.667 - 100
+                new_summary_size_ratio = 2 * this_summary_tokens / available_tokens
+                print('summary_size_ratio  {0:.2%}=>{1:.2%}'.format(summary_size_ratio, new_summary_size_ratio))
+
+            try:
+                this_available_tokens = (available_tokens - 2*this_summary_tokens)*0.667 - 100 if not is_final_stage else  (available_tokens )//2 - 100
+                # get tokens
+                if len(large_inputs) == 0 :
+                    if is_final_stage:
+                        break
+                    else:
+                        is_final_stage=True
+                        keep_summary = False
+                        available_tokens = summaryChatGpt.MAX_TOKENS - this_final_tokens - 4 - 2
+                        this_summary_tokens = 0
+                        this_available_tokens = (available_tokens )//2 - 100
+                        large_inputs=copy.deepcopy(unchanged_summary)
+                        large_inputs.extend(summary_history.split('\n'))
+                        keep_summary = True
+
+
+                if not is_final_stage:
+                    partial_words, large_inputs = get_next_paragraph(large_inputs, this_available_tokens)
+                    remain_tokens=builtins.sum([estimate_used_tokens(w)+1 for w in large_inputs])
+                    print('partial_words:{0} large_inputs:{1}'.format(builtins.sum([estimate_used_tokens(w)+1 for w in partial_words]),remain_tokens ))
+
+                else:
+
+                    partial_words, large_inputs = get_next_paragraph(large_inputs, this_available_tokens)
+                    remain_tokens = builtins.sum([estimate_used_tokens(w) + 1 for w in large_inputs])
+                    if remain_tokens<50:
+                        partial_words.extend(large_inputs)
+                        remain_tokens=0
+                        large_inputs=[]
+                    print('partial_words:{0} large_inputs:{1}'.format(builtins.sum([estimate_used_tokens(w) + 1 for w in partial_words]), remain_tokens))
+                    if len(large_inputs)==0:
+                        keep_summary=False
+
+                passage = "累積摘要清單:\n\n\"\"\"\n\n{0}\n\n\"\"\"\n\n輸入文字內容:\n\n\"\"\"\n\n{1}\n\n\"\"\"\n\n".format(summary_history,'\n'.join(partial_words))
+                passage_final="摘要清單:\n\n\"\"\"\n\n{0}\n\n\"\"\"\n\n標號起始數字:{1}\n".format('\n'.join(partial_words), get_last_ordered_index(cleansed_summary))
+
+                conversation = [
+                    {
+                        "role": "system",
+                        "content": _system_prompt if not is_final_stage else _final_prompt
+                    },
+                    {
+                        "role": "user",
+                        "content": passage if not is_final_stage else passage_final
+                    }
+                ]
+                print(conversation)
+
+                _max_tokens=builtins.min(summaryChatGpt.MAX_TOKENS,estimate_used_tokens(str(conversation))+estimate_used_tokens('\n'.join(partial_words))*(0.3 if not is_final_stage else 1))
+                _parameters['max_tokens']=_max_tokens
+
+                streaming_answer = summaryChatGpt.post_and_get_streaming_answer(conversation, _parameters, full_history)
+                answer = ''
+                answer_head =  """  \n## 第{0}部分摘要 {1:.2%}  \n\n\n""".format(cnt + 1,float(large_inputs_tokens-remain_tokens)/large_inputs_tokens).replace('\n\n\n','\n{0} \n') if not is_final_stage else """  \n## 最終版摘要  \n{0} \n"""
+
+                while True:
+                    try:
+                        answer, full_history = next(streaming_answer)
+                        if not is_final_stage:
+                            yield answer_head.format(text2markdown(('\n'.join(unchanged_summary) if len(unchanged_summary)>0 else '')+'  \n'+'  \n'.join(get_rolling_summary_results(answer)))), full_history
+                        else:
+                            yield answer_head.format(text2markdown(('\n'.join(cleansed_summary) if len(cleansed_summary)>0 else '')+'  \n'+'  \n'.join(get_rolling_summary_results(answer)))), full_history
+                    except StopIteration:
+                        break
+                    except Exception as e:
+                        gr.Error(str(e))
+                print(answer_head.format(answer))
+                print('\n\n')
+                if not is_final_stage :
+                    merged_summary_history=summary_history.split('\n') if summary_history!='空的清單' else[]
+                    number_list=[extract_numbered_list_member(txt) for txt in merged_summary_history]
+                    max_number=0
+                    if len(merged_summary_history)>0:
+                        max_number=int(extract_numbered_list_member(merged_summary_history[-1]).split('.')[0])
+
+                    new_summary=get_rolling_summary_results(answer)
+                    for i in range(len(new_summary)):
+                        this_summary=new_summary[i]
+                        this_number=extract_numbered_list_member(this_summary)
+                        if this_number in number_list:
+                            merged_summary_history[number_list.index(this_number)]=this_summary
+                        else:
+                            merged_summary_history.append(this_summary)
+
+                    summary_repository[cnt+1]=merged_summary_history
+                    summary_history ='\n'.join(summary_repository[cnt+1])
+                    this_summary_tokens = estimate_used_tokens(summary_history)
+                    this_available_tokens = (available_tokens - 2*this_summary_tokens) * 0.667 - 100
+
+                    cnt += 1
+                else:
+                    cleansed_summary.extend(get_rolling_summary_results(answer))
+                    this_available_tokens =(available_tokens )//2 - 100
+
+                yield answer_head.format(text2markdown('\n'.join(unchanged_summary)+'\n'+'  \n'.join(get_rolling_summary_results(answer)))) if not is_final_stage else answer_head.format(text2markdown('\n'.join(cleansed_summary)+'\n'+'  \n'.join(get_rolling_summary_results(answer)))) , full_history
+
+            except Exception as e:
+                PrintException()
+                raise gr.Error(str(e))
+    elif summary_method==2:
+        _system_prompt = open("prompt4all/prompts/parallel_chunks_summary.md", encoding="utf-8").read()
+        _final_prompt = open("prompt4all/prompts/summary_final_cleansing.md", encoding="utf-8").read()
+
+        summary_history = '空的清單'
+        this_system_tokens = estimate_used_tokens(_system_prompt) + estimate_used_tokens('system',model_name=summaryChatGpt.API_MODEL) + 4
+        this_final_tokens = estimate_used_tokens(_final_prompt) + estimate_used_tokens('system',model_name=summaryChatGpt.API_MODEL) + 4
+        available_tokens = summaryChatGpt.MAX_TOKENS - this_system_tokens - 4 - 2
+        this_summary_tokens = estimate_used_tokens(summary_history)
+        this_available_tokens= (available_tokens - 2*this_summary_tokens)*0.667 - 100
+
 
+        text_dict = OrderedDict()
+        tasks = []
+        cnt = 0
         while keep_summary:
-            this_system_tokens = estimate_used_tokens(summary_history)
-            this_available_tokens = (available_tokens - this_system_tokens) - 100
-            # get tokens
             partial_words, large_inputs = get_next_paragraph(large_inputs, this_available_tokens)
+            remain_tokens = builtins.sum([estimate_used_tokens(w) + 1 for w in large_inputs])
+            print('partial_words:{0} large_inputs:{1}'.format(builtins.sum([estimate_used_tokens(w) + 1 for w in partial_words]), remain_tokens))
 
-            if len(large_inputs) == 0 or len(partial_words) == 0:
+            summary_repository[cnt] = OrderedDict()
+            summary_repository[cnt]['text'] = '\n'.join(partial_words)
+            tasks.append(summarize_text('\n'.join(partial_words), _system_prompt))
+            time.sleep(2)
+            if len(large_inputs) == 0:
+                keep_summary = False
+        print('預計切成{0}塊'.format(len(tasks)))
+        return_values = await asyncio.gather(*tasks)
+        print(datetime.now() - st)
+        print(return_values)
+        for k in range(len(return_values)):
+            #handle process fail
+            if isinstance(return_values[k],str) and 'Error' in return_values[k]:
+                _parameters = copy.deepcopy(summaryChatGpt.API_PARAMETERS)
+                _parameters['temperature'] = 0.001
+                _parameters['presence_penalty'] = 1.2
+                passage = "輸入文字內容:\n\n\"\"\"\n\n{0}\n\n\"\"\"\n\n".format(summary_repository[k]['text'])
+                conversation = [
+                    {
+                        "role": "system",
+                        "content": system_prompt
+                    },
+                    {
+                        "role": "user",
+                        "content": passage
+                    }
+                ]
+                payload = summaryChatGpt.parameters2payload(summaryChatGpt.API_MODEL, conversation, _parameters, stream=False)
+                response = requests.post(summaryChatGpt.BASE_URL, headers=summaryChatGpt.API_HEADERS, json=payload, stream=False)
+                return_values[k]=json.loads(response.content.decode())['choices'][0]["message"]
+
+        all_summary=aggregate_summary(return_values)
+        is_final_stage = True
+        keep_summary = False
+        available_tokens = summaryChatGpt.MAX_TOKENS - this_final_tokens - 4 - 2
+        this_summary_tokens = 0
+        this_available_tokens = (available_tokens) // 2 - 100
+        large_inputs = copy.deepcopy(all_summary)
+        keep_summary = True
+        while keep_summary:
+            partial_words, large_inputs = split_summary(large_inputs, this_available_tokens)
+            remain_tokens = builtins.sum([estimate_used_tokens(w) + 1 for w in large_inputs])
+            print('partial_words:{0} large_inputs:{1}'.format(builtins.sum([estimate_used_tokens(w) + 1 for w in partial_words]), remain_tokens))
+            if len(large_inputs) == 0:
                 keep_summary = False
-                break
+            passage_final = "摘要清單:\n\n\"\"\"\n\n{0}\n\n\"\"\"\n\n標號起始數字:{1}\n".format('\n'.join(partial_words),get_last_ordered_index(cleansed_summary))
 
             conversation = [
                 {
                     "role": "system",
-                    "content": _system_prompt
+                    "content":  _final_prompt
                 },
                 {
                     "role": "user",
-                    "content": "#zh-tw \n累積摘要清單: \"\"\"\n{0}\n\"\"\"\n\n輸入文字內容:\"\"\"\n{1}\n\"\"\"".format(
-                        summary_history, '\n'.join(partial_words))
+                    "content":  passage_final
                 }
             ]
             print(conversation)
-            streaming_answer = baseChatGpt.post_and_get_streaming_answer(conversation, _parameters, full_history)
+            streaming_answer = summaryChatGpt.post_and_get_streaming_answer(conversation, _parameters, full_history)
             answer = ''
-            answer_head = '[第{0}部分摘要]\n\r'.format(cnt + 1)
+            answer_head = """  \n## 最終版摘要  \n{0} \n"""
             while True:
                 try:
                     answer, full_history = next(streaming_answer)
-                    yield answer_head + answer, full_history
+                    yield answer_head.format(text2markdown('\n'.join(cleansed_summary) if len(cleansed_summary) > 0 else ''+ '  \n' + '  \n'.join(get_rolling_summary_results(answer))))   , full_history
                 except StopIteration:
                     break
-            print(answer_head)
-            print(answer)
+            print(answer_head.format(answer))
             print('\n\n')
-            summary_history = answer
-            available_tokens = int(
-                (baseChatGpt.MAX_TOKENS - 200 - estimate_used_tokens(answer) - this_system_tokens - 4 - 2) * 0.75)
-            cnt += 1
-            yield summary_history, full_history
 
+            cleansed_summary.extend(get_rolling_summary_results(answer))
+            this_available_tokens = (available_tokens) // 2 - 100
+            yield answer_head.format(text2markdown('\n'.join(cleansed_summary))), full_history
+
+    if '心智圖' in summary_options:
+        _system_prompt =open("prompt4all/prompts/mindmap_summary.md", encoding="utf-8").read()
+
+        base_summary=copy.deepcopy(cleansed_summary)
+        keep_summary=True
+        this_system_tokens = estimate_used_tokens(_system_prompt) + estimate_used_tokens('system',model_name=summaryChatGpt.API_MODEL) + 4
+        available_tokens = summaryChatGpt.MAX_TOKENS - this_system_tokens - 4 - 2
+        this_summary_tokens = estimate_used_tokens(mindmap_history)
+        this_available_tokens = (available_tokens - 2 * this_summary_tokens) * 0.4 - 100
+        large_inputs=base_summary
+        partial_words = ''
+
+        cnt = 0
+        try:
+            while keep_summary:
+                this_system_tokens = estimate_used_tokens(str(mindmap_history))
+                this_available_tokens = (available_tokens - this_system_tokens) - 100
+                # get tokens
+
+                partial_words, large_inputs = get_next_paragraph(large_inputs, this_available_tokens)
+                print('partial_words:{0} large_inputs:{1}'.format(len(''.join(partial_words)),len(''.join(large_inputs))))
+
+                passage="摘要心智圖:\n\n{0}\n\n摘要清單:\n\n{1}\n\n".format(mindmap_history,'\n'.join(partial_words))
+
+                conversation = [
+                    {
+                        "role": "system",
+                        "content": _system_prompt
+                    },
+                    {
+                        "role": "user",
+                        "content": passage
+                    }
+                ]
+                print(conversation)
+                streaming_answer = summaryChatGpt.post_and_get_streaming_answer(conversation, _parameters, full_history)
+                answer = ''
+
+                while True:
+                    try:
+                        answer, full_history = next(streaming_answer)
+                    except StopIteration:
+                        break
+                print(mindmap_head)
+                print(answer)
+                print('\n\n')
+                if len(large_inputs) == 0 :
+                    keep_summary = False
+
+                mindmap_history = answer
+                available_tokens = int((summaryChatGpt.MAX_TOKENS - 200 - estimate_used_tokens(answer) - this_system_tokens - 4 - 2) * 0.667)
+                cnt += 1
+
+        except Exception as e:
+            raise gr.Error(str(e))
+        yield answer_head.format(text2markdown('\n'.join(cleansed_summary)))+'\n\n\n'+mindmap_head+'\n'+mindmap_history,full_history
+
+
+    if '會議記錄' in summary_options:
+        _system_prompt = open("prompt4all/prompts/meeting_minutes_summary.md", encoding="utf-8").read()
+        base_summary = copy.deepcopy(cleansed_summary)
+        keep_summary = True
+        this_system_tokens = estimate_used_tokens(_system_prompt) + estimate_used_tokens('system',
+                                                                                         model_name=summaryChatGpt.API_MODEL) + 4
+        this_final_tokens = estimate_used_tokens(_final_prompt) + estimate_used_tokens('system',
+                                                                                       model_name=summaryChatGpt.API_MODEL) + 4
+        available_tokens = summaryChatGpt.MAX_TOKENS - this_system_tokens - 4 - 2
+        this_summary_tokens = estimate_used_tokens(meeting_minutes)
+        this_available_tokens = (available_tokens - 2 * this_summary_tokens) * 0.667 - 100
+        large_inputs = base_summary
+        partial_words = ''
+        unchanged_summary = []
+        cnt = 0
+        try:
+            while keep_summary:
+                this_system_tokens = estimate_used_tokens(str(meeting_minutes))
+                this_available_tokens = (available_tokens - this_system_tokens) - 100
+                # get tokens
+
+
+                partial_words, large_inputs = get_next_paragraph(large_inputs, this_available_tokens)
+                print('partial_words:{0} large_inputs:{1}'.format(len(''.join(partial_words)),len(''.join(large_inputs))))
+
+                passage = "會議記錄重點:\n\n{0}\n\n摘要清單:\n\n{1}\n\n".format(meeting_minutes, '\n'.join(partial_words))
+
+                conversation = [
+                    {
+                        "role": "system",
+                        "content": _system_prompt
+                    },
+                    {
+                        "role": "user",
+                        "content": passage
+                    }
+                ]
+                print(conversation)
+                streaming_answer = summaryChatGpt.post_and_get_streaming_answer(conversation, _parameters, full_history)
+                answer = ''
+                meeting_head = '# 會議記錄'
+                while True:
+                    try:
+                        answer, full_history = next(streaming_answer)
+                    except StopIteration:
+                        break
+                print(meeting_head)
+                print(answer)
+                print('\n\n')
+                if len(large_inputs) == 0:
+                    keep_summary = False
+
+                meeting_minutes = answer
+                available_tokens = int((summaryChatGpt.MAX_TOKENS - 200 - estimate_used_tokens(
+                    answer) - this_system_tokens - 4 - 2) * 0.667)
+                cnt += 1
+
+        except Exception as e:
+            raise gr.Error(str(e))
+        yield answer_head.format(text2markdown('\n'.join(cleansed_summary)))+'\n\n\n'+mindmap_head+'\n'+mindmap_history+'\n\n\n'+meeting_head+'\n'+meeting_minutes,full_history
+
+    if '重點主題' in summary_options:
+        _system_prompt =open("prompt4all/prompts/topic_driven_summary.md", encoding="utf-8").read()
+        base_summary = copy.deepcopy(cleansed_summary)
+        keep_summary = True
+        this_system_tokens = estimate_used_tokens(_system_prompt) + estimate_used_tokens('system',model_name=summaryChatGpt.API_MODEL) + 4
+        this_final_tokens = estimate_used_tokens(_final_prompt) + estimate_used_tokens('system',model_name=summaryChatGpt.API_MODEL) + 4
+        available_tokens = summaryChatGpt.MAX_TOKENS - this_system_tokens - 4 - 2
+        this_summary_tokens = estimate_used_tokens(topic_shortcuts)
+        this_available_tokens = (available_tokens - 2 * this_summary_tokens) * 0.667 - 100
+        large_inputs = base_summary
+        partial_words = ''
+
+        cnt = 0
+        try:
+            while keep_summary:
+                this_system_tokens = estimate_used_tokens(str(topic_shortcuts))
+                this_available_tokens = (available_tokens - this_system_tokens) - 100
+                # get tokens
+
+
+                partial_words, large_inputs = get_next_paragraph(large_inputs, this_available_tokens)
+                print('partial_words:{0} large_inputs:{1}'.format(len(''.join(partial_words)),len(''.join(large_inputs))))
+
+                passage = "重點主題:\n\n{0}\n\n摘要清單:\n\n{1}\n\n".format(topic_shortcuts, '\n'.join(partial_words))
+
+                conversation = [
+                    {
+                        "role": "system",
+                        "content": _system_prompt
+                    },
+                    {
+                        "role": "user",
+                        "content": passage
+                    }
+                ]
+                print(conversation)
+                streaming_answer = summaryChatGpt.post_and_get_streaming_answer(conversation, _parameters, full_history)
+                answer = ''
+
+                while True:
+                    try:
+                        answer, full_history = next(streaming_answer)
+                    except StopIteration:
+                        break
+                print(topic_head)
+                print(answer)
+                print('\n\n')
+                if len(large_inputs) == 0:
+                    keep_summary = False
+                topic_shortcuts = answer
+                available_tokens = int((summaryChatGpt.MAX_TOKENS - 200 - estimate_used_tokens( topic_shortcuts) - this_system_tokens - 4 - 2) * 0.667)
+                cnt += 1
+
+        except Exception as e:
+            raise gr.Error(str(e))
+        yield answer_head.format(text2markdown('\n'.join(cleansed_summary))) + '\n\n\n' + mindmap_history, full_history
 
 
 def estimate_tokens(text,text2,state):
-    t1= '輸入文本長度為{0},預計耗用tokens數為:{1}'.format(len(text),estimate_used_tokens(text,baseChatGpt.API_MODEL)+4)
+    text='' if text is None else text
+    text2 = '' if text2 is None else text2
+    t1= '輸入文本長度為{0},預計耗用tokens數為:{1}'.format(len(text),estimate_used_tokens(text,summaryChatGpt.API_MODEL)+4)
     if len(text2)==0:
         return t1, state
     else:
-        t2='輸出文本長度為{0},預計耗用tokens數為:{1}'.format(len(text2),estimate_used_tokens(text2,baseChatGpt.API_MODEL)+4)
+        t2='輸出文本長度為{0},預計耗用tokens數為:{1}'.format(len(text2),estimate_used_tokens(text2,summaryChatGpt.API_MODEL)+4)
         return t1+'\t\t'+t2, state
 
 
+def reformat_freq(sr, y):
+    if sr not in (
+        48000,
+        16000,
+    ):  # Deepspeech only supports 16k, (we convert 48k -> 16k)
+        raise ValueError("Unsupported rate", sr)
+    if sr == 48000:
+        y = (
+            ((y / max(np.max(y), 1)) * 32767)
+            .reshape((-1, 3))
+            .mean(axis=1)
+            .astype("int16")
+        )
+        sr = 16000
+    return sr, y
+
+
+def transcribe(audio, need_timestamp=False,state=None):
+    # if audio == None : return ""
+    time.sleep(2)
+    print(datetime.now(),audio)
+
+    #_, y = reformat_freq(*audio)
+    # phrase_complete=True
+    # if state is None:
+    #     state=[]
+    # if len(state)==0:
+    #     state.append(OrderedDict())
+    #     state[0]['phrase_time']=  None
+    #     state[0]['last_sample'] = bytes()
+    #
+    #     state[0]['data_queue'] = Queue()
+    #     state[0]['phrase_complete']=False
+    # now = datetime.utcnow()
+    # Pull raw recorded audio from the queue.
+    # if not state[0]['data_queue'].empty():
+    #     state[0]['phrase_complete'] = False
+    #     # If enough time has passed between recordings, consider the phrase complete.
+    #     # Clear the current working audio buffer to start over with the new data.
+    #     if state[0]['phrase_time'] and now - state[0]['phrase_time'] > timedelta(seconds=phrase_timeout):
+    #         state[0]['last_sample']  = bytes()
+    #         state[0]['phrase_complete'] = True
+    #     # This is the last time we received new audio data from the queue.
+    #     state[0]['phrase_time']  = now
+
+        # Concatenate our current audio data with the latest audio data.
+        # while not state[0]['data_queue'].empty():
+        #     data = state[0]['data_queue'].get()
+        #     state[0]['last_sample'] += data
+
+    # while True:
+    try:
+        results = recognize_whisper(audio_data=audio,word_timestamps=need_timestamp)
+        state.append(results)
+        if len(state[-1]['text'] if len(state)>0 else '')>0:
+            print(state[-1]['text'] if len(state)>0 else '')
+
+        return  '\n'.join([result['text'] for result in state if len(result['text']) > 0]) if len(state) > 0 else '',state
+
+    except KeyboardInterrupt:
+        return '\n'.join([result['text'] for result in state  if len(result['text']) > 0]) if len(state) > 0 else '',state
+
+def update_rolling_state(state):
+    return '\n'.join([result['text'] for result in state  if len(result['text']) > 0]) if len(state) > 0 else '',state
+def SpeechToText(audio,need_timestamp=False,state=None):
+    if audio == None : return ""
+    time.sleep(1)
+
+    audio = whisper.load_audio(audio)
+    audio = whisper.pad_or_trim(audio)
+
+    # make log-Mel spectrogram and move to the same device as the model
+    mel = whisper.log_mel_spectrogram(audio).to(model.device)
+
+    # Detect the Max probability of language ?
+    _, probs = model.detect_language(mel)
+    language = max(probs, key=probs.get)
+
+    #  Decode audio to Text
+    options = whisper.DecodingOptions(fp16 = False)
+    result = whisper.decode(model, mel, options)
+    return (language , result.text)
 def process_file(file,state):
     if file is None:
         return '', state
-    elif file.name.lower().endswith('.pdf'):
-        doc_map=get_document_text(file.name)
-        return_text=''
-        for pg,offset,text in doc_map:
-            return_text+=text+'\n'
-            return_text += 'page {0}'.format(pg+1) + '\n''\n'
-        return  return_text ,state
     else:
-        with open(file.name, encoding="utf-8") as f:
-          content = f.read()
-          print(content)
-        return content,state
+        folder, filename, ext = context.split_path(file.name)
+        if file.name.lower().endswith('.pdf'):
+            doc_map=get_document_text(file.name)
+            return_text=''
+            for pg,offset,text in doc_map:
+                return_text+=text+'\n'
+                return_text += 'page {0}'.format(pg+1) + '\n''\n'
+            yield  return_text ,state
+        else:
+            with open(file.name, encoding="utf-8") as f:
+              content = f.read()
+              print(content)
+            yield content,state
+
+def process_audio_file(file,state,initial_prompt,need_timestamp=False):
+
+    if file is None:
+        return '', state
+    else:
+        folder, filename, ext = context.split_path(file.name)
+        transcript = ""
+        chunk_start=0
+        if ext.lower() in ['.mp4','.avi']:
+            import moviepy.editor
+            video = moviepy.editor.VideoFileClip(file.name)
+            audio = video.audio
+            context.make_dir_if_need(os.path.join(cxt.get_prompt4all_dir(),'audio',filename+'.wav'))
+            audio.write_audiofile(os.path.join(cxt.get_prompt4all_dir(),'audio',filename+'.wav'))
+            audio_file = AudioSegment.from_wav(os.path.join(cxt.get_prompt4all_dir(),'audio',filename+'.wav'))
+        elif ext.lower() in ['.mp3']:
+            audio_file = AudioSegment.from_mp3(file.name)
+        elif ext.lower() in ['.wav']:
+            audio_file = AudioSegment.from_wav(file.name)
+        load_whisper_model()
+        # audio_samples = np.array(audio_file.get_array_of_samples() )   # 獲取採樣點數據陣列
+        # audio_samples = audio_samples.reshape( (-1, audio_file.channels))
+        # rms = np.sqrt(np.mean(audio_samples ** 2, axis=-1))
+        #
+        # ref = 2 ** (8 * audio_file.sample_width - 1)  # 計算參考值
+        # dBFS = 20 * np.log10(np.abs(samples) / ref)  # 計算每個採樣點的分貝數
+
+        chunk_size = 100 * 1000  # 100 秒
+        chunks = [audio_file[i:i + chunk_size] for i in range(0, len(audio_file), chunk_size)]
+        for chunk in chunks:
+            dbfs=chunk.dBFS
+            if dbfs==-math.inf or dbfs<-30:
+                chunk_start += chunk.duration_seconds
+                pass
+            else:
+                with chunk.export("temp.wav", format="wav") as f:
+                    result = cxt.whisper_model.transcribe("temp.wav", word_timestamps=need_timestamp,verbose=False, language="zh", fp16=False,
+                                                          no_speech_threshold=0.5,logprob_threshold=-1, temperature=0.2,initial_prompt="#zh-tw 會議逐字稿。"+initial_prompt)
+
+                    for seg in result["segments"]:
+                        if need_timestamp:
+                            start, end, text = seg["start"]+chunk_start, seg["end"]+chunk_start, seg["text"]
+                            if len(text)==0:
+                                pass
+                            else:
+                                line = f"[{to_formated_time(start)} --> {to_formated_time(end)} {text}"
+                                print(line,flush=True)
+                                transcript += line + '\n'
+                        else:
+                            if len(seg['text']) == 0:
+                                pass
+                            else:
+                                print('{0}'.format(seg['text']),flush=True)
+                                transcript += '{0}'.format(seg['text']) + '\n'
+
+                    chunk_start+=chunk.duration_seconds
+            yield transcript, state
+        yield transcript, state
+
+
+
+
 
 
 
 def clear_history():
     FULL_HISTORY = [{"role": "system", "content": baseChatGpt.SYSTEM_MESSAGE,
                      "estimate_tokens": estimate_used_tokens(baseChatGpt.SYSTEM_MESSAGE,
                                                              model_name=baseChatGpt.API_MODEL)}]
@@ -326,50 +940,66 @@
     title = """<h1 align="center">🔥🤖Prompt is All You Need! 🚀</h1>"""
     if "OPENAI_API_KEY" not in os.environ:
         title = """<h1 align="center">🔥🤖Prompt is All You Need! 🚀</h1><br><h2 align="center"><span style='color:red'>你尚未設置api key</span></h2>"""
     description = ""
     cancel_handles = []
     with gr.Blocks(title="Prompt is what you need!", css=advanced_css, analytics_enabled=False,
                    theme=adjust_theme()) as demo:
-        baseChatGpt = GptBaseApi(model="gpt-3.5-turbo")
+        baseChatGpt = GptBaseApi(model="gpt-3.5-turbo-0613")
+        summaryChatGpt = GptBaseApi(model="gpt-3.5-turbo-0613")
+        otherChatGpt = GptBaseApi(model="gpt-3.5-turbo-0613")
         state = gr.State([{"role": "system", "content": '所有內容以繁體中文書寫',
                            "estimate_tokens": estimate_used_tokens('所有內容以繁體中文書寫',
                                                                    model_name=baseChatGpt.API_MODEL)}])  # s
 
         baseChatGpt.FULL_HISTORY = state
         gr.HTML(title)
 
         with gr.Tabs():
             with gr.TabItem("對話"):
                 with gr.Row():
+                    with gr.Tabs():
+                        with gr.TabItem("設定"):
+                            with gr.Column(scale=1):
+                                with gr.Row():
+                                    inputs = gr.Textbox(placeholder="你與語言模型Bert有何不同?",
+                                                        label="輸入文字後按enter", lines=10, max_lines=2000)  # t
+                                    context_type = gr.Dropdown(
+                                        ["[@PROMPT] 一般指令", "[@GLOBAL] 全局指令", "[@SKIP] 跳脫上文",
+                                         "[@SANDBOX] 沙箱隔絕",
+                                         "[@EXPLAIN] 解釋上文", "[@OVERRIDE] 覆寫全局"],
+                                        value="[@PROMPT] 一般指令", type='index', label="context處理",
+                                        elem_id='context_type',
+                                        interactive=True)
+                                with gr.Row():
+                                    b1 = gr.Button(value='送出')
+                                    with gr.Row():
+                                        b3 = gr.Button(value='清除')
+                                        b2 = gr.Button(value='中止')
+                                with gr.Accordion("超參數", open=False):
+                                    top_p = gr.Slider(minimum=-0, maximum=1.0, value=1, step=0.05, interactive=True,
+                                                      label="限制取樣範圍(Top-p)", )
+                                    temperature = gr.Slider(minimum=-0, maximum=2.0, value=0.9, step=0.1,
+                                                            interactive=True,
+                                                            label="溫度 (Temperature)", )
+                                    top_k = gr.Slider(minimum=1, maximum=50, value=1, step=1, interactive=True,
+                                                      label="候選結果個數(Top-k)", )
+                                    frequency_penalty = gr.Slider(minimum=-2, maximum=2, value=0, step=0.01,
+                                                                  interactive=True,
+                                                                  label="重複性處罰(Frequency Penalty)",
+                                                                  info='值域為-2~+2，數值越大，對於重複用字會給予懲罰，數值越負，則鼓勵重複用字')
+                        with gr.TabItem("對話紀錄"):
+                            with gr.Row():
+                                with gr.Group():
+                                    conversation_history_delete_btm = gr.Button('刪除', scale=1,size='sm')
+                                    conversation_history_share_btm = gr.Button('分享', scale=1,size='sm')
+                            with gr.Row():
+                                gr.templates.List()
                     with gr.Column(scale=3.5,elem_id="col_container"):
                         chatbot = gr.Chatbot(elem_id='chatbot',container=True,scale=1,height=550)
-                    with gr.Column(scale=1):
-                        with gr.Row():
-                            inputs = gr.Textbox(placeholder="你與語言模型Bert有何不同?", label="輸入文字後按enter",lines=10,max_lines=2000)  # t
-                            context_type = gr.Dropdown(
-                                ["[@PROMPT] 一般指令", "[@GLOBAL] 全局指令", "[@SKIP] 跳脫上文", "[@SANDBOX] 沙箱隔絕",
-                                 "[@EXPLAIN] 解釋上文", "[@OVERRIDE] 覆寫全局"],
-                                value="[@PROMPT] 一般指令", type='index', label="context處理", elem_id='context_type',
-                                interactive=True)
-                        with gr.Row():
-                            b1 = gr.Button(value='送出')
-                            with gr.Row():
-                                b3 = gr.Button(value='🧹')
-                                b2 = gr.Button(value='⏹️')
-                        with gr.Accordion("超參數", open=False):
-                            top_p = gr.Slider(minimum=-0, maximum=1.0, value=1, step=0.05, interactive=True,
-                                              label="限制取樣範圍(Top-p)", )
-                            temperature = gr.Slider(minimum=-0, maximum=2.0, value=0.9, step=0.1, interactive=True,
-                                                    label="溫度 (Temperature)", )
-                            top_k = gr.Slider(minimum=1, maximum=50, value=1, step=1, interactive=True,
-                                              label="候選結果個數(Top-k)", )
-                            frequency_penalty = gr.Slider(minimum=-2, maximum=2, value=0, step=0.01, interactive=True,
-                                                          label="重複性處罰(Frequency Penalty)",
-                                                          info='值域為-2~+2，數值越大，對於重複用字會給予懲罰，數值越負，則鼓勵重複用字')
             with gr.TabItem("歷史"):
                 with gr.Column(elem_id="col_container"):
                     history_viewer = gr.JSON(elem_id='history_viewer')
             with gr.TabItem("NLU"):
                 with gr.Column(elem_id="col_container"):
                     gr.Markdown(
                         "將文本輸入到下面的方塊中，按下「送出」按鈕將文本連同上述的prompt發送至OpenAI ChatGPT API，然後將返回的JSON顯示在視覺化界面上。")
@@ -524,28 +1154,78 @@
                     with gr.Row():
                         with gr.Column(scale=1):
                             rewrite_inputs = gr.Textbox(lines=30, placeholder="輸入句子...")
                         with gr.Column(scale=1):
                             rewrite_output = gr.Text(label="改寫", interactive=True, lines=30,show_copy_button=True)
                     rewrite_button = gr.Button("送出")
             with gr.TabItem("長文本摘要"):
-                rolling_state = gr.State([])
-                with gr.Column(elem_id="col_container"):
-                    text_statistics=gr.Markdown()
-                    with gr.Row():
-                        rolliing_source_file=gr.File(file_count="single", file_types=["text", ".json", ".csv", ".pdf"])
-                        rolling_parallel_checkbox=gr.Checkbox(label="平行計算",value=True)
-                        rolling_button = gr.Button("送出")
-                    with gr.Row():
-                        with gr.Column(scale=1):
-                            large_inputs = gr.Textbox(label="來源文字", lines=30, max_lines=5000,
-                                                      placeholder="大量輸入...")
-                        with gr.Column(scale=1):
-                            summary_output = gr.Text(label="摘要", interactive=True, lines=30,
-                                                     max_lines=500,show_copy_button=True)
+                with gr.Tabs():
+                    with gr.TabItem("長文本處理"):
+                        rolling_state = gr.State([])
+                        text_statistics=gr.Markdown()
+                        with gr.Row():
+                            with gr.Column(scale=1.2):
+                                with gr.Row():
+                                    with gr.Tabs():
+                                        with gr.TabItem("文字"):
+                                            rolliing_source_file = gr.File(value=None, file_count="single",label='請將檔案拖曳至此或是點擊後上傳',
+                                                                           file_types=[".txt",".json", ".csv", ".pdf"], scale=2,
+                                                                           elem_id='rolling_file')
+                                        with gr.TabItem("影音"):
+                                            whisper_timestamp_checkbox1 = gr.Checkbox(label="附加時間戳", value=True,
+                                                                                     scale=1)
+                                            initial_prompt_textbox=gr.Textbox(placeholder="請輸入描述影音內容的初始prompt",label="初始prompt")
+                                            audio_source_file = gr.File(value=None, file_count="single",label='請將檔案拖曳至此或是點擊後上傳',
+                                                                           file_types=[".mp3",".mp4",".avi",".wav"], scale=2,
+                                                                           elem_id='rolling_file')
+
+                                        with gr.TabItem("即時whisper"):
+                                            with gr.Row():
+                                                whisper_state=gr.State([])
+                                                whisper_timestamp_checkbox=gr.Checkbox(label="附加時間戳",value=False,scale=1)
+                                                rolling_audio =gr.Button('🎙️',size='sm',)
+                                                invisible_whisper_text=gr.Text(visible=False)
+                                        with gr.TabItem("Arxiv"):
+                                            gr.Textbox(label="請輸入Arxiv完整網址或是論文編號")
+                                        with gr.TabItem("Youtube"):
+                                            gr.Textbox(label="請輸入Youtube影片完整網址")
+                                            gr.Radio(["字幕檔", "音檔轉文字"], label="信息來源")
+                            with gr.Column(scale=1):
+                                with gr.Box():
+                                    summary_radio = gr.Dropdown(
+                                        ["滾動式整合摘要", "滾動式累加摘要", "平行分塊摘要"], multiselect=False, label="摘要技術", type="index",
+                                        value="滾動式整合摘要", interactive=True,min_width=150)
+                                    summary_options=gr.CheckboxGroup(["心智圖", "會議記錄", "重點主題"], label="輔助功能")
+                                    rolling_button = gr.Button("▶️", size='sm', scale=1,min_width=80)
+                                    rolling_clear_button = gr.ClearButton([rolliing_source_file], value="🗑️",size='sm',scale=1,min_width=80)
+                                    rolling_cancel_button = gr.Button("⏹️", size='sm', scale=1,min_width=80)
+
+
+                        with gr.Row():
+                            with gr.Column(scale=1):
+                                large_inputs =gr.Text(label="來源文字", lines=30, max_lines=5000)
+                            with gr.Column(scale=1,elem_id="col_container"):
+                                summary_output = gr.Markdown(label="摘要", interactive=True,elem_classes='markdown',container=True)
+                            rolling_clear_button.add(large_inputs)
+                            rolling_clear_button.add(summary_output)
+                    with gr.TabItem("存檔"):
+                        with gr.Column(elem_id="col_container"):
+                            with gr.Row():
+                                file_obj = gr.File(label="摘要檔", file_types=[".md"], value=None, interactive=False, min_width=60, show_label=False)
+                                rolling_save_button = gr.Button("💾", size='sm', scale=1)
+                    with gr.TabItem("紀錄"):
+                        with gr.Column(elem_id="col_container"):
+                            rolling_history_viewer = gr.JSON(elem_id='rolling_history_viewer')
+            with gr.TabItem("設定"):
+                with gr.Column():
+                    dropdown_api1=gr.Dropdown(choices=[k for k in model_info.keys()],value="gpt-3.5-turbo-0613",label="對話使用之api",interactive=True)
+                    dropdown_api2=gr.Dropdown(choices=[k for k in model_info.keys()], value="gpt-3.5-turbo-0613",label="長文本摘要使用之api",interactive=True)
+                    dropdown_api3=gr.Dropdown(choices=[k for k in model_info.keys()], value="gpt-3.5-turbo-0613",label="其他功能使用之api",interactive=True)
+
+
 
 
         inputs_event = inputs.submit(prompt_api,
                                      [inputs, context_type, top_p, temperature, top_k, frequency_penalty, state],
                                      [chatbot, state, history_viewer])
         cancel_handles.append(inputs_event)
         inputs_event.then(reset_context, [], [context_type]).then(reset_textbox, [], [inputs])
@@ -561,18 +1241,59 @@
 
         image_text.submit(image_api, [image_text, image_size, temperature2], [image_prompt, image_gallery])
         image_btn.click(image_api, [image_text, image_size, temperature2], [image_prompt, image_gallery])
 
         rewrite_inputs.submit(rewrite_api, [rewrite_inputs, rewrite_dropdown], rewrite_output)
         rewrite_button.click(rewrite_api, [rewrite_inputs, rewrite_dropdown], rewrite_output)
 
-        rolling_button.click(rolling_summary, [large_inputs, rolling_state,rolling_parallel_checkbox], [summary_output, rolling_state]).then(estimate_tokens, [large_inputs,summary_output, rolling_state],[text_statistics,rolling_state])
-        large_inputs.submit(rolling_summary, [large_inputs, rolling_state,rolling_parallel_checkbox], [summary_output, rolling_state]).then(estimate_tokens, [large_inputs,summary_output, rolling_state],[text_statistics,rolling_state])
-        large_inputs.change(estimate_tokens, [large_inputs,summary_output, rolling_state],[text_statistics,rolling_state])
-        rolliing_source_file.change(process_file,[rolliing_source_file,rolling_state],[large_inputs,rolling_state])
+
+        rolling_cancel_handel=[]
+
+        rolling_inputs_event = rolling_button.click(rolling_summary, [large_inputs, rolling_state,summary_radio,summary_options], [summary_output, rolling_state]).then(estimate_tokens, [large_inputs,summary_output, rolling_state],[text_statistics,rolling_state])
+        #large_inputs.submit(rolling_summary.md, [large_inputs, rolling_state,rolling_parallel_checkbox], [summary_output, rolling_state]).then(estimate_tokens, [large_inputs,summary_output, rolling_state],[text_statistics,rolling_state])
+        large_inputs_change_event=large_inputs.change(estimate_tokens, [large_inputs,summary_output, rolling_state],[text_statistics,rolling_state])
+        source_file_change_event=rolliing_source_file.change(process_file,[rolliing_source_file,rolling_state],[large_inputs,rolling_state])
+        audio_file_change_event=audio_source_file.change(process_audio_file, [audio_source_file, whisper_state,initial_prompt_textbox,whisper_timestamp_checkbox1],[large_inputs, whisper_state])
+        rolling_cancel_handel.append(rolling_inputs_event)
+        rolling_cancel_handel.append(large_inputs_change_event)
+        rolling_cancel_handel.append(source_file_change_event)
+        rolling_cancel_handel.append(audio_file_change_event)
+        rolling_cancel_button.click(fn=None, inputs=None, outputs=None, cancels=rolling_cancel_handel)
+
+
+
+        def save_file(contents,state):
+            text_file = "generate_text/summary_{0}.txt".format(str(datetime.now()).replace(' ', '').replace(':', '').replace('-', '').replace('.', ''))
+            if summary_radio.value==0:
+                text_file = "generate_text/rolling_summary_{0}.txt".format(str(datetime.now()).replace(' ', '').replace(':', '').replace('-','').replace('.', ''))
+            elif summary_radio.value == 1:
+                text_file = "generate_text/incremental_rolling_summary_{0}.txt".format(str(datetime.now()).replace(' ', '').replace(':', '').replace('-', '').replace('.', ''))
+            elif summary_radio.value == 2:
+                text_file = "generate_text/parallel_summary_{0}.txt".format(str(datetime.now()).replace(' ', '').replace(':', '').replace('-', '').replace('.', ''))
+            elif summary_radio.value == 3:
+                text_file = "generate_text/mindmap_summary_{0}.txt".format(str(datetime.now()).replace(' ', '').replace(':', '').replace('-', '').replace('.', ''))
+            elif summary_radio.value ==4 :
+                text_file = "generate_text/meeting_summary_{0}.txt".format(str(datetime.now()).replace(' ', '').replace(':', '').replace('-', '').replace('.', ''))
+            elif summary_radio.value == 5:
+                text_file = "generate_text/topic_summary_{0}.txt".format(str(datetime.now()).replace(' ', '').replace(':', '').replace('-', '').replace('.', ''))
+
+            with open(text_file, 'w', encoding='utf-8') as f:
+                f.write(contents)
+            return text_file,state
+
+
+        rolling_save_button.click(save_file,[summary_output,rolling_state],[file_obj,rolling_state])
+
+        invisible_whisper_text.change(update_rolling_state,[whisper_state],[large_inputs,rolling_history_viewer])
+
+        dropdown_api1.change(lambda x:baseChatGpt.change_model(x),[dropdown_api1],[])
+        dropdown_api2.change(lambda x:summaryChatGpt.change_model(x),[dropdown_api2],[])
+        dropdown_api3.change(lambda x:otherChatGpt.change_model(x),[dropdown_api3],[])
+
+
 
         gr.Markdown(description)
 
 
         # gradio的inbrowser触发不太稳定，回滚代码到原始的浏览器打开函数
         def auto_opentab_delay():
             import threading, webbrowser, time
@@ -588,8 +1309,8 @@
                 else:
                     webbrowser.open_new_tab(f"http://localhost:{PORT}")
 
             threading.Thread(target=open, name="open-browser", daemon=True).start()
 
 
         auto_opentab_delay()
-        demo.queue(concurrency_count=3, api_open=False).launch(show_error=True, max_threads=200, share=True)
+        demo.queue(concurrency_count=5, api_open=False).launch(show_error=True, max_threads=200, share=True,server_port=PORT)
```

## prompt4all/theme.py

```diff
@@ -106,14 +106,15 @@
         set_theme = None
         print('gradio版本较旧, 不能自定义字体和颜色')
     return set_theme
 
 
 advanced_css = """
 .contain { display: flex; flex-direction: column; }
+#rolling_file {height: 75px;}
 #component-0 { height: 100%; }
 #chatbot { flex-grow: 1; }
 
 /* 设置表格的外边距为1em，内部单元格之间边框合并，空单元格显示. */
 .markdown-body table {
     margin: 1em 0;
     border-collapse: collapse;
```

## prompt4all/api/base_api.py

```diff
@@ -21,15 +21,18 @@
 import prompt4all.api.context_type as ContextType
 from prompt4all.utils.regex_utils import *
 
 from prompt4all.utils.chatgpt_utils import process_context, process_chat
 from prompt4all.utils.tokens_utils import num_tokens_from_history, estimate_used_tokens
 #from tiktoken import Tokenizer, TokenizerWrapper
 from typing import List, Dict, TypedDict
+from prompt4all import context
+from prompt4all.context import *
 
+cxt=context._context()
 
 __all__ = ["model_info", "GptBaseApi"]
 
 model_info = {
     # openai
     "gpt-3.5-turbo": {
         "endpoint": 'https://api.openai.com/v1/chat/completions',
@@ -37,14 +40,27 @@
     },
 
     "gpt-4": {
         "endpoint": 'https://api.openai.com/v1/chat/completions',
         "max_token": 8192
     },
 
+    "gpt-3.5-turbo-0613": {
+        "endpoint": 'https://api.openai.com/v1/chat/completions',
+        "max_token": 4096
+    },
+    "gpt-3.5-turbo-16k-0613": {
+        "endpoint": 'https://api.openai.com/v1/chat/completions',
+        "max_token": 16384
+    },
+
+    "gpt-4-0613": {
+        "endpoint": 'https://api.openai.com/v1/chat/completions',
+        "max_token": 8192
+    },
     "gpt-4-0314": {
         "endpoint": 'https://api.openai.com/v1/chat/completions',
         "max_token": 8192
     },
 
     "gpt-4-32k": {
         "endpoint": 'https://api.openai.com/v1/chat/completions',
@@ -109,38 +125,63 @@
 #
 #     def raw(self) -> MessageDict:
 #         return {"role": self.role, "content": self.content}
 #
 
 
 class GptBaseApi:
-    def __init__(self, model="gpt-3.5-turbo", temperature=0.5, system_message='所有內容以繁體中文書寫'):
+    def __init__(self, model="gpt-3.5-turbo-0613", temperature=0.5, system_message='所有內容以繁體中文書寫'):
         if model.startswith('azure '):
             self.API_MODEL = model.replace('azure ','')
             self.API_TYPE ='azure'
             openai.api_type='azure'
         else:
             self.API_MODEL = model
+            self.API_TYPE = 'openai'
+            openai.api_type = None
 
         self.BASE_URL = model_info[model]["endpoint"]
         self.MAX_TOKENS = model_info[model]["max_token"]
         self.BASE_IDENTITY = uuid.uuid4()
         self.API_KEY = os.getenv("OPENAI_API_KEY")
 
         self.API_HEADERS = {
             'Accept': 'text/event-stream',
+            'Accept-Language': 'zh-TW',
             "Content-Type": "application/json",
             "Authorization": f"Bearer {self.API_KEY}"
         }
         self.SYSTEM_MESSAGE = system_message
         self.API_PARAMETERS = {'top_p': 1, 'temperature': temperature, 'top_k': 1, 'presence_penalty': 0,
                                'frequency_penalty': 0, 'max_tokens': 2500}
         self.FULL_HISTORY = [{"role": "system", "content": self.SYSTEM_MESSAGE,
                               "estimate_tokens": estimate_used_tokens(self.SYSTEM_MESSAGE, model_name=self.API_MODEL)}]
 
+
+    def change_model(self,model="gpt-3.5-turbo-0613"):
+        need_change=True
+        if model.startswith('azure '):
+            if self.API_TYPE =='azure' and model.replace('azure ','')==self.API_MODEL:
+                need_change=False
+            else:
+                self.API_MODEL = model.replace('azure ','')
+                self.API_TYPE ='azure'
+                openai.api_type='azure'
+        else:
+            if self.API_TYPE =='openai' and model == self.API_MODEL:
+                need_change = False
+            else:
+                self.API_MODEL = model
+                self.API_TYPE = 'openai'
+                openai.api_type = None
+        if need_change:
+            self.BASE_URL = model_info[model]["endpoint"]
+            self.MAX_TOKENS = model_info[model]["max_token"]
+            self.API_KEY = os.getenv("OPENAI_API_KEY")
+
     def build_message(self, role, content):
         """
         Build a chat message with the given role and content.
 
         Args:
         role (str): The role of the message sender, e.g., "system", "user", or "assistant".
         content (str): The content of the message.
@@ -293,23 +334,24 @@
 
                     except Exception as e:
                         finish_reason = '[EXCEPTION]'
                         if len(partial_words) == 0:
                             pass
                         else:
                             full_history[-1]['exception'] = str(e)
-
+                        PrintException()
                     chat = [(process_chat(full_history[i]), process_chat(full_history[i + 1])) for i in
                             range(1, len(full_history) - 1, 2) if full_history[i]['role'] != 'system']
                     answer = full_history[-1]['content']
 
                     yield chat, answer, full_history
             except Exception as e:
                 finish_reason = '[EXCEPTION]'
                 print(e)
+                PrintException()
             # 檢查finish_reason是否為length
             while finish_reason != '[DONE]' and finish_reason != '[EXCEPTION]':
                 # 自動以user角色發出「繼續寫下去」的PROMPT
                 prompt = "繼續"
                 # 調用openai.ChatCompletion.create來生成機器人的回答
                 message_context,context_tokens = self.process_context(prompt, context_type, full_history)
                 payload = self.parameters2payload(self.API_MODEL, message_context, self.API_PARAMETERS)
@@ -423,14 +465,15 @@
                 except Exception as e:
                     finish_reason = '[EXCEPTION]'
                     if len(partial_words) == 0:
                         pass
                     else:
                         full_history[-1]['exception'] = str(e)
 
+
                 answer = full_history[-1]['content']
 
                 yield answer, full_history
 
             # 檢查finish_reason是否為length
             while finish_reason != '[DONE]' and finish_reason != '[EXCEPTION]':
                 # 自動以user角色發出「繼續寫下去」的PROMPT
@@ -474,14 +517,15 @@
             full_history[-1]["estimate_tokens"] = estimate_used_tokens(partial_words, model_name=self.API_MODEL)
             chat = [(process_chat(full_history[i]), process_chat(full_history[i + 1])) for i in
                     range(1, len(full_history) - 1, 2) if full_history[i]['role'] != 'system']
             answer = full_history[-1]['content']
             yield answer, full_history
         except Exception as e:
             print(e)
+            PrintException()
 
 
 
     async def summarize_text(self, text_input,timeout=120):
         """post 串流形式的對話
         :param text_input:
         :param timeout:
@@ -539,21 +583,22 @@
              message_context]) + 2
         payload = self.parameters2payload(self.API_MODEL, message_context, parameters,stream=False)
 
         response = requests.post(self.BASE_URL, headers=self.API_HEADERS, json=payload, stream=False)
 
         try:
             # 解析返回的JSON結果
-            tt=response.content.decode()
+
             this_choice = json.loads(response.content.decode())['choices'][0]
             # prompt_tokens =  response.json()["usage"]["usage"]['prompt_tokens']
             # completion_tokens = response.json()["usage"]["usage"]['completion_tokens']
             # estimate_tokens2 = estimate_used_tokens(answer) + 4
             return this_choice["message"]['content'].strip()
         except Exception as e:
+            PrintException()
             return str(response.json()) + "\n" + str(e)
 
 
     def generate_images(self, input_prompt, shorter_prompt=None, image_size=256):
         response = openai.Image.create(
             api_key=os.getenv("OPENAI_API_KEY"),
             prompt=input_prompt,
```

## prompt4all/utils/__init__.py

```diff
@@ -0,0 +1,2 @@
+00000000: 6672 6f6d 2070 726f 6d70 7434 616c 6c2e  from prompt4all.
+00000010: 7574 696c 7320 696d 706f 7274 202a       utils import *
```

## prompt4all/utils/chatgpt_utils.py

```diff
@@ -7,18 +7,21 @@
 import requests
 from pypdf import PdfReader, PdfWriter
 from PIL import Image
 from bs4 import BeautifulSoup
 from prompt4all.utils.tokens_utils import *
 from prompt4all.utils.regex_utils import *
 
-__all__ = ['process_chat','process_url','process_context','build_message','regular_txt_to_markdown','get_next_paragraph','aggregate_summary','get_document_text']
+__all__ = ['process_chat','process_url','process_context','build_message','regular_txt_to_markdown','get_next_paragraph','get_document_text']
+
+
+
 
 def regular_txt_to_markdown(text):
-    text = text.replace('\n', '\n\n')
+    text = text.replace('\n', '  \n')
     text = text.replace('\n\n\n', '\n\n')
     text = text.replace('\n\n\n', '\n\n')
     return text
 
 def process_chat(conversation_dict: dict):
     if conversation_dict['role'] == 'user':
         return '😲:\n' + regular_txt_to_markdown(conversation_dict['content']) + "\n"
@@ -222,30 +225,21 @@
 def get_next_paragraph(paragraphs, max_tokens):
     paragraphs_bk = list(copy.deepcopy(paragraphs))
 
     result = []
     current_tokens = 0
     for paragraph in paragraphs:
         tokens =estimate_used_tokens(paragraph)
-        if current_tokens + tokens + 1 > max_tokens:
+        if paragraph==paragraphs[-1]:
+            result.append(paragraph)
+            current_tokens += tokens + 1
+            paragraphs_bk.pop(0)
+        elif (current_tokens + tokens + 1 > max_tokens):
             break
         else:
             result.append(paragraph)
             current_tokens += tokens + 1
             paragraphs_bk.pop(0)
-    paragraphs=paragraphs_bk
     return result, paragraphs_bk
 
 
-def aggregate_summary(results):
-    aggs=[]
-    for  result in results:
-
-        if isinstance(result,dict):
-            if len(aggs) == 0:
-                aggs.append(result['content'].split('\n')[0])
-            aggs.extend([c for c in result['content'].split('\n') if c.startswith('-')])
-        elif isinstance(result,str):
-            if len(aggs) == 0:
-                aggs.append(result.split('\n')[0])
-            aggs.extend([c for c in result.split('\n') if c.startswith('-')])
-    return  '\n'.join(aggs)
+
```

## prompt4all/utils/regex_utils.py

```diff
@@ -1,18 +1,25 @@
 
 import regex
 
 __all__ = [
-    "choice_pattern","delta_pattern","json_pattern","replace_special_chars",'extract_score']
+    "choice_pattern","delta_pattern","json_pattern",'numbered_list_member_pattern','unordered_listitem_pattern',"replace_special_chars",'extract_score','triplequote_pattern','is_numbered_list_member','is_unordered_list_member','extract_numbered_list_member']
 choice_pattern =regex.compile(r'"choices":\s*\[(\{.*?\})\]')
 
 delta_pattern = regex.compile(r'"delta":\s*{"content":"([^"]*)"}')
 
 json_pattern = regex.compile(r'\{(?:[^{}]|(?R))*\}')
 
+triplequote_pattern=regex.compile(r"```(.*)```")
+
+unordered_listitem_pattern=regex.compile(r"\s*([-*+])\s+(.*)$")
+numbered_list_member_pattern=regex.compile(r'\s*(\d+(\.\d+)*\.?)(?=\s)')
+
+
+
 def replace_special_chars(input_str):
     # 匹配除了英文、數字、漢字以外的字符
     pattern = r"[^a-zA-Z0-9\u4e00-\u9fa5]"
 
     # 使用 "_" 替換匹配到的字符
     result = regex.sub(pattern, "_", input_str)
 
@@ -21,8 +28,66 @@
 
 def extract_score(text):
     pattern = r"(\d+)分"
     result = regex.search(pattern, text)
     if result:
         return int(result.group(1))
     else:
-        return None
+        return None
+
+def is_numbered_list_member(string):
+    return bool(regex.match(numbered_list_member_pattern, string))
+
+def is_unordered_list_member(string):
+    return bool(regex.match(unordered_listitem_pattern, string))
+
+def extract_numbered_list_member(string):
+    """
+
+    :param string:
+    :return:
+
+    Examples:
+        >>> print(extract_numbered_list_member("1. This is a numbered list member."))
+        1.
+        >>> print(extract_numbered_list_member("19. This is a numbered list member."))
+        19.
+        >>> print(extract_numbered_list_member("    1.3.1 This is a numbered list member."))
+        1.3.1
+        >>> print(extract_numbered_list_member("   1.2 This is a numbered list member."))
+        1.2
+        >>> print(extract_numbered_list_member("    1.2 This is a numbered list member."))
+        1.2
+        >>> print(extract_numbered_list_member("    1This is a numbered list member."))
+        None
+    """
+    match = regex.search(numbered_list_member_pattern, string)
+    if match:
+        return match.group(1)
+    else:
+        return ''
+
+def extract_unordered_list_member(string):
+    """
+
+    :param string:
+    :return:
+
+    Examples:
+        >>> print(extract_unordered_list_member("1. This is a numbered list member."))
+        <BLANKLINE>
+        >>> print(extract_unordered_list_member("- This is a numbered list member."))
+        -
+        >>> print(extract_unordered_list_member("    - This is a numbered list member."))
+        -
+        >>> print(extract_unordered_list_member("   -This is a numbered list member."))
+        -
+        >>> print(extract_unordered_list_member("    + This is a numbered list member."))
+        +
+        >>> print(extract_unordered_list_member("This is a numbered list member."))
+        <BLANKLINE>
+    """
+    match = regex.search(unordered_listitem_pattern, string)
+    if match:
+        return match.group(1)
+    else:
+        return ''
```

## Comparing `prompt4all-0.0.2.dist-info/LICENSE` & `prompt4all-0.0.4.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `prompt4all-0.0.2.dist-info/METADATA` & `prompt4all-0.0.4.dist-info/METADATA`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: prompt4all
-Version: 0.0.2
+Version: 0.0.4
 Summary: Prompt is all you need
 Download-URL: https://test.pypi.org/project/prompt4all
 Author: Allan Yiin
 Author-email: allanyiin.ai@gmail.com
 License: MIT
 Keywords: chatgpt,gpt4
 Classifier: Development Status :: 4 - Beta
@@ -16,33 +16,40 @@
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Topic :: Software Development :: Libraries
 Classifier: Topic :: Software Development :: Libraries :: Python Modules
 Requires-Python: >=3.7
 Description-Content-Type: text/markdown
 License-File: LICENSE
-Requires-Dist: gradio (>=3.35.0)
-Requires-Dist: regex (>=2022.10.31)
-Requires-Dist: openai (>=0.27.2)
-Requires-Dist: requests (>=2.28.1)
-Requires-Dist: pillow (>=9.4.0)
+Requires-Dist: asyncio (>=3.4.3)
 Requires-Dist: beautifulsoup4 (>=4.11.1)
+Requires-Dist: gradio (>=3.35.2)
+Requires-Dist: latex2mathml
+Requires-Dist: Markdown
+Requires-Dist: markdown2 (>=2.4.8)
+Requires-Dist: nest-asyncio
 Requires-Dist: numpy (>=1.23.5)
+Requires-Dist: openai (>=0.27.8)
 Requires-Dist: opencv-python (>=4.7.0.68)
-Requires-Dist: pypdf (>=3.7.1)
-Requires-Dist: markdown2 (>=2.4.8)
-Requires-Dist: tiktoken (>=0.3.3)
-Requires-Dist: openai-async
-Requires-Dist: asyncio (>=3.4.3)
-Requires-Dist: whisper (>=1.1.10)
+Requires-Dist: pillow (>=9.4.0)
 Requires-Dist: pytube (>=12.1.2)
 Requires-Dist: pydub (>=0.25.1)
-Requires-Dist: nest-asyncio
-Requires-Dist: Markdown
-Requires-Dist: latex2mathml
+Requires-Dist: pypdf (>=3.7.1)
+Requires-Dist: pandas
+Requires-Dist: regex (>=2022.10.31)
+Requires-Dist: requests (>=2.31.0)
+Requires-Dist: soundfile (>=0.7)
+Requires-Dist: tiktoken (>=0.3.3)
+Requires-Dist: moviepy
+Requires-Dist: pathlib (>=1.0.1)
+Requires-Dist: setuptools (>=67.8.0)
+Requires-Dist: torch (>=2.0.0)
+Requires-Dist: tqdm
+Requires-Dist: transformers
+Requires-Dist: ffmpeg-python (>=0.2.0)
 Provides-Extra: tests
 Requires-Dist: pytest ; extra == 'tests'
 Requires-Dist: pytest-pep8 ; extra == 'tests'
 Requires-Dist: pytest-xdist ; extra == 'tests'
 Requires-Dist: pytest-cov ; extra == 'tests'
 Requires-Dist: requests ; extra == 'tests'
 Requires-Dist: markdown ; extra == 'tests'
@@ -59,19 +66,21 @@
 * streaming 對話
 * 自動繼續(當因為字數因素中斷，會自動背景重發繼續，然後將回傳結果接在前面中斷的位置，使用者將完全沒有中斷的感覺，這是我開始這專案時最想完成的功能)
 * 歷史對話紀錄長文自動摘要(以縮減tokens數)
 * 超參數調整
 * ChatGPT inside的傳統自然語言處理任務
 * ChatGPT 與Dall.E2協作
 * 語氣改寫
+* 長文本滾動式摘要以及平行分塊摘要
 * ~~coming soon 會議錄音自動整理!!!!
 ![prompt](prompt4all/images/dark1.png)
 ![prompt](prompt4all/images/ui_2.png)
 ![prompt](prompt4all/images/dalle2_1.png)
 ![prompt](prompt4all/images/rewrite1.png)
+![prompt](prompt4all/images/blocks_summary.png)
 
 ### 目前這個專案正在實現: ###
 * 更彈性的上下文管理機制，目前預計會有
     * [@PROMPT] 一般指令
     * [@GLOBAL] 全局指令(以append形式累加)
     * [@SKIP] 跳脫上文(只要全局指令，無須上文)
     * [@SANDBOX] 沙箱隔絕(連全局指令都不需要)
@@ -95,15 +104,15 @@
 
     pip install prompt4all --upgrade
 
 
 請依照指示將金鑰值賦值至環境變數OPENAI_API_KEY
 執行以下語法即可啟動
 
-    python prompt4all.app
+    python -m prompt4all.app
 
 
 ### 金鑰值賦值方法: ###
 如果您是在windows平台:
 請在系統->進階系統設定->環境變數->使用者變數
 中點選「新增」，在變數名稱中輸入:“OPENAI_API_KEY”
 在值的部分輸入您的OpenAI API密鑰
```

